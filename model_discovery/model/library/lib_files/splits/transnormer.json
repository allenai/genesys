{
    "transnormer-0": "# The Devil in Linear Transformer \n\n${ }^{\\star}$ Zhen Qin ${ }^{1}$, ${ }^{\\text {X }}$ Xiaodong Han ${ }^{1}$, Weixuan Sun ${ }^{2,3}$, Dongxu Li ${ }^{2}$,<br>Lingpeng Kong ${ }^{4,5}$, Nick Barnes ${ }^{2},{ }^{\\boxtimes}$ Yiran Zhong ${ }^{4}$<br>${ }^{1}$ SenseTime Research, ${ }^{2}$ Australian National University,<br>${ }^{3}$ OPPO Research Institute, ${ }^{4}$ Shanghai AI Laboratory, ${ }^{5}$ The University of Hong Kong<br>https://github.com/OpenNLPLab/Transnormer\n\n\n#### Abstract\n\nLinear transformers aim to reduce the quadratic space-time complexity of vanilla transformers.",
    "transnormer-1": "However, they usually suffer from degraded performances on various tasks and corpora. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, TransNORMER, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at TRANSNORMER. ## 1 Introduction\n\nTransformer models show great performance on a wide range of natural language processing and computer vision tasks (Qin et al., 2022; Sun et al., 2022b; Cheng et al., 2022a,b; Zhou et al., 2022). One issue of the vanilla transformer model lies in\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_1e87ac89fa2bec7d4d6fg-01.jpg?height=441&width=780&top_left_y=750&top_left_x=1049)\n\nFigure 1: TransNORMER has smaller memory footprints (circle sizes) and produces clearly favorable speed ( $x$-axis) and overall scores ( $y$-axis), when evaluated on the challenging Long-Range Arena benchmark than the vanilla transformer and other competing methods. its quadratic space-time complexity with respect to the input length. Various prior works attempt to alleviate this inefficiency (Zaheer et al., 2020; Beltagy et al., 2020; Tay et al., 2020a; Kitaev et al., 2020; Child et al., 2019; Liu et al., 2022; Sun et al., 2022b). In this work, we focus on a particular subset of these methods, known as kernel-based linear transformers (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2020; Qin et al., 2022) considering their desirable linear space-time complexity. Despite their space-time efficiency, linear transformers are not always in favor for practical adoption, largely due to the degraded performance than the vanilla model. To address this issue, we take a close look at existing kernel-based linear transformers and identify $\\boldsymbol{t w o}$ deficiencies that lead to such a performance gap. Unbounded gradients. Most existing linear transformers inherit attention formulation from the vanilla transformer, which scales attention scores to ensure they are bounded within $[0,1]$. However, we theoretically show that such a scaling strategy\nrenders unbounded gradients for linear transformer models. As a result, the unbounded gradients empirically lead to unstable convergence as our preliminary experiments suggest. Attention dilution. Previous works (Titsias, 2016; Jang et al., 2016; Gao and Pavel, 2017; Qin et al., 2022; Sun et al., 2022b,a) suggest that in vanilla transformer, softmax attention maps tend to be local. In contrast, as shown in Fig 2, we observe that linear transformers often trivially distribute attention scores over the entire sequence even in early layers. Due to this issue, which we refer as attention dilution, important local information is less well preserved in linear models, resulting in inferior performance. This negative impact of attention dilution is also evidenced by the performance drop in our controlled experiments if partly replacing vanilla attention in transformer layers with linear attention ones. To mitigate these issues, we propose a linear transformer model, called TrANSNORMER, which shows better performance than vanilla transformer on a wide range of task while being significantly faster during runtime, as shown in Fig. 1. To avoid the unbounded gradients, we introduce NORMATTENTION, which gets rid of scaling over attention matrices while appending an additional normalization only after the attention layer. The choice of the normalization operator is unrestricted, for example, LayerNorm (Ba et al., 2016) or RMSNorm (Zhang and Sennrich, 2019) both serve the purpose. We show empirical results demonstrating that with Normattention, the gradients are more stable during training, which in turn leads to more consistent convergence. To alleviate the attention dilution issue, we modify the vanilla attention and allow each token to only attend to its neighbouring tokens, resulting in a diagonal attention. To mimic the behaviors on local semantics of the vanilla transformer, we employ the diagonal attention on early layers while using NormAttention for later ones. In this way, we encourage the model to capture both local and global language context. Note that our diagonal attention can be efficiently computed such that the overall linear space-time complexity of TRANSNORMER is preserved. We perform extensive experiments on standard tasks, where TransNORmER demonstrates lower language modeling perplexities on WikiText-103 and overall higher text classification accuracy on\nGLUE than vanilla model and other competing methods. In addition, on the challenging LongRange Arena benchmark, TransNormer also shows favorable results while being faster and more scalable with longer inputs during both training and inference time. ## 2 Background and related work\n\nWe first briefly review vanilla transformer (Vaswani et al., 2017) and its efficient variants. The key component of transformers is the self-attention, which operates on query $\\mathbf{Q}$, key $\\mathbf{K}$ and value $\\mathbf{V}$ matrices; each of them is the image of a linear projection taking $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ as input:\n\n$$\n\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{Q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{K}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{V} \\in \\mathbb{R}^{n \\times d}\n$$\n\nwith $n$ the input length, $d$ the hidden dimension. The output $\\mathbf{O} \\in \\mathbb{R}^{n \\times d}$ is formulated as:\n\n$$\n\\mathbf{O}=\\operatorname{Softmax}\\left(\\mathbf{Q K}^{\\boldsymbol{\\top}} / \\sqrt{d}\\right) \\mathbf{V}\n$$\n\nwhere the $\\operatorname{Softmax}(\\cdot)$ step renders quadratic spacetime complexity with respect to the input length, making it prohibitive for vanilla transformer to scale to long input sequences. To address this issue, numerous efficient transformers have been explored in the literature. These methods can be generally categorized into two families, i.e., pattern based methods and kernel based methods. Pattern based methods (Zaheer et al., 2020; Beltagy et al., 2020; Tay et al., 2020a; Kitaev et al., 2020; Child et al., 2019) sparsify the attention calculation with handcrafted or learnable masking patterns. Kernel-based methods adopt kernel functions to decompose softmax attention, which reduces the theoretical space-time complexity to linear. In this paper, we refer the kernel-based variants as linear transformers for simplicity. In the kernel-based methods (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2020; Qin et al., 2022; Zheng et al., 2022; Wang et al., 2020), a kernel function $\\phi(\\cdot)$ maps queries and keys to their hidden representations. Then the output of the linear attention can be rewritten as:\n\n$$\n\\begin{aligned}\n\\mathbf{O} & =\\boldsymbol{\\Delta}^{-1} \\phi(\\mathbf{Q})\\left[\\phi(\\mathbf{K})^{\\top} \\mathbf{V}\\right] \\\\\n\\boldsymbol{\\Delta} & =\\operatorname{diag}\\left(\\phi(\\mathbf{Q})\\left[\\phi(\\mathbf{K})^{\\top} \\mathbf{1}_{n}\\right]\\right)\n\\end{aligned}\n$$\n\nwhere the product of keys and values are computed to avoid the quadratic $n \\times n$ matrix.",
    "transnormer-2": "Existing methods mainly differ in the design of kernel functions. For example, Choromanski et al. (2020)\nand Katharopoulos et al. (2020) adopt activation function $1+e l u$ to process query and key. Wang et al. (2020) assumes attention matrices are lowrank. Peng et al. (2020) and Zheng et al. (2022) approximate softmax under constrained theoretical bounds. Qin et al. (2022) propose a linear alternative to the attention based on empirical properties of the softmax function. These methods focus on either approximating or altering the softmax operator while preserving its properties. Compared with the vanilla transformer, these methods often trade performance for efficiency, usually resulting in worse task performance. In this paper, we argue that there are two essential reasons leading to such a performance gap, discussed in detail as follows. ## 3 The devil in linear attention\n\nIn this section, we motivate the design principles of TRANSNORMER by providing theoretical evidence for the unbounded gradients, and empirical results showing the adverse influence of attention dilution. ### 3.1 Unbounded gradients\n\nFew work on linear transformers analyzes their gradients during training. Our first key observation is that kernel-based linear attention suffer from unbounded gradients, causing unstable convergence during training. In the following, we highlight the main theoretical results while referring readers to Appendix D for the full derivation. Consider a self-attention module, either vanilla or linear attention. Its attention matrix $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ can be represented in the following unified form ${ }^{1}$ :\n\n$$\np_{i j}=\\frac{f\\left(s_{i j}\\right)}{\\sum_{k=1}^{n} f\\left(s_{i k}\\right)}, f: \\mathbb{R} \\rightarrow \\mathbb{R}\n$$\n\nVanilla and linear attention differ mainly in their computation of token-wise similarities $s_{i j}{ }^{2}$. In vanilla attention, $s_{i j}$ is computed as:\n\n$$\ns_{i j}=\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{j} / \\sqrt{d}, f(x)=\\exp (x)\n$$\n\nwhile for linear attentions, $s_{i j}$ can be decomposed using a kernel function $\\phi$, such that:\n\n$$\ns_{i j}=\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\phi\\left(\\mathbf{k}_{j}\\right), f(x)=x\n$$\n\n[^1]Given the above definitions, the gradients of the attention matrix $\\mathbf{P}$ is derived as:\n\n$$\n\\frac{\\partial p_{i j}}{\\partial s_{i k}}=\\frac{f^{\\prime}\\left(s_{i k}\\right)}{f\\left(s_{i k}\\right)}\\left(1_{j=k} p_{i j}-p_{i j} p_{i k}\\right)\n$$\n\nTherefore, for the vanilla attention, the partial derivative $\\frac{\\partial p_{i j}}{\\partial s_{i k}}$ is:\n\n$$\n\\begin{aligned}\nf^{\\prime}(x) & =\\exp (x)=f(x) \\\\\n\\frac{\\partial p_{i j}}{\\partial s_{i k}} & =1_{j=k} p_{i j}-p_{i j} p_{i k} \\\\\n& = \\begin{cases}p_{i k}-p_{i k} p_{i k} \\in[0,1 / 4] & j=k \\\\\n-p_{i j} p_{i k} \\in[-1 / 4,0] & j \\neq k\\end{cases}\n\\end{aligned}\n$$\n\nand it is bounded as:\n\n$$\n\\left|\\frac{\\partial p_{i j}}{\\partial s_{i k}}\\right| \\leq \\frac{1}{4}\n$$\n\nHowever, for linear attentions, we have:\n\n$$\n\\begin{aligned}\nf^{\\prime}(x) & =1 \\\\\n\\frac{\\partial p_{i j}}{\\partial s_{i k}} & =\\frac{1}{s_{i k}}\\left(1_{j=k} p_{i j}-p_{i j} p_{i k}\\right) \\\\\n& = \\begin{cases}\\frac{1}{s_{i k}}\\left(p_{i k}-p_{i k} p_{i k}\\right) & j=k \\\\\n\\frac{1}{s_{i k}}\\left(-p_{i j} p_{i k}\\right) & j \\neq k\\end{cases}\n\\end{aligned}\n$$\n\nand $^{3}$\n\n$$\n\\left|\\frac{\\partial p_{i j}}{\\partial s_{i k}}\\right| \\leq \\frac{1}{4\\left|s_{i k}\\right|}\n$$\n\nSince $\\left|s_{i k}\\right|^{-1}=\\left|\\phi\\left(\\mathbf{q}_{i}\\right) \\phi\\left(\\mathbf{k}_{j}\\right)^{\\mathrm{T}}\\right|^{-1}$ can be arbitrarily large, the gradient of linear attention has no upper bound. On the other hand, we can also show that the gradient of linear attention has no lower bound ${ }^{4}$ :\nProposition 3.1. $\\forall M>0$, there exists $\\mathbf{q}_{i}, \\mathbf{k}_{j} \\in$ $\\mathbb{R}^{d}, j=1, \\ldots, n$, such that:\n\n$$\n\\left|\\frac{\\partial p_{i j}}{\\partial s_{i k}}\\right|>M\n$$\n\nThe unbounded gradients lead to less stable optimization and worse convergence results in our preliminary studies. ### 3.2 Attention dilution\n\nIt is a known property of vanilla attention to emphasize on neighbouring tokens (Titsias, 2016; Qin et al., 2022). However, this property does not directly inherit to the linear transformer variants. To quantify the attention dilution issue, we introduce a metric called locally accumulated attention\n\n[^2]![](https://cdn.mathpix.com/cropped/2024_09_17_1e87ac89fa2bec7d4d6fg-04.jpg?height=382&width=1236&top_left_y=259&top_left_x=237)\n\nFigure 2: (a): Comparison of locally accumulated attention scores of different transformer variants. The x-axis denotes ratio of neighbourhood size relative to the input length; the $y$-axis denotes accumulated attention scores inside this neighbourhood for the centering token. The curve for the vanilla transformer model increases more sharply, indicating that the attention scores are more concentrated. Our model greatly alleviates the attention dilution issue for linear models. (b): Qualitative comparison of attention matrices in early model layers. The proposed TRANSNORMER produces more similar patterns to the original vanilla transformer, benefiting to better capture local-global language context, while the linear model suffers clearly from the issue of attention dilution and gets distracted by distant tokens in early layers. score, which measures how much attention scores are distributed within the local neighbourhood of a particular token. For an input sequence of length $N$, consider a local neighbourhood $\\left\\{x_{\\text {start }}, \\ldots, x_{i} \\ldots, x_{\\text {end }}\\right\\}$ centering around token $x_{i}$ of total length $r \\cdot N$, with $r$ the ratio relative to the total input, the locally accumulated attention score for token $x_{i}$ is defined as $l(i, r, N)=p_{i, \\text { start }}+\\ldots+p_{i, \\text { end }}$. A higher score indicates the particular attention layer concentrates on the local neighbourhood, while a lower score tends to indicate the issue of attention dilution, where scores are distributed more evenly to local and distant tokens. For example, $l(i, 0.4, N)=0.6$ means that that $40 \\%$ of the neighbors around $i$ 'th token contribute $60 \\%$ of the attention score. In Fig. 2 (a), we compare locally accumulated attention scores ( $y$-axis) for vanilla transformer and linear transformer, with varying sizes of neighbourhood by ratio (x-axis). We show the average score over each position across the entire sequence. It can be seen that the area under the vanilla model curve is significantly larger than that of the linear model. This provides evidence that the vanilla attention is more concentrated locally, while the linear transformer suffers from the issue of attention dilution. This is further qualitatively supported by Fig. 2 (b), where the attention maps for vanilla model are more concentrated than the linear model. ## 4 Method\n\nBased on the aforementioned observations, we propose a new linear transformer network called TRANSNORMER that addresses the above two lim- itations of current linear transformers.",
    "transnormer-3": "The overall architecture is shown in Fig. 3. ### 4.1 The overall architecture\n\nVanilla attention suffers less in attention dilution while linear attention is more efficient and scalable on longer sequences. This motivate us to design a method that exploits the best of the both worlds by using these mechanisms in combined. Specifically, our network consists of two types of attention: DiagAttention for the early stage of the model and NORMATTENTION for the later stage. The former addresses the attention dilution issue and the later aims to stabilize training gradients. Note that by properly reshaping the inputs, the diagonal attention can be efficiently computed in linear space-time, thus preserving the overall linear complexity. ### 4.2 NormAtTEntion\n\nTable 1: Ablation of linear attention with scaling operation.",
    "transnormer-4": "Directly removing scaling operation i.e., the denominator in Eq. 4, leads to significant performance drop. Our normalization strategy achieves better result. | method | $\\operatorname{ppl}(\\mathrm{val})$ |\n| :--- | :---: |\n| $1+$ elu | 4.98 |\n| $1+$ elu w/o scaling | 797.08 |\n| NORMATTENTION | 4.94 |\n\nAs proved in Sec.",
    "transnormer-5": "3, the scaling operation, i.e., the denominator in Eq. 4, in the linear transformers hinder the optimization due to the unbounded gradients. To solve this issue, we propose to remove the\nscaling operation in the linear transformers. However, as shown in Table. 1, directly removing the scaling operation leads to critical performance drop since the attention map becomes unbounded in the forward pass. Therefore, an alternative is required to bound both attention maps during forward and their gradients during backward passes in linear attentions. Our proposed solution is simple yet effective. Given a linear attention, the attention without scaling can be formulated as:\n\n$$\n\\mathbf{O}=\\mathbf{Q}\\left(\\mathbf{K}^{\\top} \\mathbf{V}\\right)\n$$\n\nWe empirically find that we can apply an arbitrary normalization on this attention to bound it, which leads to our NORMATTENTION as:\n\n$$\n\\mathbf{O}_{\\text {norm }}=\\operatorname{XNorm}\\left(\\mathbf{Q}\\left(\\mathbf{K}^{\\top} \\mathbf{V}\\right)\\right)\n$$\n\nwhere the XNorm can be Layernorm(Ba et al., 2016) or RMSNorm (Zhang and Sennrich, 2019) and etc. We use the RMSNorm in our experiments as it is slightly faster than other options. It can be proved that the gradients of NormAtTENTION is bounded by ${ }^{5}$ :\n\n$$\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| \\leq \\frac{3 c_{1} c_{2} d}{2 \\sqrt{\\epsilon}}<\\infty\n$$\n\nwhere $\\mathcal{L}$ is the loss function, $\\epsilon$ is the small constant that used in RMSNorm, $d$ is the embedding dimension and\n\n$$\n\\begin{aligned}\n& c_{1}=\\max _{i=1}^{n}\\left\\|\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right\\|_{2}<\\infty \\\\\n& c_{2}=\\max _{i=1}^{n}\\left\\|\\mathbf{V}_{i}\\right\\|_{2}<\\infty\n\\end{aligned}\n$$\n\nTo demonstrate the gradients stability of the NORMATTENTION, we compare the relative standard deviation of gradients during each training iterations to other linear transformers and vanilla transformer. Specifically, we train our model for 50k iterations with RoBERTa architecture on the WikiText103 (Merity et al., 2017) and obtain the relative standard deviation of all iterations' gradients. As shown in Table 2, existing linear methods (Choromanski et al., 2020; Katharopoulos et al., 2020) have substantially higher deviations compared to vanilla attention, which leads to inferior results. The NORmAtTEntion produces more stable gradients, which validates the effectiveness of our method. [^3]Table 2: Relative standard deviation of training gradients over 50k iterations. Our proposed NORMATTENTION provides more stable gradients which are closer to vanilla transformer. ![](https://cdn.mathpix.com/cropped/2024_09_17_1e87ac89fa2bec7d4d6fg-05.jpg?height=1169&width=784&top_left_y=432&top_left_x=1047)\n\nFigure 3: Architecture overview of the proposed TranSNORMER. In the early stages, we leverage DiAGATtENTION, where attention is only calculated inside the blocks to enforce neighbouring focus. In late stages, NORMATTENTION assists to obtain a more stable gradients in linear complexity. ### 4.3 DiAGATTENTION\n\nTo better understand the design principles, we show in Table 3 that by replacing partial layers of linear transformers with vanilla attention, the performance on language modeling is evidently improved. The results also suggest that capturing more local information in early layers are more helpful than otherwise. To this end, we leverage none-overlapped blockbased strategy to reduce the space-time complexity of the vanilla attention. Based on the observation in Fig. 2, we utilize a strict diagonal blocked pattern to constraint the attention in a certain range. Since the attentions are calculated inside each block, the computation complexity of our diagonal attention\n\nTable 3: Ablation on attention dilution issue. We implement all structures under the same setting: Vanilla (Vaswani et al., 2017), $1+$ elu (Katharopoulos et al., 2020). | Early layers | Later layers | ppl (val) |\n| :---: | :---: | :---: |\n| $1+e l u$ | $1+e l u$ | 4.98 |\n| $1+e l u$ | Vanilla | 3.90 |\n| Vanilla | $1+e l u$ | 3.76 |\n\nis $O(n w d)$, where $n$ is sequence length, $w$ is the block size and $d$ is feature dimension. When $d \\ll n$, the complexity scales linearly respect to the sequence length $n$. In subsequent sections, we use DiAGATTENTION to refer to Diagonal attention. We empirically find that applying DiAGAtTEnTION to the later stages of a model hurts the performance as shown in Table. 9. It indicates that the model requires a global field of view in the later layers, which also justifies our choices of NORMATTENTION in later layers of TRANSNORMER. ## 5 Experiments\n\nIn this section, we compare our method to other linear transformers and the vanilla transformer on autoregressive language modeling, bidirectional language modeling as well as the Long Range Arena benchmark (Tay et al., 2020b).",
    "transnormer-6": "We also provide an extensive ablation study to vindicate our choice in designing the TRANSNORMER. We validate our method on two variants of the TransNormer. The TransNormer T1 uses the ReLA attention (Zhang et al., 2021) in the DiAGATTENTION and the elu as the activation function in the NormAtTEntion. The TransNormer T2 uses the Softmax attention (Vaswani et al., 2017) in the DiAGAtTENTION and the $1+e l u$ as the activation function in the NORMAtTENTION. For experiments, we first study the autoregressive language modeling on WikiText-103 (Merity et al., 2017) in section 5.2. Then in section 5.2 we test our method on bidirectional language modeling, which is pre-trained on WikiText-103 (Merity et al., 2017) and then fine-tuned on several downstream tasks from the GLUE benchmark (Wang et al., 2018). Finally, we test TransNormer on the Long-Range Arena benchmark (Tay et al., 2020b) to evaluate its ability in modeling longrange dependencies and efficiencies in section 5.2. ### 5.1 Settings\n\nWe implement our models in the Fairseq framework (Ott et al., 2019) and train them on 8 V100 GPUS. We use the same training configuration for all competitors and we list detailed hyperparameters in Appendix F. We choose the FLASHquad, FLASH (Hua et al., 2022), Transformer-LS (Zhu et al., 2021), Performer (Choromanski et al., 2020), 1+elu (Katharopoulos et al., 2020) as our main competing methods. For the autoregressive language modeling, we use 6 decoder layers ( 10 layers for the FlASH/FLASH-quad) as our base model and all models are trained on the WikiText-103 dataset (Merity et al., 2017) for 100 K steps with a learning rate of 0.005 . We use the perplexity (PPL) as the evaluation metric. For the bidirectional language modeling, we choose the RoBERTa base (Liu et al., 2019) for all methods. It consists of 12 encoder layers ( 24 layers for the FLASH and FLASH-quad to match the number of parameters). All models are pretrained on the WikiText-103 (Merity et al., 2017) for 50 K steps with $\\mathrm{lr}=0.005$ and fine-tuned on the GLUE dataset (Wang et al., 2018). We use different learning rates among $1 \\mathrm{e}-5,3 \\mathrm{e}-5,6 \\mathrm{e}-5,1 \\mathrm{e}-4$ and choosing the best result after fine-tuning for 3 epochs. For the Long-Range Arena benchmark, to make sure it reflect the practical speed in Pytorch platform, we re-implement the benchmark in Pytorch. We adopt the same configuration from the Skyformer (Chen et al., 2021) and make sure all models have a similar parameter size. We use the same training hyper parameters for all models as well. Table 4: Quantitative results in autoregressive language modeling. The best result is highlighted with bold and the second with underlined. The smaller the better for the PPL metric. LS stands for transformerLS. | Method | PPL (val) | PPL (test) | Params (m) |\n| :--- | :---: | :---: | :---: |\n| Vanilla | $\\underline{29.63}$ | $\\mathbf{3 1 . 0 1}$ | 156.00 |\n| LS | 32.37 | 32.59 | 159.46 |\n| FLASH-quad | 31.88 | 33.50 | 153.51 |\n| FLASH | 33.18 | 34.63 | 153.52 |\n| 1+elu | 32.63 | 34.25 | 156.00 |\n| Performer | 75.29 | 77.65 | 156.00 |\n| TRANSNORMER T1 | 29.89 | $\\mathbf{3 1 . 3 5}$ | 155.99 |\n| TRANSNORMER T2 | $\\mathbf{2 9 . 5 7}$ | $\\mathbf{3 1 . 0 1}$ | 155.99 |\n\nTable 5: Quantitative results of the GLUE benchmark. MNLI is reported by the match/mismatch splits. MRPC is reported by F1 score. CoLA is reported by Matthews correlation coefficient. All the other tasks are measured by the accuracy. LS stands for transformer-LS. The best result is highlighted with bold and the second with underlined. The larger the better for all metrics. \"-\" means unconverged. | Method | MNLI | QNLI | QQP | SST-2 | MRPC | CoLA | AVG | Params (m) |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Vanilla | $79.37 / 79.07$ | 87.79 | 88.04 | 90.25 | 88.35 | 38.63 | $\\underline{78.79}$ | 124.70 |\n| FLASH-quad | $78.71 / 79.43$ | 86.36 | 88.95 | 90.94 | 81.73 | 41.28 | 78.20 | 127.11 |\n| FLASH | $79.45 / 80.08$ | 87.10 | 88.83 | 90.71 | 82.50 | 29.40 | 76.87 | 127.12 |\n| LS | $77.01 / 76.78$ | 84.86 | 86.85 | 90.25 | 82.65 | 40.65 | 77.01 | 128.28 |\n| Performer | $58.85 / 59.52$ | 63.44 | 79.10 | 81.42 | 82.11 | 19.41 | 63.41 | 124.70 |\n| 1+elu | $74.87 / 75.37$ | 82.59 | 86.9 | 87.27 | 83.03 | - | 70.00 | 124.0 |\n| TRANSNORMER T1 | $79.06 / 79.93$ | 87.00 | 88.61 | 91.17 | 84.50 | 45.38 | $\\mathbf{7 9 . 3 8}$ | 124.67 |\n| TRANSNORMER T2 | $77.28 / 78.53$ | 85.39 | 88.56 | 90.71 | 85.06 | 45.90 | 78.78 | 124.67 |\n\nTable 6: Quantitative results on the Long-Range Arena benchmark.",
    "transnormer-7": "The best result is highlighted with bold and the second with underlined. The larger the better for all metrics. | Model | Text | ListOps | Retrieval | Pathfinder | Image | AVG. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Transformer | 61.95 | 38.37 | 80.69 | 65.26 | 40.57 | 57.37 |\n| Kernelized Attention | 60.22 | 38.78 | 81.77 | 70.73 | 41.29 | 58.56 |\n| Nystromformer | 64.83 | 38.51 | 80.52 | 69.48 | 41.30 | 58.93 |\n| Linformer | 58.93 | 37.45 | 78.19 | 60.93 | 37.96 | 54.69 |\n| Informer | 62.64 | 32.53 | 77.57 | 57.83 | 38.10 | 53.73 |\n| Performer | 64.19 | 38.02 | 80.04 | 66.30 | 41.43 | 58.00 |\n| Reformer | 62.93 | 37.68 | 78.99 | 66.49 | 48.87 | 58.99 |\n| BigBird | 63.86 | 39.25 | 80.28 | 68.72 | 43.16 | 59.05 |\n| Skyformer | 64.70 | 38.69 | 82.06 | 70.73 | 40.77 | 59.39 |\n| LS | 66.62 | 40.30 | 81.68 | 69.98 | 47.60 | 61.24 |\n| cosFormer | $\\mathbf{6 7 . 7 0}$ | 36.50 | 83.15 | 71.96 | $\\underline{51.23}$ | 62.11 |\n| FLASH-quad | 64.10 | $\\mathbf{4 2 . 2 0}$ | 83.00 | 63.28 | 48.30 | 60.18 |\n| FLASH | 64.10 | 38.70 | $\\mathbf{8 6 . 1 0}$ | 70.25 | 47.40 | 61.31 |\n| TRANSNORMER T1 | 66.90 | 41.03 | 83.11 | 75.92 | $\\mathbf{5 1 . 6 0}$ | $\\underline{63.71}$ |\n| TRANSNORMER T2 | $\\mathbf{7 2 .",
    "transnormer-8": "2 0}$ | $\\underline{41.60}$ | $\\underline{83.82}$ | $\\mathbf{7 6 . 8 0}$ | 49.60 | $\\mathbf{6 4 . 8 0}$ |\n\n### 5.2 Results\n\nAutoregressive language modeling We report the results in Table 4. It can be found that both TRANSNORMER variants get comparable or better perplexity to the vanilla attention and outperform all existing linear models with a clear margin. For example, compared to previous state-of-the-art linear methods on validation set(Hua et al., 2022) and test set(Zhu et al., 2021), TrANSNORMER T2 achieves substantially lower perplexity by 2.31 and 1.58 respectively. It demonstrates the effectiveness of our method in causal models. Bidirectional language modeling We show our bidirectional results on the GLUE benchmark in Table. 5. Our method achieves superior performance to all the competing methods in average. On three tasks, i.e., SST-2, MRPC, CoLA, TRANSNORMER reports comprehensively better results than all competing linear methods, such as 4.62 higher on CoLA. Further, one of our variants i.e., TRANSNORMER T1, even outperforms the vanilla attention with a notable margin. It proves the effectiveness of our method in bidirectional language modeling. Long Range Arena Benchmark The results before the transformer Long-short (abbr. LS) are taken from the Skyformer (Chen et al., 2021). As shown in Table. 6, we achieve either first or second places across all five tasks. In terms of overall results, both TRANSNORMER variants (T1,T2) outperform all other competing methods including vanilla transformer (Vaswani et al., 2017), which validates our capability to encode long sequences. ### 5.3 Speed comparison\n\nWe compare the training and inference speed of the TRANSNORMER with other methods. For a fair and comprehensive comparison, we follow exactly the same configurations of the Skyformer(Chen et al., 2021) and report step per second under different sequence lengths. Timing is conducted on a Nvidia A6000 GPU with 48G GPU memory. Table. 7 suggests that the vanilla transformer is substantially slow and exhausts GPU memory with sequence longer than 3 k . Compared to other efficient transformers, our TransNORMER achieves\n\nTable 7: Speed comparison on Long-Range Arena benchmark.",
    "transnormer-9": "We mark it with a dash if a method exhausts GPU memory.",
    "transnormer-10": "The higher the better for all metrics. The $\\mathbf{1 K}, \\ldots, \\mathbf{5 K}$ represent the input sequence length. |  | Inference Speed(steps per sec) |  |  |  | Train Speed(steps per sec) |  |  |  |  |  |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| model | $\\mathbf{1 K}$ | $\\mathbf{2 K}$ | $\\mathbf{3 K}$ | $\\mathbf{4 K}$ | $\\mathbf{5 K}$ | $\\mathbf{1 K}$ | $\\mathbf{2 K}$ | $\\mathbf{3 K}$ | $\\mathbf{4 K}$ | $\\mathbf{5 K}$ |\n| Transformer | 39.06 | 10.05 | - | - | - | 15.34 | 3.05 | - | - | - |\n| FLASH-quad | 44.64 | 16.45 | 9.40 | 6.54 | 5.39 | 19.84 | 8.47 | 5.19 | 3.59 | 2.92 |\n| FLASH | 40.32 | 23.15 | 16.89 | 14.04 | 13.16 | 20.49 | 11.06 | 8.47 | 7.23 | 6.93 |\n| LS | 32.05 | 17.36 | 12.14 | 10.16 | 9.06 | 15.43 | 8.68 | 6.28 | 5.24 | 4.76 |\n| Performer | 104.17 | 56.82 | 42.37 | 33.78 | 31.25 | 28.41 | 16.23 | 12.02 | 10.04 | 9.06 |\n| cosFormer | 86.21 | 46.30 | 32.47 | 27.47 | 25.0 | 22.94 | 12.82 | 9.19 | 7.79 | 7.14 |\n| Linformer | 10.17 | 58.14 | 40.32 | 31.25 | 26.32 | 27.17 | 15.63 | 11.26 | 8.77 | 7.42 |\n| Reformer | 78.13 | 38.46 | 26.04 | 19.84 | 16.23 | 20.16 | 10.87 | 7.46 | 5.69 | 4.70 |\n| Nystorm | 58.14 | 38.46 | 29.07 | 23.81 | 20.33 | 14.12 | 9.62 | 7.46 | 6.11 | 5.26 |\n| TRANSNORMER T1 | 113.64 | 65.79 | 46.30 | 39.06 | 35.71 | 28.41 | 17.12 | 12.76 | 10.87 | 10.12 |\n| TRANSNORMER T2 | 119.05 | 65.79 | 47.17 | 39.68 | 36.23 | 29.41 | 17.24 | 12.95 | 10.96 | 10.16 |\n\nfaster speed with comparable GPU memory footprints, while competing efficient methods all report worse results compared to our TRANSNORMER. For instance, compared to FLASH-quad (Hua et al., 2022) that achieves previous best linear results on both autoregressive and bidirectional benchmarks, our model performs over $300 \\%$ faster during training and $150 \\%$ faster during inference. ### 5.4 Ablation study\n\nIn this section, we justify our design choice of the TRANSNORMER, including, the selection of the FFN module, and the size of the attention block in DiagAttention. We use the PPL from the Roberta pre-training stage as our evaluation metric. Table 8: Ablation of the proportion of the attentions. We empirically find that the balanced structure achieves the best result. We abbreviate the DiagAtTEntion as BlockAtt and NORMAtTENTION as NormAtt. | Early stage <br> BlockAtt | Later stage <br> NormAtt | $\\mathrm{T} 1 \\mathrm{ppl}(\\mathrm{val})$ | $\\mathrm{T} 2 \\mathrm{ppl}(\\mathrm{val})$ |\n| :---: | :---: | :---: | :---: |\n| 0 | 12 | 4.23 | 4.48 |\n| 3 | 9 | 4.13 | 3.83 |\n| 6 | 6 | $\\mathbf{3 . 8 2}$ | $\\mathbf{3 . 8 1}$ |\n| 9 | 3 | 3.87 | 3.86 |\n| 12 | 0 | 4.75 | 4.66 |\n\nTable 9: Ablation of the order of two proposed attention. Using DiagAtTENTION in the early stage achieves better results than using it on later stage. | Early stage | Later stage | T1 ppl(val) | T2 ppl(val) |\n| :--- | :--- | :---: | :---: |\n| NormAtt | BlockAtt | 4.13 | 4.21 |\n| BlockAtt | NormAtt | $\\mathbf{3 . 8 2}$ | $\\mathbf{3 . 8 1}$ |\n\nStructure design As aforementioned, we empirically choose the first 6 layers as the early stage of the model and the rest as the later stage. We provide the designing ground for this choice in Table. 8. It can be also observed that either choosing the DIAGATTENTION or NORMATTENTION for the entire model will lead to inferior performance. We also provide the ablation results of swapping the order of the DiagAtTEntion and the NormAttenTION in Table. 9. Using DiagAttention in the early stage achieves significantly better results than using it on later stage. It further proves our claim that the early stage focuses on neighbouring tokens while the later stage needs long-range attentions. Table 10: Ablation of the selection of the FFN modules. The GLU leads to better results. | FFN type | T1 ppl(val) | T2 ppl(val) |\n| :--- | :---: | :---: |\n| FFN | 3.93 | 3.93 |\n| GLU(ours) | $\\mathbf{3 . 8 2}$ | $\\mathbf{3 . 8 1}$ |\n\nFFN module We ablate the selection of the FFN modules in Table. 10. Compared with the traditional FFN (Vaswani et al., 2017), the GLU (Shazeer, 2020) achieves better results. Table 11: Ablation of on block sizes in the DiagAtTENTION. The larger block size the better results. | Block size | T1 ppl(val) | T2 ppl(val) |\n| :---: | :---: | :---: |\n| 32 | 3.92 | 3.90 |\n| 64 | 3.82 | 3.81 |\n| 128 | $\\mathbf{3 . 7 2}$ | $\\mathbf{3 . 6 9}$ |\n\nBlock size From the Table. 11, we observe clear performance improvements with increased block sizes. However, since the complexity of the DiAGATTENTION is $O(n w d)$, larger block size $w$ leads to heavier computational overhead. We choose a block size as 64 as a trade-off between performance and computational cost. Combination of attentions Finally, we study the effect that whether we should use both atten-\ntions in one layer. In particular, we compare either to 1) use DiagAttention and NormAtTENTION sequentially in a layer with different orders; or to 2) use them in parallel in each attention layer and then concatenate their embedding output. Table. 12 shows that we should not use these attentions sequentially within a layer and apply them in parallel will double the computation complexities without improving the performance. Table 12: Ablation of the combination of two proposed attention. In first two rows, the two attention layers appear in an interleaved manner. D for the DIAGAtTEntion and N for the NormAttention. | approach | T1 ppl(val) | T2 ppl(val) |\n| :--- | :---: | :---: |\n| altering D $\\rightarrow$ N | 4.19 | 4.23 |\n| altering $\\rightarrow \\rightarrow$ D | 4.11 | 4.21 |\n| parallel | $\\mathbf{3 . 7 7}$ | 3.82 |\n| TrANSNORMER | 3.82 | $\\mathbf{3 . 8 1}$ |\n\n## 6 Conclusion\n\nIn this paper, we identified two key issues that cause the inferior performance of existing linear transformer models: 1) unbounded gradients; 2) attention dilution. For the former issue, we proposed a new NORMATtENTION to stabilize the training gradients. For the latter, we develop DiAGATtENTION to force the model concentrate attention in neighbouring tokens. The resultant model TRANSNORMER marries the strength of the vanilla transformers and the linear transformers, outperforming competing linear transformers on both autoregressive and bidirectional language modeling, text classification tasks and the challenging Longrange arena benchmark. ## Limitations\n\nIn this paper, we identified two main issues of current linear transformers and provided a comprehensive analysis in natural language processing tasks. However, with the booming development of vision transformers, whether they share the same issues of linear NLP transformers is yet to be discovered. We will validate our method on the linear vision transformers in our future work. ## Ethics Statement\n\nThe proposed technique is beneficial to develop large-scale environment-friendly language models by reducing computing resource demand. Corpus used to train the model is from public web sources, which may contain biased, explicit or improper content. Further assessment and regulation have to be in-place before deploying the model in practice. ## References\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.",
    "transnormer-11": "Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. 2021. Skyformer: Remodel self-attention with gaussian kernel and nystr\u00f6m method. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual. Xuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge. 2022a. Implicit motion handling for video camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13864-13873. Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Tom Drummond, Zhiyong Wang, and Zongyuan Ge. 2022b. Deep laparoscopic stereo matching with transformers. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages $464-474$. Springer. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794.",
    "transnormer-12": "Bolin Gao and Lacra Pavel. 2017. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. 2022. Transformer quality in linear time. arXiv preprint arXiv:2202.10447. Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144.",
    "transnormer-13": "Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156-5165.",
    "transnormer-14": "PMLR. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. 5th International Conference on Learning Representations, ICLR, Toulon, France. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2020. Random feature attention. In International Conference on Learning Representations. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations. Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202. Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, and Yiran Zhong. 2022a. Locality matters: A locality-biased linear attention for automatic speech recognition. arXiv preprint arXiv:2203.15609. Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. 2022b. Vicinity vision transformer. arXiv preprint arXiv:2206.10552. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020a. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020b. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations. Michalis K Titsias. 2016. One-vs-each approximation to softmax for scalable estimation of probabilities. arXiv preprint arXiv:1609.07410. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages $353-355$.",
    "transnormer-15": "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Selfattention with linear complexity. arXiv preprint arXiv:2006.04768. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. In NeurIPS. Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada. Biao Zhang, Ivan Titov, and Rico Sennrich. 2021. Sparse attention with linear units. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6507-6520, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Lin Zheng, Chong Wang, and Lingpeng Kong. 2022. Linear complexity randomized self-attention mechanism. arXiv preprint arXiv:2204.04667. Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. 2022. Audio-visual segmentation. In European Conference on Computer Vision. Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. 2021. Long-short transformer: Efficient transformers for language and vision. In $A d$ vances in Neural Information Processing Systems. ## Appendix\n\n## A Mathematical Notations\n\nWe use bold uppercase letters for matrices(M), bold lowercase letters for vectors( $\\mathbf{m}$ ), and lowercase letters for scalars $\\left(m_{i j}\\right)$. We represent all vectors as column vectors and denote the $i$ th row of matrix $\\mathbf{M}$ by $\\mathbf{m}_{i}^{\\top}$ or $\\mathbf{M}_{i}$. We use $\\|.\\|_{2}$ to denote the $l_{2}$ norm and $\\|\\cdot\\|_{F}$ to denote the Frobenius norm of the matrix and the vector. The main mathematical symbols are input $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, Q (Query), $\\mathbf{K}$ (Key) and $\\mathbf{V}$ (Value), which has the following form:\n\n$$\n\\begin{aligned}\n& \\mathbf{X}=\\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{x}_{n}^{\\top}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d}, \\\\\n& \\mathbf{Q}=\\left[\\begin{array}{c}\n\\mathbf{q}_{1}^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{q}_{n}^{\\top}\n\\end{array}\\right]=\\mathbf{X} \\mathbf{W}_{Q}=\\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{\\top} \\mathbf{W}_{Q} \\\\\n\\vdots \\\\\n\\mathbf{x}_{n}^{\\top} \\mathbf{W}_{Q}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d} \\\\\n& \\mathbf{K}=\\left[\\begin{array}{c}\n\\mathbf{k}_{1}^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{k}_{n}^{\\top}\n\\end{array}\\right]=\\mathbf{X} \\mathbf{W}_{K}=\\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{\\top} \\mathbf{W}_{K} \\\\\n\\vdots \\\\\n\\mathbf{x}_{n}^{\\top} \\mathbf{W}_{K}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d} \\\\\n& \\mathbf{V}=\\left[\\begin{array}{c}\n\\mathbf{v}_{1}^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{v}_{n}^{\\top}\n\\end{array}\\right]=\\mathbf{X} \\mathbf{W}_{V}=\\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{\\top} \\mathbf{W}_{V} \\\\\n\\vdots \\\\\n\\mathbf{x}_{n}^{\\top} \\mathbf{W}_{V}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}_{Q}, \\mathbf{W}_{K}, \\mathbf{W}_{V} \\in \\mathbb{R}^{d \\times d}$. ## B Proof of gradients' upper bound\n\nIn this part, we will proof the bound in (8) and (10), all we need to prove is:\n\n$$\n0 \\leq p_{i k}\\left(1-p_{i k}\\right) \\leq \\frac{1}{4}, 0 \\leq p_{i j} p_{i k} \\leq \\frac{1}{4}\n$$\n\nWe adopt the theorem that geometric mean is bounded by arithmetic mean, i.e.,\n$\\sqrt{a b} \\leq \\frac{a+b}{2} \\Longleftrightarrow a b \\leq\\left(\\frac{a+b}{2}\\right)^{2}, \\forall a, b \\geq 0$. We take $a=p_{i k}, b=1-p_{i k}$ to complete the proof. The first bound can be proven by:\n\n$$\n0 \\leq p_{i k}\\left(1-p_{i k}\\right) \\leq\\left(\\frac{p_{i k}+1-p_{i k}}{2}\\right)^{2}=\\frac{1}{4}\n$$\n\nFor the second bound, we first use the fact that:\n\n$$\n0 \\leq p_{i j}+p_{i k} \\leq 1 \\Rightarrow p_{i j} \\leq 1-p_{i k}\n$$\n\nSo we have:\n\n$$\n0 \\leq p_{i j} p_{i k} \\leq\\left(1-p_{i k}\\right) p_{i k} \\leq \\frac{1}{4}\n$$\n\n## C Proof of Proposition 3.1\n\nProof of Proposition 3.1. $\\forall \\epsilon>0$ and kernel function $\\phi$, let ${ }^{6}$ :\n\n$$\n\\begin{gathered}\n\\mathbf{q}_{i}=\\mathbf{k}_{j}=\\phi^{-1}\\left(\\mathbf{x}_{0}\\right) \\\\\n0<\\left\\|\\mathbf{x}_{0}\\right\\|_{2} \\leq \\sqrt{\\epsilon}, i, j=1, \\ldots, n\n\\end{gathered}\n$$\n\nThen\n\n$$\n\\phi\\left(\\mathbf{q}_{i}\\right)=\\phi\\left(\\mathbf{k}_{j}\\right)=\\mathbf{x}_{0}, i, j=1, \\ldots, n\n$$\n\nSo\n\n$$\ns_{i j}=\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\phi\\left(\\mathbf{k}_{j}\\right)=\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0} \\in(0, \\epsilon]\n$$\n\nand\n\n$$\np_{i j}=\\frac{s_{i j}}{\\sum_{k=1}^{n} s_{i k}}=\\frac{\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}}{\\sum_{k=1}^{n} \\mathbf{x}_{0}^{\\top} \\mathbf{x}_{0}}=\\frac{1}{n}\n$$\n\nAccording to (10), we have:\n\n$$\n\\begin{aligned}\n& \\frac{\\partial p_{i j}}{\\partial s_{i k}}=\\left\\{\\begin{array}{ll}\n\\frac{1}{\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}} \\frac{1}{n}\\left(1-\\frac{1}{n}\\right) & j=k \\\\\n-\\frac{1}{\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}} \\frac{1}{n^{2}} & j \\neq k\n\\end{array},\\right.",
    "transnormer-16": "\\\\\n& \\left|\\frac{\\partial p_{i j}}{\\partial s_{i k}}\\right|= \\begin{cases}\\frac{1}{\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}} \\frac{1}{n}\\left(1-\\frac{1}{n}\\right) & j=k \\\\\n\\frac{1}{\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}} \\frac{1}{n^{2}} & j \\neq k\\end{cases} \\\\\n& \\geq\\left\\{\\begin{array}{ll}\n\\frac{1}{\\epsilon n}\\left(1-\\frac{1}{n}\\right) & j=k \\\\\n\\frac{1}{\\epsilon n^{2}} & j \\neq k\n\\end{array} .\\right. \\end{aligned}\n$$\n\nLet $\\epsilon \\rightarrow 0^{+}$, then $\\frac{1}{\\epsilon n^{2}}, \\frac{1}{\\epsilon n}\\left(1-\\frac{1}{n}\\right) \\rightarrow \\infty$, so $\\left|\\frac{\\partial p_{i j}}{\\partial s_{i k}}\\right| \\rightarrow \\infty$. ## D Analyze the gradient of each method\n\nIn this section, let's consider a one-layer Transformer, for a multi-layer Transformer, we can prove our conclusion using induction.",
    "transnormer-17": "We begin this section by introducing some mathematical notations. ## D. 1 Notations\n\nIn vanilla attention, we have:\n\n$$\n\\begin{aligned}\n& \\mathbf{S}=\\mathbf{Q K}^{\\top} \\in \\mathbb{R}^{n \\times n} \\\\\n& \\mathbf{P}=\\operatorname{Softmax}(\\mathbf{S}) \\in \\mathbb{R}^{n \\times n} \\\\\n& \\mathbf{O}=\\mathbf{P V} \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\n[^4]In linear attention, we have:\n\n$$\n\\begin{aligned}\n\\mathbf{S} & =\\phi(\\mathbf{Q}) \\phi(\\mathbf{K})^{\\top} \\in \\mathbb{R}^{n \\times n} \\\\\n\\boldsymbol{\\Delta} & =\\operatorname{diag}\\left(\\mathbf{S 1}_{n}\\right) \\in \\mathbb{R}^{n \\times n} \\\\\n\\mathbf{P} & =\\boldsymbol{\\Delta}^{-1} \\mathbf{S} \\in \\mathbb{R}^{n \\times n} \\\\\n\\mathbf{O} & =\\mathbf{P V} \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\nAlthough this term is not calculated in linear attention, we discuss it conceptually. Note that the above formulations can be unified into the following form ${ }^{7}$ :\n\n$$\n\\begin{aligned}\n\\mathbf{S} & =f\\left(\\psi(\\mathbf{Q}) \\psi(\\mathbf{K})^{\\top}\\right) \\in \\mathbb{R}^{n \\times n} \\\\\n\\boldsymbol{\\Delta} & =\\operatorname{diag}\\left(\\mathbf{S} \\mathbf{1}_{n}\\right) \\in \\mathbb{R}^{n \\times n} \\\\\n\\mathbf{P} & =\\boldsymbol{\\Delta}^{-1} \\mathbf{S} \\in \\mathbb{R}^{n \\times n} \\\\\n\\mathbf{O} & =\\mathbf{P V} \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\nwhere in vanilla attention, we have:\n\n$$\n\\psi(\\mathbf{x})=\\mathbf{x}, f(x)=\\exp (x)\n$$\n\nand in linear attention, we have:\n\n$$\n\\psi(\\mathbf{x})=\\phi(\\mathbf{x}), f(x)=x\n$$\n\nIn NORMAtTENTION, we have:\n\n$$\n\\begin{aligned}\n\\mathbf{S} & =\\phi(\\mathbf{Q}) \\phi(\\mathbf{K})^{\\top} \\in \\mathbb{R}^{n \\times n} \\\\\n\\mathbf{T} & =\\mathbf{S V} \\in \\mathbb{R}^{n \\times d} \\\\\n\\mathbf{O} & =\\operatorname{RMSNorm}(\\mathbf{T}) \\\\\n& \\triangleq\\left[\\begin{array}{c}\n\\mathrm{RMSNorm}\\left(\\mathbf{t}_{1}\\right)^{\\top} \\\\\n\\vdots \\\\\n\\operatorname{RMSNorm}\\left(\\mathbf{t}_{n}\\right)^{\\top}\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\nwhere RMSNorm is defined as follows:\n\n## Definition D.1.",
    "transnormer-18": "$$\n\\begin{aligned}\n\\operatorname{RMSNorm}(\\mathbf{x}) & =\\frac{\\mathbf{x}}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\\n\\sigma^{2} & =\\frac{\\sum_{i=1}^{d} x_{i}^{2}}{d} \\\\\n\\epsilon & >0 \\\\\n\\mathbf{x} & \\in \\mathbb{R}^{d}\n\\end{aligned}\n$$\n\nIn the subsequent discussion, we define gradient $\\nabla_{\\mathrm{M}} \\mathcal{L}$ as:\n\n[^5]\n## Definition D.2. $$\n\\left[\\nabla_{\\mathbf{M}} \\mathcal{L}\\right]_{i j}=\\frac{\\partial \\mathcal{L}}{\\partial m_{i j}}\n$$\n\nwhere $\\mathcal{L}$ stands for loss function, M is a parameter matrix. Then we define the mapping $h$ as:\n\n## Definition D.3. $$\n\\begin{aligned}\nh: \\mathbb{R}^{n \\times m} & \\rightarrow \\mathbb{R}, h(\\mathbf{X})=\\max _{i=1}^{n}\\left\\|\\mathbf{X}_{i}\\right\\|_{2} \\\\\n& \\mathbf{X} \\in \\mathbb{R}^{n \\times m}\n\\end{aligned}\n$$\n\nThe mapping $h$ has the following property:\n\nProposition D.4. $\\forall \\mathbf{X} \\in \\mathbb{R}^{n \\times m}, \\mathbf{Y} \\in \\mathbb{R}^{r \\times m}$, we have:\n\n$$\nh\\left(\\mathbf{X} \\mathbf{Y}^{\\boldsymbol{\\top}}\\right) \\leq \\sqrt{r} h(\\mathbf{X}) h(\\mathbf{Y})\n$$\n\nProof. Since\n\n$$\n\\begin{aligned}\n{\\left[\\mathbf{X Y}^{\\top}\\right]_{i j} } & =\\mathbf{X}_{i}\\left[\\mathbf{Y}_{j}\\right]^{\\top} \\\\\n& \\leq\\left\\|\\mathbf{X}_{i}\\right\\|_{2}\\left\\|\\mathbf{Y}_{\\boldsymbol{j}}\\right\\|_{2} \\\\\n& \\leq h(\\mathbf{X}) h(\\mathbf{Y})\n\\end{aligned}\n$$\n\nso\n\n$$\n\\begin{aligned}\n\\left\\|\\left[\\mathbf{X Y}^{\\top}\\right]_{i}\\right\\|_{2} & =\\sqrt{\\sum_{j=1}^{r}\\left(\\left[\\mathbf{X Y}^{\\top}\\right]_{i j}\\right)^{2}} \\\\\n& \\leq \\sqrt{r(h(\\mathbf{X}) h(\\mathbf{Y}))^{2}} \\\\\n& =\\sqrt{r} h(\\mathbf{X}) h(\\mathbf{Y}) \\\\\nh\\left(\\mathbf{X} \\mathbf{Y}^{\\top}\\right) & =\\max _{i=1}\\left\\|\\left[\\mathbf{X} \\mathbf{Y}^{\\top}\\right]_{i}\\right\\|_{2} \\\\\n& \\leq \\sqrt{r} h(\\mathbf{X}) h(\\mathbf{Y})\n\\end{aligned}\n$$\n\n## D. 2 Gradient analysis\n\n## D.2.1 Preliminary\n\nGiven gradient $\\nabla_{\\mathbf{O}} \\mathcal{L} \\in \\mathbb{R}^{n \\times d}$, let's compute $\\nabla_{\\mathrm{S}} \\mathcal{L}$ in every situation. We first define:\n\n$$\n\\begin{aligned}\nc_{1} & =h\\left(\\nabla_{\\mathbf{O}} \\mathcal{L}\\right) \\\\\n& =\\max _{i=1}^{n}\\left\\|\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right\\|_{2} \\\\\nc_{2} & =h(\\mathbf{V}) \\\\\n& =\\max _{i=1}^{n}\\left\\|\\mathbf{V}_{i}\\right\\|_{2}<\\infty \\\\\nc_{3} & =\\min _{i, j}\\left|s_{i j}\\right| \\geq 0\n\\end{aligned}\n$$\n\nBefore we get started, we have the following propositions.",
    "transnormer-19": "The proof can be found in Appendix D.3. Proposition D.5. $c_{1}<\\infty$. Proposition D.6. $\\forall \\mathbf{X} \\in \\mathbb{R}^{n \\times m}$, we have:\n\n$$\n\\|\\mathbf{X}\\|_{2} \\leq \\sqrt{n} h(\\mathbf{X})\n$$\n\nTake $\\mathbf{X}=\\mathbf{V}$, we get:\n\n$$\n\\|\\mathbf{V}\\|_{2} \\leq \\sqrt{n} h(\\mathbf{V})=\\sqrt{n} c_{2}\n$$\n\n## D.2.2 Vanilla/Linear attention\n\nAccording to (30), we can discuss vanilla and linear attention under one formula:\n\n$$\n\\nabla_{\\mathbf{P}} \\mathcal{L}=\\left[\\nabla_{\\mathbf{O}} \\mathcal{L}\\right] \\mathbf{V}^{\\top} \\in \\mathbb{R}^{n \\times n}\n$$\n\nThen define matrix $\\mathbf{U}^{(i)} \\in \\mathbb{R}^{n \\times n}$ :\n\n$$\n\\left[\\mathbf{U}^{(i)}\\right]_{j k}=\\frac{\\partial p_{i k}}{\\partial s_{i j}}\n$$\n\nAccording to (9), in vanilla attention, we have:\n\n$$\n\\left|\\left[\\mathbf{U}^{(i)}\\right]_{j k}\\right| \\leq \\frac{1}{4}\n$$\n\nwhile in linear attention, we have:\n\n$$\n\\left|\\left[\\mathbf{U}^{(i)}\\right]_{j k}\\right| \\leq \\frac{1}{4\\left|s_{i j}\\right|} \\leq \\frac{1}{4 c_{3}}\n$$\n\nSince:\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}} & =\\sum_{k=1}^{n} \\frac{\\partial \\mathcal{L}}{\\partial p_{i k}} \\frac{\\partial p_{i k}}{\\partial s_{i j}} \\\\\n& =\\left(\\nabla_{\\mathbf{P}_{i}} \\mathcal{L}\\right)\\left(\\mathbf{U}_{j}^{(i)}\\right)^{\\top} \\\\\n& =\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right) \\mathbf{V}^{\\top}\\left(\\mathbf{U}_{j}^{(i)}\\right)^{\\top}\n\\end{aligned}\n$$\n\nSo we have:\n\n$$\n\\begin{aligned}\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| & \\leq\\left\\|\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right) \\mathbf{V}^{\\boldsymbol{\\top}}\\right\\|_{2}\\left\\|\\mathbf{U}_{j}^{(i)^{\\top}}\\right\\|_{2} \\\\\n& \\leq\\left\\|\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right\\|_{2}\\left\\|\\mathbf{V}^{\\boldsymbol{\\top}}\\right\\|_{2}\\left\\|\\mathbf{U}_{j}^{(i)}\\right\\|_{2} \\\\\n& \\leq c_{1} \\times \\sqrt{n} c_{2} \\times \\frac{1}{4 t} \\\\\n& =\\frac{\\sqrt{n} c_{1} c_{2}}{4 t}\n\\end{aligned}\n$$\n\nwhere $t=1$ in vanilla attention and $t=c_{3}$ in linear attention. On the other hand, according to Appendix C , in linear attention, there exist $\\mathbf{q}_{i}, \\mathbf{k}_{j}$, such that:\n\n$$\n\\begin{aligned}\n\\frac{\\partial p_{i k}}{\\partial s_{i j}} & =\\frac{1}{\\left\\|\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}\\right\\|} t_{i j k}, \\\\\nt_{i j k} & = \\begin{cases}\\frac{1}{n}\\left(1-\\frac{1}{n}\\right) & j=k \\\\\n-\\frac{1}{n^{2}} & j \\neq k\\end{cases}\n\\end{aligned}\n$$\n\nThen\n\n$$\n\\begin{aligned}\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| & =\\left|\\sum_{k=1}^{n} \\frac{\\partial \\mathcal{L}}{\\partial p_{i k}} \\frac{\\partial p_{i k}}{\\partial s_{i j}}\\right| \\\\\n& =\\frac{1}{\\left\\|\\mathbf{x}_{0}{ }^{\\top} \\mathbf{x}_{0}\\right\\|}\\left|\\sum_{k=1}^{n} \\frac{\\partial \\mathcal{L}}{\\partial p_{i k}} t_{i j k}\\right| \\\\\n& \\geq \\frac{1}{\\epsilon}\\left|\\sum_{k=1}^{n} \\frac{\\partial \\mathcal{L}}{\\partial p_{i k}} t_{i j k}\\right|\n\\end{aligned}\n$$\n\nLet $\\epsilon \\rightarrow 0^{+}$, then $\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i k}}\\right| \\rightarrow \\infty$. This means that the gradient in linear attention is unbounded. ## D.2.3 NORMATTENTION\n\nWe first define the second-moment of $i$ 'th row of T :\n\n$$\n\\sigma_{i}^{2}=\\frac{\\sum_{j=1}^{d} t_{i j}^{2}}{d}\n$$\n\nThen $\\frac{\\partial o_{i j}}{\\partial t_{i k}}$ is as follows:\n\n$$\n\\frac{\\partial o_{i j}}{\\partial t_{i k}}=\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1\\{j=k\\}-\\frac{1}{d} \\frac{t_{i j} t_{i k}}{\\sigma_{i}^{2}+\\epsilon}\\right] . $$\n\nNotice that we have the following upper bound:\n\n$$\n\\begin{aligned}\n& \\left|\\frac{\\partial o_{i j}}{\\partial t_{i k}}\\right|\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1\\{j=k\\}+\\frac{t_{i j} t_{i k}}{\\sum_{s=1}^{d} t_{i s}^{2}+d \\epsilon}\\right] \\\\\n& \\leq \\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1\\{j=k\\}+\\frac{1}{2} \\frac{t_{i j}^{2}+t_{i k}^{2}}{\\sum_{s=1}^{d} t_{i s}^{2}}\\right] \\\\\n& \\leq \\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1+\\frac{1}{2}\\right] \\\\\n& \\leq \\frac{3}{2 \\sqrt{\\sigma_{i}^{2}+\\epsilon}} \\text {. }\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1e87ac89fa2bec7d4d6fg-13.jpg?height=153&width=692&top_left_y=1985&top_left_x=1087)\n\nLet's define matrix $\\mathbf{R}^{(i)} \\in \\mathbb{R}^{d \\times d}$ as follows:\n\n$$\n\\left[\\mathbf{R}^{(i)}\\right]_{j k}=\\frac{\\partial o_{i k}}{\\partial t_{i j}}\n$$\n\nSince\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial t_{i j}} & =\\sum_{k=1}^{n} \\frac{\\partial \\mathcal{L}}{\\partial o_{i k}} \\frac{\\partial o_{i k}}{\\partial t_{i j}} \\\\\n& =\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right)\\left(\\mathbf{R}_{j}^{(i)}\\right)^{\\top}\n\\end{aligned}\n$$\n\nThen we can get:\n\n$$\n\\nabla_{\\mathbf{T}_{i}} \\mathcal{L}=\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right)\\left(\\mathbf{R}^{(i)}\\right)^{\\top} \\in \\mathbb{R}^{1 \\times d}\n$$\n\nAccording to (53), we have:\n\n$$\n\\begin{aligned}\n\\left\\|\\mathbf{R}^{(i)}\\right\\|_{2} & \\leq\\left\\|\\mathbf{R}^{(i)}\\right\\|_{F} \\\\\n& \\leq \\sqrt{\\sum_{j=1}^{d} \\sum_{k=1}^{d}\\left[\\frac{\\partial o_{i j}}{\\partial t_{i k}}\\right]^{2}} \\\\\n& \\leq \\frac{3 d}{2 \\sqrt{\\sigma_{i}^{2}+\\epsilon}} \\\\\n& \\leq \\frac{3 d}{2 \\sqrt{\\epsilon}}\n\\end{aligned}\n$$\n\nFinally, we get:\n\n$$\n\\begin{aligned}\n\\nabla_{\\mathbf{S}_{i}} \\mathcal{L} & =\\left(\\nabla_{\\mathbf{T}_{i}} \\mathcal{L}\\right) \\mathbf{V}^{\\top} \\\\\n& =\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right)\\left(\\mathbf{R}^{(i)}\\right)^{\\top} \\mathbf{V}^{\\top} \\in \\mathbb{R}^{1 \\times n} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}} & =\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right)\\left(\\mathbf{R}^{(i)}\\right)^{\\top} \\mathbf{V}_{j} \\\\\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| & =\\left|\\left(\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right)\\left(\\mathbf{R}^{(i)}\\right)^{\\top} \\mathbf{V}_{j}\\right| \\\\\n& \\leq\\left\\|\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right\\|_{2}\\left\\|\\mathbf{R}^{(i)} \\mathbf{V}_{j}\\right\\|_{2} \\\\\n& \\leq\\left\\|\\nabla_{\\mathbf{O}_{i}} \\mathcal{L}\\right\\|_{2}\\left\\|\\mathbf{R}^{(i)}\\right\\|_{2}\\left\\|\\mathbf{V}_{j}\\right\\|_{2} \\\\\n& \\leq c_{1} \\times \\frac{3 d}{2 \\sqrt{\\epsilon}} \\times c_{2} \\\\\n& =\\frac{3 c_{1} c_{2} d}{2 \\sqrt{\\epsilon}}\n\\end{aligned}\n$$\n\nLet's summarize the previous results. In vanilla attention, we have:\n\n$$\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| \\leq \\frac{\\sqrt{n} c_{1} c_{2}}{4}<\\infty\n$$\n\nIn linear attention, there exist $\\mathbf{q}_{i}, \\mathbf{k}_{j}$, such that:\n\n$$\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| \\rightarrow \\infty\n$$\n\nIn NORMAtTENTION, we have:\n\n$$\n\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}\\right| \\leq \\frac{3 c_{1} c_{2} d}{2 \\sqrt{\\epsilon}}<\\infty\n$$\n\nSo $\\frac{\\partial \\mathcal{L}}{\\partial s_{i j}}$ is bounded in vanilla attention and NORMATTENTION, while it's unbounded in linear attention.",
    "transnormer-20": "This makes the training of linear transformer unstable. ## D. 3 Proof of the proposition\n\nProof of Proposition D.5. Let's consider a one layer Transformer for classification tasks. The input is $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, the label is $\\mathbf{Y} \\in$ $\\mathbb{R}^{n \\times m}$, where $m$ is the number of categories and $\\mathbf{Y}_{i}$ is one-hot vector,. $f_{1}, f_{2}$ are the activation functions, here we take $f_{1}=f_{2}=\\operatorname{ReLU}$ as an example. The parameters of the model are:\n\n$$\n\\begin{gathered}\n\\mathbf{W}_{1} \\in \\mathbb{R}^{d \\times d_{1}}, \\mathbf{W}_{2} \\in \\mathbb{R}^{d_{1} \\times d} \\\\\n\\mathbf{W}_{3} \\in \\mathbb{R}^{d \\times m}\n\\end{gathered}\n$$\n\nThe forward pass of the model is ${ }^{8}$ :\n\n- $\\mathbf{X}_{1}=\\operatorname{XAttention}(\\mathbf{X}) \\in \\mathbb{R}^{n \\times d}$.",
    "transnormer-21": "- $\\mathbf{X}_{2}=f_{1}\\left(\\mathbf{X}_{1} \\mathbf{W}_{1}\\right) \\in \\mathbb{R}^{n \\times d_{1}}$. - $\\mathbf{X}_{3}=f_{2}\\left(\\mathbf{X}_{2} \\mathbf{W}_{2}\\right) \\in \\mathbb{R}^{n \\times d}$. - $\\mathbf{O}=\\mathbf{T W}_{3} \\in \\mathbb{R}^{n \\times m}$. - $\\mathbf{P}=\\operatorname{Softmax}(\\mathbf{O}) \\in \\mathbb{R}^{n \\times m}$. - $\\mathcal{L}=$ corss_entropy $(\\mathbf{P}, \\mathbf{Y}) \\in \\mathbb{R}$. The backward pass of the model is:\n\n1. $\\nabla_{\\mathbf{O}} \\mathcal{L}=\\mathbf{P}-\\mathbf{Y} \\in \\mathbb{R}^{n \\times m}$. (a) The upper bound is:\n\n$$\n\\begin{aligned}\n& h\\left(\\nabla_{\\mathbf{O}} \\mathcal{L}\\right) \\\\\n= & \\max \\left\\{\\sum_{i=1}^{m} p_{i}^{2}-2 p_{1}+1\\right. \\\\\n& \\left.p_{i} \\geq 0, \\sum_{i=1}^{m} p_{i}=1\\right\\} \\\\\n\\triangleq & a_{0} \\\\\n& <\n\\end{aligned}\n$$\n\n2. $\\nabla_{\\mathbf{X}_{3}} \\mathcal{L}=\\left(\\nabla_{\\mathbf{O}} \\mathcal{L}\\right) W_{3}^{\\top} \\in \\mathbb{R}^{n \\times d}$. [^6](a) The upper bound is:\n\\[\n\n$$\n\\begin{aligned}\n& h\\left(\\nabla_{\\mathbf{x}_{3}} \\mathcal{L}\\right) \\\\\n\\leq & \\sqrt{d} h\\left(\\nabla_{\\mathbf{O}} \\mathcal{L}\\right) h\\left(\\mathbf{W}_{3}\\right) \\\\\n\\leq & \\sqrt{d} a_{0} h\\left(\\mathbf{W}_{3}\\right) \\\\\n\\triangleq & a_{1}<\\infty\n\\end{aligned}\n$$\n\\]\n\n3. $\\nabla_{\\mathbf{x}_{2}} \\mathcal{L}=\\left(f_{2}^{\\prime}\\left(\\mathbf{X}_{2} \\mathbf{W}_{2}\\right) \\odot \\nabla_{\\mathbf{x}_{3}} \\mathcal{L}\\right) \\mathbf{W}_{2}^{\\top} \\in$ $\\mathbb{R}^{n \\times d}$. (a) The upper bound is:\n\n$$\n\\begin{aligned}\n& h\\left(\\nabla_{\\mathbf{x}_{2}} \\mathcal{L}\\right) \\\\\n\\leq & \\sqrt{d_{1}} h\\left(f_{2}^{\\prime}\\left(\\mathbf{X}_{2} \\mathbf{W}_{2}\\right) \\odot \\nabla_{\\mathbf{X}_{3}} \\mathcal{L}\\right) h\\left(\\mathbf{W}_{2}\\right) \\\\\n\\leq & \\sqrt{d_{1}} a_{1} h\\left(\\mathbf{W}_{2}\\right) \\\\\n\\triangleq & a_{2} \\\\\n& <\\infty\n\\end{aligned}\n$$\n\n4. $\\nabla_{\\mathbf{x}_{1}} \\mathcal{L}=\\left(f_{1}^{\\prime}\\left(\\mathbf{X}_{1} \\mathbf{W}_{1}\\right) \\odot \\nabla_{\\mathbf{X}_{2}} \\mathcal{L}\\right) \\mathbf{W}_{1}^{\\top} \\in$ $\\mathbb{R}^{n \\times d_{1}}$. (a) The upper bound is:\n\n$$\n\\begin{aligned}\n& h\\left(\\nabla_{\\mathbf{X}_{1}} \\mathcal{L}\\right) \\\\\n\\leq & \\sqrt{d} h\\left(f_{1}^{\\prime}\\left(\\mathbf{X}_{1} \\mathbf{W}_{1}\\right) \\odot \\nabla_{\\mathbf{X}_{2}} \\mathcal{L}\\right) h\\left(\\mathbf{W}_{1}\\right) \\\\\n\\leq & \\sqrt{d} a_{2} h\\left(\\mathbf{W}_{2}\\right) \\\\\n\\triangleq & a_{3} \\\\\n& <\n\\end{aligned}\n$$\n\nSo the gradient passed to XAttention module is bounded, i.e., $c_{1}=a_{3}<\\infty$. Proof of Proposition D.6. $$\n\\begin{aligned}\n\\|\\mathbf{X}\\|_{2} & \\leq\\|\\mathbf{X}\\|_{F} \\\\\n& =\\sqrt{\\sum_{i=1}^{n}\\left\\|\\mathbf{X}_{i}\\right\\|_{2}^{2}} \\\\\n& \\leq \\sqrt{\\sum_{i=1}^{n}[h(\\mathbf{X})]^{2}} \\\\\n& =\\sqrt{n} h(\\mathbf{X})\n\\end{aligned}\n$$\n\n## E Experiment configs\n\nIn this section, we will introduce detailed training hyperparameters. We introduce the configurations for autoregressive/bidirectional language model in table F. For LRA benchmark, we use the same configuration as Skyformer, which use 2-layer transformer model with 64 hidden dimensions, 2 attention heads, 85 GLU dimensions, Swish as GLU activation function. For batch size and learning rate, we use 16,1e4 for Text Classification, 32,1e-4 for ListOps, 16,2e-4 for Document Retrieval, 128,2e-4 for Pathfinder, 256,1e-4 for Image Classification, the same as Skyformer. ## F Pseudocode for visualization. In this section, we provide pseudo codes for the 4th column of Figure 2 in Python:\n\n```\nimport torch\ndef get_curve(w):\n    n, m = w.shape\n    num = 100\n    P = torch.linspace(0, 1, num)\n    cnts = torch.zeros(num)\n    for i in range(n):\n        cnt = torch.zeros(num)\n        w1 = w[i].clone()\n        center = i % m\n        s = w1[center].item()\n        L = 1\n        l = center - 1\n        r = center + 1\n        j}=\n        l_thre = 0\n        r_thre = m\n        flag = 0\n        while L < m and j < num:\n            if (s >= P[j].item()):\n                cnt[j] = L\n                j += 1\n                continue\n            if flag == 1:\n                if r != r_thre:\n                    s += w1[r].item()\n                    r = min(r_thre, r + 1)\n                    flag = 0\n            else:\n                if l != l_thre:\n                    s += w1[l].item()\n                    l = max(l_thre, l - 1)\n                flag = 1\n            L = min(r - l + 1, m)\n        if L >= m:\n            for u in range( }j,\\mathrm{ num):\n                cnt[u] = min(L, m)\n            cnt[0] = 0\n            cnts += cnt\n    cnts = cnts / n / m\n    plt.plot(cnts, P)\n    return cnts\n```\n\nTable 13: Detailed configurations used in our experiments. \"Total batch size\" means batch_per_gpu $\\times$ update_freq $\\times$ num_gpus.",
    "transnormer-22": "\"Attention dropout\" is only used for vanilla attention. \"ALM\": autoregressive Language Model. \"BLM\": bidirectional Language Model. |  | AML | BLM |\n| :--- | :--- | :--- |\n| Data | WikiText-103 | WikiText-103 |\n| Tokenizer method | BPE | BPE |\n| Src Vocab size | 267744 | 50265 |\n| Encoder layers | 0 | 12 |\n| Decoder layers | 6 | 0 |\n| Hidden dimensions | 512 | 768 |\n| Number of heads | 8 | 12 |\n| GLU dimensions | 2048 | 1365 |\n| GLU activation function | Swish | Swish |\n| Sequence length | 512 | 512 |\n| Total batch size | 128 | 512 |\n| Number of updates | 100 k | 50 k |\n| Warmup steps | 8 k | 3 k |\n| Peak learning rate | $5 \\mathrm{e}-4$ | $5 \\mathrm{e}-4$ |\n| Learning rate scheduler | Inverse sqrt | Polynomial decay |\n| Optimizer | Adam | Adam |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-8$ | $1 \\mathrm{e}-6$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.98)$ | $(0.9,0.98)$ |\n| Weight decay | 0.01 | 0.01 |\n| Gradient clipping | 0.0 | 0 |\n| Hidden dropout | 0.1 | 0.1 |\n| Attention dropout | 0 | 0.1 |\n\n\n[^0]:    ${ }^{\\star}$ Equal contribution.",
    "transnormer-23": "${ }^{\\square}$ The corresponding author (Email: zhongyiran@gmail.com). This work was done when Weixuan Sun and Yiran Zhong were in the SenseTime Research. [^1]:    ${ }^{1}$ Here we assume that $f\\left(s_{i j}\\right) \\geq 0$, the conclusion is satisfied in most cases. ${ }^{2}$ Note that $s_{i j}$ is not directly computed in linear attention, but can still be represented in this unified form, see Appendix D for more detailed derivation\n\n[^2]:    ${ }^{3}$ A detailed proof of the upper bound can be found at Appendix B.",
    "transnormer-24": "${ }^{4}$ The proof can be found in Appendix C. [^3]:    ${ }^{5}$ The full derivation can be found in Appendix D. [^4]:    ${ }^{6}$ We assume that the image of $\\phi$ contains vectors arbitrary close to 0 , which is a common case in kernel function. [^5]:    ${ }^{7}$ Here, the function $f(\\mathbf{X})$ is applied element-wise to the matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$, that is, $[f(\\mathbf{X})]_{i j}=\\left[f\\left(x_{i j}\\right)\\right]$\n\n[^6]:    ${ }^{8}$ XAttention stands for vanilla/norm attention. "
}