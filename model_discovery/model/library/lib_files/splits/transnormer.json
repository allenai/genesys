{
    "transnormer-0": "The Devil in Linear Transformer\n\n\u22c6Zhen Qin1, \u22c6Xiaodong Han1, Weixuan Sun2,3, Dongxu Li2, Lingpeng Kong4,5, Nick Barnes2, Yiran Zhong4 1SenseTime Research, 2Australian National University, 3OPPO Research Institute, 4Shanghai AI Laboratory, 5The University of Hong Kong https://github.com/OpenNLPLab/Transnormer\n\nAbstract\n\nLinear transformers aim to reduce the quadratic space-time complexity of vanilla transformers.",
    "transnormer-1": "However, they usually suffer from degraded performances on various tasks and corpora. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, TransNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient.",
    "transnormer-2": "The code is available at TransNormer.",
    "transnormer-3": "\u2020\u2020\u22c6Equal contribution. The corresponding author (Email: zhongyiran@gmail.com). This work was done when Weixuan Sun and Yiran Zhong were in the SenseTime Research. 1 Introduction\n\nTransformer models show great performance on a wide range of natural language processing and computer vision tasks Qin et al.",
    "transnormer-4": "(2022); Sun et al. (2022b); Cheng et al. (2022a, b); Zhou et al. (2022). One issue of the vanilla transformer model lies in its quadratic space-time complexity with respect to the input length. Various prior works attempt to alleviate this inefficiency Zaheer et al.",
    "transnormer-5": "(2020); Beltagy et al. (2020); Tay et al. (2020a); Kitaev et al. (2020); Child et al. (2019); Liu et al. (2022); Sun et al. (2022b). In this work, we focus on a particular subset of these methods, known as kernel-based linear transformers Choromanski et al.",
    "transnormer-6": "(2020); Wang et al. (2020); Katharopoulos et al. (2020); Peng et al. (2020); Qin et al. (2022) considering their desirable linear space-time complexity. Despite their space-time efficiency, linear transformers are not always in favor for practical adoption, largely due to the degraded performance than the vanilla model. To address this issue, we take a close look at existing kernel-based linear transformers and identify two deficiencies that lead to such a performance gap. Unbounded gradients. Most existing linear transformers inherit attention formulation from the vanilla transformer, which scales attention scores to ensure they are bounded within . However, we theoretically show that such a scaling strategy renders unbounded gradients for linear transformer models. As a result, the unbounded gradients empirically lead to unstable convergence as our preliminary experiments suggest. Attention dilution. Previous works Titsias (2016); Jang et al. (2016); Gao and Pavel (2017); Qin et al. (2022); Sun et al. (2022b, a) suggest that in vanilla transformer, softmax attention maps tend to be local. In contrast, as shown in Fig 2, we observe that linear transformers often trivially distribute attention scores over the entire sequence even in early layers. Due to this issue, which we refer as attention dilution, important local information is less well preserved in linear models, resulting in inferior performance. This negative impact of attention dilution is also evidenced by the performance drop in our controlled experiments if partly replacing vanilla attention in transformer layers with linear attention ones. To mitigate these issues, we propose a linear transformer model, called TransNormer, which shows better performance than vanilla transformer on a wide range of task while being significantly faster during runtime, as shown in Fig. 1. To avoid the unbounded gradients, we introduce NormAttention, which gets rid of scaling over attention matrices while appending an additional normalization only after the attention layer. The choice of the normalization operator is unrestricted, for example, LayerNorm Ba et al. (2016) or RMSNorm Zhang and Sennrich (2019) both serve the purpose. We show empirical results demonstrating that with NormAttention, the gradients are more stable during training, which in turn leads to more consistent convergence. To alleviate the attention dilution issue, we modify the vanilla attention and allow each token to only attend to its neighbouring tokens, resulting in a diagonal attention. To mimic the behaviors on local semantics of the vanilla transformer, we employ the diagonal attention on early layers while using NormAttention for later ones. In this way, we encourage the model to capture both local and global language context. Note that our diagonal attention can be efficiently computed such that the overall linear space-time complexity of TransNormer is preserved. We perform extensive experiments on standard tasks, where TransNormer demonstrates lower language modeling perplexities on WikiText-103 and overall higher text classification accuracy on GLUE than vanilla model and other competing methods. In addition, on the challenging Long-Range Arena benchmark, TransNormer also shows favorable results while being faster and more scalable with longer inputs during both training and inference time. 2 Background and related work\n\nWe first briefly review vanilla transformer Vaswani et al. (2017) and its efficient variants. The key component of transformers is the self-attention, which operates on query , key and value matrices; each of them is the image of a linear projection taking as input:\n\n\ud835\udc10 = \ud835\udc17\ud835\udc16 Q , \ud835\udc0a = \ud835\udc17\ud835\udc16 K , \ud835\udc15 = \ud835\udc17\ud835\udc16 V \u2208 \u211d n \u00d7 d , formulae-sequence \ud835\udc10 subscript \ud835\udc17\ud835\udc16 \ud835\udc44 formulae-sequence \ud835\udc0a subscript \ud835\udc17\ud835\udc16 \ud835\udc3e \ud835\udc15 subscript \ud835\udc17\ud835\udc16 \ud835\udc49 superscript \u211d \ud835\udc5b \ud835\udc51 \\textstyle\\begin{gathered}\\mathbf{Q}=\\mathbf{X}\\mathbf{W}_{Q},\\mathbf{K}=\\mathbf{X}\\mathbf{W}_{K},\\mathbf{V}=\\mathbf{X}\\mathbf{W}_{V}\\in\\mathbb{R}^{n\\times d},\\end{gathered} (1)\n\nwith the input length, the hidden dimension. The output is formulated as:\n\n\ud835\udc0e = Softmax \u200b ( \ud835\udc10\ud835\udc0a \ud835\uddb3 / d ) \u200b \ud835\udc15 , \ud835\udc0e Softmax superscript \ud835\udc10\ud835\udc0a \ud835\uddb3 \ud835\udc51 \ud835\udc15 \\textstyle\\mathbf{O}=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\mkern-1.5mu\\mathsf{T}}/\\sqrt{d})\\mathbf{V}, (2)\n\nwhere the step renders quadratic space-time complexity with respect to the input length, making it prohibitive for vanilla transformer to scale to long input sequences.",
    "transnormer-7": "To address this issue, numerous efficient transformers have been explored in the literature. These methods can be generally categorized into two families, i.e., pattern based methods and kernel based methods. Pattern based methods Zaheer et al. (2020); Beltagy et al. (2020); Tay et al. (2020a); Kitaev et al. (2020); Child et al. (2019) sparsify the attention calculation with handcrafted or learnable masking patterns. Kernel-based methods adopt kernel functions to decompose softmax attention, which reduces the theoretical space-time complexity to linear. In this paper, we refer the kernel-based variants as linear transformers for simplicity. In the kernel-based methods Choromanski et al. (2020); Katharopoulos et al. (2020); Peng et al. (2020); Qin et al.",
    "transnormer-8": "(2022); Zheng et al. (2022); Wang et al. (2020), a kernel function maps queries and keys to their hidden representations. Then the output of the linear attention can be rewritten as:\n\n\ud835\udc0e \ud835\udc0e \\displaystyle\\mathbf{O} = \ud835\udeab \u2212 1 \u200b \u03d5 \u200b ( \ud835\udc10 ) \u200b [ \u03d5 \u200b ( \ud835\udc0a ) \ud835\uddb3 \u200b \ud835\udc15 ] , absent superscript \ud835\udeab 1 italic-\u03d5 \ud835\udc10 delimited-[] italic-\u03d5 superscript \ud835\udc0a \ud835\uddb3 \ud835\udc15 \\displaystyle=\\mathbf{\\Delta}^{-1}\\phi(\\mathbf{Q})[\\phi(\\mathbf{K})^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{V}], (3) \ud835\udeab \ud835\udeab \\displaystyle\\mathbf{\\Delta} = diag \u200b ( \u03d5 \u200b ( \ud835\udc10 ) \u200b [ \u03d5 \u200b ( \ud835\udc0a ) \ud835\uddb3 \u200b \ud835\udfcf n ] ) .",
    "transnormer-9": "absent diag italic-\u03d5 \ud835\udc10 delimited-[] italic-\u03d5 superscript \ud835\udc0a \ud835\uddb3 subscript 1 \ud835\udc5b \\displaystyle=\\mathrm{diag}(\\phi(\\mathbf{Q})[\\phi(\\mathbf{K})^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{1}}_{n}]). where the product of keys and values are computed to avoid the quadratic matrix. Existing methods mainly differ in the design of kernel functions. For example, Choromanski et al. (2020) and Katharopoulos et al. (2020) adopt activation function to process query and key. Wang et al. (2020) assumes attention matrices are low-rank. Peng et al. (2020) and Zheng et al. (2022) approximate softmax under constrained theoretical bounds. Qin et al. (2022) propose a linear alternative to the attention based on empirical properties of the softmax function. These methods focus on either approximating or altering the softmax operator while preserving its properties. Compared with the vanilla transformer, these methods often trade performance for efficiency, usually resulting in worse task performance. In this paper, we argue that there are two essential reasons leading to such a performance gap, discussed in detail as follows. 3 The devil in linear attention\n\nIn this section, we motivate the design principles of TransNormer by providing theoretical evidence for the unbounded gradients, and empirical results showing the adverse influence of attention dilution. 3.1 Unbounded gradients\n\nFew work on linear transformers analyzes their gradients during training. Our first key observation is that kernel-based linear attention suffer from unbounded gradients, causing unstable convergence during training. In the following, we highlight the main theoretical results while referring readers to Appendix D for the full derivation. Consider a self-attention module, either vanilla or linear attention. Its attention matrix can be represented in the following unified form 111Here we assume that , the conclusion is satisfied in most cases.:\n\np i \u200b j = f \u200b ( s i \u200b j ) \u2211 k = 1 n f \u200b ( s i \u200b k ) , f : \u211d \u2192 \u211d . : subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 \ud835\udc53 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc58 1 \ud835\udc5b \ud835\udc53 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \ud835\udc53 \u2192 \u211d \u211d p_{ij}=\\frac{f(s_{ij})}{\\sum_{k=1}^{n}f(s_{ik})},f:\\mathbb{R}\\to\\mathbb{R}. (4)\n\nVanilla and linear attention differ mainly in their computation of token-wise similarities 222Note that is not directly computed in linear attention, but can still be represented in this unified form, see Appendix D for more detailed derivation. In vanilla attention, is computed as:\n\ns i \u200b j = \ud835\udc2a i \ud835\uddb3 \u200b \ud835\udc24 j / d , f \u200b ( x ) = exp \u2061 ( x ) , formulae-sequence subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc2a \ud835\udc56 \ud835\uddb3 subscript \ud835\udc24 \ud835\udc57 \ud835\udc51 \ud835\udc53 \ud835\udc65 \ud835\udc65 s_{ij}=\\mathbf{q}_{i}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{k}_{j}/\\sqrt{d},f(x)=\\exp(x), (5)\n\nwhile for linear attentions, can be decomposed using a kernel function , such that:\n\ns i \u200b j = \u03d5 \u200b ( \ud835\udc2a i ) \ud835\uddb3 \u200b \u03d5 \u200b ( \ud835\udc24 j ) , f \u200b ( x ) = x . formulae-sequence subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc2a \ud835\udc56 \ud835\uddb3 italic-\u03d5 subscript \ud835\udc24 \ud835\udc57 \ud835\udc53 \ud835\udc65 \ud835\udc65 s_{ij}=\\phi(\\mathbf{q}_{i})^{\\mkern-1.5mu\\mathsf{T}}\\phi(\\mathbf{k}_{j}),f(x)=x. (6)\n\nGiven the above definitions, the gradients of the attention matrix is derived as:\n\n\u2202 p i \u200b j \u2202 s i \u200b k = f \u2032 \u200b ( s i \u200b k ) f \u200b ( s i \u200b k ) \u200b ( 1 j = k \u200b p i \u200b j \u2212 p i \u200b j \u200b p i \u200b k ) subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 superscript \ud835\udc53 \u2032 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \ud835\udc53 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 subscript 1 \ud835\udc57 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 \\frac{\\partial p_{ij}}{\\partial s_{ik}}=\\frac{f^{\\prime}(s_{ik})}{f(s_{ik})}\\left(1_{j=k}p_{ij}-p_{ij}p_{ik}\\right) (7)\n\nTherefore, for the vanilla attention, the partial derivative is:\n\nf \u2032 \u200b ( x ) superscript \ud835\udc53 \u2032 \ud835\udc65 \\displaystyle f^{\\prime}(x) = exp \u2061 ( x ) = f \u200b ( x ) absent \ud835\udc65 \ud835\udc53 \ud835\udc65 \\displaystyle=\\exp(x)=f(x) (8) \u2202 p i \u200b j \u2202 s i \u200b k subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \\displaystyle\\frac{\\partial p_{ij}}{\\partial s_{ik}} = 1 j = k \u200b p i \u200b j \u2212 p i \u200b j \u200b p i \u200b k absent subscript 1 \ud835\udc57 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 \\displaystyle=1_{j=k}p_{ij}-p_{ij}p_{ik} = { p i \u200b k \u2212 p i \u200b k \u200b p i \u200b k \u2208 [ 0 , 1 / 4 ] j = k \u2212 p i \u200b j \u200b p i \u200b k \u2208 [ \u2212 1 / 4 , 0 ] j \u2260 k absent cases subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 0 1 4 \ud835\udc57 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 4 0 \ud835\udc57 \ud835\udc58 \\displaystyle=\\begin{cases}p_{ik}-p_{ik}p_{ik}\\in[0,1/4]&j=k\\\\\n-p_{ij}p_{ik}\\in[-1/4,0]&j\\neq k\\end{cases}\n\nand it is bounded as:\n\n| \u2202 p i \u200b j \u2202 s i \u200b k | \u2264 1 4 . subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 1 4 \\left|\\frac{\\partial p_{ij}}{\\partial s_{ik}}\\right|\\leq\\frac{1}{4}. (9)\n\nHowever, for linear attentions, we have:\n\nf \u2032 \u200b ( x ) superscript \ud835\udc53 \u2032 \ud835\udc65 \\displaystyle f^{\\prime}(x) = 1 absent 1 \\displaystyle=1 (10) \u2202 p i \u200b j \u2202 s i \u200b k subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \\displaystyle\\frac{\\partial p_{ij}}{\\partial s_{ik}} = 1 s i \u200b k \u200b ( 1 j = k \u200b p i \u200b j \u2212 p i \u200b j \u200b p i \u200b k ) absent 1 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 subscript 1 \ud835\udc57 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 \\displaystyle=\\frac{1}{s_{ik}}\\left(1_{j=k}p_{ij}-p_{ij}p_{ik}\\right) = { 1 s i \u200b k \u200b ( p i \u200b k \u2212 p i \u200b k \u200b p i \u200b k ) j = k 1 s i \u200b k \u200b ( \u2212 p i \u200b j \u200b p i \u200b k ) j \u2260 k absent cases 1 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 \ud835\udc57 \ud835\udc58 1 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 \ud835\udc57 \ud835\udc58 \\displaystyle=\\begin{cases}\\frac{1}{s_{ik}}\\left(p_{ik}-p_{ik}p_{ik}\\right)&j=k\\\\\n\\frac{1}{s_{ik}}(-p_{ij}p_{ik})&j\\neq k\\end{cases}\n\nand333A detailed proof of the upper bound can be found at Appendix B. | \u2202 p i \u200b j \u2202 s i \u200b k | \u2264 1 4 \u200b | s i \u200b k | . subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 1 4 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \\left|\\frac{\\partial p_{ij}}{\\partial s_{ik}}\\right|\\leq\\frac{1}{4|s_{ik}|}. (11)\n\nSince can be arbitrarily large, the gradient of linear attention has no upper bound. On the other hand, we can also show that the gradient of linear attention has no lower bound444The proof can be found in Appendix C.:\n\nProposition 3.1. , there exists , such that:\n\n| \u2202 p i \u200b j \u2202 s i \u200b k | > M . subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \ud835\udc40 \\left|\\frac{\\partial p_{ij}}{\\partial s_{ik}}\\right|>M. (12)\n\nThe unbounded gradients lead to less stable optimization and worse convergence results in our preliminary studies. 3.2 Attention dilution\n\nIt is a known property of vanilla attention to emphasize on neighbouring tokens Titsias (2016); Qin et al. (2022). However, this property does not directly inherit to the linear transformer variants. To quantify the attention dilution issue, we introduce a metric called locally accumulated attention score, which measures how much attention scores are distributed within the local neighbourhood of a particular token. For an input sequence of length , consider a local neighbourhood centering around token of total length , with the ratio relative to the total input, the locally accumulated attention score for token is defined as . A higher score indicates the particular attention layer concentrates on the local neighbourhood, while a lower score tends to indicate the issue of attention dilution, where scores are distributed more evenly to local and distant tokens. For example, means that that 40% of the neighbors around \u2019th token contribute 60% of the attention score. In Fig. 2 (a), we compare locally accumulated attention scores (y-axis) for vanilla transformer and linear transformer, with varying sizes of neighbourhood by ratio (x-axis). We show the average score over each position across the entire sequence. It can be seen that the area under the vanilla model curve is significantly larger than that of the linear model. This provides evidence that the vanilla attention is more concentrated locally, while the linear transformer suffers from the issue of attention dilution. This is further qualitatively supported by Fig. 2 (b), where the attention maps for vanilla model are more concentrated than the linear model. 4 Method\n\nBased on the aforementioned observations, we propose a new linear transformer network called TransNormer that addresses the above two limitations of current linear transformers.",
    "transnormer-10": "The overall architecture is shown in Fig. 3. 4.1 The overall architecture\n\nVanilla attention suffers less in attention dilution while linear attention is more efficient and scalable on longer sequences. This motivate us to design a method that exploits the best of the both worlds by using these mechanisms in combined. Specifically, our network consists of two types of attention: DiagAttention for the early stage of the model and NormAttention for the later stage. The former addresses the attention dilution issue and the later aims to stabilize training gradients. Note that by properly reshaping the inputs, the diagonal attention can be efficiently computed in linear space-time, thus preserving the overall linear complexity. 4.2 NormAttention\n\nAs proved in Sec. 3, the scaling operation, i.e., the denominator in Eq. 4, in the linear transformers hinder the optimization due to the unbounded gradients. To solve this issue, we propose to remove the scaling operation in the linear transformers. However, as shown in Table. 1, directly removing the scaling operation leads to critical performance drop since the attention map becomes unbounded in the forward pass. Therefore, an alternative is required to bound both attention maps during forward and their gradients during backward passes in linear attentions. Our proposed solution is simple yet effective. Given a linear attention, the attention without scaling can be formulated as:\n\n\ud835\udc0e = \ud835\udc10 \u200b ( \ud835\udc0a \ud835\uddb3 \u200b \ud835\udc15 ) . \ud835\udc0e \ud835\udc10 superscript \ud835\udc0a \ud835\uddb3 \ud835\udc15 \\mathbf{O}=\\mathbf{Q}(\\mathbf{K}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{V}). (13)\n\nWe empirically find that we can apply an arbitrary normalization on this attention to bound it, which leads to our NormAttention as:\n\n\ud835\udc0e norm = XNorm \u200b ( \ud835\udc10 \u200b ( \ud835\udc0a \ud835\uddb3 \u200b \ud835\udc15 ) ) , subscript \ud835\udc0e norm XNorm \ud835\udc10 superscript \ud835\udc0a \ud835\uddb3 \ud835\udc15 \\mathbf{O}_{\\text{norm}}=\\mathrm{XNorm}(\\mathbf{Q}(\\mathbf{K}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{V})), (14)\n\nwhere the can be Layernorm(Ba et al., 2016) or RMSNorm (Zhang and Sennrich, 2019) and etc. We use the RMSNorm in our experiments as it is slightly faster than other options. It can be proved that the gradients of NormAttention is bounded by555The full derivation can be found in Appendix D.:\n\n| \u2202 \u2112 \u2202 s i \u200b j | \u2264 3 \u200b c 1 \u200b c 2 \u200b d 2 \u200b \u03f5 < \u221e , \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 3 subscript \ud835\udc50 1 subscript \ud835\udc50 2 \ud835\udc51 2 italic-\u03f5 \\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right|\\leq\\frac{3c_{1}c_{2}d}{2\\sqrt{\\epsilon}}<\\infty, (15)\n\nwhere is the loss function, is the small constant that used in RMSNorm, is the embedding dimension and\n\nc 1 subscript \ud835\udc50 1 \\displaystyle c_{1} = max i = 1 n \u2061 \u2016 \u2207 \ud835\udc0e i \u2112 \u2016 2 < \u221e absent superscript subscript \ud835\udc56 1 \ud835\udc5b subscript norm subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 2 \\displaystyle=\\max_{i=1}^{n}\\|\\nabla_{\\mathbf{O}_{i}}\\mathcal{L}\\|_{2}<\\infty (16) c 2 subscript \ud835\udc50 2 \\displaystyle c_{2} = max i = 1 n \u2061 \u2016 \ud835\udc15 i \u2016 2 < \u221e absent superscript subscript \ud835\udc56 1 \ud835\udc5b subscript norm subscript \ud835\udc15 \ud835\udc56 2 \\displaystyle=\\max_{i=1}^{n}\\|\\mathbf{V}_{i}\\|_{2}<\\infty\n\nTo demonstrate the gradients stability of the NormAttention, we compare the relative standard deviation of gradients during each training iterations to other linear transformers and vanilla transformer.",
    "transnormer-11": "Specifically, we train our model for 50k iterations with RoBERTa architecture on the WikiText103 Merity et al.",
    "transnormer-12": "(2017) and obtain the relative standard deviation of all iterations\u2019 gradients. As shown in Table 2, existing linear methods Choromanski et al. (2020); Katharopoulos et al. (2020) have substantially higher deviations compared to vanilla attention, which leads to inferior results. The NormAttention produces more stable gradients, which validates the effectiveness of our method. 4.3 DiagAttention\n\nTo better understand the design principles, we show in Table 3 that by replacing partial layers of linear transformers with vanilla attention, the performance on language modeling is evidently improved. The results also suggest that capturing more local information in early layers are more helpful than otherwise. To this end, we leverage none-overlapped block-based strategy to reduce the space-time complexity of the vanilla attention. Based on the observation in Fig. 2, we utilize a strict diagonal blocked pattern to constraint the attention in a certain range. Since the attentions are calculated inside each block, the computation complexity of our diagonal attention is , where is sequence length , is the block size and is feature dimension. When , the complexity scales linearly respect to the sequence length . In subsequent sections, we use DiagAttention to refer to Diagonal attention. We empirically find that applying DiagAttention to the later stages of a model hurts the performance as shown in Table. 9. It indicates that the model requires a global field of view in the later layers, which also justifies our choices of NormAttention in later layers of TransNormer. 5 Experiments\n\nIn this section, we compare our method to other linear transformers and the vanilla transformer on autoregressive language modeling, bidirectional language modeling as well as the Long Range Arena benchmark (Tay et al., 2020b).",
    "transnormer-13": "We also provide an extensive ablation study to vindicate our choice in designing the TransNormer. We validate our method on two variants of the TransNormer. The TransNormer T1 uses the ReLA attention (Zhang et al., 2021) in the DiagAttention and the elu as the activation function in the NormAttention. The TransNormer T2 uses the attention (Vaswani et al., 2017) in the DiagAttention and the 1+elu as the activation function in the NormAttention. For experiments, we first study the autoregressive language modeling on WikiText-103 (Merity et al., 2017) in section 5.2. Then in section 5.2 we test our method on bidirectional language modeling, which is pre-trained on WikiText-103 (Merity et al., 2017) and then fine-tuned on several downstream tasks from the GLUE benchmark (Wang et al., 2018). Finally, we test TransNormer on the Long-Range Arena benchmark (Tay et al., 2020b) to evaluate its ability in modeling long-range dependencies and efficiencies in section 5.2. 5.1 Settings\n\nWe implement our models in the Fairseq framework (Ott et al., 2019) and train them on 8 V100 GPUS. We use the same training configuration for all competitors and we list detailed hyper-parameters in Appendix F. We choose the FLASH-quad, FLASH (Hua et al., 2022), Transformer-LS (Zhu et al., 2021), Performer (Choromanski et al., 2020), 1+elu (Katharopoulos et al., 2020) as our main competing methods. For the autoregressive language modeling, we use 6 decoder layers (10 layers for the FlASH/FLASH-quad) as our base model and all models are trained on the WikiText-103 dataset (Merity et al., 2017) for 100K steps with a learning rate of . We use the perplexity (PPL) as the evaluation metric. For the bidirectional language modeling, we choose the RoBERTa base (Liu et al., 2019) for all methods. It consists of 12 encoder layers (24 layers for the FLASH and FLASH-quad to match the number of parameters). All models are pre-trained on the WikiText-103 (Merity et al., 2017) for 50K steps with lr=0.005 and fine-tuned on the GLUE dataset (Wang et al., 2018). We use different learning rates among 1e-5, 3e-5, 6e-5, 1e-4 and choosing the best result after fine-tuning for 3 epochs. For the Long-Range Arena benchmark, to make sure it reflect the practical speed in Pytorch platform, we re-implement the benchmark in Pytorch. We adopt the same configuration from the Skyformer Chen et al. (2021) and make sure all models have a similar parameter size. We use the same training hyper parameters for all models as well. 5.2 Results\n\nAutoregressive language modeling\n\nWe report the results in Table 4. It can be found that both TransNormer variants get comparable or better perplexity to the vanilla attention and outperform all existing linear models with a clear margin. For example, compared to previous state-of-the-art linear methods on validation setHua et al. (2022) and test setZhu et al. (2021), TransNormer T2 achieves substantially lower perplexity by 2.31 and 1.58 respectively. It demonstrates the effectiveness of our method in causal models. Bidirectional language modeling\n\nWe show our bidirectional results on the GLUE benchmark in Table. 5. Our method achieves superior performance to all the competing methods in average. On three tasks, i.e., SST-2, MRPC, CoLA, TransNormer reports comprehensively better results than all competing linear methods, such as 4.62 higher on CoLA. Further, one of our variants i.e., TransNormer T1, even outperforms the vanilla attention with a notable margin. It proves the effectiveness of our method in bidirectional language modeling. Long Range Arena Benchmark\n\nThe results before the transformer Long-short (abbr. LS) are taken from the Skyformer Chen et al.",
    "transnormer-14": "(2021). As shown in Table. 6, we achieve either first or second places across all five tasks. In terms of overall results, both TransNormer variants (T1,T2) outperform all other competing methods including vanilla transformer Vaswani et al. (2017), which validates our capability to encode long sequences. 5.3 Speed comparison\n\nWe compare the training and inference speed of the TransNormer with other methods. For a fair and comprehensive comparison, we follow exactly the same configurations of the SkyformerChen et al. (2021) and report step per second under different sequence lengths. Timing is conducted on a Nvidia A6000 GPU with 48G GPU memory. Table. 7 suggests that the vanilla transformer is substantially slow and exhausts GPU memory with sequence longer than 3k. Compared to other efficient transformers, our TransNormer achieves faster speed with comparable GPU memory footprints, while competing efficient methods all report worse results compared to our TransNormer. For instance, compared to FLASH-quad Hua et al. (2022) that achieves previous best linear results on both autoregressive and bidirectional benchmarks, our model performs over 300% faster during training and 150% faster during inference. 5.4 Ablation study\n\nIn this section, we justify our design choice of the TransNormer, including , the selection of the FFN module, and the size of the attention block in DiagAttention. We use the PPL from the Roberta pre-training stage as our evaluation metric. Structure design\n\nAs aforementioned, we empirically choose the first 6 layers as the early stage of the model and the rest as the later stage. We provide the designing ground for this choice in Table. 8. It can be also observed that either choosing the DiagAttention or NormAttention for the entire model will lead to inferior performance. We also provide the ablation results of swapping the order of the DiagAttention and the NormAttention in Table. 9. Using DiagAttention in the early stage achieves significantly better results than using it on later stage. It further proves our claim that the early stage focuses on neighbouring tokens while the later stage needs long-range attentions. FFN module\n\nWe ablate the selection of the FFN modules in Table. 10. Compared with the traditional FFN (Vaswani et al., 2017), the GLU (Shazeer, 2020) achieves better results. Block size\n\nFrom the Table. 11, we observe clear performance improvements with increased block sizes. However, since the complexity of the DiagAttention is , larger block size leads to heavier computational overhead. We choose a block size as 64 as a trade-off between performance and computational cost. Combination of attentions\n\nFinally, we study the effect that whether we should use both attentions in one layer. In particular, we compare either to 1) use DiagAttention and NormAttention sequentially in a layer with different orders; or to 2) use them in parallel in each attention layer and then concatenate their embedding output. Table. 12 shows that we should not use these attentions sequentially within a layer and apply them in parallel will double the computation complexities without improving the performance. 6 Conclusion\n\nIn this paper, we identified two key issues that cause the inferior performance of existing linear transformer models: 1) unbounded gradients; 2) attention dilution. For the former issue, we proposed a new NormAttention to stabilize the training gradients. For the latter, we develop DiagAttention to force the model concentrate attention in neighbouring tokens. The resultant model TransNormer marries the strength of the vanilla transformers and the linear transformers, outperforming competing linear transformers on both autoregressive and bidirectional language modeling, text classification tasks and the challenging Long-range arena benchmark. Limitations\n\nIn this paper, we identified two main issues of current linear transformers and provided a comprehensive analysis in natural language processing tasks. However, with the booming development of vision transformers, whether they share the same issues of linear NLP transformers is yet to be discovered. We will validate our method on the linear vision transformers in our future work. Ethics Statement\n\nThe proposed technique is beneficial to develop large-scale environment-friendly language models by reducing computing resource demand. Corpus used to train the model is from public web sources, which may contain biased, explicit or improper content. Further assessment and regulation have to be in-place before deploying the model in practice. References\n\nBa et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Chen et al. (2021) Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. 2021. Skyformer: Remodel self-attention with gaussian kernel and nystr\u00f6m method. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual. Cheng et al. (2022a) Xuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge. 2022a. Implicit motion handling for video camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13864\u201313873. Cheng et al. (2022b) Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Tom Drummond, Zhiyong Wang, and Zongyuan Ge. 2022b. Deep laparoscopic stereo matching with transformers. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 464\u2013474.",
    "transnormer-15": "Springer. Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. Choromanski et al. (2020) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794. Gao and Pavel (2017) Bolin Gao and Lacra Pavel. 2017. On the properties of the softmax function with application in game theory and reinforcement learning.",
    "transnormer-16": "arXiv preprint arXiv:1704.00805. Hua et al. (2022) Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. 2022. Transformer quality in linear time.",
    "transnormer-17": "arXiv preprint arXiv:2202.10447. Jang et al. (2016) Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax.",
    "transnormer-18": "arXiv preprint arXiv:1611.01144. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156\u20135165. PMLR. Kitaev et al. (2020) Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer.",
    "transnormer-19": "arXiv preprint arXiv:2001.04451. Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.",
    "transnormer-20": "arXiv preprint arXiv:1907.11692. Liu et al. (2022) Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955. Merity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. 5th International Conference on Learning Representations, ICLR, Toulon, France. Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038. Peng et al. (2020) Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2020. Random feature attention. In International Conference on Learning Representations. Qin et al. (2022) Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations. Shazeer (2020) Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202. Sun et al. (2022a) Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, and Yiran Zhong. 2022a. Locality matters: A locality-biased linear attention for automatic speech recognition. arXiv preprint arXiv:2203.15609. Sun et al. (2022b) Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. 2022b. Vicinity vision transformer. arXiv preprint arXiv:2206.10552. Tay et al. (2020a) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020a. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438\u20139447. PMLR. Tay et al. (2020b) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020b. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations. Titsias (2016) Michalis K Titsias. 2016. One-vs-each approximation to softmax for scalable estimation of probabilities.",
    "transnormer-21": "arXiv preprint arXiv:1609.07410. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008. Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355. Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.",
    "transnormer-22": "arXiv preprint arXiv:2006.04768. Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. In NeurIPS. Zhang and Sennrich (2019) Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32, Vancouver, Canada. Zhang et al. (2021) Biao Zhang, Ivan Titov, and Rico Sennrich. 2021. Sparse attention with linear units. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6507\u20136520, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Zheng et al. (2022) Lin Zheng, Chong Wang, and Lingpeng Kong. 2022. Linear complexity randomized self-attention mechanism.",
    "transnormer-23": "arXiv preprint arXiv:2204.04667. Zhou et al. (2022) Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. 2022. Audio-visual segmentation. In European Conference on Computer Vision. Zhu et al. (2021) Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. 2021. Long-short transformer: Efficient transformers for language and vision. In Advances in Neural Information Processing Systems. Appendix\n\nAppendix A Mathematical Notations\n\nWe use bold uppercase letters for matrices(), bold lowercase letters for vectors(), and lowercase letters for scalars(). We represent all vectors as column vectors and denote the th row of matrix by or . We use to denote the norm and to denote the Frobenius norm of the matrix and the vector. The main mathematical symbols are input , (Query), (Key) and (Value), which has the following form:\n\n\ud835\udc17 \ud835\udc17 \\displaystyle\\mathbf{X} = [ \ud835\udc31 1 \ud835\uddb3 \u22ee \ud835\udc31 n \ud835\uddb3 ] \u2208 \u211d n \u00d7 d , absent delimited-[] matrix superscript subscript \ud835\udc31 1 \ud835\uddb3 \u22ee superscript subscript \ud835\udc31 \ud835\udc5b \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\left[\\begin{matrix}\\mathbf{x}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\end{matrix}\\right]\\in\\mathbb{R}^{n\\times d}, (17) \ud835\udc10 \ud835\udc10 \\displaystyle\\mathbf{Q} = [ \ud835\udc2a 1 \ud835\uddb3 \u22ee \ud835\udc2a n \ud835\uddb3 ] = \ud835\udc17\ud835\udc16 Q = [ \ud835\udc31 1 \ud835\uddb3 \u200b \ud835\udc16 Q \u22ee \ud835\udc31 n \ud835\uddb3 \u200b \ud835\udc16 Q ] \u2208 \u211d n \u00d7 d , absent delimited-[] matrix superscript subscript \ud835\udc2a 1 \ud835\uddb3 \u22ee superscript subscript \ud835\udc2a \ud835\udc5b \ud835\uddb3 subscript \ud835\udc17\ud835\udc16 \ud835\udc44 delimited-[] matrix superscript subscript \ud835\udc31 1 \ud835\uddb3 subscript \ud835\udc16 \ud835\udc44 \u22ee superscript subscript \ud835\udc31 \ud835\udc5b \ud835\uddb3 subscript \ud835\udc16 \ud835\udc44 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\left[\\begin{matrix}\\mathbf{q}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\\\\n\\vdots\\\\\n\\mathbf{q}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\end{matrix}\\right]=\\mathbf{XW}_{Q}=\\left[\\begin{matrix}\\mathbf{x}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{W}_{Q}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{W}_{Q}\\end{matrix}\\right]\\in\\mathbb{R}^{n\\times d}, \ud835\udc0a \ud835\udc0a \\displaystyle\\mathbf{K} = [ \ud835\udc24 1 \ud835\uddb3 \u22ee \ud835\udc24 n \ud835\uddb3 ] = \ud835\udc17\ud835\udc16 K = [ \ud835\udc31 1 \ud835\uddb3 \u200b \ud835\udc16 K \u22ee \ud835\udc31 n \ud835\uddb3 \u200b \ud835\udc16 K ] \u2208 \u211d n \u00d7 d , absent delimited-[] matrix superscript subscript \ud835\udc24 1 \ud835\uddb3 \u22ee superscript subscript \ud835\udc24 \ud835\udc5b \ud835\uddb3 subscript \ud835\udc17\ud835\udc16 \ud835\udc3e delimited-[] matrix superscript subscript \ud835\udc31 1 \ud835\uddb3 subscript \ud835\udc16 \ud835\udc3e \u22ee superscript subscript \ud835\udc31 \ud835\udc5b \ud835\uddb3 subscript \ud835\udc16 \ud835\udc3e superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\left[\\begin{matrix}\\mathbf{k}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\\\\n\\vdots\\\\\n\\mathbf{k}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\end{matrix}\\right]=\\mathbf{XW}_{K}=\\left[\\begin{matrix}\\mathbf{x}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{W}_{K}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{W}_{K}\\end{matrix}\\right]\\in\\mathbb{R}^{n\\times d}, \ud835\udc15 \ud835\udc15 \\displaystyle\\mathbf{V} = [ \ud835\udc2f 1 \ud835\uddb3 \u22ee \ud835\udc2f n \ud835\uddb3 ] = \ud835\udc17\ud835\udc16 V = [ \ud835\udc31 1 \ud835\uddb3 \u200b \ud835\udc16 V \u22ee \ud835\udc31 n \ud835\uddb3 \u200b \ud835\udc16 V ] \u2208 \u211d n \u00d7 d , absent delimited-[] matrix superscript subscript \ud835\udc2f 1 \ud835\uddb3 \u22ee superscript subscript \ud835\udc2f \ud835\udc5b \ud835\uddb3 subscript \ud835\udc17\ud835\udc16 \ud835\udc49 delimited-[] matrix superscript subscript \ud835\udc31 1 \ud835\uddb3 subscript \ud835\udc16 \ud835\udc49 \u22ee superscript subscript \ud835\udc31 \ud835\udc5b \ud835\uddb3 subscript \ud835\udc16 \ud835\udc49 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\left[\\begin{matrix}\\mathbf{v}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\\\\n\\vdots\\\\\n\\mathbf{v}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\end{matrix}\\right]=\\mathbf{XW}_{V}=\\left[\\begin{matrix}\\mathbf{x}_{1}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{W}_{V}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{W}_{V}\\end{matrix}\\right]\\in\\mathbb{R}^{n\\times d},\n\nwhere . Appendix B Proof of gradients\u2019 upper bound\n\nIn this part, we will proof the bound in (8) and (10), all we need to prove is:\n\n0 \u2264 p i \u200b k \u200b ( 1 \u2212 p i \u200b k ) \u2264 1 4 , 0 \u2264 p i \u200b j \u200b p i \u200b k \u2264 1 4 . formulae-sequence 0 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 4 0 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 4 0\\leq p_{ik}(1-p_{ik})\\leq\\frac{1}{4},0\\leq p_{ij}p_{ik}\\leq\\frac{1}{4}. (18)\n\nWe adopt the theorem that geometric mean is bounded by arithmetic mean, i.e.,\n\na \u200b b \u2264 a + b 2 \u27fa a \u200b b \u2264 ( a + b 2 ) 2 , \u2200 a , b \u2265 0 . \u27fa \ud835\udc4e \ud835\udc4f \ud835\udc4e \ud835\udc4f 2 formulae-sequence \ud835\udc4e \ud835\udc4f superscript \ud835\udc4e \ud835\udc4f 2 2 for-all \ud835\udc4e \ud835\udc4f 0 \\sqrt{ab}\\leq\\frac{a+b}{2}\\Longleftrightarrow ab\\leq\\left(\\frac{a+b}{2}\\right)^{2},\\forall a,b\\geq 0. (19)\n\nWe take to complete the proof. The first bound can be proven by:\n\n0 \u2264 p i \u200b k \u200b ( 1 \u2212 p i \u200b k ) \u2264 ( p i \u200b k + 1 \u2212 p i \u200b k 2 ) 2 = 1 4 . 0 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 superscript subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 2 2 1 4 0\\leq p_{ik}(1-p_{ik})\\leq\\left(\\frac{p_{ik}+1-p_{ik}}{2}\\right)^{2}=\\frac{1}{4}. (20)\n\nFor the second bound, we first use the fact that:\n\n0 \u2264 p i \u200b j + p i \u200b k \u2264 1 \u21d2 p i \u200b j \u2264 1 \u2212 p i \u200b k . 0 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 \u21d2 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 1 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 0\\leq p_{ij}+p_{ik}\\leq 1\\Rightarrow p_{ij}\\leq 1-p_{ik}. (21)\n\nSo we have:\n\n0 \u2264 p i \u200b j \u200b p i \u200b k \u2264 ( 1 \u2212 p i \u200b k ) \u200b p i \u200b k \u2264 1 4 . 0 subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 1 4 0\\leq p_{ij}p_{ik}\\leq(1-p_{ik})p_{ik}\\leq\\frac{1}{4}. (22)\n\nAppendix C Proof of Proposition 3.1\n\nProof of Proposition 3.1. and kernel function , let666We assume that the image of contains vectors arbitrary close to , which is a common case in kernel function.:\n\n\ud835\udc2a i = \ud835\udc24 j = \u03d5 \u2212 1 \u200b ( \ud835\udc31 0 ) , 0 < \u2016 \ud835\udc31 0 \u2016 2 \u2264 \u03f5 , i , j = 1 , \u2026 , n . formulae-sequence subscript \ud835\udc2a \ud835\udc56 subscript \ud835\udc24 \ud835\udc57 superscript italic-\u03d5 1 subscript \ud835\udc31 0 0 subscript delimited-\u2225\u2225 subscript \ud835\udc31 0 2 italic-\u03f5 \ud835\udc56 \ud835\udc57 1 \u2026 \ud835\udc5b \\begin{gathered}\\mathbf{q}_{i}=\\mathbf{k}_{j}=\\phi^{-1}(\\mathbf{x}_{0}),\\\\\n0<\\|\\mathbf{x}_{0}\\|_{2}\\leq\\sqrt{\\epsilon},i,j=1,\\ldots,n.\\end{gathered} (23)\n\nThen\n\n\u03d5 \u200b ( \ud835\udc2a i ) = \u03d5 \u200b ( \ud835\udc24 j ) = \ud835\udc31 0 , i , j = 1 , \u2026 , n . formulae-sequence italic-\u03d5 subscript \ud835\udc2a \ud835\udc56 italic-\u03d5 subscript \ud835\udc24 \ud835\udc57 subscript \ud835\udc31 0 \ud835\udc56 \ud835\udc57 1 \u2026 \ud835\udc5b \\phi(\\mathbf{q}_{i})=\\phi(\\mathbf{k}_{j})=\\mathbf{x}_{0},i,j=1,...,n. (24)\n\nSo\n\ns i \u200b j = \u03d5 \u200b ( \ud835\udc2a i ) \ud835\uddb3 \u200b \u03d5 \u200b ( \ud835\udc24 j ) = \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u2208 ( 0 , \u03f5 ] , subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc2a \ud835\udc56 \ud835\uddb3 italic-\u03d5 subscript \ud835\udc24 \ud835\udc57 superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 0 italic-\u03f5 s_{ij}=\\phi(\\mathbf{q}_{i})^{\\mkern-1.5mu\\mathsf{T}}\\phi(\\mathbf{k}_{j})={\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}\\in(0,\\epsilon], (25)\n\nand\n\np i \u200b j = s i \u200b j \u2211 k = 1 n s i \u200b k = \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u2211 k = 1 n \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 = 1 n . subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc58 1 \ud835\udc5b subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 superscript subscript \ud835\udc58 1 \ud835\udc5b superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 1 \ud835\udc5b p_{ij}=\\frac{s_{ij}}{\\sum_{k=1}^{n}s_{ik}}=\\frac{{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}}{\\sum_{k=1}^{n}{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}}=\\frac{1}{n}. (26)\n\nAccording to (10), we have:\n\n\u2202 p i \u200b j \u2202 s i \u200b k subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \\displaystyle\\frac{\\partial p_{ij}}{\\partial s_{ik}} = { 1 \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u200b 1 n \u200b ( 1 \u2212 1 n ) j = k \u2212 1 \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u200b 1 n 2 j \u2260 k , absent cases 1 superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 1 \ud835\udc5b 1 1 \ud835\udc5b \ud835\udc57 \ud835\udc58 1 superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 1 superscript \ud835\udc5b 2 \ud835\udc57 \ud835\udc58 \\displaystyle=\\begin{cases}\\frac{1}{{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}}\\frac{1}{n}(1-\\frac{1}{n})&j=k\\\\\n-\\frac{1}{{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}}\\frac{1}{n^{2}}&j\\neq k\\end{cases}, (27) | \u2202 p i \u200b j \u2202 s i \u200b k | subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc58 \\displaystyle\\left|\\frac{\\partial p_{ij}}{\\partial s_{ik}}\\right| = { 1 \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u200b 1 n \u200b ( 1 \u2212 1 n ) j = k 1 \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u200b 1 n 2 j \u2260 k absent cases 1 superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 1 \ud835\udc5b 1 1 \ud835\udc5b \ud835\udc57 \ud835\udc58 1 superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 1 superscript \ud835\udc5b 2 \ud835\udc57 \ud835\udc58 \\displaystyle=\\begin{cases}\\frac{1}{{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}}\\frac{1}{n}(1-\\frac{1}{n})&j=k\\\\\n\\frac{1}{{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}}\\frac{1}{n^{2}}&j\\neq k\\end{cases} \u2265 { 1 \u03f5 \u200b n \u200b ( 1 \u2212 1 n ) j = k 1 \u03f5 \u200b n 2 j \u2260 k .",
    "transnormer-24": "absent cases 1 italic-\u03f5 \ud835\udc5b 1 1 \ud835\udc5b \ud835\udc57 \ud835\udc58 1 italic-\u03f5 superscript \ud835\udc5b 2 \ud835\udc57 \ud835\udc58 \\displaystyle\\geq\\begin{cases}\\frac{1}{\\epsilon n}(1-\\frac{1}{n})&j=k\\\\\n\\frac{1}{\\epsilon n^{2}}&j\\neq k\\end{cases}. Let , then , so . Appendix D Analyze the gradient of each method\n\nIn this section, let\u2019s consider a one-layer Transformer, for a multi-layer Transformer, we can prove our conclusion using induction. We begin this section by introducing some mathematical notations. D.1 Notations\n\nIn vanilla attention, we have:\n\n\ud835\udc12 \ud835\udc12 \\displaystyle\\mathbf{S} = \ud835\udc10\ud835\udc0a \ud835\uddb3 \u2208 \u211d n \u00d7 n , absent superscript \ud835\udc10\ud835\udc0a \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\mathbf{Q}\\mathbf{K}^{\\mkern-1.5mu\\mathsf{T}}\\in\\mathbb{R}^{n\\times n}, (28) \ud835\udc0f \ud835\udc0f \\displaystyle\\mathbf{P} = Softmax \u200b ( \ud835\udc12 ) \u2208 \u211d n \u00d7 n , absent Softmax \ud835\udc12 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\mathrm{Softmax}(\\mathbf{S})\\in\\mathbb{R}^{n\\times n}, \ud835\udc0e \ud835\udc0e \\displaystyle\\mathbf{O} = \ud835\udc0f\ud835\udc15 \u2208 \u211d n \u00d7 d . absent \ud835\udc0f\ud835\udc15 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\mathbf{P}\\mathbf{V}\\in\\mathbb{R}^{n\\times d}. In linear attention, we have:\n\n\ud835\udc12 \ud835\udc12 \\displaystyle\\mathbf{S} = \u03d5 \u200b ( \ud835\udc10 ) \u200b \u03d5 \u200b ( \ud835\udc0a ) \ud835\uddb3 \u2208 \u211d n \u00d7 n , absent italic-\u03d5 \ud835\udc10 italic-\u03d5 superscript \ud835\udc0a \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^{\\mkern-1.5mu\\mathsf{T}}\\in\\mathbb{R}^{n\\times n}, (29) \ud835\udeab \ud835\udeab \\displaystyle\\mathbf{\\Delta} = diag \u200b ( \ud835\udc12\ud835\udfcf n ) \u2208 \u211d n \u00d7 n , absent diag subscript \ud835\udc12\ud835\udfcf \ud835\udc5b superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\mathrm{diag}(\\mathbf{S}{\\mathbf{1}}_{n})\\in\\mathbb{R}^{n\\times n}, \ud835\udc0f \ud835\udc0f \\displaystyle\\mathbf{P} = \ud835\udeab \u2212 1 \u200b \ud835\udc12 \u2208 \u211d n \u00d7 n , absent superscript \ud835\udeab 1 \ud835\udc12 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\mathbf{\\Delta}^{-1}\\mathbf{S}\\in\\mathbb{R}^{n\\times n}, \ud835\udc0e \ud835\udc0e \\displaystyle\\mathbf{O} = \ud835\udc0f\ud835\udc15 \u2208 \u211d n \u00d7 d . absent \ud835\udc0f\ud835\udc15 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\mathbf{P}\\mathbf{V}\\in\\mathbb{R}^{n\\times d}. Although this term is not calculated in linear attention, we discuss it conceptually. Note that the above formulations can be unified into the following form 777Here, the function is applied element-wise to the matrix , that is, :\n\n\ud835\udc12 \ud835\udc12 \\displaystyle\\mathbf{S} = f \u200b ( \u03c8 \u200b ( \ud835\udc10 ) \u200b \u03c8 \u200b ( \ud835\udc0a ) \ud835\uddb3 ) \u2208 \u211d n \u00d7 n , absent \ud835\udc53 \ud835\udf13 \ud835\udc10 \ud835\udf13 superscript \ud835\udc0a \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=f(\\psi(\\mathbf{Q})\\psi(\\mathbf{K})^{\\mkern-1.5mu\\mathsf{T}})\\in\\mathbb{R}^{n\\times n}, (30) \ud835\udeab \ud835\udeab \\displaystyle\\mathbf{\\Delta} = diag \u200b ( \ud835\udc12\ud835\udfcf n ) \u2208 \u211d n \u00d7 n , absent diag subscript \ud835\udc12\ud835\udfcf \ud835\udc5b superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\mathrm{diag}(\\mathbf{S}{\\mathbf{1}}_{n})\\in\\mathbb{R}^{n\\times n}, \ud835\udc0f \ud835\udc0f \\displaystyle\\mathbf{P} = \ud835\udeab \u2212 1 \u200b \ud835\udc12 \u2208 \u211d n \u00d7 n , absent superscript \ud835\udeab 1 \ud835\udc12 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\mathbf{\\Delta}^{-1}\\mathbf{S}\\in\\mathbb{R}^{n\\times n}, \ud835\udc0e \ud835\udc0e \\displaystyle\\mathbf{O} = \ud835\udc0f\ud835\udc15 \u2208 \u211d n \u00d7 d , absent \ud835\udc0f\ud835\udc15 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\mathbf{P}\\mathbf{V}\\in\\mathbb{R}^{n\\times d},\n\nwhere in vanilla attention, we have:\n\n\u03c8 \u200b ( \ud835\udc31 ) = \ud835\udc31 , f \u200b ( x ) = exp \u2061 ( x ) , formulae-sequence \ud835\udf13 \ud835\udc31 \ud835\udc31 \ud835\udc53 \ud835\udc65 \ud835\udc65 \\displaystyle\\psi(\\mathbf{x})=\\mathbf{x},f(x)=\\exp(x), (31)\n\nand in linear attention, we have:\n\n\u03c8 \u200b ( \ud835\udc31 ) = \u03d5 \u200b ( \ud835\udc31 ) , f \u200b ( x ) = x . formulae-sequence \ud835\udf13 \ud835\udc31 italic-\u03d5 \ud835\udc31 \ud835\udc53 \ud835\udc65 \ud835\udc65 \\displaystyle\\psi(\\mathbf{x})=\\phi(\\mathbf{x}),f(x)=x. (32)\n\nIn NormAttention, we have:\n\n\ud835\udc12 \ud835\udc12 \\displaystyle\\mathbf{S} = \u03d5 \u200b ( \ud835\udc10 ) \u200b \u03d5 \u200b ( \ud835\udc0a ) \ud835\uddb3 \u2208 \u211d n \u00d7 n , absent italic-\u03d5 \ud835\udc10 italic-\u03d5 superscript \ud835\udc0a \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc5b \\displaystyle=\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^{\\mkern-1.5mu\\mathsf{T}}\\in\\mathbb{R}^{n\\times n}, (33) \ud835\udc13 \ud835\udc13 \\displaystyle\\mathbf{T} = \ud835\udc12\ud835\udc15 \u2208 \u211d n \u00d7 d , absent \ud835\udc12\ud835\udc15 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle=\\mathbf{S}\\mathbf{V}\\in\\mathbb{R}^{n\\times d}, \ud835\udc0e \ud835\udc0e \\displaystyle\\mathbf{O} = RMSNorm \u200b ( \ud835\udc13 ) absent RMSNorm \ud835\udc13 \\displaystyle=\\mathrm{RMSNorm}(\\mathbf{T}) \u225c [ RMSNorm \u200b ( \ud835\udc2d 1 ) \ud835\uddb3 \u22ee RMSNorm \u200b ( \ud835\udc2d n ) \ud835\uddb3 ] \u2208 \u211d n \u00d7 d , \u225c absent delimited-[] matrix RMSNorm superscript subscript \ud835\udc2d 1 \ud835\uddb3 \u22ee RMSNorm superscript subscript \ud835\udc2d \ud835\udc5b \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc51 \\displaystyle\\triangleq\\left[\\begin{matrix}\\mathrm{RMSNorm}(\\mathbf{t}_{1})^{\\mkern-1.5mu\\mathsf{T}}\\\\\n\\vdots\\\\\n\\mathrm{RMSNorm}(\\mathbf{t}_{n})^{\\mkern-1.5mu\\mathsf{T}}\\end{matrix}\\right]\\in\\mathbb{R}^{n\\times d},\n\nwhere is defined as follows:\n\nDefinition D.1. RMSNorm \u200b ( \ud835\udc31 ) RMSNorm \ud835\udc31 \\displaystyle\\mathrm{RMSNorm}(\\mathbf{x}) = \ud835\udc31 \u03c3 2 + \u03f5 , absent \ud835\udc31 superscript \ud835\udf0e 2 italic-\u03f5 \\displaystyle=\\frac{\\mathbf{x}}{\\sqrt{\\sigma^{2}+\\epsilon}}, (34) \u03c3 2 superscript \ud835\udf0e 2 \\displaystyle\\sigma^{2} = \u2211 i = 1 d x i 2 d , absent superscript subscript \ud835\udc56 1 \ud835\udc51 superscript subscript \ud835\udc65 \ud835\udc56 2 \ud835\udc51 \\displaystyle=\\frac{\\sum_{i=1}^{d}x_{i}^{2}}{d}, \u03f5 italic-\u03f5 \\displaystyle\\epsilon > 0 , absent 0 \\displaystyle>0, \ud835\udc31 \ud835\udc31 \\displaystyle\\mathbf{x} \u2208 \u211d d .",
    "transnormer-25": "absent superscript \u211d \ud835\udc51 \\displaystyle\\in\\mathbb{R}^{d}. In the subsequent discussion, we define gradient as:\n\nDefinition D.2. [ \u2207 \ud835\udc0c \u2112 ] i \u200b j = \u2202 \u2112 \u2202 m i \u200b j , subscript delimited-[] subscript \u2207 \ud835\udc0c \u2112 \ud835\udc56 \ud835\udc57 \u2112 subscript \ud835\udc5a \ud835\udc56 \ud835\udc57 [\\nabla_{\\mathbf{M}}\\mathcal{L}]_{ij}=\\frac{\\partial\\mathcal{L}}{\\partial m_{ij}}, (35)\n\nwhere stands for loss function, is a parameter matrix. Then we define the mapping as:\n\nDefinition D.3. h : \u211d n \u00d7 m \u2192 \u211d , h \u200b ( \ud835\udc17 ) = max i = 1 n \u2061 \u2016 \ud835\udc17 i \u2016 2 , \ud835\udc17 \u2208 \u211d n \u00d7 m . : \u210e formulae-sequence \u2192 superscript \u211d \ud835\udc5b \ud835\udc5a \u211d formulae-sequence \u210e \ud835\udc17 superscript subscript \ud835\udc56 1 \ud835\udc5b subscript delimited-\u2225\u2225 subscript \ud835\udc17 \ud835\udc56 2 \ud835\udc17 superscript \u211d \ud835\udc5b \ud835\udc5a \\begin{gathered}h:\\mathbb{R}^{n\\times m}\\to\\mathbb{R},h(\\mathbf{X})=\\max_{i=1}^{n}\\|\\mathbf{X}_{i}\\|_{2},\\\\\n\\mathbf{X}\\in\\mathbb{R}^{n\\times m}.\\end{gathered} (36)\n\nThe mapping has the following property:\n\nProposition D.4. , we have:\n\nh \u200b ( \ud835\udc17\ud835\udc18 \u22a4 ) \u2264 r \u200b h \u200b ( \ud835\udc17 ) \u200b h \u200b ( \ud835\udc18 ) . \u210e superscript \ud835\udc17\ud835\udc18 top \ud835\udc5f \u210e \ud835\udc17 \u210e \ud835\udc18 h(\\mathbf{X}\\mathbf{Y}^{\\top})\\leq\\sqrt{r}h(\\mathbf{X})h(\\mathbf{Y}). (37)\n\nProof. Since\n\nij = \ud835\udc17 i \u200b [ \ud835\udc18 j ] \u22a4 absent subscript \ud835\udc17 \ud835\udc56 superscript delimited-[] subscript \ud835\udc18 \ud835\udc57 top \\displaystyle=\\mathbf{X}_{i}[\\mathbf{Y}_{j}]^{\\top} (38) \u2264 \u2016 \ud835\udc17 i \u2016 2 \u200b \u2016 \ud835\udc18 j \u2016 2 absent subscript norm subscript \ud835\udc17 \ud835\udc56 2 subscript norm subscript \ud835\udc18 \ud835\udc57 2 \\displaystyle\\leq{\\|\\mathbf{X}_{i}\\|_{2}}\\|\\mathbf{Y}_{j}\\|_{2} \u2264 h \u200b ( \ud835\udc17 ) \u200b h \u200b ( \ud835\udc18 ) , absent \u210e \ud835\udc17 \u210e \ud835\udc18 \\displaystyle\\leq h(\\mathbf{X})h(\\mathbf{Y}),\n\nso\n\n\u2016 [ \ud835\udc17\ud835\udc18 \u22a4 ] i \u2016 2 subscript norm subscript delimited-[] superscript \ud835\udc17\ud835\udc18 top \ud835\udc56 2 \\displaystyle\\|[\\mathbf{X}\\mathbf{Y}^{\\top}]_{i}\\|_{2} = \u2211 j = 1 r ( [ \ud835\udc17\ud835\udc18 \u22a4 ] i \u200b j ) 2 absent superscript subscript \ud835\udc57 1 \ud835\udc5f superscript subscript delimited-[] superscript \ud835\udc17\ud835\udc18 top \ud835\udc56 \ud835\udc57 2 \\displaystyle=\\sqrt{\\sum_{j=1}^{r}([\\mathbf{X}\\mathbf{Y}^{\\top}]_{ij})^{2}} (39) \u2264 r \u200b ( h \u200b ( \ud835\udc17 ) \u200b h \u200b ( \ud835\udc18 ) ) 2 absent \ud835\udc5f superscript \u210e \ud835\udc17 \u210e \ud835\udc18 2 \\displaystyle\\leq\\sqrt{r(h(\\mathbf{X})h(\\mathbf{Y}))^{2}} = r \u200b h \u200b ( \ud835\udc17 ) \u200b h \u200b ( \ud835\udc18 ) , absent \ud835\udc5f \u210e \ud835\udc17 \u210e \ud835\udc18 \\displaystyle=\\sqrt{r}h(\\mathbf{X})h(\\mathbf{Y}), h \u200b ( \ud835\udc17\ud835\udc18 \u22a4 ) \u210e superscript \ud835\udc17\ud835\udc18 top \\displaystyle h(\\mathbf{X}\\mathbf{Y}^{\\top}) = max i = 1 r \u2061 \u2016 [ \ud835\udc17\ud835\udc18 \u22a4 ] i \u2016 2 absent superscript subscript \ud835\udc56 1 \ud835\udc5f subscript norm subscript delimited-[] superscript \ud835\udc17\ud835\udc18 top \ud835\udc56 2 \\displaystyle=\\max_{i=1}^{r}\\left\\|[\\mathbf{X}\\mathbf{Y}^{\\top}]_{i}\\right\\|_{2} \u2264 r \u200b h \u200b ( \ud835\udc17 ) \u200b h \u200b ( \ud835\udc18 ) . absent \ud835\udc5f \u210e \ud835\udc17 \u210e \ud835\udc18 \\displaystyle\\leq\\sqrt{r}h(\\mathbf{X})h(\\mathbf{Y}). D.2 Gradient analysis\n\nD.2.1 Preliminary\n\nGiven gradient , let\u2019s compute in every situation. We first define:\n\nc 1 subscript \ud835\udc50 1 \\displaystyle c_{1} = h \u200b ( \u2207 \ud835\udc0e \u2112 ) absent \u210e subscript \u2207 \ud835\udc0e \u2112 \\displaystyle=h(\\nabla_{\\mathbf{O}}\\mathcal{L}) (40) = max i = 1 n \u2061 \u2016 \u2207 \ud835\udc0e i \u2112 \u2016 2 , absent superscript subscript \ud835\udc56 1 \ud835\udc5b subscript norm subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 2 \\displaystyle=\\max_{i=1}^{n}\\|\\nabla_{\\mathbf{O}_{i}}\\mathcal{L}\\|_{2}, c 2 subscript \ud835\udc50 2 \\displaystyle c_{2} = h \u200b ( \ud835\udc15 ) absent \u210e \ud835\udc15 \\displaystyle=h(\\mathbf{V}) = max i = 1 n \u2061 \u2016 \ud835\udc15 i \u2016 2 < \u221e , absent superscript subscript \ud835\udc56 1 \ud835\udc5b subscript norm subscript \ud835\udc15 \ud835\udc56 2 \\displaystyle=\\max_{i=1}^{n}\\|\\mathbf{V}_{i}\\|_{2}<\\infty, c 3 subscript \ud835\udc50 3 \\displaystyle c_{3} = min i , j \u2061 | s i \u200b j | \u2265 0 . absent subscript \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 0 \\displaystyle=\\min_{i,j}|s_{ij}|\\geq 0. Before we get started, we have the following propositions. The proof can be found in Appendix D.3. Proposition D.5. Proposition D.6. , we have:\n\n\u2016 \ud835\udc17 \u2016 2 \u2264 n \u200b h \u200b ( \ud835\udc17 ) . subscript norm \ud835\udc17 2 \ud835\udc5b \u210e \ud835\udc17 \\|\\mathbf{X}\\|_{2}\\leq\\sqrt{n}h(\\mathbf{X}).",
    "transnormer-26": "(41)\n\nTake , we get:\n\n\u2016 \ud835\udc15 \u2016 2 \u2264 n \u200b h \u200b ( \ud835\udc15 ) = n \u200b c 2 . subscript norm \ud835\udc15 2 \ud835\udc5b \u210e \ud835\udc15 \ud835\udc5b subscript \ud835\udc50 2 \\|\\mathbf{V}\\|_{2}\\leq\\sqrt{n}h(\\mathbf{V})=\\sqrt{n}c_{2}. (42)\n\nD.2.2 Vanilla/Linear attention\n\nAccording to (30), we can discuss vanilla and linear attention under one formula:\n\n\u2207 \ud835\udc0f \u2112 = [ \u2207 \ud835\udc0e \u2112 ] \u200b \ud835\udc15 \ud835\uddb3 \u2208 \u211d n \u00d7 n . subscript \u2207 \ud835\udc0f \u2112 delimited-[] subscript \u2207 \ud835\udc0e \u2112 superscript \ud835\udc15 \ud835\uddb3 superscript \u211d \ud835\udc5b \ud835\udc5b \\nabla_{\\mathbf{P}}\\mathcal{L}=[\\nabla_{\\mathbf{O}}\\mathcal{L}]\\mathbf{V}^{\\mkern-1.5mu\\mathsf{T}}\\in\\mathbb{R}^{n\\times n}. (43)\n\nThen define matrix :\n\n[ \ud835\udc14 ( i ) ] j \u200b k = \u2202 p i \u200b k \u2202 s i \u200b j . subscript delimited-[] superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 [\\mathbf{U}^{(i)}]_{jk}=\\frac{\\partial p_{ik}}{\\partial s_{ij}}. (44)\n\nAccording to (9), in vanilla attention, we have:\n\n| [ \ud835\udc14 ( i ) ] j \u200b k | \u2264 1 4 , subscript delimited-[] superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 \ud835\udc58 1 4 \\left|[\\mathbf{U}^{(i)}]_{jk}\\right|\\leq\\frac{1}{4}, (45)\n\nwhile in linear attention, we have:\n\n| [ \ud835\udc14 ( i ) ] j \u200b k | \u2264 1 4 \u200b | s i \u200b j | \u2264 1 4 \u200b c 3 . subscript delimited-[] superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 \ud835\udc58 1 4 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 1 4 subscript \ud835\udc50 3 \\left|[\\mathbf{U}^{(i)}]_{jk}\\right|\\leq\\frac{1}{4|s_{ij}|}\\leq\\frac{1}{4c_{3}}. (46)\n\nSince:\n\n\u2202 \u2112 \u2202 s i \u200b j \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}} = \u2211 k = 1 n \u2202 \u2112 \u2202 p i \u200b k \u200b \u2202 p i \u200b k \u2202 s i \u200b j absent superscript subscript \ud835\udc58 1 \ud835\udc5b \u2112 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle=\\sum_{k=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial p_{ik}}\\frac{\\partial p_{ik}}{\\partial s_{ij}} (47) = ( \u2207 \ud835\udc0f i \u2112 ) \u200b ( \ud835\udc14 j ( i ) ) \u22a4 absent subscript \u2207 subscript \ud835\udc0f \ud835\udc56 \u2112 superscript subscript superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 top \\displaystyle=(\\nabla_{\\mathbf{P}_{i}}\\mathcal{L})(\\mathbf{U}^{(i)}_{j})^{\\top} = ( \u2207 \ud835\udc0e i \u2112 ) \u200b \ud835\udc15 \ud835\uddb3 \u200b ( \ud835\udc14 j ( i ) ) \u22a4 . absent subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript \ud835\udc15 \ud835\uddb3 superscript subscript superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 top \\displaystyle=(\\nabla_{\\mathbf{O}_{i}}\\mathcal{L})\\mathbf{V}^{\\mkern-1.5mu\\mathsf{T}}(\\mathbf{U}^{(i)}_{j})^{\\top}. So we have:\n\n| \u2202 \u2112 \u2202 s i \u200b j | \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle\\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right| \u2264 \u2016 ( \u2207 \ud835\udc0e i \u2112 ) \u200b \ud835\udc15 \ud835\uddb3 \u2016 2 \u200b \u2016 \ud835\udc14 j ( i ) \ud835\uddb3 \u2016 2 absent subscript norm subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript \ud835\udc15 \ud835\uddb3 2 subscript norm superscript subscript superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 \ud835\uddb3 2 \\displaystyle\\leq\\|(\\nabla_{\\mathbf{O}_{i}}\\mathcal{L})\\mathbf{V}^{\\mkern-1.5mu\\mathsf{T}}\\|_{2}\\left\\|{\\mathbf{U}^{(i)}_{j}}^{\\mkern-1.5mu\\mathsf{T}}\\right\\|_{2} (48) \u2264 \u2016 \u2207 \ud835\udc0e i \u2112 \u2016 2 \u200b \u2016 \ud835\udc15 \ud835\uddb3 \u2016 2 \u200b \u2016 \ud835\udc14 j ( i ) \u2016 2 absent subscript norm subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 2 subscript norm superscript \ud835\udc15 \ud835\uddb3 2 subscript norm subscript superscript \ud835\udc14 \ud835\udc56 \ud835\udc57 2 \\displaystyle\\leq\\|\\nabla_{\\mathbf{O}_{i}}\\mathcal{L}\\|_{2}\\|\\mathbf{V}^{\\mkern-1.5mu\\mathsf{T}}\\|_{2}\\|\\mathbf{U}^{(i)}_{j}\\|_{2} \u2264 c 1 \u00d7 n \u200b c 2 \u00d7 1 4 \u200b t absent subscript \ud835\udc50 1 \ud835\udc5b subscript \ud835\udc50 2 1 4 \ud835\udc61 \\displaystyle\\leq c_{1}\\times\\sqrt{n}c_{2}\\times\\frac{1}{4t} = n \u200b c 1 \u200b c 2 4 \u200b t , absent \ud835\udc5b subscript \ud835\udc50 1 subscript \ud835\udc50 2 4 \ud835\udc61 \\displaystyle=\\frac{\\sqrt{n}c_{1}c_{2}}{4t},\n\nwhere in vanilla attention and in linear attention. On the other hand, according to Appendix C, in linear attention, there exist , such that:\n\n\u2202 p i \u200b k \u2202 s i \u200b j subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle\\frac{\\partial p_{ik}}{\\partial s_{ij}} = 1 \u2016 \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u2016 \u200b t i \u200b j \u200b k , absent 1 norm superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \ud835\udc58 \\displaystyle=\\frac{1}{\\|{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}\\|}t_{ijk}, (49) t i \u200b j \u200b k subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \ud835\udc58 \\displaystyle t_{ijk} = { 1 n \u200b ( 1 \u2212 1 n ) j = k \u2212 1 n 2 j \u2260 k . absent cases 1 \ud835\udc5b 1 1 \ud835\udc5b \ud835\udc57 \ud835\udc58 1 superscript \ud835\udc5b 2 \ud835\udc57 \ud835\udc58 \\displaystyle=\\begin{cases}\\frac{1}{n}(1-\\frac{1}{n})&j=k\\\\\n-\\frac{1}{n^{2}}&j\\neq k\\end{cases}. Then\n\n| \u2202 \u2112 \u2202 s i \u200b j | \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle\\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right| = | \u2211 k = 1 n \u2202 \u2112 \u2202 p i \u200b k \u200b \u2202 p i \u200b k \u2202 s i \u200b j | absent superscript subscript \ud835\udc58 1 \ud835\udc5b \u2112 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle=\\left|\\sum_{k=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial p_{ik}}\\frac{\\partial p_{ik}}{\\partial s_{ij}}\\right| (50) = 1 \u2016 \ud835\udc31 0 \ud835\uddb3 \u200b \ud835\udc31 0 \u2016 \u200b | \u2211 k = 1 n \u2202 \u2112 \u2202 p i \u200b k \u200b t i \u200b j \u200b k | absent 1 norm superscript subscript \ud835\udc31 0 \ud835\uddb3 subscript \ud835\udc31 0 superscript subscript \ud835\udc58 1 \ud835\udc5b \u2112 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \ud835\udc58 \\displaystyle=\\frac{1}{\\|{\\mathbf{x}_{0}}^{\\mkern-1.5mu\\mathsf{T}}{\\mathbf{x}_{0}}\\|}\\left|\\sum_{k=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial p_{ik}}t_{ijk}\\right| \u2265 1 \u03f5 \u200b | \u2211 k = 1 n \u2202 \u2112 \u2202 p i \u200b k \u200b t i \u200b j \u200b k | . absent 1 italic-\u03f5 superscript subscript \ud835\udc58 1 \ud835\udc5b \u2112 subscript \ud835\udc5d \ud835\udc56 \ud835\udc58 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \ud835\udc58 \\displaystyle\\geq\\frac{1}{\\epsilon}\\left|\\sum_{k=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial p_{ik}}t_{ijk}\\right|. Let , then . This means that the gradient in linear attention is unbounded. D.2.3 NormAttention\n\nWe first define the second-moment of \u2019th row of :\n\n\u03c3 i 2 superscript subscript \ud835\udf0e \ud835\udc56 2 \\displaystyle\\sigma_{i}^{2} = \u2211 j = 1 d t i \u200b j 2 d . absent superscript subscript \ud835\udc57 1 \ud835\udc51 superscript subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 2 \ud835\udc51 \\displaystyle=\\frac{\\sum_{j=1}^{d}t_{ij}^{2}}{d}. (51)\n\nThen is as follows:\n\n\u2202 o i \u200b j \u2202 t i \u200b k subscript \ud835\udc5c \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 \\displaystyle\\frac{\\partial o_{ij}}{\\partial t_{ik}} = 1 \u03c3 i 2 + \u03f5 \u200b [ 1 \u200b { j = k } \u2212 1 d \u200b t i \u200b j \u200b t i \u200b k \u03c3 i 2 + \u03f5 ] . absent 1 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 delimited-[] 1 \ud835\udc57 \ud835\udc58 1 \ud835\udc51 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 \\displaystyle=\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[{1\\{j=k\\}}-\\frac{1}{d}\\frac{t_{ij}t_{ik}}{\\sigma_{i}^{2}+\\epsilon}\\right]. (52)\n\nNotice that we have the following upper bound:\n\n| \u2202 o i \u200b j \u2202 t i \u200b k | subscript \ud835\udc5c \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 \\displaystyle\\left|\\frac{\\partial o_{ij}}{\\partial t_{ik}}\\right| (53) = \\displaystyle= 1 \u03c3 i 2 + \u03f5 \u200b [ 1 \u200b { j = k } \u2212 1 d \u200b t i \u200b j \u200b t i \u200b k \u2211 s = 1 d t i \u200b s 2 d + \u03f5 ] 1 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 delimited-[] 1 \ud835\udc57 \ud835\udc58 1 \ud835\udc51 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 superscript subscript \ud835\udc60 1 \ud835\udc51 superscript subscript \ud835\udc61 \ud835\udc56 \ud835\udc60 2 \ud835\udc51 italic-\u03f5 \\displaystyle\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1\\{j=k\\}-\\frac{1}{d}\\frac{t_{ij}t_{ik}}{\\frac{\\sum_{s=1}^{d}t_{is}^{2}}{d}+\\epsilon}\\right] = \\displaystyle= 1 \u03c3 i 2 + \u03f5 \u200b [ 1 \u200b { j = k } + t i \u200b j \u200b t i \u200b k \u2211 s = 1 d t i \u200b s 2 + d \u200b \u03f5 ] 1 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 delimited-[] 1 \ud835\udc57 \ud835\udc58 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 superscript subscript \ud835\udc60 1 \ud835\udc51 superscript subscript \ud835\udc61 \ud835\udc56 \ud835\udc60 2 \ud835\udc51 italic-\u03f5 \\displaystyle\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1\\{j=k\\}+\\frac{t_{ij}t_{ik}}{\\sum_{s=1}^{d}t_{is}^{2}+d\\epsilon}\\right] \u2264 \\displaystyle\\leq 1 \u03c3 i 2 + \u03f5 \u200b [ 1 \u200b { j = k } + 1 2 \u200b t i \u200b j 2 + t i \u200b k 2 \u2211 s = 1 d t i \u200b s 2 ] 1 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 delimited-[] 1 \ud835\udc57 \ud835\udc58 1 2 superscript subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 2 superscript subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 2 superscript subscript \ud835\udc60 1 \ud835\udc51 superscript subscript \ud835\udc61 \ud835\udc56 \ud835\udc60 2 \\displaystyle\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1\\{j=k\\}+\\frac{1}{2}\\frac{t_{ij}^{2}+t_{ik}^{2}}{\\sum_{s=1}^{d}t_{is}^{2}}\\right] \u2264 \\displaystyle\\leq 1 \u03c3 i 2 + \u03f5 \u200b [ 1 + 1 2 ] 1 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 delimited-[] 1 1 2 \\displaystyle\\frac{1}{\\sqrt{\\sigma_{i}^{2}+\\epsilon}}\\left[1+\\frac{1}{2}\\right] \u2264 \\displaystyle\\leq 3 2 \u200b \u03c3 i 2 + \u03f5 . 3 2 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 \\displaystyle\\frac{3}{2\\sqrt{\\sigma_{i}^{2}+\\epsilon}}. Let\u2019s define matrix as follows:\n\njk = \u2202 o i \u200b k \u2202 t i \u200b j . absent subscript \ud835\udc5c \ud835\udc56 \ud835\udc58 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \\displaystyle=\\frac{\\partial o_{ik}}{\\partial t_{ij}}. (54)\n\nSince\n\n\u2202 \u2112 \u2202 t i \u200b j \u2112 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial t_{ij}} = \u2211 k = 1 n \u2202 \u2112 \u2202 o i \u200b k \u200b \u2202 o i \u200b k \u2202 t i \u200b j absent superscript subscript \ud835\udc58 1 \ud835\udc5b \u2112 subscript \ud835\udc5c \ud835\udc56 \ud835\udc58 subscript \ud835\udc5c \ud835\udc56 \ud835\udc58 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \\displaystyle=\\sum_{k=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial o_{ik}}\\frac{\\partial o_{ik}}{\\partial t_{ij}} (55) = ( \u2207 \ud835\udc0e i \u2112 ) \u200b ( \ud835\udc11 j ( i ) ) \u22a4 . absent subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript subscript superscript \ud835\udc11 \ud835\udc56 \ud835\udc57 top \\displaystyle=(\\nabla_{\\mathbf{O}_{i}}\\mathcal{L})(\\mathbf{R}^{(i)}_{j})^{\\top}. Then we can get:\n\n\u2207 \ud835\udc13 i \u2112 = ( \u2207 \ud835\udc0e i \u2112 ) \u200b ( \ud835\udc11 ( i ) ) \ud835\uddb3 \u2208 \u211d 1 \u00d7 d . subscript \u2207 subscript \ud835\udc13 \ud835\udc56 \u2112 subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript superscript \ud835\udc11 \ud835\udc56 \ud835\uddb3 superscript \u211d 1 \ud835\udc51 {\\nabla_{\\mathbf{T}_{i}}}\\mathcal{L}=({\\nabla_{\\mathbf{O}_{i}}}\\mathcal{L})(\\mathbf{R}^{(i)})^{\\mkern-1.5mu\\mathsf{T}}\\in\\mathbb{R}^{1\\times d}. (56)\n\nAccording to (53), we have:\n\n\u2016 \ud835\udc11 ( i ) \u2016 2 subscript norm superscript \ud835\udc11 \ud835\udc56 2 \\displaystyle\\|\\mathbf{R}^{(i)}\\|_{2} \u2264 \u2016 \ud835\udc11 ( i ) \u2016 F absent subscript norm superscript \ud835\udc11 \ud835\udc56 \ud835\udc39 \\displaystyle\\leq\\|\\mathbf{R}^{(i)}\\|_{F} (57) \u2264 \u2211 j = 1 d \u2211 k = 1 d [ \u2202 o i \u200b j \u2202 t i \u200b k ] 2 absent superscript subscript \ud835\udc57 1 \ud835\udc51 superscript subscript \ud835\udc58 1 \ud835\udc51 superscript delimited-[] subscript \ud835\udc5c \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc58 2 \\displaystyle\\leq\\sqrt{\\sum_{j=1}^{d}\\sum_{k=1}^{d}\\left[\\frac{\\partial o_{ij}}{\\partial t_{ik}}\\right]^{2}} \u2264 3 \u200b d 2 \u200b \u03c3 i 2 + \u03f5 absent 3 \ud835\udc51 2 superscript subscript \ud835\udf0e \ud835\udc56 2 italic-\u03f5 \\displaystyle\\leq\\frac{3d}{2\\sqrt{\\sigma_{i}^{2}+\\epsilon}} \u2264 3 \u200b d 2 \u200b \u03f5 . absent 3 \ud835\udc51 2 italic-\u03f5 \\displaystyle\\leq\\frac{3d}{2\\sqrt{\\epsilon}}. Finally, we get:\n\n\u2207 \ud835\udc12 i \u2112 subscript \u2207 subscript \ud835\udc12 \ud835\udc56 \u2112 \\displaystyle\\nabla_{\\mathbf{S}_{i}}\\mathcal{L} = ( \u2207 \ud835\udc13 i \u2112 ) \u200b \ud835\udc15 \ud835\uddb3 absent subscript \u2207 subscript \ud835\udc13 \ud835\udc56 \u2112 superscript \ud835\udc15 \ud835\uddb3 \\displaystyle=(\\nabla_{\\mathbf{T}_{i}}\\mathcal{L})\\mathbf{V}^{\\mkern-1.5mu\\mathsf{T}} (58) = ( \u2207 \ud835\udc0e i \u2112 ) \u200b ( \ud835\udc11 ( i ) ) \ud835\uddb3 \u200b \ud835\udc15 \ud835\uddb3 \u2208 \u211d 1 \u00d7 n , absent subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript superscript \ud835\udc11 \ud835\udc56 \ud835\uddb3 superscript \ud835\udc15 \ud835\uddb3 superscript \u211d 1 \ud835\udc5b \\displaystyle=({\\nabla_{\\mathbf{O}_{i}}}\\mathcal{L})(\\mathbf{R}^{(i)})^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{V}^{\\mkern-1.5mu\\mathsf{T}}\\in\\mathbb{R}^{1\\times n}, \u2202 \u2112 \u2202 s i \u200b j \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}} = ( \u2207 \ud835\udc0e i \u2112 ) \u200b ( \ud835\udc11 ( i ) ) \ud835\uddb3 \u200b \ud835\udc15 j , absent subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript superscript \ud835\udc11 \ud835\udc56 \ud835\uddb3 subscript \ud835\udc15 \ud835\udc57 \\displaystyle=(\\nabla_{\\mathbf{O}_{i}}\\mathcal{L})(\\mathbf{R}^{(i)})^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{V}_{j}, | \u2202 \u2112 \u2202 s i \u200b j | \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\displaystyle\\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right| = | ( \u2207 \ud835\udc0e i \u2112 ) \u200b ( \ud835\udc11 ( i ) ) \ud835\uddb3 \u200b \ud835\udc15 j | absent subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 superscript superscript \ud835\udc11 \ud835\udc56 \ud835\uddb3 subscript \ud835\udc15 \ud835\udc57 \\displaystyle=\\left|(\\nabla_{\\mathbf{O}_{i}}\\mathcal{L})(\\mathbf{R}^{(i)})^{\\mkern-1.5mu\\mathsf{T}}\\mathbf{V}_{j}\\right| \u2264 \u2016 \u2207 \ud835\udc0e i \u2112 \u2016 2 \u200b \u2016 \ud835\udc11 ( i ) \u200b \ud835\udc15 j \u2016 2 absent subscript norm subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 2 subscript norm superscript \ud835\udc11 \ud835\udc56 subscript \ud835\udc15 \ud835\udc57 2 \\displaystyle\\leq\\|\\nabla_{\\mathbf{O}_{i}}\\mathcal{L}\\|_{2}\\|\\mathbf{R}^{(i)}\\mathbf{V}_{j}\\|_{2} \u2264 \u2016 \u2207 \ud835\udc0e i \u2112 \u2016 2 \u200b \u2016 \ud835\udc11 ( i ) \u2016 2 \u200b \u2016 \ud835\udc15 j \u2016 2 absent subscript norm subscript \u2207 subscript \ud835\udc0e \ud835\udc56 \u2112 2 subscript norm superscript \ud835\udc11 \ud835\udc56 2 subscript norm subscript \ud835\udc15 \ud835\udc57 2 \\displaystyle\\leq\\|\\nabla_{\\mathbf{O}_{i}}\\mathcal{L}\\|_{2}\\|\\mathbf{R}^{(i)}\\|_{2}\\|\\mathbf{V}_{j}\\|_{2} \u2264 c 1 \u00d7 3 \u200b d 2 \u200b \u03f5 \u00d7 c 2 absent subscript \ud835\udc50 1 3 \ud835\udc51 2 italic-\u03f5 subscript \ud835\udc50 2 \\displaystyle\\leq c_{1}\\times\\frac{3d}{2\\sqrt{\\epsilon}}\\times c_{2} = 3 \u200b c 1 \u200b c 2 \u200b d 2 \u200b \u03f5 . absent 3 subscript \ud835\udc50 1 subscript \ud835\udc50 2 \ud835\udc51 2 italic-\u03f5 \\displaystyle=\\frac{3c_{1}c_{2}d}{2\\sqrt{\\epsilon}}. Let\u2019s summarize the previous results. In vanilla attention, we have:\n\n| \u2202 \u2112 \u2202 s i \u200b j | \u2264 n \u200b c 1 \u200b c 2 4 < \u221e . \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \ud835\udc5b subscript \ud835\udc50 1 subscript \ud835\udc50 2 4 \\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right|\\leq\\frac{\\sqrt{n}c_{1}c_{2}}{4}<\\infty. (59)\n\nIn linear attention, there exist , such that:\n\n| \u2202 \u2112 \u2202 s i \u200b j | \u2192 \u221e . \u2192 \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 \\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right|\\to\\infty. (60)\n\nIn NormAttention, we have:\n\n| \u2202 \u2112 \u2202 s i \u200b j | \u2264 3 \u200b c 1 \u200b c 2 \u200b d 2 \u200b \u03f5 < \u221e . \u2112 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 3 subscript \ud835\udc50 1 subscript \ud835\udc50 2 \ud835\udc51 2 italic-\u03f5 \\left|\\frac{\\partial\\mathcal{L}}{\\partial s_{ij}}\\right|\\leq\\frac{3c_{1}c_{2}d}{2\\sqrt{\\epsilon}}<\\infty. (61)\n\nSo is bounded in vanilla attention and NormAttention, while it\u2019s unbounded in linear attention. This makes the training of linear transformer unstable. D.3 Proof of the proposition\n\nProof of Proposition D.5. Let\u2019s consider a one layer Transformer for classification tasks. The input is , the label is , where is the number of categories and is one-hot vector,. are the activation functions, here we take as an example. The parameters of the model are:\n\n\ud835\udc16 1 \u2208 \u211d d \u00d7 d 1 , \ud835\udc16 2 \u2208 \u211d d 1 \u00d7 d , \ud835\udc16 3 \u2208 \u211d d \u00d7 m . formulae-sequence subscript \ud835\udc16 1 superscript \u211d \ud835\udc51 subscript \ud835\udc51 1 formulae-sequence subscript \ud835\udc16 2 superscript \u211d subscript \ud835\udc51 1 \ud835\udc51 subscript \ud835\udc16 3 superscript \u211d \ud835\udc51 \ud835\udc5a \\begin{gathered}\\mathbf{W}_{1}\\in\\mathbb{R}^{d\\times d_{1}},\\mathbf{W}_{2}\\in\\mathbb{R}^{d_{1}\\times d},\\\\\n\\mathbf{W}_{3}\\in\\mathbb{R}^{d\\times m}.\\end{gathered} (62)\n\nThe forward pass of the model is888XAttention stands for vanilla/norm attention.:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nThe backward pass of the model is: {outline}[enumerate] \\1 \\2 The upper bound is:\n\nh \u200b ( \u2207 \ud835\udc0e \u2112 ) \u210e subscript \u2207 \ud835\udc0e \u2112 \\displaystyle h\\left(\\nabla_{\\mathbf{O}}{\\mathcal{L}}\\right) (63) = \\displaystyle= max { \u2211 i = 1 m p i 2 \u2212 2 p 1 + 1 , \\displaystyle\\max\\{\\sum_{i=1}^{m}p_{i}^{2}-2p_{1}+1, p i \u2265 0 , \u2211 i = 1 m p i = 1 } \\displaystyle p_{i}\\geq 0,\\sum_{i=1}^{m}p_{i}=1\\} \u225c \u225c \\displaystyle\\triangleq a 0 subscript \ud835\udc4e 0 \\displaystyle a_{0} < \\displaystyle< \u221e .",
    "transnormer-27": "\\displaystyle\\infty. \\1\n\n\\2 The upper bound is:\n\nh \u200b ( \u2207 \ud835\udc17 3 \u2112 ) \u210e subscript \u2207 subscript \ud835\udc17 3 \u2112 \\displaystyle h\\left(\\nabla_{\\mathbf{X}_{3}}{\\mathcal{L}}\\right) (64) \u2264 \\displaystyle\\leq d \u200b h \u200b ( \u2207 \ud835\udc0e \u2112 ) \u200b h \u200b ( \ud835\udc16 3 ) \ud835\udc51 \u210e subscript \u2207 \ud835\udc0e \u2112 \u210e subscript \ud835\udc16 3 \\displaystyle\\sqrt{d}h\\left(\\nabla_{\\mathbf{O}}{\\mathcal{L}}\\right)h\\left(\\mathbf{W}_{3}\\right) \u2264 \\displaystyle\\leq d \u200b a 0 \u200b h \u200b ( \ud835\udc16 3 ) \ud835\udc51 subscript \ud835\udc4e 0 \u210e subscript \ud835\udc16 3 \\displaystyle\\sqrt{d}a_{0}h\\left(\\mathbf{W}_{3}\\right) \u225c \u225c \\displaystyle\\triangleq a 1 < \u221e . subscript \ud835\udc4e 1 \\displaystyle a_{1}<\\infty. \\1\n\n\\2 The upper bound is:\n\nh \u200b ( \u2207 \ud835\udc17 2 \u2112 ) \u210e subscript \u2207 subscript \ud835\udc17 2 \u2112 \\displaystyle h\\left(\\nabla_{\\mathbf{X}_{2}}{\\mathcal{L}}\\right) (65) \u2264 \\displaystyle\\leq d 1 \u200b h \u200b ( f 2 \u2032 \u200b ( \ud835\udc17 2 \u200b \ud835\udc16 2 ) \u2299 \u2207 \ud835\udc17 3 \u2112 ) \u200b h \u200b ( \ud835\udc16 2 ) subscript \ud835\udc51 1 \u210e direct-product superscript subscript \ud835\udc53 2 \u2032 subscript \ud835\udc17 2 subscript \ud835\udc16 2 subscript \u2207 subscript \ud835\udc17 3 \u2112 \u210e subscript \ud835\udc16 2 \\displaystyle\\sqrt{d_{1}}h\\left(f_{2}^{\\prime}(\\mathbf{X}_{2}\\mathbf{W}_{2})\\odot{\\nabla_{\\mathbf{X}_{3}}\\mathcal{L}}\\right)h\\left(\\mathbf{W}_{2}\\right) \u2264 \\displaystyle\\leq d 1 \u200b a 1 \u200b h \u200b ( \ud835\udc16 2 ) subscript \ud835\udc51 1 subscript \ud835\udc4e 1 \u210e subscript \ud835\udc16 2 \\displaystyle\\sqrt{d_{1}}a_{1}h(\\mathbf{W}_{2}) \u225c \u225c \\displaystyle\\triangleq a 2 subscript \ud835\udc4e 2 \\displaystyle a_{2} < \\displaystyle< \u221e . \\displaystyle\\infty. \\1\n\n\\2 The upper bound is:\n\nh \u200b ( \u2207 \ud835\udc17 1 \u2112 ) \u210e subscript \u2207 subscript \ud835\udc17 1 \u2112 \\displaystyle h\\left(\\nabla_{\\mathbf{X}_{1}}{\\mathcal{L}}\\right) (66) \u2264 \\displaystyle\\leq d \u200b h \u200b ( f 1 \u2032 \u200b ( \ud835\udc17 1 \u200b \ud835\udc16 1 ) \u2299 \u2207 \ud835\udc17 2 \u2112 ) \u200b h \u200b ( \ud835\udc16 1 ) \ud835\udc51 \u210e direct-product superscript subscript \ud835\udc53 1 \u2032 subscript \ud835\udc17 1 subscript \ud835\udc16 1 subscript \u2207 subscript \ud835\udc17 2 \u2112 \u210e subscript \ud835\udc16 1 \\displaystyle\\sqrt{d}h\\left(f_{1}^{\\prime}(\\mathbf{X}_{1}\\mathbf{W}_{1})\\odot{\\nabla_{\\mathbf{X}_{2}}{\\mathcal{L}}}\\right)h\\left(\\mathbf{W}_{1}\\right) \u2264 \\displaystyle\\leq d \u200b a 2 \u200b h \u200b ( \ud835\udc16 2 ) \ud835\udc51 subscript \ud835\udc4e 2 \u210e subscript \ud835\udc16 2 \\displaystyle\\sqrt{d}a_{2}h(\\mathbf{W}_{2}) \u225c \u225c \\displaystyle\\triangleq a 3 subscript \ud835\udc4e 3 \\displaystyle a_{3} < \\displaystyle< \u221e . \\displaystyle\\infty. So the gradient passed to XAttention module is bounded, i.e., . \u220e\n\nProof of Proposition D.6. \u2016 \ud835\udc17 \u2016 2 subscript norm \ud835\udc17 2 \\displaystyle\\|\\mathbf{X}\\|_{2} \u2264 \u2016 \ud835\udc17 \u2016 F absent subscript norm \ud835\udc17 \ud835\udc39 \\displaystyle\\leq\\|\\mathbf{X}\\|_{F} (67) = \u2211 i = 1 n \u2016 \ud835\udc17 i \u2016 2 2 absent superscript subscript \ud835\udc56 1 \ud835\udc5b superscript subscript norm subscript \ud835\udc17 \ud835\udc56 2 2 \\displaystyle=\\sqrt{\\sum_{i=1}^{n}\\|\\mathbf{X}_{i}\\|_{2}^{2}} \u2264 \u2211 i = 1 n [ h \u200b ( \ud835\udc17 ) ] 2 absent superscript subscript \ud835\udc56 1 \ud835\udc5b superscript delimited-[] \u210e \ud835\udc17 2 \\displaystyle\\leq\\sqrt{\\sum_{i=1}^{n}[h(\\mathbf{X})]^{2}} = n \u200b h \u200b ( \ud835\udc17 ) . absent \ud835\udc5b \u210e \ud835\udc17 \\displaystyle=\\sqrt{n}h(\\mathbf{X}). Appendix E Experiment configs\n\nIn this section, we will introduce detailed training hyperparameters. We introduce the configurations for autoregressive/bidirectional language model in table F. For LRA benchmark, we use the same configuration as Skyformer, which use 2-layer transformer model with 64 hidden dimensions, 2 attention heads, 85 GLU dimensions, Swish as GLU activation function. For batch size and learning rate , we use 16,1e-4 for Text Classification, 32,1e-4 for ListOps, 16,2e-4 for Document Retrieval, 128,2e-4 for Pathfinder, 256,1e-4 for Image Classification, the same as Skyformer. Appendix F Pseudocode for visualization. In this section, we provide pseudo codes for the 4th column of Figure 2 in Python:\n\nimport torch\n\ndef get_curve(w):\n\nn, m = w.shape\n\nnum = 100\n\nP = torch.linspace(0, 1, num)\n\ncnts = torch.zeros(num)\n\nfor i in range(n):\n\ncnt = torch.zeros(num)\n\nw1 = w[i].clone()\n\ncenter = i % m\n\ns = w1[center].item()\n\nL = 1\n\nl = center - 1\n\nr = center + 1\n\nj = 1\n\nl_thre = 0\n\nr_thre = m\n\nflag = 0\n\nwhile L < m and j < num:\n\nif (s >= P[j].item()):\n\ncnt[j] = L\n\nj += 1\n\ncontinue\n\nif flag == 1:\n\nif r != r_thre:\n\ns += w1[r].item()\n\nr = min(r_thre, r + 1)\n\nflag = 0\n\nelse:\n\nif l != l_thre:\n\ns += w1[l].item()\n\nl = max(l_thre, l - 1)\n\nflag = 1\n\nL = min(r - l + 1, m)\n\nif L >= m:\n\nfor u in range(j, num):\n\ncnt[u] = min(L, m)\n\ncnt[0] = 0\n\ncnts += cnt\n\ncnts = cnts / n / m\n\nplt.plot(cnts, P)\n\nreturn cnts\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 02:12:40 2024 by LaTeXML"
}