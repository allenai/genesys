{
    "posicoupling-0": "# Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers \n\nHanseul Cho*<br>Graduate School of AI, KAIST<br>jhs4015@kaist.ac.kr\n\nJaeyoung Cha*<br>Graduate School of AI, KAIST<br>chajaeyoung@kaist.ac.kr\n\nPranjal Awasthi\nGoogle Research\npranjalawasthi@google.com\nAnupam Gupta\nNew York University \\& Google Research\nanupam.g@nyu.edu\n\nSrinadh Bhojanapalli<br>Google Research<br>bsrinadh@google.com\n\nChulhee Yun<br>Graduate School of AI, KAIST<br>chulhee.yun@kaist.ac.kr\n\n\n#### Abstract\n\nEven for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more \"relevant\" tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30 -digit additions can generalize up to 200 -digit additions ( $6.67 \\times$ of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, $N \\times 2$ multiplication, copy/reverse, and a two-dimensional task. ${ }^{1}$\n\n\n## 1 Introduction\n\nSince the appearance of a sequence-to-sequence deep neural architecture called Transformer (Vaswani et al., 2017), it has brought tremendous success in various fields including natural language process (NLP) (Chowdhery et al., 2023; Gemini et al., 2023; OpenAI, 2023; Thoppilan et al., 2022) and many applications such as mathematical reasoning and theorem proving (Lewkowycz et al., 2022; Trinh et al., 2024; Wu et al., 2022). Despite its triumph, it has recently been illuminated that Transformers often lack the ability of length generalization (Anil et al., 2022; Deletang et al., 2023; Press et al., 2022; Zhang et al., 2023). It refers to a special kind of out-of-distribution generalization capability to extrapolate the model's performance to longer sequences than those encountered during training. Understanding length generalization is of great importance because the lack of length generalization provides evidence that language models do not genuinely understand the structure of a given task. Improving Transformer's length generalization has received much attention, particularly because the time/memory complexities for training Transformers grow up to quadratically in the sequence length. Even for simple arithmetic tasks such as integer addition, length generalization is still difficult for Transformers (Kazemnejad et al., 2023; Kim et al., 2021; Lee et al., 2024; Nogueira et al., 2021; Zhou et al., 2024a,b). Humans\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-02.jpg?height=416&width=1573&top_left_y=231&top_left_x=271)\n\nFigure 1: Methods for length generalization in the integer addition task. We report exact-match accuracies (markers: medians over experiments; light area: $95 \\%$ confidence intervals). We employ the reversed format and zero-paddings (Lee et al., 2024) into the input sequence. With position coupling, we achieve more than $95 \\%$ exact-match (EM) accuracy for up to 200-digit additions with Transformers trained on up to 30-digit additions. For index hinting (Zhou et al., 2024a), we separately test absolute positional embedding (APE) with a random starting position ID (mimicking the original implementation by Zhou et al. (2024a)) and without positional encoding (NoPE) (Kazemnejad et al., 2023) (as tested by Zhou et al. (2024b)). can length-generalize in integer addition because they understand the essential principle of the task. Nevertheless, it is observed that Transformers typically learn to solve addition only up to the training sequence length (Lee et al., 2024), which is different from the true arithmetic algorithm that humans \"implement\". This raises an important question: can we make a Transformer truly understand the structure of a task so that it can generalize to the longer sequences without training on them? In other words, can we inject the known structure of a task into a Transformer so that it can automatically length-generalize? In this paper, we propose position coupling, a simple yet effective method for length generalization that directly embeds the structure of the tasks into a Transformer. In contrast to the vanilla absolute position mechanism assigning unique and consecutive position IDs to each token, we assign the same position IDs to certain input tokens that are semantically relevant. Coupling such tokens together helps the model learn to solve the task regardless of the length of the given input sequence. For example, in the addition task, it is important to consider the significance of digits, so we couple the positions at the same significance. ### 1.1 Summary of Contributions\n\n- We propose position coupling to tackle the length generalization problem of decoder-only Transformers. Our approach injects the structure of the task into the absolute position encoding by assigning the same position IDs to relevant tokens (Section 3). - With position coupling, we achieve a robust and near-perfect generalization up to 200-digit additions by training Transformers on up to 30 -digit additions, which is a $6.67 \\times$ extrapolation of the operand lengths (Figure 1, Section 4). It is promising since it was not clear whether the length generalization on the addition task can be solved reliably with Transformers (Zhou et al., 2024b). - We theoretically prove by concrete construction that a small (1-layer, 2-head) Transformer equipped with coupled position IDs can add two decimal integers whose lengths are exponential in the embedding dimension (Theorem 5.1). Interestingly, we observe a striking similarity between the attention patterns from our theoretical construction and those extracted from a Transformer trained with a standard optimizer (Section 5.1.1). As a complementary result, we also prove that any 1-layer Transformer without positional information cannot fully solve any permutation-sensitive tasks such as addition (Section 5.2). - We empirically show that position coupling can effectively address various tasks beyond addition, including addition with multiple summands (Section 6.1), multiplication between $N$-digit and 2-digit integers (Section 6.2), and copy/reverse operations (Section 7.1). In particular, we provide a theoretical construction of a 2-layer Transformer that solves the multiplication between $N$-digit and 2-digit integers for exponentially large $N$ (Theorem 6.1). We also verify that position coupling can aid Transformers in learning tasks with multi-dimensional structures (Section 7.2). ## 2 Preliminaries\n\nWe focus on decoder-only Transformers that solve the tasks using next-token prediction. Since we study deterministic tasks with a unique answer, we consider greedy decoding throughout the paper. ### 2.1 Next-token Prediction with Decoder-only Transformers\n\nA decoder-only Transformer returns an output sequence of the same length as the input sequence. One difference from a Transformer encoder is that the attention mechanism in a Transformer decoder occurs only in a single forward direction due to the causal attention mask. Due to this causal nature, the Transformer decoder is mostly used for inferring the next token of each token, just based on the information of the current and the previous tokens. ### 2.2 Data Formats\n\nEach task in this work is represented as a collection of sequences of the form '(query) $=$ (response)': given a query, our task is to infer the response correctly. Thus, we only care about the result of the next-token prediction for the $'=$ ' token and the tokens in the response (except its last token). That is, we only compute the losses and accuracies for those output tokens, as depicted in Figure 2. Previous works commonly observe that data formats play an important role in solving downstream tasks with Transformers because a proper data format enables the model to learn a simple function to solve a task. Here we overview some well-known methods we apply, focusing on the addition task. Reversed Format. Lee et al. (2024) observe that reversing the response leads to improvement in both performance and sample efficiency. For example, ' $653+49=702$ ' becomes ' $653+49=207$ ' in a reversed format. This enables a decoder-only Transformer to infer the response from the least significant digit to the most significant digit, similar to how humans add two numbers. Zero-padding. Zero-paddings ensure that the length of both operands in a query is the same and the length of a response is fixed when the length of the operand is given. That is, by padding the query and the response of an $M$-digit $+N$-digit addition with 0 's, the input sequence becomes a $\\max \\{M, N\\}$-digit addition with $(\\max \\{M, N\\}+1)$-digit response. For example, ' $653+49=702$ ' becomes ' $653+049=0702$ '. Wrapping with BOS/EOS token(s). It is conventional in NLP to put BOS/EOS (beginning-/end-ofsequence) tokens at the beginning/end of the sequence. Lee et al. (2024) use the same token ' $\\$$ ' for BOS and EOS tokens and observe that it is beneficial to wrap each sequence with the $\\$$ token when solving the addition task. We do not observe any significant difference in the performance between sequences with the same and different BOS/EOS tokens. ### 2.3 Positional Embeddings/Encodings (PE)\n\nVaswani et al. (2017) introduce the absolute positional embedding (APE) to Transformers to inject the positional information into the model. The usual APE works as follows: given an input sequence of tokens, we assign a sequence of consecutive position IDs (integers). Each position ID is mapped to a unique PE vector, and the vector is either added or concatenated to the corresponding token embedding vector. We focus on the learned APE initially proposed by Gehring et al. (2017). Length Generalization and PE. It is actively studied whether PE is a crucial factor in solving the length generalization problem of Transformers. Kazemnejad et al. (2023) argue that decoder-only Transformers with no positional encoding (NoPE) can achieve length generalization of downstream tasks since a Transformer decoder can implicitly capture the generalizable positional information due to its causal nature. However, there is a line of works proposing new PE methods to improve the length generalization performance of Transformers (Li et al., 2024; Ruoss et al., 2023). ## 3 Position Coupling: A Method for Length Generalization\n\nWe propose position coupling, which assigns position IDs that directly encode the structure of given tasks. Here, we explain the general position ID assignment rule of position coupling in two steps and then move on to the specific example of addition. First, we partition the tokens of the input sequence. The detailed principles for grouping the tokens differ by task, but the common desiderata are the following: there are two or more groups of consecutive tokens, and each token in a group must have a unique semantic meaning so that a one-to-one correspondence between tokens in different groups can be made. Next, for each group of tokens, we assign a sequence of consecutive numbers (usually, positive integers) as position IDs, starting from a random number (at training time) or a fixed predetermined number (at evaluation time). We use random position IDs at training time for inducing length generalization by enabling all position embedding vectors to be trained, up to a pre-defined hyperparameter of maximum position ID (max_pos). ${ }^{2}$ Very importantly, we assign the same position IDs to the tokens in all groups that are relevant to each other for solving the given task: we refer to this as \"coupling the positions\". Lastly, we set 0 as the position IDs of special tokens like BOS/EOS tokens and the PAD token (padding for minibatch training and evaluation). ### 3.1 Position Coupling for Decimal Integer Addition Task\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-04.jpg?height=296&width=1289&top_left_y=1031&top_left_x=407)\n\nFigure 3: Position coupling for decimal integer addition task, showing $653+49=702$ with appropriate input formats. The starting position ID ' 6 ' is an arbitrarily chosen number. We illustrate position coupling for the decimal integer addition task (or addition task for short). To study the length generalization of the addition task, we regard each digit (0-9) as a single token. For illustration, if we need a concrete example, we will use the addition $653+49=702$. Before applying the position coupling, we adopt an input format similar to Lee et al. (2024) so that we reverse the response, but we use zero-padding and wrapping with BOS/EOS token ' $\\$$ ' at the same time. For example, $' 653+49=702$ ' becomes ' $\\$ 653+049=2070 \\$$ '. We partition the tokens in the sequence into three groups: (1) first operand \\& ' + ', (2) second operand, and (3) ' $=$ ' $\\&$ response (the sum). Then each number token is \"unique\" in the corresponding group in terms of significance, which naturally induces a one-to-one correspondence between (most of) the tokens across different groups. We group ' $=$ ' and the sum together because these tokens are where we perform next-token prediction. Now we assign the coupled position IDs to the tokens. Let us say that the random starting number is 6 . We assign 6,7 , and 8 to the tokens in the operands, and assign $5,6,7$, and 8 to the tokens in the sum in a reversed order: see Figure 3. We remark that, first, we assign 9 as position IDs of ' + ' and ' $=$ ' tokens because they are adjacent to the number token with position ID 8, even if there are no 'significances' for those tokens. Second, we assign 5 as a position ID of the most significant digit of the sum (which may be ' 0 ' due to the zero-padding) just because it is next to the number token with position ID 6 , even though there are no other corresponding tokens in the other groups (operands). We also note that we do not group the ' + ' token with the second operand to prevent unnecessary coupling between ' + ' and the most significant digit of the sum (position ID 5).",
    "posicoupling-1": "Remark. A concurrent work by McLeish et al. (2024) proposes an analogous approach in the context of solving arithmetic tasks, while they employ a different input format; they flip both the query and the response but do not apply any paddings. A more detailed comparison with our work is provided in Section 8. [^1]\n### 3.2 Comparison with Index Hinting\n\nEven though the idea of implanting the structure of a task into the positional encoding is novel, there is an existing approach named index hinting (Zhou et al., 2024a) that applies a similar idea but to the input sequence. Index hinting is an input augmentation technique that places position markers in front of the tokens to couple the semantically relevant tokens. For example, Zhou et al. (2024a) transform ' $653+49=702$ ' into ' $\\mathrm{a} 0 \\mathrm{~b} 6 \\mathrm{c} 5 \\mathrm{~d} 3+\\mathrm{a} 0 \\mathrm{~b} 0 \\mathrm{c} 4 \\mathrm{~d} 9=\\mathrm{a} 0 \\mathrm{~b} 7 \\mathrm{c} 0 \\mathrm{~d} 2$ ' with some zero-paddings, where $\\mathrm{a}, \\mathrm{b}, \\mathrm{c}$, and d are consecutive index hints. Here, the starting hint character a is randomly selected during training, similar to our method of choosing the starting position ID. The reversed format and BOS/EOS tokens can be applied as well. One way in which index hinting differs from position coupling is that it doubles the input sequence length. This is because the position information and the token information do not merge: the index hints and the normal tokens are mapped to separate token embedding vectors which are alternately placed in the input embedding matrix. As a result, a Transformer must figure out the correspondence between each adjacent pair of an index hint and a normal token. Moreover, the doubled input length can require up to four times the training time and memory. In contrast, position coupling explicitly combines token and position information: every token embedding and corresponding position embedding are mixed into a single vector. Hence, a Transformer can effortlessly utilize the positional structure of the task, without hurting the training time. We highlight that, as will be mentioned in Section 4.1, position coupling exhibits better length generalization than index hinting. Another difference is that the index hints should be inferred by Transformers in addition to the normal tokens in the response, which might be an additional burden. Our position coupling circumvents this difficulty, eliminating the need to estimate anything other than the tokens in the original response. ## 4 Experiments on Addition Task\n\nIn this section, we empirically demonstrate that position coupling allows extensive length generalization of Transformers on the addition task. We delve into the impact of training length and architecture on the length generalization performance and provide comparisons with NoPE, APE with a random starting position ID (we call random-start APE), and index hinting (Zhou et al., 2024a).",
    "posicoupling-2": "Data Sampling. We opt for the balanced sampling in terms of the number of digits (Nogueira et al., 2021). Given the maximum number of digits $D_{\\max }$, we do balanced sampling for each operand in two steps. First, we sample the number of digits $D \\in\\left[1, D_{\\max }\\right]$ uniformly at random. Next, we sample an operand from $\\left[10^{D-1}, 10^{D}-1\\right]$ uniformly at random, except for $D=1$ where we sample from $[0,9]$.",
    "posicoupling-3": "This procedure addresses the imbalance problem in the number of digits of operands. Model and Training. As baseline models, we train 1-layer, 4-head, decoder-only Transformers (with roughly 4M trainable parameters) from scratch. We set max_pos as 202 so that the maximum testable length of summands is 200 . We do not use packing or shifting for simplicity of implementation. Since we manually put coupled position IDs with a random starting index during training, we can train all the position embeddings without packing and shifting. ### 4.1 Results\n\nLonger Training Sequences Lead to Longer Generalizable Lengths. We train separate models with $D_{\\max } \\in\\{10,20,30,40\\}$ and evaluate the models on (up to) 200-digit additions. For each run of training, we choose and evaluate the best model in terms of the validation loss for 200-digit additions. We run each experiment 8 times with 2 different random seeds for data generation and 4 different random seeds for model initialization \\& stochastic optimization, unless mentioned otherwise.",
    "posicoupling-4": "We summarize all hyperparameters in Table 1 of Appendix B. The result is showcased in Figure 4. We decide that a model successfully generalizes to a certain length of operands (referred to as \"generalizable length\") if the median EM accuracy exceeds $95 \\%$. We observe that the generalizable length becomes longer as we train on longer training sequences. The generalizable length is 70 for the models trained on additions involving $1-10$ digits, 135 for models trained on $1-20$, and 200 for $1-30$ and $1-40$. We believe that we could achieve even longer generalizable length for the models trained on 1-40 if a larger max_pos is used with careful training techniques. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-06.jpg?height=402&width=1491&top_left_y=230&top_left_x=317)\n\nFigure 4: Ablation on the trained operand lengths. A prior work by Zhou et al. (2024b) provides a similar analysis on the addition tasks. Combining appropriate input format and advanced PE, they achieve $\\geq 98 \\%$ EM accuracy for 100-digit additions with a 6-layer 8-head model trained on 1-40. Moreover, they achieve a generalizable length of 45 for a model trained on 1-30, 25 for $1-20$, and 10 for $1-10$ (no length generalization). One big difference between their analysis and ours is they report the maximum accuracy for each testing length over trials, while we report the medians. Thus, we choose a bit lower threshold $(95 \\%)$ for generalizability than theirs. To better the comparison, we also report the maximum accuracies in Appendix A.1. Shallower Models Generalize Better. Since Zhou et al. (2024a) and Zhou et al. (2024b) choose 6-layer 8 -head models as their base models, we also test our method to deeper models. The experimental details can be found in Table 2 of Appendix B. The results (with models trained on 1-40 digits) displayed in Figure 5 show that, even if deeper models can generalize up to some extent, the performance deteriorates as the model gets deeper. We attribute this phenomenon to the problem of optimization. Since the theoretical construction of a 1-layer addition Transformer (that will appear in Section 5.1) naturally extends to larger architectures, deeper models have at least as much generalization capability as shallower models. Thus, the difficulty of training deep neural networks must be the reason for the degrading performance. The performance of the 1-layer 8 -head model is a bit worse than that of our base model (shown in Figure 4), which can be attributed to the reduction of expressive power due to inversely-proportional shrink of the dimension per attention head (Bhojanapalli et al., 2020). ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-06.jpg?height=468&width=1492&top_left_y=1639&top_left_x=314)\n\nFigure 5: Ablation on the number of layers. Comparison with NoPE and APE (with random starting position ID). Our experiments demonstrate that simple PE methods like NoPE and random-start APE cannot length-generalize well on the addition task. In particular, we implement random-start APE to mimic the training process with the usual APE combined with packing and shifting. The results showcased in Figure 1 imply that naively training all position embeddings does not necessarily help produce a strictly better model in terms of length generalization than that does not use position embeddings at all. We also remark that even training itself is difficult for shallower models (e.g., 1-layer) with NoPE and random-start APE. Comparison with Index Hinting. We test index hinting by running the code we implemented ourselves since the original code is unavailable. From Figure 1, we observe that index hinting indeed helps the model to length-generalize more than the baselines (NoPE \\& random-start APE). However, the generalizable lengths of the models trained with index hinting do not extend further than 50 ; the models completely fail starting from the length 70 . We also observe that enough amount of depth is required for Transformers with index hinting to achieve high enough training and in-distribution validation accuracies. Particularly, the training accuracies of 1-layer models do not deviate from near zero. Thus, we only present the results for the 6 -layer 8 -head model as done by Zhou et al. (2024a). Scaling the Generalizable Lengths Up to 500. We demonstrate the scalability of our proposed position coupling approach for large lengths of up to 500. Specifically, we again train a depth-1 decoder-only model for the addition task and evaluate the performance for instances with up to 500 digits. The results are shown in Figure 6. We notice that at a train length of 160, we achieve excellent length generalization for 500-digit addition. On the other hand, training on sequences of length up to 40 or 80 is insufficient for extreme length generalization. While these results lead to a smaller length generalization ratio of $500 / 160=3.125$, as opposed to the ratio of 6.67 achieved when training on sequences of length up to 30 , the results demonstrate that position coupling, as an approach, is highly scalable. The experimental details can be found in Table 3 of Appendix B. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-07.jpg?height=454&width=1329&top_left_y=955&top_left_x=387)\n\nFigure 6: The full sequence accuracy obtained by training a depth-1 decoder-only model on the addition task. We see that while training with sequences of length up to 40 and 80 is insufficient for generalization to large lengths, at training length 160 we achieve strong performance for lengths up to 500 . ## 5 Theoretical Analyses on 1-layer Transformers\n\nIn the previous section, we provided empirical results exhibiting the outstanding performance of position coupling. One might ask why and how position coupling works so effectively. In Section 5.1, we provide a theoretical explanation by carefully constructing a 1-layer Transformer model that is capable of solving the addition task involving exponentially long operands when the input is encoded with position coupling. We also present the necessity of proper positional information for a 1-layer Transformer to solve the addition task in Section 5.2. ### 5.1 1-layer Transformer with Coupled Positions can Perform Long Additions\n\nFor the sake of simplicity of presentation, we consider a Transformer without any normalization layers, as conventionally done in theoretical constructions by previous works (Awasthi and Gupta, 2023; Yun et al., 2020a,b). For the sake of completeness, readers can find a mathematical formulation of the decoder-only Transformer architecture in Appendix C. Theorem 5.1. With the input format described in Section 3.1, there exists a depth-1 two-head decoder-only Transformer with coupled positions that solves the addition task with next-token prediction. Here, the operand length is at most $2^{\\lfloor(d-17) / 2\\rfloor}-2$, where the embedding dimension is $d \\geq 21$.",
    "posicoupling-5": "We provide a detailed construction in Appendix D. We highlight that our proof is constructive and does not rely on any universal approximation result of neural networks. Theorem 5.1 shows that a 1-layer 2-head Transformer is sufficient for implementing addition between two exponentially long integers. We remark that this result can be naturally extended to larger architectures with more layers/heads, with the help of residual connections. ### 5.1.1 Probing the Attention Patterns in Trained Transformers with Position Coupling\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-08.jpg?height=751&width=1359&top_left_y=297&top_left_x=364)\n\nFigure 7: Probing attention matrices of a 1-layer 2-head Transformer with position coupling, trained on up to 5 -digit additions. (Left) There are two heatmaps (clipped to zero below 0.01) corresponding to the (transposed) attention matrices observed from the attention heads. Averaged over 10 K sequences of 6 -digit additions. (Right) We magnify parts of the attention matrices that are involved in inferring the response (sum). The arrows explain the process of inferring the next token ' 0 ' from ' 3 '. We discover a striking similarity between the attention patterns in our theoretical construction (Theorem 5.1) and those extracted from a Transformer trained with position coupling and a standard optimizer. In particular, the manually constructed attention patterns described in Tables 11 and 17 in Appendix D closely resemble the actual attention patterns in Figure $7 .{ }^{3}$ We were surprised because the similarity was discovered after we came up with the proof of the theorem. Drawn from this discovery, we claim that a Transformer trained with position coupling spontaneously learns two separate components of the addition task: (1) adding two numbers without carries, and (2) predicting the carries. Let us revisit the example in Figure 3 and consider predicting ' 7 ' (position ID 6) as the next token of ' 0 ' (position ID 7). Note that the token ' 7 ' is the result of combining the digit-wise sum $6+0=6$ and a propagated carry 1 . To find out the sum without carry, it is enough for the model to attend to the two previous positions with ID 6 : tokens ' 6 ' and ' 0 '. On the other hand, to predict the carry, the model may attend to the three positions with ID 7 : tokens ' 5 ', ' 4 ', and ' 0 '. The reason way we should care about ' 0 ' is that considering the sum $5+4(=9)$ of the two digits in the operands is not sufficient to determine the existence of the carry. By looking at the token ' 0 ' in the response (with position ID 7), we can detect that the actual sum in this position is $\\underline{10}(=5+4+\\mathbf{1}$, where $\\mathbf{1}$ is another carry propagated from the previous position) and hence we need to propagate a carry $\\underline{1}$ to the next position (with ID 6). Now we inspect the aforementioned claim by examining the attention matrices of an actual trained Transformer. In the model, we discover two different patterns of attention matrices, ${ }^{4}$ playing distinct roles. The first attention pattern (top of the figure) seems to correspond to the addition without carries: each token in the response (including ' $=$ ') attends to two positions needed to find out the sum without carry. Conversely, the second attention pattern (bottom of the figure) seems to correspond to the carry prediction: again, each token in the response attends to three positions required to find out the carry.",
    "posicoupling-6": "Remark. Similarly to our analysis, Quirke et al. (2023) study the attention patterns of a 1-layer 3-head decoder-only Transformer model trained solely on 5-digit addition. They also observe that each head handles different subtasks of addition, such as digit-wise summation and carry detection. [^2]\n### 5.2 1-layer Transformers Require Positional Information\n\nIn Section 4.1, we observed that 1-layer Transformers fail to perform the addition task without position coupling. Here, we provide a partial result that theoretically explains why this happens inevitably, particularly in the case of NoPE. We start with a general proposition: a 1-layer Transformer without positional encoding cannot distinguish queries that are identical up to permutation when inferring the first token of the response using greedy next-token prediction. Proposition 5.2. Consider any depth-1 finite-head decoder-only Transformer model $\\mathcal{T}$ without positional encoding (NoPE). Given an input sequence $\\mathcal{I}$ and its arbitrary permutation $\\mathcal{I}^{\\prime}$, if the last tokens of $\\mathcal{I}$ and $\\mathcal{I}^{\\prime}$ are identical, then the next tokens predicted by $\\mathcal{T}$ will also be identical for both sequences when applying a greedy decoding scheme. The proof is deferred to Appendix E. According to the proposition above, the 1-layer Transformer without positional encoding will always output the same values starting from the ' $=$ ' token, provided that the combination of query tokens is identical, even if their order varies. However, the addition task is permutation-sensitive, meaning that the permuted queries may result in different responses. Therefore, the 1-layer Transformer cannot completely solve the task without positional encoding. It is important to note that this result remains unchanged regardless of the input format: neither reversed format nor index hinting provides any benefit. We also highlight that this impossibility result can be extended to any other permutation-sensitive tasks, such as arithmetic tasks and copy/reverse tasks. Based on this fact, we write code to directly calculate the maximum EM accuracy on the $m$-digit addition task that a 1-layer decoder-only Transformer can achieve (see Appendix E for the code). The accuracies rapidly decrease to zero: $6.2 \\%$ for 3-digit addition, $1 \\%$ for 4 -digit integers, and $0.13 \\%$ for 5 -digit integers. We leave it for future work to investigate the necessary conditions of the architecture for implementing addition when other positional encoding schemes are employed. ## 6 Extension to Other Arithmetic Tasks\n\nThe possible utilization of position coupling is not limited to adding two decimal integers. In the following subsections, we explore the effectiveness of position coupling for the addition task with more than two operands (Section 6.1) and the multiplication task with the length of the second operand being fixed (Section 6.2). ### 6.1 Addition Task with Multiple Summands\n\nThe position coupling scheme for the vanilla addition task (with two operands) can naturally extend to the addition task with multiple summands: assign position IDs in ascending order from most significant digits to least significant digits for every operand and the response. Here, we focus on the addition of three summands. To the best of our knowledge, there is no prior work considering multiple summands. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-09.jpg?height=453&width=1505&top_left_y=1866&top_left_x=299)\n\nFigure 8: Exact-match accuracy (median over 4 runs) for triple addition task, trained on sequences of length 1-40 with position coupling, NoPE, and random-start APE.",
    "posicoupling-7": "For further experiment details, refer to Table 6. Experiments. We train on sequences with operands of 1-40 digits. Our choice of max_pos is 102, so we test the operands of up to 100 digits. We investigate the performance of 3 different architectures, each with a different depth. The experimental results are described in Figure 8. 1-layer models keep their generalization capability until 100 digits, whereas the 3-layer models exhibit great stability across random seeds and achieve the highest generalizable length of 90 . Lastly, we note that the result of Theorem 5.1 can be extended to addition tasks with multiple summands with slight adjustments to the feed-forward layer in the construction. ### 6.2 Position Coupling for $N \\times 2$ Multiplication Tasks\n\nIn this subsection, we study length generalization on the $N$-digit $\\times 2$-digit multiplication task in terms of the length $N$ of the first operand, while fixing the length of the second operand by 2 . Similar tasks have been studied before (Duan and Shi, 2023; Jelassi et al., 2023); Jelassi et al. (2023) investigates $N \\times 3$ using an encoder-only model and Duan and Shi (2023) studies $N \\times 1$ with an encoder-decoder Transformer architecture. Besides the architectural difference, Jelassi et al. (2023) fail to observe length generalization with RPE and only achieve it by supplementing a small number of long samples to the training set. Furthermore, although Duan and Shi (2023) provides perfect length generalization results even for test samples $10 \\times$ longer than those observed during training, their approach requires a retraining step with hand-crafted bias correction on attention score matrices.",
    "posicoupling-8": "Data Format \\& Position Coupling. We reverse and zero-pad the response, setting the length of it as $N+2$. We couple the position starting from the least significant digits of both operands and response, decrementing the ID as we move to their most significant digits: see Figure 9. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-10.jpg?height=335&width=1186&top_left_y=1231&top_left_x=467)\n\nFigure 9: Illustration of position coupling for $N \\times 2$ multiplication task. Experiments. The experimental results showcased in Figure 10 verify the efficacy of position coupling compared to NoPE and random-start APE. We observe that a 1-layer model fails even with position coupling, even for training. However, as the depth increases to 2 or more, it immediately becomes capable of length generalization. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-10.jpg?height=491&width=1311&top_left_y=1923&top_left_x=407)\n\nFigure 10: Exact-match accuracies for $N \\times 2$ multiplication task, trained on sequences of length 1-40. Unlike addition, position coupling for $N \\times 2$ multiplication is less intuitive, as predicting the token in the middle of the response requires multiple digits from both operands while each token in the response is linked with at most 2 tokens in the query. Perhaps surprisingly, we can still construct a Transformer that provably solves this task for exponentially long sequences. Theorem 6.1. Given an appropriate format of the input sequence, there exists a depth-2 decoder-only Transformer model with coupled positions that can perform the $N \\times 2$ multiplication task with next-token prediction. Here, the number of the total heads is 10 and the length of the first operand is at most $2^{\\lfloor(d-34) / 6\\rfloor}-3$, where we denote the token embedding dimension by $d \\geq 46$. We defer the proof to Appendix F. This result suggests that the proposed position coupling scheme for the $N \\times 2$ multiplication task sufficiently captures the inherent structure of the task, and thus provides the potential for the trained model to generalize across unseen lengths. Also, we believe that Theorem 6.1 is optimal in terms of the number of attention layers, as the depth-1 model exhibits total failure even for in-distribution samples in our experiment. ## 7 Applying Position Coupling Beyond Arithmetic Tasks\n\nIn this section, we investigate the versatility of position coupling beyond arithmetic tasks. We begin by examining copy/reverse tasks in Section 7.1. Then, we investigate the efficacy of position coupling in managing multi-dimensional tasks in Section 7.2. ### 7.1 Position Coupling for Copy/Reverse Tasks\n\nWe consider the copy task and the reverse task, allowing duplicate tokens in the query. In particular, only 10 characters are considered to appear in the query, while the length of the query can be much longer. Notably, the copy task with duplicates is identified by Zhou et al. (2024a) as a difficult task for decoder-only Transformers to length-generalize, whereas the Transformers can easily length-generalize if there are no duplicate tokens. Data Format \\& Position Coupling. We couple the positions in the query and the response by their correspondence. Note that the position ID assigned to the equal token is different for the two tasks because as per our design principle (Section 3), the equal token is grouped to the response tokens and position IDs have to be consecutive numbers within each group. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-11.jpg?height=282&width=1411&top_left_y=1607&top_left_x=354)\n\nFigure 11: Illustration of position coupling for copy/reverse tasks. Experiments. We compare the performance of position coupling with NoPE and random-start APE. We train the model on lengths 1-40 and evaluate its performance on lengths from 5 to 300 , at intervals of 5 . While a 1-layer 4 -head model is used for the position coupling, we observe that the same architecture fails to memorize training samples for both NoPE and random-start APE. Therefore, we use a 6 -layer 8 -head model for the latter cases as it is commonly used in the literature (Zhou et al., 2024a). The experimental results are described in Figures 12 and 13. For both copy and reverse, position coupling exhibits near-perfect accuracy across the entire test length ( $7.5 \\times$ for the trained length). In contrast, NoPE and random-start APE immediately fail to length-generalize. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-12.jpg?height=389&width=1316&top_left_y=223&top_left_x=402)\n\nFigure 12: Exact-match accuracy (median over 4 runs) for copying task, trained on sequences of length 1-40 with position coupling, NoPE, and random-start APE. For further experiment details, refer to Table 7. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-12.jpg?height=383&width=1318&top_left_y=790&top_left_x=401)\n\nFigure 13: Exact-match accuracy (median over 4 runs) for reversing task, trained on sequences of length 1-40 with position coupling, NoPE, and random-start APE. For further experiment details, refer to Table 7. ### 7.2 Two-dimensional Position Couplings for Minesweeper Generator Task\n\nThe tasks considered thus far are 1D, indicating that both the query and response are structured as 1D objects. This leads us to a natural question: Can position coupling still be effective for multi-dimensional tasks? To examine this question, we explore extending position coupling to accommodate a 2 D task, where both the query and response are originally 2D objects. Specifically, we define and investigate a new task called the minesweeper generator. Given a rectangular board where each cell is filled with either ' M ' (mine) or ' $*$ ' (empty cell), the task is to generate a new board of the same size, with each cell containing:\n\n- ' M ', if the corresponding cell in the original board contains ' M ';\n- The count of mines in 8 adjacent cells, if the corresponding cell in the original board contains ' $*$ '.",
    "posicoupling-9": "Data Format \\& Position Coupling. We introduce two position coupling modules: one for the row direction and another for the column direction. Following this, we flatten the board to feed it into a Transformer: see Figure 14. Within the model, an embedding vector for each token (cell) is generated by adding the token embedding vector and corresponding two PE vectors. Experiments. To assess the efficacy of position coupling, we contrast its performance with NoPE. The training samples are designed with the width and height of the board between 5 and 9 inclusively. We allow the width and height to be different for training samples. We evaluate the test performance on a square board with a width between 5 and 14 inclusively. We also employ a 4 -layer 8 -head model for position coupling and a 6 -layer 8 -head model for NoPE. In particular, for position coupling, we use the same embedding layer for both position coupling modules, as this approach empirically performs better than using distinct embedding layers for each module (see Appendix A.2). The experimental results are described in Figure 15. Position coupling maintains over $98 \\%$ accuracy until a width of 12 and near $90 \\%$ accuracy even at a width of 14 . In contrast, NoPE fails even for in-distribution samples. One might worry that the generalizable length of 12 seems only slightly higher than the trained length of 9 . However, we stress that our query is a 2D board, therefore the actual length generalization is from 81 to 144 . ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-13.jpg?height=711&width=1530&top_left_y=214&top_left_x=295)\n\nFigure 14: Position coupling for the two-dimensional 'minesweeper generator' task. (Left) The idea of assigning coupled position IDs. (Right) The model receives a flattened sequence of input tokens and 2D position IDs. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-13.jpg?height=391&width=1315&top_left_y=1070&top_left_x=405)\n\nFigure 15: EM accuracies for minesweeper generator task, trained on sequences of length $(5-9) \\times(5-9)$. ## 8 Related Works\n\nLength Generalization on Arithmetic/Algorithmic Tasks. Despite of simplicity of the tasks, arithmetic and algorithmic tasks have received a lot of attention as subjects to study the length generalization of Transformers. Shen et al. (2023) propose \"Random Spacing\" and \"Recursive Scratchpad\", achieving near-perfect generalization from 10-digits to 12-digits addition. Awasthi and Gupta (2023) devise an auxiliary task for the sorting task and achieves significant length generalization through multitask learning. Ruoss et al. (2023) propose Randomized Positional Encodings to address the problem of the appearance of unseen position IDs in test time. Most recently, Zhou et al. (2024b) demonstrate a possibility of extrapolation to the length 100 with training length 1-40 in the addition task by combining appropriate input format and advanced PE , yet they also observe that the performances are not robust and highly depend on the random seeds. Analyzing Length Generalization in Theoretical Perspectives An emerging line of research seeks to theoretically address why length generalization is difficult and under what conditions it can be achieved. In Abbe et al. (2023), the authors demonstrate that various neural network models have an implicit bias towards min-degree interpolators, which may not be ideal for various reasoning tasks. Xiao and Liu $(2023,2024)$ investigate problems whose reasoning processes can be formulated as directed acyclic graph (DAG) structures, introducing the concept of maximal input element distance to identify a sufficient condition for length generalization. Recently, Ahuja and Mansouri (2024) formulate the conditions of function classes required to guarantee the length generalization of the empirical risk minimizer function.",
    "posicoupling-10": "Comparison with McLeish et al. (2024) A very recent concurrent work by McLeish et al. (2024) proposes a new position embedding method called \"Abacus\". From a methodological perspective, Abacus is almost identical to our position coupling except for two main differences: Abacus reverses both the query and the response and does not use padding.",
    "posicoupling-11": "From now on, we outline the differences between their work and ours beyond the methodology. In terms of the model architecture, they use a depth-16 decoder-only Transformer model. They combine their method with looped Transformers and input injection and report an improved performance. In contrast, our main results are obtained with shallower models (up to 6 layers) with standard Transformer architecture of stacked decoder layers. Besides the addition task, they study multiplication, sorting, and Bitwise OR. On the other hand, we study multiplication, triple addition, copy/reverse, and a 2D task. Specifically, for the multiplication task, their study mainly considers the case where the length of both operands could vary up to 15 . In contrast, we focus solely on the $N \\times 2$ task, fixing the length of the second operand by 2 . While we achieve length generalization up to 90 -digit multiplication by training the model on up to 40-digit multiplication, they report near-perfect in-distribution performance but poor length generalization. Finally and notably, we provide novel theoretical analyses, including (1) the constructive proof that a depth-1 Transformer equipped with position coupling can completely solve the addition task for exponentially long digits and (2) the impossibility of the same model being capable of the addition task. We also present theoretical results for the $N \\times 2$ multiplication task. ## 9 Conclusion\n\nAchieving length generalization of Transformers even in the simple case of the addition task has been a challenge that received a lot of attention. We propose position coupling, a variant of learned APE, which enables capturing task structure to improve the length generalization performance of Transformers for addition. We show that a Transformer trained on 1-30 digit addition can generalize up to 200-digit addition. We also provide the construction of a 1-layer Transformer model capable of adding two exponentially long integers when position coupling is applied. Furthermore, we verify the efficacy of position coupling for length generalization in other arithmetic and algorithmic tasks. Limitations \\& Future Directions. We intentionally limited ourselves to the tasks with an explicit structure between the tokens in each sequence. This is because we are proposing a method to instill the known structure of the task into a Transformer by training on short sequences. Designing the coupling of positions for tasks whose structure is implicit or black-box (e.g., for general NLP tasks) remains a fascinating next step: we leave the methodology for uncovering hidden structures and autonomously creating appropriate couplings (without manually designing them) for future work. We also leave two challenging arithmetic tasks to length-generalize for future work. The first is addition with a varying number of summands, i.e., determining if the model can generalize to adding more integers than the trained samples. The second task is multiplication, where the lengths of both operands can vary. ## Acknowledgements\n\nHC, JC, and CY acknowledge support from a Google Gift on the research related to Long Context Transformers. The experiments contained in this work were supported in part through the Google Cloud Platform Credit Award. ## References\n\nEmmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In International Conference on Machine Learning, pages 31-60. PMLR, 2023. 8\n\nKartik Ahuja and Amin Mansouri. On provable length and compositional generalization. arXiv preprint arXiv:2402.04875, 2024. 8\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546-38556, 2022. 1\n\nPranjal Awasthi and Anupam Gupta. Improving length-generalization in transformers via task hinting.",
    "posicoupling-12": "arXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.",
    "posicoupling-13": "arXiv preprint arXiv:2312.11805, 2023. 1\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).",
    "posicoupling-14": "arXiv preprint arXiv:1606.08415, 2016. C\nKevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th international conference on computer vision, pages 2146-2153.",
    "posicoupling-15": "IEEE, 2009. C\n\nSamy Jelassi, St\u00e9phane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran\u00e7ois Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023. 6.2\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2023. 1, 1, 2.3, B\nJeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, 2021. 1\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. 1\n\nNayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=dsUB4bst9S. 1, 1, 2.2, 3.1\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022. 1\n\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=rR03qFesqk. 2.3\nDavid Lindner, J\u00e1nos Kram\u00e1r, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability.",
    "posicoupling-16": "Advances in Neural Information Processing Systems, 36, 2023. 11\nSean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, and Tom Goldstein. Transformers can do arithmetic with the right embeddings, 2024. $3.1,8$\n\nVinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807-814, 2010. C\n\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021. 1, 4\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. B\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=R8sQPpGCv0. 1\n\nPhilip Quirke et al. Understanding addition in transformers. arXiv preprint arXiv:2310.13121, 2023. 5.1.1\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020. B\nAnian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023. 2.3, 8\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 1, C\nRuoqi Shen, S\u00e9bastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic.",
    "posicoupling-17": "arXiv preprint arXiv:2311.14737, 2023. 8\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications.",
    "posicoupling-18": "arXiv preprint arXiv:2201.08239, 2022. 1\nTrieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 627(8004):E8-E8, 2024.",
    "posicoupling-19": "doi: 10.1038/s41586-024-07115-7. URL https: //doi.org/10.1038/s41586-024-07115-7. 1\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, $2.3, \\mathrm{~B}, \\mathrm{C}$\n\nGail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pages 11080-11090. PMLR, 2021. 11\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. B\n\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. Advances in Neural Information Processing Systems, 35:32353-32368, 2022. 1\n\nChangnan Xiao and Bing Liu. Conditions for length generalization in learning reasoning skills. arXiv preprint arXiv:2311.16173, 2023. 8\nChangnan Xiao and Bing Liu. A theory for length generalization in learning to reason.",
    "posicoupling-20": "arXiv preprint arXiv:2404.00560, 2024. 8\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.",
    "posicoupling-21": "In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. B\n\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions?",
    "posicoupling-22": "In International Conference on Learning Representations, 2020a. URL https://openreview.net/forum?id=ByxRMONtvr. 5.1, C, C\nChulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. $\\mathrm{O}(\\mathrm{n})$ connections are expressive enough: Universal approximability of sparse transformers.",
    "posicoupling-23": "Advances in Neural Information Processing Systems, 33:13783-13794, 2020b. 5.1\nBiao Zhang and Rico Sennrich. Root mean square layer normalization.",
    "posicoupling-24": "Advances in Neural Information Processing Systems, 32, 2019. B, 1, E\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-115, 2021. F.8, F. 8\n\nYi Zhang, Arturs Backurs, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with LEGO: A synthetic reasoning task, 2023.",
    "posicoupling-25": "URL https://openreview.net/forum?id= 1jDN-RfQfrb. 1\nHattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id= AssIuHnmHX. 1, $1,3.2,4,4.1,4.1,7.1,7.1,11$\n\nYongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly.",
    "posicoupling-26": "arXiv preprint arXiv:2402.09371, 2024b. 1, 1, 1.1, 4.1, 8, A.1, B\n\n## Contents\n\n1 Introduction ..... 1\n1.1 Summary of Contributions ..... 2\n2 Preliminaries ..... 3\n2.1 Next-token Prediction with Decoder-only Transformers ..... 3\n2.2 Data Formats ..... 3\n2.3 Positional Embeddings/Encodings (PE) ..... 3\n3 Position Coupling: A Method for Length Generalization ..... 4\n3.1 Position Coupling for Decimal Integer Addition Task ..... 4\n3.2 Comparison with Index Hinting ..... 5\n4 Experiments on Addition Task ..... 5\n4.1 Results ..... 5\n5 Theoretical Analyses on 1-layer Transformers ..... 7\n5.1 1-layer Transformer with Coupled Positions can Perform Long Additions ..... 7\n5.1.1 Probing the Attention Patterns in Trained Transformers with Position Coupling ..... 8\n5.2 1-layer Transformers Require Positional Information ..... 9\n6 Extension to Other Arithmetic Tasks ..... 9\n6.1 Addition Task with Multiple Summands ..... 9\n6.2 Position Coupling for $N \\times 2$ Multiplication Tasks ..... 10\n7 Applying Position Coupling Beyond Arithmetic Tasks ..... 11\n7.1 Position Coupling for Copy/Reverse Tasks ..... 11\n7.2 Two-dimensional Position Couplings for Minesweeper Generator Task ..... 12\n8 Related Works ..... 13\n9 Conclusion ..... 14\nA More Experimental Results on Position Couping ..... 20\nA. 1 Decimal Integer Addition Task: Maximum Exact-Match Accuracies ..... 20\nA. 2 Two-dimensional Task: Sharing the Position Embedding Modules or Not ..... 20\nB Experiment Details and Hyperparameters ..... 21\nC Decoder-only Transformer Architecture ..... 26\nD Formal Construction of Addition Transformer with Position Coupling ..... 27\nD. 1 Notation ..... 27\nD. 2 Input Sequence ..... 27\nD. 3 Encoding Function ..... 29\nD.3.1 Token Embedding ..... 29\nD.3.2 Coupled Position IDs and Position Embedding ..... 29\nD. 4 Transformer Block - Causal Attention Layer ..... 30\nD.4.1 Attention Head 1: Digit-wise Addition without Carries ..... 30\nD.4.2 Attention Head 2: Carry \\& EOS Detection ..... 34\nD.4.3 Residual Connection ..... 38\nD. 5 Transformer Block - Token-wise Feed-forward Layer ..... 38\nD.5.1 Subnetwork 1: Construction for SUM (dimension 7-16). ..... 39\nD.5.2 Subnetwork 2: Construction for IS_EOS (dimension 17) ..... 41\nD.5.3 Residual Connection .....",
    "posicoupling-27": "41\nD. 6 Decoding Function ..... 42\nE Impossibility of Addition with No Positional Encoding ..... 44\nF (Formal) Construction of $N \\times 2$ Multiplication Transformer with Position Coupling ..... 46\nF. 1 Notation ..... 46\nF. 2 Input Sequence ..... 46\nF. 3 Encoding Function ..... 47\nF.3.1 Token Embedding ..... 47\nF.3.2 Coupled Position IDs and Position Embedding .....",
    "posicoupling-28": "48\nF.",
    "posicoupling-29": "4 Construction Idea ..... 49\nF. 5 Transformer Block 1 - Causal Attention Layer ..... 50\nF.5.1 Attention Head 1: Detecting the Ones Digit of the Second Operand ..... 50\nF.5.2 Attention Head 2: Detecting the Tens Digit of the Second Operand ..... 52\nF.5.3 Attention Head 3: Position Masking ..... 54\nF.5.4 Residual Connection ..... 56\nF. 6 Transformer Block 1 - Token-wise Feed-forward Layer ..... 56\nF.6.1 Residual Connection ..... 57\nF. 7 Transformer Block 2 - Causal Attention Layer ..... 57\nF.7.1 Attention Head 1: Copying the Ones Digit of the Second Operand ..... 57\nF.7.2 Attention Head 2: Copying the Tens Digit of the Second Operand ..... 59\nF.7.3 Attention Head 3: Copying the Appropriate Digit from the First Operand I ..... 61\nF.7.4 Attention Head 4: Copying the Appropriate Digit from the First Operand II ..... 63\nF.7.5 Attention Head 5: Copying the Appropriate Digit from the First Operand III ..... 64\nF.7.6 Attention Head 6: Copying the Appropriate Digit from the First Operand IV ..... 66\nF.7.7 Attention Head 7: Copying the Appropriate Digit from the First Operand V ..... 67\nF.7.8 Residual Connection ..... 69\nF. 8 Transformer Block 2 - Token-wise Feed-forward Layer ..... 69\nF.8.1 Residual Connection ..... 71\nF. 9 Decoding Function ..... 71\n\n## A More Experimental Results on Position Couping\n\n## A. 1 Decimal Integer Addition Task: Maximum Exact-Match Accuracies\n\nFor a better comparison with Zhou et al. (2024b), we report the maximum exact-match (EM) accuracies. See Figures 16 and 17 . ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-20.jpg?height=526&width=1318&top_left_y=496&top_left_x=401)\n\nFigure 16: Ablation on the trained lengths. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-20.jpg?height=418&width=1315&top_left_y=1187&top_left_x=405)\n\nFigure 17: Ablation on the number of layers. ## A. 2 Two-dimensional Task: Sharing the Position Embedding Modules or Not\n\nHere, we present the extra experimental results for training the minesweeper generator task with position coupling. Specifically, we compare the performance of two configurations: one where the model shares the same positional embedding layer for both position coupling modules, and another where the model uses separate positional embedding layers for each position coupling module. The results are described in Figure 18. When sharing the same positional embedding layer, position coupling achieves over $98 \\%$ accuracy on a $12 \\times 12$ board, and maintains near $90 \\%$ accuracy on a $14 \\times 14$ board. However, with distinct positional embedding layers, position coupling only successfully generalizes to a $10 \\times 10$ board.",
    "posicoupling-30": "We currently do not have a clear explanation for why the former method exhibits significantly better performance than the latter one. We leave the investigation and explanation of this phenomenon for future work. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-21.jpg?height=383&width=1319&top_left_y=226&top_left_x=403)\n\nFigure 18: Exact-match accuracy (median over 4 runs) for minesweeper generator task, trained on sequences of length $(5-9) \\times(5-9)$ with position coupling. For further experiment details, refer to the Table 8 . ## B Experiment Details and Hyperparameters\n\nPosition coupling can be easily implemented on top of usual libraries of training transformer models like HuggingFace (Wolf et al., 2019) and Flaxformer ${ }^{5}$ since these libraries support an arbitrary array of position IDs (in the case of using APE). All we need is to build up a short routine implementing the assigning rule of position IDs when establishing the dataset and data loaders.",
    "posicoupling-31": "To compare with NoPE, we use the code base provided by Kazemnejad et al. (2023) for most of the experiments. ${ }^{6}$ It contains a custom implementation of decoder-only T5 (Raffel et al., 2020) established on top of PyTorch (Paszke et al., 2019) and Huggingface, including several PE methods. We additionally implement a custom RMSNorm module (Zhang and Sennrich, 2019) and various positioning schemes of normalization layers (e.g., PreNorm (Xiong et al., 2020), PostNorm (Vaswani et al., 2017), and their combination), to follow the implementation details of Zhou et al.",
    "posicoupling-32": "(2024b). Table 1: Hyperparameter summary for decimal integer addition task in the main text: comparison between trained lengths (Figure 4). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | 1 |\n| Number of Attention Heads | 4 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 128 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU (Shazeer, 2020) |\n| Normalization Layer | RMSNorm (Zhang and Sennrich, 2019) |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Training Steps | 50,000 |\n| Batch Size | 1,000 |\n| Optimizer | Adam (Kingma and Ba, 2015) |\n| Learning Rate (LR) | 0.0001 |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 202 |\n| Training Dataset Size | $1,000,000$ |\n| Evaluation Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 10$ hours |\n\n[^3]Table 2: Hyperparameter summary for decimal integer addition task in the main text: comparison between the number of layers (Figure 5). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | $1-6$ |\n| Number of Attention Heads | 8 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 64 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | RMSNorm |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Trained Lengths of Operands | $1-40$ |\n| Training Steps | 50,000 |\n| Batch Size | 400 |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.0001 |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 202 |\n| Training Dataset Size | $1,000,000$ |\n| Evaluation Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 10$ hours |\n\nTable 3: Hyperparameter summary for decimal integer addition task: generalization up to length 500 (Figure 6). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | 1 |\n| Number of Attention Heads | 2 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 256 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | LayerNorm (Ba et al., 2016) |\n| Normalization Layer Position | PostNorm |\n| Training Steps | $1,000,000$ |\n| Batch Size | 128 |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.0001 |\n| LR Warm-up | Linear (From 0 to LR), 500 steps |\n| LR Cool-down | Cosine Decay (From LR to 0.0) |\n| Maximum Position ID (max_pos) | 1003 |\n| Training Dataset Size | $1,000,000$ |\n| Evaluation Dataset Size | 100,000 |\n| Device | 64 TPU V4 Chips |\n| Training Time | $\\leq 4$ hours |\n\nTable 4: Hyperparameter summary for decimal integer addition task: extracting attention patterns (Figure 7). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | 1 |\n| Number of Attention Heads | 2 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 256 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | RMSNorm |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Trained Lengths of Operands | $1-5$ |\n| Training Steps | 50,000 |\n| Batch Size | 100 |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.00005 |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 17 |\n| Training Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 6$ hours |\n\nTable 5: Hyperparameter summary for $N \\times 2$ multiplication task (Figure 10). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | $1-4$ (Ours), 3 (NoPE \\& Random-start APE) |\n| Number of Attention Heads | 8 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 64 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | RMSNorm |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Trained Lengths of Operands | $1-40$ |\n| Training Steps | 50,000 |\n| Batch Size | 200 (Ours), 800 (NoPE \\& Random-start APE) |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.0001 |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 203 (Ours), 1023 (Random-start APE) |\n| Training Dataset Size | 50,000 (Ours), 500,000 (Others) |\n| Evaluation Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 8$ hours |\n\nTable 6: Hyperparameter summary for addition task with three summands (Figure 8). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | $1-3$ (Ours), 6 (NoPE \\& Random-start APE) |\n| Number of Attention Heads | 4 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 128 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | RMSNorm |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Trained Lengths of Operands | $1-40$ |\n| Training Steps | 50,000 |\n| Batch Size | 1000 (Ours), 800 (Others) |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.0001 |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 102 (Ours), 1023 (Random-start APE) |\n| Training Dataset Size | $1,000,000$ |\n| Evaluation Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 12$ hours |\n\nTable 7: Hyperparameter summary for copy/reverse task (Figures 12 and 13). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | 1 (Ours), 6 (NoPE \\& Random-start APE) |\n| Number of Attention Heads | $4($ Ours), 8 (NoPE \\& Random-start APE) |\n| Embedding Dimension | 512 |\n| Dimension per Head | 128 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | RMSNorm |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Trained Lengths of Query | $1-40$ |\n| Training Steps | 50,000 |\n| Batch Size | 1000 (Ours), 500 (Others) |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.0001 |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 301 (Ours), 601 (Random-start APE) |\n| Training Dataset Size | $1,000,000$ |\n| Evaluation Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 8$ hours |\n\nTable 8: Hyperparameter summary for minesweeper generator task (Figures 15 and 18). | Hyperparameter | Value |\n| :--- | :--- |\n| Architecture | Decoder-only Transformer |\n| Number of Layers | $4($ Ours $), 6$ (NoPE) |\n| Number of Attention Heads | 8 |\n| Embedding Dimension | 512 |\n| Dimension per Head | 64 |\n| Hidden Width of Feed-forward Layer | 2048 |\n| Activation Function of Feed-forward Layer | GEGLU |\n| Normalization Layer | RMSNorm |\n| Normalization Layer Position | PreNorm and PostNorm |\n| Trained Lengths of Query | $(5-9) \\times(5-9)$ |\n| Training Steps | 100,000 |\n| Batch Size | 200 |\n| Optimizer | Adam |\n| Learning Rate (LR) | 0.0001 (Ours), 0.0002 (NoPE) |\n| LR Warm-up | Linear (From 0 to LR), 1\\% of total steps |\n| LR Cool-down | Cosine Decay (From LR to 0.1LR) |\n| Maximum Position ID (max_pos) | 15 |\n| Training Dataset Size | 100,000 |\n| Evaluation Dataset Size | 100,000 |\n| Device | NVIDIA RTX A6000 48GB |\n| Training Time | $\\leq 30$ hours |\n\n## C Decoder-only Transformer Architecture\n\nHere we detail the architecture of an depth- $L, H$-head decoder-only Transformer (Vaswani et al., 2017).",
    "posicoupling-33": "For a simple presentation, we ignore the normalization layers, as in Yun et al.",
    "posicoupling-34": "(2020a). Let $\\mathcal{V}$ be the (ordered) vocabulary, a set of all tokens. Given an input sequence $\\mathcal{I} \\in \\mathcal{V}^{N}$ and its length $N$, the encoding function Enc: $\\mathcal{V}^{N} \\rightarrow \\mathbb{R}^{d \\times N}$ maps it to\n\n$$\n\\boldsymbol{X}^{(0)}:=\\operatorname{Enc}(\\mathcal{I})\n$$\n\nIt is a sum of the token embedding and the position embedding. Next, there are $L$ Transformer blocks that sequentially transform this input. We denote by $\\mathrm{Tf}_{l}: \\mathbb{R}^{d \\times N} \\rightarrow \\mathbb{R}^{d \\times N}$ the operation of the $l$-th block $(l \\in[L])$, so that\n\n$$\n\\boldsymbol{X}^{(l)}:=\\operatorname{Tf}_{l}\\left(\\boldsymbol{X}^{(l-1)}\\right)\n$$\n\nThe block $\\mathrm{Tf}_{l}$ consists of a (causal) attention layer $\\operatorname{Att}_{l}: \\mathbb{R}^{d \\times N} \\rightarrow \\mathbb{R}^{d \\times N}$ and a (token-wise) feed-forward layer $\\mathrm{FF}_{l}: \\mathbb{R}^{d \\times N} \\rightarrow \\mathbb{R}^{d \\times N}$, each of which contains a residual connection:\n\n$$\n\\mathrm{Tf}_{l}:=\\left(\\mathrm{id}+\\mathrm{FF}_{l}\\right) \\circ\\left(\\mathrm{id}+\\mathrm{Att}_{l}\\right)\n$$\n\nwhere we denote by id: $\\mathbb{R}^{d \\times N} \\rightarrow \\mathbb{R}^{d \\times N}$ an identity map. Each attention layer $\\mathrm{Att}_{l}$ consists of $H$ attention heads. Its $h$-th head $(h \\in[H])$ has matrices $\\boldsymbol{Q}_{h}^{(l)}, \\boldsymbol{K}_{h}^{(l)} \\in$ $\\mathbb{R}^{d_{Q K, h}^{(l)} \\times d}, \\boldsymbol{V}_{h}^{(l)} \\in \\mathbb{R}^{d_{V, h}^{(l)} \\times d}$ and $\\boldsymbol{U}_{h}^{(l)} \\in \\mathbb{R}^{d \\times d_{V, h}^{(l)}}$ as its parameters. ${ }^{7}$ With these matrices, borrowing the notation from Yun et al. (2020a), the attention layer with an input $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$ can be written as\n\n$$\n\\operatorname{Att}_{l}(\\boldsymbol{X}):=\\sum_{h=1}^{H} \\boldsymbol{U}_{h}^{(l)} \\boldsymbol{V}_{h}^{(l)} \\boldsymbol{X} \\cdot \\operatorname{softmax}\\left(\\left(\\boldsymbol{K}_{h}^{(l)} \\boldsymbol{X}\\right)^{\\top} \\boldsymbol{Q}_{h}^{(l)} \\boldsymbol{X}\\right)\n$$\n\nHere the softmax operator takes a square matrix $\\boldsymbol{M} \\in \\mathbb{R}^{N \\times N}$ and outputs an $N \\times N$ upper-triangular columnstochastic ${ }^{8}$ matrix\n\n$$\n[\\operatorname{softmax}(\\boldsymbol{M})]_{i j}=\\frac{e^{\\boldsymbol{M}_{i j}}}{\\sum_{1 \\leq i^{\\prime} \\leq j} e^{\\boldsymbol{M}_{i^{\\prime} j}}} \\mathbb{1}_{\\{i \\leq j\\}}\n$$\n\nwhere $\\mathbb{1}_{\\{\\mathcal{E}\\}}$ is an indicator function for a predicate $\\mathcal{E}$ : it equals 1 if $\\mathcal{E}$ is true and 0 otherwise. Note that the upper triangularity captures the auto-regressive behavior of the causal attention. For the sake of convenience, we denote by $\\boldsymbol{Y}^{(l)}:=\\boldsymbol{X}^{(l-1)}+\\operatorname{Att}_{l}\\left(\\boldsymbol{X}^{(l-1)}\\right) \\in \\mathbb{R}^{d \\times N}$ which is a consequence of residual connection right after the attention layer. Each feed-forward layer $\\mathrm{FF}_{l}$ is a two-layer perceptron having $\\boldsymbol{W}_{1}^{(l)} \\in \\mathbb{R}^{d_{F} \\times d}, \\boldsymbol{b}_{1}^{(l)} \\in \\mathbb{R}^{d_{F}}, \\boldsymbol{W}_{2}^{(l)} \\in \\mathbb{R}^{d \\times d_{F}}, \\boldsymbol{b}_{2}^{(l)} \\in \\mathbb{R}^{d}$ as its parameters. It applies the following map to each column $\\boldsymbol{y}$ of an input $\\boldsymbol{Y}$ :\n\n$$\n\\boldsymbol{y} \\mapsto \\boldsymbol{W}_{2}^{(l)} \\phi\\left(\\boldsymbol{W}_{1}^{(l)} \\boldsymbol{y}+\\boldsymbol{b}_{1}^{(l)}\\right)+\\boldsymbol{b}_{2}^{(l)}\n$$\n\nwhere $\\phi$ is a component-wise activation function. That is, the feed-forward layer is defined as\n\n$$\n\\mathrm{FF}_{l}(\\boldsymbol{Y}):=\\boldsymbol{W}_{2}^{(l)} \\phi\\left(\\boldsymbol{W}_{1}^{(l)} \\boldsymbol{Y}+\\boldsymbol{b}_{1}^{(l)} \\mathbf{1}_{d_{F}}^{\\top}\\right)+\\boldsymbol{b}_{2}^{(l)} \\mathbf{1}_{d}^{\\top}\n$$\n\nwhere $\\mathbf{1}_{d}$ is the $d$-dimensional vectors filled with 1 's. Here we mainly use the $\\operatorname{ReLU}$ operation $\\phi(\\cdot)=\\max \\{\\cdot, 0\\}$ (Jarrett et al., 2009; Nair and Hinton, 2010), but there are many other popular choices such as GeLU (Hendrycks and Gimpel, 2016), GLU (Dauphin et al., 2017), ReGLU, and GEGLU (Shazeer, 2020). The final component of the Transformer model is the decoding function Dec : $\\mathbb{R}^{d \\times N} \\rightarrow \\mathcal{V}^{N}$, which is composed of a linear readout and a (token-wise) arg-max operation. Here, the linear readout is simply a linear layer having $\\boldsymbol{W}_{\\text {out }} \\in \\mathbb{R}^{|\\mathcal{V}| \\times d}$ as its parameter. The decoding function produces the output sequence\n\n$$\n\\mathcal{O}:=\\operatorname{Dec}\\left(\\boldsymbol{X}^{(L)}\\right) \\in \\mathcal{V}^{N}\n$$\n\n[^4]\n## D Formal Construction of Addition Transformer with Position Coupling\n\nHere we show how to implement the addition by employing a depth- 1 two-head decoder-only Transformer equipped with position coupling. We restate the theorem for the sake of readability. Theorem 5.1. With the input format described in Section 3.1, there exists a depth-1 two-head decoder-only Transformer with coupled positions that solves the addition task with next-token prediction.",
    "posicoupling-35": "Here, the operand length is at most $2^{\\lfloor(d-17) / 2\\rfloor}-2$, where the embedding dimension is $d \\geq 21$.",
    "posicoupling-36": "Organization of the Proof. A whole section is dedicated to prove Theorem 5.1. - We start with the notation (Appendix D.1). - We review and formalize the format of the input sequence (zero-padding, reversed format, and wrapping with BOS/EOS) (Appendix D.2). - We define the encoding function Enc with a table of a concrete example (Appendix D.3), where Enc maps an input sequence of length $N$ to a $d \\times N$ encoding matrix $\\boldsymbol{X}^{(0)}$. - We devote a lot of pages to the construction of the parameters of a causal attention layer $\\mathrm{Att}_{1}$ to generate desired attention patterns (Appendix D.4). The attention layer has two heads playing distinct roles: (1) preparing for a sum without considering carries; and (2) preparing for the carry prediction \\& EOS detection. - We provide a construction of a token-wise feed-forward neural network $\\mathrm{FF}_{1}$ which is a two-layer ReLU network (Appendix D.5). It consists of two subnetworks playing different roles: (1) producing one-hot vectors, each of which indicates a digit of the sum (response); and (2) binary values indicating whether the position is the end of the sequence. - We conclude the proof by defining the decoding function Dec which performs the linear readout and the arg-max operation to generate the output sequence (Appendix D.6). We illustrate the roadmap of the proof in Figure 19. ## D. 1 Notation\n\nFor the architecture of the decoder-only Transformer, we follow the notation introduced in Appendix C. Let $\\boldsymbol{e}_{i}^{d}$ denote the $i$-th standard basis vector of $\\mathbb{R}^{d}$. For example, $\\boldsymbol{e}_{1}^{3}=\\left[\\begin{array}{lll}1 & 0 & 0\\end{array}\\right]^{\\top}$. Let $\\boldsymbol{I}_{m}$ be the $m \\times m$ identity matrix. Let $\\mathbf{0}_{p}$ and $\\mathbf{1}_{p}$ denote the $p$-dimensional vectors filled with 0 's and 1 's, respectively. Similarly, let $\\mathbf{0}_{m \\times n}$ denote the $m \\times n$ zero matrix. For a positive integer $n$, we frequently use the set $[n]:=\\{1, \\ldots, n\\}$. For any matrix $\\boldsymbol{A}$, denote the $i$-th row and $j$-th column of $\\boldsymbol{A}$ by $\\boldsymbol{A}_{i}$ and $\\boldsymbol{A}_{\\bullet}$, respectively. Given two non-negative integers $a$ and $b$, let $\\ell(a, b)$ be the length of a longer one between $a$ and $b$. For example, $\\ell(12,3456)=4$. Consider an ordered vocabulary $\\mathcal{V}=(0,1,2,3,4,5,6,7,8,9,+,=, \\$)$. We include a special token ' $\\$$ ' that plays the role of both the beginning-of-sequence (BOS) token and the end-of-sequence (EOS) token. ${ }^{9}$ We denote $\\mathcal{V}_{k}$ as $k$-th element of $\\mathcal{V}$. For instance, $\\mathcal{V}_{4}=3$ and $\\mathcal{V}_{13}=\\$$. Lastly, since we employ only one Transformer block, we omit the superscripts $(l)$ in the parameter matrices/vectors and the size of dimensions $d_{Q K, h}^{(l)}$ and $d_{V, h}^{(l)}$. ## D. 2 Input Sequence\n\nWe seek to perform an addition $a+b=c$ using next-token prediction. To this end, we want to transform it into an input sequence $\\mathcal{I}=\\overline{\\$ A+B=C}$ of an appropriate format. Note that the EOS token is the last token that needs to be predicted, so we exclude EOS in the input sequence.",
    "posicoupling-37": "Let $\\ell:=\\ell(a, b)$. We first zero-pad the shorter one between $a$ and $b$ to match the length of the part $A$ and part $B$ as $\\ell$. Sometimes, the sum $c$ might be longer than $a$ or $b$ due to a carry. To make the length of the part $C$ consistent, we also put a zero-pad in front of $c$ to set its length as $\\ell+1$. Also, to ease calculating the addition with next-token prediction, we reverse the sum $c$ to make the part $C$. For example, if we have a sum $3812+98=3910$, we use\n\n[^5]![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-28.jpg?height=1391&width=1695&top_left_y=215&top_left_x=215)\n\nFigure 19: Roadmap to the formal construction of addition Transformer with position coupling. $\\overline{\\$ 3812+0098=01930}$ as an input sequence; if a sum $98+9907=10005$ is given, we use $\\overline{\\$ 0098+9907=50001}$ as an input sequence. The red digits are zero-paddings, and the blue digits are the reversed sum. To recap, the input sequence $\\mathcal{I}=\\overline{\\sigma_{1} \\sigma_{2} \\ldots \\sigma_{N}} \\in \\mathcal{V}^{N}$ of length $N=3 \\ell+4$ consists of six parts:\n\n1. the BOS token $\\sigma_{1}=' \\$$ '\n2. the first operand $A=\\overline{\\sigma_{2} \\ldots \\sigma_{\\ell+1}}$ where $\\sigma_{i} \\in\\{0, \\ldots, 9\\}$\n3. the addition symbol $\\sigma_{\\ell+2}='+$ ';\n4. the second operand $B=\\overline{\\sigma_{\\ell+3} \\cdots \\sigma_{2 \\ell+2}}$ where $\\sigma_{i} \\in\\{0, \\ldots, 9\\}$\n5. the equality symbol $\\sigma_{2 \\ell+3}='=$ ';\n6. the (reversed) sum $C=\\overline{\\sigma_{2 \\ell+4} \\ldots \\sigma_{3 \\ell+4}}$ where $\\sigma_{i} \\in\\{0, \\ldots, 9\\}$. Note that the part $C$ might be incomplete (i.e., $N<3 \\ell+4$ ) at the inference time; we infer the digits of the part $C$ one by one using next-token prediction. Throughout this section on a formal construction, however, we only consider the train time setup in which we infer all the digits of the part $C$ at once using simultaneous next-token prediction in a single forward pass. Precisely, we want to use an input sequence $\\mathcal{I}=\\overline{\\sigma_{1} \\ldots \\sigma_{N}}$ to produce an output sequence $\\mathcal{O}=\\overline{\\sigma_{1}^{\\prime} \\ldots \\sigma_{N}^{\\prime}}$ where $\\overline{\\sigma_{2 \\ell+3}^{\\prime} \\ldots \\sigma_{N-1}^{\\prime}}=C=\\overline{\\sigma_{2 \\ell+4} \\ldots \\sigma_{N}}$ and $\\sigma_{N}^{\\prime}=' \\$$ ' (EOS).",
    "posicoupling-38": "## D. 3 Encoding Function\n\nWe plan to produce an input encoding, given an input sequence $\\mathcal{I}$ designed as above. The encoding matrix $\\boldsymbol{X}^{(0)}$ is of size $d \\times N$ : each column represents an embedding vector for a token, while each row represents a particular named dimension. What we mean by named dimension is that we give a name to each dimension for a clear description of our formal construction. We construct an input encoding by concatenating the token embedding and the position embedding, which can be viewed as a sum of two different embedding matrices of the same size. Table 9: Example initial encoding. Here we consider the input sequence $\\overline{\\$ 653+049=2070}$ and the starting position ID is chosen as $s=2$. The vectors $\\boldsymbol{v}_{\\square}^{P}$ are defined in Equation (11). The gray rows will be filled in later. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 6 | 5 | 3 | 0 | 0 | 4 | 9 | 0 | 2 | 0 | 7 | 0 |\n| 2: IS_BOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 4: PRE_SUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: PRE_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7-16: SUM | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18-(P+17): POS_1 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ |\n| $(P+18)-(2 P+17):$ POS_2 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ |\n\n## D.3.1 Token Embedding\n\nThe token embedding consists of 17 dimensions: we call them\n\n$$\n\\begin{gathered}\n1=\\mathrm{NUM}, 2=\\mathrm{IS} \\_ \\text {BOS }, 3=\\text { FULL_ONES, } \\\\\n4=\\text { PRE } \\_ \\text {SUM, } 5=\\text { PRE } \\_ \\text {CARRY, } 6=\\text { PRE } \\_ \\text {EOS, } \\\\\n\\{7, \\ldots, 16\\}=\\text { SUM, and } 17=\\mathrm{IS} \\_ \\text {EOS.",
    "posicoupling-39": "}\n\\end{gathered}\n$$\n\nInitially, we let the last 14 dimensions be empty (i.e., all zeros). Thus, we explain the first three dimensions, NUM, IS_BOS, and FULL_ONES. Dimension 1 (NUM). For a number token $(0, \\ldots, 9)$, we put itself in the dimension NUM. For the other tokens $(+,=, \\$)$, we put 0 . Dimension 2 (Is_bOS). For a special token ' $\\$$ ', we put 1 in the dimension IS_BOS. We put 0 otherwise. Dimension 3 (FULL_ONES). We put 1 everywhere in this dimension. ## D.3.2 Coupled Position IDs and Position Embedding\n\nBefore constructing a position embedding, we specify the coupled position IDs for the addition task. Let max_pos be a hyperparameter of the maximum position IDs, where position IDs are non-negative integers. Basically, we match the significance of the digits: e.g., a least significant digit is always coupled to the other least significant digits. To this end, we first randomly choose a starting position $I D s \\in\\left[\\max \\_\\right.$pos $\\left.-\\ell-1\\right]$. (For that, $\\max \\_$pos $\\geq \\ell+2$ must hold.) Then we allocate the position IDs of token $\\sigma_{i}$ in the input sequence $\\mathcal{I}=\\overline{\\sigma_{1} \\ldots \\sigma_{N}}$ as\n\n$$\np(i)= \\begin{cases}0, & i=1 \\\\ s+i-1, & i=2, \\ldots, \\ell+2 \\\\ s+i-(\\ell+2), & i=\\ell+3, \\ldots, 2 \\ell+3 \\\\ s+(3 \\ell+4)-i & i=2 \\ell+4, \\ldots, 3 \\ell+4\\end{cases}\n$$\n\nRecall that $N=3 \\ell+4$. Also, observe that for $i \\in\\{2, \\ldots, \\ell+1\\}$\n\n$$\np(i)=p(i+\\ell+1)=p(3 \\ell+5-i)=s+i\n$$\n\nwhich couples the position of $(\\ell-i+2)$-th significant digit in the first operand $(A)$, the second operand $(B)$, and the sum $(C)$.",
    "posicoupling-40": "Also, the position of tokens ' + ' and ' $=$ ' are coupled. Lastly, the only token that has the position ID 0 is the special token ' $\\$$ '. Before moving on to the positional embedding, we define $\\boldsymbol{v}_{k}^{D}\\left(k \\in\\left[2^{D}\\right]\\right)$ as\n\n$$\n\\boldsymbol{v}_{k}^{D}=\\left[(-1)^{b_{i}^{(D, k)}}\\right]_{i=1}^{D} \\in \\mathbb{R}^{D}\n$$\n\nwhere $b_{i}^{(D, k)}$ is defined as the $i$-th (from left) digit of $D$-digit binary representation of $k-1$. For example, if $D=2$,\n\n$$\n\\boldsymbol{v}_{1}^{2}=\\left[\\begin{array}{ll}\n1 & 1\n\\end{array}\\right]^{\\top}, \\boldsymbol{v}_{2}^{2}=\\left[\\begin{array}{ll}\n-1 & 1\n\\end{array}\\right]^{\\top}, \\boldsymbol{v}_{3}^{2}=\\left[\\begin{array}{ll}\n1 & -1\n\\end{array}\\right]^{\\top}, \\boldsymbol{v}_{4}^{2}=\\left[\\begin{array}{ll}\n-1 & -1\n\\end{array}\\right]^{\\top}\n$$\n\nWe remark that the points $\\boldsymbol{v}_{k}^{D}$ are the vertices of $D$-dimensional hypercube with side length 2 , centered at the origin. ${ }^{10}$ Note that for $k \\neq l$,\n\n$$\n\\left\\|\\boldsymbol{v}_{k}^{D}\\right\\|^{2}=D, \\quad\\left\\langle\\boldsymbol{v}_{k}^{D}, \\boldsymbol{v}_{l}^{D}\\right\\rangle \\leq D-2\n$$\n\nNow we explain the position embedding. It consists of $2 P$ dimensions, which eventually become from 18-th to $(2 P+17)$-th dimension after concatenation. If $p(i)=0$, we let $\\mathbf{0}_{2 P}$ as a position embedding vector. For the positive position IDs $p(i) \\geq 1$, we let a concatenation\n\n$$\n\\left[\\begin{array}{c}\n\\boldsymbol{v}_{p(i)}^{P} \\\\\n\\boldsymbol{v}_{p(i)+1}^{P}\n\\end{array}\\right]\n$$\n\nas a position embedding vector of a token $\\sigma_{i}$. (In case of $p(i)=2^{P}$, we use $\\boldsymbol{v}_{1}^{P}$ instead of $\\boldsymbol{v}_{p(i)+1}^{P}$. .) We call the former $P$ dimensions for the position embedding as POS $\\_1$ and the latter $P$ dimensions as POS $\\_2$. Concatenating the token embedding and the position embedding, we get the input embedding $\\boldsymbol{X}^{(0)}$. See Table 9 for an example. As a result, the total embedding dimension is $d=2 P+17$. Note the maximum possible position ID that can be represented with $\\boldsymbol{v}_{k}^{P}$ 's is max_pos $=2^{P}=2^{\\lfloor(d-17) / 2\\rfloor}$. Therefore, the length of an operand must be $\\ell \\leq$ max_pos $-2=2^{\\lfloor(d-17) / 2\\rfloor}-2$. ## D. 4 Transformer Block - Causal Attention Layer\n\nThe goal of the causal attention layer is to fill in the zero-blanks ${ }^{11}$ of the encoding matrix at dimensions PRE_SUM, PRE_CARRY, and PRE_EOS. We divide the roles into two different heads. ## D.4.1 Attention Head 1: Digit-wise Addition without Carries\n\nThe goal of the first head is to perform a digit-wise addition without carries and to fill in the blanks of the encoding matrix at dimension PRE_SUM. Later, using this dimension, combined with the dimension PRE_CARRY, we will be able to perform the next-token prediction for addition. For now, we do not care about the carries, which will be dealt with in a later section. Formally, we aim to perform $\\sigma_{i}+\\sigma_{i+\\ell+1}$ for each $i \\in\\{2, \\cdots, \\ell+1\\}$ and put its result at the $(3 \\ell+4-i)$-th position (column) of the dimension PRE_SUM (row). To this end, we utilize our position embedding. [^6]Recall that $d=2 P+17$ and let $d_{Q K, 1}=P+1$. Let $M>0$ be a number determined later. Let\n\n$$\n\\begin{aligned}\n\\boldsymbol{Q}_{1} & =\\left(\\begin{array}{ccc}\n\\mathbf{0}_{P \\times 17} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{FULL} \\_\\mathrm{ONES}}^{17}\\right)^{\\top} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 1} \\times d} \\\\\n\\boldsymbol{K}_{1} & =\\left(\\begin{array}{ccc}\n\\mathbf{0}_{P \\times 17} & \\mathbf{0}_{P \\times P} & \\sqrt{M} \\boldsymbol{I}_{P} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{IS} \\_\\mathrm{BOS}}^{17}\\right)^{\\top} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 1 \\times d}}\n\\end{aligned}\n$$\n\nThe linear transformations with matrices $\\boldsymbol{Q}_{1}$ and $\\boldsymbol{K}_{1}$ do two different jobs at once. (1) $\\boldsymbol{Q}_{1}$ ( $\\boldsymbol{K}_{1}$, resp.) takes the dimensions POS $\\_1$ (POS_2, resp.) from the input encoding matrix and scale them up by $\\sqrt{M}$; (2) $\\boldsymbol{Q}_{1}\\left(\\boldsymbol{K}_{1}\\right.$, resp.) takes the dimension FULL_ONES (IS_BOS, resp.) and scale it up by $\\sqrt{M P}$. For concrete examples, please refer to Tables 12 and 13. By these, the attention score matrix $\\boldsymbol{C}_{1}:=\\left(\\boldsymbol{K}_{1} \\boldsymbol{X}^{(0)}\\right)^{\\top} \\boldsymbol{Q}_{1} \\boldsymbol{X}^{(0)}$ becomes as in Table 10. The blanks in Table 10 are the numbers smaller than $M(P-2)$; the asterisks ( ${ }^{(*)}$ ) are the entries (or lower triangular submatrices) ignored by the causal softmax operator; the dots represents the hidden MP's. Table 10: Exact attention score matrix $\\boldsymbol{C}_{1}$ (with explicit row/column indices) of Head 1. $$\n\\begin{array}{r|ccccccccccccc}\n\\text { row } \\backslash \\text { col } & j=1 & 2 & 3 & \\cdots & \\ell+2 & \\ell+3 & \\ell+4 & \\cdots & 2 \\ell+3 & \\cdots & 3 \\ell+2 & 3 \\ell+3 & 3 \\ell+4 \\\\\n\\hline i=1 & M P & M P & M P & \\cdots & M P & M P & M P & \\cdots & M P & \\cdots & M P & M P & M P \\\\\n2 & * & & M P & & & & M P & & & & M P & & \\\\\n\\vdots & * & * & & \\ddots & & & & \\ddots & & . & & & \\\\\n\\ell+1 & * & * & * & & M P & & & & M P & & & & \\\\\n\\ell+2 & * & * & * & * & & & & & & & & \\\\\n\\ell+3 & * & * & * & * & * & & M P & & & & M P & \\\\\n\\vdots & * & * & * & * & * & * & & \\ddots & & & \\\\\n2 \\ell+2 & * & * & * & * & * & * & * & & M P & & & \\\\\n2 \\ell+3 & * & * & * & * & * & * & * & * & M P & & & \\\\\n\\vdots & * & * & * & * & * & * & * & * & * & & & & \\\\\n3 \\ell+2 & * & * & * & * & * & * & * & * & * & * & & & \\\\\n3 \\ell+3 & * & * & * & * & * & * & * & * & * & * & * & & \\\\\n3 \\ell+4 & * & * & * & * & * & * & * & * & * & * & * & * &\n\\end{array}\n$$\n\nNow consider the attention matrix $\\boldsymbol{A}_{1}:=\\operatorname{sof} \\operatorname{tmax}\\left(\\boldsymbol{C}_{1}\\right) \\in \\mathbb{R}^{N \\times N}$. Its exact form is a bit messy due to the softmax operation of finite numbers. However, one can observe that, if the number $M$ is large enough, it gets close to the column-stochastic matrix $\\boldsymbol{T}_{1} \\in \\mathbb{R}^{N \\times N}$ described in Table 11. The blanks in Table 11 are zeros; the dots represent the omitted nonzero entries. Table 11: Limiting attention matrix $\\boldsymbol{T}_{1}$ (with explicit row/column indices) of Head 1, as $M$ gets large.",
    "posicoupling-41": "| row $\\backslash$ col | $j=1$ | 2 | 3 | $\\cdots$ | $\\ell+2$ | $\\ell+3$ | $\\ell+4$ | $\\cdots$ | $2 \\ell+3$ | $\\cdots$ | $3 \\ell+2$ | $3 \\ell+3$ | $3 \\ell+4$ |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | 1 | $1 / 2$ | $\\cdots$ | $1 / 2$ | 1 | $1 / 3$ | $\\cdots$ | $1 / 3$ | $\\cdots$ | $1 / 3$ | 1 | 1 |\n| 2 | 0 | 0 | $1 / 2$ |  | 0 | 0 | $1 / 3$ |  | 0 |  | $1 / 3$ | 0 | 0 |\n| $\\vdots$ |  |  |  | $\\ddots$ |  |  |  | $\\ddots$ |  | . |  |  |  |\n| $\\ell+1$ | 0 | 0 | 0 |  | $1 / 2$ | 0 | 0 |  | $1 / 3$ |  | 0 | 0 | 0 |\n| $\\ell+2$ | 0 | 0 | 0 |  | 0 | 0 | 0 |  | 0 |  | 0 | 0 | 0 |\n| $\\ell+3$ | 0 | 0 | 0 |  | 0 | 0 | $1 / 3$ |  | 0 |  | $1 / 3$ | 0 | 0 |\n| $\\vdots$ |  |  |  |  |  |  |  | $\\ddots$ |  | . |  |  |  |\n| $2 \\ell+2$ | 0 | 0 | 0 |  | 0 | 0 | 0 |  | $1 / 3$ |  | 0 | 0 | 0 |\n| $2 \\ell+3$ | 0 | 0 | 0 |  | 0 | 0 | 0 |  | 0 |  | 0 | 0 | 0 |\n| $\\vdots$ |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| $3 \\ell+4$ | 0 | 0 | 0 |  | 0 | 0 | 0 |  | 0 |  | 0 | 0 | 0 |\n\nLet $\\boldsymbol{R}_{1}=\\boldsymbol{A}_{1}-\\boldsymbol{T}_{1} \\in \\mathbb{R}^{N \\times N}$ be the error matrix, which is upper triangular.",
    "posicoupling-42": "Its exact form is messy as well, but we can obtain the bounds of their entries.",
    "posicoupling-43": "Consider a pair of indices $(i, j) \\in[N]^{2}$ such that $i \\leq j$. Let $x_{j}=1 /\\left[\\boldsymbol{T}_{1}\\right]_{1 j} \\in\\{1,2,3\\}$. If $\\left[\\boldsymbol{T}_{1}\\right]_{i j}=\\frac{1}{x_{j}},\\left[\\boldsymbol{R}_{1}\\right]_{i j}<0$ and\n\n$$\n-\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{1}{x_{j}}-\\frac{e^{M P}}{x_{j} e^{M P}+\\left(j-x_{j}\\right) e^{M(P-2)}}=\\frac{j-x_{j}}{x_{j}\\left(x_{j} e^{2 M}+\\left(j-x_{j}\\right)\\right)}\n$$\n\nOn the other hand, if $\\left[\\boldsymbol{T}_{1}\\right]_{i j}=0,\\left[\\boldsymbol{R}_{1}\\right]_{i j}>0$ and\n\n$$\n\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{e^{M(P-2)}}{x_{j} e^{M P}+\\left(j-x_{j}\\right) e^{M(P-2)}}=\\frac{1}{x_{j} e^{2 M}+\\left(j-x_{j}\\right)}\n$$\n\nNow let $d_{V, 1}=1$ and\n\n$$\n\\begin{aligned}\n\\boldsymbol{V}_{1} & =3\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 1} \\times d} \\\\\n\\boldsymbol{U}_{1} & =\\boldsymbol{e}_{\\mathrm{PRE} \\_\\mathrm{SUM}}^{d} \\in \\mathbb{R}^{d \\times d_{V, 1}}\n\\end{aligned}\n$$\n\nThe linear transformation with matrix $\\boldsymbol{U}_{1} \\boldsymbol{V}_{1}$ takes the dimension NUM from the input encoding matrix, scales it up by 3 , and puts it to the dimension PRE_SUM. A concrete example is provided in Table 14. Obtaining $\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{1}$, its every entry is zero except at the dimension PRE_SUM. Observe that $\\left[\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)}\\right]_{\\text {(PRE_sum) }}=$ 0 , because in the input encoding matrix, the dimension NUM starts with 0 . Also, note that it is enough to focus on the columns $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+4\\}$ since we only care about the next-token prediction of the tokens after $\\sigma_{2 \\ell+3}=^{\\prime}=$ '. Specifying the dimension (i.e., the particular row) for these columns, we have\n\n$$\n\\begin{aligned}\n{\\left[\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{1}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{SUM}\\right) j} } & = \\begin{cases}\\boldsymbol{X}_{(\\mathrm{NUM})(3 \\ell+4-j)}^{(0)}+\\boldsymbol{X}_{(\\mathrm{NUM})(4 \\ell+5-j)}^{(0)} & \\text { if } j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+2\\}, \\\\\n0 & \\text { if } j \\in\\{3 \\ell+3,3 \\ell+4\\},\\end{cases} \\\\\n& = \\begin{cases}\\sigma_{(3 \\ell+4)-j}+\\sigma_{(4 \\ell+5)-j} & \\text { if } j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+2\\}, \\\\\n0 & \\text { if } j \\in\\{3 \\ell+3,3 \\ell+4\\} .\\end{cases}\n\\end{aligned}\n$$\n\nRefer to Table 15 for a concrete example of computing $\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{1}$. Also, for the softmax errors,\n\n$$\n\\left[\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{1}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{SUM}\\right) j}=\\sum_{2 \\leq i \\leq j} 3 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}\n$$\n\nSpecifically, if $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+2\\}$ (thus $x_{j}=3$ ),\n\n$$\n\\left[\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{1}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{SUM}\\right) j}=\\underbrace{\\sum_{i \\in\\{(3 \\ell+4)-j,(4 \\ell+5)-j\\}} 3 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}}_{\\text {negative }}+\\underbrace{\\sum_{\\substack{2 \\leq i \\leq j \\\\(0)+4)}} 3 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}}_{\\substack{i \\neq(3 \\ell+4)-j \\\\ i \\neq(4 \\ell+5)-j}}\n$$\n\nwhere\n\n$$\n0 \\leq-\\sum_{i \\in\\{(3 \\ell+4)-j,(4 \\ell+5)-j\\}} 3 \\boldsymbol{X}_{(\\text {NUM }) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{2 \\cdot 9(j-3)}{3 e^{2 M}+(j-3)}\n$$\n\nholds by Equation (17), and\n\n$$\n0 \\leq \\sum_{\\substack{2 \\leq i \\leq j \\\\ i \\neq(3 \\ell+4)-j \\\\ i \\neq(4 \\ell+5)-j}} 3 \\boldsymbol{X}_{(\\text {NUM }) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{27(j-3)}{3 e^{2 M}+(j-3)}\n$$\n\nholds by Equation (18). On the other hand, if $j \\in\\{3 \\ell+3,3 \\ell+4\\}$,\n\n$$\n0 \\leq\\left[\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{1}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{sUM}\\right) j}=\\sum_{2 \\leq i \\leq j} 3 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{27(j-1)}{e^{2 M}+(j-1)}\n$$\n\nOne can easily prove these inequalities by using the bounds of $\\left[\\boldsymbol{R}_{1}\\right]_{i j}$ 's and the fact that the entries in $\\boldsymbol{X}_{\\text {(NUM) }}^{(0)}$ lie in the interval $[0,9]$. If we let $M \\geq \\frac{1}{2} \\log (N-1)+3$, we can ensure that $\\left|\\left[\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{1}\\right]_{\\left(\\mathrm{PRE}_{-} \\mathrm{sUM}\\right) j}\\right|$ smaller than 0.1 for each $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+4\\}$. The proof is simple: it is enough to check\n\n$$\n\\frac{27(N-3)}{3 e^{2 M}+(N-3)}<\\frac{1}{10}, \\quad \\frac{27(N-1)}{e^{2 M}+(N-1)}<\\frac{1}{10}\n$$\n\nTable 12: Example of $\\boldsymbol{Q}_{1} \\boldsymbol{X}^{(0)}$, continuing from Table 9. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{0}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 13: Example of $\\boldsymbol{K}_{1} \\boldsymbol{X}^{(0)}$, continuing from Table 9. |  | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{0}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 14: Example of $\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)}$, continuing from Table 9. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: PRE_SUM | 0 | 18 | 15 | 9 | 0 | 0 | 12 | 27 | 0 | 6 | 0 | 21 | 0 |\n| 5: PRE_CARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: PRE_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7-16: SUM | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18-end: POS_1,POS_2 | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ |\n\nTable 15: Example of $\\boldsymbol{U}_{1} \\boldsymbol{V}_{1} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{1}$, continuing from Table 14. See Table 11 for the definition of $\\boldsymbol{T}_{1}$. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: PRE_SUM | 0 | 0 | 9 | 7.5 | 4.5 | 0 | 6 | 9 | 12 | 9 | 6 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: PRE_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7-16: SUM | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18-end: POS_1,POS_2 | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ |\n\n## D.4.2 Attention Head 2: Carry \\& EOS Detection\n\nThe goal of the second head is to fill in the blanks of the encoding matrix at dimensions PRE_CARRY and PRE_EOS. At dimension PRE_EOS, we will put (approximately) 1 if the next token would be the EOS token (' $\\$$ '), otherwise, we will put strictly smaller numbers like (approximately) $2 / 3$ and $1 / 2$. What we will put at dimension PRE_CARRY is the evidence of the presence of an additional carry, which is not quite straightforward to understand. Let us take a look at some examples. Consider an addition $3+9=12$. Since it is greater than or equal to 10 , the least significant digits in the operands generate a carry 1 . But in some cases, a pair of digits with a sum less than 10 can make a carry. Next, consider an addition $53+49=102$. In the second least significant digits, An addition of 5 and 4 occurs. However, a carry is already produced in the least significant digits $(3+9=12)$, so the total sum including the carry is 10 , not 9 . Thus, it also produces a carry. But how can we know the presence of a carry while only looking at the second least significant digits? The answer is to observe the second least significant digit in the sum, 0 of 102 . Somehow, the consequence of adding 5 and 4 is 0 , (or 10 , implicitly) so it makes a carry. To generalize this explanation, let $a$ and $b$ be digits of the operands in the same significance, and $c$ be a digit of the sum in the same significance as $a$ and $b$. We find that the rule of recognizing that the addition of $a$ and $b$ generates a carry is that\n\n$$\n\\begin{cases}\\text { If } a+b-c \\in\\{9,10\\}, & \\text { then a carry is generated, } \\\\ \\text { Otherwise } & \\text { then the carry is not generated. }\\end{cases}\n$$\n\nThus, it is crucial to store the information of $a+b-c$ or any related one somewhere. In fact, we can store $a+b+c$ at dimension PRE_CARRY of the encoding matrix, and it can be transformed into $a+b-c$ and used later in the feed-forward layer. Formally, we aim to perform $\\sigma_{i}+\\sigma_{i+\\ell+1}+\\sigma_{3 \\ell+5-i}$ for each $i \\in\\{2, \\ldots, \\ell+1\\}$ and put its result at the $(3 \\ell+5-i)$-th position (column) of the dimension PRE_CARRY (row).",
    "posicoupling-44": "To this end, we again utilize our position embedding. Recall that $d=2 P+17$ and let $d_{Q K, 2}=P+1$. Let\n\n$$\n\\begin{aligned}\n\\boldsymbol{Q}_{2} & =\\left(\\begin{array}{ccc}\n\\mathbf{0}_{P \\times 17} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{FULL} \\_\\mathrm{ONES}}^{17}\\right)^{\\top} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 2 \\times d}} \\\\\n\\boldsymbol{K}_{2} & =\\left(\\begin{array}{ccc}\n\\mathbf{0}_{P \\times 17} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{IS}}^{17}{ }_{-\\mathrm{BOS}}\\right)^{\\top} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 2 \\times d}}\n\\end{aligned}\n$$\n\nThe linear transformations with matrices $\\boldsymbol{Q}_{2}$ and $\\boldsymbol{K}_{2}$ do two different jobs at once. (1) they take the dimensions POS _ 1 from the input encoding matrix and scale them up by $\\sqrt{M} ;(2) \\boldsymbol{Q}_{2}\\left(\\boldsymbol{K}_{2}\\right.$, resp. $)$ takes the dimension FULL_ONES (IS_BOS, resp.) and scale it up by $\\sqrt{M P}$. For concrete examples, refer to Tables 18 and 19. By these, the attention score matrix $\\boldsymbol{C}_{2}:=\\left(\\boldsymbol{K}_{2} \\boldsymbol{X}^{(0)}\\right)^{\\top} \\boldsymbol{Q}_{2} \\boldsymbol{X}^{(0)}$ becomes as in Table 16. The blanks in Table 16 are the numbers less than equal to $M(P-2)$; the asterisks ( $\\left.{ }^{(* *}\\right)$ are the entries (or lower triangular submatrices) ignored by the causal softmax operator; the dots represent the hidden MP's. Now consider the attention matrix $\\boldsymbol{A}_{2}:=\\operatorname{sof} \\operatorname{tmax}\\left(\\boldsymbol{C}_{2}\\right) \\in \\mathbb{R}^{N \\times N}$. Similarly to the previous head, if the number $M$ is large enough, it gets close to the column-stochastic matrix $\\boldsymbol{T}_{2} \\in \\mathbb{R}^{N \\times N}$ described in Table 17. The blanks in Table 17 are zeros; the dots represent the omitted nonzero entries. Let $\\boldsymbol{R}_{2}=\\boldsymbol{A}_{2}-\\boldsymbol{T}_{2} \\in \\mathbb{R}^{N \\times N}$ be the error matrix, which is upper triangular as well. Its exact form is messy as well, but we can obtain the bounds of their entries.",
    "posicoupling-45": "Consider a pair of indices $(i, j) \\in[N]^{2}$ such that $i \\leq j$. Let $x_{j}=1 /\\left[\\boldsymbol{T}_{2}\\right]_{1 j} \\in\\{1,2,3,4\\}$. If $\\left[\\boldsymbol{T}_{2}\\right]_{i j}=\\frac{1}{x_{j}},\\left[\\boldsymbol{R}_{2}\\right]_{i j}<0$ and\n\n$$\n-\\left[\\boldsymbol{R}_{2}\\right]_{i j} \\leq \\frac{1}{x_{j}}-\\frac{e^{M P}}{x_{j} e^{M P}+\\left(j-x_{j}\\right) e^{M(P-2)}}=\\frac{j-x_{j}}{x_{j}\\left(x_{j} e^{2 M}+\\left(j-x_{j}\\right)\\right)}\n$$\n\nOn the other hand, if $\\left[\\boldsymbol{T}_{2}\\right]_{i j}=0,\\left[\\boldsymbol{R}_{2}\\right]_{i j}>0$ and\n\n$$\n\\left[\\boldsymbol{R}_{2}\\right]_{i j} \\leq \\frac{e^{M(P-2)}}{x_{j} e^{M P}+\\left(j-x_{j}\\right) e^{M(P-2)}}=\\frac{1}{x_{j} e^{2 M}+\\left(j-x_{j}\\right)}\n$$\n\nTable 16: Exact attention score matrix $\\boldsymbol{C}_{2}$ (with explicit row/column indices) of Head 2. | row $\\backslash$ col | $j=1$ | 2 | $\\cdots$ | $\\ell+1$ | $\\ell+2$ | $\\ell+3$ | $\\cdots$ | $2 \\ell+2$ | $2 \\ell+3$ | $2 \\ell+4$ | $\\cdots$ | $3 \\ell+3$ | $3 \\ell+4$ |  |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | $M P$ | $M P$ | $\\cdots$ | $M P$ | $M P$ | $M P$ | $\\cdots$ | $M P$ | $M P$ | $M P$ | $\\cdots$ | $M P$ | $M P$ |  |\n| 2 | $*$ | $M P$ |  |  |  | $M P$ |  |  |  |  |  | $M P$ |  |  |\n| $\\vdots$ | $*$ | $*$ | $\\ddots$ |  |  |  | $\\ddots$ |  |  |  | . |  |  |  |\n| $\\ell+1$ | $*$ | $*$ | $*$ | $M P$ |  |  |  | $M P$ |  | $M P$ |  |  |  |  |\n| $\\ell+2$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  |  |  | $M P$ |  |  |  |  |  |\n| $\\ell+3$ | $*$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  |  |  |  |  | $M P$ |  |  |\n| $\\vdots$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $\\ddots$ |  |  |  |  | $*$ |  |  |\n| $2 \\ell+2$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  | $M P$ |  |  |  |  |\n| $2 \\ell+3$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  |  |  |  |  |\n| $2 \\ell+4$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  |  |  |  |\n| $\\vdots$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $\\ddots$ |  |  |  |\n| $3 \\ell+3$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  |  |\n| $3 \\ell+4$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $M P$ |  |\n\nTable 17: Limiting attention matrix $\\boldsymbol{T}_{2}$ (with explicit row/column indices) of Head 2, as $M$ gets large.",
    "posicoupling-46": "| row $\\backslash$ col | $j=1$ | 2 | $\\cdots$ | $\\ell+1$ | $\\ell+2$ | $\\ell+3$ | $\\cdots$ | $2 \\ell+2$ | $2 \\ell+3$ | $2 \\ell+4$ | $\\cdots$ | $3 \\ell+3$ | $3 \\ell+4$ |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | $1 / 2$ | $\\cdots$ | $1 / 2$ | $1 / 2$ | $1 / 3$ | $\\cdots$ | $1 / 3$ | $1 / 3$ | $1 / 4$ | $\\cdots$ | $1 / 4$ | $1 / 2$ |\n| 2 | $*$ | $1 / 2$ |  | 0 | 0 | $1 / 3$ |  | 0 | 0 | 0 |  | $1 / 4$ | 0 |\n| $\\vdots$ | $*$ | $*$ | $\\ddots$ |  |  |  | $\\ddots$ |  |  |  | . |  |  |\n| $\\ell+1$ | $*$ | $*$ | $*$ | $1 / 2$ | 0 | 0 |  | $1 / 3$ | 0 | $1 / 4$ |  | 0 | 0 |\n| $\\ell+2$ | $*$ | $*$ | $*$ | $*$ | $1 / 2$ | 0 |  | 0 | $1 / 3$ | 0 |  | 0 | 0 |\n| $\\ell+3$ | $*$ | $*$ | $*$ | $*$ | $*$ | $1 / 3$ |  | 0 | 0 | 0 |  | $1 / 4$ | 0 |\n| $\\vdots$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $\\ddots$ |  |  |  | . |  |  |\n| $2 \\ell+2$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $1 / 3$ | 0 | $1 / 4$ |  | 0 | 0 |\n| $2 \\ell+3$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $1 / 3$ | 0 |  | 0 | 0 |\n| $2 \\ell+4$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $1 / 4$ |  | 0 | 0 |\n| $\\vdots$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $\\ddots$ |  |  |\n| $3 \\ell+3$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $1 / 4$ | 0 |\n| $3 \\ell+4$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $*$ | $1 / 2$ |\n\nNow let $d_{V, 2}=2$ and\n\n$$\n\\begin{aligned}\n& U_{2}=\\left(e_{\\mathrm{PRE}_{2} \\mathrm{CARRY}}^{d} e_{\\mathrm{PRE}_{-} \\mathrm{EOS}^{d}}^{d}\\right) \\in \\mathbb{R}^{d \\times d d_{V, 2}{ }^{2}} . \\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-35.jpg?height=104&width=483&top_left_y=1864&top_left_x=756)\n\nThe linear combination with matrix $\\boldsymbol{U}_{2} \\boldsymbol{V}_{2}$ does two jobs at once. First, it takes the dimension NuM from the encoding matrix, scales it up by 4 , and puts it to the dimension PRE_CARRY. Second, it takes the dimension IS _ BOS from the encoding matrix, scales it up by 2, and puts it to the dimension PRE_EOS. A concrete example is provided in Table 20. Obtaining $\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{2}$, its every entry is zero except at the dimensions PRE_CARRY and PRE_EOS. Observe that $\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)}\\right]_{\\text {(PRE_CARRY) } 1}=0$, because in the input encoding matrix, the dimension NUM starts with 0 . Also, note again that it is enough to focus on the columns $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+4\\}$, since we only care about the next-token prediction of the tokens after $\\sigma_{2 \\ell+3}=^{\\prime}=$ '. Specifying the dimensions (i.e., the particular rows) for\nthese columns, we have\n\n$$\n\\begin{aligned}\n{\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{2}\\right]_{(\\mathrm{PRE}-\\mathrm{CARRY}) j} } & = \\begin{cases}\\frac{4}{3}\\left(\\boldsymbol{X}_{(\\mathrm{NUM})(\\ell+2)}^{(0)}+\\boldsymbol{X}_{(\\mathrm{NUM}) j}^{(0)}\\right) & \\text { if }(2 \\ell+3)=2 \\ell+3, \\\\\n\\boldsymbol{X}_{(\\mathrm{NUM})(3 \\ell+5-j)}^{(0)}+\\boldsymbol{X}_{(\\mathrm{NUM})(4 \\ell+6-j)}^{(0)}+\\boldsymbol{X}_{(\\mathrm{NUM}) j}^{(0)} & \\text { if } j \\in\\{2 \\ell+4, \\ldots, 3 \\ell+3\\}, \\\\\n0 & \\text { if } j=3 \\ell+4,\\end{cases} \\\\\n& = \\begin{cases}0 & \\text { if } j \\in\\{2 \\ell+3,3 \\ell+4\\}, \\\\\n\\sigma_{(3 \\ell+5)-j}+\\sigma_{(4 \\ell+6)-j}+\\sigma_{j} & \\text { if } j \\in\\{2 \\ell+4, \\ldots, 3 \\ell+3\\},\\end{cases} \\\\\n{\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{2}\\right]_{\\left(\\mathrm{PRE}_{-} \\mathrm{EOS}\\right) j} } & = \\begin{cases}2 / 3 & \\text { if } j=2 \\ell+3, \\\\\n1 / 2 & \\text { if } j \\in\\{2 \\ell+4, \\ldots, 3 \\ell+3\\}, \\\\\n1 & \\text { if } j=3 \\ell+4 .\\end{cases}\n\\end{aligned}\n$$\n\nRefer to Table 21 for a concrete example of computing $\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{2}$. Also, for the softmax errors,\n\n$$\n\\begin{aligned}\n{\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}\\right) j} } & =\\sum_{2 \\leq i \\leq j} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\\\\n{\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{(\\mathrm{PRE}-\\mathrm{EOS}) j} } & =\\sum_{1 \\leq i \\leq j} 2 \\boldsymbol{X}_{\\left(\\mathrm{IS} \\_{ }^{\\mathrm{BOS}) i}\\right.}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}\n\\end{aligned}\n$$\n\nLet us first obtain a bound of the softmax error term at dimension PRE_CARRY. If $j=2 \\ell+3$, since $\\boldsymbol{X}_{(\\mathrm{NUM})(\\ell+2)}^{(0)}=\\boldsymbol{X}_{(\\mathrm{NUM})(2 \\ell+3)}^{(0)}=0$,\n\n$$\n\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}\\right)(2 \\ell+3)}=\\sum_{\\substack{2 \\leq i \\leq 2 \\ell+2 \\\\ i \\neq \\ell+2}} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}\n$$\n\nand\n\n$$\n0 \\leq \\sum_{\\substack{2 \\leq i \\leq 2 \\ell+2 \\\\ i \\neq \\ell+2}} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{36(2 \\ell)}{3 e^{2 M}+2 \\ell}\n$$\n\nIf $j \\in\\{2 \\ell+4, \\ldots, 3 \\ell+3\\}$\n\n$$\n\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{(\\mathrm{PRE}-\\mathrm{CARRY}) j}=\\underbrace{\\sum_{i \\in\\{(3 \\ell+5)-j,(4 \\ell+6)-j, j\\}} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}}_{\\text {negative }}+\\underbrace{}_{\\substack{2 \\leq i \\leq j-1 \\\\ i \\neq(3 \\ell+5)-j \\\\ i \\neq(4 \\ell+6)-j}} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j},\n$$\n\nwhere\n\n$$\n0 \\leq-\\sum_{i \\in\\{(3 \\ell+5)-j,(4 \\ell+6)-j, j\\}} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{3 \\cdot 9(j-4)}{4 e^{2 M}+(j-4)}\n$$\n\nand\n\n$$\n0 \\leq \\sum_{\\substack{2 \\leq i \\leq j-1 \\\\ i \\neq(3 \\ell+5)-j \\\\ i \\neq(4 \\ell+6)-j}} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j} \\leq \\frac{36(j-4)}{4 e^{2 M}+(j-4)}\n$$\n\nAnd if $j=3 \\ell+4=N$,\n\n$$\n\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}\\right) N}=\\underbrace{4 \\boldsymbol{X}_{(\\mathrm{NUM}) N}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{N N}}_{\\text {negative }}+\\underbrace{\\sum_{2 \\leq i \\leq N-1} 4 \\boldsymbol{X}_{(\\text {NUM }) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i N}}_{\\text {positive }}\n$$\n\nwhere\n\n$$\n0 \\leq-4 \\boldsymbol{X}_{(\\mathrm{NUM}) N}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{N N} \\leq \\frac{18(N-2)}{2 e^{2 M}+N-2}\n$$\n\nand\n\n$$\n0 \\leq \\sum_{2 \\leq i \\leq N-1} 4 \\boldsymbol{X}_{(\\mathrm{NUM}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i N} \\leq \\frac{36(N-2)}{2 e^{2 M}+N-2}\n$$\n\nNext, we obtain a bound of the softmax error term at dimension PRE_EOS. Since\n\n$$\n\\sum_{1 \\leq i \\leq j} 2 \\boldsymbol{X}_{(\\mathrm{Is}-\\mathrm{BOS}) i}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{i j}=2 \\boldsymbol{X}_{(\\mathrm{Is}-\\mathrm{BOS}) 1}^{(0)}\\left[\\boldsymbol{R}_{1}\\right]_{1 j}\n$$\n\nthe error term can be bounded as\n\n$$\n0 \\leq-\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{(\\mathrm{PRE}-\\mathrm{EOS}) j} \\leq \\begin{cases}\\frac{2(j-3)}{3\\left(3 e^{2 M}+j-3\\right)} & \\text { if } j=2 \\ell+3 \\\\ \\frac{2(j-4)}{4\\left(4 e^{2 M}+j-4\\right)} & \\text { if } j \\in\\{2 \\ell+4, \\ldots, 3 \\ell+3\\} \\\\ \\frac{(j-2)}{2 e^{2 M}+j-2} & \\text { if } j=3 \\ell+4\\end{cases}\n$$\n\nWe then can ensure that both $\\left|\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{\\left(\\mathrm{PRE} \\_ \\text {sUm }\\right) j}\\right|$ and $\\left|\\left[\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{2}\\right]_{\\left.\\text {(PRE }{ }_{\\text {ERS }}\\right) j}\\right|$ smaller than 0.1 for each $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+4\\}$, by letting $M \\geq \\frac{1}{2} \\log (N)+3$. The proof is similar to the one that is presented for head 1 . Table 18: Example of $\\boldsymbol{Q}_{2} \\boldsymbol{X}^{(0)}$, continuing from Table 9. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{0}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 19: Example of $\\boldsymbol{K}_{2} \\boldsymbol{X}^{(0)}$, continuing from Table 9. |  | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | 0 | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{0}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 20: Example of $\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)}$, continuing from Table 9. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: PRE_SUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 24 | 20 | 12 | 0 | 0 | 16 | 36 | 0 | 8 | 0 | 28 | 0 |\n| 6: PRE_EOS | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7-16: SUM | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18-end: POS_1 | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ |\n\nTable 21: Example of $\\boldsymbol{U}_{2} \\boldsymbol{V}_{2} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{2}$, continuing from Table 20. See Table 17 for definition of $\\boldsymbol{T}_{2}$. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: PRE_SUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 12 | 10 | 6 | 0 | 8 | 12 | 16 | 0 | 14 | 9 | 13 | 0 |\n| 6: PRE_EOS | 2 | 1 | 1 | 1 | 1 | $2 / 3$ | $2 / 3$ | $2 / 3$ | $2 / 3$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n| 7-16: SUM | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18-end: POS_1 | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ |\n\n## D.4.3 Residual Connection\n\nSo far we have computed the output of $\\mathrm{Att}_{1}$ operation. Passing through the residual connection, the output of the attention layer is the sum of the original input encoding matrix and the output of Att operation:\n\n$$\n\\boldsymbol{Y}^{(1)}=\\boldsymbol{X}^{(0)}+\\sum_{h \\in\\{1,2\\}} \\boldsymbol{U}_{h} \\boldsymbol{V}_{h} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{h}+\\underbrace{\\sum_{h \\in\\{1,2\\}} \\boldsymbol{U}_{h} \\boldsymbol{V}_{h} \\boldsymbol{X}^{(0)} \\boldsymbol{R}_{h}}_{\\text {softmax error term }}\n$$\n\nSince the term $\\sum_{h \\in\\{1,2\\}} \\boldsymbol{U}_{h} \\boldsymbol{V}_{h} \\boldsymbol{X}^{(0)} \\boldsymbol{T}_{h}$ has nonzero entries only at dimensions PRE_SUM, PRE_CARRY, and PRE_EOS, the residual connection plays a role of \"filling in some blanks\" in the input encoding matrix. A concrete example of the output of residual connection is presented in Table 22, ignoring the softmax error term, whose entries have an absolute value smaller than 0.1 . Table 22: Example output of residual connection, continuing from Tables 9, 15 and 21. Here we ignore the softmax error terms in the orange rows. The gray rows will be filled in later. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 6 | 5 | 3 | 0 | 0 | 4 | 9 | 0 | 2 | 0 | 7 | 0 |\n| 2: IS_BOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 4: PRE_SUM | 0 | 0 | 9 | 7.5 | 4.5 | 0 | 6 | 9 | 12 | 9 | 6 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 12 | 10 | 6 | 0 | 8 | 12 | 16 | 0 | 14 | 9 | 13 | 0 |\n| 6: PRE_EOS | 2 | 1 | 1 | 1 | 1 | $2 / 3$ | $2 / 3$ | $2 / 3$ | $2 / 3$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n| 7-16:SUM | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18-(P+17):POS_1 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ |\n| $(P+18)-(2 P+17):$ POS_2 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ |\n\n## D. 5 Transformer Block - Token-wise Feed-forward Layer\n\nThe goal of the feed-forward layer is to fill in the blanks of the encoding matrix at dimensions SUM and IS_EOS. Be careful that the feed-forward layer can only implement token-wise mappings; a token-wise mapping takes inputs only from the entries in the same column of the encoding matrix. Besides, the architecture of our feed-forward layer (except for the residual connection) is a one-hidden-layer ReLU network. For a token $\\sigma_{i}$ for $i \\in\\{2 \\ell+3, \\ldots, 3 \\ell+3\\}$ (from ' $=$ ' token to the second), we will put a standard unit vector $\\boldsymbol{e}_{k+1}^{10}$ to dimensions SUM if the next token is $k \\in\\{0, \\ldots, 9\\}$. Recall from the discussion in Appendix D.4.2 that we can judge whether a carry 1 is generated at a certain position by exploiting only the digits (of the operands and the sum) in the same significance. Bringing the\nnotation, let $a$ and $b$ be digits of the operands in the same significance, and $c$ be a digit of the sum in the same significance as $a$ and $b$. Then the rule of recognizing that the addition of $a$ and $b$ generates a carry is that\n\n$$\n\\begin{cases}\\text { If } a+b-c \\in\\{9,10\\}, & \\text { then a carry is generated, } \\\\ \\text { Otherwise: if } a+b-c \\in\\{-1,0\\}, & \\text { then the carry is not generated. }\\end{cases}\n$$\n\nA simple case analysis shows that the value of $a+b-c$ must be one of $-1,0,9$, and 10 . Let us briefly check this claim in our example:\n\n$$\n\\begin{array}{lrl}\n6+0-7 & =-1 ; & \\text { no carry from } 6+0 \\\\\n5+4 & -0=9 ; & \\text { there is a carry from } 5+4 \\\\\n3+9 & -2=10 . & \\text { there is a carry from } 3+9\n\\end{array}\n$$\n\nRecall that a noisy version of $a+b+c$ is already stored at dimension PRE_CARRY of $\\boldsymbol{Y}^{(1)}$, and $c$ is exactly at dimension NUM. Thus, we can (approximately) implement $a+b-c$ for a token $\\sigma_{j}$ by\n\n$$\n\\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}\\right) j}^{(0)}-2 \\boldsymbol{Y}_{(\\mathrm{NUM}) j}^{(0)}\n$$\n\nThis is a kind of token-wise linear transform, so we do not need to consume any hidden layer (with ReLU activation $\\phi$ ) to implement it. Combining with $\\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{sUM}\\right) j}^{(0)}$, a noisy version of addition without carry, we can indeed implement the addition. Note that a digit-wise addition should be done as\n\n$$\n\\text { digit-wise addition }=\\left(\\text { addition without carry }+\\mathbb{1}_{\\{\\text {carry propagates }\\}}\\right) \\bmod 10\n$$\n\nWe first describe the formal construction of feed-forward network $\\mathrm{FF}_{1}$ for dimensions SUM and IS_EOS and then explain the intuition behind the construction. For the example result of applying the feed-forward network is presented in Table 23 . ## D.5.1 Subnetwork 1: Construction for SUM (dimension 7-16). Given a vector $\\boldsymbol{y}=\\left[\\boldsymbol{y}_{j}\\right]_{j=1}^{d} \\in \\mathbb{R}^{d}$, define a linear function $g: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ as\n\n$$\ng(\\boldsymbol{y}):=\\boldsymbol{y}_{\\mathrm{PRE}_{-} \\mathrm{SUM}}+\\frac{\\boldsymbol{y}_{\\mathrm{PRE}_{-} \\mathrm{CARRY}}-2 \\boldsymbol{y}_{\\mathrm{NUM}}}{10}+0.21=\\boldsymbol{y}_{3}+\\frac{\\boldsymbol{y}_{4}-2 \\boldsymbol{y}_{1}}{10}+0.21\n$$\n\nand consider a one-hidden-layer ReLU network $f_{k}: \\mathbb{R} \\rightarrow \\mathbb{R}(k=0,1, \\ldots, 9)$ defined as\n\n$$\n\\begin{aligned}\nf_{k}(x)=2[ & \\phi(x-(k-0.5))-\\phi(x-k)-\\phi(x-(k+0.5))+\\phi(x-(k+1)) \\\\\n& +\\phi(x-(k+9.5))-\\phi(x-(k+10))-\\phi(x-(k+10.5))+\\phi(x-(k+11))]\n\\end{aligned}\n$$\n\nThen we construct the first subnetwork of our feed-forward network by\n\n$$\n\\left[\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)\\right]_{(\\mathrm{sUM}) j}=\\left[\\begin{array}{lll}\nf_{0}\\left(g\\left(\\boldsymbol{Y}_{\\bullet j}^{(1)}\\right)\\right) & \\cdots & f_{9}\\left(g\\left(\\boldsymbol{Y}_{\\bullet j}^{(1)}\\right)\\right)\n\\end{array}\\right]^{\\top} \\cdot \\quad(j \\in[N])\n$$\n\nExplanation. The purpose of the first subnetwork is to generate a 10-dimensional one-hot vector whose position of 1 indicates the next digit: $\\boldsymbol{e}_{k}^{10}$ for the answer of next-token prediction ' $k$ '. There are two cases where we need to predict the next token as ' $k$ ':\n\n- Case 1: (Addition without carry) $=k \\bmod 10$ and no carry propagates. - Case 2: (Addition without carry) $=k-1 \\bmod 10$ and there is a propagating carry 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-40.jpg?height=961&width=1490&top_left_y=234&top_left_x=316)\n\nFigure 20: Example plots of $f_{k}(x)$ defined in Equation (59). $(k=0,1,2,3)$\n\nIn the first case, due to the softmax error (with magnitude at most 0.1 ),\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Y}_{\\left(\\text {PRE } \\_ \\text {SUM }\\right) j}^{(0)} \\in[k-0.1, k+0.1] \\cap[k+9.9, k+10.1], \\\\\n& \\boldsymbol{Y}_{(\\text {PRE_CARRY }) j}^{(0)}-2 \\boldsymbol{Y}_{(\\text {NUM }) j}^{(0)} \\in[-1.1,-0.9] \\cap[-0.1,0.1] \\subset[-1.1,0.1] . \\end{aligned}\n$$\n\nIn the second case, again due to the softmax error (with magnitude at most 0.1 ),\n\n$$\n\\begin{array}{r}\n\\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{SUM}\\right) j}^{(0)}+1 \\in[k-0.1, k+0.1] \\cap[k+9.9, k+10.1], \\\\\n\\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}\\right) j}^{(0)}-2 \\boldsymbol{Y}_{(\\mathrm{NUM}) j}^{(0)}-10 \\in[-1.1,-0.9] \\cap[-0.1,0.1] \\subset[-1.1,0.1] . \\end{array}\n$$\n\nIn both cases,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{SUM}\\right) j}^{(0)}+\\frac{\\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}\\right) j}^{(0)}-2 \\boldsymbol{Y}_{(\\mathrm{NUM}) j}^{(0)}+0.21}{10} \\in[k, k+0.32] \\cap[k+10, k+10.32] \\\\\n& \\subset[k, k+0.5] \\cap[k+10, k+10.5] . \\end{aligned}\n$$\n\nWe can map the column $\\boldsymbol{Y}_{\\bullet j}^{(0)}$ to the set $[k, k+0.5] \\cap[k+10, k+10.5]$ if the next token is $\\sigma_{j+1}=$ ' $k$ '. This job is done by the function $g$. Note that the resulting sets $[k, k+0.5] \\cap[k+10, k+10.5]$ are disjoint for different k's. Recall that our objective is to output 1 to the dimension $k+6$ (among the dimensions $7,8, \\ldots, 16$ in SUM) and to output 0 to the other dimensions in SUM if we need to predict ' $k$ ' as the next token. To this end, it is enough to map the set $[k, k+0.5] \\cap[k+10, k+10.5]$ to 1 and to map the other sets (for different $k$ 's) to 0 . This can be done by a ReLU network $f_{k}(x)$ is a ReLU network having two bumps at intervals $[k-0.5, k+1]$ and $[k+9.5, k+11]$. In particular, $f_{k}(x)=1$ if $x \\in[k, k+0.5] \\cup[k+10, k+10.5]$ : see Figure 20 for an illustration. Lastly, we have a desired one-hot vector output for each $j$ by taking a composition between $g$ and $\\left[f_{0}(\\cdot), \\ldots, f_{9}(\\cdot)\\right]^{\\top}$ as written in Equation (60). ## D.5.2 Subnetwork 2: Construction for IS_EOS (dimension 17). We move on to the dimension IS_EOS. For a token $\\sigma_{j}$ for $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+4\\}$, if $k$ is the next token, we will put $\\mathbb{1}_{\\{k=\\$\\}}$ to dimension IS_EOS: 1 if $k$ is the special token ' $\\$$ ' and 0 otherwise. To this end, we define a ReLU network $h: \\mathbb{R} \\rightarrow \\mathbb{R}$ as\n\n$$\nh(x)=10 \\phi(x-0.8)-10 \\phi(x-0.9)\n$$\n\nThen, we construct the second subnetwork of our feed-forward network by\n\n$$\n\\left[\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)\\right]_{\\left(\\mathrm{Is} \\_\\mathrm{EOS}\\right) j}=h\\left(\\boldsymbol{Y}_{\\left(\\mathrm{PRE}_{-} \\mathrm{EOS}^{2}\\right) j}^{(1)}\\right) \\cdot \\quad(j \\in[N])\n$$\n\nExplanation. Note that for columns $j \\in\\{2 \\ell+3, \\ldots, 3 \\ell+4\\}$, if we consider the presence of softmax errors with magnitude at most 0.1, the values that $\\boldsymbol{Y}_{\\left(\\mathrm{PRE} \\_\\mathrm{EOS}\\right) j}^{(1)}$ can have lie in the set $[0.4,0.6] \\cap[2 / 3-0.1,2 / 3+0.1] \\cap[0.9,1.1] \\subset$ $(-\\infty, 0.8) \\cap[0.9, \\infty)$. We want to output 1 if $\\boldsymbol{Y}_{\\text {(PRE_EOS }) j}^{(1)} \\geq 0.9$ and 0 otherwise: this can be done with the ReLU network $h$ with two neurons. ## Remark:\n\n- In total, we consume $8 \\times 10+2=82 \\mathrm{ReLU}$ neurons in our feed-forward network $\\mathrm{FF}_{1}$. However, it is possible to construct the addition Transformer with a smaller number of neurons, with a slight modification in the linear readout of the decoding function (Appendix D.6). - Unlike in the attention layer, now we do not have to worry about softmax errors in the output since the feed-forward ReLU network plays the role of denoiser. Table 23: Example output after applying the feed-forward network. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: PRE_SUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: PRE_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7-16: SUM | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{10}^{10}$ | $\\boldsymbol{e}_{8}^{10}$ | $\\boldsymbol{e}_{5}^{10}$ | $\\boldsymbol{e}_{2}^{10}$ | $\\boldsymbol{e}_{7}^{10}$ | $\\boldsymbol{e}_{10}^{10}$ | $\\boldsymbol{e}_{3}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{8}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ |\n| 17: IS_EOS | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n| 18-(P+17):POS_1 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ |\n| $(P+18)-(2 P+17):$ POS_2 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ |\n\n## D.5.3 Residual Connection\n\nThe last task of the feed-forward layer is to pass $\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)$ through the residual connection. As a result, we have\n\n$$\n\\boldsymbol{X}^{(1)}=\\boldsymbol{Y}^{(1)}+\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)\n$$\n\nA concrete example of the output of the second residual connection is showcased in Table 24.",
    "posicoupling-47": "Table 24: Example output of residual connection, continuing from Table 23. Here we ignore the softmax error terms in the orange rows. | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 6 | 5 | 3 | 0 | 0 | 4 | 9 | 0 | 2 | 0 | 7 | 0 |\n| 2: IS_BOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 4: PRE_SUM | 0 | 0 | 9 | 7.5 | 4.5 | 0 | 6 | 9 | 12 | 9 | 6 | 0 | 0 |\n| 5: PRE_CARRY | 0 | 12 | 10 | 6 | 0 | 8 | 12 | 16 | 0 | 14 | 9 | 13 | 0 |\n| 6: PRE_EOS | 2 | 1 | 1 | 1 | 1 | $2 / 3$ | $2 / 3$ | $2 / 3$ | $2 / 3$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n| 7-16:SUM | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{10}^{10}$ | $\\boldsymbol{e}_{8}^{10}$ | $\\boldsymbol{e}_{5}^{10}$ | $\\boldsymbol{e}_{2}^{10}$ | $\\boldsymbol{e}_{7}^{10}$ | $\\boldsymbol{e}_{10}^{10}$ | $\\boldsymbol{e}_{3}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{8}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ |\n| 17: IS_EOS | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n| 18-(P+17): POS_1 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ |\n| $(P+18)-(2 P+17):$ POS_2 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ |\n\n## D. 6 Decoding Function\n\nAs mentioned in Appendix C, the decoding function performs a linear readout (with a weight matrix $\\boldsymbol{W}_{\\text {out }} \\in$ $\\mathbb{R}^{|\\mathcal{V}| \\times d}$ ) and a (token-wise) arg-max operation. That is,\n\n$$\n\\operatorname{Dec}\\left(\\boldsymbol{X}^{(1)}\\right):=\\left(\\mathcal{V}_{k_{i}}\\right)_{i=1, \\ldots, N} \\in \\mathcal{V}^{N}\n$$\n\nwhere $\\mathcal{V}_{k}$ is the $k$-th element of $\\mathcal{V}$ and\n\n$$\nk_{i}:=\\underset{k \\in[|\\mathcal{V}|]}{\\arg \\max }\\left\\{o_{k}: \\boldsymbol{W}_{\\text {out }} \\boldsymbol{X}_{\\bullet i}^{(1)}=\\left[\\begin{array}{lll}\no_{1} & \\cdots & o_{|\\mathcal{V}|}\n\\end{array}\\right]^{\\top}\\right\\}\n$$\n\nThe objective of the decoding function is to perform a proper next-token prediction for addition, especially utilizing the dimensions SUM and IS_EOS of $\\boldsymbol{X}^{(1)}$. We now construct the weight matrix $\\boldsymbol{W}_{\\text {out }}$. For a token $\\sigma_{i}$, if the value of dimension IS _EOS of $\\boldsymbol{X}^{(1)}$ is 0 , then the linear readout output the dimensions SUM as it is to return one of a number token (0-9). On the other hand, if the value of dimension IS _EOS is 1 , then the linear readout outputs a large number (like 100 for example) for the token ' $\\$$ ' to return $\\operatorname{EOS}(\\$)$. This can be implemented by the weight matrix $\\boldsymbol{W}_{\\text {out }}$ described in Table 25. Also, an example of applying the linear transform is showcased in Table 26. Table 25: The transposed weight matrix $\\boldsymbol{W}_{\\text {out }}^{\\top}$ of the linear readout in decoding function. | $\\mathcal{V}$ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | + | $=$ | $\\$$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1-6: NUM-PRE_EOS | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ | $\\mathbf{0}_{6}$ |\n| 7: SUM $_{1}$ | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: $\\mathrm{SUM}_{2}$ | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9: SUM $_{3}$ | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10: SUM $_{4}$ | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11: $\\mathrm{SUM}_{5}$ | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12: SUM $_{6}$ | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13: SUM $_{7}$ | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14: SUM $_{8}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n| 15: SUM $_{9}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n| 16: | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n| 17: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 100 |\n| 18-end:POS_1, POS_2 | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ | $\\mathbf{0}_{2 P}$ |\n\nTable 26: Example output of linear readout $\\left(\\boldsymbol{W}_{\\text {out }} \\boldsymbol{X}^{(1)}\\right)$, continuing from Tables 24 and 25 . The yellow cells represent the maximum value of each column, from the $'=$ ' token's column to the rightmost column (which are used for next-token prediction). | $\\mathcal{I}$ | $\\$$ | 6 | 5 | 3 | + | 0 | 4 | 9 | $=$ | 2 | 0 | 7 | 0 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 |\n| 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n| 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n| + | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| $=$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| $\\$$ | 9 | 9 | 9 | 9 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 |\n\nTable 27: Example output sequence $\\mathcal{O}=\\operatorname{Dec}\\left(\\boldsymbol{X}^{(1)}\\right)$, continuing from Table 26. The yellow cells in the bottom row exactly predict the next tokens. | I | \\$ | 6 | 5 | 3 | + | 0 | 4 | 9 | = | 2 | 0 | 7 | 0 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $\\mathcal{O}$ | \\$ | \\$ | \\$ | \\$ | \\$ | 1 | 6 | 9 | 2 | 0 | 7 | 0 | \\$ |\n\n## E Impossibility of Addition with No Positional Encoding\n\nFor the sake of readability, we restate the proposition below.",
    "posicoupling-48": "Proposition 5.2. Consider any depth-1 finite-head decoder-only Transformer model $\\mathcal{T}$ without positional encoding (NoPE). Given an input sequence $\\mathcal{I}$ and its arbitrary permutation $\\mathcal{I}^{\\prime}$, if the last tokens of $\\mathcal{I}$ and $\\mathcal{I}^{\\prime}$ are identical, then the next tokens predicted by $\\mathcal{T}$ will also be identical for both sequences when applying a greedy decoding scheme.",
    "posicoupling-49": "Remark. We assume the 1-layer $(L=1) H$-head Transformer achitecture specified in Appendix C. Although it omits normalization layers, we remark that Proposition 5.2 remains valid even for the architecture with a standard layer normalization (Ba et al., 2016) or its variants (e.g., Zhang and Sennrich, 2019).",
    "posicoupling-50": "Proof. We keep following the notation about matrices introduced in Appendix D.1. Throughout the proof, we denote the value/vector/matrix related to $\\mathcal{I}^{\\prime}$ by appending ${ }^{\\prime \\prime}$ ' to it. Let encoding matrices generated from the input sequences $\\mathcal{I}, \\mathcal{I}^{\\prime} \\in \\mathcal{V}^{N}$ as\n\n$$\n\\boldsymbol{X}:=\\operatorname{Enc}(\\mathcal{I}) \\in \\mathbb{R}^{d \\times N} \\quad \\text { and } \\quad \\boldsymbol{X}^{\\prime}:=\\operatorname{Enc}\\left(\\mathcal{I}^{\\prime}\\right) \\in \\mathbb{R}^{d \\times N}\n$$\n\nSince there is no positional encoding, the encoding function $\\operatorname{Enc}(\\cdot)$ maps the same tokens to the same columns.",
    "posicoupling-51": "In particular, $\\mathcal{I}_{i}=\\mathcal{I}_{j}^{\\prime}$ implies $\\boldsymbol{X}_{\\bullet i}=\\boldsymbol{X}_{\\bullet j}^{\\prime}$. Since we assume that $\\mathcal{I}^{\\prime}$ is a permutation of $\\mathcal{I}$ such that $\\mathcal{I}_{N}=\\mathcal{I}_{N}^{\\prime}$, there exists a bijection $\\pi:[N] \\rightarrow[N]$ such that $\\mathcal{I}_{i}^{\\prime}=\\mathcal{I}_{\\pi(i)}$ for each $i \\in[N]$ and $\\pi(N)=N$. Then, it follows that $\\boldsymbol{X}_{\\bullet i}^{\\prime}=\\boldsymbol{X}_{\\bullet(\\pi(i))}$ for each $i$ and, specifically, $\\boldsymbol{X}_{\\bullet N}^{\\prime}=\\boldsymbol{X}_{\\bullet N}$\nRecall that the single $H$-head attention layer Att : $\\mathbb{R}^{d \\times N} \\rightarrow \\mathbb{R}^{d \\times N}$ operates as $\\operatorname{Att}(\\boldsymbol{X})=\\sum_{h=1}^{H} \\operatorname{Head}_{h}(\\boldsymbol{X})$ where the attention head $h$ is defined as\n\n$$\n\\operatorname{Head}_{h}(\\boldsymbol{X}):=\\boldsymbol{U}_{h} \\boldsymbol{V}_{h} \\boldsymbol{X} \\cdot \\operatorname{softmax}\\left(\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}\\right) \\in \\mathbb{R}^{d \\times N}\n$$\n\nwhere $\\boldsymbol{Q}_{h}, \\boldsymbol{K}_{h} \\in \\mathbb{R}^{d_{Q K} \\times d}, \\boldsymbol{V}_{h} \\in \\mathbb{R}^{d_{V} \\times d}$ and $\\boldsymbol{U}_{h} \\in \\mathbb{R}^{d \\times d_{V}}$. Claim: $\\left[\\operatorname{Head}_{h}(\\boldsymbol{X})\\right]_{\\bullet N}=\\left[\\operatorname{Head}_{h}\\left(\\boldsymbol{X}^{\\prime}\\right)\\right]_{\\bullet N}$ for all $h \\in[H]$. The claim suffices to prove the proposition because of the following: first, the claim implies that the last ( $N$-th) column of the attention layer outputs are the same, i.e., $[\\operatorname{Att}(\\boldsymbol{X})]_{\\bullet N}=\\left[\\operatorname{Att}\\left(\\boldsymbol{X}^{\\prime}\\right)\\right]_{\\bullet N}$. Note that the operations after the attention layer-residual connections, FF, and Dec-all operate in a token-wise (column-by-column) manner: the $j$-th column of the output of a token-wise operation is a function of $j$-th column of the input for the operation. Therefore, the last column of the attention layer output totally determines the next-token prediction at $N$-th input token. As a result, the predicted next-tokens are the same for $\\mathcal{I}$ and $\\mathcal{I}^{\\prime}$.",
    "posicoupling-52": "The rest of the proof is devoted to prove the aforementioned claim. Fix any $h \\in[H]$. Let\n\n$$\n\\begin{gathered}\n{\\left[\\operatorname{softmax}\\left(\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}\\right)\\right]_{\\bullet N}=\\left[\\begin{array}{lll}\ns_{1} & \\ldots & s_{N}\n\\end{array}\\right]^{\\top}} \\\\\n{\\left[\\operatorname{softmax}\\left(\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}^{\\prime}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}^{\\prime}\\right)\\right]_{\\bullet N}=\\left[\\begin{array}{lll}\ns_{1}^{\\prime} & \\ldots & s_{N}^{\\prime}\n\\end{array}\\right]^{\\top}}\n\\end{gathered}\n$$\n\nwhich are both stochastic (sum to 1) column vectors. Considering that we are taking the last column of the softmax output, it follows that $s_{i}^{\\prime}=s_{\\pi(i)}$ for each $i \\in[N]$ : this can be proved by applying the definition of the softmax operation and the fact\n\n$$\n\\left[\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}^{\\prime}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}^{\\prime}\\right]_{i N}=\\boldsymbol{X}_{\\bullet i}^{\\prime \\top} \\boldsymbol{K}_{h}^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}_{\\bullet N}^{\\prime}=\\boldsymbol{X}_{\\bullet \\pi(i)}^{\\top} \\boldsymbol{K}_{h}^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}_{\\bullet N}=\\left[\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}\\right]_{(\\pi(i)) N}\n$$\n\nConsequently, since\n\n$$\n\\sum_{i=1}^{N} s_{i}^{\\prime} \\boldsymbol{X}_{\\bullet i}^{\\prime}=\\sum_{i=1}^{N} s_{\\pi(i)} \\boldsymbol{X}_{\\bullet(\\pi(i))}=\\sum_{i=1}^{N} s_{i} \\boldsymbol{X}_{\\bullet}\n$$\n\nwe have\n\n$$\n\\boldsymbol{X}^{\\prime} \\cdot\\left[\\operatorname{softmax}\\left(\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}^{\\prime}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}^{\\prime}\\right)\\right]_{\\bullet N}=\\boldsymbol{X} \\cdot\\left[\\operatorname{softmax}\\left(\\left(\\boldsymbol{K}_{h} \\boldsymbol{X}\\right)^{\\top} \\boldsymbol{Q}_{h} \\boldsymbol{X}\\right)\\right]_{\\bullet N}\n$$\n\nTherefore, the claim holds. This concludes the proof. Here, we provide the Python code that calculates the maximum possible exact-match accuracy that a 1-layer Transformer with NoPE can achieve for the $m$-digit addition problem. ```\nfrom itertools import product\nfrom collections import defaultdict\nm = 4 # Change here\ntotal = 0\ncounter_dict = defaultdict(dict)\nfor a, b in product(product(range(10), repeat=m), product(range(10), repeat=m)):\n    if a[0] == 0 or b[0] == 0: continue\n    total += 1\n    c = tuple(sorted(a+b))\n    a_num = int(','join(map(str, a)))\n    b_num = int(,,.join(map(str, b)))\n    ab_sum = a_num + b_num\n    if ab_sum in counter_dict[c]:\n        counter_dict[c][ab_sum] += 1\n    else:\n        counter_dict[c][ab_sum] = 1\ncount = sum(max(d.values()) for _, d in counter_dict.items())\nprint(\"m =\", m)\nprint(\"Permutation Invariant Additions Count:\", count)\nprint(\" Total m-digit Additions Count:\", total)\nprint(\" Ratio:\", count / total)\n\" \"\"\n[Example Outputs]\nm = 1\nPermutation Invariant Additions Count: 81\n    Total m-digit Additions Count: 81\n                                Ratio: 1.0\nm}=\nPermutation Invariant Additions Count: 2668\n    Total m-digit Additions Count: 8100\n                                Ratio: 0.32938271604938274\nm = 3\nPermutation Invariant Additions Count: 50150\n    Total m-digit Additions Count: 810000\n                                Ratio: 0.06191358024691358\nm}=\nPermutation Invariant Additions Count: 765139\n    Total m-digit Additions Count: 81000000\n                                Ratio: 0.00944616049382716\nm = 5\nPermutation Invariant Additions Count: 10033314\n    Total m-digit Additions Count: 8100000000\n                                    Ratio: 0.0012386807407407407\n\" \" \"\n```\n\n\n## F (Formal) Construction of $N \\times 2$ Multiplication Transformer with Position Coupling\n\nHere we show how to implement the $N \\times 2$ multiplication using a depth- 2 decoder-only Transformer equipped with position coupling. Our construction involves 3 heads in the first Transformer block and 7 heads in the second Transformer block, requiring a total of 10 heads. Theorem 6.1. Given an appropriate format of the input sequence, there exists a depth-2 decoder-only Transformer model with coupled positions that can perform the $N \\times 2$ multiplication task with next-token prediction. Here, the number of the total heads is 10 and the length of the first operand is at most $2^{\\lfloor(d-34) / 6\\rfloor}-3$, where we denote the token embedding dimension by $d \\geq 46$. We note that our construction for the $N \\times 2$ multiplication task permits the use of multiple FFN layers at the second decoder block. However, we believe that there exists a potential improvement in our construction, wherein a single FFN layer could suffice for each decoder block, leveraging the expressivity of the neural network. Additionally, we do not provide a detailed error analysis but assume that the softmax operation with sufficiently large attention weights can reduce small attention scores to zero values, thereby clearly revealing the desired attention patterns.",
    "posicoupling-53": "## F. 1 Notation\n\nConsider an ordered vocabulary $\\mathcal{V}=(0,1,2,3,4,5,6,7,8,9, \\times,=, \\$)$. We include a special token ' $\\$$ ' that plays the role of both the beginning-of-sequence (BOS) token and the end-of-sequence (EOS) token.",
    "posicoupling-54": "We denote $\\mathcal{V}_{k}$ as $k$-th element of $\\mathcal{V}$. For instance, $\\mathcal{V}_{4}=3$ and $\\mathcal{V}_{13}=\\$$. Unlike the addition task, our construction for the multiplication involves multiple layers and hence we do not omit the superscripts ( $l$ ) in the parameter matrices/vectors and the size of dimensions. ## F. 2 Input Sequence\n\nOur objective is to use next-token prediction for implementing $a \\times b=c$. To this end, we want to transform it into an input sequence $\\mathcal{I}=\\overline{\\$ A \\times B=C}$ of an appropriate format. Let $\\ell_{a}$ and $\\ell_{b}$ represent the lengths of $a$ and $b$, respectively, and we denote their sum as $\\ell=\\ell_{a}+\\ell_{b}$. While our immediate focus is on the case where $\\ell_{b}=2$, it is worth noting that our approach can be extended to the case where $\\ell_{b}>2$, as the key insight for the construction does not rely on $\\ell_{b}$. Thus, we present the input sequence and encoding function in a more general form applicable to $\\ell_{b} \\geq 2$. Unlike the addition case, we do not zero-pad both $a$ and $b$. Instead, we only zero-pad the response, as the length of $c$ may either equal the sum of the lengths of $a$ and $b$, or be less than the sum of their lengths by 1 . Hence, we zero-pad in front of $c$ for the latter case to fix the length of $c$ by $\\ell$. We also reverse the response $c$ to make the part $C$. For instance, if we have $312 \\times 24=7488$, the input sequence transforms to $\\overline{\\$ 312 \\times 24=88470}$. If we have $589 \\times 62=36518$, then the input sequence would be $\\overline{\\$ 589 \\times 62=81563}$. The red digit is a zero-padding, and the blue digits are the reversed product. To recap, the input sequence $\\mathcal{I}=\\overline{\\sigma_{1} \\sigma_{2} \\ldots \\sigma_{N}} \\in \\mathcal{V}^{N}$ of length $N=2 \\ell+3$ consists of six parts:\n\n1. the BOS token $\\sigma_{1}=$ ' $\\$$ '\n2. the first operand $A=\\overline{\\sigma_{2} \\ldots \\sigma_{\\ell_{a}+1}}$ where $\\sigma_{i} \\in\\{0, \\ldots, 9\\}$\n3. the multiplication symbol $\\sigma_{\\ell_{a}+2}=' \\times$ ';\n4. the second operand $B=\\overline{\\sigma_{\\ell_{a}+3} \\cdots \\sigma_{\\ell+2}}$ (note that $\\ell=\\ell_{a}+\\ell_{b}$ ) where $\\sigma_{i} \\in\\{0, \\ldots, 9\\}$;\n5. the equality symbol $\\sigma_{\\ell+3}=$ ' $=$ ';\n6. the (reversed) product $C=\\overline{\\sigma_{\\ell+4} \\cdots \\sigma_{2 \\ell+3}}$ where $\\sigma_{i} \\in\\{0, \\ldots, 9\\}$. Note that the part $C$ might be incomplete (i.e., $N<2 \\ell+3$ ) at the inference time; we infer the digits of the part $C$ one by one using next-token prediction. Throughout this section on a formal construction, however, we only consider the train time setup in which we infer all the digits of the part $C$ at once using simultaneous next-token prediction in a single forward pass. Precisely, we want to use an input sequence $\\mathcal{I}=\\overline{\\sigma_{1} \\ldots \\sigma_{N}}$ to produce an output sequence $\\mathcal{O}=\\overline{\\sigma_{1}^{\\prime} \\ldots \\sigma_{N}^{\\prime}}$ where $\\overline{\\sigma_{\\ell+3}^{\\prime} \\ldots \\sigma_{N-1}^{\\prime}}=C=\\overline{\\sigma_{\\ell+4} \\ldots \\sigma_{N}}$ and $\\sigma_{N}^{\\prime}=$ ' $\\$$ ' (EOS). ## F. 3 Encoding Function\n\nWe now explain the input embedding for given an input sequence $\\mathcal{I}$ designed as above. The embedding matrix $\\boldsymbol{X}^{(0)}$ is of size $d \\times N$ : each column represents an embedding vector for a token, while each row represents a particular named dimension. We concatenate the token embedding and the position embedding, which can be viewed as a sum of two different embedding matrices of the same size. Table 28: Example initial encoding. Here we consider the input sequence $\\overline{\\$ 7595 \\times 79=500006}$ and the starting position ID is chosen as $s=1$. The vectors $\\boldsymbol{v}_{\\square}^{P}$ are defined in Equation (79). The gray rows will be filled in later. | I | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n| 2: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 3: is_bOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9: OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10: OP1_SHIFT0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11: OP1_SHIFT 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12: OP1_SHIFT2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13: OP1_SHIFT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14: OP1_SHIFT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15: ReSult1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 16: RESult2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 17: RESULT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18: RESULT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 19: PRE_PROD | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 20: PRE_CARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 21: PRE_EOS1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 22: PRE_EOS2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 23-32: PROD | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 33: IS _EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 35-(P+34): POS_2_MASK | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $(P+35)-(2 P+34):$ POS $\\_1$ | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $v_{2}^{P}$ | $\\boldsymbol{v}_{1}^{P}$ |\n| $(2 P+35)-(3 P+34):$ POS_ 2 | $\\mathbf{0}_{P}$ | $v_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{6}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $v_{5}^{P}$ | $v_{4}^{P}$ | $v_{3}^{P}$ | $v_{2}^{P}$ |\n| $(3 P+35)-(4 P+34):$ POS_3 | $\\mathbf{0}_{P}$ | $v_{P}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ |\n| $(4 P+35)-(5 P+34):$ POS_4 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS_5 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{11}^{P}$ | $v_{9}^{P}$ | $v_{10}^{P}$ | $\\boldsymbol{v}_{11}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ |\n\n## F.3.1 Token Embedding\n\nThe token embedding consists of $(34+P)$ dimensions, where $P$ represents the dimension for the position embedding which will be described in the very next section. While the token embedding dimension for the addition task was independent of $P$, our construction strategy for the multiplication task involves copying the position embedding into the token embedding. This is why we have the $P$ term in our token embedding dimension. For the first 34 dimensions, we label them as:\n\n$$\n\\begin{gathered}\n1=\\mathrm{NUM}, 2=\\mathrm{FULL} \\_\\mathrm{ONES}, 3=\\mathrm{IS} \\_ \\text {BOS }, 4=\\mathrm{IS} \\_ \\text {MUL }, 5=\\mathrm{IS} \\_ \\text {EQUAL, } \\\\\n6=\\mathrm{IS} \\_\\mathrm{OP} 2 \\_\\mathrm{ONE}, 7=\\mathrm{IS} \\_\\mathrm{OP} 2 \\_\\mathrm{TEN}, 8=\\mathrm{OP} 2 \\_\\mathrm{ONE}, 9=\\mathrm{OP} 2 \\_\\mathrm{TEN}, \\\\\n10=\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 0,11=\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 1,12=\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 2,13=\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 3,14=\\mathrm{OP} 1 \\_ \\text {SHIFT } 4, \\\\\n15=\\mathrm{RESULT} 1,16=\\text { RESULT } 2,17=\\text { RESULT } 3,18=\\mathrm{RESULT} 4 \\\\\n19=\\text { PRE } \\_ \\text {PROD }, 20=\\text { PRE } \\_ \\text {CARRY, } 21=\\text { PRE } \\_ \\text {EOS } 1,22=\\text { PRE } \\_ \\text {EOS } 2 \\\\\n\\{23, \\ldots, 32\\}=\\text { PROD }, 33=\\mathrm{IS} \\_ \\text {EOS }, 34=\\text { MASK }\n\\end{gathered}\n$$\n\nand for the last $P$ dimensions $(\\{35, \\ldots, 34+P\\})$, we named them as POS_2_MASK. The initial token embedding fills only NUM, FULL_ONES, IS_BOS, IS_MUL, and IS_EQUAL, leaving the other $(29+P)$ dimensions empty (i.e., all zeros).",
    "posicoupling-55": "These $(29+P)$ dimensions will be filled by passing through the layers. Here we describe how we fill the first 5 dimensions. Dimension 1 (NUM). For a number token $(0, \\ldots, 9)$, we put itself into the dimension NUM. For the other tokens $(\\times,=, \\$)$, we put 0 . Dimension 2 (FULl_ones). We put 1 everywhere in this dimension. Dimension 3 (IS_BOS). For a token ' $\\$$ ', we put 1 into the dimension IS_BOS. Otherwise, we put 0. Dimension 4 (IS_MUL). For a token ' $\\times$ ', we put 1 into the dimension IS_MUL. Otherwise, we put 0. Dimension 5 (Is_EQUAL).",
    "posicoupling-56": "For a token ' $=$ ', we put 1 into the dimension IS_EQUAL. Otherwise, we put 0 . ## F.3.2 Coupled Position IDs and Position Embedding\n\nWe now specify the allocation of coupled position IDs for the $N \\times M$ multiplication task as the following: given an input sequence $\\mathcal{I}=\\overline{\\sigma_{1} \\ldots \\sigma_{N}}$,\n\n$$\np(i)= \\begin{cases}0, & i=1 \\\\ s+i-2+\\ell_{b}, & i=2, \\ldots, \\ell_{a}+2 \\\\ s+i-3, & i=\\ell_{a}+3, \\ldots, \\ell+3 \\\\ s-i+3+2 \\ell & i=\\ell+4, \\ldots, 2 \\ell+3\\end{cases}\n$$\n\nCompared to the addition case, the position allocating function $p$ becomes more complicated since the length of two operands can be different, but the core remains simple: coupling the position IDs for the least significant digit in the first operand $(A)$, the second operand $(B)$, and the result $(C)$, and then decreasing the IDs as the digit position increases for each $A, B$, and $C$. Now we explain the position embedding. We utilize the same $\\boldsymbol{v}_{k}^{D}\\left(k \\in\\left[2^{D}\\right]\\right)$ defined for the addition task, specifically\n\n$$\n\\boldsymbol{v}_{k}^{D}=\\left[(-1)^{b_{i}^{(D, k)}}\\right]_{i=1}^{D} \\in \\mathbb{R}^{D}\n$$\n\nwhere $b_{i}^{(D, k)}$ is defined as the $i$-th (from left) digit of $D$-digit binary representation of $k-1$. Using $\\boldsymbol{v}_{k}^{D}$, we design the position embedding for each position ID $p(i)$ by\n\n$$\n\\left[\\begin{array}{c}\n\\boldsymbol{v}_{p(i)}^{P} \\\\\n\\boldsymbol{v}_{p(i)+1}^{P} \\\\\n\\boldsymbol{v}_{p(i)+2}^{P} \\\\\n\\boldsymbol{v}_{p(i)+3}^{P} \\\\\n\\boldsymbol{v}_{p(i)+4}^{P}\n\\end{array}\\right]\n$$\n\nThe first $P$ dimensions of the position embedding are named as POS_1, and subsequent sets of $P$ dimensions are named as $\\mathrm{POS} \\_2, \\mathrm{POS} \\_3, \\mathrm{POS} \\_4$, and $\\mathrm{POS} \\_5$, respectively. Thus, the position embedding is a $5 P$-dimensional vector. In case of $p(i)+j(j \\in[4])$ exceeding $2^{P}$, we use $\\boldsymbol{v}_{p(i)+j-2^{P}}^{P}$ instead of $\\boldsymbol{v}_{p(i)+j}^{P}$. If $p(i)=0$, we let $\\mathbf{0}_{5 P}$ as a position embedding vector. By concatenating the token embedding and the position embedding, we get the input embedding $\\boldsymbol{X}^{(0)}$. Specifically, the position embedding is placed under the token embedding $((P+35)$-th to $(6 P+34)$-th dimension $)$. See Table 9 for an example. As a result, the total embedding dimension is $d=6 P+34$. Note the maximum possible position ID that can be represented with $\\boldsymbol{v}_{k}^{P}$ 's is max_pos $=2^{P}=2^{\\lfloor(d-34) / 6\\rfloor}$. Therefore, the length of the first operand must be $\\ell_{a} \\leq$ max_pos $-\\ell_{b}-1=2^{\\lfloor(d-34) / 6\\rfloor}-\\ell_{b}-1$. For the case when $\\ell_{b}=2$, this inequality becomes $\\ell_{a} \\leq 2^{\\lfloor(d-34) / 6\\rfloor}-3$\n\n## F. 4 Construction Idea\n\nHere, we provide an example that demonstrates how we construct the $N \\times 2$ multiplication. Consider the calculation $7595 \\times 79=600005$. While a typical method for computing such a multiplication is illustrated in Table 29, we consider an alternative approach, as shown in Table 30. In this method, we pair the digits from the first and second operands at each step where the sum of their digit positions is the same, and then calculate the sum of the pairwise products. For example, the number 116 in Table 30 is generated by $9 \\times 9+5 \\times 7$, and the number 108 is generated by $5 \\times 9+9 \\times 7$, where blue indicates numbers from the first operand and red indicates numbers from the second operand. The main reason for considering such a method is to provide a clearer intuition for determining which numbers from each operand we should attend to when predicting the next token. Table 30: Multiplication II\nTable 29: Multiplication I\n\n|  |  | 7 | 5 | 9 | 5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| $\\times$ |  |  |  | 7 | 9 |\n|  | 6 | 8 | 3 | 5 | 5 |\n| 5 | 3 | 1 | 6 | 5 |  |\n| 6 | 0 | 0 | 0 | 0 | 5 |\n\n\n|  |  | 7 | 5 | 9 | 5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| $\\times$ |  |  |  | 7 | 9 |\n|  |  |  |  | 4 | 5 |\n|  |  | 1 | 1 | 6 |  |\n|  | 1 | 0 | 8 |  |  |\n|  | 9 | 8 |  |  |  |\n| 4 | 9 |  |  |  |  |\n| 6 | 0 | 0 | 0 | 0 | 5 |\n\nSuppose the current input sequence is $\\$ 7595 \\times 79=5000$. During this step, the model is tasked with predicting 0 (the 0 just before 6) for the next token. As illustrated in Table 30, this 0 is computed from the sum of $9,9,1$, and an additional 1 , representing the carry from the previous step. Similar to the explanation in D.4.2, we highlight that the carry 1 can be detected by computing 8 (ones digit of 98$)+0($ tens digit of 108) + 1 (hundreds digit of 116$)-0$ (current token): yielding a result of 9 , indicating the occurrence of a carry 1 . In summary, the correct prediction of the next token 0 (the 0 just before 6) can be achieved by summing the main summation part and the carry part, where the main summation part is computed using 49, 98, 108, and the carry part is calculated using 98,108 , and 116. Additionally, it's noteworthy to detail the breakdowns:\n\n$$\n49=0 \\times 9+7 \\times 7, \\quad 98=7 \\times 9+5+7, \\quad 108=5 \\times 9+9+7, \\quad 116=9 \\times 9+5+7\n$$\n\nThus, for predicting the next token, we need $0,7,5,9,5,9,7$. Here, we highlight that this structure, requiring 5 consecutive tokens from the first operand and every token from the second operand for the next-token prediction, remains unchanged for any prediction time and any query length. As we will see in the later subsection, a depth-2 decoder-only Transformer model can be constructed to fill OP $2 \\_$ONE by 9 , OP $2 \\_$TEN by 7 , and OP $1 \\_$SHIFT0 to OP 1 _SHIFT 4 by $0,7,5,9$, and 5 , respectively. One may be concerned that 0 is not given in the first operand at the input sequence. This requirement of 0 beyond the most significant digit arises in the later stage of the prediction, i.e., predicting the token that is near the most significant digit of the response. Although 0 is not explicitly given in the first operand, our construction can automatically manage as if the 0 were originally at the start of the first operand. A similar situation occurs in the early stage of the prediction that 0 is required before the least significant digit of the first operand, and our construction is also capable of handling this issue. Consequently, the embedding vector of the current token 0 (the 0 preceding 60 ) will be structured as the left-most table in Table 31, with some irrelevant dimensions omitted for readability. We then utilize a feed-forward layer to fill\n\n- RESULT1 with OP1_SHIFT0 \u00d7 OP2_ONE + OP1_SHIFT1 \u00d7 OP2_TEN,\n- RESULT 2 with $\\mathrm{OP} 1 \\_$SHIFT1 \u00d7 OP2_ONE + OP1_SHIFT $2 \\times \\mathrm{OP} 2 \\_$TEN,\n- RESULT 3 with OP1_SHIFT2 \u00d7 OP2_ONE + OP1_SHIFT3 \u00d7 OP2_TEN,\n- RESULT4 with OP1_SHIFT3 \u00d7 OP2_ONE + OP1_SHIFT4 \u00d7 OP2_TEN. The result is illustrated in the center table of Table 31. Next, we employ an additional feed-forward layer to fill\n\n- PRE_PROD with ones digit of RESULT1 + tens digit of RESULT2 + hundreds digit of RESULT3,\n- PRE_CARRY with ones digit of RESULT2 + tens digit of RESULT3 + hundreds digit of RESULT4. These computations yield the result illustrated in the right-most table of Table 31. Once this process is done, we can finally predict the next token by the following two steps:\n\n- CARRY $= \\begin{cases}0, & \\text { if PRE_CARRY - NUM } \\in\\{-2,-1,0\\}, \\\\ 1, & \\text { if PRE_CARRY - NUM } \\in\\{8,9,10\\}, \\\\ 2, & \\text { if PRE_CARRY - NUM } \\in\\{18,19,20\\},\\end{cases}$\n- NEXT_TOKEN $=$ PRE_PROD + CARRY $(\\bmod 10)$. Table 31: Illustration of the construction idea. | $\\mathcal{I}$ | 0 |\n| :--- | :---: |\n| 1: NUM | 0 |\n| 2: FULL_ONES | 1 |\n| 3: IS_BOS | 0 |\n| 4: IS_MUL | 0 |\n| 5: IS_EQUAL | 0 |\n| 8: OP2_ONE | 9 |\n| 9: OP2_TEN | 7 |\n| 10: OP1_SHIFT0 | 0 |\n| 11: OP1_SHIFT1 | 7 |\n| 12: OP1_SHIFT2 | 5 |\n| 13: OP1_SHIFT3 | 9 |\n| 14: OP1_SHIFT4 | 5 |\n| 15: RESULT1 | 0 |\n| 16: RESULT2 | 0 |\n| 17: RESULT3 | 0 |\n| 18: RESULT4 | 0 |\n| 19: PRE_PROD | 0 |\n| 20: PRE_CARRY | 0 |\n| $(P+35)-(2 P+34):$ POS_1 | $\\boldsymbol{v}_{3}^{P}$ |\n| (2P+35)-(3P+34): POS_2 | $\\boldsymbol{v}_{4}^{P}$ |\n| (3P+35)-(4P+34): POS_3 | $\\boldsymbol{v}_{5}^{P}$ |\n| (4P+35)-(5P+34): POS_4 | $\\boldsymbol{v}_{6}^{P}$ |\n| (5P+35)-(6P+34): POS_5 | $\\boldsymbol{v}_{7}^{P}$ |\n\n\n| I | 0 |\n| :--- | :---: |\n| 1: NUM | 0 |\n| 2: FULL_ONES | 1 |\n| 3: IS_BOS | 0 |\n| 4: IS_MUL | 0 |\n| 5: IS_EQUAL | 0 |\n| 8: OP2_ONE | 9 |\n| 9: OP2_TEN | 7 |\n| 10: OP1_SHIFT0 | 7 |\n| 11: OP1_SHIFT1 | 5 |\n| 12: OP1_SHIFT2 | 9 |\n| 13: OP1_SHIFT3 | 5 |\n| 14: OP1_SHIFT4 | 99 |\n| 15: RESULT1 | 98 |\n| 16: RESULT2 | 108 |\n| 17: RESULT3 | 0 |\n| 18: RESULT4 | 0 |\n| 19: PRE_PROD | $\\boldsymbol{v}_{3}^{P}$ |\n| 20: PRE_CARRY | $\\boldsymbol{v}_{4}^{P}$ |\n| ( $P+35)-(2 P+34):$ POS_1 |  |\n| (2P+35)-(3P+34): POS_2 |  |\n| (3P+35)-(4P+34): POS_3 | $\\boldsymbol{v}_{5}^{P}$ |\n| (4P+35)-(5P+34): POS_4 | $\\boldsymbol{v}_{6}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS_5 | $\\boldsymbol{v}_{7}^{P}$ |\n\n\n| $\\mathcal{I}$ | 0 |\n| :---: | :---: |\n| 1: NUM | 0 |\n| 2: FULL_ONES | 1 |\n| 3: is_bOS | 0 |\n| 4: IS_MUL | 0 |\n| 5: Is_EQUAL | 0 |\n| 8: OP2_ONE | 9 |\n| 9: OP2_TEN | 7 |\n| 10: OP1_SHIFT0 | 0 |\n| 11: OP1_SHIFT1 | 7 |\n| 12: OP1_SHIFT2 | 5 |\n| 13: OP1_SHIFT3 | 9 |\n| 14: OP1_SHIFT4 | 5 |\n| 15: RESULT1 | 49 |\n| 16: RESUlt2 | 98 |\n| 17: RESULT3 | 108 |\n| 18: RESULT4 | 116 |\n| 19: PRE_PROD | 19 |\n| 20: PRE_CARRY | 9 |\n| $(P+35)-(2 P+34):$ POS _ 1 | $\\boldsymbol{v}_{3}^{P}$ |\n| $(2 P+35)-(3 P+34):$ POS_2 | $\\boldsymbol{v}_{4}^{P}$ |\n| $(3 P+35)-(4 P+34):$ POS_3 | $\\boldsymbol{v}_{5}^{P}$ |\n| $(4 P+35)-(5 P+34):$ POS_4 | $\\boldsymbol{v}_{6}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS_ 5 | $\\boldsymbol{v}_{7}^{P}$ |\n\n## F. 5 Transformer Block 1 - Causal Attention Layer\n\nTo implement the concept introduced in Appendix F.4, it is essential to design a Transformer block capable of generating an embedding matrix depicted in the left-most table of Table 31. The goal of the first Transformer block is to fill IS_OP2 _ONE ( 6 -th dimension) and IS_OP 2 _TEN ( 7 -th dimension) by 1 if the token corresponds to the ones or tens digit of the second operand, respectively, and 0 otherwise. These two dimensions enable the filling of OP2_ONE (8-th dimension) and OP2_TEN (9-th dimension) at the second Transformer block. Furthermore, we will fill MASK ( 34 -th dimension) in the first block, which will serve as a base for filling OP1_SHIFT0 to OP1_SHIFT4 in the second block. Thus, we currently have 3 objectives, each of which will be addressed by an individual head. ## F.5.1 Attention Head 1: Detecting the Ones Digit of the Second Operand\n\nThe goal of the first head is to make the dimension IS_OP2_ONE as a one-hot row vector, where 1 is placed only at the token corresponding to the ones digit of the second operand.",
    "posicoupling-57": "Recall that $d=6 P+34$ and let $d_{Q K, 11}=P+1$. Let $M>0$ be a sufficiently large positive real number. Let\n\n$$\n\\begin{aligned}\n& \\left.\\boldsymbol{Q}_{1}^{(1)}=\\binom{\\mathbf{0}_{P \\times(P+34)}}{\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{FULL}}^{P+34}{ }_{\\mathrm{ONES}}\\right.}^{\\top} \\begin{array}{ccccc}\n\\mathbf{0}_{P \\times P} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} \\\\\n\\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 11 \\times d}},\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-51.jpg?height=131&width=1286&top_left_y=455&top_left_x=390)\n\nUnlike the construction for the addition task, we do not provide the table for the exact matrix and detailed error analysis due to their complex characterization. Instead, we provide an illustrative example for each step. We will also simply regard $M$ as a sufficiently large real scalar and thus the attention values can be clearly separated after going through the softmax operation. The matrix $\\boldsymbol{Q}_{1}^{(1)}$ maps the embedding matrix $\\boldsymbol{X}^{(0)}$ into a query matrix $\\boldsymbol{Q}_{1}^{(1)} \\boldsymbol{X}^{(0)} \\in \\mathbb{R}^{(P+1) \\times N}$, where the first $P$ rows are obtained by copying from the dimensions POS_2 and scaling by $\\sqrt{M}$, while the last row is the copy of the dimension FULL_ONES scaled by $\\sqrt{M P}$. Similarly, the matrix $\\boldsymbol{K}_{1}^{(1)}$ maps the embedding matrix to a key matrix $\\boldsymbol{K}_{1}^{(1)} \\boldsymbol{X}^{(0)} \\in \\overline{\\mathbb{R}^{(P+1) \\times N}}$. In this case, the first $P$ rows are obtained by copying from the dimensions POS_1 and scaled by $\\sqrt{M}$, with the last row being the dimension is_bOs, scaled by $\\sqrt{M P}$. For concrete examples, refer to Tables 32 and 33 . Table 32: Example of $\\boldsymbol{Q}_{1}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. | I | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ <br> $P+1:$ | $\\mathbf{o}_{P}$ <br> $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |  |  |  |  |  |  |  |  |  |  | $\\sqrt{\\sqrt{M} v_{2}^{P}} \\sqrt{M P}$ |\n\nTable 33: Example of $\\boldsymbol{K}_{1}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{o}_{P}$ | $\\sqrt{M} \\boldsymbol{v} P$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{1}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nBy these, the attention score matrix $\\boldsymbol{C}_{1}^{(1)}:=\\left(\\boldsymbol{K}_{1}^{(1)} \\boldsymbol{X}^{(0)}\\right)^{\\top} \\boldsymbol{Q}_{1}^{(1)} \\boldsymbol{X}^{(0)}$ and the attention matrix $\\boldsymbol{A}_{1}^{(1)}:=\\operatorname{softmax}\\left(\\boldsymbol{C}_{1}^{(1)}\\right) \\in$ $\\mathbb{R}^{N \\times N}$ can be obtained. We provide the example of $\\boldsymbol{A}_{1}^{(1)}$ in Table 34. Now let $d_{V, 11}=1$ and define\n\n$$\n\\begin{aligned}\n& \\boldsymbol{U}_{1}^{(1)}=\\boldsymbol{e}_{\\mathrm{IS}}^{d}{ }_{\\mathrm{OP} 2 \\_{ }_{\\mathrm{ONE}}} \\in \\mathbb{R}^{d \\times d_{V, 11}} . \\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-51.jpg?height=63&width=657&top_left_y=1909&top_left_x=734)\n\nThe matrix $\\boldsymbol{U}_{1}^{(1)} \\boldsymbol{V}_{1}^{(1)} \\boldsymbol{X}^{(0)}$ takes the dimension IS_MUL and IS _EQUAL from the embedding matrix $\\boldsymbol{X}^{(0)}$, subtracts one from the other, scales the result by 2 , and puts it to the dimension IS_OP2_SUM. Consequently, the matrix $\\boldsymbol{U}_{1}^{(1)} \\boldsymbol{V}_{1}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{1}^{(1)}$ is a matrix that matches the size of the input embedding matrix $\\boldsymbol{X}^{(0)}$ and is filled with zeroes, except for a unique 1 located at the ones place of the second operand in the input sequence, in the dimension IS_OP2_ONE (6-th). A concrete example is provided in Tables 35 and 36. Table 34: Example of $\\boldsymbol{A}_{1}^{(1)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 32 and 33 . | row $\\backslash$ col | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | 1 | 1 | 1 | 1 | 1 | $1 / 2$ | $1 / 2$ | 1 | $1 / 3$ | $1 / 4$ | $1 / 4$ | $1 / 3$ | $1 / 3$ | $1 / 2$ |\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 |\n| 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | $1 / 3$ | 0 | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 35: Example of $\\boldsymbol{U}_{1}^{(1)} \\boldsymbol{V}_{1}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | -2 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 36: Example of $\\boldsymbol{U}_{1}^{(1)} \\boldsymbol{V}_{1}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{1}^{(1)}$, continuing from Tables 34 and 35. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\n## F.5.2 Attention Head 2: Detecting the Tens Digit of the Second Operand\n\nIn the previous head, we set the dimension IS_OP2_ONE ( 6 -th dimension) to a one-hot row vector, where 1 is placed only in the token corresponding to the ones digit of the second operand. The objective of Attention head 2 is to fill the dimension IS_OP2_TEN ( 7 -th dimension) in a similar manner to IS_OP2_ONE, but with 1 placed only in the tens digit of the second operand. The design of the query, key, and value weight is not significantly different from the previous head. Compared to the construction of Attention head 1, we only push $\\sqrt{M} \\boldsymbol{I}_{P}$ to the next block for designing $\\boldsymbol{Q}_{2}^{(1)}$. Specifically,\n$\\boldsymbol{Q}_{2}^{(1)}$ and $\\boldsymbol{K}_{2}^{(1)}$ are defined as\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-53.jpg?height=128&width=1332&top_left_y=294&top_left_x=402)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-53.jpg?height=126&width=1288&top_left_y=425&top_left_x=402)\nwhere $d_{Q K, 12}$ is set to $P+1$. We refer to Tables 37 and 38 for specific examples. Table 37: Example of $\\boldsymbol{Q}_{2}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28 . |  | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{o}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{9}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{9}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 38: Example of $\\boldsymbol{K}_{2}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. |  | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{o}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{1}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nBy these, the attention score matrix $\\boldsymbol{C}_{2}^{(1)}:=\\left(\\boldsymbol{K}_{2}^{(1)} \\boldsymbol{X}^{(0)}\\right)^{\\top} \\boldsymbol{Q}_{2}^{(1)} \\boldsymbol{X}^{(0)}$ and the attention matrix $\\boldsymbol{A}_{2}^{(1)}:=\\operatorname{softmax}\\left(\\boldsymbol{C}_{2}^{(1)}\\right) \\in$ $\\mathbb{R}^{N \\times N}$ can be obtained, and the example of $\\boldsymbol{A}_{2}^{(1)}$ is provided in Table 39. Table 39: Example of $\\boldsymbol{A}_{2}^{(1)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 37 and 38 . | row $\\backslash$ col | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | 1 | 1 | 1 | 1 | 1 | $1 / 2$ | 1 | 1 | 1 | $1 / 3$ | $1 / 4$ | $1 / 4$ | $1 / 3$ | $1 / 3$ |\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ |\n| 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 |\n| 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | $1 / 3$ | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 4$ | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 3$ |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nFinally, we set $\\boldsymbol{V}_{2}^{(1)}$ and $\\boldsymbol{U}_{2}^{(1)}$ the same to that of the previous head. That is, with $d_{V, 12}=1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{2}^{(1)}=2\\left(\\boldsymbol{e}_{\\mathrm{IS} \\_\\mathrm{MUL}}^{d}-\\boldsymbol{e}_{\\mathrm{IS} \\_\\mathrm{EQUAL}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 12} \\times d} \\\\\n& \\boldsymbol{U}_{2}^{(1)}=\\boldsymbol{e}_{\\mathrm{IS} \\_\\mathrm{OP}_{2}{ }^{d} \\mathrm{TEN}}^{d} \\in \\mathbb{R}^{d \\times d_{V, 12}}\n\\end{aligned}\n$$\n\nand the example of $\\boldsymbol{U}_{2}^{(1)} \\boldsymbol{V}_{2}^{(1)} \\boldsymbol{X}^{(0)}$ and $\\boldsymbol{U}_{2}^{(1)} \\boldsymbol{V}_{2}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{2}^{(1)}$ is provided in Tables 40 and 41 . Consequently, the matrix $\\boldsymbol{U}_{2}^{(1)} \\boldsymbol{V}_{2}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{2}^{(1)}$ is a matrix that matches the size of the input embedding matrix and is filled with\nzeroes, except for a unique 1 located at the tens place of the second operand in the input sequence, with the dimension IS_OP2_TEN (7-th dimension).",
    "posicoupling-58": "Table 40: Example of $\\boldsymbol{U}_{2}^{(1)} \\boldsymbol{V}_{2}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | -2 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 41: Example of $\\boldsymbol{U}_{2}^{(1)} \\boldsymbol{V}_{2}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{2}^{(1)}$, continuing from Tables 39 and 40. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\n## F.5.3 Attention Head 3: Position Masking\n\nThe goal of Attention head 3 is to generate a binary mask at the dimension MASK (34-th dimension), with ' 0 ' placed before the multiplication symbol ( $\\times$ ) and ' 1 ' placed starting from the multiplication symbol to the end. To this end, we set $d_{Q K, 13}=1$ and design query and key weights by\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Q}_{3}^{(1)}=\\left(e_{\\text {FULL_ONES }}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{Q K, 13} \\times d} \\\\\n& \\boldsymbol{K}_{3}^{(1)}=\\left(e_{\\text {IS_MUL }}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{Q K, 13} \\times d}\n\\end{aligned}\n$$\n\nThe matrices $\\boldsymbol{Q}_{3}^{(1)} \\boldsymbol{X}^{(0)}$ and $\\boldsymbol{K}_{3}^{(1)} \\boldsymbol{X}^{(0)}$ take the dimension FulL_ONES and is_MuL, respectively, from the input embedding matrix.",
    "posicoupling-59": "For concrete examples, please refer to Tables 42 and 43. Table 42: Example of $\\boldsymbol{Q}_{3}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1:$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n\nTable 43: Example of $\\boldsymbol{K}_{3}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1:$ | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nBy these, the attention score matrix $\\boldsymbol{C}_{3}^{(1)}:=\\left(\\boldsymbol{K}_{3}^{(1)} \\boldsymbol{X}^{(0)}\\right)^{\\top} \\boldsymbol{Q}_{3}^{(1)} \\boldsymbol{X}^{(0)}$ and the attention matrix $\\boldsymbol{A}_{3}^{(1)}:=\\operatorname{softmax}\\left(\\boldsymbol{C}_{3}^{(1)}\\right) \\in$ $\\mathbb{R}^{N \\times N}$ can be obtained and the example of $\\boldsymbol{A}_{3}^{(1)}$ is provided in Table 44. Table 44: Example of $\\boldsymbol{A}_{3}^{(1)}$ (with explicit row/column indices), continuing from Tables 42 and 43. | {{ row $\\\\ ) col }} & \\(j=1$ |  | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| $i=1$ | 0 | 1 | 1/2 | $1 / 3$ | 1/4 | $1 / 5$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2 | 0 | 0 | $1 / 2$ | $1 / 3$ | $1 / 4$ | $1 / 5$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3 | 0 | 0 | 0 | $1 / 3$ | $1 / 4$ | $1 / 5$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0 | $1 / 4$ | $1 / 5$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | $1 / 5$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nFinally, we set $\\boldsymbol{V}_{3}^{(1)}$ and $\\boldsymbol{U}_{3}^{(1)}$ by $d_{V, 13}=1$ and\n\n$$\n\\begin{aligned}\n\\boldsymbol{V}_{3}^{(1)} & =\\left(\\boldsymbol{e}_{\\mathrm{IS}-\\mathrm{MUL}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 13} \\times d} \\\\\n\\boldsymbol{U}_{3}^{(1)} & =\\boldsymbol{e}_{\\mathrm{MASK}}^{d} \\in \\mathbb{R}^{d \\times d_{V, 13}}\n\\end{aligned}\n$$\n\nThe example of $\\boldsymbol{U}_{3}^{(1)} \\boldsymbol{V}_{3}^{(1)} \\boldsymbol{X}^{(0)}$ and $\\boldsymbol{U}_{3}^{(1)} \\boldsymbol{V}_{3}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{3}^{(1)}$ is provided in Tables 45 and 46 . Consequently, the matrix $\\boldsymbol{U}_{3}^{(1)} \\boldsymbol{V}_{3}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{3}^{(1)}$ is a matrix that matches the size of the input embedding matrix and is filled with 1 only at the dimension MASK (34-th dimension) starting from the $\\times$ token to the end of sequence, and 0 otherwise. At this point, the objective of Attention head 3 may seem somewhat unclear. We note that the output of Attention head 3 will be utilized to fill the dimensions POS_2_MASK in the subsequent FFN layer, and this POS_2_MASK plays a crucial role in designing the key matrices in the Attention heads 3 to 7 at the second Transformer block. Table 45: Example of $\\boldsymbol{U}_{3}^{(1)} \\boldsymbol{V}_{3}^{(1)} \\boldsymbol{X}^{(0)}$, continuing from Table 28. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 46: Example of $\\boldsymbol{U}_{3}^{(1)} \\boldsymbol{V}_{3}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{3}^{(1)}$, continuing from Tables 44 and 45. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n\n## F.5.4 Residual Connection\n\nSo far we have computed the output of $\\mathrm{Att}_{1}$ operation. Passing through the residual connection, the output of the attention layer becomes the sum of the original input embedding matrix and the output of Att $_{1}$ operation:\n\n$$\n\\boldsymbol{Y}^{(1)}=\\boldsymbol{X}^{(0)}+\\sum_{h \\in\\{1,2,3\\}} \\boldsymbol{U}_{h}^{(1)} \\boldsymbol{V}_{h}^{(1)} \\boldsymbol{X}^{(0)} \\boldsymbol{A}_{h}^{(1)}\n$$\n\nAn example of the output of residual connection is presented in Table 47. Table 47: Example output of residual connection, continuing from Tables 28, 36, 41 and 46. Uncolored rows represent the initial embedding. Yellow rows indicate the rows filled by the attention heads in the first Transformer block. A pink row indicates the row that will be filled by the subsequent FFN layer. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n| 2: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 3: IS_BOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| $35-(P+34):$ POS_2_MASK | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $(P+35)-(2 P+34):$ POS_1 | $\\mathbf{0}_{P}$ | $v_{3}^{P}$ | $v_{4}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ | $v_{3}^{P}$ | $v_{2}^{P}$ | $v_{1}^{P}$ |\n| $(2 P+35)-(3 P+34):$ POS $\\_2$ | $\\mathbf{0}_{P}$ |  |  | $\\boldsymbol{v}_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ | $v_{3}^{P}$ | $v_{2}^{P}$ |\n| $(3 P+35)-(4 P+34):$ POS_ 3 | $\\mathbf{0}_{P}$ |  | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{9}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | ${ }^{5}$ | P | $v_{3}^{P}$ |\n| $(4 P+35)-(5 P+34):$ POS_4 | $\\mathbf{0}_{P}$ |  | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{9}^{P}$ | $v_{10}^{P}$ | $v_{8}^{P}$ | $v_{9}^{P}$ | $v_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | P | $v_{4}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS -5 | $\\mathbf{0}_{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{9}^{P}$ | $v_{10}^{P}$ |  |  |  |  |  |  |  |  | P |  |\n\n## F. 6 Transformer Block 1 - Token-wise Feed-forward Layer\n\nThe goal of the feed-forward layer involves filling the dimensions POS_2_MASK. Specifically, for each token $\\sigma_{i}$, if the dimension MASK is 1 (i.e., $\\boldsymbol{Y}_{(\\text {MASK }) i}^{(1)}=1$ ), we want to fill the dimensions POS_ 2 _MASK by copying the the corresponding token's POS_2; otherwise, we want to fill with $\\mathbf{0}_{P}$. Be careful that the feed-forward operation is restricted to a token-wise mapping, meaning it only takes inputs from entries within the same column of the encoding matrix. Construction for POS_2_MASK. Given a vector $\\boldsymbol{y}=\\left[\\boldsymbol{y}_{j}\\right]_{j=1}^{d} \\in \\mathbb{R}^{d}$, define functions $g_{l}, h_{l}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ for every $j \\in[P]$ as\n\n$$\n\\begin{aligned}\n& g_{l}(\\boldsymbol{y}):=\\boldsymbol{y}_{\\mathrm{POS} \\_2, l}-2 \\boldsymbol{y}_{\\mathrm{MASK}} \\\\\n& h_{l}(\\boldsymbol{y}):=-\\boldsymbol{y}_{\\mathrm{POS} \\_} 2, l-2 \\boldsymbol{y}_{\\mathrm{MASK}}\n\\end{aligned}\n$$\n\nwhere $\\boldsymbol{y}_{\\mathrm{Pos}}{ }_{2}^{2, l} \\in \\mathbb{R}$ is the $l$-th dimension of $\\boldsymbol{y}_{\\mathrm{POs}}{ }_{2} \\in \\mathbb{R}^{P}(l \\in 1,2, \\ldots, P)$. Consider a simple one-hidden-layer ReLU networks $f_{l}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ defined as\n\n$$\nf_{l}(\\boldsymbol{y})=\\phi\\left(g_{l}(\\boldsymbol{y})\\right)-\\phi\\left(h_{l}(\\boldsymbol{y})\\right)\n$$\n\nUsing the fact that $\\boldsymbol{y}_{\\text {Pos } \\_2, l}$ is either -1 or 1 , we can easily check that $f_{l}(\\boldsymbol{y})=\\boldsymbol{y}_{\\text {POS } \\_2, l}$ if $\\boldsymbol{y}_{\\text {MAsK }}$ is 0 , and $f_{l}(\\boldsymbol{y})=0$ if $\\boldsymbol{y}_{\\text {MASK }}$ is 1 . Now, we can construct the width- $2 P$ feed-forward network that outputs the desired value at the dimension POS_2_MASK by\n\n$$\n\\left[\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)\\right]_{(\\text {Pos_2_MAsk }) i}=\\left[\\begin{array}{lll}\nf_{1}\\left(\\boldsymbol{Y}_{\\bullet i}^{(1)}\\right) & \\cdots & f_{P}\\left(\\boldsymbol{Y}_{\\bullet i}^{(1)}\\right)\n\\end{array}\\right]^{\\top} \\in \\mathbb{R}^{P \\times 1}\n$$\n\nand 0 for any other dimensions. The example output for this layer is presented in Table 48. Table 48: Example output of FFN layer at the first Transformer block, continuing from Table 47. Omitted entries are filled with 0 . | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $35-(P+34):$ POS_2_MASK | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n\n## F.6.1 Residual Connection\n\nThe last task of the feed-forward layer is to pass $\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)$ through the residual connection. As a result, we have\n\n$$\n\\boldsymbol{X}^{(1)}=\\boldsymbol{Y}^{(1)}+\\mathrm{FF}_{1}\\left(\\boldsymbol{Y}^{(1)}\\right)\n$$\n\nThis is the end of the first Transformer block, and a concrete example of $\\boldsymbol{X}^{(1)}$ is illustrated in Table 49. ## F. 7 Transformer Block 2 - Causal Attention Layer\n\nConsider a scenario where the model is at the step of predicting the $i$-th least significant digit of the multiplication result. There are two goals for the causal attention layer at the second Transformer block. The first goal is to generate the embedding matrix as the left-most figure in Table 31, that is, fill OP2_ONE, OP2_TEN, and OP1_SHIFT0 to OP1_SHIFT4 with the ones digit of the second operand, the tens digit of the second operand, and the $\\bar{i},(i-1),(i-2),(i-3),(i-4)$-th least significant digit of the first operand, respectively. Our construction assigns each head to each dimension. The second goal is to fill PRE_EOS1 and PRE_EOS2 with appropriate values. These 2 dimensions will be utilized in the subsequent FFN layer to predict whether we should predict the next token as EOS or not. Also, we note that filling these 2 dimensions can be implemented within the same head for OP1_SHIFT0 and OP1_SHIFT2 respectively, thus requiring a total of seven heads. ## F.7.1 Attention Head 1: Copying the Ones Digit of the Second Operand\n\nThe objective of Attention head 1 is to fill the dimension OP2_ONE with the ones digit of the second operand. To do so, we design the weights by defining $d_{Q K, 21}=1$ and\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Q}_{1}^{(2)}=\\left(\\boldsymbol{e}_{\\text {FULL_ONES }}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{Q K, 21} \\times d}, \\\\\n& \\boldsymbol{K}_{1}^{(2)}=\\left(e_{\\text {IS }}^{d}{ }_{\\text {OP2_ONE }}\\right)^{\\top} \\in \\mathbb{R}^{d_{Q K, 21} \\times d} . \\end{aligned}\n$$\n\nWe also define $d_{V, 21}=1$ and\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{1}^{(2)}=\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 21} \\times d} \\\\\n& \\boldsymbol{U}_{1}^{(2)}=\\boldsymbol{e}_{\\mathrm{OP}{ }_{2}^{d} \\text { _ONE }}^{d} \\in \\mathbb{R}^{d \\times d_{V, 21}} . \\end{aligned}\n$$\n\nTable 49: Example embedding matrix after the first Transformer block. The yellow rows represent the results introduced during the first block, while the gray rows will be filled in the second block. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n| 2: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 3: IS_BOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS__MUL | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: Is_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9: OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10: OP1_SHIFT0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11: OP1_SHIFT1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12: OP1_SHIFT2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13: OP1_SHIFT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14: OP1_SHIFT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15: RESult1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 16: RESUlT2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 17: RESULT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18: RESULT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 19: PRE_PROD | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 20: PRE_CCARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 21: PRE_ EOS1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 22: PRE_EOS2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 23-32: PROD | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $0_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $0_{10}$ | $\\mathbf{0}_{10}$ | $0_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ | $\\mathbf{0}_{10}$ |\n| 33: IS _EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 35- $(P+34):$ POS_2_MASK | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | $\\mathbf{0}_{P}$ |\n| $(P+35)-(2 P+34): \\overline{\\text { POS }} \\_1$ | $\\mathbf{0}_{P}$ |  |  |  | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $v_{5}^{P}$ |  | $\\boldsymbol{v}_{3}^{P}$ | $v_{2}^{P}$ | $\\boldsymbol{v}_{1}^{P}$ |\n| $(2 P+35)-(3 P+34):$ POS_2 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{P}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{6}^{P}$ | $v_{P}^{P}$ | $v_{P}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{5}^{P}$ | $v_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ |\n| $(3 P+35)-(4 P+34):$ POS_ 3 | $\\mathbf{0}_{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $v_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ |\n| $(4 P+35)-(5 P+34): \\mathrm{POS}_{-}-4$ | $\\mathbf{0}_{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $v_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS_ $^{-5}$ | $\\mathbf{0}_{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{11}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $v_{11}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $v_{9}^{P}$ | $v_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ |\n\nA concrete example of $\\boldsymbol{Q}_{1}^{(2)} X^{(1)}, \\boldsymbol{K}_{1}^{(2)} X^{(1)}, \\boldsymbol{A}_{1}^{2}, \\boldsymbol{U}_{1}^{(2)} \\boldsymbol{V}_{1}^{(2)} \\boldsymbol{X}^{(1)}$, and $\\boldsymbol{U}_{1}^{(2)} \\boldsymbol{V}_{1}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{1}^{(2)}$ is provided in Tables 50 to 54 . One might be concerned that in Table 54 , the dimension OP 2 _onE is not completely filled with ' 9 ', but only the latter part. However, we note that given our focus on next-token prediction, it suffices to accurately fill values starting from the $=$ token, and filling the preceding tokens with placeholder values does not cause any issues. Table 50: Example of $\\boldsymbol{Q}_{1}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| $1:$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n\nTable 51: Example of $\\boldsymbol{K}_{1}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1:$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 52: Example of $\\boldsymbol{A}_{1}^{(2)}$ (with explicit row/column indices), continuing from Tables 50 and 51. | row $\\backslash \\mathrm{col}$ |  | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| $i=1$ | 0 | 1 | 1/2 | $1 / 3$ | 1/4 | $1 / 5$ | 1/6 | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2 | 0 | 0 | $1 / 2$ | $1 / 3$ | $1 / 4$ | $1 / 5$ | $1 / 6$ | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3 | 0 | 0 | 0 | $1 / 3$ | $1 / 4$ | $1 / 5$ | 1/6 | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0 | $1 / 4$ | $1 / 5$ | 1/6 | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | $1 / 5$ | $1 / 6$ | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | 1/6 | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 7$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 53: Example of $\\boldsymbol{U}_{1}^{(2)} \\boldsymbol{V}_{1}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: OP2_ONE | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n\nTable 54: Example of $\\boldsymbol{U}_{1}^{(2)} \\boldsymbol{V}_{1}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{1}^{(2)}$, continuing from Tables 52 and 53. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: OP2_ONE | 0 | $7 / 2$ | 4 | $21 / 4$ | $26 / 5$ | $13 / 3$ | $33 / 7$ | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 |\n\n## F.7.2 Attention Head 2: Copying the Tens Digit of the Second Operand\n\nThe objective of Attention head 2 is to fill the dimension OP2_TEN with the tens digit of the second operand. We take a similar approach to Attention head 1, but the main difference is that we utilize the dimension IS_OP2 _TEN instead of IS_OP2_ONE for generating the key weight. We design the weights by defining $d_{Q K, 22}=1$ and\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Q}_{2}^{(2)}=\\left(\\boldsymbol{e}_{\\mathrm{FULL} \\_\\mathrm{ONES}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{Q K, 22} \\times d}, \\\\\n& \\boldsymbol{K}_{2}^{(2)}=\\left(e_{\\text {IS }}^{d}{ }_{\\text {OP } 2 \\_ \\text {TEN }}\\right)^{\\top} \\in \\mathbb{R}^{d_{Q K, 22 \\times d}} . \\end{aligned}\n$$\n\nWe also define $d_{V, 22}=1$ and\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{2}^{(2)}=\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 22} \\times d} \\\\\n& \\boldsymbol{U}_{2}^{(2)}=\\boldsymbol{e}_{\\mathrm{OP} 2 \\_\\mathrm{TEN}}^{d} \\in \\mathbb{R}^{d \\times d_{V, 22}}\n\\end{aligned}\n$$\n\nA concrete example of $\\boldsymbol{Q}_{2}^{(2)} X^{(1)}, \\boldsymbol{K}_{2}^{(2)} X^{(1)}, \\boldsymbol{A}_{2}^{2}, \\boldsymbol{U}_{2}^{(2)} \\boldsymbol{V}_{2}^{(2)} \\boldsymbol{X}^{(1)}$, and $\\boldsymbol{U}_{2}^{(2)} \\boldsymbol{V}_{2}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{2}^{(2)}$ is provided in Tables 55 to 59 . Once again, the dimension OP2_TEN is not entirely filled with ' 7 ' in Table 59. As mentioned in the previous head, this does not cause any issues because the front part (before $=$ ) does not affect the final prediction unless additional attention blocks are introduced after the second Transformer block. Table 55: Example of $\\boldsymbol{Q}_{2}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1:$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n\nTable 56: Example of $\\boldsymbol{K}_{2}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1:$ | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 57: Example of $\\boldsymbol{A}_{2}^{(2)}$ (with explicit row/column indices), continuing from Tables 55 and 56. | row $\\backslash$ col | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |  |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| $i=1$ | 0 | 1 | $1 / 2$ | $1 / 3$ | $1 / 4$ | $1 / 5$ | $1 / 6$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2 | 0 | 0 | $1 / 2$ | $1 / 3$ | $1 / 4$ | $1 / 5$ | $1 / 6$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3 | 0 | 0 | 0 | $1 / 3$ | $1 / 4$ | $1 / 5$ | $1 / 6$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0 | $1 / 4$ | $1 / 5$ | $1 / 6$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | $1 / 5$ | $1 / 6$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 6$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 58: Example of $\\boldsymbol{U}_{2}^{(2)} \\boldsymbol{V}_{2}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9: OP2_TEN | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n\nTable 59: Example of $\\boldsymbol{U}_{2}^{(2)} \\boldsymbol{V}_{2}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{2}^{(2)}$, continuing from Tables 57 and 58. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9: OP2_TEN | 0 | $7 / 2$ | 4 | $21 / 4$ | $26 / 5$ | $13 / 3$ | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 |\n\n## F.7.3 Attention Head 3: Copying the Appropriate Digit from the First Operand I\n\nThe objectives of the first and the second Attention heads were to extract the ones and tens digits of the second operand and display them in the dimensions OP2_onE and OP2_ten, respectively. For Attention head 3 to 7, we mainly focus on the first operand. Specifically, in Attention head 3, the goal is to fill the dimension OP1_SHIFT0 at the $i$-th least significant digit of the response (when predicting the $(i+1)$-th least significant digit of the response) with the $(i+1)$-th least significant digit of the first operand. For our example, we want to fill OP1_SHIFT0 of the token $=$ by 5 . Here, $i$ ranges from 0 to $\\ell_{a}+2$, where the 0 -th least significant digit of the response denotes the equal token. In cases where $i \\geq \\ell_{a}$, we fill by 0 . Additionally, the third head has an extra objective: filling the dimension PRE_EOS1. This dimension is utilized for EOS prediction in the subsequent FFN layer along with PRE_EOS2, which is filled by the fifth head of the same layer. We observed that both objectives can be achieved by utilizing the same attention map. Thus, instead of implementing these objectives in separate heads, we can achieve them by utilizing the matrices $\\boldsymbol{V}_{3}^{(2)}$ and $\\boldsymbol{U}_{3}^{(2)}$ described below. Unlike previous heads, $\\boldsymbol{V}_{3}^{(2)}$ and $\\boldsymbol{U}_{3}^{(2)}$ each have two elements, with each element contributing to one of the objectives. Our specific construction is as follows. With $d_{Q K, 23}=P+1$,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-61.jpg?height=129&width=1452&top_left_y=1475&top_left_x=304)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-61.jpg?height=131&width=1394&top_left_y=1604&top_left_x=303)\nand with $d_{V, 23}=2$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{3}^{(2)}=\\binom{2\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top}}{\\left(\\boldsymbol{e}_{\\mathrm{IS} \\_ \\text {BOS }}^{d}\\right)^{\\top}} \\in \\mathbb{R}^{d_{V, 23} \\times d}, \\\\\n& \\boldsymbol{U}_{3}^{(2)}=\\left(\\boldsymbol{e}_{\\mathrm{OP} 1 \\_ \\text {SHIFT0 }}^{d} \\quad \\boldsymbol{e}_{\\mathrm{PRE} \\_\\mathrm{EOS} 1}^{d}\\right) \\in \\mathbb{R}^{d \\times d_{V, 23}} . \\end{aligned}\n$$\n\nWe provide the examples in Tables 60 to 64 . We note that within the dimension PRE_EOS1 of the matrix $\\boldsymbol{U}_{3}^{(2)} \\boldsymbol{V}_{3}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{3}^{(2)}$, if we restrict our view to the equal symbol $=$ and the response sequence, 1 is only assigned to the first, second, and third most significant digits of the response (regardless of the query length). Table 60: Example of $\\boldsymbol{Q}_{3}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{o}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{1}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 61: Example of $\\boldsymbol{K}_{3}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P$ : | $\\mathbf{o}_{P}$ | $\\sqrt{M} v_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} v_{6}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{o}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 62: Example of $\\boldsymbol{A}_{3}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 60 and 61 .",
    "posicoupling-60": "$$\n\\begin{array}{r|ccccccccccccccc}\n\\text { row } \\backslash \\operatorname{col} & j=1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\\\\n\\hline i=1 & 1 & 1 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 / 2 & 1 & 1 & 1 \\\\\n2 & 0 & 0 & 1 / 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 / 2 & 0 & 0 & 0 \\\\\n3 & 0 & 0 & 0 & 1 / 2 & 0 & 0 & 1 / 2 & 0 & 0 & 0 & 1 / 2 & 0 & 0 & 0 & 0 \\\\\n4 & 0 & 0 & 0 & 0 & 1 / 2 & 0 & 0 & 1 / 2 & 0 & 1 / 2 & 0 & 0 & 0 & 0 & 0 \\\\\n5 & 0 & 0 & 0 & 0 & 0 & 1 / 2 & 0 & 0 & 1 / 2 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n10 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n11 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n12 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n13 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n14 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n15 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\n$$\n\nTable 63: Example of $\\boldsymbol{U}_{3}^{(2)} \\boldsymbol{V}_{3}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10: OP1_SHIFT0 | 0 | 14 | 10 | 18 | 10 | 0 | 14 | 18 | 0 | 10 | 0 | 0 | 0 | 0 | 12 |\n| 21: PRE_EOS1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 64: Example of $\\boldsymbol{U}_{3}^{(2)} \\boldsymbol{V}_{3}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{3}^{(2)}$, continuing from Tables 62 and 63. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_BOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10: OP1_SHIFT0 | 0 | 0 | 7 | 5 | 9 | 5 | 5 | 9 | 5 | 9 | 5 | 7 | 0 | 0 | 0 |\n| 21: PRE_EOS1 | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 | 1 | 1 |\n\n## F.7.4 Attention Head 4: Copying the Appropriate Digit from the First Operand II\n\nThe objective of Attention head 4 is to fill the dimension OP1_SHIFT1 at the $i$-th least significant digit of the response (when predicting the $(i+1)$-th least significant digit of the response) with the $i$-th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$. In cases where the $i$-th least significant digit of the first operand is not well-defined (i.e., $i \\in\\left\\{0, \\ell_{a}+1, \\ell_{a}+2\\right\\}$ ), we assign 0 . The design of Attention head 4 is as follows. With $d_{Q K, 24}=P+1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{K}_{4}^{(2)}=\\left(\\begin{array}{ccccccc}\n\\mathbf{0}_{P \\times 34} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{IS} \\_\\mathrm{BOS}}^{34}\\right)^{\\top} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} \\\\\n\\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 24} \\times d},\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-63.jpg?height=131&width=1452&top_left_y=544&top_left_x=304)\nand with $d_{V, 24}=1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{4}^{(2)}=2\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 24} \\times d} \\\\\n& \\boldsymbol{U}_{4}^{(2)}=\\boldsymbol{e}_{\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 1}^{d} \\in \\mathbb{R}^{d \\times d_{V, 24}}\n\\end{aligned}\n$$\n\nWe provide the examples in Tables 65 to 69. Table 65: Example of $\\boldsymbol{Q}_{4}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{o}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{2}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 66: Example of $\\boldsymbol{K}_{4}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P$ : | $\\mathbf{0}_{P}$ | $\\sqrt{M} v_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 67: Example of $\\boldsymbol{A}_{4}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 65 and 66 . | row $\\backslash$ col | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 | $1 / 2$ | $1 / 2$ | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 | 1 |\n| 2 | 0 | $1 / 2$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 |\n| 3 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 |\n| 4 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | $1 / 2$ | 0 | $1 / 2$ | 0 | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 68: Example of $\\boldsymbol{U}_{4}^{(2)} \\boldsymbol{V}_{4}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11: OP1_SHIFT1 | 0 | 14 | 10 | 18 | 10 | 0 | 14 | 18 | 0 | 10 | 0 | 0 | 0 | 0 | 12 |\n\nTable 69: Example of $\\boldsymbol{U}_{4}^{(2)} \\boldsymbol{V}_{4}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{4}^{(2)}$, continuing from Tables 67 and 68. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11: OP1_SHIFT1 | 0 | 7 | 5 | 9 | 5 | 0 | 9 | 5 | 0 | 5 | 9 | 5 | 7 | 0 | 0 |\n\n## F.7.5 Attention Head 5: Copying the Appropriate Digit from the First Operand III\n\nThe main objective of Attention head 5 is to fill the dimension OP1_SHIFT2 at the $i$-th least significant digit of the response (when predicting the $(i+1)$-th least significant digit of the response) with the $(i-1)$-th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$, and in cases where the $i$-th least significant digit of the first operand is not well-defined (i.e., $i \\in\\left\\{0,1, \\ell_{a}+2\\right\\}$ ), we assign 0 . As mentioned in Attention head 3, we assign an extra goal to Attention head 5, which is to fill the dimension PRE_EOS2. The design of the fifth head is as follows. With $d_{Q K, 25}=P+1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Q}_{5}^{(2)}=\\left(\\begin{array}{c}\n\\mathbf{0}_{P \\times 34} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{FULL} \\_\\mathrm{ONES}}^{34}\\right)^{\\top}\n\\end{array} \\begin{array}{cccccc}\n\\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} \\\\\n\\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 25} \\times d}, \\\\\n& \\boldsymbol{K}_{5}^{(2)}=\\left(\\begin{array}{ccccccc}\n\\mathbf{0}_{P \\times 34} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{IS} \\_ \\text {\u0432\u043eS }}^{34}\\right)^{\\top} & \\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} \\\\\n\\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 25} \\times d},\n\\end{aligned}\n$$\n\nand with $d_{V, 25}=2$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{5}^{(2)}=\\binom{2\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top}}{\\left(\\boldsymbol{e}_{\\mathrm{IS}-\\mathrm{BOS}}^{d}\\right)^{\\top}} \\in \\mathbb{R}^{d_{V, 25} \\times d} \\\\\n& \\boldsymbol{U}_{5}^{(2)}=\\left(\\begin{array}{ll}\n\\boldsymbol{e}_{\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 2}^{d} \\quad \\boldsymbol{e}_{\\mathrm{PRE} \\_\\mathrm{EOS} 2}^{d}\n\\end{array}\\right) \\in \\mathbb{R}^{d \\times d_{V, 25}}\n\\end{aligned}\n$$\n\nWe provide the examples in Tables 70 to 74 . Note that within the dimension PRE_EOS 2 of the matrix $\\boldsymbol{U}_{5}^{(2)} \\boldsymbol{V}_{5}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{5}^{(2)}$, if we restrict our view to the equal symbol $=$ and the response sequence, 1 is only assigned to the most and the least significant digit of the response, and the equal token. An important observation is that upon comparing PRE_EOS1 and PRE_EOS2, the most significant digit of the response is the only token that has a value of 1 in both dimensions. This observation plays a crucial role in predicting EOS for the next token, and we will elaborate further in the later section discussing the FFN layer. Table 70: Example of $\\boldsymbol{Q}_{5}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49 . | I | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1-P | $\\mathrm{o}_{P}$ | $\\sqrt{M} v_{5}^{P}$ | $\\sqrt{M} v_{6}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\sqrt{M} v_{8}^{P}$ | $\\sqrt{M} v_{9}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\sqrt{M} v_{8}^{P}$ | $\\sqrt{M} v_{9}^{P}$ | $\\sqrt{M} v_{8}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\sqrt{M} v_{6}^{P}$ | $\\sqrt{M} v_{5}^{P}$ | $\\sqrt{M} v_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{3}^{P}$ |\n| $P+1$ : | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 71: Example of $\\boldsymbol{K}_{5}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P$ : | $\\mathbf{o}_{P}$ | $\\sqrt{M} v_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-65.jpg?height=28&width=42&top_left_y=631&top_left_x=1124) | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-65.jpg?height=28&width=35&top_left_y=631&top_left_x=1558) | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 72: Example of $\\boldsymbol{A}_{5}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 70 and 71 . | row $\\backslash$ col | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | 1 | 1 | 1 | 1 | 1 | $1 / 2$ | 1 | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 |\n| 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 73: Example of $\\boldsymbol{U}_{5}^{(2)} \\boldsymbol{V}_{5}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12: OP1_SHIFT2 | 0 | 14 | 10 | 18 | 10 | 0 | 14 | 18 | 0 | 10 | 0 | 0 | 0 | 0 | 12 |\n| 22: PRE_EOS2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 74: Example of $\\boldsymbol{U}_{5}^{(2)} \\boldsymbol{V}_{5}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{5}^{(2)}$, continuing from Tables 72 and 73. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12: OP1_SHIFT2 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 | 0 | 0 | 5 | 9 | 5 | 7 | 0 |\n| 22: PRE_EOS2 | 1 | 1 | 1 | 1 | 1 | 1 | $1 / 2$ | 1 | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n\n## F.7.6 Attention Head 6: Copying the Appropriate Digit from the First Operand IV\n\nThe objective of Attention head 6 is to fill the dimension OP1_SHIFT3 at the $i$-th least significant digit of the response (when predicting the $(i+1)$-th least significant digit of the response) with the $(i-2)$-th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$. In cases where the $i$-th least significant digit of the first operand is not well-defined (i.e., $i \\in\\{0,1,2\\}$ ), we assign 0 . The design of Attention head 6 is as follows. With $d_{Q K, 26}=P+1$,\n\n$$\n\\begin{aligned}\n& \\left.\\boldsymbol{K}_{6}^{(2)}=\\binom{\\mathbf{0}_{P \\times 34}}{\\sqrt{M P}\\left(e_{\\mathbf{S s}_{-}}^{34}{ }_{\\text {BOS }}\\right.}^{\\top} \\begin{array}{cccccc}\n\\sqrt{M} \\boldsymbol{I}_{P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} \\\\\n\\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 26} \\times d} . \\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-66.jpg?height=123&width=1452&top_left_y=551&top_left_x=304)\n\nWith $d_{V, 26}=1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{6}^{(2)}=2\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 26} \\times d} \\\\\n& \\boldsymbol{U}_{6}^{(2)}=\\boldsymbol{e}_{\\mathrm{OP} 1 \\_\\mathrm{sHIFT} 3}^{d} \\in \\mathbb{R}^{d \\times d_{V, 26}}\n\\end{aligned}\n$$\n\nWe provide the examples in Tables 75 to 79 . Table 75: Example of $\\boldsymbol{Q}_{6}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | I | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P$ : | $\\mathrm{o}_{P}$ | $\\sqrt{M} v_{6}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\sqrt{M} v_{8}^{P}$ | $\\sqrt{M} v_{9}^{P}$ | $\\sqrt{M} v_{10}^{P}$ | $\\sqrt{M} v_{8}^{P}$ | $\\sqrt{M} v_{9}^{P}$ | $\\sqrt{M} v_{10}^{P}$ | $\\sqrt{M} v_{9}^{P}$ | $\\sqrt{M} v_{8}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\sqrt{M} v_{6}^{P}$ | $\\sqrt{M} v_{5}^{P}$ | $\\sqrt{M} v_{4}^{P}$ |\n| $P+1$ : | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 76: Example of $\\boldsymbol{K}_{6}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P$ : | $\\mathbf{o}_{P}$ | $\\sqrt{M} v_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} v_{6}^{P}$ | $\\sqrt{M} v_{7}^{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 78: Example of $\\boldsymbol{U}_{6}^{(2)} \\boldsymbol{V}_{6}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13: OP1_SHIFT3 | 0 | 14 | 10 | 18 | 10 | 0 | 14 | 18 | 0 | 10 | 0 | 0 | 0 | 0 | 12 |\n\nTable 77: Example of $\\boldsymbol{A}_{6}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 75 and 76 . | row $\\backslash$ col | $j=1$ | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $i=1$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ |\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ |\n| 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 |\n| 4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | $1 / 2$ | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 12 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 79: Example of $\\boldsymbol{U}_{6}^{(2)} \\boldsymbol{V}_{6}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{6}^{(2)}$, continuing from Tables 77 and 78. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 13: OP1_SHIFT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 9 | 5 | 7 |\n\n## F.7.7 Attention Head 7: Copying the Appropriate Digit from the First Operand V\n\nThe objective of Attention head 7 is to fill the dimension OP1_SHIFT4 at the $i$-th least significant digit of the response (when predicting the $(i+1)$-th least significant digit of the response) with the $(i-3)$-th least significant digit of the first operand. Similarly to the previous head, $i$ ranges from 0 to $\\ell_{a}+2$. In cases where the $i$-th least significant digit of the first operand is not well-defined (i.e., $i \\in\\{0,1,2,3\\}$ ), we assign 0 . The design of Attention head 7 is as follows. With $d_{Q K, 27}=P+1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Q}_{7}^{(2)}=\\left(\\begin{array}{c}\n\\mathbf{0}_{P \\times 34} \\\\\n\\sqrt{M P}\\left(\\boldsymbol{e}_{\\mathrm{FULL} \\_\\mathrm{ONES}}^{34}\\right)^{\\top}\n\\end{array} \\begin{array}{cccccc}\n\\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\mathbf{0}_{P \\times P} & \\sqrt{M} \\boldsymbol{I}_{P} \\\\\n\\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P} & \\mathbf{0}_{1 \\times P}\n\\end{array}\\right) \\in \\mathbb{R}^{d_{Q K, 27} \\times d},\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-67.jpg?height=131&width=1397&top_left_y=2016&top_left_x=302)\n\nWith $d_{V, 27}=1$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{V}_{7}^{(2)}=2\\left(\\boldsymbol{e}_{\\mathrm{NUM}}^{d}\\right)^{\\top} \\in \\mathbb{R}^{d_{V, 27} \\times d} \\\\\n& \\boldsymbol{U}_{7}^{(2)}=\\boldsymbol{e}_{\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 4}^{d} \\in \\mathbb{R}^{d \\times d_{V, 27}}\n\\end{aligned}\n$$\n\nWe provide the examples in Tables 80 to 84 . Table 80: Example of $\\boldsymbol{Q}_{7}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. |  | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P:$ | $\\mathbf{o}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{9}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{10}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{11}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{9}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{10}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{11}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{10}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{9}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{8}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ | $\\sqrt{M P}$ |\n\nTable 81: Example of $\\boldsymbol{K}_{7}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. | $\\mathcal{I}$ | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $1-P$ : | $\\mathbf{0}_{P}$ | $\\sqrt{M} \\boldsymbol{v}_{4}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{5}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{6}^{P}$ | $\\sqrt{M} \\boldsymbol{v}_{7}^{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | $0_{P}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-68.jpg?height=28&width=49&top_left_y=632&top_left_x=1124) | $0_{P}$ | $\\mathbf{0}_{P}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-68.jpg?height=28&width=48&top_left_y=632&top_left_x=1446) | ![](https://cdn.mathpix.com/cropped/2024_09_12_03e94af45cc0c85dd761g-68.jpg?height=28&width=49&top_left_y=632&top_left_x=1554) | $0_{P}$ | $0_{P}$ |\n| $P+1:$ | $\\sqrt{M P}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\nTable 82: Example of $\\boldsymbol{A}_{7}^{(2)}$ (with explicit row/column indices and sufficiently large $M$ ), continuing from Tables 80 and 81 . $$\n\\begin{array}{r|ccccccccccccccc}\n\\text { row } \\backslash \\operatorname{col} & j=1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\\\\n\\hline i=1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 / 2 & 1 / 2 & 1 / 2 \\\\\n2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 / 2 \\\\\n4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 / 2 & 0 \\\\\n5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 / 2 & 0 & 0 \\\\\n6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n10 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n11 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n12 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n13 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n14 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n15 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\n$$\n\nTable 83: Example of $\\boldsymbol{U}_{7}^{(2)} \\boldsymbol{V}_{7}^{(2)} \\boldsymbol{X}^{(1)}$, continuing from Table 49. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14: OP1_SHIFT4 | 0 | 14 | 10 | 18 | 10 | 0 | 14 | 18 | 0 | 10 | 0 | 0 | 0 | 0 | 12 |\n\nTable 84: Example of $\\boldsymbol{U}_{7}^{(2)} \\boldsymbol{V}_{7}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{7}^{(2)}$, continuing from Tables 82 and 83. (Irrelevant dimensions are omitted for readability)\n\n| $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1: NUM | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2: FULL_ONES | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3: IS_MUL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_EEUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 14: OP1_SHIFT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 9 | 5 |\n\n## F.7.8 Residual Connection\n\nSo far we have computed the output of $\\mathrm{Att}_{2}$ operation. Passing through the residual connection, the output of the attention layer becomes the sum of $\\boldsymbol{X}^{(1)}$ (the input to the second Transformer block) and the output of Att $_{2}$ operation:\n\n$$\n\\boldsymbol{Y}^{(2)}=\\boldsymbol{X}^{(1)}+\\sum_{h \\in[7]} \\boldsymbol{U}_{h}^{(2)} \\boldsymbol{V}_{h}^{(2)} \\boldsymbol{X}^{(1)} \\boldsymbol{A}_{h}^{(2)}\n$$\n\nA concrete example of the output of residual connection is presented in Table 85. Table 85: Example output of residual connection, continuing from Tables 49, 54, 59, 64, 69, 74, 79 and 84. Uncolored rows represent the initial embedding. Gray rows indicate the rows filled by the first Transformer block. Yellow rows indicate the rows filled by the attention layers at the second Transformer block. Pink rows indicate the rows that will be filled by the subsequent FFN layer. | I | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n| 2: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 3: IS_BOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: IS_MUL | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: OP2_ONE | 0 | $7 / 2$ | 4 | $21 / 4$ | 26/5 | $13 / 3$ | $33 / 7$ | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 |\n| 9: OP2_TEN | 0 | $7 / 2$ | 4 | $21 / 4$ | 26/5 | $13 / 3$ | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 | 7 |\n| 10: OP1_SHIFT0 | 0 | 0 | 7 | 5 | 9 | 5 | 5 | 9 | 5 | 9 | 5 | 7 | 0 | 0 | 0 |\n| 11: OP1_SHIFT1 | 0 | 7 | 5 | 9 | 5 | 0 | 9 | 5 | 0 | 5 | 9 | 5 | 7 | 0 | 0 |\n| 12: OP1_SHIFT2 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 | 0 | 0 | 5 | 9 | 5 | 7 | 0 |\n| 13: OP1_SHIFT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 9 | 5 | 7 |\n| 14: OP1_SHIFT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 9 | 5 |\n| 15: RESULT1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 16: RESULT2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 17: RESULT3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 18: RESULT4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 19: PRE_PROD | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 20: PRE_CARRY | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 21: PRE_EOS1 | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 | 1 | 1 |\n| 22: PRE_EOS2 | 1 | 1 | 1 | 1 | 1 | 1 | $1 / 2$ | 1 | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n| 23-32: PROD | $\\mathbf{0}_{10}$ | $0_{10}$ | $0_{10}$ | $\\mathbf{0}_{10}$ | $0_{10}$ | $0_{10}$ | $0_{10}$ | $0_{10}$ | $0_{10}$ | $0_{10}$ | $0_{10}$ | $\\mathbf{0}_{10}$ | $0_{10}$ | $\\mathbf{0}_{10}$ | $0_{10}$ |\n| 33: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| $35-(P+34):$ POS_2_MASK | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $0_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $(P+35)-(2 P+34):$ POS_1 | $\\mathbf{0}_{P}$ | $v_{3}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $v_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $v_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ | $\\boldsymbol{v}_{1}^{P}$ |\n| $(2 P+35)-(3 P+34):$ POS_2 | $\\mathbf{0}_{P}$ | $v_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $v_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $v_{2}^{P}$ |\n| $(3 P+35)-(4 P+34):$ POS_3 | $\\mathbf{0}_{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $v_{3}^{P}$ |\n| $(4 P+35)-(5 P+34):$ POS_ 4 | $\\mathbf{0}_{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $v_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $v_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS_5 | $\\mathbf{0}_{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{11}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{11}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ |\n\n## F. 8 Transformer Block 2 - Token-wise Feed-forward Layer\n\nAs mentioned in the theorem statement below, we allow $\\mathrm{FF}_{2}$ to be a multi-layer MLP.",
    "posicoupling-61": "Our ultimate goal is to fill the dimensions PROD and IS_EOS with appropriate values. The dimensions RESULT1 to RESUlT4, PRE_PROD, and PRE_CARRY serve as temporary memories for storing intermediate values, which will help us achieve our ultimate goal. Our construction involves sequentially stacking the MLP networks step-by-step to generate each of these temporary values. While our current construction for $\\mathrm{FF}_{2}$ involves multiple hidden layers, we believe that our construction can be improved to employ a single hidden layer. If employing multiple hidden layers in the FFN is not feasible, this\nissue can be addressed by introducing additional Transformer blocks. Specifically, we can bypass the attention layer in these extra blocks by residual connection and only utilize their FFNs. Step 1. Filling result_1 to result_ $\\mathbf{4}$ Here, we first assume the existence of a single-hidden-layer MLP network, denoted as $\\bar{f}: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}$, such that given any integers $a, b \\in\\{0,1, \\ldots, 9\\}, f(a, b)$ equals to their multiplication, i.e., $a b$. Such a network can be implemented with 100 hidden nodes (Zhang et al., 2021). Recalling Appendix F.4, we construct the first MLP network by utilizing eight instances of the function $f$ in parallel as follows:\n\n1. RESULT $1=f\\left(\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 0, \\mathrm{OP} 2 \\_\\mathrm{ONE}\\right)+f\\left(\\mathrm{OP} 1 \\_\\right.$SHIFT1, OP $2 \\_$TEN $) \\in\\{0,1, \\ldots, 162\\}$,\n2. $\\mathrm{RESULT} 2=f\\left(\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 1, \\mathrm{OP} 2 \\_\\mathrm{ONE}\\right)+f\\left(\\mathrm{OP} 1 \\_\\right.$SHIFT2, OP $\\left.2 \\_\\mathrm{TEN}\\right) \\in\\{0,1, \\ldots, 162\\}$,\n3. RESULT $3=f\\left(\\mathrm{OP} 1 \\_\\mathrm{SHIFT} 2, \\mathrm{OP} 2 \\_\\mathrm{ONE}\\right)+f\\left(\\mathrm{OP} 1 \\_\\right.$SHIFT3, $\\left.\\mathrm{OP} 2 \\_\\mathrm{TEN}\\right) \\in\\{0,1, \\ldots, 162\\}$,\n4. $\\operatorname{RESULT} 4=f\\left(\\mathrm{OP} 1 \\_\\right.$SHIFT3, OP2_ONE $)+f\\left(\\mathrm{OP} 1 \\_\\right.$SHIFT4, OP2_TEN $) \\in\\{0,1, \\ldots, 162\\}$. ## Step 2. Filling PRE_PROD and PRE_CARRY\n\nHere, we assume the existence of the following three single-hidden-layer MLP networks, denoted as $g_{1}, g_{2}, g_{3}$ : $\\mathbb{R} \\rightarrow \\mathbb{R}$, such that given any at most 3 -digit integer $a \\in\\{0,1, \\ldots, 162\\}, g_{1}(a), g_{2}(a)$ and $g_{3}(a)$ output the ones, tens, and hundreds digit of $a$, respectively. Similarly to the previous step, each network can be implemented with 163 hidden nodes (Zhang et al., 2021). Recalling Appendix F.4, we construct the second MLP network on top of the first MLP network, by utilizing 2 instances of each of the function $g_{1}, g_{2}$, and $g_{3}$ in parallel as follows:\n\n- $\\mathrm{PRE}_{-} \\mathrm{PROD}=g_{1}(\\operatorname{RESULT} 1)+g_{2}(\\operatorname{RESULT} 2)+g_{3}(\\operatorname{RESULT} 3) \\in\\{0,1, \\ldots, 27\\}$\n- $\\operatorname{PRE} \\_$CARRY $=g_{1}(\\operatorname{RESULT} 2)+g_{2}(\\operatorname{RESULT} 3)+g_{3}(\\operatorname{RESULT} 4) \\in\\{0,1, \\ldots, 27\\}$. Step 3. Filling Prod Here, we assume the existence of a single-hidden-layer MLP network, denoted as $h$ : $\\mathbb{R}^{2} \\rightarrow \\mathbb{R}$, such that given any integers $a \\in\\{0,1, \\ldots, 27\\}, b \\in\\{0,1, \\ldots, 9\\}$ satisfying $a-b \\in\\{-2,-1,0,8,9,10,18,19,20\\}$ $h$ satisfies\n\n$$\nh(a, b)= \\begin{cases}0, & \\text { if } a-b \\in\\{-2,-1,0\\} \\\\ 1, & \\text { if } a-b \\in\\{8,9,10\\} \\\\ 2, & \\text { if } a-b \\in\\{18,19,20\\}\\end{cases}\n$$\n\nWe also assume the existence of a single-hidden-layer MLP network, denoted as $h^{\\prime}: \\mathbb{R} \\rightarrow \\mathbb{R}$, such that given any integer $a \\in\\{0,1, \\ldots, 19\\}, h^{\\prime}(a)$ equals to $a(\\bmod 10)$. We finally assume the existence of a single-hidden-layer MLP network $q_{i}: \\mathbb{R} \\rightarrow \\mathbb{R}$ for each $i \\in\\{0,1, \\ldots, 9\\}$, such that given any integers $a \\in\\{0,1, \\ldots, 9\\}, q_{i}$ satisfies\n\n$$\nq_{i}(a)=\\mathbb{1}(i=a)\n$$\n\nSimilarly to the previous step, each network can be implemented with 280, 20, and 10 hidden nodes. Recalling Appendix F.4, we construct the third MLP network, on top of the second MLP network, by\n\n$$\n\\operatorname{PROD}=\\left(\\begin{array}{c}\nq_{0}\\left(h^{\\prime}\\left(\\mathrm{PRE} \\_\\mathrm{PROD}+h\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}, \\mathrm{NUM}\\right)\\right)\\right) \\\\\nq_{1}\\left(h^{\\prime}\\left(\\mathrm{PRE} \\_\\mathrm{PROD}+h\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}, \\mathrm{NUM}\\right)\\right)\\right) \\\\\n\\vdots \\\\\nq_{9}\\left(h^{\\prime}\\left(\\mathrm{PRE} \\_\\mathrm{PROD}+h\\left(\\mathrm{PRE} \\_\\mathrm{CARRY}, \\mathrm{NUM}\\right)\\right)\\right)\n\\end{array}\\right) \\in \\mathbb{R}^{10}\n$$\n\nOne can easily check that $h^{\\prime}$ (PRE_PROD $+h($ PRE_CARRY, NUM $\\left.)\\right)$ yields an element of $0,1, \\ldots, 9$, and thus PROD is an one-hot column vector. Specifically, if $h^{\\prime}($ PRE_PROD $+h$ (PRE_CARRY, NUM $\\left.)\\right)=i$, then PROD becomes $\\boldsymbol{e}_{i+1}^{10}$. Step 4. Filling IS_EOS We construct a single-hidden-layer MLP network $r: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}$ by\n\n$$\nr(a, b)=2 \\phi(a+b-1.5)\n$$\n\nWe then can fill the dimension IS_EOS by\n\n- IS_EOS $=r($ PRE_EOS1, PRE_EOS2). Since PRE_EOS1 and PRE_EOS2 can have either $1 / 2$ or 1, IS_EOS equals 1 only when both PRE_EOS1 and PRE_EOS2 are 1. Additionally, we note that PRE_EOS1 and PRE_EOS2 are the direct outputs from the attention layer. Therefore, the network $r$ can be deployed in parallel with the first MLP network and does not require an additional FFN layer. The example output resulting from passing through all these steps is presented in Table 86. Table 86: Example output of FFN layer in the second Transformer block, continuing from Table 85. Here, we mark - for the entries before the equal token, as these entries do not affect the next-token prediction in our construction and are thus not important. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 15: RESULT1 | - | - | - | - | - | - | - | - | 45 | 116 | 108 | 98 | 49 | 0 | 0 |\n| 16: RESULT2 | - | - | - | - | - | - | - | - | 0 | 45 | 116 | 108 | 98 | 49 | 0 |\n| 17: RESULT3 | - | - | - | - | - | - | - | - | 0 | 0 | 45 | 116 | 108 | 98 | 49 |\n| 18: RESULT4 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 45 | 116 | 108 | 98 |\n| 19: PRE_PROD | - | - | - | - | - | - | - | - | 5 | 10 | 9 | 9 | 19 | 4 | 0 |\n| 20: PRE_CARRY | - | - | - | - | - | - | - | - | 0 | 5 | 10 | 9 | 9 | 19 | 4 |\n| 23-32: PROD | - | - | - | - | - | - | - | - | $\\boldsymbol{e}_{6}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ | $\\boldsymbol{e}_{7}^{10}$ | $\\boldsymbol{e}_{1}^{10}$ |\n| 33: IS_EOS | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n\n## F.8.1 Residual Connection\n\nThe last task of the feed-forward layer is to pass $\\mathrm{FF}_{2}\\left(\\boldsymbol{Y}^{(2)}\\right)$ through the residual connection. As a result, we have\n\n$$\n\\boldsymbol{X}^{(2)}=\\boldsymbol{Y}^{(2)}+\\mathrm{FF}_{2}\\left(\\boldsymbol{Y}^{(2)}\\right)\n$$\n\nThis is the end of the second Transformer block, and an example of $\\boldsymbol{X}^{(2)}$ is illustrated in Table 87. ## F. 9 Decoding Function\n\nAs mentioned in Appendix C, the decoding function performs a linear readout (with a weight matrix $\\boldsymbol{W}_{\\text {out }} \\in$ $\\mathbb{R}^{|\\mathcal{V}| \\times d}$ ) and a (token-wise) arg-max operation. That is,\n\n$$\n\\operatorname{Dec}\\left(\\boldsymbol{X}^{(1)}\\right):=\\left(\\mathcal{V}_{k_{i}}\\right)_{i=1, \\ldots, N} \\in \\mathcal{V}^{N}\n$$\n\nwhere $\\mathcal{V}_{k}$ is the $k$-th element of $\\mathcal{V}$ and\n\n$$\nk_{i}:=\\underset{k \\in[|\\mathcal{V}|]}{\\arg \\max }\\left\\{o_{k}: \\boldsymbol{W}_{\\text {out }} \\boldsymbol{X}_{\\bullet i}^{(1)}=\\left[\\begin{array}{lll}\no_{1} & \\cdots & o_{|\\mathcal{V}|}\n\\end{array}\\right]^{\\top}\\right\\}\n$$\n\nThe objective of the decoding function is to perform a proper next-token prediction for $N \\times 2$ multiplication, especially utilizing the dimensions PROD and IS _EOS of $\\boldsymbol{X}^{(2)}$. We now construct the weight matrix $\\boldsymbol{W}_{\\text {out }}$. For a token $\\sigma_{i}$, if the value of dimension IS _EOS of $\\boldsymbol{X}^{(2)}$ is 0 , then the linear readout output the dimensions PROD as it is to return one of a number token (0-9). On the other hand, if the value of dimension IS _EOS is 1 , then the linear readout outputs a large number (like 9 for example)\n\nTable 87: Example embedding matrix after the second Transformer block.",
    "posicoupling-62": "The yellow rows represent the results introduced during the second block, while the gray rows indicate the results from the first block. Similarly to Table 85, we mark - for the entries before the equal token, as these entries do not affect the next-token prediction in our construction and are thus not important. | I | \\$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1: NUM | 0 | 7 | 5 | 9 | 5 | 0 | 7 | 9 | 0 | 5 | 0 | 0 | 0 | 0 | 6 |\n| 2: FULL_ONES | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 3: is_bOS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4: Is_MUL | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5: IS_EQUAL | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6: IS_OP2_ONE | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 7: IS_OP2_TEN | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8: OP2_ONE | - | - | - | - | - | - | - | - | 9 | 9 | 9 | 9 | 9 | 9 | 9 |\n| 9: OP2_TEN | - | - | - | - | - | - | - | - | 7 | 7 | 7 | 7 | 7 | 7 | 7 |\n| 10: OP1_SHIFT0 | - | - | - | - | - | - | - | - | 5 | 9 | 5 | 7 | 0 | 0 | 0 |\n| 11: OP1_SHIFT1 | - | - | - | - | - | - | - | - | 0 | 5 | 9 | 5 | 7 | 0 | 0 |\n| 12: OP1_SHIFT2 | - | - | - | - | - | - | - | - | 0 | 0 | 5 | 9 | 5 | 7 | 0 |\n| 13: OP1_SHIFT3 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 5 | 9 | 5 | 7 |\n| 14: OP1_SHIFT4 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 5 | 9 | 5 |\n| 15: RESULt1 | - | - | - | - | - | - | - | - | 45 | 116 | 108 | 98 | 49 | 0 | 0 |\n| 16: RESULT2 | - | - | - | - | - | - | - | - | 0 | 45 | 116 | 108 | 98 | 49 | 0 |\n| 17: RESULT3 | - | - | - | - | - | - | - | - | 0 | 0 | 45 | 116 | 108 | 98 | 49 |\n| 18: RESULT4 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 45 | 116 | 108 | 98 |\n| 19: PRE_PROD | - | - | - | - | - | - | - | - | 5 | 10 | 9 | 9 | 19 | 4 | 0 |\n| 20: PRE_CARRY | - | - | - | - | - | - | - | - | 0 | 5 | 10 | 9 | 9 | 19 | 4 |\n| 21: PRE_EOS1 | - | - | - | - | - | - | - | - | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 | 1 | 1 |\n| 22: PRE_EOS2 | - | - | - | - | - | - | - | - | 1 | 1 | $1 / 2$ | $1 / 2$ | $1 / 2$ | $1 / 2$ | 1 |\n| 23-32: PROD | - | - | - | - | - | - | - | - | $e_{6}^{10}$ | $e_{1}^{10}$ | $e_{1}^{10}$ | $e_{1}^{10}$ | $e_{1}^{10}$ | $e_{7}^{10}$ | $e_{1}^{10}$ |\n| 33: IS _EOS | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n| 34: MASK | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 35- $(P+34):$ POS_2_MASK | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ | $\\mathbf{0}_{P}$ |\n| $(P+35)-(2 P+34):$ POS $\\_1$ | $\\mathbf{0}_{P}$ | $v_{3}^{P}$ | $v_{4}^{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $v_{5}^{P}$ | $v_{4}^{P}$ | $v_{3}^{P}$ | $v_{2}^{P}$ | $\\boldsymbol{v}_{1}^{P}$ |\n| $(2 P+35)-(3 P+34):$ POS_2 | $\\mathbf{0}_{P}$ | $v_{4}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $\\boldsymbol{v}_{4}^{P}$ | $\\boldsymbol{v}_{3}^{P}$ | $\\boldsymbol{v}_{2}^{P}$ |\n| $(3 P+35)-(4 P+34):$ POS_3 | $\\mathbf{0}_{P}$ | $v_{5}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{7}^{P}$ | $\\boldsymbol{v}_{6}^{P}$ | $v_{5}^{P}$ | $v_{4}^{P}$ | $v_{3}^{P}$ |\n| $(4 P+35)-(5 P+34):$ POS_4 | $\\mathbf{0}_{P}$ | $v_{6}^{P}$ | $v_{7}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $v_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $\\boldsymbol{v}_{5}^{P}$ | $v_{4}^{P}$ |\n| $(5 P+35)-(6 P+34):$ POS_5 | $\\mathbf{0}_{P}$ | $v_{7}^{P}$ | $v_{8}^{P}$ | $\\boldsymbol{v}_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $\\boldsymbol{v}_{11}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $v_{11}^{P}$ | $\\boldsymbol{v}_{10}^{P}$ | $v_{9}^{P}$ | $\\boldsymbol{v}_{8}^{P}$ | $v_{7}^{P}$ | $v_{6}^{P}$ | $v_{5}^{P}$ |\n\nfor the token ' $\\$$ ' to return $\\operatorname{EOS}(\\$)$.",
    "posicoupling-63": "This can be implemented by the weight matrix $\\boldsymbol{W}_{\\text {out }}$ described in Table 88 . Also, an example of applying the linear transform is showcased in Tables 89 and 90. Table 88: The transposed weight matrix $\\boldsymbol{W}_{\\text {out }}^{\\top}$ of the linear readout in decoding function. $P^{\\prime}$ represents $6 P+1$. | $\\mathcal{V}$ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | $\\times$ | $=$ | \\$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1-22: NUM-PRE_EOS_2 | $\\mathbf{0}_{22}$ | $0_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $0_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ | $\\mathbf{0}_{22}$ |\n| 23: $\\mathrm{PROD}_{1}$ | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 24: $\\mathrm{PROD}_{2}$ | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 25: $\\mathrm{PROD}_{3}$ | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 26: $\\mathrm{PROD}_{4}$ | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 27: $\\mathrm{PROD}_{5}$ | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 28: $\\mathrm{PROD}_{6}$ | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 29: $\\mathrm{PROD}_{7}$ | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 30: $\\mathrm{PROD}_{8}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n| 31: $\\mathrm{PROD}_{9}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n| 32: $\\mathrm{PROD}_{10}$ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n| 33: IS_EOS | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 100 |\n| 34-end | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ | $\\mathbf{0}_{P^{\\prime}}$ |\n\nTable 89: Example output of linear readout $\\left(\\boldsymbol{W}_{\\text {out }} \\boldsymbol{X}^{(2)}\\right.$ ), continuing from Tables 87 and 88. The yellow cells represent the maximum value of each column, from the ${ }^{\\prime}=$ ' token's column to the rightmost column (which are used for next-token prediction). | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 0 | - | - | - | - | - | - | - | - | 0 | 1 | 1 | 1 | 1 | 0 | 0 |\n| 1 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | - | - | - | - | - | - | - | - | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n| 7 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 8 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 9 | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| $\\times$ | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| $=$ | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| $\\$$ | - | - | - | - | - | - | - | - | 0 | 0 | 0 | 0 | 0 | 0 | 9 |\n\nTable 90: Example output sequence $\\mathcal{O}=\\operatorname{Dec}\\left(\\boldsymbol{X}^{(2)}\\right)$, continuing from Table 89. The yellow cells in the bottom row exactly predict the next tokens. | $\\mathcal{I}$ | $\\$$ | 7 | 5 | 9 | 5 | $\\times$ | 7 | 9 | $=$ | 5 | 0 | 0 | 0 | 0 | 6 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| $\\mathcal{O}$ | - | - | - | - | - | - | - | - | 5 | 0 | 0 | 0 | 0 | 6 | $\\$$ |\n\n\n[^0]:    *Authors contributed equally to this paper. ${ }^{1}$ Our codebase is available at github.com/HanseulJo/position-coupling. [^1]:    ${ }^{2}$ It explicitly determines the maximum testable length of sequence that a transformer can handle. In the case of the decimal integer addition task, the maximum possible generalizable length is max_pos -2 . Models trained with larger max_pos can handle longer sequences; however, training becomes more difficult as the frequency of each position ID appearing during training decreases. [^2]:    ${ }^{3}$ Note that they match up to matrix transpose, which is due to the difference in the formulations. ${ }^{4}$ Note that the attention matrices depicted in Figure 7 are square, lower-triangular (due to causal attention pattern), and row-stochastic (all entries are nonnegative and the sum of each row equals 1). [^3]:    ${ }^{5}$ github.com/google/flaxformer\n    6 github.com/McGill-NLP/length-generalization\n\n[^4]:    ${ }^{7}$ One can let $d_{H}=\\max _{l, h} \\max \\left\\{d_{Q K, h}^{(l)}, d_{V, h}^{(l)}\\right\\}$ as an inner dimension of each head.",
    "posicoupling-64": "This makes our formal constructions a bit messier with redundant entries 0 . ${ }^{8}$ Every entry is non-negative and the sum of entries in each column is 1. [^5]:    ${ }^{9} \\mathrm{BOS}$ and EOS tokens do not need to be identical. We regard them as the same token just for the simplicity of the presentation. [^6]:    ${ }^{10}$ The choice of the vectors $\\boldsymbol{v}_{k}^{D}$ is not strict. They only need to have the same length and be distinguishable (for at least a constant order) in terms of inner products. That is, there should be a noticeable difference between $\\left\\|\\boldsymbol{v}_{k}^{D}\\right\\|^{2}$ and $\\left\\langle\\boldsymbol{v}_{k}^{D}, \\boldsymbol{v}_{l}^{D}\\right\\rangle$ for $k \\neq l$. ${ }^{11}$ Such an idea of filling in the blacks of the encoding matrix is borrowed from the literature of RASP language(s) (Friedman et al., 2023; Lindner et al., 2023; Weiss et al., 2021; Zhou et al., 2024a).",
    "posicoupling-65": "This can be done with the help of residual connections.",
    "posicoupling-66": ""
}