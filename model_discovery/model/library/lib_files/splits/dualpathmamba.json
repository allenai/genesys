{
    "dualpathmamba-0": "# DUAL-PATH MAMBA: SHORT AND LONG-TERM BIDIRECTIONAL SELECTIVE STRUCTURED STATE SPACE MODELS FOR SPEECH SEPARATION \n\nXilin Jiang, Cong Han, Nima Mesgarani<br>Department of Electrical Engineering, Columbia University, USA\n\n\n#### Abstract\n\nTransformers have been the most successful architecture for various speech modeling tasks, including speech separation.",
    "dualpathmamba-1": "However, the self-attention mechanism in transformers with quadratic complexity is inefficient in computation and memory. Recent models incorporate new layers and modules along with transformers for better performance but also introduce extra model complexity. In this work, we replace transformers with Mamba, a selective state space model, for speech separation. We propose dual-path Mamba, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces. Our experimental results on the WSJ0-2mix data show that our dual-path Mamba models of comparably smaller sizes outperform state-of-the-art RNN model DPRNN, CNN model WaveSplit, and transformer model Sepformer. Code: https://github.com/xi-j/Mamba-TasNet\n\n\nIndex Terms- Speech separation, source separation, speech sequence modeling, state space model, deep learning\n\n## 1. INTRODUCTION\n\nSpeech is an inherently long signal of thousands of samples. Speech separation models separate multiple individual speech, each of thousands of samples, from a single overlapped speech mixture of the same length. Therefore, an effective mechanism for modeling long speech sequences is crucial to achieving high performance in speech separation. With the advancement of deep learning, a variety of neural sequence modeling architectures, including convolutional neural networks (CNN) [1], recurrent neural networks (RNN) [2], and transformers [3], have been adopted into speech separation models and achieved a state-of-the-art (SOTA) performance at the time they were adopted [4, 5, 6]. However, each architecture faces its own challenges: CNNs are restricted by a finite receptive field; RNNs are hard to parallelize and struggle with vanishing or exploding gradients [7]; Transformers often outperform CNNs and RNNs but require quadratic computation in computing self-attention [3]. Although alternative self-attention mechanisms with sub-quadratic complexity exist, they fail to match the performance of the original attention mechanism in speech separation [8]. Meanwhile, the attempt to scale up transformer parameters, as seen from Sepformer [6] to almost eight times larger QDPN [9], yields a modest performance improvement, suggesting diminishing returns with increasing model size. Recently, MossFormer2, which integrates transformers and RNN-free recurrent networks, has achieved better performance but is much smaller than QDPN, hinting that speech separation might still benefit from exploring beyond transformer architectures. State space model (SSM) [10, 11, 12] is another unique class of sequence modeling architecture. Recently, a novel selective SSM named Mamba is proposed [13]. Distinct from earlier SSMs, Mamba incorporates an input-dependent selection mechanism that improves sequence modeling performance but still enjoys linear complexity with respect to the sequence length. Mamba models have matched and even surpassed transformers of comparable sizes in sequence modeling tasks of text, audio, image, and genomics (13, 14]. Inspired by Mamba's effectiveness and efficiency in sequence modeling, we introduce it into speech separation and propose a new model named dual-path Mamba (DPMamba). As the name suggests, we follow the long sequence modeling method in dual-path RNN [5] to split a long speech into multiple short chunks and apply Mamba models within each chunk, across all chunks, in the original direction, and in the reversed direction of time. Our experiments of DPMamba of three different sizes (XS, S, M) on the WSJO-2mix dataset [15] demonstrate on-par or superior performance over SOTA models of similar or larger sizes, including the CNN-based Wavesplit [16], the RNN-based DPRNN [5] and VSUNOS [17], and the transformer-based DPTNet [18] and SepFormer [6]. We are scaling up the model parameters and will report the performance of our largest model (L) soon. ## 2. RELATED WORKS\n\n### 2.1. Speech Separation\n\nFollowing the shift to adaptive front-ends over the short-time Fourier transform for waveform encoding [19, 20], time-domain speech separation models have become the mainstream of research. The core of these models is a mask estimation network which consists of multiple separation blocks of the same architecture. The architecture of these blocks plays a crucial role in separation performance and has thus become a primary focus of research efforts. CNN and RNN models were first introduced, succeeded by better transformer and hybrid models. We spotlight models that have set benchmarks in speech separation across different architecture families and will compare them with our mamba separation models in Section 4\nCNN: Conv-TasNet [4] first outperforms ideal time-frequency masking in speech separation. It uses temporal convolutional networks (TCN) with progressively increasing dilation factors to expand its receptive field. Sudo rm -rf [21, 22] employs a more efficient CNN architecture with down-sampling layers to expand the receptive field. Wavesplit [16] builds on TCNs but introduces additional speaker embeddings, which leads to a better performance. RNN: An earliest time-domain model TasNet [20] utilizes long short-term memory (LSTM) to separate speech. To improve the modeling ability of long sequence, DPRNN [5] proposes splitting a speech sequence into smaller chunks and utilizing two LSTMs for intra-chunk and inter-chunk processing. VSUNOS [17] outperforms DPRNN by substituting LSTMs with gated RNNs. Transformer: DPTNet [18] and Sepformer [6] adopt the dual-path architecture of DPRNN and replace LSTMs with transformers. They differ in the stride size and the use of a RNN or positional encoding\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2f0eb3a5a67c377baa45g-2.jpg?height=732&width=1602&top_left_y=249&top_left_x=276)\n\nFig. 1. A top-down view of DPMamba from I to IV. to embed the order information of a speech sequence. QDPN (9] proposes an improved quasi-dual-path architecture and scales up parameters to 200M. This marks the best performance of the largest transformer model in WSJ0-2mix. Hybrid: More recent separation models employ a hybrid architecture which often augments a vanilla transformer using another model, like convolution-augmented transformers in TD-Conformer [23], gated transformer with convolution-augmented attentions in Mossformer [24], and transformer and RNN-free recurrent network in Mossformer2 [25]. Separate and Diffuse [26] adds a generative model to post-refine the speeches separated by Sepformer. ### 2.2. Applications of Mamba\n\nMamba has been proven a transformer-level performance in multiple modalities and tasks. Mamba is first applied to text, audio, and genomics modeling tasks in its own paper [13]. Later applications extend to images [14], biomedical data [27], graphs [28], motions [29], videos [30], and point clouds [31]. For speech, [32] adopts Mamba for multi-channel speech enhancement. [33] adopts S4 (the predecessor of Mamba) [12] for single-channel speech separation. To our knowledge, ours is the first work that adopts Mamba in singlechannel speech separation. ## 3. DPMAMBA\n\nIn this section, we will first review the selective state space model which is the core component of Mamba. Then, we will explain DPMamba in a top-down way with Fig. 1 We focus on our adoption of Mamba (purple parts in Fig. 1] into separation since the rest is commonly shared with other dual-path separation models. ### 3.1. Selective State Space Model\n\nA state space model (SSM) performs a sequence-to-sequence mapping $\\mathbf{x}(t) \\in \\mathbb{R} \\mapsto \\mathbf{y}(t) \\in \\mathbb{R}$ with a hidden state $\\mathbf{h} \\in \\mathbb{R}^{H}$, a state transition matrix $\\mathbf{A} \\in \\mathbb{R}^{H \\times H}$, an input projection matrix $\\mathbf{B} \\in \\mathbb{R}^{H \\times 1}$, and an output projection matrix $\\mathbf{C} \\in \\mathbb{R}^{1 \\times H} . H$ is the dimension of the hidden state:\n\n$$\n\\mathbf{h}^{\\prime}(t)=\\mathbf{A h}(t)+\\mathbf{B x}(t), \\quad \\mathbf{y}(t)=\\mathbf{C h}(t)\n$$\n\nTo compute Eq 1 for discrete-time signals in digital devices, we need to discretize the SSM with discretized matrices $\\overline{\\mathbf{A}}$ and $\\overline{\\mathbf{B}}$ :\n\n$$\n\\mathbf{h}_{t}=\\overline{\\mathbf{A}} \\mathbf{h}_{t-1}+\\overline{\\mathbf{B}} \\mathbf{x}_{t}, \\mathbf{y}_{t}=\\mathbf{C} \\mathbf{h}_{t}\n$$\n\n$\\overline{\\mathbf{A}}$ and $\\overline{\\mathbf{B}}$ are approximated by zero-order hold. A learnable parameter $\\Delta$ balances how much to focus or ignore the current state and input:\n\n$$\n\\overline{\\mathbf{A}}=\\exp (\\Delta \\mathbf{A}), \\overline{\\mathbf{B}}=(\\Delta \\mathbf{A})^{-1}(\\exp (\\Delta \\mathbf{A})-I) \\cdot \\Delta \\mathbf{B}\n$$\n\nBy unrolling the computation in Eq 2 along the sequence, it can be observed that the output sequence $\\mathbf{y}_{t}$ is the input sequence $\\mathbf{x}_{t}$ convolved with a structured kernel $\\overline{\\mathbf{K}}$ made up by $\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}$, and $\\mathbf{C}$ :\n\n$$\n\\overline{\\mathbf{K}}=\\left(C \\overline{\\mathbf{B}}, \\overline{\\mathbf{A B}}, \\ldots, C \\overline{\\mathbf{A}}^{L-1} \\overline{\\mathbf{B}}\\right), \\mathbf{y}=\\mathbf{x} * \\overline{\\mathbf{K}}\n$$\n\nThe kernel $\\overline{\\mathbf{K}}$ can be pre-computed if $\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}$, and $\\mathbf{C}$ are fixed. However, the SSM in Mamba is input selective to enable input contentawareness: It updates the parameters $\\Delta_{t}$ (and therefore $\\overline{\\mathbf{A}}_{t}$ ), $\\overline{\\mathbf{B}}_{t}$, and $\\mathbf{C}_{t}$ based on the input $\\mathbf{x}_{t}$ at each timestep $t$. To accommodate these dynamic updates, [13] proposes an efficient selective scan algorithm for the calculation of Eq 4\n\n### 3.2. Time-domain Dual-path Model\n\nDPMamba follows the time-domain dual-path structure of previous SOTA separation models [5, 6, 24]. A linear encoder encodes a single-channel wave mixture $x \\in \\mathbb{R}^{1 \\times T}$ of $T$ samples to a twodimensional latent representation $h_{x} \\in \\mathbb{R}^{D \\times N}$ of $D$ dimensions and $N$ frames. The encoder has a kernel size of 16 and a stride of 8 , resulting in $N=T / 8$. The encoder dimension $D$ is a hyperparameter. Next, for two speakers in the mixture, a Mamba masking network (Mamba MaskNet) estimates two masks $m_{1}, m_{2} \\in \\mathbb{R}^{D \\times N}$. These masks are then applied through element-wise multiplication with $h_{x}$\nto isolate individual speech sources $h_{\\hat{s}_{1}}, h_{\\hat{s}_{2}} \\in \\mathbb{R}^{D \\times N}$. Finally, a linear decoder decodes $h_{\\hat{s}_{1}}, h_{\\hat{s}_{2}}$ back to waveforms $\\hat{s}_{1}, \\hat{s}_{2} \\in \\mathbb{R}^{1 \\times T}$.",
    "dualpathmamba-2": "Fig. 1 I draws the same workflow. Mamba MaskNet is a dual-path network that operates on a threedimensional tensor $h_{0} \\in \\mathbb{R}^{D \\times K \\times S}$. $h_{0}$ is obtained by Chunking $N$ frames of $h_{x}$ into multiple smaller chunks. $K=250$ is the chunk size, and $S$ is the resulting number of chunks, with an overlap factor of $50 \\%$ between adjacent chunks. Mamba MaskNet processes withinand across-chunk sequences of much smaller lengths $K$ and $S$ than $N$, which are two substantially easier sequence modeling tasks [5]. The output of Mamba MaskNet, $h_{R} \\in \\mathbb{R}^{D \\times K \\times S}$, is converted back to two-dimensional by OverlapAdd operation. Mask $m_{1}$ and $m_{2}$ are estimated by a linear layer followed. Additional linear layers are placed before Chunking and OverlapAdd operation, as drawn in Fig. 1 II. Note that we use the same chunk size, kernel size, and stride as other dual-path models like Sepformer [6] and Mossformer [24, 25] for a fair comparison. ### 3.3. Dual-path Bidirectional Mamba\n\nMamba MaskNet comprises a stack of $R$ dual-path (DP) blocks iteratively processing the chunked features $h_{0}$, resulting in $h_{1}$ to $h_{R}$ of the same shape. Within each DP block, four SSMs process the features in four different ways: intra-chunk forward, intra-chunk backward, inter-chunk forward, and inter-chunk backward. An intrachunk SSM locally processes all $K$ frames within a chunk; An interchunk SSM globally processes all $S$ chunks of the entire signal. A forward SSM processes in the original direction; A backward SSM processes in the opposite direction of the sequence. As shown in Fig. 1 III and IV, a DP block contains an intra-chunk and an inter-chunk unit interleaved, and an intra-chunk or inter-chunk unit each contains a forward and a backward SSM running in parallel. ### 3.3.1. Local and Global\n\nWe adopt computation units of the same structure for local and global processing as shown in Fig. 11III. Both intra-chunk and interchunk units contain a normalization layer, a bidirectional Mamba (BiMamba), and a skip connection. By default, we use RMSNorm [34] as the normalization method due to its computational efficiency. An intermediate feature $h_{i} \\in \\mathbb{R}^{D \\times K \\times S}$ is first processed by the intra-chunk unit across $K$ frames in every chunk and then processed by the inter-chunk unit across $S$ chunks in total. The computation in $i$ th DP block can be expressed as follows:\n\n$$\n\\begin{aligned}\nh_{i}^{\\prime} & =h_{i}+\\overleftarrow{\\operatorname{BiMamb} a}\\left(\\operatorname{Norm}\\left(h_{i}\\right)\\right) \\\\\nh_{i+1} & =h_{i}^{\\prime}+\\overleftarrow{\\operatorname{BiMamb} a}\\left(\\operatorname{Norm}\\left(h_{i}^{\\prime}\\right)\\right)\n\\end{aligned}\n$$\n\nThe intra-chunk duration is around 30 ms with our default configuration for 8 kHz speech. In implementation, we permute the last two dimensions $K$ and $S$ of $h_{i}\\left(h_{i}^{\\prime}\\right)$ to alter between intra-chunk or inter-chunk processing. ### 3.3.2. Forward and Backward\n\nThe original Mamba [13] uses one SSM to process the input sequence in the forward direction. However, it has been shown that bidirectional models, which utilize future context, usually outperform unidirectional models in speech separation [4]. To make Mamba bidirectional, we borrow the BiMamba design from [14] to run one forward and one backward SSM in parallel, as shown in Fig. Table 1. Hyperparameters of DPMamba XS, S, M, and L. $R \\times 2$ means $R$ DP blocks and each block consists of one intra-chunk and one inter-chunk BiMamba unit. | Model | Dimension $D$ | \\#Layers | \\#Params (M) |\n| :---: | :---: | :---: | :---: |\n| DPMamba (XS) | 128 | $8 \\times 2$ | 2.3 |\n| DPMamba (S) | 256 | $8 \\times 2$ | 8.1 |\n| DPMamba (M) | 256 | $16 \\times 2$ | 15.9 |\n\n1 IV. For clarity of discussion, we omit the subscripts used in the previous sections and denote the input and output of BiMamba as $h$ and $h^{+}$, respectively. We start with an input sequence $h \\in \\mathbb{R}^{D \\times L}$, where the dimension $D$ is the same as the encoder dimension and the length $L \\in\\{K, S\\}$ is either the number of frames per chunk or the number of chunks. A linear layer projects $h$ to $h^{\\rightarrow} \\in \\mathbb{R}^{E \\times L}$, where $E=2 D$ is the dimension expanded by $2 . h^{\\leftarrow}$ is $h^{\\rightarrow}$ with $L$ samples flipped. Meanwhile, another linear layer projects $h$ to $z \\in \\mathbb{R}^{E \\times L}$ in order to later gate SSM outputs:\n\n$$\n\\begin{aligned}\nh^{\\rightarrow} & =\\text { Linear }_{i}(h), h^{\\leftarrow}=\\operatorname{Flip}\\left(h^{\\prime}\\right) \\\\\nz & =\\text { Linear }_{g}(h)\n\\end{aligned}\n$$\n\nThe forward and the backward sequence $h^{\\rightarrow}$ and $h^{\\leftarrow}$ are then processed by their own convolution followed by activation layers, resulting in $\\mathbf{x}^{\\rightarrow}$ and $\\mathbf{x}^{+}$. The convolution has a kernel size of 4 , and the activation $\\sigma$ is the Sigmoid Linear Unit (SiLU) [35, 36]:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2f0eb3a5a67c377baa45g-3.jpg?height=117&width=364&top_left_y=1291&top_left_x=1333)\n$\\mathbf{x}^{\\rightarrow}$ and $\\mathbf{x}^{\\leftarrow}$ are the inputs to two SSMs detailed in Section 3.1 The SSM outputs are gated by $\\sigma(z)$ :\n\n$$\n\\left\\{\\begin{array}{l}\n\\mathbf{y}^{\\rightarrow}=\\sigma(z) \\otimes S S M_{\\rightarrow}\\left(\\mathbf{x}^{\\rightarrow}\\right) \\\\\n\\mathbf{y}^{+}=\\sigma(z) \\otimes S S M_{\\leftarrow}\\left(\\mathbf{x}^{+}\\right)\n\\end{array}\\right. $$\n\nFinally, the final output $h^{+}$is obtained by a linear projection of the average of the forward and the backward processed sequences $\\mathbf{y} \\rightarrow$ and $\\mathbf{y}^{+}$, after the latter flipped back to the original direction:\n\n$$\nh^{+}=\\operatorname{Linear}_{o}\\left(\\frac{\\mathbf{y}^{\\rightarrow}+\\operatorname{Flip}\\left(\\mathbf{y}^{+}\\right)}{2}\\right)\n$$\n\n## 4. RESULTS\n\n### 4.1. Experiments\n\nWe implemented DPMamba of three different sizes as documented in Table 1 We trained them with scale-invariant signal-to-noise ratio (SI-SNR) 37] and evaluated them with the improvement of SI-SNR (SI-SNRi) and the improvement of signal-to-distortion ratio (SDRi) on the WSJ0-2mix dataset [15]. We trained all models with a batch size of 1 , an Adam optimizer [38], a peak learning rate of $1.5 e^{-4}$, a linear learning rate warmup of 20,000 steps ( 1 epoch), a cosine learning rate decay to 0.01 from the peak, and a total of 200 epochs. We apply speed perturbation with a random ratio between $95 \\%$ and $100 \\%$ and dynamic mixing as data augmentation. Note that this training setting is similar to the one in Sepformer [6] but with a different learning rate schedule. All the experiments are conducted in a NVIDIA L40 GPU with mixed (bfloat16) precision. Table 2. A comparison of DPMamba with previous SOTA separation models from each architecture family on WSJ0-2mix. n.r. stands for not reported. | Model | SI-SNRi (dB) | SDRi (dB) | \\#Params (M) | Stride |\n| :---: | :---: | :---: | :---: | :---: |\n| CNN |  |  |  |  |\n| Conv-TasNet [4] | 15.3 | 15.6 | 5.1 | 8 |\n| Sudo rm -rf (B=36) [22] | 19.5 | n.r. | 23.2 | 10 |\n| Wavesplit [16] | 22.2 | 22.3 | 29 | 1 |\n| RNN |  |  |  |  |\n| TasNet [20] | 10.8 | 11.1 | n.r. | 20 |\n| DPRNN [5] | 18.8 | 19.0 | 2.6 | 1 |\n| VSUNOS [17] | 20.1 | 20.4 | 7.5 | 2 |\n| Transformer |  |  |  |  |\n| DPTNet [18] | 20.2 | 20.6 | 2.6 | 1 |\n| Sepformer [6] | 22.3 | 22.4 | 25.7 | 8 |\n| QDPN [9] | 23.6 | n.r. | 200 | 8 |\n| Hybrid |  |  |  |  |\n| Mossformer (L) [24] | 22.8 | n.r. | 42.1 | 8 |\n| TD-Conformer-XL [23] | 21.2 | n.r. | 102.7 | 8 |\n| Separate And Diffuse [26] | 23.9 | n.r. | n.r. | 8 |\n| Mossformer2 (L) [25] | 24.1 | n.r. | 55.7 | 8 |\n| SSM |  |  |  |  |\n| S4M-tiny [33] | 19.4 | 19.7 | 1.8 | 8 |\n| S4M [33] | 20.5 | 20.7 | 3.6 | 8 |\n| DPMamba (XS) | 19.2 | 19.4 | 2.3 | 8 |\n| DPMamba (S) | 21.4 | 21.6 | 8.1 | 8 |\n| DPMamba (M) | 22.6 | 22.7 | 15.9 | 8 |\n\n### 4.2. Performance Comparison\n\nIn Table 2, we compare the performance of our models with the performance of the SOTA models from different architecture families: CNN, RNN, Transformer, and Hybrid, which we have introduced in Section 2.1. We also include the performance of S4M [33], which adopts the $S 4$ model for separation and also belongs to the SSM family. The numbers in the table correspond to their best models trained with data augmentation, as reported in their papers. Our three DPMamba models, spanning from XS of 2.3 million parameters to M of 15.9 million parameters, achieve SI-SNRi values between 19.2 dB and 22.6 dB and SDRi values from 19.4 dB to $22.7 \\mathrm{~dB}^{1}$ Notice that DPMamba models achieve a high performance with a relatively smaller model size or larger stride, compared to models from other families. For instance, DPMamba (XS), despite having slightly fewer parameters and a stride of 8 , outperforms DPRNN, which uses a stride of 1. A larger stride means a larger down-sampling ratio by the encoder, leading to faster inference and smaller memory usage. DPMamba (S) is marginally larger than VSUNOS but achieves an SI-SNRi 1.3 dB higher, with our stride 4 times larger. DPMamba (M) outperforms Sepformer and is only 0.2 dB lower than Mossformer in SI-SNRi, however, it only requires around $60 \\%$ and $40 \\%$ of their parameters, respectively. Finally, our S and M models outperform the previous SSM model S4M, although the XS model slightly lags behind the performance of S4M-tiny. The S4M paper [33] does not implement models of comparable sizes to our $S$ and $M$, preventing a conclusive comparison across all model sizes. In Fig. 2, we benchmark the GPU memory consumption during a forward pass of the models on waveforms of different lengths. We compare our models against Sepformer and DPRNN with default model configurations implemented by Speechbrain [39]. DPMamba (M) maintains a Sepformer-level performance but reduces memory usage by $30 \\%$. DPMamba (XS) outperforms DPRNN and consumes\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_2f0eb3a5a67c377baa45g-4.jpg?height=505&width=682&top_left_y=257&top_left_x=1166)\n\nFig. 2. A comparsion of GPU memory usage of DPMamba with Sepformer and DPRNN. Table 3. Ablations on DPMamba (S). | Configuration | SI-SNRi $(\\mathrm{dB})$ | SDRi $(\\mathrm{dB})$ | \\#Params $(\\mathrm{M})$ |\n| :---: | :---: | :---: | :---: |\n| Unidirectional | 16.9 | 17.2 | 7.4 |\n| LayerNorm | 20.6 | 20.8 | 8.1 |\n| H=8 | 20.6 | 20.8 | 7.7 |\n| $\\mathrm{H}=32$ | 20.6 | 20.8 | 8.9 |\n| Without DM | 20.0 | 20.2 | 8.1 |\n| Default (S, 100 epochs) | 20.6 | 20.8 | 8.1 |\n\nonly $10 \\%$ of memory in separating 10 -second speeches. The small memory usage marks the potential of our model in mobile devices. ### 4.3. Ablations\n\nWe conducted ablation studies of the DPMamba (S) models. These involved switching from bidirectional to unidirectional models, decreasing or increasing the dimension $H$ of the hidden state of SSMs, eliminating dynamic mixing in training, and replacing RMSNorm with LayerNorm. We trained all model for 100 epochs. Our numbers in Table 3 reveal that a variation in the hidden state dimension above 8 and the choice between RMSNorm and LayerNorm have little impact on the separation performance. Training with dynamic mixing boosts both the SI-SNRi and SDRi by 0.6 dB . The most significant performance improvement, a boost of over 3 dB , comes from adding backward SSMs in the bidirectional model, compared to only using forward SSMs in the unidirectional model. ## 5. CONCLUSION\n\nIn this work, we introduce DPMamba, a new model for speech separation. DPMamba utilizes a dual-path network to model local and global aspects of speech sequences and incorporates bidirectional Mamba blocks for processing the sequences in forward and backward directions. Our models of three different sizes either meet or surpass the performance of existing CNN, RNN, and transformer models of similar or large sizes. Moving forward, we will explore two directions: enhancing the efficiency of the Mamba separation model and improving the performance by integrating Mamba with other network layers. ## 6. ACKNOWLEDGEMENT\n\nThis work was funded by the National Institutes of Health (NIHNIDCD) and a grant from Marie-Josee and Henry R. Kravis. ## 7. REFERENCES\n\n[1] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, \"Backpropagation applied to handwritten zip code recognition,\" Neural Computation, vol.",
    "dualpathmamba-3": "1, no. 4, pp. 541-551, 1989. [2] Long Short-Term Memory, \"Long short-term memory,\" Neural computation, vol.",
    "dualpathmamba-4": "9, no.",
    "dualpathmamba-5": "8, pp. 1735-1780, 2010. [3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017. [4] Yi Luo and Nima Mesgarani, \"Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation,\" IEEE/ACM transactions on audio, speech, and language processing, vol.",
    "dualpathmamba-6": "27, no.",
    "dualpathmamba-7": "8, pp. 1256-1266, 2019. [5] Yi Luo, Zhuo Chen, and Takuya Yoshioka, \"Dual-path rnn: efficient long sequence modeling for time-domain singlechannel speech separation,\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "dualpathmamba-8": "IEEE, 2020, pp. 46-50. [6] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong, \"Attention is all you need in speech separation,\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "dualpathmamba-9": "IEEE, 2021, pp. 21-25. [7] Phong Le and Willem Zuidema, \"Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs,\" in Proceedings of the 1st Workshop on Representation Learning for NLP, Phil Blunsom, Kyunghyun Cho, Shay Cohen, Edward Grefenstette, Karl Moritz Hermann, Laura Rimell, Jason Weston, and Scott Wen-tau Yih, Eds., Berlin, Germany, Aug.",
    "dualpathmamba-10": "2016, pp. 87-93, Association for Computational Linguistics. [8] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Fran\u00e7ois Grondin, and Mirko Bronzi, \"Exploring self-attention mechanisms for speech separation,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 21692180, 2022. [9] Joel Rixen and Matthias Renz, \"QDPN - Quasi-dual-path Network for single-channel Speech Separation,\" in Proc.",
    "dualpathmamba-11": "Interspeech 2022, 2022, pp. 5353-5357. [10] Rudolph Emil Kalman, \"A new approach to linear filtering and prediction problems,\" 1960. [11] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9, \"Combining recurrent, convolutional, and continuous-time models with linear state space layers,\" Advances in neural information processing systems, vol. 34, pp. 572-585, 2021. [12] Albert Gu, Karan Goel, and Christopher R\u00e9, \"Efficiently modeling long sequences with structured state spaces,\" in The International Conference on Learning Representations (ICLR), 2022. [13] Albert Gu and Tri Dao, \"Mamba: Linear-time sequence modeling with selective state spaces,\" arXiv preprint arXiv:2312.00752, 2023. [14] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang, \"Vision mamba: Efficient visual representation learning with bidirectional state space model,\" arXiv preprint arXiv:2401.09417, 2024. [15] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, \"Deep clustering: Discriminative embeddings for segmentation and separation,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 31-35. [16] Neil Zeghidour and David Grangier, \"Wavesplit: End-to-end speech separation by speaker clustering,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2840-2849, 2021. [17] Eliya Nachmani, Yossi Adi, and Lior Wolf, \"Voice separation with an unknown number of multiple speakers,\" in International Conference on Machine Learning. PMLR, 2020, pp. 7164-7175. [18] Jingjing Chen, Qirong Mao, and Dong Liu, \"Dual-Path Transformer Network: Direct Context-Aware Modeling for End-toEnd Monaural Speech Separation,\" in Proc. Interspeech 2020, 2020, pp. 2642-2646. [19] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis, \"End-to-end source separation with adaptive front-ends,\" in 2018 52nd asilomar conference on signals, systems, and computers. IEEE, 2018, pp. 684-688. [20] Yi Luo and Nima Mesgarani, \"Tasnet: time-domain audio separation network for real-time, single-channel speech separation,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 696700. [21] Efthymios Tzinis, Zhepei Wang, and Paris Smaragdis, \"Sudo rm-rf: Efficient networks for universal audio source separation,\" in 2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2020, pp. 1-6. [22] Efthymios Tzinis, Zhepei Wang, Xilin Jiang, and Paris Smaragdis, \"Compute and memory efficient universal sound source separation,\" Journal of Signal Processing Systems, vol.",
    "dualpathmamba-12": "94, no.",
    "dualpathmamba-13": "2, pp. 245-259, 2022. [23] William Ravenscroft, Stefan Goetze, and Thomas Hain, \"On time domain conformer models for monaural speech separation in noisy reverberant acoustic environments,\" in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023, pp. 1-7. [24] Shengkui Zhao and Bin Ma, \"Mossformer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions,\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1-5. [25] Shengkui Zhao, Yukun Ma, Chongjia Ni, Chong Zhang, Hao Wang, Trung Hieu Nguyen, Kun Zhou, Jiaqi Yip, Dianwen Ng , and Bin Ma, \"Mossformer2: Combining transformer and rnn-free recurrent network for enhanced time-domain monaural speech separation,\" 2023. [26] Shahar Lutati, Eliya Nachmani, and Lior Wolf, \"Separate and diffuse: Using a pretrained diffusion model for better source separation,\" in The Twelfth International Conference on Learning Representations, 2024. [27] Jun Ma, Feifei Li, and Bo Wang, \"U-mamba: Enhancing longrange dependency for biomedical image segmentation,\" arXiv preprint arXiv:2401.04722, 2024. [28] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang, \"Graphmamba: Towards long-range graph sequence modeling with selective state spaces,\" arXiv preprint arXiv:2402.00789, 2024. [29] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang, \"Motion mamba: Efficient and long sequence motion generation with hierarchical and bidirectional selective ssm,\" arXiv preprint arXiv:2403.07487, 2024. [30] Yijun Yang, Zhaohu Xing, and Lei Zhu, \"Vivim: a video vision mamba for medical video object segmentation,\" arXiv preprint arXiv:2401.14168, 2024. [31] Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei Xu, Zhikang Zou, Xiaoqing Ye, and Xiang Bai, \"Pointmamba: A simple state space model for point cloud analysis,\" arXiv preprint arXiv:2402.10739, 2024. [32] Changsheng Quan and Xiaofei Li, \"Multichannel long-term streaming neural speech enhancement for static and moving speakers,\" arXiv preprint arXiv:2403.07675, 2024. [33] Chen Chen, Chao-Han Huck Yang, Kai Li, Yuchen Hu, PinJui Ku, and Eng Siong Chng, \"A neural state-space model approach to efficient speech separation,\" 2023. [34] Biao Zhang and Rico Sennrich, \"Root mean square layer normalization,\" Advances in Neural Information Processing Systems, vol. 32, 2019. [35] Dan Hendrycks and Kevin Gimpel, \"Gaussian error linear units (gelus),\" arXiv preprint arXiv:1606.08415, 2016. [36] Prajit Ramachandran, Barret Zoph, and Quoc V Le, \"Searching for activation functions,\" arXiv preprint arXiv:1710.05941, 2017. [37] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R Hershey, \"Sdr-half-baked or well done?,\" in ICASSP 20192019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 626-630. [38] Diederik P. Kingma and Jimmy Ba, \"Adam: A method for stochastic optimization,\" in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun, Eds., 2015. [39] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Fran\u00e7ois Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio, \"SpeechBrain: A general-purpose speech toolkit,\" 2021, arXiv:2106.04624.",
    "dualpathmamba-14": "[^0]:    ${ }^{1}$ There was an error in our training and evaluation code. We have fixed it and updated the results from the previous version. We are training and will report the performance of our largest model DPMamba (L) soon. "
}