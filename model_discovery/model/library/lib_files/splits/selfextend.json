{
    "selfextend-0": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning\n\nHongye Jin Xiaotian Han Jingfeng Yang Zhimeng Jiang Zirui Liu Chia-Yuan Chang Huiyuan Chen Xia Hu\n\nAbstract\n\nIt is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length.",
    "selfextend-1": "This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handle long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model\u2019s self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs\u2019 context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs\u2019 context window length. The code can be found at https://github.com/datamllab/LongLM. Machine Learning, ICML\n\n1 Introduction\n\nThe context window length of most existing LLMs (Zhao et al., 2023; Yang et al., 2023) is limited since they are trained with a fixed length of training sequences. It\u2019s determined by the context window length during the pretraining stage. Once the length of the input texts exceeds the pretraining context window during the inference, the behavior of LLMs will be unpredictable and suffer from severe performance degradation. The perplexity (PPL) of the model will explode with the long input sequences (Xiao et al., 2023; Peng et al., 2023; Han et al., 2023; Chen et al., 2023b). Recently, a variety of context window extension methods have been developed to tackle the challenge of extending the context window size of pretrained LLMs. A straightforward approach is to fine-tune these models on enough extensive texts. Besides this, some methods seek to extend context window length in more efficient fine-tuning ways. Among these contemporary methods, some notable techniques include \u2018PI\u2019 (Chen et al., 2023b), \u2018CLEX\u2019 (Chen et al., 2023a) \u2018Yarn\u2019 (Peng et al., 2023), \u2018LongLora\u2019 (Chen et al., 2023c), and \u2018ABF\u2019 (Xiong et al., 2023). These methods aim to extend the content window based on the implicit assumption that pretrained LLMs lack the ability to handle long content. However, these methods typically require finetuning to achieve extension, which can be resource and time intensive given the quadratic complexity of Transformers. Additionally, high-quality long text data is scarce, hindering such fine-tuning approaches. Most real-world data is short, and much long text lacks meaningful long-range dependencies. With limited appropriate data, finetuning risks degrading existing strong performance on shorter sequences from pretraining or overfitting models to the tuning set. LLMs\u2019 generalizability to broad tasks may reduce. Instead of extending the content window, in this paper, we believe LLMs should have inherent capabilities to handle long contexts. Our belief stems from the fact that when we, as human beings, are children, we are taught how to read and write using relatively short texts, such as articles spanning several pages. We rarely use extremely long texts like entire books or complete documents as learning materials. Yet, we are still able to understand long texts effectively. With this strong motivation, the poor performance of LLMs while facing long text out of the pretraining context window is not due to the lack of long context understanding capabilities. In our analysis, the key challenge preventing LLMs from effectively handling longer contexts is the Out-of-Distribution (O.O.D) issues related to positional encoding, which we call the positional O.O.D111Here, the position refers to relative position rather than absolute position. The relative position is in RoPE, where and are the absolute positions of two tokens. The positional O.O.D refers to cases where the value of during inference is unseen, i.e., larger than the values observed during pretraining. In this paper, we map unseen large relative positions to those observed during pretraining. More details about are provided in Section 2. issue. This problem arises when LLMs encounter text sequences during inference exceeding the length of their pretraining context window, where LLMs are exposed to new relative distances that were not present during their pretraining phase. It is widely recognized that Neural Networks (NNs) are susceptible to unpredictable behaviors when dealing with O.O.D inputs (Liu et al., 2021; Shen et al., 2021; Bai et al., 2021; Zhang et al., 2023). To address this, an intuitive and practical solution would be to remap the unseen relative positions to those encountered during the pretraining, thus extending the LLMs\u2019 ability to handle longer contexts naturally. This paper proposes SelfExtend to elicit LLMs\u2019 inherent long context capabilities. SelfExtend addresses the issue of O.O.D. positional information by using a simple floor division operation to map unseen large relative positions to those encountered during pretraining. The core idea hinges on the observation that, in long texts, exacting word positions becomes less crucial. The overall meaning and the relative order of information hold greater significance. Just like when answering questions about lengthy texts, we rely on the general location and order, not the specific word-by-word placement. Natural language exhibits a characteristic where meaning stays relatively consistent within short ranges like paragraphs. Therefore, using close or even identical position encodings effectively captures the necessary relative ordering of important information. This intuitive approach aligns perfectly with the floor operation\u2019s functionality. Additionally, T5 (Raffel et al., 2020) and iRPE (Wu et al., 2021) also share this similar intuition. Our SelfExtend is a plug-and-play method that takes effect at the inference stage, allowing existing large language models to easily adopt it. We evaluate SelfExtend with some popular LLMs (Llama-2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), SOLAR (Kim et al., 2023), and Phi-2 (Javaheripi et al., 2023)) on three types of tasks: language modeling, synthetic long context tasks, and real-world long context tasks. The proposed SelfExtend substantially improves the long context understanding ability and even outperforms many finetuning-based methods on some tasks. These results underscore SelfExtend as an effective solution for context window extension. The superior performance of SelfExtend also demonstrated the potential of large language models to effectively handle long contexts. Our main contributions are summarized as follows:\n\n\u2022\n\nWe think LLMs with RoPE have a natural ability to handle long texts, even if they have not encountered super-long ones during training.",
    "selfextend-2": "The previous limitation stems from O.O.D. positions, meaning the \u201dlarger\u201d positions have not been seen during training. We call this the positional O.O.D. issue. \u2022\n\nBased on this belief and to address the positional O.O.D. issue, we propose SelfExtend to extend the context window of LLMs without any fine-tuning. We map the unseen large relative positions (at inference) to known positions (at training), thus allowing LLMs to maintain coherence over longer texts without additional fine-tuning. \u2022\n\nIn both synthetic and real-world long context tasks, SelfExtend has proven its ability to deliver performance that matches or surprisingly surpasses many existing fine-tuning-based models. This highlights the superior capabilities of our SelfExtend model. 2 Preliminary\n\nPosition Encoding. Transformers (Vaswani et al., 2017) incorporate position information via different positional embedding designs. The positional embedding design can be categorized into two classes: absolute position embeddings and relative positional encodings. The absolute position embedding provides the absolute positions, which embeds each absolute position into position vector and adds word embeddings to their corresponding before feeding them to the model. Examples of such include sinusoidal position embeddings (Vaswani et al., 2017) and learned position embeddings in GPT3 (Brown et al., 2020) and OPT (Zhang et al., 2022), or adding the dot product between two tokens\u2019 position embeddings on the attention logit (Ke et al., 2020). On the other hand, relative positional encodings have been proposed to use relative distance information between tokens and have become the mainstream of position embedding. This information is usually applied in attention layers. Examples of such include a learnable attention logit bias as in T5 (Xue et al., 2020), Transformer-XL (Dai et al., 2019); a fixed linear attention decay called Alibi (Press et al., 2021); rotating query and key sequences based on distance such as RoPE (Su et al., 2022), and XPos (Sun et al., 2023). The proposed method is based on the Rotary Position Embedding (RoPE) introduced in (Su et al., 2022). RoPE. Here, we introduce the basic concept of RoPE. Let\u2019s consider a sequence of tokens represented as , and their corresponding embeddings are denoted as , where is the dimension of the embedding. The basic idea of RoPE is to incorporate the positional information into the query and the key vectors , respectively. This integration ensures that their inner product will contain the relative positional embedding information inherently. To achieve this, RoPE employs the following vector transformations:\n\n\ud835\udc2a m = f q \u200b ( x m , m ) \u2208 \u211d | L | , \ud835\udc24 n = f k \u200b ( x n , n ) \u2208 \u211d | L | , formulae-sequence subscript \ud835\udc2a \ud835\udc5a subscript \ud835\udc53 \ud835\udc5e subscript x \ud835\udc5a \ud835\udc5a superscript \u211d \ud835\udc3f subscript \ud835\udc24 \ud835\udc5b subscript \ud835\udc53 \ud835\udc58 subscript x \ud835\udc5b \ud835\udc5b superscript \u211d \ud835\udc3f \\displaystyle\\mathbf{q}_{m}=f_{q}(\\textbf{x}_{m},m)\\in\\mathbb{R}^{|L|},~{}\\mathbf{k}_{n}=f_{k}(\\textbf{x}_{n},n)\\in\\mathbb{R}^{|L|}, (1)\n\nwhere is the hidden dimension of per head. The functions and responsible for injecting positional information, are defined as where , and projectors . RoPE keeps the real part of the inner product , which is . This operation ensures that the dot product of the query and key vectors depends entirely on the relative distance between the tokens, represented by of the tokens as follows:\n\n\u27e8 f q \u200b ( x m , m ) , f k \u200b ( x n , n ) \u27e9 \u211d = Re \u200b ( \u27e8 f q \u200b ( x m , m ) , f k \u200b ( x n , n ) \u27e9 \u2102 ) subscript subscript \ud835\udc53 \ud835\udc5e subscript x \ud835\udc5a \ud835\udc5a subscript \ud835\udc53 \ud835\udc58 subscript x \ud835\udc5b \ud835\udc5b \u211d Re subscript subscript \ud835\udc53 \ud835\udc5e subscript x \ud835\udc5a \ud835\udc5a subscript \ud835\udc53 \ud835\udc58 subscript x \ud835\udc5b \ud835\udc5b \u2102 \\displaystyle\\langle f_{q}(\\textbf{x}_{m},m),f_{k}(\\textbf{x}_{n},n)\\rangle_{\\mathbb{R}}=\\text{Re}(\\langle f_{q}(\\textbf{x}_{m},m),f_{k}(\\textbf{x}_{n},n)\\rangle_{\\mathbb{C}}) = \\displaystyle= Re \u200b ( x m \u2217 \u200b W q \u2217 \u200b W k \u200b x n \u200b e i \u200b \u03b8 \u200b ( m \u2212 n ) ) = g \u200b ( x m , x n , m \u2212 n ) , Re superscript subscript x \ud835\udc5a superscript subscript \ud835\udc4a \ud835\udc5e subscript \ud835\udc4a \ud835\udc58 subscript x \ud835\udc5b superscript \ud835\udc52 \ud835\udc56 \ud835\udf03 \ud835\udc5a \ud835\udc5b \ud835\udc54 subscript x \ud835\udc5a subscript x \ud835\udc5b \ud835\udc5a \ud835\udc5b \\displaystyle\\text{Re}(\\textbf{x}_{m}^{*}W_{q}^{*}W_{k}\\textbf{x}_{n}e^{i\\theta(m-n)})=g(\\textbf{x}_{m},\\textbf{x}_{n},m-n), (2)\n\nwhere is an abstract mapping function. 3 SelfExtend\n\nIn this section, we first conduct a preliminary investigation on the inherent ability of the LLMs to handle long content. Then, we propose our SelfExtend that effectively extends existing LLMs\u2019 context window without any fine-tuning. 3.1 Preliminary Analysis\n\n\u2460 Why do LLMs fail on sequences during inference that are longer than their pre-training context window? For a pretrained LLM with relative position encodings, such as RoPE, the behavior of the LLMs becomes unpredictable during inference if the length of a sequence is longer than its pretraining context window length. This has been explored by (Han et al., 2023; Chen et al., 2023b) that with unseen relative positions, the attention distributions are very different compared to those within the pretraining context window length. We argue that such failure stems from the Out-of-Distribution (O.O.D.) relative distance in the same sense that neural networks are not robust to O.O.D. inputs (Shen et al., 2021). \u2461 How to solve positional O.O.D. problem? One feasible and straightforward way to handle unseen relative positions is to map them to positions that were seen during pretraining. We can use the floor operation to map the unseen positions to positions within the pretraining context window, as shown in Figure 1. The proposed method is identical to the original self-attention mechanism except that the floor operation is applied to each token\u2019s original position before the inner product. We denote the self attention with the floor operation applied as \u201cgrouped attention\u201d. In Python style, the \u201cgrouped attention\u201d is denoted as:\n\nP g = P / / G s , P_{g}=P~{}~{}~{}//~{}~{}~{}G_{s}, (3)\n\nwhere is the original position encoding, in which is the batch size and is the length of the input text sequence. denotes the group size, which is the base of the floor operation. Taking the floor of the position divided by the group size maps the original large position values to a smaller discrete set of values, avoiding the issue of out-of-distribution position values during inference. \u2462 Can LLMs work well without accurate position information? \u2014 Yes, but not that perfect. We show the perplexity (PPL) on the PG-19 (Rae et al., 2019) dataset with the floor operation applied to Llama-2-7b-chat across different sequence lengths, in Figure 2. As a comparison, we also show the PPL of the original model without the floor operation as the dotted lines. From this figure, with the floor operation, LLMs keep a relatively low and stable PPL on the sequences whose lengths exceed the pretraining context window. Meanwhile, with grouped attention, the PPL is a little higher than the original LLMs, which is expected. However, the model\u2019s PPL behavior is similar to the original model, as the PPL is nearly unchanged within the \u201ccontext window\u201d (for Llama-2: 2 - 8192, 4 - 16384, and 8 - 32768), demonstrating the effectiveness of group attention. Once the length of a sequence is longer than the new \u201ccontext window\u201d (e.g., sequences with 10k tokens as the input, with a group size of 2 ), the PPL explodes again due to the positional O.O.D issue. \u2463 How to restore degraded language modeling ability caused by grouped attention? \u2014 Re-introducing normal attention in the neighboring area. In the process of generating next tokens, the immediate neighbors of a target token play a crucial role, which is well-supported by existing methods of sparse attention mechanisms (Zaheer et al., 2020; Shi et al., 2021) and methods for extending the contextual window (Han et al., 2023; Xiong et al., 2023; Chen et al., 2023c). These studies consistently highlight the importance of maintaining the standard attention mechanism for tokens in close proximity to the target token. This proximity-based focus is essential for the accurate generation of the next token, ensuring the coherence and fluency of the generated text, as evidenced by acceptable perplexity (PPL) levels. Employing grouped attention might not significantly affect the overall quality of generated sentences; however, it necessitates the accurate positioning of attention to maintain generation quality. Therefore, it is imperative to preserve the standard attention mechanism within the vicinity of the target token, as utilized during the pretraining phase, to ensure the precision and effectiveness of language models in capturing the nuances of local context. 3.2 SelfExtend LLM Context Window Without Tuning\n\nWe introduce SelfExtend, a method that enhances LLMs\u2019 natural capability to process extensive contexts without the need for fine-tuning. SelfExtend incorporates two distinct types of attention mechanisms: 1) Grouped attention, specifically designed for tokens that are far apart. This approach applies a floor operation to the positions to manage long-distance relationships between tokens; 2) Standard attention, which employs the conventional attention mechanism for adjacent tokens within a specified range. The SelfExtend framework is depicted in Figure 3. Notably, SelfExtend modifies only the attention mechanism during inference, eliminating the need for additional fine-tuning. Maximum Extended Length of SelfExtend Suppose that we have the pretraining context window size as , the group size for grouped attention as , and the window size for neighbor tokens as . We shift the relative position of grouped attention by before merging the two pieces of attention together. This ensures that the transition from the normal attention area to the grouped attention area smooth. We merge the two parts of attention by replacing the attention values out of the neighbor token window with the attention values from the grouped attention. All the modifications are applied before the softmax operation and other parts remain unchanged. Ideally, the maximum length of the extended context window is:\n\n( L \u2212 w n ) \u2217 G s + w n . \ud835\udc3f subscript \ud835\udc64 \ud835\udc5b subscript \ud835\udc3a \ud835\udc60 subscript \ud835\udc64 \ud835\udc5b (L-w_{n})*G_{s}+w_{n}. (4)\n\nFor example, in Figure 3, the context window is extended from its pretraining length of to . The pseudo code for SelfExtend are presented in Algorithm 1. Relation to Existing Work The grouped attention in SelfExtend can be viewed as a form of position interpolation (Chen et al., 2023b), where some positions are interpolated to be infinitely close to pretraining positions. Another finetuning-free method, ReRoPE (Su, 2023), is equivalent to a special case of SelfExtend: the group size is large enough that all tokens outside the neighbor window fall into the same group (e.g. group size 10,000 in Figure 5). T5 (Raffel et al., 2020) and iRPE (Wu et al., 2021) also share the high-level idea of multi-level positional encodings, while applying it during pretraining. T5 is more similar to ReRoPE for using the same position for distant tokens. iRPE has finer distant position encodings, more akin to SelfExtend. 4 Experiments\n\nWe evaluate SelfExtend with Llama-2 (Touvron et al., 2023) and its families, Phi-2 (Javaheripi et al., 2023), Mistral (Jiang et al., 2023) and SOLAR (Kim et al., 2023) on language modeling task, synthetic long context tasks, real-world long context tasks and standard short-context tasks. 4.1 Performance on Language Modeling Tasks\n\nLanguage modeling task is the most fundamental and the least requirement for LLMs, which is usually measured by perplexity (PPL) on the test text data. A low PPL does not guarantee good performance on real tasks (Pal et al., 2023), however, a higher PPL suggests severe performance degradation of LLMs. We evaluate SelfExtend\u2019s language modeling performance on dataset PG19 (Rae et al., 2019), which contains lengthy books. PPL is used as the metric. More experimental details are presented in Section D.1\n\nThe results show that SelfExtend can successfully maintain a low PPL out of the pretraining context window for both Llama-2-7b-chat and Mistral. Without SelfExtend, the PPL explodes when the length of test sequence is larger than the context window. Mistral with SWA can also maintain a low PPL out of its context window. But later in the next section, we will demonstrate that a low PPL score does not necessarily indicate proficiency in handling long contexts. More discussion about PPL can be found in Appendix B. 4.2 Performance on Synthetic Long Context Tasks\n\nThe passkey retrieval task is the same as what is defined in Landmark Attention (Mohtashami & Jaggi, 2023), which is a synthetic long context task. It requires a language model to retrieve a simple passkey (i.e., a 5-digit random number) in a long meaningless text sequence. The passkey is placed with various document depths (where the passkey is placed in the input texts) and context lengths (ranging from 4k to 24k). We tested multiple passkey retrievals for each context length and depth. The passkey was randomly placed within a span of tokens. For a depth of and context of 8k, the passkey was placed between tokens . We performed iterations per span, so total for that setting. Experimental setting details and an example of passkey retrieval task can be found in Section D.2. The results in Figure 4 show that without any fine-tuning, SelfExtend obtains 100% passkey retrieval accuracy across all tested depths and context lengths. The results also demonstrate that: although Mistral w/ SWA has low PPL beyond its pretraining context window, it can only access information (i.e. the passkey) within its sliding window. Considering the simplicity of this task, these results strongly suggest it still does not have the true ability to handle long contexts. 4.3 Performance on Real-World Long Context Tasks\n\nEvaluation solely on language modeling (measured by perplexity) and synthetic tasks like passkey retrieval cannot fully assess the long-context capabilities of LLMs. The task of Passkey retrieval is overly straightforward, and an LLM may still struggle with long context despite low perplexity. To comprehensively evaluate long-context performance, we further use two recent real-world long context benchmarks: LongBench (Bai et al., 2023) and L-Eval (An et al., 2023). The results are presented in Table 2 and Table 3. On the LongBench in Table 2, for all four different base LLMs and most datasets, with SelfExtend, the LLM can obtain significant performance improvments. Llama-2-7B: We use SelfExtend to increase Llama-2-7b-chat\u2019s context from 4k to 16k and 25k. Both significantly outperform Llama-2-7b-chat and most fine-tuned models on several datasets like HotpotQA. We also extend vicuna1.5-7B from 4k to 16k and 25k. With SelfExtend, vicuna1.5-7B surpasses its fine-tuned counterpart vicuna1.5-7B-16k and ranks among top Llama-2-7b models. On some datasets, the 25k variant underperforms the 16k one due to the trade-off between larger context and positional precision. More details about the trade-off is in Section 4.5. Mistral-7B: We extend Mistral-7B\u2019s context to 16k, significantly improving its long context ability over the base model, with or without SWA applied. The fine-tuned variant MistralLite ((amazon, 2023)) achieves the best performance on most datasets. However, many of these datasets were included in MistralLite\u2019s fine-tuning data, such as NarrativeQA222More details about MistralLite\u2019s fine-tuning data can be found at https://huggingface.co/amazon/MistralLite. At least, GovReport, QMSum, NarrativeQA, Qasper, QuALITY, and HotpotQA are included. Meanwhile, Multi-passage QA and summarization tasks are also in fine-tuning data. This also violates zero-shot evaluation conditions.. SOLAR-10.7B and Phi-2: They have no finetuned variant for context window extension yet. SelfExtend can also obtain substantial performance improvements. On the LEval benchmark in Table 3, we observe similar results. Compared to fine-tuning free baselines like NTK or further fine-tuned models like Longchat1.5-7b-32k and Vicuna1.5-7b-32k, SelfExtend achieves superior performance on nearly all datasets333LEval performance seems sensitive to prompt engineering for these sub-13B LLMs. For example, on some datasets, vanilla vicuna-13b underperforms vanilla vicuna-7b.. In summary, on the two benchmarks, SelfExtend achieves comparable or better performance, compared to methods that requires further fine-tuning. Despite our initial expectation being that SelfExtend would simply outperform the base model without additional extension methods, it is remarkable that our SelfExtend, which solely operates during inference without the need for fine-tuning or training, achieves such impressive performance. 4.4 Performance on Short Context Tasks\n\nWe argue that an ideal context length extension method should not degrade performance on standard short-context tasks. Previous fine-tuning based methods usually undergo performance degradation on short-context tasks (Peng et al., 2023; Xiong et al., 2023). Following (Peng et al., 2023), we use Hugging Face Open LLM Leaderboard (Gao et al., 2023) to evaluate SelfExtend\u2019s performance on five public short context tasks. Specifically, we use 25-shot ARC-Challenge (Clark et al., 2018), 10-shot HellaSwag (Zellers et al., 2019), 5-shot MMLU (Hendrycks et al., 2020), 0-shot TruthfulQA (Lin et al., 2021), and 5-shot GSM8K (Cobbe et al., 2021).",
    "selfextend-3": "The results are shown in Table 4. We also investigate the influence of varying group sizes and neighbor window sizes on short-context tasks and we present the results in Appendix C. The results show that SelfExtend can maintain the performance of the short-context tasks, while enhance the performance on long-context tasks. Moreover, because SeldExtend does not require any fine-tuning and only takes effect during inference, SelfExtend can be readily adopted as a plug-in component for LLMs. This means SelfExtend can be automatically and inherently disabled while encountering short-text sequences. Then, with the parameters remaining unchanged, LLMs can maintain its original inference mechanism on those short-context scenarios. 4.5 Ablations on Group Size and Neighbor Window\n\nWe investigate the influence of varying the group size and the neighbor window . We experiments with Phi-2 on four real-world datasets from Longbench: narrativeqa, qasper, triviaqa, and repobench-p. The results are presented in Figure 5. Form the results, we observe two trade-offs:\n\n1) There is a trade-off with respect to group size in SelfExtend. Generally, both too small and too large group sizes can result in inferior performance compared to an optimal level. With a large group size, position information becomes more coarse, potentially causing performance drops. Conversely, small group sizes require SelfExtend to utilize larger position embeddings to extend the context window. These larger position embeddings are less trained compared to smaller ones. For example, in Llama-2 with its 4096 context window, the relative position 4095 accounts for only 1/2048 the frequency of the relative position 2048 in training. These under-trained relative positions can also degrade performance. This trade-off produces the \u2019peak\u2019 shape in the figure, indicating the extended context window differs from the ideal case described in Equation 4. 2) There is also another trade-off w.r.t. neighbor window size. With larger neighbor window sizes, there is more precise information about neighbor tokens, which is the most important. But a larger neighbor window size means SelfExtend has to use a larger group size for a long sequence, compared to using a smaller neighbor window size & smaller group size, the information about the whole sequence becomes coarse. 4.6 Performance with Varying Context Window Length\n\nTo validate SelfExtend\u2019s efficacy in enabling LLMs to utilize extended context windows, we assess Phi-2\u2019s performance across varying context lengths with SelfExtend, referencing Table 5. Across four task types from LongBench, results are generally improved with longer contexts. Notably, SelfExtend monotonically enhances performance on NarrativeQA and Qmsum. While significant improvements are observed across most datasets, a \u2019peak\u2019 in performance suggests a trade-off, as discussed in Section 4.5: longer contexts offer more relevant information, but the larger group sizes required by SelfExtend to extend the context window may cause less precise positional information444Other possible reasons include: Phi-2 is a base model without instruction tuning, and SelfExtend\u2019s performance is not optimal as we use the same set of hyperparameters across all datasets, which cannot showcase SelfExtend\u2019s full potential. Regarding Lcc, performance remains consistent, possibly due to its reliance on local codes and shorter dataset lengths555With Phi-2 tokenizer, over of Lcc instances are under 4096 tokens, with an average length of 4069.7. 4.7 Varying-Length Passkey Retrieval Task\n\nThe conventional passkey retrieval task, along with prevalent benchmark datasets, primarily assesses the proficiency of LLMs in identifying and leveraging pertinent information. Traditionally, this task involves passkeys not exceeding 5 digits in length. To evaluate the LLMs\u2019 capabilities of producing consistent and precise outcomes for long sequences, we extended the task to incorporate passkeys with larger lengths. We test passkeys in digits. The input sequence contains characters. More details are presented in Section D.3. The results, depicted in Figure 6, illustrate a common trend: while short passkeys of 5 or 8 digits are easily managed by all, divergences in performance emerge as the length of passkey increases. Notably, with the exception of Yarn, many tuning-based methods are unable to accurately reproduce passkeys beyond 64 digits, and some of them even experience a marked decline in performance when the passkey length exceeds 16 digits. Remarkably, although without tuning, SelfExtend maintains its superiority. These findings suggest that we should carefully choose the training approach when fine-tuning models to handle long contexts. 5 Conclusion and Discussion\n\nIn this paper, we argue that LLMs themselves have the inherent ability to handle long sequences and propose SelfExtend to elicit the inherent long context abilities for LLMs by simply mapping unseen relative positions into those seen during pretraining via the Floor operation. Without any tuning or further training, SelfExtend can effectively improve LLMs\u2019 long context performance, as extensive experiments show. Limitations: SelfExtend increases computation cost with naive implementations since it performs extra attention across all query-key pairs. However, with optimizations like blocked kernels (e.g., Flash Attention (Dao et al., 2022)), this becomes linear rather than quadratic, and the marginal cost is small enough to be ignored for long input sequences. Also, the performance degrades with large group size, preventing indefinitely long contexts. Additionally, evaluation methodologies for assessing long context abilities remain open research questions. Future Work: We are interested in testing SelfExtend on models using other positional encoding. Larger models, longer contexts, and more challenging tasks will be tested if we can access more computational resources in the future. In the meantime, more sophisticated mapping methods will be considered as the replacement of the simple floor operation to achieve better long context understanding abilities and extended context window length. References\n\namazon (2023) amazon. Mistrallite model. https://huggingface.co/amazon/MistralLite, 2023.",
    "selfextend-4": "[Online; accessed 29-December-2023]. An et al. (2023) An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023. Anothropic (2023) Anothropic. Long context prompting for claude 2.1. https://www.anthropic.com/news/claude-2-1-prompting, 2023. Bai et al. (2021) Bai, T., Luo, J., Zhao, J., Wen, B., and Wang, Q. Recent advances in adversarial training for adversarial robustness. arXiv preprint arXiv:2102.01356, 2021. Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for long context understanding.",
    "selfextend-5": "arXiv preprint arXiv:2308.14508, 2023. Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Chen et al. (2023a) Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L. Clex: Continuous length extrapolation for large language models.",
    "selfextend-6": "arXiv preprint arXiv:2310.16450, 2023a. Chen et al. (2023b) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation.",
    "selfextend-7": "arXiv preprint arXiv:2306.15595, 2023b. Chen et al. (2023c) Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models.",
    "selfextend-8": "arXiv preprint arXiv:2309.12307, 2023c. Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems.",
    "selfextend-9": "arXiv preprint arXiv:2110.14168, 2021. Dai et al. (2019) Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness.",
    "selfextend-10": "Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022. Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, 12 2023.",
    "selfextend-11": "URL https://zenodo.org/records/10256836. gkamradt (2023) gkamradt. Llmtest_needleinahaystack: Doing simple retrieval from llm models. https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main, 2023. [Online; accessed 29-December-2023]. Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models.",
    "selfextend-12": "arXiv preprint arXiv:2308.16137, 2023. Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.",
    "selfextend-13": "arXiv preprint arXiv:2009.03300, 2020. Javaheripi et al. (2023) Javaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck, S., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan, R., Gopi, S., Gunasekar, S., Javaheripi, M., Kauffmann, P., Lee, Y. T., Li, Y., Nguyen, A., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Santacroce, M., Behl, H.",
    "selfextend-14": "S., Kalai, A. T., Wang, X., Ward, R., Witte, P., Zhang, C., and Zhang, Y. Phi-2: The surprising power of small language models, 2023.",
    "selfextend-15": "Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Ke et al. (2020) Ke, G., He, D., and Liu, T.-Y. Rethinking positional encoding in language pre-training. arXiv preprint arXiv:2006.15595, 2020. Kim et al. (2023) Kim, D., Park, C., Kim, S., Lee, W., Song, W., Kim, Y., Kim, H., Kim, Y., Lee, H., Kim, J., et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling.",
    "selfextend-16": "arXiv preprint arXiv:2312.15166, 2023. Lin et al. (2021) Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods.",
    "selfextend-17": "arXiv preprint arXiv:2109.07958, 2021. Liu et al. (2021) Liu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021. Mohtashami & Jaggi (2023) Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. Pal et al. (2023) Pal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundararajan, A., and Naidu, S. Giraffe: Adventures in expanding context lengths in llms.",
    "selfextend-18": "arXiv preprint arXiv:2308.10882, 2023. Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models.",
    "selfextend-19": "arXiv preprint arXiv:2309.00071, 2023. Press et al. (2021) Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. Rozi\u00e8re et al. (2023) Rozi\u00e8re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Shen et al. (2021) Shen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021. Shi et al. (2021) Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and Kwok, J. T.-Y. Sparsebert: Rethinking the importance analysis in self-attention. In International Conference on Machine Learning, pp. 9547\u20139557. PMLR, 2021. Su (2023) Su, J. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.",
    "selfextend-20": "Su et al. (2022) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. RoFormer: Enhanced transformer with rotary position embedding, 2022.",
    "selfextend-21": "arXiv: 2104.09864. Sun et al. (2023) Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. A length-extrapolatable transformer. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14590\u201314604, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.816. URL https://aclanthology.org/2023.acl-long.816. Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wu et al. (2021) Wu, K., Peng, H., Chen, M., Fu, J., and Chao, H. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10033\u201310041, 2021. Xiao et al. (2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks.",
    "selfextend-22": "arXiv preprint arXiv:2309.17453, 2023. Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Xue et al. (2020) Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. mt5: A massively multilingual pre-trained text-to-text transformer.",
    "selfextend-23": "arXiv preprint arXiv:2010.11934, 2020. Yang et al. (2023) Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Yin, B., and Hu, X. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712, 2023. Yin Song and Chen Wu and Eden Duthie (2023) Yin Song and Chen Wu and Eden Duthie. amazon/MistralLite, 2023. URL https://huggingface.co/amazon/MistralLite. Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020. Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?",
    "selfextend-24": "arXiv preprint arXiv:1905.07830, 2019. Zhang et al. (2023) Zhang, J., Chao, H., Dhurandhar, A., Chen, P.-Y., Tajer, A., Xu, Y., and Yan, P. When neural networks fail to generalize? a model sensitivity perspective. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 11219\u201311227, 2023. Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhao et al. (2023) Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Appendix A Pseudocode of SelfExtend\n\nAppendix B Perplexity as a Metric for Long Context Capabilities\n\nPPL is not an effective metric for measuring the ability of LLMs to handle long contexts. In Figure 7, we introduce a seemingly plausible context window extension method named \u2019Infinite\u2019. When evaluated on PG19 using the same protocol, Llama-2-7b-chat with \u2019Infinite\u2019 achieves PPL scores that are comparable to, or even lower than, those achieved by SelfExtend, as demonstrated in Table 6. However, \u2019Infinite\u2019 essentially mimics the process of dividing a long sequence into short sub-sequences before processing them with LLMs, indicating that it does not genuinely address long context handling. The discrepancy between Perplexity (PPL) and long context ability primarily stems from how PPL is calculated by averaging over numerous tokens. As long as the majority of tokens are modeled accurately, PPL will remain low. This is closely related to the influence of neighboring tokens. Information from neighboring tokens\u2014such as those within the local attention window of \u2019Infinite\u2019\u2014can suffice for predicting most tokens, thus leading to a low PPL. However, a few critical tokens, which are crucial for understanding long contexts and answering questions, may not be predicted accurately. Additionally, unlike the pre-training process where the cross-entropy loss corresponds directly to perplexity, measuring PPL during inference is static. It resembles a specific point on the loss curve observed during pre-training. While a decreasing trend in loss during pre-training indicates good performance, a single point on the training loss curve cannot determine the performance. In summary, while low PPL is essential for a good model, lower PPL does not necessarily equate to better performance in understanding long contexts. Appendix C SelfExtend on Group Size and Neighbor Window\n\nTo comprehensively understand SelfExtend\u2019s influence on LLMs, unlike previous experiments which used long context settings, we evaluate with smaller neighbor window sizes on four standard benchmark tasks: ARC-c, GSM8k, Hellaswag and MMLU.",
    "selfextend-25": "We use Phi-2 as the extended LLM. The results are shown in Figure 8. We didn\u2019t include TruthfulQA because its average length is less than 300 words, while the four datasets we used have an average length greater than 700 words. In general, SelfExtend has a minor influence on Phi-2 as long as the neighbor window size is over 128. In many cases, SelfExtend even performs slightly better than vanilla Phi-2. When the neighbor window is too small (e.g. 64 tokens), if the group size is large, as expected, the positional information loss is too high and Phi-2\u2019s performance degrades. Also, on difficult tasks such as MMLU and Helleswag, we observe a monotonic decrease in performance with increasing group size for all neighbor windows. In summary, even when applying SelfExtend to short context tasks, as long as the hyperparameters are not extreme, SelfExtend does not harm the model. Appendix D Detailed Experimental Setting\n\nIn this appendix, we present the details of the experiments in our paper. D.1 Experimental Setting on Language Modeling Tasks\n\nThis is not the standard setting for PPL testing on PG-19. We use the first sentence of each book in PG19\u2019s test set (100 books) to test the language modeling ability. The results cannot be directly compared to the PPL reported by other papers. We chose this setting because our computation resources are very limited. This setting saves a lot and it can still show the behavior of LLMs w.r.t. PPL. All PPL results were calculated using the sliding window method (Press et al., 2021) with . We evaluated how the PPL changes as the input length increases. In Table 1, SelfExtend extends the original Llama-2\u2019s context window length from (4k) to over (16k) with group size set as and neighbor window set as (1k). For Mistral model, without SWA, the context window is (8k) and it is also extended by SelfExtend with the same setting to larger than 16k. With SWA, Mistral can digest an infinite length of sequences and its default sliding window is . D.2 Experimental Setting on Passkey Retrieval Task\n\nCompared to other synthetic tasks, such as \u201dNeedle in a Haystack\u201d (gkamradt, 2023), the model\u2019s performance on this is not sensitive to the prompt (Anothropic, 2023). This may come from the fact that the sentence carrying the passkey is very different from those repeated random texts surrounding it. Empirically, within the effective context window, almost all LLMs, including those without any instruction tuning or alignment, can locate the sentence carrying the passkey. Although this task is easy and far from real-world scenarios, it tests two fundamental capabilities of LLMs: 1. The model should be able to recognize and locate the useful information across all positions of the input sequence (the most fundamental understanding capability); 2. The model should be able to use the perceived information to finish tasks (the most fundamental generation capability). An example of passkey is as the following:\n\nD.3 Experimental Setting on Varying-Length Passkey Retrieval Task\n\nIn this experiment, we use the following models: Llama2-7b-chat with SelfExtend, LongLora-7b-16k666We use its fully fine-tuned variant. For more details about the model: https://huggingface.co/Yukang/Llama-2-7b-longlora-16k. We didn\u2019t use the latest version named \u2019LongAlpaca\u2019 as we cannot get reasonable performance for this specific task with LongAlpaca, which may be due to our improper configuration.,vicuna-1.5-7b-16k, Together AI\u2019s Llama-2-7b-32k777Both vicuna-1.5-7b-16k and Together AI\u2019s Llama-2-7b-32k were fine-tuned using position interpolation, and Yarn-Llama-2-7b-64k. Appendix E Details of LLMs\n\nHere, we list the links to the details of the LLMs utilized in our experiments. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 10:34:56 2024 by LaTeXML"
}