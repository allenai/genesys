{
    "deepwienerssm-0": "# Structured state-space models are deep Wiener models \n\nFabio Bonassi *, Carl Andersson*, Per Mattsson*,<br>Thomas B. Sch\u00f6n*<br>* Department of Information Technology, Uppsala University,<br>75105 Uppsala, Sweden. E-mail: name.surname@it.uu.se\n\n\n#### Abstract\n\nThe goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows us to reframe SSMs as an extension of a model class commonly used in system identification. To stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions. Keywords: Structured State-space models; system identification; deep learning. ## 1. INTRODUCTION\n\nRecent years have been characterized by a remarkable research interest towards deep learning tools for data-driven control. An interesting use case is that of nonlinear system identification. Over the years, a variety of (increasingly complex) Neural Network (NN) architectures have indeed been proposed for nonlinear system identification, ranging from gated Recurrent NNs (RNNs) (Bonassi et al., 2022) to Transformer NNs (TNNs) (Sun and Wei, 2022). These models have proven to work well in many challenging identification problems, enabling the synthesis of accurate model predictive control laws, e.g.",
    "deepwienerssm-1": "Lanzetti et al. (2019). Despite their modeling power, these architectures are known to suffer from computational efficiency problems at training. On the one hand, RNNs are inherently sequential models, implying they have to be iteratively unrolled over the time axis $T$ of training sequences (Bianchi et al., 2017). On the other hand, TNNs are plagued by a quadratic scaling issue $\\left(\\mathcal{O}\\left(T^{2}\\right)\\right)$, thus calling for tailored architectures like Longformers, which are however still untapped in the nonlinear system identification realm. Unfortunately, these problems have limited the use of RNNs and TNNs for longterm nonlinear system identification to those cases where enough computational budget is available. What is more, both RNNs and TNNs are still fairly unexplored when it comes to their system-theoretical properties, such as Incremental Input-to-State Stability ( $\\delta$ ISS) (Bonassi et al., 2022), which are however essential for utilizing these models for control design (Bonassi et al., 2024). Most of the research in this area is tailored to specific\n\n[^0]RNNs, see Miller and Hardt (2019); Bonassi et al.",
    "deepwienerssm-2": "(2022), and mainly involves stability promotion via regularization. To address these problems, Gu et al. (2021) proposed a Structured State-space Model (SSM) architecture named S4, which consists of multiple layers composed by LTI discrete-time systems followed by a nonlinear function. The term \"structured\" stems from the fact that this LTI system is given a specific structure to improve the architecture's modeling performances while also reducing the computational cost at training (Yu et al., 2018). Nonlinear state-space models are not new, see Marconato et al. (2013), yet their adoption has been hampered by their crucial reliance on the model structure and on the initialization method of learnable parameters. The contribution of the S4 approach towards SSMs has therefore been that of providing ( $i$ ) a novel, intrinsically stable, parametrization of the LTI system obtained by discretizing a continuoustime Diagonal Plus-Low Rank (DPLR) system, (ii) a new strategy towards the parameters' initialization problem, (iii) a computationally efficient approach to simulate (and train) these models over extremely long sequences, and (iv) an empirical proof of the state-of-the-art performances of these models in long-term sequence learning problems. Motivated by these appealing features, many works have continued to build on the S4 architecture. For example, Gupta et al. (2022) and Gu et al. (2022) have explored the benefits entailed by stricter SSM structures, namely the parametrization via diagonal continuous-time systems (S4D), and by simpler initialization strategies. Smith et al. (2022) have explored a novel, and somewhat more computationally efficient, simulation method for diagonal continuous-time parametrizations, named S5. Orvieto et al. (2023) recently investigated the parametrization of the LTI subsystems directly in the discrete time domain, resulting in the Linear Recurrent Unit (LRU) architecture. Contribution Despite the appealing results achieved by SSMs in the long-range arena benchmarks sequence classification problems, their use for nonlinear system identification is still unexplored. With this paper, we want to change that by making the following contributions. First of all we show that it is possible to interpret SSMs as deep Wiener models, i.e. model structures where several Wiener models are interconnected in series. An interesting note here is that even though the Wiener models have been extremely popular within system identification - see e.g. Schoukens and Tiels (2017) and references therein - their structure has been limited to \"single-layer\" or parallel architectures (Wills and Ninness, 2012). Our second contribution is to dissect the recent developments on SSMs and explain them in terms of their structure and parameterization, and to clearly separate this from their initialization, simulation, and training strategies. The presentation in the paper is also done using the language commonly used in the system identification community in order to speed up the use of these tools within this area. Notation The imaginary unit is denoted by $i=\\sqrt{-1}$. Given a vector $v$, we denote by $v^{\\prime}$ its real transpose. For a time-dependent vector, the discrete-time index $k$ is reported as a subscript, e.g., $v_{k}$. Moreover, we denote by $v_{a: b}$ (where $a \\leqslant b$ ) the sequence $v_{a: b}=\\left(v_{a}, v_{a+1}, \\ldots, v_{b}\\right)$. For this sequence, we indicate by $\\operatorname{cat}\\left(v_{a: b}\\right)$ the concatenation of its elements, i.e. $\\operatorname{cat}\\left(v_{a: b}\\right)=\\left[v_{a}^{\\prime}, \\ldots, v_{b}^{\\prime}\\right]^{\\prime}$, and by uncat $(\\cdot)$ its inverse operation returning a sequence of vectors given their concatenation. Given a complex matrix $A$, we let $\\operatorname{conj}(A)$ be its element-wise complex conjugate and $A^{*}$ be its Hermitian transpose. Diagonal matrices may be defined via the $\\operatorname{diag}$ operator, as $A=\\operatorname{diag}\\left(a_{1}, \\ldots, a_{n}\\right)$. ## 2. STRUCTURED STATE-SPACE MODELS\n\nConsider the model depicted in Figure 1, which consists of $L$ Wiener systems interconnected in series. Each of these layers is here referred to as Structured Statespace Layer (SSL). Their interconnection results in an SSM, which can be interpreted as a specific configuration of a deep Wiener system. We let the generic $\\ell$-th SSL $(\\ell \\in\\{1, \\ldots, L\\})$ be represented by a discrete-time statespace model\n\n$$\n\\mathrm{SSL}_{\\ell}:\\left\\{\\begin{array}{l}\nx_{k+1}=A x_{k}+B u_{k} \\\\\n\\eta_{k}=C x_{k}+D u_{k} \\\\\ny_{k}=\\sigma\\left(\\eta_{k}\\right)+F u_{k}\n\\end{array}\\right. $$\n\nwhere, for compactness, the layer index is omitted. System (1) is characterized by the input vector $u \\in \\mathbb{R}^{n_{u}}$, the intermediate vector $\\eta \\in \\mathbb{R}^{n_{y}}$, the output vector $y \\in \\mathbb{R}^{n_{y}}$, and the complex-valued state vector $x \\in \\mathbb{C}^{n_{x}}$. The SSL is parametrized by the matrices $\\{A, B, C, D, F\\}$. The output transformation $\\sigma(\\cdot)$ can be any nonlinear, Lipschitzcontinuous activation function, such as the tanh, ELU, or Swish, see Ramachandran et al. (2017). In what follows, we aim to provide an overview of the possible structure, parametrization, initialization, and simulation strategies for this SSL. Remark 1. When a deep SSM is considered $(L>1)$, each layer is parametrized and initialized independently from the others. The simulation is carried out iteratively over the set of layers, meaning that the output sequence of the $\\ell$-th layer is used as input of the layer $\\ell+1$.",
    "deepwienerssm-3": "![](https://cdn.mathpix.com/cropped/2024_09_12_c80c09bf8a76527d7976g-2.jpg?height=498&width=769&top_left_y=231&top_left_x=1112)\n\nFig. 1. Schematic of a Structured State-space Model. We start by observing that, because both the input and output vector of the SSL are real-valued, the system matrices $A, B$, and $C$ of (1) might be re-written as\n\n$$\n\\begin{aligned}\n& A=\\left[\\begin{array}{lr}\n\\tilde{A} & \\\\\n\\operatorname{conj}(\\tilde{A})\n\\end{array}\\right], \\quad B=\\left[\\begin{array}{c}\n\\tilde{B} \\\\\n\\operatorname{conj}(\\tilde{B})\n\\end{array}\\right] \\\\\n& C=\\left[\\begin{array}{ll}\n\\tilde{C} & \\operatorname{conj}(\\tilde{C})]\n\\end{array}\\right. \\end{aligned}\n$$\n\nwhere $\\tilde{A} \\in \\mathbb{C}^{n_{\\lambda} \\times n_{\\lambda}}, \\tilde{B} \\in \\mathbb{C}^{n_{\\lambda} \\times n_{u}}, \\tilde{C} \\in \\mathbb{C}^{n_{y} \\times n_{\\lambda}}$, while $D \\in \\mathbb{R}^{n_{y} \\times n_{u}}$, with $n_{\\lambda}=\\frac{n_{x}}{2}$. Matrix $F \\in \\mathbb{R}^{n_{y} \\times n_{u}}$ can be interpreted as a skip connection, fixed to $I$ if $n_{u}=n_{y}$. Note that if the SSLs are parametrized to be structurally Schur stable, the stability of the SSM is guaranteed. Proposition 2. If each layer $\\ell \\in\\{1, \\ldots, L\\}$ is parametrized by a Schur-stable matrix $\\tilde{A}$, then the SSM is Incrementally Input-to-State Stable ( $\\delta$ ISS) (Bonassi et al., 2022, Def. 4). Proof. Owing to the Schur stability of $\\tilde{A}$ and Lipschitz continuity of $\\sigma(\\cdot)$, the SSL can be easily proven (globally) $\\delta$ ISS. The SSM is then the series of $\\delta$ ISS systems and is therefore $\\delta$ ISS (Angeli, 2002). While (2) ensures that the eigenvalues come in complexconjugate pairs, both in the linear (Yu et al., 2018) and the nonlinear realm (Marconato et al., 2013) it is well known that state-space models call for additional structure to reduce the number of learnable parameters and hence achieve improved modeling performances and computational efficiency.",
    "deepwienerssm-4": "Besides, the initialization of these parameters is also paramount to achieving satisfactory performances. In the following, the strategies proposed in recent literature to address these problems are discussed. ## 3. DISCRETE-TIME SSL PARAMETRIZATIONS\n\n### 3.1 Discrete-time diagonal parametrization\n\nOne intuitive approach is that of parametrizing the SSL as a discrete-time complex-valued diagonal system, as proposed by Orvieto et al. (2023). In particular, $\\tilde{A}$ can be parametrized as a Schur-stable diagonal matrix\n\n$$\n\\tilde{A}=\\tilde{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n_{\\lambda}}\\right)\n$$\n\nEach eigenvalue $j \\in\\left\\{1, \\ldots, n_{\\lambda}\\right\\}$ is, in turn, parametrized by the modulus $-\\exp \\left(\\mu_{j}\\right) \\in(0,1)$ and the phase $\\exp \\left(\\theta_{j}\\right)$,\n\n$$\n\\lambda_{j}=\\exp \\left(-\\exp \\left(\\mu_{j}\\right)+i \\exp \\left(\\theta_{j}\\right)\\right)\n$$\n\nNote that since $\\left|\\lambda_{j}\\right|<1$ this parametrization guarantees that the SSM is structurally $\\delta$ ISS (Proposition 2). A\nrelevant design choice advocated by Orvieto et al. (2023) is that of reparametrizing $\\tilde{B}$ as\n\n$$\n\\tilde{B}=\\operatorname{diag}\\left(\\gamma_{1}, \\ldots, \\gamma_{n_{\\lambda}}\\right) \\check{B}\n$$\n\nwhere the normalization factor is defined as\n\n$$\n\\gamma_{j}=\\left(1-\\left|\\lambda_{j}\\right|^{2}\\right)^{\\frac{1}{2}}, \\quad \\forall j \\in\\left\\{1, \\ldots, n_{\\lambda}\\right\\}\n$$\n\nThis ensures that the rows of $\\tilde{B}$ are normalized so that white noise inputs yield state trajectories with the same energy content as of the input. Therefore, the set of learnable parameters of this SSL parametrization reads\n\n$$\n\\Theta=\\left\\{\\left\\{\\mu_{j}, \\theta_{j}\\right\\}_{j \\in\\left\\{1, \\ldots, n_{\\lambda}\\right\\}}, \\check{B}, \\tilde{C}, D, F\\right\\}\n$$\n\n### 3.2 Initialization strategy\n\nOrvieto et al. (2023) propose to initialize the dynamics matrix randomly inside a portion of the circular crown lying within the unit circle. Letting $0 \\leqslant \\underline{r}<\\bar{r}<1$ be the minimum and maximum modulus of each eigenvalue, respectively, and $0 \\leqslant \\underline{\\theta}<\\bar{\\theta}<\\pi$ be the minimum and maximum phase,\n\n$$\n\\begin{aligned}\n\\mu_{j} & \\sim \\mathfrak{U}[\\log (-\\log (\\bar{r})), \\log (-\\log (\\underline{r}))] \\\\\n\\theta_{j} & \\sim \\mathfrak{U}[\\log (\\log (\\underline{\\theta})), \\log (\\log (\\bar{\\theta}))]\n\\end{aligned}\n$$\n\nwhere $\\mathfrak{U}[a, b]$ denotes the uniform distribution with support $[a, b]$. The complex matrices $\\check{B}$ and $\\tilde{C}$, and the real matrices $D$ and $F^{1}$ can be initialized with the Xavier initialization method (Kumar, 2017).",
    "deepwienerssm-5": "That is, they are sampled from a Normal distribution whose variance is scaled by a factor proportional to the number of columns. ## 4. CONTINUOUS-TIME REPARAMETRIZATIONS\n\nAnother approach is that of parametrizing the SSL via a discretized continuous-time system, as proposed by Gu et al. (2021) in the context of S4 models. To this end, we let the SSL be parametrized as\n\n$$\n\\left\\{\\begin{array}{l}\n\\dot{x}(t)=A_{c} x(t)+B_{c} u(t) \\\\\n\\eta(t)=C_{c} x(t) \\\\\ny(t)=\\sigma(\\eta(t))+F u(t)\n\\end{array}\\right. $$\n\nwhere - similarly to (2) $-\\left\\{A_{c}, B_{c}, C_{c}\\right\\}$ are structured in terms of the blocks $\\left\\{\\tilde{A}_{c}, \\tilde{B}_{c}, \\tilde{C}_{c}\\right\\}$\n\n$$\n\\begin{aligned}\n& A_{c}=\\left[\\begin{array}{ll}\n\\Gamma \\tilde{A}_{c} & \\\\\n& \\Gamma \\operatorname{conj}\\left(\\tilde{A}_{c}\\right)\n\\end{array}\\right], \\quad B_{c}=\\left[\\begin{array}{c}\n\\Gamma \\tilde{B}_{c} \\\\\n\\Gamma \\operatorname{conj}\\left(\\tilde{B}_{c}\\right)\n\\end{array}\\right] \\\\\n& C_{c}=\\left[\\begin{array}{ll}\n\\tilde{C}_{c} & \\left.\\operatorname{conj}\\left(\\tilde{C}_{c}\\right)\\right]\n\\end{array}\\right. \\end{aligned}\n$$\n\nwith $\\tilde{A}_{c} \\in \\mathbb{C}^{n_{\\lambda} \\times n_{\\lambda}}, \\tilde{B}_{c} \\in \\mathbb{C}^{n_{\\lambda} \\times n_{u}}, \\tilde{C}_{c} \\in \\mathbb{C}^{n_{y} \\times n_{\\lambda}}$. The factor $\\Gamma$ in (8) is a learnable timescale parameter, which can either be a real and positive scalar (Gu et al., 2021) or a real-valued, positive-definite diagonal matrix (Smith et al., 2022), i.e., $\\Gamma=\\operatorname{diag}\\left(\\gamma_{1}, \\ldots, \\gamma_{n_{\\lambda}}\\right)$. Under this parametrization, the discrete-time system matrices $\\{A, B, C, D\\}$ can finally be expressed in terms of their continuous-time counterparts, $\\left\\{A_{c}, B_{c}, C_{c}\\right\\}$, by defining the sampling time $\\tau$ and a discretization method, such as forward Euler, the bilinear transform, or ZeroOrder Hold ( ZOH$)$. As shown in Section 3.1, an additional structure can now be imposed on $\\tilde{A}_{c}$ to mitigate the overparametrization, thus making the training procedure\n\n[^1]more scalable, and to enforce the structural stability of the SSL and hence, owing to Proposition 2, the stability of the SSM. The possible structures of $\\tilde{A}_{c}$ are now discussed. ### 4.1 Continuous-time diagonal parametrization\n\nOne strategy, advocated by Gu et al. (2022) and Smith et al. (2022), is that of parametrizing $\\tilde{A}_{c}$ as a diagonal matrix, i.e. $\\tilde{A}_{c}=\\tilde{\\Lambda}_{c}$, where\n\n$$\n\\tilde{\\Lambda}_{c}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n_{\\lambda}}\\right)\n$$\n\nEach eigenvalue $j \\in\\left\\{1, \\ldots, n_{\\lambda}\\right\\}$ is, in turn, parametrized by the logarithm of its real and imaginary part, denoted as $\\alpha_{j}^{\\mathrm{re}}$ and $\\alpha_{j}^{\\mathrm{im}}$, respectively. That is,\n\n$$\n\\lambda_{j}=-\\exp \\left(\\alpha_{j}^{\\mathrm{re}}\\right)+i \\exp \\left(\\alpha_{j}^{\\mathrm{im}}\\right)\n$$\n\nNote that, because $\\mathfrak{R e}\\left(\\lambda_{j}\\right)<0$, this parametrization structurally guarantees the asymptotic stability of the SSL, and hence the $\\delta$ ISS of the SSM (Proposition 2). Overall, the set of learnable parameters is\n\n$$\n\\Theta=\\left\\{\\left\\{\\alpha_{j}^{\\mathrm{re}}, \\alpha_{j}^{\\mathrm{im}}\\right\\}_{j \\in\\left\\{1, \\ldots, n_{\\lambda}\\right\\}}, \\tilde{B}_{c}, \\tilde{C}_{c}, F, \\Gamma\\right\\}\n$$\n\nAs a side note, it is worth pointing out that the eigenvalues can also be parametrized in terms of the modulus and phase instead, as in (3b). ### 4.2 Continuous-time DPLR parametrization\n\nThe slightly more general Diagonal Plus Low-Rank (DPLR, Gu et al. (2021)) structure has also been proposed. According to this strategy, $\\tilde{A}_{c}$ is parametrized as\n\n$$\n\\tilde{A}_{c}=\\tilde{\\Lambda}_{c}-\\tilde{P} \\tilde{Q}^{*}\n$$\n\nwhere $\\tilde{\\Lambda}_{c}$ is a diagonal component defined as\n\n$$\n\\tilde{\\Lambda}_{c}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n_{\\lambda}}\\right)\n$$\n\nand $\\tilde{P}, \\tilde{Q} \\in \\mathbb{C}^{n_{\\lambda} \\times n_{r}}$ yield a low rank component, with $n_{r} \\ll n_{\\lambda}$. In order to guarantee the structural stability of the SSL, and hence the $\\delta$ ISS of the SSM, one can take $\\tilde{P}=\\tilde{Q}$ (see Gu et al. (2021)) and\n\n$$\n\\lambda_{j}=-\\phi\\left(\\alpha_{j}^{\\mathrm{re}}\\right)+i \\alpha_{j}^{\\mathrm{im}}\n$$\n\nwith $\\phi(\\cdot)$ being any strictly-positive function, applied element-wise, e.g. $\\phi(\\alpha)=\\max (0, \\alpha)+\\varepsilon$, with $\\varepsilon>0$. This ensures the negative definiteness of $\\tilde{A}_{c}$. The set of learnable parameters, under the DPLR parametrization, is\n\n$$\n\\Theta=\\left\\{\\left\\{\\alpha_{j}^{\\mathrm{re}}, \\alpha_{j}^{\\mathrm{im}}\\right\\}_{j \\in\\left\\{1, \\ldots, n_{\\lambda}\\right\\}}, \\tilde{P}, \\tilde{Q}, \\tilde{B}_{c}, \\tilde{C}_{c}, F, \\Gamma\\right\\}\n$$\n\n### 4.3 Initialization strategies\n\nIn the case of continuous-time DPLR parametrizations Gu et al. (2021) proposed to resort to the so-called HiPPO framework (Gu et al., 2020) to initialize the learnable matrices $\\tilde{B}_{c}, \\tilde{\\Lambda}_{c}, \\tilde{P}$, and $\\tilde{Q}$. In particular, being it associated with long-term memory properties, the HiPPO matrix is regarded as a suitable initialization for (11). Such initialization is carried out by ( $i$ ) building the HiPPOLegS matrix, (ii) computing its normal and low-rank components, and ( $i i i$ ) applying the eigenprojection, which yields the diagonal matrix $\\tilde{\\Lambda}_{c}$ and the projected low-rank components $\\tilde{P}$ and $\\tilde{Q}$. The HiPPO framework also provides an initialization for the input matrix $\\tilde{B}_{c}$. For more details,\nthe interested reader is referred to Appendix A. The complex matrix $\\tilde{C}_{c}$ and the real matrix $F$ are randomly initialized, e.g., with the Xavier method. Gu et al. (2022) proposed a similar strategy to initialize the learnable parameters of the continuous-time diagonal parametrizations (10). They suggested to initialize $\\tilde{\\Lambda}_{c}$ with the diagonal component of the normalized HiPPOLegS matrix, discarding the low-rank terms $\\tilde{P}$ and $\\tilde{Q}$. As an alternative, we point out that a strategy similar to the one described in Section 3.2 could be also applied to continuous-time diagonal parametrizations. Given a range for the modulus of eigenvalues, $0<\\underline{r}<\\bar{r}<\\frac{\\pi}{\\tau}$, and for their phase, $\\frac{\\pi}{2}<\\underline{\\theta}<\\bar{\\theta} \\leqslant \\pi$, one can sample\n\n$$\n\\lambda_{j}=r \\exp (i \\theta), \\quad r \\sim \\mathfrak{U}[\\underline{r}, \\bar{r}], \\quad \\theta \\sim \\mathfrak{U}[\\underline{\\theta}, \\bar{\\theta}]\n$$\n\nand initialize $\\alpha_{j}^{\\mathrm{re}}$ and $\\alpha_{j}^{\\mathrm{im}}$ as\n\n$$\n\\alpha_{j}^{\\mathrm{re}}=\\log \\left(-\\mathfrak{R e}\\left(\\lambda_{j}\\right)\\right), \\quad \\alpha_{j}^{\\mathrm{im}}=\\log \\left(\\mathfrak{I m}\\left(\\lambda_{j}\\right)\\right)\n$$\n\nUnder (14), $\\Gamma$ can be initialized to the identity matrix. The complex-valued matrices $\\tilde{B}_{c}$ and $\\tilde{C}_{c}$, and the real-valued matrix $F$ can be randomly initialized. ### 4.4 Comments on the parametrization strategies\n\nFor clarity purposes, the parameterizations have been introduced here in reverse chronological order. The continuoustime DPLR parametrization of S4 has indeed been the one that sparked a renewed research interest in SSMs (Gu et al., 2021). This strategy of parametrizing SSLs in the continuous-time domain has been proposed for two reasons. Firstly, it enables the HiPPO framework to be used for the weights' initialization. Second, it allows simulation of the learned SSMs with a sampling time potentially different from that of the training data, while conventional (discrete-time) RNNs would call for a new training. We point out, however, that the choice of parametrizing SSLs in the continuous-time domain implies the aliasing problem, usually overlooked in the SSMs literature. Since (7) is discretized with the sampling time $\\tau$ of the available training data, one should ensure that $(i)$ the eigenvalues are not initialized beyond the NyquistShannon bandwidth $\\frac{\\pi}{\\tau}$ and (ii) the eigenvalues remains within this bandwidth throughout the model's training. Eigenvalues beyond such bandwidth likely lead to reduced modeling capabilities and poor performances in extrapolation especially when different sampling times are adopted. In this regard, we note that the HiPPO-based initialization strategies proposed by Gu et al.",
    "deepwienerssm-6": "(2021), Gu et al. (2022), and Smith et al. (2022) do not, in general, meet these requirements. It is indeed well-known that the HiPPO matrix's eigenvalues grow quickly with $n_{\\lambda}$. To mitigate this issue, one should initialize the timescale parameter $\\Gamma$ so that $\\tilde{A}_{c}$ has eigenvalues within the Nyquist bandwidth. ${ }^{2}$\n\n## 5. COMPUTATIONAL EFFICIENCY OF SSM\n\nBecause SSMs are learned with a simulation error minimization strategy, it is paramount to compute the output trajectory of the model - given the input sequence $u_{0: T}$\n\n[^2]\u2014 as efficiently as possible. As discussed in Remark 1, an SSM can be simulated by sequentially evaluating its SSLs. In what follows, we therefore focus on how the generic SSL (1) can be efficiently simulated. Since the SSL's dynamics are linear and asymptotically stable, and the static nonlinearity only affects the output variable, the state trajectory can be computed via the truncated Lagrange equation\n$$\nx_{t+1} \\approx \\underbrace{\\left[A^{R-1} B A^{R-2} B \\ldots A B B\\right]}_{\\mathfrak{B}_{R}} \\operatorname{cat}\\left(u_{t-R+1: t}\\right)\n$$\nwith the truncation $R \\leqslant T$ being sufficiently large to ensure that the spectral radius of $A^{R}$ is negligible. In the SSM literature, (15) is known as the convolutional form, as one can retrieve the state trajectory by convolving the input sequence $u_{0: T}$ with the filter $\\mathfrak{B}_{R}$,\n\\[\n\n$$\n\\begin{aligned}\nx_{1: T+1} & =\\operatorname{uncat}\\left(\\mathfrak{B}_{R} \\odot \\operatorname{cat}\\left(u_{0: T}\\right)\\right) \\\\\n\\eta_{t} & =C x_{t}+D u_{t}, \\quad \\forall t \\in\\{0, \\ldots, T\\}\n\\end{aligned}\n$$\n\\]\n\nThe output $y_{0: T}$ can then be straightforwardly computed from $\\eta_{0: T}$ by applying a static output transformation,\n\n$$\ny_{t}=\\sigma\\left(\\eta_{t}\\right)+F u_{t}, \\quad \\forall t \\in\\{0, \\ldots, T\\}\n$$\n\nThe convolutional form (16) enjoys a noteworthy computational efficiency since, as discussed in the remainder of this section, both the materialization of the filter $\\mathfrak{B}_{R}$ and the convolution itself can be easily parallelized. This allows the computational cost of simulating the SSL over a $T$-long input sequence to scale as $\\mathcal{O}(T \\log T)$ in place of the $\\mathcal{O}\\left(T^{2}\\right)$ entailed by the iterative, sequential, application of (1) over the time axis (Smith et al., 2022). The algorithms allowing such a cheap computation of (16a) are now outlined. ### 5.1 Convolution via Parallel Scan\n\nAs proposed by Smith et al. (2022), the task of scalably computing (16a) can be addressed via Parallel Scan (Blelloch, 1990). This approach stems from the idea that intermediate blocks of $\\mathfrak{B}_{R}$ can be computed in parallel and combined, allowing to scale as $\\mathcal{O}\\left(T \\log _{2} T\\right)$ times the complexity of the product $A \\cdot A$. While such a product in the particular case of diagonal parametrization scales as $\\mathcal{O}\\left(n_{\\lambda}\\right)$, for non-diagonal matrices its complexity is $\\mathcal{O}\\left(n_{\\lambda}^{3}\\right)$ thus rapidly increasing with the state dimension. Thus, while the parallel scan approach can, in principle, be applied to any parametrization, it has primarily been utilized for diagonal ones, see Orvieto et al. (2023). For more details on how parallel scan is defined and implemented the reader is addressed to Smith et al.",
    "deepwienerssm-7": "(2022), Appendix H. Remark 3. Given the structure (2) of the SSL (1), one can instead simulate the subsystem $\\tilde{x}_{k+1}=\\tilde{A} \\tilde{x}_{k}+\\tilde{B} u_{k}$ and $\\tilde{\\eta}_{k}=\\tilde{C} \\tilde{x}_{k}+D u_{k}$, and then compute the output as $y_{k}=\\sigma\\left(2 \\mathfrak{R e}\\left(\\tilde{\\eta}_{k}\\right)\\right)+F u_{k}$. This allows for further reduction of the computational burden without any approximations. ### 5.2 Convolution via FFT\n\nFor SSLs parametrized by the DPLR structure described in Section 4.2, it is more efficient to carry out the convolution (16a) in the frequency domain. Operating in this domain, DPLR-parametrized SSLs can be simulated as efficiently as in diagonal parametrizations (Gu et al., 2021). Let us consider, for the purpose of explaination, the case $n_{u}=n_{y}=1 .^{3}$ Then, this approach consists in (i) computing the Fast Fourier Transform (FFT) of the input signal $\\mathcal{U}(\\omega)=\\operatorname{FFT}\\left(u_{0: T}\\right)$, (ii) multiplying it (frequencywise) with the Fourier transform $\\tilde{\\mathcal{H}}(\\omega)$ of the discretized linear subsystem defined by the triplet $\\left(\\tilde{A}_{c}, \\tilde{B}_{c}, \\tilde{C}_{c}\\right)$, $(i i i)$ computing the inverse FFT (iFFT), and (iv) applying the nonlinear output transformation. That is,\n\n$$\n\\begin{array}{rlrl}\n\\tilde{\\eta}_{0: T} & =\\operatorname{iFFT}(\\tilde{\\mathcal{H}}(\\omega) \\cdot \\mathcal{U}(\\omega)) \\\\\n\\eta_{t} & =2 \\mathfrak{R e}\\left(\\tilde{\\eta}_{t}\\right), & & t \\in\\{0, \\ldots, T\\} \\\\\ny_{t} & =\\sigma\\left(\\eta_{t}\\right)+F u_{t}, & & t \\in\\{0, \\ldots, T\\} . \\end{array}\n$$\n\nNote that both the FFT and iFFT operations scale with complexity $\\mathcal{O}(T \\log T)$. Gu et al. (2021) have shown that, for continuous-time DPLR-parametrized SSLs (like S4), the Fourier transform $\\tilde{\\mathcal{H}}(\\omega)$ can be efficiently evaluated by resorting to black-box Cauchy kernels, see Appendix B. With this approach, one issue to pay attention to is frequency leakage due to the input's nonperiodicity. ## 6. NUMERICAL EXAMPLE\n\nTo preliminary quantify the modeling capabilities of the described SSM architectures, we considered the well-known Silverbox benchmark Wigren and Schoukens (2013), for which a public dataset is available. The Silverbox is an electronic circuit that mimics the input-output behavior of a damped mechanical system with a nonlinear elastic coefficient, and it has often been used for benchmarking, e.g., Wiener models (Tiels, 2015).",
    "deepwienerssm-8": "The training and validation data consist of ten experiments in which a multisine input with random frequency components is applied to the apparatus. Each experiment features 8192 input-output datapoints, collected with a sampling rate of 610.35 Hz . The validation dataset has been constructed by extracting 76 subsequences (of length $T=512$ steps) from a random experiment. From the remaining nine experiments 684 training subsequences have been extracted. The Silverbox dataset also contains an independent test dataset that can be used to assess the accuracy of the identified models. This test dataset consists of 40500 samples collected by exciting the system with a filtered Gaussian noise having linearly increasing mean value. As noted by Tiels (2015), the test dataset is characterized by two regions. The first 25000 time-steps allow quantification of the model's accuracy in interpolation, i.e., within operating regions well explored by the training and validation data. The successive time-steps allow, instead, enable the assessment of the model's extrapolation performances in operating regions not explored in the training and validation datasets, particularly in terms of the input signal amplitude. For this reason, the modeling performances scored by the identified SSMs - measured by RMSE [mV] and FIT index [\\%]-are reported both on the overall test dataset and limitedly to the interpolatory regime. ### 6.1 Identification results\n\nThe training procedure was implemented in PyTorch 2.1 and is described in more detail in the accompanying code ${ }^{4}$\n\n[^3]Table 1. Performance of the identified SSMs\n\n| Model | First 25000 steps |  |  | Full |  |\n| :--- | :---: | :---: | :--- | :---: | :---: |\n|  | RMSE | FIT |  | RMSE | FIT |\n| S4 $\\left(L=4, n_{\\lambda}=10\\right)^{5}$ | 0.81 | 97.60 |  | 4.73 | 95.49 |\n| S5 $\\left(L=4, n_{\\lambda}=10\\right)^{5}$ | 0.73 | 97.78 |  | 3.56 | 96.48 |\n| S5R $\\left(L=4, n_{\\lambda}=10\\right)$ | 0.37 | 97.62 |  | 4.58 | 95.92 |\n| LRU $\\left(L=4, n_{\\lambda}=10\\right)$ | 0.73 | 97.71 |  | 4.18 | 96.37 |\n| TCNN (Andersson et al., 2019) | 0.75 | - |  | 4.9 | - |\n| LSTM (Andersson et al., 2019) | 0.31 | - |  | 4.0 | - |\n| BLA (Tiels, 2015) | - | - |  | 13.7 | - |\n| Wiener (Tiels, 2015) | 1.9 | - |  | 9.2 | - |\n| Grey-box NARX(Ljung et al., 2004) | - | - |  | 0.3 | - |\n\nor in Appendix C. Our hope is that the code can help speed up continued research on these architectures. The following SSM configurations have been considered for identification. Additional details about their structures, initialization, and training hyperparameters are reported in Appendix D. S4 (Gu et al., 2021) \u2014 SSM whose layers are parametrized in the continuous-time domain (7)-(8) by a DLPRstructured state matrix (11) initialized via HiPPO. S5 (Smith et al., 2022) - SSM whose layers are parametrized by continuous-time diagonal systems (7)-(9) initialized via HiPPO. S5R - SSM whose layers are parametrized by continuoustime diagonal systems (7)-(9). Similar to S5, but initialized by random sampling of the eigenvalues, (14). LRU (Orvieto et al., 2023) - SSM whose layers are parametrized by discrete-time diagonal systems (2)-(4). In Table 1 the performance metrics scored by the best identified SSM are reported, and they are compared to some of those reported in the literature ${ }^{6}$, while in Appendix D the trajectory of the simulation error is reported. Note that, although these SSMs are more accurate than traditional Wiener models, they are still in line with those achieved by other deep learning models like TCNNs and LSTMs, while featuring significantly less learnable parameters ${ }^{7}$ and high parallelizability of the training procedure. At last, let us point out that training competitive SSMs requires, in general, a careful selection of the architecture hyperparameters. These not only include the number of layers and the state and output size of each layer, but also a suitable initialization strategy for the selected parametrization. The computational efficiency of these models and their reduced parameter footprint come at the cost of a slightly increased architecture design effort. Further testing of SSMs for nonlinear system identification is thus advisable to establish empirical design criteria. ## 7. CONCLUSIONS AND RESEARCH DIRECTIONS\n\nStructured State-space Models represent an interesting approach to identifying deep Wiener models. In this paper we\n5 At least one eigenvalue falls beyond the Nyquist frequency.",
    "deepwienerssm-9": "6 For detailed accounts of the literature, the reader is referred to Andersson et al.",
    "deepwienerssm-10": "(2019) and Maroli et al. (2019). 7 A diagonal SSL parametrized in continuous-time has $n_{\\lambda}\\left(3+2 n_{y}+\\right.$ $\\left.2 n_{u}\\right)+n_{y} n_{u}$ learnable parameters, compared to the $n_{x}\\left(4+3 n_{x}+\\right.$ $\\left.n_{u}+n_{y}\\right)+n_{y}$ learnable parameters of an LSTM layer. summarized recent developments of SSMs in the machine learning community, attempting to disentangle their structure, parameterization, initialization, and simulation aspects. The goal was to make these models more accessible for the system identification community, to stimulate further investigation of these models for identification. We believe there is scope for interesting future developmentspossible research directions are summarized below. Minimal structures Structures with fewer learnable parameters should be considered, as long as they ensure the model's stability and allow for an efficient simulation. In this context, it would be advisable to evaluate whether parameterizations in the transfer function domain, similar to dynoNet Forgione and Piga (2021), yield less overparametrized and more effective models. Data-driven initializations Novel strategies could exploit training data to improve and tailor the initialization of learnable parameters. For Wiener models, for example, initialization based on best linear approximators are known to boost the identified model performances. Discrete- or continuous-time parametrizations? Despite the revived interest in SSMs originated from continuoustime parameterizations, Orvieto et al. (2023) questioned their real need, showing how similar performances can be achieved also with discrete-time parameterizations. This issue should also be further investigated in light of the aliasing problem discussed in Section 4.4. Extensive benchmarking The performances of SSMs for nonlinear system identification should be assessed on a broader set of benchmarks, possibly featuring long-term time dependencies, for which SSMs were proposed. ## REFERENCES\n\nAndersson, C., Ribeiro, A.H., Tiels, K., Wahlstr\u00f6m, N., and Sch\u00f6n, T.B. (2019). Deep convolutional networks in system identification. In 2019 IEEE 58th Conference on Decision and Control (CDC), 3670-3676.",
    "deepwienerssm-11": "IEEE. Angeli, D. (2002). A lyapunov approach to incremental stability properties. IEEE Transactions on Automatic Control, 47(3), 410-421. Bengio, Y., Goodfellow, I., and Courville, A. (2017). Deep learning, volume 1. MIT press Massachusetts, USA.",
    "deepwienerssm-12": "Bianchi, F.M., Maiorino, E., Kampffmeyer, M.C., Rizzi, A., and Jenssen, R. (2017). Recurrent neural networks for short-term load forecasting: an overview and comparative analysis.",
    "deepwienerssm-13": "Springer. Blelloch, G.E. (1990). Prefix sums and their applications. In J.H. Reif (ed.), Synthesis of parallel algorithms. Morgan Kaufmann Publishers Inc. Bonassi, F., Farina, M., Xie, J., and Scattolini, R. (2022). On Recurrent Neural Networks for learning-based control: recent results and ideas for future developments. Journal of Process Control, 114, 92-104. Bonassi, F., La Bella, A., Farina, M., and Scattolini, R. (2024). Nonlinear MPC design for incrementally ISS systems with application to GRU networks.",
    "deepwienerssm-14": "Automatica, $159,111381$.",
    "deepwienerssm-15": "Forgione, M. and Piga, D. (2021). dynoNet: A neural network architecture for learning dynamical systems. International Journal of Adaptive Control and Signal Processing, 35(4), 612-626. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. (2020). Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 1474-1487. Gu, A., Goel, K., Gupta, A., and R\u00e9, C. (2022). On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 35971-35983. Gu, A., Goel, K., and R\u00e9, C. (2021). Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Gupta, A., Gu, A., and Berant, J. (2022). Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35, $22982-22994$. Kumar, S.K. (2017). On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863. Lanzetti, N. et al. (2019). Recurrent neural network based MPC for process industries. In 2019 18th European Control Conference (ECC), 1005-1010. IEEE. Ljung, L., Zhang, Q., Lindskog, P., and Juditski, A. (2004). Estimation of grey box and black box models for nonlinear circuit data. IFAC Proceedings Volumes, 37(13), $399-404$. Marconato, A., Sj\u00f6berg, J., Suykens, J.A., and Schoukens, J. (2013). Improved initialization for nonlinear statespace modeling. IEEE Transactions on instrumentation and Measurement, 63(4), 972-980.",
    "deepwienerssm-16": "Maroli, J.M., \u00d6zg\u00fcner, \u00dc., and Redmill, K. (2019). Nonlinear system identification using temporal convolutional networks: a silverbox study. IFAC-PapersOnLine, $52(29), 186-191$.",
    "deepwienerssm-17": "Miller, J. and Hardt, M. (2019). Stable recurrent models. In International Conference on Learning Representations. ArXiv preprint arXiv:1805.10369. Orvieto, A. et al. (2023). Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349. Ramachandran, P., Zoph, B., and Le Quoc, V. (2017). Searching for activation functions. arXiv preprint arXiv:1710.05941. Schoukens, M. and Tiels, K. (2017). Identification of block-oriented nonlinear systems starting from linear approximations: A survey.",
    "deepwienerssm-18": "Automatica, 85, 272-292.",
    "deepwienerssm-19": "Smith, J.T., Warrington, A., and Linderman, S. (2022). Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations. Sun, Y. and Wei, H.L. (2022). Efficient mask attentionbased narmax (mab-narmax) model identification. In 2022 27th International Conference on Automation and Computing (ICAC), 1-6.",
    "deepwienerssm-20": "IEEE. Tiels, K. (2015). Wiener system identification with generalized orthonormal basis functions. PhD thesis, Vrije Universiteit Brussell. Wigren, T. and Schoukens, J. (2013). Three free data sets for development and benchmarking in nonlinear system identification. In 2013 European control conference $(E C C), 2933-2938$.",
    "deepwienerssm-21": "IEEE. Wills, A. and Ninness, B. (2012). Generalised hammerstein-wiener system estimation and a benchmark application. Control Engineering Practice, 20(11), $1097-1108$. Yu, C., Ljung, L., and Verhaegen, M. (2018). Identification of structured state-space models. Automatica, 90, 54-61. ## Appendix A. HIPPO-LEGS MATRIX\n\nIn this appendix we provide more information on the construction of the HiPPO-LegS matrix, and how this matrix is used to initialize the $\\tilde{\\Lambda}_{c}, \\tilde{P}$, and $\\tilde{Q}$ matrices of the DPLR parameterization (11). The HiPPO framework is built on the idea of projecting the data into a high-dimensional latent space characterized by an orthogonal polynomial basis Gu et al. (2020). Consistently with the Koopman theory, within this latent state any dynamical system can be represented by an infinite-dimensional LTI system. The HiPPO framework gained popularity since both the projection operator and the latent linear representation admit a simple closed-form expression. Depending on the metric used to define the projection operator, different HiPPO matrices can be defined - we here focus on the HiPPO-LegS matrix, constructed by taking a scaled Legendre measure. This matrix has the form\n\n$$\nA_{\\mathrm{LegS}}=A_{\\mathrm{LegS}}^{(N)}-p p^{\\prime}\n$$\n\nwhere $A_{\\mathrm{LegS}}^{(N)} \\in \\mathbb{R}^{n_{\\lambda} \\times n_{\\lambda}}$ is the orthonormal component, and $p \\in \\mathbb{R}^{n_{\\lambda}}$ some 1-rank component. Denoting by $\\left[A_{\\text {LegS }}^{(N)}\\right]_{a, b}$ the element of $A_{\\mathrm{LegS}}^{(N)}$ at position $(a, b)$, it holds that\n\n$$\n\\left[A_{\\mathrm{LegS}}^{(N)}\\right]_{a, b}=- \\begin{cases}-\\left(a-\\frac{1}{2}\\right)^{\\frac{1}{2}} \\cdot\\left(b-\\frac{1}{2}\\right)^{\\frac{1}{2}} & a<b \\\\ \\frac{1}{2} & a=b \\\\ \\left(a-\\frac{1}{2}\\right)^{\\frac{1}{2}} \\cdot\\left(b-\\frac{1}{2}\\right)^{\\frac{1}{2}} & a>b\\end{cases}\n$$\n\nThe low-rank component $p$ is instead defined as\n\n$$\n[p]_{a}=\\left(a-\\frac{1}{2}\\right)^{\\frac{1}{2}}\n$$\n\nLetting the normal component be eigendecomposed as $A_{\\mathrm{LegS}}^{(N)}=V \\Lambda_{c} V^{*}$, the HiPPO-LegS matrix is projected as\n\n$$\nV^{*} A_{\\mathrm{LegS}} V=\\tilde{\\Lambda}_{c}-\\underbrace{V^{*} p}_{\\tilde{P}} \\underbrace{p^{\\prime} V}_{\\tilde{Q}^{*}}:=\\tilde{A}_{c} . $$\n\nThe $\\tilde{B}_{c}$ matrix is also given by the HiPPO framework as\n\n$$\n\\left[\\tilde{B}_{c}\\right]_{a, b}=\\sqrt{2(a-1)+1}\n$$\n\n## Appendix B. FFT-BASED CONVOLUTION FOR DPLR PARAMETRIZATIONS\n\nConsider the transfer function $\\tilde{H}(s)=\\tilde{C}_{c}\\left(s I-\\tilde{A}_{c}\\right)^{-1} \\tilde{B}_{c}$, where the state matrix $\\tilde{A}_{c}$ (8) is structured according to the DPLR parametrization (11) and, without loss of generality, $\\Gamma=1$. Gu et al. (2021) propose to compute $\\tilde{\\eta}_{0: T}$ as the response of $\\tilde{H}(s)$, from which $\\eta$ can then be computed as $\\eta_{t}=2 \\mathfrak{R e}\\left(\\tilde{\\eta}_{t}\\right)$ for any $t \\in\\{0, \\ldots, T\\}$. To retrieve $\\tilde{\\eta}_{0: T}$ efficiently, $\\tilde{H}(s)$ is first discretized via the bilinear transformation $s=\\frac{2}{\\tau} \\frac{1-z^{-1}}{1+z^{-1}}$, which yields\n\n$$\n\\tilde{H}(z)=\\tilde{C}_{c}\\left(\\frac{2}{\\tau} \\frac{1-z^{-1}}{1+z^{-1}} I-\\tilde{A}_{c}\\right)^{-1} \\tilde{B}_{c}\n$$\n\nComputing (B.1) involves inverting an $n_{\\lambda} \\times n_{\\lambda}$ matrix which might be impractical. Gu et al. (2021) noted that since $\\tilde{A}_{c}=\\tilde{\\Lambda}_{c}-\\tilde{P} \\tilde{Q}^{*}$, by applying the Matrix Inversion Lemma (B.1) can be re-worked as\n\n$$\n\\tilde{H}(z)=\\tilde{C}_{c}\\left[\\tilde{W}(z)-\\tilde{W}(z) \\tilde{P}\\left(I+\\tilde{Q}^{*} \\tilde{W}(z) \\tilde{P}\\right)^{-1} \\tilde{Q}^{*} \\tilde{W}(z)\\right]^{-1} \\tilde{B}_{c},\n$$\n\nwhere\n\n$$\n\\tilde{W}(z)=\\left(\\frac{2}{\\tau} \\frac{1-z^{-1}}{1+z^{-1}} I-\\tilde{\\Lambda}_{c}\\right)^{-1}\n$$\n\nNote that (B.2) involves the inversion of an $n_{r} \\times n_{r}$ matrix, where the low-rank dimension $n_{r}$ is often 1 - as in Appendix A. Moreover, $\\tilde{W}(z)$ can be easily computed since $\\tilde{\\Lambda}_{c}$ is diagonal. Finally, the Fourier transform is retrieved evaluating the transfer function on the unit circle,\n\n$$\n\\tilde{\\mathcal{H}}(\\omega)=\\left.\\tilde{H}(z)\\right|_{z=\\exp (i \\omega \\tau)}\n$$\n\nfor $\\omega \\in\\left\\{\\omega_{0}, \\ldots, \\omega_{T-1}\\right\\}$, where $\\omega_{r}=\\frac{2 \\pi}{\\tau} \\frac{r}{T}$. As shown by Gu et al. (2021), (B.2) and (B.3) can be reconducted to a black-box Cauchy kernel, and can be easily (and efficiently) implemented even for high-dimensional systems. ## Appendix C. TRAINING PROCEDURE\n\nLike most RNN architectures, SSMs are learned via simulation error minimization, see Bianchi et al. (2017). To this end, assume that a collection of input-output sequences are collected from the plant to be identified via a suitablydesigned experiment campaign. We let such a dataset be denoted as $\\mathcal{D}=\\left\\{\\left(u_{0: T}^{\\{l\\}}, y_{0: T}^{\\{l\\}}\\right)_{l \\in \\mathcal{I}}\\right\\}$, where the superscript ${ }^{\\{l\\}}$ is used to index the sequences over the set $\\mathcal{I}=\\{1, \\ldots, N\\}$. For compactness, it is assumed that these input-output sequences have a fixed length $T$, and that the inputouput data have been normalized (Bengio et al., 2017). The sequences are partitioned into a training set $\\mathcal{I}_{\\text {tr }} \\subset \\mathcal{I}$ and a validation set $\\mathcal{I}_{\\text {val }} \\subset \\mathcal{I}$, where $\\mathcal{I}_{\\text {tr }} \\cup \\mathcal{I}_{\\text {val }}=\\mathcal{I}$ and $\\mathcal{I}_{\\text {tr }} \\cap \\mathcal{I}_{\\text {val }}=\\varnothing$\nThe training procedure is carried out iteratively. At every such iteration (epoch), the training set is randomly split into $M$ independent partitions (batches), denoted as $\\mathcal{I}^{\\{m\\}}$, with $m \\in\\{1, \\ldots, M\\}$. For each of these batches, the loss function is defined as the average simulation Mean Squared Error (MSE), i.e. $$\n\\mathcal{L}(\\mathcal{I} ; \\Theta)=\\frac{1}{|\\mathcal{I}|} \\sum_{l \\in \\mathcal{I}} \\operatorname{MSE}\\left(y_{0: T}\\left(u_{0: T}^{\\{l\\}}, \\Theta\\right), y_{0: T}^{\\{l\\}}\\right)\n$$\n\nwhere $y_{0: T}\\left(u_{0: T}^{\\{l\\}}, \\Theta\\right)$ denotes the free-run simulation of the SSM (1), computed applying (16) sequentially over the $L$ SSLs. The parameters are then updated via a gradient descent step. In the simplest case of first-order gradient methods, this update reads\n\n$$\n\\Theta \\leftarrow \\Theta-\\rho \\nabla_{\\Theta} \\mathcal{L}\\left(\\mathcal{I}^{\\{m\\}} ; \\Theta\\right)\n$$\n\nwhere $\\nabla_{\\Theta}$ denotes the gradient operator and $\\rho>0$ is the learning rate. The training procedure is generally repeated for a prescribed number of epochs, or until some validation metrics, e.g. $\\mathcal{L}\\left(\\mathcal{I}_{\\mathrm{val}} ; \\Theta\\right)$, stops improving (Bianchi et al., 2017). The resulting training procedure is summarized in Algorithm 1. ```\nAlgorithm 1 Training procedure\n    Selection of the SSM structure and parameterization\n    Initialization of the learnable parameters \\(\\Theta\\)\n    for epoch \\(e \\in\\{0, \\ldots, E\\}\\) do\n        Partition \\(\\mathcal{I}_{\\text {tr }}\\) into random batches \\(\\mathcal{I}^{\\{1\\}}, \\ldots, \\mathcal{I}^{\\{M\\}}\\)\n        for batch \\(m \\in\\{1, \\ldots, M\\}\\) do\n            Compute the loss \\(\\mathcal{L}\\left(\\mathcal{I}^{\\{m\\}} ; \\Theta\\right)\\) via (C.1)\n            Backpropagation e.g. via (C.2)\n        end for\n        Evaluate validation loss \\(\\mathcal{L}\\left(\\mathcal{I}_{\\text {val }} ; \\Theta\\right)\\) for early stopping\n    end for\n```\n\nAppendix D. HYPERPARAMETERS AND PERFORMANCES OF THE TRAINED SSMS\n\nIn this appendix details on the structures, initializations, and training hyperparameters of the SSMs described in Section 6 are reported. Note that, for all these architectures, $(i)$ the ELU activation function (Ramachandran et al., 2017) was used for $\\sigma(\\cdot)$, and (ii) the output size of intermediate layers $(\\ell \\in\\{1, \\ldots, L-1\\})$ was fixed to 4 . S4 - The adopted S4 model consists of $L=4$ SSLs, each parametrized in continuous-time with $n_{\\lambda}=10$ learnable eigenvalues. The HiPPO-LegS matrix (A.2) was used to initialize the DLPR state matrix (11), and (A.3) was used to initialize the input matrix $\\tilde{B}_{c}$. The timescale parameter $\\Gamma$ was initialized randomly in the range $(20,100)$. The output matrix $\\tilde{C}_{c}$ was initialized with the Xavier initialization (Kumar, 2017). S5 - The adopted S5 model features $L=4$ layers with $n_{\\lambda}=10$ learnable eigenvalues parametrized in continuoustime. The diagonal state matrix $\\tilde{\\Lambda}_{c}$ was initialized to the diagonalized normal component of the HiPPO-LegS matrix, while (A.3) was used to initialize the input matrix $\\tilde{B}_{c}$. The scalar $\\Gamma$ was initialized to 25 , whereas $\\tilde{C}_{c}$ was initialized with the Xavier method. S5R - The S5R model has $L=4$ layers with $n_{\\lambda}=10$ learnable eigenvalues parametrized in continuous-time. For the initialization, the eigenvalues of the diagonal matrix $\\tilde{\\Lambda}_{c}$ were randomly sampled using (14), with $(\\underline{r}, \\bar{r})=\\left(0.1 \\frac{\\pi}{\\tau}, \\frac{\\pi}{\\tau}\\right)$ and $(\\underline{\\theta}, \\bar{\\theta})=\\left(\\frac{\\pi}{6}, \\frac{3 \\pi}{4}\\right)$. The scalar $\\Gamma$ was initialized to 0.9 , whereas $\\tilde{B}_{c}$ and $\\tilde{C}_{c}$ were initialized with the Xavier method. LRU - The adopted LRU has $L=4$ layers with $n_{\\lambda}=$ 10 learnable eigenvalues. This model is parametrized in the discrete-time domain. The diagonal state matrix was initialized via (6) by randomly sampling its eigenvalues from the circular crown sector delimited by $(\\underline{r}, \\bar{r})=$ $(0.05,0.975) \\subset(0,1)$ and $(\\underline{\\theta}, \\bar{\\theta})=(0,2 \\pi)$. The Xavier initialization method was used for matrices $\\tilde{\\tilde{B}}$ and $\\tilde{C}$, while $D$ was initialized to the null matrix. The training procedures were conducted with the Adam optimizer (Bengio et al., 2017), with a batch size of 40 and an initial learning rate $\\rho=0.003$. A learning rate scheduler was used to dynamically adjust $\\rho$ on plateaus, reducing it by $20 \\%$ after 30 epochs without improvements on the training data. An early stopping procedure was included to halt the training after 150 epochs without improvement\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_c80c09bf8a76527d7976g-8.jpg?height=950&width=919&top_left_y=270&top_left_x=1055)\nFig. D.1. Free-run simulation error of the trained SSMs (black dotted line) with respect to the ground truth (blue line) over the entire test dataset. on the validation metrics, and still within at most 2750 epochs. In Figure D. 1 the free-run simulation error on the test dataset of each SSM's best training instance is depicted. As expected, this error is fairly limited in the first 25000 time-steps of the test dataset, where the model operates in a region well explored by the training data, while it is larger in the second part of the dataset, where the model operates in extrapolation (Andersson et al., 2019). [^0]:    ^ This research was financially supported by Kjell och M\u00e4rta Beijer Foundation and by the project Deep probabilistic regression - new models and learning algorithms (contract number: 2021-04301), funded by the Swedish Research Council. [^1]:    ${ }^{1}$ If $n_{u}=n_{y}$ the skip connection $F$ is usually fixed to the identity. [^2]:    ${ }^{2}$ In the SSM literature $\\Gamma$ is often randomly sampled from a distribution which is uniform in its log-space (Smith et al., 2022). [^3]:    ${ }^{3}$ This approach has been generalized to the case $n_{u}>1$ and/or $n_{y}>1$ by applying it separately on each input-output pair. ${ }^{4}$ Source code: https://github.com/bonassifabio/SSM-sysid\n\n"
}