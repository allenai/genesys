{
    "ssaugtrans-0": "Efficient Long Sequence Modeling via State Space Augmented Transformer\n\nSimiao Zuo, Xiaodong Liu\u2217, Jian Jiao\u2020\u22c4, Denis Charles\u22c4, Eren Manavoglu\u22c4, Tuo Zhao\u2021 and Jianfeng Gao\u22c4 \u2021Georgia Institute of Technology \u22c4Microsoft Simiao Zuo and Xiaodong Liu made equal contributions.",
    "ssaugtrans-1": "Work was done during Simiao Zuo\u2019s internship at Microsoft.",
    "ssaugtrans-2": "Correspondence to simiaozuo@gatech.edu and xiaodl@microsoft.com. Jian Jiao and Xiaodong Liu led the project. Abstract\n\nTransformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State sPace AugmenteD TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model (250M parameters) consumes over 80G of GPU memory when the sequence length is 8k. Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models\u2019 practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a 57.5% accuracy, nearly 30% less than state-of-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are proposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models\u2019 ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State sPace AugmenteD TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models\u2019 ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and fine-tuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines. Finally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code111https://github.com/microsoft/EfficientLongSequenceModeling and pre-trained model checkpoints222https://github.com/namisan/mt-dnn are publicly available. 2 Background\n\n2.1 Attention Mechanism\n\nSuppose the input to the layer is , where is the sequence length and is the embedding dimension, then the attention mechanism outputs\n\nAttn \u200b ( \ud835\udc17 ) = softmax \u200b ( \ud835\udc10\ud835\udc0a \u22a4 d ) \u200b \ud835\udc15 , Attn \ud835\udc17 softmax superscript \ud835\udc10\ud835\udc0a top \ud835\udc51 \ud835\udc15 \\displaystyle\\mathrm{Attn}(\\mathbf{X})=\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d}}\\right)\\mathbf{V}, (1) where \u200b \ud835\udc10 = \ud835\udc17\ud835\udc16 q , \ud835\udc0a = \ud835\udc17\ud835\udc16 k , \ud835\udc15 = \ud835\udc17\ud835\udc16 v .",
    "ssaugtrans-3": "formulae-sequence where \ud835\udc10 subscript \ud835\udc17\ud835\udc16 \ud835\udc5e formulae-sequence \ud835\udc0a subscript \ud835\udc17\ud835\udc16 \ud835\udc58 \ud835\udc15 subscript \ud835\udc17\ud835\udc16 \ud835\udc63 \\displaystyle\\text{where }\\mathbf{Q}=\\mathbf{X}\\mathbf{W}_{q},\\ \\mathbf{K}=\\mathbf{X}\\mathbf{W}_{k},\\ \\mathbf{V}=\\mathbf{X}\\mathbf{W}_{v}. Here are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix . Then, captures the alignment between the -th and the -th input tokens. 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a -dimensional input signal to a -dimensional latent state , after which is mapped to a -dimensional output signal . Concretely,\n\nx \u2032 \u200b ( t ) = \ud835\udc00 \u200b x \u200b ( t ) + \ud835\udc01 \u200b u \u200b ( t ) , y \u200b ( t ) = \ud835\udc02 \u200b x \u200b ( t ) . formulae-sequence superscript \ud835\udc65 \u2032 \ud835\udc61 \ud835\udc00 \ud835\udc65 \ud835\udc61 \ud835\udc01 \ud835\udc62 \ud835\udc61 \ud835\udc66 \ud835\udc61 \ud835\udc02 \ud835\udc65 \ud835\udc61 \\displaystyle x^{\\prime}(t)=\\mathbf{A}x(t)+\\mathbf{B}u(t),\\quad y(t)=\\mathbf{C}x(t). (2)\n\nHere, , and . Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters , and cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, high-order polynomial projection operators) are proposed to initialize . The HiPPO matrices are designed such that the state at time can memorize the history of the input up to time . Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs , where is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size , such that\n\nx k = \ud835\udc00 \u00af \u200b x k \u2212 1 + \ud835\udc01 \u00af \u200b u k , y k = \ud835\udc02 \u00af \u200b x k , formulae-sequence subscript \ud835\udc65 \ud835\udc58 \u00af \ud835\udc00 subscript \ud835\udc65 \ud835\udc58 1 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 subscript \ud835\udc66 \ud835\udc58 \u00af \ud835\udc02 subscript \ud835\udc65 \ud835\udc58 \\displaystyle x_{k}=\\overline{\\mathbf{A}}x_{k-1}+\\overline{\\mathbf{B}}u_{k},\\quad y_{k}=\\overline{\\mathbf{C}}x_{k}, (3) where \u200b \ud835\udc00 \u00af = ( \ud835\udc08 \u2212 \u0394 / 2 \u22c5 \ud835\udc00 ) \u2212 1 \u200b ( \ud835\udc08 + \u0394 / 2 \u22c5 \ud835\udc00 ) , where \u00af \ud835\udc00 superscript \ud835\udc08 \u22c5 \u0394 2 \ud835\udc00 1 \ud835\udc08 \u22c5 \u0394 2 \ud835\udc00 \\displaystyle\\text{where }\\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta/2\\cdot\\mathbf{A})^{-1}(\\mathbf{I}+\\Delta/2\\cdot\\mathbf{A}), \ud835\udc01 \u00af = ( \ud835\udc08 \u2212 \u0394 / 2 \u22c5 \ud835\udc00 ) \u2212 1 \u200b \u0394 \u200b \ud835\udc01 , \ud835\udc02 \u00af = \ud835\udc02 . formulae-sequence \u00af \ud835\udc01 superscript \ud835\udc08 \u22c5 \u0394 2 \ud835\udc00 1 \u0394 \ud835\udc01 \u00af \ud835\udc02 \ud835\udc02 \\displaystyle\\quad\\quad\\ \\ \\ \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta/2\\cdot\\mathbf{A})^{-1}\\Delta\\mathbf{B},\\quad\\overline{\\mathbf{C}}=\\mathbf{C}. We unroll the above recurrent representation, after which we have\n\ny k = \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af k \u200b \ud835\udc01 \u00af \u200b u 0 + \u22ef + \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u k \u2212 1 + \ud835\udc02 \u00af \u200b \ud835\udc01 \u00af \u200b u k . subscript \ud835\udc66 \ud835\udc58 \u00af \ud835\udc02 superscript \u00af \ud835\udc00 \ud835\udc58 \u00af \ud835\udc01 subscript \ud835\udc62 0 \u22ef \u00af \ud835\udc02 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 1 \u00af \ud835\udc02 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 \\displaystyle y_{k}=\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}^{k}\\overline{\\mathbf{B}}u_{0}+\\cdots+\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{k-1}+\\overline{\\mathbf{C}}\\overline{\\mathbf{B}}u_{k}. This can be written as a convolutional representation , where the convolution kernel\n\n\ud835\udc0a \u00af \u2208 \u211d L = ( \ud835\udc02 \u00af \u200b \ud835\udc01 \u00af , \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af , \u22ef , \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af L \u2212 1 \u200b \ud835\udc01 \u00af ) . \u00af \ud835\udc0a superscript \u211d \ud835\udc3f \u00af \ud835\udc02 \u00af \ud835\udc01 \u00af \ud835\udc02 \u00af \ud835\udc00 \u00af \ud835\udc01 \u22ef \u00af \ud835\udc02 superscript \u00af \ud835\udc00 \ud835\udc3f 1 \u00af \ud835\udc01 \\displaystyle\\overline{\\mathbf{K}}\\in\\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C}}\\overline{\\mathbf{B}},\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}\\overline{\\mathbf{B}},\\cdots,\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}^{L-1}\\overline{\\mathbf{B}}\\right). (4)\n\nHere, \u201c\u201d is the discrete convolution operator, represents the input sequence , and represents the corresponding output sequence . In Eq. 4, the output can be computed efficiently given that the convolution kernel is known. However, computing the kernel is non-trivial. Most of existing algorithms have time and space complexity. Structured State Space Sequence model (S4). Gu et al. (2021) develop the S4 model to efficiently compute Eq.",
    "ssaugtrans-4": "4. Specifically, in Eq. 2 is randomly initialized, and and are initialized as\n\n\ud835\udc00 = \ud835\udc00 ( d s ) \u2212 \ud835\udc0f\ud835\udc0f \u22a4 , \ud835\udc01 i = ( 2 \u200b i + 1 ) 1 2 , formulae-sequence \ud835\udc00 superscript \ud835\udc00 subscript \ud835\udc51 \ud835\udc60 superscript \ud835\udc0f\ud835\udc0f top subscript \ud835\udc01 \ud835\udc56 superscript 2 \ud835\udc56 1 1 2 \\displaystyle\\mathbf{A}=\\mathbf{A}^{(d_{s})}-\\mathbf{P}\\mathbf{P}^{\\top},\\quad\\mathbf{B}_{i}=(2i+1)^{\\frac{1}{2}}, (5) where \u200b \ud835\udc0f i = ( i + 1 / 2 ) 1 / 2 , where subscript \ud835\udc0f \ud835\udc56 superscript \ud835\udc56 1 2 1 2 \\displaystyle\\text{where }\\mathbf{P}_{i}=\\left(i+1/2\\right)^{1/2}, \ud835\udc00 i \u200b j ( d s ) = \u2212 { ( i + 1 2 ) 1 2 \u200b ( j + 1 2 ) 1 2 , i > j , 1 2 , i = j , \u2212 ( i + 1 2 ) 1 2 \u200b ( j + 1 2 ) 1 2 , i < j . subscript superscript \ud835\udc00 subscript \ud835\udc51 \ud835\udc60 \ud835\udc56 \ud835\udc57 cases superscript \ud835\udc56 1 2 1 2 superscript \ud835\udc57 1 2 1 2 \ud835\udc56 \ud835\udc57 1 2 \ud835\udc56 \ud835\udc57 superscript \ud835\udc56 1 2 1 2 superscript \ud835\udc57 1 2 1 2 \ud835\udc56 \ud835\udc57 \\displaystyle\\mathbf{A}^{(d_{s})}_{ij}=-\\begin{cases}(i+\\frac{1}{2})^{\\frac{1}{2}}(j+\\frac{1}{2})^{\\frac{1}{2}},&i>j,\\\\\n\\frac{1}{2},&i=j,\\\\\n-(i+\\frac{1}{2})^{\\frac{1}{2}}(j+\\frac{1}{2})^{\\frac{1}{2}},&i<j.\\\\\n\\end{cases}\n\nSubsequently, the convolution kernel in Eq.",
    "ssaugtrans-5": "4 can be computed efficiently with time and space complexity. Subsequently, for an input , the S4 output can be computed efficiently. 3 Method\n\nWe first conduct experiments to demonstrate that SSMs do not model local information well. Then, we present SPADE, which effectively combines global and local information by augmenting SSMs into the Transformer architecture. 3.1 Attention vs. State Space Models\n\nThe motivation behind SPADE is that even though SSMs perform well on several long sequence classification tasks (Gu et al., 2021), they perform poorly on language modeling, which is a fundamental task in natural language processing. To demonstrate such an observation, we compare S4 with Transformer with full attention and Transformer with local (window and chunk) attention. In local attention, each token can only attend to its neighboring tokens (see Figure 1 for illustrations). We conduct experiments on token-level language modeling. In this setting, local information is more important than global information. This is because in practice, we rarely see words (tokens) that are thousands of positions apart exhibit strong dependencies (Sukhbaatar et al., 2019). Experimental results are illustrated in Figure 2. We see that both Transformer with full attention and Transformer with local attention (e.g., window and chunk) outperforms S4. Notice that replacing full attention with local attention does not significantly hurt model performance, indicating that local information is more important in this setting. We remark that SSMs such as S4 produces a fixed dependency pattern, e.g., the convolution kernel in Eq. 4. Moreover, unlike the attention mechanism, SSMs do not explicitly compute dependencies between tokens. Therefore, SSMs are not refined enough to capture local information, such that they perform poorly on language modeling tasks. 3.2 SPADE: State Space Augmented Transformer\n\nWe propose SPADE, which is a multi-layer Transformer model that can capture complicated global and local information. The overall architecture of SPADE is shown in Figure 3 (left). The proposed model employs a hierarchical structure. Specifically, at the bottom layer of SPADE (termed the global layer), we capture global dependencies using a SSM. Because the SSM only provides coarse global information, the subsequent local layers facilitate the model to handle more refined and complicated local dependencies. In other words, the SSM induces a strong structural bias that augments global information to the inputs. To instantiate the local layer, we replace the full attention in the conventional Transformer layer with off-the-shelf efficient local attention methods. SPADE is flexible to accommodate different approaches, such as window attention and chunk attention (see Figure 1 for illustrations). In the global layer (Figure 3, right), given the input to the layer, we have the output as\n\n\ud835\udc17 local = Local \u200b ( LN \u200b ( \ud835\udc17 ) ) , subscript \ud835\udc17 local Local LN \ud835\udc17 \\displaystyle\\mathbf{X}_{\\text{local}}=\\mathrm{Local}\\left(\\mathrm{LN}(\\mathbf{X})\\right), \ud835\udc17 global = SSM \u200b ( LN \u200b ( \ud835\udc17 ) ) , subscript \ud835\udc17 global SSM LN \ud835\udc17 \\displaystyle\\mathbf{X}_{\\text{global}}=\\mathrm{SSM}\\left(\\mathrm{LN}(\\mathbf{X})\\right), \ud835\udc17 a = \ud835\udc16 \u200b [ LN \u200b ( \ud835\udc17 local ) , LN \u200b ( \ud835\udc17 global ) ] + \ud835\udc17 , subscript \ud835\udc17 \ud835\udc4e \ud835\udc16 LN subscript \ud835\udc17 local LN subscript \ud835\udc17 global \ud835\udc17 \\displaystyle\\mathbf{X}_{a}=\\mathbf{W}\\left[\\mathrm{LN}(\\mathbf{X}_{\\text{local}}),\\mathrm{LN}(\\mathbf{X}_{\\text{global}})\\right]+\\mathbf{X}, \ud835\udc18 = FFN \u200b ( LN \u200b ( \ud835\udc17 a ) ) + \ud835\udc17 a . \ud835\udc18 FFN LN subscript \ud835\udc17 \ud835\udc4e subscript \ud835\udc17 \ud835\udc4e \\displaystyle\\mathbf{Y}=\\mathrm{FFN}\\left(\\mathrm{LN}(\\mathbf{X}_{a})\\right)+\\mathbf{X}_{a}. Here, denotes layer normalization (Ba et al., 2016), denotes a two-layer feed-forward neural network, and is a trainable weight that combines local and global representations. Notice that we apply normalization to and to align their scales. In this work, we choose S4 as the state space model. We remark that because of the sequential nature of SSMs (Eq. 3), the global layer can encode positional information of the inputs. Therefore, we do not need additional fixed-length positional embedding techniques (Devlin et al., 2019). Such a property enables SPADE to extrapolate to longer sequence length during testing, e.g., we can train a model with sequence length 512 and test the model with sequence length 1k. 4 Experiments\n\nIn the experiments, we implement all the models using PyTorch (Paszke et al., 2019) and Fairseq (Ott et al., 2019). Training details such as hyper-parameter settings are deferred to the appendix. 4.1 Long Range Arena\n\nDataset. We evaluate the effectiveness of the proposed model on Long Range Arena (LRA, Tay et al. 2021b), which is a benchmark tailored for evaluating models\u2019 ability in modeling long sequences. The benchmark contains six tasks: ListOps, which tests the capability of modeling hierarchically structured data (Nangia and Bowman, 2018); byte-level text classification on the IMDB movie review dataset (Text, Maas et al. 2011); byte-level document retrieval on the ACL anthology network (Retrieval, Radev et al. 2013); pixel-level image classification on CIFAR-10 (Image, Krizhevsky et al. 2009); Pathfinder, which tests the capability in modeling spatial dependency (Linsley et al., 2018); and a longer version of Pathfinder (Path-X, Tay et al.",
    "ssaugtrans-6": "2021b). Models. Following the standard setting (Tay et al., 2021b), we use small models (less than 2M parameters) for all the tasks. We limit the computational budget such that all the models are trained with similar speed for the same amount of time. To aggregate local information, we consider two approaches: window attention and chunk attention. For window attention, we sparsify the conventional softmax attention (termed softmax-window); and for chunk attention, we sparsify MEGA (Ma et al., 2022), which employs a gated attention technique (termed MEGA-chunk). For window attention, we set the window size to 128, except Path-X, where we set the window size to 1024. For chunk attention, we set the chunk size to 128, except Path-X, where we set the chunk size to 4096.",
    "ssaugtrans-7": "Results. Experimental results are summarized in Table 1. We see that both variants of SPADE (softmax-window and MEGA-chunk) significantly outperform all the baselines in terms of average accuracy. For example, the window attention variant outperforms the best-performing baseline (MEGA-chunk) by 0.5%, and the chunk attention variant has a 1.8% performance gain. Therefore, SPADE is more suitable to model long sequences than existing approaches. 4.2 Language Modeling\n\nWe further evaluate our model by conducting language modeling experiments on Wikitext-103. The dataset contains English-language Wikipedia articles, and the total number of tokens is 103M. In all the experiments, we follow the settings in Baevski and Auli (2019), where we use a large-scale Transformer model with 16 layers and about 250M parameters. We set the input sequence length to 3k and train for 286k steps. Similar to the LRA experiments, we equip SPADE with either window attention (softmax-window) or chunk attention (MEGA-chunk). Additionally, we evaluate another efficient attention variant: FLASH-chunk, where we sparsify FLASH (Hua et al., 2022), a gated attention method similar to MEGA.",
    "ssaugtrans-8": "Experimental results are presented in Table 2. From the results, we see that by combining global and local information, the proposed model achieves significant performance improvement and outperform all the baselines. For example, the vanilla window attention has a 19.7 perplexity on the test set, and by integrating a SSM into SPADE, we achieve a 1.2 perplexity gain. We remark that SPADE with softmax-window is not only significantly faster than the Transformer with full attention, but also yields a better performance. Remark. We remark that we do not need to train the S4 in the bottom layer of SPADE to achieve the performance in Table 2. That is, we initialize the parameters in S4 using Eq. 5, and the parameters are frozen during training. This is because even without training, the initialization of S4 yields intriguing theoretical properties, which facilitates S4\u2019s ability to capture global information. 5 Language Model Pre-Training\n\nWe implement model pre-training using Fairseq, and we implement model fine-tuning using MT-DNN (Liu et al., 2019a, 2020b).",
    "ssaugtrans-9": "Note that all our experiments only use single task fine-tuning. Details such as hyper-parameter settings are deferred to the appendix. 5.1 Pre-Training Details\n\nTo demonstrate the scalability of the proposed method, we pre-train an encoder-decoder variant of SPADE. The model architecture is the same as T5base (Raffel et al., 2020), except that we use post-layernorm instead of pre-layernorm to improve model performance Liu et al.",
    "ssaugtrans-10": "(2020a); Xiong et al. (2020). The embedding dimension is 768, the hidden dimension of the FFN is 3072, the number of attention heads is 12, and both the encoder and the decoder have 12 layers. We add a S4 module to the bottom layer of SPADE, and the parameters of the S4 are fixed after initialization (Eq.",
    "ssaugtrans-11": "5). We use the softmax-window attention as the local information extractor, where we set the window size to 128. The model contains about 290M parameters. We consider two pre-training settings with different datasets and number of training steps:\n\nSPADEbase: We follow the pre-training settings in BERT (Devlin et al., 2019). Specifically, we train the model on Wikipedia (Devlin et al., 2019) and BookCorpus (Zhu et al., 2015). We set the sequence length to 1024 and the batch size to 2048. We train the model for 125k steps. SPADEbase++: We follow the pre-training settings in RoBERTa (Liu et al., 2019b). Specifically, we train the model on Wikipedia (Devlin et al., 2019), BookCorpus (Zhu et al., 2015), STORIES (Trinh and Le, 2018), CC-News (Liu et al., 2019b), and OpenWebText (Gokaslan et al., 2019). We train the model for 2M steps, with sequence length 1024 and batch size 2048. We remark that because the S4 module is not trained after proper initialization, and we do not use fixed-length positional embedding, our pre-trained model can extrapolate to any sequence length. For example, we can set the sequence length to 2k during fine-tuning, which is longer than the sequence length used in pre-training. 5.2 Natural Language Understanding\n\nWe fine-tune the pre-trained models on the General Language Understanding Evaluation (GLUE) benchmark Wang et al. (2019), which is a collection of natural language understanding tasks. The benchmark includes two single-sentence classification tasks: CoLA (Warstadt et al., 2019) is a linguistic acceptability task; and SST-2 (Socher et al., 2013) is a binary classification task that classifies movie reviews to positive or negative. The benchmark also contains three similarity and paraphrase tasks: STS-B (Cer et al., 2017) is a text similarity task; MRPC (Dolan and Brockett, 2005) is a paraphrase detection task; and QQP is a duplication detection task. Additionally, there are natural language inference tasks: MNLI (Williams et al., 2018); QNLI (Rajpurkar et al., 2016); RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009).",
    "ssaugtrans-12": "Dataset details are summarized in Appendix B. We do not consider the long sequence setting in these tasks. All models (T5 and SPADE) are fine-tuned under the sequence length of 512. Experimental results are presented in Table 3. We see that SPADEbase++ significantly outperforms T5base. For example, T5base has a 85.1 average score, and our model outperforms it by 1.7 average score. Recall that the sequence length is set to 512, which is the standard setting instead of the long-sequence setting. Therefore, the results indicate that SPADE is universal in that it is suitable to model both long and short sequences. 5.3 Natural Language Generation\n\nWe also fine-tune the pre-trained models on several abstractive summarization datasets.",
    "ssaugtrans-13": "The sources and statistics are summarized in Table 4. We use ROUGE-2 scores as the evaluation metric. We compare SPADE with LongT5 (Guo et al., 2022), which is a T5 variant that is tailored for long sequences. Note that LongT5 uses PEGASUS-style (Zhang et al., 2020) Principle Sentences Generation as its pre-training objective, such that LongT5 is tailored for natural language generation tasks (e.g., summarization). In contrast, SPADE is a more general model in that it also has superior performance on natural language understanding tasks. We remark that LongT5 uses C4 (Raffel et al., 2020) as its pre-training dataset, which is much larger and diverse than the dataset we adopted. Experimental results are summarized in Table 5. From the results, we see that our model significantly outperforms LongT5. Note that SPADEbase++ have about 290M parameters, while LongT5large contains about 770M parameters and LongT5xl contains about 3B parameters. From the results, we see that in all the tasks, our base-sized models have on par or better performance compared with LongT5large. On the MultiNews dataset, our model even outperforms LongT5xl, which is over ten times larger. 6 Analysis\n\n6.1 Efficiency Comparison\n\nWe compare the efficiency of SPADE with other models: Transformer with softmax-window attention, Trnasformer with MEGA-chunk attention, and S4. The results are illustrated in Figure 4. We see that SPADE is efficient in terms of both training speed and GPU memory usage. For example, when the sequence length is 6k, Transformer uses about 60GB of GPU memory, whereas SPADE with softmax-window uses only 27GB. Moreover, notice that SPADE also trains significantly faster than the vanilla Transformer under all settings. Notice that S4 may be less efficient than the vanilla Transformer (e.g., when the sequence length is 3k). This is because in the multi-layer model in Gu et al. 2021, each layer contains multiple S4 modules and expensive non-linear components. Therefore, the per-layer computational cost can exceed full attention when the sequence is not extremely long. We remark that even though we add a S4 module to the bottom layer of SPADE, such an additional module does not induce much computational overhead. We see that both the training speed and the memory usage of SPADE with softmax-window is only marginally different from those of window-attention Transformer. We have similar observations for the chunk attention variant. 6.2 Location and Number of Global Layers\n\nRecall that in SPADE, the bottom layer is equipped with a SSM and serves as the global layer, while the rest are local layers (see Figure 3). In Figure 5, we empirically justify this design choice. We first investigate the possibility of incorporating more global layers: we set the bottom 1 (the default choice), 4, 8, and 16 (all) layers as global layers. From the results, we see that model performance decreases as we use more global layers. This is because the SSM in the bottom layer captures and filters out global information, such that subsequent SSMs only introduce noise to the intermediate representations. We also investigate whether the global layer can be the top instead of the bottom layer in SPADE. From Figure 5, we see that model performance drops significantly. This is because as a global information extractor, the global layer encodes positional information, on which the local attention modules rely. Therefore, using the global layer as the top layer is akin to using Transformer models without positional encoding, which will yield unsatisfactory performance. 6.3 Different Configurations\n\nWe examine how performance changes when we change the sequence length and window size. From Figure 6 (left), we see that when we increase the sequence length from 512 to 3k, performance of Transformer with full attention increases. However, when we further increase the sequence length to 4k, model performance drastically drops. This is because in long sequences, the signal-to-noise ratio is low, such that the full attention may easily fit to the noise. From Figure 6 (right), we see that performance of Transformer with window attention increases when we increase the window size. Moreover, model performance is better with shorter sequences for the same window size. Such findings indicate that performance of window attention depends on the proportion of information within its perception. From Table 6, we see that for the same sequence length, performance of SPADE increases when we increase the window size. Also, we see that performance of SPADE marginally decreases when we increase the sequence length from 4k to 6k. Recall from Figure 6 (left) that performance of Transformer with full attention drastically deteriorates when we increase the length from 3k to 4k. Such a result indicates that the proposed model is more suitable to model long sequences. 7 Related Works\n\n7.1 Efficient Transformer Models\n\nIn Eq. 1, we have , such that computing the attention introduces time and space costs. Such quadratic costs are prohibitive when the sequence length is large. There are various attempts to reduce the quadratic time and space complexity of the vanilla attention. One approach is to employ sparse attention. That is, each token only attends to a subset of all the tokens according to pre-defined patterns, e.g., neighboring tokens within a fixed size window. Some examples include Sparse Transformer (Child et al., 2019), BlockBERT (Qiu et al., 2020), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), BigBird (Zaheer et al., 2020), HEPOS (Huang et al., 2021), and Poolingformer (Zhang et al., 2021). Another approach is to use low-rank projection. For example, in Linformer (Wang et al., 2020b), the attention mechanism in Eq. 1 becomes . Here, the two additional parameters satisfy , where is the projection rank such that . Similar methods include Nystr\u00f6mformer (Xiong et al., 2021), Synthesizer (Tay et al., 2021a), Transformer-LS (Zhu et al., 2021a), and Luna (Ma et al., 2021).",
    "ssaugtrans-14": "However, these approaches face difficulty when handling causal tasks, such as auto-regressive language modeling. Specifically, in Eq. 1, we mask out the upper triangular part in the attention score matrix such that each token can only attend to its previous tokens. However, this is implausible in Linformer since we project the matrix to a matrix. Kernel-based approaches can be used to approximate the full attention . In these approaches, the quadratic-time softmax attention is replaced by fast linear-time kernel approximations (e.g., Gaussian and arc-cosine kernel). Some examples include Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention (Peng et al., 2021), and FMMformer (Nguyen et al., 2021). Both low-rank projection and kernel-based approaches approximate the full attention, and thus, they often suffer from non-negligible approximation error. We can also adopt clustering-based approaches, where we divide or into several clusters, and only perform inter-cluster attention. Such methods include Reformer (Kitaev et al., 2020), Clusterformer (Wang et al., 2020a), Sinkhorn Transformer (Tay et al., 2020), Fast Transformer (Vyas et al., 2020), Routing Transformer (Roy et al., 2021), and FLASH (Hua et al., 2022). 7.2 Pre-Trained Language Models\n\nPre-trained language models (Devlin et al., 2019; Liu et al., 2019b; Raffel et al., 2020; Brown et al., 2020; He et al., 2021) have achieved state-of-the-art performance on various natural language processing tasks. However, most of these models are not suitable for long sequences. For example, BERT (Devlin et al., 2019) uses a fixed-length positional embedding, such that it cannot handle sequences with length more than 512. In contrast, LongT5 (Guo et al., 2022) facilitates training on long sequences by leveraging relative positional embedding (Shaw et al., 2018) and efficient attention methods. The model targets long sequence modeling tasks such as text summarization. 8 Conclusion\n\nIn this work, we propose SPADE, a state space augmented Transformer model that targets long sequence modeling. SPADE is a multi-layer Transformer model, where the bottom layer is a global layer and the rest are local layers. In the global layer, we use a SSM to augment coarse global information, which are subsequently refined by the following local layers. We instantiate the local layers with off-the-shelf efficient attention methods, such as window attention. The proposed model has linear time and space computationally complexity, facilitating it to handle long sequences. We conduct extensive experiments on the Long Range Arena (LRA) benchmark and language modeling datasets to demonstrate the effectiveness and efficiency of SPADE. We also pre-train encoder-decoder models to demonstrate the scalability of SPADE, and we perform fine-tuning experiments on natural language understanding (GLUE) and natural language generation (summarization) tasks. In all the experiments, SPADE exhibits superior performance and outperforms the baselines. Acknowledgments\n\nWe thank Hao Cheng, Bin Yu, Jianwei Yang, Baolin Peng, Minjia Zhang and Linyuan Gong for valuable discussions and comments, and Microsoft Research Technology Engineering team for setting up GPU machines. References\n\nAinslie et al. (2020) Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268\u2013284, Online.",
    "ssaugtrans-15": "Association for Computational Linguistics. Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. ArXiv preprint, abs/1607.06450. Baevski and Auli (2019) Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Bar-Haim et al. (2006) Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment. Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.",
    "ssaugtrans-16": "ArXiv preprint, abs/2004.05150. Bentivogli et al. (2009) Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC\u201909.",
    "ssaugtrans-17": "Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Cer et al. (2017) Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancouver, Canada.",
    "ssaugtrans-18": "Association for Computational Linguistics. Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. ArXiv preprint, abs/1904.10509. Choromanski et al. (2021) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.",
    "ssaugtrans-19": "Colwell, and Adrian Weller. 2021. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
    "ssaugtrans-20": "OpenReview.net. Cohan et al. (2018) Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615\u2013621, New Orleans, Louisiana.",
    "ssaugtrans-21": "Association for Computational Linguistics. Dagan et al. (2006) Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW\u201905, pages 177\u2013190, Berlin, Heidelberg.",
    "ssaugtrans-22": "Springer-Verlag. Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978\u20132988, Florence, Italy. Association for Computational Linguistics. Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). Fabbri et al. (2019) Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074\u20131084, Florence, Italy. Association for Computational Linguistics. Giampiccolo et al. (2007) Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1\u20139, Prague.",
    "ssaugtrans-23": "Association for Computational Linguistics. Gokaslan et al. (2019) Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus. Gu et al. (2020) Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2020. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Gu et al. (2021) Albert Gu, Karan Goel, and Christopher R\u00e9. 2021. Efficiently modeling long sequences with structured state spaces. ArXiv preprint, abs/2111.00396. Guo et al. (2022) Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724\u2013736, Seattle, United States. Association for Computational Linguistics. He et al. (2021) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Hua et al. (2022) Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099\u20139117. PMLR. Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online. Association for Computational Linguistics. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5156\u20135165. PMLR. Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
    "ssaugtrans-24": "Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466. Lee-Thorp et al. (2022) James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2022. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4296\u20134313, Seattle, United States. Association for Computational Linguistics. Lin et al. (2022) Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI Open. Linsley et al. (2018) Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. 2018. Learning long-range spatial dependencies with horizontal gated recurrent units. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 152\u2013164. Liu et al. (2020a) Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020a. Understanding the difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5747\u20135763, Online. Association for Computational Linguistics. Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487\u20134496, Florence, Italy. Association for Computational Linguistics. Liu et al. (2020b) Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He, Weizhu Chen, Hoifung Poon, Guihong Cao, and Jianfeng Gao. 2020b. The Microsoft toolkit of multi-task deep neural networks for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 118\u2013126, Online. Association for Computational Linguistics. Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692. Ma et al. (2021) Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34. Ma et al. (2022) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2022. Mega: Moving average equipped gated attention.",
    "ssaugtrans-25": "ArXiv preprint, abs/2209.10655. Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics. Nallapati et al. (2016) Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 280\u2013290, Berlin, Germany. Association for Computational Linguistics. Nangia and Bowman (2018) Nikita Nangia and Samuel Bowman. 2018. ListOps: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 92\u201399, New Orleans, Louisiana, USA. Association for Computational Linguistics. Nguyen et al. (2021) Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang. 2021. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention.",
    "ssaugtrans-26": "Advances in Neural Information Processing Systems, 34. Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\u201353, Minneapolis, Minnesota. Association for Computational Linguistics. Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024\u20138035. Peng et al. (2021) Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. 2021. Random feature attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. 2020. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555\u20132565, Online. Association for Computational Linguistics. Radev et al. (2013) Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919\u2013944. Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "ssaugtrans-27": "J. Mach. Learn. Res., 21(140):1\u201367. Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics. Roy et al. (2021) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53\u201368. Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464\u2013468, New Orleans, Louisiana. Association for Computational Linguistics. Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics. Sukhbaatar et al. (2019) Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331\u2013335, Florence, Italy. Association for Computational Linguistics. Tay et al. (2021a) Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183\u201310192. PMLR. Tay et al. (2020) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438\u20139447. PMLR. Tay et al. (2021b) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trinh and Le (2018) Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008. Vyas et al. (2020) Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Wang et al. (2020a) Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. 2020a. Cluster-former: Clustering-based sparse transformer for long-range dependency encoding.",
    "ssaugtrans-28": "ArXiv preprint, abs/2009.06097. Wang et al. (2020b) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Self-attention with linear complexity.",
    "ssaugtrans-29": "ArXiv preprint, abs/2006.04768. Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641. Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics. Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10524\u201310533. PMLR. Xiong et al. (2021) Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. Nystr\u00f6mformer: A nyst\u00f6m-based algorithm for approximating self-attention.",
    "ssaugtrans-30": "In Proceedings of the\u2026 AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 35, page 14138. NIH Public Access. Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Zhang et al. (2021) Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long document modeling with pooling attention. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12437\u201312446.",
    "ssaugtrans-31": "PMLR. Zhang et al. (2020) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.",
    "ssaugtrans-32": "Liu. 2020. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11328\u201311339.",
    "ssaugtrans-33": "PMLR. Zhu et al. (2021a) Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. 2021a. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34. Zhu et al. (2021b) Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021b. MediaSum: A large-scale media interview dataset for dialogue summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5927\u20135934, Online.",
    "ssaugtrans-34": "Association for Computational Linguistics. Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.",
    "ssaugtrans-35": "In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 19\u201327. IEEE Computer Society. Appendix A Training Details\n\nA.1 Language Model Pre-Training and Fine-Tuning\n\nFor language model pre-training and fine-tuning experiments, we use Adam (Kingma and Ba, 2015) as the optimizer. Hyper-parameters for pre-training are detailed in Table 7; and hyper-parameters for fine-tuning are detailed in Table 8. A.2 Long Range Arena\n\nWe follow the model architecture settings in Ma et al. 2022. The rest of the hyper-parameters are detailed in Table 9. A.3 Language Modeling\n\nWe follow the settings in Baevski and Auli 2019, including model architecture and hyper-parameters. Appendix B Dataset Details\n\nStatistics of the GLUE benchmark is summarized in Table 10.",
    "ssaugtrans-36": "\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Mar 1 10:36:58 2024 by LaTeXML"
}