{
    "ssaugtrans-0": "# Efficient Long Sequence Modeling via State Space Augmented Transformer \n\nSimiao Zuo ${ }^{* \\ddagger}$, Xiaodong Liu ${ }^{* \\dagger \\wedge}$, Jian Jiao ${ }^{\\dagger \\diamond}$, Denis Charles ${ }^{\\diamond}$, Eren Manavoglu ${ }^{\\wedge}$,<br>Tuo Zhao ${ }^{\\ddagger}$ and Jianfeng Gao ${ }^{\\circ}$<br>${ }^{\\ddagger}$ Georgia Institute of Technology ${ }^{\\diamond}$ Microsoft\n\n\n#### Abstract\n\nTransformer models have achieved superior performance in various natural language processing tasks.",
    "ssaugtrans-1": "However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State space $\\underline{A} u g m e n t e \\underline{D}$ TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. ## 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair\n\n[^0]of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model ( 250 M parameters) consumes over 80 G of GPU memory when the sequence length is 8 k . Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models' practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a $57.5 \\%$ accuracy, nearly $30 \\%$ less than stateof-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right)$ in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are pro-\nposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models' ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State space $\\underline{\\text { Augmente }} \\underline{\\mathbf{D}}$ TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines.",
    "ssaugtrans-2": "Finally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.",
    "ssaugtrans-3": "In Eq. 4 , the output $y$ can be computed efficiently given that the convolution kernel $\\bar{K}$ is known. However, computing the kernel is non-trivial. Most of existing algorithms have $O\\left(L^{2}\\right)$ time and space complexity. Structured State Space Sequence model (S4). Gu et al. (2021) develop the S4 model to efficiently compute Eq. 4. Specifically, $\\mathbf{C}$ in Eq. 2 is randomly initialized, and $\\mathbf{A}$ and $\\mathbf{B}$ are initialized as\n\n$$\n\\begin{aligned}\n& \\mathbf{A}=\\mathbf{A}^{\\left(d_{s}\\right)}-\\mathbf{P} \\mathbf{P}^{\\top}, \\quad \\mathbf{B}_{i}=(2 i+1)^{\\frac{1}{2}} \\\\\n& \\text { where } \\mathbf{P}_{i}=(i+1 / 2)^{1 / 2}, \\\\\n& \\mathbf{A}_{i j}^{\\left(d_{s}\\right)}=- \\begin{cases}\\left(i+\\frac{1}{2}\\right)^{\\frac{1}{2}}\\left(j+\\frac{1}{2}\\right)^{\\frac{1}{2}}, & i>j \\\\\n\\frac{1}{2}, & i=j \\\\\n-\\left(i+\\frac{1}{2}\\right)^{\\frac{1}{2}}\\left(j+\\frac{1}{2}\\right)^{\\frac{1}{2}}, & i<j\\end{cases}\n\\end{aligned}\n$$\n\nSubsequently, the convolution kernel $\\overline{\\mathbf{K}}$ in Eq. 4 can be computed efficiently with $O(L)$ time and space complexity. Subsequently, for an input $u$, the S4 output $y=\\overline{\\mathbf{K}} * u$ can be computed efficiently. ![](https://cdn.mathpix.com/cropped/2024_09_17_f8afc1e7093f15ce9006g-03.jpg?height=372&width=780&top_left_y=225&top_left_x=1048)\n\nFigure 1: Illustration of window attention (left) and chunk attention (right). For window attention, the window size is 2 (on each side); for chunk attention, the chunk size is 2 . ## 3 Method\n\nWe first conduct experiments to demonstrate that SSMs do not model local information well. Then, we present SPADE, which effectively combines global and local information by augmenting SSMs into the Transformer architecture. ### 3.1 Attention vs. State Space Models\n\nThe motivation behind SPADE is that even though SSMs perform well on several long sequence classification tasks (Gu et al., 2021), they perform poorly on language modeling, which is a fundamental task in natural language processing. To demonstrate such an observation, we compare S4 with Transformer with full attention and Transformer with local (window and chunk) attention. In local attention, each token can only attend to its neighboring tokens (see Figure 1 for illustrations). We conduct experiments on token-level language modeling. In this setting, local information is more important than global information. This is because in practice, we rarely see words (tokens) that are thousands of positions apart exhibit strong dependencies (Sukhbaatar et al., 2019). ![](https://cdn.mathpix.com/cropped/2024_09_17_f8afc1e7093f15ce9006g-03.jpg?height=349&width=549&top_left_y=2024&top_left_x=1162)\n\nFigure 2: Performance of Transformer with full attention, window attention, chunk attention, and S4. We conduct language modeling experiments (see Section 4.2), and the sequence length is 3 k . Experimental results are illustrated in Figure 2. We see that both Transformer with full attention\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_f8afc1e7093f15ce9006g-04.jpg?height=659&width=792&top_left_y=236&top_left_x=224)\n\nFigure 3: Demonstration of SPADE with 4 layers. Left: model overview; Right: details of the global layer. and Transformer with local attention (e.g., window and chunk) outperforms S4. Notice that replacing full attention with local attention does not significantly hurt model performance, indicating that local information is more important in this setting. We remark that SSMs such as S4 produces a fixed dependency pattern, e.g., the convolution kernel in Eq. 4. Moreover, unlike the attention mechanism, SSMs do not explicitly compute dependencies between tokens. Therefore, SSMs are not refined enough to capture local information, such that they perform poorly on language modeling tasks. ### 3.2 SPADE: State Space Augmented Transformer\n\nWe propose SPADE, which is a multi-layer Transformer model that can capture complicated global and local information. The overall architecture of SPADE is shown in Figure 3 (left). The proposed model employs a hierarchical structure. Specifically, at the bottom layer of SPADE (termed the global layer), we capture global dependencies using a SSM. Because the SSM only provides coarse global information, the subsequent local layers facilitate the model to handle more refined and complicated local dependencies. In other words, the SSM induces a strong structural bias that augments global information to the inputs. To instantiate the local layer, we replace the full attention in the conventional Transformer layer with off-the-shelf efficient local attention methods. SPADE is flexible to accommodate different approaches, such as window attention and chunk attention (see Figure 1 for illustrations). In the global layer (Figure 3, right), given the input $\\mathbf{X}$ to the layer, we have the output $\\mathbf{Y}$ as\n\n$$\n\\begin{aligned}\n& \\mathbf{X}_{\\text {local }}=\\operatorname{Local}(\\operatorname{LN}(\\mathbf{X})) \\\\\n& \\mathbf{X}_{\\text {global }}=\\operatorname{SSM}(\\operatorname{LN}(\\mathbf{X})) \\\\\n& \\mathbf{X}_{a}=\\mathbf{W}\\left[\\operatorname{LN}\\left(\\mathbf{X}_{\\text {local }}\\right), \\operatorname{LN}\\left(\\mathbf{X}_{\\text {global }}\\right)\\right]+\\mathbf{X} \\\\\n& \\mathbf{Y}=\\operatorname{FFN}\\left(\\operatorname{LN}\\left(\\mathbf{X}_{a}\\right)\\right)+\\mathbf{X}_{a}\n\\end{aligned}\n$$\n\nHere, $\\mathrm{LN}(\\cdot)$ denotes layer normalization (Ba et al., 2016), $\\mathrm{FFN}(\\cdot)$ denotes a two-layer feed-forward neural network, and $\\mathbf{W}$ is a trainable weight that combines local and global representations. Notice that we apply normalization to $\\mathbf{X}_{\\text {local }}$ and $\\mathbf{X}_{\\text {global }}$ to align their scales. In this work, we choose S 4 as the state space model. We remark that because of the sequential nature of SSMs (Eq. 3), the global layer can encode positional information of the inputs. Therefore, we do not need additional fixed-length positional embedding techniques (Devlin et al., 2019). Such a property enables SPADE to extrapolate to longer sequence length during testing, e.g., we can train a model with sequence length 512 and test the model with sequence length 1 k . ## 4 Experiments\n\nIn the experiments, we implement all the models using PyTorch (Paszke et al., 2019) and Fairseq (Ott et al., 2019). Training details such as hyperparameter settings are deferred to the appendix. ### 4.1 Long Range Arena\n\nDataset. We evaluate the effectiveness of the proposed model on Long Range Arena (LRA, Tay et al. 2021b), which is a benchmark tailored for evaluating models' ability in modeling long sequences. The benchmark contains six tasks: ListOps, which tests the capability of modeling hierarchically structured data (Nangia and Bowman, 2018); bytelevel text classification on the IMDB movie review dataset (Text, Maas et al. 2011); byte-level document retrieval on the ACL anthology network (Retrieval, Radev et al. 2013); pixel-level image classification on CIFAR-10 (Image, Krizhevsky et al. 2009); Pathfinder, which tests the capability in modeling spatial dependency (Linsley et al., 2018); and a longer version of Pathfinder (Path-X, Tay et al.",
    "ssaugtrans-4": "2021b). Models. Following the standard setting (Tay et al., 2021b), we use small models (less than 2M parameters) for all the tasks. We limit the computational\n\n| Dataset | Listops | Text | Retrieval | Image | Pathfinder | Path-X | Avg. |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Sequence length | 2 k | 4 k | 8 k | 1 k | 1 k | 16 k | $\\mathbf{-}$ |\n| Random | 10.00 | 50.00 | 50.00 | 10.00 | 50.00 | 50.00 | 36.67 |\n| Transformer (full) (Vaswani et al., 2017) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | $\\boldsymbol{x}$ | 53.66 |\n| Transformer (window) | 15.82 | 52.98 | 53.39 | 41.46 | 66.63 | $\\boldsymbol{x}$ | 46.71 |\n| Sparse Trans. (Child et al., 2019) | 17.07 | 63.58 | 59.59 | 44.24 | 71.71 | $\\boldsymbol{x}$ | 51.03 |\n| Longformer (Beltagy et al., 2020) | 35.63 | 62.85 | 56.89 | 42.22 | 69.71 | $\\boldsymbol{x}$ | 52.88 |\n| Linformer (Wang et al., 2020b) | 35.70 | 53.94 | 52.27 | 38.56 | 76.34 | $\\boldsymbol{x}$ | 51.14 |\n| Reformer (Kitaev et al., 2020) | 37.27 | 56.10 | 53.40 | 38.07 | 68.50 | $\\boldsymbol{x}$ | 50.56 |\n| Sinkhorn Trans. (Tay et al., 2020) | 33.67 | 61.20 | 53.83 | 41.23 | 67.45 | $\\boldsymbol{x}$ | 51.23 |\n| Synthesizer (Tay et al., 2021a) | 36.99 | 61.68 | 54.67 | 41.61 | 69.45 | $\\boldsymbol{x}$ | 52.40 |\n| BigBird (Zaheer et al., 2020) | 36.05 | 64.02 | 59.29 | 40.83 | 74.87 | $\\boldsymbol{x}$ | 54.17 |\n| Linear Trans. (Katharopoulos et al., 2020) | 16.13 | 65.90 | 53.09 | 42.34 | 75.30 | $\\boldsymbol{x}$ | 50.46 |\n| Performer (Choromanski et al., 2021) | 18.01 | 65.40 | 53.82 | 42.77 | 77.05 | $\\boldsymbol{x}$ | 51.18 |\n| FNet (Lee-Thorp et al., 2022) | 35.33 | 65.11 | 59.61 | 38.67 | 77.80 | $\\boldsymbol{x}$ | 54.42 |\n| Nystromformer (Xiong et al., 2021) | 37.15 | 65.52 | 79.56 | 41.58 | 70.94 | $\\boldsymbol{x}$ | 57.46 |\n| Luna-256 (Ma et al., 2021) | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | $\\boldsymbol{x}$ | 59.37 |\n| FMMformer (Nguyen et al., 2021) | 36.74 | 67.84 | 81.88 | 45.10 | 72.12 | $\\boldsymbol{x}$ | 60.74 |\n| S4 (Gu et al., 2021) | 58.35 | 76.02 | 87.09 | 87.26 | 86.05 | 88.10 | 80.48 |\n| MEGA-chunk (Ma et al., 2022) | 58.76 | 90.19 | 90.97 | 85.80 | 94.41 | 93.81 | 85.66 |\n| SPADE (softmax-window) | 59.70 | 87.55 | 90.13 | $\\mathbf{8 9 .",
    "ssaugtrans-5": "1 1}$ | $\\mathbf{9 6 . 4 2}$ | 94.22 | 86.19 |\n| SPADE (MEGA-chunk) | $\\mathbf{6 0 . 5 0}$ | $\\mathbf{9 0 . 6 9}$ | $\\mathbf{9 1 . 1 7}$ | 88.22 | 96.23 | $\\mathbf{9 7 . 6 0}$ | $\\mathbf{8 7 . 4 0}$ |\n\nTable 1: Experimental results on Long Range Arena (LRA). Path-X uses 16 k as the input sequence length, and \" $x$ \" indicates unavailable results due to computational constraints. All the baseline results, except for MEGA-chunk, are from Gu et al. (2021). MEGA-chunk results are from Ma et al. (2022). budget such that all the models are trained with similar speed for the same amount of time. To aggregate local information, we consider two approaches: window attention and chunk attention. For window attention, we sparsify the conventional softmax attention (termed softmax-window); and for chunk attention, we sparsify MEGA (Ma et al., 2022), which employs a gated attention technique (termed MEGA-chunk). For window attention, we set the window size to 128 , except Path-X, where we set the window size to 1024 . For chunk attention, we set the chunk size to 128 , except Path-X, where we set the chunk size to 4096.",
    "ssaugtrans-6": "Results. Experimental results are summarized in Table 1. We see that both variants of SPADE (softmax-window and MEGA-chunk) significantly outperform all the baselines in terms of average accuracy. For example, the window attention variant outperforms the best-performing baseline (MEGAchunk) by $0.5 \\%$, and the chunk attention variant has a $1.8 \\%$ performance gain. Therefore, SPADE is more suitable to model long sequences than existing approaches. ### 4.2 Language Modeling\n\nWe further evaluate our model by conducting language modeling experiments on Wikitext-103. The dataset contains English-language Wikipedia arti-\n\n|  | Test ppl |\n| :--- | :---: |\n| Transformer (Vaswani et al., 2017) | 18.8 |\n| Transformer (window) | 19.7 |\n| S4 (Gu et al., 2021) | 23.2 |\n| FLASH-chunk (Hua et al., 2022) | 20.9 |\n| MEGA-chunk (Ma et al., 2022) | 19.8 |\n| SPADE (FLASH-chunk) | 19.9 |\n| SPADE (MEGA-chunk) | 19.5 |\n| SPADE (softmax-window) | $\\mathbf{1 8 . 5}$ |\n\nTable 2: Experimental results on Wikitext-103. For window attention (softmax-window), we set the window size to 512; for chunk attention (FLASH and MEGA), we set the chunk size to 512 . cles, and the total number of tokens is 103 M . In all the experiments, we follow the settings in Baevski and Auli (2019), where we use a large-scale Transformer model with 16 layers and about 250 M parameters. We set the input sequence length to 3 k and train for 286 k steps. Similar to the LRA experiments, we equip SPADE with either window attention (softmax-window) or chunk attention (MEGAchunk). Additionally, we evaluate another efficient attention variant: FLASH-chunk, where we sparsify FLASH (Hua et al., 2022), a gated attention method similar to MEGA. Experimental results are presented in Table 2. From the results, we see that by combining global and local information, the proposed model achieves\n\n|  | RTE <br> Acc | MRPC <br> Acc/F1 | CoLA <br> Mcc | SST-2 <br> Acc | STS-B <br> P/S Corr | QNLI <br> Acc | QQP <br> Acc/F1 | MNLI-m/mm <br> Acc | Avg. <br> Score |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| T5 base (Raffel et al., 2020) | 76.9 | 90.8/- | 55.5 | 92.8 | 86.5 | 91.9 | 90.9/- | 84.4/83.5 | - |\n| T5 base (re-imp) | 78.0 | 91.7/88.6 | 61.5 | 93.6 | 88.2 | 92.9 | 91.2/87.9 | 87.0/86.9 | 85.1 |\n| SPADE $_{\\text {base }}$ | 77.9 | $92.2 / 89.0$ | 63.2 | 94.0 | 87.9 | 92.8 | 91.6/88.2 | 87.1/87.2 | 85.4 |\n| SPADE $_{\\text {baset+ }}$ | 80.5 | 92.3/89.2 | 64.7 | 95.9 | 89.2 | 93.9 | 91.7/88.4 | 89.6/89.2 | 86.8 |\n\nTable 3: Experimental results on GLUE development set.",
    "ssaugtrans-7": "\"T5 base\" results are from Raffel et al. 2020. For \"T5 base (re-imp)\", we re-implement the T5 pre-training procedures, and we fine-tune the re-implemented T5 model. significant performance improvement and outperform all the baselines. For example, the vanilla window attention has a 19.7 perplexity on the test set, and by integrating a SSM into SPADE, we achieve a 1.2 perplexity gain. We remark that SPADE with softmax-window is not only significantly faster than the Transformer with full attention, but also yields a better performance. Remark. We remark that we do not need to train the S4 in the bottom layer of SPADE to achieve the performance in Table 2. That is, we initialize the parameters in S4 using Eq. 5, and the parameters are frozen during training. This is because even without training, the initialization of $S 4$ yields intriguing theoretical properties, which facilitates S4's ability to capture global information. ## 5 Language Model Pre-Training\n\nWe implement model pre-training using Fairseq, and we implement model fine-tuning using $M T$ DNN (Liu et al., 2019a, 2020b). Note that all our experiments only use single task fine-tuning. Details such as hyper-parameter settings are deferred to the appendix. ### 5.1 Pre-Training Details\n\nTo demonstrate the scalability of the proposed method, we pre-train an encoder-decoder variant of SPADE. The model architecture is the same as $\\mathrm{T} 5_{\\text {base }}$ (Raffel et al., 2020), except that we use postlayernorm instead of pre-layernorm to improve model performance (Liu et al., 2020a; Xiong et al., 2020). The embedding dimension is 768 , the hidden dimension of the FFN is 3072, the number of attention heads is 12 , and both the encoder and the decoder have 12 layers. We add a S 4 module to the bottom layer of SPADE, and the parameters of the S4 are fixed after initialization (Eq. 5). We use the softmax-window attention as the local information extractor, where we set the window size to 128 . The model contains about 290M parameters. We consider two pre-training settings with different datasets and number of training steps:\n$\\diamond$ SPADE $_{\\text {base }}$ : We follow the pre-training settings in BERT (Devlin et al., 2019). Specifically, we train the model on Wikipedia (Devlin et al., 2019) and BookCorpus (Zhu et al., 2015). We set the sequence length to 1024 and the batch size to 2048. We train the model for 125 k steps. $\\diamond$ SPADE $_{\\text {base++ }}$ : We follow the pre-training settings in RoBERTa (Liu et al., 2019b). Specifically, we train the model on Wikipedia (Devlin et al., 2019), BookCorpus (Zhu et al., 2015), STORIES (Trinh and Le, 2018), CC-News (Liu et al., 2019b), and OpenWebText (Gokaslan et al., 2019). We train the model for 2 M steps, with sequence length 1024 and batch size 2048. We remark that because the S 4 module is not trained after proper initialization, and we do not use fixed-length positional embedding, our pre-trained model can extrapolate to any sequence length. For example, we can set the sequence length to 2 k during fine-tuning, which is longer than the sequence length used in pre-training. ### 5.2 Natural Language Understanding\n\nWe fine-tune the pre-trained models on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019), which is a collection of natural language understanding tasks.",
    "ssaugtrans-8": "The benchmark includes two single-sentence classification tasks: CoLA (Warstadt et al., 2019) is a linguistic acceptability task; and SST-2 (Socher et al., 2013) is a binary classification task that classifies movie reviews to positive or negative. The benchmark also contains three similarity and paraphrase tasks: STS-B (Cer et al., 2017) is a text similarity task; MRPC (Dolan and Brockett, 2005) is a paraphrase detection task; and QQP is a duplication detection task. Additionally, there are natural language inference tasks: MNLI (Williams et al., 2018); QNLI (Rajpurkar et al., 2016); RTE (Dagan\n\n|  | \\# Train | \\# Validation | \\# Test | Mean | Median | Max | 90th percentile |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| arXiv (Cohan et al., 2018) | 203,037 | 6,436 | 6,440 | 10,720 | 8,519 | 378,825 | 20,170 |\n| CNN/DailyMail (Nallapati et al., 2016) | 287,113 | 13,368 | 11,490 | 982 | 894 | 5,268 | 1,659 |\n| MediaSum (Zhu et al., 2021b) | 443,596 | 10,000 | 10,000 | 2,302 | 1,748 | 125,974 | 4,128 |\n| MultiNews (Fabbri et al., 2019) | 44,972 | 5,622 | 5,622 | 2,594 | $1,902.5$ | 683,544 | 4,853 |\n\nTable 4: Statistics and sources of abstractive summarization datasets.",
    "ssaugtrans-9": "et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009).",
    "ssaugtrans-10": "Dataset details are summarized in Appendix B. We do not consider the long sequence setting in these tasks. All models (T5 and SPADE) are fine-tuned under the sequence length of 512. Experimental results are presented in Table 3. We see that SPADE $_{\\text {base++ }}$ significantly outperforms $\\mathrm{T} 5_{\\text {base }}$. For example, $T 5_{\\text {base }}$ has a 85.1 average score, and our model outperforms it by 1.7 average score. Recall that the sequence length is set to 512 , which is the standard setting instead of the longsequence setting. Therefore, the results indicate that SPADE is universal in that it is suitable to model both long and short sequences. ### 5.3 Natural Language Generation\n\nWe also fine-tune the pre-trained models on several abstractive summarization datasets.",
    "ssaugtrans-11": "The sources and statistics are summarized in Table 4. We use ROUGE-2 scores as the evaluation metric. We compare SPADE with LongT5 (Guo et al., 2022), which is a 55 variant that is tailored for long sequences. Note that LongT5 uses PEGASUSstyle (Zhang et al., 2020) Principle Sentences Generation as its pre-training objective, such that LongT5 is tailored for natural language generation tasks (e.g., summarization). In contrast, SPADE is a more general model in that it also has superior performance on natural language understanding tasks. We remark that LongT5 uses C4 (Raffel et al., 2020) as its pre-training dataset, which is much larger and diverse than the dataset we adopted. Experimental results are summarized in Table 5. From the results, we see that our model significantly outperforms LongT5. Note that SPADE $_{\\text {base++ }}$ have about 290 M parameters, while LongT5 $5_{\\text {large }}$ contains about 770 M parameters and LongT5 ${ }_{x 1}$ contains about 3B parameters. From the results, we see that in all the tasks, our base-sized models have on par or better performance compared with LongT5 ${ }_{\\text {large }}$. On the MultiNews dataset, our model even outperforms LongT5 $5_{\\mathrm{xl}}$, which is over ten times larger. |  | arXiv |  | CNN/DailyMail |  |\n| :--- | :---: | :---: | :---: | :---: |\n|  | Length | R-2 | Length | R-2 |\n| LongT5 $_{\\text {base }}$ | 4 k | 18.54 | 4 k | 20.11 |\n| LongT5 $_{\\text {large }}$ | 16 k | 21.63 | 4 k | 20.51 |\n| LongT5 $_{\\mathrm{xl}}$ | 16 k | 21.92 | 4 k | 21.40 |\n| SPADE $_{\\text {base++ }+}$ | 16 k | 21.65 | 4 k | 20.40 |\n|  | MediaSum |  | MultiNews |  |\n|  | Length | R-2 | Length | R-2 |\n| LongT5 $_{\\text {base }}$ | 4 k | 18.35 | 4 k | 17.37 |\n| LongT5 $_{\\text {large }}$ | 4 k | 19.04 | 8 k | 18.44 |\n| LongT5 $_{\\mathrm{xl}}$ | 4 k | 19.66 | 8 k | 19.43 |\n| SPADE $_{\\text {base++ }}$ | 4 k | 19.03 | 8 k | 19.63 |\n\nTable 5: Experimental results (ROUGE-2) on test sets.",
    "ssaugtrans-12": "LongT5 results are from Guo et al. 2022. ## 6 Analysis\n\n### 6.1 Efficiency Comparison\n\nWe compare the efficiency of SPADE with other models: Transformer with softmax-window attention, Trnasformer with MEGA-chunk attention, and S4. The results are illustrated in Figure 4. We see that SPADE is efficient in terms of both training speed and GPU memory usage. For example, when the sequence length is 6 k , Transformer uses about 60 GB of GPU memory, whereas SPADE with softmax-window uses only 27 GB . Moreover, notice that SPADE also trains significantly faster than the vanilla Transformer under all settings. Notice that S 4 may be less efficient than the vanilla Transformer (e.g., when the sequence length is 3 k ). This is because in the multi-layer model in Gu et al. 2021, each layer contains multiple S4 modules and expensive non-linear components. Therefore, the per-layer computational cost can exceed full attention when the sequence is not extremely long. We remark that even though we add a S 4 module to the bottom layer of SPADE, such an additional module does not induce much computational overhead. We see that both the training speed and the memory usage of SPADE with softmax-window is only marginally different from those of windowattention Transformer. We have similar observations for the chunk attention variant. ![](https://cdn.mathpix.com/cropped/2024_09_17_f8afc1e7093f15ce9006g-08.jpg?height=726&width=860&top_left_y=234&top_left_x=238)\n\nFigure 4: Efficiency comparison on language modeling. ![](https://cdn.mathpix.com/cropped/2024_09_17_f8afc1e7093f15ce9006g-08.jpg?height=375&width=578&top_left_y=244&top_left_x=1207)\n\nFigure 5: Performance vs. location of SSM. We conduct language modeling experiments with softmax-window attention (window=256). By default, the model has 16 layers, where the bottom layer is a global layer and the rest are local layers. Here, \" $b-k \"$ means the bottom-k layers are global layers, \"all\" means all layers are global layers, and \"top-l\" means the top-1 layer is a global layer. ![](https://cdn.mathpix.com/cropped/2024_09_17_f8afc1e7093f15ce9006g-08.jpg?height=388&width=1026&top_left_y=1043&top_left_x=241)\n\nFigure 6: Performance with different configurations. Left: Transformer with full attention using different sequence length; Right: Transformer with window attention using different window size and sequence length. |  | Sequence length |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | 2 k | 3 k | 4 k | 6 k |\n| 128 | 19.55 | 19.42 | 19.50 | 19.55 |\n| 256 | 18.90 | 18.95 | 18.99 | 19.00 |\n| 512 | 18.63 | 18.64 | 18.52 | 18.67 |\n\nTable 6: Performance of SPADE with softmax-window using different sequence length and window size.",
    "ssaugtrans-13": "The first column is window size. We conduct language modeling experiments on Wikitext-103. ### 6.2 Location and Number of Global Layers\n\nRecall that in SPADE, the bottom layer is equipped with a SSM and serves as the global layer, while the rest are local layers (see Figure 3). In Figure 5, we empirically justify this design choice. We first investigate the possibility of incorporating more global layers: we set the bottom 1 (the default choice), 4,8 , and 16 (all) layers as global layers. From the results, we see that model performance decreases as we use more global layers. This is because the SSM in the bottom layer captures and filters out global information, such that subsequent SSMs only introduce noise to the intermediate representations. We also investigate whether the global layer can be the top instead of the bottom layer in SPADE. From Figure 5, we see that model performance drops significantly. This is because as a global information extractor, the global layer encodes positional information, on which the local attention modules rely. Therefore, using the global layer as the top layer is akin to using Transformer mod- els without positional encoding, which will yield unsatisfactory performance. ### 6.3 Different Configurations\n\nWe examine how performance changes when we change the sequence length and window size. From Figure 6 (left), we see that when we increase the sequence length from 512 to 3 k , performance of Transformer with full attention increases. However, when we further increase the sequence length to 4 k , model performance drastically drops. This is because in long sequences, the signal-tonoise ratio is low, such that the full attention may easily fit to the noise. From Figure 6 (right), we see that performance of Transformer with window attention increases when we increase the window size. Moreover, model performance is better with shorter sequences for the same window size. Such findings indicate that performance of window attention depends on the proportion of information within its perception. From Table 6, we see that for the same sequence\nlength, performance of SPADE increases when we increase the window size. Also, we see that performance of SPADE marginally decreases when we increase the sequence length from 4 k to 6 k . Recall from Figure 6 (left) that performance of Transformer with full attention drastically deteriorates when we increase the length from 3 k to 4 k . Such a result indicates that the proposed model is more suitable to model long sequences. ## 7 Related Works\n\n### 7.1 Efficient Transformer Models\n\nIn Eq. 1 , we have $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{L \\times d}$, such that computing the attention $\\operatorname{Attn}(\\mathbf{X})$ introduces $O\\left(L^{2}\\right)$ time and space costs. Such quadratic costs are prohibitive when the sequence length $L$ is large. There are various attempts to reduce the quadratic time and space complexity of the vanilla attention. One approach is to employ sparse attention. That is, each token only attends to a subset of all the tokens according to pre-defined patterns, e.g., neighboring tokens within a fixed size window. Some examples include Sparse Transformer (Child et al., 2019), BlockBERT (Qiu et al., 2020), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), BigBird (Zaheer et al., 2020), HEPOS (Huang et al., 2021), and Poolingformer (Zhang et al., 2021). Another approach is to use low-rank projection. For example, in Linformer (Wang et al., 2020b), the attention mechanism in Eq. 1 becomes $\\operatorname{Attn}(\\mathbf{X})=$ $\\operatorname{softmax}\\left(\\mathbf{Q}(\\mathbf{E K})^{\\top} / \\sqrt{d}\\right)(\\mathbf{F V})$. Here, the two additional parameters satisfy $\\mathbf{E}, \\mathbf{F} \\in \\mathbb{R}^{r \\times L}$, where $r$ is the projection rank such that $r \\ll L$. Similar methods include Nystr\u00f6mformer (Xiong et al., 2021), Synthesizer (Tay et al., 2021a), TransformerLS (Zhu et al., 2021a), and Luna (Ma et al., 2021). However, these approaches face difficulty when handling causal tasks, such as auto-regressive language modeling. Specifically, in Eq. 1, we mask out the upper triangular part in the attention score matrix $\\mathbf{A} \\in \\mathbb{R}^{L \\times L}$ such that each token can only attend to its previous tokens. However, this is implausible in Linformer since we project the $L \\times L$ matrix to a $L \\times r$ matrix. Kernel-based approaches can be used to approximate the full attention $\\operatorname{Attn}(\\mathbf{X})$. In these approaches, the quadratic-time softmax attention is replaced by fast linear-time kernel approximations (e.g., Gaussian and arc-cosine kernel). Some examples include Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention (Peng et al., 2021), and FMMformer (Nguyen et al., 2021). Both low-rank projection and kernel-based approaches approximate the full attention, and thus, they often suffer from non-negligible approximation error. We can also adopt clustering-based approaches, where we divide $\\mathbf{Q}$ or $\\mathbf{K}$ into several clusters, and only perform inter-cluster attention. Such methods include Reformer (Kitaev et al., 2020), Clusterformer (Wang et al., 2020a), Sinkhorn Transformer (Tay et al., 2020), Fast Transformer (Vyas et al., 2020), Routing Transformer (Roy et al., 2021), and FLASH (Hua et al., 2022). ### 7.2 Pre-Trained Language Models\n\nPre-trained language models (Devlin et al., 2019; Liu et al., 2019b; Raffel et al., 2020; Brown et al., 2020; He et al., 2021) have achieved state-of-theart performance on various natural language processing tasks. However, most of these models are not suitable for long sequences. For example, BERT (Devlin et al., 2019) uses a fixed-length positional embedding, such that it cannot handle sequences with length more than 512 . In contrast, LongT5 (Guo et al., 2022) facilitates training on long sequences by leveraging relative positional embedding (Shaw et al., 2018) and efficient attention methods. The model targets long sequence modeling tasks such as text summarization. ## 8 Conclusion\n\nIn this work, we propose SPADE, a state space augmented Transformer model that targets long sequence modeling. SPADE is a multi-layer Transformer model, where the bottom layer is a global layer and the rest are local layers. In the global layer, we use a SSM to augment coarse global information, which are subsequently refined by the following local layers. We instantiate the local layers with off-the-shelf efficient attention methods, such as window attention. The proposed model has linear time and space computationally complexity, facilitating it to handle long sequences. We conduct extensive experiments on the Long Range Arena (LRA) benchmark and language modeling datasets to demonstrate the effectiveness and efficiency of SPADE. We also pre-train encoder-decoder models to demonstrate the scalability of SPADE, and we perform fine-tuning experiments on natural language understanding (GLUE) and natural language\ngeneration (summarization) tasks. In all the experiments, SPADE exhibits superior performance and outperforms the baselines. ## Acknowledgments\n\nWe thank Hao Cheng, Bin Yu, Jianwei Yang, Baolin Peng, Minjia Zhang and Linyuan Gong for valuable discussions and comments, and Microsoft Research Technology Engineering team for setting up GPU machines. ## References\n\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268-284, Online. Association for Computational Linguistics. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. ArXiv preprint, $\\mathrm{abs} / 1607.06450$. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In 7 th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment. Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. ArXiv preprint, abs/2004.05150. Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC'09.",
    "ssaugtrans-14": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
    "ssaugtrans-15": "Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14, Vancouver, Canada. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. ArXiv preprint, abs/1904.10509. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. 2021. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, pages 177-190, Berlin, Heidelberg. Springer-Verlag. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota.",
    "ssaugtrans-16": "Association for Computational Linguistics. William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
    "ssaugtrans-17": "Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074-1084, Florence, Italy. Association for Computational Linguistics. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1-9, Prague. Association for Computational Linguistics. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2020. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
    "ssaugtrans-18": "Albert Gu, Karan Goel, and Christopher R\u00e9. 2021. Efficiently modeling long sequences with structured state spaces. ArXiv preprint, abs/2111.00396. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724-736, Seattle, United States. Association for Computational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419-1436, Online. Association for Computational Linguistics. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165.",
    "ssaugtrans-19": "PMLR. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
    "ssaugtrans-20": "Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020. OpenReview.net. Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.",
    "ssaugtrans-21": "Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2022. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4296-4313, Seattle, United States. Association for Computational Linguistics. Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. $A I$ Open. Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. 2018. Learning long-range spatial dependencies with horizontal gated recurrent units. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 152-164.",
    "ssaugtrans-22": "Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020a. Understanding the difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57475763, Online. Association for Computational Linguistics. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487-4496, Florence, Italy. Association for Computational Linguistics. Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He,\n\nWeizhu Chen, Hoifung Poon, Guihong Cao, and Jianfeng Gao. 2020b. The Microsoft toolkit of multitask deep neural networks for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 118-126, Online. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2022. Mega: Moving average equipped gated attention.",
    "ssaugtrans-23": "ArXiv preprint, abs/2209.10655.",
    "ssaugtrans-24": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 280-290, Berlin, Germany. Association for Computational Linguistics. Nikita Nangia and Samuel Bowman. 2018. ListOps: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 9299, New Orleans, Louisiana, USA. Association for Computational Linguistics. Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang. 2021. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention. Advances in Neural Information Processing Systems, 34. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48-53, Minneapolis, Minnesota. Association for Computational Linguistics. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024-8035. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. 2021. Random feature attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. 2020. Blockwise selfattention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555-2565, Online. Association for Computational Linguistics. Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919-944. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "ssaugtrans-25": "J. Mach. Learn. Res., 21(140):1-67. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana. Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank.",
    "ssaugtrans-26": "In Proceedings of the 2013 Conference on\n\nEmpirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335, Florence, Italy. Association for Computational Linguistics. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183-10192. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.",
    "ssaugtrans-27": "Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In $7 t h$ International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and\nJingjing Liu. 2020a. Cluster-former: Clusteringbased sparse transformer for long-range dependency encoding. ArXiv preprint, abs/2009.06097. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Self-attention with linear complexity. ArXiv preprint, abs/2006.04768. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625-641. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10524-10533. PMLR. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. Nystr\u00f6mformer: A nyst\u00f6m-based algorithm for approximating selfattention.",
    "ssaugtrans-28": "In Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 35, page 14138. NIH Public Access. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
    "ssaugtrans-29": "Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long document modeling with pooling attention. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages $12437-12446$. PMLR. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.",
    "ssaugtrans-30": "Liu. 2020. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18\n\nJuly 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11328-11339. PMLR. Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. 2021a. Long-short transformer: Efficient transformers for language and vision. $A d$ vances in Neural Information Processing Systems, 34.",
    "ssaugtrans-31": "Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021b. MediaSum: A large-scale media interview dataset for dialogue summarization.",
    "ssaugtrans-32": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5927-5934, Online. Association for Computational Linguistics. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 19-27.",
    "ssaugtrans-33": "IEEE Computer Society. ## A Training Details\n\n## A. 1 Language Model Pre-Training and Fine-Tuning\n\nFor language model pre-training and fine-tuning experiments, we use Adam (Kingma and Ba, 2015) as the optimizer. Hyper-parameters for pre-training are detailed in Table 7; and hyper-parameters for fine-tuning are detailed in Table 8. ## A. 2 Long Range Arena\n\nWe follow the model architecture settings in Ma et al. 2022. The rest of the hyper-parameters are detailed in Table 9. ## A. 3 Language Modeling\n\nWe follow the settings in Baevski and Auli 2019, including model architecture and hyper-parameters. ## B Dataset Details\n\nStatistics of the GLUE benchmark is summarized in Table 10. | Parameters | Base | Base++ |\n| :--- | :---: | :---: |\n| Peak Learning Rate | $4 \\mathrm{e}-4$ | $2 \\mathrm{e}-4$ |\n| Batch Size | 2,048 | 2,048 |\n| Warmup Steps | 10,000 | 10,000 |\n| Total Steps | 125,000 | $2,000,000$ |\n| Sequence Length | 1024 | 1024 |\n| Relative Position Encoding Buckets | 32 | 32 |\n| Relative Position Encoding Max Distance | 128 | 128 |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-6$ | $1 \\mathrm{e}-6$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.98)$ | $(0.9,0.98)$ |\n| Clip Norm | - | 1.0 |\n| Dropout | 0.1 | 0.1 |\n| Weight Decay | 0.01 | 0.01 |\n\nTable 7: Hyper-parameters for pre-training. | Parameters | Range |\n| :--- | :---: |\n| Learning Rate | $\\{2 \\mathrm{e}-5,4 \\mathrm{e}-5,5 \\mathrm{e}-5,1 \\mathrm{e}-4\\}$ |\n| Batch Size | $\\{16,32\\}$ |\n| Maximum Training Epochs | $\\{3,5,10\\}$ |\n| Dropout | 0.1 |\n| Warmup Step Rate | 0.1 |\n| Weight Decay | 0.1 |\n\nTable 8: Hyper-parameters for fine-tuning. | Parameters | Range |\n| :--- | :---: |\n| Learning Rate | $\\{1 \\mathrm{e}-3,3 \\mathrm{e}-3,5 \\mathrm{e}-3,1 \\mathrm{e}-2\\}$ |\n| Dropout | $\\{0.0,0.1,0.2\\}$ |\n| Weight Decay | $\\{0.0,0.01,0.03,0.05\\}$ |\n\nTable 9: Hyper-parameters for training on LRA. | Corpus | Task | \\# Train | \\# Dev | \\# Test | \\# Labels | Metrics |\n| :--- | :--- | :---: | :---: | :---: | :---: | :---: |\n| Single-Sentence Classification |  |  |  |  |  |  |\n| CoLA | Acceptability | 8.5 k | 1 k | 1 k | 2 | Matthews corr |\n| SST | Sentiment | 67 k | 872 | 1.8 k | 2 | Accuracy |\n| Pairwise Text Classification |  |  |  |  |  |  |\n| MNLI | NLI | 393 k | 20 k | 20 k | 3 | Accuracy |\n| RTE | NLI | 2.5 k | 276 | 3 k | 2 | Accuracy |\n| QQP | Paraphrase | 364 k | 40 k | 391 k | 2 | Accuracy/F1 |\n| MRPC | Paraphrase | 3.7 k | 408 | 1.7 k | 2 | Accuracy/F1 |\n| QNLI | QA/NLI | 108 k | 5.7 k | 5.7 k | 2 | Accuracy |\n| Text Similarity |  |  |  |  |  |  |\n| STS-B | Similarity | 7 k | 1.5 k | 1.4 k | 1 | Pearson/Spearman corr |\n\nTable 10: Statistics of the GLUE benchmark.",
    "ssaugtrans-34": "[^0]:    *Simiao Zuo and Xiaodong Liu made equal contributions. Work was done during Simiao Zuo's internship at Microsoft. Correspondence to simiaozuo@gatech.edu and xiaodl@microsoft.com. ${ }^{\\dagger}$ Jian Jiao and Xiaodong Liu led the project. [^1]:    ${ }^{1}$ https://github.com/microsoft/ EfficientLongSequenceModeling\n    ${ }^{2}$ https://github.com/namisan/mt-dnn\n\n"
}