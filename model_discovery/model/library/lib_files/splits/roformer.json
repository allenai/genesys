{
    "roformer-0": "# RoFormer: Enhanced Transformer With Rotary Position EMbedDing \n\nJianlin Su<br>Zhuiyi Technology Co., Ltd.<br>Shenzhen<br>bojonesu@wezhuiyi.com\n\nYu Lu<br>Zhuiyi Technology Co., Ltd.<br>Shenzhen<br>julianlu@wezhuiyi.com<br>Bo Wen Zhuiyi Technology Co., Ltd.",
    "roformer-1": "Shenzhen mengjiayi@wezhuiyi.com<br>Zhuiyi Technology Co., Ltd.<br>Shenzhen<br>brucewen@wezhuiyi.com\n\nShengfeng Pan<br>Zhuiyi Technology Co., Ltd.<br>Shenzhen<br>nickpan@wezhuiyi.com<br>Yunfeng Liu<br>Zhuiyi Technology Co., Ltd.<br>Shenzhen<br>glenliu@wezhuiyi.com\n\nNovember 9, 2023\n\n\n#### Abstract\n\nPosition encoding recently has shown effective in the transformer architecture.",
    "roformer-2": "It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer. Keywords Pre-trained Language Models $\\cdot$ Position Information Encoding $\\cdot$ Pre-training $\\cdot$ Natural Language Processing. ## 1 Introduction\n\nThe sequential order of words is of great value to natural language understanding. Recurrent neural networks (RRNs) based models encode tokens' order by recursively computing a hidden state along the time dimension. Convolution neural networks (CNNs) based models (CNNs) Gehring et al. 2017] were typically considered position-agnostic, but recent work Islam et al. [2020] has shown that the commonly used padding operation can implicitly learn position information. Recently, the pre-trained language models (PLMs), which were built upon the transformer Vaswani et al. [2017], have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including context representation learning Devlin et al. [2019], machine translation Vaswani et al. [2017], and language modeling Radford et al. [2019], to name a few. Unlike, RRNs and CNNs-based models, PLMs utilize the self-attention mechanism to semantically capture the contextual representation of a given corpus. As a consequence, PLMs achieve a significant improvement in terms of parallelization over RNNs and improve the modeling ability of longer intra-token relations compared to CNNSt\n\n[^0]It is noteworthy that the self-attention architecture of the current PLMs has shown to be position-agnostic Yun et al.",
    "roformer-3": "[2020]. Following this claim, various approaches have been proposed to encode the position information into the learning process. On one side, generated absolute position encoding through a pre-defined function Vaswani et al. [2017] was added to the contextual representations, while a trainable absolute position encoding Gehring et al.",
    "roformer-4": "[[2017], Devlin et al. [2019], Lan et al. [2020], Clark et al. [2020], Radford et al. [2019], Radford and Narasimhan|[2018]. On the other side, the previous work Parikh et al. [2016], Shaw et al. [[2018], Huang et al. [2018], Dai et al. [2019], Yang et al. [2019], Raffel et al. [2020], Ke et al. [2020], He et al.|[2020], Huang et al. [2020] focuses on relative position encoding, which typically encodes the relative position information into the attention mechanism. In addition to these approaches, the authors of Liu et al. [2020] have proposed to model the dependency of position encoding from the perspective of Neural ODE [hen et al. [2018a], and the authors of Wang et al. [2020] have proposed to model the position information in complex space. Despite the effectiveness of these approaches, they commonly add the position information to the context representation and thus render them unsuitable for the linear self-attention architecture. In this paper, we introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the proposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length flexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Experimental results on various long text classification benchmark datasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better performance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE. In brief, our contributions are three-folds as follows:\n\n- We investigated the existing approaches to the relative position encoding and found that they are mostly built based on the idea of the decomposition of adding position encoding to the context representations. We introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. The key idea is to encode relative position by multiplying the context representations with a rotation matrix with a clear theoretical interpretation. - We study the properties of RoPE and show that it decays with the relative distance increased, which is desired for natural language encoding. We kindly argue that previous relative position encoding-based approaches are not compatible with linear self-attention. - We evaluate the proposed RoFormer on various long text benchmark datasets. Our experiments show that it consistently achieves better performance compared to its alternatives. Some experiments with pre-trained language models are available on GitHub: https://github.com/ZhuiyiTechnology/roformer\n\nThe remaining of the paper is organized as follows. We establish a formal description of the position encoding problem in self-attention architecture and revisit previous works in Section (2). We then describe the rotary position encoding (RoPE) and study its properties in Section (3) We report experiments in Section (4). Finally, we conclude this paper in Section (5)\n\n## 2 Background and Related Work\n\n### 2.1 Preliminary\n\nLet $\\mathbb{S}_{N}=\\left\\{w_{i}\\right\\}_{i=1}^{N}$ be a sequence of $N$ input tokens with $w_{i}$ being the $i^{t h}$ element. The corresponding word embedding of $\\mathbb{S}_{N}$ is denoted as $\\mathbb{E}_{N}=\\left\\{\\boldsymbol{x}_{i}\\right\\}_{i=1}^{N}$, where $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{d}$ is the d-dimensional word embedding vector of token $w_{i}$ without position information. The self-attention first incorporates position information to the word embeddings and transforms them into queries, keys, and value representations. $$\n\\begin{aligned}\n\\boldsymbol{q}_{m} & =f_{q}\\left(\\boldsymbol{x}_{m}, m\\right) \\\\\n\\boldsymbol{k}_{n} & =f_{k}\\left(\\boldsymbol{x}_{n}, n\\right) \\\\\n\\boldsymbol{v}_{n} & =f_{v}\\left(\\boldsymbol{x}_{n}, n\\right)\n\\end{aligned}\n$$\n\nwhere $\\boldsymbol{q}_{m}, \\boldsymbol{k}_{n}$ and $\\boldsymbol{v}_{n}$ incorporate the $m^{t h}$ and $n^{t h}$ positions through $f_{q}, f_{k}$ and $f_{v}$, respectively. The query and key values are then used to compute the attention weights, while the output is computed as the weighted sum over the value\nrepresentation. $$\n\\begin{aligned}\na_{m, n} & =\\frac{\\exp \\left(\\frac{\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}}{\\sqrt{d}}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\frac{\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{j}}{\\sqrt{d}}\\right)} \\\\\n\\mathbf{o}_{m} & =\\sum_{n=1}^{N} a_{m, n} \\boldsymbol{v}_{n}\n\\end{aligned}\n$$\n\nThe existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form Equation 11 . ### 2.2 Absolute position embedding\n\nA typical choice of Equation (1) is\n\n$$\nf_{t: t \\in\\{q, k, v\\}}\\left(\\boldsymbol{x}_{i}, i\\right):=\\boldsymbol{W}_{t: t \\in\\{q, k, v\\}}\\left(\\boldsymbol{x}_{i}+\\boldsymbol{p}_{i}\\right)\n$$\n\nwhere $\\boldsymbol{p}_{i} \\in \\mathbb{R}^{d}$ is a d-dimensional vector depending of the position of token $\\boldsymbol{x}_{i}$.",
    "roformer-5": "Previous work Devlin et al. [2019], Lan et al. [2020], Clark et al. [2020], Radford et al. [2019], Radford and Narasimhan [2018] introduced the use of a set of trainable vectors $\\boldsymbol{p}_{i} \\in\\left\\{\\boldsymbol{p}_{t}\\right\\}_{t=1}^{L}$, where $L$ is the maximum sequence length. The authors of Vaswani et al. 2017] have proposed to generate $\\boldsymbol{p}_{i}$ using the sinusoidal function. $$\n\\begin{cases}\\boldsymbol{p}_{i, 2 t} & =\\sin \\left(k / 10000^{2 t / d}\\right) \\\\ \\boldsymbol{p}_{i, 2 t+1} & =\\cos \\left(k / 10000^{2 t / d}\\right)\\end{cases}\n$$\n\nin which $\\boldsymbol{p}_{i, 2 t}$ is the $2 t^{t h}$ element of the d-dimensional vector $\\boldsymbol{p}_{i}$. In the next section, we show that our proposed RoPE is related to this intuition from the sinusoidal function perspective. However, instead of directly adding the position to the context representation, RoPE proposes to incorporate the relative position information by multiplying with the sinusoidal functions. ### 2.3 Relative position embedding\n\nThe authors of Shaw et al. [2018] applied different settings of Equation (1) as following:\n\n$$\n\\begin{array}{r}\nf_{q}\\left(\\boldsymbol{x}_{m}\\right):=\\boldsymbol{W}_{q} \\boldsymbol{x}_{m} \\\\\nf_{k}\\left(\\boldsymbol{x}_{n}, n\\right):=\\boldsymbol{W}_{k}\\left(\\boldsymbol{x}_{n}+\\tilde{\\boldsymbol{p}}_{r}^{k}\\right) \\\\\nf_{v}\\left(\\boldsymbol{x}_{n}, n\\right):=\\boldsymbol{W}_{v}\\left(\\boldsymbol{x}_{n}+\\tilde{\\boldsymbol{p}}_{r}^{v}\\right)\n\\end{array}\n$$\n\nwhere $\\tilde{\\boldsymbol{p}}_{r}^{k}, \\tilde{\\boldsymbol{p}}_{r}^{v} \\in \\mathbb{R}^{d}$ are trainable relative position embeddings. Note that $r=\\operatorname{clip}\\left(m-n, r_{\\min }, r_{\\max }\\right)$ represents the relative distance between position $m$ and $n$. They clipped the relative distance with the hypothesis that precise relative position information is not useful beyond a certain distance. Keeping the form of Equation (3), the authors Dai et al. [2019] have proposed to decompose $\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}$ of Equation (2) as\n\n$$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{p}_{n}+\\boldsymbol{p}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+\\boldsymbol{p}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{p}_{n}\n$$\n\nthe key idea is to replace the absolute position embedding $\\boldsymbol{p}_{n}$ with its sinusoid-encoded relative counterpart $\\tilde{\\boldsymbol{p}}_{m-n}$, while the absolute position $\\boldsymbol{p}_{m}$ in the third and fourth term with two trainable vectors $\\mathbf{u}$ and $\\mathbf{v}$ independent of the query positions. Further, $\\boldsymbol{W}_{k}$ is distinguished for the content-based and location-based key vectors $\\boldsymbol{x}_{n}$ and $\\boldsymbol{p}_{n}$, denoted as $\\boldsymbol{W}_{k}$ and $\\widetilde{\\boldsymbol{W}}_{k}$, resulting in:\n\n$$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\widetilde{\\boldsymbol{W}}_{k} \\tilde{\\boldsymbol{p}}_{m-n}+\\mathbf{u}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+\\mathbf{v}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\widetilde{\\boldsymbol{W}}_{k} \\tilde{\\boldsymbol{p}}_{m-n}\n$$\n\nIt is noteworthy that the position information in the value term is removed by setting $f_{v}\\left(\\boldsymbol{x}_{j}\\right):=\\boldsymbol{W}_{v} \\boldsymbol{x}_{j}$.",
    "roformer-6": "Later work Raffel et al. [2020], He et al. [2020], Ke et al. [2020], Huang et al. [2020] followed these settings by only encoding the relative position information into the attention weights. However, the authors of Raffel et al. [2020] reformed Equation (6) as:\n\n$$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+b_{i, j}\n$$\n\nwhere $b_{i, j}$ is a trainable bias. The authors of Ke et al. [2020] investigated the middle two terms of Equation (6) and found little correlations between absolute positions and words. The authors of Raffel et al. [2020] proposed to model a pair of words or positions using different projection matrices. $$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+\\boldsymbol{p}_{m}^{\\top} \\mathbf{U}_{q}^{\\top} \\mathbf{U}_{k} \\boldsymbol{p}_{n}+b_{i, j}\n$$\n\nThe authors of He et al. [2020] argued that the relative positions of two tokens could only be fully modeled using the middle two terms of Equation (6). As a consequence, the absolute position embeddings $\\boldsymbol{p}_{m}$ and $\\boldsymbol{p}_{n}$ were simply replaced with the relative position embeddings $\\tilde{\\boldsymbol{p}}_{m-n}$ :\n\n$$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}+\\boldsymbol{x}_{m}^{\\top} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\tilde{\\boldsymbol{p}}_{m-n}+\\tilde{\\boldsymbol{p}}_{m-n}^{\\boldsymbol{\\top}} \\boldsymbol{W}_{q}^{\\top} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\n$$\n\nA comparison of the four variants of the relative position embeddings Radford and Narasimhan [2018] has shown that the variant similar to Equation 10 p is the most efficient among the other three. Generally speaking, all these approaches attempt to modify Equation (6) based on the decomposition of Equation (3) under the self-attention settings in Equation (2), which was originally proposed in Vaswani et al. [2017]. They commonly introduced to directly add the position information to the context representations. Unlikely, our approach aims to derive the relative position encoding from Equation (1) under some constraints. Next, we show that the derived approach is more interpretable by incorporating relative position information with the rotation of context representations. ## 3 Proposed approach\n\nIn this section, we discuss the proposed rotary position embedding (RoPE). We first formulate the relative position encoding problem in Section (3.1) we then derive the RoPE in Section (3.2) and investigate its properties in Section (3.3)\n\n### 3.1 Formulation\n\nTransformer-based language modeling usually leverages the position information of individual tokens through a selfattention mechanism. As can be observed in Equation (2), $\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}$ typically enables knowledge conveyance between tokens at different positions. In order to incorporate relative position information, we require the inner product of query $\\boldsymbol{q}_{m}$ and key $\\boldsymbol{k}_{n}$ to be formulated by a function $g$, which takes only the word embeddings $\\boldsymbol{x}_{m}, \\boldsymbol{x}_{n}$, and their relative position $m-n$ as input variables. In other words, we hope that the inner product encodes position information only in the relative form:\n\n$$\n\\left\\langle f_{q}\\left(\\boldsymbol{x}_{m}, m\\right), f_{k}\\left(\\boldsymbol{x}_{n}, n\\right)\\right\\rangle=g\\left(\\boldsymbol{x}_{m}, \\boldsymbol{x}_{n}, m-n\\right)\n$$\n\nThe ultimate goal is to find an equivalent encoding mechanism to solve the functions $f_{q}\\left(\\boldsymbol{x}_{m}, m\\right)$ and $f_{k}\\left(\\boldsymbol{x}_{n}, n\\right)$ to conform the aforementioned relation. ### 3.2 Rotary position embedding\n\n### 3.2.1 A 2D case\n\nWe begin with a simple case with a dimension $d=2$. Under these settings, we make use of the geometric property of vectors on a 2D plane and its complex form to prove (refer Section (3.4.1) for more details) that a solution to our formulation Equation 11 is:\n\n$$\n\\begin{aligned}\nf_{q}\\left(\\boldsymbol{x}_{m}, m\\right) & =\\left(\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}\\right) e^{i m \\theta} \\\\\nf_{k}\\left(\\boldsymbol{x}_{n}, n\\right) & =\\left(\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right) e^{i n \\theta} \\\\\ng\\left(\\boldsymbol{x}_{m}, \\boldsymbol{x}_{n}, m-n\\right) & =\\operatorname{Re}\\left[\\left(\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}\\right)\\left(\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right)^{*} e^{i(m-n) \\theta}\\right]\n\\end{aligned}\n$$\n\nwhere $\\operatorname{Re}[\\cdot]$ is the real part of a complex number and $\\left(\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right)^{*}$ represents the conjugate complex number of $\\left(\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right)$. $\\theta \\in \\mathbb{R}$ is a preset non-zero constant. We can further write $f_{\\{q, k\\}}$ in a multiplication matrix:\n\n$$\nf_{\\{q, k\\}}\\left(\\boldsymbol{x}_{m}, m\\right)=\\left(\\begin{array}{cc}\n\\cos m \\theta & -\\sin m \\theta \\\\\n\\sin m \\theta & \\cos m \\theta\n\\end{array}\\right)\\left(\\begin{array}{cc}\nW_{\\{q, k\\}}^{(11)} & W_{\\{q, k\\}}^{(12)} \\\\\nW_{\\{q, k\\}}^{(21)} & W_{\\{q, k\\}}^{(22)}\n\\end{array}\\right)\\binom{x_{m}^{(1)}}{x_{m}^{(2)}}\n$$\n\nwhere $\\left(x_{m}^{(1)}, x_{m}^{(2)}\\right)$ is $\\boldsymbol{x}_{m}$ expressed in the 2D coordinates. Similarly, $g$ can be viewed as a matrix and thus enables the solution of formulation in Section (3.1) under the 2D case. Specifically, incorporating the relative position embedding is straightforward: simply rotate the affine-transformed word embedding vector by amount of angle multiples of its position index and thus interprets the intuition behind Rotary Position Embedding. ### 3.2.2 General form\n\nIn order to generalize our results in 2D to any $\\boldsymbol{x}_{i} \\in \\mathbb{R}^{d}$ where $d$ is even, we divide the d-dimension space into $d / 2$ sub-spaces and combine them in the merit of the linearity of the inner product, turning $f_{\\{q, k\\}}$ into:\n\n$$\nf_{\\{q, k\\}}\\left(\\boldsymbol{x}_{m}, m\\right)=\\boldsymbol{R}_{\\Theta, m}^{d} \\boldsymbol{W}_{\\{q, k\\}} \\boldsymbol{x}_{m}\n$$\n\nwhere\n\n$$\n\\boldsymbol{R}_{\\Theta, m}^{d}=\\left(\\begin{array}{ccccccc}\n\\cos m \\theta_{1} & -\\sin m \\theta_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\sin m \\theta_{1} & \\cos m \\theta_{1} & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos m \\theta_{2} & -\\sin m \\theta_{2} & \\cdots & 0 & 0 \\\\\n0 & 0 & \\sin m \\theta_{2} & \\cos m \\theta_{2} & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos m \\theta_{d / 2} & -\\sin m \\theta_{d / 2} \\\\\n0 & 0 & 0 & 0 & \\cdots & \\sin m \\theta_{d / 2} & \\cos m \\theta_{d / 2}\n\\end{array}\\right)\n$$\n\nis the rotary matrix with pre-defined parameters $\\Theta=\\left\\{\\theta_{i}=10000^{-2(i-1) / d}, i \\in[1,2, \\ldots, d / 2]\\right\\}$. A graphic illustration of RoPE is shown in Figure (1) Applying our RoPE to self-attention in Equation (2), we obtain:\n\n$$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\left(\\boldsymbol{R}_{\\Theta, m}^{d} \\boldsymbol{W}_{q} \\boldsymbol{x}_{m}\\right)^{\\boldsymbol{\\top}}\\left(\\boldsymbol{R}_{\\Theta, n}^{d} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right)=\\boldsymbol{x}^{\\top} \\boldsymbol{W}_{q} R_{\\Theta, n-m}^{d} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\n$$\n\nwhere $\\boldsymbol{R}_{\\Theta, n-m}^{d}=\\left(\\boldsymbol{R}_{\\Theta, m}^{d}\\right)^{\\top} \\boldsymbol{R}_{\\Theta, n}^{d}$. Note that $\\boldsymbol{R}_{\\Theta}^{d}$ is an orthogonal matrix, which ensures stability during the process of encoding position information. In addition, due to the sparsity of $R_{\\Theta}^{d}$, applying matrix multiplication directly as in Equation (16) is not computationally efficient; we provide another realization in theoretical explanation. In contrast to the additive nature of position embedding method adopted in the previous works, i.e., Equations (3) to (10), our approach is multiplicative. Moreover, RoPE naturally incorporates relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding when applied with self-attention. ![](https://cdn.mathpix.com/cropped/2024_09_12_1f43ef2086823d449664g-05.jpg?height=663&width=1175&top_left_y=1387&top_left_x=475)\n\nFigure 1: Implementation of Rotary Position Embedding(RoPE). ### 3.3 Properties of RoPE\n\nLong-term decay: Following Vaswani et al. [2017], we set $\\theta_{i}=10000^{-2 i / d}$. One can prove that this setting provides a long-term decay property (refer to Section [3.4.3) for more details), which means the inner-product will decay when the relative position increase. This property coincides with the intuition that a pair of tokens with a long relative distance should have less connection. RoPE with linear attention: The self-attention can be rewritten in a more general form. $$\n\\operatorname{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})_{m}=\\frac{\\sum_{n=1}^{N} \\operatorname{sim}\\left(\\boldsymbol{q}_{m}, \\boldsymbol{k}_{n}\\right) \\boldsymbol{v}_{n}}{\\sum_{n=1}^{N} \\operatorname{sim}\\left(\\boldsymbol{q}_{m}, \\boldsymbol{k}_{n}\\right)}\n$$\n\nThe original self-attention chooses $\\operatorname{sim}\\left(\\boldsymbol{q}_{m}, \\boldsymbol{k}_{n}\\right)=\\exp \\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n} / \\sqrt{d}\\right)$. Note that the original self-attention should compute the inner product of query and key for every pair of tokens, which has a quadratic complexity $\\mathbb{O}\\left(N^{2}\\right)$. Follow Katharopoulos et al. [2020], the linear attentions reformulate Equation (17]) as\n\n$$\n\\operatorname{Attention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})_{m}=\\frac{\\sum_{n=1}^{N} \\phi\\left(\\boldsymbol{q}_{m}\\right)^{\\top} \\varphi\\left(\\boldsymbol{k}_{n}\\right) \\boldsymbol{v}_{n}}{\\sum_{n=1}^{N} \\phi\\left(\\boldsymbol{q}_{m}\\right)^{\\top} \\varphi\\left(\\boldsymbol{k}_{n}\\right)}\n$$\n\nwhere $\\phi(\\cdot), \\varphi(\\cdot)$ are usually non-negative functions. The authors of Katharopoulos et al. [2020] have proposed $\\phi(x)=\\varphi(x)=\\operatorname{elu}(x)+1$ and first computed the multiplication between keys and values using the associative property of matrix multiplication. A softmax function is used in Shen et al. [2021] to normalize queries and keys separately before the inner product, which is equivalent to $\\phi\\left(\\boldsymbol{q}_{i}\\right)=\\operatorname{softmax}\\left(\\boldsymbol{q}_{i}\\right)$ and $\\phi\\left(\\boldsymbol{k}_{j}\\right)=\\exp \\left(\\boldsymbol{k}_{j}\\right)$. For more details about linear attention, we encourage readers to refer to original papers. In this section, we focus on discussing incorporating RoPE with Equation 18). Since RoPE injects position information by rotation, which keeps the norm of hidden representations unchanged, we can combine RoPE with linear attention by multiplying the rotation matrix with the outputs of the non-negative functions. $$\n\\operatorname{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})_{m}=\\frac{\\sum_{n=1}^{N}\\left(\\boldsymbol{R}_{\\Theta, m}^{d} \\phi\\left(\\boldsymbol{q}_{m}\\right)\\right)^{\\top}\\left(\\boldsymbol{R}_{\\Theta, n}^{d} \\varphi\\left(\\boldsymbol{k}_{n}\\right)\\right) \\boldsymbol{v}_{n}}{\\sum_{n=1}^{N} \\phi\\left(\\boldsymbol{q}_{m}\\right)^{\\top} \\varphi\\left(\\boldsymbol{k}_{n}\\right)}\n$$\n\nIt is noteworthy that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in the numerator could contain negative terms. Although the weights for each value $\\boldsymbol{v}_{i}$ in Equation (19) are not strictly probabilistic normalized, we kindly argue that the computation can still model the importance of values. ### 3.4 Theoretical Explanation\n\n### 3.4.1 Derivation of RoPE under 2D\n\nUnder the case of $d=2$, we consider two-word embedding vectors $\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}$ corresponds to query and key and their position $m$ and $n$, respectively. According to eq. (1), their position-encoded counterparts are:\n\n$$\n\\begin{aligned}\n\\boldsymbol{q}_{m} & =f_{q}\\left(\\boldsymbol{x}_{q}, m\\right) \\\\\n\\boldsymbol{k}_{n} & =f_{k}\\left(\\boldsymbol{x}_{k}, n\\right)\n\\end{aligned}\n$$\n\nwhere the subscripts of $\\boldsymbol{q}_{m}$ and $\\boldsymbol{k}_{n}$ indicate the encoded positions information. Assume that there exists a function $g$ that defines the inner product between vectors produced by $f_{\\{q, k\\}}$ :\n\n$$\n\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}=\\left\\langle f_{q}\\left(\\boldsymbol{x}_{m}, m\\right), f_{k}\\left(\\boldsymbol{x}_{n}, n\\right)\\right\\rangle=g\\left(\\boldsymbol{x}_{m}, \\boldsymbol{x}_{n}, n-m\\right)\n$$\n\nwe further require below initial condition to be satisfied:\n\n$$\n\\begin{aligned}\n\\boldsymbol{q} & =f_{q}\\left(\\boldsymbol{x}_{q}, 0\\right) \\\\\n\\boldsymbol{k} & =f_{k}\\left(\\boldsymbol{x}_{k}, 0\\right)\n\\end{aligned}\n$$\n\nwhich can be read as the vectors with empty position information encoded. Given these settings, we attempt to find a solution of $f_{q}, f_{k}$. First, we take advantage of the geometric meaning of vector in 2 D and its complex counter part, decompose functions in Equations 20, and 21) into:\n\n$$\n\\begin{aligned}\nf_{q}\\left(\\boldsymbol{x}_{q}, m\\right) & =R_{q}\\left(\\boldsymbol{x}_{q}, m\\right) e^{i \\Theta_{q}\\left(\\boldsymbol{x}_{q}, m\\right)} \\\\\nf_{k}\\left(\\boldsymbol{x}_{k}, n\\right) & =R_{k}\\left(\\boldsymbol{x}_{k}, n\\right) e^{i \\Theta_{k}\\left(\\boldsymbol{x}_{k}, n\\right)} \\\\\ng\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n-m\\right) & =R_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n-m\\right) e^{i \\Theta_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n-m\\right)}\n\\end{aligned}\n$$\n\nwhere $R_{f}, R_{g}$ and $\\Theta_{f}, \\Theta_{g}$ are the radical and angular components for $f_{\\{q, k\\}}$ and $g$, respectively. Plug them into Equation 21 , we get the relation:\n\n$$\n\\begin{aligned}\nR_{q}\\left(\\boldsymbol{x}_{q}, m\\right) R_{k}\\left(\\boldsymbol{x}_{k}, n\\right) & =R_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n-m\\right) \\\\\n\\Theta_{k}\\left(\\boldsymbol{x}_{k}, n\\right)-\\Theta_{q}\\left(\\boldsymbol{x}_{q}, m\\right) & =\\Theta_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n-m\\right)\n\\end{aligned}\n$$\n\nwith the corresponding initial condition as:\n\n$$\n\\begin{aligned}\n\\boldsymbol{q} & =\\|\\boldsymbol{q}\\| e^{i \\theta_{q}}=R_{q}\\left(\\boldsymbol{x}_{q}, 0\\right) e^{i \\Theta_{q}\\left(\\boldsymbol{x}_{q}, 0\\right)} \\\\\n\\boldsymbol{k} & =\\|\\boldsymbol{k}\\| e^{i \\theta_{k}}=R_{k}\\left(\\boldsymbol{x}_{k}, 0\\right) e^{i \\Theta_{k}\\left(\\boldsymbol{x}_{k}, 0\\right)}\n\\end{aligned}\n$$\n\nwhere $\\|\\boldsymbol{q}\\|,\\|\\boldsymbol{k}\\|$ and $\\theta_{q}, \\theta_{k}$ are the radial and angular part of $\\boldsymbol{q}$ and $\\boldsymbol{k}$ on the 2D plane. Next, we set $m=n$ in Equation (24) and take into account initial conditions in Equation (25):\n\n$$\n\\begin{aligned}\n& R_{q}\\left(\\boldsymbol{x}_{q}, m\\right) R_{k}\\left(\\boldsymbol{x}_{k}, m\\right)=R_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 0\\right)=R_{q}\\left(\\boldsymbol{x}_{q}, 0\\right) R_{k}\\left(\\boldsymbol{x}_{k}, 0\\right)=\\|\\boldsymbol{q}\\|\\|\\boldsymbol{k}\\|, \\\\\n& \\Theta_{k}\\left(\\boldsymbol{x}_{k}, m\\right)-\\Theta_{q}\\left(\\boldsymbol{x}_{q}, m\\right)=\\Theta_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 0\\right)=\\Theta_{k}\\left(\\boldsymbol{x}_{k}, 0\\right)-\\Theta_{q}\\left(\\boldsymbol{x}_{q}, 0\\right)=\\theta_{k}-\\theta_{q} . \\end{aligned}\n$$\n\nOn one hand, from, a straightforward solution of $R_{f}$ could be formed from Equation (26a) :\n\n$$\n\\begin{aligned}\nR_{q}\\left(\\boldsymbol{x}_{q}, m\\right) & =R_{q}\\left(\\boldsymbol{x}_{q}, 0\\right)=\\|\\boldsymbol{q}\\| \\\\\nR_{k}\\left(\\boldsymbol{x}_{k}, n\\right) & =R_{k}\\left(\\boldsymbol{x}_{k}, 0\\right)=\\|\\boldsymbol{k}\\| \\\\\nR_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n-m\\right) & =R_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 0\\right)=\\|\\boldsymbol{q}\\|\\|\\boldsymbol{k}\\|\n\\end{aligned}\n$$\n\nwhich interprets the radial functions $R_{q}, R_{k}$ and $R_{g}$ are independent from the position information. On the other hand, as can be noticed in Equation 26b), $\\Theta_{q}\\left(\\boldsymbol{x}_{q}, m\\right)-\\theta_{q}=\\Theta_{k}\\left(\\boldsymbol{x}_{k}, m\\right)-\\theta_{k}$ indicates that the angular functions does not dependent on query and key, we set them to $\\Theta_{f}:=\\Theta_{q}=\\Theta_{k}$ and term $\\Theta_{f}\\left(\\boldsymbol{x}_{\\{q, k\\}}, m\\right)-\\theta_{\\{q, k\\}}$ is a function of position $m$ and is independent of word embedding $\\boldsymbol{x}_{\\{q, k\\}}$, we denote it as $\\phi(m)$, yielding:\n\n$$\n\\Theta_{f}\\left(\\boldsymbol{x}_{\\{q, k\\}}, m\\right)=\\phi(m)+\\theta_{\\{q, k\\}}\n$$\n\nFurther, by plugging $n=m+1$ to Equation and consider the above equation, we can get:\n\n$$\n\\phi(m+1)-\\phi(m)=\\Theta_{g}\\left(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 1\\right)+\\theta_{q}-\\theta_{k}\n$$\n\nSince RHS is a constant irrelevant to $m, \\phi(m)$ with continuous integer inputs produce an arithmetic progression:\n\n$$\n\\phi(m)=m \\theta+\\gamma\n$$\n\nwhere $\\theta, \\gamma \\in \\mathbb{R}$ are constants and $\\theta$ is non-zero. To summarize our solutions from Equations (27) to (30):\n\n$$\n\\begin{aligned}\nf_{q}\\left(\\boldsymbol{x}_{q}, m\\right) & =\\|\\boldsymbol{q}\\| e^{i \\theta_{q}+m \\theta+\\gamma}=\\boldsymbol{q} e^{i(m \\theta+\\gamma)} \\\\\nf_{k}\\left(\\boldsymbol{x}_{k}, n\\right) & =\\|\\boldsymbol{k}\\| e^{i \\theta_{k}+n \\theta+\\gamma}=\\boldsymbol{k} e^{i(n \\theta+\\gamma)}\n\\end{aligned}\n$$\n\nNote that we do not apply any constrains to $f_{q}$ and $f_{k}$ of Equation 22 , thus $f_{q}\\left(\\boldsymbol{x}_{m}, 0\\right)$ and $f_{k}\\left(\\boldsymbol{x}_{n}, 0\\right)$ are left to choose freely. To make our results comparable to Equation (3), we define:\n\n$$\n\\begin{aligned}\n& \\boldsymbol{q}=f_{q}\\left(\\boldsymbol{x}_{m}, 0\\right)=\\boldsymbol{W}_{q} \\boldsymbol{x}_{n} \\\\\n& \\boldsymbol{k}=f_{k}\\left(\\boldsymbol{x}_{n}, 0\\right)=\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\n\\end{aligned}\n$$\n\nThen, we simply set $\\gamma=0$ in Equation 31) of the final solution:\n\n$$\n\\begin{aligned}\nf_{q}\\left(\\boldsymbol{x}_{m}, m\\right) & =\\left(\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}\\right) e^{i m \\theta} \\\\\nf_{k}\\left(\\boldsymbol{x}_{n}, n\\right) & =\\left(\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right) e^{i n \\theta}\n\\end{aligned}\n$$\n\n### 3.4.2 Computational efficient realization of rotary matrix multiplication\n\nTaking the advantage of the sparsity of $\\boldsymbol{R}_{\\Theta, m}^{d}$ in Equation 15, a more computational efficient realization of a multiplication of $R_{\\Theta}^{d}$ and $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ is:\n\n$$\n\\boldsymbol{R}_{\\Theta, m}^{d} \\boldsymbol{x}=\\left(\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\vdots \\\\\nx_{d-1} \\\\\nx_{d}\n\\end{array}\\right) \\otimes\\left(\\begin{array}{c}\n\\cos m \\theta_{1} \\\\\n\\cos m \\theta_{1} \\\\\n\\cos m \\theta_{2} \\\\\n\\cos m \\theta_{2} \\\\\n\\vdots \\\\\n\\cos m \\theta_{d / 2} \\\\\n\\cos m \\theta_{d / 2}\n\\end{array}\\right)+\\left(\\begin{array}{c}\n-x_{2} \\\\\nx_{1} \\\\\n-x_{4} \\\\\nx_{3} \\\\\n\\vdots \\\\\n-x_{d} \\\\\nx_{d-1}\n\\end{array}\\right) \\otimes\\left(\\begin{array}{c}\n\\sin m \\theta_{1} \\\\\n\\sin m \\theta_{1} \\\\\n\\sin m \\theta_{2} \\\\\n\\sin m \\theta_{2} \\\\\n\\vdots \\\\\n\\sin m \\theta_{d / 2} \\\\\n\\sin m \\theta_{d / 2}\n\\end{array}\\right)\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1f43ef2086823d449664g-08.jpg?height=600&width=1158&top_left_y=237&top_left_x=478)\n\nFigure 2: Long-term decay of RoPE. ### 3.4.3 Long-term decay of RoPE\n\nWe can group entries of vectors $\\boldsymbol{q}=\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}$ and $\\boldsymbol{k}=\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}$ in pairs, and the inner product of RoPE in Equation (16) can be written as a complex number multiplication.",
    "roformer-7": "$$\n\\left(\\boldsymbol{R}_{\\Theta, m}^{d} \\boldsymbol{W}_{q} \\boldsymbol{x}_{m}\\right)^{\\top}\\left(\\boldsymbol{R}_{\\Theta, n}^{d} \\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\\right)=\\operatorname{Re}\\left[\\sum_{i=0}^{d / 2-1} \\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^{*} e^{i(m-n) \\theta_{i}}\\right]\n$$\n\nwhere $\\boldsymbol{q}_{[2 i: 2 i+1]}$ represents the $2 i^{\\text {th }}$ to $(2 i+1)^{\\text {th }}$ entries of $\\boldsymbol{q}$. Denote $h_{i}=\\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^{*}$ and $S_{j}=$ $\\sum_{i=0}^{j-1} e^{i(m-n) \\theta_{i}}$, and let $h_{d / 2}=0$ and $S_{0}=0$, we can rewrite the summation using Abel transformation\n\n$$\n\\sum_{i=0}^{d / 2-1} \\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^{*} e^{i(m-n) \\theta_{i}}=\\sum_{i=0}^{d / 2-1} h_{i}\\left(S_{i+1}-S_{i}\\right)=-\\sum_{i=0}^{d / 2-1} S_{i+1}\\left(h_{i+1}-h_{i}\\right)\n$$\n\nThus,\n\n$$\n\\begin{aligned}\n\\left|\\sum_{i=0}^{d / 2-1} \\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^{*} e^{i(m-n) \\theta_{i}}\\right| & =\\left|\\sum_{i=0}^{d / 2-1} S_{i+1}\\left(h_{i+1}-h_{i}\\right)\\right| \\\\\n& \\leq \\sum_{i=0}^{d / 2-1}\\left|S_{i+1}\\right|\\left|\\left(h_{i+1}-h_{i}\\right)\\right| \\\\\n& \\leq\\left(\\max _{i}\\left|h_{i+1}-h_{i}\\right|\\right) \\sum_{i=0}^{d / 2-1}\\left|S_{i+1}\\right|\n\\end{aligned}\n$$\n\nNote that the value of $\\frac{1}{d / 2} \\sum_{i=1}^{d / 2}\\left|S_{i}\\right|$ decay with the relative distance $m-n$ increases by setting $\\theta_{i}=10000^{-2 i / d}$, as shown in Figure (2). ## 4 Experiments and Evaluation\n\nWe evaluate the proposed RoFormer on various NLP tasks as follows. We validate the performance of the proposed solution on machine translation task Section (4.1) Then, we compare our RoPE implementation with BERTDevlin et al. 2019] during the pre-training stage in Section (4.2) Based on the pre-trained model, in Section (4.3) we further carry out evaluations across different downstream tasks from GLUE benchmarks Singh et al. [2018]. In Addition, we conduct experiments using the proposed RoPE with the linear attention of PerFormer Choromanski et al. [2020] in\n\nTable 1: The proposed RoFormer gives better BLEU scores compared to its baseline alternative Vaswani et al. [2017] on the WMT 2014 English-to-German translation taskBojar et al. [2014]. | Model | BLEU |\n| :---: | :---: |\n| Transformer-base Vaswani et al. \\|2017] | 27.3 |\n| RoFormer | 27.5 |\n\nSection (4.4) By the end, additional tests on Chinese data are included in Section (4.5). All the experiments were run on two cloud severs with $4 \\times$ V100 GPUs. ### 4.1 Machine Translation\n\nWe first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks. ### 4.1.1 Experimental Settings\n\nWe choose the standard WMT 2014 English-German datase Bojar et al. [2014], which consists of approximately 4.5 million sentence pairs. We compare to the transformer-based baseline alternative Vaswani et al. [2017]. ### 4.1.2 Implementation details\n\nWe carry out some modifications on self-attention layer of the baseline model Vaswani et al. [2017] to enable RoPE to its learning process. We replicate the setup for English-to-German translation with a vocabulary of 37 k based on a joint source and target byte pair encoding(BPE)Sennrich et al. [2015]. During the evaluation, a single model is obtained by averaging the last 5 checkpoints. The result uses beam search with a beam size of 4 and length penalty 0.6 . We implement the experiment in PyTorch in the fairseq toolkit (MIT License)Ott et al. [2019]. Our model is optimized with the Adam optimizer using $\\beta_{1}=0.9, \\beta_{2}=0.98$, learning rate is increased linearly from $1 e-7$ to $5 e-4$ and then decayed proportionally to the inverse square root of the step number. Label smoothing with 0.1 is also adopted. We report the BLELPapineni et al. 2002] score on the test set as the final metric. ### 4.1.3 Results\n\nWe train the baseline model and our RoFormer under the same settings and report the results in Table (1) As can be seen, our model gives better BLEU scores compared to the baseline Transformer. ### 4.2 Pre-training Language Modeling\n\nThe second experiment is to validate the performance of our proposal in terms of learning contextual representations. To achieve this, we replace the original sinusoidal position encoding of BERT with our RoPE during the pre-training step. ### 4.2.1 Experimental Settings\n\nWe use the BookCorpus Zhu et al. [2015] and the Wikipedia Corpus Foundation [2021] from Huggingface Datasets library (Apache License 2.0) for pre-training. The corpus is further split into train and validation sets at 8:2 ratio. We use the masked language-modeling (MLM) loss values of the training process as an evaluation metric. The well-known BERT Devlin et al. [2019] is adopted as our baseline model. Note that we use bert-base-uncased in our experiments. ### 4.2.2 Implementation details\n\nFor RoFormer, we replace the sinusoidal position encoding in the self-attention block of the baseline model with our proposed RoPE and realizes self-attention according to Equation (16). We train both BERT and RoFormer with batch size 64 and maximum sequence length of 512 for 100k steps. AdamW Loshchilov and Hutter [2017] is used as the optimizer with learning rate $1 \\mathrm{e}-5$. ### 4.2.3 Results\n\nThe MLM loss during pre-training is shown on the left plot of Figure (3). Compare to the vanilla BERT, RoFormer experiences faster convergence. ![](https://cdn.mathpix.com/cropped/2024_09_12_1f43ef2086823d449664g-10.jpg?height=483&width=1489&top_left_y=296&top_left_x=297)\n\nFigure 3: Evaluation of RoPE in language modeling pre-training. Left: training loss for BERT and RoFormer. Right: training loss for PerFormer with and without RoPE. ### 4.3 Fine-tuning on GLUE tasks\n\nConsistent with the previous experiments, we fine-tune the weights of our pre-trained RoFormer across various GLUE tasks in order to evaluate its generalization ability on the downstream NLP tasks. ### 4.3.1 Experimental Settings\n\nWe look at several datasets from GLUE, i.e. MRPC Dolan and Brockett [2005], SST-2 Socher et al. [2013], QNLI Rajpurkar et al. [2016], STS-B Al-Natsheh [2017], QQP Chen et al. [2018b] and MNLI Williams et al. [2018]. We use F1-score for MRPC and QQP dataset, spearman correlation for STS-B, and accuracy for the remaining as the evaluation metrics. ### 4.3.2 Implementation details\n\nWe use Huggingface Transformers library (Apache License 2.0 Wolf et al. [2020] to fine-tune each of the aforementioned downstream tasks for 3 epochs, with a maximum sequence length of 512, batch size of 32 and learning rates 2,3,4,5e-5. Following Devlin et al. [2019], we report the best-averaged results on the validation set. Table 2: Comparing RoFormer and BERT by fine tuning on downstream GLEU tasks. | Model | MRPC | SST-2 | QNLI | STS-B | QQP | MNLI(m/mm) |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| BERT Devlin et al. [2019] | 88.9 | 93.5 | 90.5 | 85.8 | 71.2 | $84.6 / 83.4$ |\n| RoFormer | $\\mathbf{8 9 . 5}$ | 90.7 | 88.0 | $\\mathbf{8 7 . 0}$ | $\\mathbf{8 6 . 4}$ | $80.2 / 79.8$ |\n\n### 4.3.3 Results\n\nThe evaluation results of the fine-tuning tasks are reported in Table (2) As can be seen, RoFormer can significantly outperform BERT in three out of six datasets, and the improvements are considerable. ### 4.4 Performer with RoPE\n\nPerformer Choromanski et al. [2020] introduces an alternative attention mechanism, linear attention, which is designed to avoid quadratic computation cost that scales with input sequence length. As discussed in Section (3.3) the proposed RoPE can be easily implemented in the PerFormer model to realize the relative position encoding while keeping its linearly scaled complexity in self-attention. We demonstrate its performance with the pre-training task of language modeling. ### 4.4.1 Implementation details\n\nWe carry out tests on the Enwik8 dataset Mahoney [2006], which is from English Wikipedia that includes markup, special characters and text in other languages in addition to English text. We incorporate RoPE into the 12 layer char-based PerFormer with 768 dimensions and 12 heads ${ }^{2}$ To better illustrate the efficacy of RoPE, we report the loss curves of the pre-training process with and without RoPE under the same settings, i.e., learning rate $1 \\mathrm{e}-4$, batch size 128 and a fixed maximum sequence length of 1024 , etc. ### 4.4.2 Results\n\nAs shown on the right plot of Figure (3) substituting RoPE into Performer leads to rapid convergence and lower loss under the same amount of training steps. These improvements, in addition to the linear complexity, make Performer more attractive. ### 4.5 Evaluation on Chinese Data\n\nIn addition to experiments on English data, we show additional results on Chinese data. To validate the performance of RoFormer on long texts, we conduct experiments on long documents whose length exceeds 512 characters. ### 4.5.1 Implementation\n\nIn these experiments, we carried out some modifications on WoBERT Su [2020] by replacing the absolute position embedding with our proposed RoPE. As a cross-comparison with other pre-trained Transformer-based models in Chinese, i.e. BERT Devlin et al. [2019], WoBERT Su [2020], and NEZHA Wei et al. [2019], we tabulate their tokenization level and position embedding information in Table (3)\n\nTable 3: Cross-comparison between our RoFormer and other pre-trained models on Chinese data. 'abs' and 'rel' annotates absolute position embedding and relative position embedding, respectively. | Model | BERT Devlin et al. 2019] | WoBERT Su [2020] | NEZHAWei et al. [2019] | RoFormer |\n| :---: | :---: | :---: | :---: | :---: |\n| Tokenization level | char | word | char | word |\n| Position embedding | abs.",
    "roformer-8": "| abs. | rel. | RoPE |\n\n### 4.5.2 Pre-training\n\nWe pre-train RoFormer on approximately 34GB of data collected from Chinese Wikipedia, news and forums. The pre-training is carried out in multiple stages with changing batch size and maximum input sequence length in order to adapt the model to various scenarios. As shown in Table (4) the accuracy of RoFormer elevates with an increasing upper bound of sequence length, which demonstrates the ability of RoFormer in dealing with long texts. We claim that this is the attribute to the excellent generalizability of the proposed RoPE. Table 4: Pre-training strategy of RoFormer on Chinese dataset. The training procedure is divided into various consecutive stages. In each stage, we train the model with a specific combination of maximum sequence length and batch size. | Stage | Max seq length | Batch size | Training steps | Loss | Accuracy |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1 | 512 | 256 | 200 k | 1.73 | $65.0 \\%$ |\n| 2 | 1536 | 256 | 12.5 k | 1.61 | $66.8 \\%$ |\n| 3 | 256 | 256 | 120 k | 1.75 | $64.6 \\%$ |\n| 4 | 128 | 512 | 80 k | 1.83 | $63.4 \\%$ |\n| 5 | 1536 | 256 | 10 k | 1.58 | $67.4 \\%$ |\n| 6 | 512 | 512 | 30 k | 1.66 | $66.2 \\%$ |\n\n### 4.5.3 Downstream Tasks \\& Dataset\n\nWe choose Chinese AI and Law 2019 Similar Case Matching (CAIL2019-SCM) Xiao et al. [2019] dataset to illustrate the ability of RoFormer in dealing with long texts, i.e., semantic text matching. CAIL2019-SCM contains 8964 triplets\n\n[^1]of cases published by the Supreme People's Court of China. The input triplet, denoted as (A, B and C), are fact descriptions of three cases. The task is to predict whether the pair $(A, B)$ is closer than $(A, C)$ under a predefined similarity measure. Note that existing methods mostly cannot perform significantly on CAIL2019-SCM dataset due to the length of documents (i.e., mostly more than 512 characters). We split train, validation and test sets based on the well-known ratio 6:2:2. ### 4.5.4 Results\n\nWe apply the pre-trained RoFormer model to CAIL2019-SCM with different input lengths. The model is compared with the pre-trained BERT and WoBERT model on the same pre-training data, as shown in Table (5). With short text cut-offs, i.e., 512 , the result from RoFormer is comparable to WoBERT and is slightly better than the BERT implementation. However, when increasing the maximum input text length to 1024 , RoFormer outperforms WoBERT by an absolute improvement of $1.5 \\%$.",
    "roformer-9": "Table 5: Experiment results on CAIL2019-SCM task. Numbers in the first column denote the maximum cut-off sequence length. The results are presented in terms of percent accuracy. | Model | Validation | Test |\n| :--- | :--- | :--- |\n| BERT-512 | $64.13 \\%$ | $67.77 \\%$ |\n| WoBERT-512 | $64.07 \\%$ | $68.10 \\%$ |\n| RoFormer-512 | $64.13 \\%$ | $68.29 \\%$ |\n| RoFormer-1024 | $\\mathbf{6 6 . 0 7} \\%$ | $\\mathbf{6 9 . 7 9 \\%}$ |\n\n### 4.5.5 Limitations of the work\n\nAlthough we provide theoretical groundings as well as promising experimental justifications, our method is limited by following facts:\n\n- Despite the fact that we mathematically format the relative position relations as rotations under 2D sub-spaces, there lacks of thorough explanations on why it converges faster than baseline models that incorporates other position encoding strategies. - Although we have proved that our model has favourable property of long-term decay for intern-token products, Section (3.3) which is similar to the existing position encoding mechanisms, our model shows superior performance on long texts than peer models, we have not come up with a faithful explanation. Our proposed RoFormer is built upon the Transformer-based infrastructure, which requires hardware resources for pre-training purpose. ## 5 Conclusions\n\nIn this work, we proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative position can be naturally formulated using vector production in self-attention, with absolution position information being encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of the proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark datasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show that our proposed RoFormer can achieve better performance on long texts task. ## References\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning, pages 1243-1252. PMLR, 2017. Md. Amirul Islam, Sen Jia, and Neil D.",
    "roformer-10": "B. Bruce. How much position information do convolutional neural networks encode? ArXiv, abs/2001.08248, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.",
    "roformer-11": "In NAACL-HLT, 2019. A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.",
    "roformer-12": "2019. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions?",
    "roformer-13": "In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=ByxRMONtvr\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020. URL https://openreview.net/pdf?id=r1xMH1BtvB. A. Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. Ankur P. Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In $E M N L P, 2016$. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In NAACL-HLT, 2018. Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, I. Simon, C. Hawthorne, Andrew M. Dai, M. Hoffman, M. Dinculescu, and D. Eck. Music transformer. arXiv: Learning, 2018. Zihang Dai, Z. Yang, Yiming Yang, J. Carbonell, Quoc V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In $A C L, 2019$. Z. Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "roformer-14": "J. Mach. Learn. Res., 21: 140:1-140:67, 2020. Guolin Ke, Di He, and T. Liu. Rethinking positional encoding in language pre-training. ArXiv, abs/2006.15595, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention.",
    "roformer-15": "ArXiv, abs/2006.03654, 2020. Zhiheng Huang, Davis Liang, Peng Xu , and Bing Xiang. Improve transformer models with better relative position embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3327-3335, Online, November 2020. Association for Computational Linguistics. doi $10.18653 / \\mathrm{v} 1 / 2020$.findings-emnlp.298. URL https://www.aclweb.org/anthology/2020.findings-emnlp. 298\nXuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and Cho-Jui Hsieh. Learning to encode position for transformer with continuous dynamical model. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 6327-6335. PMLR, 2020. URL http://proceedings.mlr.press/v119/liu20n.html. Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 6572-6583, 2018a. URL https: //proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html. Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding word order in complex embeddings. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Hke-WTVtwr. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "roformer-16": "In International Conference on Machine Learning, pages 5156-5165. PMLR, 2020. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages $3531-3539,2021$. Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.",
    "roformer-17": "042018. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, A. Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, J. Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. ArXiv, abs/2009.14794, 2020. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Alevs Tamchyna. Findings of the 2014 workshop on statistical machine translation. pages 12-58, 06 2014. doi $10.3115 / \\mathrm{v} 1 / \\mathrm{W} 14-3302$\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. 08 2015. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling.",
    "roformer-18": "pages 48-53, 01 2019. doi 10.18653/v1/N19-4009. Kishore Papineni, Salim Roukos, Todd Ward, and Wei Jing Zhu. Bleu: a method for automatic evaluation of machine translation.",
    "roformer-19": "10 2002. doi $10.3115 / 1073083.1073135$. Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.",
    "roformer-20": "In arXiv preprint arXiv:1506.06724, 2015. Wikimedia Foundation. Wikimedia downloads, https://dumps.wikimedia.org, 2021. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv e-prints, art. arXiv:1711.05101, November 2017. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www. aclweb.org/ anthology/I05-5002. Richard Socher, A.",
    "roformer-21": "Perelygin, J.Y. Wu, J. Chuang, C.D. Manning, A.Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. EMNLP, 1631:1631-1642, 012013. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text.",
    "roformer-22": "pages 2383-2392, 01 2016. doi 10.18653/v1/D16-1264. Hussein Al-Natsheh. Udl at semeval-2017 task 1: Semantic textual similarity estimation of english sentence pairs using regression model over pairwise features.",
    "roformer-23": "082017. Z. Chen, H. Zhang, and L. Zhang, X.and Zhao. Quora question pairs., 2018b. URL https://www.kaggle.com/c/ quora-question-pairs. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. pages 1112-1122, 01 2018. doi $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 18-1101$. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M.",
    "roformer-24": "Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. Matt Mahoney. Large text compression benchmark, http://www.mattmahoney.net/dc/text.html, 2006. Jianlin Su. Wobert: Word-based chinese bert model - zhuiyiai. Technical report, 2020. URLhttps://github.com/ ZhuiyiTechnology/WoBERT. Victor Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen, and Qun Liu. Nezha: Neural contextualized representation for chinese language understanding. 082019. Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang, Xianpei Han, Zhen hu, Heng Wang, and Jianfeng Xu. Cail2019-scm: A dataset of similar case matching in legal domain. 112019. [^0]:    ${ }^{1} \\mathrm{~A}$ stack of multiple CNN layers can also capture longer intra-token relation, here we only consider single layer setting. [^1]:    ${ }^{2}$ For this experiment, we adopt code (MIT License) from https://github.com/lucidrains/performer-pytorch\n\n"
}