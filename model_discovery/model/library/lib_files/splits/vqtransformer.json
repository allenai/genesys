{
    "vqtransformer-0": "Transformer-VQ: Linear-Time Transformers via Vector Quantization\n\nLucas D. Lingle Independent Researcher lucasdaxlingle@gmail.com\n\nAbstract\n\nWe introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ\u2019s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq\n\n1 Introduction\n\nTransformer (Vaswani et al., 2017) language models would ideally scale to long sequences, since their predictive abilities often improve as context length increases (Dai et al., 2019; Kaplan et al., 2020). Unfortunately, the standard transformer uses a self-attention mechanism with a quadratic time complexity with respect to sequence length. This limits the practicality of applying transformers to very long sequences, since increasing the sequence length by a factor of increases the attention computations by a factor of . Transformer variants that overcome this efficiency bottleneck have the potential to facilitate new long-context applications and enable new breakthroughs. Up to this point, a variety of efficient transformers (Tay et al., 2020b) have been proposed to scale to long sequences. Techniques include sparsity (Child et al., 2019; Ye et al., 2019; Beltagy et al., 2020; Kitaev et al., 2020; Qiu et al., 2020; Roy et al., 2021; Tay et al., 2020a; Sukhbaatar et al., 2021; Wu et al., 2022; Liu et al., 2023; Zhang et al., 2023), compression (Liu et al., 2018; Rae et al., 2020; Ainslie et al., 2020; Zhu et al., 2021; Ren et al., 2021; Nawrot et al., 2021; 2023), low-rank approximations (Wang et al., 2020; Vyas et al., 2020; Katharopoulos et al., 2020; Xiong et al., 2021; Tay et al., 2021; Choromanski et al., 2021), and cross-attention operations (Dai et al., 2019; Ma et al., 2021; Hutchins et al., 2022; Hawthorne et al., 2022). Other efficient sequence models have also been proposed (Gu et al., 2022; Lee-Thorp et al., 2022; Poli et al., 2023; Peng et al., 2023). In this paper, we present Transformer-VQ, a transformer decoder with dense self-attention computible in linear time with respect to sequence length. This is made possible through a combination of vector-quantized keys, localized positional biases, and a truncation-free yet fixed-size cache mechanism. Beyond its efficiency, Transformer-VQ is also simple to implement sampling for, as it does not require any periodic operations beyond those occurring at every token. 2 Preliminaries\n\n2.1 Notation\n\nThe real numbers are denoted by and the extended real numbers by . Zero-based indices are used for all tensors. When indexing a matrix along the first axis, we use to denote a column vector and to denote a row vector. The functions , , denote LayerNorm (Ba et al., 2016), softmax, and concatenation, each applied row-wise. The symbols denote equality by definition, proportionality, element-wise product, element-wise exponentiation, the Kronecker delta function, and the stop-gradient operator. We assume familiarity with transformers (Vaswani et al., 2017), and use the notation to denote the model width, to denote the number of attention heads per layer, to denote the query/key vector width, to denote the value vector width, to denote the feedforward fan-out width. 2.2 Vector Quantization\n\nVector quantization (VQ) is a technique used extensively throughout this work. Here we briefly review VQ, motivate its use in self-attention, and discuss the VQ scheme introduced for representation learning by van den Oord et al. (2017). All proofs are given in Appendix A. 2.3 Vector Quantizers and Codebooks\n\nDefinition 2.1. A vector quantizer is a function with domain and codomain . For an input , its output is given by\n\nz \ud835\udc67 \\displaystyle z \u225c arg \u200b min s \u200b \u2016 \ud835\udc31 \u2212 \ud835\udc02 s \u2016 2 \u225c absent subscript arg min \ud835\udc60 superscript norm \ud835\udc31 subscript \ud835\udc02 \ud835\udc60 2 \\displaystyle\\triangleq\\operatorname*{arg\\,min}_{s}||\\mathbf{x}-\\mathbf{C}_{s}||^{2} (1) \ud835\udc31 ^ ^ \ud835\udc31 \\displaystyle\\hat{\\mathbf{x}} \u225c \ud835\udc02 z \u225c absent subscript \ud835\udc02 \ud835\udc67 \\displaystyle\\triangleq\\mathbf{C}_{z} (2)\n\nwhere is known as the codebook. The row indices of are called shortcodes, and the rows themselves are called codewords. Theorem 2.2 (Based on Guo et al. (2019)). Let be a random variable with , and let be a random variable independent of . Let be a deterministic function. Then\n\n\ud835\udd3c \ud835\udc2a , \ud835\udc24 \u200b \u2016 \ud835\udc2a \u22a4 \u200b \ud835\udc24 \u2212 \ud835\udc2a \u22a4 \u200b \u03c6 \u200b ( \ud835\udc24 ) \u2016 2 subscript \ud835\udd3c \ud835\udc2a \ud835\udc24 superscript norm superscript \ud835\udc2a top \ud835\udc24 superscript \ud835\udc2a top \ud835\udf11 \ud835\udc24 2 \\displaystyle\\mathbb{E}_{\\mathbf{q},\\mathbf{k}}||\\mathbf{q}^{\\top}\\mathbf{k}-\\mathbf{q}^{\\top}\\varphi(\\mathbf{k})||^{2} \u221d \ud835\udd3c \ud835\udc24 \u200b \u2016 \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) \u2016 2 . proportional-to absent subscript \ud835\udd3c \ud835\udc24 superscript norm \ud835\udc24 \ud835\udf11 \ud835\udc24 2 \\displaystyle\\propto\\mathbb{E}_{\\mathbf{k}}||\\mathbf{k}-\\varphi(\\mathbf{k})||^{2}. (3)\n\nCorollary 2.3. Let the conditions of Theorem 2.2 hold. Given the constraint that , the choice minimizes . Corollary 2.4. Let the conditions of Theorem 2.2 hold. With we have\n\narg \u200b min \ud835\udc02 \u2061 \ud835\udd3c \ud835\udc2a , \ud835\udc24 \u200b \u2016 \ud835\udc2a \u22a4 \u200b \ud835\udc24 \u2212 \ud835\udc2a \u22a4 \u200b \ud835\udc24 ^ \u2016 2 subscript arg min \ud835\udc02 subscript \ud835\udd3c \ud835\udc2a \ud835\udc24 superscript norm superscript \ud835\udc2a top \ud835\udc24 superscript \ud835\udc2a top ^ \ud835\udc24 2 \\displaystyle\\operatorname*{arg\\,min}_{\\mathbf{C}}\\mathbb{E}_{\\mathbf{q},\\mathbf{k}}||\\mathbf{q}^{\\top}\\mathbf{k}-\\mathbf{q}^{\\top}\\hat{\\mathbf{k}}||^{2} = arg \u200b min \ud835\udc02 \u2061 \ud835\udd3c \ud835\udc24 \u200b \u2016 \ud835\udc24 \u2212 \ud835\udc24 ^ \u2016 2 . absent subscript arg min \ud835\udc02 subscript \ud835\udd3c \ud835\udc24 superscript norm \ud835\udc24 ^ \ud835\udc24 2 \\displaystyle=\\operatorname*{arg\\,min}_{\\mathbf{C}}\\mathbb{E}_{\\mathbf{k}}||\\mathbf{k}-\\hat{\\mathbf{k}}||^{2}. (4)\n\nRemark 2.5. Since finding the global minimizer can be expensive, we approximate it using a minibatch variant of streaming k-means, same as van den Oord et al. (2017). 2.4 Vector-Quantized Representation Learning\n\nDefinition 2.6 (Based on van den Oord et al. (2017)). A vector-quantizer with straight-through estimator is a function with domain and codomain . For an input , its output is given by\n\nz \ud835\udc67 \\displaystyle z \u225c arg \u200b min s \u200b \u2016 \ud835\udc31 \u2212 \ud835\udc02 s \u2016 2 \u225c absent subscript arg min \ud835\udc60 superscript norm \ud835\udc31 subscript \ud835\udc02 \ud835\udc60 2 \\displaystyle\\triangleq\\operatorname*{arg\\,min}_{s}||\\mathbf{x}-\\mathbf{C}_{s}||^{2} (5) \ud835\udc31 ^ ^ \ud835\udc31 \\displaystyle\\hat{\\mathbf{x}} \u225c \ud835\udc31 + SG \u200b ( \ud835\udc02 z \u2212 \ud835\udc31 ) . \u225c absent \ud835\udc31 SG subscript \ud835\udc02 \ud835\udc67 \ud835\udc31 \\displaystyle\\triangleq\\mathbf{x}+\\text{SG}(\\mathbf{C}_{z}-\\mathbf{x}). (6)\n\nRemark 2.7. For any , evaluates to . However, the computed Jacobian of the quantizer w.r.t. its input will now be an identity matrix everywhere, instead of a zero matrix almost everywhere. Intuitively, when using STVQ, gradients are \u2018transplanted\u2019 onto the unquantized vectors from their quantized counterparts. Remark 2.8. We overload the notation to operate row-wise on matrix-valued inputs. 3 Transformer-VQ\n\nWe now propose Transformer-VQ, a decoder-only transformer that can compute dense self-attention in linear time. Proofs for all theoretical results are given in Appendix A. 3.1 Quadratic-Time Formulation\n\nDefinition 3.1. Vector-Quantized Self-Attention is a function with domain and codomain . For an input its output is defined via\n\n\ud835\udc17 ~ ~ \ud835\udc17 \\displaystyle\\tilde{\\mathbf{X}} \u225c LN \u200b ( \ud835\udc17 ) \u2208 \u211d T \u00d7 D m \u225c absent LN \ud835\udc17 superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc5a \\displaystyle\\triangleq\\text{LN}(\\mathbf{X})\\in\\mathbb{R}^{T\\times D_{m}} (7) \ud835\udc10 \ud835\udc10 \\displaystyle\\mathbf{Q} \u225c \u03c4 \u2212 0.5 \u200b LN \u200b ( \ud835\udc17 ~ \u200b \ud835\udc16 Q ) \u2208 \u211d T \u00d7 D k \u225c absent superscript \ud835\udf0f 0.5 LN ~ \ud835\udc17 subscript \ud835\udc16 \ud835\udc44 superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc58 \\displaystyle\\triangleq\\tau^{-0.5}\\text{LN}(\\tilde{\\mathbf{X}}\\mathbf{W}_{Q})\\in\\mathbb{R}^{T\\times D_{k}} (8) \ud835\udc0a \ud835\udc0a \\displaystyle\\mathbf{K} \u225c \u03c4 \u2212 0.5 \u200b LN \u200b ( \ud835\udc17 ~ \u200b \ud835\udc16 K ) \u2208 \u211d T \u00d7 D k \u225c absent superscript \ud835\udf0f 0.5 LN ~ \ud835\udc17 subscript \ud835\udc16 \ud835\udc3e superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc58 \\displaystyle\\triangleq\\tau^{-0.5}\\text{LN}(\\tilde{\\mathbf{X}}\\mathbf{W}_{K})\\in\\mathbb{R}^{T\\times D_{k}} (9) \ud835\udc15 \ud835\udc15 \\displaystyle\\mathbf{V} \u225c \u03d5 v \u200b ( \ud835\udc17 ~ \u200b \ud835\udc16 V ) \u2208 \u211d T \u00d7 D v \u225c absent subscript italic-\u03d5 \ud835\udc63 ~ \ud835\udc17 subscript \ud835\udc16 \ud835\udc49 superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc63 \\displaystyle\\triangleq\\phi_{v}(\\tilde{\\mathbf{X}}\\mathbf{W}_{V})\\in\\mathbb{R}^{T\\times D_{v}} (10) \ud835\udc06 \ud835\udc06 \\displaystyle\\mathbf{G} \u225c \u03d5 g \u200b ( \ud835\udc17 ~ \u200b \ud835\udc16 G ) \u2208 \u211d T \u00d7 D v \u225c absent subscript italic-\u03d5 \ud835\udc54 ~ \ud835\udc17 subscript \ud835\udc16 \ud835\udc3a superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc63 \\displaystyle\\triangleq\\phi_{g}(\\tilde{\\mathbf{X}}\\mathbf{W}_{G})\\in\\mathbb{R}^{T\\times D_{v}} (11) \ud835\udc0a ^ ^ \ud835\udc0a \\displaystyle\\hat{\\mathbf{K}} \u225c STVQ \u200b ( \ud835\udc0a ; \ud835\udc02 ) \u2208 \u211d T \u00d7 D k \u225c absent STVQ \ud835\udc0a \ud835\udc02 superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc58 \\displaystyle\\triangleq\\text{STVQ}(\\mathbf{K};\\mathbf{C})\\in\\mathbb{R}^{T\\times D_{k}} (12) \ud835\udc16 \ud835\udc16 \\displaystyle\\mathbf{W} \u225c \u03d5 w \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 + \ud835\udc01 ) \u2208 \u211d T \u00d7 T \u225c absent subscript italic-\u03d5 \ud835\udc64 \ud835\udc10 superscript ^ \ud835\udc0a top \ud835\udc01 superscript \u211d \ud835\udc47 \ud835\udc47 \\displaystyle\\triangleq\\phi_{w}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}+\\mathbf{B})\\in\\mathbb{R}^{T\\times T} (13) \ud835\udc0e \ud835\udc0e \\displaystyle\\mathbf{O} \u225c ( \ud835\udc16\ud835\udc15 ) \u2299 \ud835\udc06 \u2208 \u211d T \u00d7 D v \u225c absent direct-product \ud835\udc16\ud835\udc15 \ud835\udc06 superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc63 \\displaystyle\\triangleq(\\mathbf{W}\\mathbf{V})\\odot\\mathbf{G}\\in\\mathbb{R}^{T\\times D_{v}} (14) \ud835\udc18 \ud835\udc18 \\displaystyle\\mathbf{Y} \u225c \ud835\udc17 + \ud835\udc0e\ud835\udc16 O \u2208 \u211d T \u00d7 D m \u225c absent \ud835\udc17 subscript \ud835\udc0e\ud835\udc16 \ud835\udc42 superscript \u211d \ud835\udc47 subscript \ud835\udc37 \ud835\udc5a \\displaystyle\\triangleq\\mathbf{X}+\\mathbf{O}\\mathbf{W}_{O}\\in\\mathbb{R}^{T\\times D_{m}} (15)\n\nwhere is a fixed constant, are element-wise or row-wise nonlinearities, the query/key LayerNorms use unit gain and zero bias, and denotes row-wise application of vector-quantization with a straight-through gradient estimator (van den Oord et al., 2017).",
    "vqtransformer-1": "Remark 3.2. Our attention mechanism is applied to a gated activation unit (GAU) design inspired by Hua et al. (2022). GAU is a single-headed gated attention mechanism and generally uses , , with two GAUs replacing a single transformer layer. This yields a similar parameter count and compute requirement as the transformer layer, assuming the latter uses , , and . Remark 3.3. Prior work has also applied LayerNorm or similar to the queries and keys in attention (Henry et al., 2020; Roy et al., 2021; Zhu et al., 2021; Wu et al., 2022; Hutchins et al., 2022), generally finding it to improve numerical stability and convergence. 3.2 Warmup: Linear-Time Encoder Attention\n\nTheorem 3.4. Suppose for all , and is an element-wise nonlinearity. Then the attention weights in Definition 3.1 can be factored:\n\n\ud835\udc16 \ud835\udc16 \\displaystyle\\mathbf{W} \u225c \u03d5 w \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 + \ud835\udc01 ) \u225c absent subscript italic-\u03d5 \ud835\udc64 \ud835\udc10 superscript ^ \ud835\udc0a top \ud835\udc01 \\displaystyle\\triangleq\\phi_{w}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}+\\mathbf{B}) (16) = \u03d5 w \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 ) absent subscript italic-\u03d5 \ud835\udc64 \ud835\udc10 superscript ^ \ud835\udc0a top \\displaystyle=\\phi_{w}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}) (17) = \u03d5 w \u200b ( \ud835\udc10\ud835\udc02 \u22a4 ) \u200b \ud835\udeab absent subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10\ud835\udc02 top \ud835\udeab \\displaystyle=\\phi_{w}(\\mathbf{Q}\\mathbf{C}^{\\top})\\mathbf{\\Delta} (18)\n\nwhere , and . Here, denotes the Kronecker delta function and is the VQ shortcode for timestep .",
    "vqtransformer-2": "Theorem 3.5. Suppose for all , and is the row-wise softmax nonlinearity. Then the attention weights in Definition 3.1 can be factored:\n\n\ud835\udc16 \ud835\udc16 \\displaystyle\\mathbf{W} \u225c \u03d5 w \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 + \ud835\udc01 ) \u225c absent subscript italic-\u03d5 \ud835\udc64 \ud835\udc10 superscript ^ \ud835\udc0a top \ud835\udc01 \\displaystyle\\triangleq\\phi_{w}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}+\\mathbf{B}) (19) = \u03d5 w \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 ) absent subscript italic-\u03d5 \ud835\udc64 \ud835\udc10 superscript ^ \ud835\udc0a top \\displaystyle=\\phi_{w}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}) (20) = Diag ( exp ( \ud835\udc10\ud835\udc02 \u22a4 ) \ud835\udeab \ud835\udfcf ) \u2212 1 exp ( \ud835\udc10\ud835\udc02 \u22a4 ) \ud835\udeab \\displaystyle=\\operatorname{Diag}(\\exp(\\mathbf{Q}\\mathbf{C}^{\\top})\\mathbf{\\Delta}\\mathbf{1})^{-1}\\exp(\\mathbf{Q}\\mathbf{C}^{\\top})\\mathbf{\\Delta} (21)\n\nwhere , , and . Here, denotes the Kronecker delta function and is the VQ shortcode for timestep . 3.3 Linear-Time Decoder Attention\n\nTheorem 3.6. Let be a divisor of . Suppose for , and for . Let with , same as before. Let be an element-wise nonlinearity with . For a tensor , let denote the slice . For a specific tensor, if an axis is not sliced over, each ellipsis will be replaced by the appropriate number of \u2018\u2019. Then the product in Definition 3.1 can be computed using the recursion:\n\n\ud835\udc14 \u200b ( n ) \ud835\udc14 \ud835\udc5b \\displaystyle\\mathbf{U}(n) \u225c { \ud835\udc14 \u200b ( n \u2212 1 ) + \ud835\udeab ( : , n ) \u200b \ud835\udc15 ( n , : ) if \u200b n \u2265 0 \ud835\udfce otherwise \u225c absent cases \ud835\udc14 \ud835\udc5b 1 superscript \ud835\udeab : \ud835\udc5b superscript \ud835\udc15 \ud835\udc5b : if \ud835\udc5b 0 0 otherwise \\displaystyle\\triangleq\\begin{cases}\\mathbf{U}(n-1)+\\mathbf{\\Delta}^{(:,n)}\\mathbf{V}^{(n,:)}&\\text{ if }n\\geq 0\\\\\n\\bm{0}&\\text{ otherwise }\\\\\n\\end{cases} (22) [ \ud835\udc16\ud835\udc15 ] ( n , : ) superscript delimited-[] \ud835\udc16\ud835\udc15 \ud835\udc5b : \\displaystyle[\\mathbf{W}\\mathbf{V}]^{(n,:)} = \u03d5 w \u200b ( \ud835\udc10 ( n , : ) \u200b \ud835\udc02 \u22a4 ) \u200b \ud835\udc14 \u200b ( n \u2212 2 ) absent subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10 \ud835\udc5b : superscript \ud835\udc02 top \ud835\udc14 \ud835\udc5b 2 \\displaystyle=\\phi_{w}(\\mathbf{Q}^{(n,:)}\\mathbf{C}^{\\top})\\mathbf{U}(n-2) (23) + \u03d5 w \u200b ( \ud835\udc10 ( n , : ) \u200b [ \ud835\udc0a ^ ( n \u2212 1 , : ) ] \u22a4 + \ud835\udc01 ( n , n \u2212 1 ) ) \u200b \ud835\udc15 ( n \u2212 1 , : ) subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10 \ud835\udc5b : superscript delimited-[] superscript ^ \ud835\udc0a \ud835\udc5b 1 : top superscript \ud835\udc01 \ud835\udc5b \ud835\udc5b 1 superscript \ud835\udc15 \ud835\udc5b 1 : \\displaystyle\\quad\\quad+\\phi_{w}(\\mathbf{Q}^{(n,:)}[\\hat{\\mathbf{K}}^{(n-1,:)}]^{\\top}+\\mathbf{B}^{(n,n-1)})\\mathbf{V}^{(n-1,:)} (24) + \u03d5 w \u200b ( \ud835\udc10 ( n , : ) \u200b [ \ud835\udc0a ^ ( n , : ) ] \u22a4 + \ud835\udc01 ( n , n ) ) \u200b \ud835\udc15 ( n , : ) subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10 \ud835\udc5b : superscript delimited-[] superscript ^ \ud835\udc0a \ud835\udc5b : top superscript \ud835\udc01 \ud835\udc5b \ud835\udc5b superscript \ud835\udc15 \ud835\udc5b : \\displaystyle\\quad\\quad+\\phi_{w}(\\mathbf{Q}^{(n,:)}[\\hat{\\mathbf{K}}^{(n,:)}]^{\\top}+\\mathbf{B}^{(n,n)})\\mathbf{V}^{(n,:)} (25)\n\nwhere any tensor slice is defined as a zero tensor of width in the sliced dimension(s) if any block slice index is less than zero.",
    "vqtransformer-3": "Theorem 3.7. Let be a divisor of . Suppose for , and for . Let with , same as before. Suppose is the row-wise softmax nonlinearity. Let the block tensor slice notation from Theorem 3.6 apply.",
    "vqtransformer-4": "Let . Let . Then the product in Definition 3.1 can be computed using the recursions:\n\n\ud835\udc14 \u200b ( n ) \ud835\udc14 \ud835\udc5b \\displaystyle\\mathbf{U}(n) \u225c { \ud835\udc14 \u200b ( n \u2212 1 ) + \ud835\udeab ( : , n ) \u200b \ud835\udc15 ( n , : ) if \u200b n \u2265 0 \ud835\udfce otherwise \u225c absent cases \ud835\udc14 \ud835\udc5b 1 superscript \ud835\udeab : \ud835\udc5b superscript \ud835\udc15 \ud835\udc5b : if \ud835\udc5b 0 0 otherwise \\displaystyle\\triangleq\\begin{cases}\\mathbf{U}(n-1)+\\mathbf{\\Delta}^{(:,n)}\\mathbf{V}^{(n,:)}&\\text{ if }n\\geq 0\\\\\n\\bm{0}&\\text{ otherwise }\\\\\n\\end{cases} (26) \ud835\udc0b \u200b ( n ) \ud835\udc0b \ud835\udc5b \\displaystyle\\mathbf{L}(n) \u225c { \ud835\udc0b \u200b ( n \u2212 1 ) + \ud835\udeab ( : , n ) \u200b \ud835\udfcf ( n ) if \u200b n \u2265 0 \ud835\udfce otherwise \u225c absent cases \ud835\udc0b \ud835\udc5b 1 superscript \ud835\udeab : \ud835\udc5b superscript 1 \ud835\udc5b if \ud835\udc5b 0 0 otherwise \\displaystyle\\triangleq\\begin{cases}\\mathbf{L}(n-1)+\\mathbf{\\Delta}^{(:,n)}\\mathbf{1}^{(n)}&\\text{ if }n\\geq 0\\\\\n\\bm{0}&\\text{ otherwise }\\\\\n\\end{cases} (27) [ \ud835\udc00\ud835\udc15 ] ( n , : ) superscript delimited-[] \ud835\udc00\ud835\udc15 \ud835\udc5b : \\displaystyle[\\mathbf{A}\\mathbf{V}]^{(n,:)} = exp \u2061 ( \ud835\udc10 ( n , : ) \u200b \ud835\udc02 \u22a4 ) \u200b \ud835\udc14 \u200b ( n \u2212 2 ) absent superscript \ud835\udc10 \ud835\udc5b : superscript \ud835\udc02 top \ud835\udc14 \ud835\udc5b 2 \\displaystyle=\\exp(\\mathbf{Q}^{(n,:)}\\mathbf{C}^{\\top})\\mathbf{U}(n-2) (28) + exp \u2061 ( \ud835\udc10 ( n , : ) \u200b [ \ud835\udc0a ^ ( n \u2212 1 , : ) ] \u22a4 + \ud835\udc01 ( n , n \u2212 1 ) ) \u200b \ud835\udc15 ( n \u2212 1 , : ) superscript \ud835\udc10 \ud835\udc5b : superscript delimited-[] superscript ^ \ud835\udc0a \ud835\udc5b 1 : top superscript \ud835\udc01 \ud835\udc5b \ud835\udc5b 1 superscript \ud835\udc15 \ud835\udc5b 1 : \\displaystyle\\quad\\quad+\\exp(\\mathbf{Q}^{(n,:)}[\\hat{\\mathbf{K}}^{(n-1,:)}]^{\\top}+\\mathbf{B}^{(n,n-1)})\\mathbf{V}^{(n-1,:)} (29) + exp \u2061 ( \ud835\udc10 ( n , : ) \u200b [ \ud835\udc0a ^ ( n , : ) ] \u22a4 + \ud835\udc01 ( n , n ) ) \u200b \ud835\udc15 ( n , : ) superscript \ud835\udc10 \ud835\udc5b : superscript delimited-[] superscript ^ \ud835\udc0a \ud835\udc5b : top superscript \ud835\udc01 \ud835\udc5b \ud835\udc5b superscript \ud835\udc15 \ud835\udc5b : \\displaystyle\\quad\\quad+\\exp(\\mathbf{Q}^{(n,:)}[\\hat{\\mathbf{K}}^{(n,:)}]^{\\top}+\\mathbf{B}^{(n,n)})\\mathbf{V}^{(n,:)} (30) [ \ud835\udc00\ud835\udfcf ] ( n ) superscript delimited-[] \ud835\udc00\ud835\udfcf \ud835\udc5b \\displaystyle[\\mathbf{A}\\mathbf{1}]^{(n)} = exp \u2061 ( \ud835\udc10 ( n , : ) \u200b \ud835\udc02 \u22a4 ) \u200b \ud835\udc0b \u200b ( n \u2212 2 ) absent superscript \ud835\udc10 \ud835\udc5b : superscript \ud835\udc02 top \ud835\udc0b \ud835\udc5b 2 \\displaystyle=\\exp(\\mathbf{Q}^{(n,:)}\\mathbf{C}^{\\top})\\mathbf{L}(n-2) (31) + exp \u2061 ( \ud835\udc10 ( n , : ) \u200b [ \ud835\udc0a ^ ( n \u2212 1 , : ) ] \u22a4 + \ud835\udc01 ( n , n \u2212 1 ) ) \u200b \ud835\udfcf ( n \u2212 1 ) superscript \ud835\udc10 \ud835\udc5b : superscript delimited-[] superscript ^ \ud835\udc0a \ud835\udc5b 1 : top superscript \ud835\udc01 \ud835\udc5b \ud835\udc5b 1 superscript 1 \ud835\udc5b 1 \\displaystyle\\quad\\quad+\\exp(\\mathbf{Q}^{(n,:)}[\\hat{\\mathbf{K}}^{(n-1,:)}]^{\\top}+\\mathbf{B}^{(n,n-1)})\\mathbf{1}^{(n-1)} (32) + exp \u2061 ( \ud835\udc10 ( n , : ) \u200b [ \ud835\udc0a ^ ( n , : ) ] \u22a4 + \ud835\udc01 ( n , n ) ) \u200b \ud835\udfcf ( n ) superscript \ud835\udc10 \ud835\udc5b : superscript delimited-[] superscript ^ \ud835\udc0a \ud835\udc5b : top superscript \ud835\udc01 \ud835\udc5b \ud835\udc5b superscript 1 \ud835\udc5b \\displaystyle\\quad\\quad+\\exp(\\mathbf{Q}^{(n,:)}[\\hat{\\mathbf{K}}^{(n,:)}]^{\\top}+\\mathbf{B}^{(n,n)})\\mathbf{1}^{(n)} (33) [ \ud835\udc16\ud835\udc15 ] ( n , : ) superscript delimited-[] \ud835\udc16\ud835\udc15 \ud835\udc5b : \\displaystyle[\\mathbf{W}\\mathbf{V}]^{(n,:)} = Diag ( [ \ud835\udc00\ud835\udfcf ] ( n ) ) \u2212 1 [ \ud835\udc00\ud835\udc15 ] ( n , : ) .",
    "vqtransformer-5": "\\displaystyle=\\operatorname{Diag}([\\mathbf{A}\\mathbf{1}]^{(n)})^{-1}[\\mathbf{A}\\mathbf{V}]^{(n,:)}. (34)\n\nRemark 3.8. Intuitively, Theorem 3.7 shows that VQ-Attention is computable by processing the sequence in blocks of length , applying two steps to each block. The first step is to form the corresponding block of and use it to sum the value vectors and shortcode indicators into the appropriate rows of the \u2018cache\u2019 variables .",
    "vqtransformer-6": "The second step is to incorporate directly into the retrieval process with the help of the codebook . Remark 3.9. Theorem 3.7 provides an algorithm to compute VQ-Attention from the queries, keys, values, gates, and codebook in time per query block, and therefore time per sequence.",
    "vqtransformer-7": "Remark 3.10. In the experiments, we use as row-wise softmax, and use the relative positional biases from Dai et al. (2019) for the band of nonzero biases in . In the experiments, we rely on a numerically stable reformulation of Theorem 3.7, where the logarithm of the counts are moved inside the exponentials appearing in and . 3.4 Learning Algorithm\n\n3.4.1 Training Loss\n\nLet denote the set of non-codebook parameters of a transformer with VQ-Attention layers, and let denote the set of the layers\u2019 codebooks. For autoregressive modeling of a sequence , we define the Transformer-VQ training loss as\n\n\u2112 \u200b ( \ud835\udc17 ; \ud835\udf3d , \ud835\udc9e ) \u2112 \ud835\udc17 \ud835\udf3d \ud835\udc9e \\displaystyle\\mathcal{L}(\\mathbf{X};\\bm{\\theta},\\mathcal{C}) = \u2112 CE \u200b ( \ud835\udc17 ; \ud835\udf3d , \ud835\udc9e ) + \u03b2 \u200b \u2112 VQ \u200b ( \ud835\udc17 ; \ud835\udf3d , \ud835\udc9e ) absent subscript \u2112 CE \ud835\udc17 \ud835\udf3d \ud835\udc9e \ud835\udefd subscript \u2112 VQ \ud835\udc17 \ud835\udf3d \ud835\udc9e \\displaystyle=\\mathcal{L}_{\\text{CE}}(\\mathbf{X};\\bm{\\theta},\\mathcal{C})+\\beta\\mathcal{L}_{\\text{VQ}}(\\mathbf{X};\\bm{\\theta},\\mathcal{C}) (35)\n\nwhere is a hyperparameter known as the commit loss coefficient, and\n\n\u2112 CE \u200b ( \ud835\udc17 ; \ud835\udf3d , \ud835\udc9e ) subscript \u2112 CE \ud835\udc17 \ud835\udf3d \ud835\udc9e \\displaystyle\\mathcal{L}_{\\text{CE}}(\\mathbf{X};\\bm{\\theta},\\mathcal{C}) \u225c 1 T \u200b \u2211 t = 0 T \u2212 1 \u2212 ln \u2061 p \u200b ( \ud835\udc31 t + 1 | \ud835\udc31 \u2264 t , \ud835\udf3d , \ud835\udc9e ) \u225c absent 1 \ud835\udc47 superscript subscript \ud835\udc61 0 \ud835\udc47 1 \ud835\udc5d conditional subscript \ud835\udc31 \ud835\udc61 1 subscript \ud835\udc31 absent \ud835\udc61 \ud835\udf3d \ud835\udc9e \\displaystyle\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}-\\ln p(\\mathbf{x}_{t+1}|\\mathbf{x}_{\\leq t},\\bm{\\theta},\\mathcal{C}) (36) \u2112 VQ \u200b ( \ud835\udc17 ; \ud835\udf3d , \ud835\udc9e ) subscript \u2112 VQ \ud835\udc17 \ud835\udf3d \ud835\udc9e \\displaystyle\\mathcal{L}_{\\text{VQ}}(\\mathbf{X};\\bm{\\theta},\\mathcal{C}) \u225c 1 T \u200b \u2211 t = 0 T \u2212 1 \u2211 \u2113 = 0 N \u2212 1 \u2016 \ud835\udc0a t ( \u2113 ) \u2212 SG \u200b ( \ud835\udc02 z t ( \u2113 ) ) \u2016 2 2 . \u225c absent 1 \ud835\udc47 superscript subscript \ud835\udc61 0 \ud835\udc47 1 superscript subscript \u2113 0 \ud835\udc41 1 superscript subscript norm superscript subscript \ud835\udc0a \ud835\udc61 \u2113 SG superscript subscript \ud835\udc02 subscript \ud835\udc67 \ud835\udc61 \u2113 2 2 \\displaystyle\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\sum_{\\ell=0}^{N-1}||\\mathbf{K}_{t}^{(\\ell)}-\\text{SG}(\\mathbf{C}_{z_{t}}^{(\\ell)})||_{2}^{2}. (37)\n\nThus, the training loss is the average next-token cross-entropy loss, plus the average token\u2019s commitment losses (van den Oord et al., 2017), summed over layer codebooks. Non-codebook parameters receive a gradient from both loss terms. Following van den Oord et al. (2017), codebooks are parameterized via smoothed quantizer statistics. 3.4.2 Training Updates\n\nInstead of updating on the full sequence loss given above, we generally update every query blocks, where , which resembles a strategy used in prior works (Dai et al., 2019; Wu et al., 2022; Hutchins et al., 2022). Each update is obtained by backpropagating through a window of timesteps, with gradients computed from the corresponding terms in the per-token average losses above. Codebooks are also updated every query blocks. When , using Theorem 3.7 is an efficient alternative to using a non-differentiable long-range key-value cache. When , a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. 4 Related Work\n\n4.1 Hierarchical Attention\n\nCombiner (Ren et al., 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu & Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al., 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al., 2021) and MegaByte (Yu et al., 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every tokens, there are no sporadic \u2018feature consolidation\u2019 operations required during sampling. 4.2 Kernelizable Attention\n\nKernelizable attention (Katharopoulos et al., 2020; Choromanski et al., 2021) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al., 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd\u2019s iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. 4.3 Compressive Attention\n\nCompressive Transformers (Rae et al., 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022; Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.",
    "vqtransformer-8": "(2021); Hutchins et al. (2022). More generally, Transformer-VQ differs from compressive/recurrent transformers in that it has an equivalence to quadratic-time attention over vector-quantized keys. In other words, if the keys are already vector-quantized, the Transformer-VQ cache losslessly reduces the cost to linear time. Perceivers (Jaegle et al., 2021; Hawthorne et al., 2022) use cross-attention to attend to long sequences, and compute self-attention over only a narrow stack of \u2018latents\u2019. Transformer-VQ differs from Perceivers in that it computes dense self-attention in linear time, instead of just cross-attention. Thus, while Perceivers\u2019 long-range layers incur a quadratic time complexity during sampling, Transformer-VQ generates sequences in linear time. 4.4 Gated Sequence Models\n\nGated attention was introduced in FLASH (Hua et al., 2022) as a fusion of attention sublayers (Vaswani et al., 2017) and GLU-based MLP sublayers (Shazeer, 2020). Various gating mechanisms have previously been used to stabilize training of transformers (Parisotto et al., 2019) and other sequence models including S4 (Gu et al., 2022), GSS (Mehta et al., 2022), MEGA (Ma et al., 2023) and RWKV (Peng et al., 2023). Transformer-VQ uses the original gating formulation from Hua et al. (2022), and develops a new attention mechanism. 4.5 VQ, K-Means, and Beyond\n\nIdeas relating to -means, vector quantization, and/or codebooks have also been applied in transformers for sparse attention (Roy et al., 2021; Wang et al., 2021; 2022), feature learning (Mao et al., 2022; Roy et al., 2022), sparsely-activated MLPs (Lample et al., 2019), and expert selection (Roller et al., 2021). These works generally feature codebooks or similar within a transformer architecture. Several works also have proposed models that feature a codebook somewhere outside a transformer, e.g., when transformers are priors for VQ-VAEs (Kaiser et al., 2018; Dhariwal et al., 2020; Ramesh et al., 2021; Lee et al., 2022; Zhou et al., 2022). Transformer-VQ uses one codebook within each layer and, in contrast to all of the aforementioned works, computes dense self-attention in linear time. Transformer-VQ is not directly related to methods which quantize the weights of a transformer e.g., Dettmers et al.",
    "vqtransformer-9": "(2022); Dettmers & Zettlemoyer (2023); Frantar et al. (2023). Such methods are typically applied after training, and do not reduce the complexity of self-attention. However, we expect these approaches may prove complementary during inference. 5 Experiments\n\nFor all experiments, we use a TPU v3-128 accelerator (Jouppi et al., 2017). Hyperparameters follow Appendix B unless specifically noted. For efficient training on TPUs, Transformer-VQ was implemented using Jax (Bradbury et al., 2018) and Flax (Heek et al., 2023). 5.1 Ablation Studies\n\n5.1.1 Codebook Size\n\nLarger codebook sizes may allow more flexible attention patterns and could improve the fidelity of the gradients, both of which are likely to benefit model quality at the expense of additional wall time. To investigate, we ablate the codebook size using the Enwik8 dataset (see 5.2.1), and report the lowest validation bits-per-byte (BPB, lower is better) obtained by each model in Table 2. Table 2 confirms the intuition that larger codebooks improve the prediction quality (lower BPB) in return for additional wall time per training step. In particular, increasing the codebook size by a factor of two appears to decrease the validation BPB by about and increase the wall time per step by a factor of about . A formal characterization of the scaling laws (Kaplan et al., 2020) for codebook size could be an interesting direction for future work. 5.1.2 Long-Range Cache\n\nSince our model has several architectural differences from most prior works, the benefit of the long-range cache must be shown directly. To investigate, we train a model with the long-range cache omitted, using codebook size . We report the validation BPB for Enwik8 in Table 2. As shown in Table 2, removing the long-range cache reduces the wall time per step by a factor of about , but leads to a significant drop in quality (higher bits-per-byte). This confirms the importance of our long-range cache mechanism. 5.2 Quantitative Results\n\nTo assess the ability of Transformer-VQ to learn long-range dependencies, we now conduct a series of large-scale experiments, benchmarking on several long-range autoregressive modeling tasks.",
    "vqtransformer-10": "For fair comparison, we only benchmark against models (a) trained without using any extra data or augmentation, and (b) evaluated with fixed parameters. In all cases, we use codebook size . 5.2.1 Enwik8\n\nEnwik8 is a byte-level language modeling dataset consisting of 100 million bytes of unprocessed English-language Wikipedia articles (Mahoney, 2011), with long-term dependencies that may span tens of thousands of bytes. Per convention, it is split into train, validation, and test sets of 90 million, 5 million, and 5 million bytes, respectively (Child et al., 2019; Rae et al., 2020). For this dataset, we trained a Transformer-VQ with 190M parameters, smaller than the model by Dai et al. (2019). We report test bits-per-byte (BPB) in Table 3. Transformer-VQ obtains a BPB of 0.99, notably matching the result of Transformer-XL (Dai et al., 2019), while using an entirely different cache mechanism not based on position and also shorter in length at test time. For this dataset, we found overfitting was a significant issue, and due to the compressive cache mechanism, using attention dropout was not possible. Sweeping over the residual dropout rate, weight decay coefficient, and layerdrop (Fan et al., 2020a) rate, we found a setting yielding good generalization. Nonetheless Transformer-VQ does fall short of state-of-the-art here, with several works using complex recurrence or forgetting mechanisms and obtaining better Enwik8 results. 5.2.2 PG-19\n\nPG-19 is an open-vocabulary language modeling dataset consisting of 11 gigabytes of text from over 28,000 freely-available Project Gutenberg books published prior to 1919 (Rae et al., 2020). The average number of words per book is nearly 70,000, enabling learning long-term dependencies, especially in novels (Sun et al., 2021; Hutchins et al., 2022). For this dataset, we trained a Transformer-VQ with 1.3B parameters, similar to the largest model by Hutchins et al. (2022). Since PG-19 is an open-vocabulary dataset, we first learned a SentencePiece vocabulary (Kudo & Richardson, 2018) of size 32,000 using the BPE method. Following the calculations of Rae et al. (2020), we report the test set word-level perplexity (WLP) in Table 5. Transformer-VQ obtains a WLP of 26.6, very close to the state-of-the-art by Hutchins et al. (2022). Interestingly, since our Transformer-VQ design is equivalent to using dense self-attention with vector-quantized keys, our strong result shows that models using long-range attention only (no recurrence) can also be highly competitive on PG-19, which reaffirms the efficacy of standalone self-attention as a method for sequence processing at scale. 5.2.3 ImageNet64\n\nImageNet64 is an image dataset consisting of over 1.2 million images downsampled to 64x64 resolution (Chrabaszcz et al., 2017; Deng et al., 2009). Flattening the images yields an autoregressive density estimation task on sequences of over 12,000 bytes each. Note since the official test set is not public for this dataset, we report results on the official validation set. For validation purposes we used a held-out set of about 80,000 examples from the training split. For this dataset, we trained a Transformer-VQ with 1.2B parameters, similar to the PG-19 model. We report the bits-per-byte on the official validation set in Table 5. Several of the earlier baselines used an earlier variant of downsampled ImageNet prepared by van den Oord et al. (2016) with a different downsampling algorithm. Since that variant has been unavailable through official channels for about a year, we used the newer variant following Lipman et al. (2023). We emphasize that our results using the newer variant cannot be directly compared with baselines using the earlier variant; however, due to several reporting ambiguities, Table 5 does not symbolically distinguish variants used. Transformer-VQ obtains a BPB of 3.16, significantly improving on prior results reported by Hazami et al.",
    "vqtransformer-11": "(2022); Lipman et al. (2023). Our model has 7x more parameters than the one by Hazami et al. (2022), but thanks to the large dataset it showed no signs of overfitting. Our favorable results on this dataset show that the Transformer-VQ architecture can be directly applied to other modalities beyond natural language, which we attribute to its efficient emulation of the standard transformer\u2019s flexible attention patterns. 5.3 Qualitative Analysis\n\nWe provide extensive samples for all models in Appendix C. 5.3.1 ImageNet64\n\nWe generate batches of 128 sequences using nucleus sampling (Holtzman et al., 2020). Figures 1-2 show a subset of samples with the same indices from two batches with different nucleus settings. Many of the samples between the two batches are perceptually similar, which is a consequence of using the same random seed to directly observe the impact of the nucleus sampling hyperparameter. In Figure 1, we observe that our unconditional ImageNet64 model can synthesize sequences of over 12,000 bytes and appears to be capable of depicting relatively high-fidelity ocean water, shorelines, leaves, insects, trees, animals, people, mountains, and architecture. The model does make some mistakes, particularly involving perspective or object identity. For instance, in the second row of Figure 1, the rightmost image appears to be a bird wearing a shell, while in the first row of Figure 2, the rightmost image appears to be a wooden galleon with legs. It is unclear if these effects are due to vector quantization or lack of image-specific inductive biases. Interestingly, we have not used separate embeddings to specify the row, column, and color channel to the model, which is in contrast to some prior works (Child et al., 2019; Hawthorne et al., 2022). Finally, while some mistakes dissipate when using nucleus 0.999, some new ones do appear; one possible explanation is that using a fixed nucleus is suboptimal for images. 5.3.2 PG-19\n\nIn Figure 3, we observe that our PG-19 model can synthesize relatively high-quality text, maintaining a consistent tone, remaining on topic, and generating reasonably coherent content. These qualitative observations were found to hold for the vast majority of the samples we generated. The excerpt shown was preceded by a book title \u2018Elementary Photography\u2019, a non-existent author\u2019s name, and publisher information. Though this information was synthesized by the model, it suggests the model may be amenable to generating text on a particular topic simply by conditioning on a prompt, similar to larger language models. 6 Conclusion\n\nTransformer-VQ is a decoder-only transformer architecture that computes softmax-based dense self-attention in linear time with respect to sequence length. Its efficient attention is enabled by vector-quantized keys and a new truncation-free fixed-size cache. Our large-scale experiments show Transformer-VQ is an efficient and flexible autoregressive model with successful applications to byte-level language modeling, open-vocabulary language modeling, and image synthesis. Future work directions include formal scaling laws, scaling to even larger models, and applying Transformer-VQ to long-context program synthesis and reinforcement learning tasks. Reproducibility Statement\n\nTo facilitate reproducibility, our attention mechanism is described mathematically in Section 3, our hyperparameters and other implementation details are given in Appendix B, and our implementation is open-sourced at the link in the abstract. Acknowledgments\n\nWe thank the anonymous reviewers for helpful feedback on this work, and acknowledge the Python community, especially the Jax ecosystem contributors, for effective libraries used in this project. This work was generously supported by Cloud TPUs from Google\u2019s TPU Research Cloud (TRC). References\n\nAinslie et al. (2020) Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268\u2013284, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.19. URL https://aclanthology.org/2020.emnlp-main.19. Ba et al. (2016) Jimmy Lei Ba, Jamie Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL https://arxiv.org/abs/1607.06450. Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.",
    "vqtransformer-12": "Bulatov et al. (2022) Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=Uynr3iPhksa. Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski et al. (2021) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH. Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.",
    "vqtransformer-13": "URL https://arxiv.org/abs/2204.02311. Chrabaszcz et al. (2017) Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the CIFAR datasets.",
    "vqtransformer-14": "CoRR, abs/1707.08819, 2017. URL http://arxiv.org/abs/1707.08819. Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978\u20132988, Florence, Italy, jul 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285. Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848. Dettmers & Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws, 2023.",
    "vqtransformer-15": "URL http://arxiv.org/abs/2212.09720. Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale.",
    "vqtransformer-16": "In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD. Dhariwal et al. (2020) Prafulla Dhariwal, Heewoo Jun, Christine McLeavey Paine, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music, 2020.",
    "vqtransformer-17": "URL https://arxiv.org/abs/2005.00341. Elfwing et al. (2017) Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning.",
    "vqtransformer-18": "CoRR, abs/1702.03118, 2017. URL http://arxiv.org/abs/1702.03118. Fan et al. (2020a) Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations, 2020a. URL https://openreview.net/forum?id=SylO2yStDr. Fan et al. (2020b) Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory, 2020b.",
    "vqtransformer-19": "URL https://arxiv.org/abs/2002.09402. Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS. Grcic et al. (2021) Matej Grcic, Ivan Grubisic, and Sinisa Segvic. Densely connected normalizing flows.",
    "vqtransformer-20": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 23968\u201323982. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/c950cde9b3f83f41721788e3315a14a3-Paper.pdf. Gu et al. (2022) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=uYLFoz1vlAC. Guo et al. (2019) Ruiqi Guo, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar, and Xiang Wu. New loss functions for fast maximum inner product search.",
    "vqtransformer-21": "CoRR, abs/1908.10396, 2019. URL http://arxiv.org/abs/1908.10396. Hawthorne et al. (2022) Curtis Hawthorne, Andrew Jaegle, C\u0103t\u0103lina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, Joao Carreira, and Jesse Engel. General-purpose, long-context autoregressive modeling with Perceiver AR. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8535\u20138558. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/hawthorne22a.html. Hazami et al. (2022) Louay Hazami, Rayhane Mama, and Ragavan Thurairatnam. Efficient-VDVAE: Less is more, 2022.",
    "vqtransformer-22": "URL http://arxiv.org/abs/2203.13751. Heek et al. (2023) Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/google/flax.",
    "vqtransformer-23": "Henry et al. (2020) Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key normalization for transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4246\u20134253, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.379. URL https://aclanthology.org/2020.findings-emnlp.379. Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygGQyrFvH. Hua et al. (2022) Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 9099\u20139117. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/hua22a.html. Hutchins et al. (2022) DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=uloenYmLCAo. Jaegle et al. (2021) Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4651\u20134664. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/jaegle21a.html. Jouppi et al. (2017) Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit.",
    "vqtransformer-24": "SIGARCH Comput. Archit. News, 45(2):1\u201312, jun 2017. ISSN 0163-5964. doi: 10.1145/3140659.3080246. URL https://doi.org/10.1145/3140659.3080246. Kaiser et al. (2018) Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam Shazeer. Fast decoding in sequence models using discrete latent variables. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2390\u20132399. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/kaiser18a.html. Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5156\u20135165. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html. Kingma et al. (2021) Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 21696\u201321707. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf. Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "vqtransformer-25": "In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB. Kudo & Richardson (2018) Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
    "vqtransformer-26": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66\u201371, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012. Lample et al. (2019) Guillaume Lample, Alexandre Sablayrolles, Marc' Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys.",
    "vqtransformer-27": "In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/9d8df73a3cfbf3c5b47bc9b50f214aff-Paper.pdf. Lee et al. (2022) Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11523\u201311532, June 2022. URL https://openaccess.thecvf.com/content/CVPR2022/html/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.html.",
    "vqtransformer-28": "Lee-Thorp et al. (2022) James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4296\u20134313, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.319. URL https://aclanthology.org/2022.naacl-main.319. Lei (2021) Tao Lei. When attention meets fast recurrence: Training language models with reduced compute. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7633\u20137648, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.602. URL https://aclanthology.org/2021.emnlp-main.602. Lipman et al. (2023) Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Liu et al. (2018) Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-. Liu et al. (2023) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time, 2023.",
    "vqtransformer-29": "URL http://arxiv.org/abs/2305.17118. Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Lutati et al. (2023) Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive IIR filters), 2023.",
    "vqtransformer-30": "URL http://arxiv.org/abs/2305.14952. Ma et al. (2021) Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. LUNA: Linear unified nested attention.",
    "vqtransformer-31": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=GWRkOYr4jxQ. Ma et al. (2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention.",
    "vqtransformer-32": "In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=qNLe3iq2El. Mahoney (2011) Matt Mahoney. Large text compression benchmark, 2011. URL: http://mattmahoney.net/dc/text.html. Mao et al. (2022) Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, and Irfan Essa. Discrete representations strengthen vision transformer robustness. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=8hWs60AZcWk. Mehta et al. (2022) Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces, 2022. URL http://arxiv.org/abs/2206.13947. Nawrot et al. (2021) Piotr Nawrot, Szymon Tworkowski, Michal Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. CoRR, abs/2110.13711, 2021. URL https://arxiv.org/abs/2110.13711. Nawrot et al. (2023) Piotr Nawrot, Jan Chorowski, Adrian \u0141a\u0144cucki, and Edoardo M. Ponti. Efficient transformers with dynamic token pooling, 2023. URL http://arxiv.org/abs/2211.09761. Parisotto et al. (2019) Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, \u00c7aglar G\u00fcl\u00e7ehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning. CoRR, abs/1910.06764, 2019. URL http://arxiv.org/abs/1910.06764. Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era, 2023. URL http://arxiv.org/abs/2305.13048. Poli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models, 2023. URL http://arxiv.org/abs/2302.10866. Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 2555\u20132565, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.232. URL https://aclanthology.org/2020.findings-emnlp.232. Rabe & Staats (2021) Markus N. Rabe and Charles Staats. Self-attention does not need o(n) memory. CoRR, abs/2112.05682, 2021. URL https://arxiv.org/abs/2112.05682. Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf Last visited on 2023/09/07. Rae et al. (2020) Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH. Rae et al. (2021) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. CoRR, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446. Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. URL https://arxiv.org/abs/2102.12092. Ren et al. (2021) Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost.",
    "vqtransformer-33": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 22470\u201322482. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/bd4a6d0563e0604510989eb8f9ff71f5-Paper.pdf. Roller et al. (2021) Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. Hash layers for large sparse models.",
    "vqtransformer-34": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=lMgDDWb1ULW. Roy et al. (2021) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53\u201368, 2021. doi: 10.1162/tacl\u02d9a\u02d900353. URL https://aclanthology.org/2021.tacl-1.4. Roy et al. (2022) Aurko Roy, Rohan Anil, Guangda Lai, Benjamin Lee, Jeffrey Zhao, Shuyuan Zhang, Shibo Wang, Ye Zhang, Shen Wu, Rigel Swavely, Yu Tao, Phuong Dao, Christopher Fifty, Zhifeng Chen, and Yonghui Wu. N-Grammer: Augmenting transformers with latent n-grams, 2022. URL https://arxiv.org/abs/2207.06366. Shazeer (2020) Noam Shazeer. GLU variants improve transformer, 2020.",
    "vqtransformer-35": "URL https://arxiv.org/abs/2002.05202. Shazeer & Stern (2018) Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.",
    "vqtransformer-36": "CoRR, abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235. Sukhbaatar et al. (2019a) Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331\u2013335, Florence, Italy, July 2019a. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032. URL https://aclanthology.org/P19-1032. Sukhbaatar et al. (2019b) Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herv\u00e9 J\u00e9gou, and Armand Joulin. Augmenting self-attention with persistent memory. CoRR, abs/1907.01470, 2019b. URL http://arxiv.org/abs/1907.01470. Sukhbaatar et al. (2021) Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9902\u20139912. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/sukhbaatar21a.html. Sun et al. (2021) Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 807\u2013822, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.62. URL https://aclanthology.org/2021.emnlp-main.62. Tay et al. (2020a) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. CoRR, abs/2002.11296, 2020a. URL https://arxiv.org/abs/2002.11296. Tay et al. (2020b) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. CoRR, abs/2009.06732, 2020b. URL https://arxiv.org/abs/2009.06732. Tay et al. (2021) Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10183\u201310192. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/tay21a.html. van den Oord et al. (2017) Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. van den Oord et al. (2016) A\u00e4ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1747\u20131756, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/oord16.html. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "vqtransformer-37": "In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Vyas et al. (2020) Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. Fast transformers with clustered attention. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21665\u201321674. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/f6a8dd1c954c8506aadc764cc32b895e-Paper.pdf. Wang et al. (2022) Ningning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, and Xin Jiang. ClusterFormer: Neural clustering attention for efficient and effective transformer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2390\u20132402, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.170. URL https://aclanthology.org/2022.acl-long.170. Wang et al. (2021) Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. Cluster-former: Clustering-based sparse transformer for question answering. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3958\u20133968, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.346. URL https://aclanthology.org/2021.findings-acl.346. Wang et al. (2020) Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: self-attention with linear complexity.",
    "vqtransformer-38": "CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768. Wu et al. (2022) Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-. Xiong et al. (2021) Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14138\u201314148, May 2021. doi: 10.1609/aaai.v35i16.17664. URL https://ojs.aaai.org/index.php/AAAI/article/view/17664. Ye et al. (2019) Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. BP-Transformer: Modelling long-range context via binary partitioning.",
    "vqtransformer-39": "CoRR, abs/1911.04070, 2019. URL http://arxiv.org/abs/1911.04070. Yu et al. (2023) Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023.",
    "vqtransformer-40": "URL http://arxiv.org/abs/2305.07185. Zhang & Sennrich (2019) Biao Zhang and Rico Sennrich. Root mean square layer normalization.",
    "vqtransformer-41": "In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf. Zhang et al. (2023) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models, 2023.",
    "vqtransformer-42": "URL https://arxiv.org/abs/2306.14048. Zhou et al. (2022) Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=XdDl3bFUNn5. Zhu et al. (2021) Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision.",
    "vqtransformer-43": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 17723\u201317736. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/9425be43ba92c2b4454ca7bf602efad8-Paper.pdf. Zhu & Soricut (2021) Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801\u20133815, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021.acl-long.294. Appendix A Proofs for Theoretical Results\n\nA.1 Proof of Theorem 2.2\n\nProof.",
    "vqtransformer-44": "This proof is based on Guo et al. (2019). We have\n\n\ud835\udd3c \ud835\udc2a , \ud835\udc24 \u200b [ \ud835\udc2a \u22a4 \u200b \ud835\udc24 \u2212 \ud835\udc2a \u22a4 \u200b \u03c6 \u200b ( \ud835\udc24 ) ] 2 subscript \ud835\udd3c \ud835\udc2a \ud835\udc24 superscript delimited-[] superscript \ud835\udc2a top \ud835\udc24 superscript \ud835\udc2a top \ud835\udf11 \ud835\udc24 2 \\displaystyle\\mathbb{E}_{\\mathbf{q},\\mathbf{k}}[\\mathbf{q}^{\\top}\\mathbf{k}-\\mathbf{q}^{\\top}\\varphi(\\mathbf{k})]^{2} (38) = \ud835\udd3c \ud835\udc2a , \ud835\udc24 \u200b [ \ud835\udc2a \u22a4 \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) ] 2 absent subscript \ud835\udd3c \ud835\udc2a \ud835\udc24 superscript delimited-[] superscript \ud835\udc2a top \ud835\udc24 \ud835\udf11 \ud835\udc24 2 \\displaystyle=\\mathbb{E}_{\\mathbf{q},\\mathbf{k}}[\\mathbf{q}^{\\top}(\\mathbf{k}-\\varphi(\\mathbf{k}))]^{2} (39) = \ud835\udd3c \ud835\udc2a , \ud835\udc24 \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) \u22a4 \u200b \ud835\udc2a\ud835\udc2a \u22a4 \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) absent subscript \ud835\udd3c \ud835\udc2a \ud835\udc24 superscript \ud835\udc24 \ud835\udf11 \ud835\udc24 top superscript \ud835\udc2a\ud835\udc2a top \ud835\udc24 \ud835\udf11 \ud835\udc24 \\displaystyle=\\mathbb{E}_{\\mathbf{q},\\mathbf{k}}(\\mathbf{k}-\\varphi(\\mathbf{k}))^{\\top}\\mathbf{q}\\mathbf{q}^{\\top}(\\mathbf{k}-\\varphi(\\mathbf{k})) (40) = \ud835\udd3c \ud835\udc24 \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) \u22a4 \u200b \ud835\udd3c \ud835\udc2a \u200b [ \ud835\udc2a\ud835\udc2a \u22a4 ] \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) absent subscript \ud835\udd3c \ud835\udc24 superscript \ud835\udc24 \ud835\udf11 \ud835\udc24 top subscript \ud835\udd3c \ud835\udc2a delimited-[] superscript \ud835\udc2a\ud835\udc2a top \ud835\udc24 \ud835\udf11 \ud835\udc24 \\displaystyle=\\mathbb{E}_{\\mathbf{k}}(\\mathbf{k}-\\varphi(\\mathbf{k}))^{\\top}\\mathbb{E}_{\\mathbf{q}}[\\mathbf{q}\\mathbf{q}^{\\top}](\\mathbf{k}-\\varphi(\\mathbf{k})) (41) \u221d \ud835\udd3c \ud835\udc24 \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) \u22a4 \u200b \ud835\udc08 d \u200b ( \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) ) proportional-to absent subscript \ud835\udd3c \ud835\udc24 superscript \ud835\udc24 \ud835\udf11 \ud835\udc24 top subscript \ud835\udc08 \ud835\udc51 \ud835\udc24 \ud835\udf11 \ud835\udc24 \\displaystyle\\propto\\mathbb{E}_{\\mathbf{k}}(\\mathbf{k}-\\varphi(\\mathbf{k}))^{\\top}\\mathbf{I}_{d}(\\mathbf{k}-\\varphi(\\mathbf{k})) (42) = \ud835\udd3c \ud835\udc24 \u200b \u2016 \ud835\udc24 \u2212 \u03c6 \u200b ( \ud835\udc24 ) \u2016 2 . absent subscript \ud835\udd3c \ud835\udc24 superscript norm \ud835\udc24 \ud835\udf11 \ud835\udc24 2 \\displaystyle=\\mathbb{E}_{\\mathbf{k}}||\\mathbf{k}-\\varphi(\\mathbf{k})||^{2}. (43)\n\nA.2 Proof of Corollary 2.3\n\nProof. Under the constraint , it can be seen that for any given , the assignment minimizes . Thus, under the constraint , the function is a minimizer of , and thus a minimizer of . \u220e\n\nA.3 Proof of Theorem 3.4\n\nProof. When is an element-wise nonlinearity, is well-defined, where is any scalar. Then,\n\n[ \u03d5 w \u200b ( \ud835\udc10\ud835\udc02 \u22a4 ) \u200b \ud835\udeab ] i , j subscript delimited-[] subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10\ud835\udc02 top \ud835\udeab \ud835\udc56 \ud835\udc57 \\displaystyle[\\phi_{w}(\\mathbf{Q}\\mathbf{C}^{\\top})\\mathbf{\\Delta}]_{i,j} = \u03d5 w \u200b ( \ud835\udc10\ud835\udc02 \u22a4 ) i , : \u200b \ud835\udeab : , j absent subscript italic-\u03d5 \ud835\udc64 subscript superscript \ud835\udc10\ud835\udc02 top \ud835\udc56 : subscript \ud835\udeab : \ud835\udc57 \\displaystyle=\\phi_{w}(\\mathbf{Q}\\mathbf{C}^{\\top})_{i,:}\\mathbf{\\Delta}_{:,j} (44) = \u2211 s = 0 S \u2212 1 \u03d5 w \u200b ( \ud835\udc10\ud835\udc02 \u22a4 ) i , s \u200b \ud835\udeab s , j absent superscript subscript \ud835\udc60 0 \ud835\udc46 1 subscript italic-\u03d5 \ud835\udc64 subscript superscript \ud835\udc10\ud835\udc02 top \ud835\udc56 \ud835\udc60 subscript \ud835\udeab \ud835\udc60 \ud835\udc57 \\displaystyle=\\sum_{s=0}^{S-1}\\phi_{w}(\\mathbf{Q}\\mathbf{C}^{\\top})_{i,s}\\mathbf{\\Delta}_{s,j} (45) = \u2211 s = 0 S \u2212 1 \u03d5 w \u200b ( \ud835\udc10 i , : \u200b \ud835\udc02 s , : \u22a4 ) \u200b \u03b4 s , z j absent superscript subscript \ud835\udc60 0 \ud835\udc46 1 subscript italic-\u03d5 \ud835\udc64 subscript \ud835\udc10 \ud835\udc56 : superscript subscript \ud835\udc02 \ud835\udc60 : top subscript \ud835\udeff \ud835\udc60 subscript \ud835\udc67 \ud835\udc57 \\displaystyle=\\sum_{s=0}^{S-1}\\phi_{w}(\\mathbf{Q}_{i,:}\\mathbf{C}_{s,:}^{\\top})\\delta_{s,z_{j}} (46) = \u03d5 w \u200b ( \ud835\udc10 i , : \u200b \ud835\udc02 z j , : \u22a4 ) absent subscript italic-\u03d5 \ud835\udc64 subscript \ud835\udc10 \ud835\udc56 : superscript subscript \ud835\udc02 subscript \ud835\udc67 \ud835\udc57 : top \\displaystyle=\\phi_{w}(\\mathbf{Q}_{i,:}\\mathbf{C}_{z_{j},:}^{\\top}) (47) = \u03d5 w \u200b ( \ud835\udc10 i , : \u200b \ud835\udc0a ^ j , : \u22a4 ) absent subscript italic-\u03d5 \ud835\udc64 subscript \ud835\udc10 \ud835\udc56 : superscript subscript ^ \ud835\udc0a \ud835\udc57 : top \\displaystyle=\\phi_{w}(\\mathbf{Q}_{i,:}\\hat{\\mathbf{K}}_{j,:}^{\\top}) (48) = [ \u03d5 w \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 ) ] i , j absent subscript delimited-[] subscript italic-\u03d5 \ud835\udc64 \ud835\udc10 superscript ^ \ud835\udc0a top \ud835\udc56 \ud835\udc57 \\displaystyle=[\\phi_{w}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top})]_{i,j} (49)\n\nA.4 Proof of Theorem 3.5\n\nProof. By Theorem 3.4, . From the definition of Softmax,\n\nSoftmax \u200b ( \ud835\udc10 \u200b \ud835\udc0a ^ \u22a4 ) Softmax \ud835\udc10 superscript ^ \ud835\udc0a top \\displaystyle\\text{Softmax}(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}) = Diag ( exp ( \ud835\udc10 \ud835\udc0a ^ \u22a4 ) \ud835\udfcf ) \u2212 1 exp ( \ud835\udc10 \ud835\udc0a ^ \u22a4 ) \\displaystyle=\\operatorname{Diag}(\\exp(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top})\\mathbf{1})^{-1}\\exp(\\mathbf{Q}\\hat{\\mathbf{K}}^{\\top}) (50) = Diag ( exp ( \ud835\udc10\ud835\udc02 \u22a4 ) \ud835\udeab \ud835\udfcf ) \u2212 1 exp ( \ud835\udc10\ud835\udc02 \u22a4 ) \ud835\udeab . \\displaystyle=\\operatorname{Diag}(\\exp(\\mathbf{Q}\\mathbf{C}^{\\top})\\mathbf{\\Delta}\\mathbf{1})^{-1}\\exp(\\mathbf{Q}\\mathbf{C}^{\\top})\\mathbf{\\Delta}. (51)\n\nA.5 Proof of Theorem 3.6\n\nProof. For the result follows by inspection. For , we have\n\n\ud835\udc14 \u200b ( n \u2212 2 ) \ud835\udc14 \ud835\udc5b 2 \\displaystyle\\mathbf{U}(n-2) = \u2211 j = 0 ( n \u2212 1 ) \u200b L \u2212 1 \ud835\udeab : , j \u200b \ud835\udc15 j , : absent superscript subscript \ud835\udc57 0 \ud835\udc5b 1 \ud835\udc3f 1 subscript \ud835\udeab : \ud835\udc57 subscript \ud835\udc15 \ud835\udc57 : \\displaystyle=\\sum_{j=0}^{(n-1)L-1}\\bm{\\Delta}_{:,j}\\mathbf{V}_{j,:} (52) = \ud835\udeab ( : , 0 : n \u2212 1 ) \u200b \ud835\udc15 ( 0 : n \u2212 1 , : ) . absent superscript \ud835\udeab : : 0 \ud835\udc5b 1 superscript \ud835\udc15 : 0 \ud835\udc5b 1 : \\displaystyle=\\bm{\\Delta}^{(:,0:n-1)}\\mathbf{V}^{(0:n-1,:)}. (53)\n\nThus,\n\n\u03d5 w \u200b ( \ud835\udc10 ( n , : ) \u200b \ud835\udc02 \u22a4 ) \u200b \ud835\udc14 \u200b ( n \u2212 2 ) subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10 \ud835\udc5b : superscript \ud835\udc02 top \ud835\udc14 \ud835\udc5b 2 \\displaystyle\\phi_{w}(\\mathbf{Q}^{(n,:)}\\mathbf{C}^{\\top})\\mathbf{U}(n-2) (54) = \u03d5 w \u200b ( \ud835\udc10 ( n , : ) \u200b \ud835\udc02 \u22a4 ) \u200b \ud835\udeab ( : , 0 : n \u2212 1 ) \u200b \ud835\udc15 ( 0 : n \u2212 1 , : ) absent subscript italic-\u03d5 \ud835\udc64 superscript \ud835\udc10 \ud835\udc5b : superscript \ud835\udc02 top superscript \ud835\udeab : : 0 \ud835\udc5b 1 superscript \ud835\udc15 : 0 \ud835\udc5b 1 : \\displaystyle=\\phi_{w}(\\mathbf{Q}^{(n,:)}\\mathbf{C}^{\\top})\\bm{\\Delta}^{(:,0:n-1)}\\mathbf{V}^{(0:n-1,:)} (55) = \ud835\udc16 ( n , 0 : n \u2212 1 ) \u200b \ud835\udc15 ( 0 : n \u2212 1 , : ) absent superscript \ud835\udc16 : \ud835\udc5b 0 \ud835\udc5b 1 superscript \ud835\udc15 : 0 \ud835\udc5b 1 : \\displaystyle=\\mathbf{W}^{(n,0:n-1)}\\mathbf{V}^{(0:n-1,:)} (56)\n\nwhere the last line follows from a similar argument as used in the proof of Theorem 3.4. Since the sum of the three terms is , the result follows. \u220e\n\nA.6 Proof of Theorem 3.7\n\nProof. Recall that we defined . The proposed expression for follows from Theorem 3.6 with . The proposed expression for follows by a substitution argument using . Normalizing by and iterating over thus yields all blocks of the product when the nonlinearity is row-wise softmax. \u220e\n\nAppendix B Training Details\n\nB.1 Hyperparameters\n\nPer-dataset hyperparameters are provided below. B.2 Implementation\n\nWeights and token embeddings were initialized following Chowdhery et al. (2022). For small datasets, the classifier layer omits LayerNorm and is independently parameterized. For larger datasets, the classifier layer uses LayerNorm and its projection is tied with the token embedding table, then scaled down by a large constant. For image datasets, absolute sinusoidal position embeddings, scaled by a trainable scalar, were added to the token embeddings (Hua et al., 2022; Vaswani et al., 2017). We used a maximum angular wavelength of for all sinusoidal embeddings. We used the pre-norm placement of LayerNorm (Radford et al., 2019), and always used the RMS LayerNorm variant (Zhang & Sennrich, 2019). For the activations, we used and , the self-gated activation (Elfwing et al., 2017; Ramachandran et al., 2017). Several models use LayerDrop for regularization (Fan et al., 2020a), and following the Transformer-XL codebase (Dai et al., 2019) models apply dropout to the flipped sinusoidal embeddings used for (local) relative positional biases. We used float32 parameters, with bfloat16 precision for most computations (Rae et al., 2021). For the AdamW optimizer (Loshchilov & Hutter, 2019), we used gradient clip , max learning rate and hyperparameters . For the Adafactor optimizer (Shazeer & Stern, 2018), we used relative stepsizes, update clip , max learning rate , and hyperparameters . We used weight decay with a constant schedule throughout training and omit decay on any one-dimensional parameter tensors (Radford et al., 2019). The codebook commit coefficient was always and codebook EMA rate was always . Learning rates were linearly warmed up for 10,000 steps, then decayed by a 10x factor using a cosine schedule. All models were trained with a global batch size of 128 sequences. Appendix C Generated Samples\n\nC.1 Extensive Samples\n\nSamples can be browsed at the double-blind anonymized URLs below. We emphasize that the samples have not been curated in any way and may include factual hallucinations and biased content. C.2 PG-19 Excerpt\n\nC.3 ImageNet64 Batch\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Feb 28 14:19:20 2024 by LaTeXML"
}