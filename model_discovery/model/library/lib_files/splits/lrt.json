{
    "lrt-0": "# LIGHTWEIGHT AND EFFICIENT END-TO-END SPEECH RECOGNITION USING LOW-RANK TRANSFORMER \n\nGenta Indra Winata ${ }^{\\star}$ Samuel Cahyawijaya ${ }^{\\star}$ Zhaojiang Lin Zihan Liu Pascale Fung\n\nCenter for Artificial Intelligence Research (CAiRE)<br>The Hong Kong University of Science and Technology<br>\\{giwinata, scahyawijaya, zlinao, zliucr\\}@connect.ust.hk\n\n\n#### Abstract\n\nHighly performing deep neural networks come at the cost of computational complexity that limits their practicality for deployment on portable devices.",
    "lrt-1": "We propose the low-rank transformer (LRT), a memory-efficient and fast neural architecture that significantly reduces the parameters and boosts the speed of training and inference for end-to-end speech recognition. Our approach reduces the number of parameters of the network by more than $50 \\%$ and speeds up the inference time by around 1.35 x compared to the baseline transformer model. The experiments show that our LRT model generalizes better and yields lower error rates on both validation and test sets compared to an uncompressed transformer model. The LRT model outperforms those from existing works on several datasets in an end-to-end setting without using an external language model or acoustic data. Index Terms- transformer, low-rank, speech recognition, end-to-end, model compression\n\n## 1. INTRODUCTION\n\nEnd-to-end automatic speech recognition (ASR) models have shown great success in replacing traditional hybrid HMMbased models by integrating acoustic, pronunciation, and language models into a single model structure. They rely only on paired acoustic and text data, without additional acoustic knowledge, such as from phone sets and dictionaries. There are two main kinds of end-to-end encoder-decoder ASR architectures. The first is RNN-based sequence-tosequence (Seq2Seq) models with attention [1, 2], which learn the alignment between sequences of audio and their corresponding text. The second [3, 4] applies a fully-attentional feed-forward architecture transformer [5], which improves on RNN-based ASR in terms of performance and training speed with a multi-head self-attention mechanism and parallel-intime computation. However, the modeling capacity of both\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_178f5a925eb34ce9139eg-1.jpg?height=679&width=602&top_left_y=799&top_left_x=1206)\n\nFig. 1. Low-Rank Transformer Architecture. approaches relies on a large number of parameters. Scaling up the model's size increases the computational overhead, which limits its practicality for deployment on portable devices without connectivity and slows down both the training and inference processes. We propose a novel factorized transformer-based model architecture, the low-rank transformer (LRT), to reduce the number of parameters in the transformer model by replacing large high-rank matrices with low-rank matrices to eliminate the computational bottleneck. It optimizes the space and time complexity of the original model when we choose the factorization rank that is relatively smaller than the original matrix dimensions. We design the LRT by taking the idea of the autoencoder that compresses the high-dimensional data input into a compressed vector representation. And, it decodes back to a high-rank matrix to learn latent space representations of the high-rank matrix. This approach is considered an in-training compression method, where we compress the parameters of the model prior to the training process. Our contributions are as follows. ![](https://cdn.mathpix.com/cropped/2024_09_12_178f5a925eb34ce9139eg-2.jpg?height=676&width=1510&top_left_y=239&top_left_x=210)\n\nFig. 2. Low-Rank Transformer Unit. Left: Low-Rank Multi-head Attention (LRMHA), Center: Low-Rank Feed-forward (LRFF), and Right: Linear Encoder-Decoder (LED). - We introduce a novel lightweight transformer architecture leveraging low-rank matrices that outperforms those from existing baselines on the AiShell-1 and HKUST test sets in an end-to-end setting. - We successfully reduce the inference time by up to $1.35 x$ speed-up in the GPU and 1.23 x speed-up in the CPU by shrinking the number of parameters by more than $50 \\%$ from the baseline. - Interestingly, based on our experiments, we show that our LRT model generalizes better and yield lower error rates on both validation and test performance compared to an uncompressed transformer model. ## 2. RELATED WORK\n\n### 2.1. Low-Rank Model Compression\n\nTraining end-to-end deep learning ASR models requires high computational resources and long training time to make convergence possible. [6] proposed a low-rank matrix factorization of the final weighting layer, which reduced the number of parameters by up to $31 \\%$ on a large-vocabulary continuous speech recognition task. [7] introduced a reinforcement learning method to compress the ASR model iteratively and learn compression ranks, but it requires more than a week to train. In another line of work, a post-training compression method on LSTM using non-negative matrix factorization was proposed by [8] to compress large pre-trained models. However, this technique does not speed-up the training process. The aforementioned approaches reduce the number of model parameters while keeping the performance loss low. In this work, we extend idea of the in-training compression method proposed in [9] by implementing low-rank units on the transformer model [5], which is suitable for effectively shrinking the whole network size and at the same time, reducing the computational cost in training and evaluation, with improvements in the error rate. ### 2.2. End-to-end Speech Recognition\n\nCurrent end-to-end automatic speech recognition models are of two main types: (a) CTC-based models [10, 11], and (b) Seq2Seq-based models, such as LAS [1]. A combination of both models is also proposed by [12]. Recent work by [3, 4] employ a different approach by utilizing the transformer block. The present study is related to these recent transformer ASR approaches, while also leveraging the effectiveness of in-training low-rank compression methods, which was not considered in the aforementioned works. ## 3. LOW-RANK TRANSFORMER ASR\n\nWe propose a compact and more generalized low-rank transformer unit by extending the idea of the in-training compression method proposed in [9]. In our transformer architecture, we replace the linear feed-forward unit [5] with a factorized linear unit called a linear encoder-decoder (LED) unit. Figure 1 shows the architecture of our proposed low-rank transformer, and Figure 2 shows the low-rank version of the multihead attention and position-wise feed-forward network, including the LED. The proposed end-to-end ASR model accepts a spectrogram as the input and produces a sequence of characters as the output similar to [16]. It consists of $M$ layers of the encoder and $N$ layers of the decoder. We employ multi-head attention to allow the model to jointly attend to\n\nTable 1. Results on AiShell-1 (left) and HKUST (right) test sets. For the end-to-end approach, we limit the evaluation to systems without any external data and perturbation to examine the effectiveness of the approach. We approximate the number of parameters based on the description in the previous studies. | Model | Params | CER |\n| :---: | :---: | :---: |\n| Hybrid approach |  |  |\n| HMM-DNN 12 | - | $8.5 \\%$ |\n| End-to-end approach |  |  |\n| Attention Model 13 | - | $23.2 \\%$ |\n| + RNNLM 13] | - | $22.0 \\%$ |\n| CTC 14 | $\\approx 11.7 \\mathrm{M}$ | $19.43 \\%$ |\n| Framewise-RNN [14] | $\\approx 17.1 \\mathrm{M}$ | $19.38 \\%$ |\n| ACS + RNNLM 13 | $\\approx 14.6 \\mathrm{M}$ | $18.7 \\%$ |\n| Transformer (large) | 25.1 M | $13.49 \\%$ |\n| Transformer (medium) | 12.7 M | $14.47 \\%$ |\n| Transformer (small) | 8.7 M | $15.66 \\%$ |\n| LRT $(r=100)$ | 12.7 M | $13.09 \\%$ |\n| LRT $(r=75)$ | 10.7 M | 13.23\\% |\n| LRT ( $r=50$ ) | 8.7 M | $13.60 \\%$ |\n\n\n| Model | Params | CER |\n| :---: | :---: | :---: |\n| Hybrid approach |  |  |\n| DNN-hybrid 12 | - | $35.9 \\%$ |\n| LSTM-hybrid (with perturb.) 12 | - | $33.5 \\%$ |\n| TDNN-hybrid, lattice-free MMI <br> (with perturb.) 12 | - | $28.2 \\%$ |\n| End-to-end approach |  |  |\n| Attention Model 12 | - | $37.8 \\%$ |\n| CTC + LM 15 | $\\approx 12.7 \\mathrm{M}$ | $34.8 \\%$ |\n| MTL + joint dec. (one-pass) 12 | $\\approx 9.6 \\mathrm{M}$ | $33.9 \\%$ |\n| + RNNLM (joint train) 12 | $\\approx 16.1 \\mathrm{M}$ | $32.1 \\%$ |\n| Transformer (large) | 22 M | $29.21 \\%$ |\n| Transformer (medium) | 11.5 M | $29.73 \\%$ |\n| Transformer (small) | 7.8 M | $31.30 \\%$ |\n| LRT $(r=100)$ | 11.5 M | $\\mathbf{2 8 . 9 5 \\%}$ |\n| LRT ( $r=75$ ) | 9.7 M | $29.08 \\%$ |\n| LRT $(r=50)$ | 7.8 M | $30.74 \\%$ |\n\ninformation from different representation subspaces in a different position. ### 3.1. Linear Encoder-Decoder\n\nWe propose linear encoder-decoder (LED) units in the transformer model instead of a single linear layer. The design is based on matrix factorization by approximating the matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ in the linear feed-forward unit using two smaller matrices, $\\mathbf{E} \\in \\mathbb{R}^{m \\times r}$ and $\\mathbf{D} \\in \\mathbb{R}^{r \\times n}$ :\n\n$$\n\\mathbf{W} \\approx \\mathbf{E} \\times \\mathbf{D}\n$$\n\nThe matrix $\\mathbf{W}$ requires $m n$ parameters and $m n$ flops, while $\\mathbf{E}$ and $\\mathbf{D}$ require $r m+r n=r(m+n)$ parameters and $r(m+n)$ flops.",
    "lrt-2": "If we take the rank to be very low $r<<m, n$, the number of parameters and flops in $\\mathbf{E}$ and $\\mathbf{D}$ are much smaller compared to $\\mathbf{W}$. ### 3.2. Low-Rank Multi-Head Attention\n\nThe LED is incorporated into the multi-head attention by factorizing the projection layers of keys $W_{i}^{Q}$, values $W_{i}^{V}$, queries $W_{i}^{Q}$, and the output layer $W^{O}$. A residual connection from a query $Q$ to the output is added. $$\n\\begin{aligned}\n& \\operatorname{Attention}(Q, K, V)=\\operatorname{Softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}} V\\right) \\\\\n& h d_{i}=\\operatorname{Attention}\\left(Q E_{i}^{Q} D_{i}^{Q}, K E_{i}^{K} D_{i}^{K}, V E_{i}^{V} D_{i}^{V}\\right) \\\\\n& f(Q, K, V)=\\operatorname{Concat}\\left(h_{1}, \\cdots, h_{H}\\right) E^{O} D^{O}+Q\n\\end{aligned}\n$$\n\nwhere $f$ is a low-rank multi-head attention (LRMHA) function, $h_{i}$ is the head of $i, H$ is the number of heads, and\nthe projections are parameter matrices $E_{i}^{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{r}}$; $D_{i}^{Q} \\in \\mathbb{R}^{d_{r} \\times d_{k}} ; E_{i}^{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{r}} ; D_{i}^{K} \\in \\mathbb{R}^{d_{r} \\times d_{k}} ;$ $E_{i}^{V} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{r}} ; D_{i}^{V} \\in \\mathbb{R}^{d_{r} \\times d_{v}} . d_{\\text {model }}, d_{r}, d_{k}$, and $d_{v}$ are dimensions of hidden size, rank, key, and value, respectively. ### 3.3. Low-Rank Feed-Forward\n\nEach encoder and decoder layer has a position-wise feedforward network that contains two low-rank LED units and applies a ReLU function in between. To alleviate the gradient vanishing issue, a residual connection is added, as shown in Figure 2\n\n$$\ng(x)=\\operatorname{LayerNorm}\\left(\\max \\left(0, x E_{1} D_{1}\\right) E_{2} D_{2}+x\\right)\n$$\n\nwhere $g$ is a low-rank feed-forward (LRFF) function.",
    "lrt-3": "### 3.4. Training Phase\n\nThe encoder module uses a VGG net [17] with a 6-layer CNN architecture. The VGG consists of convolutional layers that are added to learn a universal audio representation and generate input embedding. The input of the unit is a spectrogram. The decoder receives the encoder outputs and applies multihead attention to the decoder input. We apply a mask in the attention layer to avoid any information flow from future tokens. Then, we run a non-autoregressive step and calculate the cross-entropy loss. ### 3.5. Evaluation Phase\n\nIn the inference time, we decode the sequence using autoregressive beam-search by selecting the best sub-sequence\nscored using the softmax probability of the characters. We define $P(Y)$ as the probability of the sentence. A word count is added to avoid generating very short sentences. $P(Y)$ is calculated as follows:\n\n$$\nP(Y)=\\alpha P_{\\text {trans }}(Y \\mid X)+\\gamma \\sqrt{w c(Y)}\n$$\n\nwhere $\\alpha$ is the parameter to control the decoding probability from the decoder $P_{\\text {trans }}(Y \\mid X)$, and $\\gamma$ is the parameter to control the effect of the word count $w c(Y)$. ## 4. EXPERIMENTS\n\nExperiments were conducted on two dataset benchmarks: AiShell-1 [18], a multi-accent Mandarin speech dataset, and HKUST [19], a conversational telephone speech recognition dataset. The former consists of 150 hours, 10 hours, and 5 hours of training, validation, and testing, respectively, while the latter consists of a 5 hour test set, 4.2 hours extracted from the training data as the validation set, and the remaining 152 hours as the training set. We concatenate all characters in the corpus, including three special tokens, such as $\\langle\\mathrm{PAD}\\rangle$, $<$ SOS $>$, and $<$ EOS $>$. In our models, we use two encoder layers and four decoder layers. The large transformer consists\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_178f5a925eb34ce9139eg-4.jpg?height=53&width=868&top_left_y=1231&top_left_x=173) For the smaller transformers, we select the same parameters as the LRT model with $r=100, r=75$ and $r=50$. In the beam-search decoding, we take $\\alpha=1, \\gamma=0.1$, and a beam size of 8 . We evaluate our model using a single GeForce GTX 1080Ti GPU and three Intel Xeon E5-2620 v4 CPU cores. We use character error rate (CER) as the evaluation metric. ## 5. RESULTS AND DISCUSSION\n\n### 5.1. Evaluation Performance\n\nTable 1 shows the experiment results. LRT models gain slight improvement even with a greater than a $50 \\%$ compression rate, and they outperform the vanilla transformers on both the AiShell-1 and HKUST test sets, with a $13.09 \\%$ CER and a $28.95 \\%$ CER, respectively. In addition, we further minimize the gap between the HMM-based hybrid and end-to-end approaches without leveraging a perturbation strategy or external language model. Interestingly, our LRT models achieve lower validation loss compared to the uncompressed Transformer (large) baseline model, which implies that our LRT models regularize better, as shown in Figure 3 . The models are faster to converge and stop in a better local minimum compared to the vanilla transformers. ### 5.2. Memory and Time Efficiency\n\nAs shown in Table 1, our LRT ( $r=50$ ) model achieves similar performance to the large transformer model despite having only one-third of the large transformer parameters. In terms\nTable 2. Compression rate and inference speed-up of LRT models vs. Transformer (large). $\\triangle$ CER and $|\\bar{X}|$ denote the improvement, and the mean length of generated sequences. | dataset | r | $\\Delta$ CER | compress. | speed-up |  | $\\|\\bar{X}\\|$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  | GPU | CPU only |  |\n| AiShell-1 | base | 0 | 0 | 1 | 1 | 23.08 |\n|  | 100 | $0.40 \\%$ | $49.40 \\%$ | 1.17 x | 1.15 x | 23.15 |\n|  | 75 | $0.26 \\%$ | $57.37 \\%$ | 1.23 x | 1.16 x | 23.17 |\n|  | 50 | $-1.10 \\%$ | $65.34 \\%$ | 1.30 x | 1.23 x | 23.19 |\n| HKUST | base | 0 | 0 | 1 | 1 | 22.43 |\n|  | 100 | $0.26 \\%$ | $47.72 \\%$ | 1.21 x | 1.14 x | 22.32 |\n|  | 75 | $0.13 \\%$ | $55.90 \\%$ | 1.26 x | 1.15 x | 22.15 |\n|  | 50 | $-1.53 \\%$ | $64.54 \\%$ | 1.35 x | 1.22 x | 22.49 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_178f5a925eb34ce9139eg-4.jpg?height=592&width=838&top_left_y=839&top_left_x=1097)\n\nFig. 3. Training and validation losses on AiShell-1 data. of time efficiency, our LRT models gain inference time speedup by up to 1.35 x in the GPU and 1.23 x in the CPU, and 1.10x training time speed-up in the GPU compared to the uncompressed Transformer (large) baseline model, as shown in Table 2. We also compute the average length of the generated sequences to get a precise comparison. In general, both the LRT and baseline models generate sequences with a similar length, which implies that our speed-up scores are valid. ## 6. CONCLUSION\n\nWe propose low-rank transformer (LRT), a memory-efficient and fast neural architecture that compress the network parameters and boosts the speed of the inference time by up to 1.35 x in the GPU and 1.23 x in the CPU, as well as the speed of the training time for end-to-end speech recognition. Our LRT improves the performance even though the number of parameters is reduced by $50 \\%$ compared to the baseline transformer model. Our approach generalizes better than uncompressed vanilla transformers and outperforms those from existing baselines on the AiShell-1 and HKUST datasets in an end-to-end setting without using additional external data. ## 7. REFERENCES\n\n[1] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960-4964. [2] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, \"Joint ctc-attention based end-to-end speech recognition using multi-task learning,\" in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "lrt-4": "IEEE, 2017, pp. 4835-4839. [3] Linhao Dong, Shuang Xu , and Bo Xu , \"Speechtransformer: a no-recurrence sequence-to-sequence model for speech recognition,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5884-5888. [4] Jie Li, Xiaorui Wang, Yan Li, et al., \"The speechtransformer for large-scale mandarin chinese speech recognition,\" in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "lrt-5": "IEEE, 2019, pp. 7095-7099. [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008. [6] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran, \"Low-rank matrix factorization for deep neural network training with high-dimensional output targets,\" in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6655-6659. [7] ukasz Dudziak, Mohamed Abdelfattah, Ravichander Vipperla, Stefanos Laskaridis, and Nicholas Lane, \"Shrinkml: End-to-end asr model compression using reinforcement learning,\" in INTERSPEECH, 2019. [8] Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J Barezi, and Pascale Fung, \"On the effectiveness of low-rank matrix factorization for lstm model compression,\" in Proceedings of the 33rd Pacific Asia Conference on Language, Information and Computation, Hakodate, Japan, 13-15 Sept. 2019, Association for Computational Linguistics. [9] Oleksii Kuchaiev and Boris Ginsburg, \"Factorization tricks for lstm networks,\" ICLR Workshop, 2017. [10] Alex Graves and Navdeep Jaitly, \"Towards end-to-end speech recognition with recurrent neural networks,\" in International conference on machine learning, 2014, pp. 1764-1772. [11] Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran, George Saon, and Michael Picheny, \"Building competitive direct acoustics-to-word models for english conversational speech recognition,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4759-4763. [12] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan, \"Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnnlm,\" Proc. Interspeech 2017, pp. 949-953, 2017. [13] Mohan Li, Min Liu, and Hattori Masanori, \"End-to-end speech recognition with adaptive computation steps,\" in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6246-6250. [14] Mohan Li, Yuanjiang Cao, Weicong Zhou, and Min Liu, \"Framewise supervised training towards end-toend speech recognition models: First results,\" Proc.",
    "lrt-6": "Interspeech 2019, pp. 1641-1645, 2019. [15] Yajie Miao, Mohammad Gowayyed, Xingyu Na, Tom Ko, Florian Metze, and Alexander Waibel, \"An empirical exploration of ctc acoustic models,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "lrt-7": "IEEE, 2016, pp. 2623-2627. [16] Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung, \"Code-switched language models using neural based synthetic data from parallel sentences,\" in Proceedings of the 23rd Conference on Computational Natural Language Learning, Hong Kong, Nov. 2019, Association for Computational Linguistics. [17] Karen Simonyan and Andrew Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" in ICLR, 2015. [18] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng, \"Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,\" in 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1-5. [19] Yi Liu, Pascale Fung, Yongsheng Yang, Christopher Cieri, Shudong Huang, and David Graff, \"Hkust/mts: A very large scale mandarin telephone speech corpus,\" in International Symposium on Chinese Spoken Language Processing.",
    "lrt-8": "Springer, 2006, pp.",
    "lrt-9": "724-735. [^0]:    *Equal contributions. This work has been partially funded by ITF/319/16FP and MRP/055/18 of the Innovation Technology Commission, the Hong Kong SAR Government, and School of Engineering Ph.D. Fellowship Award, HKUST, and RDC 1718050-0 of EMOS.AI. "
}