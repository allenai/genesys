{
    "scatterbrain-0": "# Scatterbrain: Unifying Sparse and Low-rank Attention Approximation \n\nBeidi Chen* ${ }^{* \\dagger}$, Tri Dao ${ }^{* \\dagger}$, Eric Winsor ${ }^{\\dagger}$, Zhao Song ${ }^{\\S}$, Atri Rudra ${ }^{\\ddagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$<br>${ }^{\\dagger}$ Department of Computer Science, Stanford University<br>${ }^{\\S}$ Adobe Research<br>${ }^{\\ddagger}$ Department of Computer Science and Engineering, University at Buffalo, SUNY<br>\\{beidic,trid,winsor\\}@stanford.edu, zsong@adobe.com, atri@buffalo.edu,<br>chrismre@cs.stanford.edu\n\nOctober 29, 2021\n\n\n#### Abstract\n\nRecent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences.",
    "scatterbrain-1": "However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve $2.1 \\times$ lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce $98 \\%$ of attention memory at the cost of only $1 \\%$ drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks. ## 1 Introduction\n\nTransformer models 63 have been adapted in a wide variety of applications, including natural language processing [7, 26, 50, image processing [10, 47], and speech recognition 42]. Training large Transformers requires extensive computational and memory resources, especially when modeling long sequences, mainly due to the quadratic complexity (w.r.t. sequence length) in attention layers. Recent advances in efficient transformers [17, 22, 35, 36, 65] leverage attention approximation to overcome the bottleneck by approximating the attention matrices. However, it is challenging to find a robust approximation method that balances the efficiency-accuracy trade-off on a wide variety of tasks [57, 58]. We categorize most of the existing approaches for efficient attention matrix computation into two major groups: exploiting either the sparsity, e.g., Reformer 36], SMYRF [22], or low-rank properties of the attention matrices, e.g., Linformer 65], Linear Transformer 35], and Performer 17]. However, these techniques usually have different strengths and focus on the performance of specific tasks, so their approximations still cause accuracy degradation on many other tasks. For instance, according to a recent benchmark paper 57 and our experiments, low-rank-based attention might be less effective on hierarchically structured data or language modeling tasks, while sparse-based variants do not perform well on classification tasks. [^0]![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-02.jpg?height=532&width=1684&top_left_y=246&top_left_x=248)\n\nFigure 1: Left: regimes that sparse+low-rank approximation is more accurate, based on the entropy of the attention matrices. Right: Scatterbrain Workflow. For the attention layer in Transformers, after computing Query $Q$, Key $K$, and Value $V$ matrices, we approximate $\\operatorname{softmax}\\left(Q K^{\\top}\\right) V$ with two components: (i) sparse $S V$ (ii) low-rank $\\phi(Q)\\left(\\phi(K)^{\\top} V\\right)$. We observe that sparse and low-rank approximations are complementary for many attention matrices in practice, and sparse + low-rank could outperform each individually (Figure 1 left). We empirically categorize the regimes in which sparse or low-rank approximation achieves better error based on the softmax temperature of attention (of which the entropy of softmax distribution can be used as a proxy). We expect that sparse methods perform well if the attention depends on a few entries (low entropy softmax). In contrast, low-rank methods do better if the attention depends on a mixture of many components (high entropy softmax). This explains the phenomenon that current sparse and low-rank-based approaches excel on different kinds of tasks. A natural question is whether one could understand and unify the strength of both approaches. While it is NP-hard to find the optimal combination of sparse and low-rank approximations, Robust PCA [9] is a polynomial-time solution with tight approximation error. We observe that Robust PCA achieves lower approximation error than sparse or low-rank alone on attention matrices. The difference is most pronounced for \"mid-range\" entropy, where we observe that up to $95 \\%$ error reduction is possible. The connection between Robust PCA and attention matrix estimation provides an opportunity to realize a more robust approximation. Specifically, given an attention matrix, one could adaptively perform sparse + lowrank approximation to obtain a low error. However, it comes with three challenges: (i) How to decompose the attention matrices into sparse and low-rank components and estimate them efficiently and accurately; Robust PCA is accurate but slow and requires materializing the full attention, while straightforward addition of sparse and low-rank attention will be inaccurate due to double counting. (ii) It is not clear if there is a theoretical guarantee that sparse + low-rank approximation is strictly better than sparse or low-rank in some regimes, though we observe the separation empirically. (iii) How does the lower approximation error transfer to end-to-end performance in real tasks. In this paper, we propose Scatterbrain, an accurate and efficient robust estimation of attention matrices with theoretical guarantees to address the above challenges. Specifically:\n\n- In Section 3, we observe that sparse and low-rank approximation are complementary and demonstrate that sparse + low-rank structure arises naturally when elements in the input sequence form clusters. We theoretically characterize and analyze the regimes where sparse, low-rank, and sparse + low-rank excel, dictated by the softmax temperature of attention. - In Section 4, inspired by the classical Robust PCA algorithm, we propose Scatterbrain, which efficiently combines sparse and low-rank matrices to approximate attention. In particular, we use Locality Sensitive Hashing (LSH) to identify large entries of the attention matrix (after softmax) without materializing the full matrix and then leverage kernel approximation to parameterize the low-rank part. We prove that our method has a strictly lower approximation error than the low-rank baseline. - In Section 5, we empirically validate our theory and the proposed method, showing that Scatterbrain accurately approximates the attention matrix, is memory efficient for long sequences, and works well across different tasks. First, we show that its approximation accuracy is close to our oracle Robust PCA and\nachieves $2.1 \\times$ lower error compared to other efficient baselines on real benchmarks. This leads to a direct application of Scatterbrain as a drop-in replacement to pre-trained full attention, thus reducing up to $98 \\%$ of the memory required for attention computations in pre-trained T2T-ViT and BigGAN while maintaining similar quality. Last we show that its superior accuracy and efficiency can improve the efficiency-accuracy trade-offs of Transformer end-to-end training. On the WikiText-103 language modeling task, Scatterbrain achieves up to 1 point better perplexity compared to Reformer and Performer. On 5 benchmark long-range tasks, Scatterbrain improves the average accuracy by up to 5 points ${ }^{1}$\n\n\n## 2 Problem Setting and Related Work\n\nWe first define the approximation problem we aim to solve in this paper. Then we discuss the applications of sparse and low-rank techniques in efficient Transformers and introduce robust PCA algorithm. Problem Formulation: In the attention matrix approximation problem, we are given three matrices, query, key, and value, $Q, K, V \\in \\mathbb{R}^{n \\times d}$ to compute $\\operatorname{softmax}\\left(Q K^{\\top}\\right) V$. We seek to reduce the quadratic complexity of $\\operatorname{softmax}\\left(Q K^{\\top}\\right)$ (applied row-wise) with low approximation error. More precisely, for an approximation procedure $f$, we minimize two objectives, the approximation error $\\mathbf{E}\\left[\\left\\|f(Q, K)-\\operatorname{softmax}\\left(Q K^{\\top}\\right)\\right\\|_{F}^{2}\\right]$, and the computation/memory cost $\\mathcal{C}(f(\\cdot))$. Sparse, Low-rank Approximation for Attention Matrices: Recent work exploits the sparsity patterns or finds a low-rank mapping of the original attention matrices to overcome the computational and memory bottlenecks in Transformers [17, 22, 35, 36, 53, 65. Generally, we can divide most of the techniques into two categories - sparse and low-rank approximations. Reformer 36] is a representative sparse variant that uses LSH [3] to retrieve or detect the locations of the attention matrices with large values and reduce the computation from $O\\left(n^{2}\\right)$ to $O(n \\log n)$. Performer [17] is an example of the low-rank variant, which uses kernelization to avoid explicit $O\\left(n^{2} d\\right)$ computation. One problem of either the sparse or low-rank approximation is that the structure of the attention matrices varies in practice, and it is challenging to perform robust approximation on a wide range of attention matrices. For example, Wang et al. 65] observes that attentions tend to have more low-rank structures in lower layers and Ramsauer et al. [51] shows that they are sparser in the later stage of the training. Ideally, we want to unify the strength of both techniques, but it is NP-hard to find the best combination of sparse and low-rank approximation. Sparse + Low-rank and Robust PCA: Fortunately, classical Robust PCA 97 presents a polynomial algorithm to find the approximately optimal or good combinations of sparse and low-rank approximation of the matrices. The sparse + low-rank matrix structure has been well studied in statistics and signal processing since the late 2000s 9]. This structure naturally generalizes low-rank [33, 62, and sparse 60] matrices. Scatterbrain is built on a line of work, e.g., Bigbird [70, Longformer [5] with the theme of combining multiple types of attention. However, despite the multitude of papers, this sparse + low-rank matrix approximation has not been rigorously studied in the context of attention matrices. We undertake this study and show how we can relax the sparse + low-rank approximation from robust PCA, making it efficient while still retaining PCA's accuracy. In fact, our results shed further light on why Bigbird or Longformer work, as they are special cases of a single principled structure. An extended discussion of related work is in Appendix A. ## 3 Characterization of Sparse + Low-rank Approx. to Attention Matrices\n\nWe motivate the use of sparse + low-rank approximation of the attention matrices with the key observation that for many attention matrices, sparse and low-rank approximation are complementary, and their ideal combination (via Robust PCA) can outperform both (Section 3.1). Furthermore, we argue that the sparse + low-rank structure can arise naturally when elements in the input sequence form clusters, as dictated by the softmax temperature (Section 3.2. [^1]\n### 3.1 Motivating Observations: Low-rank and Sparse Structures of Attention Matrices\n\nWe empirically characterize regimes where sparse and low-rank approximation are well-suited, based on the softmax temperature (for which we use the softmax distribution entropy is a proxy). Specifically, in Fig. 1 (left), we present the approximation error of the original attention matrices and the approximation (sparse or low-rank) of matrices sampled from a 4-layer Transformer trained on IMDb reviews classification 57. We make two observations:\n\n1. Sparse and low-rank approximation are complementary: sparse excels when the softmax temperature scale is low (i.e., low entropy), and low-rank excels when the softmax temperature is high (i.e., high entropy). 2. An ideal combination of sparse and low-rank (orange line in Fig. 1 left), obtained with robust PCA, can achieve lower error than both. Similar observations on other benchmarks and details are presented in Appendix B. ### 3.2 A Generative Model of How Sparse + Low-rank Structure Can Arise\n\nSparse + low-rank parameterization is more expressive than either sparse or low-rank alone. Indeed, in the Appendix, we construct a family of attention matrices to show the separation between the approximation capability of sparse + low-rank vs. sparse or low-rank alone: for an $n \\times n$ attention matrix, sparse or low-rank alone requires a $O\\left(n^{2}\\right)$ parameters to get $\\epsilon$ approximation error in Frobenius norm, while sparse + low-rank only requires $O(n)$ parameters. Moreover, we argue here that sparse + low-rank is a natural candidate to approximate generic attention matrices. We describe a generative model of how the sparse + low-rank structure in attention matrices could arise when the elements of the input sequence\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-04.jpg?height=279&width=816&top_left_y=907&top_left_x=1053)\n\nFigure 2: Visualization of the generative process, for three different values of the intra-cluster distance $\\Delta$ (small, medium, and large).",
    "scatterbrain-2": "The vectors from the input sequence (rows of $Q$ ) form clusters that lie approximately on the unit sphere.",
    "scatterbrain-3": "Different colors represent different clusters. form clusters. Under this process, we characterize how the softmax temperature dictates when we would need sparse, low-rank, or sparse + low-rank matrices to approximate the attention matrix. This result corroborates the observation in Section 3.1 . Generative process of clustered elements in input sequence We describe here a generative model of an input sequence to attention, parameterized by the inverse temperature $\\beta \\in \\mathbb{R}$ and the intra-cluster distance $\\Delta \\in \\mathbb{R}$. Process 1. Let $Q \\in \\mathbb{R}^{n \\times d}$, where $d \\geq \\Omega\\left(\\log ^{3 / 2}(n)\\right)$, with every row of $Q$ generated randomly as follows: 1. For $C=\\Omega(n)$, sample $C$ number of cluster centers $c_{1}, \\ldots, c_{C} \\in \\mathbb{R}^{d}$ independently from $\\mathcal{N}\\left(0, I_{d} / \\sqrt{d}\\right)$. 2. For each cluster around $c_{i}$, sample $n_{i}=O(1)$ number of elements around $c_{i}$, of the form $z_{i j}=c_{i}+r_{i j}$ for $j=1, \\ldots, n_{i}$ where $r_{i j} \\sim \\mathcal{N}\\left(0, I_{d} \\Delta / \\sqrt{d}\\right)$. Assume that the total number of elements is $n=n_{1}+\\cdots+n_{C}$ and $\\Delta \\leq O\\left(1 / \\log ^{1 / 4} n\\right)$\nLet $Q$ be the matrix whose rows are the vectors $z_{i j}$ where $i=1, \\ldots, C$ and $j=1, \\ldots, n_{i}$. Let $A=Q Q^{\\top}$ and let the attention matrix be $M_{\\beta}=\\exp (\\beta \\cdot A)$.",
    "scatterbrain-4": "We visualize this generative process in Fig. 2 . Softmax temperature and approx. error We characterize when to use sparse, low-rank, or sparse + low-rank to approximate the attention matrices in Process 1, depending on the inverse temperature $\\beta$. The intuition here is that the inverse temperature corresponds to the strength of interaction between the clusters. If $\\beta$ is large, intra-cluster interaction dominates the attention matrix, the softmax distribution is peaked, and so we only need a sparse matrix to approximate the attention. If $\\beta$ is small, then the inter-cluster attention is similar to intra-cluster attention, the softmax distribution is diffuse, and we can approximate it with a\nlow-rank matrix. In the middle regime of $\\beta$, we need the sparse part to cover the intra-cluster attention and the low-rank part to approximate the inter-cluster attention.",
    "scatterbrain-5": "We formalize this intuition in Theorem 1(in bounds below we think of $\\epsilon$ as a constant).",
    "scatterbrain-6": "All the proofs are in Appendix D. Theorem 1. Let $M_{\\beta}$, be the attention matrix in Process 1. Fix $\\epsilon \\in(0,1)$. Let $R \\in \\mathbb{R}^{n \\times n}$ be a matrix. Consider low-rank, sparse, and sparse + low-rank approximations to $M_{\\beta}$. 1. High temperature: Assume $\\beta=o(\\log n / \\log d)$. (a) Low-rank: There exists $R$ with $n^{o(1)}$ rank (and hence $n^{1+o(1)}$ parameters) such that $\\left\\|M_{\\beta}-R\\right\\|_{F} \\leq \\epsilon n$. (b) Sparse: If $R$ has sparsity $o\\left(n^{2}\\right)$, then $\\left\\|M_{\\beta}-R\\right\\|_{F} \\geq \\Omega(n)$. 2. Mid temperature: Assume $\\left(1-\\Delta^{2}\\right) \\log n \\leq \\beta \\leq O(\\log n)$. (a) Sparse + low-rank: There exists a sparse + low-rank $R$ with $n^{1+o(1)}$ parameters with $\\left\\|M_{\\beta}-R\\right\\|_{F} \\leq$ $\\epsilon n$. (b) Low-rank: If $R$ is such that $n-\\operatorname{rank}(R)=\\Omega(n)$, then $\\left\\|M_{\\beta}-R\\right\\|_{F} \\geq \\Omega(n)$. (c) Sparse: If $R$ has sparsity o $\\left(n^{2}\\right)$, then $\\left\\|M_{\\beta}-R\\right\\|_{F} \\geq \\Omega(n)$. 3. Low temperature: Assume $\\beta=\\Omega(\\log n)$. (a) Low-rank: If $R$ is such that $n-\\operatorname{rank}(R)=\\Omega(n)$, then $\\left\\|M_{\\beta}-R\\right\\|_{F} \\geq \\Omega\\left(e^{\\beta\\left(1-\\Delta^{2}\\right)}\\right)$. (b) Sparse: There exists $R$ with sparsity $O(n)$ such that $\\left\\|M_{\\beta}-R\\right\\|_{F} \\leq \\epsilon \\cdot e^{\\beta\\left(1-\\Delta^{2}\\right)}$\n\n## 4 Scatterbrain: Unifying Sparse and Low-rank Attention\n\nWe present Scatterbrain, and show that it approximates attention accurately and efficiently. Section 4.1 describes the challenges of designing an accurate and efficient approximation, and how obvious baselines such as Robust PCA or a simple combination of sparse attention and low-rank attention fail to meet both criteria. Section 4.2 demonstrates how Scatterbrain address the challenges (Fig. 1 contains a schematic of Scatterbrain). In Section 4.3, we show that Scatterbrain is unbiased with provably lower variance than low-rank baselines such as Performer. Fig. 3 shows a qualitative comparison between different methods of approximating the attention matrix: Robust PCA is accurate but slow, sparse (e.g., Reformer), and low-rank (e.g., Performer) attention are fast and memory-efficient but may not be very accurate, while Scatterbrain is more accurate than its sparse and low-rank counterparts while remaining just as efficient. More details about the efficient implementation of Scatterbrain are in Appendix C. ![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-05.jpg?height=451&width=635&top_left_y=1341&top_left_x=1252)\n\nFigure 3: Qualitative comparison of approx. accuracy and efficiency, among Robust PCA, sparse (Reformer) and low-rank (Performer) attention, and Scatterbrain. Scatterbrain is more accurate while being efficient. ### 4.1 Challenges of Designing an Accurate and Efficient Sparse + Low-rank Approximation\n\nWe seek a sparse + low-rank approximation of the attention matrix ${ }^{2} A$ that is both accurate and efficient. The natural theoretical baseline of Robust PCA is too slow and requires too much memory, while the most straightforward way of combining sparse attention and low-rank attention fails due to double counting on the support of the sparse attention. [^2]1. If the goal is accuracy, Robust PCA is the most studied algorithm to find a sparse + low-rank approximation to a given matrix. It relaxes the NP-hard problem of finding the best sparse + low-rank approximation into a convex optimization problem, with the nuclear norm and $\\ell_{1}$ constraints. Even though it can be solved in polynomial time, it is orders of magnitude too slow to be used in each iteration of a training loop. Moreover, it requires materializing the attention matrix, which defeats the main purpose of reducing compute and memory requirements. 2. On the other hand, one efficient way to get sparse + low-rank approximation of an attention matrix is to simply add the entries of a sparse approximation $S$ (say, from Reformer) and a low-rank approximation $\\widetilde{Q} \\widetilde{K}^{\\top}$ for $\\widetilde{Q}, \\widetilde{K} \\in \\mathbb{R}^{n \\times m}$ (say, from Performer). The sparse matrix $S$ typically has support determined randomly [16], by LSH [22, 36], or by clustering [53]. On the support of S, which likely includes the locations of the large entries of the attention matrix $A$, the entries of $S$ match those of $A$. One can multiply $\\left(S+\\widetilde{Q} \\widetilde{K}^{\\top}\\right) V=S V+\\widetilde{Q}\\left(\\widetilde{K}^{\\top} V\\right)$ efficiently because $S$ is sparse, and grouping $\\widetilde{Q}\\left(\\widetilde{K}^{\\top} V\\right)$ reduces the matrix multiplication complexity when $m \\ll n$, from $O\\left(n^{2} m\\right)$ to $O(n m d)$. The approximation $S+\\widetilde{Q} \\widetilde{K}^{\\top}$ matches $\\widetilde{Q} \\widetilde{K}^{\\top}$ outside $\\operatorname{supp}(S)$, hence it could be accurate there if $\\widetilde{Q} \\widetilde{K}^{\\top}$ is accurate. However, $S+\\widetilde{Q} \\widetilde{K}^{\\top}$ will not be accurate on the support of $S$ due to the contributions from both $S$ and from $\\widetilde{Q} \\widetilde{K}^{\\top}$. Adjusting $\\widetilde{Q} \\widetilde{K}^{\\top}$ to discount the contribution from $S$ is difficult, especially if we want to avoid materializing $\\widetilde{Q} \\widetilde{K}^{\\top}$ for efficiency. ### 4.2 Scatterbrain: Algorithm Intuition and Description\n\nThe simple insight behind our method is that on the support of the sparse matrix $S$, instead of trying to match the entries of the attention matrix $A$, we can set the entries of $S$ to discount the contribution from the low-rank part $\\widetilde{Q} \\widetilde{K}^{\\top}$. This way, the approximation $S+\\widetilde{Q} \\widetilde{K}^{\\top}$ will match $A$ exactly on the support of $S$, and will match $\\widetilde{Q} \\widetilde{K}^{\\top}$ outside $\\operatorname{supp}(S)$, which means it will still be accurate there if $\\widetilde{Q} \\widetilde{K}^{\\top}$ is accurate. We do not need to materialize the full matrix $\\widetilde{Q} \\widetilde{K}^{\\top}$ as need a subset of its entries is required, hence our approximation will be compute and memory efficient. Scatterbrain thus proceeds in three steps: we construct a low-rank approximation $\\widetilde{Q} \\widetilde{K}^{\\top} \\approx A$, and construct a sparse matrix $S$ such that $S+\\widetilde{Q} \\widetilde{K}^{\\top}$ matches $A$ on the support of $S$, then finally multiply $S V$ and $\\widetilde{Q}\\left(\\widetilde{K}^{\\top} V\\right)$ and combine the result. More specifically:\n\n1. Low-rank Approximation. We define a procedure LowRANK that returns two matrices $\\widetilde{Q}, \\widetilde{K} \\in \\mathbb{R}^{n \\times m}$ such that $\\widetilde{Q} \\widetilde{K}^{\\top}$ approximates $A$. In particular, we use a randomized kernel feature map $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{m}$ where $\\phi(x)=\\frac{1}{\\sqrt{m}} \\exp \\left(W x-\\|x\\|^{2} / 2\\right)$ with $W \\in \\mathbb{R}^{m \\times d}$ randomly sampled, entry-wise, from the standard normal distribution $\\mathcal{N}(0,1)$. We apply $\\phi$ to each row vector of $Q, K$ matrices, and denote $\\widetilde{Q}=\\phi(Q)$ and $\\widetilde{K}=\\phi(K)$ (row-wise). Note that we do not materialize $\\widetilde{Q} \\widetilde{K}^{\\top}$. 2. Sparse Approximation. We define a procedure SPARSE that returns a sparse matrix $S$ that matches $A-\\widetilde{Q} \\widetilde{K}^{\\top}$ on $\\operatorname{supp}(S)$. In particular, using a family of locality sensitive hash functions, compute the hash codes of each query and key vectors in $Q, K$ matrices (row-wise). Let $\\mathcal{S}$ be the set of locations $(i, j)$ where $q_{i}$ and $k_{j}$ have the same hash codes (i.e, fall into the same hash bucket). Let $S$ be the sparse matrix whose support is $\\mathcal{S}$, and for each $(i, j) \\in \\mathcal{S}$, define\n\n$$\nS_{i, j}=\\exp \\left(q_{i}^{\\top} k_{j}\\right)-\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)=\\exp \\left(q_{i}^{\\top} k_{j}\\right)-\\widetilde{q}_{i}^{\\top} \\widetilde{k}_{j}\n$$\n\nwhere $q_{i}, k_{j}, \\widetilde{q}_{i}, \\widetilde{k}_{j}$ are the $i$-th and $j$-th rows of $Q, K, \\widetilde{Q}, \\widetilde{K}$ respectively. Note that we do not materialize $\\widetilde{Q} \\widetilde{K}^{\\top}$. 3. Scatterbrain Approximation. With $\\widetilde{Q}, \\widetilde{K}$ returned from LowRANK and $S$ from Sparse, we compute the (unnormalized) attention output with\n\n$$\n\\widetilde{O}=\\left(\\widetilde{Q} \\widetilde{K}^{\\top}+S\\right) V=\\widetilde{Q}\\left(\\widetilde{K}^{\\top} V\\right)+S V\n$$\n\nThe precise algorithm, including the normalization step, as well as the causal/unidirectional variant, is described in Appendix C. We also note Scatterbrain's flexibility: it can use different kinds of low-rank and sparse approximation as its sub-components. The combination of Reformer and Performer is simply one instance of Scatterbrain. Instead of using Reformer as a sparse component, we could use local attention [5] or random block-sparse attention [16. Instead of using Performer [17] as a low-rank component, we could also use Linear attention [35] or global tokens as in BigBird 70. The Scatterbrain method would work exactly the same way. As long as the low-rank component is unbiased (e.g., Performer), its combination with any sparse component in Scatterbrain would yield an unbiased estimator of the attention matrix as shown below. ### 4.3 Scatterbrain: Analysis\n\nOur method combines a low-rank approximation $\\widetilde{Q} \\widetilde{K}^{\\top}$ (which has rank $m \\ll n$ ) with a sparse approximation $S$. We argue that it is accurate (lower approximation error than baselines) and efficient (scaling the same as sparse or low-rank alone). The main insight of the analysis is that our approximation is exact for entries on the support of $S$ (picked by LSH), which are likely to be large. For entries not in the support of $S$ (likely to be small), our approximation matches the low-rank part (Performer) $\\widetilde{Q} \\widetilde{K}^{\\top}$, which is unbiased and has low variance for these entries. As a result, Scatterbrain retains the unbiasedness of Performer [17] but with strictly lower variance. We compare Scatterbrain to its low-rank baseline (Performer) and sparse baseline (Reformer). Performer is also based on the kernel approximation $\\phi$, and simply uses $\\widetilde{Q} \\widetilde{K}^{\\top}$ to approximate the attention matrix $A$. Reformer uses LSH to identify large entries of $A$, then compute a sparse matrix $S$ such that $S_{i j}=\\exp \\left(q_{i}^{\\top} k_{j}\\right)$ for $i j \\in \\operatorname{supp}(S)$. Accuracy: Because of the way $S$ is defined in Eq. 11, $\\widetilde{Q} \\widetilde{K}^{\\top}+S$ matches $A=\\exp \\left(Q K^{\\top}\\right)$ exactly on locations $(i, j) \\in \\mathcal{S}$, which are locations with likely large values. This addresses a weakness of low-rank methods (e.g., Performer) where the low-rank estimate is not accurate for locations with large values. We analyze the expectation and variance per entry of our estimator below (proof in Appendix D). Theorem 2. Define $\\sigma(q, k)=\\exp \\left(q^{\\top} k\\right)$, $\\widehat{\\sigma}^{\\text {pfe }}$ as Performer's estimator and $\\widehat{\\sigma}^{\\text {sbe }}$ as Scatterbrain estimator. Denote $\\mathcal{S}^{d-1} \\subset \\mathbb{R}^{d}$ as the unit sphere. Suppose $q, k \\in S^{d-1}$ are such that $\\|q-k\\|<\\tau$. Then:\n\n$$\n\\mathbb{E}\\left[\\widehat{\\sigma}^{\\mathrm{sbe}}(q, k)\\right]=\\sigma(q, k), \\quad \\operatorname{Var}\\left[\\widehat{\\sigma}^{\\mathrm{sbe}}(q, k)\\right]=(1-p) \\cdot \\operatorname{Var}\\left[\\widehat{\\sigma}^{\\mathrm{pfe}}(q, k)\\right]<\\operatorname{Var}\\left[\\widehat{\\sigma}^{\\mathrm{pfe}}(q, k)\\right]\n$$\n\nwhere $p=\\exp \\left(-\\frac{\\tau^{2}}{4-\\tau^{2}} \\ln d-O_{\\tau}(\\ln \\ln d)\\right)$. Hence Scatterbrain is unbiased, similar to Performer [17, but with strictly lower variance. The variance is small if $\\exp \\left(q^{\\top} k\\right)$ is small (since $\\operatorname{Var}\\left(\\widehat{\\sigma}^{\\text {pfe }}(q, k)\\right)$ will be small), or if $\\exp \\left(q^{\\top} k\\right)$ is large (since the probability of not being selected by LSH, $1-p$, will be small).",
    "scatterbrain-7": "In Fig. 4. we plot the per-entry MSE of different methods from Theorem 2 when approximating the unnormalized softmax attention $\\exp \\left(Q K^{\\top}\\right)$. Scatterbrain can approximate well both small entries (similar to the low-rank baseline, Performer), as well as large entries (similar to the sparse baseline, Reformer). Thus Scatterbrain has much lower MSE than Performer for large entries, and lower MSE than Reformer for small entries. Efficiency: In Eq. 22, the computation $S V$ is efficient because $S$ is sparse, and $\\widetilde{Q}\\left(\\widetilde{K}^{\\top} V\\right)$ is efficient because of the way we associate matrix multiplication (scaling as $O(n m d)$ instead of $O\\left(n^{2} d\\right)$, which is much bigger if $m \\ll n$ ). We validate these two properties of our approach in Section 5\n\n## 5 Experiments\n\nWe validate three claims that suggest Scatterbrain provides an accurate and efficient approximation to attention matrices, allowing it to outperform its sparse and low-rank baselines on benchmark datasets. - In Section 5.1, we evaluate the approximation error and testing accuracy of different approximation methods on pre-trained models such as BigGAN and Vision Transformer. We show that the approximation by Scatterbrain is close to the Robust PCA oracle and up to $2.1 \\times$ lower approximation error than other efficient baselines. ![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-08.jpg?height=386&width=1633&top_left_y=204&top_left_x=238)\n\nFigure 5: First: approximation comparison between Scatterbrain and its \"lowerbound\" Robust PCA. Second: comparison of error vs. entropy among SMYRF, Performer and Scatterbrain, three representatives of sparse, low-rank and sparse + low-rank approximations. Third and forth: Inception score (higher is better) and FID score (lower is better) of different attention variants for pretrained BigGAN. - In Section 5.2, we validate that when trained end-to-end, Scatterbrain outperforms baselines (sparse or low-rank attention) on a wide variety of benchmark tasks, including language modeling, classification, and the Long-range Arena (LRA) benchmarks. Scatterbrain achieves up to 5 points higher average accuracy on the LRA benchmark compared to Performer and Reformer. - In Section 5.3. we demonstrate the scalability of Scatterbrain, showing that it has comparable memory and time usage with simpler baselines (sparse or low-rank alone) across a range of input sequence lengths (Section 5.3), while requiring up to $12 \\times$ smaller memory than full attention. All details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix E\n\n\n### 5.1 Scatterbrain's Approximation Accuracy\n\nWe evaluate Scatterbrain's approximation accuracy in three steps: (1) compare it with of Robust PCA (sparse+lowrank), our theoretical foundation and oracle (2) compare it with SMYRF3 [22], Performer [17], which are popular variants of sparse and low-rank approximation to attention respectively and a naive baseline that directly adds SMYRF and Performer, (3) evaluate the inference accuracy when replacing full attention with Scatterbrain approximation. Scatterbrain achieves error within $20 \\%$ of the oracle robust PCA, and up to $2.1 \\times$ lower error than SMYRF and Performer. When serving as a drop-in replacement for full attention, even without training, Scatterbrain can reduce\n\nTable 1: Top-1 Accuracy of pre-trained T2T Vision Transformer on ImageNet with different attention replacements. Error represents the average normalized approximation error to full attention. | Attention | Top-1 Acc | Error (avg) |\n| :---: | :---: | :---: |\n| Full Attention | $81.7 \\%$ | - |\n| SMYRF | $79.8 \\%$ | $11.4 \\%$ |\n| Performer | $80.1 \\%$ | $7.5 \\%$ |\n| Baseline SMYRF + Performer | $79.7 \\%$ | $12.6 \\%$ |\n| Scatterbrain | $\\mathbf{8 0 . 7} \\%$ | $\\mathbf{5 . 3 \\%}$ |\n\nthe attention memory of Vision Transformer by $98 \\%$ at the cost of only $0.8 \\%$ drop of accuracy. Setup: We use the attention matrices from pre-trained BigGAN and T2T-ViT. BigGAN is a state-of-theart model in Image Generation for ImageNet. BigGAN has a single attention layer at resolution $64 \\times 64$ (4096 queries). T2T-ViT has 14 attention layers. Scatterbrain sets the ratio between SMYRF and Performer based on the entropy of an observed subset of attention matrices in different layers. We allocate more memory to the low-rank component compared to the sparse part if the entropy is high. Scatterbrain and Robust PCA: We first show that Scatterbrain approximates pre-trained attention matrices $10^{5} \\times$ faster while its approximation error is within $20 \\%$ on average. We also provide an example visualization on 100 attention matrices from the BigGAN generation process in Figure 5 (left). Scatterbrain vs. SMYRF and Performer: We show that Scatterbrain approximates pre-trained dense attention matrices with very low error compared to sparse (Reformer) or low-rank (Performer). Measuring Frobenius approx. error on the BigGAN image generation task, Scatterbrain achieves $2 \\times$ lower error compared to Performer. [^3]Table 2: The performance of Scatterbrain, Reformer, Performer and Full-Attention on Long-Range-Arena benchmarks and 2 popular language modeling tasks. We fix the same number of parameters $(1 / 8$ of the full) used for approximating the attention matrix for each method. | Attention | Copy (ppl) | WikiText-103 (ppl) |\n| :---: | :---: | :---: |\n| Full Attention | 1 | 25.258 |\n| Reformer | 6.8 | 27.68 |\n| Performer | 49 | 66 |\n| Scatterbrain | $\\mathbf{2 . 5 8}$ | $\\mathbf{2 6 . 7 2}$ |\n\n\n| Attention | ListOps | Text | Retrieval | Image | Pathfinder | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Full Attention | 38.2 | 63.29 | 80.85 | 41.78 | 73.98 | 59.62 |\n| Reformer | 36.85 | 58.12 | 78.36 | 28.3 | 67.95 | 53.9 |\n| Performer | 35.75 | 62.36 | 78.83 | 39.71 | 68.6 | 57.05 |\n| Scatterbrain | 38.6 | 64.55 | 80.22 | $\\mathbf{4 3 . 6 5}$ | $\\mathbf{6 9 . 9 1}$ | 59.38 |\n\nDrop-in replacement for full attention: We show that accurate approximation directly leads to efficient Inference. We replace BigGAN's dense attention with a Scatterbrain layer without other modifications. In 5 (right two), we show Inception and FID scores for Scatterbrain and other baselines under different memory budgets. Similarly, we use T2T-ViT 69], which is a token-to-token vision Transformer pre-trained on ImageNet 25]. In Table 1] we show the average approximation error of Scatterbrain for each layer and the end-to-end testing accuracy after replacing full attention with Scatterbrain and other baselines. Notably, Scatterbrain achieves $80.7 \\%$ Top-1 accuracy, which is only $1 \\%$ drop from the original $81.7 \\%$ by full attention reducing up to $98 \\%$ of the memory usage. ### 5.2 End-to-end Training Performance\n\nScatterbrain's accurate approximation of attention matrices allows it to outperform other efficient Transformer methods on benchmark tasks. Across a range of diverse tasks, both commonly used autoregressive tasks (sequence modeling) and benchmark long-range classification tasks (Long-Range Arena), Scatterbrain outperforms Performer (low-rank baseline) and Reformer (sparse baseline) by up to 4 points. ### 5.2.1 Auto-regressive Tasks\n\nOn the standard language modeling task of Wikitext-103, Scatterbrain obtains 1 point better perplexity than Reformer (sparse baseline), coming within 1.5 points of full attention. Settings: We compare the performance of Scatterbrain against Reformer and Performer on one popular synthetic task, Copy, and one large language modeling task: WikiText103 45]. Reformer is a representative sparse-approximation-based variant and Performer is a low-rank-approximation-based variant. The base model is vanilla Transformer [63]. We observed that generally allocating more memory budget to sparse tends to perform better, so Scatterbrain sets the ratio to $3: 1$ (sparse: low-rank component) for simplicity. The statistics of each dataset and model hyper-parameters are in Appendix E. We report the best results of each method in perplexity. Results: Table 2 shows the testing perplexity for Scatterbrain and other baselines under the same parameter budget (each approximation is only allowed to compute $\\frac{1}{8}$ of the full computation). Scatterbrain achieves comparable perplexity compared to the full attention Transformer model on Copy, and WikiText-103. Notably, Scatterbrain achieves 4 points lower perplexity on Copy and 1 point lower on WikiText-103 compared to Reformer, while Performer does not train stably on auto-regressive tasks (loss does not go down). Analysis: We also analyze the results by visualizing the error of Reformer (sparse), Performer (low-rank), and Scatterbrain (sparse + low-rank) given the same number of parameters when approximating the full attention matrices for each attention layer during training (Appendix E). The conclusion is for language modeling tasks, sparse+low-rank has the smallest approximation error in most of the cases, and sparse has the largest error, which matches with the end-to-end results. It also confirms the observation in the popular benchmark paper [57] that kernel or low-rank based approximations are less effective for hierarchical structured data. ### 5.2.2 Classification Tasks\n\nOn a suite of long-range benchmark tasks (Long Range Area), Scatterbrain outperforms Reformer (sparse baseline) and Performer (low-rank baseline) by up to 5 points on average. Settings: We compare the performance of Scatterbrain against Reformer and Performer on ListOps, two classifications: byte-level IMDb reviews text classification, image classification on sequences of pixels, a text retrieval, and pathfinder tasks. The datasets are obtained from the Long Range Arena (LRA) Benchmark [57, which is a recent popular benchmark designed for testing efficient Transformers. Similar to the auto-regressive tasks above, we use Reformer and Performer as baselines. The base model is also a vanilla Transformer. We follow the evaluation protocol from [57. We report the best accuracy of each method. Results: Table 2 shows the individual and average accuracy of each task for Scatterbrain and other baselines under the same parameters budget. Specially, each approximation is only allowed to use $12.5 \\%$ of the full computation. We can see Scatterbrain is very close to full attention even with a large reduction in computation and memory. Further more, it outperforms all the other baselines consistently on every task and achieves more than 5 point average accuracy improvement than sparse-based approximation Reformer and more than 2 point average accuracy improvement than low-rank-based variant Performer. Analysis: Similarly, in order to analyze the performance of Reformer, Performer and Scatterbrain, we visualize their approximation error given the same number of parameters when approximating the full attention matrices for each attention layer during training (Appendix E. We again find that Scatterbrain has the smallest approximation error, while Performer is the worst on ListOps and Reformer has the largest error on classification tasks, which matches with the end-to-end results and confirms our observations earlier (sparse and low-rank approximation excel in different regimes). ![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-10.jpg?height=473&width=968&top_left_y=956&top_left_x=857)\n\nFigure 6: Speed and memory required by different efficient attention methods. Scatterbrain is competitive with SMYRF (sparse baseline) and Performer (low-rank baseline), while up to $3 \\times$ faster and $12 \\times$ more memory efficient than full attention for sequence length 4096. ### 5.3 Scatterbrain's Efficiency, Scaling with Input Sequence Length\n\nWe include ablation studies on the scalability of Scatterbrain in Fig. 6. showing that it is as computation and memory-efficient as simpler baselines such as SMYRF and Performer, while up to $3 \\times$ faster and $12 \\times$ more memory efficient than full attention for sequence length 4096. This demonstrates that our combination of sparse and low-rank inherits their efficiency. We report run times and memory consumption of the sequence lengths ranging from 512 to 32768 . We use a batch size of 16 for all runs and conduct experiments a V100 GPU. Since the efficiency would be largely conditioned on hardware and implementation details, we perform best-effort fair comparisons. We adapt the Pytorch implementation from pytorch-fast-transformers library for our baselines and implement Scatterbrain similarly without any customized cuda kernels. ## 6 Discussion\n\nLimitations. As Scatterbrain has sparse attention as a component, it is not yet as hardware friendly (on GPUs and TPUs) as the low-rank component, which uses the very optimized dense matrix multiplication. This is the same limitation suffered by other sparse attention methods, but we are excited that more efficient sparse GPU kernels are being developed [29, 31]. Potential negative societal impacts. Our work seeks to understand the role of matrix approximation (and potentially energy savings) in the attention layer, which may improve a wide range of applications, each with their own potential benefits and harms. For example, making it language modeling more compute and memory efficient might facilitate spreading misinformation, and better image and video processing may make automatic surveillance easier. To mitigate these risks, one needs to address application-specific issues such as privacy and fairness, going beyond the error metrics we considered. Specially, for language models (LMs), while our work partially addresses the issue of environmental cost of LMs raised in [6], it does not address other issues such as unfathomable training data 6]. Discussion and future work. In this work, we make an observation on the sparse + low-rank structure of the attentions in Transformer models and theoretically characterize the regimes where sparse, low-rank and sparse + low-rank excel, based on the softmax temperature of the attention matrices. Motivated by this observation, we present Scatterbrain, a novel way to unify the strengths of both sparse and low-rank methods for accurate and efficient attention approximation with provable guarantees. We empirically verify the effectiveness of Scatterbrain on pretrained BigGAN, vision transformers, as well as end-to-end training of vanilla transformer. We anticipate that the study of this core approximation problem can prove useful in other contexts, such as generalized attention layers with other non-linearity beside softmax, and wide output layer in language modeling or extreme-classification. ## Acknowledgments\n\nWe thank Xun Huang, Sarah Hooper, Albert Gu, Ananya Kumar, Sen Wu, Trenton Chang, Megan Leszczynski, and Karan Goel for their helpful discussions and feedback on early drafts of the paper. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare.",
    "scatterbrain-8": "The Mobilize Center is a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and Bioengineering through Grant P41EB027060.",
    "scatterbrain-9": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh for angular distance.",
    "scatterbrain-10": "In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS), pages 1225-1233. 2015. [3] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal LSH for angular distance.",
    "scatterbrain-11": "In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pages 1225-1233, 2015. [4] R Artusi, P Verderio, and E Marubini. Bravais-pearson and spearman correlation coefficients: meaning, test of hypothesis and confidence interval.",
    "scatterbrain-12": "The International journal of biological markers, 17(2):148-151, 2002 . [5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, New York, NY, USA, 2021. Association for Computing Machinery. [7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [8] Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717-772, 2009. [9] Emmanuel J Cand\u00e8s, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?",
    "scatterbrain-13": "Journal of the ACM (JACM), 58(3):1-37, 2011. [10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213-229. Springer, 2020. [11] Beidi Chen and Anshumali Shrivastava. Densified winner take all (wta) hashing for sparse datasets.",
    "scatterbrain-14": "In Uncertainty in artificial intelligence, 2018. [12] Beidi Chen, Anshumali Shrivastava, and Rebecca C Steorts. Unique entity estimation with application to the syrian conflict. The Annals of Applied Statistics, 12(2):1039-1067, 2018. [13] Beidi Chen, Yingchen Xu, and Anshumali Shrivastava. Fast and accurate stochastic gradient estimation. 2019. [14] Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, et al. SLIDE: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and Systems, 2:291-306, 2020. [15] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher R\u00e9. Mongoose: A learnable lsh framework for efficient neural network training.",
    "scatterbrain-15": "In The International Conference on Learning Representations (ICLR), 2021. [16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [17] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "scatterbrain-16": "arXiv preprint arXiv:2009.14794, 2020. [18] Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen, Mengnan Zhao, and Anshumali Shrivastava. A tale of two efficient and informative negative sampling distributions. In International Conference on Machine Learning, pages 2319-2329. PMLR, 2021. [19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context.",
    "scatterbrain-17": "arXiv preprint arXiv:1901.02860, 2019. [20] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In The International Conference on Machine Learning $(I C M L), 2019$. [21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In The International Conference on Learning Representations (ICLR), 2020. [22] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf: Efficient attention using asymmetric clustering.",
    "scatterbrain-18": "arXiv preprint arXiv:2010.05315, 2020. [23] Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gradient descent for some non-convex matrix problems. In International Conference on Machine Learning, pages 2332-2341. PMLR, 2015. [24] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication.",
    "scatterbrain-19": "In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060-1079. SIAM, 2018. [25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.",
    "scatterbrain-20": "In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255, 2009. doi: $10.1109 /$ CVPR.2009.5206848. [26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.",
    "scatterbrain-21": "arXiv preprint arXiv:1810.04805, 2018. [27] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest neighbor search. In International Conference on Learning Representations (ICLR), 2019. [28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [29] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse GPU kernels for deep learning. In Supercomputing, 2020. [30] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing. In Vldb, volume 99, pages $518-529,1999$. [31] Scott Gray, Alec Radford, and Diederik P Kingma. GPU kernels for block-sparse weights.",
    "scatterbrain-22": "arXiv preprint arXiv:1711.09224, 3, 2017. [32] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in neural information processing systems (NeurIPS), 2020. [33] Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933. [34] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.",
    "scatterbrain-23": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages $604-613,1998$. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "scatterbrain-24": "In International Conference on Machine Learning, pages 5156-5165. PMLR, 2020. [36] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In The International Conference on Machine Learning (ICML), 2020. [37] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [38] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.",
    "scatterbrain-25": "In The International Conference on Learning Representations (ICLR), 2020. [39] Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear memory: How to make performers slim.",
    "scatterbrain-26": "arXiv preprint arXiv:2012.11346, 2020. [40] Drew Linsley, Junkyung Kim, Vijay Veerabadran, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated-recurrent units.",
    "scatterbrain-27": "arXiv preprint arXiv:1805.08315, 2018. [41] Zichang Liu, Zhaozhuo Xu, Alan Ji, Jonathan Li, Beidi Chen, and Anshumali Shrivastava. Climbing the wol: Training for cheaper inference.",
    "scatterbrain-28": "arXiv preprint arXiv:2007.01230, 2020. [42] Haoneng Luo, Shiliang Zhang, Ming Lei, and Lei Xie. Simplified self-attention for transformer-based end-to-end speech recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 75-81. IEEE, 2021. [43] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention.",
    "scatterbrain-29": "arXiv preprint arXiv:2106.01540, 2021. [44] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49 th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150, 2011. [45] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [46] Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning.",
    "scatterbrain-30": "arXiv preprint arXiv:1804.06028, 2018. [47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055-4064. PMLR, 2018 . [48] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919-944, 2013. [49] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In The International Conference on Learning Representations $(I C L R), 2020$. [50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "scatterbrain-31": "arXiv preprint arXiv:1910.10683, 2019. [51] Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. [52] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12 (12), 2011. [53] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.",
    "scatterbrain-32": "Transactions of the Association for Computational Linguistics, 9: $53-68,2021$. [54] Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips).",
    "scatterbrain-33": "In Advances in Neural Information Processing Systems (NeurIPS), pages 2321-2329, 2014. [55] Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Advances in Neural Information Processing Systems, pages 3088-3096, 2015. [56] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2019. [57] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020. [58] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.",
    "scatterbrain-34": "arXiv preprint arXiv:2009.06732, 2020. [59] Richard Taylor. Interpretation of the correlation coefficient: a basic review. Journal of diagnostic medical sonography, 6(1):35-39, 1990. [60] Reginald P Tewarson and Reginald P Tewarson. Sparse matrices, volume 69. Academic Press New York, 1973 . [61] Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher R\u00e9. Learning compressed transforms with low displacement rank. In Advances in neural information processing systems (NeurIPS), pages $9052-9060,2018$. [62] Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144-160, 2019. [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. [64] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.",
    "scatterbrain-35": "arXiv preprint arXiv:1804.07461, 2018. [65] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [66] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In The International Conference on Learning Representations $(I C L R), 2019$. [67] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint arXiv:2102.03902, 2021. [68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019 . [69] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
    "scatterbrain-36": "arXiv preprint arXiv:2101.11986, 2021. [70] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. [71] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. arXiv preprint arXiv:2107.02192, 2021. ## Appendix\n\nTable of Contents\nA Extended Related Work ..... 17\nA. 1 Robust PCA ..... 17\nA. 2 Efficient Transformers ..... 17\nA. 3 Locality Sensitive Hashing for Efficient Neural Network Training ..... 18\nA. 4 Structured Matrices for Efficient Machine Learning Models ..... 18\nB Motivating Observations: Low-rank and Sparse Structures of Attention Matrices .....",
    "scatterbrain-37": "19\nB.",
    "scatterbrain-38": "1 Setup ..... 19\nB. 2 Observation 1: Sparse and low-rank approximation errors are negatively correlated ..... 19\nB. 3 Observation 2: Sparse approximation error is lower when softmax entropy is low and\nlow-rank approximation error is lower error when entropy is high ..... 19\nB. 4 Observation 3: Sparse + Low-rank achieves better approximation error than sparse or\nlow-rank alone ..... 21\nC Scatterbrain Algorithm and Implementation Details ..... 22\nD Proofs ..... 23\nD. 1 Expressiveness of Sparse + Low-rank Matrices ..... 23\nD. 2 Generative Model, Softmax Temperature, and Matrix Approximation ..... 26\nD. 3 Scatterbrain: Analysis ..... 30\nE Additional Experiments and Details ..... 31\nE. 1 Datasets ..... 31\nE. 2 Settings ..... 31\nE. 3 More Ablation Studies ..... 33\nE. 4 Analysis ..... 33\nE. 5 Additional Experiments of Fine-tuning Bert on GLUE .....",
    "scatterbrain-39": "34\nF Further Discussions and Future Work ..... 34\n\n## A Extended Related Work\n\n## A. 1 Robust PCA\n\nRobust Principle Component Analysis (robust PCA) is the problem of finding a composition of a matrix $M$ into a sum of sparse and low-rank components: $M=S+L$. It is a modification of PCA to accommodate corrupted observations (aka, noise). The sparse part covers the noise, while the low-rank part recovers the principle components. The most popular method to solve the problem is convex relaxation [8, where one minimizes the error $\\|M-S-L\\|_{F}^{2}$ subject to $\\ell_{1}$ constraint on $\\|S\\|_{1}$ and nuclear norm constraint on $\\|L\\|_{*}$, in order to promote the sparsity of $S$ and the low-rankness of $L$. This convex problem can be solved with a variety of methods, such as interior point methods or the method of Augmented Lagrange Multipliers. In our context, to find a sparse + low-rank decomposition of the attention matrix, one can also heuristically \"peel off\" the sparse part by finding the large entries of the attention matrix, then find a low-rank decomposition of the remainder. To avoid materializing the full attention matrix, one can use LSH to find potential locations of large entries, and use matrix completion [52] to find a low-rank decomposition. Gradient descent can find global optimum for this matrix completion problem [23]. However, it still requires too many iterations to be used in each training step. ## A. 2 Efficient Transformers\n\nSparse, Low-rank Approx.: Transformer-based model such as BERT [38 has achieved unprecedented performance in natural language processing. Recently, Vision Transformers [28, 69, has also achieved comparable performance to the traditional convolutional neural network in computer vision tasks [66]. However, the quadratic computation of the attention layers constrains the scalability of Transformers. There are many existing directions to overcome this bottleneck, including attention matrix approximation such as Reformer [36], Performer [17], leveraging a side memory module that can access multiple tokens at once [38, 39, 56] such as Longformer [5] and BigBird [70, segment-based recurrence such as TransformerXL 19 and Compressive Transformer [49. Please refer to a recent survey [58 for more details. In this paper, we mainly explore within the scope of approximating dense or full attention matrices. Existing combination of Sparse and Low-rank Attention: Our focus on the classical and welldefined problem of matrix approximation, as opposed to simply designing an efficient model that performs well on downstream tasks (e.g., Longformer, Luna, Long-short transformer, etc.) affords us several advantages: (i) Easier understanding and theoretical analysis (Section 3, 4). We see that Scatterbrain yields an unbiased estimate of the attention matrix, and we can also understand how its variance changes. (ii) Clear-cut evaluation based on approximation error, as well as the ability to directly replace a full attention layer with Scatterbrain attention without re-training (Section 5). This setting is increasingly important as transformer models are getting larger and training them from scratch has become prohibitively costly. Other methods such as Luna and Long-short transformer are not backward compatible with pre-trained models. Here we compare Scatterbrain with other work mentioned by the reviewer, showing how most of them are special cases of Scatterbrain. We will also add this discussion in the updated version of the manuscript. - Longformer [5: a special case of Scatterbrain where the sparse component is local attention, and the low-rank component is the global tokens. Global tokens can be considered a restricted form of low-rank approximation. - BigBird 70: a special case of Scatterbrain where the sparse component is local + random sparse attention, and the low-rank component is the global tokens. The use of global tokens makes the model unsuited for autoregressive modeling. On the other hand, Scatterbrain's generality allows it to use other kinds of low-rank attention (e.g., Performer), and thus Scatterbrain works on both the causal/autoregressive and the bidirectional/non-causal attention settings. BigBird's motivation is also quite different from ours: they aim to design efficient attention such that the whole Transformer model is still a universal approximator and is Turing complete. Our goal is more concrete and easier to evaluate: we approximate the attention matrices, to get a small Frobenius error between the Scatterbrain attention and the full attention matrices. - Luna [43 (concurrent work): they use a fixed-length extra sequence and two consecutive attention steps: the context sequence attends to the extra sequence, and then the query sequence attends to the extra sequence. This is similar in spirit to low-rank attention (Linformer) and global tokens, but it is not a\nlow-rank approximation due to the non-linearity between the two attention steps. It is not clear to us that it combines different kinds of attention. - Long-short transformer 71] (concurrent work): a special case of Scatterbrain where the sparse component is local attention and the low-rank component is Linformer. ## A. 3 Locality Sensitive Hashing for Efficient Neural Network Training\n\nLocality Sensitive Hashing (LSH) has been well-studied in approximate nearest-neighbor search 2, 11, 27, 30, 34, 54. Since the brute-force approach for similarity search is computationally expensive, researchers have come up with various indexing structures to expedite the search process. Usually this comes with trade-offs on the search quality. Based on these indexing structures, one can achieve sub-linear search time. LSH has been used in estimation problem as well [12, 13]. Recently, there has been several work taking advantage of LSH data structures for efficient neural network training. During training process, the weight matrices are slowly modified via gradients derived from objective functions. If we consider the weights as the search data and input as queries, we can view neural network training as a similarity search problem. For example, 14, 18, 41] proposes an algorithm which performs sparse forward and backward computations via maximum inner product search during training. It is based on the observation that the model is usually over-parameterized so the activation for a given input could be sparse and LSH is used to find or impose the sparse structure. Similarly, LSH based algorithms have also been used in Transformers [14, [15], where LSH is used to capture the sparse structure of the attention matrices. They can largely reduce the memory bottleneck of self-attention modules especially over long sequences in Transformer. Though [15] has done some exploration to improve LSH accuracy-efficiency trade-offs through learnable LSH, most of the above works have limited understanding on when and where LSH can perform well. ## A. 4 Structured Matrices for Efficient Machine Learning Models\n\nSparse + low-rank is an example of a class of structured matrices: those with asymptotically fast matrix-vector multiplication algorithm ( $o\\left(n^{2}\\right)$ time complexity) and few parameters ( $o\\left(n^{2}\\right)$ space complexity). Common examples include sparse, low-rank matrices, and matrices based on fast transforms (e.g., Fourier transform, circulant, Toeplitz, Legendre transform, Chebyshev transform, and more generally orthogonal polynomial transforms). These classes of matrices, and their generalization, have been used in machine learning to replace dense matrices in fully connected, convolutional, and recurrent layers [32, 55, 61. De Sa et al. [24] shows that any structured matrix can be written as product of sparse matrices, and products of sparse matrices even with fixed sparsity pattern have been shown to be effective at parameterizing compressed models [1, 20, 21]. In our setting, it remains difficult to approximate the attention matrix with these more general classes of structured matrices. This is because many of them are fixed (e.g., Fourier transform, orthogonal polynomial transforms), and there lacks efficient algorithms to find the closest structured matrix to a given attention matrix. ## B Motivating Observations: Low-rank and Sparse Structures of Attention Matrices\n\nWe aim to build a deeper understanding of sparse and low-rank structures in real attention matrices: where each of them excel, and the potential for their combination. Specifically, we\n\n- show that sparse and low-rank approximation errors are negatively correlated (through statistical tests),\n- characterize regimes where each of sparse and low-rank approximation are well-suited, as dictated by the entropy of the softmax attention distribution, and\n- demonstrate that sparse + low-rank has the potential to achieve better approximation than either. ## B. 1 Setup\n\nDenote $M$ as the attention matrix (after softmax) and $\\mathcal{H}$ as entropy. We measure approximation error by the Frobenius norm or the original matrix and the approximation (sparse or low-rank). All the observed attention matrices in this section are from (1) a 4-layer vanilla Transformer trained from scratch on char-level IMDb reviews classification [57] (2) a 16-layer vanilla Transformer trained from scratch on WikiText103 45 (3) a 1-layer (attention) pre-trained BigGAN on ImageNet [25]. To collect attention matrices for IMDb and WikiText103, we first save checkpoint of the models in every epoch; then evaluate 100 samples from validate data for each checkpoint and collect attention matrices from each layer each head. Note we take the median of the stats (error) for those 100 samples if it is difficult to visualize. To collect attention matrices for BigGAN, we generate 100 samples and collect the attention on the fly. ## B. 2 Observation 1: Sparse and low-rank approximation errors are negatively correlated\n\nTable 3: The Spearman's rank, Pearson and Kendall's Tau correlation coefficients between Sparse and Low-rank approx. error on IMDb, WikiText-103, and BigGAN-ImageNet.",
    "scatterbrain-40": "P-values of $<0.05$ indicate statistical significance. The two errors are negatively correlated. |  | IMDb |  | WikiText103 |  | BigGAN-ImageNet |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Coef | p-value | Coef | p-value | Coef | p-value |\n| Spearman's rank | -0.89 | $<.00001$ | -0.63 | $<.00001$ | -0.21 | $<.00001$ |\n| Pearson | -0.78 | $<.00001$ | -0.61 | $<.00001$ | -0.31 | $<.00001$ |\n| Kendall's Tau | -0.74 | $<.00001$ | -0.51 | $<.00001$ | -0.18 | $<.00001$ |\n\nWe fixed the number of parameters, $K$, allowed for each attention matrix approximation and collect the errors from ideal sparse and low-rank approximations: top- $K$ entries for each row of the matrix for sparse and top $-K$ eigenvalues for low-rank. Then we run three standard statistical correlation tests 4, 59], Spearman, Pearson and Kendall's Tau on sparse and low-rank approximation error for all the matrices. We can see from Table 3 that errors are significantly negatively correlated (p-value $<0.05$ ). Further more, the left three plots on Figure 7 visualizes the correlation between the two errors on three datasets. This negative correlation suggests that there is some property of the softmax attention distribution which determines when sparse or low-rank excels. We validate this claim in the next observation. ## B. 3 Observation 2: Sparse approximation error is lower when softmax entropy is low and low-rank approximation error is lower error when entropy is high\n\nWe visualize the sparse and low-rank approximation error against the entropy of attention matrices $\\mathcal{H}(M)$ (applied to each row, then averaged) on the right plot in Figure 7. The attention matrices are $\\in \\mathbb{R}^{1024 \\times 1024}$ (padded) so the x -axis has range from $[0, \\ln (1024)]$. For high-entropy distributions (more diffused) low-rank\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-20.jpg?height=2075&width=1373&top_left_y=210&top_left_x=354)\n\nFigure 7: Characterization of the relationship between the softmax distribution of each attention matrix row and approximation error of sparse, low-rank and sparse+low-rank. The top, middle and bottom plots are for IMDb, WikiText103 and BigGAN-ImageNet respectively. Left: The approximation error of sparse and low-rank are negatively correlated. Sparse performs well when low-rank does not, and vice versa. Right: Entropy of the softmax attention distribution (i.e., scale of logits) determines the regimes where sparse, low-rank, or sparse + low-rank perform well. Sparse + low-rank yields better approximation than sparze or low-rank alone, across the board. matrices approximates the attention matrix well. For low-entropy distributions (more peaked), sparse matrices are better-suited. This implies that sparse and low-rank approximations could be complementary: if we can combine the strength of both, it is possible to come up with a better approximation across more general scenarios. Therefore, in the next observation, we try to combine sparse and low-rank approximations. ## B. 4 Observation 3: Sparse + Low-rank achieves better approximation error than sparse or low-rank alone\n\nWe find an approximation of the attention matrix of the form $S+L$, where $S$ is sparse and $L$ is low-rank. This problem has a rich history and is commonly solved with Robust PCA. As shown in 7 , across the range of entropy, sparse + low-rank approximation can achieve lower error than either sparse or low-rank when choosing the correct mix ratio of sparse and low rank approximation ideally (with robust-PCA). Motivated by the fact that sparse and low-rank approximations of attention matrices have complementary strengths (Observations 1 and 2), one might want to combine them (Observation 3) in hope of yielding a more robust approximation that works well across different kinds of attention matrices. The above introduces three main challenges that we have addressed in the main paper:\n\n- how to find sparse + low-rank decomposition of an attention matrix that is compute efficient (the most studied algorithm, robust PCA, is orders of magnitude too slow to be done at each training iteration) and memory efficient (i.e., without materializing the full matrix) (Section 4),\n- if we can find such a sparse + low-rank decomposition, how accurate is the approximation (Section 4.3),\n- how expressive is the sparse + low-rank parameterization, i.e., are there natural classes of matrices where sparse + low-rank yields asymptotically better approximation than sparse or low-rank alone) (Section 3)? ## C Scatterbrain Algorithm and Implementation Details\n\nLet $Q, K \\in \\mathbb{R}^{n \\times d}$ be the query and key matrices respectively, and $V \\in \\mathbb{R}^{n \\times d}$ be the value matrix.",
    "scatterbrain-41": "Let the rows of $Q$ be $q_{1}, \\ldots, q_{n}$, and the rows of $K$ be $k_{1}, \\ldots, k_{n}$. The attention computes:\n\n$$\n\\operatorname{softmax}\\left(Q K^{\\top}\\right) V\n$$\n\nwith softmax applied row-wise, where for each vector $v \\in \\mathbb{R}^{n}, \\operatorname{softmax}(v)=\\frac{1}{\\sum_{j=1}^{n} e^{v_{j}}}\\left[e^{v_{1}}, \\ldots, e^{v_{n}}\\right]^{\\top}$. Here we omit the usual scaling of $\\frac{Q K^{\\top}}{\\sqrt{d}}$ for simplicity since that could be folded into $Q$ or $K$. Note that $\\operatorname{softmax}\\left(Q K^{\\top}\\right)=D^{-1} \\exp \\left(Q K^{\\top}\\right)$, where the exponential function is applied element-wise and $D$ is a diagonal matrix containing the softmax normalization constants $\\left(D_{i, i}=\\sum_{j=1}^{n} \\exp \\left(q_{i}^{\\top} k_{j}\\right)\\right)$. Then attention has the form $D^{-1} \\exp \\left(Q K^{\\top}\\right) V$\n\nWe describe the Scatterbrain approximation algorithm in Algorithm 1. This includes the normalization step. ```\nAlgorithm 1 Scatterbrain Approximation of Attention\n    Input: \\(Q, K, V \\in \\mathbb{R}^{n \\times d}\\), hyper-parameters \\(m, k, l\\)\n    procedure \\(\\operatorname{Init}(m, k, l)\\)\n        Sample \\(W \\in \\mathbb{R}^{m \\times d}\\) where \\(W_{i} \\sim \\mathcal{N}(0,1)\\) i.i.d. Kernels \\(\\phi: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{m}, \\phi(x)=\\frac{\\exp \\left(W x-\\|x\\|^{2} / 2\\right)}{\\sqrt{m}}\\)\n        Hash \\(\\forall l \\in[L], \\mathcal{H}_{l}=\\left\\{h_{l, k}\\right\\}_{k \\in[K]}, \\mathcal{H}=\\cup_{l \\in[L]} \\mathcal{H}_{l}\\)\n    end procedure\n    procedure LowRankApprox \\((Q, K, V, \\phi)\\)\n        \\(\\widetilde{Q}=\\phi(Q), \\widetilde{K}=\\phi(K) \\quad \\triangleright\\) applied to each row\n        return \\(\\widetilde{Q}\\left(\\widetilde{K}^{\\top} V\\right), \\widetilde{Q}\\left(\\widetilde{K}^{\\top}\\right) 1_{n}\\)\n    end procedure\n    procedure \\(\\operatorname{SparseApprox}(Q, K, V, \\phi, \\mathcal{H})\\)\n        \\(\\mathcal{S}=\\left\\{(i, j) \\mid \\mathcal{H}\\left(Q_{i}\\right)=\\mathcal{H}\\left(K_{j}\\right)\\right\\}\\)\n        \\(S \\leftarrow\\) sparse matrix whose support is \\(\\mathcal{S}\\)\n        for \\((i, j) \\in \\mathcal{S}\\) do\n            \\(S_{i j}=\\exp \\left(q_{i}^{\\top} k_{j}\\right)-\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)\\). end for\n        return \\(S V, S 1_{n}\\). end procedure\n    procedure ScatterbrainApprox \\((Q, K, V)\\)\n        \\(\\phi, h \\leftarrow \\operatorname{Init}(m, k, l)\\). \\(O_{\\mathrm{lr}}, D_{\\mathrm{lr}} \\leftarrow \\operatorname{LowRankAprrox}(Q, K, V, \\phi)\\). \\(O_{\\mathrm{s}}, D_{\\mathrm{s}} \\leftarrow \\operatorname{SparseApprox}(Q, K, V, \\phi, h)\\). \\(\\operatorname{return} \\operatorname{diag}\\left(D_{\\mathrm{lr}}+D_{\\mathrm{s}}\\right)^{-1}\\left(O_{\\mathrm{lr}}+O_{\\mathrm{s}}\\right)\\). end procedure\n```\n\nAutoregressive / Causal / Unidirectional Attention To approximate autoregressive attention, we simply use the autoregressive variant of low-rank attention, and apply the autoregressive mask to the sparse attention. In particular, let $M \\in \\mathbb{R}^{n \\times n}$ be the autoregressive mask, whose lower triangle is all ones and the rest of the entries are zero. The unnormalized attention matrix is $\\exp \\left(\\left(Q K^{\\top}\\right) \\odot M\\right)$, and the unnormalized output is $\\exp \\left(\\left(Q K^{\\top}\\right) \\odot M\\right) V$, where $\\odot$ is elementwise multiplication. The low-rank autoregressive variant computes $\\left(\\left(\\widetilde{Q} \\widetilde{K}^{\\top}\\right) \\odot M\\right) V$, though with a custom GPU kernel / implementation so as not to materialize the $n \\times n$ matrix. For the sparse component, we simply mask out locations $S_{i j}$ where $i>j$. That is, we can perform $S \\odot M$ efficiently. As a result, we can compute the Scatterbrain output $\\left(\\left(\\widetilde{Q} \\widetilde{K}^{\\top}\\right) \\odot M\\right) V+(S \\odot M) V$ efficiently. ## D Proofs\n\n## D. 1 Expressiveness of Sparse + Low-rank Matrices\n\nTo motivate the use of sparse + low-rank matrices, we describe a family of attention matrices where sparse + low-rank matrices need asymptotically fewer parameters to approximate the attention matrix, compared to sparse or low-rank matrices alone. For there cases, either sparse or low-rank alone requires a quadratic number of parameters $\\left(O\\left(n^{2}\\right)\\right.$, where $n \\times n$ is the dimension of the attention matrix) to get $\\epsilon$ approximation error in Frobenius norm, while sparse + low-rank only requires $O(n)$ parameters. We construct a matrix family that shows the separation between the approximation capability of sparse + low-rank vs. sparse or low-rank alone. More specifically, we will use diagonal + low-rank (a special case of sparse + low-rank). Example 1. Let $\\epsilon$ denote a parameter that satisfies $\\epsilon \\in(0,1 / 2]$. Consider the following randomized construction of a matrix $Q \\in \\mathbb{R}^{n \\times d}$ with $d \\geq 6 \\epsilon^{-2} \\log n$ and $d=\\Theta\\left(\\epsilon^{-2} \\log n\\right)$, where each entry of $Q$ is picked independently and uniformly at random from $\\{ \\pm 1 / \\sqrt{d}\\}$. Let $M=\\sigma\\left(Q Q^{\\top}\\right)$ where $\\sigma$ is the elementwise exponential function (we first ignore the normalization term of softmax here). It can be shown (e.g. by Hoeffding's inequality) that with high probability\n\n$$\n\\left(Q Q^{\\top}\\right)_{i, j}= \\begin{cases}1, & \\text { if } i=j \\\\ \\epsilon[-\\epsilon, \\epsilon], & \\text { otherwise }\\end{cases}\n$$\n\nSince $M=\\sigma\\left(Q Q^{\\top}\\right)$ where $\\sigma$ is the elementwise exponential function,\n\n$$\nM_{i, j}= \\begin{cases}e, & \\text { if } i=j \\\\ \\in[1-O(\\epsilon), 1+O(\\epsilon)], & \\text { otherwise }\\end{cases}\n$$\n\nIntuitively, as the attention matrix $M$ has large diagonal entries, low-rank matrices will not be able to approximate it well. However, the off-diagonals are also of reasonable size, thus making sparse approximation difficult. With sparse + low-rank, we can use the sparse part to represent the diagonal, and the low-rank part to represent the remaining elements, allowing it to approximate this matrix well. We formalize this separation in the theorem below. Theorem 3. Let $M$ be the attention matrix from Example 1. For any $\\gamma \\in[0,1]$, with probability at least $1-n^{-1}$, there exists a sparse + low-rank estimator with $O\\left(\\gamma^{-1} n^{3 / 2} \\log n\\right)$ parameters that achieve $\\gamma \\sqrt{n}$ Frobenius error. For any matrix $R \\in \\mathbb{R}^{n \\times n}$ with rank such that $n-\\operatorname{rank}=\\Omega(n)$ (e.g., $R$ has o $\\left(n^{2}\\right)$ parameters), with probability at least $1-n^{-1}$, we have $\\|M-R\\|_{F} \\geq \\Omega(\\sqrt{n})$. Moreover, any matrix $E_{S}$ that has row sparsity $k$ (each row has less than $k$ non-zeros) such that $n-k=\\omega(1)$ (e.g., $E_{\\mathrm{S}}$ has o( $\\left.n^{2}\\right)$ parameters) will have error $\\left\\|M-E_{S}\\right\\|_{F} \\geq \\Omega(\\sqrt{n})$ with probability at least $1-n^{-1}$. We see that for any $\\gamma \\in[0,1]$, any low-rank or sparse estimator for $M$ with $\\left(n^{2}\\right)$ parameters has $\\Omega\\left(\\gamma^{-1}\\right)$ times the error of the sparse + low-rank estimator with $O\\left(\\gamma^{-1} n^{1.5} \\log n\\right)$ parameters.",
    "scatterbrain-42": "Proof of Theorem [3. For each $i \\in[n]$, let $q_{i}$ denote the $i$-th row of $Q \\in \\mathbb{R}^{n \\times d}$. Define $J \\in \\mathbb{R}^{n \\times n}$ to be the all 1s matrix. Define $T=M-J-Q Q^{\\top}$. Therefore,\n\n$$\nT_{i, j}= \\begin{cases}e-2 & \\text { if } i=j \\\\ e^{q_{i}^{\\top} q_{j}}-1-q_{i}^{\\top} q_{j} & \\text { otherwise }\\end{cases}\n$$\n\nBy Hoeffding's inequality, for a pair $i \\neq j$, we have that\n\n$$\n\\mathbb{P}\\left(\\left|q_{i}^{\\top} q_{j}-\\mathbb{E}\\left[q_{i}^{\\top} q_{j}\\right]\\right| \\geq \\epsilon\\right) \\leq 2 \\exp \\left(-\\frac{2 \\epsilon^{2}}{\\left(\\frac{1}{\\sqrt{d}}-\\frac{-1}{\\sqrt{d}}\\right)^{2}}\\right)=2 \\exp \\left(-d \\epsilon^{2} / 2\\right)\n$$\n\nNote that $\\mathbb{E}\\left[q_{i}^{\\top} q_{j}\\right]=0$\n\nBy a union bound over all pairs $i \\neq j$ (there are $n(n-1) / 2$ such pairs), with probability at least $1-n^{2} \\exp \\left(-d \\epsilon^{2} / 2\\right)$, we have that\n\n$$\nq_{i}^{\\top} q_{j} \\in[-\\epsilon, \\epsilon] \\text { for all } i \\neq j\n$$\n\nSince we assume that $d \\geq 6 \\epsilon^{-2} \\log n$, we have that\n\n$$\nn^{2} \\exp \\left(-d \\epsilon^{2} / 2\\right) \\leq n^{2} \\exp (-3 \\log n)=n^{-1}\n$$\n\nHence $q_{i}^{\\top} q_{j} \\in[-\\epsilon, \\epsilon]$ for all $i \\neq j$ with probability at least $1-n^{-1}$. For the rest of the proof, we only consider this case (where $q_{i}^{\\top} q_{j} \\in[-\\epsilon, \\epsilon]$ for all $i \\neq j$ ). Since $1+x \\leq e^{x} \\leq 1+x+x^{2}$ for $|x|<1$, we can bound the off diagonal elements $\\left|T_{i, j}\\right| \\leq \\epsilon^{2}$. In particular, for all $i \\neq j$,\n\n$$\n\\left|T_{i j}\\right|=\\left|e^{q_{i}^{\\top} q_{j}}-1-q_{i}^{\\top} q_{j}\\right| \\leq\\left(q_{i}^{\\top} q_{j}\\right) \\leq \\epsilon^{2}\n$$\n\nSparse + low-rank estimator: We use the following sparse + low-rank estimator:\n\n$$\nE_{\\mathrm{SL}}=\\underbrace{(e-2) \\cdot I}_{\\text {sparse }}+\\underbrace{J+Q Q^{\\top}}_{\\text {low }-\\mathrm{rank}}\n$$\n\nwhere $(e-2) I$ has row sparsity 1 and $\\operatorname{rank}\\left(J+Q Q^{\\top}\\right) \\leq d+1=O\\left(\\epsilon^{-2} \\log n\\right)$. Notice that the $E_{\\text {SL }}$ estimate matches $M$ exactly on the diagonal, and on the off-diagonal it differs from $M$ by $T_{i j}$. Thus, the Frobenious error of the sparse + low-rank estimator is\n\n$$\n\\left\\|M-E_{\\mathrm{SL}}\\right\\|_{F} \\leq \\epsilon^{2} \\sqrt{n(n-1)} \\leq \\epsilon^{2} n\n$$\n\nSet $\\epsilon=\\frac{\\sqrt{\\gamma}}{n^{1 / 4}}$ for $0 \\leq \\gamma \\leq 1$, Then\n(i) The sparse + low-rank parameter count is $n+n \\cdot \\operatorname{rank} \\leq n \\cdot O\\left(\\epsilon^{-2} \\log n\\right) \\leq O\\left(\\gamma^{-1} n^{1.5} \\log n\\right)$. (ii) The Frobenius error is $\\leq \\gamma \\sqrt{n}$. Low-rank estimator: We want to argue that low-rank approximation would require more parameters. If we approximate the matrix $(e-2) I$ by a matrix $R$ with rank $r$, then the difference matrix will have at least $n-d$ singular values of magnitude $e-2 \\geq 1 / 2$. As a result, by the Eckart-Young-Mirsky theorem,\n\n$$\n\\|(e-2) \\cdot I-R\\|_{F} \\geq \\frac{1}{2} \\sqrt{n-r}\n$$\n\nDefine $T^{\\prime}=T-(e-2) \\cdot I$, then $T^{\\prime}$ is all 0 on the diagonal and has absolute value $\\leq \\epsilon^{2}$ on off-diagonal entries. Thus $\\left\\|T^{\\prime}\\right\\|_{F} \\leq \\epsilon^{2} n=\\gamma \\sqrt{n}$. We want to show that if $R^{\\prime}$ is a rank $r^{\\prime}$ matrix, then $\\left\\|M-R^{\\prime}\\right\\|_{F} \\geq \\frac{1}{2} \\sqrt{n-r^{\\prime}-d-1}-\\left\\|T^{\\prime}\\right\\|_{F}$. We argue by contradiction. Suppose that there exists some matrix $R^{\\prime}$ with rank $r^{\\prime}$ such that\n\n$$\n\\left\\|M-R^{\\prime}\\right\\|_{F} \\leq \\frac{1}{2} \\sqrt{n-r^{\\prime}-d-1}-\\left\\|T^{\\prime}\\right\\|_{F}\n$$\n\nDefine $R=R^{\\prime}-J-Q Q^{\\top}$, so $M-R^{\\prime}=(e-2) \\cdot I-R+T^{\\prime}$. We see that:\n\n$$\n\\begin{aligned}\n\\|(e-2) \\cdot I-R\\|_{F} & =\\left\\|M-R^{\\prime}-T^{\\prime}\\right\\|_{F} \\\\\n& \\leq\\left\\|M-R^{\\prime}\\right\\|_{F}+\\left\\|T^{\\prime}\\right\\|_{F} \\\\\n& \\leq \\frac{1}{2} \\sqrt{n-r^{\\prime}-d-1} \\\\\n& \\leq \\frac{1}{2} \\sqrt{n-\\operatorname{rank}(R)}\n\\end{aligned}\n$$\n\nThis contradicts the result above, which states that $\\|(e-2) \\cdot I-R\\|_{F} \\geq \\frac{1}{2} \\sqrt{n-\\operatorname{rank}(R)}$. Therefore any low-rank estimator with rank $r$ such that $n-r=\\Omega(n)$, which has $\\Omega\\left(n^{2}\\right)$ parameters, will have error at least $\\Omega(\\sqrt{n-r-d-1})-\\left\\|T^{\\prime}\\right\\|_{F}=\\Omega(\\sqrt{n})$, which is $\\Omega\\left(\\gamma^{-1}\\right)$ times the error of the sparse + low-rank estimator above. Sparse estimator: For our sparse estimator, it is easy to see that for any $E_{\\mathrm{S}} \\in \\mathbb{R}^{n \\times n}$ that has row sparsity $k$ (each row has fewer than $k$ non-zeros),\n\n$$\n\\left\\|M-E_{\\mathrm{S}}\\right\\|_{F} \\geq \\Omega(\\sqrt{n(n-k)})\n$$\n\nThis implies that in order to achieve error $O(\\sqrt{n})$, we would need $n-k=O(1)$, which requires $\\Omega\\left(n^{2}\\right)$ parameters. Now we construct a matrix that shows better separation between the approximation capability of sparse + low-rank vs sparse or low-rank alone. Example 2. Consider the following randomized construction of matrix $Q \\in \\mathbb{R}^{n \\times d}$ with $d \\geq 6 \\epsilon^{-2} r \\log n$ and $d=\\Theta\\left(\\epsilon^{-2} r \\log n\\right)(\\epsilon \\in(0,1]$ and close to 0 and $r$ is $\\Theta(\\log n))$ : each entry of $Q$ is picked independently and uniformly at random from $\\{ \\pm \\sqrt{r / d}\\}$. Let $M=\\sigma\\left(Q Q^{\\top}\\right)$ where $\\sigma$ is the elementwise exponential function. Similar to Example 1, with high probability, we have:\n\n$$\n\\left(Q Q^{\\top}\\right)_{i, j}= \\begin{cases}r, & \\text { if } i=j \\\\ \\in[-\\epsilon, \\epsilon], & \\text { otherwise }\\end{cases}\n$$\n\nWe also have:\n\n$$\nM_{i, j}= \\begin{cases}e^{r}, & \\text { if } i=j \\\\ \\in[1-O(\\epsilon), 1+O(\\epsilon)], & \\text { otherwise }\\end{cases}\n$$\n\nBy setting $r$ appropriately, we can formalize the separation between the approximation ability of sparse, low-rank, and sparse + low-rank matrices:\n\nTheorem 4. Let $M$ be the attention matrix from Example 2. Any sparse or low-rank estimator of $M$ needs $\\Omega\\left(n^{2}\\right)$ parameters for $O(n)$ error with probability at least $1-n^{-1}$ while a sparse + low-rank estimator needs $O(n)$ parameters for $O(n)$ error with probability at least $1-n^{-1}$. Proof of Theorem 4. Similar to the proof of Theorem 3, by Hoeffding's inequality, for a pair $i \\neq j$, we have that\n\n$$\n\\mathbb{P}\\left(\\left|q_{i}^{\\top} q_{j}-\\mathbb{E}\\left[q_{i}^{\\top} q_{j}\\right]\\right| \\geq \\epsilon\\right) \\leq 2 \\exp \\left(-\\frac{2 \\epsilon^{2}}{\\left(\\frac{r}{\\sqrt{d}}-\\frac{-r}{\\sqrt{d}}\\right)^{2}}\\right)=2 \\exp \\left(-\\frac{d \\epsilon^{2}}{2 r}\\right)\n$$\n\nNote that $\\mathbb{E}\\left[q_{i}^{\\top} q_{j}\\right]=0$. By a union bound over all pairs $i \\neq j$ (there are $n(n-1) / 2$ such pairs), with probability at least $1-n^{-1}$ (since $d \\geq 6 \\epsilon^{-2} r \\log n$ ), we have that\n\n$$\nq_{i}^{\\top} q_{j} \\in[-\\epsilon, \\epsilon] \\quad \\text { for all } i \\neq j\n$$\n\nSince we assume that $d \\geq 6 \\epsilon^{-2} \\log n$, we have that For the rest of the proof, we only consider this case (where $q_{i}^{\\top} q_{j} \\in[-\\epsilon, \\epsilon]$ for all $\\left.i \\neq j\\right)$. Let $T=M-\\left(e^{r}-1\\right) \\cdot I+J$, where $J$ is the all one matrix. We see that $T$ is zero on the diagonal. Moreover, using the fact that $e^{x} \\leq 1+2|x|$ for all $x \\in[-1,1]$, the off-diagonal entries of $T$ have of magnitude at most $2 \\epsilon$. We consider 3 different estimators. Sparse + low-rank estimator: Our estimator is\n\n$$\nE_{\\mathrm{SL}}=\\underbrace{\\left(e^{r}-1\\right) \\cdot I}_{\\text {sparse }}+\\underbrace{J}_{\\text {low }-\\mathrm{rank}}\n$$\n\nwhere $(e-1) I$ has row sparsity 1 and $\\operatorname{rank}(J)=1$. The Frobenious error of sparse + low-rank approximation is\n\n$$\n\\left\\|M-E_{\\mathrm{SL}}\\right\\|_{F} \\leq O\\left(\\sqrt{\\epsilon^{2} n(n-1)}\\right) \\leq O(\\epsilon n)\n$$\n\nWe have that:\n(i) Sparse + low-rank parameter count is $n \\cdot(1+1) \\leq O(n)$. (ii) Its Frobenius error is $\\leq O(n)$. Low-rank estimator: We want to argue that low-rank approximation would require more parameters. From a similar observation that any matrix $R$ with rank that $n-\\operatorname{rank}=\\Omega(1)$,\n\n$$\n\\left\\|\\left(e^{r}-1\\right) I-R\\right\\|_{F} \\geq \\Omega\\left(e^{r}\\right)\n$$\n\n(by Eckart-Young-Mirsky theorem), we obtain a similar result to the proof of Theorem 3 . If $R^{\\prime}$ is a matrix with rank such that $n-\\operatorname{rank}=\\Omega(1)$, then $\\left\\|M-R^{\\prime}\\right\\|_{F} \\geq \\Omega(n)-\\|T\\|_{F} \\geq \\Omega(n)-O(\\epsilon n) \\geq$ $\\Omega(n)$. Hence any low-rank matrix with $O\\left(n^{2}\\right)$ parameters would have error $\\Omega(n)$. Sparse estimator: Similar to the proof of Theorem 3, for our sparse estimator, it is easy to see that for any $E_{\\mathrm{S}} \\in \\mathbb{R}^{n \\times n}$ that has row sparsity $k$ (each row has fewer than $k$ non-zeros),\n\n$$\n\\left\\|M-E_{\\mathrm{S}}\\right\\|_{F} \\geq \\Omega(\\sqrt{n(n-k)})\n$$\n\nThis implies that to get $O(n)$ error, we would need $\\Omega\\left(n^{2}\\right)$ parameters. ## D. 2 Generative Model, Softmax Temperature, and Matrix Approximation\n\nHere we show 3 cases where depending on the softmax temperature, either we'll need low-rank, low-rank + sparse, or sparse to approximate the attention matrix.",
    "scatterbrain-43": "We start with some notation first. Given a matrix $B$, let $B[i, j]$ be the entry in the $i$ th row and $j$ th column. For a range $[l, r]$, we define a matrix $B_{[l, r]}$ where $B_{[l, r]}[i, j]=B[i, j]$ if $B[i, j] \\in[l, r]$ and $B_{[l, r]}=0$ otherwise (that is, $B_{[l, r]}$ only keep entries for $B$ that are in the range $[l, r]$, with other entries zeroed out). We write $\\operatorname{supp}(C)$ for the set of locations of non-zeros in $C$. We let $\\lambda_{i}(D)$ be the $i$-th largest (in absolute value) eigenvalue of $D$. To prove Theorem 1, we first define a more general matrix class, prove that the attention matrix in Process 1 is a subset of this class (with high probability), and then show that Theorem 1holds for this more general class. We introduce an extra parameter $l \\in \\mathbb{R}$, in addition to the inverse temperature $\\beta$ and the intro-cluster distance $\\Delta$. Matrix Class 1. Let $Q \\in \\mathbb{R}^{n \\times d}$ with every row of $Q$ having $\\ell_{2}$-norm in $[1-O(\\Delta), 1+O(\\Delta)]$, and let $A=Q Q^{\\top}$. Further:\n\n1. Let $H=A_{[1 / l, 2-1 / l]}$ for some $l \\geq \\Omega(1)$. Assume that $H$ is block diagonal with $\\Omega(n)$ blocks, and $\\operatorname{supp}(H)$ is o( $\\left.n^{2}\\right)$. That is, the large entries of $Q Q^{\\top}$ form a block diagonal matrix. 2. Let $L=A-H$ then $L=A_{[-\\Delta, \\Delta]}$ where $\\Delta=o(1 / \\log d)$. Assume that there is a constant fraction of elements in $\\operatorname{supp}(L)$ falling in $[0, \\Delta]$. Assume that $\\operatorname{supp}\\left(A_{[0, \\Delta]}\\right)$ is $\\Omega\\left(n^{2}\\right)$. Let $M_{\\beta}=\\exp (\\beta \\cdot A)$. We now show that Process 1 is a subset of Matrix Class 1, with high probability. Lemma 5. The matrix $M_{\\beta}$ in Process 1 is a subset of Matrix Class 1 , where $l=\\frac{1}{1-\\Delta^{2}}$. Proof. We first bound the norm of each row in $Q$ in Process 1. For any $i, j$, we have\n\n$$\n\\left\\|z_{i j}\\right\\|^{2}=\\left\\|c_{i}+r_{i j}\\right\\|^{2}=\\left\\|c_{i}\\right\\|^{2}+2 c_{i}^{\\top} r_{i j}+\\left\\|r_{i j}\\right\\|^{2}\n$$\n\nSince $c_{i} \\sim \\mathcal{N}\\left(0, I_{d} / \\sqrt{d}\\right),\\left\\|c_{i}\\right\\|^{2} \\in\\left[1-\\Delta^{2}, 1+\\Delta^{2}\\right]$ with probability at least $1-2 e^{-d \\Delta^{2} / 8}$ (by the standard argument using the fact that $\\chi^{2}$-random variables are sub-exponential). Similarly, $\\left\\|r_{i j}\\right\\|^{2} \\in\\left[\\Delta^{2}-\\Delta^{4}, \\Delta^{2}+\\Delta^{4}\\right]$ with probability at least $1-2 e^{-d \\Delta^{2} / 8}$. By concentration of measure, we can also bound $2 c_{i}^{\\top} r_{i j} \\in[2 \\Delta-$ $\\left.2 \\Delta^{3}, 2 \\Delta+2 \\Delta^{3}\\right]$ as well. Therefore, we have that $\\left\\|z_{i j}\\right\\|^{2} \\in[1-O(\\Delta), 1+O(\\Delta)]$. Now we show that the large entries of $Q Q^{\\top}$ form a block diagonal matrix. With high probability, the large entries come from intra-cluster dot product, and the small entries come from inter-cluster dot product. We bound the intra-cluster dot product:\n\n$$\n\\begin{aligned}\nz_{i j}^{\\top} z_{i k} & =\\left(c_{i}+r_{i j}\\right)^{\\top}\\left(c_{i}+r_{i k}\\right) \\\\\n& =\\left\\|c_{i}\\right\\|^{2}+c_{i}^{\\top} r_{i j}+c_{i}^{\\top} r_{i k}+r_{i j}^{\\top} r_{i k}\n\\end{aligned}\n$$\n\nSimilar to the argument above, by concentration of measure, $\\left\\|c_{i}\\right\\|^{2} \\in[1+\\epsilon \\Delta, 1-\\epsilon \\Delta]$ with high probability (we will pick $\\epsilon=\\theta(\\Delta)$ ). The cross terms $c_{i}^{\\top} r_{i j}$ and $c_{i}^{\\top} r_{i k}$ can be bounded using Cauchy-Schwarz inequality to be in $[-\\epsilon \\Delta, \\epsilon \\Delta]$ with high probability. And the fourth term $r_{i j}^{\\top} r_{i k}$ is in $\\left[-\\epsilon \\Delta^{2}, \\epsilon \\Delta^{2}\\right]$ with high probability. Therefore, the inner product is in $1 \\pm O(\\epsilon \\Delta)$ with high probability. This satisfies the first condition in Matrix Class 1 , for $l=\\frac{1}{1-\\Delta^{2}}$, assuming $\\epsilon \\leq \\Delta$. We use a similar argument to bound the inter-cluster dot product. For $i \\neq i^{\\prime}$\n\n$$\n\\begin{aligned}\nz_{i j}^{\\top} z_{i^{\\prime} k} & =\\left(c_{i}+r_{i j}\\right)^{\\top}\\left(c_{i^{\\prime}}+r_{i^{\\prime} k}\\right) \\\\\n& =c_{i}^{\\top} c_{i^{\\prime}}^{\\top}+c_{i}^{\\top} r_{i^{\\prime} k}+c_{i^{\\prime}}^{\\top} r_{i j}+r_{i j}^{\\top} r_{i^{\\prime} k}\n\\end{aligned}\n$$\n\nBy concentration of measure, $c_{i}^{\\top} c_{i^{\\prime}} \\in[-\\epsilon, \\epsilon]$. Similar to the argument in the intra-cluster case, we can bound the other three terms, so this dot product is in $[-O(\\epsilon), O(\\epsilon)]$. This satisfies the second condition in Matrix Class 1 . To prove Theorem 1 for Matrix Class 1 we start with some technical lemmas. Lemma 6. Let $F \\in \\mathbb{R}_{\\geq 0}^{N \\times N}$ be a symmetric matrix. Let $\\lambda_{\\max }$ be the largest eigenvalue of $F$. Assuming $N \\geq 2$, we have that\n\n$$\n\\lambda_{\\max } \\geq \\min _{i \\neq j} F[i, j]\n$$\n\nProof. Since $F$ is symmetric, $\\lambda_{\\max }$ is real and\n\n$$\n\\lambda_{\\max }=\\max _{u \\neq 0} \\frac{u^{\\top} F u}{u^{T} u}\n$$\n\nLet $u$ be the all 1's vector, then\n\n$$\n\\begin{aligned}\n\\lambda_{\\max } & \\geq \\frac{1}{N} \\sum_{i=j} F[i, j] \\\\\n& \\geq \\frac{1}{N} \\sum_{i \\neq j} F[i, j] \\\\\n& \\geq \\frac{1}{N} \\cdot N(N-1) \\min _{i \\neq j} F[i, j] \\\\\n& \\geq \\min _{i \\neq j} F[i, j]\n\\end{aligned}\n$$\n\nwhere the second step follows from all the diagonal entries are non-negative, the last step follows from $N \\geq 2$\n\nThe above implies the following result:\nCorollary 7. Let $F \\in \\mathbb{R}_{\\geq 0}^{N \\times N}$ be a block diagonal matrix. Let $r$ be the number of $m \\times m$ blocks in $F$ for some $m \\geq 2$. The $\\lambda_{r}(F)$ is at least the smallest non-diagonal element in any $m \\times m$ block ( $m \\geq 2$ ) in $F$. Proof. By Lemma 6, each $m \\times m$ block $B(m \\geq 2)$ by itself has max eigenvalue at least $\\min _{i \\neq j \\in[m]} B[i, j]$. The claim then follows from the fact that any eigenvalue of $B$ is also an eigenvalue of $F$. We'll need the following function for our low-rank argument:\n\n$$\nf_{k}(x)=\\sum_{i=0}^{k} \\frac{x^{i}}{i!}\n$$\n\nNote that $f_{\\infty}(x)=e^{x}$. Definition 1. Let $\\epsilon \\in(0,1 / 10)$ and $L>0$. We say a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is $(\\epsilon, L)$-close to $e^{y}$ if\n\n$$\n\\left|e^{y}-f(y)\\right| \\leq \\epsilon \\quad \\text { for any } y \\in[-L, L]\n$$\n\nLemma 8. For any $\\epsilon \\in(0,1 / 10)$ and $L>0$. If\n\n$$\nD \\geq 10(L+\\log (1 / \\epsilon))\n$$\n\nthen function $f_{D}(y)$ is $(\\epsilon, L)$-close to $e^{y}$. Proof. Recall the definition of function $f_{D}$,\n\n$$\ne^{x}=f_{D}(x)+\\sum_{i=D+1}^{\\infty} \\frac{x^{i}}{i!}\n$$\n\nIt is sufficient to show that $\\left|e^{y}-f(y)\\right|<\\epsilon$ if we have\n\n$$\n\\frac{x^{D+1}}{(D+1)!} \\leq \\frac{\\epsilon}{2}\n$$\n\nWe can show that\n\n$$\n\\begin{aligned}\n\\frac{y^{D}}{D!} & \\leq \\frac{L^{D}}{D!} \\\\\n& \\leq \\frac{L^{D}}{(D / 4)^{D}} \\\\\n& =\\left(\\frac{4 L}{D}\\right)^{D} \\\\\n& \\leq(1 / 2)^{D} \\\\\n& \\leq \\epsilon / 10\n\\end{aligned}\n$$\n\nwhere the first step follows from $|y| \\leq L$, the second step follows $n!\\geq(n / 4)^{n}$, the forth step follows from $D \\geq 10 L$, the last step follows $D \\geq 10 \\log (1 / \\epsilon)$ and $\\epsilon \\in(0,1 / 10)$. We'll also use the following fact:\nLemma 9. For any $D=o(\\log n / \\log d)$, we have\n\n$$\n\\operatorname{rank}\\left(f_{D}\\right) \\leq n^{o(1)}\n$$\n\nProof. We can upper bound $\\operatorname{rank}\\left(f_{D}(A)\\right)$ in the following sense:\n\n$$\n\\begin{aligned}\n\\operatorname{rank}\\left(f_{D}(A)\\right) & \\leq(\\operatorname{rank}(A))^{D} \\\\\n& \\leq d^{D} \\\\\n& =2^{D \\cdot \\log d} \\\\\n& =2^{o(\\log n)} \\\\\n& =n^{o(1)}\n\\end{aligned}\n$$\n\nwhere the second step follows from $\\operatorname{rank}(A) \\leq d$, the forth step follows from $D=o\\left(\\frac{\\log n}{\\log d}\\right)$. Finally we're ready to prove the theorem:\nProof. The basic idea is: (i) Use $f_{k^{*}}(b \\cdot A)$ to get the low-rank approximation (ii) Use $\\exp (b \\cdot H)$ to get the sparse part. Small $\\beta$ range, i.e., $\\beta$ is $o\\left(\\frac{\\log n}{\\log d}\\right)$. Low rank approximation: $R=f_{k^{*}}(b \\cdot A)$. Since each entry of $A$ is in $[-1,1]$, each entry of $\\beta \\cdot A$ is in $[-\\beta, \\beta]$. But note that $\\beta$ in this case is $o\\left(\\frac{\\log n}{d}\\right)=O(\\log n \\cdot \\Delta)$. By the definition of $k^{*}$, each entry of $\\exp (\\beta \\cdot A)-f_{k^{*}}(\\beta \\cdot A)$ has absolute value $\\leq \\epsilon$ Therefore the overall error is $\\leq \\epsilon n$. For sparse only: By assumption, $m=\\Omega\\left(\\|L\\|_{0}\\right)$ entries in $A$ are $\\geq 0$, which are exactly the entries in $\\exp (\\beta \\cdot A)$ that are $\\geq 1$. Hence any (say) $\\frac{m}{2}$ sparse approximation has error $\\geq \\sqrt{\\frac{m}{2}} \\geq \\Omega\\left(\\sqrt{\\|L\\|_{0}}\\right)$. By our assumption, $\\|L\\|_{0}=\\Omega\\left(n^{2}\\right)$\n\nMid-range $\\beta$, i.e., $\\beta \\geq \\frac{1}{l} \\cdot \\log n$ and $\\beta$ is $O(\\log n)$. Sparse only: the argument is the same as in the low $\\beta$ range. Sparse + low-rank: The low-rank part $R=f \\operatorname{st}(\\beta \\cdot A)$. By Lemma 9, this has rank $n^{o(1)}$, so it has $n^{(1+o(1))}$ parameters. The sparse part is $S=e^{\\beta \\cdot H}-R_{\\operatorname{supp}(H)}$. Clearly this needs $|\\operatorname{supp}(H)|$ parameters. Let $E=M_{\\beta}-(S+R)$. Then (i) in $\\operatorname{supp}(H), E$ is all 0 . (ii) output of $\\operatorname{supp}(H)$, by definition, entries of $\\beta \\cdot A$ are in $[-\\beta \\Delta, \\beta \\Delta]$, which in the current range of $\\beta$ is $[-O(\\log n \\Delta), O(\\log n \\Delta)]$. Therefore all the entries of $E$ have absolute value $\\leq \\epsilon$. By the definition of $k^{*}$, we have that $\\|E\\|_{F} \\leq \\epsilon n$. Low-rank only: Let $\\widetilde{R}$ be rank $r-n^{o(1)}-1$ that approximates $M_{\\beta}$. Then using the same argument as our existing lower bound argument, we get that $\\widetilde{R}-R \\approx_{E} S$ (this means that the error $\\leq\\|E\\|_{F}+\\left\\|M_{\\beta}-\\widetilde{R}\\right\\|_{F}$ ). Now note that $S=e^{\\beta \\cdot H}-\\left(f_{k^{*}}(\\beta \\cdot A)\\right)_{\\operatorname{supp} H}$ is a symmetric, block diagonal matrix with $r=\\Omega(n)$ blocks. Corollary 7 implies that $\\lambda_{r}(S)$ is at least the smallest non-diagonal value in $S$. Now the smallest non-diagonal value in $e^{\\beta \\cdot H}$ is $\\geq e^{\\frac{1}{l} \\log n}=n$. On the other hand, the largest value in $\\left(f_{k^{*}}(\\beta \\cdot A)\\right)_{\\operatorname{supp} H}$ is\n\n$$\n\\begin{aligned}\n& \\leq k^{*} \\frac{\\beta^{k^{*}}}{k^{*}!} \\leq \\beta \\cdot\\left(\\frac{e \\beta}{k^{*}-1}\\right)^{k^{*}-1} \\\\\n& \\lesssim \\log n\\left(\\frac{e \\cdot \\log n}{\\log n \\cdot \\Delta}\\right)^{O(\\log n \\cdot \\Delta)} \\\\\n& \\lesssim \\log n e^{O\\left(\\log n \\cdot \\Delta \\cdot \\log \\frac{1}{\\Delta}\\right)} \\\\\n& \\lesssim \\log n \\cdot n^{o(1)} \\\\\n& =n^{o(1)}\n\\end{aligned}\n$$\n\nHence $\\lambda_{r}(S)$ is $\\Omega(n)$. The claimed result then follows since $\\|E\\|_{F} \\leq \\epsilon n$ and $\\operatorname{rank} \\widetilde{R}-R \\leq r-1$ (EckartYoung-Mirsky theorem). Large $\\beta$ range, i.e., $\\beta \\geq \\omega(\\log n)$. Sparse only: $S=e^{\\beta \\cdot H}$. Note that each entry in $E=M_{\\beta}-S$ is upper bounded by $e^{\\Delta \\cdot \\beta} \\leq e^{o\\left(\\frac{\\beta}{\\log d}\\right)}$. Then\n\n$$\n\\begin{aligned}\n\\|E\\|_{F} & \\leq n \\cdot e^{o\\left(\\frac{\\beta}{\\log d}\\right)} \\\\\n& \\leq \\epsilon \\cdot e^{\\log \\frac{n}{\\epsilon}+o\\left(\\frac{\\beta}{\\log d}\\right)} \\\\\n& \\leq \\epsilon \\cdot e^{o(\\beta)+o\\left(\\frac{\\beta}{\\log d}\\right)} \\\\\n& \\leq \\epsilon \\cdot e^{o(\\beta)} \\\\\n& \\leq \\epsilon \\cdot e^{\\beta / l}\n\\end{aligned}\n$$\n\nLow-rank only: since $\\|E\\|_{F}$ is $\\leq \\epsilon e^{\\beta / l}$, it is enough to argue that any rank $r$-approximation to $S$ has error $\\geq e^{\\beta / l}$. But the latter follows since $\\lambda_{r}(S) \\geq e^{\\beta / l}$. This is because $e^{b \\cdot H}$ is symmetric and each entry in $H$ is $\\geq \\frac{1}{\\lambda}$. Then we can use Corollary 7 . Eckart-Young-Mirsky then completes the proof. ## D. 3 Scatterbrain: Analysis\n\nHere we prove Theorem 2, which shows that Scatterbrain approximation is unbiased and analyses its variance.",
    "scatterbrain-44": "We restate the theorem here for the reader's convenience. Theorem. Define $\\sigma(q, k)=\\exp \\left(q^{\\top} k\\right)$, $\\widehat{\\sigma}^{\\text {pfe }}$ as Performer's estimator and $\\widehat{\\sigma}^{\\text {sbe }}$ as Scatterbrain estimator. Denote $\\mathcal{S}^{d-1} \\subset \\mathbb{R}^{d}$ as the unit sphere. Suppose $q, k \\in S^{d-1}$ are such that $\\|q-k\\|<\\tau$. Then:\n\n$$\n\\mathbb{E}\\left[\\hat{\\sigma}^{\\mathrm{sbe}}(q, k)\\right]=\\sigma(q, k), \\quad \\operatorname{Var}\\left[\\hat{\\sigma}^{\\mathrm{sbe}}(q, k)\\right]=(1-p) \\cdot \\operatorname{Var}\\left[\\hat{\\sigma}^{\\mathrm{pe}}(q, k)\\right]<\\operatorname{Var}\\left[\\hat{\\sigma}^{\\mathrm{pfe}}(q, k)\\right]\n$$\n\nwhere $p=\\exp \\left(-\\frac{\\tau^{2}}{4-\\tau^{2}} \\ln d-O_{\\tau}(\\ln \\ln d)\\right)$. Proof. Let $A_{i j}=\\exp \\left(q_{k}^{\\top} k_{j}\\right)$ be $i j$-entry of the unnormalized attention matrix, $A_{i j}^{\\mathrm{lr}}=\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)$ the entry of the low-rank approximation (Performer), and let $A_{i j}^{\\text {sb }}$ be the entry of the Scatterbrain (sparse + low-rank) approximation. By the construction of the Scatterbrain attention matrix (Eq. (11), if ij $\\mathcal{S}$, where $\\mathcal{S}$ is the set of indices selected by the LSH, then:\n\n$$\nA_{i j}^{\\mathrm{sb}}=\\left(\\widetilde{Q} \\widetilde{K}^{\\top}+S\\right)_{i j}=\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)+\\exp \\left(q_{i}^{\\top} k_{j}\\right)-\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)=\\exp \\left(q_{i}^{\\top} k_{j}\\right) . $$\n\nIf $i j \\notin \\mathcal{S}$, then\n\n$$\nA_{i j}^{\\mathrm{sb}}=\\left(\\widetilde{Q} \\tilde{K}^{\\top}+S\\right)_{i j}=\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)+0=\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)\n$$\n\nIn other words, $A^{\\text {sb }}$ matches $A$ on the indices in $\\mathcal{S}$, and matches $A^{\\text {lr }}$ on the indices not in $\\mathcal{S}$. To show that $A^{\\text {sb }}$ is an unbiased estimator of $A$, we simply use the fact that $A^{\\mathrm{lr}}$ is also an unbiased estimator of $A$ [17, Lemma 1]:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[A_{i j}^{\\mathrm{sb}}\\right] & =\\mathbb{P}(i j \\in \\mathcal{S}) \\mathbb{E}\\left[A_{i j} \\mid i j \\in \\mathcal{S}\\right]+\\mathbb{P}(i j \\notin \\mathcal{S}) \\mathbb{E}\\left[A_{i j}^{\\mathrm{lr}} \\mid i j \\notin \\mathcal{S}\\right] \\\\\n& =\\mathbb{P}(i j \\in \\mathcal{S}) A_{i j}+\\mathbb{P}(i j \\notin \\mathcal{S}) A_{i j} \\\\\n& =A_{i j}\n\\end{aligned}\n$$\n\nIn other words, $\\mathbb{E}\\left[{ }^{\\mathrm{\\sigma}} \\mathrm{sbe}(q, k)\\right]=\\sigma(q, k)$. Now we analyze the per-entry variance of $A^{\\mathrm{sb}}$. Since $A^{\\mathrm{sb}}$ is an unbiased estimator of $A$, by the law of total variance,\n\n$$\n\\begin{aligned}\n\\mathbb{V} \\backslash \\backslash\\left(A_{i j}^{\\mathrm{sb}}\\right) & =\\mathbb{P}(i j \\in \\mathcal{S}) \\mathbb{V} \\supset\\left(A_{i j} \\mid i j \\in \\mathcal{S}\\right)+\\mathbb{P}(i j \\notin \\mathcal{S}) \\mathbb{V} \\backslash\\left(A_{i j}^{\\mathrm{lr}} \\mid i j \\notin \\mathcal{S}\\right) \\\\\n& =\\mathbb{P}(i j \\in \\mathcal{S}) \\cdot 0+\\mathbb{P}(i j \\notin \\mathcal{S}) \\mathbb{V} D \\backslash\\left(A_{i j}^{\\mathrm{lr}}\\right) \\\\\n& =\\mathbb{P}(i j \\notin \\mathcal{S}) \\mathbb{V} D\\left(A_{i j}^{\\mathrm{lr}}\\right) . \\end{aligned}\n$$\n\nTo compute the probability that the index $i j$ is not in $\\mathcal{S}$ (i.e., not selected by LSH), we use the standard bound on cross-polytope LSH [3, Theorem 1]:\n\n$$\np:=\\mathbb{P}(i j \\in \\mathcal{S})=\\exp \\left(-\\frac{\\tau^{2}}{4-\\tau^{2}} \\ln d-O_{\\tau}(\\ln \\ln d)\\right)\n$$\n\nTherefore,\n\n$$\n\\mathbb{V} \\supset \\backslash\\left(A_{i j}^{\\mathrm{sb}}\\right)=(1-p) \\mathbb{V} \\supset \\backslash\\left(A_{i j}^{\\mathrm{lr}}\\right)<\\mathbb{V} \\supset \\backslash\\left(A_{i j}^{\\mathrm{lr}}\\right)\n$$\n\nIn other words, $\\mathbb{V} \\supset \\backslash\\left[\\hat{\\sigma}^{\\text {sbe }}(q, k)\\right]=(1-p) \\cdot \\mathbb{V} \\supset \\backslash\\left[\\widehat{\\sigma}^{\\mathrm{pfe}}(q, k)\\right]<\\mathbb{V} \\supset \\backslash\\left[\\hat{\\sigma}^{\\mathrm{pfe}}(q, k)\\right]$. More explicitly, by plugging in the variance of $A^{\\mathrm{lr}}$ [17, Lemma 2], we have\n\n$$\n\\mathbb{V} \\backslash\\left(A_{i j}^{\\mathrm{sb}}\\right)=(1-p) \\frac{1}{m} \\exp \\left(\\left\\|q_{i}+k_{j}\\right\\|^{2}\\right) \\exp \\left(2 q_{i}^{\\top} k_{j}\\right)\\left(1-\\exp \\left(-\\left\\|q_{i}+k_{j}\\right\\|^{2}\\right)\\right)\n$$\n\nwhere $p=\\exp \\left(-\\frac{\\tau^{2}}{4-\\tau^{2}} \\ln d-O_{\\tau}(\\ln \\ln d)\\right)$\n\n## E Additional Experiments and Details\n\n## E. 1 Datasets\n\nImageNet [25]: ImageNet is one of the most widely-used image classification benchmarks. In our experiments in Section 5.1 of evaluating the approximation accuracy of Scatterbrain, both BigGAN and Vision Transformer are pre-trained on this dataset. It has roughly 1.2 million training images and 50,000 validation images. WikiText103 [45] and Copy [36]: WikiText103 is a popular dataset for auto-regressive models. It is from a collection of over 100 million tokens extracted from the set of verified good and featured articles on Wikipedia. It has 28,475 training articles, 60 for validation and 60 for testing. Copy is a synthetic a synthetic sequence duplication task where inputs are of the form $0 w 0 \\mathrm{w}$ and $w \\in\\{0, \\ldots, N\\}^{*}$. It is previously used in [15, 36]. This task is useful for demonstrating the effectiveness of long range attention: it requires non-local attention lookups. It cannot be solved by any model relying on sparse attention with a limited range such as, local attention. Long Range Arena (LRA) [57]: This is a recent benchmark for evaluating efficient transformers with long input sequence. We used ListOps [46, byte-level IMDb reviews text classification 44, byte-level document retrieval [48], image classification on sequences of pixels [37] and Pathfinder [40]. We follow the same evaluation mechanism from [57] but implement our own version in Pytorch (like data loader). GIUE [64]: GLUE is a standard multi-task benchmark in NLP. It has single-sentence tasks, CoLA and SST-2; similarity and paraphrase tasks, MRPC, STS-B, QQP; and inference tasks, MNLI, QNLI, RTE and WNLI. For our additional experiments below (not enough space to be included in the main paper), we follow the tradition from [22, 26, 68] and truncate all the input sequences to 128 tokens. ## E. 2 Settings\n\nBigGAN: We adapt the same pre-trained BigGAN model from 22 with no additional training. The model has a single attention layer at resolution $64 \\times 64$ (4096). Similar to the prior work, we also replace its full attention layer with Scatterbrain at the same resolution. Figure 5 in the main paper shows the best-effort comparison with $[1 / 32,1 / 16,1 / 8,1 / 4,1 / 2]$ of the parameter budget. For example, if given parameter budget $1 / 2$, we report the best performance of Smyrf from choice of $32 / 64 / 128$ hash round $64 / 32 / 16$ cluster size. T2-ViT: We use the pre-trained vision transformer model T2T-ViT-14 from [69] with $224 \\times 224$ image size. Without any additional training, we just replace the attention layer with Scatterbrain and other baselines and evaluate the approximation error and classification accuracy on ImageNet testings. Again, we report the best-effort best performance of each approximation given the certain parameter budget. Auto-regressive Model: We follow the settings from the popular repo https://github.com/NVIDIA/ DeepLearningExamples for training vanilla Transformer from scratch on WikiText103, except for chunking WikiText103 into sequence length 1024 in order to simulate long input sequences. The model is 16 layer with 8 head and 512 model dimension. We train all the models for 30 epochs and report the best Testing Perplexity. The model we use for Copy task is simply a 2-layer-4-head transformer and sequence length is also 1024. We make 5 runs and report average. Table 4 presents the results with standard deviation. Classification Model: We follow the model setting from [57, 67. We share the same finding with 67] that the acuracy for the Retrieval tasks is actually higher than reported in 57. Ratio between Sparse and Low-rank components: There are some rules that we used in our experiments to set this ratio. For inference, we set this ratio based on the entropy of an observed subset of attention matrices in different layers: we allocate more memory to the low-rank component compared to the sparse component if the entropy is high. For training, generally allocating more memory budget to sparse tends to perform better, so in the experiment, we set the ratio to $3: 1$ (sparse: low-rank component) for simplicity. Moreover, in future work, it could be useful to make this ratio adaptive during training. For example, in the early stage of the training and early layers, attention matrices are usually more uniform (higher entropy). Thus, the approximation error could be even lower if the ratio favors low-rank-based components. One approach could be to monitor the approximation error of sparse and low-rank components compared to full attention regularly and adjust the memory budget accordingly. We will add the above discussion to the updated manuscript. Table 4: The performance of Scatterbrain, REFORMER, PERFORMER and Full-Attention on Long-Range-Arena benchmarks and 2 popular language modeling tasks. We fix the same number of parameters $(1 / 8$ of the full) used for approximating the attention matrix for each method. | Attention | Copy $(\\mathrm{ppl})$ | WikiText-103 (ppl) |\n| :---: | :---: | :---: |\n| Full Attention | 1 | $25.258 \\pm 0.37$ |\n| Reformer | $6.8 \\pm 0.64$ | $27.68 \\pm 0.53$ |\n| Performer | $49 \\pm 2.7$ | $66 \\pm 5.8$ |\n| Scatterbrain | $\\mathbf{2 . 5 8} \\pm 0.21$ | $\\mathbf{2 6 . 7 2} \\pm 0.44$ |\n\n\n| Attention | ListOps | Text | Retrieval | Image | Pathfinder | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Full Attention | $38.2 \\pm 0.17$ | $63.29 \\pm 0.38$ | $80.85 \\pm 0.12$ | $41.78 \\pm 0.26$ | $73.98 \\pm 0.31$ | 59.62 |\n| Reformer | $36.85 \\pm 0.37$ | $58.12 \\pm 0.42$ | $78.36 \\pm 0.29$ | $28.3 \\pm 0.39$ | $67.95 \\pm 0.28$ | 53.9 |\n| Performer | $35.75 \\pm 0.29$ | $62.36 \\pm 0.49$ | $78.83 \\pm 0.33$ | $39.71 \\pm 0.48$ | $68.6 \\pm 0.36$ | 57.05 |\n| Scatterbrain | $38.6 \\pm 0.22$ | $64.55 \\pm 0.34$ | $\\mathbf{8 0 . 2 2} \\pm 0.31$ | $\\mathbf{4 3 . 6 5} \\pm 0.46$ | $69.91 \\pm 0.25$ | 59.38 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_4420c59ec674bac5f31eg-32.jpg?height=1377&width=1388&top_left_y=703&top_left_x=352)\n\nFigure 8: Top two plots present Approximation Error vs. Entropy of attention matrices for REFORMER, PERFORMER and Scatterbrain on Copy (left) and WikiText103 (right). Bottom two plots present Approximation Error vs. Entropy of attention matrices for REFORMEr, PERformer and Scatterbrain on Text-IMDb (left) and Image-Cifar10 (right). Recall we observe that entropy of the softmax attention distribution (i.e., scale of logits) determines the regimes where sparse, low-rank, or sparse + low-rank perform well. Scatterbrain yields better approximation than REFORMER or PERFORMER in most of the cases; PERFORMER performs the worst on language modeling tasks while REFORMER performs the worst on classification tasks.",
    "scatterbrain-45": "These plots for approximation error analysis match with their performance on downstream tasks. ## E. 3 More Ablation Studies\n\n## E.3.1 Memory Budget\n\nWe present an ablation study on the parameter budget for the WikiText-103 language modeling task. We show that Scatterbrain outperforms its sparse and low-rank baselines across a range of parameter budgets. The results are presented in Table 5 . Analysis: We have observed that Scatterbrain outperforms its sparse and low-rank baselines under different memory budgets. Similar to what we found in Section 5.2. Performer does not train stably even with $\\frac{1}{4}$ of the full attention memory. However, under the Scatterbrain framework, Performer can be combined with Reformer in an elegant way to achieve the same accuracy while using only half of the memory and faster than Reformer by exploiting the sparse+low-rank structure in attention matrices. Table 5: We run WikiText-103 LM with a sweep of $1 / 4,1 / 8,1 / 16$ memory budget. We show the validation perplexity and speed-up with respect to the full attention with different efficient Attention layers. |  | $\\frac{1}{4}$ Mem | $\\frac{1}{8} \\mathrm{Mem}$ | $\\frac{1}{16} \\mathrm{Mem}$ |\n| :---: | :---: | :---: | :---: |\n|  | Perplexity (Speed-up) | Perplexity | Perplexity |\n| SmYrF | $26.76(1.6 \\times)$ | $27.68(1.39 \\times)$ | $28.7(1.85 \\times)$ |\n| Performer | $58(2.13 \\times)$ | $66(2.01 \\times)$ | $85(1.77 \\times)$ |\n| Scatterbrain | $\\mathbf{2 6 .",
    "scatterbrain-46": "2 6 ( 1 . 5 8} \\times)$ | $\\mathbf{2 6 . 7 2}(\\mathbf{1 .",
    "scatterbrain-47": "8 7} \\times)$ | $\\mathbf{2 7 . 7 4 ( 2 . 0 3} \\times)$ |\n\n## E.3.2 Different Sparse and Low-rank baselines\n\nScatterbrain is general enough to accommodate different kinds of sparse and low-rank approximations as its sub-components. In particular, we can combine Local attention or block sparse (from Sparse Transformer and BigBird) + Performer (instead of Reformer + Performer) in a similar fashion. The support of the sparse matrix $S$ will thus be fixed and not adaptive to input, but all the other steps are exactly the same. We have run additional experiments on the Local attention + Performer combination and BigBird. Recall that in Appendix E we have shown Scatterbrain can reduce the attention memory of Vision Transformer by $98 \\%$ at the cost of only $0.8 \\%$ drop of accuracy when serving as a drop-in replacement for full attention without training on ImageNet. We show the results for local+performer variation with the same memory budget in Table 6 . We have also run additional experiments on Local attention on Copy and Wikitext-103 language modeling task ( Table 7). We see that Local attention is reasonably competitive on Wikitext-103 but does not perform well on Copy. The results are not surprising as noted in the Reformer paper that Copy requires non-local attention lookups. ## E.3.3 Different Sparse and Low-rank baselines\n\n## E. 4 Analysis\n\nRecall in Section 5 , we have reported the analysis after visualizing the error of REFORMER (sparse), PERFORMER (low-rank), and Scatterbrain (sparse + low-rank) given the same number of parameters when approximating the full attention matrices for each attention layer during training. In Figure 8 we present the visualization. Table 6: Top-1 Accuracy of pre-trained T2T Vision Transformer on ImageNet with different attention replacements. Error represents the average normalized approximation error to full attention. | Attention | Top-1 Acc |\n| :---: | :---: |\n| Full Attention | $81.7 \\%$ |\n| SMYRF | $79.8 \\%$ |\n| Local | $79.6 \\%$ |\n| Performer | $80.1 \\%$ |\n| BigBird | $80.3 \\%$ |\n| Scatterbrain (Local + Performer) | $80.3 \\%$ |\n| Scatterbrain (SMYRF + Performer) | $\\mathbf{8 0 . 7 \\%}$ |\n\nTable 7: Additional experiments for Local attention on the Copy and Wikitext-103 language modeling task. | Attention | Copy (ppl) | WikiText-103 (ppl) |\n| :---: | :---: | :---: |\n| Full Attention | 1 | 25.258 |\n| Reformer | 6.8 | 27.68 |\n| Performer | 49 | 66 |\n| Local | 53 | 30.72 |\n| Scatterbrain | $\\mathbf{2 . 5 8}$ | $\\mathbf{2 6 . 7 2}$ |\n\nThe conclusion for language modeling tasks is that sparse + low-rank has the smallest approximation error in most of the cases, and sparse has the largest error, which matches with the end-to-end results. It also confirms the observation in the popular benchmark paper [57] that kernel or low-rank based approximations are less effective for hierarchical structured data. For classification tasks, we again find that Scatterbrain has the smallest approximation error, while PERFORMER is the worst on ListOps and REFORMER has the largest error on classification tasks, which matches with the end-to-end results and confirms our observations earlier (sparse and low-rank approximation excel in different regimes). ## E. 5 Additional Experiments of Fine-tuning Bert on GLUE\n\nWe provide additional experiments of fine-tuning Bert on GLUE in Table 8. We follow the similar setting as 22 . We replace all the attention layers in Bert base model with Scatterbrain and other baselines. Then we fine-tune Bert on 9 downstream tasks for 3 epochs with batch size 32 and learning rate 3 e-5. The parameter budget is $1 / 2$ of the full attention because sequence length 128 is not very long. We can see Scatterbrain outperforms all the other baselines in most of the downstream tasks. Table 8: Results of GLUE when replacing dense attention matrices with SMYRF, PERFORMER and Scatterbrain in BERT base model. We fix the same number ofparameters ( $1 / 2$ of the full) used for approximating the attention matrix for each method. |  | CoLA | SST-2 | MRPC | STS-B | QQP | MNLI | QNLI | RTE | WNLI |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | mcc | acc | acc | corr | acc | acc | acc | acc | acc |\n| FULL | 0.576 | 0.934 | 0.874 | 0.879 | 0.905 | 0.813 | 0.916 | 0.668 | 0.43 |\n| SMYRF | 0.538 | 0.912 | 0.833 | 0.856 | 0.898 | 0.775 | 0.879 | $\\mathbf{0 . 6 2 6}$ | 0.412 |\n| PERFORMER | 0.508 | 0.838 | 0.782 | 0.203 | 0.831 | 0.563 | 0.763 | 0.556 | $\\mathbf{0 . 4 4 9}$ |\n| Scatterbrain | $\\mathbf{0 . 5 6 9}$ | $\\mathbf{0 . 9 2 7}$ | $\\mathbf{0 . 8 6 3}$ | $\\mathbf{0 . 8 6 7}$ | $\\mathbf{0 .",
    "scatterbrain-48": "9 0 2}$ | $\\mathbf{0 . 8 1 3}$ | $\\mathbf{0 . 8 9 3}$ | 0.619 | 0.428 |\n\n## F Further Discussions and Future Work\n\nIn this paper, we present Scatterbrain, unifying the strength of sparse and low-rank approximation. It is inspired by the observations on the attention matrix structures induced by the data and softmax function as well as the classical robust-PCA algorithm. In our implementation and analysis, we have REFORMER/Smyrf and PERFORMER as the back-bone for sparse and low-rank approximations because of their properties, e.g. Performer is unbiased. Scatterbrain is fundamentally a framework for combining the strength of sparse and low-rank variants, so it can be easily extended to other variants, such as Routing Transformer [53] or Nystromformer [67]. Further more, our observations on the connection between entropy and low-rank/sparse approximation error also provide an opportunity for efficiently detecting the approximation or compression method to choose for different architectures or benchmarks.",
    "scatterbrain-49": "[^0]:    *Equal contribution. Order determined by coin flip. [^1]:    ${ }^{1}$ Scatterbrain code is available at https://github.com/HazyResearch/scatterbrain\n\n[^2]:    ${ }^{2}$ For simplicity of discussion, we consider the unnormalized attention matrix $A=\\exp \\left(Q K^{\\top}\\right)$, omitting the usual scaling of $\\sqrt{d}$ and the softmax normalization constant. [^3]:    ${ }^{3}$ SMYRF is a variant of Reformer that does not require the key and query to be the same, which is necessary for experiments in this section.",
    "scatterbrain-50": ""
}