{
    "scatterbrain-0": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation\n\nBeidi Chen Equal contribution. Order determined by coin flip. Department of Computer Science, Stanford University Tri Dao\u2020\u2020footnotemark: Department of Computer Science, Stanford University Eric Winsor Department of Computer Science, Stanford University Zhao Song Adobe Research Atri Rudra Department of Computer Science and Engineering, University at Buffalo, SUNY Christopher R\u00e9 Department of Computer Science, Stanford University\n\nAbstract\n\nRecent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce of attention memory at the cost of only drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks. \\doparttoc\\faketableofcontents\n\n1 Introduction\n\nTransformer models [63] have been adapted in a wide variety of applications, including natural language processing [26, 7, 50], image processing [10, 47], and speech recognition [42]. Training large Transformers requires extensive computational and memory resources, especially when modeling long sequences, mainly due to the quadratic complexity (w.r.t. sequence length) in attention layers. Recent advances in efficient transformers [36, 17, 35, 65, 22] leverage attention approximation to overcome the bottleneck by approximating the attention matrices. However, it is challenging to find a robust approximation method that balances the efficiency-accuracy trade-off on a wide variety of tasks [57, 58]. We categorize most of the existing approaches for efficient attention matrix computation into two major groups: exploiting either the sparsity, e.g., Reformer [36], SMYRF [22], or low-rank properties of the attention matrices, e.g., Linformer [65], Linear Transformer [35], and Performer [17]. However, these techniques usually have different strengths and focus on the performance of specific tasks, so their approximations still cause accuracy degradation on many other tasks. For instance, according to a recent benchmark paper [57] and our experiments, low-rank-based attention might be less effective on hierarchically structured data or language modeling tasks, while sparse-based variants do not perform well on classification tasks. We observe that sparse and low-rank approximations are complementary for many attention matrices in practice, and sparse+low-rank could outperform each individually (Figure 1 left). We empirically categorize the regimes in which sparse or low-rank approximation achieves better error based on the softmax temperature of attention (of which the entropy of softmax distribution can be used as a proxy). We expect that sparse methods perform well if the attention depends on a few entries (low entropy softmax). In contrast, low-rank methods do better if the attention depends on a mixture of many components (high entropy softmax). This explains the phenomenon that current sparse and low-rank-based approaches excel on different kinds of tasks. A natural question is whether one could understand and unify the strength of both approaches. While it is NP-hard to find the optimal combination of sparse and low-rank approximations, Robust PCA [9] is a polynomial-time solution with tight approximation error. We observe that Robust PCA achieves lower approximation error than sparse or low-rank alone on attention matrices. The difference is most pronounced for \u201cmid-range\u201d entropy, where we observe that up to error reduction is possible. The connection between Robust PCA and attention matrix estimation provides an opportunity to realize a more robust approximation. Specifically, given an attention matrix, one could adaptively perform sparse+low-rank approximation to obtain a low error. However, it comes with three challenges: (i) How to decompose the attention matrices into sparse and low-rank components and estimate them efficiently and accurately; Robust PCA is accurate but slow and requires materializing the full attention, while straightforward addition of sparse and low-rank attention will be inaccurate due to double counting. (ii) It is not clear if there is a theoretical guarantee that sparse + low-rank approximation is strictly better than sparse or low-rank in some regimes, though we observe the separation empirically. (iii) How does the lower approximation error transfer to end-to-end performance in real tasks. In this paper, we propose Scatterbrain, an accurate and efficient robust estimation of attention matrices with theoretical guarantees to address the above challenges. Specifically:\n\n\u2022\n\nIn Section 3, we observe that sparse and low-rank approximation are complementary and demonstrate that sparse + low-rank structure arises naturally when elements in the input sequence form clusters. We theoretically characterize and analyze the regimes where sparse, low-rank, and sparse+low-rank excel, dictated by the softmax temperature of attention. \u2022\n\nIn Section 4, inspired by the classical Robust PCA algorithm, we propose Scatterbrain, which efficiently combines sparse and low-rank matrices to approximate attention. In particular, we use Locality Sensitive Hashing (LSH) to identify large entries of the attention matrix (after softmax) without materializing the full matrix and then leverage kernel approximation to parameterize the low-rank part. We prove that our method has a strictly lower approximation error than the low-rank baseline. \u2022\n\nIn Section 5, we empirically validate our theory and the proposed method, showing that Scatterbrain accurately approximates the attention matrix, is memory efficient for long sequences, and works well across different tasks. First, we show that its approximation accuracy is close to our oracle Robust PCA and achieves 2.1 lower error compared to other efficient baselines on real benchmarks. This leads to a direct application of Scatterbrain as a drop-in replacement to pre-trained full attention, thus reducing up to of the memory required for attention computations in pre-trained T2T-ViT and BigGAN while maintaining similar quality. Last we show that its superior accuracy and efficiency can improve the efficiency-accuracy trade-offs of Transformer end-to-end training. On the WikiText-103 language modeling task, Scatterbrain achieves up to 1 point better perplexity compared to Reformer and Performer. On 5 benchmark long-range tasks, Scatterbrain improves the average accuracy by up to 5 points.111Scatterbrain code is available at https://github.com/HazyResearch/scatterbrain\n\n2 Problem Setting and Related Work\n\nWe first define the approximation problem we aim to solve in this paper. Then we discuss the applications of sparse and low-rank techniques in efficient Transformers and introduce robust PCA algorithm. Problem Formulation: In the attention matrix approximation problem, we are given three matrices, query, key, and value, to compute . We seek to reduce the quadratic complexity of (applied row-wise) with low approximation error. More precisely, for an approximation procedure , we minimize two objectives, the approximation error , and the computation/memory cost . Sparse, Low-rank Approximation for Attention Matrices: Recent work exploits the sparsity patterns or finds a low-rank mapping of the original attention matrices to overcome the computational and memory bottlenecks in Transformers [36, 22, 53, 17, 35, 65]. Generally, we can divide most of the techniques into two categories \u2013 sparse and low-rank approximations. Reformer [36] is a representative sparse variant that uses LSH [3] to retrieve or detect the locations of the attention matrices with large values and reduce the computation from to . Performer [17] is an example of the low-rank variant, which uses kernelization to avoid explicit computation. One problem of either the sparse or low-rank approximation is that the structure of the attention matrices varies in practice, and it is challenging to perform robust approximation on a wide range of attention matrices. For example, Wang et al. [65] observes that attentions tend to have more low-rank structures in lower layers and Ramsauer et al. [51] shows that they are sparser in the later stage of the training. Ideally, we want to unify the strength of both techniques, but it is NP-hard to find the best combination of sparse and low-rank approximation. Sparse + Low-rank and Robust PCA: Fortunately, classical Robust PCA [9] presents a polynomial algorithm to find the approximately optimal or good combinations of sparse and low-rank approximation of the matrices. The sparse + low-rank matrix structure has been well studied in statistics and signal processing since the late 2000s [9]. This structure naturally generalizes low-rank [33, 62], and sparse [60] matrices. Scatterbrain is built on a line of work, e.g., Bigbird [70], Longformer [5] with the theme of combining multiple types of attention. However, despite the multitude of papers, this sparse + low-rank matrix approximation has not been rigorously studied in the context of attention matrices. We undertake this study and show how we can relax the sparse + low-rank approximation from robust PCA, making it efficient while still retaining PCA\u2019s accuracy. In fact, our results shed further light on why Bigbird or Longformer work, as they are special cases of a single principled structure. An extended discussion of related work is in Appendix A. 3 Characterization of Sparse + Low-rank Approx. to Attention Matrices\n\nWe motivate the use of sparse + low-rank approximation of the attention matrices with the key observation that for many attention matrices, sparse and low-rank approximation are complementary, and their ideal combination (via Robust PCA) can outperform both (Section 3.1). Furthermore, we argue that the sparse + low-rank structure can arise naturally when elements in the input sequence form clusters, as dictated by the softmax temperature (Section 3.2). 3.1 Motivating Observations: Low-rank and Sparse Structures of Attention Matrices\n\nWe empirically characterize regimes where sparse and low-rank approximation are well-suited, based on the softmax temperature (for which we use the softmax distribution entropy is a proxy). Specifically, in Fig. 1 (left), we present the approximation error of the original attention matrices and the approximation (sparse or low-rank) of matrices sampled from a 4-layer Transformer trained on IMDb reviews classification [57]. We make two observations:\n\n1. Sparse and low-rank approximation are complementary: sparse excels when the softmax temperature scale is low (i.e., low entropy), and low-rank excels when the softmax temperature is high (i.e., high entropy). 2. An ideal combination of sparse and low-rank (orange line in Fig. 1 left), obtained with robust PCA, can achieve lower error than both. Similar observations on other benchmarks and details are presented in Appendix B. 3.2 A Generative Model of How Sparse + Low-rank Structure Can Arise\n\nSparse + low-rank parameterization is more expressive than either sparse or low-rank alone. Indeed, in the Appendix, we construct a family of attention matrices to show the separation between the approximation capability of sparse + low-rank vs. sparse or low-rank alone: for an attention matrix, sparse or low-rank alone requires a parameters to get approximation error in Frobenius norm, while sparse + low-rank only requires parameters. Moreover, we argue here that sparse + low-rank is a natural candidate to approximate generic attention matrices. We describe a generative model of how the sparse + low-rank structure in attention matrices could arise when the elements of the input sequence form clusters. Under this process, we characterize how the softmax temperature dictates when we would need sparse, low-rank, or sparse + low-rank matrices to approximate the attention matrix. This result corroborates the observation in Section 3.1. Generative process of clustered elements in input sequence\n\nWe describe here a generative model of an input sequence to attention, parameterized by the inverse temperature and the intra-cluster distance . Process 1. Let , where , with every row of generated randomly as follows:\n\n1. For , sample number of cluster centers independently from . 2. For each cluster around , sample number of elements around , of the form for where . Assume that the total number of elements is and . Let be the matrix whose rows are the vectors where and . Let and let the attention matrix be . We visualize this generative process in Fig. 2. Softmax temperature and approx. error\n\nWe characterize when to use sparse, low-rank, or sparse + low-rank to approximate the attention matrices in 1, depending on the inverse temperature . The intuition here is that the inverse temperature corresponds to the strength of interaction between the clusters. If is large, intra-cluster interaction dominates the attention matrix, the softmax distribution is peaked, and so we only need a sparse matrix to approximate the attention. If is small, then the inter-cluster attention is similar to intra-cluster attention, the softmax distribution is diffuse, and we can approximate it with a low-rank matrix. In the middle regime of , we need the sparse part to cover the intra-cluster attention and the low-rank part to approximate the inter-cluster attention.",
    "scatterbrain-1": "We formalize this intuition in Theorem 1 (in bounds below we think of as a constant). All the proofs are in Appendix D. Theorem 1. Let , be the attention matrix in 1. Fix . Let be a matrix. Consider low-rank, sparse, and sparse + low-rank approximations to . 1. High temperature: Assume . (a)\n\nLow-rank: There exists with rank (and hence parameters) such that . (b)\n\nSparse: If has sparsity , then . 2. Mid temperature: Assume . (a)\n\nSparse + low-rank: There exists a sparse + low-rank with parameters with . (b)\n\nLow-rank: If is such that , then . (c)\n\nSparse: If has sparsity , then . 3. Low temperature: Assume . (a)\n\nLow-rank: If is such that , then . (b)\n\nSparse: There exists with sparsity such that\n\n4 Scatterbrain: Unifying Sparse and Low-rank Attention\n\nWe present Scatterbrain, and show that it approximates attention accurately and efficiently. Section 4.1 describes the challenges of designing an accurate and efficient approximation, and how obvious baselines such as Robust PCA or a simple combination of sparse attention and low-rank attention fail to meet both criteria. Section 4.2 demonstrates how Scatterbrain address the challenges (Fig. 1 contains a schematic of Scatterbrain). In Section 4.3, we show that Scatterbrain is unbiased with provably lower variance than low-rank baselines such as Performer. Fig. 3 shows a qualitative comparison between different methods of approximating the attention matrix: Robust PCA is accurate but slow, sparse (e.g., Reformer), and low-rank (e.g., Performer) attention are fast and memory-efficient but may not be very accurate, while Scatterbrain is more accurate than its sparse and low-rank counterparts while remaining just as efficient. More details about the efficient implementation of Scatterbrain are in Appendix C. 4.1 Challenges of Designing an Accurate and Efficient Sparse + Low-rank Approximation\n\nWe seek a sparse + low-rank approximation of the attention matrix222For simplicity of discussion, we consider the unnormalized attention matrix , omitting the usual scaling of and the softmax normalization constant. that is both accurate and efficient. The natural theoretical baseline of Robust PCA is too slow and requires too much memory, while the most straightforward way of combining sparse attention and low-rank attention fails due to double counting on the support of the sparse attention. 1. If the goal is accuracy, Robust PCA is the most studied algorithm to find a sparse + low-rank approximation to a given matrix. It relaxes the NP-hard problem of finding the best sparse + low-rank approximation into a convex optimization problem, with the nuclear norm and constraints. Even though it can be solved in polynomial time, it is orders of magnitude too slow to be used in each iteration of a training loop. Moreover, it requires materializing the attention matrix, which defeats the main purpose of reducing compute and memory requirements. 2. On the other hand, one efficient way to get sparse + low-rank approximation of an attention matrix is to simply add the entries of a sparse approximation (say, from Reformer) and a low-rank approximation for (say, from Performer). The sparse matrix typically has support determined randomly [16], by LSH [36, 22], or by clustering [53]. On the support of S, which likely includes the locations of the large entries of the attention matrix , the entries of match those of . One can multiply efficiently because is sparse, and grouping reduces the matrix multiplication complexity when , from to . The approximation matches outside , hence it could be accurate there if is accurate. However, will not be accurate on the support of due to the contributions from both and from . Adjusting to discount the contribution from is difficult, especially if we want to avoid materializing for efficiency. 4.2 Scatterbrain: Algorithm Intuition and Description\n\nThe simple insight behind our method is that on the support of the sparse matrix , instead of trying to match the entries of the attention matrix , we can set the entries of to discount the contribution from the low-rank part . This way, the approximation will match exactly on the support of , and will match outside , which means it will still be accurate there if is accurate. We do not need to materialize the full matrix as need a subset of its entries is required, hence our approximation will be compute and memory efficient. Scatterbrain thus proceeds in three steps: we construct a low-rank approximation , and construct a sparse matrix such that matches on the support of , then finally multiply and and combine the result. More specifically:\n\n1. Low-rank Approximation. We define a procedure LowRank that returns two matrices such that approximates . In particular, we use a randomized kernel feature map where with randomly sampled, entry-wise, from the standard normal distribution . We apply to each row vector of matrices, and denote and (row-wise). Note that we do not materialize . 2. Sparse Approximation. We define a procedure Sparse that returns a sparse matrix that matches on . In particular, using a family of locality sensitive hash functions, compute the hash codes of each query and key vectors in matrices (row-wise). Let be the set of locations where and have the same hash codes (i.e, fall into the same hash bucket). Let be the sparse matrix whose support is , and for each , define\n\nS i , j = exp \u2061 ( q i \u22a4 \u200b k j ) \u2212 \u03d5 \u200b ( q i ) \u22a4 \u200b \u03d5 \u200b ( k j ) = exp \u2061 ( q i \u22a4 \u200b k j ) \u2212 q ~ i \u22a4 \u200b k ~ j , subscript \ud835\udc46 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc58 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc5e \ud835\udc56 top italic-\u03d5 subscript \ud835\udc58 \ud835\udc57 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc58 \ud835\udc57 superscript subscript ~ \ud835\udc5e \ud835\udc56 top subscript ~ \ud835\udc58 \ud835\udc57 S_{i,j}=\\exp(q_{i}^{\\top}k_{j})-\\phi(q_{i})^{\\top}\\phi(k_{j})=\\exp(q_{i}^{\\top}k_{j})-\\widetilde{q}_{i}^{\\top}\\widetilde{k}_{j}, (1)\n\nwhere are the -th and -th rows of respectively.",
    "scatterbrain-2": "Note that we do not materialize . 3. Scatterbrain Approximation. With returned from LowRank and from Sparse, we compute the (unnormalized) attention output with\n\nO ~ = ( Q ~ \u200b K ~ \u22a4 + S ) \u200b V = Q ~ \u200b ( K ~ \u22a4 \u200b V ) + S \u200b V . ~ \ud835\udc42 ~ \ud835\udc44 superscript ~ \ud835\udc3e top \ud835\udc46 \ud835\udc49 ~ \ud835\udc44 superscript ~ \ud835\udc3e top \ud835\udc49 \ud835\udc46 \ud835\udc49 \\widetilde{O}=(\\widetilde{Q}{\\widetilde{K}}^{\\top}+S)V=\\widetilde{Q}({\\widetilde{K}}^{\\top}V)+SV. (2)\n\nThe precise algorithm, including the normalization step, as well as the causal/unidirectional variant, is described in Appendix C. We also note Scatterbrain\u2019s flexibility: it can use different kinds of low-rank and sparse approximation as its sub-components. The combination of Reformer and Performer is simply one instance of Scatterbrain. Instead of using Reformer as a sparse component, we could use local attention [5] or random block-sparse attention [16]. Instead of using Performer [17] as a low-rank component, we could also use Linear attention [35] or global tokens as in BigBird [70]. The Scatterbrain method would work exactly the same way. As long as the low-rank component is unbiased (e.g., Performer), its combination with any sparse component in Scatterbrain would yield an unbiased estimator of the attention matrix as shown below. 4.3 Scatterbrain: Analysis\n\nOur method combines a low-rank approximation (which has rank ) with a sparse approximation . We argue that it is accurate (lower approximation error than baselines) and efficient (scaling the same as sparse or low-rank alone). The main insight of the analysis is that our approximation is exact for entries on the support of (picked by LSH), which are likely to be large. For entries not in the support of (likely to be small), our approximation matches the low-rank part (Performer) , which is unbiased and has low variance for these entries. As a result, Scatterbrain retains the unbiasedness of Performer [17] but with strictly lower variance. We compare Scatterbrain to its low-rank baseline (Performer) and sparse baseline (Reformer). Performer is also based on the kernel approximation , and simply uses to approximate the attention matrix . Reformer uses LSH to identify large entries of , then compute a sparse matrix such that for . Accuracy: Because of the way is defined in Eq. 1, matches exactly on locations , which are locations with likely large values. This addresses a weakness of low-rank methods (e.g., Performer) where the low-rank estimate is not accurate for locations with large values. We analyze the expectation and variance per entry of our estimator below (proof in Appendix D). Theorem 2. Define , as Performer\u2019s estimator and as Scatterbrain estimator. Denote as the unit sphere. Suppose are such that . Then:\n\n\ud835\udd3c \u200b [ \u03c3 ^ \ud835\uddcc\ud835\uddbb\ud835\uddbe \u200b ( q , k ) ] = \u03c3 \u200b ( q , k ) , Var \u200b [ \u03c3 ^ \ud835\uddcc\ud835\uddbb\ud835\uddbe \u200b ( q , k ) ] = ( 1 \u2212 p ) \u22c5 Var \u200b [ \u03c3 ^ \ud835\uddc9\ud835\uddbf\ud835\uddbe \u200b ( q , k ) ] < Var \u200b [ \u03c3 ^ \ud835\uddc9\ud835\uddbf\ud835\uddbe \u200b ( q , k ) ] formulae-sequence \ud835\udd3c delimited-[] superscript ^ \ud835\udf0e \ud835\uddcc\ud835\uddbb\ud835\uddbe \ud835\udc5e \ud835\udc58 \ud835\udf0e \ud835\udc5e \ud835\udc58 Var delimited-[] superscript ^ \ud835\udf0e \ud835\uddcc\ud835\uddbb\ud835\uddbe \ud835\udc5e \ud835\udc58 \u22c5 1 \ud835\udc5d Var delimited-[] superscript ^ \ud835\udf0e \ud835\uddc9\ud835\uddbf\ud835\uddbe \ud835\udc5e \ud835\udc58 Var delimited-[] superscript ^ \ud835\udf0e \ud835\uddc9\ud835\uddbf\ud835\uddbe \ud835\udc5e \ud835\udc58 \\displaystyle\\mathbb{E}[\\widehat{\\sigma}^{\\mathsf{sbe}}(q,k)]=\\sigma(q,k),\\quad\\mathrm{Var}[\\widehat{\\sigma}^{\\mathsf{sbe}}(q,k)]=(1-p)\\cdot\\mathrm{Var}[\\widehat{\\sigma}^{\\mathsf{pfe}}(q,k)]<\\mathrm{Var}[\\widehat{\\sigma}^{\\mathsf{pfe}}(q,k)] (3)\n\nwhere . Hence Scatterbrain is unbiased, similar to Performer [17], but with strictly lower variance.",
    "scatterbrain-3": "The variance is small if is small (since will be small), or if is large (since the probability of not being selected by LSH, , will be small). In Fig. 4, we plot the per-entry MSE of different methods from Theorem 2 when approximating the unnormalized softmax attention . Scatterbrain can approximate well both small entries (similar to the low-rank baseline, Performer), as well as large entries (similar to the sparse baseline, Reformer). Thus Scatterbrain has much lower MSE than Performer for large entries, and lower MSE than Reformer for small entries. Efficiency: In Eq. 2, the computation is efficient because is sparse, and is efficient because of the way we associate matrix multiplication (scaling as instead of , which is much bigger if ). We validate these two properties of our approach in Section 5. 5 Experiments\n\nWe validate three claims that suggest Scatterbrain provides an accurate and efficient approximation to attention matrices, allowing it to outperform its sparse and low-rank baselines on benchmark datasets. \u2022\n\nIn Section 5.1, we evaluate the approximation error and testing accuracy of different approximation methods on pre-trained models such as BigGAN and Vision Transformer. We show that the approximation by Scatterbrain is close to the Robust PCA oracle and up to lower approximation error than other efficient baselines. \u2022\n\nIn Section 5.2, we validate that when trained end-to-end, Scatterbrain outperforms baselines (sparse or low-rank attention) on a wide variety of benchmark tasks, including language modeling, classification, and the Long-range Arena (LRA) benchmarks. Scatterbrain achieves up to 5 points higher average accuracy on the LRA benchmark compared to Performer and Reformer. \u2022\n\nIn Section 5.3, we demonstrate the scalability of Scatterbrain, showing that it has comparable memory and time usage with simpler baselines (sparse or low-rank alone) across a range of input sequence lengths (Section 5.3), while requiring up to smaller memory than full attention. All details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix E. 5.1 Scatterbrain\u2019s Approximation Accuracy\n\nWe evaluate Scatterbrain\u2019s approximation accuracy in three steps: (1) compare it with of Robust PCA (sparse+low-rank), our theoretical foundation and oracle (2) compare it with SMYRF333SMYRF is a variant of Reformer that does not require the key and query to be the same, which is necessary for experiments in this section. [22], Performer [17], which are popular variants of sparse and low-rank approximation to attention respectively and a naive baseline that directly adds SMYRF and Performer, (3) evaluate the inference accuracy when replacing full attention with Scatterbrain approximation. Scatterbrain achieves error within 20% of the oracle robust PCA, and up to lower error than SMYRF and Performer. When serving as a drop-in replacement for full attention, even without training, Scatterbrain can reduce the attention memory of Vision Transformer by 98% at the cost of only 0.8% drop of accuracy. Setup: We use the attention matrices from pre-trained BigGAN and T2T-ViT. BigGAN is a state-of-the-art model in Image Generation for ImageNet. BigGAN has a single attention layer at resolution 64 \u00d7 64 (4096 queries). T2T-ViT has 14 attention layers. Scatterbrain sets the ratio between SMYRF and Performer based on the entropy of an observed subset of attention matrices in different layers. We allocate more memory to the low-rank component compared to the sparse part if the entropy is high. Scatterbrain and Robust PCA: We first show that Scatterbrain approximates pre-trained attention matrices faster while its approximation error is within 20% on average. We also provide an example visualization on 100 attention matrices from the BigGAN generation process in Figure 5 (left). Scatterbrain vs. SMYRF and Performer: We show that Scatterbrain approximates pre-trained dense attention matrices with very low error compared to sparse (Reformer) or low-rank (Performer). Measuring Frobenius approx. error on the BigGAN image generation task, Scatterbrain achieves lower error compared to Performer. Drop-in replacement for full attention: We show that accurate approximation directly leads to efficient Inference. We replace BigGAN\u2019s dense attention with a Scatterbrain layer without other modifications. In 5 (right two), we show Inception and FID scores for Scatterbrain and other baselines under different memory budgets. Similarly, we use T2T-ViT [69], which is a token-to-token vision Transformer pre-trained on ImageNet [25]. In Table 1, we show the average approximation error of Scatterbrain for each layer and the end-to-end testing accuracy after replacing full attention with Scatterbrain and other baselines. Notably, Scatterbrain achieves Top-1 accuracy, which is only drop from the original by full attention reducing up to of the memory usage. 5.2 End-to-end Training Performance\n\nScatterbrain\u2019s accurate approximation of attention matrices allows it to outperform other efficient Transformer methods on benchmark tasks. Across a range of diverse tasks, both commonly used autoregressive tasks (sequence modeling) and benchmark long-range classification tasks (Long-Range Arena), Scatterbrain outperforms Performer (low-rank baseline) and Reformer (sparse baseline) by up to 4 points. 5.2.1 Auto-regressive Tasks\n\nOn the standard language modeling task of Wikitext-103, Scatterbrain obtains 1 point better perplexity than Reformer (sparse baseline), coming within 1.5 points of full attention. Settings: We compare the performance of Scatterbrain against Reformer and Performer on one popular synthetic task, Copy, and one large language modeling task: WikiText103 [45]. Reformer is a representative sparse-approximation-based variant and Performer is a low-rank-approximation-based variant. The base model is vanilla Transformer [63]. We observed that generally allocating more memory budget to sparse tends to perform better, so Scatterbrain sets the ratio to 3:1 (sparse: low-rank component) for simplicity. The statistics of each dataset and model hyper-parameters are in Appendix E. We report the best results of each method in perplexity. Results: Table 2 shows the testing perplexity for Scatterbrain and other baselines under the same parameter budget (each approximation is only allowed to compute of the full computation). Scatterbrain achieves comparable perplexity compared to the full attention Transformer model on Copy, and WikiText-103. Notably, Scatterbrain achieves 4 points lower perplexity on Copy and 1 point lower on WikiText-103 compared to Reformer, while Performer does not train stably on auto-regressive tasks (loss does not go down). Analysis: We also analyze the results by visualizing the error of Reformer (sparse), Performer (low-rank), and Scatterbrain (sparse + low-rank) given the same number of parameters when approximating the full attention matrices for each attention layer during training (Appendix E). The conclusion is for language modeling tasks, sparse+low-rank has the smallest approximation error in most of the cases, and sparse has the largest error, which matches with the end-to-end results. It also confirms the observation in the popular benchmark paper [57] that kernel or low-rank based approximations are less effective for hierarchical structured data. 5.2.2 Classification Tasks\n\nOn a suite of long-range benchmark tasks (Long Range Area), Scatterbrain outperforms Reformer (sparse baseline) and Performer (low-rank baseline) by up to 5 points on average. Settings: We compare the performance of Scatterbrain against Reformer and Performer on ListOps, two classifications: byte-level IMDb reviews text classification, image classification on sequences of pixels, a text retrieval, and pathfinder tasks. The datasets are obtained from the Long Range Arena (LRA) Benchmark [57], which is a recent popular benchmark designed for testing efficient Transformers. Similar to the auto-regressive tasks above, we use Reformer and Performer as baselines. The base model is also a vanilla Transformer. We follow the evaluation protocol from [57]. We report the best accuracy of each method. Results: Table 2 shows the individual and average accuracy of each task for Scatterbrain and other baselines under the same parameters budget. Specially, each approximation is only allowed to use of the full computation. We can see Scatterbrain is very close to full attention even with a large reduction in computation and memory. Further more, it outperforms all the other baselines consistently on every task and achieves more than 5 point average accuracy improvement than sparse-based approximation Reformer and more than 2 point average accuracy improvement than low-rank-based variant Performer. Analysis: Similarly, in order to analyze the performance of Reformer, Performer and Scatterbrain, we visualize their approximation error given the same number of parameters when approximating the full attention matrices for each attention layer during training (Appendix E). We again find that Scatterbrain has the smallest approximation error, while Performer is the worst on ListOps and Reformer has the largest error on classification tasks, which matches with the end-to-end results and confirms our observations earlier (sparse and low-rank approximation excel in different regimes). 5.3 Scatterbrain\u2019s Efficiency, Scaling with Input Sequence Length\n\nWe include ablation studies on the scalability of Scatterbrain in Fig. 6, showing that it is as computation and memory-efficient as simpler baselines such as SMYRF and Performer, while up to faster and more memory efficient than full attention for sequence length 4096. This demonstrates that our combination of sparse and low-rank inherits their efficiency. We report run times and memory consumption of the sequence lengths ranging from 512 to 32768. We use a batch size of 16 for all runs and conduct experiments a V100 GPU. Since the efficiency would be largely conditioned on hardware and implementation details, we perform best-effort fair comparisons. We adapt the Pytorch implementation from pytorch-fast-transformers library for our baselines and implement Scatterbrain similarly without any customized cuda kernels. 6 Discussion\n\nLimitations. As Scatterbrain has sparse attention as a component, it is not yet as hardware friendly (on GPUs and TPUs) as the low-rank component, which uses the very optimized dense matrix multiplication. This is the same limitation suffered by other sparse attention methods, but we are excited that more efficient sparse GPU kernels are being developed [31, 29]. Potential negative societal impacts. Our work seeks to understand the role of matrix approximation (and potentially energy savings) in the attention layer, which may improve a wide range of applications, each with their own potential benefits and harms. For example, making it language modeling more compute and memory efficient might facilitate spreading misinformation, and better image and video processing may make automatic surveillance easier. To mitigate these risks, one needs to address application-specific issues such as privacy and fairness, going beyond the error metrics we considered. Specially, for language models (LMs), while our work partially addresses the issue of environmental cost of LMs raised in [6], it does not address other issues such as unfathomable training data [6]. Discussion and future work. In this work, we make an observation on the sparse + low-rank structure of the attentions in Transformer models and theoretically characterize the regimes where sparse, low-rank and sparse + low-rank excel, based on the softmax temperature of the attention matrices. Motivated by this observation, we present Scatterbrain, a novel way to unify the strengths of both sparse and low-rank methods for accurate and efficient attention approximation with provable guarantees. We empirically verify the effectiveness of Scatterbrain on pretrained BigGAN, vision transformers, as well as end-to-end training of vanilla transformer. We anticipate that the study of this core approximation problem can prove useful in other contexts, such as generalized attention layers with other non-linearity beside softmax, and wide output layer in language modeling or extreme-classification. Acknowledgments\n\nWe thank Xun Huang, Sarah Hooper, Albert Gu, Ananya Kumar, Sen Wu, Trenton Chang, Megan Leszczynski, and Karan Goel for their helpful discussions and feedback on early drafts of the paper. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare.",
    "scatterbrain-4": "The Mobilize Center is a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and Bioengineering through Grant P41EB027060.",
    "scatterbrain-5": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra\u2019s research is supported by NSF grant CCF-1763481. References\n\nAlizadeh et al. [2020] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Andoni et al. [2015a] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh for angular distance.",
    "scatterbrain-6": "In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems (NeurIPS), pages 1225\u20131233. 2015a. Andoni et al. [2015b] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal LSH for angular distance. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, pages 1225\u20131233, 2015b.",
    "scatterbrain-7": "Artusi et al. [2002] R Artusi, P Verderio, and E Marubini. Bravais-pearson and spearman correlation coefficients: meaning, test of hypothesis and confidence interval.",
    "scatterbrain-8": "The International journal of biological markers, 17(2):148\u2013151, 2002. Beltagy et al. [2020] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.",
    "scatterbrain-9": "arXiv preprint arXiv:2004.05150, 2020. Bender et al. [2021] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, New York, NY, USA, 2021. Association for Computing Machinery. Brown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.",
    "scatterbrain-10": "arXiv preprint arXiv:2005.14165, 2020. Cand\u00e8s and Recht [2009] Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717\u2013772, 2009. Cand\u00e8s et al. [2011] Emmanuel J Cand\u00e8s, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):1\u201337, 2011. Carion et al. [2020] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229. Springer, 2020. Chen and Shrivastava [2018] Beidi Chen and Anshumali Shrivastava. Densified winner take all (wta) hashing for sparse datasets.",
    "scatterbrain-11": "In Uncertainty in artificial intelligence, 2018. Chen et al. [2018] Beidi Chen, Anshumali Shrivastava, and Rebecca C Steorts. Unique entity estimation with application to the syrian conflict. The Annals of Applied Statistics, 12(2):1039\u20131067, 2018. Chen et al. [2019] Beidi Chen, Yingchen Xu, and Anshumali Shrivastava. Fast and accurate stochastic gradient estimation.",
    "scatterbrain-12": "2019. Chen et al. [2020] Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, et al. SLIDE: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and Systems, 2:291\u2013306, 2020. Chen et al. [2021] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher R\u00e9. Mongoose: A learnable lsh framework for efficient neural network training.",
    "scatterbrain-13": "In The International Conference on Learning Representations (ICLR), 2021. Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski et al. [2020] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "scatterbrain-14": "arXiv preprint arXiv:2009.14794, 2020. Daghaghi et al. [2021] Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen, Mengnan Zhao, and Anshumali Shrivastava. A tale of two efficient and informative negative sampling distributions. In International Conference on Machine Learning, pages 2319\u20132329. PMLR, 2021. Dai et al. [2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context.",
    "scatterbrain-15": "arXiv preprint arXiv:1901.02860, 2019. Dao et al. [2019] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In The International Conference on Machine Learning (ICML), 2019. Dao et al. [2020] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In The International Conference on Learning Representations (ICLR), 2020. Daras et al. [2020] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf: Efficient attention using asymmetric clustering.",
    "scatterbrain-16": "arXiv preprint arXiv:2010.05315, 2020. De Sa et al. [2015] Christopher De Sa, Christopher Re, and Kunle Olukotun. Global convergence of stochastic gradient descent for some non-convex matrix problems. In International Conference on Machine Learning, pages 2332\u20132341. PMLR, 2015. De Sa et al. [2018] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060\u20131079. SIAM, 2018. Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848. Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.",
    "scatterbrain-17": "arXiv preprint arXiv:1810.04805, 2018. Dong et al. [2019] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest neighbor search. In International Conference on Learning Representations (ICLR), 2019. Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale.",
    "scatterbrain-18": "arXiv preprint arXiv:2010.11929, 2020. Gale et al. [2020] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse GPU kernels for deep learning.",
    "scatterbrain-19": "In Supercomputing, 2020. Gionis et al. [1999] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing. In Vldb, volume 99, pages 518\u2013529, 1999. Gray et al. [2017] Scott Gray, Alec Radford, and Diederik P Kingma. GPU kernels for block-sparse weights.",
    "scatterbrain-20": "arXiv preprint arXiv:1711.09224, 3, 2017. Gu et al. [2020] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in neural information processing systems (NeurIPS), 2020. Hotelling [1933] Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933. Indyk and Motwani [1998] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998. Katharopoulos et al. [2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "scatterbrain-21": "In International Conference on Machine Learning, pages 5156\u20135165. PMLR, 2020. Kitaev et al. [2020] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In The International Conference on Machine Learning (ICML), 2020. Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.",
    "scatterbrain-22": "2009. Lan et al. [2020] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.",
    "scatterbrain-23": "In The International Conference on Learning Representations (ICLR), 2020. Likhosherstov et al. [2020] Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear memory: How to make performers slim.",
    "scatterbrain-24": "arXiv preprint arXiv:2012.11346, 2020. Linsley et al. [2018] Drew Linsley, Junkyung Kim, Vijay Veerabadran, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated-recurrent units. arXiv preprint arXiv:1805.08315, 2018. Liu et al. [2020] Zichang Liu, Zhaozhuo Xu, Alan Ji, Jonathan Li, Beidi Chen, and Anshumali Shrivastava. Climbing the wol: Training for cheaper inference.",
    "scatterbrain-25": "arXiv preprint arXiv:2007.01230, 2020. Luo et al. [2021] Haoneng Luo, Shiliang Zhang, Ming Lei, and Lei Xie. Simplified self-attention for transformer-based end-to-end speech recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 75\u201381. IEEE, 2021. Ma et al. [2021] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. arXiv preprint arXiv:2106.01540, 2021. Maas et al. [2011] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142\u2013150, 2011. Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.",
    "scatterbrain-26": "arXiv preprint arXiv:1609.07843, 2016. Nangia and Bowman [2018] Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning.",
    "scatterbrain-27": "arXiv preprint arXiv:1804.06028, 2018. Parmar et al. [2018] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055\u20134064. PMLR, 2018. Radev et al. [2013] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919\u2013944, 2013. Rae et al. [2020] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In The International Conference on Learning Representations (ICLR), 2020. Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "scatterbrain-28": "arXiv preprint arXiv:1910.10683, 2019. Ramsauer et al. [2020] Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. Recht [2011] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011. Roy et al. [2021] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.",
    "scatterbrain-29": "Transactions of the Association for Computational Linguistics, 9:53\u201368, 2021. Shrivastava and Li [2014] Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips).",
    "scatterbrain-30": "In Advances in Neural Information Processing Systems (NeurIPS), pages 2321\u20132329, 2014. Sindhwani et al. [2015] Vikas Sindhwani, Tara N. Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In Advances in Neural Information Processing Systems, pages 3088\u20133096, 2015. Sukhbaatar et al. [2019] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2019. Tay et al. [2020a] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020a. Tay et al. [2020b] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.",
    "scatterbrain-31": "arXiv preprint arXiv:2009.06732, 2020b. Taylor [1990] Richard Taylor. Interpretation of the correlation coefficient: a basic review. Journal of diagnostic medical sonography, 6(1):35\u201339, 1990. Tewarson and Tewarson [1973] Reginald P Tewarson and Reginald P Tewarson. Sparse matrices, volume 69. Academic Press New York, 1973. Thomas et al. [2018] Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher R\u00e9. Learning compressed transforms with low displacement rank. In Advances in neural information processing systems (NeurIPS), pages 9052\u20139060, 2018. Udell and Townsend [2019] Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank?",
    "scatterbrain-32": "SIAM Journal on Mathematics of Data Science, 1(1):144\u2013160, 2019. Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "scatterbrain-33": "arXiv preprint arXiv:1706.03762, 2017. Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.",
    "scatterbrain-34": "arXiv preprint arXiv:1804.07461, 2018. Wang et al. [2020] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "scatterbrain-35": "arXiv preprint arXiv:2006.04768, 2020. Wu et al. [2019] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In The International Conference on Learning Representations (ICLR), 2019. Xiong et al. [2021] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention.",
    "scatterbrain-36": "arXiv preprint arXiv:2102.03902, 2021. Yang et al. [2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. Yuan et al. [2021] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
    "scatterbrain-37": "arXiv preprint arXiv:2101.11986, 2021. Zaheer et al. [2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. Zhu et al. [2021] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. arXiv preprint arXiv:2107.02192, 2021. Appendix\n\n\\parttoc\n\nAppendix A Extended Related Work\n\nA.1 Robust PCA\n\nRobust Principle Component Analysis (robust PCA) is the problem of finding a composition of a matrix into a sum of sparse and low-rank components: . It is a modification of PCA to accommodate corrupted observations (aka, noise). The sparse part covers the noise, while the low-rank part recovers the principle components. The most popular method to solve the problem is convex relaxation [8], where one minimizes the error subject to constraint on and nuclear norm constraint on , in order to promote the sparsity of and the low-rankness of . This convex problem can be solved with a variety of methods, such as interior point methods or the method of Augmented Lagrange Multipliers. In our context, to find a sparse + low-rank decomposition of the attention matrix, one can also heuristically \u201cpeel off\u201d the sparse part by finding the large entries of the attention matrix, then find a low-rank decomposition of the remainder. To avoid materializing the full attention matrix, one can use LSH to find potential locations of large entries, and use matrix completion [52] to find a low-rank decomposition. Gradient descent can find global optimum for this matrix completion problem [23]. However, it still requires too many iterations to be used in each training step. A.2 Efficient Transformers\n\nSparse, Low-rank Approx.: Transformer-based model such as BERT [38] has achieved unprecedented performance in natural language processing. Recently, Vision Transformers [28, 69] has also achieved comparable performance to the traditional convolutional neural network in computer vision tasks [66]. However, the quadratic computation of the attention layers constrains the scalability of Transformers. There are many existing directions to overcome this bottleneck, including attention matrix approximation such as Reformer [36], Performer [17], leveraging a side memory module that can access multiple tokens at once [56, 39, 38] such as Longformer [5] and BigBird [70], segment-based recurrence such as Transformer-XL [19] and Compressive Transformer [49]. Please refer to a recent survey [58] for more details. In this paper, we mainly explore within the scope of approximating dense or full attention matrices. Existing combination of Sparse and Low-rank Attention: Our focus on the classical and well-defined problem of matrix approximation, as opposed to simply designing an efficient model that performs well on downstream tasks (e.g., Longformer, Luna, Long-short transformer, etc.) affords us several advantages: (i) Easier understanding and theoretical analysis (Section 3, 4). We see that Scatterbrain yields an unbiased estimate of the attention matrix, and we can also understand how its variance changes. (ii) Clear-cut evaluation based on approximation error, as well as the ability to directly replace a full attention layer with Scatterbrain attention without re-training (Section 5). This setting is increasingly important as transformer models are getting larger and training them from scratch has become prohibitively costly. Other methods such as Luna and Long-short transformer are not backward compatible with pre-trained models. Here we compare Scatterbrain with other work mentioned by the reviewer, showing how most of them are special cases of Scatterbrain. We will also add this discussion in the updated version of the manuscript. \u2022\n\nLongformer [5]: a special case of Scatterbrain where the sparse component is local attention, and the low-rank component is the global tokens. Global tokens can be considered a restricted form of low-rank approximation. \u2022\n\nBigBird [70]: a special case of Scatterbrain where the sparse component is local + random sparse attention, and the low-rank component is the global tokens. The use of global tokens makes the model unsuited for autoregressive modeling. On the other hand, Scatterbrain\u2019s generality allows it to use other kinds of low-rank attention (e.g., Performer), and thus Scatterbrain works on both the causal/autoregressive and the bidirectional/non-causal attention settings. BigBird\u2019s motivation is also quite different from ours: they aim to design efficient attention such that the whole Transformer model is still a universal approximator and is Turing complete. Our goal is more concrete and easier to evaluate: we approximate the attention matrices, to get a small Frobenius error between the Scatterbrain attention and the full attention matrices. \u2022\n\nLuna [43] (concurrent work): they use a fixed-length extra sequence and two consecutive attention steps: the context sequence attends to the extra sequence, and then the query sequence attends to the extra sequence. This is similar in spirit to low-rank attention (Linformer) and global tokens, but it is not a low-rank approximation due to the non-linearity between the two attention steps. It is not clear to us that it combines different kinds of attention. \u2022\n\nLong-short transformer[71] (concurrent work): a special case of Scatterbrain where the sparse component is local attention and the low-rank component is Linformer. A.3 Locality Sensitive Hashing for Efficient Neural Network Training\n\nLocality Sensitive Hashing (LSH) has been well-studied in approximate nearest-neighbor search [30, 34, 54, 2, 27, 11].",
    "scatterbrain-38": "Since the brute-force approach for similarity search is computationally expensive, researchers have come up with various indexing structures to expedite the search process. Usually this comes with trade-offs on the search quality. Based on these indexing structures, one can achieve sub-linear search time. LSH has been used in estimation problem as well [13, 12]. Recently, there has been several work taking advantage of LSH data structures for efficient neural network training. During training process, the weight matrices are slowly modified via gradients derived from objective functions. If we consider the weights as the search data and input as queries, we can view neural network training as a similarity search problem. For example, [14, 18, 41] proposes an algorithm which performs sparse forward and backward computations via maximum inner product search during training. It is based on the observation that the model is usually over-parameterized so the activation for a given input could be sparse and LSH is used to find or impose the sparse structure. Similarly, LSH based algorithms have also been used in Transformers [14, 15], where LSH is used to capture the sparse structure of the attention matrices. They can largely reduce the memory bottleneck of self-attention modules especially over long sequences in Transformer. Though [15] has done some exploration to improve LSH accuracy-efficiency trade-offs through learnable LSH, most of the above works have limited understanding on when and where LSH can perform well. A.4 Structured Matrices for Efficient Machine Learning Models\n\nSparse + low-rank is an example of a class of structured matrices: those with asymptotically fast matrix-vector multiplication algorithm ( time complexity) and few parameters ( space complexity). Common examples include sparse, low-rank matrices, and matrices based on fast transforms (e.g., Fourier transform, circulant, Toeplitz, Legendre transform, Chebyshev transform, and more generally orthogonal polynomial transforms). These classes of matrices, and their generalization, have been used in machine learning to replace dense matrices in fully connected, convolutional, and recurrent layers [55, 61, 32]. De Sa et al. [24] shows that any structured matrix can be written as product of sparse matrices, and products of sparse matrices even with fixed sparsity pattern have been shown to be effective at parameterizing compressed models [20, 1, 21]. In our setting, it remains difficult to approximate the attention matrix with these more general classes of structured matrices. This is because many of them are fixed (e.g., Fourier transform, orthogonal polynomial transforms), and there lacks efficient algorithms to find the closest structured matrix to a given attention matrix. Appendix B Motivating Observations: Low-rank and Sparse Structures of Attention Matrices\n\nWe aim to build a deeper understanding of sparse and low-rank structures in real attention matrices: where each of them excel, and the potential for their combination. Specifically, we\n\n\u2022\n\nshow that sparse and low-rank approximation errors are negatively correlated (through statistical tests),\n\n\u2022\n\ncharacterize regimes where each of sparse and low-rank approximation are well-suited, as dictated by the entropy of the softmax attention distribution, and\n\n\u2022\n\ndemonstrate that sparse + low-rank has the potential to achieve better approximation than either. B.1 Setup\n\nDenote as the attention matrix (after softmax) and as entropy. We measure approximation error by the Frobenius norm or the original matrix and the approximation (sparse or low-rank). All the observed attention matrices in this section are from (1) a 4-layer vanilla Transformer trained from scratch on char-level IMDb reviews classification [57] (2) a 16-layer vanilla Transformer trained from scratch on WikiText103 [45] (3) a 1-layer (attention) pre-trained BigGAN on ImageNet [25]. To collect attention matrices for IMDb and WikiText103, we first save checkpoint of the models in every epoch; then evaluate 100 samples from validate data for each checkpoint and collect attention matrices from each layer each head. Note we take the median of the stats (error) for those 100 samples if it is difficult to visualize. To collect attention matrices for BigGAN, we generate 100 samples and collect the attention on the fly. B.2 Observation 1: Sparse and low-rank approximation errors are negatively correlated\n\nWe fixed the number of parameters, , allowed for each attention matrix approximation and collect the errors from ideal sparse and low-rank approximations: top entries for each row of the matrix for sparse and top eigenvalues for low-rank. Then we run three standard statistical correlation tests [4, 59], Spearman, Pearson and Kendall\u2019s Tau on sparse and low-rank approximation error for all the matrices. We can see from Table 3 that errors are significantly negatively correlated (p-value ). Further more, the left three plots on Figure 7 visualizes the correlation between the two errors on three datasets. This negative correlation suggests that there is some property of the softmax attention distribution which determines when sparse or low-rank excels. We validate this claim in the next observation. B.3 Observation 2: Sparse approximation error is lower when softmax entropy is low and low-rank approximation error is lower error when entropy is high\n\nWe visualize the sparse and low-rank approximation error against the entropy of attention matrices (applied to each row, then averaged) on the right plot in Figure 7. The attention matrices are (padded) so the x-axis has range from . For high-entropy distributions (more diffused) low-rank matrices approximates the attention matrix well. For low-entropy distributions (more peaked), sparse matrices are better-suited. This implies that sparse and low-rank approximations could be complementary: if we can combine the strength of both, it is possible to come up with a better approximation across more general scenarios. Therefore, in the next observation, we try to combine sparse and low-rank approximations. B.4 Observation 3: Sparse + Low-rank achieves better approximation error than sparse or low-rank alone\n\nWe find an approximation of the attention matrix of the form , where is sparse and is low-rank. This problem has a rich history and is commonly solved with Robust PCA. As shown in 7, across the range of entropy, sparse + low-rank approximation can achieve lower error than either sparse or low-rank when choosing the correct mix ratio of sparse and low rank approximation ideally (with robust-PCA). Motivated by the fact that sparse and low-rank approximations of attention matrices have complementary strengths (Observations 1 and 2), one might want to combine them (Observation 3) in hope of yielding a more robust approximation that works well across different kinds of attention matrices. The above introduces three main challenges that we have addressed in the main paper:\n\n\u2022\n\nhow to find sparse + low-rank decomposition of an attention matrix that is compute efficient (the most studied algorithm, robust PCA, is orders of magnitude too slow to be done at each training iteration) and memory efficient (i.e., without materializing the full matrix) (Section 4),\n\n\u2022\n\nif we can find such a sparse + low-rank decomposition, how accurate is the approximation (Section 4.3),\n\n\u2022\n\nhow expressive is the sparse + low-rank parameterization, i.e., are there natural classes of matrices where sparse + low-rank yields asymptotically better approximation than sparse or low-rank alone) (Section 3)? Appendix C Scatterbrain Algorithm and Implementation Details\n\nLet be the query and key matrices respectively, and be the value matrix. Let the rows of be , and the rows of be . The attention computes:\n\nsoftmax \u200b ( Q \u200b K \u22a4 ) \u200b V , softmax \ud835\udc44 superscript \ud835\udc3e top \ud835\udc49 \\mathrm{softmax}(QK^{\\top})V,\n\nwith applied row-wise, where for each vector , Here we omit the usual scaling of for simplicity since that could be folded into or . Note that , where the exponential function is applied element-wise and is a diagonal matrix containing the softmax normalization constants (). Then attention has the form . We describe the Scatterbrain approximation algorithm in Algorithm 1. This includes the normalization step. Autoregressive / Causal / Unidirectional Attention\n\nTo approximate autoregressive attention, we simply use the autoregressive variant of low-rank attention, and apply the autoregressive mask to the sparse attention. In particular, let be the autoregressive mask, whose lower triangle is all ones and the rest of the entries are zero. The unnormalized attention matrix is , and the unnormalized output is , where is elementwise multiplication. The low-rank autoregressive variant computes , though with a custom GPU kernel / implementation so as not to materialize the matrix. For the sparse component, we simply mask out locations where . That is, we can perform efficiently. As a result, we can compute the Scatterbrain output efficiently. Appendix D Proofs\n\nD.1 Expressiveness of Sparse + Low-rank Matrices\n\nTo motivate the use of sparse + low-rank matrices, we describe a family of attention matrices where sparse + low-rank matrices need asymptotically fewer parameters to approximate the attention matrix, compared to sparse or low-rank matrices alone. For there cases, either sparse or low-rank alone requires a quadratic number of parameters (, where is the dimension of the attention matrix) to get approximation error in Frobenius norm, while sparse + low-rank only requires parameters. We construct a matrix family that shows the separation between the approximation capability of sparse + low-rank vs. sparse or low-rank alone. More specifically, we will use diagonal + low-rank (a special case of sparse + low-rank). Example 1. Let denote a parameter that satisfies . Consider the following randomized construction of a matrix with and , where each entry of is picked independently and uniformly at random from . Let where is the elementwise exponential function (we first ignore the normalization term of softmax here). It can be shown (e.g. by Hoeffding\u2019s inequality) that with high probability\n\n( Q \u200b Q \u22a4 ) i , j = { 1 , if \u200b i = j ; \u2208 [ \u2212 \u03f5 , \u03f5 ] , otherwise . subscript \ud835\udc44 superscript \ud835\udc44 top \ud835\udc56 \ud835\udc57 cases 1 if \ud835\udc56 \ud835\udc57 absent italic-\u03f5 italic-\u03f5 otherwise \\displaystyle(QQ^{\\top})_{i,j}=\\begin{cases}1,&\\text{if }i=j;\\\\\n\\in[-\\epsilon,\\epsilon],&\\text{otherwise}.\\end{cases}\n\nSince where is the elementwise exponential function,\n\nM i , j = { e , if \u200b i = j ; \u2208 [ 1 \u2212 O \u200b ( \u03f5 ) , 1 + O \u200b ( \u03f5 ) ] , otherwise . subscript \ud835\udc40 \ud835\udc56 \ud835\udc57 cases \ud835\udc52 if \ud835\udc56 \ud835\udc57 absent 1 \ud835\udc42 italic-\u03f5 1 \ud835\udc42 italic-\u03f5 otherwise \\displaystyle M_{i,j}=\\begin{cases}e,&\\text{if }i=j;\\\\\n\\in[1-O(\\epsilon),1+O(\\epsilon)],&\\text{otherwise}.\\end{cases}\n\nIntuitively, as the attention matrix has large diagonal entries, low-rank matrices will not be able to approximate it well. However, the off-diagonals are also of reasonable size, thus making sparse approximation difficult. With sparse + low-rank, we can use the sparse part to represent the diagonal, and the low-rank part to represent the remaining elements, allowing it to approximate this matrix well. We formalize this separation in the theorem below. Theorem 3. Let be the attention matrix from Example 1. For any , with probability at least , there exists a sparse + low-rank estimator with parameters that achieve Frobenius error. For any matrix with rank such that (e.g., has parameters), with probability at least , we have . Moreover, any matrix that has row sparsity (each row has less than non-zeros) such that (e.g., has parameters) will have error with probability at least . We see that for any , any low-rank or sparse estimator for with parameters has times the error of the sparse + low-rank estimator with parameters.",
    "scatterbrain-39": "Proof of Theorem 3. For each , let denote the -th row of . Define to be the all 1s matrix. Define . Therefore,\n\nT i , j = { e \u2212 2 if \u200b i = j e q i \u22a4 \u200b q j \u2212 1 \u2212 q i \u22a4 \u200b q j otherwise . subscript \ud835\udc47 \ud835\udc56 \ud835\udc57 cases \ud835\udc52 2 if \ud835\udc56 \ud835\udc57 superscript \ud835\udc52 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 1 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 otherwise \\displaystyle T_{i,j}=\\begin{cases}e-2&\\text{if }i=j\\\\\ne^{q_{i}^{\\top}q_{j}}-1-q_{i}^{\\top}q_{j}&\\text{otherwise}\\end{cases}. By Hoeffding\u2019s inequality, for a pair , we have that\n\n\u2119 \u200b ( | q i \u22a4 \u200b q j \u2212 \ud835\udd3c \u200b [ q i \u22a4 \u200b q j ] | \u2265 \u03f5 ) \u2264 2 \u200b exp \u2061 ( \u2212 2 \u200b \u03f5 2 ( 1 d \u2212 \u2212 1 d ) 2 ) = 2 \u200b exp \u2061 ( \u2212 d \u200b \u03f5 2 / 2 ) . \u2119 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 \ud835\udd3c delimited-[] superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 italic-\u03f5 2 2 superscript italic-\u03f5 2 superscript 1 \ud835\udc51 1 \ud835\udc51 2 2 \ud835\udc51 superscript italic-\u03f5 2 2 \\mathbb{P}\\left(\\left\\lvert q_{i}^{\\top}q_{j}-\\mathbb{E}[q_{i}^{\\top}q_{j}]\\right\\rvert\\geq\\epsilon\\right)\\leq 2\\exp\\left(-\\frac{2\\epsilon^{2}}{\\left(\\frac{1}{\\sqrt{d}}-\\frac{-1}{\\sqrt{d}}\\right)^{2}}\\right)=2\\exp(-d\\epsilon^{2}/2). Note that . By a union bound over all pairs (there are such pairs), with probability at least , we have that\n\nq i \u22a4 \u200b q j \u2208 [ \u2212 \u03f5 , \u03f5 ] for all \u200b i \u2260 j . formulae-sequence superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 italic-\u03f5 italic-\u03f5 for all \ud835\udc56 \ud835\udc57 q_{i}^{\\top}q_{j}\\in[-\\epsilon,\\epsilon]\\quad\\text{for all }i\\neq j. Since we assume that , we have that\n\nn 2 \u200b exp \u2061 ( \u2212 d \u200b \u03f5 2 / 2 ) \u2264 n 2 \u200b exp \u2061 ( \u2212 3 \u200b log \u2061 n ) = n \u2212 1 . superscript \ud835\udc5b 2 \ud835\udc51 superscript italic-\u03f5 2 2 superscript \ud835\udc5b 2 3 \ud835\udc5b superscript \ud835\udc5b 1 n^{2}\\exp(-d\\epsilon^{2}/2)\\leq n^{2}\\exp(-3\\log n)=n^{-1}. Hence for all with probability at least . For the rest of the proof, we only consider this case (where for all ). Since for , we can bound the off diagonal elements . In particular, for all ,\n\n| T i \u200b j | = | e q i \u22a4 \u200b q j \u2212 1 \u2212 q i \u22a4 \u200b q j | \u2264 ( q i \u22a4 \u200b q j ) \u2264 \u03f5 2 . subscript \ud835\udc47 \ud835\udc56 \ud835\udc57 superscript \ud835\udc52 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 1 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 superscript italic-\u03f5 2 \\left\\lvert T_{ij}\\right\\rvert=\\left\\lvert e^{q_{i}^{\\top}q_{j}}-1-q_{i}^{\\top}q_{j}\\right\\rvert\\leq\\left(q_{i}^{\\top}q_{j}\\right)\\leq\\epsilon^{2}. (4)\n\nSparse + low-rank estimator:\n\nWe use the following sparse + low-rank estimator:\n\nE SL = ( e \u2212 2 ) \u22c5 I \u23df sparse + J + Q \u200b Q \u22a4 \u23df low \u2212 rank , subscript \ud835\udc38 SL subscript \u23df \u22c5 \ud835\udc52 2 \ud835\udc3c sparse subscript \u23df \ud835\udc3d \ud835\udc44 superscript \ud835\udc44 top low rank \\displaystyle E_{\\mathrm{SL}}=\\underbrace{(e-2)\\cdot I}_{\\mathrm{sparse}}+\\underbrace{J+QQ^{\\top}}_{\\mathrm{low-rank}},\n\nwhere has row sparsity 1 and . Notice that the estimate matches exactly on the diagonal, and on the off-diagonal it differs from by . Thus, the Frobenious error of the sparse + low-rank estimator is\n\n\u2016 M \u2212 E SL \u2016 F \u2264 \u03f5 2 \u200b n \u200b ( n \u2212 1 ) \u2264 \u03f5 2 \u200b n . subscript norm \ud835\udc40 subscript \ud835\udc38 SL \ud835\udc39 superscript italic-\u03f5 2 \ud835\udc5b \ud835\udc5b 1 superscript italic-\u03f5 2 \ud835\udc5b \\|M-E_{\\mathrm{SL}}\\|_{F}\\leq\\epsilon^{2}\\sqrt{n(n-1)}\\leq\\epsilon^{2}n. Set for , Then\n\n(i) The sparse + low-rank parameter count is . (ii) The Frobenius error is . Low-rank estimator:\n\nWe want to argue that low-rank approximation would require more parameters. If we approximate the matrix by a matrix with rank , then the difference matrix will have at least singular values of magnitude . As a result, by the Eckart\u2013Young\u2013Mirsky theorem,\n\n\u2016 ( e \u2212 2 ) \u22c5 I \u2212 R \u2016 F \u2265 1 2 \u200b n \u2212 r . subscript norm \u22c5 \ud835\udc52 2 \ud835\udc3c \ud835\udc45 \ud835\udc39 1 2 \ud835\udc5b \ud835\udc5f \\|(e-2)\\cdot I-R\\|_{F}\\geq\\frac{1}{2}\\sqrt{n-r}. Define , then is all 0 on the diagonal and has absolute value on off-diagonal entries. Thus . We want to show that if is a rank matrix, then . We argue by contradiction. Suppose that there exists some matrix with rank such that\n\n\u2016 M \u2212 R \u2032 \u2016 F \u2264 1 2 \u200b n \u2212 r \u2032 \u2212 d \u2212 1 \u2212 \u2016 T \u2032 \u2016 F . subscript norm \ud835\udc40 superscript \ud835\udc45 \u2032 \ud835\udc39 1 2 \ud835\udc5b superscript \ud835\udc5f \u2032 \ud835\udc51 1 subscript norm superscript \ud835\udc47 \u2032 \ud835\udc39 \\|M-R^{\\prime}\\|_{F}\\leq\\frac{1}{2}\\sqrt{n-r^{\\prime}-d-1}-\\|T^{\\prime}\\|_{F}. Define , so . We see that:\n\n\u2016 ( e \u2212 2 ) \u22c5 I \u2212 R \u2016 F subscript norm \u22c5 \ud835\udc52 2 \ud835\udc3c \ud835\udc45 \ud835\udc39 \\displaystyle\\|(e-2)\\cdot I-R\\|_{F} = \u2016 M \u2212 R \u2032 \u2212 T \u2032 \u2016 F absent subscript norm \ud835\udc40 superscript \ud835\udc45 \u2032 superscript \ud835\udc47 \u2032 \ud835\udc39 \\displaystyle=\\|M-R^{\\prime}-T^{\\prime}\\|_{F} \u2264 \u2016 M \u2212 R \u2032 \u2016 F + \u2016 T \u2032 \u2016 F absent subscript norm \ud835\udc40 superscript \ud835\udc45 \u2032 \ud835\udc39 subscript norm superscript \ud835\udc47 \u2032 \ud835\udc39 \\displaystyle\\leq\\|M-R^{\\prime}\\|_{F}+\\|T^{\\prime}\\|_{F} \u2264 1 2 \u200b n \u2212 r \u2032 \u2212 d \u2212 1 absent 1 2 \ud835\udc5b superscript \ud835\udc5f \u2032 \ud835\udc51 1 \\displaystyle\\leq\\frac{1}{2}\\sqrt{n-r^{\\prime}-d-1} \u2264 1 2 \u200b n \u2212 rank \u200b ( R ) . absent 1 2 \ud835\udc5b rank \ud835\udc45 \\displaystyle\\leq\\frac{1}{2}\\sqrt{n-\\mathrm{rank}(R)}. This contradicts the result above, which states that . Therefore any low-rank estimator with rank such that , which has parameters, will have error at least , which is times the error of the sparse + low-rank estimator above. Sparse estimator:\n\nFor our sparse estimator, it is easy to see that for any that has row sparsity (each row has fewer than non-zeros),\n\n\u2016 M \u2212 E S \u2016 F \u2265 \u03a9 \u200b ( n \u200b ( n \u2212 k ) ) . subscript norm \ud835\udc40 subscript \ud835\udc38 S \ud835\udc39 \u03a9 \ud835\udc5b \ud835\udc5b \ud835\udc58 \\displaystyle\\|M-E_{\\mathrm{S}}\\|_{F}\\geq\\Omega(\\sqrt{n(n-k)}). This implies that in order to achieve error , we would need , which requires parameters. Now we construct a matrix that shows better separation between the approximation capability of sparse + low-rank vs sparse or low-rank alone. Example 2. Consider the following randomized construction of matrix with and ( and close to 0 and is ): each entry of is picked independently and uniformly at random from . Let where is the elementwise exponential function. Similar to Example 1, with high probability, we have:\n\n( Q \u200b Q \u22a4 ) i , j = { r , if \u200b i = j ; \u2208 [ \u2212 \u03f5 , \u03f5 ] , otherwise . subscript \ud835\udc44 superscript \ud835\udc44 top \ud835\udc56 \ud835\udc57 cases \ud835\udc5f if \ud835\udc56 \ud835\udc57 absent italic-\u03f5 italic-\u03f5 otherwise \\displaystyle(QQ^{\\top})_{i,j}=\\begin{cases}r,&\\text{if }i=j;\\\\\n\\in[-\\epsilon,\\epsilon],&\\text{otherwise}.\\end{cases}\n\nWe also have:\n\nM i , j = { e r , if \u200b i = j ; \u2208 [ 1 \u2212 O \u200b ( \u03f5 ) , 1 + O \u200b ( \u03f5 ) ] , otherwise . subscript \ud835\udc40 \ud835\udc56 \ud835\udc57 cases superscript \ud835\udc52 \ud835\udc5f if \ud835\udc56 \ud835\udc57 absent 1 \ud835\udc42 italic-\u03f5 1 \ud835\udc42 italic-\u03f5 otherwise \\displaystyle M_{i,j}=\\begin{cases}e^{r},&\\text{if }i=j;\\\\\n\\in[1-O(\\epsilon),1+O(\\epsilon)],&\\text{otherwise}.\\end{cases}\n\nBy setting appropriately, we can formalize the separation between the approximation ability of sparse, low-rank, and sparse + low-rank matrices:\n\nTheorem 4. Let be the attention matrix from Example 2. Any sparse or low-rank estimator of needs parameters for error with probability at least while a sparse + low-rank estimator needs parameters for error with probability at least . Proof of Theorem 4. Similar to the proof of Theorem 3, by Hoeffding\u2019s inequality, for a pair , we have that\n\n\u2119 \u200b ( | q i \u22a4 \u200b q j \u2212 \ud835\udd3c \u200b [ q i \u22a4 \u200b q j ] | \u2265 \u03f5 ) \u2264 2 \u200b exp \u2061 ( \u2212 2 \u200b \u03f5 2 ( r d \u2212 \u2212 r d ) 2 ) = 2 \u200b exp \u2061 ( \u2212 d \u200b \u03f5 2 2 \u200b r ) . \u2119 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 \ud835\udd3c delimited-[] superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 italic-\u03f5 2 2 superscript italic-\u03f5 2 superscript \ud835\udc5f \ud835\udc51 \ud835\udc5f \ud835\udc51 2 2 \ud835\udc51 superscript italic-\u03f5 2 2 \ud835\udc5f \\mathbb{P}\\left(\\left\\lvert q_{i}^{\\top}q_{j}-\\mathbb{E}[q_{i}^{\\top}q_{j}]\\right\\rvert\\geq\\epsilon\\right)\\leq 2\\exp\\left(-\\frac{2\\epsilon^{2}}{\\left(\\frac{r}{\\sqrt{d}}-\\frac{-r}{\\sqrt{d}}\\right)^{2}}\\right)=2\\exp\\left(-\\frac{d\\epsilon^{2}}{2r}\\right). Note that . By a union bound over all pairs (there are such pairs), with probability at least (since ), we have that\n\nq i \u22a4 \u200b q j \u2208 [ \u2212 \u03f5 , \u03f5 ] for all \u200b i \u2260 j . formulae-sequence superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc5e \ud835\udc57 italic-\u03f5 italic-\u03f5 for all \ud835\udc56 \ud835\udc57 q_{i}^{\\top}q_{j}\\in[-\\epsilon,\\epsilon]\\quad\\text{for all }i\\neq j.",
    "scatterbrain-40": "Since we assume that , we have that For the rest of the proof, we only consider this case (where for all ). Let , where is the all one matrix. We see that is zero on the diagonal. Moreover, using the fact that for all , the off-diagonal entries of have of magnitude at most . We consider 3 different estimators. Sparse + low-rank estimator:\n\nOur estimator is\n\nE SL = ( e r \u2212 1 ) \u22c5 I \u23df sparse + J \u23df low \u2212 rank , subscript \ud835\udc38 SL subscript \u23df \u22c5 superscript \ud835\udc52 \ud835\udc5f 1 \ud835\udc3c sparse subscript \u23df \ud835\udc3d low rank \\displaystyle E_{\\mathrm{SL}}=\\underbrace{(e^{r}-1)\\cdot I}_{\\mathrm{sparse}}+\\underbrace{J}_{\\mathrm{low-rank}},\n\nwhere has row sparsity 1 and . The Frobenious error of sparse + low-rank approximation is\n\n\u2016 M \u2212 E SL \u2016 F \u2264 O \u200b ( \u03f5 2 \u200b n \u200b ( n \u2212 1 ) ) \u2264 O \u200b ( \u03f5 \u200b n ) . subscript norm \ud835\udc40 subscript \ud835\udc38 SL \ud835\udc39 \ud835\udc42 superscript italic-\u03f5 2 \ud835\udc5b \ud835\udc5b 1 \ud835\udc42 italic-\u03f5 \ud835\udc5b \\|M-E_{\\mathrm{SL}}\\|_{F}\\leq O(\\sqrt{\\epsilon^{2}n(n-1)})\\leq O(\\epsilon n). We have that:\n\n(i) Sparse + low-rank parameter count is . (ii) Its Frobenius error is . Low-rank estimator: We want to argue that low-rank approximation would require more parameters. From a similar observation that any matrix with rank that ,\n\n\u2016 ( e r \u2212 1 ) \u200b I \u2212 R \u2016 F \u2265 \u03a9 \u200b ( e r ) , subscript norm superscript \ud835\udc52 \ud835\udc5f 1 \ud835\udc3c \ud835\udc45 \ud835\udc39 \u03a9 superscript \ud835\udc52 \ud835\udc5f \\|(e^{r}-1)I-R\\|_{F}\\geq\\Omega(e^{r}),\n\n(by Eckart\u2013Young\u2013Mirsky theorem), we obtain a similar result to the proof of Theorem 3. If is a matrix with rank such that , then . Hence any low-rank matrix with parameters would have error . Sparse estimator:\n\nSimilar to the proof of Theorem 3, for our sparse estimator, it is easy to see that for any that has row sparsity (each row has fewer than non-zeros),\n\n\u2016 M \u2212 E S \u2016 F \u2265 \u03a9 \u200b ( n \u200b ( n \u2212 k ) ) .",
    "scatterbrain-41": "subscript norm \ud835\udc40 subscript \ud835\udc38 S \ud835\udc39 \u03a9 \ud835\udc5b \ud835\udc5b \ud835\udc58 \\displaystyle\\|M-E_{\\mathrm{S}}\\|_{F}\\geq\\Omega(\\sqrt{n(n-k)}). This implies that to get error, we would need parameters. \u220e\n\nD.2 Generative Model, Softmax Temperature, and Matrix Approximation\n\nHere we show 3 cases where depending on the softmax temperature, either we\u2019ll need low-rank, low-rank + sparse, or sparse to approximate the attention matrix.",
    "scatterbrain-42": "We start with some notation first. Given a matrix , let be the entry in the th row and th column. For a range , we define a matrix where if and otherwise (that is, only keep entries for that are in the range , with other entries zeroed out). We write for the set of locations of non-zeros in . We let be the -th largest (in absolute value) eigenvalue of . To prove Theorem 1, we first define a more general matrix class, prove that the attention matrix in 1 is a subset of this class (with high probability), and then show that Theorem 1 holds for this more general class. We introduce an extra parameter , in addition to the inverse temperature and the intro-cluster distance . Matrix Class 1. Let with every row of having -norm in , and let . Further:\n\n1. Let = for some . Assume that is block diagonal with blocks, and is . That is, the large entries of form a block diagonal matrix.",
    "scatterbrain-43": "2. Let then where . Assume that there is a constant fraction of elements in falling in .",
    "scatterbrain-44": "Assume that is . Let . We now show that 1 is a subset of 1, with high probability. Lemma 5. The matrix in 1 is a subset of 1, where . Proof. We first bound the norm of each row in in 1. For any , we have\n\n\u2016 z i \u200b j \u2016 2 = \u2016 c i + r i \u200b j \u2016 2 = \u2016 c i \u2016 2 + 2 \u200b c i \u22a4 \u200b r i \u200b j + \u2016 r i \u200b j \u2016 2 . superscript norm subscript \ud835\udc67 \ud835\udc56 \ud835\udc57 2 superscript norm subscript \ud835\udc50 \ud835\udc56 subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 2 superscript norm subscript \ud835\udc50 \ud835\udc56 2 2 superscript subscript \ud835\udc50 \ud835\udc56 top subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 superscript norm subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 2 \\left\\|{z_{ij}}\\right\\|^{2}=\\left\\|{c_{i}+r_{ij}}\\right\\|^{2}=\\left\\|{c_{i}}\\right\\|^{2}+2c_{i}^{\\top}r_{ij}+\\left\\|{r_{ij}}\\right\\|^{2}. Since , with probability at least (by the standard argument using the fact that -random variables are sub-exponential). Similarly, with probability at least . By concentration of measure, we can also bound as well. Therefore, we have that . Now we show that the large entries of form a block diagonal matrix. With high probability, the large entries come from intra-cluster dot product, and the small entries come from inter-cluster dot product. We bound the intra-cluster dot product:\n\nz i \u200b j \u22a4 \u200b z i \u200b k superscript subscript \ud835\udc67 \ud835\udc56 \ud835\udc57 top subscript \ud835\udc67 \ud835\udc56 \ud835\udc58 \\displaystyle z_{ij}^{\\top}z_{ik} = ( c i + r i \u200b j ) \u22a4 \u200b ( c i + r i \u200b k ) absent superscript subscript \ud835\udc50 \ud835\udc56 subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 top subscript \ud835\udc50 \ud835\udc56 subscript \ud835\udc5f \ud835\udc56 \ud835\udc58 \\displaystyle=(c_{i}+r_{ij})^{\\top}(c_{i}+r_{ik}) = \u2016 c i \u2016 2 + c i \u22a4 \u200b r i \u200b j + c i \u22a4 \u200b r i \u200b k + r i \u200b j \u22a4 \u200b r i \u200b k . absent superscript norm subscript \ud835\udc50 \ud835\udc56 2 superscript subscript \ud835\udc50 \ud835\udc56 top subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc50 \ud835\udc56 top subscript \ud835\udc5f \ud835\udc56 \ud835\udc58 superscript subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 top subscript \ud835\udc5f \ud835\udc56 \ud835\udc58 \\displaystyle=\\left\\|{c_{i}}\\right\\|^{2}+c_{i}^{\\top}r_{ij}+c_{i}^{\\top}r_{ik}+r_{ij}^{\\top}r_{ik}. Similar to the argument above, by concentration of measure, with high probability (we will pick ). The cross terms and can be bounded using Cauchy-Schwarz inequality to be in with high probability. And the fourth term is in with high probability. Therefore, the inner product is in with high probability. This satisfies the first condition in 1, for , assuming . We use a similar argument to bound the inter-cluster dot product. For\n\nz i \u200b j \u22a4 \u200b z i \u2032 \u200b k superscript subscript \ud835\udc67 \ud835\udc56 \ud835\udc57 top subscript \ud835\udc67 superscript \ud835\udc56 \u2032 \ud835\udc58 \\displaystyle z_{ij}^{\\top}z_{i^{\\prime}k} = ( c i + r i \u200b j ) \u22a4 \u200b ( c i \u2032 + r i \u2032 \u200b k ) absent superscript subscript \ud835\udc50 \ud835\udc56 subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 top subscript \ud835\udc50 superscript \ud835\udc56 \u2032 subscript \ud835\udc5f superscript \ud835\udc56 \u2032 \ud835\udc58 \\displaystyle=(c_{i}+r_{ij})^{\\top}(c_{i^{\\prime}}+r_{i^{\\prime}k}) = c i \u22a4 \u200b c i \u2032 \u22a4 + c i \u22a4 \u200b r i \u2032 \u200b k + c i \u2032 \u22a4 \u200b r i \u200b j + r i \u200b j \u22a4 \u200b r i \u2032 \u200b k . absent superscript subscript \ud835\udc50 \ud835\udc56 top superscript subscript \ud835\udc50 superscript \ud835\udc56 \u2032 top superscript subscript \ud835\udc50 \ud835\udc56 top subscript \ud835\udc5f superscript \ud835\udc56 \u2032 \ud835\udc58 superscript subscript \ud835\udc50 superscript \ud835\udc56 \u2032 top subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 top subscript \ud835\udc5f superscript \ud835\udc56 \u2032 \ud835\udc58 \\displaystyle=c_{i}^{\\top}c_{i^{\\prime}}^{\\top}+c_{i}^{\\top}r_{i^{\\prime}k}+c_{i^{\\prime}}^{\\top}r_{ij}+r_{ij}^{\\top}r_{i^{\\prime}k}. By concentration of measure, . Similar to the argument in the intra-cluster case, we can bound the other three terms, so this dot product is in . This satisfies the second condition in 1. To prove Theorem 1 for 1, we start with some technical lemmas. Lemma 6. Let be a symmetric matrix. Let be the largest eigenvalue of . Assuming , we have that\n\n\u03bb max \u2265 min i \u2260 j \u2061 F \u200b [ i , j ] . subscript \ud835\udf06 subscript \ud835\udc56 \ud835\udc57 \ud835\udc39 \ud835\udc56 \ud835\udc57 \\lambda_{\\max}\\geq\\min_{i\\neq j}F[i,j]. Proof. Since is symmetric, is real and\n\n\u03bb max = max u \u2260 0 \u2061 u \u22a4 \u200b F \u200b u u T \u200b u . subscript \ud835\udf06 subscript \ud835\udc62 0 superscript \ud835\udc62 top \ud835\udc39 \ud835\udc62 superscript \ud835\udc62 \ud835\udc47 \ud835\udc62 \\lambda_{\\max}=\\max_{u\\neq 0}\\frac{u^{\\top}Fu}{u^{T}u}. Let be the all 1\u2019s vector, then\n\n\u03bb max \u2265 subscript \ud835\udf06 absent \\displaystyle\\lambda_{\\max}\\geq 1 N \u200b \u2211 i = j F \u200b [ i , j ] 1 \ud835\udc41 subscript \ud835\udc56 \ud835\udc57 \ud835\udc39 \ud835\udc56 \ud835\udc57 \\displaystyle\\leavevmode\\nobreak\\ \\frac{1}{N}\\sum_{i=j}F[i,j] \u2265 \\displaystyle\\geq 1 N \u200b \u2211 i \u2260 j F \u200b [ i , j ] 1 \ud835\udc41 subscript \ud835\udc56 \ud835\udc57 \ud835\udc39 \ud835\udc56 \ud835\udc57 \\displaystyle\\leavevmode\\nobreak\\ \\frac{1}{N}\\sum_{i\\neq j}F[i,j] \u2265 \\displaystyle\\geq 1 N \u22c5 N \u200b ( N \u2212 1 ) \u200b min i \u2260 j \u2061 F \u200b [ i , j ] \u22c5 1 \ud835\udc41 \ud835\udc41 \ud835\udc41 1 subscript \ud835\udc56 \ud835\udc57 \ud835\udc39 \ud835\udc56 \ud835\udc57 \\displaystyle\\leavevmode\\nobreak\\ \\frac{1}{N}\\cdot N(N-1)\\min_{i\\neq j}F[i,j] \u2265 \\displaystyle\\geq min i \u2260 j \u2061 F \u200b [ i , j ] , subscript \ud835\udc56 \ud835\udc57 \ud835\udc39 \ud835\udc56 \ud835\udc57 \\displaystyle\\leavevmode\\nobreak\\ \\min_{i\\neq j}F[i,j],\n\nwhere the second step follows from all the diagonal entries are non-negative, the last step follows from \u220e\n\nThe above implies the following result:\n\nCorollary 7. Let be a block diagonal matrix. Let be the number of blocks in for some . The is at least the smallest non-diagonal element in any block () in . Proof. By Lemma 6, each block () by itself has max eigenvalue at least . The claim then follows from the fact that any eigenvalue of is also an eigenvalue of . \u220e\n\nWe\u2019ll need the following function for our low-rank argument:\n\nf k \u200b ( x ) = \u2211 i = 0 k x i i ! . subscript \ud835\udc53 \ud835\udc58 \ud835\udc65 superscript subscript \ud835\udc56 0 \ud835\udc58 superscript \ud835\udc65 \ud835\udc56 \ud835\udc56 f_{k}(x)=\\sum_{i=0}^{k}\\frac{x^{i}}{i!}. Note that .",
    "scatterbrain-45": "Definition 1. Let and . We say a function is -close to if\n\n| e y \u2212 f \u200b ( y ) | \u2264 \u03f5 for any \u200b y \u2208 [ \u2212 L , L ] . formulae-sequence superscript \ud835\udc52 \ud835\udc66 \ud835\udc53 \ud835\udc66 italic-\u03f5 for any \ud835\udc66 \ud835\udc3f \ud835\udc3f |e^{y}-f(y)|\\leq\\epsilon\\quad\\text{ for any }y\\in[-L,L]. Lemma 8. For any and . If\n\nD \u2265 10 \u200b ( L + log \u2061 ( 1 / \u03f5 ) ) \ud835\udc37 10 \ud835\udc3f 1 italic-\u03f5 D\\geq 10(L+\\log(1/\\epsilon))\n\nthen function is -close to . Proof. Recall the definition of function ,\n\ne x = f D \u200b ( x ) + \u2211 i = D + 1 \u221e x i i ! , superscript \ud835\udc52 \ud835\udc65 subscript \ud835\udc53 \ud835\udc37 \ud835\udc65 superscript subscript \ud835\udc56 \ud835\udc37 1 superscript \ud835\udc65 \ud835\udc56 \ud835\udc56 \\displaystyle e^{x}=f_{D}(x)+\\sum_{i=D+1}^{\\infty}\\frac{x^{i}}{i!},\n\nIt is sufficient to show that if we have\n\nx D + 1 ( D + 1 ) ! \u2264 \u03f5 2 , superscript \ud835\udc65 \ud835\udc37 1 \ud835\udc37 1 italic-\u03f5 2 \\displaystyle\\frac{x^{D+1}}{(D+1)!}\\leq\\frac{\\epsilon}{2},\n\nWe can show that\n\ny D D ! \u2264 superscript \ud835\udc66 \ud835\udc37 \ud835\udc37 absent \\displaystyle\\frac{y^{D}}{D!}\\leq L D D ! superscript \ud835\udc3f \ud835\udc37 \ud835\udc37 \\displaystyle\\leavevmode\\nobreak\\ \\frac{L^{D}}{D!} \u2264 \\displaystyle\\leq L D ( D / 4 ) D superscript \ud835\udc3f \ud835\udc37 superscript \ud835\udc37 4 \ud835\udc37 \\displaystyle\\leavevmode\\nobreak\\ \\frac{L^{D}}{(D/4)^{D}} = \\displaystyle= ( 4 \u200b L D ) D superscript 4 \ud835\udc3f \ud835\udc37 \ud835\udc37 \\displaystyle\\leavevmode\\nobreak\\ (\\frac{4L}{D})^{D} \u2264 \\displaystyle\\leq ( 1 / 2 ) D superscript 1 2 \ud835\udc37 \\displaystyle\\leavevmode\\nobreak\\ (1/2)^{D} \u2264 \\displaystyle\\leq \u03f5 / 10 italic-\u03f5 10 \\displaystyle\\leavevmode\\nobreak\\ \\epsilon/10\n\nwhere the first step follows from , the second step follows , the forth step follows from , the last step follows and . We\u2019ll also use the following fact:\n\nLemma 9. For any , we have\n\nrank \u200b ( f D ) \u2264 n o \u200b ( 1 ) . rank subscript \ud835\udc53 \ud835\udc37 superscript \ud835\udc5b \ud835\udc5c 1 \\mathrm{rank}(f_{D})\\leq n^{o(1)}. Proof. We can upper bound in the following sense:\n\nrank \u200b ( f D \u200b ( A ) ) \u2264 rank subscript \ud835\udc53 \ud835\udc37 \ud835\udc34 absent \\displaystyle\\mathrm{rank}(f_{D}(A))\\leq ( rank \u200b ( A ) ) D superscript rank \ud835\udc34 \ud835\udc37 \\displaystyle\\leavevmode\\nobreak\\ (\\mathrm{rank}(A))^{D} \u2264 \\displaystyle\\leq d D superscript \ud835\udc51 \ud835\udc37 \\displaystyle\\leavevmode\\nobreak\\ d^{D} = \\displaystyle= 2 D \u22c5 log \u2061 d superscript 2 \u22c5 \ud835\udc37 \ud835\udc51 \\displaystyle\\leavevmode\\nobreak\\ 2^{D\\cdot\\log d} = \\displaystyle= 2 o \u200b ( log \u2061 n ) superscript 2 \ud835\udc5c \ud835\udc5b \\displaystyle\\leavevmode\\nobreak\\ 2^{o(\\log n)} = \\displaystyle= n o \u200b ( 1 ) .",
    "scatterbrain-46": "superscript \ud835\udc5b \ud835\udc5c 1 \\displaystyle\\leavevmode\\nobreak\\ n^{o(1)}.",
    "scatterbrain-47": "where the second step follows from , the forth step follows from . \u220e\n\nFinally we\u2019re ready to prove the theorem:\n\nProof. The basic idea is: (i) Use to get the low-rank approximation (ii) Use to get the sparse part. Small range,\n\ni.e., is . Low rank approximation: . Since each entry of is in , each entry of is in . But note that in this case is . By the definition of , each entry of has absolute value . Therefore the overall error is . For sparse only: By assumption, entries in are , which are exactly the entries in that are . Hence any (say) sparse approximation has error . By our assumption, . Mid-range ,\n\ni.e., and is . Sparse only: the argument is the same as in the low range. Sparse + low-rank: The low-rank part . By Lemma 9, this has rank , so it has parameters. The sparse part is . Clearly this needs parameters. Let . Then (i) in , is all . (ii) output of , by definition, entries of are in , which in the current range of is . Therefore all the entries of have absolute value . By the definition of , we have that . Low-rank only: Let be rank that approximates . Then using the same argument as our existing lower bound argument, we get that (this means that the error ). Now note that is a symmetric, block diagonal matrix with blocks. Corollary 7 implies that is at least the smallest non-diagonal value in . Now the smallest non-diagonal value in is . On the other hand, the largest value in is\n\n\u2264 k \u2217 \u200b \u03b2 k \u2217 k \u2217 ! \u2264 \u03b2 \u22c5 ( e \u200b \u03b2 k \u2217 \u2212 1 ) k \u2217 \u2212 1 absent superscript \ud835\udc58 superscript \ud835\udefd superscript \ud835\udc58 superscript \ud835\udc58 \u22c5 \ud835\udefd superscript \ud835\udc52 \ud835\udefd superscript \ud835\udc58 1 superscript \ud835\udc58 1 \\displaystyle\\leq k^{*}\\frac{\\beta^{k^{*}}}{k^{*}!}\\leq\\beta\\cdot\\left(\\frac{e\\beta}{k^{*}-1}\\right)^{k^{*}-1} \u2272 log \u2061 n \u200b ( e \u22c5 log \u2061 n log \u2061 n \u22c5 \u0394 ) O \u200b ( log \u2061 n \u22c5 \u0394 ) less-than-or-similar-to absent \ud835\udc5b superscript \u22c5 \ud835\udc52 \ud835\udc5b \u22c5 \ud835\udc5b \u0394 \ud835\udc42 \u22c5 \ud835\udc5b \u0394 \\displaystyle\\lesssim\\log n\\left(\\frac{e\\cdot\\log n}{\\log n\\cdot\\Delta}\\right)^{O(\\log n\\cdot\\Delta)} \u2272 log \u2061 n \u200b e O \u200b ( log \u2061 n \u22c5 \u0394 \u22c5 log \u2061 1 \u0394 ) less-than-or-similar-to absent \ud835\udc5b superscript \ud835\udc52 \ud835\udc42 \u22c5 \ud835\udc5b \u0394 1 \u0394 \\displaystyle\\lesssim\\log ne^{O(\\log n\\cdot\\Delta\\cdot\\log\\frac{1}{\\Delta})} \u2272 log \u2061 n \u22c5 n o \u200b ( 1 ) less-than-or-similar-to absent \u22c5 \ud835\udc5b superscript \ud835\udc5b \ud835\udc5c 1 \\displaystyle\\lesssim\\log n\\cdot n^{o(1)} = n o \u200b ( 1 ) .",
    "scatterbrain-48": "absent superscript \ud835\udc5b \ud835\udc5c 1 \\displaystyle=n^{o(1)}. Hence is . The claimed result then follows since and (Eckart-Young-Mirsky theorem). Large range,\n\ni.e., .",
    "scatterbrain-49": "Sparse only: . Note that each entry in is upper bounded by . Then\n\n\u2016 E \u2016 F subscript norm \ud835\udc38 \ud835\udc39 \\displaystyle\\left\\|{E}\\right\\|_{F} \u2264 n \u22c5 e o \u200b ( \u03b2 log \u2061 d ) absent \u22c5 \ud835\udc5b superscript \ud835\udc52 \ud835\udc5c \ud835\udefd \ud835\udc51 \\displaystyle\\leq n\\cdot e^{o\\left(\\frac{\\beta}{\\log d}\\right)} \u2264 \u03f5 \u22c5 e log \u2061 n \u03f5 + o \u200b ( \u03b2 log \u2061 d ) absent \u22c5 italic-\u03f5 superscript \ud835\udc52 \ud835\udc5b italic-\u03f5 \ud835\udc5c \ud835\udefd \ud835\udc51 \\displaystyle\\leq\\epsilon\\cdot e^{\\log\\frac{n}{\\epsilon}+o\\left(\\frac{\\beta}{\\log d}\\right)} \u2264 \u03f5 \u22c5 e o \u200b ( \u03b2 ) + o \u200b ( \u03b2 log \u2061 d ) absent \u22c5 italic-\u03f5 superscript \ud835\udc52 \ud835\udc5c \ud835\udefd \ud835\udc5c \ud835\udefd \ud835\udc51 \\displaystyle\\leq\\epsilon\\cdot e^{o(\\beta)+o\\left(\\frac{\\beta}{\\log d}\\right)} \u2264 \u03f5 \u22c5 e o \u200b ( \u03b2 ) absent \u22c5 italic-\u03f5 superscript \ud835\udc52 \ud835\udc5c \ud835\udefd \\displaystyle\\leq\\epsilon\\cdot e^{o(\\beta)} \u2264 \u03f5 \u22c5 e \u03b2 / l . absent \u22c5 italic-\u03f5 superscript \ud835\udc52 \ud835\udefd \ud835\udc59 \\displaystyle\\leq\\epsilon\\cdot e^{\\beta/l}. Low-rank only: since is , it is enough to argue that any rank -approximation to has error . But the latter follows since . This is because is symmetric and each entry in is . Then we can use Corollary 7. Eckart-Young-Mirsky then completes the proof. \u220e\n\nD.3 Scatterbrain: Analysis\n\nHere we prove Theorem 2, which shows that Scatterbrain approximation is unbiased and analyses its variance. We restate the theorem here for the reader\u2019s convenience. Theorem. Define , as Performer\u2019s estimator and as Scatterbrain estimator. Denote as the unit sphere. Suppose are such that . Then:\n\n\ud835\udd3c \u200b [ \u03c3 ^ \ud835\uddcc\ud835\uddbb\ud835\uddbe \u200b ( q , k ) ] = \u03c3 \u200b ( q , k ) , Var \u200b [ \u03c3 ^ \ud835\uddcc\ud835\uddbb\ud835\uddbe \u200b ( q , k ) ] = ( 1 \u2212 p ) \u22c5 Var \u200b [ \u03c3 ^ \ud835\uddc9\ud835\uddbf\ud835\uddbe \u200b ( q , k ) ] < Var \u200b [ \u03c3 ^ \ud835\uddc9\ud835\uddbf\ud835\uddbe \u200b ( q , k ) ] formulae-sequence \ud835\udd3c delimited-[] superscript ^ \ud835\udf0e \ud835\uddcc\ud835\uddbb\ud835\uddbe \ud835\udc5e \ud835\udc58 \ud835\udf0e \ud835\udc5e \ud835\udc58 Var delimited-[] superscript ^ \ud835\udf0e \ud835\uddcc\ud835\uddbb\ud835\uddbe \ud835\udc5e \ud835\udc58 \u22c5 1 \ud835\udc5d Var delimited-[] superscript ^ \ud835\udf0e \ud835\uddc9\ud835\uddbf\ud835\uddbe \ud835\udc5e \ud835\udc58 Var delimited-[] superscript ^ \ud835\udf0e \ud835\uddc9\ud835\uddbf\ud835\uddbe \ud835\udc5e \ud835\udc58 \\displaystyle\\mathbb{E}[\\widehat{\\sigma}^{\\mathsf{sbe}}(q,k)]=\\sigma(q,k),\\quad\\mathrm{Var}[\\widehat{\\sigma}^{\\mathsf{sbe}}(q,k)]=(1-p)\\cdot\\mathrm{Var}[\\widehat{\\sigma}^{\\mathsf{pfe}}(q,k)]<\\mathrm{Var}[\\widehat{\\sigma}^{\\mathsf{pfe}}(q,k)]\n\nwhere . Proof. Let be -entry of the unnormalized attention matrix, the entry of the low-rank approximation (Performer), and let be the entry of the Scatterbrain (sparse + low-rank) approximation. By the construction of the Scatterbrain attention matrix (Eq. 1), if , where is the set of indices selected by the LSH, then:\n\nA i \u200b j sb = ( Q ~ \u200b K ~ \u22a4 + S ) i \u200b j = \u03d5 \u200b ( q i ) \u22a4 \u200b \u03d5 \u200b ( k j ) + exp \u2061 ( q i \u22a4 \u200b k j ) \u2212 \u03d5 \u200b ( q i ) \u22a4 \u200b \u03d5 \u200b ( k j ) = exp \u2061 ( q i \u22a4 \u200b k j ) . subscript superscript \ud835\udc34 sb \ud835\udc56 \ud835\udc57 subscript ~ \ud835\udc44 superscript ~ \ud835\udc3e top \ud835\udc46 \ud835\udc56 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc5e \ud835\udc56 top italic-\u03d5 subscript \ud835\udc58 \ud835\udc57 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc58 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc5e \ud835\udc56 top italic-\u03d5 subscript \ud835\udc58 \ud835\udc57 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc58 \ud835\udc57 A^{\\mathrm{sb}}_{ij}=(\\widetilde{Q}\\widetilde{K}^{\\top}+S)_{ij}=\\phi(q_{i})^{\\top}\\phi(k_{j})+\\exp(q_{i}^{\\top}k_{j})-\\phi(q_{i})^{\\top}\\phi(k_{j})=\\exp(q_{i}^{\\top}k_{j}). If , then\n\nA i \u200b j sb = ( Q ~ \u200b K ~ \u22a4 + S ) i \u200b j = \u03d5 \u200b ( q i ) \u22a4 \u200b \u03d5 \u200b ( k j ) + 0 = \u03d5 \u200b ( q i ) \u22a4 \u200b \u03d5 \u200b ( k j ) . subscript superscript \ud835\udc34 sb \ud835\udc56 \ud835\udc57 subscript ~ \ud835\udc44 superscript ~ \ud835\udc3e top \ud835\udc46 \ud835\udc56 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc5e \ud835\udc56 top italic-\u03d5 subscript \ud835\udc58 \ud835\udc57 0 italic-\u03d5 superscript subscript \ud835\udc5e \ud835\udc56 top italic-\u03d5 subscript \ud835\udc58 \ud835\udc57 A^{\\mathrm{sb}}_{ij}=(\\widetilde{Q}\\widetilde{K}^{\\top}+S)_{ij}=\\phi(q_{i})^{\\top}\\phi(k_{j})+0=\\phi(q_{i})^{\\top}\\phi(k_{j}). In other words, matches on the indices in , and matches on the indices not in . To show that is an unbiased estimator of , we simply use the fact that is also an unbiased estimator of [17, Lemma 1]:\n\n\ud835\udd3c \u200b [ A i \u200b j sb ] \ud835\udd3c delimited-[] subscript superscript \ud835\udc34 sb \ud835\udc56 \ud835\udc57 \\displaystyle\\mathbb{E}[A^{\\mathrm{sb}}_{ij}] = \u2119 \u200b ( i \u200b j \u2208 \ud835\udcae ) \u200b \ud835\udd3c \u200b [ A i \u200b j \u2223 i \u200b j \u2208 \ud835\udcae ] + \u2119 \u200b ( i \u200b j \u2209 \ud835\udcae ) \u200b \ud835\udd3c \u200b [ A i \u200b j lr \u2223 i \u200b j \u2209 \ud835\udcae ] absent \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae \ud835\udd3c delimited-[] conditional subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \ud835\udc56 \ud835\udc57 \ud835\udcae \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae \ud835\udd3c delimited-[] conditional subscript superscript \ud835\udc34 lr \ud835\udc56 \ud835\udc57 \ud835\udc56 \ud835\udc57 \ud835\udcae \\displaystyle=\\mathbb{P}(ij\\in\\mathcal{S})\\mathbb{E}[A_{ij}\\mid ij\\in\\mathcal{S}]+\\mathbb{P}(ij\\notin\\mathcal{S})\\mathbb{E}[A^{\\mathrm{lr}}_{ij}\\mid ij\\notin\\mathcal{S}] = \u2119 \u200b ( i \u200b j \u2208 \ud835\udcae ) \u200b A i \u200b j + \u2119 \u200b ( i \u200b j \u2209 \ud835\udcae ) \u200b A i \u200b j absent \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \\displaystyle=\\mathbb{P}(ij\\in\\mathcal{S})A_{ij}+\\mathbb{P}(ij\\notin\\mathcal{S})A_{ij} = A i \u200b j .",
    "scatterbrain-50": "absent subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \\displaystyle=A_{ij}. In other words, . Now we analyze the per-entry variance of . Since is an unbiased estimator of , by the law of total variance,\n\n\ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j sb ) \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 sb \ud835\udc56 \ud835\udc57 \\displaystyle\\mathbb{Var}(A^{\\mathrm{sb}}_{ij}) = \u2119 \u200b ( i \u200b j \u2208 \ud835\udcae ) \u200b \ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j \u2223 i \u200b j \u2208 \ud835\udcae ) + \u2119 \u200b ( i \u200b j \u2209 \ud835\udcae ) \u200b \ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j lr \u2223 i \u200b j \u2209 \ud835\udcae ) absent \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae \ud835\udd4d \ud835\udd52 \ud835\udd63 conditional subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \ud835\udc56 \ud835\udc57 \ud835\udcae \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae \ud835\udd4d \ud835\udd52 \ud835\udd63 conditional subscript superscript \ud835\udc34 lr \ud835\udc56 \ud835\udc57 \ud835\udc56 \ud835\udc57 \ud835\udcae \\displaystyle=\\mathbb{P}(ij\\in\\mathcal{S})\\mathbb{Var}(A_{ij}\\mid ij\\in\\mathcal{S})+\\mathbb{P}(ij\\notin\\mathcal{S})\\mathbb{Var}(A^{\\mathrm{lr}}_{ij}\\mid ij\\notin\\mathcal{S}) = \u2119 \u200b ( i \u200b j \u2208 \ud835\udcae ) \u22c5 0 + \u2119 \u200b ( i \u200b j \u2209 \ud835\udcae ) \u200b \ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j lr ) absent \u22c5 \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae 0 \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 lr \ud835\udc56 \ud835\udc57 \\displaystyle=\\mathbb{P}(ij\\in\\mathcal{S})\\cdot 0+\\mathbb{P}(ij\\notin\\mathcal{S})\\mathbb{Var}(A^{\\mathrm{lr}}_{ij}) = \u2119 \u200b ( i \u200b j \u2209 \ud835\udcae ) \u200b \ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j lr ) . absent \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 lr \ud835\udc56 \ud835\udc57 \\displaystyle=\\mathbb{P}(ij\\notin\\mathcal{S})\\mathbb{Var}(A^{\\mathrm{lr}}_{ij}). To compute the probability that the index is not in (i.e., not selected by LSH), we use the standard bound on cross-polytope LSH [3, Theorem 1]:\n\np := \u2119 \u200b ( i \u200b j \u2208 \ud835\udcae ) = exp \u2061 ( \u2212 \u03c4 2 4 \u2212 \u03c4 2 \u200b ln \u2061 d \u2212 O \u03c4 \u200b ( ln \u2061 ln \u2061 d ) ) . assign \ud835\udc5d \u2119 \ud835\udc56 \ud835\udc57 \ud835\udcae superscript \ud835\udf0f 2 4 superscript \ud835\udf0f 2 \ud835\udc51 subscript \ud835\udc42 \ud835\udf0f \ud835\udc51 p:=\\mathbb{P}(ij\\in\\mathcal{S})=\\exp(-\\frac{\\tau^{2}}{4-\\tau^{2}}\\ln d-O_{\\tau}(\\ln\\ln d)). Therefore,\n\n\ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j sb ) = ( 1 \u2212 p ) \u200b \ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j lr ) < \ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j lr ) . \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 sb \ud835\udc56 \ud835\udc57 1 \ud835\udc5d \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 lr \ud835\udc56 \ud835\udc57 \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 lr \ud835\udc56 \ud835\udc57 \\mathbb{Var}(A^{\\mathrm{sb}}_{ij})=(1-p)\\mathbb{Var}(A^{\\mathrm{lr}}_{ij})<\\mathbb{Var}(A^{\\mathrm{lr}}_{ij}). In other words, . More explicitly, by plugging in the variance of [17, Lemma 2], we have\n\n\ud835\udd4d \u200b \ud835\udd52 \u200b \ud835\udd63 \u200b ( A i \u200b j sb ) = ( 1 \u2212 p ) \u200b 1 m \u200b exp \u2061 ( \u2016 q i + k j \u2016 2 ) \u200b exp \u2061 ( 2 \u200b q i \u22a4 \u200b k j ) \u200b ( 1 \u2212 exp \u2061 ( \u2212 \u2016 q i + k j \u2016 2 ) ) , \ud835\udd4d \ud835\udd52 \ud835\udd63 subscript superscript \ud835\udc34 sb \ud835\udc56 \ud835\udc57 1 \ud835\udc5d 1 \ud835\udc5a superscript norm subscript \ud835\udc5e \ud835\udc56 subscript \ud835\udc58 \ud835\udc57 2 2 superscript subscript \ud835\udc5e \ud835\udc56 top subscript \ud835\udc58 \ud835\udc57 1 superscript norm subscript \ud835\udc5e \ud835\udc56 subscript \ud835\udc58 \ud835\udc57 2 \\mathbb{Var}(A^{\\mathrm{sb}}_{ij})=(1-p)\\frac{1}{m}\\exp\\left(\\left\\|{q_{i}+k_{j}}\\right\\|^{2}\\right)\\exp(2q_{i}^{\\top}k_{j})\\left(1-\\exp\\left(-\\left\\|{q_{i}+k_{j}}\\right\\|^{2}\\right)\\right),\n\nwhere\n\nAppendix E Additional Experiments and Details\n\nE.1 Datasets\n\nImageNet [25]: ImageNet is one of the most widely-used image classification benchmarks. In our experiments in Section 5.1 of evaluating the approximation accuracy of Scatterbrain, both BigGAN and Vision Transformer are pre-trained on this dataset. It has roughly 1.2 million training images and 50,000 validation images. WikiText103 [45] and Copy [36]: WikiText103 is a popular dataset for auto-regressive models. It is from a collection of over 100 million tokens extracted from the set of verified good and featured articles on Wikipedia. It has 28,475 training articles, 60 for validation and 60 for testing. Copy is a synthetic a synthetic sequence duplication task where inputs are of the form and . It is previously used in [36, 15]. This task is useful for demonstrating the effectiveness of long range attention: it requires non-local attention lookups. It cannot be solved by any model relying on sparse attention with a limited range such as, local attention. Long Range Arena (LRA) [57]: This is a recent benchmark for evaluating efficient transformers with long input sequence. We used ListOps [46], byte-level IMDb reviews text classification [44], byte-level document retrieval [48], image classification on sequences of pixels [37] and Pathfinder [40]. We follow the same evaluation mechanism from [57] but implement our own version in Pytorch (like data loader). GlUE [64]: GLUE is a standard multi-task benchmark in NLP. It has single-sentence tasks, CoLA and SST-2; similarity and paraphrase tasks, MRPC, STS-B, QQP; and inference tasks, MNLI, QNLI, RTE and WNLI. For our additional experiments below (not enough space to be included in the main paper), we follow the tradition from [26, 68, 22] and truncate all the input sequences to 128 tokens. E.2 Settings\n\nBigGAN: We adapt the same pre-trained BigGAN model from [22] with no additional training. The model has a single attention layer at resolution (4096). Similar to the prior work, we also replace its full attention layer with Scatterbrain at the same resolution. Figure 5 in the main paper shows the best-effort comparison with [1/32, 1/16, 1/8, 1/4, 1/2] of the parameter budget. For example, if given parameter budget 1/2, we report the best performance of Smyrf from choice of 32/64/128 hash round 64/32/16 cluster size. T2-ViT: We use the pre-trained vision transformer model T2T-ViT-14 from [69] with image size. Without any additional training, we just replace the attention layer with Scatterbrain and other baselines and evaluate the approximation error and classification accuracy on ImageNet testings. Again, we report the best-effort best performance of each approximation given the certain parameter budget. Auto-regressive Model: We follow the settings from the popular repo https://github.com/NVIDIA/DeepLearningExamples for training vanilla Transformer from scratch on WikiText103, except for chunking WikiText103 into sequence length 1024 in order to simulate long input sequences. The model is 16 layer with 8 head and 512 model dimension. We train all the models for 30 epochs and report the best Testing Perplexity. The model we use for Copy task is simply a 2-layer-4-head transformer and sequence length is also 1024. We make 5 runs and report average. Table 4 presents the results with standard deviation. Classification Model: We follow the model setting from [57, 67]. We share the same finding with [67] that the acuracy for the Retrieval tasks is actually higher than reported in [57]. Ratio between Sparse and Low-rank components: There are some rules that we used in our experiments to set this ratio. For inference, we set this ratio based on the entropy of an observed subset of attention matrices in different layers: we allocate more memory to the low-rank component compared to the sparse component if the entropy is high. For training, generally allocating more memory budget to sparse tends to perform better, so in the experiment, we set the ratio to 3:1 (sparse: low-rank component) for simplicity. Moreover, in future work, it could be useful to make this ratio adaptive during training. For example, in the early stage of the training and early layers, attention matrices are usually more uniform (higher entropy). Thus, the approximation error could be even lower if the ratio favors low-rank-based components. One approach could be to monitor the approximation error of sparse and low-rank components compared to full attention regularly and adjust the memory budget accordingly. We will add the above discussion to the updated manuscript. E.3 More Ablation Studies\n\nE.3.1 Memory Budget\n\nWe present an ablation study on the parameter budget for the WikiText-103 language modeling task. We show that Scatterbrain outperforms its sparse and low-rank baselines across a range of parameter budgets. The results are presented in Table 5. Analysis: We have observed that Scatterbrain outperforms its sparse and low-rank baselines under different memory budgets. Similar to what we found in Section 5.2, Performer does not train stably even with of the full attention memory. However, under the Scatterbrain framework, Performer can be combined with Reformer in an elegant way to achieve the same accuracy while using only half of the memory and faster than Reformer by exploiting the sparse+low-rank structure in attention matrices. E.3.2 Different Sparse and Low-rank baselines\n\nScatterbrain is general enough to accommodate different kinds of sparse and low-rank approximations as its sub-components. In particular, we can combine Local attention or block sparse (from Sparse Transformer and BigBird) + Performer (instead of Reformer + Performer) in a similar fashion. The support of the sparse matrix S will thus be fixed and not adaptive to input, but all the other steps are exactly the same. We have run additional experiments on the Local attention + Performer combination and BigBird. Recall that in Appendix E, we have shown Scatterbrain can reduce the attention memory of Vision Transformer by 98% at the cost of only 0.8% drop of accuracy when serving as a drop-in replacement for full attention without training on ImageNet. We show the results for local+performer variation with the same memory budget in Table 6. We have also run additional experiments on Local attention on Copy and Wikitext-103 language modeling task ( Table 7). We see that Local attention is reasonably competitive on Wikitext-103 but does not perform well on Copy. The results are not surprising as noted in the Reformer paper that Copy requires non-local attention lookups. E.3.3 Different Sparse and Low-rank baselines\n\nE.4 Analysis\n\nRecall in Section 5, we have reported the analysis after visualizing the error of reformer (sparse), performer (low-rank), and Scatterbrain (sparse + low-rank) given the same number of parameters when approximating the full attention matrices for each attention layer during training. In Figure LABEL:fig:error_analysis, we present the visualization. The conclusion for language modeling tasks is that sparse+low-rank has the smallest approximation error in most of the cases, and sparse has the largest error, which matches with the end-to-end results. It also confirms the observation in the popular benchmark paper [57] that kernel or low-rank based approximations are less effective for hierarchical structured data. For classification tasks, we again find that Scatterbrain has the smallest approximation error, while performer is the worst on ListOps and reformer has the largest error on classification tasks, which matches with the end-to-end results and confirms our observations earlier (sparse and low-rank approximation excel in different regimes). E.5 Additional Experiments of Fine-tuning Bert on GLUE\n\nWe provide additional experiments of fine-tuning Bert on GLUE in Table 8. We follow the similar setting as [22]. We replace all the attention layers in Bert base model with Scatterbrain and other baselines. Then we fine-tune Bert on 9 downstream tasks for 3 epochs with batch size 32 and learning rate 3e-5. The parameter budget is 1/2 of the full attention because sequence length 128 is not very long. We can see Scatterbrain outperforms all the other baselines in most of the downstream tasks. Appendix F Further Discussions and Future Work\n\nIn this paper, we present Scatterbrain, unifying the strength of sparse and low-rank approximation. It is inspired by the observations on the attention matrix structures induced by the data and softmax function as well as the classical robust-PCA algorithm. In our implementation and analysis, we have reformer/Smyrf and performer as the back-bone for sparse and low-rank approximations because of their properties, e.g. Performer is unbiased. Scatterbrain is fundamentally a framework for combining the strength of sparse and low-rank variants, so it can be easily extended to other variants, such as Routing Transformer [53] or Nystromformer [67]. Further more, our observations on the connection between entropy and low-rank/sparse approximation error also provide an opportunity for efficiently detecting the approximation or compression method to choose for different architectures or benchmarks. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Sat Mar 2 02:54:22 2024 by LaTeXML"
}