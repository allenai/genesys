{
    "approxdiag-0": "# Robustifying State-space Models for Long Sequences via Approximate Diagonalization \n\nAnnan Yu, ${ }^{1 *}$ Arnur Nigmetov, ${ }^{2} \\quad$ Dmitriy Morozov, ${ }^{2}$ Michael W. Mahoney, ${ }^{2,3,4}$<br>N. Benjamin Erichson ${ }^{2,3}$<br>${ }^{1}$ Center for Applied Mathematics, Cornell University<br>${ }^{2}$ Lawrence Berkeley National Laboratory<br>${ }^{3}$ International Computer Science Institute<br>${ }^{4}$ Department of Statistics, University of California at Berkeley\n\n\n#### Abstract\n\nState-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable \"perturb-thendiagonalize\" (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the nonnormal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the $\\mathrm{S} 4 \\mathrm{D} / \\mathrm{S} 5$ models. In addition to improved robustness, our S5PTD model averages $87.6 \\%$ accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models. ## 1 Introduction\n\nSequential data are pervasive across a wide range of fields, including natural language processing, speech recognition, robotics and autonomous systems, as well as scientific machine learning and financial time-series analysis, among others. Given that many of these applications produce exceedingly long sequences, sequential models need to capture long-range temporal dependencies in order to yield accurate predictions. To this end, many specialized deep learning methods have been developed to deal with long sequences, including recurrent neural networks (RNNs) [2, 7, 13, 30, 14, 28], convolutional neural networks (CNNs) [4, 29], continuous-time models (CTMs) $[18,37]$, and transformers $[21,8,23,39,26]$. Over the past few years, the new class of state-space models (SSMs) gained vast popularity for sequential modeling due to their outstanding performance on the Long-Range Arena (LRA)\n\n[^0]dataset [33]. An SSM is built upon a continuous-time linear time-invariant (LTI) dynamical system $\\Sigma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$, which is a system of linear ODEs given by\n\\[\n\n$$\n\\begin{aligned}\n\\mathbf{x}^{\\prime}(t) & =\\mathbf{A x}(t)+\\mathbf{B u}(t) \\\\\n\\mathbf{y}(t) & =\\mathbf{C x}(t)+\\mathbf{D u}(t)\n\\end{aligned}\n$$\n\\]\n\nwhere $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}, \\mathbf{B} \\in \\mathbb{C}^{n \\times m}, \\mathbf{C} \\in \\mathbb{C}^{p \\times n}, \\mathbf{D} \\in \\mathbb{C}^{p \\times m}$ are the state, input, output and feedthrough matrices; and $\\mathbf{u}(t) \\in \\mathbb{C}^{m}, \\mathbf{x}(t) \\in \\mathbb{C}^{n}, \\mathbf{y}(t) \\in \\mathbb{C}^{p}$ are the inputs, states, and outputs of the system, respectively. The system can be discretized at time steps $j \\Delta t$, where $\\Delta t>0$ and $j=1, \\ldots, L$, to be fed with sequential inputs of length $L$. To store and process the information of the long sequential inputs online, the SSMs are often initialized by a pre-designed LTI system. One of the most popular schemes is called \"HiPPO initialization\" [35, 15], in which the Legendre coefficients of the input history at time $t$, i.e., $\\mathbf{u} \\cdot \\mathbb{1}_{[0, t]}$, are stored and updated in the state vector $\\mathbf{x}(t)$. This initialization is specifically designed to model long-range dependencies in sequential data. The recently proposed S4 model [17] leverages the HiPPO initialization and accelerates training and inference by decomposing $\\mathbf{A}$ into the sum of a diagonal matrix and a low-rank one. The diagonal-plus-low-rank (DPLR) structure yields a barycentric representation [1] of the transfer function of eq. (1) that maps inputs to outputs in the frequency domain, enabling fast computation in the frequency domain [3]. While the DPLR structure achieves an asymptotic speed-up of the model, considering A to be a diagonal matrix results in a simpler structure. Compared to a DPLR matrix A, a diagonal SSM is not only faster to compute and easier to implement, but it also allows integrating channel communication via parallel scans [32], thereby improving its performance on long-range tasks. Unfortunately, the problem of diagonalizing the HiPPO framework is exponentially illconditioned, as $n$ increases. Hence, while [17] shows analytic forms of the eigenvalues and eigenvectors of HiPPO matrices, they suffer from an exponentially large variance and cannot be used in practice. So far, the most popular way of obtaining a diagonal SSM is to simply discard the low-rank part from the DPLR structure, leveraging a stable diagonalization algorithm for a normal matrix. Discarding the low-rank component changes the underlying diagonalization problem, however; and it abandons the theoretical insights about HiPPO. Still, the resulting model almost matches S4's performance, in practice. Such diagonal models are called S4D [16] when the systems are single-input/single-output (i.e., $m=p=1$ ) and S5 [32] when the systems are multiple-input/multiple-output (i.e., $m=p>1$ ), which enables channel communication. The issue of ill-posed diagonalization problems is not merely specific to SSMs. For example, it is known that non-normal matrices make RNNs more expressive [22, 27]. More generally, non-normality plays an important role in the training of certain neural networks [31, 25]. While the ill-posedness of the diagonalization problem essentially prevents accurate computation of eigenvalues and eigenvectors (i.e., we cannot have a small forward error) - in fact, the true spectral information becomes meaningless in this case - using a backward stable eigensolver, one can recover the non-normal matrix accurately (i.e., we can have a small backward error) from the wrong eigenvalues and eigenvectors. In this paper, we propose a generic \"perturb-then-diagonalize\" (PTD) methodology as a backward stable eigensolver. PTD is based on the idea that a small random perturbation remedies the problem of the blowing up of eigenvector condition number $[11,10,6]$, regularizing the ill-posed problem into a close but well-posed one. It is based on the pseudospectral theory of non-normal operators [34] and may be interpreted as the approximate diagonalization of the non-normal matrices. In the context of SSMs, our PTD method can be used to diagonalize the highly non-normal HiPPO framework. Based on this, we introduce the S4-PTD and S5-PTD models. Our method is flexible, and it can be used to diagonalize many SSM initialization schemes that may be invented in the future. Contribution. Here are our main contributions:\n\n1. We propose a \"perturb-then-diagonalize\" (PTD) methodology that solves ill-posed diagonalization problems in machine learning when only the backward error is important.",
    "approxdiag-1": "2. We provide a fine-grained analysis that compares the S 4 and the S 4 D initialization. In particular, we quantify the change of the transfer function when discarding the low-rank part of HiPPO, which is done in the diagonal S4D/S5 initialization. We show that while the outputs of the S4D/S5 system on a fixed smooth input converge to those of the S4 system at a linear rate as $n \\rightarrow \\infty$ (see section 3.2), the convergence is not uniform across all input functions (see section 3.3). 3. Based on our theoretical analysis, we observe, using both a synthetic example (see section 3.4) and the sequential CIFAR task (see section 5.2 ), that the S4D/S5 models are very sensitive to certain Fourier-mode input perturbations, which impairs the robustness of the models. 4. Based on diagonalizing a perturbed HiPPO matrix, we propose the S4-PTD and S5-PTD models. Our models are robust to Fourier-mode input perturbations. We theoretically estimate the effect of the perturbation (see section 4). We propose computing the perturbation matrix by solving an optimization problem with a soft constraint. Moreover, our method is not restricted to the HiPPO matrix but can be applied to any initializations.",
    "approxdiag-2": "5. We provide an ablation study for the size of the perturbation in our models. We also evaluate our S4-PTD and S5-PTD models on LRA tasks, which reveals that the S4-PTD model outperforms the S4D model, while the S5-PTD model is comparable with the S5 model (see section 5.1 ). ## 2 Preliminaries and notation\n\nGiven an LTI system in eq. (1), we say it is asymptotically stable if the eigenvalues $\\lambda_{j}$ of $\\mathbf{A}$ are all contained in the left half-plane, i.e., if $\\operatorname{Re}\\left(\\lambda_{j}\\right)<0$ for all $1 \\leq j \\leq n$. The transfer function of the LTI system is defined by\n\n$$\nG(s)=\\mathbf{C}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\mathbf{D}, \\quad s \\in \\mathbb{C} \\backslash \\Lambda(\\mathbf{A})\n$$\n\nwhere $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ is the identity matrix and $\\Lambda(\\mathbf{A})$ is the spectrum of $\\mathbf{A}$. The transfer function $G$ is a rational function with $n$ poles (counting multiplicities) at the eigenvalues of $\\mathbf{A}$. Assume $\\mathbf{x}(0)=\\mathbf{0}$. Then the transfer function maps the inputs to the outputs of the LTI system in the Laplace domain by multiplication, i.e., $(\\mathcal{L} \\mathbf{y})(s)=G(s)(\\mathcal{L} \\mathbf{u})(s)$ for all $s \\in \\mathbb{C}$, where $\\mathcal{L}$ is the Laplace transform operator (see [38]). Assume the LTI system in eq. (1) is asymptotically stable and the input $\\mathbf{u}(t)$ is bounded and integrable (with respect to the Lebesgue measure) as $t$ ranges over $\\mathbb{R}$. Then the Laplace transform reduces to the Fourier transform:\n\n$$\n\\hat{\\mathbf{y}}(s)=G(i s) \\hat{\\mathbf{u}}(s), \\quad s \\in \\mathbb{R}\n$$\n\nwhere $\\hat{\\mathbf{y}}$ and $\\hat{\\mathbf{u}}$ are the Fourier transforms of $\\mathbf{y}$ and $\\mathbf{u}$, respectively, and $i$ is the imaginary unit. Let $\\mathbf{V} \\in \\mathbb{C}^{n \\times n}$ be an invertible matrix. We can conjugate the system $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ by $\\mathbf{V}$, which yields $\\left(\\mathbf{V}^{-1} \\mathbf{A V}, \\mathbf{V}^{-1} \\mathbf{B}, \\mathbf{C V}, \\mathbf{D}\\right)$. Since the transfer function is conjugation-invariant, the two systems map the same inputs $\\mathbf{u}(\\cdot)$ to the same outputs $\\mathbf{y}(\\cdot)$, while the states $\\mathbf{x}(\\cdot)$ are transformed by $\\mathbf{V}$. If $\\mathbf{A}$ is a normal matrix, i.e., $\\mathbf{A A}^{*}=\\mathbf{A}^{*} \\mathbf{A}$, then $\\mathbf{V}$ is unitary, in which case transforming the states by $\\mathbf{V}$ is a well-conditioned problem and can be done without loss of information. Issues arise, however, when $\\mathbf{A}$ is non-normal and $\\mathbf{V}$ is ill-conditioned. The state-space models use LTI systems to process time series inputs. Different initializations can be tailored to tasks with different natures, such as the range of dependency [19]. A particularly successful initialization scheme used in the S 4 model is the so-called HiPPO initialization. While there exist several variants of HiPPO, the most popular HiPPO-LegS matrices are defined by\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-04.jpg?height=146&width=1382&top_left_y=358&top_left_x=337)\nfor all $1 \\leq j, k \\leq n$ and $1 \\leq \\ell \\leq m$, where $\\mathbb{1}_{\\{j>k\\}}$ is the indicator that equals 1 if $j>k$ and 0 otherwise. Such a system guarantees that the Legendre coefficients of the input history $\\mathbf{u} \\cdot \\mathbb{1}_{[0, t]}$ (with respect to a scaled measure) are stored in the states $\\mathbf{x}(t)$ over time [15]. Since computing with the dense matrix $\\mathbf{A}_{H}$ is practically inefficient, one conjugates the HiPPO system with a matrix $\\mathbf{V}_{H}$ to simplify the structure of $\\mathbf{A}_{H}$. The matrix $\\mathbf{A}_{H}$ in eq. (4) has an ill-conditioned eigenvector matrix [17]; consequently, instead of solving the ill-posed problem that diagonalizes $\\mathbf{A}_{H}$, one exploits a diagonal-plus-low-rank (DPLR) structure:\n\n$$\n\\mathbf{A}_{H}=\\mathbf{A}_{H}^{\\perp}-\\frac{1}{2} \\mathbf{B}_{H} \\mathbf{B}_{H}^{\\top}, \\quad\\left(A_{H}^{\\perp}\\right)_{j k}=-\\frac{1}{2} \\begin{cases}(-1)^{\\mathbb{1}_{\\{j<k\\}} \\sqrt{2 j-1} \\sqrt{2 k-1}}, & j \\neq k \\\\ 1 & j=k\\end{cases}\n$$\n\nwhere $\\mathbf{A}_{H}^{\\perp}$ is a skew-symmetric matrix that can be unitarily diagonalized into $\\mathbf{A}_{H}^{\\perp}=\\mathbf{V}_{H} \\boldsymbol{\\Lambda}_{H} \\mathbf{V}_{H}^{-1}$. The S 4 model leverages the HiPPO matrices by initializing\n\n$$\n\\mathbf{A}_{\\mathrm{DPLR}}=\\boldsymbol{\\Lambda}_{H}-\\frac{1}{2} \\mathbf{V}_{H}^{-1} \\mathbf{B}_{H} \\mathbf{B}_{H}^{\\top} \\mathbf{V}_{H}, \\quad \\mathbf{B}_{\\mathrm{DPLR}}=\\mathbf{V}_{H}^{-1} \\mathbf{B}_{H}\n$$\n\nand $\\mathbf{C}_{\\text {DPLR }}$ and $\\mathbf{D}_{\\text {DPLR }}$ randomly. Such a system $\\Sigma_{\\text {DPLR }}=\\left(\\mathbf{A}_{\\text {DPLR }}, \\mathbf{B}_{\\text {DPLR }}, \\mathbf{C}_{\\text {DPLR }}, \\mathbf{D}_{\\text {DPLR }}\\right)$ is conjugate via $\\mathbf{V}_{H}$ to $\\left(\\mathbf{A}_{H}, \\mathbf{B}_{H}, \\mathbf{C}_{\\mathrm{DPLR}} \\mathbf{V}_{H}^{-1}, \\mathbf{D}_{\\mathrm{DPLR}}\\right)$. Hence, they share the transfer function and the same mapping from the inputs $\\mathbf{u}(\\cdot)$ to the outputs $\\mathbf{y}(\\cdot)$. The S4D model further simplifies the structure by discarding the rank-1 part from $\\mathbf{A}_{H}$ and therefore initializes\n\n$$\n\\mathbf{A}_{\\text {Diag }}=\\boldsymbol{\\Lambda}_{H}, \\quad \\mathbf{B}_{\\text {Diag }}=\\frac{1}{2} \\mathbf{V}_{H}^{-1} \\mathbf{B}_{H}\n$$\n\nand $\\mathbf{A}_{\\text {Diag }}$ is henceforth restricted to be diagonal. While both the S4 and S4D models restrict that $m=p=1$, i.e., the LTI systems are single-input/single-output (SISO), the S5 model, which also initializes $\\mathbf{A}_{\\text {Diag }}=\\boldsymbol{\\Lambda}_{H}$ and requires it to be diagonal throughout training, leverages multiple-input/multiple-output (MIMO) systems by allowing $m=p>1$. We provide more background information on LTI systems and state-space models in sequential modeling in Appendix B. Throughout this paper, we use $\\|\\cdot\\|$ to denote a vector or matrix 2-norm. Given an invertible square matrix $\\mathbf{V}$, we use $\\kappa(\\mathbf{V})=\\|\\mathbf{V}\\|\\left\\|\\mathbf{V}^{-1}\\right\\|$ to denote its condition number. Given a number $1 \\leq p \\leq \\infty$ and a measurable function $f: \\mathbb{R} \\rightarrow \\mathbb{C}$, we use $\\|f\\|_{L^{p}}$ for the standard $L^{p}$-norm of $f$ with respect to the Lebesgue measure on $\\mathbb{R}$ and $L^{p}(\\mathbb{R})=\\left\\{f: \\mathbb{R} \\rightarrow \\mathbb{C} \\mid\\|f\\|_{L^{p}}<\\infty\\right\\}$. ## 3 Theory of the diagonal initialization of state-space models\n\nThe S4 model proposes to initialize the SSM to store the Legendre coefficients of the input signal in the states $\\mathbf{x}[15]$. This initialization, however, has an ill-conditioned spectrum, preventing a stable diagonalization of the SSM. On the other hand, the S4D model uses a different initialization scheme that has a stable spectrum, allowing for stable diagonalization; however, such initialization lacks an interpretation of the states $\\mathbf{x}$. In this section, we conduct a fine-grained analysis of the two initializations, which shows that:\n\n1. for any fixed input signal $\\mathbf{u}(\\cdot)$ with sufficient smoothness, the outputs of the two systems $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$ converge to each other with a linear rate (of which the previous analysis is devoid) as $n \\rightarrow \\infty$ (see section 3.2); and\n2. by viewing $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$ as linear operators that map input signals to the outputs, the operators do not converge in the operator norm topology as $n \\rightarrow \\infty$ (see section 3.3). While the first observation partially justifies the success of the S4D model, the second one allows us to observe that the diagonal initialization is unstable under certain Fourier-mode input perturbations (see section 3.4). In this section, we assume $m=p=1$, which is consistent with the S4 and S4D models. Still, our theory can be related to the S5 model, as shown in [32]. ### 3.1 A simplified formula for the transfer function deviation\n\nFix an integer $1 \\leq \\ell \\leq n$. We assume that $\\mathbf{C}_{\\text {DPLR }}=\\mathbf{C}_{\\text {Diag }}=\\mathbf{e}_{\\ell}^{\\top} \\mathbf{V}_{H}$, where $\\mathbf{e}_{\\ell}^{\\top}$ is the $\\ell$ th standard basis, and $\\mathbf{D}_{\\text {DPLR }}=\\mathbf{D}_{\\text {Diag }}$. For a general $\\mathbf{C}_{\\text {DPLR }}=\\mathbf{C}_{\\text {Diag }}$, we can decompose it onto the orthonormal basis $\\left\\{\\mathbf{e}_{\\ell}^{\\top} \\mathbf{V}_{H} \\mid 1 \\leq \\ell \\leq n\\right\\}$ and study each component separately using the theory developed in this section. Let $G_{\\text {DPLR }}$ and $G_{\\text {Diag }}$ be the transfer functions of $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$, respectively, i.e.,\n$G_{\\text {DPLR }}(s)=\\mathbf{C}_{\\text {DPLR }}\\left(s \\mathbf{I}-\\mathbf{A}_{\\text {DPLR }}\\right)^{-1} \\mathbf{B}_{\\text {DPLR }}+\\mathbf{D}_{\\text {DPLR }}, G_{\\text {Diag }}(s)=\\mathbf{C}_{\\text {Diag }}\\left(s \\mathbf{I}-\\mathbf{A}_{\\text {Diag }}\\right)^{-1} \\mathbf{B}_{\\text {Diag }}+\\mathbf{D}_{\\text {Diag }}$. Our first result (Lemma 1) concerns $G_{\\text {DPLR }}-G_{\\text {Diag. }}$ Recall that by eq. (3), the difference between the two LTI systems is controlled by $G_{\\text {DPLR }}-G_{\\text {Diag. }}$. In particular, $\\mid G_{\\mathrm{DPLR}}(s i)-$ $G_{\\text {Diag }}(s i) \\mid$ measures the difference between the outputs of the two systems given a frequency- $s$ input. We use Lemma 1 to study the (non-) convergence of the two systems (see Theorem 1 and 2); moreover, it reveals the ways that the S4D model can become unstable (see Figure 1 and section 3.4).",
    "approxdiag-3": "Lemma 1. Let $G_{\\text {DPLR }}$ and $G_{\\text {Diag }}$ be defined by eq. (8). For any $s \\in \\mathbb{C}$ with $\\operatorname{Re}(s)=0$, we have\n\n$$\nG_{\\mathrm{DPLR}}(s)-G_{\\mathrm{Diag}}(s)=\\frac{-s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)} \\sqrt{2 \\ell-1} \\frac{\\prod_{j=0}^{\\ell-2}(s-j)}{\\prod_{j=1}^{\\ell}(s+j)}}{\\sqrt{2}\\left(1+s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)}\\right)}\n$$\n\nThe proof of Lemma 1 is technical and therefore deferred to Appendix C. The idea is to expand the term $\\left(s \\mathbf{I}-\\mathbf{A}_{H}^{\\perp}+\\mathbf{B}_{H} \\mathbf{B}_{H}^{\\top} / 2\\right)^{-1}$ in the expression of $G_{\\text {Diag }}$ using the Woodbury matrix identity [36], which leads to a primary term $\\left(s \\mathbf{I}-\\mathbf{A}_{H}^{\\perp}\\right)^{-1}$ that gets canceled with that in $G_{\\text {DPLR }}$ and residual terms that are expanded by Cramer's rule. The importance of eq. (9) is that it reduces the complicated matrix inversions to elementary operations, enabling further analysis. In Figure 1, we plot the magnitude of transfer functions $\\left|G_{\\text {DPLR }}\\right|$ and $\\left|G_{\\text {Diag }}\\right|$ when $\\ell=1$ and $n=10,10^{2}, 10^{3}$, and $10^{4}$, respectively. Note that when $\\ell=1, G_{\\mathrm{DPLR}}$ is independent of $n$ (see Appendix C). We see that as $n$ increases, $G_{\\text {Diag }}$ approaches $G_{\\text {DPLR }}$ in the low-frequency domain, i.e., when $|s|$ is small. The spikes in the plot of $\\left|G_{\\text {Diag }}\\right|$ are caused by the winding of $\\left(s(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)\\right) /\\left(\\prod_{j=1}^{n}(j+s)\\right)$ around the origin in eq. (9). Moreover, for every $n \\geq 1$, zooming into the last spike located at $|s|=\\Theta\\left(n^{2}\\right)$ reveals that it has a constant magnitude (see the subplots on the right in Figure 1). Hence, the convergence of $G_{\\text {Diag }}$ to $G_{\\text {DPLR }}$ is nonuniform (see section 3.3). Moreover, the frequency response is unstable at input frequencies $s$ near these spikes, suggesting that the S 4 D model is not robust to certain input perturbations (see section 3.4). ### 3.2 Input-wise convergence of the diagonal initialization\n\nHere, we apply Lemma 1 to show that for a fixed input signal $\\mathbf{u}(\\cdot)$, the outputs of $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$ converge to each other as $n \\rightarrow \\infty$. If we think of $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$ as linear operators on the Hilbert space $L^{1}(\\mathbb{R}) \\cap L^{2}(\\mathbb{R})$ that map the input $\\mathbf{u}$ to the output $\\mathbf{y}$, then in the language of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-06.jpg?height=597&width=1315&top_left_y=244&top_left_x=339)\n\nFigure 1: The magnitude of transfer function of the S 4 model, $\\left|G_{\\text {DPLR }}(s i)\\right|$, and that of the S4D model, $\\left|G_{\\text {Diag }}(s i)\\right|$ with $\\mathbf{C}_{\\text {DPLR }}=\\mathbf{C}_{\\text {Diag }}=\\mathbf{e}_{1}^{\\top} \\mathbf{V}_{H}$ and the SSM size $n$ set to different values. Note that $G_{\\text {DPLR }}$ stays the same regardless of $n$. By zooming into the last spike of $\\left|G_{\\text {Diag }}(s i)\\right|$, we see that the peak remains $\\Theta(1)$ as $n \\rightarrow \\infty$ (see the right panels). The figure shows that $G_{\\text {Diag }}$ is oscillatory while $G_{\\text {DPLR }}$ is smooth; moreover, $\\left|G_{\\text {Diag }}(s i)\\right|$ does not converge to $\\left|G_{\\text {DPLR }}(s i)\\right|$ uniformly. functional analysis, we have $\\Sigma_{\\text {DPLR }}-\\Sigma_{\\text {Diag }}$ converges to zero in the weak* topology. Moreover, while the previous result [16] does not have a rate of convergence, we show that the convergence is linear. In fact, the rate is sharp (see Appendix F). This partially explains why the S4D model matches the performance of the S4 model in practice. Theorem 1. Let $\\mathbf{u}(\\cdot) \\in L^{2}(\\mathbb{R})$ be an input function with $\\|\\mathbf{u}\\|_{L^{2}}=1$. Let $\\mathbf{y}_{\\text {DPLR }}(\\cdot)$ and $\\mathbf{y}_{\\text {Diag }}(\\cdot)$ be the outputs of $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$ given the input $\\mathbf{u}(\\cdot)$ and the initial states $\\mathbf{x}(0)=\\mathbf{0}$, respectively. For some $q>1 / 2$, suppose $|\\hat{\\mathbf{u}}(s)|=\\mathcal{O}\\left(|s|^{-q}\\right)$ as $|s| \\rightarrow \\infty$. Then, we have $\\left\\|y_{\\text {DPLR }}-\\mathbf{y}_{\\text {Diag }}\\right\\|_{L^{2}}=\\mathcal{O}\\left(n^{-1}\\right) \\sqrt{\\ell}$ as $n \\rightarrow \\infty$, where the constant in the $\\mathcal{O}$-notation only depends on $q$ and the constant in $\\hat{\\mathbf{x}}(s)=\\mathcal{O}\\left(|s|^{-q}\\right)$. The constant does not depend on $q$ if we restrict $q \\in\\left[q^{\\prime}, \\infty\\right)$ for a fixed $q^{\\prime}>1 / 2$. The proof is deferred to Appendix E. Since the Fourier transform interchanges smoothness and decay, what Theorem 1 says is that under a mild assumption that $\\mathbf{u}(\\cdot)$ is sufficiently smooth, the output of the diagonal system converges linearly to that of the DPLR system as $n \\rightarrow \\infty$. ### 3.3 System-wise divergence of the diagonal initialization\n\nWe showed in section 3.2 that the outputs of the diagonal system and the DPLR system converge for a fixed smooth input $\\mathbf{u}(\\cdot)$.",
    "approxdiag-4": "Two questions naturally arise from this result:\n\n1. Can we remove the smoothness assumption on the input? 2. Is the convergence uniform across all input signals? Unfortunately, the answers to both questions are negative: the outputs of the two systems diverge given the unit impulse (i.e., the Dirac delta function) as the input (see Appendix F); moreover, the two systems are proven to diverge from each other in the operator norm topology. Theorem 2. The function $G_{\\text {DPLR }}(s)-G_{\\text {Diag }}(s)$ does not converge to zero uniformly on the imaginary axis as $n \\rightarrow \\infty$. In particular, for every $n \\geq 1$, there exists an input signal $\\mathbf{u}_{n}(\\cdot) \\in$ $L^{1}(\\mathbb{R}) \\cap L^{2}(\\mathbb{R})$ such that if we let $\\mathbf{y}_{n, \\text { DPLR }}$ and $\\mathbf{y}_{n, \\text { Diag }}$ be the outputs of $\\Sigma_{\\text {DPLR }}$ and $\\Sigma_{\\text {Diag }}$ of degree $n$, respectively, then we have $\\left\\|\\mathbf{y}_{n, \\text { DPLR }}-\\mathbf{y}_{n, \\text { Diag }}\\right\\|_{L^{2}}$ does not converge to 0 as $n \\rightarrow \\infty$.",
    "approxdiag-5": "The construction of $\\mathbf{u}_{n}(\\cdot)$ can be made explicit, as we show in the next subsection. We defer\nthe proof to Appendix D. The strategy is to carefully analyze the location and magnitude of the last spike in the plot of $\\left|G_{\\text {Diag }}\\right|$ (see Figure 1) and show that it does not vanish as $n \\rightarrow \\infty$. In summary, while a sufficiently large S4D model mimics its S4 alternative on a fixed smooth input, when we predetermine a size $n$, they inevitably disagree, by a large amount, on some inputs. Moreover, the smoothness condition is important: for a non-smooth input, the outputs of the two systems can diverge even if we allow $n$ to be arbitrarily large (see Appendix F). ### 3.4 Implication of the theory: non-robustness of the diagonal initialization\n\nThe analysis of $G_{\\text {DPLR }}$ and $G_{\\text {Diag }}$ in this section suggests the following caveat: while the S4 and the S4D models tend to pertain similar behaviors as $n$ gets large, the diagonal initialization scheme used by the S4D model is less robust to perturbations in the frequency domain (see Figure 1). In particular, by eq. (3), for a fixed state size $n$, input signals with frequency modes dense at the spikes of the plot of $\\left|G_{\\text {Diag }}\\right|$ are harder to process for the SSM. In turn, the S4D model is unstable near these modes. This does not happen with the S4 model. Our observation suggests that instead of replacing the ill-posed diagonalization problem with a wellconditioned but distinct one (i.e., the S4D initialization), which creates a large backward error $\\left|G_{\\text {DPLR }}(s)-G_{\\text {Diag }}(s)\\right|$, one should solve the ill-posed problem using a backward stable algorithm, even if the forward error (i.e., the miscalculation of eigenvalues and eigenvectors) will be large (see section 4). We demonstrate on a synthetic example that the S4D model, regardless of its size $n$, is not robust under input perturbation of certain frequency modes (which depend on $n$ ). Our training set contains sinusoidal signals parameterized by a frequency $s$ and an amplitude $A$ :\n\n$$\n\\mathbf{u}_{j}(t)=A_{j} \\sin \\left(s_{j} t\\right)\n$$\n\nwhere $A_{j} \\in[0,1]$ and $s_{j} \\in S_{\\text {interp }}:=[0,40] \\cup[60,100]$ for an interpolation problem or $s_{j} \\in$ $S_{\\text {extrap }}:=[0,80]$ for an extrapolation problem. We sample each input function $\\mathbf{u}_{j}(\\cdot)$ uniformly on $t \\in\\left[0,10^{4}\\right]$ and train an S4 model and an S4D model, respectively. Our goal is to learn $s$ and $A$ from the sequential input. In Figure 2, we plot the model prediction of the amplitude $A$ over a test set of signals for which $s$ and $A$ are on a uniform grid on $[0,100] \\times[0,1]$. Figure 2 shows that while both the S4 and S4D models predict well on sampled domains (i.e., $S_{\\text {interp }}$ and $S_{\\text {extrap }}$ ), the S 4 model is significantly better at interpolating and extrapolating on the unsampled domains. In particular, the S4D model suffers from an extrapolation disaster: for $s>80$, as the true amplitude of the signal increases from 0 to 1 , the predicted amplitude decreases monotonically with a minimum value less than -4 . This happens because $\\left|G_{\\text {Diag }}\\right|$ has a spike around $|s|=83$, making the information of $s \\in[0,80]$ impossible to transfer to $s \\in[80,100]$. Hence, while the S4D initialization stabilizes the diagonalization process, making the computation more efficient, its underlying state-space model is unstable near certain Fourier modes, impairing its robustness (see also section 5.2). ## 4 Perturbing the HiPPO initialization: a new way of diagonalizing the state-space model\n\nIn section 3.4, we saw the instability of the S4D model at certain Fourier modes. Nevertheless, the diagonal structure of $\\mathbf{A}$ is preferred over the DPLR one due to its training and inference efficiency and its adaptivity to the MIMO model (i.e., the S5 model) [32]. To avoid the instability in a diagonal model, we want to leverage the HiPPO initialization in eq. (4) instead of the one in eq. (7) that discards the rank-1 part. One obvious solution is to diagonalize the HiPPO matrix $\\mathbf{A}_{H}=\\mathbf{V}_{H} \\boldsymbol{\\Lambda}_{H} \\mathbf{V}_{H}^{-1}$ and conjugate $\\left(\\mathbf{A}_{H}, \\mathbf{B}_{H}, \\mathbf{C}, \\mathbf{D}\\right)$ using $\\mathbf{V}_{H}$. However, as shown in [16], the eigenvector matrix $\\mathbf{V}_{H}$ is exponentially ill-conditioned with respect to $n$, making the spectral\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-08.jpg?height=674&width=1615&top_left_y=197&top_left_x=224)\n\nFigure 2: The predicted amplitude $A$ of signals in the test set whose frequencies $s$ are sampled uniformly from $[0,100]$ and amplitudes from $[0,1]$.",
    "approxdiag-6": "The top row shows the interpolation result, where the functions in the training datasets have frequencies only in $[0,40] \\cup[60,100]$. The bottom row shows the extrapolation result, where the functions in the training datasets have frequencies only in $[0,80]$. The figure shows the interpolation and extrapolation results for the S4 model, the S4D model, and the S4-PTD model (see section 4). We observe that our S4-PTD model interpolates and extrapolates better than the S4D model. In particular, the S4D model is not stable around $s=80$, where the predicted amplitude decreases to -4 when the true value increases from 0 to 1 .",
    "approxdiag-7": "More quantitative results for the interpolation and extrapolation errors can be found in Appendix G. information meaningless. While the exact eigenvalues and eigenvectors of $\\mathbf{A}_{H}$ are very illconditioned, since we only care about the backward error of diagonalization, we propose the following initialization scheme. let $\\mathbf{E} \\in \\mathbb{C}^{n \\times n}$ be a perturbation matrix. We diagonalize the perturbed HiPPO matrix as\n\n$$\n\\tilde{\\mathbf{A}}_{H}=\\mathbf{A}_{H}+\\mathbf{E}=\\tilde{\\mathbf{V}}_{H} \\tilde{\\boldsymbol{\\Lambda}}_{H} \\tilde{\\mathbf{V}}_{H}^{-1}\n$$\n\nWe then initialize the systems using $\\Sigma_{\\text {Pert }}=\\left(\\mathbf{A}_{\\text {Pert }}, \\mathbf{B}_{\\text {Pert }}, \\mathbf{C}_{\\text {Pert }}, \\mathbf{D}_{\\text {Pert }}\\right)=\\left(\\tilde{\\mathbf{\\Lambda}}_{H}, \\tilde{\\mathbf{V}}_{H}^{-1} \\mathbf{B}_{H}, \\mathbf{C}, \\mathbf{D}\\right)$, where $\\mathbf{C}$ and $\\mathbf{D}$ are random matrices. Therefore, we approximately diagonalize the HiPPO initialization in the sense that although the diagonal entries in $\\tilde{\\boldsymbol{\\Lambda}}$ do not approximate the eigenvalues of $\\mathbf{A}_{H}$, the transfer function of $\\Sigma_{\\text {Pert }}$ is an approximation of that of $\\Sigma_{\\text {DPLR }}$ (see Theorem 3). This is enough to guarantee a good initialization. Our proposed perturb-then-diagonalize method is not restricted to the HiPPO-LegS matrices in eq. (4). This endows our method with adaptivity to any (dense) initialization scheme. This adaptivity was absent from the previous line of work on SSMs. We call our model S4-PTD or S5-PTD, depending on whether the model architecture is adapted from the S4D or the S5 model, where \"PTD\" stands for \"perturb-then-diagonalize.\" Since our models are only different from the S4D and the S5 models in initialization, we refer interested readers to $[16,32]$ for a discussion of computation details and time/space complexity.",
    "approxdiag-8": "Centered around the perturbed initialization scheme eq. (10) are two important questions:\n\n1. What is the difference between the perturbed initialization ( $\\mathbf{A}_{\\text {Pert }}, \\mathbf{B}_{\\text {Pert }}, \\mathbf{C}_{\\text {Pert }}, \\mathbf{D}_{\\text {Pert }}$ ) and the HiPPO initialization ( $\\left.\\mathbf{A}_{\\mathrm{DPLR}}, \\mathbf{B}_{\\mathrm{DPLR}}, \\mathbf{C}_{\\mathrm{DPLR}}, \\mathbf{D}_{\\mathrm{DPLR}}\\right)$ ? 2. What is the condition number of $\\tilde{\\mathbf{V}}_{H}$ ? The first question is important because it controls the deviation of our perturbed initialization from the successful and robust DPLR initialization. The second question is important because it shadows the numerical robustness of conjugating the LTI system by $\\tilde{\\mathbf{V}}_{H}$. Moreover, since the state vector $\\mathbf{x}(t)$ is transformed by $\\tilde{\\mathbf{V}}_{H}$ via conjugation (see section 2), a small condition number of $\\tilde{\\mathbf{V}}_{H}$ shows that its singular values are more evenly distributed. Hence, the transformation $\\tilde{\\mathbf{V}}_{H}$ does not significantly magnify or compress $\\mathbf{x}(t)$ onto some particular modes. ### 4.1 Estimating the transfer function perturbation\n\nTo study the first question, we define the transfer function of the perturbed system to be\n\n$$\nG_{\\text {Pert }}(s)=\\mathbf{C}_{\\text {Pert }}\\left(s \\mathbf{I}-\\mathbf{A}_{\\text {Pert }}\\right)^{-1} \\mathbf{B}_{\\text {Pert }}+\\mathbf{D}_{\\text {Pert }}\n$$\n\nWe control the size of the transfer function perturbation by proving the following theorem. Theorem 3. Assume $\\mathbf{C}_{\\text {Pert }} \\tilde{\\mathbf{V}}_{H}^{-1}=\\mathbf{C}_{\\text {DPLR }} \\mathbf{V}_{H}^{-1}$ and $\\mathbf{D}_{\\text {Pert }}=\\mathbf{D}_{\\text {DPLR }}$. Suppose $\\|\\mathbf{E}\\|_{2} \\leq \\epsilon$ and we normalize the matrices so that $\\left\\|\\tilde{\\mathbf{V}}_{H} \\mathbf{B}_{\\text {Pert }}\\right\\|=\\left\\|\\mathbf{V}_{H} \\mathbf{B}_{\\text {DPLR }}\\right\\|=\\left\\|\\mathbf{C}_{\\text {Pert }} \\tilde{\\mathbf{V}}_{H}^{-1}\\right\\|=\\left\\|\\mathbf{C}_{\\text {DPLR }} \\mathbf{V}_{H}^{-1}\\right\\|=1$. For any $s$ on the imaginary axis, we have\n\n$$\n\\left|G_{\\text {Pert }}(s)-G_{\\mathrm{DPLR}}(s)\\right|=(2 \\ln (n)+4) \\epsilon+\\mathcal{O}\\left(\\sqrt{\\log (n)} \\epsilon^{2}\\right)\n$$\n\nWhile our perturb-then-diagonalize method works for a general initialization and a bound on the transfer function error can always be established, the proof of Theorem 3 leverages the structure of HiPPO matrices to improve this bound. The error in Theorem 3 is the uniform error on the imaginary axis. Using H\u00f6lder's inequality, for any bounded and integrable input function $\\mathbf{u}(\\cdot)$, if $\\mathbf{y}_{\\text {Pert }}$ and $\\mathbf{y}_{\\text {DPLR }}$ are the outputs of $\\Sigma_{\\text {Pert }}$ and $\\Sigma_{\\text {DPLR }}$, respectively, then we have\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{y}_{\\text {Pert }}-\\mathbf{y}_{\\mathrm{DPLR}}\\right\\|_{L^{2}}=\\left\\|\\hat{\\mathbf{y}}_{\\text {Pert }}-\\hat{\\mathbf{y}}_{\\text {DPLR }}\\right\\|_{L^{2}}=\\left\\|\\hat{\\mathbf{x}}(s)\\left(G_{\\text {Pert }}(i s)-G_{\\text {DPLR }}(i s)\\right)\\right\\|_{L^{2}} \\\\\n& \\quad \\leq\\|\\hat{\\mathbf{x}}(s)\\|_{L^{2}}\\left\\|G_{\\text {Pert }}(i s)-G_{\\text {DPLR }}(i s)\\right\\|_{L^{\\infty}} \\leq\\|\\mathbf{x}\\|_{L^{2}}\\left((2 \\ln (n)+4) \\epsilon+\\mathcal{O}\\left(\\sqrt{\\log (n)} \\epsilon^{2}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere the first and the last steps follow from Parseval's identity. Hence, Theorem 3 gives us an upper bound on the distance between $\\Sigma_{\\text {Pert }}$ and $\\Sigma_{\\text {DPLR }}$ in the operator norm topology. The theorem states that the error made by the perturbation is linear in the size of the perturbation. Moreover, the error depends only logarithmically on the dimension $n$ of the state space. ### 4.2 Perturbed initialization as an optimization problem\n\nNext, we consider the conditioning of $\\tilde{\\mathbf{V}}_{H}$, which affects the accuracy of computing $\\tilde{\\mathbf{V}}_{H}^{-1} \\mathbf{B}_{\\text {Pert }}$ and the scaling ratio of the states in $\\mathbf{x}(\\cdot)$ (see Appendix B). This problem was studied by some numerical analysts $[11,10,6,5]$. The following theorem can be derived from their results on perturbing a general square matrix by a Ginibre matrix. Theorem 4. Given any matrix $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}$, perturbation size $\\epsilon \\in(0,1)$, and spectral radius $R>0$. Let $\\mathbf{G}_{n} \\in \\mathbb{C}^{n \\times n}$ be the Ginibre matrix and let $\\Omega$ be the event that the spectrum of $\\mathbf{A}+\\epsilon \\mathbf{G}_{n}$ is contained in $D_{R}(0)$, the disk centered at zero of radius $R$. Then, we have\n\n$$\n\\mathbb{E}\\left[\\kappa_{\\mathrm{eig}}\\left(\\mathbf{A}+\\epsilon \\mathbf{G}_{n}\\right)^{2} \\mid \\Omega\\right] \\leq\\|\\mathbf{A}\\|^{2} \\frac{R^{2} n^{3}}{\\epsilon^{2} \\mathbb{P}(\\Omega)}\n$$\n\nwhere the eigenvector condition number of $\\mathbf{A}+\\epsilon \\mathbf{G}_{n}$ is defined by\n\n$$\n\\kappa_{\\text {eig }}\\left(\\mathbf{A}+\\epsilon \\mathbf{G}_{n}\\right)=\\inf \\left\\{\\kappa(\\tilde{\\mathbf{V}}) \\mid \\mathbf{A}+\\epsilon \\mathbf{G}_{n}=\\tilde{\\mathbf{V}} \\tilde{\\mathbf{\\Lambda}} \\tilde{\\mathbf{V}}^{-1}, \\tilde{\\mathbf{\\Lambda}} \\text { diagonal }\\right\\}\n$$\n\nTheorem 4 states that we can get around the exponentially growing condition number of $\\tilde{\\mathbf{V}}_{H}$ by adding a small Gaussian perturbation to $\\mathbf{A}_{H}$, which justifies our S4-PTD and S5-PTD models. While it shows an upper bound on the eigenvector condition number of the perturbed HiPPO matrix, in most circumstances, perturbation by the Ginibre matrix does not give us the smallest eigenvector condition number. The following theorem provides a deterministic estimate of the eigenvector condition number for the \"best perturbation scheme.\"\nTheorem 5 ([6, Thm. 1.1.]). Given any $\\mathbf{A} \\in \\mathbb{C}_{\\tilde{n} \\times n}$ and $\\epsilon \\in(0,1)$, there exists a matrix $\\mathbf{E} \\in \\mathbb{C}^{n \\times n}$ with $\\|\\mathbf{E}\\| \\leq \\epsilon$ and an eigenvector matrix $\\tilde{\\mathbf{V}}$ of $\\mathbf{A}+\\mathbf{E}$ such that\n\n$$\n\\kappa(\\tilde{\\mathbf{V}}) \\leq 4 n^{3 / 2}\\left(1+\\epsilon^{-1}\\|\\mathbf{A}\\|\\right)\n$$\n\nTheorem 5 shows the promise of finding a good perturbation matrix to reduce the eigenvector condition number. In this paper, we propose to compute $\\mathbf{E}$ by solving the following optimization problem with a soft constraint:\n\n$$\n\\operatorname{minimize} \\Phi(\\mathbf{E})=\\kappa\\left(\\tilde{\\mathbf{V}}_{H}\\right)+\\gamma\\|\\mathbf{E}\\| \\quad \\text { s.t. } \\quad \\mathbf{A}_{H}+\\mathbf{E}=\\tilde{\\mathbf{V}}_{H} \\tilde{\\mathbf{\\Lambda}} \\tilde{\\mathbf{V}}_{H}^{-1}, \\quad \\tilde{\\mathbf{\\Lambda}} \\text { diagonal, }\n$$\n\nwhere $\\gamma>0$ is a hyperparameter that controls the trade-off between $\\kappa\\left(\\tilde{\\mathbf{V}}_{H}\\right)$ and $\\|\\mathbf{E}\\|$. We implement a solver to this optimization problem using gradient descent. As $\\gamma$ increases, it is harder to recover the original states $\\mathbf{x}(\\cdot)$ from the transformed states $\\tilde{\\mathbf{V}}_{H} \\mathbf{x}(\\cdot)$ because $\\kappa\\left(\\tilde{\\mathbf{V}}_{H}\\right)$ increases, but $\\|\\mathbf{E}\\|$ decreases, resulting in a more robust SSM that is closer to the flawless HiPPO initialization. ## 5 Empirical evaluation and discussion\n\nIn this section, we present empirical evaluations of our proposed S4-PTD and S5-PTD models. In section 5.1 we compare the performance of our full model with the existing ones in the Long Range Arena (LRA). In section 5.2, we perform a sensitivity analysis using the CIFAR-10 dataset to provide real-world evidence that our perturbed initialization scheme is more robust than the one in the S4D/S5 model. Finally, in section 5.3, we study the relationship between the size of the perturbation matrix $\\mathbf{E}$ and the performance of our models. ### 5.1 Performance in the Long-Range Arena\n\nThe LRA benchmark comprises six tasks with sequential data [33]. This collection, with its sequence lengths ranging from 1024 to 16000 , is designed to measure the model's capability of processing the long-range inputs. We train an S4-PTD model and an S5-PTD model to learn these tasks, respectively. We adopt the same SSM architectures, and thus the same number of parameters, from the original S4D [16] and S5 papers [32]. Results are reported in Table 1, along with the accuracies of other sequential models, including the Liquid-S4 model which is built upon S4 [20]. We report details of hyperparameters in Appendix J. While the perturbation matrix $\\mathbf{E}$ is also tunable, we restrict its size to be less than $10 \\%$ of that of the HiPPO matrix $\\mathbf{A}_{H}$, promoting the worst-case robustness of our model (see section 5.2). We note that the S4-PTD model outperforms the S4D model (and even the S4 model with the DPLR structure for most tasks), while the S5-PTD model matches the performance of the S5 model. ### 5.2 Robustness of our perturbed model over the diagonal model\n\nIn section 3.4, we see the instability of the S4D model using a synthetic example. Here, we demonstrate its practical implication regarding the robustness of the model. We train an S4D model and an S4-PTD model (with $\\|\\mathbf{E}\\| /\\left\\|\\mathbf{A}_{H}\\right\\| \\approx 10^{-1}$ ) to learn the sCIFAR task, where the images in the CIFAR-10 dataset [24] are flattened into sequences of pixels. We test the two\n\n| Model | ListOps | Text | Retrieval | Image | Pathfinder | Path-X | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer | 36.37 | 64.27 | 57.56 | 42.44 | 71.40 | $\\boldsymbol{x}$ | 53.66 |\n| Luna-256 | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | $\\boldsymbol{x}$ | 59.37 |\n| H-Trans.-1D | 49.53 | 78.69 | 63.99 | 46.05 | 68.78 | $\\boldsymbol{x}$ | 61.41 |\n| CCNN | 43.60 | 84.08 | $\\boldsymbol{x}$ | 88.90 | 91.51 | $\\boldsymbol{x}$ | 68.02 |\n| S4 | 59.60 | 86.82 | 90.90 | 88.65 | 94.20 | 96.35 | 86.09 |\n| Liquid-S4 | $\\underline{\\mathbf{6 2 . 7 5}}$ | $\\underline{89.02}$ | $\\underline{91.20}$ | $\\underline{\\mathbf{8 9 . 5 0}}$ | $\\underline{94.80}$ | $\\underline{96.66}$ | $\\underline{87.32}$ |\n| S4D | 60.47 | 86.18 | 89.46 | 88.19 | 93.06 | 91.95 | 84.89 |\n| S4-PTD (ours) | $\\underline{60.65}$ | $\\underline{88.32}$ | $\\underline{91.07}$ | $\\underline{88.27}$ | $\\underline{94.79}$ | $\\underline{96.39}$ | $\\underline{86.58}$ |\n| S5 | 62.15 | 89.31 | 91.40 | $\\underline{88.00}$ | 95.33 | $\\underline{\\mathbf{9 8 . 5 8}}$ | 87.46 |\n| S5-PTD (ours) | $\\underline{\\mathbf{6 2 .",
    "approxdiag-9": "7 5}}$ | $\\underline{\\mathbf{8 9 . 4 1}}$ | $\\underline{\\mathbf{9 1 . 5 1}}$ | 87.92 | $\\underline{\\mathbf{9 5 . 5 4}}$ | 98.52 | $\\underline{\\mathbf{8 7 . 6 1}}$ |\n\nTable 1: Test accuracies on LRA, where $\\boldsymbol{X}$ means the model isn't outperforming random guessing.",
    "approxdiag-10": "We use the boldface number to indicate the highest test accuracy among all models for each task. We use the underlined number to indicate the highest test accuracy within the comparable group. ![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-11.jpg?height=451&width=1557&top_left_y=1036&top_left_x=239)\n\nFigure 3: (a) and (b): the training and test accuracies of the S4D model and the S4-PTD model on contaminated and uncontaminated CIFAR-10 dataset (see section 5.2). (c): The effect of the perturbation size on the accuracy (shown in red) of the S4-PTD model and the eigenvector condition number (shown in blue) of the perturbed HiPPO matrix (see section 5.3). models against two different test sets: one is taken from the original CIFAR-10 dataset while the other one is contaminated by $10 \\%$ of sinusoidal noises whose frequencies are located near the spikes of $\\left|G_{\\text {Diag }}\\right|$. We plot the training and test accuracies of the two models in Figure 3a and b. Whereas the two models both achieve high accuracies on the uncontaminated test set, the S4D model does not generalize to the noisy dataset as the S4-PTD model does. That is, the S4D model is not robust to these noises. In comparison, since the S4-PTD initialization is uniformly close to the S4 initialization (see Theorem 3) when $\\|\\mathbf{E}\\|$ is small, the S4-PTD model is robust to noises with any mode. We remark, nevertheless, that the noises in this experiment are the \"worst-case\" noises and intentionally made to fail the S4D model; in practice, the distribution of sensitive modes of S4D in the frequency domain gets sparser as $n$ increases (see Figure 1), which improves its \"average-case\" robustness. ### 5.3 Ablation study of our model\n\nAs mentioned in section 4, the size of the perturbation plays a key role in the performance of our S4-PTD and S5-PTD models. When $\\mathbf{E}=0$, the eigenvector condition number of $\\mathbf{A}_{H}$ is exponential in $n$, making it numerically impossible to diagonalize when $n$ is moderately large. On the other hand, when $\\mathbf{E}$ overshadows $\\mathbf{A}_{H}$, the initialization scheme becomes a random one, often leading to poor performance [18]. In this section, we train an S4-PTD model to learn the sequential CIFAR (sCIFAR) task. We control the size of the perturbation $\\|\\mathbf{E}\\|$ by changing\nthe hyperparameter $\\gamma$ in the optimization problem eq. (12). For each perturbation matrix $\\mathbf{E}$, we then initialize our S4-PTD model by diagonalizing $\\mathbf{A}_{H}+\\mathbf{E}$. In Figure 3c, we plot (in red) the test accuracies with respect to different perturbation sizes. We see that our S4-PTD model achieves its best performance when the ratio between the perturbation size and the size of the HiPPO matrix is between $10^{-2}$ and 1 , while the accuracy drops when this ratio gets too small or too large. This aligns with our expectations. In addition, the (blue) curve of the eigenvector condition number admits a straight-line pattern with a slope of roughly -1 , corroborating the factor $\\epsilon^{-1}$ in Theorem 5. ## Acknowledgments\n\nThis work was supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Scientific Discovery through Advanced Computing (SciDAC) program, under Contract Number DE-AC02-05CH11231 at Lawrence Berkeley National Laboratory. It used the Lawrencium computational cluster provided by the IT Division at the Lawrence Berkeley National Laboratory and resources of the National Energy Research Scientific Computing Center (NERSC, using award ASCR-ERCAP0023337), a U.S. Department of Energy Office of Science User Facility located at Lawrence Berkeley National Laboratory, both operated under Contract No. DE-AC02-05CH11231. NBE would also like to acknowledge NSF, under Grant No. 2319621, for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. ## References\n\n[1] Athanasios C. Antoulas and Brian D.O. Anderson. On the scalar rational interpolation problem. IMA Journal of Mathematical Control and Information, 3(2-3):61-88, 1986. [2] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pages 1120-1128. PMLR, 2016 . [3] Quirin Aumann and Ion Victor Gosea. Practical challenges in data-driven interpolation: dealing with noise, enforcing stability, and computing realizations. arXiv preprint arXiv:2301.04906, 2023. [4] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.",
    "approxdiag-11": "arXiv preprint arXiv:1803.01271, 2018 . [5] Jess Banks, Jorge Garza-Vargas, Archit Kulkarni, and Nikhil Srivastava. Pseudospectral shattering, the sign function, and diagonalization in nearly matrix multiplication time. Foundations of Computational Mathematics, pages 1-89, 2022. [6] Jess Banks, Archit Kulkarni, Satyaki Mukherjee, and Nikhil Srivastava. Gaussian regularization of the pseudospectrum and davies' conjecture. Communications on Pure and Applied Mathematics, 74(10):2114-2131, 2021. [7] Bo Chang, Minmin Chen, Eldad Haber, and Ed H Chi. Antisymmetricrnn: A dynamical system view on recurrent neural networks.",
    "approxdiag-12": "In International Conference on Machine Learning, 2019. [8] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Machine Learning, 2020 .",
    "approxdiag-13": "[9] P. M. Cohn. Further algebra and applications. Springer-Verlag London, Ltd., London, 2003 . [10] E Brian Davies and Mildred Hager. Perturbations of Jordan matrices. Journal of Approximation Theory, 156(1):82-94, 2009. [11] E.B. Davies. Approximate diagonalization. SIAM journal on matrix analysis and applications, 29(4):1051-1064, 2008. [12] James Demmel. The componentwise distance to the nearest singular matrix. SIAM Journal on Matrix Analysis and Applications, 13(1):10-19, 1992. [13] N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. [14] N Benjamin Erichson, Soon Hoe Lim, and Michael W Mahoney. Gated recurrent neural networks with weighted time-delay feedback. arXiv preprint arXiv:2212.00228, 2022. [15] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections.",
    "approxdiag-14": "Advances in neural information processing systems, 33:1474-1487, 2020. [16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021. [19] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections. International Conference on Learning Representations, 2023. [20] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. International Conference on Learning Representations, 2023. [21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "approxdiag-15": "In International conference on machine learning, pages 5156-5165. PMLR, 2020. [22] Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov, Yoshua Bengio, and Guillaume Lajoie. Non-normal recurrent neural network (nnrnn): learning long time dependencies while improving expressivity with transient dynamics.",
    "approxdiag-16": "Advances in neural information processing systems, 32, 2019. [23] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Machine Learning, 2020. [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.",
    "approxdiag-17": "2009. [25] Ankit Kumar and Kristofer Bouchard. Non-normality in neural networks. In AI and Optical Data Sciences III, volume 12019, pages 70-76. SPIE, 2022. [26] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. [27] A Emin Orhan and Xaq Pitkow. Improved memory in recurrent neural networks with sequential non-normal dynamics. Internation Conference on Learning Representations, 2020 . [28] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [29] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. In International Conference on Machine Learning, 2022. [30] T Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies.",
    "approxdiag-18": "In International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.",
    "approxdiag-19": "[36] Max A. Woodbury. Inverting modified matrices. Princeton University, Princeton, N. J., 1950. Statistical Research Group, Memo.",
    "approxdiag-20": "Rep.",
    "approxdiag-21": "no. 42,. [37] Cagatay Yildiz, Markus Heinonen, and Harri L\u00e4hdesm\u00e4ki. Continuous-time model-based reinforcement learning. In International Conference on Machine Learning, pages 1200912018. PMLR, 2021. [38] Kemin Zhou and John Comstock Doyle. Essentials of robust control, volume 104.",
    "approxdiag-22": "Prentice Hall, Upper Saddle River, NJ, 1998. [39] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.",
    "approxdiag-23": "In International Conference on Machine Learning, pages 27268-27286. PMLR, 2022. ## Appendix\n\nThe Appendix is organized as follows. In Appendix A, we survey the background of ill-posed problems, including conditioning, stability, and backward and forward errors. In Appendix B, we provide more background information on LTI systems, including the derivation of the transfer function, system diagonalization, and system discretization, which provides insights into our theory and models. In Appendix C, we prove Lemma 1 on the difference between the two transfer functions. Using this result, we prove Theorem 2 and 1 on the uniform divergence and the input-wise convergence in Appendix D and E, respectively. We then present in Appendix F some numerical experiments corroborating these two theorems and in Appendix G some plots to show the quantitative interpolation and extrapolation errors in our synthetic example in section 3.4. In Appendix H, we prove the results in section 4 that are related to perturbing the HiPPO matrix, which are then verified in Appendix I by a numerical experiment. Finally, in Appendix J we give the details of our experiments in section 5 . ## A More background information of ill-posed problems\n\nTo make the phrase \"ill-posed\" precise, we need to introduce the idea of condition numbers. The conditioning is a property of a problem and it does not depend on the algorithm that we use. Abstractly, we let the problem space $\\mathcal{X}$ and the solution space $\\mathcal{Y}$ be two normed vector spaces with the norms $\\|\\cdot\\| \\mathcal{X}$ and $\\|\\cdot\\| \\mathcal{Y}$, respectively. Each element in $x \\in \\mathcal{X}$ is considered as an instance of the problem and its solution in the solution space $\\mathcal{Y}$ is defined by a map $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$. For example, if we want to solve a system $\\mathbf{A x}=\\mathbf{b}_{0}$ with different matrices $\\mathbf{A}$ and a fixed vector $\\mathbf{b}_{0}$, then we can make $\\mathcal{X}$ the space of $n$-by- $n$ matrices and $\\mathcal{Y}$ the space of vectors of length $n$. In that case, we have $f(\\mathbf{A})=\\mathbf{A}^{-1} \\mathbf{b}_{0} \\cdot{ }^{1}$ Likewise, consider the problem of finding eigenvalues. We can make $\\mathcal{X}$ and $\\mathcal{Y}$ both equal to the space of $n$-by- $n$ matrices and $f(\\mathbf{A})=\\boldsymbol{\\Lambda}$, the eigenvalue matrix of $\\mathbf{A}$. Now, given an instance $x \\in \\mathcal{X}$, we define the (absolute) condition number of problem $f$ at $x$ to be\n\n$$\n\\kappa(x ; f)=\\lim _{\\epsilon \\rightarrow 0} \\sup _{\\|\\delta x\\|_{\\mathcal{X}} \\leq \\epsilon} \\frac{\\|f(x)-f(x+\\delta x)\\|_{\\mathcal{Y}}}{\\|\\delta x\\|_{\\mathcal{X}}}\n$$\n\nIntuitively, a large condition number means that if we perturb the problem by a little bit, then the solution may become drastically different. Hence, in general, we do not expect that a solution of an ill-conditioned problem can be found accurately using floating-point arithmetic because a small rounding error has a large effect on the computed solution. Unlike the conditioning of a problem $x$, stability is a property of an algorithm. Let $\\tilde{f}: \\mathcal{X} \\rightarrow \\mathcal{Y}$ be an algorithm that solves $f$. We are particularly interested in the \"backward stability\" of $\\tilde{f}$. That is, if for any $x \\in \\mathcal{X}$, there exists an element $\\tilde{x} \\in \\mathcal{X}$ so that $\\tilde{f}(x)=f(\\tilde{x})$ and\n\n$$\nE_{b}(x)=\\frac{\\|x-\\tilde{x}\\|_{\\mathcal{X}}}{\\|x\\|_{\\mathcal{X}}}\n$$\n\nis small, then we say that $\\tilde{f}$ is backward stable. Intuitively, this is saying that our algorithm is computing the solution to a nearby problem $\\tilde{x}$. Note that this is different from saying that\n\n$$\nE_{f}(x)=\\frac{\\|f(x)-\\tilde{f}(x)\\|_{\\mathcal{Y}}}{\\|f(x)\\|_{\\mathcal{Y}}}\n$$\n\nis small. This error measures how accurately we solved our problem. The error $E_{f}$ is called a forward error, while $E_{b}$ is called a backward error. We can control the forward error using the\n\n[^1]![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-16.jpg?height=961&width=1132&top_left_y=142&top_left_x=468)\n\nFigure 4: An illustration of the perturbation of an ill-posed problem. backward error, and this bound is established through the condition number of the problem. (Intuitively, this says that if a problem is well-conditioned, then a small perturbation to the problem does not change the solution by too much. Hence, a small backward error leads to a small forward error.) The advantage of studying backward stability is two-fold. First, the backward error is decoupled from the conditioning of the problem. Hence, backward stable algorithms are much more common than forward stable algorithms, because in many cases, the ill-conditioned problems essentially prevent an algorithm from being forward stable. On the other hand, if an algorithm is backward stable, then we know that its forward error must be small on well-conditioned problems. In our paper, we consider the case where we are forced to solve an ill-conditioned problem $x$. We propose to use a backward stable algorithm $\\tilde{f}$ to solve it. Since the problem is ill-conditioned, we do not have that $f(x)$ is close to $\\tilde{f}(x)$, i.e., we cannot find the solutions accurately. However, we know that $\\tilde{f}(x)$ is the solution to $\\tilde{x}$, where $x \\approx \\tilde{x}$. In many machine learning applications, this is enough to guarantee an acceptable solution. (See Figure 4.)\n\n## B More background information of state-space models\n\nRecall that an LTI system $\\Sigma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ is given by\n\n$$\n\\begin{aligned}\n\\mathbf{x}^{\\prime}(t) & =\\mathbf{A x}(t)+\\mathbf{B u}(t) \\\\\n\\mathbf{y}(t) & =\\mathbf{C x}(t)+\\mathbf{D u}(t)\n\\end{aligned}\n$$\n\nAssume the initial condition is given by $\\mathbf{x}(0)=\\mathbf{0}$ and the input function $\\mathbf{u}(\\cdot)$ is bounded and integrable. Suppose the system is asymptotically stable, i.e., $\\Lambda(\\mathbf{A})$ is contained in the left halfplane. Then, we have $\\mathbf{x}(\\cdot)$ and $\\mathbf{y}(\\cdot)$ are also bounded and integrable. By taking the Fourier transform of the LTI system, we have\n\n$$\n\\begin{aligned}\ns i \\hat{\\mathbf{x}}(s) & =\\mathbf{A} \\hat{\\mathbf{x}}(s)+\\mathbf{B} \\hat{\\mathbf{u}}(s) \\\\\n\\hat{\\mathbf{y}}(s) & =\\mathbf{C} \\hat{\\mathbf{x}}(s)+\\mathbf{D} \\hat{\\mathbf{u}}(s)\n\\end{aligned}\n$$\n\nRearranging the first equation gives us\n\n$$\n\\hat{\\mathbf{x}}(s)=(s i \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B} \\hat{\\mathbf{u}}(s), \\quad s \\in \\mathbb{R}\n$$\n\nPlugging it into the second equation of eq. (14), we can derive the transfer function on the imaginary axis:\n\n$$\n\\hat{\\mathbf{y}}(s)=\\underbrace{\\left[\\mathbf{C}(s i \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\mathbf{D}\\right]}_{G(s i)} \\hat{\\mathbf{u}}(s), \\quad s \\in \\mathbb{R}\n$$\n\nLet $\\mathbf{V} \\in \\mathbb{C}^{n \\times n}$ be an invertible matrix. Consider the system $\\tilde{\\Sigma}=\\left(\\mathbf{V}^{-1} \\mathbf{A V}, \\mathbf{V}^{-1} \\mathbf{B}, \\mathbf{C V}, \\mathbf{D}\\right)$ :\n\n$$\n\\begin{aligned}\n\\mathbf{x}^{\\prime}(t) & =\\mathbf{V}^{-1} \\mathbf{A} \\mathbf{V}(t)+\\mathbf{V}^{-1} \\mathbf{B u}(t) \\\\\n\\mathbf{y}(t) & =\\mathbf{C V x}(t)+\\mathbf{D u}(t)\n\\end{aligned}\n$$\n\nBy multiplying the first equation by $\\mathbf{V}$, we have\n\n$$\n\\mathbf{V} \\mathbf{x}(t)=\\mathbf{A V} \\mathbf{x}(t)+\\mathbf{B u}(t)\n$$\n\nand defining the new state variable $\\boldsymbol{\\xi}(t)=\\mathbf{V} \\mathbf{x}(t)$, we can write $\\tilde{\\Sigma}$ into\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\xi}(t) & =\\mathbf{A} \\boldsymbol{\\xi}(t)+\\mathbf{B u}(t) \\\\\n\\mathbf{y}(t) & =\\mathbf{C} \\boldsymbol{\\xi}(t)+\\mathbf{D u}(t)\n\\end{aligned}\n$$\n\nHence eq. (13) and (15) are equivalent with their states connected via V. We also can verify this by computing the transfer function of $\\tilde{\\Sigma}$ :\n\n$$\n\\tilde{G}(s):=\\mathbf{C V}\\left(s \\mathbf{I}-\\mathbf{V}^{-1} \\mathbf{A} \\mathbf{V}\\right)^{-1} \\mathbf{V}^{-1} \\mathbf{B}+\\mathbf{D}=\\mathbf{C}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\mathbf{D}=G(s)\n$$\n\nThe LTI system $\\Sigma$ is continuous-time. In order to apply it to sequential input, we need to discretize the system. Given a step size $\\Delta t$, there are two common ways of discretizing the system:\n\n$$\n\\begin{aligned}\n& \\text { Bilinear }: \\overline{\\mathbf{A}}=\\left(\\mathbf{I}-\\frac{\\Delta t}{2} \\mathbf{A}\\right)^{-1}\\left(\\mathbf{I}+\\frac{\\Delta t}{2} \\mathbf{A}\\right), \\quad \\overline{\\mathbf{B}}=\\Delta t\\left(\\mathbf{I}-\\frac{\\Delta t}{2} \\mathbf{A}\\right)^{-1} \\mathbf{B}, \\quad(\\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})=(\\mathbf{C}, \\mathbf{D}), \\\\\n& \\mathrm{ZOH}: \\overline{\\mathbf{A}}=\\exp (\\Delta t \\mathbf{A}), \\quad \\overline{\\mathbf{B}}=\\mathbf{A}^{-1}(\\exp (\\Delta t \\mathbf{A})-\\mathbf{I}) \\mathbf{B}, \\quad(\\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})=(\\mathbf{C}, \\mathbf{D})\n\\end{aligned}\n$$\n\nThen, the discrete system\n\n$$\n\\begin{aligned}\n& \\mathbf{x}_{t}=\\overline{\\mathbf{A}} \\mathbf{x}_{t-1}+\\overline{\\mathbf{B}} \\mathbf{u}_{t-1} \\\\\n& \\mathbf{y}_{t}=\\overline{\\mathbf{C}} \\mathbf{x}_{t}+\\overline{\\mathbf{D}} \\mathbf{u}_{t}\n\\end{aligned}\n$$\n\ntakes the discrete sequential input $\\left(\\mathbf{u}_{0}, \\mathbf{u}_{1}, \\ldots\\right)$. The discrete system eq. (16) mimics the continuous system eq. (13) by sampling the continuous input signal $\\mathbf{u}(\\cdot)$ at time intervals $\\Delta t$ : $\\left(\\mathbf{u}_{0}, \\mathbf{u}_{1}, \\ldots\\right)=(\\mathbf{u}(0 \\Delta t), \\mathbf{u}(1 \\Delta t), \\ldots)$. The SSMs store the continuous LTI systems. When evaluating on a discrete input, they discretize the continuous systems using a trainable step size $\\Delta t$ and either the Bilinear or the ZOH descritization. ## C Proof of Lemma 1\n\nIn this section, we prove Lemma 1 on the difference between the transfer functions $G_{\\text {DPLR }}$ and $G_{\\text {Diag }}$. The starting point is to use the Woodbury matrix identity to separate out the rank-1 part in the resolvent that appears in $G_{\\text {DPLR }}$. In section 3 , we let $\\mathbf{C}=\\mathbf{e}_{\\ell}^{\\top} \\mathbf{V}_{H}$ for some fixed $\\ell$. Since we will reserve the letter $\\ell$ as an index in the proof, in the appendices, we change the notation and assume $\\mathbf{C}=\\mathbf{e}_{p}^{\\top} \\mathbf{V}_{H}$. While this introduces a notation collision with the length of the output vector $\\mathbf{y}$, it does not cause any confusion in the proofs. Proof of Lemma 1. For notational cleanliness, in this proof, we define $\\mathbf{A}=\\mathbf{A}_{H}, \\mathbf{A}^{\\perp}=\\mathbf{A}_{H}^{\\perp}$, and $\\mathbf{B}=\\mathbf{B}_{H}$. To begin with, we expand $\\left(s \\mathbf{I}-\\mathbf{A}_{H}^{\\perp}\\right)^{-1} \\mathbf{B}_{H}$ using the Woodbury matrix identity [36]:\n\n$$\n\\begin{aligned}\n(s \\mathbf{I}- & \\left.\\mathbf{A}_{H}^{\\perp}\\right)^{-1} \\mathbf{B}_{H}=\\left(s \\mathbf{I}-\\mathbf{A}-\\mathbf{B B}^{\\top}\\right)^{-1} \\mathbf{B} \\\\\n& =\\left[(s \\mathbf{I}-\\mathbf{A})^{-1}+(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}\\left(1-\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}\\right)^{-1} \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1}\\right] \\mathbf{B} \\\\\n& =(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\frac{\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}}{1-\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B} \\\\\n& =(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\left(1+\\frac{2 \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}-1}{1-\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}}\\right)(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B} \\\\\n& =2(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\frac{2 \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}-1}{1-\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B} . \\end{aligned}\n$$\n\nHence, when $\\mathbf{C}_{\\text {DPLR }}=\\mathbf{C}_{\\text {Diag }}=\\mathbf{I}$, the difference between $G_{\\text {DPLR }}$ and $G_{\\text {Diag }}$ can be written as\n\n$$\n\\frac{1}{2}\\left(s \\mathbf{I}-\\mathbf{A}_{H}^{\\perp}\\right)^{-1} \\mathbf{B}_{H}-\\left(s \\mathbf{I}-\\mathbf{A}_{H}\\right)^{-1} \\mathbf{B}_{H}=\\frac{2 \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}-1}{2-2 \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B} . $$\n\nOur next step is to study $\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}$ that appears in eq. (17). To wit, we use Hua's identity $[9]$ to obtain\n\n$$\n\\begin{aligned}\n\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B} & =\\mathbf{B}^{\\top}\\left(-\\mathbf{A}^{-1}+\\left(\\mathbf{A}-\\frac{1}{s} \\mathbf{A}^{2}\\right)^{-1}\\right) \\mathbf{B} \\\\\n& =\\mathbf{B}^{\\top}\\left(-\\mathbf{A}^{-1}+s(\\mathbf{A}(s \\mathbf{I}-\\mathbf{A}))^{-1}\\right) \\mathbf{B}\n\\end{aligned}\n$$\n\nIt is easy to see that $\\mathbf{B}^{\\top} \\mathbf{A}^{-1} \\mathbf{B}=-1 / 2$. Hence, we have\n\n$$\n\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}=\\frac{1}{2}+s \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{A}^{-1} \\mathbf{B}\n$$\n\nNote that when $s=0$, the second term in the expression above vanishes, and therefore we already have that $\\left(\\mathbf{A}^{\\perp}\\right)^{-1} \\mathbf{B} / 2=\\mathbf{A}^{-1} \\mathbf{B}$. To deal with the general case when $s$ is a purely imaginary number, we first note that $\\mathbf{A}^{-1} \\mathbf{B}=-\\mathbf{e}_{1} / \\sqrt{2}$ because $\\mathbf{B}$ is $-1 / \\sqrt{2}$ times the first column of $\\mathbf{A}$. Hence, $s \\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{A}^{-1} \\mathbf{B}$ is equal to $s$ times the first coordinate of $\\mathbf{B}^{\\top}(s \\mathbf{I}-$ $\\mathbf{A})^{-1}$, which we now compute using Cramer's rule. The first coordinate of $\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1}$ can be written as\n\n$$\n\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{e}_{1}=\\overline{\\mathbf{e}_{1}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-*} \\mathbf{B}}=\\overline{\\mathbf{e}_{1}^{\\top}\\left(-s \\mathbf{I}-\\mathbf{A}^{*}\\right)^{-1} \\mathbf{B}}\n$$\n\nwhere $\\bar{s}=-s$ since $s$ is purely imaginary. By Cramer's rule, we have that\n\n$$\n\\mathbf{e}_{1}^{\\top}\\left(-s \\mathbf{I}-\\mathbf{A}^{*}\\right)^{-1} \\mathbf{B}=\\frac{\\left[\\begin{array}{ccccc}\n1 & \\sqrt{3} & \\sqrt{5} & \\cdots & \\sqrt{2 n-1} \\\\\n\\sqrt{3} & 2-s & \\sqrt{15} & \\cdots & \\sqrt{3(2 n-1)} \\\\\n\\sqrt{5} & 0 & 3-s & \\cdots & \\sqrt{5(2 n-1)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sqrt{2 n-1} & 0 & 0 & \\cdots & n-s\n\\end{array}\\right]}{\\left[\\begin{array}{ccccc}\n1-s & \\sqrt{3} & \\sqrt{5} & \\cdots & \\sqrt{2 n-1} \\\\\n0 & 2-s & \\sqrt{15} & \\cdots & \\sqrt{3(2 n-1)} \\\\\n0 & 0 & 3-s & \\cdots & \\sqrt{5(2 n-1)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & n-s\n\\end{array}\\right]}\n$$\n\nObviously, the denominator is $\\sqrt{2} \\prod_{j=1}^{n}(j-s)$. We compute the numerator by solving a recurrence. We use $D_{n}$ to denote this determinant. Hence, we have $D_{1}=1$ and $D_{2}=-1-s$. To compute $D_{n}$, we expand the last row and obtain\n\n$$\nD_{n}=(-1)^{n+1} \\sqrt{2 n-1} \\operatorname{det}\\left[\\begin{array}{ccccc}\n\\sqrt{3} & \\sqrt{5} & \\cdots & \\sqrt{2 n-3} & \\sqrt{2 n-1} \\\\\n2-s & \\sqrt{15} & \\cdots & \\sqrt{3(2 n-3)} & \\sqrt{3(2 n-1)} \\\\\n0 & 3-s & \\cdots & \\sqrt{5(2 n-3)} & \\sqrt{5(2 n-1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & n-1-s & \\sqrt{(2 n-3)(2 n-1)}\n\\end{array}\\right]+(n-s) D_{n-1}\n$$\n\nTo compute the determinant of this submatrix, we have\n\n$$\n\\begin{aligned}\n& \\operatorname{det}\\left[\\begin{array}{cccccc}\n\\sqrt{3} & \\sqrt{5} & \\cdots & \\sqrt{2 n-3} & \\sqrt{2 n-1} \\\\\n2-s & \\sqrt{15} & \\cdots & \\sqrt{3(2 n-3)} & \\sqrt{3(2 n-1)} \\\\\n0 & 3-s & \\cdots & \\sqrt{5(2 n-3)} & \\sqrt{5(2 n-1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & n-1-s & \\sqrt{(2 n-3)(2 n-1)}\n\\end{array}\\right] \\\\\n&=(-1)^{n-2} \\operatorname{det}\\left[\\begin{array}{ccccc}\n\\sqrt{2 n-1} & \\sqrt{3} & \\sqrt{5} & \\cdots & \\sqrt{2 n-3} \\\\\n\\sqrt{3(2 n-1)} & 2-s & \\sqrt{15} & \\cdots & \\sqrt{3(2 n-3)} \\\\\n\\sqrt{5(2 n-1)} & 0 & 3-s & \\cdots & \\sqrt{5(2 n-3)} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sqrt{(2 n-3)(2 n-1)} & 0 & 0 & \\cdots & n-1-s\n\\end{array}\\right] \\\\\n& \\quad=(-1)^{n-1} \\sqrt{2 n-1} D_{n-1} . \\end{aligned}\n$$\n\nHence, combining the two equations above, we obtain the following recurrence:\n\n$$\nD_{n}=-(2 n-1) D_{n-1}+(n-s) D_{n-1}=(-n+1-s) D_{n-1}\n$$\n\nIt is then easy to show that\n\n$$\nD_{n}=(-1-s)(-2-s) \\cdots(-(n-1)-s)=(-1)^{n-1} \\prod_{j=1}^{n-1}(j+s)\n$$\n\nPutting everything together, we have\n\n$$\n\\mathbf{B}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}=\\frac{1}{2}-s \\operatorname{conj}\\left(\\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j+s)}{2 \\prod_{j=1}^{n}(j-s)}\\right)=\\frac{1}{2}-s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{2 \\prod_{j=1}^{n}(j+s)}\n$$\n\nNow, it remains to study the term $(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}$ in eq.",
    "approxdiag-24": "(17).",
    "approxdiag-25": "Since it is a vector of length $n$, we study it component-wise, and the derivation is similar to the one above. To begin with, we fix\na component $p$ that we wish to study. Then, by Cramer's rule, we have\n\n$$\n\\begin{aligned}\n& \\mathbf{e}_{p}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-20.jpg?height=644&width=1326&top_left_y=386&top_left_x=408)\nClearly, we have that the denominator is equal to $\\sqrt{2} \\prod_{j=1}^{n}(j+s)$. To compute the numerator, we first subtract the $p$ th column from the first column. This shows that the numerator is equal to\n\n$$\ns \\operatorname{det}\\left[\\begin{array}{cccccc}\ns+2 & 0 & \\cdots & \\sqrt{3} & \\cdots & 0 \\\\\n\\sqrt{15} & s+3 & \\cdots & \\sqrt{5} & \\cdots & 0 \\\\\n\\sqrt{21} & \\sqrt{35} & \\cdots & \\sqrt{7} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\sqrt{3(2 n-1)} & \\sqrt{5(2 n-1)} & \\cdots & \\sqrt{2 n-1} & \\cdots & s+n\n\\end{array}\\right]\n$$\n\nWe can then subtract $\\sqrt{3}$ times the $(p-1)$ th column of the submatrix from the first column, showing that the numerator is equal to\n\n$$\ns(s-1) \\operatorname{det}\\left[\\begin{array}{ccccc}\ns+3 & \\cdots & \\sqrt{5} & \\cdots & 0 \\\\\n\\sqrt{35} & \\cdots & \\sqrt{7} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n\\sqrt{5(2 n-1)} & \\cdots & \\sqrt{2 n-1} & \\cdots & s+n\n\\end{array}\\right]\n$$\n\nContinuing in this manner, we have that the numerator is equal to\n\n$$\n\\begin{aligned}\n& s(s-1) \\cdots(s-p+2) \\operatorname{det}\\left[\\begin{array}{ccccc}\n\\sqrt{2 p-1} & 0 & 0 & \\cdots & 0 \\\\\n\\sqrt{2 p+1} & s+p+1 & 0 & \\cdots & 0 \\\\\n\\sqrt{2 p+3} & \\sqrt{(2 p+3)(2 p+1)} & s+p+2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sqrt{2 n-1} & \\sqrt{(2 n-1)(2 p+1)} & \\sqrt{(2 n-1)(2 p+2)} & \\cdots & s+n\n\\end{array}\\right] \\\\\n& =\\sqrt{2 p-1} s(s-1) \\cdots(s-p+2)(s+p+1)(s+p+2) \\cdots(s+n) . \\end{aligned}\n$$\n\nHence, we have\n\n$$\n\\mathbf{e}_{p}^{\\top}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}=\\frac{\\sqrt{2 p-1} \\prod_{j=0}^{p-2}(s-j)}{\\sqrt{2} \\prod_{j=1}^{p}(s+j)}\n$$\n\nNote that the expression above does not depend on $n$. Combining eq. (17), (18), (20), when\n$\\mathbf{C}_{\\text {DPLR }}=\\mathbf{C}_{\\text {Diag }}=\\mathbf{e}_{p}^{\\top}$, we have\n\n$$\n\\begin{aligned}\nG_{\\text {DPLR }}(s)-G_{\\text {Diag }}(s) & =\\mathbf{e}_{p}^{\\top}\\left[\\frac{1}{2}\\left(s \\mathbf{I}-\\mathbf{A}^{\\perp}\\right)^{-1} \\mathbf{B}-(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}\\right] \\\\\n& =\\frac{2\\left(\\frac{1}{2}-s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{2 \\prod_{j=1}^{n}(j+s)}\\right)-1}{2-2\\left(\\frac{1}{2}-s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)}\\right)} \\frac{\\sqrt{2 p-1} \\prod_{j=0}^{p-2}(s-j)}{\\sqrt{2} \\prod_{j=1}^{p}(s+j)} \\\\\n& =\\frac{-s \\frac{(-1)^{n-1} \\prod_{j=1}^{n=1}(j-s)}{\\prod_{j=1}^{n}(j+s)} \\sqrt{2 p-1} \\frac{\\prod_{j=0}^{p-2}(s-j)}{\\prod_{j=1}^{n}(s+j)}}{\\sqrt{2}\\left(1+s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)}\\right)}\n\\end{aligned}\n$$\n\nThis completes the proof of the lemma. ## D Proof of Theorem 2\n\nIn this section, we prove Theorem 2. The idea is to locate the last spike in the figure of $G_{\\text {Diag }}$ (see Figure 1) and control the height of its peak by lower-bounding the denominator of eq.",
    "approxdiag-26": "(9). Proof of Theorem 2. Fix an $n \\geq p$. Define $s_{n}$ by\n\n$$\ns_{n}=\\max \\left\\{s \\geq 0 \\mid A(s):=s i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s i)}{\\prod_{j=1}^{n}(j+s i)} \\text { is real and } \\leq 0\\right\\}\n$$\n\nNote that this set is finite because $A(s) \\rightarrow 1$ as $s \\rightarrow \\infty$; thus, its supremum is attained. Therefore, we have that\n\n$$\n\\left|1+s_{n} i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}\\left(j-s_{n} i\\right)}{\\prod_{j=1}^{n}\\left(j+s_{n} i\\right)}\\right|=1-\\left|s_{n} i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}\\left(j-s_{n} i\\right)}{\\prod_{j=1}^{n}\\left(j+s_{n} i\\right)}\\right|=\\frac{\\left|n+s_{n} i\\right|-s_{n}}{\\left|n+s_{n} i\\right|}\n$$\n\nIn what follows, we show that $s_{n}=\\Omega\\left(n^{2}\\right)^{2}$ Then, combined with Lemma 1 , we have that as $n \\rightarrow \\infty$,\n\n$$\n\\begin{aligned}\n\\left|G_{\\mathrm{DPLR}}\\left(s_{n} i\\right)-G_{\\text {Diag }}\\left(s_{n} i\\right)\\right| & =\\frac{s_{n}^{2} \\sqrt{2 p-1}}{\\sqrt{2}\\left|p-1+s_{n} i\\right| p+s_{n} i \\mid\\left(\\left|n+s_{n} i\\right|-s_{n}\\right)}=\\Theta(1) \\frac{1}{\\sqrt{n^{2}+s_{n}^{2}}-s_{n}} \\\\\n& =\\Theta(1) \\frac{\\sqrt{n^{2}+s_{n}^{2}}+s_{n}}{n^{2}}\n\\end{aligned}\n$$\n\nIf we can show that $s_{n}=\\Omega\\left(n^{2}\\right)$, then we have that $\\left|G_{\\text {DPLR }}\\left(s_{n} i\\right)-G_{\\text {Diag }}\\left(s_{n} i\\right)\\right|=\\Omega(1)$ and does not converge to zero. To this end, we first rewrite the expression into\n\n$$\nA\\left(s_{n}\\right)=s_{n} i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}\\left(j-s_{n} i\\right)}{\\prod_{j=1}^{n}\\left(j+s_{n} i\\right)}=\\frac{s_{n} i}{n+s_{n} i} \\prod_{j=1}^{n-1} \\frac{s_{n} i-j}{s_{n} i+j}=\\frac{s_{n} i}{n+s_{n} i} \\exp \\left(-i 2 \\sum_{j=1}^{n-1} \\arctan \\frac{j}{s_{n}}\\right)\n$$\n\nSince $\\arctan x=\\Theta(x)$ as $x \\rightarrow 0$, if we assume, for a contradiction, that $s_{n_{k}}=o\\left(n_{k}{ }^{2}\\right)$ for a subsequence $s_{n_{k}}$ of $s_{n}$, then we must have that\n\n$$\n\\sum_{j=1}^{n_{k}-1} \\arctan \\frac{j}{s_{n_{k}}}-\\sum_{j=1}^{n_{k}-1} \\arctan \\frac{j}{\\max \\left\\{n_{k}, 2 s_{n_{k}}\\right\\}} \\rightarrow \\infty \\quad \\text { as } \\quad k \\rightarrow \\infty\n$$\n\n[^2]We pick some index $n_{k} \\geq p$ large enough such that $\\sum_{j=1}^{n_{k}-1} \\arctan \\left(j / s_{n_{k}}\\right)-\\sum_{j=1}^{n_{k}-1} \\arctan \\left(j / \\max \\left\\{n_{k}, 2 s_{n_{k}}\\right\\}\\right) \\geq$ $2 \\pi$. Hence, as $s$ increases from $s_{n_{k}}$ to $\\max \\left\\{n_{k}, 2 s_{n_{k}}\\right\\}$, the angle of the unit imaginary number\n\n$$\n\\exp \\left(-i 2 \\sum_{j=1}^{n_{k}-1} \\arctan \\frac{j}{s}\\right)\n$$\n\nchanges by at least $4 \\pi$ whereas the angle of $s i /(n+s i)$ changes by at most $\\pi / 2$. Hence, the winding number of the curve\n\n$$\n\\Gamma: s \\mapsto \\frac{s i}{n_{k}+s i} \\exp \\left(-i 2 \\sum_{j=1}^{n_{k}-1} \\arctan \\frac{j}{s}\\right), \\quad s \\in\\left[s_{n_{k}}, \\max \\left\\{n_{k}, 2 s_{n_{k}}\\right\\}\\right]\n$$\n\nis non-zero. That is, we must have an $s \\in\\left(s_{n_{k}}, \\max \\left\\{n_{k}, 2 s_{n_{k}}\\right\\}\\right)$ such that the angle of $A(s)$ is equal to $\\pi$ modulo $2 \\pi$, but this is a contradiction because $s_{n_{k}}<s$. Hence, we have $s_{n}=$ $\\Omega\\left(n^{2}\\right)$. ## E Proof of Theorem 1\n\nIn this section, we prove Theorem 1. Since the proof is very involved, we provide some intuition here. In Figure 1, we observe that for a sufficiently large $n$, as $|s|$ increases, the difference between the two transfer functions, $G_{\\text {DPLR }}-G_{\\text {Diag }}$, goes through three stages. In the first stage (i.e., the pre-spike stage), the large spikes have yet developed. In this stage, as $n$ increases, $\\left|G_{\\text {DPLR }}-G_{\\text {Diag }}\\right|$ decreases uniformly. In the second stage (i.e., the spike stage), the spikes start to occur. This is the stage in which we do not get uniform convergence. However, by carefully controlling the locations and the total measure of the spikes, we can show that when the Fourier transform of a fixed input function with a sufficient decay is multiplied with $G_{\\text {DPLR }}-G_{\\text {Diag }}$, its integral on the second stage vanishes linearly as $n \\rightarrow \\infty$. Finally, after the last spike, we enter the third stage (i.e., the post-spike stage). In this stage, $\\left|G_{\\mathrm{DPLR}}-G_{\\text {Diag }}\\right|$ enjoys rapid decay. In what follows, we carefully analyze the three stages separately to prove Theorem 1. Proof of Theorem 1. Let u satisfy the assumptions in Theorem 1. Without loss of generality, we assume $\\hat{\\mathbf{u}}(s)$ vanishes on $(-\\infty, 0]$ because the argument would be symmetric for a negative s. Let\n\n$$\nH_{n}(s)=G_{\\operatorname{DPLR}}(s)-G_{\\text {Diag }}(s)=\\frac{-s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)} \\sqrt{2 p-1} \\frac{\\prod_{j=0}^{p-2}(s-j)}{\\prod_{j=1}^{n}(s+j)}}{\\sqrt{2}\\left(1+s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)}\\right)}\n$$\n\nWe set $s_{n}^{(1)}=c n$, where $c$ is a universal constant determined later on, and\n\n$$\ns_{n}^{(2)}=\\max \\left\\{s \\geq 0 \\mid A(s):=s i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s i)}{\\prod_{j=1}^{n}(j+s i)} \\text { is purely imaginary and } \\operatorname{Im}(A(s)) \\geq 0\\right\\}\n$$\n\nBy the same argument as in the proof of Theorem 2, we have $s_{n}^{(2)}=\\mathcal{O}\\left(n^{2}\\right)$. To compute the integral of $\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2}$ on $[0, \\infty)$, we do so on each of the three stages, marked by $\\left[0, s_{n}^{(1)}\\right)$, $\\left[s_{n}^{(1)}, s_{n}^{(2)}\\right)$, and $\\left[s_{n}^{(2)}, \\infty\\right)$, respectively. Since $2 p-1$ is a constant appearing unanimously in $\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2}$ for all $n$, we absorb it into the asymptotic notations in this proof. Unless otherwise stated, the constants in the asymptotic bounds in this proof are universal constants depending only on $p$; in particular, they do not depend on $n$ or $s$. Integrate on the pre-spike stage: Since\n\n$$\n\\left|s \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s)}{\\prod_{j=1}^{n}(j+s)}\\right|=\\frac{|s|}{|n+s|}\n$$\n\nand $s=\\mathcal{O}(n)$ whenever $0 \\leq s \\leq s_{n}^{(1)}$, the denominator of $H_{n}$ is lower-bounded by a constant independent of $n$. Hence, we have $\\left|H_{n}(s i)\\right|=\\mathcal{O}\\left(n^{-1}\\right)$ on $\\left[0, s_{n}^{(1)}\\right)$. Using H\u00f6lder's inequality, we have\n\n$$\n\\int_{0}^{s_{n}^{(1)}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s \\leq\\left\\|H_{n}(s i)\\right\\|_{L^{\\infty}\\left(\\left[0, s_{n}^{(1)}\\right)\\right)}^{2}\\|\\hat{\\mathbf{u}}(s)\\|_{L^{2}\\left(\\left[0, s_{n}^{(1)}\\right)\\right)}^{2}=\\mathcal{O}\\left(n^{-2}\\right)\n$$\n\nIntegrate on the post-spike stage: For $s \\geq s_{n}^{(2)}$, the denominator of $H_{n}$ is lower-bounded by a constant independent of $n$. Hence, we have $\\left|H_{n}(s i)\\right|=\\mathcal{O}\\left(s^{-1}\\right)$, where the constant does not depend on $n$. Hence, we have\n\n$$\n\\int_{s_{n}^{(2)}}^{\\infty}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s=\\int_{s_{n}^{(2)}}^{\\infty} \\mathcal{O}\\left(s^{-2-2 q}\\right) d s=\\mathcal{O}\\left(n^{-2-4 q}\\right)\n$$\n\nbecause $s_{n}^{(2)}=\\mathcal{O}\\left(n^{2}\\right)$. Integrate on the spike stage: To integrate $\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2}$ on $\\left[s_{n}^{(1)}, s_{n}^{(2)}\\right]$, we first define the angle function by\n\n$$\n\\begin{aligned}\na(s) & :=\\arg \\left(\\frac{s i}{n+s i}\\right)+2 \\sum_{j=1}^{n-1} \\arctan \\left(\\frac{j}{s}\\right)=\\arctan \\left(\\frac{n}{s}\\right)+2 \\sum_{j=1}^{n-1} \\arctan \\left(\\frac{j}{s}\\right) \\\\\n& \\equiv \\arg \\left(s i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s i)}{\\prod_{j=1}^{n}(j+s i)}\\right) \\quad(\\bmod 2 \\pi) . \\end{aligned}\n$$\n\nThe importance of $a(s)$ is that when $a(s)$ is close to $(2 k+1) \\pi$ for some integer $k$, we get a spike in the figure of $\\left|H_{n}\\right|$. We therefore partition the oscillation stage into two parts:\n\n$$\nS_{1}=\\left\\{s \\in\\left[s_{n}^{(1)}, s_{n}^{(2)}\\right)|| a(s)-(2 k+1) \\pi \\mid<\\pi / 4 \\text { for some } k \\in \\mathbb{N}\\right\\}, \\quad S_{2}=\\left[s_{n}^{(1)}, s_{n}^{(2)}\\right) \\backslash S_{1}\n$$\n\nThe integral on $S_{2}$ is studied in the same way as the decay stage:\n\n$$\n\\int_{S_{2}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s \\leq \\int_{\\mathcal{O}(n)}^{\\mathcal{O}\\left(n^{2}\\right)} \\mathcal{O}\\left(s^{-2-2 q}\\right) d s=\\mathcal{O}\\left(n^{-1-2 q}\\right)\n$$\n\nTo study the spikes, we first need to derive a simplified expression of the denominator. Fix an $s \\in S_{1}$. We let\n\n$$\n\\alpha(s)=\\min _{k}|a(s)-(2 k+1) \\pi|\n$$\n\nand\n\n$$\nd(s)=\\left|1+s i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s i)}{\\prod_{j=1}^{n}(j+s i)}\\right|\n$$\n\nSince\n\n$$\nr(s):=1-\\left|s i \\frac{(-1)^{n-1} \\prod_{j=1}^{n-1}(j-s i)}{\\prod_{j=1}^{n}(j+s i)}\\right|=1-\\frac{s}{\\sqrt{s^{2}+n^{2}}}\n$$\n\nby the cosine law, we have (see Figure 5)\n\n$$\n\\begin{aligned}\n& \\cos \\left(\\frac{\\pi}{2}-\\frac{\\alpha(s)}{2}\\right)=\\frac{-d^{2}+r(s)^{2}+4 \\sin ^{2}(\\alpha(s) / 2)}{4 r(s) \\sin (\\alpha(s) / 2)} \\\\\n& \\quad \\Rightarrow d(s)^{2}=r(s)^{2}+4 \\sin ^{2}\\left(\\frac{\\alpha(s)}{2}\\right)-4 r(s) \\sin ^{2}\\left(\\frac{\\alpha(s)}{2}\\right) \\geq r(s)^{2}+\\sin ^{2}\\left(\\frac{\\alpha(s)}{2}\\right)\n\\end{aligned}\n$$\n\nwhere the last inequality follows from the fact that $r(s)<1 / 2$ for a sufficiently large constant $c$ in the definition of $s_{n}^{(1)} \\cdot{ }^{3}$ Therefore, we have\n\n$$\n\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2}=\\mathcal{O}\\left(s^{-2-2 q}\\right) \\frac{1}{d(s)^{2}} \\leq \\mathcal{O}\\left(s^{-2-2 q}\\right) \\frac{1}{r(s)^{2}+\\alpha(s)^{2}}\n$$\n\nwhere we used the fact that $x / \\pi \\leq \\sin (x) \\leq x$ for all $0 \\leq x \\leq \\pi / 2$. Clearly, we have\n\n$$\nr(s)^{2}=\\left(\\frac{\\sqrt{s^{2}+n^{2}}-s}{\\sqrt{s^{2}+n^{2}}}\\right)^{2}=\\left(\\frac{s^{2}+n^{2}-s^{2}}{\\sqrt{s^{2}+n^{2}}\\left(\\sqrt{s^{2}+n^{2}}+s\\right)}\\right)^{2}=\\mathcal{O}\\left(\\frac{n^{4}}{s^{4}}\\right)\n$$\n\nbecause $s=\\Omega(n)$. To study $\\alpha(s)$, we first need to compute $a(s)$. To this end, note that since we assume $s=\\Omega(n)$, there exist two universal constants $C_{1}, C_{2}>0$, independent of $n$, such that\n\n$$\nC_{1} \\frac{j}{s} \\leq \\arctan \\left(\\frac{j}{s}\\right) \\leq C_{2} \\frac{j}{s}, \\quad 1 \\leq j \\leq n-1\n$$\n\nHence, we have\n\n$$\na(s)=\\Theta\\left(\\frac{n^{2}}{s}\\right)\n$$\n\nBy the intermediate value theorem and monotonicity of $a$, there are $k_{n}=\\mathcal{O}(n)$ frequencies $s_{1}, \\ldots, s_{k_{n}}$ between $s=s_{n}^{(1)}$ and $s=s_{n}^{(2)}$ such that $a\\left(s_{j}\\right) \\equiv \\pi(\\bmod 2 \\pi)$ for all $1 \\leq j \\leq k_{n}$. Each $s_{j}$ is contained in a connected component $S_{1}^{(j)}=\\left(\\xi_{j}, \\zeta_{j}\\right)$ of $S_{1}$ and $S_{1}=\\bigcup_{j=1}^{k_{n}} S_{1}^{(j)}$. That is, we have\n\n$$\ns_{n}^{(1)}<\\xi_{k_{n}}<s_{k_{n}}<\\zeta_{k_{n}}<\\xi_{k_{n}-1}<s_{k_{n}-1}<\\zeta_{k_{n}-1}<\\cdots<\\xi_{1}<s_{1}<\\zeta_{1}<s_{n}^{(2)}\n$$\n\nMoreover, there are two universal constants $C_{1}, C_{2}>0$, independent of $n$ or $j$, such that\n\n$$\nC_{1} j^{-1} n^{2} \\leq s_{j} \\leq C_{2} j^{-1} n^{2}\n$$\n\nCombined with eq. (30), we have\n\n$$\nr(s)^{2}=\\mathcal{O}\\left(\\frac{j^{4}}{n^{4}}\\right), \\quad s \\in S_{1}^{(j)}, \\quad 1 \\leq j \\leq k_{n}\n$$\n\nwhere the constant is universal and does not depend on $n$ or $j$. To integrate $\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2}$ on $S_{1}$, we integrate it on each of $\\left(\\xi_{j}, \\zeta_{j}\\right)$. To do so, we study the value of $\\alpha(s)$ using the Mean Value Theorem. First, we note that for any given $s_{j}$, we have\n\n$$\n\\begin{aligned}\n\\frac{d}{d s} a\\left(s_{j}\\right) & =-\\Theta(1) \\sum_{k=1}^{n-1} \\frac{1}{1+\\frac{k^{2}}{s_{j}^{2}}} \\frac{k}{s_{j}^{2}}=-\\Theta(1) \\frac{1}{s_{j}^{2}} \\sum_{k=1}^{n-1} \\frac{k}{\\frac{s_{j}^{2}+k^{2}}{s_{j}^{2}}} \\\\\n& =-\\Theta(1) \\sum_{k=1}^{n-1} \\frac{k}{s_{j}^{2}}=-\\Theta(1) \\frac{n^{2}}{s_{j}^{2}}=-\\Theta(1) \\frac{j^{2}}{n^{2}}\n\\end{aligned}\n$$\n\n[^3]where the constant in the $\\Theta$-notation does not depend on $n$ or $j$. Hence, fixing a $1 \\leq j \\leq k_{n}$ and choosing $s \\in\\left(\\xi_{j}, \\zeta_{j}\\right)$, by the Mean Value Theorem, we have\n$$\n\\alpha(s)=\\left|a(s)-a\\left(s_{j}\\right)\\right|=\\Theta(1) \\frac{j^{2}}{n^{2}}\\left|s-s_{j}\\right|\n$$\n\nThis shows $\\zeta_{j}-s_{j}, s_{j}-\\xi_{j}=\\Theta\\left(n^{2} / j^{2}\\right)$. Hence, we have\n\n$$\n\\begin{aligned}\n\\int_{\\xi_{j}}^{\\zeta_{j}} \\mid & \\left.H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s=\\mathcal{O}\\left(\\left(j^{-1} n^{2}\\right)^{-2-2 q}\\right) \\int_{\\xi_{j}}^{\\zeta_{j}} \\frac{1}{r(s)^{2}+\\alpha(s)^{2}} d s \\\\\n& \\leq \\mathcal{O}\\left(j^{2+2 q} n^{-4-4 q}\\right)\\left(\\int_{s_{j}-1}^{s_{j}+1} \\frac{1}{r(s)^{2}} d s+\\int_{\\xi_{j}}^{s_{j}-1} \\frac{1}{\\alpha(s)^{2}} d s+\\int_{s_{j}+1}^{\\zeta_{j}} \\frac{1}{\\alpha(s)^{2}} d s\\right) \\\\\n& \\leq \\mathcal{O}\\left(j^{2+2 q} n^{-4-4 q}\\right)\\left(\\frac{n^{4}}{j^{4}}+\\frac{n^{4}}{j^{4}} \\int_{1}^{\\Theta\\left(n^{2} / j^{2}\\right)} \\delta^{-2} d \\delta\\right)=\\mathcal{O}\\left(j^{-2+2 q} n^{-4 q}\\right)\n\\end{aligned}\n$$\n\nSuppose $q>1 / 2$ and let $q^{\\prime}=q-1 / 2$. Then, we have\n\n$$\n\\begin{aligned}\n\\int_{S_{1}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s & =\\sum_{j=1}^{k_{n}} \\int_{\\xi_{j}}^{\\zeta_{j}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s=\\mathcal{O}(1) \\sum_{j=1}^{k_{n}} j^{-1+2 q^{\\prime}} n^{-2-4 q^{\\prime}} \\\\\n& \\leq \\mathcal{O}\\left(n^{-2}\\right) \\sum_{j=1}^{k_{n}} j^{-1-2 q^{\\prime}}=\\mathcal{O}\\left(n^{-2}\\right)\n\\end{aligned}\n$$\n\nwhere the constant in the last $\\mathcal{O}$-notation only depends on $p$. Combining eq. (28) and (32), we have that when $q>1 / 2$, it holds that\n\n$$\n\\int_{s_{n}^{(1)}}^{s_{n}^{(2)}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s=\\mathcal{O}\\left(n^{-2}\\right)\n$$\n\nPut everything together: Combining eq. (26), (27), and (33) and applying Parseval's identity, we obtain\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{y}_{\\text {DPLR }}-\\mathbf{y}_{\\text {Diag }}\\right\\|_{L^{2}}=\\left\\|\\hat{\\mathbf{y}}_{\\text {DPLR }}-\\hat{\\mathbf{y}}_{\\text {Diag }}\\right\\|_{L^{2}}=\\left\\|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right\\|_{L^{2}} \\\\\n& \\quad=\\sqrt{\\int_{0}^{s_{n}^{(1)}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s+\\int_{s_{n}^{(1)}}^{s_{n}^{(2)}}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s+\\int_{s_{n}^{(2)}}^{\\infty}\\left|H_{n}(s i) \\hat{\\mathbf{u}}(s)\\right|^{2} d s}=\\mathcal{O}\\left(n^{-1}\\right)\n\\end{aligned}\n$$\n\nThis completes the proof. ## F Numerical experiments on Theorem 1 and 2\n\nIn this section, we present three numerical experiments that elaborate our theory in section 3. The first experiment examines the behaviors of the DPLR system and the diagonal system given a single Fourier mode as an input. By doing so, we observe the \"numerical unstable modes\" of the S4D model. This corroborates Theorem 2. Then, we compare the two systems using two different input functions: an exponentially decaying function and the unit impulse. We will show that the smoothness condition in Theorem 1 is necessary and the linear convergence rate is tight. In each of these experiments, we simulate LTI systems on some continuous input signals. It is done as follows: given an input signal $u(t)^{4}$ and an LTI system $\\Sigma$, we fix the step size to be\n\n[^4]![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-26.jpg?height=686&width=544&top_left_y=254&top_left_x=699)\n\nFigure 5: Illustration of the proof of Theorem 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-26.jpg?height=453&width=1531&top_left_y=1034&top_left_x=235)\n\nFigure 6: Simulated outputs of the DPLR and diagonal systems with cosine wave inputs of different frequencies $s$.",
    "approxdiag-27": "Note the scale of the $y$-axis when $s=322.5$. $\\Delta t=10^{-3}$. For some final time step $N$, we discretize our input function to obtain a vector $\\mathbf{u}=(u(0), u(\\Delta t), \\ldots, u(N \\Delta t))$. We then discretize the LTI system bilinearly (see Appendix B) and compute its output $\\mathbf{y}$ on the input $\\mathbf{u}$. We call this procedure \"simulate\", i.e.,\n\n$$\n\\mathbf{y}=\\operatorname{simulate}(u, \\Sigma, N)\n$$\n\nIn this section, we let $\\Sigma_{\\text {DPLR, } n}$ to be the DPLR system with state size $n$ of S4 and $\\Sigma_{\\text {Diag, } n}$ to be the diagonal system with state size $n$ of $S 4 D$, where we always take $\\mathbf{C}=\\mathbf{e}_{1}$ and $\\mathbf{D}=\\mathbf{0}$. ## F. 1 The diagonal system behaves differently for distinct Fourier modes\n\nOur first experiment considers the outputs of $\\Sigma_{\\mathrm{DPLR}, n}$ and $\\Sigma_{\\text {Diag, } n}$ when the input is a cosine wave\n\n$$\nu_{s}(t)=\\cos (s t)\n$$\n\nThis function has a dense Fourier mode at frequency $s$. We fix $n=32$ and let $s$ change. In Figure 6 , we plot simulate $\\left(u_{s}, \\Sigma_{\\text {DPLR }, 32}, 10^{3}\\right)$ and simulate $\\left(u_{s}, \\Sigma_{\\text {Diag, } 32}, 10^{3}\\right)$ with $s=200,322.5$, and 500 , respectively. We see that when $s=200$ or 500 , the outputs of the two systems are close to each other - at least, they are on the same order of magnitude. However, when $s=322.5$, the output of the diagonal system blows up. In fact, this value of $s$ is exactly where the spike in the plot of $\\left\\|G_{\\text {Diag }}\\right\\|$ occurs when $n=32$.",
    "approxdiag-28": "Hence, we visualize the counter-example that shows the divergence in Theorem 2. ## F. 2 The DPLR and diagonal systems converge on the exponentially decaying function\n\nTo test the function-wise convergence of the diagonal system to the DPLR system (see Theorem 1 ), we consider the following exponentially decaying function:\n\n$$\nu_{e}(t)=e^{-t} H(t)\n$$\n\nwhere $H=\\mathbb{1}_{[0, \\infty)}$ is the Heaviside function. The Fourier transform of this function is\n\n$$\n\\hat{u}_{e}(s)=\\frac{1}{1+i s}\n$$\n\nHence, it is a function that satisfies the assumptions of Theorem 1. In the left panel of Figure 7 , we show the difference between the two simulated outputs $\\| \\operatorname{simulate}\\left(u_{e}, \\Sigma_{\\text {DPLR, }}, 10^{4}\\right)-$ simulate $\\left(u_{e}, \\Sigma_{\\text {Diag }, n}, 10^{4}\\right) \\|$ as $n$ increases. We see that as $n$ increases, simulate $\\left(u_{e}, \\Sigma_{\\text {Diag }, n}\\right.$ converges to simulate $\\left(u_{e}, \\Sigma_{\\mathrm{DPLR}, n}, 10^{4}\\right)$. Moreover, the slope of the curve is roughly -1 , indicating a linearly convergence rate as $n \\rightarrow \\infty$. This matches the theoretical statement in Theorem 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-27.jpg?height=669&width=1384&top_left_y=1032&top_left_x=254)\n\nFigure 7: The difference between the outputs $\\|$ simulate $\\left(u, \\Sigma_{\\text {DPLR }, n}, 10^{4}\\right)-$ simulate $\\left(u, \\Sigma_{\\text {Diag }, n}, 10^{4}\\right) \\|$ for difference values of $n$ when $u$ is the exponentially decaying function $u_{e}$ (left) and the unit impulse signal $\\delta_{0}$ (right). In Figure 8, we show the behaviors of the two simulated outputs as $n$ increases. For the exponentially decaying input function $u_{e}$, the outputs demonstrate a clear pattern of convergence as $n \\rightarrow \\infty$. ![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-27.jpg?height=461&width=1564&top_left_y=2038&top_left_x=216)\n\nFigure 8: Simulated outputs of the DPLR and diagonal systems with the exponentially decaying input function and varying state-space dimension $n$. ![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-28.jpg?height=449&width=1543&top_left_y=208&top_left_x=223)\n\nFigure 9: Simulated outputs of the DPLR and diagonal systems with the unit impulse input and varying state-space dimension $n$. ## F. 3 The DPLR and diagonal systems diverge on the unit impulse\n\nOur experiment with the exponentially decaying input shows that the DPLR and diagonal systems converge on a sufficiently smooth input function. One may wonder, however, if the smoothness condition is necessary. To show that a mild one is indeed required, we consider the Dirac delta function $\\delta_{0}$. It is well-known that the Fourier transform of it is constantly one:\n\n$$\n\\hat{\\delta}_{0}(s)=1, \\quad s \\in \\mathbb{R}\n$$\n\nIn that sense, $\\delta_{0}$ is highly non-smooth as its Fourier transform does not decay at all. Since $\\delta_{0}$ is a distribution rather than a classical function, we cannot sample it directly. However, we can mimic it by setting the discrete input to be the unit impulse $(1,0,0, \\ldots, 0)$. In the right panel of Figure 7 , we see that $\\|$ simulate $\\left(\\delta_{0}, \\Sigma_{\\text {DPLR }, n}, 10^{4}\\right)-\\operatorname{simulate}\\left(\\delta_{0}, \\Sigma_{\\text {Diag }, n}, 10^{4}\\right) \\|$ does not decay as $n$ increases. We can take a closer look in Figure 9, where we plot the two output functions with different state-space dimensions $n$. In particular, we see that as $n$ increases, the output of the DPLR system remains the same, whereas the output of the diagonal system becomes more oscillatory. We do not have convergence. The oscillatory behavior can be explained by our observation in Figure 1: the larger the $n$, the later the spike emerges. This means that for a larger $n$, the outputs of two systems differ at a higher frequency (i.e., a more oscillatory mode). ## G Quantitative interpolation and extrapolation errors in section 3.4\n\nIn section 3.4, we show a task of predicting the amplitude of a sinusoidal wave\n\n$$\n\\mathbf{u}(t)=A \\sin (s t)\n$$\n\nWe define four domains by\n\n$$\n\\begin{aligned}\n& S_{\\text {interp }}=[0,40] \\cup[60,100], \\quad S_{\\text {extrap }}=[0,80] \\\\\n& U_{\\text {interp }}=[0,100] \\backslash S_{\\text {interp }}=(40,60), \\quad U_{\\text {extrap }}=[0,100] \\backslash S_{\\text {extrap }}=(80,100]\n\\end{aligned}\n$$\n\nWe are given training samples only with $s \\in S_{\\text {interp }}$ (resp. $s \\in S_{\\text {extrap }}$ ) and we test the sequential model on the entire domain $[0,100]$. Given a test set, we measure the mean-squared error of our model. We uniformly sample the test set from $s \\in S_{\\text {interp }}$ (resp. $s \\in S_{\\text {extrap }}$ ) and $s \\in U_{\\text {interp }}$ (resp. $s \\in U_{\\text {extrap }}$ ) to evaluate our models' performance on generalization to unseen data in the seen domain, and their performance on interpolation (resp. extrapolation). The results are shown in Figure 10. We see that the S4D model performs even better on the seen domain (i.e., $S_{\\text {interp }}$ or $S_{\\text {extrap }}$ ), but its interpolation and extrapolation capabilities are much worse than those of the S4 and our S4-PTD models. Interpolation Errors\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-29.jpg?height=504&width=649&top_left_y=265&top_left_x=264)\n\nExtrapolation Errors\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-29.jpg?height=501&width=649&top_left_y=269&top_left_x=975)\n\nFigure 10: The interpolation and extrapolation errors of predicting the amplitude of a sinusoidal signal (see section 3.4) made by the S4, S4D, and S4-PTD models.",
    "approxdiag-29": "Each curve shows the mean-squared test error of one model on either the seen domain, $S_{\\mathrm{i}}$ or $S_{\\mathrm{e}}$, or the unseen domain, $U_{\\mathrm{i}}$ or $U_{\\mathrm{e}}$. The yellow curve for the S 4 -PTD model and the red curve for the S 4 model almost overlap in the extrapolation problem. ## H Proof of results in section 4\n\nIn this section, we present the proof of Theorem 3. In addition, we introduce a probabilistic statement of the eigenvector condition number of a matrix perturbed by a random Gaussian matrix. The proof of Theorem 3 is a classical forward error analysis, but to maintain the best result, we need to explicitly compute the resolvent of $\\mathbf{A}_{H}$. Proof of Theorem 3. For notational cleanliness, in this proof, we define $\\mathbf{A}=\\mathbf{A}_{H}, \\mathbf{B}=\\mathbf{B}_{H}$, and $\\mathbf{C}=\\mathbf{C}_{\\text {DPLR }} \\mathbf{V}_{H}^{-1}$. We have\n\n$$\n\\begin{aligned}\n\\left|G_{\\text {Pert }}(s)-G_{\\operatorname{DPLR}}(s)\\right| & =\\left|\\mathbf{B}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{C}-\\mathbf{B}(s \\mathbf{I}-\\mathbf{A}-\\mathbf{E})^{-1} \\mathbf{C}\\right| \\\\\n& =\\left|\\mathbf{B}\\left((s \\mathbf{I}-\\mathbf{A})^{-1}-(s \\mathbf{I}-\\mathbf{A}-\\mathbf{E})^{-1}\\right) \\mathbf{C}\\right| \\\\\n& \\leq\\|\\mathbf{B}\\|_{2}\\left\\|(s \\mathbf{I}-\\mathbf{A})^{-1}-(s \\mathbf{I}-\\mathbf{A}-\\mathbf{E})^{-1}\\right\\|_{2}\\|\\mathbf{C}\\|_{2}\n\\end{aligned}\n$$\n\nwhere, by a result in [12], we have\n\n$$\n\\left\\|(s \\mathbf{I}-\\mathbf{A})^{-1}-(s \\mathbf{I}-\\mathbf{A}-\\mathbf{E})^{-1}\\right\\|_{2} \\leq\\|\\mathbf{E}\\|_{2}\\left\\|(s \\mathbf{I}-\\mathbf{A})^{-1}\\right\\|_{2}^{2}+\\mathcal{O}\\left(\\|\\mathbf{E}\\|_{2}^{2}\\right)\\left\\|(s \\mathbf{I}-\\mathbf{A})^{-1}\\right\\|_{2}\n$$\n\nWe set\n\n$$\n\\left[\\begin{array}{ccccc}\nc_{1,1} & 0 & 0 & \\cdots & 0 \\\\\nc_{2,1} & c_{2,2} & 0 & \\cdots & 0 \\\\\nc_{3,1} & c_{3,2} & c_{3,3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{n, 1} & c_{n, 2} & c_{n, 3} & \\cdots & c_{n, n}\n\\end{array}\\right]=(-s \\mathbf{I}+\\mathbf{A})^{-1}=\\left[\\begin{array}{ccccc}\n1-s & 0 & 0 & \\cdots & 0 \\\\\n\\sqrt{3} & 2-s & 0 & \\cdots & 0 \\\\\n\\sqrt{5} & \\sqrt{15} & 3-s & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sqrt{2 n-1} & \\sqrt{3(2 n-1)} & \\sqrt{5(2 n-1)} & \\cdots & n-s\n\\end{array}\\right]^{-1}\n$$\n\nThen, fixing a column $i$ and a row $j \\geq i$, we have\n\n$$\n\\begin{cases}\\sum_{k=i}^{j-1} c_{k, i} \\sqrt{2 j-1} \\sqrt{2 k-1}+c_{j, i}(j-s) & =0 \\\\ \\sum_{k=i}^{j} c_{k, i} \\sqrt{2 j+1} \\sqrt{2 k-1}+c_{j+1, i}(j+1-s) & =0\\end{cases}\n$$\n\nMultiplying eq.",
    "approxdiag-30": "(35) by $\\sqrt{2 j+1} / \\sqrt{2 j-1}$, we have\n\n$$\n\\sum_{k=i}^{j-1} c_{k, i} \\sqrt{2 j+1} \\sqrt{2 k-1}+\\frac{\\sqrt{2 j+1}}{\\sqrt{2 j-1}} c_{j, i}(j-s)=0\n$$\n\nSubtracting eq. (37) from eq. (35), we have\n\n$$\nc_{j, i} \\sqrt{2 j+1} \\sqrt{2 j-1}-c_{j, i} \\frac{\\sqrt{2 j+1}}{\\sqrt{2 j-1}}(j-s)+c_{j+1, i}(j+1-s)=0\n$$\n\nAfter simplifying, we get the recurrence relation\n\n$$\n\\begin{aligned}\n& c_{i, i}=\\frac{1}{i-s}, \\quad c_{i+1, i}=-\\frac{\\sqrt{2 i-1} \\sqrt{2 i+1}}{(i-s)(i+1-s)} \\\\\n& c_{j+1, i}=-\\frac{(j+s-1) \\sqrt{2 j+1}}{(j-s+1) \\sqrt{2 j-1}} c_{j, i}, \\quad j \\geq i+1\n\\end{aligned}\n$$\n\nSolving this recurrence relation gives us\n\n$$\nc_{k, i}=(-1)^{k-i} \\frac{\\sqrt{2 i-1} \\sqrt{2 i+1}}{(i-s)(i+1-s)} \\frac{\\sqrt{2 k-1}}{\\sqrt{2 i+1}} \\frac{\\prod_{\\ell=i}^{k-2}(\\ell+s)}{\\prod_{\\ell=i+2}^{k}(\\ell-s)}, \\quad k \\geq i+1\n$$\n\nSince $s$ is purely imaginary, we have\n\n$$\n\\left|\\frac{\\ell+s}{\\ell-s}\\right|=1\n$$\n\nTherefore, we can control the size of $c_{k, i}$ by $^{5}$\n\n$$\n\\left|c_{k, i}\\right|=\\frac{\\sqrt{2 i-1} \\sqrt{2 k-1}}{|i-s||i+1-s|} \\frac{|i+s||i+1+s|}{|k-1-s||k-s|}=\\frac{\\sqrt{2 i-1} \\sqrt{2 k-1}}{|k-1-s||k-s|}, \\quad k \\geq i+2\n$$\n\nClearly, this value is maximized when $s=0$. Hence, we have\n\n$$\n\\left|c_{k, i}\\right|^{2} \\leq \\frac{(2 i-1)(2 k-1)}{(k-1)^{2} k^{2}} \\leq \\frac{4 i}{(k-1)^{2} k}\n$$\n\nNote that this inequality holds also for the case when $k=i+1$. Now, we have\n\n$$\n\\begin{aligned}\n\\left\\|(s \\mathbf{I}-\\mathbf{A})^{-1}\\right\\|_{2}^{2} & \\leq\\left\\|(s \\mathbf{I}-\\mathbf{A})^{-1}\\right\\|_{F}^{2} \\leq \\sum_{k=2}^{n} \\sum_{i=1}^{k-1} \\frac{4 i}{(k-1)^{2} k}+\\sum_{i=1}^{n} \\frac{1}{i^{2}} \\\\\n& \\leq \\sum_{k=2}^{n} \\frac{2(k-1) k}{(k-1)^{2} k}+2 \\leq 2 \\ln (n)+4\n\\end{aligned}\n$$\n\nThe result follows from eq.",
    "approxdiag-31": "(34). Next, we prove Theorem 4. The proof is heavily based on [6, Thm. 1.5].",
    "approxdiag-32": "Proof of Theorem 4. By [6, Thm. 1.5], we have that\n\n$$\n\\mathbb{E}\\left[\\sum_{j=1}^{n} \\kappa\\left(\\lambda_{i}\\right)^{2} \\mathbb{1}_{\\left\\{\\lambda_{i} \\in D_{R}(0)\\right\\}}\\right] \\leq\\|\\mathbf{A}\\|^{2} \\frac{R^{2} n^{2}}{\\epsilon^{2}}\n$$\n\n[^5]| $\\gamma$ | 10 | $10^{2}$ | $10^{3}$ | $10^{4}$ | $10^{5}$ | $10^{6}$ | $10^{7}$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 8 | 4.40 e 0 | 8.62 e 0 | 1.73 e 1 | 3.51 e 1 | 7.12 e 1 | 1.45 e 2 | 2.96 e 2 |\n| 16 | 6.59 e 0 | 1.32 e 1 | 2.69 e 1 | 5.53 e 1 | 1.14 e 2 | 2.35 e 2 | 4.86 e 2 |\n| 32 | 9.98 e 0 | 2.02 e 1 | 4.16 e 1 | 8.63 e 1 | 1.79 e 2 | 3.72 e 2 | 7.75 e 2 |\n| 64 | 1.52 e 1 | 3.12 e 1 | 6.45 e 1 | 1.34 e 2 | 2.80 e 2 | 5.84 e 2 | 1.22 e 3 |\n| 128 | 2.34 e 1 | 4.82 e 1 | 1.00 e 2 | 2.09 e 2 | 4.37 e 2 | 9.14 e 2 | 1.91 e 3 |\n\nTable 2: The eigenvector condition number $\\kappa_{\\text {eig }}\\left(\\tilde{\\mathbf{A}}_{H}\\right)$ when the optimization problem eq. (12) is solved with different values of $n$ and $\\gamma$. where $\\lambda_{1}, \\ldots, \\lambda_{n}$ are eigenvalues of $\\mathbf{A}+\\epsilon \\mathbf{G}_{n}$ and $\\kappa\\left(\\lambda_{i}\\right)$ is defined in [6]. When $\\lambda_{j} \\in D_{R}(0)$ for all $1 \\leq j \\leq n$, we have\n\n$$\n\\kappa_{\\mathrm{eig}}\\left(\\mathbf{A}+\\epsilon \\mathbf{G}_{n}\\right)^{2} \\leq n \\sum_{j=1}^{n} \\kappa\\left(\\lambda_{i}\\right)^{2}\n$$\n\nHence, this shows\n\n$$\n\\frac{1}{n} \\mathbb{E}\\left[\\kappa_{\\text {eig }}\\left(\\mathbf{A}+\\epsilon \\mathbf{G}_{n}\\right)^{2} \\mid \\Omega\\right] \\mathbb{P}(\\Omega)+\\mathbb{E}\\left[\\sum_{j=1}^{n} \\kappa\\left(\\lambda_{i}\\right)^{2} \\mathbb{1}_{\\left\\{\\lambda_{i} \\in D_{R}(0)\\right\\}} \\mid \\Omega^{C}\\right] \\mathbb{P}\\left(\\Omega^{C}\\right) \\leq\\|\\mathbf{A}\\|^{2} \\frac{R^{2} n^{2}}{\\epsilon^{2}}\n$$\n\nWe are done. Comparing Theorem 4 to Theorem 5, we note that the bound in Theorem 5 is slightly better than that in Theorem 4. However, the Gaussian perturbation in Theorem 4 is problemindependent and can be generically implemented, whereas it is not necessarily easy to identify the perturbation in Theorem 5 . ## I Numerical experiments on Theorem 5\n\nThe performance of our perturbed model is heavily based on two things: the perturbation size $\\|\\mathbf{E}\\|$ and the condition number of $\\tilde{\\mathbf{V}}_{H}$. The former value controls the difference between our initialization to the known-to-be-good HiPPO initialization, whereas the latter one controls the unfairness when transforming the states via $\\tilde{\\mathbf{V}}_{H}$. By Theorem 5, the condition number $\\kappa\\left(\\tilde{\\mathbf{V}}_{H}\\right)$ should depend linearly on $\\|E\\|^{-1}$ and depend sub-quadratically on $n$, the state space size. In this section, we present a numerical experiment that investigates the relationship between these three values. To do so, we solve the optimization problem in eq. (12) with different state space dimensions $n$ and values of $\\gamma$. We then record the size of the perturbation and the eigenvector condition number of the perturbed matrix. In Figure 11, we see that the eigenvector condition number $\\kappa_{\\mathrm{eig}}\\left(\\tilde{\\mathbf{A}}_{H}\\right)$ depends polynomially on both the state space dimension $n$ and the relative perturbation size $\\|\\mathbf{E}\\| /\\left\\|\\mathbf{A}_{H}\\right\\|$. Numerical values are reported in Table 2 and 3. Using the data, one can compute that we have $\\kappa_{\\mathrm{eig}}\\left(\\tilde{\\mathbf{A}}_{H}\\right)=\\mathcal{O}\\left(\\left(\\|\\mathbf{E}\\| /\\left\\|\\mathbf{A}_{H}\\right\\|\\right)^{-p}\\right)$, where $p \\approx 0.87$. Hence, we are doing slightly better than the theory of Theorem 5 . Another surprising observation that can be made with a little bit computation is that if we normalize $\\mathbf{A}_{H}$ to have a spectral norm of 1 , then the eigenvector condition number $\\kappa_{\\text {eig }}\\left(\\tilde{\\mathbf{A}}_{H}\\right)$ does not depend on $n$ at all. This is much better than the bound proposed in Theorem 5 . ## J Details of experiments in section 5\n\nIn this section, we provide the details of the experiments presented in section 5 . ![](https://cdn.mathpix.com/cropped/2024_09_12_87430b83c8bfef5d5a73g-32.jpg?height=604&width=575&top_left_y=286&top_left_x=752)\n\nFigure 11: The relationship among the state space dimension $n$, the relative perturbation size $\\|\\mathbf{E}\\| /\\left\\|\\mathbf{A}_{H}\\right\\|$, and the eigenvector condition number $\\kappa_{\\text {eig }}\\left(\\tilde{\\mathbf{A}}_{H}\\right)$\n\n| $\\gamma$ | 10 | $10^{2}$ | $10^{3}$ | $10^{4}$ | $10^{5}$ | $10^{6}$ | $10^{7}$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 8 | 2.81 e 0 | 1.16 e 0 | $4.78 \\mathrm{e}-1$ | $1.98 \\mathrm{e}-1$ | $8.24 \\mathrm{e}-2$ | $3.45 \\mathrm{e}-2$ | $1.45 \\mathrm{e}-2$ |\n| 16 | 6.77 e 0 | 2.86 e 0 | 1.22 e 0 | $5.18 \\mathrm{e}-1$ | $2.22 \\mathrm{e}-1$ | $9.50 \\mathrm{e}-2$ | $4.09 \\mathrm{e}-2$ |\n| 32 | 1.62 e 1 | 6.96 e 0 | 3.00 e 0 | 1.30 e 0 | $5.62 \\mathrm{e}-1$ | $2.45 \\mathrm{e}-1$ | $1.07 \\mathrm{e}-1$ |\n| 64 | 3.89 e 1 | 1.68 e 1 | 7.32 e 0 | 3.19 e 0 | 1.39 e 0 | $6.11 \\mathrm{e}-1$ | $2.69 \\mathrm{e}-1$ |\n| 128 | 9.37 e 1 | 4.07 e 1 | 1.78 e 1 | 7.80 e 0 | 3.42 e 0 | 1.51 e 0 | $6.65 \\mathrm{e}-1$ |\n\nTable 3: The perturbation size $\\|\\mathbf{E}\\|$ when the optimization problem eq.",
    "approxdiag-33": "(12) is solved with different values of $n$ and $\\gamma$. ## J. 1 Details of the evaluation of our model in the Long Range Arena\n\nTo compare our perturbed models with the diagonal S4D and S5 models, we adopt the same model parameters used in $[16,32]$ but possibly change the training parameters, such as the learning rate, number of epochs, batch size, and weight decay rate. For choosing the perturbation matrix, we again solve the optimization problem in eq. (12). Instead of allowing $\\gamma$ to be an unbounded positive tuning parameter, we require that $\\gamma$ is large enough so that $\\|\\mathbf{E}\\| /\\left\\|\\mathbf{A}_{H}\\right\\| \\leq 0.1$. This improves the worst-case robustness of our model (see section 5.2). We provide the detailed configuration of our S4-PTD model in Table 4 and that of our S5-PTD model in Table 5. In particular, we note that the first two columns of Table 4 are almost the same as those in $[16]^{6}$ and the first four columns of Table 5 match those in [32] - these are model parameters. The only remaining non-trivial thing is that in the Path-X task, we start with a batch size of 32 . We half the batch size after epoch 30 and epoch 60 . By making the batch size smaller, we improve the generalization power of our model. ## J. 2 Details of the robustness test of the diagonal model and our model\n\nIn the robustness test presented in section 5.2 , we train both an S4D model and an S4-PTD model. Our models have 4 layers, 128 channels, and each layer contains an SSM with $n=$ 32 states. The perturbation matrix in the S4-PTD model is computed by setting $\\gamma=0.03$ in eq. (12). From Figure 3c, it can be seen that the perturbation thence computed has a\n\n[^6]| Task | Depth | \\#Features | Norm | Prenorm | DO | LR | BS | Epochs | WD | $\\Delta$ Range |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 8 | 256 | BN | False | 0. | 0.002 | 50 | 80 | 0.05 | $(1 \\mathrm{e}-3,1 \\mathrm{e} 0)$ |\n| Text | 6 | 256 | BN | True | 0. | 0.01 | 16 | 80 | 0.05 | $(1 \\mathrm{e}-3,1 \\mathrm{e}-1)$ |\n| Retrieval | 6 | 128 | BN | True | 0. | 0.004 | 64 | 40 | 0.03 | $(1 \\mathrm{e}-3,1 \\mathrm{e}-1)$ |\n| Image | 6 | 128 | LN | False | 0.1 | 0.01 | 128 | 2000 | 0.01 | $(1 \\mathrm{e}-3,1 \\mathrm{e}-1)$ |\n| Pathfinder | 6 | 512 | BN | True | 0. | 0.004 | 64 | 300 | 0.03 | $(1 \\mathrm{e}-2,1 \\mathrm{e} 0)$ |\n| Path-X | 6 | 128 | BN | True | 0. | 0.001 | 20 | 100 | 0.03 | $(1 \\mathrm{e}-4,1 \\mathrm{e}-1)$ |\n\nTable 4: Configurations of the S4-PTD model, where DO, LR, BS, and WD stand for dropout rate, learning rate, batch size, and weight decay, respectively. | Task | Depth | H | P | J | DO | LR | SSM LR | BS | Epochs | WD |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 8 | 128 | 16 | 8 | 0. | 0.003 | 0.001 | 50 | 35 | 0.05 |\n| Text | 6 | 256 | 192 | 12 | 0.1 | 0.004 | 0.001 | 50 | 40 | 0.07 |\n| Retrieval | 6 | 128 | 256 | 16 | 0. | 0.002 | 0.001 | 32 | 20 | 0.05 |\n| Image | 6 | 512 | 384 | 3 | 0.1 | 0.0055 | 0.001 | 50 | 250 | 0.07 |\n| Pathfinder | 6 | 192 | 256 | 8 | 0.05 | 0.0045 | 0.0009 | 64 | 230 | 0.05 |\n| Path-X | 6 | 128 | 256 | 16 | 0. | 0.0018 | 0.0006 | 32 | 90 | 0.06 |\n\nTable 5: Configurations of the S5-PTD model.",
    "approxdiag-34": "See [32] for the meaning of the parameter labels. magnitude of roughly $10 \\%$ of the magnitude of $\\mathbf{A}_{H}$. We leave the training dataset and the validation dataset unchanged, but we add $10 \\%$ of noises in the form of $\\cos (325.4 t)$ to the test dataset. The frequency 325.4 is chosen at one of the sensitivity regions of the diagonal SSM when $n=32$. We train both models for 50 epochs and report the evolution of the training accuracy, the test accuracy on uncontaminated data, and that on noisy data. ## J. 3 Details of the ablation study of our model\n\nIn section 5.3, we train models with different perturbation sizes to solve the SCIFAR task [24]. Our models have the same architecture as those in the sensitivity test (see section 5.2). We set the batch size to be 64 and the learning rate to be 0.001 for SSM parameters and 0.01 for other model parameters. These are common setups that are adapted from the original S4 and S4D papers. We use the parameter $\\gamma$ in eq. (12) to control the size of the perturbation $\\|\\mathbf{E}\\|$. We set $\\gamma=10^{-4}, 10^{-3}, \\ldots, 10^{9}$ and train the S4-PTD model for 200 epochs to learn a classifier. These correspond to the first 14 points in Figure 3c, where we report both the test accuracy of the model and the eigenvector condition number at initialization. Since setting $\\gamma$ small does not help reducing $\\kappa_{\\mathrm{eig}}\\left(\\tilde{\\mathbf{A}}_{H}\\right)$ all the way down to 1 , the smallest condition number possible, to obtain the rightmost point, we perturb $\\mathbf{A}_{H}$ by a random symmetric matrix with a large norm. [^0]:    *Corresponding author: ay262@cornell.edu. [^1]:    ${ }^{1}$ Of course, this function is not defined at singular matrices, but since they form a Lebesgue null set, $f$ is still defined almost everywhere. [^2]:    ${ }^{2}$ We say $f(n)=\\Omega(g(n))$ if there exists a constant $C>0$ such that $f(n) \\geq C g(n)$ for all $n \\in \\mathbb{N}$. [^3]:    ${ }^{3}$ This is our only requirement of the universal constant $c$ appearing in the definition of $s_{n}^{(1)}$. [^4]:    ${ }^{4}$ Since $u$ will be scalar-valued in this section, we do not make it boldface. [^5]:    ${ }^{5}$ With a slight abuse of notation, the letter $i$ here stands for a real-valued index instead of the imaginary unit. [^6]:    ${ }^{6}$ The only exception is that in the Path-X task, we half the number of features in order to reduce the computation time.",
    "approxdiag-35": "This only simplifies our perturbed model.",
    "approxdiag-36": ""
}