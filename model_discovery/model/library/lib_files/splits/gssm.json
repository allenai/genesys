{
    "gssm-0": "# Long Range Language Modeling via Gated State Spaces \n\nHarsh Mehta ${ }^{1 *} \\quad$ Ankit Gupta $^{2} \\quad$ Ashok Cutkosky ${ }^{3} \\quad$ Behnam Neyshabur ${ }^{1}$\n\n\n#### Abstract\n\nState space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further. ## 1 Introduction\n\nModeling long range dependencies on sequential data is a crucial step towards closing the gap with human-level performance on many tasks. Attention based models like Transformer [Vaswani et al., 2017] have proven to be a strong choice of backbone architecture for a considerable number of tasks across modalities and scale [Devlin et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2021]. Vanilla Multi-Head-Attention famously incurs $\\Omega\\left(L^{2}\\right)$ penalty in modeling a sequence of length $L$. This is prohibitive at best for tasks where the model is required to capture long range dependencies from various parts of the input. Over the years, a variety of improvements have been proposed to alleviate this quadratic complexity (cf. [Tay et al., 2020]). On a somewhat orthogonal direction, attention-free models based on state spaces, such as $\\mathrm{S} 4[\\mathrm{Gu}$ et al., 2022a] and DSS [Gupta et al., 2022], have shown remarkable improvements on Long Range Arena (LRA) [Tay et al., 2021], a benchmark designed with long range modeling as its focus and consists of diverse tasks with $1 \\mathrm{k}-16 \\mathrm{k}$ sequence length across modalities. These models require careful initialization, originally borrowing ideas from the theory of HiPPO matrices [Voelker et al., 2019, Gu et al., 2020], to achieve good results on LRA. In this work, we explore and extend the use of state space models by focusing solely on the task of autoregressive sequence modeling [Brown et al., 2020, Rae et al., 2021, Chowdhery et al., 2022, Zhang et al., 2022, Hoffmann et al., 2022, Srivastava et al., 2022]. Several key properties endowed by the state space model family makes it particularly attractive, to at least fully explore it, in the context of language modeling. First, it reduces the $\\Omega\\left(L^{2}\\right)$ complexity on input sequence length to $O(L \\log L)$. This complexity results from the use of Fast Fourier Transform (FFT) [Cooley and Tukey, 1965] for performing convolutions. We will describe this in detail in later sections. Second, the state space model is fully parallelizable in the length dimension. This is an arguably subtle but an important property at training time. Note that transformers are also fully parallelizable, a worthy advantage over traditional RNNs for modeling sequences, which otherwise incurs only an $O(L)$ penalty. While this parallelism is useful at training time, it may also be a curse at inference time\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_5ab9ed67021bd72a4442g-02.jpg?height=470&width=563&top_left_y=242&top_left_x=369)\n\n```\ndef gss(x, F=4096, L=4096, \\(E=1024, \\mathrm{H}=256\\) ):\n    shortcut, \\(\\mathrm{x}=\\mathrm{x}, \\operatorname{norm}(\\mathrm{x})\\)\n    \\(\\mathrm{v}=\\) dense(x, F, activation='gelu')\n    \\(\\mathrm{u}=\\) dense(x, H, activation='gelu')\n    \\(\\mathrm{y}=\\operatorname{dss}(u, \\mathrm{H}, \\mathrm{L})\\)\n    \\# yh1,..,yhL are linear in uh1,..,uhL\n    \\(\\mathrm{uc}=\\) dense(y, F)\n    \\(0=\\) dense(uc \\(*\\) v, E)\n    return o + shortcut\n```\n\nFigure 1: (a) Our proposed Gated State Space (GSS) layer, (b) Pseudocode for GSS (full implementation in \u00a7A.2).",
    "gssm-1": "where decoding every token requires attending to the whole past. The ideal model is parallelizable at training time but incurs a small constant cost (per decoded token) at inference time. This brings us to the final point. Due to the inherent convolution-recurrence equivalence of the state space model, it can be made to accumulate state and unroll like an RNN at inference time without any approximations. Despite these attractive properties, we found that current state space models (such as S4, DSS) run slower than we expected at training time on TPUs, our accelerator of choice. We take this opportunity to modify the architecture to reduce dimensionality of specific operations which we found to be bottlenecks. Our proposed changes borrow from a well-supported empirical observation around the effectiveness of gating units [Shazeer, 2020]. Specifically, Hua et al. [2022] observed that replacing the typical Feed-Forward layer in the Transformer with gating units allows for a reduced dimensionality when mixing tokens along the length dimension using self-attention. We extend the use of gating units to state space model family and observe that, even in our context, the use of gating units allows for a reduction in dimensionality when performing FFT operations, which we observed to be the main bottleneck behind slow training. Furthermore, somewhat contrary to observations made by S4 and DSS authors, we found the performance of the model on language modeling tasks to be much less sensitive to initialization. We found that only the scale and structural aspects of initialization of state space variables were important and not the exact values. We were able to successfully train the model while initializing the state space variables randomly. This departs significantly, at least in understanding, from the reliance of the design on the theory of HiPPO matrices, which led the S 4 model to employ several numerical linear algebra tricks to able to make it work. Combining both of these contributions, we propose a layer named Gated State Space (GSS) (Figure 1), which we empirically verified to be $2-3 \\times$ faster than DSS while keeping the perplexity on several language modeling benchmarks (Table 1). Going one step further, we also perform an apples-to-apples comparison with well-tuned and performant baselines reported in Block Recurrent Transformers [Hutchins et al., 2022], on several long range language modeling benchmarks over modalities such as English books, raw source code from Github and LaTeX source of ArXiv mathematics articles. As detailed in Table 2, while our GSS model currently lags behind on some tasks when compared in the fixed-parameter setting, it is fairly competitive in the fixed-compute setting where we measure compute as the exact amount of TPUv4 hours spent on a training run and serves as a fairly accurate proxy to the realistic cost of training that model. Furthermore, we also experimented with a hybrid model in which we sparingly interleave Transformer layers (having local attention) in a GSS stack to allow for a richer modeling of short range interactions. To our delight, this further improves performance at (roughly) no extra training cost, both in terms of parameters and compute. While in our experiments we train on sequences of length at most 4 k , we evaluated our GSS variants on a wide range of sequence lengths upto 65 k and found consistent generalization to longer inputs. Not only the performance doesn't degrade as the sequence length is increased but it gets significantly better, suggesting that GSS is effective at utilizing the extra context even though it was not trained with that much amount of context. At inference time, state space models including GSS are fairly efficient since decoding can happen in recurrent mode (as much as $60 \\times$ better in the case of $S 4$ [Gu et al., 2022a]). Though, the hybrid model which also uses local attention complicates this advantage a bit. In summary, we propose GSS, an alternative to $S 4$ and DSS which trains $2-3 \\times$ faster, is simple to implement and fairly competitive with well-tuned Transformer-based baselines on several long range language modeling benchmarks. ## 2 Related Work\n\nIn recent years, attention-based models have emerged as a dominant technique for sequence modeling, achieving remarkable improvements in a wide range of tasks, starting in NLP [Vaswani et al., 2017, Devlin et al., 2019, Radford et al., 2019, Liu et al., 2019], then moving to other classical machine learning areas such as computer vision [Dosovitskiy et al., 2021] and now to the physical sciences [Avsec et al., 2021, Jumper et al., 2021].",
    "gssm-2": "In brief, an attention layer takes as input three matrices $K, Q, V$ in $\\mathbb{R}^{L \\times d}$, which should usually be thought of as length $L$ lists of $d$-dimensional vectors. The attention layer then outputs $Y=\\sigma\\left(Q K^{\\top}\\right) V \\in \\mathbb{R}^{L \\times d}$ where $\\sigma$ indicates a row-wise softmax operation. In the popular self-attention variant, $K, Q$ and $V$ are themselves learned functions (usually simple linear transformations) of a single input sequence $X=\\left(x_{1}, \\ldots, x_{L}\\right)^{\\top} \\in \\mathbb{R}^{L \\times d}$. Intuitively, one might imagine that this mechanism can model long range dependencies because the \"computational path\" between any row $x_{i}$ and any row $y_{j}$ is very short: more formally, this model avoids the \"vanishing gradients\" issue that plagues RNNs, in which the partial derivative $\\frac{\\partial y_{L}}{\\partial x_{1}}$ decays exponentially in $L$. Unfortunately, this requires $O\\left(L^{2}\\right)$ time and space due to the need to construct the matrix $Q K^{\\top} \\in \\mathbb{R}^{L \\times L}$, which places a limitation on how large $L$ can be. The surge in popularity of attention has engendered a corresponding surge in interest on methods for increasing the context length $L$ while controlling the $O\\left(L^{2}\\right)$ computational cost. Broadly speaking, most approaches fall into two camps: those that attempt to \"linearize\" attention, and those that sparsify the attention matrix $Q K^{\\top}$. The first camp exploits the fact that if the softmax is removed, we can re-write attention as $Q\\left(K^{\\top} V\\right)$, where now $K^{\\top} V \\in \\mathbb{R}^{d \\times d}$ and so the whole operation is only linear in $L$ rather than quadratic. Thus, if we could replace the softmax with a different operation that allowed some version of this rearrangement, it would be possible to speed up the attention layer. This idea is the underlying principle behind methods such as the Performer [Choromanski et al., 2021], Linear Attention [Katharopoulos et al., 2020], Random Feature Attention [Peng et al., 2021] or cosFormer [Qin et al., 2022]. In the other camp, one employs the simple expedient of simply not computing the entire matrix $Q K^{\\top}$, usually by employing some kind of sparsification strategy to only compute some of the elements of this matrix. For example, BigBird [Zaheer et al., 2020], GMAT [Gupta and Berant, 2020], Longformer [Beltagy et al., 2020], and Blockwise self-attention [Qiu et al., 2020] all choose particular sparsification patterns that are intuitively likely to capture \"important\" interactions, while the Reformer [Kitaev et al., 2020] uses locality-sensitive hashing to dynamically adjust the sparsification pattern. Most recently, Hawthorne et al. [2022] compress the history in a small number of latents for long range autoregressive tasks. In essence, these approaches can be viewed as a trade-off between performance, and computation time (or memory). Despite its current empirical success, the attention mechanism is not the only approach for modeling sequence data, nor even necessarily the most natural. For example, a classical recursive or state space layer operates on an input $\\left(x_{1}, \\ldots, x_{L}\\right)^{\\top} \\in \\mathbb{R}^{d \\times L}$ by defining a sequence of \"states\" $s_{t+1}=$ $T\\left(s_{t}, x_{t+1}\\right)$ and returning the output sequence $y_{t}=E\\left(s_{t}\\right)$ where $T$ and $E$ are learned \"transition\" and \"emission\" functions. Assuming $T$ and $E$ can be computed efficiently, this requires only $O(L)$ time. Note however, that the $O(L)$ theoretical complexity is complicated by the fact that these models appear to be inherently serial, while the $O\\left(L^{2}\\right)$ matrix operations in attention are much more easily parallelizable. State space models have additional attractive properties: they naturally capture our physical intuition that the universe is described by some appropriate state, and the output $y_{t}$ can be theoretically influenced by every single prior input, rather than requiring the model to commit to some context window of fixed length $L$. Through the years, many different possibilities for $T$ and $E$ have appeared, such as the simple RNN, which sets $T$ and $E$ to both be MLPs, or more complicated LSTM layer [Hochreiter and Schmidhuber, 1997]. Nevertheless, the performance of state space models has in many cases been quickly surpassed by attention. There have been several recent efforts to recapture the favorable properties of state space models while maintaining or improving upon the performance of attention models. For example, the Transformer-\n\nXL [Dai et al., 2019] modifies attention to incorporate state via a sliding-window style approach, while the Block-Recurrent Transformer [Hutchins et al., 2022] directly parameterized the $T$ and $E$ functions using attention layers. Further, alternative models based on linear dynamical systems have shown great promise [Gu et al., 2022a, Gupta et al., 2022, Gu et al., 2022b]. Rather than attempting to \"fix\" attention, these are classical state space models where both $T$ and $E$ are linear. An important insight in these works is to recast the model as a convolution with a very large kernel, and then leverage the convolution theorem to compute the sequence $y_{t}$ in $O(L \\log (L))$ time using Fast Fourier Transform [Cooley and Tukey, 1965]. While seemingly worse than $O(L)$, this operation is easily parallelizable, and so in practice is significantly faster. Moreover, in a situation in which the $\\log (L)$ factor is onerous, one may still fall back to the serial $O(L)$ algorithm. Despite their apparent simplicity, these models have demonstrated remarkable success in tasks that require integrating information from distant parts of the input: the key innovation is that careful initialization and parameterization of the linear maps is required in order to train the models effectively. Our approach will build on these works. ## 3 Method\n\nWe start by reviewing the necessary background on state spaces required to fully describe our model (\u00a73.1). We then formally define our GSS model in $\\S 3.2$ and the GSS-Transformer-Hybrid in $\\S 3.3$. ### 3.1 State Space Preliminaries\n\nWhile in this work we are interested in modeling sequences of vectors, let us first review how state space models define a sequence-to-sequence map for 1-D sequences. State Spaces A discretized state space, parameterized by a state matrix $A \\in \\mathbb{R}^{N \\times N}$, vectors $B \\in \\mathbb{R}^{N \\times 1}, C \\in \\mathbb{R}^{1 \\times N}$ and a sample time $\\Delta \\in \\mathbb{R}_{>0}$ defines a sequence-to-sequence map from input $\\left(u_{0}, \\ldots, u_{L-1}\\right)=u \\in \\mathbb{R}^{L}$ to output $\\left(y_{0}, \\ldots, y_{L-1}\\right)=y \\in \\mathbb{R}^{L}$ via the recurrence ${ }^{2}$,\n\n$$\n\\begin{aligned}\n& x_{k}=\\bar{A} x_{k-1}+\\bar{B} u_{k} \\quad, \\quad y_{k}=\\bar{C} x_{k} \\\\\n& \\bar{A}=e^{A \\Delta}, \\bar{B}=(\\bar{A}-I) A^{-1} B, \\bar{C}=C\n\\end{aligned}\n$$\n\nAssuming $x_{-1}=0$ for simplicity, the above recurrence can be explicitly unrolled as\n\n$$\ny_{k}=\\sum_{j=0}^{k} \\overline{C A}^{j} \\bar{B} \\cdot u_{k-j}\n$$\n\nFor convenience, the scalars $\\overline{C A}^{k} \\bar{B}$ are gathered to define the SSM kernel $\\bar{K} \\in \\mathbb{R}^{L}$ as\n\n$$\n\\bar{K}=\\left(\\overline{C B}, \\overline{C A B}, \\ldots, \\overline{C A}^{L-1} \\bar{B}\\right)=\\left(C e^{A \\cdot k \\Delta}\\left(e^{A \\Delta}-I\\right) A^{-1} B\\right)_{0 \\leq k<L}\n$$\n\nwhere the last equality follows by substituting the values of $\\bar{A}, \\bar{B}, \\bar{C}$ from Equation 1. Hence,\n\n$$\ny_{k}=\\sum_{j=0}^{k} \\bar{K}_{j} \\cdot u_{k-j}\n$$\n\nwhere $\\bar{K}_{j}$ denotes the value of the kernel at position $j$. Given an input sequence $u \\in \\mathbb{R}^{L}$, it is possible to compute the output $y \\in \\mathbb{R}^{L}$ sequentially via the recurrence in Equation 1. While this property is highly desirable for autoregressive decoding, a sequential computation is prohibitively slow to train with long inputs and, instead, Equation 4 can be used to compute all elements of $y$ in parallel, provided we have already computed $\\bar{K}$. [^1]Computing $y$ from $u$ and $\\bar{K}$ is easy. Given an input sequence $u \\in \\mathbb{R}^{L}$ and the SSM kernel $\\bar{K} \\in \\mathbb{R}^{L}$, naively using Equation 4 for computing $y$ would require $O\\left(L^{2}\\right)$ multiplications. Fortunately, this can be done much more efficiently by observing that for the univariate polynomials\n\n$$\n\\bar{K}(z)=\\sum_{i=0}^{L-1} \\bar{K}_{i} z^{i} \\text { and } u(z)=\\sum_{i=0}^{L-1} u_{i} z^{i}\n$$\n\n$y_{k}$ is the coefficient of $z^{k}$ in the polynomial $\\bar{K}(z) \\cdot u(z)$, i.e. all $y_{k}$ 's can be computed simultaneously by multiplying two degree $L-1$ polynomials. It is well-known that this can be done in $O(L \\log (L))$ time via Fast Fourier Transform (FFT) [Cormen et al., 2009]. We denote this fast computation of Equation 4 via the discrete convolution as\n\n$$\ny=\\bar{K} *_{\\mathrm{c}} u\n$$\n\nDiagonal State Spaces The challenging part is computing $\\bar{K}$ itself as it involves computing $L$ distinct matrix powers (Equation 3). Gupta et al. [2022] observed that the state matrix $A$ can be assumed to be diagonal without loss in performance, thereby allowing a straighforward computation of $\\bar{K}$. Their $\\operatorname{DSS}_{\\text {EXP }}$ model (DSS from hereon) assumes $A$ to be a diagonal matrix $\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{N}\\right)$ and assumes $B=(1)_{1 \\leq i \\leq N}$. DSS has parameters $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}} \\in \\mathbb{R}^{N}, C \\in \\mathbb{C}^{N}$ and $\\Delta_{\\mathrm{log}} \\in \\mathbb{R}$. The diagonal of $A$ (i.e. $\\left(\\lambda_{1}, \\ldots, \\lambda_{N}\\right)$ ) is computed as -elementwise-exp $\\left(\\Lambda_{\\mathrm{re}}\\right)+i \\cdot \\Lambda_{\\mathrm{im}}$ where $i=\\sqrt{-1}$ and $\\Delta$ is computed as $\\exp \\left(\\Delta_{\\log }\\right) \\in \\mathbb{R}_{>0}$. For this parameterization, the kernel (Equation 3) can be computed as a matrix-vector product\n\n$$\n\\bar{K}=\\left(C *\\left(\\left(e^{\\lambda_{i} \\Delta}-1\\right) / \\lambda_{i}\\right)_{1 \\leq i \\leq N}\\right)_{1 \\times N} \\cdot \\text { elementwise-exp }\\left(P_{N \\times L}\\right)\n$$\n\nwhere $P_{i, k}=\\lambda_{i} k \\Delta$ and $*$ denotes elementwise multiplication. This formally defines a linear sequence-to-sequence map for 1-D sequences. In the case of sequences of $H$-dimensional vectors, state space models are applied individually on the $H$ features as follows. Each DSS layer receives a length- $L$ sequence $u$ of $H$-dimensional vectors as input, i.e., $u \\in \\mathbb{R}^{H \\times L}$, and produces an output $y \\in \\mathbb{R}^{H \\times L}$. The parameters of the layer are $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}} \\in \\mathbb{R}^{N}, \\Delta_{\\log } \\in \\mathbb{R}^{H}$ and $C \\in \\mathbb{C}^{H \\times N}$. For each coordinate $h=1, \\ldots, H$, a state space kernel $\\bar{K}_{h} \\in \\mathbb{R}^{L}$ is computed for $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}},\\left(\\Delta_{\\mathrm{log}}\\right)_{h}, C_{h}$ via Equation 6 . The output $y_{h} \\in \\mathbb{R}^{L}$ for coordinate $h$ is computed from $u_{h} \\in \\mathbb{R}^{L}$ and $\\bar{K}_{h}$ using Equation 5. For batch size $B$, sequence length $L$ and hidden size $H$, the DSS layer requires $O(N H L)$ time to compute the kernels, and $O(B H L \\log (L))$ time for the discrete convolution. In S4 and DSS, the authors suggest to use $N=64$ as default and for this choice of $N$, the time taken to compute the kernels becomes an important factor, specially for small batches. ### 3.2 GSS Layer\n\nArmed with the tools and techniques from the state space literature, we now turn to the main contribution of our work and describe our GSS layer in detail (Figure 1). We build on the idea of the recently introduced Gated Attention Unit (GAU) [Hua et al., 2022] and replace the $\\Omega\\left(L^{2}\\right)$ attention used in GAU by a further simplified DSS layer (\u00a73.1). Gating allows our model to be contextualized over a reduced dimensionality and the use of state spaces provides it with superior contextualizing abilities, while enjoying $O(L \\log L)$ complexity out of the box. Concretely, our Gated State Space (\"GSS\") layer maps a sequence $X \\in \\mathbb{R}^{L \\times E}$ to an output $O \\in$ $\\mathbb{R}^{L \\times E}$ as\n\n$$\n\\begin{aligned}\n& U=\\phi\\left(X W_{1}\\right) \\in \\mathbb{R}^{L \\times H} \\quad V=\\phi\\left(X W_{2}\\right) \\in \\mathbb{R}^{L \\times F} \\\\\n& Y=\\operatorname{DSS}(U) \\in \\mathbb{R}^{L \\times H} \\quad U_{\\text {context }}=Y W_{3} \\in \\mathbb{R}^{L \\times F} \\\\\n& O=\\left(U_{\\text {context }} * V\\right) W_{4} \\in \\mathbb{R}^{L \\times E}\n\\end{aligned}\n$$\n\nwhere $*$ denotes elementwise multiplication and $\\phi$ denotes GELU [Hendrycks and Gimpel, 2016]. Note that, contrary to Hua et al. [2022], in our experiments, we did not observe much benefit from using RELU ${ }^{2}$ or Swish activations instead of GELU. In Equation 8, a contextualized representation of $U$ is formed via DSS in $O(N H L)+O(B H L \\log L)$ time. In most of our experiments we used $H=E / 4=256, N=512$ and $F=4096$. Similar to GAU, gating allows us to perform a weaker contextualization using the state space layer by reducing the dimension of the input to DSS , by $4 \\times$ in our case. This offers a much needed increase in throughput since we observed that FFT ops were the main bottleneck (measured on TPUs). As shown in Table 1, this provides more than $3 \\times$ speedups over the vanilla DSS block described in $\\S 3.1$. Simpified DSS used in GSS layer In Equation 8, instead of directly using DSS as described in $\\S 3.1$, we made several key simplifications based on our exploratory results. As described in $\\S 3.1, \\mathrm{~S} 4$ and DSS train a separate $\\Delta$ for each of the $H$ coordinates resulting in the creation of $H$ separate $P$ matrices in Equation 6. In the DSS layer used in GSS, we decided to eliminate the use of $\\Delta$ by fixing it as 1 . This reduces the compute required for creating the kernels and makes the kernel computation simpler. Secondly, we found that randomly initializing the parameters corresponding to $\\Lambda$ works just as well as the Skew-Hippo initialization suggested in [Gupta et al., 2022]. In practice, we parametrize both real and imaginary parts of $\\Lambda$ in log space (\u00a7A.2). The effectiveness of random initialization is in contrast to the findings of Gu et al. [2022a] and Gupta et al. [2022] who reported their respective initializations of the state space parameters to be critical to the model performance. We do however note that the experiments in our setting of large-scale language modeling are conducted on orders of magnitude larger scale of compute than what is used in the tasks considered in these works.",
    "gssm-3": "Moreover, the data modalities we consider in this work are different from the tasks considered in these works (e.g. Long Range Arena) which are specifically designed to stress test long-range reasoning. ### 3.3 GSS-Transformer-Hybrid\n\nConceptually, GSS looks fairly different from the current workhorse of machine learning; the Transformer architecture. Given this, it is not immediately clear if one is decidedly better than the other or if they each provide some orthogonal benefits. If its the latter, one might wonder if there are synergies between these architectures which can be exploited to create a hybrid model which is stronger than either one of them individually. To that end, we also consider a conceptually simple hybrid between GSS and Transformer where we sparingly interleave traditional Transformer blocks with GSS layers. Despite its glaring simplicity, as shown in Table 2, we observed that it shows consistent and significant improvements on all tasks. Chunking long inputs In all our experiments we used sequence lengths large enough to be prohibitive for traditional Transformer layers. To get around this restriction at the Transformer layers used in our hybrid model, we chunk their inputs into non-overlapping chunks of length 512 and run the Transformer layer on each of them independently. While the GSS layers are apt at modeling both short and longer range interactions, the interleaved Transformer layers can potentially allow for a richer modeling of short range interactions. ## 4 Results\n\nWe conduct experiments with GSS on 4 different datasets, LM1B, PG19, ArXiv and Github, each of them varying qualitatively with another in terms of modality and average document length. LM1B is a standard and reasonably big dataset (1B tokens) where each training example consists of a short sentence [Chelba et al., 2014]. This is different from rest of the datasets which consists of a much larger sequence of tokens per example. Although, our primary goal is to measure GSS's ability to capture long range dependencies, we include results on LM1B benchmark and compare with vanilla Transformer baseline, which is hard to do for larger sequence lengths. PG19 dataset is constructed from extracting a large collection of full-length books from Project Gutenberg [Rae et al., 2020]. All extracted books were written before 1919 and only contain tokens from the English language. Over the years PG-19 has become a standard benchmark for measuring progress on long range dependency modeling over text. ArXiv Math dataset was recently collected by Wu et al. [2022] and contains LaTeX source for articles focusing on Mathematics. Even though articles are typically shorter than full-length books,\n\n|  |  |  |  |  | Eval Sequence Length |  |  |  |\n| :--- | :--- | :--- | :--- | :--- | :---: | :---: | :---: | :---: |\n| Dataset | Model | Params | Throughput (steps/sec) | Perplexity |  |  |  |  |\n|  |  |  |  | 512 | 4 k | 16 k | 65 k |  |\n| LM1B | Transformer | 182 M | 6.6 | 13.51 |  |  |  |  |\n|  | DSS | 188 M | 1.8 | 13.59 |  |  |  |  |\n|  | GSS | 190 M | 5.6 | 13.26 |  |  |  |  |\n| PG-19 | DSS | 209M | 1.8 | 14.51 | 13.53 | 692.1 | OOM |  |\n|  | GSS | 192 M | 5.3 | 14.01 | 12.84 | 12.94 | 12.47 |  |\n| Arxiv | DSS | 209M | 1.8 | 3.65 | 3.13 | 284.6 | OOM |  |\n|  | GSS | 192 M | 5.3 | 3.57 | 3.08 | 3.08 | 2.75 |  |\n| Github | DSS | 209M | 1.8 | 3.65 | 3.21 | 242.7 | OOM |  |\n|  | GSS | 192 M | 5.3 | 2.68 | 2.35 | 2.31 | 2.12 |  |\n\nTable 1: Comparison of DSS and GSS models in fixed-param setting. We consistently find that GSS outperforms DSS (with hyperparameters taken from Gupta et al. [2022]) while being 2-3\u00d7 faster on all tasks. Training sequence length is 4 k , except for LM1B for which it is 512. as in PG19, since LaTeX source can have many special characters, typical sub-piece vocabularies are not very effective and tends to produce examples of similar size, as measured by number of tokens. It is possible to train a custom sub-piece vocabulary for this dataset, but we stick with the vocabulary used by both Wu et al. [2022] and Hutchins et al. [2022] for fair comparison. Github was also first collected and used by Wu et al. [2022]. It is a corpus of raw source code collected from several Github repositories with open-source licences. The dataset contains code from several programming languages, including C, C++, Java, Python, Go and Typescript. In this case individual files can be small but code repositories are typically is organized so that code can be reused across file boundaries. Thus, for every repository, all the files were concatenated to produce a single document. We do token level modeling on all the datasets and report resulting perplexity numbers on a heldout set of examples. Perplexity numbers are obtained using teacher forcing (or parallel mode) where the correct output from the heldout set is used for decoding the next token at each position. For a fair comparison with the baselines we considered, we keep the vocabularies consistent as used by the baselines models. Specifically, we used custom trained 30k sized sentence-piece vocab for LM1B, T5 vocab with 32k tokens for PG19 [Raffel et al., 2020] and Meena vocab with 32k tokens [Adiwardana et al., 2020] for both ArXiv and Github datasets. ### 4.1 Comparing DSS and GSS\n\nAs shown in Table 1, in our first set of results, we compare DSS and GSS models both in terms of perplexity and throughput, as measure by steps per second on all 4 datasets.",
    "gssm-4": "Except for LM1B, all models were trained with 4 k sequence length at training time. However, at evaluation time, we present results for on a large range of sequence lengths. For LM1B, we train and evaluate with sequence length of 512, since the dataset only contains short sentences, most examples comfortably fully fitting in. Despite being a (slightly) smaller model, GSS performs quite favorably as measured by perplexity while being $2-3 \\times$ faster depending on the dataset. Furthermore, we observe significant generalization over changes in sequence length on all the datasets. Not only does the performance not degrade when increasing sequence length, it actually improves quite a bit! This suggests that the model is effective at utilizing the extra context even though the model was not trained with that amount of context. Note that we used default hyperparameters suggested by Gupta et al. [2022] for initializing state space variables for DSS. But, for GSS, since we made several structural changes, we retuned the hyperparameters related to initialization on PG19 alone and used them for all the datasets. Since we had access to evaluation metrics for all sequence lengths, length generalization factored in for our choice of hyperparameters, which is not true for DSS. It is completely possible we see similar length generalization even with DSS if initialization hyperparameters were retuned. |  |  |  |  | Eval Sequence Length |  |  |  |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Dataset | Model | Params | TPUv4 hours | Perplexity |  |  |  |\n|  |  |  |  | 512 | 4 k | 16 k | 65 k |\n| PG-19 | Rec:fixed:skip | 196M | 0.8 k |  | 11.55 |  |  |\n|  | Feedback:Istm:single | 196M+ | $0.8 \\mathrm{k}+$ |  | 11.31 |  |  |\n|  | Feedback:fixed:skip | 196M+ | $0.8 \\mathrm{k}+$ |  | 11.24 |  |  |\n|  | GSS (this work) | 192M | 0.5 k | 14.01 | 12.84 | 12.94 | 12.47 |\n|  | GSS-L | 352M | 0.8 k | 12.48 | 11.33 | 11.16 | 11.12 |\n|  | GSS-Hybrid-L | 373M | 0.8 k | 11.45 | $\\mathbf{1 0 . 5 2}$ | 10.44 | 10.1 |\n| Arxiv | Rec:fixed:skip | 196 M | 0.8 k |  | 2.36 |  |  |\n|  | Feedback:Istm:single | $196 \\mathrm{M}+$ | $0.8 \\mathrm{k}+$ |  | $\\mathbf{2 . 3 3}$ |  |  |\n|  | Feedback:fixed:skip | $196 \\mathrm{M}+$ | $0.8 \\mathrm{k}+$ |  | 2.36 |  |  |\n|  | GSS | 192 M | 0.5 k | 3.57 | 3.08 | 3.08 | 2.75 |\n|  | GSS-L | 352M | 0.8 k | 3.29 | 2.71 | 2.72 | 2.51 |\n|  | GSS-Hybrid-L | 373M | 0.8 k | 2.94 | 2.51 | 18.27 | 125.2 |\n| Github | Rec:fixed:skip | 196 M | 3 k |  | 2.04 |  |  |\n|  | Feedback:Istm:single | 196M+ | $3 \\mathrm{k}+$ |  | 2.07 |  |  |\n|  | Feedback:fixed:skip | $196 \\mathrm{M}+$ | $3 \\mathrm{k}+$ |  | 2.16 |  |  |\n|  | GSS | 192 M | 0.5 k | 2.68 | 2.35 | 2.31 | 2.12 |\n|  | GSS-L | 352M | 1.8 k | 2.31 | 1.99 | 2.20 | 2.28 |\n|  | GSS-Hybrid-L | 373M | 1.8 k | 2.34 | $\\mathbf{1 . 8 8}$ | 1.74 | 2.09 |\n\nTable 2: Comparing GSS variants with best-performing models from Hutchins et al. [2022] in both fixed-param and fixed-compute settings. While, in the fixed-param setting, GSS currently lags behind performance of best Block Recurrent Transformer baseline, it is fairly competitive in the fixed-compute setting (as measured by total TPUv4 hours taken to complete training). For the larger model GSS-L used for fixed compute comparison, we simply double the layers from 16 to 32 keeping everything else fixed. In addition, due to the inherent recurrent view of state space model family, decoding at inference time is much faster than Transformer based baselines. For block-recurrent baselines, adding feedback increases both parameter count and training time, we stick with conservative estimates derived from Hutchins et al.",
    "gssm-5": "[2022] for both, which we denote by ' + '. Hutchins et al. [2022] report $\\log _{2}$ of perplexity which we convert to raw perplexity in this table. Param count for all the models include embedding layers as well. Training details All the models were trained with $2^{19}$ tokens per batch and 125 k total training steps. We make sure to change the batch size as a function of the sequence length so that number of tokens in the batch remains the same. For example, for LM1B we set batch size to 1024 and sequence length to 512 but for rest of the datasets we use batch size of 128 and sequence length of 4 k . For datasets with longer documents, we also considered increasing the sequence length even further. Intuitively, training on longer sequences would help the model learn longer range dependencies better. On the flip side, it makes the optimization a bit more challenging due to large number of correlated tokens per batch and would even likely result in overfitting. Since GSS is able to generalize beyond the length it was trained on in most cases, we found sequence length of 4 k to be a reasonable middle ground for this set of experiments. Similar to language modeling experiments in [Gu et al., 2022a], every block of DSS baseline consists of DSS layer followed by GLU [Dauphin et al., 2017] and a Feedforward layer similar to the one used in Transformer block with GELU activation [Hendrycks and Gimpel, 2016]. DSS baseline consists 12 layers and an embedding dimension of 1024. For GSS, we increased the number of layers to 16 to match the parameter count. ### 4.2 Comparison with other baselines\n\nIn this section, we turn our attention towards apples-to-apples comparison of GSS versus well-tuned baselines on these datasets. For a complete picture of the cost of training these models, we report both the number of parameters and time spent training as measured by TPUv4 hours. For baselines, we\nselected the best performing models reported in [Hutchins et al., 2022] for every dataset and compare with GSS model both in fixed-param and fixed-compute settings. Training Details Similar to Section 4.1, unless otherwise mentioned, all the non-baseline models were trained using 64 TPUv4 cores for 125 k steps. We use batch size of 128 and 4 k sequence length at training time, with a total of $2^{19}$ tokens per batch. We increase the batch size to 256 for the Github dataset (with a token count of $2^{20}$ per batch) since we observed a lot of noise in our metrics. This is consistent with observations made by Hutchins et al. [2022]. We used Adam optimizer [Kingma and Ba , 2015] and tuned the base learning rate over a grid of $\\epsilon$ [0.0064, 0.0032, 0.0016, 0.0008]. We also employ linear warmup for 1 k steps and cosine decay until $1 \\mathrm{e}-6$. We also observed better performance by using a higher than typical weight decay rate of 0.1 , other than that we did not employ any additional regularization techniques, including dropout. Note that similar to [Gu et al., 2022a, Gupta et al., 2022], we used a constant learning rate of $1 \\mathrm{e}-3$ and set weight decay rate to 0.0 for state space parameters part of GSS Layer. In addition, we clip the gradient to norm 1.0 before passing to the optimizer. Both of these helped with certain instabilities we observed in our preliminary experiments. GSS models GSS consists of 16 layers and an embedding dimension of 1024. We also consider a larger variant with 32 layers as denoted by GSS-L. For GSS-Hybrid model, we used vanilla Transformer blocks at every 4th layer starting with the 2nd layer. Since GSS layers are inherently position aware, using them for the 1st layer eschews any need of explicit position embeddings typically used with otherwise position invariant Transformer blocks. Thus, barring position aware nature of GSS layers, we don't use any kind of explicit position embedding or bias in our models. For the Transformer blocks used in hybrid models, we use multi-head self-attention with 8 heads, each with size 128 . Baselines We considered 3 high-performing baselines and numbers reported in [Hutchins et al., 2022]. Block Recurrent Transformer leverages recurrence over blocks of Transformer layer to model long range dependencies. Hutchins et al. [2022] performed a comprehensive exploration of open design space of incorporating recurrence across blocks. Somewhat surprisingly, Rec:fixed:skip, which accumulates recurrent state vector as an exponential moving average over time performs better than more complicated gating designs. Another variation which performed well with Block Recurrence is the idea of adding a feedback mechanism over blocks similar to Feedback Transformer [Fan et al., 2020]. Note that feedback mechanism makes training more expensive due to additional cross-attention modules and corresponding paramaters. Fixed-param comparison As shown in Table 2, we see that GSS variants come very close but not quite beat the strongest block recurrent baseline in the fixed-param setting. In this case, we are comparing GSS model which has roughly 192M parameters (including embeddings) with the baselines all of which have around 196M parameters. Even though the parameter count is fixed, we see that GSS runs faster than block recurrent baselines, likely due to the fact that all the layers can be completely parallelized unlike the the recurrence in the baselines which run in a sequential fashion. Fixed-compute comparison Since the GSS model runs faster than the baselines, we also train with versions larger than GSS such that the training time (as measured by total TPUv4 hours) matches the time taken by the baselines. We simply double the number of layers from 16 to 32 to construct GSS-L. As expected, adding more parameters improves perplexity numbers on the eval set of all the datasets. Moreover, we find that the GSS-Hybrid versions of the model outperform the best baseline model on PG-19 and Github datasets. We do see significant improvements for Arxiv dataset as well but unfortunately not enough to be stronger than the baseline. We think this may be resolved by the use of a vocabulary more suited to Arxiv symbols. On the flip side, we can no longer do a fair comparison with token level perplexity if we change the vocabulary, so we stick with the vocabularies used by the baselines for this study. Length generalization We train all variants of GSS with sequence length of 4 k but evaluate on 4 different lengths $l \\in[512,4 k, 16 k, 65 k]$. On PG-19, we see significant length generalization across the board. Not only the performance doesn't degrade as the sequence length is increased but it gets significantly better for all model variants. On Arxiv and Github, the situation is a little more complicated. For smaller models, we still see length generalization but it tends to degrade when either the model is made bigger or the dataset has a lot of noise (as indicated by variation in perplexity metric over time). How to robustly achieve length generalization is an interesting research question\non it is own and we believe one can design interventions which can lead to further improvements, which we don't explore here. Note that block recurrent baselines, with or without feedback mechanism, process documents in a sequential fashion such that recurrent state from previous segment from the document is passed to the next segment, with backward pass truncated to the current segment. This means that, even though the segment sequence length is set to 4 k , block recurrent models have (arguably weak) access to almost the entire past. Thus, perplexity comparison at sequence length 4 k is slightly unfair towards GSS models since they do not employ such state caching mechanisms. ## Acknowledgments\n\nWe are grateful to the developers of Jax and Flax libraries. In addition, we would like to thank DeLesley Hutchins and Imanol Schlag for answering our questions, sharing the datasets and helping us in general with details of Block Recurrent Transformer paper. Finally, we are grateful to Ethan Dyer for discussions on long context modeling, Walid Krichene and John Anderson for providing feedback on an earlier draft of this paper. ## Bibliography\n\nDaniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. Towards a human-like open-domain chatbot. ArXiv, abs/2001.09977, 2020. \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.",
    "gssm-6": "ArXiv preprint, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, T. Brants, Phillip Todd Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH, 2014. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=Ua6zukOWRH. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.",
    "gssm-7": "ArXiv preprint, abs/2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. James W. Cooley and John W. Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of Computation, 19:297-301, 1965. Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms. The MIT Press, 3rd edition, 2009. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 933-941. JMLR.org, 2017. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTTy. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. ArXiv preprint, abs/2002.09402, 2020. URL https: //arxiv.org/abs/2002.09402. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 102fObb6efb3a6128a3c750dd16729be-Abstract.html. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum? id=uYLFoz1vlAC. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models.",
    "gssm-8": "ArXiv, abs/2206.11893, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers.",
    "gssm-9": "ArXiv preprint, abs/2006.03274, 2020. URL https://arxiv.org/abs/2006.03274. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.",
    "gssm-10": "ArXiv preprint, abs/2203.14343, 2022. URL https://arxiv.org/abs/2203.14343. Curtis Hawthorne, Andrew Jaegle, C\u0103t\u0103lina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew M.",
    "gssm-11": "Botvinick, Ian Simon, Hannah R. Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, Jo\u00e3o Carreira, and Jesse Engel. General-purpose, long-context autoregressive modeling with perceiver ar. $A r X i v$, abs/2202.07765, 2022. Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units.",
    "gssm-12": "ArXiv preprint, abs/1606.08415, 2016. URL https://arxiv.org/abs/1606.08415. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W.",
    "gssm-13": "Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv preprint, abs/2203.15556, 2022. URL https://arxiv.org/abs/2203.15556. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. URL https: //arxiv. org/abs/ 2202.10447. DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. ArXiv preprint, abs/2203.07852, 2022. URL https://arxiv.org/abs/2203. 07852. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold.",
    "gssm-14": "Nature, 596(7873):583-589, 2021. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "gssm-15": "In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5156-5165. PMLR, 2020. URL http://proceedings.mlr.press/v119/ katharopoulos20a.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, $3 r$ International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.",
    "gssm-16": "ArXiv preprint, abs/1907.11692, 2019. URL https://arxiv.org/abs/1907. 11692. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. Random feature attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https: //openreview.net/forum?id=QtTKTdVrFBB. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555-2565, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 232. URL https://aclanthology.org/2020.findings-emnlp. 232. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.",
    "gssm-17": "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=SylKikSYDH. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \\& insights from training gopher.",
    "gssm-18": "ArXiv preprint, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "gssm-19": "J. Mach. Learn. Res., 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html. Noam M. Shazeer. Glu variants improve transformer.",
    "gssm-20": "ArXiv preprint, abs/2002.05202, 2020. URL https : //arxiv.org/abs/2002.05202. Aarohi Srivastava, Abhinav Rastogi, Abhishek B Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Annasaheb Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew D. La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs, Bridget R. Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Stephen Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C'esar Ferri Ram'irez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Tatiana Ramirez, Clara Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Daniel H Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Gonz'alez, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, D. Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth P. Donoway, Ellie Pavlick, Emanuele Rodol\u00e0, Emma FC Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fan Xia, Fatemeh Siar, Fernando Mart'inez-Plumed, Francesca Happ'e, Fran\u00e7ois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L'opez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Han Sol Kim, Hannah Rashkin, Hanna Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang, Hubert Wong, Ian Aik-Soon Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00e1ndez Fisac, J. Brooker Simon, James Koppel, James Zheng, James Zou, Jan Koco'n, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jenni Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Jane W Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant, Jorg Frohberg, Jos Rozen, Jos\u00e9 Hern\u00e1ndez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Ochieng\u2019 Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Luca Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col'on, Luke Metz, Lutfi Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Madotto Andrea, Maheen Saleem Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, M Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew Leavitt, Matthias Hagen, M'aty'as Schubert, Medina Baitemirova, Melissa Arnaud, Melvin Andrew McElrath, Michael A. Yee, Michael Cohen, Mi Gu, Michael I. Ivanitskiy, Michael Starritt, Michael Strube, Michal Swkedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Monica Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, T MukundVarma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas S. Roberts, Nicholas Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W. Chang, Peter Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, Piyush S. Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, QING LYU, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ram'on Risco Delgado, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Lebras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib J. Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Sam Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Sameh Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, Simone\n\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo hwan Lee, Spencer Bradley Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Rose Biderman, Stephanie C. Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq A. Ali, Tatsuo Hashimoto, Te-Lin Wu, Theo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, T. N. Kornev, Timothy TelleenLawton, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler O. Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay V. Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, W Vossen, Xiang Ren, Xiaoyu F Tong, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yang Song, Yasaman Bahri, Ye Ji Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yushi Bai, Zachary Seid, Zhao Xinran, Zhuoye Zhao, Zi Fu Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu, Sahib Singh, and Uri Shaham. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615, 2022. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.",
    "gssm-21": "ArXiv preprint, abs/2009.06732, 2020. URL https://arxiv.org/abs/2009.06732. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.",
    "gssm-22": "Wallach, Rob Fergus, S.",
    "gssm-23": "V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/ 2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Aaron Voelker, Ivana Kajic, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks.",
    "gssm-24": "In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 15544-15553, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 952285b9b7e7a1be5aa7849f32ffff05-Abstract.html. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? $i d=T r j b x z R c n f-$. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings . neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models.",
    "gssm-25": "ArXiv preprint, abs/2205.01068, 2022. URL https://arxiv.org/abs/2205.01068. ## A Supplemental Material\n\n## A. 1 Fast convolution via FFT\n\nFor $u, v \\in \\mathbb{C}^{1 \\times L}$ the Circular Convolution Theorem states that,\n\n$$\n\\operatorname{invFFT}_{L}\\left(\\operatorname{FFT}_{L}(u) * \\operatorname{FFT}_{L}(v)\\right)=v \\cdot\\left[\\begin{array}{cccc}\nu_{0} & u_{1} & \\cdots & u_{L-1} \\\\\nu_{L-1} & u_{0} & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & u_{1} \\\\\nu_{1} & \\cdots & u_{L-1} & u_{0}\n\\end{array}\\right]=v \\cdot \\operatorname{circulant}(u)\n$$\n\nwhere $*$ denotes elementwise multiplication. As FFT, invFFT can be done in $O(L \\log L)$ time this provides a fast algorithm for circulant matrix-vector product. In practice, linear systems can often be expressed as a circulant matrix-vector product and is also true in the case of Equation 4 which can be equivalently expressed as\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5ab9ed67021bd72a4442g-15.jpg?height=65&width=1355&top_left_y=813&top_left_x=382)\n\n## A. 2 Implementation of GSS\n\n```\ndef simplified_dss_kernel(H, L, N=512):\n    # Lambda_re, Lambda_im: [N]\n    # C_re, C_im: [H N]\n    Lambda = -Lambda_re.exp() + 1j*Lambda_im.exp() # [N]\n    C = C_re + 1j*C_im # [H N]\n    S = (Lambda * arange(L).view(1,L)).exp() # [N L]\n    C = C * (Lambda.exp() - 1) / Lambda # [H N]\n    return einsum('hn,nl->hl', C, S).real # [H L]\ndef dss(u, H, L):\n```\n\n```\n    \\(\\mathrm{u}=\\operatorname{norm}(u)\\)\n    \\# compute H state space kernels\n    K = simplified_dss_kernel(H, L)\n    K_f = rfft(K, pad_to=2*L)\n    u_f = rfft(u, pad_to=2*L)\n    \\(y=\\operatorname{irfft}\\left(K_{\\_} f * u_{-} f\\right)[\\ldots,: L]\\)\n    \\# param D: [H,1]\n    return \\(\\mathrm{y}+\\mathrm{D} * \\mathrm{u}\\)\ndef gss(x, F=4096, L=4096, \\(\\mathrm{E}=1024, \\mathrm{H}=256\\) ):\n    shortcut, \\(\\mathrm{x}=\\mathrm{x}, \\operatorname{norm}(\\mathrm{x})\\)\n    \\(\\mathrm{v}=\\) dense(x, F, activation='gelu')\n    \\(\\mathrm{u}=\\) dense(x, H, activation='gelu')\n    \\(\\mathrm{y}=\\mathrm{dss}(\\mathrm{u}, \\mathrm{H}, \\mathrm{L})\\)\n    \\(\\mathrm{uc}=\\operatorname{dense}(\\mathrm{y}, \\mathrm{F})\\)\n    \\(0=\\) dense(uc \\(*\\) v, E)\n    return o + shortcut\n```\n\nFigure 2: Pseudocode of GSS (\\$3.2).",
    "gssm-26": "[^0]:    ${ }^{*}$ 1 Google Research, ${ }^{2}$ Tel Aviv University, ${ }^{3}$ Boston University. harshm@google.com, ankitgupta.iitkanpur@gmail.com, ashok@cutkosky.com, neyshabur@google.com. [^1]:    ${ }^{2}$ Discretization used in Equation 1 relies on zero order hold $(\\mathrm{ZOH})$ which assumes a constant value between sampled times. We also experimented with other discretization methods such as bilinear transform but did not see much difference in performance.",
    "gssm-27": ""
}