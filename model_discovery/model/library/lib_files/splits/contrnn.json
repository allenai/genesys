{
    "contrnn-0": "# Modeling Hierarchical Structures with Continuous Recursive Neural Networks \n\nJishnu Ray Chowdhury ${ }^{1}$ Cornelia Caragea ${ }^{1}$\n\n\n#### Abstract\n\nRecursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases.",
    "contrnn-1": "However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference (Bowman et al., 2015b) and ListOps (Nangia \\& Bowman, 2018). We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference. ${ }^{1}$\n\n\n## 1. Introduction\n\nConstructing a sentence representation is crucial for Natural Language Processing (NLP) tasks such as natural language inference, document retrieval, and text classification. In certain contexts, methods with a bias towards composing sentences according to their underlying syntactic structures (Chomsky, 1957) have been shown to outperform comparable structureagnostic methods (Socher et al., 2013; Tai et al., 2015;\n\n[^0]Nangia \\& Bowman, 2018; Choi et al., 2018; Maillard et al., 2019; Havrylov et al., 2019; Shen et al., 2019a) and some of these structure-aware methods (Shen et al., 2019a; Qian et al., 2020) also exhibit better systematicity (Fodor \\& Pylyshyn, 1988). Notably, even contemporary Transformer-based methods (Vaswani et al., 2017) have benefited from structural biases in multiple natural language tasks (Wang et al., 2019; Fei et al., 2020). Recursive Neural Networks (RvNNs) (Pollack, 1990; Socher et al., 2013) are capable of composing sequences in any arbitrary order as defined by the structure of the input. However, RvNNs follow a hard structure when composing a given sequence. Latent-Tree models such as Gumbel-Tree-LSTM (Choi et al., 2018), despite being capable of adaptively making structural decisions, still follow a hard structure of composition. As such, these models are forced to either make discrete structural decisions or rely on external parsers. In the former case, the model is often forced to rely on surrogate gradients ${ }^{2}$ (resulting in increased bias) or reinforcement learning (resulting in increased variance). The increased bias or variance can lead to poor performance (Nangia \\& Bowman, 2018) unless much care is taken (Havrylov et al., 2019). In contrast to these RvNN-based methods, chart-based parsers (Le \\& Zuidema, 2015; Maillard et al., 2019) can follow a \"soft-structure\" by recursively applying weighted average on outputs from multiple possible structures. However, these methods require the construction and maintenance of a chart of vectors, which is comparatively expensive. Given these challenges, in this work, we propose Continuous Recursive Neural Network (CRvNN), which incorporates a continuous relaxation to RvNNs making it end-toend differentiable. We aim for the following features:\n\n1. Automatic task-specific latent tree induction. Our model is not reliant on ground truth structural labels in any form. 2. Backpropagation friendly. This allows our model to\n\n[^1]be easily integrated as a module within a larger neural architecture and avoid the challenges of reinforcement learning or surrogate gradients (Havrylov et al., 2019).",
    "contrnn-2": "3. Parallelism. Unlike most prior latent tree models, CRvNN can compose multiple constituents in parallel if they are in the same level of hierarchy. This implies that, ideally, CRvNN can recurse over the induced tree-depth, which typically is much shorter than the sequence length. As such, CRvNN can also alleviate difficulties with gradient vanishing or gradient exploding (Hochreiter, 1991; Bengio et al., 1994), which can be caused by applying recurrence or recursion over longlength sequences. Although chart-based parsers technically share the above features to an extent, they parallelize over multiple paths of composition instead of straightforwardly composing multiple constituents in a single path. Overall, chart-based parsers still need to recurse over the entire sequence length. In contrast, CRvNN can instead halt early based on the induced tree-depth albeit at the cost of more greediness. ## 2. Preliminaries\n\nIn this section, we provide a brief introduction to Recursive Neural Networks. ### 2.1. Recursive Neural Network (RvNN)\n\nAn RvNN uses some composition function $f$ to recursively compose two given children into their parent:\n\n$$\np=f\\left(x_{i}, x_{i+1}\\right)\n$$\n\nHere, we consider a sequence $x_{1: n}=\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)$, and a composition function $f$ that takes two child vectors of dimension $d$ as input and returns a parent vector of the same dimension $d$. The recursion terminates when it composes every node into the root parent. As an example, consider the sequence $x_{1: 6}$ with an underlying hierarchical structure expressed as:\n\n$$\n\\left(\\left(x_{1},\\left(x_{2}, x_{3}\\right)\\right),\\left(x_{4},\\left(x_{5}, x_{6}\\right)\\right)\\right)\n$$\n\nThis kind of structured sequence can be also expressed in terms of a tree. The RvNN operates on the input as:\n\n$$\np=f\\left(f\\left(x_{1}, f\\left(x_{2}, x_{3}\\right)\\right), f\\left(x_{4}, f\\left(x_{5}, x_{6}\\right)\\right)\\right)\n$$\n\n## 3. Our Approach\n\nIn this section, we describe the technical details of $\\operatorname{CRvNN}$. However, before getting into the exact details, we first construct a new but equivalent reformulation of RvNN. This will make it easier to connect CRvNN to RvNN. ### 3.1. Recursive Neural Network: Reformulation\n\nWe now present the aforementioned reformulation of RvNN. Below we describe different components of our reformulated RvNN individually. ### 3.1.1. COMPOSITION FUNCTION\n\nThe notion of composition function is the same as in $\\S 2.1$. Example. Given a composition function (or recursive cell) $f\\left(r_{1}, r_{2}\\right)$, if one input $\\left(r_{1}\\right)$ represents $x_{1}$, and another input $\\left(r_{2}\\right)$ represents $\\left(x_{2}, x_{3}\\right)$, then the output parent will repre$\\operatorname{sent}\\left(x_{1},\\left(x_{2}, x_{3}\\right)\\right)$\n\n### 3.1.2. EXISTENTIAL PROBABILITY\n\nIn an RvNN, once two child representations are composed into a parent, the child representations themselves will no longer be needed. The unneeded child representations can be treated as \"non-existent\". Based on this idea, we introduce the notion of \"existential probability.\"\n\nDefinition 1. We define the notion of an \"existential probability\" $e_{i}$ to denote whether a representation $r_{i}$ requires further processing ( $e_{i}=1$ ) or it should be treated as \"nonexisting\" and hence ignored ( $e_{i}=0$ ). Every position has an existential probability, which initially is 1 for all positions. For now, we only consider binary values $e_{i} \\in\\{0,1\\}$. Example. If $r_{i}$ from position $i$ is composed with $r_{j}$ from position $j$, then we may update position $j$ with the composed parent representation of $\\left(r_{i}, r_{j}\\right)$ and set the existential probability of position $i$ as 0 . Typically, we can just remove the unnecessary representations. However, in this formulation, we keep them with an existential probability. This is a key change that enables us to bring a continuous relaxation to RvNNs as we will discuss later. ### 3.1.3. DECISION FUNCTION\n\nDefinition 2. We define a decision function $D$ as a function dedicated to make structural decisions about which positions to compose together into a parent representation given the model state. This definition generalizes both vanilla RvNNs and architectures such as Gumbel-Tree LSTMs. In case of vanilla RvNNs, $D$ can be conceived of as a trivial algorithm which makes the appropriate decision at every step by simply looking at the ground truth. In case of Gumbel-Tree LSTM, the function $D$ can represent its scoring function that scores all candidate representations to be chosen for composition in a particular step. In our generalized formulation, $D$ makes structural decision by assigning a composition probability $\\left(c_{i}\\right)$ to every position $i$ in a sequence $r_{1: n}$. This\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_994d3d9023d7aea12bd9g-03.jpg?height=706&width=1568&top_left_y=211&top_left_x=254)\n\nTable 1. Simulation of an RvNN based on our new formulation. We use a strikethrough to illustrate \"non-existing\" representations ( 0 existential probability) as they are ignored for further computation. We use $\\emptyset$ for composition scores of non-existing representations. In each iteration, we bold the child with a composition probability of 1 . We also use $\\mathrm{a} \\rightarrow$ symbol to indicate that the child will be sent over to the first right position with existential probability of 1 so that a composed parent representation can be formed at that position. The composition probabilities are predicted in each step by the decision function $D$. composition probability determines which positions are to be replaced by the parent representation. For now, we only consider $c_{i} \\in\\{0,1\\}$. Note, unlike Gumbel-Tree LSTM, in our formulation multiple positions can be chosen for composition. In $\\S 3.1 .5$, we discuss the exact recursive rules to update the representations based on the composition probabilities. ### 3.1.4. Left and Right FunCtions\n\nDefinition 3. Given a sequence of values $v_{1: n}=$ $\\left(v_{1}, v_{2}, \\cdots, v_{n}\\right)$, we define a function left which takes as input some value $v_{i}$ and returns the closest existing left value $v_{j}$ (i.e., $\\left.v_{j}=\\operatorname{left}\\left(v_{i}\\right)\\right)$ such that $e_{j}=1$ and $\\forall l, j<l<i, e_{l}=0$. Note that since position $i-1$ can have an existential probability $e_{i-1}=0$, left $\\left(v_{i}\\right)$ is not always $v_{i-1}$. In a similar vein, we also define a function right which takes as input some value $v_{i}$ and returns the closest existing right value $v_{j}$ (i.e., $\\left.v_{j}=\\operatorname{right}\\left(v_{i}\\right)\\right)$ such that $e_{j}=1$ and $\\forall l, i<l<j, e_{l}=0$. Similar to the left function, $\\operatorname{right}\\left(v_{i}\\right)$ is not always $v_{i+1}$. Note that the sequence $v_{1: n}$ could be any sequence including a sequence of representations $\\left(r_{1: n}\\right)$ or a sequence of composition probabilities $\\left(c_{1: n}\\right)$. ### 3.1.5. RECURSIVE UPDATE\n\nNow, we describe how the above components come together in each recursive update of our reformulated RvNN. We can formally express the rule for recursively updating some representation at position $i, r_{i}$ in the recursive step $k$ as:\n\n$$\nr_{i}^{k+1}=\\operatorname{left}\\left(c_{i}\\right) \\cdot f\\left(\\operatorname{left}\\left(r_{i}^{k}\\right), r_{i}^{k}\\right)+\\left(1-\\operatorname{left}\\left(c_{i}\\right)\\right) \\cdot r_{i}^{k}\n$$\n\nwhere $f$ refers to the composition function (recursive cell) as before and $c_{i}$ is the composition probability (as predicted by the function $D$ ) at any position $i$. Note that after this point, if $\\operatorname{left}\\left(c_{i}\\right)=1$, left $\\left(r_{i}^{k}\\right)$ is no longer needed because it has been composed together with $r_{i}^{k}$ and the composed parent representation already exists in position $i$ (as $r_{i}^{k+1}$ ) according to the above rule. Thus, when left $\\left(c_{i}\\right)=c_{j}=1$ we can set the existential probability $\\left(e_{j}\\right)$ at position left $(i)=j$ as 0 . Thus, we can express the update rule for $e_{j}$ as:\n\n$$\ne_{j}^{k+1}=e_{j}^{k} \\cdot\\left(1-c_{j}\\right)\n$$\n\nIn Table 1, we show how we can simulate an RvNN based on the rules as laid out above using a sequence $x_{1: 6} .{ }^{3}$\n\n### 3.2. Towards Continuous RvNN\n\nIn the above sections we consider only discrete binary values for composition probability and existential probability. As such, it is still a \"hard RvNN\" with a discrete order of composition. To transform it into a Continuous RvNN (soft RvNN), we simply incorporate a continuous relaxation to the decision function $D$ allowing it to predict composition\n\n[^2]probabilities $\\left(c_{i}\\right)$ in the interval $[0,1]$. Similarly, we also allow existential probabilities $\\left(e_{i}\\right)$ to be in $[0,1]$. As a result of this relaxation, we can simply use a neural network with sigmoid activation for $D$. Given that we accommodate for continuous values, we do not have to force binary decisions using some form of reparameterization or gradient approximation. Moreover, we can directly use the recursive update rules as defined in Eq. 3 and Eq. 4 without any changes because they are already compatible with real-valued $c_{i}$ and $e_{i}$. Below, we discuss our re-definition of left and right functions (neighbor retriever functions) under this new context. ### 3.2.1. NEIGHBOR RETRIEVER Functions\n\nIn our reformulation presented in $\\S 3.1$, an active role is played by the left and right functions. However, the previous definition (Def. 3) was made under the condition that the existential probabilities can only take binary values. Since now we have real-valued probabilities, the notion of \"closest existing left (or right) value\" is not well defined because existential probabilities can have non-binary values. Precisely, both left and right functions are important for CRvNN, and either of the functions can be re-purposed as the other with minimal changes. Thus, for now, we only focus on the right function which can be easily adapted into a left function. Definition 4. Given a sequence of values $v_{1: n}=$ $\\left(v_{1}, v_{2}, \\cdots, v_{n}\\right)$, and for every $v_{i}$, given a sequence of probabilities $p_{i 1: i n}=\\left(p_{i 1}, p_{i 2}, \\cdots, p_{i n}\\right)$ such that $p_{i j}$ indicates the probability that $v_{j}$ is the closest existing value right to $v_{i}$, we define the function right as: $\\operatorname{right}\\left(v_{i}\\right)=$ $\\sum_{j=1}^{n} p_{i j} \\cdot v_{j}$. Essentially, the function right returns something analogous to an expected value of the immediate right existing representation.",
    "contrnn-3": "Note, however, that $\\sum_{j} p_{i j}$ is not necessarily 1 , rather $\\leq 1$. This is because there is another possibility that there is no existing representation at the right at all. In our implementation, we transform the existential probabilities $e_{1: n}=\\left(e_{1}, e_{2}, \\cdots, e_{n}\\right)$ into the sequence $p_{i 1: i n}$. Below, we formulate the precise rules that we use:\n\n$$\np_{i j}= \\begin{cases}0 & j \\leq i \\\\ e_{j} & \\sum_{l=i+1}^{j} e_{l} \\leq 1 \\\\ \\max \\left(0,1-\\sum_{l=i+1}^{j-1} e_{l}\\right) & \\sum_{l=i+1}^{j} e_{l}>1\\end{cases}\n$$\n\nThat is, we re-purpose the existential probabilities $e_{j}$ as $p_{i j}$. Naturally, we first set all non-right values from $i$ to have 0 probability. Then we adjust them so that they sum up to $\\leq 1$ by zeroing out $p_{i j}$ for any values after the position where the accumulated probability exceeds 1. To an extent, however, Eq. 5 is an engineering choice rather\n\n```\nAlgorithm 1 Continuous Recursive Neural Network\n    Input: data \\(x_{1: n}=\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)\\)\n    \\(r_{i}^{1} \\leftarrow\\) leafTransform \\(\\left(x_{i}\\right) \\quad\\{i=1, \\cdots, n\\}\\)\n    Initialize \\(e_{i}^{1} \\leftarrow 1\\)\n    for \\(k=1\\) to \\(n-1\\) do\n        \\(c_{1: n} \\leftarrow\\) decisionFunction \\(\\left(r_{1: n}^{k}\\right)\\)\n        \\(\\alpha \\leftarrow \\operatorname{left}\\left(c_{i}\\right)\\)\n        \\(h \\leftarrow \\operatorname{left}\\left(r_{i}^{k}\\right)\\)\n        \\(r_{i}^{k+1} \\leftarrow \\alpha \\cdot \\operatorname{compose}\\left(h, r_{i}^{k}\\right)+(1-\\alpha) \\cdot r_{i}^{k}\\)\n        \\(e_{i}^{k+1} \\leftarrow e_{i}^{k} \\cdot\\left(1-c_{i}\\right)\\)\n        if dynamicHalt \\(\\left(e_{1: n}^{k+1}\\right)\\) then\n            break\n        end if\n    end for\n```\n\nthan being completely mathematically grounded. A more mathematically grounded choice would be to set $p_{i j}=$ $e_{j} \\cdot \\Pi_{l=i+1}^{j-1}\\left(1-e_{l}\\right)$. We created a log-sum-exp based formulation for the equation (to avoid potential instabilities due to cumulative multiplication) as:\n\n$$\np_{i j}=e_{j} \\cdot \\exp \\left(\\sum_{l=i+1}^{j-1} \\log \\left(1-e_{l}\\right)\\right)\n$$\n\nWe did some experiments with this on the synthetic logical inference (Bowman et al., 2015b) dataset. It performed reasonably but slightly worse than when we use Eq. 5. Overall both formulations (Eq. 5 or Eq. 6) should be equivalent in the discrete setting, and thus, both can push the model to approximate a discrete RvNN.",
    "contrnn-4": "Regardless, for focus, we only consider Eq. 5 in the rest of the paper. Given these formulations we can further generalize both left and right functions. Definition 5. Given a sequence $v_{1: n}=\\left(v_{1}, v_{2}, \\cdots, v_{n}\\right)$, we define $l e f t_{m}\\left(v_{i}\\right)$ as the function to retrieve the expected value in the $m^{t h}$ left position from $i$. Formally:\n\n$$\n\\begin{gathered}\n\\operatorname{left}_{1}\\left(r_{i}\\right)=\\operatorname{left}\\left(r_{i}\\right) \\\\\n\\text { left }_{m}\\left(r_{i}\\right)=\\text { left }\\left(\\operatorname{left}_{m-1}\\left(r_{i}\\right)\\right)\n\\end{gathered}\n$$\n\nSimilarly, we can define right $_{m}$.",
    "contrnn-5": "In the following subsection, we describe the main algorithm. ### 3.3. Continuous RvNN: Algorithm\n\nThe Continuous RvNN model is presented in Algorithm 1. We already explained the recursive update rules.",
    "contrnn-6": "Next, we describe the implementation details of all the functions in the algorithm. ### 3.3.1. LEAF TRANSFORMATION\n\nWe use the function leafTransform simply for an initial transformation of the embedded sequence. This can be ex-\n\n```\nAlgorithm 2 dynamicHalt\n    Input: data \\(e_{1: n}=\\left(e_{1}, e_{2}, \\cdots, e_{n}\\right)\\)\n    if \\(e_{i}<\\epsilon\\) then\n        \\(d_{i}=0\\)\n    \\(\\{i=1, \\cdots, n\\}\\)\n    else\n        \\(d_{i}=1\\)\n    end if\n    if \\(\\sum_{i=1}^{n} d_{i}=1\\) then\n        return True\n    else\n        return False\n    end if\n```\n\npressed as:\n\n$$\nr_{i}^{1}=L N\\left(W x_{i}+b\\right)\n$$\n\nHere, $x_{i} \\in \\mathbb{R}^{d_{\\text {embed }} \\times 1}, W \\in \\mathbb{R}^{d_{h} \\times d_{\\text {embed }}}$, and $b \\in \\mathbb{R}^{d_{h} \\times 1}$.",
    "contrnn-7": "$L N$ refers to layer normalization (Ba et al., 2016). ### 3.3.2. DECISION FUNCTION\n\nThe decision function is used to predict the composition probabilities $c_{i}$. For this, we take into account the local context using a convolutional layer. However, we want the \"true local context\" for which we need to use the left $t_{m}$ and right $_{m}$ functions, as defined above. Given a locality window size of $2 \\cdot \\tau+1$, we use the following function to get the initial un-normalized scores $u_{i}$ for computing the composition probabilities $c_{i}$ :\n\n$$\n\\begin{gathered}\nu_{i}=W_{2} \\operatorname{GeLU}\\left(\\sum_{j=-\\tau}^{\\tau} W_{\\text {conv }}^{j+\\tau} L\\left(r_{i}, j\\right)+b_{\\text {conv }}\\right)+b_{2} \\\\\nL\\left(r_{i}, j\\right)= \\begin{cases}l e f t_{-j}\\left(r_{i}\\right) & j<0 \\\\\nr_{i} & j=0 \\\\\n\\operatorname{right}_{j}\\left(r_{i}\\right) & j>0\\end{cases}\n\\end{gathered}\n$$\n\nHere, we have a set of convolutional kernel weights $\\left\\{W_{\\text {conv }}^{0}, W_{\\text {conv }}^{1}, \\cdots, W_{\\text {conv }}^{2 . \\tau}\\right\\}$, where any $W_{\\text {conv }}^{l} \\in$ $\\mathbb{R}^{d_{h} \\times d_{h}} ; L\\left(r_{i}, j\\right) \\in \\mathbb{R}^{d_{h} \\times 1}, W_{2} \\in \\mathbb{R}^{1 \\times d_{h}}, b_{\\text {conv }} \\in \\mathbb{R}^{d_{h}}$ and, $b_{2} \\in \\mathbb{R}^{1}$. Note that $u_{i}$ is now an un-normalized scalar real value. To turn it into a probability we can use sigmoid:\n\n$$\nc_{i}=\\operatorname{sigmoid}\\left(u_{i}\\right)=\\frac{\\exp \\left(u_{i}\\right)}{\\exp \\left(u_{i}\\right)+1}\n$$\n\nThe use of sigmoid allows multiple positions to have high composition probabilities $\\left(c_{i}\\right)$. As such, multiple parents can be composed in the same recursion resulting in the parallelism that we alluded to before. However, pure sigmoid in this form is very unconstrained. That is, there is no constraint that prevents multiple contiguous (as defined by left and right functions) positions from having high composition probabilities $\\left(c_{i}\\right)$. Concurrently composing contiguous representations with their expected right representation will violate the tree-structure. It can also cause information being propagated to positions that, at the same time, lose their own existential probability due to propagating their information rightwards. As such, some propagated information can henceforth be ignored due to landing in an area that loses its existential probability. Thus, to prevent contiguous positions from having high existential probabilities, we modulate the sigmoid formulation with the scores from the neighbors as follows:\n\n$$\nc_{i}=\\frac{\\exp \\left(u_{i}\\right)}{\\exp \\left(u_{i}\\right)+\\exp \\left(\\operatorname{left}\\left(u_{i}\\right)\\right)+\\exp \\left(\\operatorname{right}\\left(u_{i}\\right)\\right)+1}\n$$\n\nWe refer to this function as modulated sigmoid.",
    "contrnn-8": "### 3.3.3. COMPOSITION FunCTION\n\nWe use the same recursive gated cell as introduced by Shen et al. (2019a) for our composition function. This is originally inspired from the feedforward functions of Transformers (Vaswani et al., 2017). $$\n\\left[\\begin{array}{l}\nz_{i} \\\\\nh_{i} \\\\\nc_{i} \\\\\nu_{i}\n\\end{array}\\right]=W_{2} \\operatorname{GeLU}\\left(W_{1}^{\\text {Cell }}\\left[\\begin{array}{c}\n\\operatorname{left}\\left(r_{i}\\right) \\\\\nr_{i}\n\\end{array}\\right]+b_{1}\\right)+b_{2}\n$$\n\n$o_{i}=L N\\left(\\sigma\\left(z_{i}\\right) \\odot \\operatorname{left}\\left(r_{i}\\right)+\\sigma\\left(h_{i}\\right) \\odot r_{i}+\\sigma\\left(c_{i}\\right) \\odot u_{i}\\right)$\nHere, $\\sigma$ is sigmoid; $o_{i}$ is the output parent $\\in \\mathbb{R}^{d_{h} \\times 1}$; $r_{i}$,left $\\left(r_{i}\\right) \\in \\mathbb{R}^{d_{h} \\times 1} ; W_{1}^{\\text {cell }} \\in \\mathbb{R}^{d_{\\text {cell }} \\times 2 \\cdot d_{h}} ; b_{1} \\in$ $\\mathbb{R}^{d_{\\text {cell }} \\times 1} ; W_{2} \\in \\mathbb{R}^{d_{h} \\times d_{\\text {cell }}} ; b_{1} \\in \\mathbb{R}^{d_{h} \\times 1}$.",
    "contrnn-9": "Different from (Shen et al., 2019a), we use GeLU (Hendrycks \\& Gimpel, 2016) instead of ReLU as the activation function. ### 3.3.4. DyNAMIC HALT\n\nIdeally, CRvNN can learn to simultaneously process all children at the same hierarchy level. Thus, CRvNN will only need to recurse over the tree depth. This means that we need a mechanism to detect if the induced tree-depth has been traversed and if we can halt. Following our framework, we only need to look at the sequence of existential probabilities $e_{1: n}=\\left(e_{1}, e_{2}, \\cdots, e_{n}\\right)$. Near the ideal time for halting, all existential probabilities except the last should be close to 0 ( 0 indicates that it is no longer in need of further processing). The last position has nothing to the right of it to be composed. So we enforce the last position to always have 0 composition probability and thus, 1 existential probability. Overall, to determine when to halt, we simply check if all existential probabilities except the last are less than some small threshold $\\epsilon$. Algorithm 2 shows an implementation of this mechanism. | Model | Number of Operations |  |  |  |  |  | Systematicity |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 7 | 8 | 9 | 10 | 11 | 12 | A | B | C |\n| (Sentence representation models + ground truths) |  |  |  |  |  |  |  |  |  |\n| Tree-LSTM* | 94 | 92 | 92 | 88 | 87 | 86 | 91 | 84 | 76 |\n| Tree-Cell* | 98 | 96 | 96 | 95 | 93 | 92 | 95 | 95 | 90 |\n| Tree-RNN* | 98 | 98 | 97 | 96 | 95 | 96 | 94 | 92 | 86 |\n| (Inter-sentence interaction models) |  |  |  |  |  |  |  |  |  |\n| Transformer* | 51 | 52 | 51 | 51 | 51 | 48 | 53 | 51 | 51 |\n| Universal Transformer* | 51 | 52 | 51 | 51 | 51 | 48 | 53 | 51 | 51 |\n| (Sentence representation models) |  |  |  |  |  |  |  |  |  |\n| LSTM $^{*}$ | 88 | 84 | 80 | 78 | 71 | 69 | 84 | 60 | 59 |\n| RRNet* | 84 | 81 | 78 | 74 | 72 | 71 | - | - | - |\n| ON-LSTM* | 91 | 87 | 85 | 81 | 78 | 75 | 70 | 63 | 60 |\n| Ordered Memory* | $98_{0}$ | $97_{4}$ | $96_{5}$ | $94_{8}$ | $93_{5}$ | $92_{11}$ | 94 | 91 | 81 |\n| (Our model) |  |  |  |  |  |  |  |  |  |\n| CRvNN | $98_{1}$ | 973 | $96_{2}$ | $95_{6}$ | 948 | $93_{5}$ | $99_{1}$ | 983 | $92_{22}$ |\n\nTable 2. Accuracy on the Synthetic Logical Inference dataset (Bowman et al., 2015b) for different number of operations after training on samples with $\\leq 6$ operations.",
    "contrnn-10": "We also show results of testing systematicity of the models in specially designed splits: A, B, and C. * indicates that the results were taken from (Shen et al., 2019a). Our models were run 5 times on different random seeds. We show its mean and standard deviation. Specifically, subscript represents standard deviation. As an example, $90_{1}=90 \\pm 0.1$\n\n### 3.4. Extra components\n\nHere we describe some extra, potentially less essential, components that we use in our implementation of CRvNN. ### 3.4.1. SPECIAL EMBEDDINGS\n\nWe prepend all sequences with a special $<$ START $>$ embedding, and we append all sequences with a special $<$ END $>$ embedding. Both are trainable vectors of size $d_{\\text {embed }}$. These embeddings are enforced to always have a composition probability 0 and an existential probability of 1 . When constructing the local context, these embeddings can provide more explicit information about how close to the starting or ending boundary a representation $r_{i}$ is. ### 3.4.2. TRANSITION FEATURES\n\nIn an attempt to enhance the decision function, we also concatenate features to provide the explicit information if a position $i$ was composed and updated in the last recursion or not. For that, we construct a set of features $T \\in \\mathbb{R}^{d_{s}}$ as:\n\n$$\nT=\\operatorname{left}\\left(c_{i}\\right) \\cdot C+\\left(1-\\operatorname{left}\\left(c_{i}\\right)\\right) \\cdot \\widehat{C}\n$$\n\nHere, $C$ and $\\widehat{C}$ are both trainable vector parameters $\\in \\mathbb{R}^{d_{s}}$. $C$ can be conceived to be representing a set a features indicating that in the last iteration the concerned position was composed with the left child and updated, whereas $\\widehat{C}$ can be conceived to be representing the contrary. Here, we use the $\\operatorname{le} f t\\left(c_{i}\\right)$ values from the last recursion (initially 0 ). ### 3.4.3. Halt Penalty\n\nAs discussed before, during halting, in the sequence of existential probabilities ( $e_{1: n}$ ) all but the last existential probabilities should be close to 0 . While this is ideally supposed to happen if all the positions are processed properly, there is no guarantee that it will indeed happen. To encourage this property, we use an auxiliary loss $\\left(A\\left(e_{1: n}\\right)\\right)$, which we define as:\n\n$$\nA(e)=-\\log \\left(\\frac{e_{n}}{\\sum_{j=1}^{n} e_{j}}\\right)\n$$\n\nThis can be conceived as cross entropy between the ideal final sequence of existential probabilities (all but the last being 0 ) and the actual final sequence of the same after normalization. The overall optimization function can be formulated as:\n\n$$\n\\min _{\\theta} \\mathcal{L}(\\theta)+\\gamma \\cdot A(e)\n$$\n\nwhere $\\mathcal{L}(\\theta)$ is the main cross entropy loss and $\\gamma$ is a Lagrangian multiplier for the auxiliary objective.",
    "contrnn-11": "## 4. Experiments and Results\n\nIn this section, we discuss our experiments and results. We evaluate our model on logical inference (Bowman et al., 2015b), list operations (ListOps) (Nangia \\& Bowman, 2018), sentiment analysis-two datasets, SST2 and SST5 (Socher et al., 2013), and natural language inferencetwo datasets, SNLI (Bowman et al., 2015a) and MNLI (Williams et al., 2018b). For implementation details, refer to the appendix. | Model | Sequence length ranges (ListOps) |  |  |  |  |  |  |  |\n| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |\n|  | $200-300$ | $300-400$ | $400-500$ | $500-600$ | $600-700$ | $700-800$ | $800-900$ | $900-1000$ |\n| CRvNN | $98.51 \\pm 1.1$ | $98.46 \\pm 1.3$ | $98.04 \\pm 1.3$ | $97.95 \\pm 1.1$ | $97.17 \\pm 1.6$ | $97.84 \\pm 1.7$ | $96.94 \\pm 1.6$ | $96.78 \\pm 1.9$ |\n\nTable 3. Extrapolation of CRvNN on ListOps after training on samples of length $\\leq 100$. We used the ListOps extrapolation test set from (Havrylov et al., 2019). | Model | Accuracy |\n| :--- | :--- |\n| (Models with ground truth) |  |\n| Tree-LSTM $\\ddagger$ | 98.7 |\n| (Models without ground truth) |  |\n| Transformer* | $57.4 \\pm 0.4$ |\n| Universal Transformer* | $71.5 \\pm 7.8$ |\n| LSTM $\\dagger$ | $71.5 \\pm 1.5$ |\n| RL-SPINN $\\dagger$ | $60.7 \\pm 2.6$ |\n| Gumbel-Tree LSTM $\\dagger$ | $57.6 \\pm 2.9$ |\n| (Havrylov et al., 2019) $\\dagger$ | $99.2 \\pm 0.5$ |\n| Ordered Memory* | $99.97 \\pm 0.014$ |\n| (Our model) | $99.6 \\pm 0.3$ |\n| CRvNN |  |\n\nTable 4. Accuracy on ListOps. Results with * were taken from (Shen et al., 2019a). $\\ddagger$ indicates that the results were taken from (Nangia \\& Bowman, 2018). $\\dagger$ indicates that the results were taken from (Havrylov et al., 2019). Our models were run 5 times on different random seeds.",
    "contrnn-12": "We show its mean and standard deviation. ### 4.1. Logical Inference\n\nIn the logical inference dataset, we focus on two particular generalization properties separately - length generalization and compositional generalization (i.e., systematicity). We compare CRvNN with Tree-LSTM (Tai et al., 2015), Tree-Cell (Shen et al., 2019a) Tree-RNN (Bowman et al., 2015b), Tranformer (Vaswani et al., 2017), Universal Transformer (Dehghani et al., 2019), LSTM (Hochreiter \\& Schmidhuber, 1997), RRNet (Jacob et al., 2018), ON-LSTM (Shen et al., 2019b), Ordered Memory (Shen et al., 2019a) (see Table 2). ### 4.1.1. LENGTH GENERALIZATION\n\nTo evaluate CRvNN for length generalization, as in prior work, we train the model only on samples with $\\leq 6$ operations whereas we test it on samples with higher unseen number of operations $(\\geq 7)$. In Table 2, discounting the performance of RvNN-based models with ground-truth access, our model, along with ordered memory, achieves the best performance on length generalization.",
    "contrnn-13": "### 4.1.2. SySTEMATICITY\n\nFollowing Shen et al. (2019a), we create three different train-test splits on the logical inference dataset: A, B, and C (with increased level of difficulty from A to $\\mathrm{C}, \\mathrm{A}$ being the easiest). For each split, we filter all samples with a specific compositional pattern from the training set and put\n\n| Model | SST2 | SST5 | SNLI | MNLI |\n| :--- | :--- | :--- | :--- | :--- |\n| RL-SPINN $\\ddagger$ | - | - | 82.3 | 67.4 |\n| Gumbel-Tree-LSTM $\\dagger \\dagger$ | 90.7 | 53.7 | 85.6 | - |\n| Gumbel-Tree-LSTM $\\ddagger$ | - | - | 83.7 | 69.5 |\n| Gumbel-Tree-LSTM $\\dagger$ | $90.3_{5}$ | $51.6_{8}$ | $84.9_{1}$ | - |\n| (Havrylov et al., 2019) $\\dagger$ | $90.2_{2}$ | $51.5_{4}$ | $85.1_{2}$ | $70.7_{3}$ |\n| Ordered Memory* | 90.4 | 52.2 | - | - |\n| CRvNN | $88.3_{6}$ | $51.4_{13}$ | $85.1_{2}$ | $72.9_{2}$ |\n\nTable 5. Accuracy on multiple natural language datasets. * indicates that the results were taken from (Shen et al., 2019a). $\\dagger$ indicates that the results were taken from (Havrylov et al., 2019). $\\ddagger$ indicates that the results were taken from (Williams et al., 2018a). $\\dagger \\dagger$ indicates that the results were taken from (Choi et al., 2018). Our models were run 5 times (except, on MNLI it was run 3 times) on different random seeds.",
    "contrnn-14": "We show its mean and standard deviation. Subscript represents standard deviation. E.g., $90_{1}=90 \\pm 0.1$. them into the test set. Then we check whether our model can generalize to unseen patterns (unseen combinations of operands and operators). In set A , we filter samples according to the pattern $*(\\operatorname{and}(\\operatorname{not} a)) *$, in B, we filter according to the pattern $*(\\operatorname{and}(\\operatorname{not} *)) *$, and in C, we filter according to $*(\\{$ and, or $\\}($ not $*)) *$. As evident in Table 2, CRvNN exhibits exceptional capability for compositional generalization and outperforms all prior reported results. ### 4.2. ListOps\n\nListOps is a challenging synthetic task which explicitly demands capability for hierarchical modeling. Several prior latent-tree models were shown to perform poorly on it (Nangia \\& Bowman, 2018). As shown in Table 4, CRvNN gets close to perfect accuracy, demonstrating its capability to capture underlying structures without structural supervision. We also show the length generalization capability of CRvNN on ListOps in Table 3. CRvNN still achieves high accuracy in much higher sequence lengths ( $400-1000$ ) even when it is originally trained on sequences of lengths $\\leq 100$. ### 4.3. Natural Language Datasets\n\nIn Table 5, we also evaluate CRvNN on natural language datasets (SST2, SST5, SNLI, MNLI). Consistent with prior work (Havrylov et al., 2019), for MNLI we augment the training data with SNLI training data and report the test\nresult on the matched test set. In the real-world tasks, CRvNN obtains mixed results, but overall the results are comparable to prior work in the similar latent-tree context. Particularly, CRvNN performs comparatively weak on SST2, but on contrary it performs significantly better than prior work (besides OM) on MNLI, which is a harder task. We also ran a CRvNN model with an enforced balanced tree structure induction on MNLI but its performance was comparatively worse $(71.5 \\pm 0.4)$. For Gumbel-Tree LSTM Choi et al. (2018), we show results reported from different works because the results fluctuate from one paper to another (Havrylov et al., 2019; Williams et al., 2018a). ## 5. Analysis\n\nIn this section, we contrast CRvNN with Ordered Memory in terms of speed and show an ablation study of CRvNN. ### 5.1. Speed Test\n\nOrdered memory (OM) is a close competitor for CRvNN. Both of the models are end-to-end differentiable. Both can simulate an RvNN without ground truth structures. CRvNN neither consistently nor conclusively outperforms OM. However, there is one crucial advantage for CRvNN. Particularly, it can achieve a degree of parallelism by processing multiple positions concurrently. At the same time, it can do an early-exit using dynamic halt. In contrast, OM not only has an outer loop over the whole sequence length, but it also has an inner loop where it recurse over its memory slots, adding significant overhead. The inner loop recursion of OM involves heavy sequential matrix operations. As such, in practice, CRvNN can be much faster than OM. In Table 6, we compare the training run time for both models. We generated synthetic ListOps samples for different ranges of sequence lengths. For each range of sequence lengths, we trained both the models on 50 samples for 1 epoch and 1 batch size on an AWS P3.2\u00d7 instance (Nvidia V100). As we can see from the table, CRvNN is substantially faster than OM. Although in other settings with different tasks and different batch sizes, the gap between the speed of OM and CRvNN may not be as high as in Table 6 , we still noticed CRvNN to be roughly $2-4$ times faster than OM even when using higher batch sizes for OM (taking advantage of its lower memory complexity). We also tried running CYK-LSTM (Maillard et al., 2019) but faced memory issues when running it for longer sequences ( $>200$ length).",
    "contrnn-15": "### 5.2. Ablation Study\n\nIn Table 7, we show an ablation study on CRvNN. We find that replacing modulated sigmoid with simple sigmoid significantly degrades and destabilizes the performance in ListOps. Removing \"structure\" or the structural bias by simply using the composition function as left-to-right recurrent network, again, significantly harms the performance of the model. The gated recursive cell itself is also crucial for the performance. Replacing it with an LSTM cell (Hochreiter \\& Schmidhuber, 1997) causes severe degradation. To an extent, this is consistent with the findings of Shen et al. (2019a). They replaced the gated recursive cell in ordered memory (Shen et al., 2019a) with an RNN cell, and observed substantial degradation. However, even with an LSTM cell, CRvNN performs better, in logical inference, than any of the other reported models which do not have the gated recursive cell. Replacing the activation function, removing the transition features (\u00a73.4.2), or eliminating the halt penalty (\u00a73.4.3) makes little difference. ### 5.3. Parsing Results\n\nThe internal composition scores of each layer of CRvNN can be used to parse induced trees. While there can be multiple ways to convert the composition scores to extract trees, one method is to simply treat any particular position of a given sequence at a particular iteration in the algorithm as having a composition probability of 1 whenever the cumulative composition probability at that position over all the iterations thus far is $\\geq 0.5$. Otherwise, we treat the position at the particular iteration as having a composition probability of 0 . Once the scores are binarized, it is straightforward to extract the trees following the ideas discussed in \u00a73.1. Table 1 (in \u00a73.1) shows an example on how binary and discrete composition probabilities relate to a particular structure of composition. In Table 8, we show parsing examples obtained with CRvNN using the above procedure. ## 6. Related Work\n\nSeveral early approaches focused on adapting neural models for simulating pushdown automata or for grammar induction in general (Sun, 1990; Giles et al., 1990; Das et al., 1992; Mozer \\& Das, 1993; Zeng et al., 1994; Grefenstette et al., 2015). Also, recently, there are multiple works that focus on structure induction based on language modeling objectives (Yogatama et al., 2018; Shen et al., 2018; 2019b; Li et al., 2019; Kim et al., 2019; Drozdov et al., 2019; 2020; Shen et al., 2021). In this work, we focus on models with structural bias that are used for general downstream tasks including NLP tasks such as classification and NLI. Pollack (1990) presented RvNN as a recursive architecture to compose tree and list-like data-structures. Socher et al. (2010; 2013) aimed at capturing syntactic and semantic patterns of linguistic phrases and showed the effectiveness of RvNNs for sentiment analysis. Originally, RvNNs relied on external parsers to provide tree-structured inputs. Several works focused on augmenting RvNNs so\n\n| Model | Sequence length ranges |  |  |  |  |  |\n| :--- | ---: | ---: | ---: | ---: | ---: | :---: |\n|  | $81-100$ | $101-200$ | $201-500$ | $501-700$ | $701-1000$ |  |\n| Ordered Memory | 3.28 min | 5.54 min | 13.23 min | 25 min | 35.34 min |  |\n| CRvNN | 0.20 min | 0.52 min | 0.30 min | 0.38 min | 1.08 min |  |\n\nTable 6. Training time taken by Ordered Memory and CRvNN models to be run on 50 samples on various sequence length ranges. We used the publicly available code to run Ordered Memory. | Model | Logical inference <br> Number of Operations |  |  | ListOps |\n| :---: | :---: | :---: | :---: | :---: |\n|  | 10 | 11 | 12 |  |\n| CRvNN | $95.0_{6}$ | $93.7_{8}$ | $93.1_{5}$ | $99.6^{3}$ |\n| - modulated sigmoid | $95.0_{3}$ | $94.3_{4}$ | $92.8_{5}$ | $90.8_{163}$ |\n| - transition features | $95.0_{8}$ | $94.4_{7}$ | $92.1_{5}$ | $99.4_{5}$ |\n| - structure | $87.2_{11}$ | $86.1_{10}$ | $80.9_{20}$ | $83.4_{11}$ |\n| - halt penalty | $95.2^{5}$ | $94.6_{4}$ | $93.1_{7}$ | $99.4_{2}$ |\n| - GeLU + ReLU | $95.1_{7}$ | $94.1_{5}$ | $92.7_{6}$ | $99.2_{4}$ |\n| - Cell + LSTMCell | $89.4_{7}$ | $89.2^{7}$ | $86.5_{11}$ | $71.2_{41}$ |\n\nTable 7. Cell represents Gated Recursive Cell.",
    "contrnn-16": "All the models were run 5 times on different random seeds. Subscript represents standard deviation. For example, $90_{1}=90 \\pm 0.1$\n\n| Parsing Examples |\n| :--- |\n| $(((($ i did $)$ not) like) $((($ a single $)$ minute) $(($ of this $)($ film . $))))$ |\n| $(($ roger dodger) $(($ is (one of ) (the most $)))$ |\n| (compelling (variations (of (this theme $)))))$ |\n\nTable 8. Parsing examples obtained using CRvNN trained on MNLI. The example sentences are from (Socher et al., 2013). that they can automatically induce the tree structure from plain text. To this end, Le \\& Zuidema (2015) used a chart-based parsing algorithm (Cocke-Younger-Kasami algorithm (Sakai, 1961)) in a neural framework with a convolutional composition function. Maillard et al. (2019) extended it with an attention mechanism to pool over multiple subtree candidates and a Tree-LSTM (Tai et al., 2015) composition function. Bowman et al. (2016) presented a stack-augmented RNN (SPINN) to simulate the functions of a Tree-RNN based on the principles of shiftreduce parsing (Sch\u00fctzenberger, 1963; Knuth, 1965) but relied on ground truth structure annotations for supervision. Yogatama et al. (2017) augmented SPINN with reinforcement learning (RL-SPINN) allowing unsupervised structure induction. Maillard \\& Clark (2018) enhanced shift-reduce parsing-based stack-RNNs with beam search. Munkhdalai \\& Yu (2017) introduced Neural Tree Indexers and gained strong performance by using full binary trees on natural language tasks. Similarly, Shi et al. (2018) showed that trivial trees (e.g., binary balanced trees) can be used to get competitive performances on several natural language tasks. Dong et al. (2019) proposed a neural model to learn boolean logic rules and quantifications to reason over re- lational data with multiple arities. Choi et al. (2018) used Gumbel Softmax (Jang et al., 2017) to adaptively make discrete decisions at every recursive step in choosing a parent to compose with a tree-LSTM. Jacob et al. (2018) proposed a combination of recursive and recurrent neural structure through reinforcement learning for hierarchical composition. Havrylov et al. (2019) extended Gumbel Tree-LSTM (Choi et al., 2018) through disjoint co-operative training of the parser (with reinforcement learning) and the composition function. Shen et al. (2019a) developed ordered memory (OM) by synthesizing the principles of ordered neurons (Shen et al., 2019b) with a stack-augmented RNN. While OM is a strong contender to CRvNN, we discussed the advantages of CRvNN as a more parallelized sequence processor in \u00a75.1. In another direction, Liu \\& Lapata (2018) applied structured attention (Kim et al., 2017) based on induced dependency trees to enhance downstream performance. Niculae et al. (2018) introduced SparseMAP to apply marginalization over a sparsified set of latent structures whereas Corro \\& Titov (2019) did a Monte-Carlo estimation by stochastically sampling a projective depedency tree structure for their encoder. These methods, similar to ours, are end-to-end differentiable, but do not focus on hierarchical composition of a sequence vector (root node) from the elements (leaf nodes) in the sequence following its latent constituency structure. As such, they do not address the challenges we face here. In recent years, Transformers (Vaswani et al., 2017) have also been extended either to better support tree structured inputs (Shiv \\& Quirk, 2019; Ahmed et al., 2019) or to have a better inductive bias to induce hierarchical structures by constraining self-attention (Wang et al., 2019; Nguyen et al., 2020; Shen et al., 2021) or by pushing intermediate representations to have constituent information (Fei et al., 2020). However, the fundamental capability of Transformers for composing sequences according to their latent structures in a length-generalizable manner is shown to be lacking (Tran et al., 2018; Shen et al., 2019a; Hahn, 2020). ## 7. Conclusion and Future Directions\n\nWe proposed a reformulation of RvNNs to allow for a continuous relaxation of its structure and order of composi-\ntion. The result is CRvNN , which can dynamically induce structure within data in an end-to-end differentiable manner. One crucial difference from prior work is that it can parallely process multiple positions at each recursive step and also, dynamically halt its computation when needed. We evaluated CRvNN on six datasets, and obtain strong performance on most of them. There are, however, several limitations of the model. First, the neighbor retriever functions in CRvNN, construct an $n \\times n$ matrix ( $n$ is the sequence length) which can be memory intensive. This is similar to the memory limitations of a Transformer. Another limitation is that it is a greedy model. It is also not explicitly equipped to handle structural ambiguities in natural language. In future work, we will address these limitations. To mitigate the memory limitations, we plan to constrain the neighbor retriever functions to look at only $k$ left or right candidates so that we only need an $n \\times k$ matrix. We can compress contiguous positions with low existential probabilities so that we do not need to look beyond $k$. To handle its greedy nature, we will extend CRvNN to follow multiple paths concurrently. ## 8. Acknowledgment\n\nWe would like to sincerely thank our reviewers for their constructive feedback that helped us to improve our paper greatly. We also thank Adrian Silvescu for helpful discussions. This research is supported in part by NSF CAREER award \\#1802358, NSF CRI award \\#1823292, and an award from UIC Discovery Partners Institute. The computation for this project was performed on Amazon Web Services. ## References\n\nAhmed, M., Samee, M.",
    "contrnn-17": "R., and Mercer, R. E. You only need attention to traverse trees. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 316-322, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1030. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is difficult.",
    "contrnn-18": "IEEE Transactions on Neural Networks, 5(2):157-166, 1994. Bergstra, J., Bardenet, R., Bengio, Y., and K\u00e9gl, B. Algorithms for hyper-parameter optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS'11, pp. 2546-2554, Red Hook, NY, USA, 2011. Curran Associates Inc. ISBN 9781618395993. Bergstra, J., Yamins, D., and Cox, D. D. Making a sci- ence of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML'13, pp. I-115-I-123. JMLR.org, 2013. Bowman, S.",
    "contrnn-19": "R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015a. Bowman, S.",
    "contrnn-20": "R., Manning, C. D., and Potts, C. Treestructured composition in neural networks without treestructured architectures. In Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches - Volume 1583, COCO'15, pp. 37-42, Aachen, DEU, 2015b. CEURWS.org. Bowman, S. R., Gauthier, J., Rastogi, A., Gupta, R., Manning, C. D., and Potts, C. A fast unified model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14661477, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1139. Choi, J., Yoo, K. M., and Lee, S. Learning to compose taskspecific tree structures. In McIlraith, S.",
    "contrnn-21": "A. and Weinberger, K. Q. (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp.",
    "contrnn-22": "50945101. AAAI Press, 2018. Chomsky, N. Syntactic structures. Walter de Gruyter, 1957.",
    "contrnn-23": "Corro, C. and Titov, I. Learning latent trees with stochastic perturbations and differentiable dynamic programming. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 55085521, Florence, Italy, July 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{P} 19-1551$. Das, S., Giles, C. L., and Sun, G.-Z. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In Proceedings of The Fourteenth Annual Conference of The Cognitive Science Society, pp. 14, 1992. Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. In International Conference on Learning Representations, 2019. Dong, H., Mao, J., Lin, T., Wang, C., Li, L., and Zhou, D. Neural logic machines. In International Conference on Learning Representations, 2019. Drozdov, A., Verga, P., Yadav, M., Iyyer, M., and McCallum, A. Unsupervised latent tree induction with deep inside-outside recursive auto-encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1129-1141, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 19-1116$. Drozdov, A., Rongali, S., Chen, Y.-P., O\u2019Gorman, T., Iyyer, M., and McCallum, A. Unsupervised parsing with SDIORA: Single tree encoding for deep inside-outside recursive autoencoders. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4832-4845, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 392. Fei, H., Ren, Y., and Ji, D. Retrofitting structure-aware transformer language model for end tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2151-2161, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 168. Fodor, J. A. and Pylyshyn, Z. W. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1): 3 - 71, 1988. ISSN 0010-0277. Giles, C., Sun, G.-Z., Chen, H.-H., Lee, Y.-C., and Chen, D. Higher order recurrent networks and grammatical inference. In Touretzky, D. (ed.), Advances in Neural Information Processing Systems, volume 2, pp. 380-387. Morgan-Kaufmann, 1990. Grefenstette, E., Hermann, K. M., Suleyman, M., and Blunsom, P. Learning to transduce with unbounded memory. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28, pp.",
    "contrnn-24": "1828-1836.",
    "contrnn-25": "Curran Associates, Inc., 2015. Hahn, M. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. doi: 10.1 $162 /$ tacl $\\backslash \\_a \\backslash \\_00306$. Havrylov, S., Kruszewski, G., and Joulin, A. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1118-1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 19-1115$. Hendrycks, D. and Gimpel, K. Bridging nonlinearities and stochastic regularizers with gaussian error linear units.",
    "contrnn-26": "ArXiv, abs/1606.08415, 2016. Hochreiter, S. Untersuchungen zu dynamischen neuronalen netzen. diploma thesis. Diploma thesis, TU Munich, 1991. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. Jacob, A. P., Lin, Z., Sordoni, A., and Bengio, Y. Learning hierarchical structures on-the-fly with a recurrentrecursive model for sequences. In Proceedings of The Third Workshop on Representation Learning for NLP, pp. 154-158, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W183020 . Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. Kim, Y., Denton, C., Hoang, L., and Rush, A. M. Structured attention networks. International Conference on Learning Representations, 2017. Kim, Y., Rush, A., Yu, L., Kuncoro, A., Dyer, C., and Melis, G. Unsupervised recurrent neural network grammars. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1105-1117, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 19-1114$. Kingma, D.",
    "contrnn-27": "P. and Ba, J. Adam: A method for stochastic optimization.",
    "contrnn-28": "In Bengio, Y. and LeCun, Y. (eds.), $3 r d$ International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.",
    "contrnn-29": "Knuth, D. E. On the translation of languages from left to right. Information and Control, 8(6):607 - 639, 1965. ISSN 0019-9958. Le, P. and Zuidema, W. The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization. In Proceedings\nof the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1155-1164, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1137. Li, B., Mou, L., and Keller, F. An imitation learning approach to unsupervised parsing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3485-3492, Florence, Italy, July 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{P} 19-1338$. Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2020. Liu, Y. and Lapata, M. Learning structured text representations. Transactions of the Association for Computational Linguistics, 6:63-75, 2018. doi: 10.1162/tacl_a_00005. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Maillard, J. and Clark, S. Latent tree learning with differentiable parsers: Shift-reduce parsing and chart parsing. In Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP, pp. 13-18, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-29 03. Maillard, J., Clark, S., and Yogatama, D. Jointly learning sentence embeddings and syntax with unsupervised tree1stms.",
    "contrnn-30": "Natural Language Engineering, 25(4):433-449, 2019. doi: 10.1017/S1351324919000184. Martins, A. F. T., Mihaylova, T., Nangia, N., and Niculae, V. Latent structure models for natural language processing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pp. 1-5, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P194001. Mozer, M.",
    "contrnn-31": "C. and Das, S. A connectionist symbol manipulator that discovers the structure of context-free languages. In Hanson, S., Cowan, J., and Giles, C. (eds.), Advances in Neural Information Processing Systems, volume 5, pp. 863-870. Morgan-Kaufmann, 1993. Munkhdalai, T. and Yu, H. Neural tree indexers for text understanding. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pp. 11-21, Valencia, Spain, April 2017. Association for Computational Linguistics. Nangia, N. and Bowman, S. ListOps: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 92-99, New Orleans, Louisiana, USA, June 2018. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 18-4013$. Nguyen, X.-P., Joty, S., Hoi, S., and Socher, R. Treestructured attention with hierarchical accumulation. In International Conference on Learning Representations, 2020. Niculae, V., Martins, A., Blondel, M., and Cardie, C. SparseMAP: Differentiable sparse structured inference.",
    "contrnn-32": "In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3799-3808, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pp.",
    "contrnn-33": "1532-1543, 2014. Pollack, J. B. Recursive distributed representations. Artificial Intelligence, 46(1):77 - 105, 1990. ISSN 00043702. Qian, L., An, S., Lou, J.-G., Bei, C., Lin, Z., Gao, Y., Bin, Z., Nanning, Z., and Dongmei, Z. Compositional generalization by learning analytical expressions. In Advances in Neural Information Processing Systems 33. 2020. Sakai, I. Syntax in universal translation. 1961. Sch\u00fctzenberger, M. On context-free languages and pushdown automata.",
    "contrnn-34": "Information and Control, 6(3):246 264, 1963. ISSN 0019-9958. Shen, Y., Lin, Z., wei Huang, C., and Courville, A. Neural language modeling by jointly learning syntax and lexicon. In International Conference on Learning Representations, 2018. Shen, Y., Tan, S., Hosseini, A., Lin, Z., Sordoni, A., and Courville, A. C. Ordered memory. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 5037-5048. Curran Associates, Inc., 2019a. Shen, Y., Tan, S., Sordoni, A., and Courville, A. Ordered neurons: Integrating tree structures into recurrent neural networks. In International Conference on Learning Representations, 2019 b . Shen, Y., Tay, Y., Zheng, C., Bahri, D., Metzler, D., and Courville, A. Structformer: Joint unsupervised induction of dependency and constituency structure from masked language modeling, 2021. Shi, H., Zhou, H., Chen, J., and Li, L. On tree-based neural sentence modeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4631-4641, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: $10.18653 / v 1 /$ D18-1492. Shiv, V.",
    "contrnn-35": "L. and Quirk, C. Novel positional encodings to enable tree-structured transformers, 2019.",
    "contrnn-36": "Socher, R., Manning, C.",
    "contrnn-37": "D., and Ng, A. Y. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, 2010. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. Sun, G. Connectionist pushdown automata that learn context-free grammars. In Proceedings of International Joint Conference on Neural Networks, volume 1, pp. 577-580, 1990. Tai, K. S., Socher, R., and Manning, C. D. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1556-1566, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1150. Tran, K., Bisazza, A., and Monz, C. The importance of being recurrent for modeling hierarchical structure. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4731-4736, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-15 03. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.",
    "contrnn-38": "N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017. Wang, Y., Lee, H.-Y., and Chen, Y.-N. Tree transformer: Integrating tree structures into self-attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1061-1070, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1098. Williams, A., Drozdov, A., and Bowman, S. Do latent tree learning models identify meaningful structure in sentences?",
    "contrnn-39": "Transactions of the Association for Computational Linguistics, 6(0):253-267, 2018a. ISSN 2307387X. Williams, A., Nangia, N., and Bowman, S. A broadcoverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018b. Wright, L. Ranger - a synergistic optimizer., 2019. Yogatama, D., Blunsom, P., Dyer, C., Grefenstette, E., and Ling, W. Learning to compose words into sentences with reinforcement learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. Yogatama, D., Miao, Y., Melis, G., Ling, W., Kuncoro, A., Dyer, C., and Blunsom, P. Memory architectures in recurrent neural network language models. In International Conference on Learning Representations, 2018. Zeng, Z., Goodman, R. M., and Smyth, P. Discrete recurrent neural networks for grammatical inference. IEEE Transactions on Neural Networks, 5(2):320-330, 1994. doi: $10.1109 / 72.279194$. Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. Lookahead optimizer: k steps forward, 1 step back. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, pp. 9597-9608. Curran Associates, Inc. | Dataset | Initial <br> Embedding <br> Size | Hidden Size <br> $\\left(d_{h}\\right.$ or $\\left.d_{\\text {embed }}\\right)$ | Input <br> Dropout | Output <br> Dropout | Hidden <br> Dropout |\n| :--- | ---: | ---: | ---: | ---: | ---: |\n| Logical Infer. | 200 | 200 | 0.1 | 0.3 | 0.1 |\n| ListOps | 128 | 128 | 0.3 | 0.2 | 0.1 |\n| SST2 | 300 | 300 | 0.3 | 0.2 | 0.4 |\n| SST5 | 300 | 300 | 0.3 | 0.2 | 0.4 |\n| SNLI | 300 | 300 | 0.4 | 0.1 | 0.1 |\n| MNLI | 300 | 300 | 0.4 | 0.1 | 0.1 |\n\nTable 9. Hyperparameter details for CRvNN . ## A. Architecture Details\n\nFor every task used in our experiments we use an initial affine transformation where the initial embeddings of size $d_{\\text {initial_embed }}$ are transformed into the size $d_{\\text {embed }}$. Typically, we set $d_{\\text {embed }}$ as $d_{h}$. See Table 9 for their values. We treat the last representation in the sequence after being processed by CRvNN as the sentence encoding constructed by CRvNN. For classification tasks, we classify the sentence encoding by transforming it into logits for the classes after passing it through a series of affine layers (typically, 1 or 2 ). Intermediate layers have $d_{h}$ neurons where $d_{h}$ is also the dimension of the sentence encoding. For inference tasks (requiring sequence-pair comparison), like logical inference or SNLI and MNLI, we use a Siamese framework. Concretely, we first encode (using the same encoder with same parameters) both the premise and hypothesis (separately) into sentence vectors, say, $s_{1}$ and $s_{2}$ respectively (both with $d_{h}$ dimensions). Then, we construct a classification feature vector $o$ as:\n\n$$\no=\\left[s_{1} ; s_{2} ;\\left|s_{1}-s_{2}\\right| ; s_{1} \\odot s_{2}\\right]\n$$\n\nHere, [;] indicates concatenation. We send $o$ to a Multi Layer Perceptron (MLP) to classify the sequence relationship. The final layer activation is Softmax, but if there are intermediate MLP layers, we use $G e L U$ for them. We use a dropout (hidden dropout) in the gated recursive cell (after its first affine transformation). We use a dropout (input dropout) on the input just before sending it to CRvNN. We use another dropout (output dropout) in between the final MLP layers. All our models were trained on AWS p3.2\u00d7 instance (Nvidia v100). ## B. Implementation Details\n\nFor all experiments, as an optimizer, we use Ranger (Wright, 2019) ${ }^{4}$ or Rectified (Liu et al., 2020) Adam\n\n[^3](Kingma \\& Ba, 2015) with lookahead $(k=5, \\alpha=$ 0.8) (Zhang et al.) and decorrelated weight decay (Loshchilov \\& Hutter, 2019) $(1 e-2)$ with a learning rate of $1 e-3$. We used GloVe (300 dimensions, 840B) (Pennington et al., 2014) as un-trainable embeddings for natural language data. We set the cell size ( $d_{\\text {cell }}$ as referred in $\\S 3.3 .3$ ) as $4 \\cdot d_{h}$ ( $d_{h}$ is the hidden size). We set the size of transition features ( $d_{s}$ as referred in $\\S 3.4 .2$ ) as 64 . For convolution in the decision function, we always use a window size of 5 . For halt penalty in $\\S 3.4 .3$, we set $\\gamma$ as 0.01 . Generally, we use a two-layered MLP on the sentence encoding from CRvNN. However, on ListOps, we used a single-layer. We use a batch size of 128 for all tasks. We describe other hyperparameters of CRvNN in Table 9. We cut the learning rate by half if the validation loss does not decrease for 3 contiguous epochs. ## C. Hyperparameter Search\n\nOn ListOps, we tune different dropouts among $\\{0.1,0.2,0.3,0.4\\}$ separately using grid search (we ran for 10 epochs and 50,000 subsamples). For the Logical Inference task (length generalization task), we tune the different dropouts among $\\{0.1,0.2,0.3\\}$ for 7 epochs per trial using grid search. We use the same hyperparamters for systematicity splits. For SST5, we tune the dropouts in $\\{0.2,0.3,0.4\\}$ for 3 epochs. For SNLI, we tune the different dropouts among $\\{0.1,0.2,0.3,0.4\\}$ for 5 epochs, using a sub-sample of $100 K$ examples, and for a maximum of 20 trials using Tree of Parzen Estimators (TPE) (Bergstra et al., 2011). We use Hyperopt (Bergstra et al., 2013) for hyperparameter tuning. For other components we mostly use similar hyperparameters as Shen et al. (2019a) or default settings. We share the hyperparameters found for SST5 with SST2 and we also share the hyperparameters found for SNLI with MNLI. ## D. Datasets\n\nFor all datasets, we use the standard splits as used by prior work. For training efficiency, we filter out training samples\nof sequence size $>150$ from MNLI. We filter out training samples with sequence length $>100$ from ListOps. We use the 90 K sample version of ListOps similar to prior work. [^0]:    ${ }^{1}$ Computer Science, University of Illinois at Chicago, United States. Correspondence to: Jishnu Ray Chowdhury $<$ jraych2@uic.edu $>$, Cornelia Caragea <cornelia@uic.edu $>$. Proceedings of the $38^{\\text {th }}$ International Conference on Machine Learning, PMLR 139, 2021. ${ }^{1}$ Our code is available at:\n    https://github.com/JRC1995/Continuous-RvNN\n\n[^1]:    ${ }^{2}$ We use the term \"surrogate gradients\" in the same sense as Martins et al. (2019) to indicate methods that approximate gradients of a discrete argmax-style mapping. Straight-through estimation with Gumbel-Softmax is such an example. [^2]:    ${ }^{3}$ Note, in iteration 2 in Table 1, when we compose together the representation of $x_{1}$ (position 1$)$ and the representation of $\\left(x_{2}, x_{3}\\right)$ (position 3), the former is not at the immediate left (position 2) from the latter because position 2 has 0 existential probability. [^3]:    4https://github.com/lessw2020/\n\n"
}