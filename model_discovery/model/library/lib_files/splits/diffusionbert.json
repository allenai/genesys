{
    "diffusionbert-0": "# DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models \n\nZhengfu He* Tianxiang Sun* Kuanning Wang Xuanjing Huang Xipeng Qiu<br>School of Computer Science, Fudan University<br>Shanghai Key Laboratory of Intelligent Information Processing, Fudan University<br>\\{zfhe19, txsun19, wangkn20, xjhuang, xpqiu\\}@fudan.edu.cn\n\n\n#### Abstract\n\nWe present DiffusionBERT, a new generative masked language model based on discrete diffusion models.",
    "diffusionbert-1": "Diffusion models and many pre-trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, diffusion models offer a promising training strategy that helps improve the generation quality. On the other hand, pre-trained denoising language models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Second, we investigate several designs of incorporating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improvement over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. ${ }^{1}$\n\n\n## 1 Introduction\n\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) have recently emerged as a new class of state-of-the-art generative models, achieving high-quality synthesis results on image data (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022). Though these models captured widespread attention from not only the research community but also the public, applying diffusion models to text data is still challenging and under-explored due to the discrete\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_3ffa675d8d221bd2d32cg-01.jpg?height=459&width=788&top_left_y=732&top_left_x=1065)\n\nFigure 1: In contrast to conventional discrete diffusion models, DiffusionBERT uses BERT as its backbone to perform text generation. The main differences are highlighted in color: (1) DiffusionBERT performs decoding without knowing the current time step while canonical diffusion models are conditioned on time step. (2) The diffusion process of DiffusionBERT is non-Markovian in that it generates noise samples $\\mathbf{x}_{t}$ conditioning not only on $\\mathbf{x}_{t-1}$ but also on $\\mathbf{x}_{0}$. Such a non-Markov process is due to our proposed noise schedule. nature of the text. A few prior works that explored using diffusion models on text data can be divided into two lines. The first is to extend diffusion models to discrete state spaces (Hoogeboom et al., 2021; Austin et al., 2021). The second is to perform the diffusion process and its reverse process in the continuous domain and bridge the continuous and the discrete domain through embedding and rounding (Li et al., 2022; Gong et al., 2022). However, none of these works leveraged pre-trained language models (PLMs, Devlin et al.",
    "diffusionbert-2": "(2019); Lewis et al. (2020); Raffel et al. (2020); Brown et al. (2020); Qiu et al. (2020)), which are an unmissable treasure in the NLP community. This work, to our knowledge, is the first attempt to combine diffusion models with PLMs. Such a combination is built upon a shared training objective between diffusion models and PLMs, i.e., denoising. Diffusion models consist of a forward process (data to noise) and a reverse process (noise\nto data). In the forward process, a small amount of noise is gradually added to the data. Then, a neural network ( $p_{\\theta}$ in Figure 1) is employed to learn the reverse process step by step, i.e., learn to denoise. Such a denoising neural network is naturally related to a wide class of PLMs that are pre-trained with denoising objectives such as BERT (Devlin et al., 2019) and BART (Lewis et al., 2020). Hence, pre-trained denoising language models can serve as a good start point to learn the reverse diffusion process. On the other hand, diffusion models also offer a promising training strategy for generative PLMs. In contrast to commonly used generative PLMs (e.g., GPT (Brown et al., 2020)) that relies on an autoregressive factorization of the joint probability, diffusion models provide another way of factorization along the dimension of time and therefore allow the model to be not necessarily autoregressive. Thus, diffusion models can be combined with a variety of PLMs that may not be pre-trained for generation. In the discrete domain, the forward diffusion process can be implemented by a chain of transition matrices that gradually corrupt the clean text. As shown in Figure 1, the clean text \"Hello world !\" is gradually corrupted into \" [MASK] [MASK] [MASK] \" during the diffusion process. In this work, we explore using pre-trained denoising language models (e.g., BERT) to learn the reverse diffusion process and demonstrate their advantages in accelerating convergence and improving generation quality. Further, we propose a new noise schedule of the forward process based on the principle of distributing the corrupted information uniformly across the forward process. The noise schedule, called spindle schedule, generates noise for $\\mathbf{x}_{t}$ conditioned not only on $\\mathbf{x}_{t-1}$ but also on $\\mathbf{x}_{0}$, making the forward process non-Markovian without changing the original training objective. Note that the denoising model takes as input $\\mathbf{x}_{t}$ and time step $t$ to predict $\\mathbf{x}_{t-1}$, where $t$ is unseen during the pre-training of language models so we investigate several ways of incorporating the time step into PLMs. As a result, we find that the best result is achieved by throwing away the time information, which we call time-agnostic decoding (TAD). Experimental results on unconditional text generation demonstrate the benefit of combining diffusion models with PLMs: the proposed DiffusionBERT significantly improves the generation quality over existing diffusion models for text generation (e.g., D3PM (Austin et al., 2021) and Diffusion-LM (Li et al., 2022)) and previous generative masked language models (e.g., BERTMouth (Wang and Cho, 2019)). The effectiveness of the proposed spindle schedule and time-agnostic decoding is confirmed by ablation studies. In a nutshell, DiffusionBERT enjoys the best of both worlds. ## 2 Background\n\n### 2.1 Diffusion Models\n\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are a class of latent variable models that are originally designed for continuous domains. A diffusion model is consisting of a forward diffusion process and a reverse diffusion process. Given a sample $\\mathbf{x}_{0} \\sim q\\left(\\mathbf{x}_{0}\\right)$, a Markov chain of latent variables $\\mathbf{x}_{1}, \\cdots, \\mathbf{x}_{T}$ are produced in the forward process by progressively adding a small amount of Gaussian noise to the sample:\n\n$$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{x}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{x}_{t-1}, \\beta_{t} \\mathbf{I}\\right)\n$$\n\nwhere $\\left\\{\\beta_{t} \\in(0,1)\\right\\}_{t=1}^{T}$ is a noise schedule controlling the step size of adding noise. Eventually $\\mathbf{x}_{T}$ becomes an isotropic Gaussian distribution. If $\\beta_{t}$ is small enough, the reverse process $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)$ is also a Gaussian, which is learned by a parameterized model\n\n$$\np_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, t\\right)=\\mathcal{N}\\left(\\mathbf{x}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{x}_{t}, t\\right), \\Sigma_{\\theta}\\left(\\mathbf{x}_{t}, t\\right)\\right)\n$$\n\nwhere $\\mu_{\\theta}(\\cdot)$ and $\\Sigma_{\\theta}(\\cdot)$ can be implemented by a U-Net or a Transformer. When conditioning also on $\\mathbf{x}_{0}, q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)$ has a closed form so we can manage to minimize the variational lower bound to optimize $\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right)$ :\n\n$$\n\\begin{aligned}\n& \\mathcal{L}_{\\mathrm{vlb}}=\\mathbb{E}_{q}\\left[D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right) \\| p_{\\theta}\\left(\\mathbf{x}_{T}\\right)\\right)\\right] \\\\\n& +\\mathbb{E}_{q}\\left[\\sum_{t=2}^{T} D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\| p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, t\\right)\\right)\\right] \\\\\n& -\\log p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)\n\\end{aligned}\n$$\n\nwhere $\\mathbb{E}_{q}(\\cdot)$ denotes the expectation over the joint distribution $q\\left(\\mathbf{x}_{0: T}\\right)$. ### 2.2 Diffusion Models in Discrete Domain\n\nFor discrete domains, each element of $\\mathbf{x}_{t}$ is a discrete random variables with $K$ categories. For text data, $K=|V|$ is the size of the vocabulary. Denote $\\mathbf{x}_{t}$ as a stack of one-hot vectors, the process\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_3ffa675d8d221bd2d32cg-03.jpg?height=615&width=1589&top_left_y=229&top_left_x=239)\n\nTable 1: Examples generated by three generative masked language models showing the difference of noise schedule and generation quality. of adding noise can be written as\n\n$$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\mathbf{x}_{t-1} \\mathbf{Q}_{t}\\right)\n$$\n\nwhere $\\operatorname{Cat}(\\cdot)$ is a category distribution and $\\mathbf{Q}_{t}$ is a transition matrix that is applied to each token in the sequence independently: $\\left[\\mathbf{Q}_{t}\\right]_{i, j}=q\\left(x_{t}=\\right.$ $\\left.j \\mid x_{t-1}=i\\right)$. It is easy to obtain that\n\n$$\n\\begin{aligned}\n& q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)=\\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_{0}\\right) q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)} \\\\\n& =\\operatorname{Cat}\\left(\\mathbf{x}_{t-1} ; \\mathbf{p}=\\frac{\\mathbf{x}_{t} \\mathbf{Q}_{t}^{\\top} \\odot \\mathbf{x}_{0} \\overline{\\mathbf{Q}}_{t-1}}{\\mathbf{x}_{0} \\overline{\\mathbf{Q}}_{t} \\mathbf{x}_{t}^{\\top}}\\right)\n\\end{aligned}\n$$\n\nwhere $\\overline{\\mathbf{Q}}_{t}=\\mathbf{Q}_{1} \\mathbf{Q}_{2} \\cdots \\mathbf{Q}_{t}$. Note that $\\odot$ is elementwise multiplication and the division is row-wise. With $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)$ at hand, according to Eq. (3), we can use a parameterized model $p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, t\\right)$ to learn the reverse diffusion process. ## 3 DiffusionBERT\n\nIn contrast to recently proposed diffusion models for text, e.g., Diffusion-LM (Li et al., 2022) and DiffuSeq (Gong et al., 2022), which are based on continuous diffusion models, we instead explore discrete diffusion models to integrate PLMs as the backbone. We first introduce a specific instance of discrete diffusion models (Austin et al., 2021), which considers a transition matrix with an absorbing state for the sake of using PLMs (\u00a7 3.1). Secondly, we introduce a new noise schedule of the forward diffusion process, called spindle schedule, which is based on the principle of distributing the corrupted information uniformly across the forward process (\u00a7 3.2). Then, we investigate several alternatives of incorporating the time step into PLMs for predicting $\\mathbf{x}_{t-1}$ given $\\mathbf{x}_{t}$ and $t$ (\u00a7 3.3). ### 3.1 Diffusion Models with a Discrete Absorbing State\n\nTo be combined with pre-trained denoising language models, we incorporate an absorbing state, e.g., [MASK] for BERT, in the Markov process. In particular, each token in the sequence either stays the same or transitions to [MASK] with some probability. Formally, each entry of the transition matrix at step $t$ is as follows,\n\n$$\n\\left[\\mathbf{Q}_{t}\\right]_{i, j}= \\begin{cases}1 & \\text { if } i=j=[\\mathrm{M}] \\\\ \\beta_{t} & \\text { if } j=[\\mathrm{M}], i \\neq[\\mathrm{M}] \\\\ 1-\\beta_{t} & \\text { if } i=j \\neq[\\mathrm{M}]\\end{cases}\n$$\n\nwhere [M] is the abbreviation of [MASK]. Such a Markov process converges to a stationary distribution $q\\left(\\mathbf{x}_{T}\\right)$, which places all probability mass on a sequence with all [MASK] tokens.",
    "diffusionbert-3": "The $t$-step marginal $q\\left(\\mathbf{x}_{t}^{i} \\mid \\mathbf{x}_{0}^{i}\\right)$ can be easily obtained in a closed form,\n\n$$\nq\\left(\\mathbf{x}_{t}^{i} \\mid \\mathbf{x}_{0}^{i}\\right)= \\begin{cases}\\bar{\\alpha}_{t} & \\text { if } \\mathbf{x}_{t}^{i}=\\mathbf{x}_{0}^{i} \\\\ 1-\\bar{\\alpha}_{t} & \\text { if } \\mathbf{x}_{t}^{i}=[\\mathrm{M}]\\end{cases}\n$$\n\nwhere $\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\left(1-\\beta_{i}\\right)$, $\\mathbf{x}_{t}^{i}$ denotes the $i$-th token in the sequence at step $t$. Combining with Eq. (3) and (5), we can derive a training objective to optimize $p_{\\theta}\\left(\\mathbf{x}_{\\mathbf{t}-\\mathbf{1}} \\mid \\mathbf{x}_{\\mathbf{t}}, t\\right)$ and generate a sample by performing the reverse diffusion process:\n\n$$\np_{\\theta}\\left(\\mathbf{x}_{0: T}\\right)=p\\left(\\mathbf{x}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, t\\right)\n$$\n\n### 3.2 Spindle Noise Schedule\n\nThe noise schedule in the continuous domain, such as the linear schedule (Ho et al., 2020) and the cosine schedule (Nichol and Dhariwal, 2021a), has shown to be important to the performance of diffusion models. In contrast to the continuous domain where the noise can be easily controlled by the variance of the Gaussian, (1) it is less obvious how to control the degree of noise added at each step in the discrete domain. For the discrete domain, the noise schedule $\\beta_{t}=(T-t+1)^{-1}$ has been explored for the case of the uniform transition matrix (SohlDickstein et al., 2015; Hoogeboom et al., 2021) and the absorbing-state transition matrix (Austin et al., 2021). However, (2) such a schedule assumes all tokens carry the same amount of information and does not consider the linguistic difference among the tokens in a sequence. Besides, (3) it violates the easy-first-generation nature of denoising language models. That is, the model tends to generate tokens that are most frequently appearing (and is least surprising) in the training corpus to achieve a higher likelihood. As the context becomes richer, more details come up in the sequence. To address the above issues, we consider a noise schedule that (1) measures the added noise at each step by the corrupted information and encourage the corrupted information to be uniformly distributed across the diffusion steps. Since the information is measured independently for each token, (2) different tokens in a sequence are assigned different probabilities of transitioning to the [MASK] token. Moreover, inspired by the easyfirst-generation phenomenon, (3) we put the tokens in a sequence in descending order of their information and divide them into $T$ buckets. Each bucket is ensured to contain the same amount of information. That is, we mask the most informative tokens at the start of the forward process and mask the least informative tokens at the end of the forward process such that the learnable reverse process follows an easy-first generative behavior. In particular, distributing corrupted information uniformly across the forward steps can be formally described by\n\n$$\n1-\\frac{t}{T}=\\frac{\\sum_{i=1}^{n} H\\left(\\mathbf{x}_{t}^{i}\\right)}{\\sum_{i=1}^{n} H\\left(\\mathbf{x}_{0}^{i}\\right)}=\\frac{\\sum_{i=1}^{n} \\bar{\\alpha}_{t}^{i} H\\left(\\mathbf{x}_{0}^{i}\\right)}{\\sum_{i=1}^{n} H\\left(\\mathbf{x}_{0}^{i}\\right)}\n$$\n\nwhere $H$ denotes the entropy, which measures the amount of information of a random variable, $\\mathrm{x}^{i}$ de- notes the $i$-th token in the sequence and $n$ denotes the length of the sequence. According to Eq. (7), $\\bar{\\alpha}_{t}^{i}=\\prod_{j=1}^{t}\\left(1-\\beta_{j}^{i}\\right)$ denotes the probability that the $i$-th token remains the same at step $t$, i.e., $\\mathrm{x}_{t}^{i}=\\mathrm{x}_{0}^{i}$. We expect that $\\bar{\\alpha}_{t}^{i}>\\bar{\\alpha}_{t}^{j}$ if $H\\left(\\mathrm{x}_{t}^{i}\\right)<H\\left(\\mathrm{x}_{t}^{j}\\right)$ such that easy (low-information) tokens emerges earlier than hard (high-information) tokens during the reverse process. Considering these aforementioned properties, we construct $\\bar{\\alpha}_{t}^{i}$ as follows,\n\n$$\n\\begin{gathered}\n\\bar{\\alpha}_{t}^{i}=1-\\frac{t}{T}-S(t) \\cdot \\tilde{H}\\left(\\mathbf{x}_{0}^{i}\\right) \\\\\nS(t)=\\lambda \\sin \\frac{t \\pi}{T} \\\\\n\\tilde{H}\\left(\\mathbf{x}_{0}^{i}\\right)=1-\\frac{\\sum_{j=1}^{n} H\\left(\\mathbf{x}_{0}^{j}\\right)}{n H\\left(\\mathbf{x}_{0}^{i}\\right)}\n\\end{gathered}\n$$\n\nwhere $S(t)$ is introduced to control the effect of the informativeness at time step $t$.",
    "diffusionbert-4": "It is designed to be sinusoidal to ensure $S(0)=S(T)=0$ such that $\\mathbf{x}_{t}$ can retain all (zero) information when $t=0$ $(t=T)$. The effect of $S(t)$ is controlled by a hyperparameter $\\lambda$. When $\\lambda=0$, the noise schedule is degraded to $\\beta_{t}=(T-t+1)^{-1}$ as in Sohl-Dickstein et al.",
    "diffusionbert-5": "(2015); Hoogeboom et al.",
    "diffusionbert-6": "(2021); Austin et al. (2021). Figure 2 shows how $\\bar{\\alpha}$ progresses during the forward process. The schedule is named as spindle due to the shape of the probability curves. In our proposed schedule, the transition probability at time step $t$ depends not only on the current state but also on the original text, making the forward diffusion process non-Markovian. Nevertheless, as revealed by Eq. (5), this does not change the original training objective. ### 3.3 The Design Space of Feeding Time Steps\n\nTypically, a diffusion model takes as input a noised sample and the time step to predict the denoised sample during the reverse process, i.e., $p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, t\\right)$. However, $t$ is an additional variable that is unseen during the pre-training of language models and therefore it is less trivial how to feed the time information into the PLMs. Here we explore three design choices of feeding time steps. Layer-wise Time Embedding A straightforward choice is to include the time step as the same way as positional encoding, i.e., using the Transformer sinusoidal embedding or a learnable MLP in each Transformer layer. Note that this way is\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_3ffa675d8d221bd2d32cg-05.jpg?height=541&width=672&top_left_y=272&top_left_x=269)\n\nFigure 2: Each token in a sequence has a specific noise schedule depending on how much information is lost when they are masked. For instance, in the sentence \"Bella is sitting over there.\",\"Bella\" is the most informative word. Thus it is encouraged to be masked at the early stage so that our model learns to recover it in the last place. commonly adopted in previous work (Ho et al., 2020; Austin et al., 2021; Li et al., 2022). Prefix Time Embedding Prompting language models by prepending trainable soft tokens to the input sequence has shown promising results recently (Lester et al., 2021; Sun et al., 2022). Hence, we also explore including a time step token embedding $\\mathbf{v}(t)$ as a prefix of the input token embeddings $\\left\\langle\\mathbf{v}\\left(\\mathbf{x}_{t}^{1}\\right), \\mathbf{v}\\left(\\mathbf{x}_{t}^{2}\\right), \\cdots, \\mathbf{v}\\left(\\mathbf{x}_{t}^{n}\\right)\\right\\rangle$. In particular, the time step token is inserted in between the [CLS] token and the input sequence. These added time step token embeddings are trained along with the PLM. Time-Agnostic Decoding Another alternative is not to explicitly incorporate the time step $t$ because it can be implied by the noised sample $\\mathbf{x}_{t}$. In contrast to the image data, it is easier to implicitly infer the diffusion time step by counting the number of corrupted tokens (i.e., [MASK]) in the noised sequence. In this way, the PLM has to perform iterative decoding while being ignorant of the current time step, i.e., $p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)$. ## 4 Experiments\n\n### 4.1 Experimental Setup\n\nWe mainly focus on unconditional text generation in complex scenarios where the training data covers a wide range of topics and is composed of a large vocabulary. Experiments are conducted on the One Billion Word dataset (LM1B) (Chelba et al., 2014). LM1B is a language corpus with about 30 million sentences and a vocabulary of about 793 k . We use the standard train-test split and take $1 \\%$ of the training set for validation. All text data are lower-cased to align with the settings of Austin et al. (2021). Our DiffusionBERT is based on BERT-BASEUNCASED with about 110M parameters. We train DiffusionBERT using the AdamW optimizer (Loshchilov and Hutter, 2019) for 1.9 million steps with learning rate of $3 \\mathrm{e}-6$, dropout probability of 0.1 , batch size of 32 .",
    "diffusionbert-7": "For the first 10 K steps, we use a linear warmup schedule starting from learning rate of $1 \\mathrm{e}-8$.",
    "diffusionbert-8": "All experiments are conducted on NVIDIA A100 Tensor Core GPUs. We use 4 GPUs for training and a single GPU for sampling. ### 4.2 Baselines\n\nWe conduct comparison on unconditional text generation against several non-autoregressive (NAR) baselines: D3PM (Austin et al., 2021), DiffusionLM (Li et al., 2022), and BERT-Mouth (Wang and Cho, 2019).",
    "diffusionbert-9": "${ }^{2}$\n\nD3PM D3PM is a general framework of discrete diffusion models. We implement an instance of D3PM with the absorbing state and a layer-wise time embedding. Both DiffusionBERT and D3PM are implemented with a sequence length $n=128$ and diffusion steps $T=2048$. During inference, we perform decoding with 16 time steps in each iteration. The total inference cost is 128 iterations, which is smaller than that chosen in existing diffusion or diffusion-like models for unconditional generation (Hoogeboom et al., 2021; Savinov et al., 2022). This has no impact on our conclusions since increasing the diffusion step does not bring substantial improvement. Diffusion-LM Diffusion-LM learns an embedding to map discrete text into the continuous space where it performs Gaussian diffusion process. A rounding step is required to map the continuous embeddings into discrete texts. We re-implemented Diffusion-LM with the model architecture of BERT and diffusion steps $T=2000$. Since the performance drop of Diffusion-LM is bigger than D3PM\n\n[^1]| Method | Pretrained | Schedule | Time Step | PPL $\\downarrow$ | BLEU $\\uparrow$ | Self-BLEU $\\downarrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| D3PM (Austin et al., 2021) | $x$ | $(T-t+1)^{-1}$ | LTE | 82.34 | 0.3897 | 0.2347 |\n|  |  |  | TAD | 125.15 | 0.3390 | 0.2720 |\n|  |  | Spindle | LTE | 77.50 | 0.4241 | 0.2288 |\n| Diffusion-LM (Li et al., 2022) | $x$ | Cosine | LTE | 118.62 | 0.3553 | 0.2668 |\n|  | $\\checkmark$ | Cosine | LTE | 132.12 | 0.3562 | 0.2798 |\n| BERT-Mouth (Wang and Cho, 2019) | $\\checkmark$ | - | - | 142.89 | 0.2867 | 0.1240 |\n| DiffusionBERT | $\\checkmark$ | $(T-t+1)^{-1}$ | LTE | 92.53 | 0.3995 | 0.2118 |\n|  |  |  | PTE | 79.95 | 0.3886 | 0.2156 |\n|  |  |  | TAD | 78.76 | 0.4213 | 0.2116 |\n|  |  | Spindle | TAD | 63.78 | 0.4358 | 0.2151 |\n\nTable 2: Main results on LM1B.",
    "diffusionbert-10": "The methods proposed in this work are marked with wavy lines. The best results are in bold and the second best results are underlined. LTE: layer-wise time embedding. PTE: prefix time embedding. TAD: time-agnostic decoding. and DiffusionBERT when we sample less steps during generation, we do not skip steps so the number of inference is about 4 times that of DiffusionBERT and the exact generation time comparison is reported in $\\S 4.5$. BERT-Mouth BERT-Mouth samples text from BERT via order-agnostic autoregressive masked language modeling. Starting from a sequence of [MASK], BERT samples one token at each time step in random order. Another option is decoding from left to right, like autoregressive models. In our preliminary experiments, we find that random position sampling performs better. We continue pretraining BERT on LM1B to adapt BERT to downstream training corpus. ### 4.3 Main Results\n\nOur main results are included in Table 2. We choose BLEU-4 as the metric for generation quality and diversity. For each method, we sample 1 K text for evaluating BLEU score and another 1 K for self-BLEU. Note that with different sampling strategy, the BLEU/self-BLEU results may vary. For fair comparison, the sentences sampled by D3PM and DiffusionBERT have a fixed length $n=64$ and are sampled by a top- $K$ filter where $K=30$. Diffusion-LM and BERT-Mouth are trained and sampled following their original implementation. Overall, DiffusionBERT achieves the best generation quality and diversity trade-off among the considered NAR methods. Besides, the perplexity of DiffusionBERT with the spindle noise schedule is substantially higher. Evidence of lower bound is used as a proxy of the perplexities of DiffusionBERT and D3PM since the exact likelihood of diffusion models is intractable. DiffusionBERT vs. Other Generative BERT Models We compare DiffusionBERT with another representative generative masked language model, BERT-Mouth (Wang and Cho, 2019). Experimental results show that DiffusionBERT achieves better performance in terms of the perplexity and the BLEU score. We attribute the superior performance of DiffusionBERT to its onetime sampling of all tokens, which helps DiffusionBERT generate more coherent text, especially in a long range. Although such decoding may face the problem of multimodality (Gu et al., 2018), inappropriate phrases can be fixed in the upcoming diffusion steps. The probabilistic modeling offers more flexibility in that generated tokens with low probability are more likely to be masked and resampled. In BERT-Mouth, however, the tokens are fixed once sampled. Wang and Cho (2019) also proposed to continue masking and predicting tokens after the whole sequence is complete, revising the sentence for higher quality. But such randomness in the selection and replacement of tokens results in low inference speed. Discrete vs. Continuous Diffusion Models We then focus on the comparison of discrete and continuous diffusion models for text generation. To achieve this, we mainly compare DiffusionBERT with recently proposed Diffusion-LM,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_3ffa675d8d221bd2d32cg-07.jpg?height=524&width=718&top_left_y=292&top_left_x=246)\n\nFigure 3: BLEU scores on the LM1B test set.",
    "diffusionbert-11": "Left is better, lower is better. For GPT and Transformer decoder, we control quality-variation with sampling temperature. D3PM and DiffusionBERT are controlled by truncation sampling hyperparameter $K$. which is based on continuous diffusion models. As a result, despite of its outstanding controlling ability, we show that the texts generated by DiffusionLM have a lower quality than DiffusionBERT. Though both DiffusionBERT and Diffusion-LM adopt the same configuration of Transformer, it is worth noting that the superior performance of DiffusionBERT may be contributed by not only the discrete diffusion models but also the use of pre-trained models. To disentangle the effect of pre-training and discrete/continuous diffusion models, we also explore initializing Diffusion-LM with BERT. As shown in Table 2, training DiffusionLM from BERT initialization performs even worse than training from scratch. We conjecture that the continuous nature of Diffusion-LM is not compatible with the initialization from BERT since the embedding learned by BERT may not be suitable for the Gaussian diffusion process. In contrast, the comparison of D3PM and DiffusionBERT shows that DiffusionBERT benefits much from the BERT initialization due to its discrete diffusion process. Effect of Time Step In terms of both likelihood and generation quality, the layer-wise time embedding (LTE) lags far behind the other two time step designs for DiffusionBERT while time-agnostic decoding (TAD) achieves the best result. By contrast, D3PM without time step embedding performs significantly worse. In a nutshell, simplifying time step design has positive effect on DiffusionBERT but is quite harmful for D3PM. This suggests that initializing $p_{\\theta}$ with PLMs enables\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_3ffa675d8d221bd2d32cg-07.jpg?height=543&width=689&top_left_y=274&top_left_x=1072)\n\nFigure 4: Curve of validation ELBO during training. DiffusionBERT to perform generation without explicitly providing time information yet achieving better generation results. The resemblance between BERT pre-training objective and absorbing diffusion models makes it easier for DiffusionBERT to generalize to noisier scenarios while a Transformer encoder trained from scratch needs a specific timeaware module to model the reverse process. Effect of the Spindle Noise Schedule We try our proposed spindle noise schedule on both DiffusionBERT and D3PM. The perplexity is improved by $18 \\%$ and $19 \\%$ for D3PM and DiffusionBERT, respectively. Besides, D3PM with the spindle schedule outperforms that with the standard $(T-t+1)^{-1}$ schedule in generation quality. The same trend holds for DiffusionBERT but with a smaller margin. ### 4.4 Quality-Diversity Trade-off\n\nAs shown in Figure 3, DiffusionBERT exhibits comparable generation ability with a Transformer decoder trained from scratch and pushes the Pareto front of NAR generation quality/diversity trade-off by a large margin. However, it still falls behind pretrained AR models of the same size. ### 4.5 Efficiency of Training and Generation\n\nOne important feature of DiffusionBERT is that with time-agnostic decoding, all parameters are initialized by pretrained models. Consequently, DiffusionBERT includes fewer parameters and is free from adapting new parameters, improving training and decoding efficiency. Faster Convergence DiffusionBERT converges remarkably faster than D3PM. Figure 4 demonstrates the curve of validation ELBO in the training\n\n| Method | Steps | Inference Time (secs) | PPL |\n| :---: | :---: | :---: | :---: |\n|  | 2 | 0.66 | 313.57 |\n|  | 8 | 1.39 | 91.01 |\n| DiffusionBERT | 16 | 1.80 | 75.66 |\n|  | 64 | 4.25 | 65.83 |\n|  | 128 | 7.53 | 63.78 |\n|  | 512 | 27.48 | 54.63 |\n| Diffusion-LM | 2000 | 83.67 | 112.12 |\n| BERT-Mouth | 64 | 2.18 | 142.89 |\n|  | 512 | 14.39 | 86.78 |\n| GPT | 64 | 1.55 | 38.7 |\n\nTable 3: Comparison of inference time and perplexity among baselines and DiffusionBERT.",
    "diffusionbert-12": "process. Even if the training budget is cut to $30 \\%$ (i.e. 0.5 million steps), DiffusionBERT is still able to match the performance reported in Table 2. Sampling Speed With the $x_{0}$-parameterization proposed in Song et al. (2021) and Austin et al. (2021), DiffusionBERT is able to perform inference with any given budget by controlling the step size in the reverse process. We also control the sampling time of BERT-Mouth by adjusting the max iteration count of its mask-predict process. We list the decoding speed and the corresponding perplexity on the LM1B test set in Table 3. Overall, DiffusionBERT exhibits competitive performance even when it reaches comparable speed to GPT and outperforms BERT-Mouth in efficiency-performance tradeoff. ## 5 Related Work\n\n### 5.1 BERT for Text Generation\n\nIt has been shown by Wang and Cho (2019) that the transfer-learning ability of BERT does not only helps to achieve impressive results in natural language understanding but also benefits sequential sampling for text generation. However, its bidirectionality nature holds BERT from matching the decoder-only counterparts (Radford et al., 2018) in modeling text from left to right. ### 5.2 Diffusion Models for Text\n\nThis work lies in the line of diffusion models, a latent variable generative framework proposed by Sohl-Dickstein et al.",
    "diffusionbert-13": "(2015). It has been architecturally improved by Ho et al. (2020) and has gained broad attention for its impressive generation ability in continuous domain (e. g. image and audio)\n(Ramesh et al., 2022; Kong et al., 2021; Nichol and Dhariwal, 2021b). Despite their great success and state-of-the-art sample quality in the above domains, diffusion models for text still struggle to match autoregressive models in various generation tasks. Since the Gaussian noise proposed in SohlDickstein et al. (2015) cannot be directly applied to discrete data, they also introduced a discrete forward process with a Bernoulli transition kernel. Hoogeboom et al. (2021) made a step forward from Bernoulli to categorical distributions. A more general family of discrete diffusion processes was introduced in Austin et al. (2021); Hoogeboom et al. (2022), including absorbing kernels and combinations of absorbing and uniform transition kernels. Li et al. (2022) models text in the continuous embedding space, which is closer to the settings in earlier works of diffusion models and shows impressive performance in classifier-controlled text generation. While the decoding and convergence speed are substantially slower and the generated text lacks coherence. Moreover, in scenarios where the vocabulary is large, the k-nearest-neighbor algorithm used in decoding holds up decoding even more severely. ### 5.3 Non-Autoregressive Text Generation\n\nAbsorbing discrete diffusion models resembles conditional masked language models (CMLMs) (Ghazvininejad et al., 2019) in that both methods predict the whole sequence simultaneously and follows a construct-destruct pattern to iteratively refine the generated text. The main difference lies in the training objective: DiffusionBERT models a stochastic process and drives BERT to learn a group of distributions to gradually recover training data while CMLMs forces the neural network to deterministically recover the whole sequence in every iteration, thus it fails to explicitly model the denoising process. Savinov et al. (2022) proposed to approach the problem of non-autoregressive text modeling via unrolling the generation path to prepare the model for the partially corrupted sequences it will encounter during generation, which resembles the idea of diffusion models for unconditional text generation. Non-autoregressive models are also considered in translation but implemented in various ways, e.g., insertion/deletion ( Gu et al., 2019) and iterative sequence alignment (Saharia et al., 2020). ## 6 Conclusion\n\nThis work aims to approach the problem of unconditional text generation for non-autoregressive models. To achieve this, we combine pretrained language models with absorbing-state discrete diffusion models for text. The training procedure of our proposed DiffusionBERT includes two main deviations from current discrete diffusion models, i.e., a new family of time step designs and the spindle noise schedule. The novel spindle noise assigns a schedule for each token according to its frequency in the training corpus. Experimental results demonstrate the success of DiffusionBERT in terms of perplexity. It also pushes the Pareto front of quality-variance tradeoff of NAR methods by a large margin, comparable to a Transformer decoder trained from scratch. ## References\n\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 17981-17993.",
    "diffusionbert-14": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Ciprian Chelba, Tom\u00e1s Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In $I N$ TERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association, Singapore, September 14-18, 2014, pages 26352639. ISCA. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 61116120. Association for Computational Linguistics. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. 2022. Diffuseq: Sequence to sequence text generation with diffusion models. CoRR, abs/2210.08933. Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. 2018. Nonautoregressive neural machine translation. In $6 t h$ International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019. Levenshtein transformer. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11179-11189. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. 2022. Autoregressive diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 2529, 2022. OpenReview.net. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. 2021. Argmax flows and multinomial diffusion: Towards non-autoregressive language models. CoRR, abs/2102.05379. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2021. Diffwave: A versatile diffusion model for audio synthesis. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on\n\nEmpirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 30453059. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B.",
    "diffusionbert-15": "Hashimoto. 2022. Diffusion-lm improves controllable text generation. CoRR, abs/2205.14217. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Alexander Quinn Nichol and Prafulla Dhariwal. 2021a. Improved denoising diffusion probabilistic models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8162-8171. PMLR. Alexander Quinn Nichol and Prafulla Dhariwal. 2021b. Improved denoising diffusion probabilistic models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8162-8171. PMLR. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. SCIENCE CHINA Technological Sciences. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.",
    "diffusionbert-16": "Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "diffusionbert-17": "J. Mach. Learn. Res., 21:140:1-140:67. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with CLIP latents. CoRR, abs/2204.06125. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, pages 10674-10685.",
    "diffusionbert-18": "IEEE. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487.",
    "diffusionbert-19": "Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi. 2020. Non-autoregressive machine translation with latent alignments. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 1098-1108. Association for Computational Linguistics. Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and A\u00e4ron van den Oord. 2022. Step-unrolled denoising autoencoders for text generation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 22562265. JMLR.org. Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In Proceedings of the 39th International Conference on Machine Learning, ICML 2022, Baltimore, Maryland, USA. Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a markov random field language model. CoRR, abs/1902.04094. [^0]:    * Equal contribution. ${ }^{1}$ Our code is publicly available at https://github. com/Hzfinfdu/Diffusion-BERT\n\n[^1]:    ${ }^{2}$ Another strong baseline of NAR text generation is SUNDAE (Savinov et al., 2022) but unfortunately there is no public implementation available. We will include comparison with SUNDAE in later versions by directly using the results reported in the original paper and use the same settings to train DiffusionBERT for fair comparison.",
    "diffusionbert-20": ""
}