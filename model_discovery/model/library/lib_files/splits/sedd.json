{
    "sedd-0": "Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution\n\nAaron Lou Stanford University aaronlou@stanford.edu &Chenlin Meng Stanford University chenlin@stanford.edu &Stefano Ermon Stanford University ermon@stanford.edu\n\nAbstract\n\nDespite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within of and sometimes outperforming the baseline). Furthermore, SEDD models learn a more faithful sequence distribution (around better compared to GPT-2 models with ancestral sampling as measured by large models), can trade off compute for generation quality (needing only fewer network evaluations to match GPT-2), and enables arbitrary infilling beyond the standard left to right prompting. 1 Introduction\n\nMany recent advances in deep learning have centered around generative modeling. In this setting, neural networks learn to generate new samples given unstructured data. Remarkably, combining the powerful generalization of neural networks with this rather straightforward objective has led to unparalleled capabilities. For example, modern \u201cgenerative AI\" systems are able to generate images from arcane descriptions (Ramesh et al., 2022) and answer complex queries (Brown et al., 2020). So far, the techniques used for these advances have largely been bifurcated according to the structure of the data. For computer vision data, where one can faithfully dequantize into continuous space, diffusion modeling (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) is the core paradigm undergirding contemporary methods. Conversely, for natural language data, which is far more discrete, autoregressive modeling is indispensable for most problems (Radford et al., 2019). Despite this field-level divide, researchers have attempted to apply diffusion models to language modeling tasks (Li et al., 2022; Austin et al., 2021). This can be done by either designing a special dequantization for the discrete tokens (Dieleman et al., 2022) or by directly modeling a discrete diffusion process on said tokens (He et al., 2022; Zheng et al., 2023). However, despite considerable efforts, no such method has yet yielded a diffusion model scheme that is on par with or provides a clear benefit over standard autoregressive training. In our work, we close this gap for GPT-2 scale experiments (Radford et al., 2019), demonstrating, for the first time, a non-autoregressive modeling technique that is able to achieve similar perplexity scores as autoregressive modeling at a modern scale. Our approach has the added benefits of producing better samples from the learned distribution, allowing for a compute-quality tradeoff, and enabling prompting with arbitrary positions. Key to this success is score entropy, a novel training objective for discrete-space diffusion models that is analogous to score matching for continuous-space diffusion models (Hyv\u00e4rinen, 2005; Song & Ermon, 2019). Our contributions can be summarized as follows:\n\n1. We introduce score entropy, a discrete score matching loss that can be used to trained discrete diffusion models. Score entropy learns the concrete scores (analogous to the score function in standard diffusion) of the perturbed data distribution in a scalable and principled manner.",
    "sedd-1": "2. We use the modeled scores to develop several enhanced sampling methods. In particular, we derive a score-based ancestral sampling method and a general infilling procedure. 3. We combine our theoretical advances with architectural improvements to scale our Score Entropy Discrete Diffusion models (SEDD) to GPT-2 model sizes. As previously mentioned, SEDD is comparable to GPT-2 for perplexities but also offer several distinct advantages for high quality, fast, and controllable generation. 2 Preliminaries\n\nDiffusion models learn to generate data by reversing a Markov process that takes the data distribution to a simple noise distribution (Sohl-Dickstein et al., 2015). We want to reverse this process, allowing us to sample from to generate samples from , but the reverse transitions are difficult to approximate since they are nontrivial densities. However, as , the concept of the \u201cscore function\" emerges to enable a more faithful modeling paradigm. Learning this quantity is well-established for continuous spaces but remains an open problem for discrete spaces. 2.1 Continuous Diffusion Models\n\nWhen the data support is , one constructs the Markov process by perturbing data points with a stochastic process defined by the stochastic differential equation (SDE) (Song et al., 2020):\n\nd \u200b \ud835\udc31 t = f \u200b ( \ud835\udc31 t , t ) \u200b d \u200b t + g \u200b ( t ) \u200b d \u200b \ud835\udc01 t d subscript \ud835\udc31 \ud835\udc61 \ud835\udc53 subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 d \ud835\udc61 \ud835\udc54 \ud835\udc61 \ud835\udc51 subscript \ud835\udc01 \ud835\udc61 \\mathrm{d}\\mathbf{x}_{t}=f(\\mathbf{x}_{t},t)\\mathrm{d}t+g(t)d\\mathbf{B}_{t} (1)\n\nThe perturbed densities of the points evolve according to the corresponding Fokker-Planck partial differential equation and approaches a Gaussian limit distribution . A famous result by Anderson constructs the reverse of this stochastic differential equation (Anderson, 1982):\n\nd \u200b \ud835\udc31 t = ( f \u200b ( \ud835\udc31 t , t ) \u2212 g \u200b ( t ) 2 \u200b \u2207 x p t \u200b ( \ud835\udc31 t ) p t \u200b ( \ud835\udc31 t ) ) \u200b d \u200b t + g \u200b ( t ) \u200b d \u200b \ud835\udc01 t d subscript \ud835\udc31 \ud835\udc61 \ud835\udc53 subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 \ud835\udc54 superscript \ud835\udc61 2 subscript \u2207 \ud835\udc65 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc31 \ud835\udc61 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc31 \ud835\udc61 d \ud835\udc61 \ud835\udc54 \ud835\udc61 d subscript \ud835\udc01 \ud835\udc61 \\mathrm{d}\\mathbf{x}_{t}=\\left(f(\\mathbf{x}_{t},t)-g(t)^{2}\\frac{\\nabla_{x}p_{t}(\\mathbf{x}_{t})}{p_{t}(\\mathbf{x}_{t})}\\right)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{B}_{t} (2)\n\nwhich takes back to . One approximates this process by learning the unknown (normally written as ) with a neural network . This can be done optimizing the well known score matching (shown below) jointly over all . \u2112 SM = 1 2 \u200b \ud835\udd3c \ud835\udc31 \u223c p t \u200b \u2016 \ud835\udc2c \u03b8 \u200b ( \ud835\udc31 , t ) \u2212 \u2207 x p t \u200b ( \ud835\udc31 ) p t \u200b ( \ud835\udc31 ) \u2016 2 subscript \u2112 SM 1 2 subscript \ud835\udd3c similar-to \ud835\udc31 subscript \ud835\udc5d \ud835\udc61 superscript norm subscript \ud835\udc2c \ud835\udf03 \ud835\udc31 \ud835\udc61 subscript \u2207 \ud835\udc65 subscript \ud835\udc5d \ud835\udc61 \ud835\udc31 subscript \ud835\udc5d \ud835\udc61 \ud835\udc31 2 \\mathcal{L}_{\\rm SM}=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}\\sim p_{t}}\\left\\|\\mathbf{s}_{\\theta}(\\mathbf{x},t)-\\frac{\\nabla_{x}p_{t}(\\mathbf{x})}{p_{t}(\\mathbf{x})}\\right\\|^{2} (3)\n\nScore matching has many equivalent forms such as the implicit (Song et al., 2019) and denoising score matching losses (Vincent, 2011) that remove the unknown term . With a learned score, the diffusion model can sample and solves the parameterized reverse SDE\n\nd \u200b \ud835\udc31 t = ( f \u200b ( \ud835\udc31 t , t ) \u2212 g \u200b ( t ) 2 \u200b \ud835\udc2c \u03b8 \u200b ( \ud835\udc31 , t ) ) \u200b d \u200b t + g \u200b ( t ) \u200b d \u200b \ud835\udc01 t d subscript \ud835\udc31 \ud835\udc61 \ud835\udc53 subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 \ud835\udc54 superscript \ud835\udc61 2 subscript \ud835\udc2c \ud835\udf03 \ud835\udc31 \ud835\udc61 d \ud835\udc61 \ud835\udc54 \ud835\udc61 d subscript \ud835\udc01 \ud835\udc61 \\mathrm{d}\\mathbf{x}_{t}=\\left(f(\\mathbf{x}_{t},t)-g(t)^{2}\\mathbf{s}_{\\theta}(\\mathbf{x},t)\\right)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{B}_{t} (4)\n\nto approximately sample from . Importantly, when the score matching losses are jointly optimized with a relative weighting of for each , one is able to compute the ELBO for training and evaluating likelihoods (Song et al., 2021; Kingma et al., 2021; Huang et al., 2021). This so-called \u201cscore-parameterization\" of diffusion models has been essential for the recent success of continuous space diffusion models. In particular, modeling the score function (up to a scaling) has consistently been shown to result in both superior generation quality and improved likelihood values (Ho et al., 2020; Karras et al., 2022). 2.2 Discrete Diffusion Models\n\nWe now consider the a discrete data support . Here, the probability distributions instead become probability mass vectors that are positive and sum to , and the diffusion is best described by the discrete analogue of the Fokker-Planck partial differential equation acting on . In particular, we evolve the data distribution according to the equation (Campbell et al., 2022)\n\nd \u200b p t d \u200b t = Q t \u200b p t p 0 = p data formulae-sequence \ud835\udc51 subscript \ud835\udc5d \ud835\udc61 \ud835\udc51 \ud835\udc61 subscript \ud835\udc44 \ud835\udc61 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc5d 0 subscript \ud835\udc5d data \\frac{dp_{t}}{dt}=Q_{t}p_{t}\\quad p_{0}=p_{\\rm data} (5)\n\nwhere are the diffusion matrices that are required to have non-negative non-diagonal entries and columns which sum to zero (so that the rate sums to , meaning does not gain or lose total mass). This process is realized at the sample level by the transition densities that are defined by the columns of :\n\np \u200b ( x t + \u0394 \u200b t = y | x t = x ) = \u03b4 x \u200b y + Q t \u200b ( y , x ) \u200b \u0394 \u200b t + O \u200b ( \u0394 \u200b t 2 ) \ud835\udc5d subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 conditional \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 \ud835\udc65 subscript \ud835\udeff \ud835\udc65 \ud835\udc66 subscript \ud835\udc44 \ud835\udc61 \ud835\udc66 \ud835\udc65 \u0394 \ud835\udc61 \ud835\udc42 \u0394 superscript \ud835\udc61 2 p(x_{t+\\Delta t}=y|x_{t}=x)=\\delta_{xy}+Q_{t}(y,x)\\Delta t+O(\\Delta t^{2}) (6)\n\nwhich eanbles a Euler-Maruyama type sampling algorithm that steps according to . For certain , approaches a limiting distribution for large . Additionally, this Markov process has a well known reversal (Kelly, 1980; Sun et al., 2022) given by another diffusion matrix :\n\nd \u200b p T \u2212 t d \u200b t = Q \u00af T \u2212 t \u200b p T \u2212 t Q \u00af t \u200b ( y , x ) = { p t \u200b ( y ) p t \u200b ( x ) \u200b Q t \u200b ( x , y ) x \u2260 y \u2212 \u2211 k \u2260 i Q \u00af t \u200b ( z , x ) x = y formulae-sequence \ud835\udc51 subscript \ud835\udc5d \ud835\udc47 \ud835\udc61 \ud835\udc51 \ud835\udc61 subscript \u00af \ud835\udc44 \ud835\udc47 \ud835\udc61 subscript \ud835\udc5d \ud835\udc47 \ud835\udc61 subscript \u00af \ud835\udc44 \ud835\udc61 \ud835\udc66 \ud835\udc65 cases subscript \ud835\udc5d \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 \ud835\udc65 subscript \ud835\udc44 \ud835\udc61 \ud835\udc65 \ud835\udc66 \ud835\udc65 \ud835\udc66 subscript \ud835\udc58 \ud835\udc56 subscript \u00af \ud835\udc44 \ud835\udc61 \ud835\udc67 \ud835\udc65 \ud835\udc65 \ud835\udc66 \\frac{dp_{T-t}}{dt}=\\overline{Q}_{T-t}p_{T-t}\\quad\\overline{Q}_{t}(y,x)=\\begin{cases}\\frac{p_{t}(y)}{p_{t}(x)}Q_{t}(x,y)&x\\neq y\\\\\n-\\sum_{k\\neq i}\\overline{Q}_{t}(z,x)&x=y\\end{cases} (7)\n\nNote that the reverse process again depends on , which is defined by the data distribution and the diffusion . This is analogous to the reverse SDE (Equation 2), with the ratio , generalizing the score function111The gradient operator for discrete structures is (up to some scaling) defined for pairs by . The score function would generalize to the normalized gradients .. Similar to the continuous case, by approximating this score function, one can generate samples by sampling from and simulating a parameterized reverse process. However, there still has yet to be a consensus on how to learn these ratios (see Section 6). Concrete Score Matching. Meng et al. (2022) take a score matching view and group for each value , forming the concrete score. By generalizing the standard score matching loss, they learn with a discrete generalization of the score matching loss:\n\n\u2112 CSM = 1 2 \u200b \ud835\udd3c x \u223c p t \u200b [ \u2211 y \u2260 x ( s \u03b8 \u200b ( x t , t ) y \u2212 p t \u200b ( y ) p t \u200b ( x ) ) 2 ] subscript \u2112 CSM 1 2 subscript \ud835\udd3c similar-to \ud835\udc65 subscript \ud835\udc5d \ud835\udc61 delimited-[] subscript \ud835\udc66 \ud835\udc65 superscript subscript \ud835\udc60 \ud835\udf03 subscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 \ud835\udc65 2 \\mathcal{L}_{\\rm CSM}=\\frac{1}{2}\\mathbb{E}_{x\\sim p_{t}}\\left[\\sum_{y\\neq x}\\left(s_{\\theta}(x_{t},t)_{y}-\\frac{p_{t}(y)}{p_{t}(x)}\\right)^{2}\\right] (8)\n\nDue to its similarities with standard score matching, this approach is rather prominsing. In particular, is a general model and one recovers the true score given infinite data. However, in practice is based on the loss, which is only suitable for real value inputs. Both and are nonnegative, and this mismatch leads to suboptimal gradient behavior. As an example, and induce equal loss signals when , but the value is much worse: dropping the support of the data distribution induces an infinite KL divergence. As such, concrete score matching has seen limited success even in the non diffusion modeling regime. 3 Score Entropy Discrete Diffusion Models\n\nIn this section, we introduce score entropy, our proposed loss. Similar to concrete score matching, we model the concrete score . However, we design this loss to be compatible with the modeled values and the discrete diffusion, necessitating a significantly different expression. Definition 3.1. We define the score entropy for a discrete distribution , weights and a score network as\n\n\u2112 SE = \ud835\udd3c x \u223c p \u200b [ \u2211 y \u2260 x w x \u200b y \u200b ( s \u03b8 \u200b ( x ) y \u2212 p \u200b ( y ) p \u200b ( x ) \u200b log \u2061 s \u03b8 \u200b ( x ) y + K \u200b ( p \u200b ( y ) p \u200b ( x ) ) ) ] subscript \u2112 SE subscript \ud835\udd3c similar-to \ud835\udc65 \ud835\udc5d delimited-[] subscript \ud835\udc66 \ud835\udc65 subscript \ud835\udc64 \ud835\udc65 \ud835\udc66 subscript \ud835\udc60 \ud835\udf03 subscript \ud835\udc65 \ud835\udc66 \ud835\udc5d \ud835\udc66 \ud835\udc5d \ud835\udc65 subscript \ud835\udc60 \ud835\udf03 subscript \ud835\udc65 \ud835\udc66 \ud835\udc3e \ud835\udc5d \ud835\udc66 \ud835\udc5d \ud835\udc65 \\mathcal{L}_{\\rm SE}=\\mathbb{E}_{x\\sim p}\\left[\\sum_{y\\neq x}w_{xy}\\left(s_{\\theta}(x)_{y}-\\frac{p(y)}{p(x)}\\log s_{\\theta}(x)_{y}+K\\left(\\frac{p(y)}{p(x)}\\right)\\right)\\right] (9)\n\nwhere is a normalizing constant function. Remark. Score entropy is a natural extension of the cross-entropy loss function to general positive values (as opposed to probabilities), inspiring the name. The weights are similarity weights between and are used primarily when combining score entropy with diffusion models. While this expression is more complex than the standard score matching variants, we show that the score entropy satisfies several desiderata for a discrete diffusion training objective:\n\n3.1 Score Entropy Properties\n\nFirst, score entropy is a suitable loss function that recovers the ground truth concrete score. Proposition 3.2 (Consistency of Score Entropy). Suppose is fully supported and . As the number of samples and model capacity approaches , the optimal that minimizes Equation 9 satisfies . Furthermore, will be at . Second, score entropy directly improves upon concrete score matching by rescaling problematic gradients. For the weights , , so the gradient signals for each pair are scaled by a factor of as a normalization component. As such, this forms a natural log-barrier which keeps our valid, as shown in Figure 1. Third, similar to concrete score matching, score entropy can be made computationally tractable by removing the unknown . There are two alternative forms, the first of which is analogous to the implicit score matching loss (Hyv\u00e4rinen, 2005):\n\nProposition 3.3 (Implicit Score Entropy). is equal up to a constant independent of to the implicit score entropy\n\n\u2112 ISE = \ud835\udd3c x \u223c p \u200b [ \u2211 y \u2260 x w x \u200b y \u200b s \u03b8 \u200b ( x ) y \u2212 w y \u200b x \u200b log \u2061 s \u03b8 \u200b ( y ) x ] subscript \u2112 ISE subscript \ud835\udd3c similar-to \ud835\udc65 \ud835\udc5d delimited-[] subscript \ud835\udc66 \ud835\udc65 subscript \ud835\udc64 \ud835\udc65 \ud835\udc66 subscript \ud835\udc60 \ud835\udf03 subscript \ud835\udc65 \ud835\udc66 subscript \ud835\udc64 \ud835\udc66 \ud835\udc65 subscript \ud835\udc60 \ud835\udf03 subscript \ud835\udc66 \ud835\udc65 \\mathcal{L}_{\\rm ISE}=\\mathbb{E}_{x\\sim p}\\left[\\sum_{y\\neq x}w_{xy}s_{\\theta}(x)_{y}-w_{yx}\\log s_{\\theta}(y)_{x}\\right] (10)\n\nWe need to evaluate for all , which is intractable, so one must resort to sampling uniformly. This is analogous to the additional variance introduced by the Hutchinson trace estimator (Hutchinson, 1989) for sliced score matching (Song et al., 2019) and, in practice, renders unsuitable for high dimensional problems. Therefore, we work with the score entropy variant of the more empirically practical denoising score matching loss (Vincent, 2011):\n\nTheorem 3.4 (Denoising Score Entropy). Suppose is a perturbation of a base density and a transition kernel , ie is equivalent (up to a constant independent of ) to the denoising score entropy\n\n\u2112 DSE = \ud835\udd3c x 0 \u223c p 0 , x \u223c p ( \u22c5 | x 0 ) \u200b [ \u2211 y \u2260 x w x \u200b y \u200b ( s \u03b8 \u200b ( x ) y \u2212 p \u200b ( y | x 0 ) p \u200b ( x | x 0 ) \u200b log \u2061 s \u03b8 \u200b ( x ) y ) + K \u200b ( p \u200b ( y | x 0 ) p \u200b ( x | x 0 ) ) ] \\mathcal{L}_{\\rm DSE}=\\mathbb{E}_{x_{0}\\sim p_{0},x\\sim p(\\cdot|x_{0})}\\left[\\sum_{y\\neq x}w_{xy}\\left(s_{\\theta}(x)_{y}-\\frac{p(y|x_{0})}{p(x|x_{0})}\\log s_{\\theta}(x)_{y}\\right)+K\\left(\\frac{p(y|x_{0})}{p(x|x_{0})}\\right)\\right] (11)\n\nis scalable since it only requires the evaluation of one , namely . It is also particularly suitable for discrete diffusion since the intermediate densities are all perturbation of the base . In particular, for SEDD, we sample data points , perturb with the forward diffusion transition to sample , and then train with using the transition densities . 3.2 Likelihood Bound For Score Entropy Discrete Diffusion\n\nFourth, the score entropy can be used to define an ELBO for likelihood-based training and evaluation. Definition 3.5. For our time dependent score network , the parameterized reverse matrix is found by replacing the ground truth scores in Equation 7. Our parameterized densities thus satisfy\n\nd \u200b p T \u2212 t \u03b8 d \u200b t = Q \u00af T \u2212 t \u03b8 \u200b p T \u2212 t \u03b8 p T \u03b8 = \u03c0 \u2248 p T formulae-sequence \ud835\udc51 superscript subscript \ud835\udc5d \ud835\udc47 \ud835\udc61 \ud835\udf03 \ud835\udc51 \ud835\udc61 superscript subscript \u00af \ud835\udc44 \ud835\udc47 \ud835\udc61 \ud835\udf03 superscript subscript \ud835\udc5d \ud835\udc47 \ud835\udc61 \ud835\udf03 superscript subscript \ud835\udc5d \ud835\udc47 \ud835\udf03 \ud835\udf0b subscript \ud835\udc5d \ud835\udc47 \\frac{dp_{T-t}^{\\theta}}{dt}=\\overline{Q}_{T-t}^{\\theta}p_{T-t}^{\\theta}\\quad p_{T}^{\\theta}=\\pi\\approx p_{T} (12)\n\nThe log likelihood of data points can be bounded with an ELBO that depends only on the rate matrices (Campbell et al., 2022). Interestingly, this becomes our score entropy loss:\n\nTheorem 3.6 (Likelihood Training and Evaluation). For the diffusion and forward probabilities defined above, we can upper bound the log-likelihood of individual data points\n\n\u2212 log p 0 \u03b8 ( x 0 ) \u2264 \u2112 DWDSE ( x 0 ) + D K \u200b L ( p T | 0 ( \u22c5 | x 0 ) \u2225 \u03c0 ) -\\log p_{0}^{\\theta}(x_{0})\\leq\\mathcal{L}_{\\rm DWDSE}(x_{0})+D_{KL}(p_{T|0}(\\cdot|x_{0})\\parallel\\pi) (13)\n\nwhere is the diffusion weighted denoising score entropy for data point\n\n\u222b 0 T \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \u200b \u2211 y \u2260 x t Q t \u200b ( x t , y ) \u200b ( s \u03b8 \u200b ( x t , t ) y \u2212 p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) \u200b log \u2061 s \u03b8 \u200b ( x t , t ) y + K \u200b ( p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) ) ) \u200b d \u200b t \\int_{0}^{T}\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})}\\sum_{y\\neq x_{t}}Q_{t}(x_{t},y)\\left(s_{\\theta}(x_{t},t)_{y}-\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\log s_{\\theta}(x_{t},t)_{y}+K\\left(\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\right)\\right)dt (14)\n\nThis means that, with a particular diffusion-based weighting scheme for in the form of , our original training setup becomes maximizes likelihood training. We an also report an upper bound on for evaluation purposes. 3.3 Practical Implementation for Language Modeling\n\nFifth, the score entropy can be scaled to high dimensional tasks. In practice, our set factorizes into sequences (e.g. sentences of tokens or image pixel values) . To work with this factorization, our transition matrix instead perturbs tokens independently with a matrix acting on each component . The transition densities directly factorizes\n\np t | 0 \u200b ( \ud835\udc32 | \ud835\udc31 ) = \u220f i = 1 d p t | 0 token \u200b ( y i | x i ) d \u200b p t token d \u200b t = Q t token \u200b p t token formulae-sequence subscript \ud835\udc5d conditional \ud835\udc61 0 conditional \ud835\udc32 \ud835\udc31 superscript subscript product \ud835\udc56 1 \ud835\udc51 superscript subscript \ud835\udc5d conditional \ud835\udc61 0 token conditional superscript \ud835\udc66 \ud835\udc56 superscript \ud835\udc65 \ud835\udc56 \ud835\udc51 superscript subscript \ud835\udc5d \ud835\udc61 token \ud835\udc51 \ud835\udc61 superscript subscript \ud835\udc44 \ud835\udc61 token superscript subscript \ud835\udc5d \ud835\udc61 token p_{t|0}(\\mathbf{y}|\\mathbf{x})=\\prod_{i=1}^{d}p_{t|0}^{\\rm token}(y^{i}|x^{i})\\quad\\frac{dp_{t}^{\\rm{token}}}{dt}=Q_{t}^{\\rm token}p_{t}^{\\rm token} (15)\n\nThe full sequence transition matrix is mostly except when the indexing sequences differ at one position (e.g. and ). By the loss weighting of , we only need to model and learn the ratios between two sequences that differ at one position. This can be modeled similar to non-autoregressive language modeling tasks with our score network where\n\n( s \u03b8 \u200b ( x 1 \u200b \u2026 \u200b x i \u200b \u2026 \u200b x d , t ) ) i , y \u2248 p t \u200b ( x 1 \u200b \u2026 \u200b x ^ i \u200b \u2026 \u200b x d ) p t \u200b ( x 1 \u200b \u2026 \u200b x i \u200b \u2026 \u200b x d ) subscript subscript \ud835\udc60 \ud835\udf03 superscript \ud835\udc65 1 \u2026 superscript \ud835\udc65 \ud835\udc56 \u2026 superscript \ud835\udc65 \ud835\udc51 \ud835\udc61 \ud835\udc56 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 superscript \ud835\udc65 1 \u2026 superscript ^ \ud835\udc65 \ud835\udc56 \u2026 superscript \ud835\udc65 \ud835\udc51 subscript \ud835\udc5d \ud835\udc61 superscript \ud835\udc65 1 \u2026 superscript \ud835\udc65 \ud835\udc56 \u2026 superscript \ud835\udc65 \ud835\udc51 (s_{\\theta}(x^{1}\\dots x^{i}\\dots x^{d},t))_{i,y}\\approx\\frac{p_{t}(x^{1}\\dots\\widehat{x}^{i}\\dots x^{d})}{p_{t}(x^{1}\\dots x^{i}\\dots x^{d})} (16)\n\nTo efficiently compute the other parts of , we need to compute the (token) forward transitions We follow previous convention and define for a fixed graph Laplacian-based and a noise level . If we define as the total noise level, the forward densities thus satisfy\n\np t token = exp ( \u03c3 \u00af ( t ) Q ) p 0 token p t | 0 token ( \u22c5 | x ) = x -th column of exp ( \u03c3 \u00af ( t ) \u22c5 Q ) p_{t}^{\\rm token}=\\exp\\left(\\overline{\\sigma}(t)Q\\right)p_{0}^{\\rm token}\\quad p_{t|0}^{\\rm token}(\\cdot|x)=x\\text{-th column of }\\exp\\left(\\overline{\\sigma}(t)\\cdot Q\\right) (17)\n\nTo scale to GPT-2 experiments (where , , and the batch size is per GPU), there are some practical consequences that render most unusable.",
    "sedd-2": "In particular, one is not able to store all edge weights (since this takes around GB of GPU memory and is extremely slow to access) used to compute . Furthermore, one must be able to compute the columns to get the transition ratios, but again one can\u2019t directly store all of them in memory. We use two standard matrices with special structures that sidestep the above issues. They arise, respectively, from considering a fully connected graph structure and the MASK token used in models such as BERT (Devlin et al., 2019):\n\nQ uniform = \ud835\udfd9 \u2212 N \u200b \ud835\udd40 N \u2208 \u211d N \u00d7 N Q absorb = [ \u2212 1 0 \u22ef 0 0 0 \u2212 1 \u22ef 0 0 \u22ee \u22ee \u22f1 \u22ee \u22ee 0 0 \u22ef \u2212 1 0 1 1 \u22ef 1 0 ] \u2208 \u211d ( N + 1 ) \u00d7 ( N + 1 ) formulae-sequence superscript \ud835\udc44 uniform 1 \ud835\udc41 subscript \ud835\udd40 \ud835\udc41 superscript \u211d \ud835\udc41 \ud835\udc41 superscript \ud835\udc44 absorb matrix 1 0 \u22ef 0 0 0 1 \u22ef 0 0 \u22ee \u22ee \u22f1 \u22ee \u22ee 0 0 \u22ef 1 0 1 1 \u22ef 1 0 superscript \u211d \ud835\udc41 1 \ud835\udc41 1 Q^{\\rm uniform}=\\mathbbm{1}-N\\mathbbm{I}_{N}\\in\\mathbb{R}^{N\\times N}\\quad Q^{\\rm absorb}=\\begin{bmatrix}-1&0&\\cdots&0&0\\\\\n0&-1&\\cdots&0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots&-1&0\\\\\n1&1&\\cdots&1&0\\end{bmatrix}\\in\\mathbb{R}^{(N+1)\\times(N+1)} (18)\n\nNotably, with such a structured , one can compute all the values in quickly without much memory overhead. As such, our score entropy training iterations are about as quick and use a similar amount of memory as a standard autoregressive model training iteration. 4 Simulating Reverse Diffusion with Concrete Scores\n\nGiven our scores , we now derive various strategies for simulating the (factorized) reverse diffusion process . Notably, the additional information that we gain from being an approximate ratio of can be used to enhance the sampling process. 4.1 Time-Reversal Strategies\n\nTo simulate the diffusion in Definition 3.5, one may be tempted to use the Euler strategy from Equation 6. However, as noted in (Campbell et al., 2022), this is inefficient because the structure of means we can only alter one token per step. Instead, a natural alternative has been to use -leaping (Gillespie, 2001) to simultaneously step through all states at once\n\np i \u200b ( x t \u2212 \u0394 \u200b t i | \ud835\udc31 t ) = { \u0394 \u200b t \u22c5 Q t token \u200b ( x t i , x t \u2212 \u0394 \u200b t i ) \u200b s \u03b8 \u200b ( \ud835\udc31 t , t ) i , x t \u2212 \u0394 \u200b t i x t \u2212 \u0394 \u200b t i \u2260 x t i 1 \u2212 \u0394 \u200b t \u200b \u2211 y \u2260 x t i Q t token \u200b ( x t i , y ) \u200b s \u03b8 \u200b ( \ud835\udc31 t , t ) i , y x t \u2212 \u0394 \u200b t i = x t i superscript \ud835\udc5d \ud835\udc56 conditional superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 subscript \ud835\udc31 \ud835\udc61 cases \u22c5 \u0394 \ud835\udc61 superscript subscript \ud835\udc44 \ud835\udc61 token superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 subscript \ud835\udc60 \ud835\udf03 subscript subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc56 1 \u0394 \ud835\udc61 subscript \ud835\udc66 superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc44 \ud835\udc61 token superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc56 \ud835\udc66 subscript \ud835\udc60 \ud835\udf03 subscript subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 \ud835\udc56 \ud835\udc66 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc56 p^{i}(x_{t-\\Delta t}^{i}|\\mathbf{x}_{t})=\\begin{cases}\\Delta t\\cdot Q_{t}^{\\rm token}(x_{t}^{i},x_{t-\\Delta t}^{i})s_{\\theta}(\\mathbf{x}_{t},t)_{i,x_{t-\\Delta t}^{i}}&x_{t-\\Delta t}^{i}\\neq x_{t}^{i}\\\\\n1-\\Delta t\\sum_{y\\neq x_{t}^{i}}Q_{t}^{\\rm token}(x_{t}^{i},y)s_{\\theta}(\\mathbf{x}_{t},t)_{i,y}&x_{t-\\Delta t}^{i}=x_{t}^{i}\\end{cases} (19)\n\nwhere the probabilities are clipped and normalized to account for discretization error. However, this procedure is agnostic to the probabilistic information of . As an alternative, we introduce a discrete analogue of the famous Tweedie\u2019s theorem (Efron, 2011):\n\nTheorem 4.1 (Discrete Tweedie\u2019s Theorem). Suppose that a distribution is a perturbation of a base distribution with a diffusion matrix . Then the (exact) reverse transition is given by\n\np t \u2212 \u03f5 | t ( x t \u2212 \u03f5 | x t ) = ( exp ( \u2212 \u03c3 \u00af Q ) [ p t ( y ) ) p t \u200b ( x t ) ] i = 1 N ) x 0 exp ( \u03c3 \u00af Q ) x t , x 0 p_{t-\\epsilon|t}(x_{t-\\epsilon}|x_{t})=\\left(\\exp(-\\overline{\\sigma}Q)\\begin{bmatrix}\\frac{p_{t}(y))}{p_{t}(x_{t})}\\end{bmatrix}_{i=1}^{N}\\right)_{x_{0}}\\exp(\\overline{\\sigma}Q)_{x_{t},x_{0}} (20)\n\nNote that this denoising scheme can not be directly applied to our language modeling task. In particular, we are not modeling the ratios between any two sequences, as otherwise this would allow us to generate from to in only one step. However, we can use this intuition to build an Tweedie -leaping update:\n\np ( x t \u2212 \u0394 \u200b t i ) = ( exp ( \u03c3 \u00af \u0394 \u200b t ( t ) \\displaystyle p(x_{t-\\Delta t}^{i})=\\big{(}\\exp(\\overline{\\sigma}_{\\Delta t}(t) Q ) s \u03b8 ( \ud835\udc31 t , t ) i ) x t \u2212 \u0394 \u200b t i exp ( \u03c3 \u00af \u0394 \u200b t ( t ) Q ) x t i , x t \u2212 \u0394 \u200b t i \\displaystyle Q)s_{\\theta}(\\mathbf{x}^{t},t)_{i}\\big{)}_{x_{t-\\Delta t}^{i}}\\exp(\\overline{\\sigma}_{\\Delta t}(t)Q)_{x_{t}^{i},x_{t-\\Delta t}^{i}} (21) \u03c3 \u00af \u0394 \u200b t \u200b ( t ) = \u03c3 \u00af \u200b ( t ) \u2212 \u03c3 \u00af \u200b ( t \u2212 \u0394 \u200b t ) subscript \u00af \ud835\udf0e \u0394 \ud835\udc61 \ud835\udc61 \u00af \ud835\udf0e \ud835\udc61 \u00af \ud835\udf0e \ud835\udc61 \u0394 \ud835\udc61 \\displaystyle\\overline{\\sigma}_{\\Delta t}(t)=\\overline{\\sigma}(t)-\\overline{\\sigma}(t-\\Delta t) (22)\n\nInterestingly, this is optimal when one forces the -leaping discretization:\n\nTheorem 4.2 (Mean Parameterization and Scores). Let be the update rule defined by Equation 21 when our score function is learned perfectly. Then, minimizes the KL divergence when we restrict to factorize by dimension. Furthermore, this recover the analytic sampling method for other methods (Appendix C.2). 4.2 Arbitrary Prompting and Infilling\n\nOur concrete score can be used to enable greater control over the generative process. In particular, we consider the infilling problem defined by the conditional probabilities\n\np t \u200b ( \ud835\udc31 \u03a9 | \ud835\udc31 \u03a9 \u00af = \ud835\udc32 ) \u03a9 \u200b unfilled indices \u03a9 \u00af \u200b already filled indices. subscript \ud835\udc5d \ud835\udc61 conditional superscript \ud835\udc31 \u03a9 superscript \ud835\udc31 \u00af \u03a9 \ud835\udc32 \u03a9 unfilled indices \u00af \u03a9 already filled indices. p_{t}(\\mathbf{x}^{\\Omega}|\\mathbf{x}^{\\overline{\\Omega}}=\\mathbf{y})\\quad\\Omega\\text{ unfilled indices}\\quad\\overline{\\Omega}\\text{ already filled indices.} (23)\n\nfor example, a standard autoregressive conditional generation would have and . By Bayes\u2019 rule, the conditional scores can be recovered exactly from the unconditional score. p t \u200b ( \ud835\udc31 \u03a9 = \ud835\udc33 \u2032 | \ud835\udc31 \u03a9 \u00af = \ud835\udc32 ) p t \u200b ( \ud835\udc31 \u03a9 = \ud835\udc33 | \ud835\udc31 \u03a9 \u00af = \ud835\udc32 ) = p t \u200b ( \ud835\udc31 = \ud835\udc33 \u2032 \u2295 \ud835\udc32 ) p t \u200b ( \ud835\udc31 = \ud835\udc33 \u2295 \ud835\udc32 ) \u2295 is concatenation subscript \ud835\udc5d \ud835\udc61 superscript \ud835\udc31 \u03a9 conditional superscript \ud835\udc33 \u2032 superscript \ud835\udc31 \u00af \u03a9 \ud835\udc32 subscript \ud835\udc5d \ud835\udc61 superscript \ud835\udc31 \u03a9 conditional \ud835\udc33 superscript \ud835\udc31 \u00af \u03a9 \ud835\udc32 subscript \ud835\udc5d \ud835\udc61 \ud835\udc31 direct-sum superscript \ud835\udc33 \u2032 \ud835\udc32 subscript \ud835\udc5d \ud835\udc61 \ud835\udc31 direct-sum \ud835\udc33 \ud835\udc32 direct-sum is concatenation \\frac{p_{t}(\\mathbf{x}^{\\Omega}=\\mathbf{z}^{\\prime}|\\mathbf{x}^{\\overline{\\Omega}}=\\mathbf{y})}{p_{t}(\\mathbf{x}^{\\Omega}=\\mathbf{z}|\\mathbf{x}^{\\overline{\\Omega}}=\\mathbf{y})}=\\frac{p_{t}(\\mathbf{x}=\\mathbf{z}^{\\prime}\\oplus\\mathbf{y})}{p_{t}(\\mathbf{x}=\\mathbf{z}\\oplus\\mathbf{y})}\\quad\\oplus\\text{ is concatenation} (24)\n\nWe can therefore approximate the relevant ratios (namely those with with one changed index) with our vanilla score function , which justifies the following sampling procedure\n\n\ud835\udc31 t \u2212 \u0394 \u200b t = proj \u03a9 \u00af \u2192 \ud835\udc32 \u200b ( sample t \u2212 \u0394 \u200b t \u200b ( \ud835\udc31 t , s \u03b8 ) ) subscript \ud835\udc31 \ud835\udc61 \u0394 \ud835\udc61 subscript proj \u2192 \u00af \u03a9 \ud835\udc32 subscript sample t \u0394 t subscript \ud835\udc31 t subscript s \ud835\udf03 \\mathbf{x}_{t-\\Delta t}=\\mathrm{proj}_{\\overline{\\Omega}\\to\\mathbf{y}}(\\rm{sample}_{t-\\Delta t}(\\mathbf{x}_{t},s_{\\theta})) (25)\n\n(i.e. projecting the known indices after each score function-based step/only changing the unknown indices). In principle, we can also bound the likelihoods using Theorem 3.6. However, one major problem is that the probabilities may be low, making it hard to learn . If this is the case, we can follow previous work and approximate this with (Song et al., 2020). 5 Experiments\n\nWe now empirically validate that our score entropy discrete diffusion (SEDD) model can compete with existing large-scale autoregressive models, namely GPT-2 (Radford et al., 2019). 5.1 Model and Training Setup\n\nOur model is a standard encoder-only transformer architecture (Vaswani et al., 2017) similar to standard masked language models (Devlin et al., 2019). However, our model incorporates the time conditioning method from (Peebles & Xie, 2022) and uses rotary instead of positional encodings Su et al. (2021). Instead of outputting directly, we instead output to maintain positivity without clipping the output or gradients. We report results for both the uniform and absorbing token matrices and . For the absorbing transition for reported perplexities, we use a log-linear noise schedule that will mask out tokens. For all other experiments/generations, we used a geometric noise schedule that interpolates between and . Outside of this, we did not systemically explore noise schedules or alternative loss weighting, although these will most likely improve sample perplexity and generation (as is commonly seen for continuous diffusion). We train on OpenWebText (Gokaslan & Cohen, 2019), an open source recreation of the WebText dataset used for training GPT-2.",
    "sedd-3": "We matched the architecture sizes of GPT-2, although our models have slightly more non-embedding parameters () due to the additional time conditioning network. Further experimental and architecture details are given in Appendix B. 5.2 Perplexity Score Comparison\n\nWe follow GPT-2 and report zero-shot perplexities on the LAMBADA, WikiText2, PTB, WikiText103, and 1 Billion Words datasets. We recompute baseline likelihoods for all datasets except 1BW, where we encountered unexpected behavior with the public implementations. Our likelihood computation changes from the original setting since use different splits and evaluate unconditional likelihoods (ie without a sliding window). This results in slightly higher perplexities for GPT-2 than originally reported, although this is minor on most datasets. Our results are reported in Table 1. Our absorbing transition models effectively match the performance of GPT-2, as their perplexities are commonly within a . Prior work Song et al. (2021) has shown that this is around the gap between exact likelihoods and the variational bounded for continuous space diffusion, although it is unknown if holds true for discrete space. However, the uniform transition models consistently underperforms. 5.3 Sample Quality Comparison\n\nWe also test the quality of our generated samples. In particular, we generate unconditional samples of length and report the generative perplexity (as measured by GPT-2 Large). We use the analytic sampling method for GPT-2 and our reverse diffusion sampler for SEDD to fairly compare the modeled probability distributions. Note that other commonly used autoregressive sampling methods (ie beam search or nucleus sampling) don\u2019t sample from the true distribution and are adversarial against our evaluation objective since they are made to explicitly decrease the perplexity of the generated sequences (Freitag & Al-Onaizan, 2017; Holtzman et al., 2019). Our results (for small models) are shown in Figure 2. SEDD with absorbing transition reliably outperforms GPT-2, in addition to creating a time/quality tradeoff curve that is log-log linear. We can also see the added consistency and fluency of our approach in the generated samples. More information about the generative perplexity of other models (e.g. uniform transition, euler sampling) and additional samples are given in Appendix C.4. 5.4 Arbitrary Infilling\n\nFinally, we showcase our ability to condition our generation with inputs at arbitrary location. Our results are shown in Table 2. We can see that the additional flexibility offered by our framework can lead to more options to control the text generation. In addition to the standard autoregressive prompting at the start of the sentence, our model is able to generate from the end, middle, or with multiple prompts at different locations.",
    "sedd-4": "Additional samples are given in Appendix C.4. 6 Related Work\n\nMean Prediction Parameterization. Many previous discrete diffusion works draw inspiration from Ho et al. (2020) and parameterize with the \u201cmean\" probability distribution (Austin et al., 2021; Hoogeboom et al., 2021; Campbell et al., 2022; He et al., 2022). However, these models typically perform anywhere from to times worse than autoregressive models on non-toy language modeling datasets like 1BW (Austin et al., 2021; He et al., 2022). We discuss more similarities and differences and corroborate that mean parameterization is unsuitable for our scale in Appendix C.2. Ratio Matching Conditionals. (Sun et al., 2022) notes that the ratios parameterize the reverse distribution but instead optimizes with the ratio matching loss (Hyv\u00e4rinen, 2007), which parameterizes the conditional distributions . This setup differs greatly because one must construct specialized architectures to make sure does not affect (Chen & Duvenaud, 2019), which is tough to scale and has seen limited success in non-language modeling tasks. Learned Dequantizations. Several works instead examine continuous relaxations of the discrete diffusion process (Li et al., 2022; Dieleman et al., 2022; Gulrajani & Hashimoto, 2023). By doing so, these models can leverage the existing continuous-space diffusion framework. However, such a graph embedding is hard and can result in sparse signals for the model to learn. As such, when compared with autoregressive models, these require the parameters for similar perplexities (Gulrajani & Hashimoto, 2023) and a comparable (if not more) amount of sampling steps to generate high quality samples. 7 Future Work\n\nDespite our contribution, much work remains before discrete diffusion models can truly rival modern autoregressive models. For example, the effects of scaling on performance (Hoffmann et al., 2022) are unexplored, and diffusion models are limited since they generate full length outputs. Furthermore, existing systems level improvements such as the KV cache can cut into our algorithmic speedup. However, our work takes the crucial first step in showcasing competitive viability and demonstrating tangible benefits for language diffusion modeling. Therefore, we remain optimistic that future work in this direction can rival the domination of autoregressive models. By generalizing improvements in the continuous diffusion model framework, future work could tune the noise schedule to further improve generation quality (Dhariwal & Nichol, 2021), leverage score-based controllability methods Ho (2022), or further reduce the number of sampling steps (Song et al., 2023). 8 Conclusion\n\nWe have introduced score entropy discrete diffusion (SEDD) models, a new class of discrete diffusion model that is parameterized by the concrete score and can be learned efficiently with our novel score entropy loss. SEDD achieves competitive performance in a direct head-to-head with GPT-2, almost matching likelihoods while generating higher quality samples with more control options. We hope that future work can build off of our framework to define diffusion model alternatives to the modern autoregressive language modeling paradigm. 9 Acknowledgements\n\nThis project was supported by NSF (#1651565), ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), CZ Biohub, a Stanford HAI GCP grant, and Pika Labs. AL is supported by a NSF Graduate Research Fellowship. References\n\nAnderson (1982) Brian. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12:313\u2013326, 1982. URL https://api.semanticscholar.org/CorpusID:3897405. Austin et al. (2021) Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. ArXiv, abs/2107.03006, 2021. URL https://api.semanticscholar.org/CorpusID:235755106. Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.",
    "sedd-5": "ArXiv, abs/2005.14165, 2020. URL https://api.semanticscholar.org/CorpusID:218971783. Campbell et al. (2022) Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and A. Doucet. A continuous time framework for discrete denoising models. ArXiv, abs/2205.14987, 2022. URL https://api.semanticscholar.org/CorpusID:249192370. Chen & Duvenaud (2019) Ricky T. Q. Chen and David Kristjanson Duvenaud. Neural networks with cheap differential operators. In Neural Information Processing Systems, 2019. URL https://api.semanticscholar.org/CorpusID:202787949. Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u2019e. Flashattention: Fast and memory-efficient exact attention with io-awareness. ArXiv, abs/2205.14135, 2022. URL https://api.semanticscholar.org/CorpusID:249151871. Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. URL https://api.semanticscholar.org/CorpusID:52967399. Dhariwal & Nichol (2021) Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. ArXiv, abs/2105.05233, 2021. URL https://api.semanticscholar.org/CorpusID:234357997. Dieleman et al. (2022) Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, A. Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, R\u00e9mi Leblond, Will Grathwohl, and Jonas Adler. Continuous diffusion for categorical data.",
    "sedd-6": "ArXiv, abs/2211.15089, 2022. URL https://api.semanticscholar.org/CorpusID:254044531. Efron (2011) Bradley Efron. Tweedie\u2019s formula and selection bias.",
    "sedd-7": "Journal of the American Statistical Association, 106:1602 \u2013 1614, 2011. URL https://api.semanticscholar.org/CorpusID:23284154. Freitag & Al-Onaizan (2017) Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. In NMT@ACL, 2017. URL https://api.semanticscholar.org/CorpusID:2229477. Gillespie (2001) Daniel T. Gillespie. Approximate accelerated stochastic simulation of chemically reacting systems.",
    "sedd-8": "Journal of Chemical Physics, 115:1716\u20131733, 2001. URL https://api.semanticscholar.org/CorpusID:5109777. Gokaslan & Cohen (2019) Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. Gulrajani & Hashimoto (2023) Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. ArXiv, abs/2305.18619, 2023. URL https://api.semanticscholar.org/CorpusID:258967177. He et al. (2022) Zhengfu He, Tianxiang Sun, Kuan Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models.",
    "sedd-9": "In Annual Meeting of the Association for Computational Linguistics, 2022. URL https://api.semanticscholar.org/CorpusID:254044147. Ho (2022) Jonathan Ho. Classifier-free diffusion guidance. ArXiv, abs/2207.12598, 2022. URL https://api.semanticscholar.org/CorpusID:249145348. Ho et al. (2020) Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. ArXiv, abs/2006.11239, 2020. URL https://api.semanticscholar.org/CorpusID:219955663. Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W.",
    "sedd-10": "Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https://api.semanticscholar.org/CorpusID:247778764. Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. ArXiv, abs/1904.09751, 2019. URL https://api.semanticscholar.org/CorpusID:127986954. Hoogeboom et al. (2021) Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. ArXiv, abs/2110.02037, 2021. URL https://api.semanticscholar.org/CorpusID:238354021. Huang et al. (2021) Chin-Wei Huang, Jae Hyun Lim, and Aaron C. Courville. A variational perspective on diffusion-based generative models and score matching. ArXiv, abs/2106.02808, 2021. URL https://api.semanticscholar.org/CorpusID:235358715. Hutchinson (1989) Michael F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines.",
    "sedd-11": "Communications in Statistics - Simulation and Computation, 18:1059\u20131076, 1989. URL https://api.semanticscholar.org/CorpusID:120969358. Hyv\u00e4rinen (2005) Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res., 6:695\u2013709, 2005. URL https://api.semanticscholar.org/CorpusID:1152227. Hyv\u00e4rinen (2007) Aapo Hyv\u00e4rinen. Some extensions of score matching. Comput. Stat. Data Anal., 51:2499\u20132512, 2007. URL https://api.semanticscholar.org/CorpusID:2352990. Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models.",
    "sedd-12": "ArXiv, abs/2206.00364, 2022. URL https://api.semanticscholar.org/CorpusID:249240415. Kelly (1980) Frank Kelly. Reversibility and stochastic networks.",
    "sedd-13": "1980. URL https://api.semanticscholar.org/CorpusID:125211322. Kingma et al. (2021) Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. ArXiv, abs/2107.00630, 2021. URL https://api.semanticscholar.org/CorpusID:235694314. Li et al. (2022) Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation.",
    "sedd-14": "ArXiv, abs/2205.14217, 2022. URL https://api.semanticscholar.org/CorpusID:249192356. Meng et al. (2022) Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data.",
    "sedd-15": "ArXiv, abs/2211.00802, 2022. URL https://api.semanticscholar.org/CorpusID:253255468. Peebles & Xie (2022) William S. Peebles and Saining Xie. Scalable diffusion models with transformers. ArXiv, abs/2212.09748, 2022. URL https://api.semanticscholar.org/CorpusID:254854389. Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.",
    "sedd-16": "2019. URL https://api.semanticscholar.org/CorpusID:160025533. Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. URL https://api.semanticscholar.org/CorpusID:248097655. Sohl-Dickstein et al. (2015) Jascha Narain Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.",
    "sedd-17": "ArXiv, abs/1503.03585, 2015. URL https://api.semanticscholar.org/CorpusID:14888175. Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Neural Information Processing Systems, 2019. URL https://api.semanticscholar.org/CorpusID:196470871. Song et al. (2019) Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Conference on Uncertainty in Artificial Intelligence, 2019. URL https://api.semanticscholar.org/CorpusID:158047026. Song et al. (2020) Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ArXiv, abs/2011.13456, 2020. URL https://api.semanticscholar.org/CorpusID:227209335. Song et al. (2021) Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models.",
    "sedd-18": "In Neural Information Processing Systems, 2021. URL https://api.semanticscholar.org/CorpusID:235352469. Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:257280191. Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "sedd-19": "ArXiv, abs/2104.09864, 2021. URL https://api.semanticscholar.org/CorpusID:233307138. Sun et al. (2022) Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models.",
    "sedd-20": "ArXiv, abs/2211.16750, 2022. URL https://api.semanticscholar.org/CorpusID:254096040. Vaswani et al. (2017) Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. URL https://api.semanticscholar.org/CorpusID:13756489. Vincent (2011) Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23:1661\u20131674, 2011. URL https://api.semanticscholar.org/CorpusID:5560643. Zheng et al. (2023) Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation.",
    "sedd-21": "ArXiv, abs/2302.05737, 2023. URL https://api.semanticscholar.org/CorpusID:256826865. Appendix A Proof of Main Results\n\nProof of Prop 3.3. The trick is to decompose the (non-constant) parts of the term into two expectations. For the second, corresponding to the middle component, we have\n\n\ud835\udd3c x \u223c p \u200b \u2211 y \u2260 x p \u200b ( y ) p \u200b ( x ) \u200b f \u200b ( x , y ) subscript \ud835\udd3c similar-to \ud835\udc65 \ud835\udc5d subscript \ud835\udc66 \ud835\udc65 \ud835\udc5d \ud835\udc66 \ud835\udc5d \ud835\udc65 \ud835\udc53 \ud835\udc65 \ud835\udc66 \\displaystyle\\mathbb{E}_{x\\sim p}\\sum_{y\\neq x}\\frac{p(y)}{p(x)}f(x,y) = \u2211 x , y : x \u2260 y p \u200b ( y ) p \u200b ( x ) \u200b p \u200b ( x ) \u200b f \u200b ( x , y ) absent subscript : \ud835\udc65 \ud835\udc66 \ud835\udc65 \ud835\udc66 \ud835\udc5d \ud835\udc66 \ud835\udc5d \ud835\udc65 \ud835\udc5d \ud835\udc65 \ud835\udc53 \ud835\udc65 \ud835\udc66 \\displaystyle=\\sum_{x,y:x\\neq y}\\frac{p(y)}{p(x)}p(x)f(x,y) = \u2211 x , y : x \u2260 y p \u200b ( y ) \u200b f \u200b ( x , y ) absent subscript : \ud835\udc65 \ud835\udc66 \ud835\udc65 \ud835\udc66 \ud835\udc5d \ud835\udc66 \ud835\udc53 \ud835\udc65 \ud835\udc66 \\displaystyle=\\sum_{x,y:x\\neq y}p(y)f(x,y) = \ud835\udd3c y \u223c p \u200b \u2211 x \u2260 y f \u200b ( x , y ) absent subscript \ud835\udd3c similar-to \ud835\udc66 \ud835\udc5d subscript \ud835\udc65 \ud835\udc66 \ud835\udc53 \ud835\udc65 \ud835\udc66 \\displaystyle=\\mathbb{E}_{y\\sim p}\\sum_{x\\neq y}f(x,y) = \ud835\udd3c x \u223c p \u200b \u2211 y \u2260 x f \u200b ( y , x ) absent subscript \ud835\udd3c similar-to \ud835\udc65 \ud835\udc5d subscript \ud835\udc66 \ud835\udc65 \ud835\udc53 \ud835\udc66 \ud835\udc65 \\displaystyle=\\mathbb{E}_{x\\sim p}\\sum_{y\\neq x}f(y,x)\n\nwhere is our . \u220e\n\nProof of Thm 3.4. This is similar to the same denoising variant for concrete score matching. We just need to show that the marginalizes out, since everything else does not change or is a constant. \ud835\udd3c x t \u223c p t \u200b \u2211 y \u2260 x t f \u200b ( x t , y ) \u200b p t \u200b ( y ) p t \u200b ( x t ) subscript \ud835\udd3c similar-to subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 \ud835\udc53 subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \\displaystyle\\mathbb{E}_{x_{t}\\sim p_{t}}\\sum_{y\\neq x_{t}}f(x_{t},y)\\frac{p_{t}(y)}{p_{t}(x_{t})} = \u2211 y \u2260 x t f \u200b ( x t , y ) \u200b p t \u200b ( y ) absent subscript \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 \ud835\udc53 subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 \ud835\udc66 \\displaystyle=\\sum_{y\\neq x_{t}}f(x_{t},y)p_{t}(y) = \u2211 y \u2260 x t \u2211 x 0 f \u200b ( x t , y ) \u200b p t \u200b ( y | x 0 ) \u200b p 0 \u200b ( x 0 ) absent subscript \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript subscript \ud835\udc65 0 \ud835\udc53 subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 conditional \ud835\udc66 subscript \ud835\udc65 0 subscript \ud835\udc5d 0 subscript \ud835\udc65 0 \\displaystyle=\\sum_{y\\neq x_{t}}\\sum_{x_{0}}f(x_{t},y)p_{t}(y|x_{0})p_{0}(x_{0}) = \ud835\udd3c x 0 \u223c p 0 \u200b \u2211 y \u2260 x t f \u200b ( x t , y ) \u200b p t \u200b ( y | x 0 ) p t \u200b ( x | x 0 ) \u200b p t \u200b ( x | x 0 ) absent subscript \ud835\udd3c similar-to subscript \ud835\udc65 0 subscript \ud835\udc5d 0 subscript \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 \ud835\udc53 subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 subscript \ud835\udc5d \ud835\udc61 conditional \ud835\udc66 subscript \ud835\udc65 0 subscript \ud835\udc5d \ud835\udc61 conditional \ud835\udc65 subscript \ud835\udc65 0 subscript \ud835\udc5d \ud835\udc61 conditional \ud835\udc65 subscript \ud835\udc65 0 \\displaystyle=\\mathbb{E}_{x_{0}\\sim p_{0}}\\sum_{y\\neq x_{t}}f(x_{t},y)\\frac{p_{t}(y|x_{0})}{p_{t}(x|x_{0})}p_{t}(x|x_{0}) = \ud835\udd3c x 0 \u223c p 0 , x t \u223c p t ( \u22c5 | x 0 ) \u200b \u2211 y \u2260 x t f \u200b ( x t , y ) \u200b p t \u200b ( y | x 0 ) p t \u200b ( x | x 0 ) \\displaystyle=\\mathbb{E}_{x_{0}\\sim p_{0},x_{t}\\sim p_{t}(\\cdot|x_{0})}\\sum_{y\\neq x_{t}}f(x_{t},y)\\frac{p_{t}(y|x_{0})}{p_{t}(x|x_{0})}\n\nProof of Thm 3.6. The full bound is given by\n\n\u2212 log p 0 \u03b8 ( x 0 ) \u2264 \u2112 DWDSE ( x 0 ) + D KL ( p T | 0 ( \u22c5 | x 0 ) \u2225 \u03c0 ) -\\log p_{0}^{\\theta}(x_{0})\\leq\\mathcal{L}_{\\rm DWDSE}(x_{0})+D_{\\rm KL}(p_{T|0}(\\cdot|x_{0})\\parallel\\pi) (26)\n\nwhere is given by\n\n\u222b 0 T \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \u200b \u2211 y \u2260 x t Q t \u200b ( x t , y ) \u200b ( s \u03b8 \u200b ( x t , t ) y \u2212 p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) \u200b log \u2061 s \u03b8 \u200b ( x , t ) y + K \u200b ( p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) ) ) \u200b d \u200b t \\int_{0}^{T}\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})}\\sum_{y\\neq x_{t}}Q_{t}(x_{t},y)\\left(s_{\\theta}(x_{t},t)_{y}-\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\log s_{\\theta}(x,t)_{y}+K\\left(\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\right)\\right)dt\n\nEffectively, is the path measure KL divergence (Campbell et al., 2022; Song et al., 2021), and the proof follows similarly. In particular, we have that, by the data processing inequality\n\n\u2212 log \u2061 p 0 \u03b8 \u200b ( x 0 ) = D KL \u200b ( \u03b4 x 0 \u2225 p 0 \u03b8 ) \u2264 D KL \u200b ( \u2119 x 0 \u2225 \u2119 \u03b8 ) superscript subscript \ud835\udc5d 0 \ud835\udf03 subscript \ud835\udc65 0 subscript \ud835\udc37 KL conditional subscript \ud835\udeff subscript \ud835\udc65 0 superscript subscript \ud835\udc5d 0 \ud835\udf03 subscript \ud835\udc37 KL conditional subscript \u2119 subscript \ud835\udc65 0 superscript \u2119 \ud835\udf03 -\\log p_{0}^{\\theta}(x_{0})=D_{\\rm KL}(\\delta_{x_{0}}\\parallel p_{0}^{\\theta})\\leq D_{\\rm KL}(\\mathbb{P}_{x_{0}}\\parallel\\mathbb{P}^{\\theta}) (27)\n\nwhere is the path measure for the reverse of the noising process applied to and is the learned reverse process. Generally, we can replace with a more general data distribution , with the computation remaining the same. We have,\n\nD KL ( \u2119 x 0 \u2225 \u2119 \u03b8 ) \u2264 \ud835\udd3c x T \u223c p T | 0 ( \u22c5 | x 0 ) [ D K \u200b L ( \u2119 x 0 ( \u22c5 | x T ) \u2225 \u2119 \u03b8 ( \u22c5 | x T ) ) ] + D KL ( p T | 0 ( \u22c5 | x 0 ) \u2225 \u03c0 ) D_{\\rm KL}(\\mathbb{P}_{x_{0}}\\parallel\\mathbb{P}^{\\theta})\\leq\\mathbb{E}_{x_{T}\\sim p_{T|0}(\\cdot|x_{0})}\\left[D_{KL}(\\mathbb{P}_{x_{0}}(\\cdot|x_{T})\\parallel\\mathbb{P}^{\\theta}(\\cdot|x_{T}))\\right]+D_{\\rm KL}(p_{T|0}(\\cdot|x_{0})\\parallel\\pi) (28)\n\nWe analyze the term , which we can compute by Dynkin\u2019s formula (Campbell et al., 2022). This is given by\n\n\u222b 0 T \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \\displaystyle\\int_{0}^{T}\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})} \u2211 y \u2260 x t Q t \u03b8 \u200b ( y , x t ) \u2212 Q t \u200b ( y , x t ) \u200b log \u2061 ( Q t \u03b8 \u200b ( x t , y ) ) subscript \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 superscript subscript \ud835\udc44 \ud835\udc61 \ud835\udf03 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc44 \ud835\udc61 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 superscript subscript \ud835\udc44 \ud835\udc61 \ud835\udf03 subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 \\displaystyle\\sum_{y\\neq x_{t}}Q_{t}^{\\theta}(y,x_{t})-Q_{t}(y,x_{t})\\log(Q_{t}^{\\theta}(x_{t},y)) (29) + Q t \u200b ( y , x t ) \u200b log \u2061 Q t \u200b ( y , x t ) + Q t \u200b ( x t , y ) \u200b B \u200b ( p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) ) \u200b d \u200b t subscript \ud835\udc44 \ud835\udc61 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc44 \ud835\udc61 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc44 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 \ud835\udc35 subscript \ud835\udc5d conditional \ud835\udc61 0 conditional \ud835\udc66 subscript \ud835\udc65 0 subscript \ud835\udc5d conditional \ud835\udc61 0 conditional subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc65 0 \ud835\udc51 \ud835\udc61 \\displaystyle+Q_{t}(y,x_{t})\\log Q_{t}(y,x_{t})+Q_{t}(x_{t},y)B\\left(\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\right)dt (30)\n\nSince our reverse rate matrices are parameterized with , we can simplify the above to\n\n\u222b 0 T \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \u200b \u2211 y \u2260 x t Q t \u200b ( x t , y ) \u200b ( s \u03b8 \u200b ( x t ) y + B \u200b ( p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) ) ) \u2212 Q \u200b ( y , x t ) \u200b log \u2061 s \u03b8 \u200b ( y ) x t \\int_{0}^{T}\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})}\\sum_{y\\neq x_{t}}Q_{t}(x_{t},y)\\left(s_{\\theta}(x_{t})_{y}+B\\left(\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\right)\\right)-Q(y,x_{t})\\log s_{\\theta}(y)_{x_{t}} (31)\n\nTo finalize, we simply note that the summation over can be simplified with the (reverse of) the trick used for proving 3.3. \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \u200b \u2211 y \u2260 x t Q \u200b ( y , x t ) \u200b log \u2061 s \u03b8 \u200b ( y ) x t \\displaystyle\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})}\\sum_{y\\neq x_{t}}Q(y,x_{t})\\log s_{\\theta}(y)_{x_{t}} = \u2211 x t , y \u2260 x t p t | 0 \u200b ( x t | x 0 ) \u200b Q \u200b ( y , x t ) \u200b log \u2061 s \u03b8 \u200b ( y ) x t absent subscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc5d conditional \ud835\udc61 0 conditional subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc65 0 \ud835\udc44 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc60 \ud835\udf03 subscript \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 \\displaystyle=\\sum_{x_{t},y\\neq x_{t}}p_{t|0}(x_{t}|x_{0})Q(y,x_{t})\\log s_{\\theta}(y)_{x_{t}} (32) = \ud835\udd3c y \u223c p t | 0 ( \u22c5 | x 0 ) \u200b p t | 0 \u200b ( x t | x 0 ) p t | 0 \u200b ( y | x 0 ) \u200b Q \u200b ( y , x t ) \u200b log \u2061 s \u03b8 \u200b ( y ) x t \\displaystyle=\\mathbb{E}_{y\\sim p_{t|0}(\\cdot|x_{0})}\\frac{p_{t|0}(x_{t}|x_{0})}{p_{t|0}(y|x_{0})}Q(y,x_{t})\\log s_{\\theta}(y)_{x_{t}} (33) = \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \u200b p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) \u200b Q \u200b ( x t ) \u200b log \u2061 s \u03b8 \u200b ( x t ) y \\displaystyle=\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})}\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}Q(x_{t})\\log s_{\\theta}(x_{t})_{y} (34)\n\nwhere the last line is just a permutation of the notation of and . As such, we get the desired loss\n\n\u222b 0 T \ud835\udd3c x t \u223c p t | 0 ( \u22c5 | x 0 ) \u200b \u2211 y \u2260 x t Q t \u200b ( x t , y ) \u200b ( s \u03b8 \u200b ( x t , t ) y \u2212 p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) \u200b log \u2061 s \u03b8 \u200b ( x , t ) y + B \u200b ( p t | 0 \u200b ( y | x 0 ) p t | 0 \u200b ( x t | x 0 ) ) ) \u200b d \u200b t \\int_{0}^{T}\\mathbb{E}_{x_{t}\\sim p_{t|0}(\\cdot|x_{0})}\\sum_{y\\neq x_{t}}Q_{t}(x_{t},y)\\left(s_{\\theta}(x_{t},t)_{y}-\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\log s_{\\theta}(x,t)_{y}+B\\left(\\frac{p_{t|0}(y|x_{0})}{p_{t|0}(x_{t}|x_{0})}\\right)\\right)dt\n\nProof of Thm 4.1. This can be shown by Bayes\u2019 rule:\n\np 0 | t \u200b ( x 0 | x t ) = p t | 0 \u200b ( x t | x 0 ) \u200b p 0 \u200b ( x 0 ) p t \u200b ( x t ) = p t | 0 \u200b ( x t | x 0 ) \u200b p 0 \u200b ( x 0 ) p t \u200b ( x t ) subscript \ud835\udc5d conditional 0 \ud835\udc61 conditional subscript \ud835\udc65 0 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc5d conditional \ud835\udc61 0 conditional subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc65 0 subscript \ud835\udc5d 0 subscript \ud835\udc65 0 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc5d conditional \ud835\udc61 0 conditional subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc65 0 subscript \ud835\udc5d 0 subscript \ud835\udc65 0 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 p_{0|t}(x_{0}|x_{t})=\\frac{p_{t|0}(x_{t}|x_{0})p_{0}(x_{0})}{p_{t}(x_{t})}=p_{t|0}(x_{t}|x_{0})\\frac{p_{0}(x_{0})}{p_{t}(x_{t})} (35)\n\nWe have and , so the theorem follows. \u220e\n\nProof of Thm 4.2. This is equivalent to maximizing for each . We can reparameterize the ground truth by Bayes rule\n\np t \u2212 \u0394 \u200b t | t \u200b ( x t \u2212 \u0394 \u200b t i | \ud835\udc31 t ) subscript \ud835\udc5d \ud835\udc61 conditional \u0394 \ud835\udc61 \ud835\udc61 conditional superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 subscript \ud835\udc31 \ud835\udc61 \\displaystyle p_{t-\\Delta t|t}(x_{t-\\Delta t}^{i}|\\mathbf{x}_{t}) = p t \u2212 \u0394 \u200b t | t \u200b ( x t 1 \u200b \u2026 \u200b x t \u2212 \u0394 \u200b t i \u200b \u2026 \u200b x t d | \ud835\udc31 t ) absent subscript \ud835\udc5d \ud835\udc61 conditional \u0394 \ud835\udc61 \ud835\udc61 conditional superscript subscript \ud835\udc65 \ud835\udc61 1 \u2026 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 \u2026 superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc51 subscript \ud835\udc31 \ud835\udc61 \\displaystyle=p_{t-\\Delta t|t}(x_{t}^{1}\\dots x_{t-\\Delta t}^{i}\\dots x_{t}^{d}|\\mathbf{x}_{t}) = p t | t \u2212 \u0394 \u200b t \u200b ( \ud835\udc31 t | x t 1 \u200b \u2026 \u200b x t \u2212 \u0394 \u200b t i \u200b \u2026 \u200b x t d ) \u200b p t \u2212 \u0394 \u200b t \u200b ( x t 1 \u200b \u2026 \u200b x t \u2212 \u0394 \u200b t i \u200b \u2026 \u200b x t d ) p t \u200b ( \ud835\udc31 t ) absent subscript \ud835\udc5d conditional \ud835\udc61 \ud835\udc61 \u0394 \ud835\udc61 conditional subscript \ud835\udc31 \ud835\udc61 superscript subscript \ud835\udc65 \ud835\udc61 1 \u2026 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 \u2026 superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc51 subscript \ud835\udc5d \ud835\udc61 \u0394 \ud835\udc61 superscript subscript \ud835\udc65 \ud835\udc61 1 \u2026 superscript subscript \ud835\udc65 \ud835\udc61 \u0394 \ud835\udc61 \ud835\udc56 \u2026 superscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc51 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc31 \ud835\udc61 \\displaystyle=p_{t|t-\\Delta t}(\\mathbf{x}_{t}|x_{t}^{1}\\dots x_{t-\\Delta t}^{i}\\dots x_{t}^{d})\\frac{p_{t-\\Delta t}(x_{t}^{1}\\dots x_{t-\\Delta t}^{i}\\dots x_{t}^{d})}{p_{t}(\\mathbf{x}_{t})}\n\nwhere the second term recovers the ground truth score when we pass it back through the ODE induced by . Therefore, this value is maximized when the scores match. \u220e\n\nAppendix B Additional Experimental Details\n\nB.1 Diffusion Details\n\nThe geometric noise distribution is . The log linear noise schedule is for some small epsilon for numerical stability as , commonly or . These noise schedules were chosen such that the prior loss and the approximation of with are negligible. We typically scale the uniform transition matrix down by and take to be uniform. For the absorbing state, we take to be the MASK state with some leakage of probability to the non-MASK state (to avoid KL divergence, although this is negligible and is not used for generation in practice). B.2 Model Details\n\nOur model train with flash attention (Dao et al., 2022) with fused kernels wherever applicable. We also use the adaLN-zero time information network of (Peebles & Xie, 2022) with hidden dimension. Following previous work, we parameterize the network with the total noise level instead of the time . SEDD models have the same hidden dimensions, number of blocks, and number of heads as their corresponding GPT-2 models. However, SEDD models also use a separate word embedding matrix and output matrix. In total, SEDD small and SEDD medium have around 90M parameters and 320M non embedding parameters respectively (compared to GPT-2 small 86M and GPT-2 medium 304M non-embedding parameters respectively).",
    "sedd-22": "B.3 Training Details\n\nAll models were trained with a batch size of 512 and trained with a learning rate of .",
    "sedd-23": "We clip our gradient norm to 1 and have a linear warmup schedule for the first 2000 iterations. In accordance with previous work, we report our results using a 0.9999 EMA. We trained on nodes of 8 A100 80GB or 16 A100 40GB GPUs, using gradient accumulation when our batch size did not fit into memory (as is the case for SEDD medium). B.4 Hyperparameter Search\n\nWe did not do a hyperparameter search or achitecture search beyond trying out our log-linear noise schedule for the absorbing transition matrix, which lowered the variance and produced slightly better perplexities. B.5 Evaluation Details\n\nWe randomly sample with timesteps to Monte Carlo estimate our likelihoods. We use invertible tokenizers, as is customary for GPT-2 experiments. We report results on the test set for all datasets besides WikiText02, where we report on the train set since WikiText02 and WikiText103 share the same test set. For the generative perplexity results, we generate samples. For the GPT-2 model, since there must be some conditional generation, we simply fed the model a random initial token. For conditional generation, we used the SEDD with absorbing transition matrix. Appendix C Additional Experimental Results\n\nC.1 Toy Experiment on Text8\n\nWe validate that our method works better on simple datasets, in particular the text8 dataset. We train our SEDD model according to the training scheme (architecture size and tokenization) of Austin et al. (2021). We achieve a bpc (bits per character) for absorbing diffusion (improving upon their result of when trained with the likelihoods and with an augmented loss). We also achieve a bpc for the uniform noise, significantly improving upon their reported . We believe this gap in the uniform noise case is caused by our parameterization of the concrete score. While the absorbing state result is not a big gap over previous methods, we will now see that it makes a big difference for larger scale experiments, possibly because of the underlying data complexity or the large vocabulary size. C.2 Mean Parameterization Ablation\n\nWe note that our Tweedie -leaping amounts to the analytic sampling method proposed by previous discrete diffusion works (Austin et al., 2021). In particular, (Austin et al., 2021) minimizes the token-wise KL divergence between the true reverse and the parallel reverse . In particular, by conditioning on , we see that . The ratio recovers the marginal score by a similar trick to Theorem 4.1, so we can see that this mean parameterization is equivalent to our denoising score entropy loss. As such, it is possible to train a mean parameterization strategy network with our derived loss functions. We thus compare these parameterizations for the absorbing case, as the toy experiment on Text8 already showed the benefits of score parameterization for the uniform transition. As we see in Figure 3, the mean parameterization drastically underperforms the score parameterization (resulting in increase in perplexity scores). We believe this is because the mean parameterization only parameterizes a distribution over the non-mask tokens, meaning the probability to move from MASK to non-MASK token is fixed, while the score function can control this rate. C.3 Further Evaluation of Generative Perplexity\n\nWe further evaluate our generative perplexity for uniform models as well as different sampling schemes (analytic sampling based on Tweedie\u2019s vs Euler sampling based off of reverse diffusion). Results are shown in Figures 4 and 5. Generally, we find that uniform does not produce the same linear tradeoff curve as absorbing (most likely due to a bottleneck in generation quality). Futhermore, analytic generally outperforms Euler sampling, although the difference is very slight for absorbing and more noticeable for uniform.",
    "sedd-24": "C.4 Additional Samples\n\nContinued on next page. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 22:30:24 2024 by LaTeXML"
}