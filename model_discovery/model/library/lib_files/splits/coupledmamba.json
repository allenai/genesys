{
    "coupledmamba-0": "# Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model \n\nWenbing Li Hang Zhou Junqing Yu Zikai Song ${ }^{\\dagger}$ Wei Yang ${ }^{\\dagger}$<br>Huazhong University of Science and Technology<br>\\{wenbingli, henrryzh, yjqing, skyesong, weiyangcs\\}@hust.edu.cn\n\n\n#### Abstract\n\nThe essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities.",
    "coupledmamba-1": "However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by $0.4 \\%, 0.9 \\%$, and $2.3 \\%$ on the three datasets respectively, $49 \\%$ faster inference and $83.7 \\%$ GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion. ## 1 Introduction\n\nReal-world data captured and processed across multiple modalities, such as text, image, video, and sensor data, yield a rich tapestry of information that is inherently complementary. This complementarity profoundly enhances the capacities of deep learning models, facilitating more nuanced interpretations and predictions. As a result, deep learning models that integrate multi-modal data have shown substantial superiority over their uni-modal counterparts in various domains, including visual-language learning [25, 26, 27], multi-modal classification/segmentation [53, 54, 56, 55], sentiment analysis [30, 5, 38, 31] and etc. Given these advantages, the development of effective multi-modal fusion techniques has emerged as a center of attention. A variety of works have explored this topic on convolution or Transformer -based models, and developed specified mechanisms as early, middle, and late fusion, depending on position of fusion been conducted. A more prevalent practice is to first extract features using modality-specific backbones and then devise a fusion module to exploit the complementary information from all modalities. Existing fusion paradigms either aggregate modal-specific features into one by neglecting individual intra-modal propagation [32] or\n\n[^0]align modal-specific features into a united representation space through regulation while failing to exploit complementary inter-modal information exchange for difficulty in alignment supervision [14]. Recently, the state space models, advanced by the LSSL [33, 34, 35], S4 [40], GSS [20], and S4D [41, 42], use state variables to explicitly model the sequential evolving neural states, have being emerged as compelling alternatives to Transformers for its efficiency in modeling long-range sequences [43]. Particularly, Mamba [22], improves with a selective scanning mechanism and hardware-aware parallelism to enable very efficient training and inference, achieving comparable performances to Transformers on large-scale data. Yet, existing explorations focus on processing uni-modal data, and the multi-modal fusion mechanism on SSMs is still under-investigated. In this paper, we observe that the explicit state variables in SSMs provide great fusion anchors, i.e., from which we can extract inter-model complementary information, and to where we can fuse the complementary information into a unified representation. Inspired by the effective Coupled Hidden Markov Model (CHMM) [23], we investigate the multi-modal fusion problem of the Mamba model from a state transition perspective. For multi-modal fusion on Mamba, the bruteforce way is to direct aggregate features from all multi-modalities into one feature, i.e., the aggregation approach, and process with a sole Mamba model. However, such an approach neglects the individual intra-modal propagation. Instead, we propose the Coupled Mamba model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b6f9480bde190973c4g-02.jpg?height=646&width=654&top_left_y=631&top_left_x=1104)\n\nFigure 1: Architecture of Coupled Mamba. devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. Another challenge is to fully comply with the hardware-aware parallelism for efficiency, we achieve parallel computing by deriving multi-modal global convolution kernels. As shown in Figure 11 the entire Coupled Mamba model consists of $N$ layers, and is finally adapted to downstream tasks through pooling. Each layer has $M$ Coupled Mamba blocks, where $M$ is the number of modalities. Each Coupled Mamba block receives sequence data of multiple modalities as input, aggregate states from multiple modalities, and then transits into the state at next time of each individual modality. We conduct extensive experiments on CMU-MOSEI, CH-SIMS [37], CH-SIMSV2 [52] datasets through multi-domain input, and verify the effectiveness of our model compared to current state-of-the-art methods, with $0.4 \\%, 0.9 \\%, 2.3 \\%$ F1-Score increase, $49 \\%$ faster inference and $83.7 \\%$ GPU memory save. The results demonstrate that our Coupled Mamba model enhances the multi-modal fusion with state coupling. ## 2 Related Work\n\nMulti-modal Fusion Multi-modal fusion focuses on combining features from various modalities into unified representations to tackle multi-modal learning challenges. Traditionally, fusion methods are categorized into feature-level early fusion and decision-level late fusion, based on where fusion occurs within the model [2]. Early fusion techniques are employed by [3] to merge features from diverse modalities such as audio, text, and vision. [4] introduce a method using two separate branches for spatial and temporal modalities with a straightforward post-fusion for video action recognition. Other notable post-fusion approaches include works like [5] and [7, 6], which suggest robust late fusion via rank minimization. Recent advances in deep learning have expanded the concept of early fusion to mid-term fusion, which integrates features at multiple levels [8]. For instance, [9] develop a fused representation by progressively combining multiple fusion layers. Similarly, [10] propose a multi-layer fusion method that connects all modality-specific networks through a central network. [? ] introduce an architecture search algorithm to identify the optimal fusion architecture. Furthermore, [12, 13] incorporate attention mechanisms into multi-modal fusion, while [14] suggest exchanging\nfeature channels between modalities. Additionally, [15] integrate bilinear pooling into attention blocks, showing its effectiveness in capturing higher-level feature interactions by stacking multiple attention blocks for image captioning. The focus has recently shifted towards dynamic fusion, which selects the optimal fusion strategy from various candidate operations based on inputs from different modalities [17, 16]. This dynamic approach offers greater flexibility for different multi-modal tasks compared to static methods. Inspired by the success of dynamic fusion designs and higher-level feature interaction capture in multi-modal fusion, our work aims to dynamically capture hidden states both within and between modalities using coupled state space models via state diffusion, enabling more efficient modality fusion for complex multi-modal tasks. State Space Models State Space Models (SSM) are exceptionally effective at learning the complex correlations inherent in language sequences. The seminal work of [40] introduced the structured state space model (S4), which aims to encapsulate the extended dependency characteristics of language sequences. Conceptually, S4 combines the unique properties of CNNs and RNNs to create a powerful framework for sequential data processing. Building on the foundation laid by $S 4$, subsequent research efforts have been devoted to solving the problem of linearly scaling sequence lengths. In this regard, [19] introduced S5 utilizing MIMO-SSM and parallel scan technology, while [39] proposed H3, which greatly improved the performance of SSM, and [20] introduced GSS, which demonstrated faster training and competitive performance. Furthering the current state of research, [22] developed a novel language model called Mamba. This model uniquely combines a data-selective SSM layer and a parallel scanning algorithm to solve Transformer's quadratic complexity calculation problem in long sequence modeling and Transformer's inability to model data outside the attention window. This also illustrates the huge potential of Mamba in processing sequence data. Coupled Hidden Markov Model Hidden Markov Model (HMM) is a probabilistic model that simulates a sequence of hidden states to generate a sequence of observations. The core components of the model include the state transition matrix A , the observation probability matrix (emission matrix) B and the initial state probability vector $\\pi$. This model assumes the existence of Markov chains between hidden states, and observation events are independently generated by hidden states. To address specific needs, researchers have developed several HMM variants. For example, the Hierarchical Hidden Markov Model (HHMM) [24] introduces a state hierarchy based on standard HMMs, while the Mixed Hidden Markov Model (MHMM) [51] combines multiple HMMs to Build complex distributions. These extensions improve the applicability of HMM in various scenarios and further promote the application of sequence data analysis in multiple fields. Coupled Hidden Markov Models (CHMM) [23] are a class of tools capable of modeling multiple interrelated time series. In multimodal fusion, we usually focus on signals from different channels, such as audio, text, and facial expressions, which are all time-correlated. Coupled HMMs can effectively model such data because they can consider dynamic correlations between multiple channels simultaneously. ## 3 Coupled State Space Model\n\nIn this section, we introduce Coupled Mamba method for multi-modal fusion in detail, which performs multi-modal fusion by introducing multi-modal historical states. As shown in Figure 2 it contains two parts: state coupling and state space model. ### 3.1 Preliminary\n\nIn recent years, the state space model has developed rapidly [40, 41, 39]. Mamba introduced a selectivity mechanism based on S4, which converted the original time-invariant characteristics. Mamba is based on the concept of continuous systems by introducing hidden states $h(t) \\in \\mathbb{R}^{N}$ to map a series of inputs $x(t) \\in \\mathbb{R}^{L}$ to obtain output $y(t) \\in \\mathbb{R}^{L}$, where N denotes the number of hidden states. The continuous system can be expressed as:\n\n$$\nh^{\\prime}(t)=\\mathbf{A} h(t)+\\mathbf{B} x(t), \\quad y(t)=\\mathbf{C} h(t)\n$$\n\nwhere $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ represents the state transition matrix of the system, and $\\mathbf{B} \\in \\mathbb{R}^{N \\times 1}, \\mathbf{C} \\in \\mathbb{R}^{N \\times 1}$ are projection matrices. Mamba uses a time scale parameter $\\Delta$ to discretize the continuous parameters $\\mathbf{A}, \\mathbf{B}$ into $\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}$, the zero-order hold $(\\mathrm{ZOH})$ principle is adopted by default. The discretized statespace equation is:\n\n$$\n\\overline{\\mathbf{A}}=\\exp (\\boldsymbol{\\Delta} \\mathbf{A}), \\quad \\overline{\\mathbf{B}}=(\\boldsymbol{\\Delta} \\mathbf{A})^{-1}(\\exp (\\boldsymbol{\\Delta} \\mathbf{A})-\\mathbf{I}) \\cdot \\boldsymbol{\\Delta} \\mathbf{B}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b6f9480bde190973c4g-04.jpg?height=537&width=1376&top_left_y=252&top_left_x=380)\n\nFigure 2: Coupling Mamba receives input $x_{t-1}$, and performs internal state switching and output through three key parameter matrices, where $\\mathbf{B}, \\mathbf{C}$ and $\\mathbf{S}$ are respectively represented as the input matrix, output matrix and state transfer matrix. The hidden states are summed across modalities and used for state transition input to generate next time states.",
    "coupledmamba-2": "The state is propagated sequentially in time. Then the discretized version of Eq. (1) with step size $\\Delta$ can be rewritten as:\n\n$$\nh_{t}=\\overline{\\mathbf{A}} h_{t-1}+\\overline{\\mathbf{B}} x_{t}, \\quad y_{t}=\\mathbf{C} h_{t}\n$$\n\nFinally, by expanding $h_{t-1}$ layer by layer, the global convolution kernel $\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}$ can be obtained, and $\\overline{\\mathbf{K}}$ is used to calculate the output $y$, which is defined as follows:\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{K}} & =\\left(\\mathbf{C} \\overline{\\mathbf{B}}, \\mathbf{C} \\overline{\\mathbf{A B}}, \\ldots, \\mathbf{C}^{\\mathrm{L}-1} \\overline{\\mathbf{B}}\\right) \\\\\ny & =x \\otimes \\overline{\\mathbf{K}}\n\\end{aligned}\n$$\n\nwhere $L$ is the length of the input sequence $x$ and $\\otimes$ denotes the convolution operation. For algorithm 1, L denotes the sequence length, E denotes the extended dimension, D denotes the feature dimension, and B denotes the batch size. ### 3.2 Coupled State Transition\n\nFor multi-modality data input, one naive way is to aggregate the multi-modal features into one feature and process using a single Mamba model. However, such approach neglects intra-modal propagation. Inspired by the Coupled Hidden Markov Model (CHMM) [23], a more elegant solution is to model mutual modality transition probability as follows:\n\n$$\nP_{i=1: M, j}=P\\left(h_{t}^{j} \\mid h_{t-1}^{1}, h_{t-1}^{2}, \\ldots, h_{t-1}^{M}\\right)\n$$\n\nwhere $P_{i=1: M, j}$ is the probability transition matrix from all modalities to current modality $j$. For SSM with $M$ multi-modal input, we have $M$ state propagation sequences. In alignment with CHMM, we can model the state transition of a modality $m$ by coupling all the modality states as:\n\n$$\nh_{t}^{m}=\\sum\\left(\\overline{\\mathbf{A}}_{1, m} h_{t-1}^{1}, \\overline{\\mathbf{A}}_{2, m} h_{t-1}^{2}, \\ldots, \\overline{\\mathbf{A}}_{M, m} h_{t-1}^{M}\\right)+\\overline{\\mathbf{B}}_{m} x_{t}^{m}, y_{t}^{m}=\\mathbf{C} h_{t}^{m}\n$$\n\nwhere $\\overline{\\mathbf{A}}_{i, m}$ denotes the state transition matrix from modality $i$ to $m$. Taking into account the memory overhead and computational efficiency, such modeling increase the number of parameters and computational complexity greatly. We propose a more memory efficient way by conducting summation before state transition, which achieves similar performance and is much more efficient. So our formation of Coupled SSM is:\n\n$$\nh_{t}^{m}=\\mathbf{S}_{m} \\sum_{m=1}^{M} h_{t-1}^{m}+\\overline{\\mathbf{B}}_{m} x_{t}^{m}\n$$\n\nWhere we use $\\mathbf{S}_{m} \\in \\mathbb{R}^{B \\times L \\times D \\times N}$ to model the overall state transition after states summation. One minor drawback of this modeling is that we require all modalities to have the same state, which can be easily addressed by using projection layers. ### 3.3 Parallelism and Efficiency Analysis\n\nThe main difference between Mamba and traditional recurrent neural networks (RNNs) is that the transition between states does not rely on any activation function. This feature enables it to precalculate intermediate results through the iterative Eq.(3), thereby achieving parallel computing. However, Coupled Mamba adds multi-modal state information based on Mamba, which brings new challenges to the ability to maintain the Mamba parallelization algorithm. In order to solve this problem, we derived a global convolution kernel suitable for Coupled Mamba to ensure that Coupled Mamba can continue to enjoy the advantages brought by Mamba parallel computing, thereby effectively improving the throughput and inference speed of the model. Detailed analysis on throughput and inference speed will be discussed in depth in subsequent sections. After introducing the state information of different modals, we learned about the entire state transfer process (6) through 3.2. By deriving Eq. (6), that is, disassembling $h_{t-1}^{m}$, we can get the following results:\n\n$$\n\\mathbf{P}=\\sum_{m=1}^{M} \\mathbf{S}_{m}, \\quad \\mathbf{U}_{\\mathbf{t}}=\\sum_{m=1}^{M} \\overline{\\mathbf{B}}_{m} x_{t}^{m}, \\quad h_{t}^{m}=\\mathbf{S}_{m} \\sum_{i=0}^{t-1} \\mathbf{P}^{i} \\mathbf{U}_{t-1-i}+\\overline{\\mathbf{B}}_{m} x_{t}^{m}\n$$\n\nwhere $\\mathbf{P} \\in \\mathbb{R}^{B \\times L \\times D \\times N}$. According to Eq. 77) which can be extended to the state information of each modal, we use the following formula to calculate the output. $$\ny=\\mathbf{C} \\otimes \\sum_{m=1}^{M} h_{t}^{m}=\\mathbf{C} \\otimes \\sum_{i=0}^{t} \\mathbf{U}_{\\mathbf{i}} \\mathbf{P}^{\\mathbf{t}-\\mathbf{i}}\n$$\n\nFrom this, the global convolution kernel $\\overline{\\mathrm{K}}=\\left(\\mathbf{C P}^{\\mathbf{0}}, \\mathbf{C P} \\mathbf{P}^{\\mathbf{1}}, \\ldots, \\mathbf{C P} \\mathbf{P}^{\\mathbf{t}-\\mathbf{1}}, \\mathbf{C P} \\mathbf{P}^{\\mathbf{t}}\\right)$ suitable for Coupled Mamba can be obtained. The global convolution kernel $\\overline{\\mathbf{K}}$ can be used to perform convolution operations on sequence data. In the convolution operation, the calculations of each convolution kernel and the input sub-region are independent of each other, allowing parallel processing of different convolution kernels or input blocks. ## 4 Experiment\n\nTo evaluate the effectiveness of our proposed Coupled Mamba in multi-modal fusion, we conduct extensive experiments, with special focus on the multi-modal sentiment analysis (MSA) task as it relies heavily on multi-modal data and is in sequential form. The MSA task aims to predict people's emotional polarity by fusing audio, text, and visual information. To fully evaluate the advantages of our approach, we conduct extensive experiments on both classification and regression tasks. ### 4.1 Datasets and Implementation Details\n\nDatasets We conduct experiments on three benchmark datasets (CMU-MOSEI, CH-SIMS [37] and CH-SIMSV2 [52]). CMU-MOSEI dataset is an extension of CMU-MOSI, contains 22856 samples\nof movie review video clips. In this dataset, 16326 samples are used as the training set, and the remaining 1871 and 4659 samples are used as the validation set and test set respectively. CH-SIMS contains 2281 video clip samples, 1368 samples are used as the training set, and the remaining 456 and 457 samples are used as the validation set and test set respectively. CH-SIMSV2 is an extension of CH-SIMS, which contains 4402 video clip samples, of which 2722 samples are used as the training set, and the remaining 647 and 1034 samples are used as the validation set and test set respectively. For the feature extraction method of the dataset, please refer to the Appendix for more information. Evaluation metrics For regression tasks, We use the mean absolute error (MAE), which is the mean absolute difference between the predicted and true values, and the Pearson correlation (Corr), which measures how biased the prediction is according to the following formula: Positive/Negative and Non-Negative/Negative Classification The results calculate the binary classification accuracy (Acc-2) and F1-Score, of which Acc-2 and F1-Score are more important indicators. For classification tasks, we use Acc-2, F1-Score, Acc-3 and F1-Score3 as evaluation indicators. F1-score3 is the overall performance evaluation of all categories, and F1-score is the performance evaluation of two categories.",
    "coupledmamba-3": "Also ignore neutral categories. All experiments were performed under the same environment. Implementation details We use a hidden dimension size of 128, an expansion coefficient of 2, a convolution kernel size of $4, \\boldsymbol{\\Delta}=$ dstate / 8 as the configuration of each Mamba block, and a layer number of 3 to train our Coupled Mamba. We use Adam to optimize the model and set the learning rate to 0.0005 , weight decay coefficient is 0.0005 , epoch is 150 , the batch size is set to $1024,128,256$ on CMU-MOSEI, CH-SIMS, and CH-SIMSV2. L1 loss is used as the loss function for the regression task, and cross entropy is used as the loss function for the classification task. All experiments were conducted on a Linux workstation equipped with a single NVIDIA 32GB V100GPU and a 32-core Intel Xeon CPU. More experimental details can be found in the Appendix. ### 4.2 Comparison with the state-of-the-arts\n\nTo fully validate the performance of Coupled Mamba, we conduct extensive comparisons with the following baselines [1, 6, 3, 31, 44, 45] in Table 1.",
    "coupledmamba-4": "We ran five times and reported the average value. We use bold text to show the best results. Traditionally, models that use aligned corpora tend to perform better [3]. In our experiments, we achieve significant improvements on all evaluation metrics compared to unaligned models. Our unaligned method is able to achieve better results even when compared with aligned models. Table 1: Results on CMU-MOSEI. All models are based on language features extracted by BERT. The one with $*$ indicates that the model reproduces under the same conditions. | Model | CMU-MOSEI |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $M A E \\downarrow$ | $C o r r \\uparrow$ | $A c c-2 \\uparrow$ | $F 1-S c o r e \\uparrow$ | Data Setting |\n| TFN [5] | 0.593 | 0.700 | 82.5 | 82.1 | Unaligned |\n| LMF [6] | 0.623 | 0.677 | 82.0 | 82.1 | Unaligned |\n| MFN [38] | - | - | 76.0 | 76.0 | Aligned |\n| MFM [46] | 0.568 | 0.717 | 84.4 | 84.3 | Aligned |\n| MulT [3] | 0.580 | 0.703 | 82.5 | 82.3 | Aligned |\n| MAG-BERT [47] | - | - | 84.7 | 84.5 | Aligned |\n| ICCN [48] | 0.565 | 0.713 | 84.2 | 84.2 | Aligned |\n| MISA [31] | 0.555 | 0.756 | 85.5 | 85.3 | Aligned |\n| TETFN [45] | 0.551 | 0.748 | 85.1 | 85.2 | Unaligned |\n| DMD [44] | - | - | 84.8 | 84.7 | Unaligned |\n| IMDer3 [1] | - | - | 85.1 | 85.1 | Unaligned |\n| MAG-BERT ${ }^{*}[47]$ | 0.549 | 0.753 | 85.2 | 85.1 | Aligned |\n| Coupled Mamba (Ours) | $\\mathbf{0 .",
    "coupledmamba-5": "5 4 7}$ | $\\mathbf{0 . 7 5 6}$ | $\\mathbf{8 5 . 6}$ | $\\mathbf{8 5 . 5}$ | Unaligned |\n| Coupled Mamba (Ours) | $\\mathbf{0 . 5 4 7}$ | $\\mathbf{0 .",
    "coupledmamba-6": "7 5 8}$ | $\\mathbf{8 5 . 7}$ | $\\mathbf{8 5 . 6}$ | Aligned |\n\nIn multi-modal sentiment analysis tasks, language is a key factor because different languages may have different ways of expressing the same emotion. However, Table 2 shows that our Coupled Mamba shows robustness in both English and Chinese sentiment analysis tasks. Even with unaligned data, our method still achieves highest performance. The results of the classification task are given in Table 344, more information is given in Table ??. It can be seen from the results of the two tasks that our proposed fusion method achieves state-of-the-art (SOTA) regardless of whether the data are aligned or not and what language is used. This is sufficient to demonstrate the effectiveness and robustness of our method. Table 2: Results on CH-SIMS (Chinese). All models are based on language features extracted by BERT, and the results are compared on unaligned data. Acc-N represents N-level accuracy. | Model | $A c c-2 \\uparrow$ | $A c c-3 \\uparrow$ | $A c c-5 \\uparrow$ | $F 1-S c o r e \\uparrow$ | $M A E \\downarrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| TFN [5] []] | 78.4 | 65.1 | 39.3 | 78.6 | 0.432 |\n| LMF [6] | 77.8 | 64.7 | 40.5 | 77.9 | 0.411 |\n| MFN [38] | 77.9 | 65.7 | 39.5 | 77.9 | 0.435 |\n| MulT [3] | 78.6 | 64.8 | 37.9 | 79.7 | 0.453 |\n| Self-MM [30] | 80.0 | 65.5 | 41.5 | 80.4 | 0.425 |\n| TETFN [45] | 81.2 | 63.2 | 41.8 | 80.2 | 0.420 |\n| IMDer [1] | 76.3 | - | $\\mathbf{5 0 . 7}$ | 76.4 | - |\n| Coupled Mamba (Ours) | $\\mathbf{8 1 .",
    "coupledmamba-7": "8}$ | $\\mathbf{6 8 . 7}$ | 42.1 | $\\mathbf{8 1 . 3}$ | $\\mathbf{0 . 4 0 9}$ |\n\nTable 3: Results of classification tasks on CMU-MOSEI. All models are based on language features extracted by BERT, and the results are performed on unaligned data. We ran it five times and report the average results. | Model |  | CMU-MOSEI |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n| EF-LSTM [50] | Acc $-2 \\uparrow$ | $A c c-3 \\uparrow$ | $F 1-$ Score $\\uparrow$ | $F 1-S c o r e-3 \\uparrow$ |\n| Graph-MFN [49] | 28.73 | 66.09 | 28.12 | 63.68 |\n| TFN [5] | 28.66 | 66.39 | 28.77 | 64.00 |\n| LMF [6] | 28.66 | 66.63 | 28.75 | 63.93 |\n| MFN [38] | 28.61 | 66.59 | 28.92 | 64.86 |\n| MulT [3] | 27.38 | 67.04 | 28.70 | 64.31 |\n| MISA [31] | 28.50 | 67.63 | 28.67 | 65.01 |\n| Self-MM [30] | 29.67 | 68.15 | 29.03 | 65.39 |\n| TETFN [45] | 29.54 | 67.95 | 28.86 | 66.53 |\n| Coupled Mamba (Ours) | $\\mathbf{3 2 .",
    "coupledmamba-8": "0 2}$ | $\\mathbf{6 8 . 9 5}$ | $\\mathbf{2 9 . 7 2}$ | 66.33 |\n\nTable 4: Classification task results on CH-SIMS. All models are based on language features extracted by BERT and the results are performed on unaligned data. We ran it five times and report the average results. | Model |  |  | CH-SIMS |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | $A c c-2 \\uparrow$ | $A c c-3 \\uparrow$ | $F 1-S c o r e \\uparrow$ | $F 1-S c o r e-3 \\uparrow$ |\n| EF-LSTM [50] | 56.27 | 54.27 | 49.85 | 38.18 |\n| Graph-MFN [49] | 57.99 | 68.44 | 54.66 | 63.44 |\n| TFN [5] | 53.56 | 65.95 | 52.79 | 62.04 |\n| LMF [6] | 57.06 | 66.87 | 53.83 | 62.46 |\n| MFN [38] | 56.96 | 67.57 | 54.14 | 62.37 |\n| MulT [3] | 56.34 | 68.27 | 54.26 | 64.23 |\n| MISA [31] | 57.27 | 67.05 | 53.99 | 60.98 |\n| Self-MM [30] | 58.65 | 67.56 | 55.88 | 65.95 |\n| TETFN [45] | 57.77 | 66.83 | 55.15 | 65.23 |\n| Coupled Mamba(Ours) | $\\mathbf{6 0 .",
    "coupledmamba-9": "1 2}$ | $\\mathbf{6 8 . 7 5}$ | $\\mathbf{5 6 . 1 5}$ | $\\mathbf{6 7 . 4 7}$ |\n\nTable 5 shows the results on the CH-SIMSV2 dataset, which currently only supports regression tasks. It can be seen from the table that the method we proposed has achieved a huge improvement of $2.7 \\%, 2,3 \\%$ in F1-Score and Acc-2 respectively, indicating the effectiveness of our method. Table 5: Results on CH-SIMSV2, consistent across all experimental settings, using unaligned data. We run it five times and report the average results. | Model | $A c c-2 \\uparrow$ | $A c c-3 \\uparrow$ | $A c c-5 \\uparrow$ | $F 1-S c o r e \\uparrow$ | $M A E \\downarrow$ | $C o r r \\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 80.1 | 72.3 | 52.5 | 80.1 | 30.3 | 70.7 |\n| LMF [6] | 74.1 | 64.9 | 47.8 | 73.8 | 36.7 | 55.7 |\n| MFN [38] | 81.1 | 73.7 | 54.5 | 81.2 | 29.5 | 72.6 |\n| MulT [3] | 80.7 | 73.1 | 54.8 | 80.7 | 29.1 | 73.8 |\n| MAG-BERT [47] | 79.8 | 73.5 | 53.7 | 79.8 | 33.4 | 69.1 |\n| Self-MM [30] | 79.7 | 72.6 | 52.8 | 79.7 | 31.1 | 69.5 |\n| TETFN [45] | 79.7 | 73.6 | 54.4 | 79.8 | 31.1 | 69.5 |\n| Coupled Mamba | $\\mathbf{8 3 .",
    "coupledmamba-10": "4}$ | $\\mathbf{7 5 . 0}$ | $\\mathbf{5 5 . 1}$ | $\\mathbf{8 3 .",
    "coupledmamba-11": "5}$ | $\\mathbf{2 8 . 7}$ | $\\mathbf{7 5 . 8}$ |\n\n### 4.3 Ablation study\n\nWe evaluated the impact of each component in Coupled Mamba to verify the effectiveness of our design. It is worth noting that in order to reduce the impact of randomness on the experimental results, our entire ablation experiment was conducted on the CMU-MOSEI dataset. We use the cross-attention mechanism instead of the fusion strategy for comparison. The results are shown in Table 6 Coupled Mamba filters input through a selective mechanism and uses historical modal information to remember and perceive global context, so Coupled Mamba also performs modal fusion well on unaligned data. In contrast, cross-attention is sensitive to misaligned data, and this spatio-temporal inconsistency will lead to insufficient integration between modalities and poor performance. Table 6: All things being equal, replacing Coupled Mamba with Cross attention, we execute it five times and report the average results. | Method | CMU-MOSEI |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $M A E \\downarrow$ | $\\operatorname{Corr} \\uparrow$ | Acc $-2 \\uparrow$ | $F 1-S c o r e \\uparrow$ | Data Setting |\n| Cross Attention | 55.9 | 73.3 | 84.6 | 84.5 |  |\n| Coupled Mamba (Ours) | $\\mathbf{5 4 .",
    "coupledmamba-12": "7}$ | $\\mathbf{7 5 . 6}$ | $\\mathbf{8 5 . 6}$ | $\\mathbf{8 5 . 5}$ | Unaligned |\n\nThe number of hidden states and size of $\\Delta$ will have an impact on the results. The size of $\\Delta$ affects SSM's ability to retain historical information. An increased size of $\\Delta$ focuses more on the present input while disregarding past data, and it also raises the count of hidden states. This escalation in complexity might result in overfitting, higher computational expenses, and may not enhance the model's actual effectiveness. In order to explore the impact of these parameters on the results, we conducted multiple experiments, and the experimental results showed that the model performance changed under different parameter settings. Detailed results can be found in Tables 7 , 8. With $\\Delta=$ dstate $/ 8$ and dstate $=64$, Coupled Mamba achieves the best performance than other configurations. Table 7: Performance on CMU-MOSEI with different timescale $\\Delta$\n\n| $\\Delta$ | CMU-MOSEI |  |  |\n| :---: | :---: | :---: | :---: |\n| Corr $\\uparrow$ | Acc-2 $\\uparrow$ | F1-Score $\\uparrow$ |  |\n| dstate $/ 16$ | 75.3 | 85.2 | 85.0 |\n| dstate $/ 8$ | $\\mathbf{7 5 . 6}$ | $\\mathbf{8 5 . 6}$ | $\\mathbf{8 5 . 5}$ |\n| dstate $/ 4$ | 74.2 | 85.0 | 84.9 |\n\nTable 8: Performance on CMU-MOSEI with different dstate\n\n| dstate | CMU-MOSEI |  |  |\n| :---: | :---: | :---: | :---: |\n|  | Corr $\\uparrow$ | Acc-2 $\\uparrow$ | F1-Score $\\uparrow$ |\n| 128 | 74.1 | 84.2 | 84.1 |\n| 64 | $\\mathbf{7 5 .",
    "coupledmamba-13": "6}$ | $\\mathbf{8 5 . 6}$ | $\\mathbf{8 5 . 5}$ |\n| 32 | 75.0 | 84.9 | 84.9 |\n\nIn order to verify the effectiveness of our state coupling, we adopt the splicing fusion, average fusion, and native Mamba blocks for experiments. The result is shown in Table 9 , our Coupled Fusion obtains the best performance than others. Traditional modal fusion methods, such as averaging and concatenation, fail to fully cope with the inherent heterogeneity of multi-modal data. Such methods\nignore the different influences that different modalities may have on specific tasks, thereby failing to effectively reveal the intrinsic correlation between multi-modal data. Simple Mamba blocks are not enough to dynamically grasp the semantic relationships. The introduction of state coupling mechanism based on Mamba can make up for this shortcoming and achieve significant improvements in multiple performance indicators. Table 9: Comparison of fusion methods\n\n| Model | $M A E \\downarrow$ | Corr $\\uparrow$ | Acc $-2 \\uparrow$ | $F 1-S c o r e \\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: |\n|  | 56.4 | 73.6 | 84.2 | 84.1 |\n| Concat Fusion | 56.2 | 72.8 | 84.8 | 84.5 |\n| Mamba Fusion | 55.3 | 74.9 | 85.3 | 85.3 |\n| Coupled Fusion | $\\mathbf{5 4 .",
    "coupledmamba-14": "7}$ | $\\mathbf{7 5 .",
    "coupledmamba-15": "6}$ | $\\mathbf{8 5 . 6}$ | $\\mathbf{8 5 . 5}$ |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b6f9480bde190973c4g-09.jpg?height=522&width=662&top_left_y=929&top_left_x=382)\n\nFigure 3: GPU usage comparison\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b6f9480bde190973c4g-09.jpg?height=519&width=659&top_left_y=928&top_left_x=1080)\n\nFigure 4: Inference speed comparison\nCompared to Transformers, our approach improves performance by $\\mathbf{1 \\%} \\sim \\mathbf{2} \\%$ as shown in Table 6 and decreases memory consumption by more than $\\mathbf{8 3 . 7 \\%}$ for sequences length 500 according to Figure 3. When the sequence length increases, the GPU memory usage of the Transformer-based method increases exponentially. In comparison, our method exhibits linear growth. As the sequence grows, the advantages of Coupled Mambas become more apparent. As shown in Figure 4, we compared Coupled Mamba and Transformers with five different sequence lengths, and the results show that our inference speed is twice as fast as Transformers under the same sequence length. However, as the sequence length continues to grow, the inference speed of Coupled Mamba will far exceed that of Transformers. ## 5 Conclusion and Discussion\n\nIn this paper, we introduce Coupled Mamba, a novel approach designed to enhance multi-modal fusion by exploiting chains of state evolution within state space. Furthermore, we devise a state summation and transition scheme to address the challenges of parallel SSM with multiple inputs. Coupled Mamba integrates intermediate information from various modalities into the state space of other modalities, sequentially capturing the dynamic interactions of multi-modal data over time. This method effectively solves the limitations of multi-modal fusion. Quantitative and qualitative experiments consistently verified the efficacy of Coupled Mamba. Ethical implications discussion. Our method technique employs a state summation and transformation scheme, which requires that all modal states have the same size in regardless of whether the data are aligned or not. It also has potential negative social impacts. For example, multi-modal data often contain a large amount of facial information, raising privacy and security issues. ## References\n\n[1] Yuanzhi Wang, Yong Li, and Zhen Cui. Incomplete multimodality-diffused emotion recognition. Advances in Neural Information Processing Systems, 36, 2024. [2] Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S Kankanhalli. Multimodal fusion for multimedia analysis: a survey. Multimedia systems, 16:345-379, 2010. [3] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences.",
    "coupledmamba-16": "In Proceedings of the conference. Association for computational linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019. [4] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. Cornell University - arXiv, Cornell University - arXiv, Jun 2014. [5] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. arXiv: Computation and Language, arXiv: Computation and Language, Jul 2017. [6] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jan 2018. [7] Guangnan Ye, Dong Liu, I-Hong Jhuo, and Shih-Fu Chang. Robust late fusion with rank minimization. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2012. [8] Dhanesh Ramachandram and Graham W. Taylor. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, page 96-108, Nov 2017. [9] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2014. [10] Valentin Vielzeuf, Alexis Lechervy, Stephane Pateux, and Frederic Jurie. Centralnet: a multilayer approach for multimodal fusion. Cornell University - arXiv,Cornell University - arXiv, Aug 2018. [11] Juan-Manuel Perez-Rua, Valentin Vielzeuf, Stephane Pateux, Moez Baccouche, and Frederic Jurie. Mfas: Multimodal fusion architecture search. [12] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, JohnR. Hershey, and TimK. Marks. Attention-based multimodal fusion for video description. Cornell University - arXiv, Cornell University - arXiv, Jan 2017. [13] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. Dec 2021. [14] Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Rong Yu, and Junzhou Huang. Deep multimodal fusion by channel exchanging. Cornell University - arXiv,Cornell University arXiv, Nov 2020. [15] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2020. [16] Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, and Jianhua Yao. Multimodal dynamics: Dynamical fusion for trustworthy multimodal classification. [17] Zihui Xue and Radu Marculescu. Dynamic multimodal fusion. [18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. [19] JimmyT.H. Smith, Andrew Warrington, and ScottW. Linderman. Simplified state space layers for sequence modeling. Aug 2022. [20] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. Jun 2022. [21] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "coupledmamba-17": "Dec 2023. [23] M. Brand, N. Oliver, and A. Pentland. Coupled hidden markov models for complex action recognition. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Nov 2002. [24] Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical hidden markov model: Analysis and applications. Machine learning, 32:41-62, 1998. [25] Kyu Han Koh, Ashok Basawapatna, Vicki Bennett, and Alexander Repenning. Towards the automatic recognition of computational thinking for adaptive visual language learning. In 2010 IEEE symposium on visual languages and human-centric computing, pages 59-66. IEEE, 2010. [26] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716-23736, 2022. [27] Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al. Avlnet: Learning audio-visual language representations from instructional videos.",
    "coupledmamba-18": "arXiv preprint arXiv:2006.09199, 2020. [28] Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages $12695-12705,2020$. [29] Douwe Kiela, Edouard Grave, Armand Joulin, and Tomas Mikolov. Efficient large-scale multi-modal classification. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [30] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, page 10790-10797, Sep 2022. [31] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariant and -specific representations for multimodal sentiment analysis. Cornell University - arXiv, Cornell University - arXiv, May 2020. [32] Chaoqun Wang, Chunyan Xu, Zhen Cui, Ling Zhou, Tong Zhang, Xiaoya Zhang, and Jian Yang. Cross-modal pattern-propagation for rgb-t tracking. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 7064-7073, 2020. [33] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021. [34] S Sundhar Ram, Venugopal V Veeravalli, and Angelia Nedic. Distributed and recursive parameter estimation in parametrized linear state-space models. IEEE Transactions on Automatic Control, 55(2):488-492, 2010. [35] Vincent Verdult, Lennart Ljung, and Michel Verhaegen. Identification of composite local linear state-space models using a projected gradient search.",
    "coupledmamba-19": "International Journal of Control, $75(16-17): 1385-1398,2002$. [36] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10790-10797, 2021. [37] Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, and Kaicheng Yang. Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality.",
    "coupledmamba-20": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3718-3727, 2020. [38] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and LouisPhilippe Morency. Memory fusion network for multi-view sequential learning. Proceedings of the AAAI Conference on Artificial Intelligence, Jun 2022. [39] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022. [40] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. [41] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [42] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. [43] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling.",
    "coupledmamba-21": "arXiv preprint arXiv:1911.05507, 2019. [44] Yong Li, Yuanzhi Wang, and Zhen Cui. Decoupled multimodal distilling for emotion recognition. Mar 2023. [45] Di Wang, Xutong Guo, Yumin Tian, Jinhui Liu, Lihuo He, and Xuemei Luo. Tetfn: A text enhanced transformer fusion network for multimodal sentiment analysis. [46] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov. Learning factorized multimodal representations. In ICLR, 2019. [47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency, and Ehsan Hoque. Integrating multimodal information in large pretrained transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2359-2369, Online, July 2020. Association for Computational Linguistics. [48] Zhongkai Sun, Prathusha Sarma, William Sethares, and Yingyu Liang. Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8992-8999, 2020. [49] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236-2246, 2018. [50] Jennifer Williams, Steven Kleinegesse, Ramona Comanescu, and Oana Radu. Recognizing emotions in video using multimodal dnn feature fusion. In Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML), pages 11-19. Association for Computational Linguistics, 2018. [51] Rachel MacKay Altman. Mixed hidden markov models. Journal of the American Statistical Association, 102(477):201-210, 2007. [52] Yihe Liu, Ziqi Yuan, Huisheng Mao, Zhiyun Liang, Wanqiuyue Yang, Yuanzhe Qiu, Tie Cheng, Xiaoteng Li, Hua Xu, and Kai Gao. Make acoustic and visual cues matter: Ch-sims v2.0 dataset and av-mixup consistent module. Aug 2022. [53] Douwe Kiela, Edouard Grave, Armand Joulin, and Tomas Mikolov. Efficient large-scale multi-modal classification. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [54] Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages $12695-12705,2020$. [55] Di Feng, Christian Haase-Sch\u00fctz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems, 22(3):1341-1360, 2020. [56] Jose Dolz, Karthik Gopinath, Jing Yuan, Herve Lombaert, Christian Desrosiers, and Ismail Ben Ayed. Hyperdense-net: a hyper-densely connected cnn for multi-modal image segmentation. IEEE transactions on medical imaging, 38(5):1116-1126, 2018. [^0]:    ${ }^{\\dagger}$ Corresponding Author\n\n"
}