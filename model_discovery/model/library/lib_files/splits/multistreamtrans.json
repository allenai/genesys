{
    "multistreamtrans-0": "# Multi-Stream Transformers \n\nMikhail Burtsev<br>Artificial Intelligence Research Institute<br>Moscow, Russia<br>Moscow Institute of Physics and Technology<br>Dolgoprudny, Russia<br>burtsev@airi.net\n\nAnna Rumshisky<br>Univ.",
    "multistreamtrans-1": "of Massachusetts Lowell,<br>Lowell MA<br>arum@cs.uml . edu\n\n\n#### Abstract\n\nTransformer-based encoder-decoder models produce a fused token-wise representation after every encoder layer. We investigate the effects of allowing the encoder to preserve and explore alternative hypotheses, combined at the end of the encoding process. To that end, we design and examine a Multi-stream Transformer architecture and find that splitting the Transformer encoder into multiple encoder streams and allowing the model to merge multiple representational hypotheses improves performance, with further improvement obtained by adding a skip connection between the first and the final encoder layer. ## 1 Introduction\n\nOver the past couple of years, Transformer-based architectures (Vaswani et al., 2017) have consistently shown state-of-the-art performance on a variety of natural language processing (NLP) benchmarks. Transformer models implement an encoderdecoder architecture, in which encoder and decoder consist of several Transformer layers. Each Transformer layer is comprised by a selfattention module followed by a position-wise feedforward layer. Self-attention computes three representations for each input token (query, key, and value), then updates the embedding for each token by computing a weighted sum of \"value\" representations for the other tokens. The weights are given by a normalized dot product of the \"query\" representation for the original token with the \"key\" representations for the other tokens. Skip connections are used around both the attention layer and the feed-forward layer, with layer normalization performed after each of them. Token-wise representation computed by each Transformer layer allows for easier parallelization and therefore faster training. While it is not entirely clear what serves as the main source of Transformer's competitive advantage over other architectures, there have been claims that multi-layer Transformers pre-trained with a language modeling objective recover the traditional NLP pipeline (Tenney et al., 2019). In particular, multiple probing studies found that earlier layers produce a better representation for \"lowlevel\" NLP tasks such as POS-tagging, while the representations computed at middle layers are better suited for recovering syntactic information (Tenney et al., 2019; Hewitt and Manning, 2019; Goldberg, 2019; Jawahar et al., 2019; Vig and Belinkov, 2019). In the traditional multi-stage NLP pipeline architectures, every processing step usually ends with selection from multiple alternative hypotheses. For example, reading comprehension might depend on the results of coreference resolution, which in turn depends on extracted mentions. Here different groupings of the same set of mentions in coreference clusters can lead to totally different meanings. Usually, to address this problem, many alternative hypotheses are generated and then scored. If multilayer Transformer models indeed recover a form of an NLP pipeline, their architecture forces the fusion of alternatives at every processing stage (after every layer). There is just no storage for alternative hypotheses. In this work, we investigate the effects of allowing the Transformer encoder to preserve and explore alternative hypotheses, which can then be evaluated and combined at the end of the encoding process. To that end, we propose an architecture we term Multi-stream Transformer (see Figure 1), where only the first and the last layers compute a joint representation. In the middle layers, the encoder is split into two or more encoder streams, where computation is not merged between the separate encoder layers. A given encoder layer in each\nencoder \"stream\" of the multi-stream part of the Transformer performs computation over the full input. However, different encoder streams do not have access to, nor perform any computation over each other's representations. Results of parallel encoder stream computations are combined in the final layer of the encoder. This can be thought of as allowing the model to retain multiple hypotheses about input representation, which can then be combined at the end, following the same general idea as stacking ensemble models leveraging multiple hypotheses. As a zerolevel hypothesis, we use a skip connection from the first joint layer to the last joint layer, allowing the model to factor in the initial jointly computed representation, and at the same time improving learnability. Using one of the standard machine translation benchmarks, we show that Multi-stream Transformer outperforms its vanilla equivalent 'linear stream\" Transformer. We also show that adding a skip connection substantially improves performance of the vanilla Transformer baseline and has a comparable effect on the Multi-stream Transformer model. To our knowledge, this is the first study that examines the effects of splitting the computation in Transformer models into multiple parallel streams. ## 2 Model Architecture\n\nAs a starting point, we use vanilla Transformer (Vaswani et al., 2017). We modify its encoder portion by splitting it into three segments: the input layer $L_{i n}$, the body of the encoder with multiple streams, and the output layer $L_{\\text {out }}$. We use $S_{i}$ to denote $i$-th stream with output $Z_{i}$. Figure 1 shows a baseline \"linear stream\" encoder (1a) and a multi-stream encoder with two parallel streams containing one layer each (1b). The skip connection wrapping the body of the encoder from the input layer to the output layer is shown as a dashed line in both cases. Output for the multi-stream encoder with a skip connection is calculated as:\n\n$$\nZ_{\\text {out }}=L_{\\text {out }}\\left(\\sum_{1 \\leq i \\leq k} S_{i}\\left(Z_{i n}\\right)+Z_{\\text {in }}\\right)\n$$\n\nwhere $k$ is the number of parallel streams. We will refer to the Multi-Stream Transformer that has in encoder $k$ parallel streams of $l$ layers each as Multi-Stream $k(l)$. Figure 1 b shows Multi-Stream\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-2.jpg?height=800&width=777&top_left_y=211&top_left_x=1058)\n\nFigure 1: Multi-Stream Transformer Encoder. a. Transformer encoder is a sequence of layers, we use this architecture as a baseline. We differentiate input transformer layer $L_{i n}$, streams $S_{i}$ and output transformer layer $L_{\\text {out }}$ in the encoder. Baseline \"linear stream\" model has only one stream $S_{1}$. Skip connection is created if output of the input stream $Z_{i n}$ is added to the output $Z_{1}$ of the linear stream. b. Multi-Stream encoder with two streams $S_{1}$ and $S_{2}$. Skip connection is shown as a dashed line. 2(1) model with the total of 4 encoder layers. See Appendix A for an illustration of Multi-Stream 2(2) and Multi-Stream 4(1) architectures with 6 layers. Note that for this study, we only consider Multi-Stream Transformers with identical number of layers in all streams. ## 3 Experiments\n\n### 3.1 Model configurations\n\nWe studied multi-stream models of two sizes: models with four encoder layers and with six encoder layers. In both cases, we used a regular \"linear stream\" decoder with the same number of layers. Models with 4 layers (Figure 1b.) include MultiStream 2(1) and Multi-Stream 2(1) +skip, in which a skip connection is added. Models with 6 layers (Appendix A) include Multi-stream 2(2) with two layers per stream, Multi-stream 2(2) +skip, and Multi-stream 4(1) +skip. The last model has four single-layer streams and skip connection. Baseline model in each case is a vanilla Transformer that has the same number of layers arranged in a standard linear stream. Due to the computational\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-3.jpg?height=881&width=1605&top_left_y=198&top_left_x=235)\n\nFigure 2: Multi-stream Transformers with skip connection show better performance. a. All multi-stream variations for the 4-layer model (see Figure 1) demonstrate faster convergence and better loss values after 20 epochs compared to vanilla Transformer. Adding only skip connection (Transformer-skip) accelerates learning on the first stage ( 10 epochs) but makes no difference after.",
    "multistreamtrans-2": "b. Performance of the 6 -layer models is significantly improved by presence of the skip connection. A model with two streams consisting of two layers each (Multistream 2(2)) shows the worst results. All data points presented are average of 3 runs for each model. constraints, we train 4-layer models for 20 epochs and 6-layer models for 10 epochs, using default parameters (see Appendix B). We used a standard machine translation benchmark WMT-14 DE-EN (Bojar et al., 2014) in our experiments. ### 3.2 Results\n\nTable 1 presents averaged BLEU scores after the training for the studied models. Multi-stream architectures with skip connection significantly outperforms Transformer as well as Transformer with skip connection. We had not extensively tuned training parameters so the BLEU scores of baseline as well as multi-stream models of the 6-layer size were in the range of $24-25.5$. This is a modest but solid performance given that recent results reported in the literature mainly vary between 20 and 30 of BLEU score (see for example (Kasai et al., 2020)). Training loss for each model is shown in Figure 2. For both 4-layer and 6-layer sizes multi-stream architectures demonstrate advantage over the baseline. Although average learning curve for Multistream 2(2) converge slower compared to vanilla Transformer, closer examination of individual runs shows that 2 of them similar to the baseline and the one run is outlier. Experiments with 4-layer models (Figure 2a) also shows that training instability arise after 10-15 epochs of training especially for baseline Transformer and its skip connection modification. These results suggest that adding skip connection accelerates training but in the case of the baseline linear stream Transformer leads to inferior performance. However, note that multistream models with skip connections consistently converge faster and to a lower loss value for both 4 - and 6-layer settings. ### 3.3 Attention patterns\n\nIn order to better understand the differences in processing that occurs in different model configurations, we examined attention patterns for 6-layer Baseline, Baseline +skip and Multi-stream 4(1) +skip. Common pattern types are vertical, diagonal, diagonal shift forward, diagonal shift backward, soft diagonal and heterogeneous (see Figure 2 and 3 in the Appendix C for details). Diagonal patterns effectively copy information from the closest neighbours or the token itself. Heterogeneous patterns aggregate information over the whole sequence, and it has been argued that only heterogeneous attention heads may track linguistic structure (Koval-\n\n| 4 layers per encoder/decoder <br> (20 epochs) |  |\n| :--- | ---: |\n| Transformer | 18.99 |\n| Transformer +skip | 18.76 |\n| Multi-stream 2(1) | 19.13 |\n| Multi-stream 2(1) +skip | $\\mathbf{1 9 . 4 6}$ |\n| 6 layers per encoder/decoder |  |\n| (10 epochs) |  |\n| Transformer | 24.65 |\n| Transformer +skip | 25.49 |\n| Multi-stream 2(2) | 23.90 |\n| Multi-stream 2(2) +skip | 25.61 |\n| Multi-stream 4(1) +skip | $\\mathbf{2 5 . 6 3}$ |\n\nTable 1: Performance of Multi-Stream Transformer models on WMT-14 DE-EN translation task.",
    "multistreamtrans-3": "Values represent an average of BLEU 4 scores for 3 runs of every model. eva et al., 2019). Interestingly, some layers have the tendency for the majority of heads to have the same type of attention pattern suggesting that different layers may indeed perform specialized processing. We note that self-attention heads in the first layer of the encoder are similar for all models. However, patterns observed in layers 2-5 of the encoder differ across models. For the Baseline, heterogeneous patterns disappear and diagonal patterns become dominant in the later layers. Vertical patterns dominate the Baseline skip model for all intermediate layers. In Multi-stream 4(1) +skip models, on the other hand, all patterns types seem to occur equally often. Visual examination of attention patterns in different streams in this model shows that attention patterns indeed vary between different streams, suggesting that different streams may be processing alternative interpretations of the input. ## 4 Discussion\n\nOur experiments suggest that Multi-Stream Transformer has a competitive advantage over the corresponding linear stream baseline, with the advantage becoming more pronounced in the presence of skip connection between the first and the last layers of the Transformer.",
    "multistreamtrans-4": "Following several recent studies that analyse different types of attention patterns observed in Transformer architectures (Kovaleva et al., 2019; Clark et al., 2019), we examined the attention patterns obtained when the decoder attention heads are attending to the last layer of the encoder. Kovaleva et al. (2019) proposed to classify self-attention patterns into five categories, surmising that only \"heterogeneous\" patterns which spread attention across all input tokens might track non-trivial information about linguistic structure. Among other classes, \"vertical\" patterns characterized the situation when the attention was directed solely at a particular token (usually, SEP, CLS, or punctuation). In the BERT model (Devlin et al., 2019) fine-tuned for GLUE (Wang et al., 2018), they were estimated to account for about $30 \\%$ of all attention patterns (Kovaleva et al., 2019). Recently, Kobayashi et al. (2020) re-examined vertical patterns looking at the norms of the attention-weighted input vectors, rather than just the attention weights. They found that when \"vertical\" attention heads pay attention to SEPs and other special tokens, attention weights and input vector norms are inversely correlated. That is, \"vertical\" self-attention patterns function as a \"passthrough\" operation that collects no information from the input tokens. Self-attention computation is essentially ignored, and the skip connection passes through the information computed in the previous layer. Examining decoder attention patterns over encoder output in our models, we observed that linear stream baseline Transformer tends to have substantially more of such vertical patterns, while the Multi-stream Transformer tends to be dominated by \"heterogeneous\" patterns (see Figure 3 in Appendix C). If the hypothesis that heterogeneous self-attention patterns may encode meaningful linguistic information is correct, this may be seen as supporting the hypothesis that the encoder in multistream Transformer is delaying the resolution of ambiguities in the input. ## 5 Conclusion\n\nThis study has shown that Multi-Stream Transformer models that take advantage of separate parallel streams of computation are able to outperform equivalent linear stream Transformers. We also presented some evidence in support of the notion that parallel streams compute alternative hypothesis which are evaluated when they are merged at the final layer of the encoder. ## References\n\nOndrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling,\n\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12-58, Baltimore, Maryland, USA. Association for Computational Linguistics. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.",
    "multistreamtrans-5": "Manning. 2019. What Does BERT Look at? An Analysis of BERT's Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for $N L P$, pages 276-286, Florence, Italy. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186. Yoav Goldberg. 2019. Assessing BERT's syntactic abilities. arXiv preprint arXiv:1901.05287. John Hewitt and Christopher D. Manning. 2019. A Structural Probe for Finding Syntax in Word Representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129-4138. Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, Samuel Unicomb, Gerardo I\u00f1iguez, M\u00e1rton Karsai, Yannick L\u00e9o, M\u00e1rton Karsai, Carlos Sarraute, \u00c9ric Fleury, et al. 2019. What does BERT learn about the structure of language? In 57th Annual Meeting of the Association for Computational Linguistics (ACL), Florence, Italy.",
    "multistreamtrans-6": "Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao Gu. 2020. Parallel machine translation with disentangled context transformer. arXiv preprint arXiv:2001.05136.",
    "multistreamtrans-7": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms.",
    "multistreamtrans-8": "arXiv:2004.10102 [cs].",
    "multistreamtrans-9": "Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the Dark Secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4356-4365, Hong Kong, China. Association for Computational Linguistics. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in neural information processing systems, pages 5998-6008. Jesse Vig and Yonatan Belinkov. 2019. Analyzing the Structure of Attention in a Transformer Language Model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63-76, Florence, Italy.",
    "multistreamtrans-10": "Association for Computational Linguistics. Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics. ![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-6.jpg?height=920&width=1600&top_left_y=204&top_left_x=238)\n\nFigure 3: Multi-Stream Transformer Encoder with 6 layers. a. Multi-stream 2(2) encoder consists of two streams of depth 2. b. Multi-stream 4(1) encoder has 4 one layer streams. ## A Multi-stream architectures for 6 encoder layers\n\nFigure 3 shows Multi-Stream Transformer architectures with 6 encoder layers. ## B Training details\n\nTo implement multi-stream Transformer, we extended the code from the dedicated TensorFlow tutorial repository ${ }^{1}$. Models of both sizes used default settings. The 4 layer setup had parameters $d_{\\text {model }}=128, d_{f f}=$ $512, h=8, P_{\\text {drop }}=0.1$, warmup $_{\\text {steps }}=4000$, and the 6 layer setup had $d_{\\text {model }}=512, d_{f f}=$ $2048, h=8, P_{\\text {drop }}=0.1$, warmup $_{\\text {steps }}=32000$. For all experiments batch size was 64. [^0]We used a standard machine translation benchmark WMT-14 DE-EN (Bojar et al., 2014) in our experiments. One epoch of training covered the full training set of 4.5 M sentence pairs. Dataset was tokenized with TFDS subword text encoder ${ }^{2}$ and dictionary of 32 K per language. ## C Visualization of attention patterns. Figures 4 and 5 show (a) encoder self-attention patterns and (b) decoder attention to encoder output layer for baseline linear stream Transformer with and without skip connection, and for the MultiStream 4(1) architecture. [^1]Transformer encoder (self-attention)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-7.jpg?height=499&width=1599&top_left_y=564&top_left_x=232)\n\nTransformer skip encoder (self-attention)\n![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-7.jpg?height=606&width=1597&top_left_y=1132&top_left_x=235)\n\nFigure 4: Visualization of the baseline Transformer, baseline Transformer skip and Multi-Stream 4(1) +skip encoder self-attention patterns. ![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-8.jpg?height=1086&width=1598&top_left_y=542&top_left_x=237)\n\nMulti-stream 4 skip decoder (attention to the output layer of the encoder)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1078ddc5312877d8c7f2g-8.jpg?height=472&width=1586&top_left_y=1734&top_left_x=241)\n\nFigure 5: Transformer, Transformer skip and Multi-stream 4(1) +skip attention patterns from the different decoder layers to encoder output layer. [^0]:    ${ }^{1}$ https://github.com/tensorflow/docs/ blob/master/site/en/tutorials/text/ transformer.ipynb\n\n[^1]:    ${ }^{2}$ https://www.tensorflow.org/datasets/ api_docs/python/tfds/features/text/ SubwordTextEncoder\n\n"
}