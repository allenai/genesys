{
    "explicitencoding-0": "# Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks \n\nMahdi Sabbaghi, George Pappas, Hamed Hassani, and Surbhi Goel<br>University of Pennsylvania<br>\\{smahdi, pappasg, hassani, surbhig\\}@seas.upenn.edu\n\nJune 5, 2024\n\n\n#### Abstract\n\nDespite the success of Transformers on language understanding, code generation, and logical reasoning, they still fail to generalize over length on basic arithmetic tasks such as addition and multiplication.",
    "explicitencoding-1": "A major reason behind this failure is the vast difference in structure between numbers and text; For example, the numbers are typically parsed from right to left, and there is a correspondence between digits at the same position across different numbers.",
    "explicitencoding-2": "In contrast, for text, such symmetries are quite unnatural. In this work, we propose to encode these semantics explicitly into the model via modified number formatting and custom positional encodings. Empirically, our method allows a Transformer trained on numbers with at most 5-digits for addition and multiplication to generalize up to 50-digit numbers, without using additional data for longer sequences. We further demonstrate that traditional absolute positional encodings (APE) fail to generalize to longer sequences, even when trained with augmented data that captures task symmetries. To elucidate the importance of explicitly encoding structure, we prove that explicit incorporation of structure via positional encodings is necessary for out-of-distribution generalization. Finally, we pinpoint other challenges inherent to length generalization beyond capturing symmetries, in particular complexity of the underlying task, and propose changes in the training distribution to address them. ## 1 Introduction\n\nLarge language models (LLMs) [BMR ${ }^{+}$20, Ope23], powered by transformer architectures [VSP ${ }^{+}$17], and extensive computational resources, have demonstrated emergent abilities in language understanding, code generation, and logical reasoning [WTB ${ }^{+}$22]. Despite these capabilities, LLMs still struggle to generalize when handling unseen data in highly structured tasks like arithmetic [DLS ${ }^{+} 23, \\mathrm{LAG}^{+}$23a]. Specifically, they fail to extrapolate from shorter instances to longer ones, a challenge known as length generalization. This issue is particularly evident in arithmetic, where models fail to generalize from smaller numbers to larger ones. $\\left[\\mathrm{NAGA}^{+}\\right.$21a, $\\mathrm{NJL}^{21}, \\mathrm{LSL}^{+}$23]. It is now well understood that the major reasons for the failure due to structural issues are the use of positional encodings that do not scale with sequence length $\\left[\\mathrm{KPR}^{+} 23, \\mathrm{SBE}^{+} 23\\right]$, and format of the data itself [SBE ${ }^{+}$23]. Though the Transformer can learn to solve this task for a fixed length of numbers given enough samples, the inductive bias of the architecture is insufficient to capture such structure across unseen positions, in order to generalize to longer lengths. We posit that the challenges of length generalization in arithmetic tasks, such as addition and multiplication, can be broken down into two factors, (1) increase in complexity of the task, (2) inability to capture the positional structure in the tasks. For example, when we add two numbers, the positional structure corresponds to aligning each digit from right to left to compute the digit-wise addition, and the complexity of the task corresponds to the longest sequence of carry-overs. In contrast, when we perform multiplication of two\nnumbers (say 1 digit number multiplied with $k$ digit number), the positional structure corresponds to aligning the first number with all of the digits of the second number to compute digit-wise multiplication, and complexity corresponds similarly to longest sequence of induced carries that need to be resolved. We focus on rectifying the inability of Transformers to capture positional structure and provide strong evidence for the need to explicitly encode structural biases into the architecture for achieving length generalization. We show that with the correct data format and choice of positional encodings, a Transformer trained on only data with numbers of a bounded size (e.g., $\\leq 5$ ) can generalize to numbers of much longer length (e.g., $\\geq 50$ ) on both addition and multiplication. We further show, empirically and theoretically, that standard tricks of implicitly inducing this positional structure, such as data augmentation with shifting contexts [BMR ${ }^{+20]}$, are not sufficient to guarantee this generalization. Our key findings are summarized below:\n(F1) Positional encodings that encapsulate the structure enable length generalization for addition and multiplication. For addition, relative position encoding (RPE) allows the Transformer to naturally capture digit-wise alignment in order to achieve length generalization from adding two numbers with up to 5 -digit to two numbers with up to 50 -digit (similar generalization was observed in [JdDE ${ }^{+}$23b]). For multiplication, our proposed new uniform positional encoding (UPE) that assigns each digit of a number the same position encoding, coupled with RPE, allows the Transformer to length generalize from multiplication of 3-digit numbers with 5 -digit numbers to multiplication of a 3-digit number with 20-digit numbers. To our knowledge, this is the first positive result on length generalization for multiplication without using chain-of-thought [WWS ${ }^{+} 22$ ] or data priming [JdDE $\\left.{ }^{+} 23 \\mathrm{~b}\\right]$. (F2) Data augmentation is insufficient for length generalization. The most natural data augmentation of shifting the numbers by adding zeros to the right to implicitly induce the necessary alignment needed for both addition and multiplication helps with length generalization; However, the performance is significantly worse than the crafted positional encodings. To further elucidate this, we propose a simple linear setting that captures the relative position alignment of addition, and prove that a one-layer linear Transformer trained with gradient descent with RPE generalizes, and shifting with zeros to encode a similar bias does not help. (F3) Incorporating higher complexity shorter sequences improves generalization to longer sequences. While our proposed positional encodings address the structural aspect, we also isolate the other major reason for failure on longer samples to be related to the increase in complexity of the task. We show that the increase in coverage of higher complexity examples, which lie in the tails of the distribution over shorter sequences, leads to improvements in performance on longer sequences. For the task of addition, we show the required length of shorter sequences needed to adequately cover complex examples for larger sequences under the uniform distribution on numbers is not very large.",
    "explicitencoding-3": "This allows us to cover sufficient complexity with 5 -digit numbers to generalize to 50 -digit numbers. Related Work. Despite significant advances, Transformer models such as GPT-4 [Ope23] struggle with highcomplexity arithmetic tasks [DLS ${ }^{+} 23, \\mathrm{YDL}^{+}$23]. Investigations into these limitations have ranged from ineffective representation learned for numbers [WSC ${ }^{+} 16$, SHB16, WWL ${ }^{+} 19$, TPSI21], to the inability to achieve numeracy via unsupervised learning [RaGS22, LM22, $\\mathrm{KHK}^{+}$21, WZNZ21, MSK ${ }^{+}$23]. Attempts to mitigate these deficiencies such as fine-tuning pre-trained models[GGB20, LAD ${ }^{+}$22], scratch-padding[NAGA ${ }^{+}$21b], and leveraging chain-of thoughts reasoning [WWS ${ }^{+} 22, \\mathrm{OWJ}^{+} 22$ ] only provide marginal out-of-distribution (OOD) benefits [AWA ${ }^{+22]}$. Specifically, [AWA $\\left.{ }^{+} 22\\right]$ shows that scratchpad fine-tuning fails to generalize to longer problems, and even with chain-of-thoughts, model are prone to error-propagation, learning shortcut solutions and insufficient features, etc [DLS ${ }^{+}$23]. Recent efforts have proposed modification to data and model structures to achieve length generalization [JdDE ${ }^{+} 23 \\mathrm{a}, \\mathrm{SBE}^{+}$23]. This includes introducing new structures to positional vectors or leveraging positional information like [SUV18, $\\mathrm{HVU}^{+} 18$, $\\mathrm{SLP}^{+}$23, $\\mathrm{RSR}^{+}$23]. Some studies suggest removing positional vectors or using randomized encodings [ $\\left.\\mathrm{KPR}^{+} 23, \\mathrm{RDG}^{+} 23\\right]$. Despite these efforts, arithmetic tasks remain difficult. In this work, we assert that without explicitly encoding symmetries, generalization remains unattainable even with implicitly encoding symmetries via data augmentation (see Section 3). Building on prior work, we employ RPE [SUV18] motivated by the results of [JdDE ${ }^{+}$23a] for addition, and design our custom embedding for multiplication in Section 3, providing a new\nframework to leverage symmetries effectively and give the first theoretical explanation for the effectiveness of this approach in Sections 3.1 and 5. Beyond architectural changes, modifying the training distribution has also been a prominent direction for improving performance $\\left[\\mathrm{LSL}^{+} 23, \\mathrm{SBE}^{+}\\right.$23], particularly through curriculum learning [WCZ21, $\\mathrm{YDL}^{+}$23], and priming $\\left[J_{d D E}{ }^{+} 23 b\\right]$. Our observations in Section 4 suggest that exposing models to more complex in-distribution samples can enhance OOD performance. We provide additional related works in Appendix A. ## 2 Preliminaries: Positional Encodings\n\nTo ground our design of positional encodings, we first describe a general formalism for positional encodings in this section. Transformer Architecture. A transformer model takes as input a sequence of $d_{x}$-dimensional vectors, denoted as $\\boldsymbol{x}=\\left(x_{1}, \\ldots, x_{n}\\right)$ where $x_{i} \\in \\mathbb{R}^{d_{x}}$. These vectors are embedded using an embedding matrix $\\boldsymbol{E}: \\mathbb{R}^{d_{x}} \\rightarrow \\mathbb{R}^{d_{z}}$, producing $\\boldsymbol{z}^{(0)}=\\left(z_{1}^{(0)}, z_{2}^{(0)}, \\ldots, z_{n}^{(0)}\\right)=\\left(\\boldsymbol{E} x_{1}, \\ldots, \\boldsymbol{E} x_{n}\\right)$, where $d_{z}$ is the dimension of the embedded space. Each Transformer block consists of attention layers operating on $\\boldsymbol{z}^{(l-1)}$ to produce $\\boldsymbol{z}^{(l)}$ using $H$ heads, followed by a shared MLP layer. Each head computes attention scores:\n\n$$\nA_{i, j}^{(l)}=\\operatorname{Softmax}_{j}\\left(\\left(W_{Q} z_{i}^{(l-1)}\\right)^{T}\\left(W_{K} z_{j}^{(l-1)}\\right)\\right)\n$$\n\nThe output of each attention head is computed as follows:\n\n$$\nb_{i}^{(l)}=\\sum_{j} A_{i, j}^{(l)}\\left(W_{V} z_{j}^{(l-1)}\\right) \\in \\mathbb{R}^{d_{z} / H}\n$$\n\nwhere $W_{Q}, W_{K}, W_{V} \\in \\mathbb{R}^{d_{z} / H \\times d_{z}}$, are three matrices that together form the core of each head. The concatenated head outputs are processed by a feed-forward network to obtain $z_{i}^{(l)}$. In the formulation given above, the output at position $i$ remains invariant to the permutation of the rest of the sequence $\\left(\\left\\{x_{1}, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_{n}\\right\\}\\right)$, which is a limitation for our tasks where the sequence order is indispensable. As a result, positional encodings are employed. Absolute Positional Encoding (APE). One approach to overcome this limitation is to encode positional information by adding a collection of unique and fixed vectors to the input $x_{i}$ 's:\n\n$$\nz_{i}^{(0)}=\\boldsymbol{E} x_{i}+p_{i}\n$$\n\nThis method is known as absolute positional encoding (APE), where the positional vectors can either be predefined and fixed or learned during training, adapting to specific task requirements. Pairwise Positional Encoding. More generally, since every layer of an attention model only considers pairwise interactions between elements, we can move the positional vectors inside the calculation of attention scores either by adding the pairwise vectors ( $p_{i j}$ ) to the embedded keys, or to the embedded queries, or to the computed attention between a query and a key. In all experiments, we adhere to the method of adding them to the key, as suggested in [SUV18]. That is,\n\n$$\nA_{i, j}=\\operatorname{Softmax}\\left(\\left(W_{Q} z_{i}\\right)^{T}\\left(W_{K} z_{j}+p_{i, j}\\right)\\right)\n$$\n\nWe propose tailoring positional vectors $p_{i, j}$ to the specific relationships between positions in a task. If the connection from position $i_{1}$ to $j_{1}$, denoted by $i_{1} \\rightarrow j_{1}$, has the same functionality as $i_{2} \\rightarrow j_{2}$, we can set $p_{j_{1}, i_{1}}$ equal to $p_{j_{2}, i_{2}}$. This ensures $i_{2}$ influences $j_{2}$ similarly to how $i_{1}$ influences $j_{1}$. In general, if any positions have equivalent functionality, we will set their corresponding pairwise positional vectors to be the same. Relative Positional Encoding (RPE). For tasks with translational symmetries, we have the following property: $(i \\rightarrow j) \\sim((i+t) \\rightarrow(j+t)) \\quad \\forall t$, in which \" $\\sim$ \" stands for being equivalent in our notation. This symmetry implies that: $p_{i, j}=p_{i+t, j+t}$ for all $t$. Therefore, we can express positional vectors using the difference between $i$ and $j$ :\n\n$$\np_{i-j} \\leftarrow p_{i, j}\n$$\n\nConsequently, computations are based on relative positions, i.e. the output depends on the relative, not absolute, positions in the sequence, hence the term relative positional encoding (RPE) [SUV18]. ## 3 Length Generalization for Arithmetic tasks\n\nIn this paper, we focus on two arithmetic tasks: addition and multiplication.",
    "explicitencoding-4": "We first formalize the setup of our tasks and then describe our findings. Data format. The input consists of two integers represented per-digit as $x^{(1)}=\\left(x_{l_{1}}^{(1)}, \\ldots, x_{1}^{(1)}\\right), x^{(2)}=$ $\\left(x_{l_{2}}^{(2)}, \\ldots, x_{1}^{(2)}\\right)$, and we ensure both sequences are of the same length $\\left(l_{1}=l_{2}=l\\right.$ ) by adding \"pad\" tokens to both integers, as necessary. Consider for illustration the following example: \"123 + 4095 = 4218\", and $l=20$ :\n\n$$\n\\begin{aligned}\n& \\text { Input: }(\\underbrace{\\text { pad, }, \\ldots, \\text { pad }}_{17 \\text { times }}, 1,2,3,+, \\underbrace{\\text { pad, } \\ldots, \\text { pad }}_{16 \\text { times }}, 4,0,9,5) \\\\\n& \\text { Output: } \\quad(\\underbrace{(}_{20 \\text { times }}, \\ldots, \\underbrace{\\text { pad }, \\ldots, \\operatorname{pad}}_{17 \\text { times }}, 4,2,1,8)\n\\end{aligned}\n$$\n\nFor multiplication, we use the same input format with the difference that the multiplier is not padded and the \" + \" token is replaced with \" $\\times$ \":\n\n$$\n\\begin{aligned}\n& \\text { Input: } \\quad(5,6, \\times, \\underbrace{\\text { pad, }, \\ldots, \\text { pad }}_{16 \\text { times }}, 4,2,9,7) \\\\\n& \\text { Output: }(\\_, \\ldots,, \\underbrace{\\text { pad, }, \\ldots, \\text { pad }}_{16 \\text { times }}, 2,4,0,6,3,2)\n\\end{aligned}\n$$\n\nNote that in the example above, the maximum length of the output can reach 22 digits, accounting for the worst-case scenario of multiplying a 20-digit number by a 2 -digit number.",
    "explicitencoding-5": "Indexing from right to left for simplicity, the first digit of the answer corresponds to the first position of the input. The \"_\" symbol in the output indicates neglected tokens that are not part of the supervision signal for the addition task. Note that our formatting approach is distinct from that of $\\left[\\mathrm{JdDE}^{+}\\right.$23a] in that we pad each integer from the left to ensure that the relative coordinates of digits remain consistent regardless of the integers' lengths, and also from other prior approaches that reverse the digits [SBE ${ }^{+} 23, \\mathrm{LSL}^{+}$23]. The fixed length assumption is not unrealistic since numbers are typically represented with fixed bit-complexity in most computations. Consequently, the tokenizer can readily impose a fixed-length structure of any chosen length as sketched in Figure 6. Length generalization task. For our tasks, the output consists of the integer resulting from the arithmetic operation on $x^{(1)}$ and $x^{(2)}$. We can model our length generalization problem as an instance of generalization on the unseen [ABLR23]. We denote the domain of all pairs of integers with lengths $\\leq l$ by $\\mathcal{D}:=\\left\\{\\left(x^{(1)}, x^{(2)}\\right) \\in \\mathbb{Z}^{2}: 0 \\leq x^{(1)}, x^{(2)}<10^{l}\\right\\}$. In the problem of length generalization, we observe only a subset of this domain during training. Specifically, pairs of integers with lengths at most $l_{s}<l$ are used in training. We denote this seen domain as $\\mathcal{D}_{s}:=\\left\\{\\left(x^{(1)}, x^{(2)}\\right) \\in \\mathbb{Z}^{2}: 0 \\leq x^{(1)}, x^{(2)}<10^{l_{s}}\\right\\}$, and denote the\nunseen domain, which consists of at least one integer with longer length, as $\\mathcal{D}_{u}:=\\left\\{\\left(x^{(1)}, x^{(2)}\\right) \\in \\mathbb{Z}^{2}: 0 \\leq\\right.$ $\\left.x^{(1)}, x^{(2)}<10^{l}, 10^{l_{s}} \\leq x^{(1)} \\vee 10^{l_{s}} \\leq x^{(2)}\\right\\}$. We thus have: $\\mathcal{D}=\\mathcal{D}_{s} \\cup \\mathcal{D}_{u}$. We will focus on the following question: If we train the model using samples from $\\mathcal{D}_{s}$ to near perfect in-distribution accuracy, what can be inferred about the accuracy on $\\mathcal{D}_{u}$ ? Model. For all tasks, we utilize a BERT-based encoder-only attention model [DCLT18]. We use an encoderonly architecture because causal maps are not beneficial for our tasks.",
    "explicitencoding-6": "For instance, to compute the second digit of a sum, the model must consider the first two digits of both integers. This entails both looking forward and backward in the input. We also chose to avoid autoregressive settings because they often require several tricks to function, such as reversing the digits order either in the product or the inputs [SBE $\\left.{ }^{+} 23, \\mathrm{LSL}^{+} 23\\right]$. Since arithmetic operations naturally start from the least significant digit, generating them from left to right in an auto-regressive manner offers no computational advantage. The number of attention layers that we employ will vary depending on the difficulty of the task.",
    "explicitencoding-7": "Training and Evaluation. During training, samples are generated uniformly from $\\mathcal{D}_{s}$. At test time, we evaluate the generalization performance based on prediction accuracy over a uniform distribution of numbers up to $l$ digits. An output is deemed correct only if all digits, including \"pad\" symbols, are correct; otherwise, it is considered false. Note that this is worst-case accuracy over digits but average-case accuracy over l-digit numbers. We present results for different training checkpoints. As noted in [JdDE ${ }^{+}$23b], out-of-distribution accuracy can fluctuate even as in-distribution accuracy improves.",
    "explicitencoding-8": "Therefore, we show performance across various checkpoints, using a small validation set with longer sequences to choose the best checkpoint. Further experimental details are in Appendix B. ### 3.1 Task: Addition\n\nIn a transformer model, characterized by parallel information processing, the common iterative way to perform multi-digit addition is not effective. Instead, the model must simultaneously identify all relevant positions contributing to each of output digits. Addition in parallel. The result of [QB24] showcases that the transformer dissects the calculation of the sum into parallel streams of computation. Refer to Figure 1, which presents a parallel method for addition; First, it adds the corresponding digits across both integers, and then propagates the carries. To compute the output at position $i$, the model must identify two items: First, the pair of positions in the input sequence corresponding to the $i$-th digit of the first and second integers. The second item is a list of all other positions in the input sequence required to compute the incoming carry-over. Having described these items, we will now synthesize them into $\\sigma(i)$ to denote the list of all positions required for position $i$. Next, the model learns a function to compute the output at position $i$ based on the values at these positions in $\\sigma(i)$. For instance, to compute the output at position $i=2$ for the sample given in Equation (3.1), we have $\\sigma(2)=(1,2,22,23)$, pinpointing the first two digits of both integers. In general, for two $l$-digit integers in our format, $\\sigma$ is written as $\\sigma(i)=(1, \\cdots, i-1, i, l+2, \\cdots, i+l, i+l+1)$. The model then needs to learn and apply $\\sigma(\\cdot)$ to any pair of numbers from the unseen domain $\\mathcal{D}_{u}$. (A1) Failure of absolute positional encoding. We first demonstrate that while APE can succeed in generalizing to in-distribution data, its performance degrades significantly on longer length sequences. Figure 2a shows the performance of our trained model (on the uniform distribution of numbers with $l_{s}=5$ ) on a uniform distribution over domains with lengths varying from 6 to 20. As expected, performance drops drastically when the length increases from 5 to 6 . APE does not capture translational symmetries in the task of addition. In this regard, prior work [ABLR23] has shown that it is in general impossible to obtain length generalization without providing extra information to the model, either by imposing task symmetries\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-06.jpg?height=389&width=570&top_left_y=364&top_left_x=254)\n(a)\n\n```\nAlgorithm 1 Parallel Carry-Handle Addition\n    \\(x^{(1)}=\\left(x_{l}^{(1)}, \\ldots, x_{1}^{(1)}\\right) \\quad \\triangleright\\) first integer\n    \\(x^{(2)}=\\left(x_{l}^{(2)}, \\ldots, x_{1}^{(2)}\\right) \\quad \\triangleright\\) second integer\n    \\(\\triangleright\\) array of size \\(l+1\\) initialized to 0\n    for \\(i\\)\n    or \\(i \\leftarrow 1\\) to \\(l\\) do\n        \\(s_{i} \\leftarrow x_{i}^{(1)}+x_{i}^{(2)}\\)\n    while \\(\\exists i, s_{i} \\geq 10\\) for some \\(i\\) do\n        for \\(j \\leftarrow 1\\) to \\(l\\) do\n            \\(s_{j+1}^{\\prime} \\leftarrow s_{j+1}+\\left\\lfloor s_{j} / 10\\right\\rfloor \\quad \\triangleright\\) Adjust one carry\n        for \\(i \\leftarrow 1\\) to \\(l\\) do\n            \\(s_{j} \\leftarrow s_{j}^{\\prime} \\bmod 10 \\quad \\triangleright\\) Remainder counts\n    return \\(s\\)\n```\n\n(b)\n\nFigure 1: Parallel Carry-Handle Addition.",
    "explicitencoding-9": "(a) An illustration of a parallel algorithm that handles cascading carries in addition. The algorithm first adds the corresponding digits of the two integers and then propagates the carries to the latter positions.",
    "explicitencoding-10": "(b) Pseudo-code explanation of the process. Note that the for loop computations can take place in parallel for each position. on the model or by using data augmentation in training. Next, we will see that encoding the translational symmetries through relative positional encoding (RPE) will lead to generalization on the unseen positions in the task of addition. (A2) Relative position encoding enables length generalization. Figure $2 b$ confirms that RPE is capable of encoding translational equivariances for the task of addition. Adding RPE to the Transformers maintains its high performance at longer lengths. This effectiveness of RPE was first captured in [JdDE ${ }^{+}$23b]. Note that the abrupt fall in 50-digit sums is due to the input format. In all samples seen from $\\mathcal{D}_{s}$, the two integers are separated with a \"+\" and several \"pad\" tokens, whereas when testing on 50-digit sums, there is no \"pad\" sign that separates them. We consider this to be a spurious correlation that the model has learned. the gradual decline in accuracy for longer sequences is a consequence of two factors. More importantly, the gradual decline in accuracy for longer sequences is caused by samples with higher complexities that are more likely among larger integers, according to the definition we give for complexity in Section 4. (A3) Adding symmetries implicitly via augmented data is not enough. Knowing that the task has certain symmetries, another potential approach is to use data augmentation to implicitly encode the symmetries. For the task of addition, a natural data augmentation scheme consists of shifting a pair of integers in the training data by adding the same number of zeros from the right. ${ }^{1}$ We trained a BERT model, again with APE, on this augmented dataset, and the result can be seen in Figure 2c. The figure shows that augmentation does not lead to generalization on the unseen positions. However, we remark that the in-distribution performance (on shifted test data) is near-perfect (above 0.999 ), indicating that it has learned translation by zeros. To understand the failure of the model trained with augmentation, we examined the model's output for 10-digit sums. Most errors occurred in the least significant digit, even though the model performs well on single-digit sums in Figure 2c. The detailed explanation of this experiment can be found in Appendix C.1. Since augmentation only adds zeros when shifting, causing the model to learn spurious correlations between the first position and positions larger than $l_{s}$, which will not hold at test time. We will address this erroneous learning in second part of Theorem 5.1, where only some parameters are properly learned during training,\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-07.jpg?height=528&width=1602&top_left_y=238&top_left_x=269)\n\nFigure 2: (a) Accuracy of the model using absolute positional encoding while increasing the length of the sequence. (b) The model with RPE trained with up to 5-digit sums and tested on up to 50-digit sum. While showing high fluctuations, the inductive bias of relativity makes generalization possible. (c) The model with APE when trained with augmented data, i.e., all shifted versions of samples from the dataset are given in training. while others can grow inadvertently. We show a similar spurious correlation appearing in our simplified theoretical setup in Section 5. ### 3.2 Task: Multiplication\n\nEfficient algorithms for multiplication, such as 'shift and add', cannot compute multiplication in completely parallel streams. However, a parallel algorithm is necessary to effectively induce the task structures into a transformer architecture. Figure 3a proposes a method that, under a limited length constraint for the multiplier, splits a multi-digit multiplication into the product of the multiplier with each digit of the multiplicand, and propagates the carries subsequently. To induce this mechanism, we introduce uniform positional encoding, where each digit of the multiplier uniformly affects all digits of the multiplicand. Uniform Positional Encoding (UPE). Starting from Equation (2.1), consider a scenario where position $i$ uniformly affects every other position $j \\neq i$. In this case, the positional vector $p_{j, i}$ must remain constant for every $j$ :\n\n$$\np_{j, i}=p_{j^{\\prime}, i}=c_{i} \\quad \\forall j, j^{\\prime} \\neq i\n$$\n\nFurthermore, if a task involves multiple positions with uniform functionality, assuming $i_{1}, \\ldots, i_{m}$ is the set of such positions, we assign them the positional vectors $c_{1}, \\cdots, c_{m}$, respectively. If we denote those positions in the sequence whose outputs have to be computed as $\\left\\{j_{1}, \\cdots, j_{n}\\right\\}$, the computation will need the following collection of positional vectors: $\\left\\{p_{i, j} ; i, j \\leq n\\right\\} \\cup\\left\\{c_{1}, \\ldots, c_{m}\\right\\}$. Note that additional relationships may exist among $j_{1}, \\ldots, j_{n}$, such as translational properties, which would justify the use of relative positional encoding. For example, for multiplying a 3 -digit number by a 20 -digit number, we will use $c_{1}, c_{2}, c_{3}$ for the multiplier as well as relative positional vectors $\\left\\{p_{-21}, \\cdots, p_{21}\\right\\}$ (length of the answer is at most 22 digits). Remark. Multiplication is significantly more challenging than addition, even when the multiplier is a singledigit integer. The model has to deal with larger stack of carries compared to the addition task. This is because the probability of a carry occurring is higher at each stage, and unlike in addition, carries can exceed 1 . See Appendix C. 4 for more explanation. Consequently, we limit our experiments to 3 -digit multipliers and focus on the generalization of the multiplicand. Even in this setting, we require larger networks compared to the task of addition to generalize. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-08.jpg?height=391&width=513&top_left_y=406&top_left_x=253)\n(a)\n\n```\nAlgorithm 2 Parallel Carry-Handle Multiplication\n1: \\(x^{(1)}=\\left(x_{l_{1}}^{(1)}, \\ldots, x_{1}^{(1)}\\right) \\quad \\triangleright\\) first integer\n    \\(x^{(2)}=\\left(x_{l_{2}}^{(2)}, \\ldots, x_{1}^{(2)}\\right) \\quad \\triangleright\\) second integer\n    \\(m=\\left(m_{l_{1}+l_{2}}, \\ldots, m_{1}\\right)\\)\n    for \\(i \\leftarrow 1\\) to \\(l\\) do\n        \\(m_{i} \\leftarrow x^{(1)} * x_{i}^{(2)}\\)\n    while \\(\\exists i, m_{i} \\geq 10\\) for some \\(i\\) do\n        for \\(k \\leftarrow 1\\) to \\(l_{1}\\) do\n            for \\(j \\leftarrow 1\\) to \\(l_{1}+l_{2}-k\\) do\n                \\(m_{j+k}^{\\prime} \\leftarrow m_{j+k}+\\left\\lfloor m_{j} / 10^{k}\\right\\rfloor \\quad \\triangleright\\) Handle one carry\n        for \\(j \\leftarrow 1\\) to \\(l_{1}+l_{2}-1\\) do\n            \\(m_{j} \\leftarrow m_{j}^{\\prime} \\bmod 10\\)\n                \\(\\triangleright\\) Remainder counts\n    return \\(m\\)\n```\n\n(b)\n\nFigure 3: (a) An illustration of a parallel algorithm that computes the multiplication, inspired from the uniform functionally of the multiplier in the absence of carries.",
    "explicitencoding-11": "Besides, carries still has a relative structure for propagation to following positions. (b) Pseudo-code explanation of the process assuming $l_{1} \\ll l_{2}$. (M1) APE and RPE fail at length generalization. We evaluated both absolute and relative positional encodings, in a single-digit multiplier scenario, as provided in Figures 4a and 4b. As anticipated, APE fails to generalize on the unseen positions, and RPE shows a decline in accuracy. In the three-digit multiplier scenario, as carries become more challenging, RPE cannot generalize to longer multiplicands as shown in Figure 5a, which is consistent with the results of prior work [JdDE ${ }^{+} 23 b$ ]. RPE is unable to preserve the functionality of the multiplier on those positions of the multiplicand padded at training. (M2) UPE (along with RPE) is effective length generalization. In contrast, our UPE imposes this constraint and shows a significant improvement over RPE in the single-digit multiplier scenario (Figure 4c). This advantage becomes more apparent for the three-digit multiplier (Figure 5b); Once we incorporate the three uniform positional vectors from Equation (3.3) to the multiplier's digits and RPE for the rest, they enable the model to maintain $90 \\%$ of its original accuracy at the length of 16 . As far as we know, this is the first positive result for length generalization in multiplication purely from architectural modifications. For the same reason explained in Section 3.1 for RPE, the fall in accuracy in 20-digit sums is inevitable considering our input format. (M3) Augmentation fails in multiplication. When some zeros are added to the end of the multiplicand while keeping the multiplier unchanged, the result is a shifted answer by zeros. This highlights a symmetry of the problem, aligned with the translational symmetry of RPE for the multiplicand and the constant functionality of UPE for the multiplier. We used this to augment the training data and see if it improves results for models with absolute and relative positional encodings. However, as shown in Figure 5c, the model utilizing APE fails similarly to the same attempt in the addition task. As for the model with RPE, despite extensive training data, it still falls short of the performance of our UPE. See Figure 5d. ### 3.3 Extension: Applying to Data with Text + Numbers\n\nOur input format may initially seem limited to numeric experiments. However, we show how our proposed positional encodings can be effectively applied to a broader corpus, including text. We propose to set up the pairwise positional encodings in Equation (2.1) based on the significance of the digits within their integer; For instance, always the same pairwise positional vector is incorporated to the first digits of both integers regardless of their positions in the input. This approach leverages the fact that the structures in arithmetic\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-09.jpg?height=534&width=1602&top_left_y=238&top_left_x=269)\n\nFigure 4: (a) Accuracy of a BERT model with APE for single-digit $\\times$ multi-digit multiplication when trained only up to 5-digit multiplicands.",
    "explicitencoding-12": "(b) Same setting as in (a) but with RPE. (c) Same setting as (a) but with our proposed UPE. Using the uniform symmetry naturally gives advantages over RPE. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-09.jpg?height=426&width=1636&top_left_y=948&top_left_x=243)\n\nFigure 5: (a) Accuracy of a BERT model with RPE for 3-digit multi-digit multiplication when trained only up to 5-digit multiplicands. (b) Same setting as in (a) but with our new positional encoding. The difference between UPE and RPE becomes more apparent as the length of the multiplier increases. (c) The model using APE with the privilege of augmented data. (d) The model with RPE and trained with augmented data. Both models with APE and RPE are outperformed by our UPE. tasks like addition and multiplication are grounded on the number system. As illustrated in Figure 6, we employ separate positional encodings for text (using APE) and arithmetic calculations (using Pairwise PE). Since the latter must be implemented at the attention level, specific attention heads are designated for this purpose. For instance, Figure 6 demonstrates that attention labeled with \"Add Head ${ }_{i, j}$ \" utilizes RPE. These heads add RPE only to the pairwise interactions among the numerical parts of the input, as detailed in Equation (2.1), while the non-numerical parts remain unaffected by RPE. Note that we do not mask any part of the input sequence, allowing all attention heads to access the entire input. Addition and Multiplication in the presence of text. While we include text in our experiments, attention heads are specifically trained to produce the correct answers for the arithmetic tasks. Although fully integrating this method into LLMs requires extensive resources, our current setting is sufficient to demonstrate its effectiveness and can be easily scaled up in future work. To test the robustness of our encodings in a more complex scenario, for both tasks of addition and multiplication, we altered formats in Equations (3.1) and (3.2) to include up 20 random tokens before, between, and following the two integers, and as previously, the answer is fetched from the output of the position of the second integer. For the addition task, we modified RPE such that the same positional vector is always incorporated to the interaction of the positions of the least significant digits for both integers, and so on for other positions in spite if their variable distance in the input sequence. Figure 7a shows the method is as effective as the scenario without the text. During training,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-10.jpg?height=499&width=1651&top_left_y=241&top_left_x=237)\n\nFigure 6: Extra attention heads utilized with pairwise positional encodings are integrated into the model to exploit structures across various tasks. The diagram illustrates how certain attention heads employ relative position encodings to enable length generalization for the addition task. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-10.jpg?height=532&width=1126&top_left_y=926&top_left_x=506)\n\nFigure 7: (a) The generalization performance on the task of addition for a model that also allows for text with variable length is as good as the original implementation in Figure 2b. (b) The trained model with text for the multiplication task also follows the same generalization trend as in Figure 5b. attention heads designated for arithmetic operations learn to ignore the text, focusing solely on arithmetic tasks. In turn, for the multiplication task, analogous of this approach is applied to equip the model with UPE, labeled with \"Mult Head ${ }_{i, j}$ \" in Figure 6, and Figure 7b depicts comparable generalization performance to Figure 5b. ## 4 Beyond Structural Symmetries: Complexity of Task\n\nSo far, we have explored the importance of leveraging the structural symmetries. However, we may want the model to consistently work for all examples, not just those that are more probable. Here, we focus on analyzing the errors of these models, relating these observations to the complexity of the examples. Specifically, for the task of addition, we will assert that examples with longer dependencies across the input sequence are considerably more difficult for the model. Finally, we will show how to counteract this performance drop by adjusting the training distribution in favor of higher complexity examples. Typically, it is not necessary to consider all preceding digits of the two integers when calculating the result of the current position. Without carry-overs, the calculation simplifies to repeating single-digit additions across the sequence in parallel. However, as the chain of carries extends, managing them grows more challenging\nin parallel, as the model needs to track more positions in the input for each output position. We introduce a notion of complexity based on how far the model should look back to compute the carry-overs, suggesting that this window may not need to be large for most instances. Definition (complexity). For a general seq-to-seq task and a sequence S on which the task operates, we define 'complexity' as the maximum dependency length, i.e., number of positions such that their values are necessary to calculate the output of the current position, across the entire input. Consequently, for the task of addition, and within the seen domain that only contains samples with limited length $l_{s}=5$, the complexity of any sequence is trivially upper bounded by 10, the total number of positions. As another straightforward example, consider a scenario where no two carries occur consecutively. In this case, the required list of positions, $\\sigma(i)$, would only include $i-1, i, i+l$, and $i+l+1$. (C1) Consecutive carries are not representative of complexity. Last example might prompt one to think that the mere consecutive occurrence of carries qualifies for the long dependency complexity, and will lead to performance drop for the model. Figure $8 b$ shows the performance of the model for consecutive carries of different lengths. As seen from the figure, the performance does not drop significantly as the length of the sequence of consecutive carries increases. This signifies the fact that consecutive carries do not accurately capture the level of difficulty for this problem. Now we will quantify the complexity rule for addition. Consider a sample $\\left(x^{(1)}, x^{(2)}\\right)$ and the sum at position $i$. If $x_{i-1}^{(1)}+x_{i-1}^{(2)} \\geq 10$, the output at position $i$ is $x_{i}^{(1)}+x_{i}^{(2)}+1$ regardless of the values of all previous positions. Similarly, when $x_{i-1}^{(1)}+x_{i-1}^{(2)} \\leq 8$, the output at position $i$ is $x_{i}^{(1)}+x_{i}^{(2)}$. Thus, the output at position $i$ depends on previous positions when the carry cannot be determined without knowing the prior carries. This is the number of consecutive indices which sum to 9 that are triggered by a carry, leading to a cascade effect. Figure 1 shows an example where carries cascade for three stages, from the first to the fourth position. (C2) Cascading carries quantify failure to higher complexity. In Appendix E, we show that RPE learns a fixed list of relative positions for computing every digit of the output under a restricted complexity constraint, which is met during training (within the seen domain that only contains samples with limited length $l_{s}=5$, the complexity of any sequence is trivially upper bounded by 10). Nevertheless, this does not address the scenario where test samples exceed the constraint. If the model has seen cascades up to a certain length at training, it may not be able to generalize to samples with larger cascade sizes. Figure 8a demonstrates that an increase in cascade length causes a significant decline in the performance. However, as will discuss in the next part and in Appendix C.4, higher complexity samples are rare; in fact, most of the cascade lengths are covered by a small number and thus lower complexity. Consequently, given that our evaluation are based on the uniform distribution over the unseen domains, higher complexity samples do not tangibly affect the performance in expectation. (C3) Models trained with more complex samples generalize better. In Figure 8c, in a bid to expose the models to more complex samples during training, we adjusted the training distributions. Beyond the uniform distribution across all samples, we trained another model using samples that are selected equally for their complexity (namely, uniform distribution on the complexity). We further trained a third model based on a hybrid approach, where each sample is chosen based on a 50/50 probability from the first two distributions. These models were then tested on 40-digit sums with varying cascade lengths. Allowing the model to observe more samples from higher complexities aids in preserving its accuracy even on cascading lengths above 5 (the maximum length observed in-distribution). Notably, when a sufficient number of samples from higher complexities are observed (approximately 16 k of samples with a cascade length of 5 in the uniform complexity scenario, and 8 k in the mixed distribution scenario), the uniform complexity loses its advantage in higher complexity areas. The model trained with the mixed distribution outperforms the other two at most lengths. We conclude that smaller cascades are in fact more beneficial for learning the positional aspect of the task. This aligns with the findings in [SBE $\\left.{ }^{+} 23\\right]$ about assigning higher weights to\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-12.jpg?height=488&width=1598&top_left_y=232&top_left_x=265)\n\nFigure 8: (a) Cascading carries are noticeably hard samples for the model trained with RPE. (b) If the cascade effect is not taken into account, focusing only on a series of carries in a row fails to obtain harder samples. (c) The performance of the model trained with 100 k samples chosen based on different mixtures of easy and hard examples. Exposure to more complex samples improves the accuracy on higher complexities. shorter examples. Moreover, samples with higher complexity are necessary to enhance the performance on higher complexities. ## 5 Theoretical Results\n\nWe theoretically validate our findings using a one-layer transformer model trained on a simple regression task that captures translational symmetries. We show that in the population regime, i.e., having access to infinite amounts of data, when the transformer is trained by gradient descent (with weight decay), encoding these symmetries into the structure of the positional vectors through RPE will provably lead to length generalization on the unseen positions. However, in the same setting, the model with APE, which is oblivious to translational symmetries, will fail to generalize on the unseen positions. We further show that APE fails even if we use data augmentation to help the model learn the translational symmetries during training.",
    "explicitencoding-13": "We defer the proofs to Appendix D. Task description. We consider a seq-to-seq regression task consisting of an input sequence $\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)$ and an output sequence $\\left(y_{1}, y_{2}, \\cdots, y_{n}\\right)$, where for $i \\in[n]$ :\n\n$$\ny_{i}=\\alpha\\left\\langle\\theta, x_{i}\\right\\rangle+\\beta\\left\\langle\\theta, x_{i-1}\\right\\rangle+\\beta\\left\\langle\\theta, x_{i+1}\\right\\rangle\n$$\n\nHere, $\\alpha$ and $\\beta$ are some constants, and we assume $\\|\\theta\\|=1$. The translational symmetries should now be clear from Equation (5.1); if we apply a (circular) shift to the input, then the output will undergo the same type of shift. We consider a length-generalization scenario where, for a given integer $n_{1} \\leq n$, the training set will include sequences $\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)$ where the values at the first $n_{1}$ positions are generated according to the Gaussian distribution, i.e. for $i \\in\\left[n_{1}\\right]: x i \\stackrel{\\text { i.i.d. }}{\\sim} \\mathcal{N}\\left(0, \\mathbb{I}_{d}\\right)$, and the rest are zero-padded, i.e. for $i>n_{1} x_{i}=\\mathbf{0} \\in \\mathbb{R}^{d}$. At test time, however, the trained model will be evaluated according to the inputs $\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)$ where the values at all the positions are generated according to the Gaussian distribution. We can view the 0 padded positions as the \"unseen\" positions. To measure the performance, we consider the MSE loss $\\ell_{i}=\\left(\\hat{y}_{i}-y_{i}\\right)^{2}$, where $\\hat{y}_{i}$ is the model's prediction at position $i$. We will train a simplified linear Transformer on this task:\n\nLinear Attention model. One layer of a transformer architecture, specifically the linear modification (no Softmax bottleneck), has been deployed as the main tool in some recent work [vONR ${ }^{+}$23, MHM23, $\\mathrm{ACS}^{+}$24]\nto analyze the dynamics of training. A Linear Transformer is defined by the following parameters: three matrices including a query matrix $W_{Q} \\in \\mathbb{R}^{d \\times d}$, a key matrix $W_{K} \\in \\mathbb{R}^{d \\times d}$, and a value matrix $W_{V} \\in \\mathbb{R}^{d \\times d}$. Given the sequence of tokens $\\left\\{x_{1}, \\cdots, x_{n}\\right\\}$ the output from the attention head is then passed through a linear projection $h \\in \\mathbb{R}^{d}$ to obtain the scalar outputs of the linear attention:\n\n$$\n\\hat{y}_{i}=h^{T} \\sum_{r=1}^{n}\\left(W_{V} x_{r}\\right)\\left(W_{K} x_{r}\\right)^{T}\\left(W_{Q} x_{i}\\right)=h^{T} \\sum_{j=r}^{n}\\left(W_{V} x_{r}\\right)\\left(x_{r}^{T} W_{k}^{T} W_{Q} x_{i}\\right)\n$$\n\nTo encode the positional information, positional vectors $\\left\\{p_{1}, \\cdots, p_{n}\\right\\}$ are added to the tokens: $x_{i} \\leftarrow x_{i}+p_{i}$. Furthermore, for our analysis, we study a simplified one-layer transformer with factored linear attention, a model commonly studied in prior work [BTR ${ }^{+}$21, RGLG23, JSL22]:\n\n$$\n\\hat{y}_{i}=h^{T} W_{V} \\sum_{r=1}^{n} x_{r}\\left(p_{r}^{T} W_{K}^{T} W_{Q} p_{i}\\right)\n$$\n\nIn this model, the positional vectors and the input are decoupled. This is to isolate the attention to only depend on the position, as to be consistent with the task. This simplification abstracts the learning of position encodings which is the main focus of our study We also will demonstrate in Appendix D. 3 that a modified version of this model, expressed as: $\\hat{y}_{i}=h^{T} W_{V} \\sum_{r=1}^{n} x_{r}\\left(p_{i-r}^{T} W_{K}^{T} W_{Q} u\\right)$, where $u$ is a constant vector, is the appropriate abstraction for RPE. For simplicity, we assume that $W_{K}=W_{Q}=W$. Besides, as we aim to study the structures of the learned positional vectors, we will absorb the attention weights into the positional vectors without altering the representational capabilities of these models. This approach is equivalent to fixing the attention weights in the model, thereby training solely on the positional encodings. Note that for the model to achieve generalization on all positions, $h^{T} W_{V}$ must get aligned with $\\theta, p_{i}^{T} W^{T} W p_{i} \\propto \\alpha, p_{i}^{T} W^{T} W p_{\\{i-1, i+1\\}} \\propto \\beta$, and zero otherwise. For RPE, this similarly requires $p_{0}^{T} W^{T} W u \\propto \\alpha, p_{\\{-1,+1\\}} W^{T} W u \\propto \\beta$. Having this said, we will prove the following proposition:\nProposition 5.1 (informal). For the seq-to-seq regression task, the transformer model after training with GD with infinitesimal weight-decay on the positional vectors in the population regime:\n\n- (APE) After training we have $p_{k} \\approx 0$ for all $k \\notin\\left[n_{1}\\right]$. Hence, APE fails at generalization. - (APE with augmentation) For $\\beta=0$, after training $\\left|p_{i}^{\\top} p_{j}\\right|$ is large for all $i \\neq j$ and $p_{i}^{\\top} p_{i}=\\alpha$. Hence, APE with augmentation fails to generalize. - (RPE) After training $\\langle v, \\theta\\rangle p_{0}^{T} W^{T} W u=\\alpha,\\langle v, \\theta\\rangle p_{-1}^{T} W^{T} W u=\\langle v, \\theta\\rangle p_{1}^{T} W^{T} W u=\\beta$, and $p_{j} \\approx 0$ for all $j \\notin\\{-1,0,1\\}$, thus the model generalizes. The key idea is that APE does not receive any useful signals on the padded positions during training, and therefore, does not learn how to predict on those positions. For APE with augmentation, while the inner products of a positional vector with its adjacent vectors match the underlying task, those further apart are unsupervised and thus may behave unpredictably. Finally, making the model relative through RPE resolves the issue of undesirable trajectory of inner products occurring for the model with APE, leading to full generalization.",
    "explicitencoding-14": "We refer the reader to Appendix D for additional details of the setup and full proofs. Experimental Validation. To remain consistent with our assumptions, we conducted an experiment that positions are on a ring and for the task in Equation (5.1). Here, $n=51, n_{1}=10$, and $d=200$. Figure 9a shows that all three settings (APE, APE with augmentation, and RPE) converge to zero in their training loss. However, when we compute the test loss at each position in Figure 9 b, that is, $\\mathbb{E}\\left\\{l_{i}\\right\\}$ according to the normal distribution at all positions and not only the allowed windows in training, both APE and APE with augmentation cases cannot generalize. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-14.jpg?height=521&width=1230&top_left_y=235&top_left_x=447)\n\nFigure 9: (a) All three scenarios perform perfect in-distribution, when only a part of the sequence is not padded. At test time, all positions get non-padded values. (b) The test loss at each position in the sequence. The model with RPE outperforms the other two. The inherent noise present in APE and APE with augmentation is a characteristic of their generalization capabilities, The plots are generated by averaging over 5000 samples at each position. ## 6 Conclusion\n\nOur work provides a general approach for capturing structural symmetries in arithmetic tasks. This allows us to provide the first length generalization result for multiplication that relies only on minor architectural modifications. Exploring how to induce such structural symmetries beyond these basic arithmetic tasks, and in tandem with text data is a natural next direction. Future work will explore extending our approach to handle more than two integers and to combine multiple arithmetic tasks at once. Another possibility is exploring the compatibility of our methods with text-formatted arithmetic tasks. Orthogonally, our understanding of the current models is that, without seeing longer dependencies, the model cannot generalize to more complex samples. Next steps would be to understand the limitations of generalization abilities of these models on the complexity axis, and suggest potential architectural, data, and algorithmic fixes to improve their abilities. ## References\n\n[ABLR23] Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum, 2023. 4, 5, 19\n[ACS ${ }^{+}$24] Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization), 2024. 12\n[AWA ${ }^{22]}$ Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models, 2022. 2, 19\n[BMR ${ }^{+}$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in neural information processing systems, 2020. 1, 2\n[BTR ${ }^{+}$21] Nick Bhattacharya, Neil Thomas, Roshan Rao, Justas Daupras, Peter K Koo, David Baker, Yun S. Song, and Sergey Ovchinnikov. Single layers of attention suffice to predict protein contacts, 2021. 13\n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805, 2018. 5, 19\n[DLS ${ }^{+}$23] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality.",
    "explicitencoding-15": "arXiv preprint arXiv:2305.18654, 2023. 1, 2, 19\n$\\left[\\mathrm{ENO}^{+}\\right.$21] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. 19\n[FP88] Jerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1):3-71, 1988. 19\n[GGB20] Mor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language models, 2020. 2\n$\\left[\\mathrm{HVU}^{+}\\right.$18] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M.",
    "explicitencoding-16": "Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer, 2018. 2, 19\n[JdDE ${ }^{+}$23a] Samy Jelassi, St\u00e9phane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran\u00e7ois Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023. 2, 4, 19\n[JdDE ${ }^{+} 23 b$ Samy Jelassi, St\u00e9phane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran\u00e7ois Charton. Length generalization in arithmetic transformers, 2023.",
    "explicitencoding-17": "2, 3, 5, 6, 8, 19\n[JSL22] Samy Jelassi, Michael E. Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure, 2022. 13\n[KHK ${ }^{+}$21] Jeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. Have you seen that number? investigating extrapolation in question answering models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. 2\n$\\left[\\mathrm{KPR}^{+}\\right.$23] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers, 2023. $1,2,19$\n$\\left[\\mathrm{LAD}^{+}\\right.$22] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems, 2022. 2\n$\\left[\\mathrm{LAG}^{+}\\right.$23a] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention glitches with flip-flop language modeling. arXiv preprint arXiv:2306.00946, 2023.",
    "explicitencoding-18": "1, 19\n$\\left[\\mathrm{LAG}^{+} 23 \\mathrm{~b}\\right]$ Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata, 2023. 19\n[LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.",
    "explicitencoding-19": "20\n[LM22] Yuxuan Li and James L. McClelland. Systematic generalization and emergent structures in transformers trained on structured tasks, 2022. 2, 19\n[LSL ${ }^{+}$23] Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers, 2023.",
    "explicitencoding-20": "1, 3, 4, 5, 19\n[MHM23] Arvind Mahankali, Tatsunori B. Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention, 2023. 12\n[MSK ${ }^{+}$23] Sadegh Mahdavi, Kevin Swersky, Thomas Kipf, Milad Hashemi, Christos Thrampoulidis, and Renjie Liao. Towards better out-of-distribution generalization of neural algorithmic reasoning tasks, 2023. 2, 19\n[NAGA ${ }^{+}$21a] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. arXiv:2112.00114, 2021. 1\n[NAGA ${ }^{+}$21b] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. 2, 19\n[NJL21] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv:2102.13019, 2021. 1\n[Ope23] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2, 19\n$\\left[\\mathrm{OWJ}^{+} 22\\right]$ Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. 2, 19\n[PSL22] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. 19\n[PTDU16] Ankur P. Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference, 2016. 19\n[QB24] Philip Quirke and Fazl Barez. Understanding addition in transformers, 2024. 5, 19\n[RaGS22] Yasaman Razeghi, Robert L. Logan IV au2, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning, 2022. 2\n$\\left[\\mathrm{RDG}^{+} 23\\right]$ Anian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers, 2023. 2, 19\n[RGLG23] Riccardo Rende, Federica Gerace, Alessandro Laio, and Sebastian Goldt. What does selfattention learn from masked language modelling?, 2023. 13\n[RSR ${ }^{+}$23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. 2, 19\n[SBE ${ }^{+23]}$ Ruoqi Shen, S\u00e9bastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic, 2023. 1, 2, 3, 4, 5, 11, 19\n[SHB16] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. 2, 19\n[SLP ${ }^{+}$23] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. 2, 19\n[SMF ${ }^{+22]}$ Paul Smolensky, R. Thomas McCoy, Roland Fernandez, Matthew Goldrick, and Jianfeng Gao. Neurocompositional computing: From the central paradox of cognition to a new generation of ai systems, 2022. 19\n[SUV18] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations, 2018.",
    "explicitencoding-21": "2, 3, 4, 19\n[TPSI21] Avijit Thawani, Jay Pujara, Pedro A. Szekely, and Filip Ilievski. Representing numbers in nlp: a survey and a vision, 2021. 2, 19\n[Ver18] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. 28\n[vONR ${ }^{+}$23] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2023. 12\n[VSP ${ }^{+}$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "explicitencoding-22": "In Advances in neural information processing systems, 2017. 1\n[WCZ21] Xin Wang, Yudong Chen, and Wenwu Zhu. A survey on curriculum learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):4555-4576, 2021. 3, 19\n[WDS ${ }^{+}$19] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface's transformers: State-of-the-art natural language processing. arXiv:1910.03771, 2019. 19\n[WSC ${ }^{+}$16] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016. 2, 19\n[WTB ${ }^{+}$22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. 1\n$\\left[W^{+}{ }^{+} 19\\right]$ Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. Do nlp models know numbers? probing numeracy in embeddings, 2019. 2, 19\n[WWS ${ }^{+}$22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv:2201.11903, 2022. 2, 19\n[WZNZ21] Cunxiang Wang, Boyuan Zheng, Yuchen Niu, and Yue Zhang. Exploring generalization ability of pretrained language models on arithmetic and logical reasoning, 2021. 2\n$\\left[\\mathrm{YDL}^{+}\\right.$23] Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. Gpt can solve mathematical problems without a calculator, 2023. 2, 3, 19\n[ZBL ${ }^{+}$23] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023. 19\n\n## A Additional Related Work\n\nLength generalization. It has long been known that AI models struggle to fully grasp the underlying concept from limited experience [FP88, SMF ${ }^{+}$22, LM22]. For the special case of transformers, there is ongoing effort to understand to what capacity these models can learn the simple \"algorithms\" [LM22, $\\mathrm{LAG}^{+}$23b, $\\mathrm{LAG}^{+}$23a]. Large models tend to learn map functions rather than master the underlying tasks, failing at generalizing to variable lengths [LSL ${ }^{+}$23]. Even fine-tuning pre-trained models and scratch-padding only provides marginal benefits [NAGA ${ }^{+} 21 \\mathrm{~b}, \\mathrm{AWA}^{+}$22]. To mitigate this, recent work has suggested modifications to architectural components or data formats [JdDE ${ }^{+} 23 \\mathrm{a}, \\mathrm{SBE}^{+}$23], along with providing frameworks to argue when generalization may be feasible [ZBL ${ }^{+}$23] or infeasible [ABLR23, $\\mathrm{MSK}^{+}$23]. In particular, [ZBL ${ }^{+}$23] show feasibility of length generalization in Transformers for tasks that use positions only via certain operations such as comparison, successor, predecessor, etc., and ignore the role of positional encodings. Solving arithmetic tasks with transformers. Arithmetic tasks are among the tasks LLMs struggle with. $\\left[\\mathrm{YDL}^{+}\\right.$23] studies the performance of GPT-4 [Ope23] at solving arithmetic problems - GPT-4 gets zero accuracy on 11-digit arithmetic operations, and the best methods have achieved $40 \\sim 50 \\%$ accuracy. In this regard, a growing body of work has focused on improving the representations of the numbers fed to the model [WSC ${ }^{+}$16, SHB16, TPSI21], showing the sub-optimality of the current sub-word methods [WWL $\\left.{ }^{+} 19\\right]$. Another line of work has introduced the concept of intermediate steps by using chain-of thoughts reasoning $\\left[W^{+} 22, \\mathrm{OWJ}^{+} 22\\right]$, or by gradually increasing the complexity of tasks [WCZ21, YDL $\\left.{ }^{+} 23\\right]$. Despite all these works, transformers continue to face challenges with high-complexity tasks [DLS ${ }^{+}$23]. Authors in [DLS ${ }^{+}$23] have investigated these problems in detail, and there are many causes for this including error-propagation, learning shortcut solutions and insufficient features, etc. It has also been observed that the distribution of the samples can greatly affect generalization [LSL $\\left.{ }^{+} 23, \\mathrm{SBE}^{+} 23\\right]$. For instance, $\\left[\\mathrm{SBE}^{+} 23\\right]$ states that favoring smaller numbers more that larger numbers leads to better representations. Moreover, $\\left[J d D E{ }^{+}\\right.$23b] claims that adding a few samples from the unseen domain can significantly improve generalization for longer lengths. Here, we maintain the uniform distribution over the data and contend that this approach requires a large amount of data to better generalize, but without entailing any longer samples. Our investigation centers on the two primary arithmetic tasks, addition and multiplication. In contrary to intermediate step approaches, we tackle this in an end-to-end manner. Another line of work has focused on the mechanistic interpretability of transformers $\\left[\\mathrm{ENO}^{+}\\right.$21] for the task of addition [QB24], trying to understand the mechanism that a single-layer transformer implements this task in parallel. In the same vein, we study which input positions are necessary for each output position when doing the task in parallel, and on this basis, we build up a new approach that interprets our models. Positional encodings. In tackling the length generalization problem, various studies have altered the positional vectors by introducing new structures to them such as restiveness in [SUV18, HVU ${ }^{+}$18]. Other work has devised new schemes to leverage the positional information such as RoPE [SLP ${ }^{+}$23], and adding\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-19.jpg?height=53&width=1647&top_left_y=1887&top_left_x=239) $\\left[K^{2}{ }^{+} 23\\right]$, or using randomized positional encoding $\\left[\\mathrm{RDG}^{+} 23\\right]$ for better generalization. Another line of work has focused on adding structures to attention maps, eliminating the need for positional vectors [PTDU16, PSL22]. ## B Experiment Details\n\nFor all the experiments we used Huggingface [WDS ${ }^{+}$19] implementation of Bert [DCLT18]. Throughout the addition section we used models with the same specifications: 6 attention layer, $H=8$ heads, embedding dimension $d_{z}=768$, and dropout $=0.1$. A model with the same specification was also employed for the single-digit multiplier setting. In the 3-digit multiplier scenario, to compensate for the hardness of the task,\nwe increase the number of attention layers from 6 to 9 . We observed that we need more coverage for this setting, so all the plots in Figure 6 were trained with five times the amount of data compared to previous settings, i.e. 100 k number of samples for the addition and the single-digit multiplier settings vs 500 k number of samples for the 3-digit multipliers setting. Each element of the input is in the one-hot format, and the model will print the score for each digit of the output. All models are trained on the Cross-Entropy loss using AdamW [LH19], and from the scratch, with batch-size $=64$, learning-rate $=1 \\mathrm{e}-4$, and weight-decays in $\\{3 e-6,1 e-5,3 e-5,1 e-4\\}$. We used one NVIDIA RTX A5000 to run our experiments. ## C Additional Experiments\n\n## C. 1 Spurious correlations learned with augmentation\n\nAs we explained the experiment for Figure 2c, despite that the model does well in single-digit sums, it cannot maintain this performance on the first digit of multi-digit sums. The accuracy on every position of the output is listed in Appendix C.1. We conclude that augmentation based on the translational symmetry leads to spurious correlations between positions, which is in agreement with our findings in theory described in Section 5 . | Test Case | Accuracy |\n| :--- | :---: |\n| Shifted integers | $99.7 \\pm 0.1$ |\n| Uniform distribution | $81 \\pm 1$ |\n| Accuracy on the 1st digit | $90 \\pm 1$ |\n| Accuracy on the 2nd digit | $96 \\pm 1$ |\n| Accuracy on the 3th digit | $98.8 \\pm 0.1$ |\n| Accuracy on the 4th digit | $98.0 \\pm 0.1$ |\n| Accuracy on the 5th digit | $98.8 \\pm 0.1$ |\n| Accuracy on 6:11 digits | $96 \\pm 1$ |\n\nTable 1: 10-digit sum's accuracy of the augmented model for in-distribution, out of distribution (uniform integers in $\\left[1,10^{10}-1\\right]$ ), and isolating the performance at every coordinate of the output.",
    "explicitencoding-23": "Despite the acceptable accuracy of the model on most digits, the first digit errors hurt the model. ## C. 2 Addition with no carries\n\nOne of the experiments we did to establish that the main issue in the addition task is the existence of carries, was to try a setting with no carry, every step's output being independent of former calculations. In this case, addition is just a digit-wise operation that can be implemented in parallel for a sequence. Therefore, with our input's format and relative positional encoding, the model at each position has to only look at itself and $l+1$ (Addition in parallel Section), and calculate the sum of those tokens.",
    "explicitencoding-24": "To this end, Figure 10 approves that this problem does not cause difficulty unless carries are included. ## C. 3 Positional maps\n\nUsing weight-decay in training causes unnecessary parts of the network to vanish over time. Thus, plotting the positional vectors is informative about what positions the model is looking at. Figures 11a and 11b are the positional plots for models with RPE and UPE trained in the single-digit multiplier scenario (accordingly for models in 4 b and 4 c ). y -axis accounts for the index of the plotted vectors: $i-j+($ max-length -1$)$ for $P_{i-j}$ so that in all the plots of 11 a and 11 b the 24 th is $P_{0}$. And x represents elements of a positional vector here. Because the attention dimension is 768 in our setting and the number of heads is 8 , positional vectors\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-21.jpg?height=624&width=741&top_left_y=257&top_left_x=692)\n\nFigure 10: A relative model easily learns addition where there is no carry.",
    "explicitencoding-25": "Here, training samples are up to 5-digit sums. are 86-dimensional. We want to highlight one similarity and one major difference from the plots. In both models and in every layer, the 23rd vector is the most noticeable, which means looking at the previous position. Considering our task, this seems very intuitive, as carry comes only from the previous position in the single-digit multiplier case. The difference is that attention is less dispersed in UPE, especially in layers 2 to 6 , which will avoid overfitting to in-distribution data. We think this is one of the reasons for a better out-of-distribution performance of the model with UPE. We plot the positional vectors for the 3-digit multiplier setting in Figures 12a and 12b. This time max-length $=27$, and therefore, $P_{0}$ is the 26th row of every plot. This time besides $P_{-1}, P_{-2}$ and $P_{-3}$ are also noticeable (this can be seen at layer 2 of $5 b$ ), which again is reasonable since carries jump to at most 3 latter positions for a 3-digit multiplier. Although less conspicuous here, the model with UPE still depicts less dispersed attention than the RPE. ## C. 4 Distribution of the Complexity\n\nDistribution of cascade carries. Figures 13a and 13b show the distribution of the cascade length for randomly sampled 50-digit and 5-digit sums. As can be seen, the cascade length probability is very small beyond length 4. In fact, the probability of cascading carries up to length 4 covers 0.998 of the total probability. Training on 5-digit addition can provide us with cascades of length up to 5, however, extensive training samples are required for the model to cover and observe high-complexity samples (the probability of length 4 cascades in random 5 digit numbers is roughly $8.3 \\mathrm{e}-4$ ). This explains why the performance drops with higher cascade length. Note that despite never seeing cascades of length higher than 5 , the model still gets non-trivial performance on samples with cascade length more than 5. Complexity of the single-digit multiplier scenario Unlike addition, there is no direct way to identify samples with higher complexities in multiplication since the carry's value is not limited to 1 . However, we can use the fact that length of the maximum dependency in a sequence is equal to the total number of levels in Figure 3a. By the last level, every position's value will be a single digit. Figures 14a and 14b show the dependency distribution for the 20-digit multiplicand and the 5-digit multiplicand accordingly, when the multiplier is uniformly sampled from 1 to 9 . Similarly to addition, up to length of 4 covers 99.8 percent of the distribution in Figure 14a, and the probability of 4 is $1.9 \\mathrm{e}-3$ in Figure 14b. Hence, a sufficient number of training samples from $\\mathcal{D}_{s}$ will ensure generalization on the unseen part. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-22.jpg?height=1663&width=1631&top_left_y=397&top_left_x=243)\n\nFigure 11: (a) Plotting the pairwise positional vectors for each layer of the model with RPE in the single-digit multiplier scenario. The main signal is in the 23 rd vector, meaning that the model mainly looks at only one position back at each layer. (b) The model with UPE in the same setting as (a) exhibits more interpretable positional maps. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-23.jpg?height=2017&width=1309&top_left_y=253&top_left_x=406)\n\nFigure 12: (a) Plotting the pairwise positional vectors for each layer of the model with RPE in the 3-digit multiplier scenario. The model has to look at more positions in addition to the previous one. (b) The model with UPE in the same setting as (a) is still more interpretable. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-24.jpg?height=569&width=1325&top_left_y=235&top_left_x=398)\n\nFigure 13: (a) Distribution of cascading carries for 50-digit sums. Most of the mass lies within 1 to 4 . (b) Distribution of cascading carries in 5-digit sums. Increasing the coverage of high-complexity samples of 5-digit sums can help offset the disparities in distributions. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-24.jpg?height=555&width=1323&top_left_y=989&top_left_x=402)\n\nFigure 14: (a) Distribution of dependency for 20-digit multiplicand. (b) Distribution of dependency for the 5-digit multiplicand scenario. ## D Proofs\n\n## D. 1 Proof of Theorem 5.1: APE\n\nAfter absorbing $W$ into the positional vectors, i.e. $p_{i} \\leftarrow W p_{i}$, and letting $v:=W_{V}^{T} h$, we obtain:\n\n$$\n\\hat{y}_{i}=\\sum_{r=1}^{n} p_{i}^{T} p_{r}\\left\\langle v, x_{r}\\right\\rangle\n$$\n\nDuring training only the first $n_{1}$ positions are variable. Thus, the total loss function at every step is:\n\n$$\n\\ell\\left(\\left\\{p_{1}, \\cdots, p_{n}\\right\\}\\right)=\\sum_{i=1}^{n_{1}}\\left(\\hat{y}_{i}-y_{i}\\right)^{2}\n$$\n\nBefore diving into the proofs, it is important to note that we write the equations on a ring, where index \"-1\" points at the last element in the sequence; i.e.",
    "explicitencoding-26": "index $n$. This makes it simpler to handle the boundary conditions. To make the convergence problem easier, we set $v=\\theta$ at initialization such that it does not need to explore any other direction. This alignment assumption only holds for our proofs of negation for absolute positional encoding (APE) and APE with augmentation, and the extra assumption makes our proof of negation even stronger. We will prove with all of these simplifications that APE will not still learn the structure for padded positions, hence failing to generalize on the unseen. Proposition (Formal statement of Theorem 5.1- First part). The solution found by the gradient descent with infinitesimal weight-decay in the population regime on the loss function (D.2) satisfies $p_{k}=0$ for all $k \\notin\\left[n_{1}\\right]$. Therefore, the expected loss at each position, i.e., $\\mathbb{E}\\left\\{\\ell_{i}\\right\\}=\\mathbb{E}\\left\\{\\left(\\hat{y}_{i}-y_{i}\\right)^{2}\\right\\}$ for $i>n_{1}$ converges to $\\alpha^{2}+2 \\beta^{2}$. Proof. $$\n\\begin{aligned}\n\\ell(P): & =\\ell\\left(\\left\\{p_{1}, \\cdots, p_{n}\\right\\}\\right) \\\\\n& =\\sum_{i=1}^{n_{1}}(\\hat{y}-y)^{2} \\\\\n& =\\sum_{i=1}^{n_{1}}\\left(\\sum_{r=1}^{n_{1}} p_{i}^{T} p_{r}\\left\\langle\\theta, x_{r}\\right\\rangle+\\sum_{r=n_{1}+1}^{n} p_{i}^{T} p_{r}\\langle\\theta, \\mathbf{0}\\rangle-\\alpha\\left\\langle\\theta, x_{i}\\right\\rangle-\\beta\\left\\langle\\theta, x_{i-1}\\right\\rangle-\\beta\\left\\langle\\theta, x_{i+1}\\right\\rangle\\right)^{2} \\\\\n& =\\sum_{i=1}^{n_{1}}\\left(\\sum_{r=1}^{n_{1}} p_{i}^{T} p_{r}\\left\\langle\\theta, x_{r}\\right\\rangle-\\alpha\\left\\langle\\theta, x_{i}\\right\\rangle-\\beta\\left\\langle\\theta, x_{i-1}\\right\\rangle-\\beta\\left\\langle\\theta, x_{i+1}\\right\\rangle\\right)^{2}\n\\end{aligned}\n$$\n\nIn the second equality we have separated the contribution of padded elements from the rest. Because $x_{r} \\sim \\mathcal{N}\\left(0, I_{d}\\right)$ and $\\|\\theta\\|=1$ their inner product is a normal random variable that we call it $s_{r}$ for $r \\leq n_{1}$. Taking the gradient of the loss function with respect to an arbitrary vector $p_{k}$, we will have:\n\n$$\n\\begin{array}{rl}\n\\nabla_{p_{k}}=\\sum_{i=1}^{n_{1}} & 2\\left(\\sum_{r=1}^{n_{1}} p_{i}^{T} p_{r} s_{r}-\\alpha s_{i}-\\beta s_{i-1} \\mathbb{1}(i \\neq 1)-\\beta s_{i+1} \\mathbb{1}\\left(i \\neq n_{1}\\right)\\right) \\\\\n& \\times\\left(\\mathbb{1}(k \\in[n 1])\\left(\\mathbb{1}(k=i)\\left(2 p_{k} s_{k}+\\sum_{r \\neq k}^{n_{1}} p_{r} s_{r}\\right)+\\mathbb{1}(k \\neq i)\\left(p_{i} s_{k}\\right)\\right)+\\mathbb{1}\\left(k \\notin\\left[n_{1}\\right]\\right)(0)\\right)\n\\end{array}\n$$\n\nIn which $\\mathbb{1}$ function gives 1 if inside condition holds, 0 otherwise. Therefore the gradient for $k \\notin\\left[n_{1}\\right]$ is zero. As a result, the updating rule under SGD with weight-decay rate equal to $\\epsilon$ for vectors outside the window is:\n\n$$\np_{k}^{(t+1)}=p_{k}^{(t)}-\\epsilon p_{k}^{(t)}\n$$\n\nAnd for $\\epsilon$ being sufficiently small, we will have:\n\n$$\np_{k} \\rightarrow_{t} \\mathbf{0} \\quad \\forall k \\notin\\left[n_{1}\\right]\n$$\n\nMeanwhile generalization over elements outside of $\\left[n_{1}\\right]$ requires $p_{k}^{T} p_{k} \\rightarrow \\alpha$ simply by matching the form of $\\hat{y}$ to $y$. Thus, APE cannot learn the structure outside of the seen window. ## D. 2 Proof of Theorem 5.1: APE + Augmentation\n\nConsider the same task (5.1) and the model (5.2). However, we consider training with data augmentation in which we apply translational shifts to the training data. Before, the training data had Gaussian-generated values in the first $n_{1}$, and the other positions are zero-padded. Here, for each $i \\in[n]$ we shift the training data by $i$ positions. The shifted sequences will have Gaussian values at positions $i+1, i+2, \\cdots, i+n_{1}$ and are zero-padded elsewhere (note that we're considering circular shifts). The shifted data will then be augmented\nto the training set. The augmented data is thus generated according to the following distribution: For each $i \\in[n]:$\n\n$$\n\\begin{cases}x_{r} \\stackrel{\\text { i.i.d. }}{\\sim} \\mathcal{N}\\left(0, \\mathbb{I}_{d}\\right) & \\text { for } \\quad i+1 \\leq r \\leq i+n_{1} \\\\ x_{r}=\\mathbf{0}_{d} & r \\notin\\left\\{i+1, \\cdots, i+n_{1}\\right\\}\\end{cases}\n$$\n\nThe overall loss function for the case where data is shifted by $i$ positions is:\n\n$$\n\\ell_{i}\\left(\\left\\{p_{1}, \\cdots, p_{n}\\right\\}\\right)=\\sum_{t=1}^{n_{1}}\\left(\\hat{y}_{i+t}-y_{i+t}\\right)^{2}\n$$\n\nBy summing over all the shifts, the total augmented loss becomes:\n\n$$\n\\ell(P):=\\ell\\left(\\left\\{p_{1}, \\cdots, p_{n}\\right\\}\\right)=\\sum_{i=1}^{n} \\sum_{t=1}^{n_{1}}\\left(\\hat{y}_{i+t}-y_{i+t}\\right)^{2}\n$$\n\nProposition (Formal statement of Theorem 5.1-Second part). The solution found by the gradient flow with infinitesimal weight-decay in the population regime on the loss function (D.4) satisfies $\\mathbb{E}\\left\\{\\ell_{i}\\right\\}=\\mathbb{E}\\left\\{\\left(\\hat{y}_{i}-y_{i}\\right)^{2}\\right\\}=$ $\\Omega(n / d)$ when $x_{i} \\stackrel{\\text { i.i.d.",
    "explicitencoding-27": "}}{\\sim} \\mathcal{N}\\left(0, \\mathbb{I}_{d}\\right) \\forall i \\in[n]$. Proof. Inserting $y$ and $\\hat{y}$ into this will give:\n\n$$\n\\begin{aligned}\n\\ell(P)=\\sum_{i=1}^{n} \\sum_{t=-w}^{w}\\left(\\sum_{r \\in W_{i}} p_{i+t}^{T} p_{r}\\left\\langle\\theta, x_{r}\\right\\rangle-\\alpha\\left\\langle\\theta, x_{i+t}\\right\\rangle\\right. & -\\beta\\left\\langle\\theta, x_{i+t-1}\\right\\rangle \\mathbb{1}(t \\neq-w) \\\\\n& \\left.-\\beta\\left\\langle\\theta, x_{i+t+1}\\right\\rangle \\mathbb{1}(t \\neq w)\\right)^{2}\n\\end{aligned}\n$$\n\nBorrowing our definition from last part $s_{i}=\\left\\langle\\theta, x_{i}\\right\\rangle$ for those positions that are not padded, we compute the expected gradients:\n\n$$\n\\begin{gathered}\n\\mathbb{E}\\left\\{\\nabla_{\\left.p_{k}\\right\\}}\\right\\}=\\mathbb{E}\\left\\{\\sum_{i=1}^{n} \\sum_{t=-w}^{w} 2\\left(\\sum_{r \\in W_{i}} p_{i+t}^{T} p_{r} s_{r}-\\alpha s_{i+t}-\\beta s_{i+t-1} \\mathbb{1}(t \\neq-w)-\\beta s_{i+t+1} \\mathbb{1}(t \\neq w)\\right)\\right.",
    "explicitencoding-28": "\\\\\n\\times\\left(\\mathbb { 1 } ( k \\in W _ { i } ) \\left(\\mathbb{1}(k=i+t)\\left(2 p_{k} s_{k}+\\sum_{r \\in W_{i}, r \\neq k} p_{r} s_{r}\\right)\\right.\\right. \\\\\n\\left.\\left.\\left.+\\mathbb{1}(k \\neq i+t)\\left(p_{i+t} s_{k}\\right)\\right)\\right)\\right\\} \\\\\n=2 \\sum_{i=1}^{n} \\sum_{t=-w}^{w}\\left\\{\\mathbb { 1 } ( k \\in W _ { i } ) \\left\\{\\mathbb { 1 } ( k = i + t ) \\left(2\\left(p_{k}^{T} p_{k}\\right) p_{k}-2 \\alpha p_{k}+\\sum_{r \\in W_{i}, r \\neq k}\\left(p_{k}^{T} p_{r}\\right) p_{r}\\right.\\right.\\right. \\\\\n\\left.-\\beta p_{k+1} \\mathbb{1}(t \\neq w)-\\beta p_{k-1} \\mathbb{1}(t \\neq-w)\\right) \\\\\n+\\mathbb{1}(k \\neq i+t)\\left(\\left(p_{i+t}^{T} p_{k}\\right) p_{i+t}-\\beta \\mathbb{1}(k=i+t-1) \\mathbb{1}(t \\neq w) p_{k+1}\\right. \\\\\n\\left.\\left.\\left.-\\beta \\mathbb{1}(k=i+t+1) \\mathbb{1}(t \\neq-w) p_{k-1}\\right)\\right\\}\\right\\} \\\\\n=2 \\sum_{i=1}^{n}\\left\\{\\mathbb { 1 } ( k \\in W _ { i } ) \\left\\{2 \\sum_{r \\in W_{i}}\\left(p_{k}^{T} p_{r}\\right) p_{r}-2 \\alpha p_{k}-\\mathbb{1}(k \\neq i+w) 2 \\beta p_{k+1}\\right.\\right. \\\\\n\\left.\\left.-\\mathbb{1}(k \\neq i-w) 2 \\beta p_{k-1}\\right\\}\\right\\}\n\\end{gathered}\n$$\n\nIn the last line, we have taken the some over $t$ inside, where for each $k \\in W_{i}$ there is only one $t$ that $k=i+t$, and at most one such that $k=i+t-1$, or $k=i+t+1$.",
    "explicitencoding-29": "To apply the sum over $i$ on the first component\nof the sum, we have to count the total number of times that $k$ and $r$ fall on the window when varying $i$. The answer is $(2 w+1-|i-r|)$ for $|i-r| \\leq 2 w+1$ and zero otherwise. For other terms, every $k$ happens $(2 w+1)$ times, therefore:\n\n$$\n\\mathbb{E}\\left\\{\\nabla_{p_{k}}\\right\\}=4 \\sum_{r=-2 w}^{2 w}(2 w+1-|r|)\\left(p_{k}^{T} p_{k+r}\\right) p_{k+r}-4(2 w+1) \\alpha p_{k}-8 w \\beta p_{k+1}-8 w \\beta p_{k-1}\n$$\n\nWe insert this gradient in the gradient flow equation with weight-decay:\n\n$$\n\\begin{aligned}\n\\frac{d}{d t} p_{k}=-\\nabla_{p_{k}}-\\epsilon p_{k}= & -4 \\sum_{r=-2 w}^{2 w}(2 w+1-|r|)\\left(p_{k}^{T} p_{k+r}\\right) p_{k+r}+(4(2 w+1) \\alpha-\\epsilon) p_{k} \\\\\n& +8 w \\beta p_{k+1}+8 w \\beta p_{k-1}\n\\end{aligned}\n$$\n\nSince $\\epsilon$ is small and it appears in a component with non-zero factor, we can neglect it. It is not where each positional vector converges to that controls the generalization, but rather their inner products that show up in (D.1). Let's define $A_{k, l}:=p_{k}^{T} p_{l}$, we compute how elements of $A$ evolve over time:\n\n$$\n\\begin{aligned}\n\\frac{d}{d t} A_{k, l}= & \\left(\\frac{d}{d t} p_{k}\\right)^{T} p_{l}+p_{k}\\left(\\frac{d}{d t} p_{l}\\right)=-\\left\\{\\left(\\nabla_{p_{k}}\\right)^{T} p_{l}+p_{k}^{T} \\nabla_{p_{l}}\\right\\} \\\\\n= & -4 \\sum_{r=-2 w}^{2 w}(2 w+1-|r|)\\left(\\left(p_{k}^{T} p_{k+r}\\right)\\left(p_{l}^{T} p_{k+r}\\right)+\\left(p_{l}^{T} p_{l+r}\\right)\\left(p_{k}^{T} p_{l+r}\\right)\\right)+8(2 w+1) \\alpha p_{k}^{T} p_{l} \\\\\n& +8 w \\beta\\left(p_{l}^{T} p_{k-1}+p_{l}^{T} p_{k+1}+p_{k}^{T} p_{l-1}+p_{k}^{T} p_{l+1}\\right) \\\\\n= & 4 \\sum_{r=-2 w}^{2 w}(2 w+1-|r|)\\left(A_{k, k+r} A_{l, k+r}+A_{l, l+r} A_{k, l+r}\\right)+8(2 w+1) \\alpha A_{k, l} \\\\\n& +8 w \\beta\\left(A_{l, k-1}+A_{l, k+1}+A_{k, l-1}+A_{k, l+1}\\right)\n\\end{aligned}\n$$\n\nThis means that elements of $A$ determine their own dynamics, and it suffices to track them only. Next step is to put assumptions on the initialization of positional vectors. Final model must acquire the same translational symmetries as the underlying task for it to generalize at test time. We seed these conditions at initialization to put it on the right track, and we will show that even in this case off diagonal elements of A do not converge to desired values. Applying the translational invariance to initial inner products of $p_{k}{ }^{\\prime} \\mathrm{s}$ :\n\n$$\np_{k}^{(0) T} p_{k+j}^{(0)}=p_{l}^{(0) T} p_{l+j}^{(0)} \\quad \\forall k, l, j\n$$\n\nLemma D.1. Condition (D.7) holds for all t if positional vectors start under condition (D.7) and evolve by (D.2). Proof. In other words the lemma says $A_{k, k+j}=A_{l, l+j}$. By induction, it only suffices to show if condition holds at one time $t$, their derivatives are equal. Using equation (D.6):\n\n$$\n\\begin{aligned}\n\\frac{d}{d t} A_{k, k+j}= & -4 \\sum_{r=-2 w}^{2 w}(2 w+1-|r|)\\left(A_{k, k+r} A_{k+j, k+r}+A_{k+j, k+j+r} A_{k, k+j+r}\\right) \\\\\n& +8(2 w+1) \\alpha A_{k, k+j}+8 w \\beta\\left(A_{k+j, k-1}+A_{k+j, k+1}+A_{k, k+j-1}+A_{k, k+j+1}\\right)\n\\end{aligned}\n$$\n\nIn every term of above expression $k$ appears on both indices of $A$, hence it can be replaced by 1 everywhere under condition (D.7). Thus:\n\n$$\n\\frac{d}{d t} A_{k, k+j}=\\frac{d}{d t} A_{l, l+j}\n$$\n\nAs a result, we can address the elements of A by the difference of its two indices:\n\n$$\nA_{j} \\leftarrow A_{l, l+j}\n$$\n\n$A_{0}$ stands for all diagonal elements of $A$, and $A_{j}$ for $j \\geq 1$ addresses off diagonal elements with respect to their distance from the main diagonal. Note that due to the circular constraint, the number of independent variables is equal to $\\left\\lfloor\\frac{n}{2}\\right\\rfloor$, and $A_{\\left\\lfloor\\frac{n}{2}\\right\\rfloor+1}=A_{\\left\\lfloor\\frac{n-1}{2}\\right\\rfloor}$. This brings us to a system of coupled differential equations for elements of A :\n\n$$\n\\begin{aligned}\n\\frac{d}{d t} A_{j}= & -4 \\sum_{r=-2 w}^{2 w}(2 w+1-|r|) A_{r}\\left(A_{r-j}+A_{j+r}\\right)+8(2 w+1) \\alpha A_{j} \\\\\n& +8 w \\beta\\left(A_{-j-1}+A_{1-j}+A_{j+1}+A_{j-1}\\right)\n\\end{aligned}\n$$\n\nUsing the fact that $A_{j}=A_{-j}$ :\n\n$$\n\\begin{array}{r}\n\\frac{d}{d t} A_{j}=8(2 w+1)\\left(\\alpha-A_{0}\\right) A_{j}-8 \\sum_{r=1}^{2 w}(2 w+1-|r|) A_{r}\\left(A_{j-r}+A_{j+r}\\right)+16 w \\beta\\left(A_{j+1}+A_{j-1}\\right) \\\\\nj=0,1, \\ldots,\\left\\lfloor\\frac{n}{2}\\right\\rfloor\n\\end{array}\n$$\n\nWe will go through this system case by case:\nCase 1. $w=0, \\beta=0$\nThis means for each sample only one element is supervised and the rest of elements are zero in that sample.",
    "explicitencoding-30": "The most basic task we can consider in this setup is one where the output of each position is solely dependent on its own input. So (D.8) simplifies to:\n\n$$\n\\frac{d}{d t} A_{j}=8\\left(\\alpha-A_{0}\\right) A_{j}\n$$\n\nWriting it for $j=0$ :\n\n$$\n\\frac{d}{d t} A_{0}=8\\left(\\alpha-A_{0}\\right) A_{0}\n$$\n\nWhich is a non-linear ODE with the following solution:\n\n$$\nA_{0}^{(t)}=\\frac{\\alpha}{1+\\left(\\frac{\\alpha}{A_{0}^{(0)}}-1\\right) \\exp (-8 \\alpha t)}\n$$\n\n$A_{0}$ converges monotonically to $\\alpha$, either from below or above depending on the values of $\\alpha$. If $\\alpha>A_{0}^{(0)}$, which holds with high probability if we choose $\\alpha>1$ and initialize $p_{k}$ with an isotropic Gaussian vector [Ver18], then $a-A_{0}^{(t)}$ is always greater than 0 , and according to equation (D.9):\n\n$$\n\\begin{cases}\\frac{d}{d t} A_{j}>0 & A_{j}>0 \\\\ \\frac{d}{d t} A_{j}<0 & A_{j}<0\\end{cases}\n$$\n\nAbsolute value of all off diagonal elements increases over iterations. So if at initialization $A_{j}=\\theta\\left(\\frac{1}{\\sqrt{d}}\\right)$, the final test loss for position $i$ is:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left\\{\\ell_{i}\\right\\} & =\\mathbb{E}\\left\\{\\left(\\sum_{j=1}^{n}\\left(p_{i}^{T} p_{j}\\right) s_{j}-\\alpha s_{j}\\right)^{2}\\right\\}=\\mathbb{E}\\left\\{\\left(\\sum_{j=1}^{n}\\left(A_{i-j}\\right) s_{j}-\\alpha s_{j}\\right)^{2}\\right\\}=\\mathbb{E}\\left\\{\\left(\\sum_{j=1, j \\neq i}^{n}\\left(A_{i-j}\\right) s_{j}\\right)^{2}\\right\\} \\\\\n& =\\sum_{j=1}^{n-1}\\left|A_{r}\\right|^{2}=\\Omega\\left(\\frac{n}{d}\\right)>0\n\\end{aligned}\n$$\n\nTo sum up, the use of APE in a model does not enable generalization, even when all samples in the training domain are augmented. ## D. 3 Proof of Theorem 5.1: RPE\n\nWe consider the same task as in (5.1), and investigate a factored attention model similar to (5.2), with the difference that positional vectors are relatively structured according to RPE. Note that the positional vectors are applied only to the key (and this is consistent with the models considered in (2.1)).",
    "explicitencoding-31": "The query does not include positional information and must be treated as a constant vector in this scenario. We denote this constant vector by $u$. The model thus becomes ${ }^{2}$ :\n\n$$\n\\hat{y}_{i}=h^{T} W_{V} \\sum_{r=1}^{n} x_{r}\\left(p_{i-r}^{T} W_{K}^{T} W_{Q} u\\right)\n$$\n\nThe formula presented here involves indices of $p$ from $-(n-1)$ to $(n-1)$. Only a subset of these indices are used at each position. For example, for position 1 , only $p_{0}$ to $p_{-n_{1}+1}$ are considered. Absorbing the key and query matrices into p and u , and letting $v=W_{V}^{T} h$, we obtain the following equation:\n\n$$\n\\hat{y}_{i}=\\sum_{r=1}^{n}\\left(p_{i-r}^{T} u\\right)\\left\\langle v, x_{r}\\right\\rangle\n$$\n\nConsequently, we introduce a set of scalars that represent the model's parameters effectively: $a_{i-r}=p_{i-r}^{T} u$. The total loss function is:\n\n$$\n\\begin{aligned}\n\\ell(a, v)= & \\sum_{i=1}^{n_{1}}\\left(\\hat{y}_{i}-y_{i}\\right)^{2}=\\sum_{i=1}^{n_{1}}\\left(\\sum_{r=1}^{n_{1}} a_{i-r}\\left\\langle v, x_{r}\\right\\rangle\\right. \\\\\n& \\left.-\\alpha\\left\\langle\\theta, x_{i}\\right\\rangle-\\beta\\left\\langle\\theta, x_{i-1}\\right\\rangle-\\beta\\left\\langle\\theta, x_{i+1}\\right\\rangle\\right)^{2}\n\\end{aligned}\n$$\n\nAt the first step, we prove that $v$ gets aligned with $\\theta$ in spite of convergence for positional vectors:\nLemma D.2. For any initialization of $v$ and any value for $p$ in equation D.12, running $S G D$ with small enough step-size and infinitesimal $\\epsilon$ on optimization problem (D.13) will align $v$ to $\\theta$. Proof. Taking the gradient of loss function in (D.13) with respect to $v$ :\n\n$$\n\\begin{array}{rl}\n\\nabla_{v}=\\sum_{i=1}^{n_{1}} & 2\\left(\\sum_{r=1}^{n_{1}} a_{i-r} v^{T} x_{r}-\\alpha \\theta^{T} x_{i}-\\beta \\theta^{T} x_{i-1} \\mathbb{1}(i \\neq 1)-\\beta \\theta^{T} x_{i+1} \\mathbb{1}\\left(i \\neq n_{1}\\right)\\right) \\\\\n& \\times\\left(\\sum_{r=1}^{n_{1}} a_{i-r} x_{r}\\right)\n\\end{array}\n$$\n\nAnd in expectation:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left\\{\\nabla_{v}\\right\\} & =\\sum_{i=1}^{n_{1}} 2\\left(\\sum_{r=1}^{n_{1}} a_{i-r}^{2} v-\\alpha a_{0} \\theta-\\beta a_{1} \\theta \\mathbb{1}(i \\neq 1)-\\beta a_{-1} \\theta \\mathbb{1}\\left(i \\neq n_{1}\\right)\\right) \\\\\n& =\\left(2 \\sum_{i=1}^{n_{1}} \\sum_{r=1}^{n_{1}} a_{i-r}^{2}\\right) v-\\left(2 \\alpha n_{1} a_{0}+2 \\beta\\left(n_{1}-1\\right)\\left(a_{1}+a_{-1}\\right)\\right) \\theta\n\\end{aligned}\n$$\n\nDecomposing $v$ to one aligned vector with $\\theta$, and another orthogonal element:\n\n$$\nv=v_{\\|}+v_{\\perp}\n$$\n\n[^1]If the step-size is $\\eta$ and weight-decay equal to $\\epsilon$, the the updating rule for $v_{\\perp}$ will be:\n\n$$\nv_{\\perp}^{(t+1)}=v_{\\perp}^{(t)}-\\eta\\left(2 \\sum_{i=1}^{n_{1}} \\sum_{r=1}^{n_{1}} a_{i-r}^{2}\\right) v_{\\perp}^{(t)}-\\epsilon v_{\\perp}^{(t)}\n$$\n\n$\\epsilon$ is sufficiently small, if we take $\\eta$ small enough such that $\\eta\\left(2 \\sum_{i=1}^{n_{1}} \\sum_{r=1}^{n_{1}} a_{i-r}^{2}\\right)$ is smaller than 1 at all times, the orthogonal term vanishes over time. After demonstrating that v becomes aligned, we will move its norm into $a_{i}{ }^{\\prime} \\mathrm{s}$, resulting in $v=1$ and $v=\\theta$. It is time to concentrate on positional encodings:\nProposition (Formal statement of Theorem 5.1- Third part). The solution found by the gradient descent with infinitesimal weight-decay in the population regime on the loss function (D.13) will align $v$ with $\\theta$ and satisfies $\\langle v, \\theta\\rangle a_{0}=\\alpha,\\langle v, \\theta\\rangle a_{1}=\\langle v, \\theta\\rangle a_{-1}=\\beta$, and $a_{j}=0$ for all $j \\notin\\{-1,0,1\\}$, and thus $\\mathbb{E}\\left\\{\\ell_{i}\\right\\}=\\mathbb{E}\\left\\{\\left(\\hat{y}_{i}-y_{i}\\right)^{2}\\right\\}=0$ when $x i \\stackrel{\\text { i.i.d.",
    "explicitencoding-32": "}}{\\sim} \\mathcal{N}\\left(0, \\mathbb{I}_{d}\\right) \\forall i \\in[n]$. Proof. Let's compute the gradient of (D.13) with respect to a's. Letting $s_{r}=\\theta^{T} x_{r}$, which is a normal random variable:\n\n$$\n\\begin{array}{rl}\n\\nabla_{a_{k}}=\\sum_{i=1}^{n_{1}} & 2\\left(\\sum_{r=1}^{n_{1}} a_{i-r} s_{r}-\\alpha s_{i}-\\beta s_{i-1} \\mathbb{1}(i \\neq 1)-\\beta s_{i+1} \\mathbb{1}\\left(i \\neq n_{1}\\right)\\right) \\\\\n& \\times\\left(\\mathbb{1}\\left(k \\in\\left[i-1, i-n_{1}\\right] s_{i-k}\\right)\\right. \\end{array}\n$$\n\nSo that in expectation:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left\\{\\nabla_{a_{k}}\\right\\}= \\sum_{i=1}^{n_{1}} 2 \\mathbb{1}\\left(k \\in\\left[i-1, i-n_{1}\\right]\\right)\\left(a_{k}-\\alpha \\mathbb{1}(k=0)-\\beta \\mathbb{1}(i \\neq 1) \\mathbb{1}(k=1)\\right. \\\\\n&\\left.\\quad-\\beta \\mathbb{1}\\left(i \\neq n_{1}\\right) \\mathbb{1}(k=-1)\\right) \\\\\n&=2\\left(n_{1}-|k|\\right) a_{k}-2 n_{1} \\alpha \\mathbb{1}(k=0)-2\\left(n_{1}-1\\right) \\beta \\mathbb{1}(k=1)-2\\left(n_{1}-1\\right) \\beta \\mathbb{1}(k=-1)\n\\end{aligned}\n$$\n\nWhere in the last line to calculate the sum, we use the fact that $a_{k}$ is observed $n_{1}-|k|$ times when traversing from 1 to $n_{1}$. For values of $k$ outside the range $\\left[-n_{1}, n_{1}\\right]$, the gradient is zero. Hence with weight-decay they will diminish over time. For values of $k$ between $-n_{1}$ and $n_{1}$, excluding $-1,0$, or 1 , the update equation is given by:\n\n$$\na_{k}^{(t+1)}=a_{k}^{(t)}-2 \\eta\\left(n_{1}-|k|\\right) a_{k}^{(t)}-\\epsilon a_{k}^{(t)}\n$$\n\nAn again, for small values for the step-size and weight-decay they converge to zero:\n\n$$\na_{k} \\rightarrow_{t} \\mathbf{0} \\quad \\forall k \\notin[-1,1]\n$$\n\nThe other three variable go on a different route:\n\n$$\n\\left\\{\\begin{array}{l}\na_{0}^{(t+1)}=a_{0}^{(t)}-2 \\eta n_{1}\\left(a_{0}^{(t)}-\\alpha\\right)-\\epsilon a_{0}^{(t)} \\\\\na_{1}^{(t+1)}=a_{1}^{(t)}-2 \\eta\\left(n_{1}-1\\right)\\left(a_{1}^{(t)}-\\beta\\right)-\\epsilon a_{1}^{(t)} \\\\\na_{-1}^{(t+1)}=a_{-1}^{(t)}-2 \\eta\\left(n_{1}-1\\right)\\left(a_{-1}^{(t)}-\\beta\\right)-\\epsilon a_{-1}^{(t)}\n\\end{array}\\right. $$\n\nFor infinitesimal values of weigh-decay and small step-size, they accordingly converge to $\\alpha, \\beta$ and $\\beta$. So at test time:\n\n$$\n\\mathbb{E}\\left\\{\\ell_{i}\\right\\}=\\mathbb{E}\\left\\{\\left(\\sum_{j=1}^{n}\\left(a_{i-j}\\right) s_{j}-\\alpha s_{j}-\\beta s_{i-1}-\\alpha \\beta s_{i+1}\\right)^{2}\\right\\}=0\n$$\n\nUsing RPE the model will be able to achieve zero test loss. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-31.jpg?height=579&width=1406&top_left_y=247&top_left_x=357)\n\nFigure 15: (a) Similarly to Figure 9b, all three scenarios reach zero in the training loss. At test time, all position get non-padded values. (b) The generalization loss at each position in the sequence. Again, the model with RPE greatly outperforms the other two. ## D. 4 Experiments for the linear attention model\n\nExperiments for a more general setting. We conducted another experiment for a symmetric task more generalized than Equation (5.1) (here the dependency length is $m$ instead of 1), establishing that out claims hold beyond the scope of our assumptions:\n\n$$\ny_{i}=\\alpha\\left\\langle\\theta, x_{i}\\right\\rangle+\\beta_{1}\\left\\langle\\theta, x_{i-1}\\right\\rangle+\\beta_{1}\\left\\langle\\theta, x_{i+1}\\right\\rangle+\\cdots+\\beta_{m}\\left\\langle\\theta, x_{i-m}\\right\\rangle+\\beta_{m}\\left\\langle\\theta, x_{i+m}\\right\\rangle\n$$\n\nFor the result of Figure 15a, we set $n=51, n_{1}=20, m=5$, and $d=200$.",
    "explicitencoding-33": "As previously seen in Figure 9 b, although APE and APE with augmentation achieves zero training loss, they fails to generalize when all positions are non-padded. The causes of their failure are explained in Appendix D. 1 and Appendix D.2. We have plotted $\\left[p_{i}^{T} W^{T} W p_{j}\\right]_{i \\leq n, j \\leq n}$ for APE and APE with augmentation in Figures 16a and 16b accordingly. For $i, j>n_{1}$, APE does not learn the structure outside the focus window as described in Appendix D.1. But augmentation fails for a different reason, and Figure 16b shows that near diagonal elements of $\\left[p_{i}^{T} W^{T} W p_{j}\\right]$ are learned properly while the rest move in random directions and ruin the generalization. Figure 15b illustrates that we can address this issue with the help of RPE, here unwanted elements that keep us from generalizing in APE have been eliminated automatically. ## E Fixed map for RPE\n\nIf the maximum complexity across the training dataset is $2 * d$ (Note that $d-1$ stands for the maximum number of levels illustrated in Figure 1 among all training samples), the trained model will ignore those previous positions of both integers that do not contribute the current calculation. i.e., with our notation defined in Section 3.1 for the tracked list of positions:\n\n$$\n\\sigma_{\\text {training }}(i)=(i-d, \\cdots, i-1, i, i+l-d+1, \\cdots, i+l, i+l+1)\n$$\n\nThus, for a model utilized with RPE, the previous list will be memorized in terms of relative positions:\n\n$$\n\\sigma_{\\text {training }}^{\\mathrm{rel}}(i)=(-d, \\cdots,-1,0, l-d+1, \\cdots, l, l+1)\n$$\n\nAs a result, $\\sigma_{\\text {training }}^{\\text {rel }}(i)$ is the same list for every $i$. Hence, if the model equipped with RPE learns to place its attention on correct relative positions for in-distribution samples, it will naturally be able to generalize to longer integers as long as the complexity of the samples is limited. ![](https://cdn.mathpix.com/cropped/2024_09_12_d1db9698dad5cdce21a8g-32.jpg?height=733&width=1198&top_left_y=788&top_left_x=460)\n(c)\n\nFigure 16: (a) The set of effective parameters are $\\left[p_{i}^{T} W^{T} W p_{j}\\right]_{i \\leq n, j \\leq n}$. The model with APE does not learn the structure for $i, j>n_{1}$. (b) Even though augmentation helps to learn the main structure of the task every where, the two off-diagonal sides remain nonzero and damage the generalization. (c) Shows $p_{i-j}^{T} W_{K}^{T} W_{Q} u$ where the index on this graph represents $i-j+(n-1) / 2$. Note that $\\frac{1}{2}$ factor appears due to the ring condition, e.g. $i=1, j=n$ will have $p_{-1}$ associated to it. [^0]:    ${ }^{1}$ We are leveraging the neutral role of zero in a sum. Note that this augmentation does not introduce any new nonzero digits, avoiding the need for additional computations. It also does not append training samples that could potentially cover the entire unseen domain, which might artificially enhance performance by unfairly exploiting dependency constraints in the addition task that are oblivious to relative positional encoding. [^1]:    ${ }^{2}$ Note that if we had used $p_{i-r}$ instead of $u$, it would have made no difference in our as the effective parameter is the same.",
    "explicitencoding-34": "This becomes clear later on.",
    "explicitencoding-35": ""
}