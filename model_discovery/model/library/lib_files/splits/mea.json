{
    "mea-0": "# SElf-ATTENTION DoEs Not NEEd $O\\left(n^{2}\\right)$ MEMory \n\n## A PrEPRINT\n\nMarkus N. Rabe and Charles Staats<br>Google Research<br>$\\{m r a b e, c s t a a t s\\} @ g o o g l e . c o m$\n\n\n#### Abstract\n\nWe present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O\\left(n^{2}\\right)$ memory. While the time complexity is still $O\\left(n^{2}\\right)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59 X for inference and by 32 X for differentiation. ## 1 Introduction\n\nAttention (Bahdanau et al., 2015) is widely used in modern neural architectures. In particular, it is the heart of the Transformer architecture (Vaswani et al., 2017), which has revolutionized Natural Language Processing (Devlin et al., 2019), and found wide-spread adoption across several research areas since then. Given a query $q \\in \\mathbb{R}^{d}$ and lists of keys and values $k_{1}, \\ldots, k_{n}$ and $v_{1}, \\ldots, v_{n} \\in \\mathbb{R}^{d}$ of length $n$, attention is defined as follows:\n\n$$\ns_{i}=\\operatorname{dot}\\left(q, k_{i}\\right), \\quad s_{i}^{\\prime}=\\frac{e^{s_{i}}}{\\sum_{j} e^{s_{j}}}, \\quad \\text { attention }(q, k, v)=\\sum_{i} v_{i} s_{i}^{\\prime}\n$$\n\nThe result of the attention operation for a single query, is hence a weighted sum of the value vectors, where the weights are the softmax of the dot products of the query and the keys. The straight-forward implementation of the attention operation above requires us to first compute and remember $s_{i}$ for all $i$, leading to a $O(n)$ time and memory complexity for each query. Transformers use self-attention, which issues a separate query for each position in the sequence, so the overall time and space complexity is $O\\left(n^{2}\\right)$. In many works the quadratic time and space complexity of self-attention has been used as the motivation for the investigation of variants of the original attention mechanism and architectures with more favorable complexity classes (Kitaev et al., 2020; Roy et al., 2021; Zaheer et al., 2020; Choromanski et al., 2020; Wang et al., 2020; Ren et al., 2021; Child et al., 2019; Tay et al., 2021; Wang et al., 2020; Ma et al., 2021; Shen et al., 2021; Qiu et al., 2020). Modern accelerator hardware, such as GPUs and TPUs, are often memory constrained for applications in deep learning, while compute is relatively cheap. So the space complexity of transformers is a particular concern, c.f. Kitaev et al. (2020); Roy et al. (2021); Zaheer et al. (2020). In this work, we present new algorithms for attention and self-attention that require only constant memory and logarithmic memory, respectively. The basic algorithm is very simple; but it requires a trick to make it numerically feasible (see Section 3). We also present an implementation in JAX (Bradbury et al., 2018), which runs efficiently on TPUs, and requires $O(\\sqrt{n})$ memory for self-attention (see Section 4 ). Unlike other works that aim to reduce the memory complexity of attention, the memory-efficient algorithm for attention that we suggest is not an approximation, but computes the same function. We can hence use the memory-efficient\nalgorithm as a drop-in replacement for other attention implementations to save memory. This may allow us to reconsider architecture choices, or scale to new datasets that require longer, dense attention. However, our algorithm still requires $O\\left(n^{2}\\right)$ time complexity for self-attention and $O(n)$ time complexity for single-query attention, and the various efficient, long-context attention mechanisms remain an interesting alternative to (dense) attention. ## 2 Algorithm\n\nFirst, we present the algorithm for the attention operation with a single query and extend the algorithm to self-attention at the end of this Section. We observe that the division by $\\sum_{j} e^{s_{j}}$ can be moved to the very end of the attention operation using the distributive law:\n\n$$\ns_{i}=\\operatorname{dot}\\left(q, k_{i}\\right), \\quad s_{i}^{\\prime}=e^{s_{i}}, \\quad \\text { attention }(q, k, v)=\\frac{\\sum_{i} v_{i} s_{i}^{\\prime}}{\\sum_{j} s_{j}^{\\prime}}\n$$\n\nAfter publishing our initial draft, we were made aware that (1) is a rediscovery of the \"lazy softmax\" method of Jang et al.",
    "mea-1": "(2019, equation 4). Unfortunately their paper went in a different direction and did not discuss the memory complexity implications and other innovations we present in the remainder of this paper. For more details see Section 6\nThis can be computed with constant memory: The memory overhead of this algorithm consists of a vector $v^{*} \\in \\mathbb{R}^{d}$ and a scalar $s^{*} \\in \\mathbb{R}$, both initialized with 0 . Given the query $q$, keys $k_{1}, \\ldots, k_{n}$ and values $v_{1}, \\ldots, v_{n}$, we process the keys and values in sequence. Given a key value pair $k_{i}$, $v_{i}$, we compute $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$ and update $v^{*} \\leftarrow v^{*}+v_{i} e^{s_{i}}$ and $s^{*} \\leftarrow s^{*}+e^{s_{i}}$. After processing all keys and values, we divide $\\frac{v^{*}}{s^{*}}$ to get the final result. The analysis of space complexity assumes that inputs are given in a particular order: we first read the query, and then a list of pairs of keys and values. If the inputs are provided in a different order, we have to additionally store an index into the sequence, requiring $O(\\log n)$ memory instead. To extend this algorithm to self-attention, we compute the results to all queries sequentially. This requires just one additional index into the list of queries, giving rise to the $O(\\log n)$ memory complexity. Note that the operation produces outputs that are linear in the size of the number of queries, i.e., $O(n)$, which is not counted towards the space complexity. ## 3 Numerical Stability\n\nThe formulation of standard attention that we presented in the Introduction, as well as our memory-efficient algorithm, are not numerically stable when using floating point arithmetic, because the softmax exponentiates the scores. For scores $\\geq 89$ the exponentiation results in inf (for bfloat16 and float32), which will be carried through to the final result of the attention operation. In practice, the softmax is implemented by subtracting the maximum score from all scores. This does not change the result of the softmax, but avoids this numerical problem. Our incremental computation of the sum of exponentiated scores (and the values times the scores) does not immediately allow for the same trick, as the maximum may depend on the last score in the sequence. But the subtraction cannot be delayed either, since the scores must be exponentiated before they can be added to the cumulative sum. To resolve this problem, we introduce an additional scalar, which keeps track of the maximum score that the incremental algorithm has seen so far, and we renormalize the sums of exponentiated values as needed: We initialize the vector $v^{*} \\in \\mathbb{R}^{d}$ and scalar $s^{*} \\in \\mathbb{R}$ with 0 , and $m^{*}$ with -inf. As before, given a key value pair $k_{i}, v_{i}$, we compute $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$, but then the algorithm differs slightly from Section 2. We first compute $m_{i}=\\max \\left(m^{*}, s_{i}\\right)$ and update $v^{*} \\leftarrow v^{*} e^{m^{*}-m_{i}}+v_{i} e^{s_{i}-m_{i}}$ and $s^{*} \\leftarrow s^{*} e^{m^{*}-m_{i}}+e^{s_{i}-m_{i}}$ and $m^{*} \\leftarrow m_{i}$. After processing all keys and queries, we divide $\\frac{v^{*}}{s^{*}}$ to get the final result. ## 4 An Implementation For TPUs\n\nIn this section, we provide a version of the algorithm above that exploits the massive parallelism of modern hardware, such as GPUs or TPUs. The naive algorithm above is is not trivial to parallelize for a compiler, as the incremental sum introduces a dependency across all keys and values. We present the entire implementation, including the support for multiple attention heads and memory-efficient differentiation in Figure 1 The implementation does not optimize strictly for memory efficiency, but instead aims to strike a balance between simplicity, computational efficiency, and memory requirements. ```\nimport functools, jax, math\nfrom jax import numpy as jnp\ndef _query_chunk_attention(query, key, value, precision, key_chunk_size=4096):\n    \"\"\"Multi-head dot product attention with a limited number of queries.\"\"\"\n    num_kv, num_heads, k_features = key.shape\n    v_features = value.shape[-1]\n    key_chunk_size = min(key_chunk_size, num_kv)\n    query = query / jnp.sqrt(k_features)\n    @functools.partial(jax.checkpoint, prevent_cse=False)\n    def summarize_chunk(query, key, value):\n        attn_weights = jnp.einsum('qhd,khd->qhk', query, key, precision=precision)\n        max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n        max_score = jax.lax.stop_gradient(max_score)\n        exp_weights = jnp.exp(attn_weights - max_score)\n        exp_values = jnp.einsum('vhf,qhv->qhf', value, exp_weights, precision=precision)\n        return (exp_values, exp_weights.sum(axis=-1),\n            max_score.reshape((query.shape[0], num_heads)))\n    def chunk_scanner(chunk_idx):\n        key_chunk = jax.lax.dynamic_slice(\n            key, (chunk_idx, 0, 0),\n            slice_sizes=(key_chunk_size, num_heads, k_features))\n        value_chunk = jax.lax.dynamic_slice(\n            value, (chunk_idx, 0, 0),\n            slice_sizes=(key_chunk_size, num_heads, v_features))\n        return summarize_chunk(query, key_chunk, value_chunk)\n    chunk_values, chunk_weights, chunk_max = jax.lax.map(\n        chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size))\n    global_max = jnp.max(chunk_max, axis=0, keepdims=True)\n    max_diffs = jnp.exp(chunk_max - global_max)\n    chunk_values *= jnp.expand_dims(max_diffs, axis=-1)\n    chunk_weights *= max_diffs\n    all_values = chunk_values.sum(axis=0)\n    all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)\n    return all_values / all_weights\ndef attention(query, key, value, precision=jax.lax.Precision.HIGHEST,\n                query_chunk_size=1024):\n    \"\"\"Memory-efficient multi-head dot product attention.\"\"\"\n    num_q, num_heads, q_features = query.shape\n    def chunk_scanner(chunk_idx, _):\n        query_chunk = lax.dynamic_slice(\n            query, (chunk_idx, 0, 0),\n            slice_sizes=(min(query_chunk_size, num_q), num_heads, q_features))\n        return (chunk_idx + query_chunk_size,\n                _query_chunk_attention(query_chunk, key, value, precision=precision))\n    _, res = jax.lax.scan(\n        chunk_scanner, init=0, xs=None, length=math.ceil(num_q / query_chunk_size))\n    return res.reshape(num_q, num_heads, value.shape[-1])\n```\n\nFigure 1: Implementation of memory-efficient attention suited for TPUs.",
    "mea-2": "| Sequence length | $n=2^{8}$ | $2^{10}$ | $2^{12}$ | $2^{14}$ | $2^{16}$ | $2^{18}$ | $2^{20}$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Size of inputs and outputs | 160 KB | 640 KB | 2.5 MB | 10 MB | 40 MB | 160 MB | 640 MB |\n| Memory overhead of standard attention | 270 KB | 4.0 MB | 64 MB | 1 GB | OOM | OOM | OOM |\n| Memory overhead of memory-eff. attn. | 270 KB | 4.0 MB | 16 MB | 17 MB | 21 MB | 64 MB | 256 MB |\n| Compute time on TPUv3 | 0.06 ms | 0.11 ms | 0.7 ms | 11.3 ms | 177 ms | 2.82 s | 45.2 s |\n| Relative compute speed | $\\pm 5 \\%$ | $\\pm 5 \\%$ | $-8 \\pm 2 \\%$ | $-13 \\pm 2 \\%$ | - | - | - |\n\nTable 2: Memory and time requirements of self-attention during inference. To exploit the parallelism available in modern hardware, we split the computation into chunks at the cost of some additional memory. In the outer loop (lines 54-55), we split the queries in to chunks of constant size, resulting in a linear number of iterations. In each iteration of the outer loop, we call _query_chunk_attention, which itself processes the keys and values in chunks (lines 30-31). The chunks are processed sequentially and each chunk is summarized independently (lines 12 to 19). Assuming a chunk size of $\\sqrt{n}$ for the keys and values, we hence obtain $\\sqrt{n}$ summaries, giving rise to the $O(\\sqrt{n})$ memory complexity. After the summaries are computed, they need to be rescaled (lines 33 to 36) along the lines of Section 3, before we return the values divided by the weights (line 40). The result of each iteration of the outer loop is directly written to the output tensor res (line 54), so that no additional memory is consumed across iterations. (A multi-stage summarization approach could achieve $O(\\log n)$ but would complicate the implementation.)\nWhile a constant chunk size for the queries and a chunk size of $\\sqrt{n}$ for the keys and values is optimal for memory consumption, the runtime is also affected by the choice of chunk size in practice, which is heavily affected by the choice of hardware. Ultimately, we have to leave this trade-off to the programmer, and expose the chunk sizes as arguments query_chunk_size and key_chunk_size. In Figure 1 we provide default values for the chunk sizes that lead to minimal runtime impact on TPU, while still providing significant memory savings. ## 5 Empirical Analysis\n\nIn this section, we experimentally compare the memory requirements and runtime performance of the suggested algorithm compared to the implementation of attention currently provided by Flax (Heek et al. (2020), see flax/linen/attention.py). We open-sourced the code of our implementation and most of the evaluation as a colab to help others reproduce the results: https://github.com/google-research/google-research/tree/master/memory_efficient_attention. ### 5.1 Inference\n\nIn Table 2 we compare the memory requirements and the compute time of the memory-efficient attention implementation and the Flax implementation of attention.",
    "mea-3": "The size of inputs and outputs includes the query, key, and value tensors of dtype bfloat16, and the output tensor of dtype float32. We measure the memory overhead as the TPUs peak memory in excess of the input and output tensors. All computations were done on a single TPUv3 chip. For this experiment, we only use one attention head. Our memory-efficient implementation of attention removes the memory bottleneck of self-attention, scaling at least to a sequence length of 1 M .",
    "mea-4": "At this sequence length the algorithm is multiplying over 1 trillion combinations of queries and keys. The time complexity is still quadratic. The \"relative compute speed\" of the implementations was computed as the median over 100 runs-but the numbers still fluctuated across multiple runs of the evaluation and we only provide them to demonstrate that the runtime performance is roughly similar. Please note that this experiment analyzes the attention operation in isolation; the measured relative performance is not necessarily the same when the operations are embedded in larger architectures. In fact, we observed a slight increase in steps/sec of about $4 \\%$ when training a small Transformer. For all cases where the standard attention would not OOM (i.e. require $>16 \\mathrm{~GB}$ device memory), we checked that the results of the two implementations are within $1.8 \\times 10^{-7}$ for inputs drawn from a normal distribution with standard deviation 1 (measured as the maximal absolute difference of any dimension in a self-attention over sequence length $2^{14}$ ). | Sequence length | $n=2^{8}$ | $2^{10}$ | $2^{12}$ | $2^{14}$ | $2^{16}$ | $2^{18}$ | $2^{20}$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Size of inputs and outputs | 192 KB | 768 KB | 2.9 MB | 12 MB | 47 MB | 188 MB | 750 MB |\n| Memory overhead of standard attention | 532 KB | 8.0 MB | 128 MB | 2.0 GB | OOM | OOM | OOM |\n| Memory overhead of memory-eff. attn. | 532 KB | 8.0 MB | 41 MB | 64 MB | 257 MB | 1.0 GB | 4.0 GB |\n| Compute time on TPUv3 | 0.1 ms | 0.18 ms | 1.4 ms | 21 ms | 336 ms | 5.3 s | 85 s |\n| Relative compute speed | $\\pm 5 \\%$ | $\\pm 5 \\%$ | $-30 \\pm 5 \\%$ | $-35 \\pm 5 \\%$ | - | - | - |\n\nTable 3: Memory and time requirements of self-attention during differentiation. Note that the slowdown in compute speed is expected due to the use of checkpointing in memory-efficient attention. ![](https://cdn.mathpix.com/cropped/2024_09_12_a71cc2c7e3447e86c65ag-5.jpg?height=660&width=709&top_left_y=662&top_left_x=708)\n\nFigure 4: BLEU scores of a two Transformer models trained with standard attention and memory-efficient attention. ### 5.2 Differentiation\n\nDuring the forward pass our algorithm saves memory by summarizing parts of the attention matrix sequentially, allowing it to forget the parts of the attention matrix it has summarized already. A naive application of differentiation would have to store all those intermediate results and our algorithm would loose its memory advantage entirely. So we apply checkpointing (Chen et al., 2016) in line 11 to the function that summarizes the individual chunks. The intermediate results can thus be forgotten during the forward pass and recomputed during backpropagation. In Table 3 we compare runtime and peak memory during differentiation of our implementation to standard attention. We used the same setting as for the forward pass, but applied jax.grad to an arbitrarily chosen loss function (the sum of the results). The relative compute speed was reduced significantly compared to standard attention. This is expected when using checkpointing since some values must be recomputed during backpropagation. Note that applying checkpointing to the standard attention algorithm would not achieve these results. The standard algorithm with checkpointing would forget the attention matrix after it is formed; our algorithm never forms the full attention matrix at all. ### 5.3 Training\n\nWe integrated our memory-efficient implementation into a simple Transformer architecture provided in the Flax library, and ran the WMT en-de translation experiment with the standard attention module and with the memory-efficient attention module. Throughout the training, the two implementations behaved almost identically. After 100K training steps, the evaluation accuracy reached 62.69 for the memory-efficient implementation and 62.59 for the standard implementation. This demonstrates that our memory-efficient implementation of self-attention can be used to replace existing implementations. Figure 4 illustrates that both models resulted in very similar BLEU scores. We used the default settings for the WMT en-de experiment as given in the Flax implementation, except that we had to deactivate example packing to simplify the masking code. This also required us to lower the learning rate to 0.005 . ![](https://cdn.mathpix.com/cropped/2024_09_12_a71cc2c7e3447e86c65ag-6.jpg?height=669&width=1533&top_left_y=245&top_left_x=295)\n\nFigure 5: Left: Relative runtime of self-attention on sequence length $2^{15}$ using query chunking compared to standard attention. Right: Relative runtime of self-attention using query chunking compared to our memory-efficient algorithm, where both are restricted to the same amount of memory. ### 5.4 Comparison to Query Chunking\n\nThe algorithms introduced in this work chunk both the keys and the queries. Chunking the only queries has been explored already by Kitaev et al. (2020), but it is folklore that it slows down the computation significantly. In Figure 5 (left), we plot the runtime of self-attention using query-chunking for different query chunk sizes compared to dense self-attention: we see that for small chunk sizes (e.g. $\\leq 64$ ) the performance suffers indeed, but for large chunk sizes, the loss of performance is less significant. So, while lower memory consumption can be achieved by query chunking alone, small values for query chunking are impractical. In comparison to query chunking, memory-efficient attention can save additional memory by chunking also the keys. This can help to keep the query chunk size at a desirable point given a fixed memory limit. In Figure 5, we constrained query chunking to the amount of memory that is used by memory-efficient attention with the default settings for key and query chunk size (see Table 2, \"Memory overhead of memory-efficient att.\", we rounded the query chunk size towards the benefit of query chunking). We see that as the sequence length increases, query chunking eventually slows down significantly as the query chunk size has to be lowered to $\\leq 64$, while memory-efficient attention does not suffer a major slowdown (see Table 2 \"Relative compute speed\"). So, in memory-constrained scenarios, memory-efficient attention can outperform query chunking. ## 6 Related Work\n\nAfter publishing our initial draft, we were made aware that Jang et al. (2019) already observed that the division of the softmax operation can be delayed until the end of the attention operation (\"lazy softmax\"), similar to our Equation (1). But their paper does not discuss memory complexity at all. They also do not address numerical stability or backpropagation, and, as far as we know, there is no publicly available implementation of their work. Instead they use this algorithm to reduce the memory bandwidth for inference when sharding key-value pairs across multiple chips. Dao et al. (2022) provide a CUDA implementation of memory-efficient attention and demonstrate that the reduced memory requirements can translate to significant speedups on GPUs. One reason why we do not observe the same performance gains in this paper is that standard self-attention already balances the available FLOPs and memory bandwidth of TPUs. ## 7 Conclusion\n\nThis paper presents a simple trick to reduce the memory requirement of (self-)attention dramatically, which appears to have been simply overlooked by the community. We hope that this short paper raises awareness of the fact that attention is not intrinsically memory-hungry, which may allow us to revisit some of the design choices in popular neural architectures and hardware architectures. ## Acknowledgements\n\nWe want to thank Andrew Jaegle for discussions on this paper, and for experimenting with memory-efficient attention in the context of Perceiver (Hawthorne et al., 2022). We are glad to see that the algorithm proposed here has already found interest and would like to thank Rezaei (2021) and Wang (2022) for reimplementations in JAX and PyTorch with additional features like masking. We also want to thank DeLesley Hutchins for detailed feedback on our draft. ## References\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun (eds.), ICLR, 2015. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174, 2016. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/2205.14135\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423. Curtis Hawthorne, Andrew Jaegle, C\u0103t\u0103lina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, Jo\u00e3o Carreira, and Jesse Engel. General-purpose, long-context autoregressive modeling with perceiver AR, 2022. URL https://arxiv.org/abs/2202.07765\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/google/flax. Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA), pp. 250-263, 2019. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "mea-5": "arXiv preprint arXiv:2001.04451, 2020. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention.",
    "mea-6": "CoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.",
    "mea-7": "Trans.",
    "mea-8": "Assoc. Comput. Linguistics, 9:53-68, 2021. URL https://transacl.org/ojs/index.php/tacl/article/view/2405\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa,\n\nHI, USA, January 3-8, 2021, pp. 3530-3538. IEEE, 2021. doi: 10.1109/WACV48630.2021.00357. URL https://doi.org/10.1109/WACV48630.2021.00357\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers.",
    "mea-9": "In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "mea-10": "In Advances in neural information processing systems, pp. 5998-6008, 2017. Phil Wang. Memory efficient attention Pytorch, 2022. URLhttps://github.com/lucidrains/memory-efficient-attention-p\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "mea-11": "arXiv preprint arXiv:2006.04768, 2020. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html\n\n"
}