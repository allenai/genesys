{
    "selfcondembdiffu-0": "# SELF-CONDITIONED EMBEDDING DIFFUSION FOR TEXT GENERATION \n\nRobin Strudel ${ }^{1 *}$<br>Corentin Tallec ${ }^{2}$<br>Florent Altch\u00e9 ${ }^{2}$<br>Yilun Du ${ }^{3 *}$<br>Yaroslav Ganin ${ }^{2}$<br>Arthur Mensch ${ }^{2}$<br>Will Grathwohl ${ }^{2}$<br>Nikolay Savinov ${ }^{2}$<br>Sander Dieleman ${ }^{2}$<br>Laurent Sifre ${ }^{2}$<br>R\u00e9mi Leblond ${ }^{2}$\n\n\n#### Abstract\n\nCan continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion (SED), a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models - while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion. ## 1 INTRODUCTION\n\nContinuous diffusion models (Sohl-Dickstein et al., 2015) have taken the world of image generation by storm, advancing the state of the art further than ever before (Rombach et al., 2021; Ramesh et al., 2022). Can the same framework encounter as much success on the text modality? Diffusion for language is indeed an attractive prospect. Compared to autoregressive (AR) models (Bengio et al., 2000; Sutskever et al., 2011; Austin et al., 2021; Hoffmann et al., 2022), diffusion models can predict all tokens in a sequence at once. This allows for bidirectional, rather than causal attentionincreasing interactions between tokens, potentially leading to more coherent samples. Diffusion models can make a better usage of hardware accelerators during inference than AR models, since computations are parallelizable over the sequence axis. Yet AR models remain the mainstream approach for modelling text. A major obstacle to text diffusion is that diffusion processes typically operate in continuous space. While this naturally handle images, text is inherently discrete. Consequently, most previous attempts to apply diffusion to text have focused on discrete diffusion-like approaches. These methods do not benefit from the refinements made to continuous diffusion in the image domain. Crucially, they cannot make use of guidance (Dhariwal \\& Nichol, 2021), which drastically improves diffusion models sample quality. We address this gap by making a simple observation: language models operate mostly in continuous space, with discrete tokens only as inputs and outputs. A natural idea is then to conduct diffusion directly in a continuous token embedding space. For simplicity, we use a fixed embedding space, either random or stemming from a trained language model. Combined with the \"self-conditioning\" (Chen et al., 2022) refinement, this forms the basis of the method we propose, Self-conditioned Embedding Diffusion (SED). [^0]SED models rival mainstream AR models in both conditional and unconditional text generation. We make the following contributions:\n\n- In section 3, we introduce SED, the first continuous diffusion approach for text with good scaling properties (testing models up to 420M parameters). We analyze several continuous text diffusion settings, and identify self-conditioning and diffusion on small fixed embeddings as key factors to make continuous text diffusion work. - In section 4, we apply classifier-free guidance (Ho \\& Salimans, 2022) to text data-an original achievement. We show that SED can rival AR models on generic language tasks, for similar models sizes. SED samples achieve a better likelihood-entropy trade-off compared to these models, and are deemed comparable (if slightly worse) by human raters. ## 2 RELATED WORK\n\nWe provide an overview of diffusion models with a focus on modeling discrete data, as well as AR models and sample-based metrics for evaluating text generation. Continuous diffusion on continuous image data. Continuous diffusion has recently established itself as the method of choice for modeling continuous data such as images. While our main focus in this paper is on discrete data, we review some key works in continuous data modeling as this literature was the major source of inspiration for SED. The first continuous diffusion formulation was introduced in the seminal work by Sohl-Dickstein et al.",
    "selfcondembdiffu-1": "(2015). Ho et al. (2020) improved and simplified this formulation, relating it to denoising score matching, and creating a new method called DDPM. Nichol \\& Dhariwal (2021) further improved upon DDPM, showcasing impressive diffusion results compared to GANs. Rombach et al. (2021, Stable Diffusion) introduced diffusion in latent space. Conceptually similar to SED, it was specifically targeted at image modeling. Classifier-free guidance was proposed by Ho \\& Salimans (2022) as a mean to improve image fidelity at the cost of reduced diversity. GLIDE (Nichol et al., 2022) scaled up the ideas of guided diffusion, while DALL-E 2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) are the latest, most advanced image generation systems to date, combining most of the improvements proposed in previous works.",
    "selfcondembdiffu-2": "Discrete diffusion on discrete data. One cannot simply reuse the methods that are successful on continuous image data in the discrete text domain. A number of bespoke methods have been explored instead, forming the family of discrete diffusion approaches. In discrete diffusion, the data is corrupted by switching from one discrete value to another.",
    "selfcondembdiffu-3": "This was first proposed in the seminal work by Sohl-Dickstein et al. (2015), where it was tested on simplistic binary heartbeat data. It was extended to multinomial text modeling (Hoogeboom et al., 2021) and further scaled up in the D3PM work (Austin et al., 2021). Most recently, a similar discrete diffusion approach was applied to image modeling in VQ-Diffusion (Gu et al., 2022). In parallel, a few diffusion-like approaches were proposed in the denoising autoencoders literature. CMLM (Ghazvininejad et al., 2019) tackled machine translation. SUNDAE (Savinov et al., 2022) was the first non-AR method to show strong results both in machine translation and unconditional text generation. MaskGIT (Chang et al., 2022) demonstrated excellent results in modeling VQ-discretized images. These approaches rely on training models to predict masked tokens from their context, and iterating this reconstruction step multiple times at sampling time. Despite those positive developments, the samples from discrete diffusion methods for text modeling remains less coherent than those produced by AR methods. Continuous diffusion on discrete data. Fewer works try to tackle diffusion on discrete data from the same angle as SED - starting by turning the data into continuous representations before modeling it with continuous diffusion formulations. Mittal et al. (2021) used a VAE to generate such representations for discrete music modeling, with exciting results. Closest to SED, Diffusion- $\\mathrm{LM}(\\mathrm{Li}$ et al., 2022) trains a token embedding together with the diffusion model itself. Diffusion-LM meets success on specific language applications, in low data regime and on constrained, very formatted textual data. Most recently, Analog Bits (Chen et al., 2022) introduced self-conditioning, closely related to step-unrolls in SUNDAE (Savinov et al., 2022), together with bit-level modeling to improve the generation of discretized images. While the qualitative results of those continuous methods on text modeling show promise, they have not been shown to scale to large realistic text datasets like C4 (Raffel et al., 2020) yet, or to compare with AR approaches on generic language tasks. Auto-regressive modelling on discrete data. AR models remain the method of choice for modeling discrete data. In combination with neural networks, they were first explored by Bengio et al. (2000) and later combined with RNNs (Sutskever et al., 2011). Their breakthrough moment came with the advent of the Transformer architecture, introduced by Vaswani et al. (2017) for machine translation. Even more impressive results were shown with GPT-3 (Brown et al., 2020), which trained a large AR language model unconditionally, and used few-shot prompting to adapt it to new tasks. A few works later improved upon the results of GPT-3, including Hoffmann et al.",
    "selfcondembdiffu-4": "(2022). Sample-based evaluation of text generative models. There are traditionally two classes of metrics for generative modeling: likelihood-based and sample-based. While the likelihood-based way is mathematically appealing, its usefulness for measuring progress is reduced by the fact that not all models readily provide likelihood computation. Just like the sampled-based FID metric was important for driving the progress of diffusion in image modeling, there is a need for a sample-based metric which would be universally accepted for text modeling. Caccia et al. (2018) investigated fidelity/variance metrics for evaluating text GANs. Semeniuta et al. (2018) suggested using FID for texts. De Masson d'Autume et al. (2019) later used those previously proposed metrics to iterate on ScratchGAN but did not provide conclusive guidance on which metric a practitioner should choose essentially finding serious vulnerabilities in all investigated metrics. We opted for a middle ground, reporting both sample likelihood according to a strong AR model and human preferences. ## 3 METHOD\n\nIn this section, we outline the different components of SED: continuous diffusion in the space of token embeddings and self-conditioning, which form the basis of our approach for unconditional text generation; span masking and guided diffusion to enable conditional generation. ### 3.1 DIFFUSION MODELS FOR UNCONDITIONAL TEXT GENERATION\n\nDiffusion models in continuous space. We consider diffusion models as introduced by SohlDickstein et al. (2015) and improved by Ho et al. (2020). A diffusion model aims at modelling a data distribution $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{n} \\sim q \\in \\mathcal{D}\\left(\\mathbb{R}^{n}\\right)$ by estimating a sequence of latent variables $\\boldsymbol{x}_{T}, \\ldots, \\boldsymbol{x}_{1}$ of the same dimensionality as the data $\\boldsymbol{x}_{0}$. Starting from $\\boldsymbol{x}_{0}$, the latent variables are generated with a Markov chain called the forward process: $\\boldsymbol{x}_{t} \\sim q\\left(\\cdot \\mid \\boldsymbol{x}_{t-1}, t\\right)$. It is defined by gradually interpolating the iterate with Gaussian noise according to noise levels defined by a schedule $\\beta_{1}, \\ldots, \\beta_{T}$ :\n\n$$\n\\boldsymbol{x}_{t} \\sim q\\left(\\cdot \\mid \\boldsymbol{x}_{t-1}, t\\right)=\\mathcal{N}\\left(\\sqrt{1-\\beta_{t}} \\boldsymbol{x}_{t-1}, \\beta_{t} \\boldsymbol{I}\\right)\n$$\n\nThis parametrization gives us a closed form to sample $\\boldsymbol{x}_{t}$ for any arbitrary $t \\geq 1$, given $\\boldsymbol{x}_{0}$ :\n\n$$\n\\boldsymbol{x}_{t}=\\sqrt{\\alpha_{t}} \\boldsymbol{x}_{t-1}+\\sqrt{1-\\alpha_{t}} \\epsilon_{t}=\\sqrt{\\bar{\\alpha}_{t}} \\boldsymbol{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}} \\epsilon\n$$\n\nwhere $\\alpha_{t}:=1-\\beta_{t}, \\bar{\\alpha}_{t}:=\\prod_{s=1}^{t} \\alpha_{s}, \\epsilon_{t} \\sim \\mathcal{N}(0, \\boldsymbol{I})$ and $\\epsilon \\sim \\mathcal{N}(0, \\boldsymbol{I})$. We define our generative model by approximately inverting the diffusion process of Eq. 1 to obtain a reverse process. The reverse process starts from $\\boldsymbol{x}_{T} \\sim \\mathcal{N}(0, \\boldsymbol{I})$ and is defined as a Markov chain with learned Gaussian transitions (parameterized by $\\theta$, the weights of a neural network): $\\boldsymbol{x}_{t-1} \\sim$ $p_{\\theta}\\left(\\cdot \\mid \\boldsymbol{x}_{t}\\right)=\\mathcal{N}\\left(\\boldsymbol{\\mu}_{\\theta}\\left(\\boldsymbol{x}_{t}, t\\right), \\sigma(t)^{2} \\boldsymbol{I}\\right)$. We train a neural network to predict an estimate $\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, t, \\theta\\right)$ of the data $\\boldsymbol{x}_{0}$ and approximate the reverse process by using the following parametrization, with learnable means but fixed variances, and a fixed schedule $\\beta_{1}, \\ldots, \\beta_{T}$ :\n\n$$\n\\boldsymbol{\\mu}_{\\theta}\\left(\\boldsymbol{x}_{t}, t\\right)=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_{t}}{1-\\bar{\\alpha}_{t}} \\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, t, \\theta\\right)+\\frac{\\sqrt{\\alpha_{t}}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_{t}} \\boldsymbol{x}_{t}, \\quad \\sigma(t)^{2}=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}} \\beta_{t}\n$$\n\nWhile there exists a tractable variational lower-bound (VLB) on $\\log p_{\\theta}\\left(\\boldsymbol{x}_{0}\\right)$, Ho et al. (2020) showed that better results are obtained by optimizing a simplified objective that re-weights the terms in the VLB. We follow this approach, which simplifies the loss to a sum of mean-squared errors between the ground truth data $\\boldsymbol{x}_{0}$ and its estimates $\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, t, \\theta\\right)$ :\n\n$$\n\\mathcal{L}_{\\text {diffusion }}=\\mathbb{E}_{\\boldsymbol{x}_{0} \\sim q\\left(\\boldsymbol{x}_{0}\\right), t \\sim \\mathcal{U}(1, T)}\\left\\|\\boldsymbol{x}_{0}-\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, t, \\theta\\right)\\right\\|^{2}\n$$\n\nThough this framework works out of the box on images, which are close to continuous, we cannot apply it directly to the discrete tokens of the text modality. To resolve this issue, we perform continuous diffusion in a continuous space in which we embed text tokens. Diffusion on word embeddings. We consider textual data $\\boldsymbol{w}=\\left(w_{1}, \\ldots, w_{N}\\right)$, where each $w_{i}$ is a one-hot representation in $\\mathbb{R}^{V}$ of a discrete token in $\\{1, \\ldots, V\\}$. Each token $w$ has an associated embedding $e_{w} \\in \\mathbb{R}^{D}$, with fixed norm $\\sqrt{D}$ to match the norm of a random gaussian sample in dimension $D$ used to noise clean data. We denote by $\\boldsymbol{E} \\in \\mathbb{R}^{D \\times V}$ the matrix of all embeddings. We define our diffusion process in embedding space, rather than in token space. To that end, we define a forward discrete-to-continuous step $q_{\\boldsymbol{V}}\\left(\\boldsymbol{x}_{0} \\mid \\boldsymbol{w}\\right)=\\mathcal{N}\\left(\\boldsymbol{E} \\boldsymbol{w}, \\sigma_{0}^{2} \\boldsymbol{I}\\right)$, where $\\sigma_{0}$ is a constant scale factor with a similar order of magnitude as $\\beta_{1}$. Conversely, we define a reverse continuousto-discrete step $p_{\\boldsymbol{R}}\\left(\\boldsymbol{w} \\mid \\boldsymbol{x}_{0}\\right)=\\prod_{k=1}^{N} \\operatorname{Cat}\\left(w_{k} \\mid \\boldsymbol{E}^{\\prime}\\left(\\boldsymbol{x}_{0}\\right)_{k}\\right)$, where $\\boldsymbol{R} \\in \\mathbb{R}^{V \\times D}$ is a learnable readout matrix initialized to $\\boldsymbol{E}^{\\top}$ and $\\operatorname{Cat}\\left(w_{k} \\mid \\boldsymbol{l}\\right)$ is the softmax probability of token $k$ with logits $\\boldsymbol{l} \\in \\mathbb{R}^{V}$. To train the readout step, we add a reconstruction loss to $\\mathcal{L}_{\\text {diffusion }}$ during training. Conveniently, it naturally arises when deriving the VLB of $p_{\\theta}(\\boldsymbol{w})$ with this discretization step (Li et al., 2022), introducing a simple cross-entropy loss to maximise $p_{\\theta}\\left(\\boldsymbol{w} \\mid \\boldsymbol{x}_{0}\\right)$ :\n\n$$\n\\mathcal{L}_{\\text {recon }}=\\mathbb{E}_{\\boldsymbol{w} \\sim \\mathcal{D}, \\boldsymbol{x}_{0} \\sim q_{\\boldsymbol{V}}(\\boldsymbol{w})}\\left[-\\log p_{\\boldsymbol{R}}\\left(\\boldsymbol{w} \\mid \\boldsymbol{x}_{0}\\right)\\right], \\quad \\text { with } \\quad \\mathcal{L}_{\\text {total }}=\\mathcal{L}_{\\text {diffusion }}+\\mathcal{L}_{\\text {recon }}\n$$\n\nContrary to what is done in Li et al. (2022), we do not learn the embedding matrix $\\boldsymbol{E}$, as we identified that it was empirically unstable and could lead to drops in unigram entropy. The reconstruction loss $\\mathcal{L}_{\\text {recon }}$ therefore only depends on the trainable readout weights $\\boldsymbol{R}$. At sampling time, we run the reverse process for $T=1000$ steps, ultimately yielding a continuous embedding $\\overline{\\boldsymbol{x}}_{0}$ of size $d_{\\text {embed }}$. We multiply it by $\\boldsymbol{R}$ to obtain logits in $\\mathbb{R}^{V}$, and then use the index of the maximum component to convert it to a token $w_{i}$, with $i=\\arg \\max _{1 \\leq j \\leq V}\\left(\\boldsymbol{R} \\overline{\\boldsymbol{x}}_{0}\\right)$. This entails running $T$ full forward passes which is quite expensive compared to cached ${ }^{A R}$ sampling; however each forward pass computes all timesteps at once which is naturally parallelisable. Further, we hope to benefit from many diffusion sampling improvements to get $T$ down to low double-digits. Self-conditioning (Chen et al., 2022). In standard diffusion sampling, at each timestep $t$ the denoising network generates an estimate $\\overline{\\boldsymbol{x}}_{0}^{t}=\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, t, \\theta\\right)$ of $\\boldsymbol{x}_{0}$ given only $\\boldsymbol{x}_{t}$ as input. Selfconditioning progressively refines $\\boldsymbol{x}_{0}$ estimates by passing the estimate $\\tilde{\\boldsymbol{x}}_{0}^{t+1}$ obtained at the previous sampling step as input to the denoising network; the self-conditioned estimate is then defined as $\\tilde{\\boldsymbol{x}}_{0}^{t}=\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, \\tilde{\\boldsymbol{x}}_{0}^{t+1}, t, \\theta\\right)$, and sets the diffusion direction. In practice conditioning is performed by concatenating $\\boldsymbol{x}_{t}$ and $\\tilde{\\boldsymbol{x}}_{0}^{t+1}$ on the feature axis. To approximate the inference behavior at train time while remaining computationally efficient, we compute a first estimate $\\overline{\\boldsymbol{x}}_{0}^{t}=\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, 0, t, \\theta\\right)$ with the self-conditioning set to zero, then perform a second forward pass using a stop gradient on $\\overline{\\boldsymbol{x}}_{0}^{t}$ to obtain $\\tilde{\\boldsymbol{x}}_{0}^{t}=\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, \\overline{\\boldsymbol{x}}_{0}^{t}, t, \\theta\\right)$. The denoising network is then optimized using the output from the two forward passes in order to estimate $\\boldsymbol{x}_{0}$ accurately with and without self-conditioning. Equipped with these 3 components we can train models to generate text, though only unconditionally. To add conditional generation to our system's capabilities, we use two additional methods. ### 3.2 SPAN MASKING AND GUIDANCE FOR CONDITIONAL TEXT GENERATION\n\nBy design diffusion models for text generation are flexible and can handle a wide variety of infilling tasks. This is a key advantage over the predominant auto-regressive language models that typically generate text in a left-to-right fashion.",
    "selfcondembdiffu-5": "Span masking. We train our model on a rich set of infilling tasks with the following method. We split $\\boldsymbol{x}_{0}$ between two set of tokens, diffusion tokens $\\boldsymbol{x}$ over which we apply diffusion and optimize the diffusion loss from Eq. 4, and conditioning tokens $\\boldsymbol{c}$ that remain fixed. Conditioning tokens $\\boldsymbol{c}$ are defined by a binary conditioning mask $\\boldsymbol{m}$ set to one on conditioning positions and zero on positions to be infilled. We sample conditioning mask $\\boldsymbol{m}$ randomly as follows. Given a sequence of length $L$ and a maximum number of spans $M$, we sample a number of spans $n$ uniformly in $[1, M]$. Span starting\n\nTable 1: SED samples on unconditional generation, fill-in-the-middle and several spans in-filling. | Task | Samples |\n| :---: | :---: |\n| Unconditional | We make use of the very best supplies and solutions to ensure that the work is going to stand up <br> to the test of time, and we help you save money with techniques that do not change the quality <br> of your mission. We'll achieve this by offering you the best deals in the field and avoiding <br> pricey mistakes. If you want to spend less, Refrigerator Unit Repair Guys is the company to <br> contact. |\n| Fill-in-the- <br> middle | A year ago in Paris, I had the opportunity to take a field trip to La Rite-en-Laurences Interna- <br> tional de France where I met David Nigel Johnson, a professor of social studies. What a great <br> trip and what a great day! |\n| Spans <br> in-filling | There was no evidence, only fleeting glimpses of the killer and his fate.",
    "selfcondembdiffu-6": "In fact, it seemed <br> that there was no evidence. It was all guesswork, and one of the most unusual murder cases <br> throughout history. |\n\npositions are defined by $n-1$ integers $\\left(i_{1}, \\ldots, i_{n-1}\\right)$ sampled uniformly without replacement and sorted in increasing order to satisfy $0<i_{1}<\\ldots<i_{n-1}<L$. The tuple $\\left(i_{1}, \\ldots, i_{n-1}\\right)$ partitions the sequence of tokens in $n$ spans satisfying $\\mathbb{E}\\left[i_{k} \\mid n\\right]=\\frac{k}{n} L$. The conditioning mask $\\boldsymbol{m}$ is defined using even spans for conditioning and odd spans for infilling, and then $\\boldsymbol{m}$ is flipped with a $50 \\%$ probability. The case $n=1$ corresponds to unconditional generation; we then set $\\boldsymbol{m}$ to 0 everywhere. This span masking strategy defines a collection of text generation tasks with a large variety of conditioning which on average evenly splits the sequence between conditioning and infilling spans. It enables conditional generation, and opens the door for additional diffusion improvements. Guided diffusion. Guidance (Dhariwal \\& Nichol, 2021) often improves the sample quality of conditional diffusion models. We use classifier-free guidance (Ho \\& Salimans, 2022), which alleviates the need for a separately-trained guide model. In the conditional case, our estimator $\\tilde{\\boldsymbol{x}}_{0}$ is now a function $\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{c}, \\tilde{\\boldsymbol{x}}_{0}^{t+1}, t, \\theta\\right)$, where $\\boldsymbol{c}$ are fixed conditioning tokens. During training, with fixed probability the conditioning tokens $\\boldsymbol{c}$ used in the estimator $\\hat{\\boldsymbol{x}}_{0}$ are dropped and set to a null label $\\emptyset$ equal to zero. During sampling, the model prediction is extrapolated in the direction of $\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{c}, \\tilde{\\boldsymbol{x}}_{0}^{t+1}, t, \\theta\\right)$ and away from $\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, 0,0, t, \\theta\\right)$ as follows:\n\n$$\n\\tilde{\\boldsymbol{x}}_{0, s}^{t}=\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, 0,0, t, \\theta\\right)+s \\cdot\\left(\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, \\boldsymbol{c}, \\tilde{\\boldsymbol{x}}_{0}^{t+1}, t, \\theta\\right)-\\hat{\\boldsymbol{x}}_{0}\\left(\\boldsymbol{x}_{t}, 0,0, t, \\theta\\right)\\right)\n$$\n\nwhere $s \\geq 1$ is the guidance scale. Remark that we jointly drop conditioning and the selfconditioning $\\tilde{\\boldsymbol{x}}_{0}^{t+1}$, concretely setting both values to zero. Classifier-free guidance allows leveraging both the unconditional and conditional abilities of a model to improve its conditional generations. ## 4 EXPERIMENTS\n\n### 4.1 TRAINING DETAILS\n\nWe train all our models on the C 4 dataset (Raffel et al., 2020), using a SentencePiece tokenizer (Kudo \\& Richardson, 2018) composed of 32000 words. We use a non-causal transformer model (Vaswani et al., 2017) as our diffusion model (see Appendix A for details). SED models are trained with sequence length 256, while for ablations models are trained with sequence length 128 . We insert uniformly, i.e. not necessarily at the end of the sequence, $10 \\%$ of padding tokens in the training set to allow SED models to generate samples of varying size and provide more flexibility. To generate word embeddings, we train a BERT model of fixed size ( 150 m parameters) and feature dimension $d_{\\text {model }}=896$. The diffusion space is defined by the initial lookup table of this BERT model. We bottleneck the dimension of the word embeddings $d_{\\text {embed }}$ and add a linear projection layer from $d_{\\text {embed }}$ to $d_{\\text {model }}$ at the beginning of the model. We found this helped diffusion (see section 4.4). SED models are trained with a cosine noise schedule (Dhariwal \\& Nichol, 2021), with $\\beta_{1}=2.10^{-3}$, $\\sigma_{0}=10^{-2}$ and $T=1000$. We use batches of 65.536 tokens, thus for sequence length 256 the batch size is set to 256 . We use a maximum span count of 5 for all runs except for its specific ablation. We\ntrain SED models at two different scales: SED-S ( 135 m parameters, $10^{6}$ training steps) and SED-L ( $420 \\mathrm{~m}, 2.10^{6}$ steps). Their detailed architectures can be found in Appendix A. ### 4.2 VALIDATION\n\nWhile optimizing the perplexity of AR models for text leads to improved language models, directly optimizing the ELBO of diffusion models for images does not correlate strongly with sample quality as observed by Nichol \\& Dhariwal (2021); Kingma et al. (2021); Ho \\& Salimans (2022). For images, the sample based metric FID (Heusel et al., 2017) has been introduced as a measure of sample quality and is now widely adopted. Similarly, we need a sample-based metric for text generation that is reliable and allows comparison between a large variety of generative models. To provide a fair comparison to AR models, we rely on three metrics. The first metric measures how likely the samples produced by a model are according to an AR language model with 70B parameters, trained on 1.4B tokens (Hoffmann et al., 2022); we denote this metric AR NLL for auto-regressive negative log-likelihood. It provides a continuous measure of sample quality that has proven useful when combined with a measure of sample diversity, e.g. in the development of nucleus sampling (Holtzman et al., 2020) for improved AR model decoding. To measure diversity we rely on a second metric, the unigram entropy of samples, which helps balance the AR NLL that can be gamed by unnatural repetitive samples. For both these metrics, our target is the score of the validation set data. Deviating from the data unigram entropy in particular is a sign of degenerate modeling. Though this initial combination has provided us with a reliable signal to iterate over our model design, it remains imperfect; it too can be gamed, though it is harder to do so. To address this limitation, we also report human preferences. We presented 6 colleagues with 20 pairs of samples for each comparison, asking them to pick the best one. For all three metrics, we report results on two tasks: unconditional language modeling and suffix in-filling, the later a heavily conditioned task. ### 4.3 RESULTS\n\nSamples. We present samples generated with our SED models in Table 10. We use a single model to perform a wide variety of text generation tasks, such as unconditional generation, filling-in-the-middle or filling several spans of text. We show strong performance in the unconditional case, with samples that are syntactically correct and stay coherent on long sequences. In the conditioned case, SED models are able to infill spans with coherent transitions and links to the conditioning but also exhibit a rich diversity. By design, SED yields flexible bi-directional masking models that can perform text generation on a diverse set of conditioned task. To compare SED with AR baselines we next restrict conditioning to a prefix and consider a task of suffix in-filling. Comparison to AR models. To assess the generation ability of SED, we compare against AR baselines of similar capacity and trained following optimal scaling laws from Hoffmann et al. (2022) on suffix in-filling. We sample a batch of sequences from C 4 and use the first 128 tokens as conditioning given to the model to generate a suffix of 128 tokens. Figure 1 reports AR NLL and unigram entropy of the gen-\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_2e34c3a2b0abbed7a7afg-06.jpg?height=671&width=697&top_left_y=1540&top_left_x=1061)\n\nFigure 1: Comparison of sample quality and diversity of SED versus AR models on suffix in-filling. SED uses guidance with scales in $\\{1,2,4,8\\}$ and AR uses nucleus sampling with a top- $p$ in $\\{1.00,0.95,0.90,0.85\\}$. Top-right points are SED models with a guidance scale of 1 or AR models with a top- $p$ of 1 . Table 2: We compare SED-L (guidance scale 2.5) and AR-L (nucleus sampling, $p=0.95$ ) samples. | SED L | AR-L |\n| :---: | :---: |\n| You're going to love wearing this traditional tee <br> from our latest Wilson collection. Designed in a <br> scrapped floral styled knit with a sleeve of asym- <br> metric lines across the round sole. Lightly fluffy, <br> the square pleats will take you right to | A Koda Ram 25 is presented in sedan and a Max- <br> ima saloon. Based on the Acenta car, the power- <br> ful Koda 2014 hits Indian roads in the \"Maxima\" <br> body-con shape. Being powered by a Hyundai i20 <br> 1.4 litre diesel engine, the Koda 2014 is coupled |\n| The beaver is an interesting animal that lives in <br> rivers and lakes. He is not mainly concerned with <br> finding wolves and dolphin but also has a great <br> hunger for fish. The beaver has sharp legs, large <br> eyes, and a black coat | The beaver is an interesting animal that lives in <br> rivers and streams. It is usually seen in big num- <br> bers in the fields or upstream, and is quite docile. <br> On cold days when its pattern is perfect, the <br> beaver will have some interesting, and sometimes |\n| Once upon a time in Spain, Leonardo Pueleva had <br> the pleasure of meeting guests at Spanish restau- <br> rant, Buva Casinos. While driving, he got a chance <br> to get to know the people behind the restaurant <br> and, of course, how they made his experience very <br> interesting. After his conversation, he got to | Once upon a time in Spain, which seems pretty <br> much the same way now, the question that was <br> posed to each of us at the end of our interview was <br> \"would you like to see Froome one day?\" In ret- <br> rospect, after our interview, we have grown ever <br> closer to that answer. As you will read in the arti- <br> cle, I know that |\n\nerated suffixes for AR and SED models. As a\nreference point, we compute the AR NLL and unigram entropy of the ground truth C4 suffixes and report it on the plot. Several methods can be used to improve sampling quality at the cost of samples diversity; we use nucleus sampling (Holtzman et al., 2020) for AR models and guidance (Dhariwal \\& Nichol, 2021; Ho \\& Salimans, 2022) for SED models. We show the impact of guidance on samples quality in Table 3. To our knowledge, we are the first to show sample quality improvement when using guidance for text generation. As shown in Figure 1, both SED-S and SED-L perform strongly when compared against AR baselines - even though we report a metric favoring AR models on a task AR models have been designed to optimize. Similar to nucleus sampling for AR models, guidance has a strong positive impact on sample quality that is both observed quantitatively with improved AR NLL in Figure 1 and qualitatively in Table 3. We observe that using a top- $p$ nucleus sampling below 0.8 for AR models or a guidance scale above 4 for SED models leads to samples exhibiting a lot of repetitions, a degenerate case reflected by a lower entropy of samples even though sample AR NLL improves. Our human preference scores temper our observations in Table 4. They show that our NLL and entropy metrics do not tell the whole story, as humans still prefer AR models at equivalent size. While SED-L performs slightly worse than AR-L ( $38 \\%$ preference in suffix in-filling, 44\\% on unconditional generation), its scores remain comparable. SED-L is roughly on par with AR-S. Finally, we compare SED and AR models' qualitative examples with short prompts in Table 2. ### 4.4 Ablations\n\nSelf-conditioning and embedding pretraining. Results from Table 5 and samples from Table 6 show the influence of both the diffusion space and self-conditioning. AR NLL decreases very significantly when using self-conditioning, regardless of the rest of the setup. Diffusing at the bit-level (Chen et al., 2022) yields very high NLLs. While using random embeddings performs markedly better, using pretrained embeddings results in further improved numbers. Samples from Table 6 highlight that models trained on random word embeddings exhibit topic modelling abilities with the co-occurrence of words like child and mother even though the paragraph remains globally incoherent and meaningless tokens like gluc are generated. Self-conditioning dramatically improves sample quality; the diffusion model gets the low-level structure right and generates syntactically correct sentences, even though the global text is not intelligible. Combining self-conditioning and pretrained embeddings leads to globally coherent paragraphs that stay on topic with proper sentence structure. Table 3: Impact of guidance on samples quality using our SED model. | Guidance | 1.0 | 2.5 | 5.0 |\n| :---: | :---: | :---: | :---: |\n|  | In the cold, cold night sky, a <br> fairy princess sits in a chair <br> and surrounded by tea leaves <br> in a pond. Meanwhile, she <br> bies back into the cold, with <br> bluish hair on her hips and el- <br> bows on her forehead - and <br> her fingers numbed by the <br> freezing temperature. | In the cold, cold night of <br> November 2018, a little girl <br> sits in a chair hidden under <br> a light blanket on a patio. <br> Meanwhile, she bends back <br> into the chair with bluish <br> hair on her forehead, her <br> hands on her face, her fingers <br> numbed by the freezing tem- <br> perature. | In the cold, cold night of <br> December, my oldest daugh- <br> ter sits in a chair accentuated <br> in cotton fabrics and a pil- <br> low. Meanwhile, she yearns <br> straight in the cold air, her <br> wrists covering her neck, her <br> eyes straight on her forehead, <br> and her fingers numbed by the <br> freezing temperature. |\n|  | Barbara was one of our many <br> wonderful women that really <br> helped so I am so blown off <br> by her purpose, civility; and <br> adversity. Once I started inter- <br> acting with her, it proved to <br> me that no matter how hard <br> this was, she always strove for <br> excellence. | Barbara was one of the most <br> gifted women in the world. <br> She was creative and stood <br> up by her integrity and ci- <br> vility; against adversity. Al- <br> though she placed herself <br> higher than her peers, it <br> proved to me that no matter <br> how hard this was, she al- <br> ways strove for excellence. | Barbara was one of the most <br> brilliant women in the world. <br> She was amazing in her heart, <br> her spirit, her mind and in the <br> soul.",
    "selfcondembdiffu-7": "She never turned people <br> off in her absence. It proved <br> to me that no matter how hard <br> this was, she always strove for <br> excellence. |\n\nTable 4: SED-L vs other models human preference scores on conditional and unconditional tasks. |  | SED-S (cond) | AR-S (cond) | AR-L (uncond) | AR-L (cond) |\n| :---: | :---: | :---: | :---: | :---: |\n| SED-L | $63.4 \\% \\pm 4.3 \\%$ | $51.0 \\% \\pm 5.0 \\%$ | $43.8 \\% \\pm 4.4 \\%$ | $37.7 \\% \\pm 4.4 \\%$ |\n\nEmbedding dimension. An important design choice for SED is the word embeddings space. We study the influence of pretrained embedding size in Table 7. Surprisingly, there is a threshold after which performance degrades when increasing the dimension of embeddings. We visualize the forward process for different embedding sizes by displaying the nearest neighbor of a noised token while running the forward process. In high dimension we observe that the nearest neighbor of a noised token remains the starting token itself until it switches to a completely random, unrelated token. In low dimension, we often observe that the closest neighbor of a noised token goes through several semantically related tokens (nearest neighbor of the starting token) before ultimately becoming random. We hypothesize that the random walks defined by diffusion are more likely to drift towards neighbors of the starting token in low dimension. As a result, when diffusing in low dimension information is destroyed in a more semantically meaningful fashion, which leads to an easier learning problem for the denoising function. Number of spans. In order to enable in-filling, we train the model not only to do unconditional generation but also to conditionally fill spans of tokens. For each data point we sample a span number uniformly at random and span delimiters to generate the span mask. Picking the maximum allowable number of spans has a significant effect on model performance, as we can see in Table 8. Somewhat counter-intuitively, adding span masking improves even unconditional generation NLLs. It also appears that using a relatively high maximum span number is optimal. We hypothesize that this results in a varied mix of task difficulty at training time, between \"easy\", very conditioned problems on the one hand and \"harder\", unconditional ones on the other. Scaling. We show encouraging results when scaling from SED-S (150m) to SED-L (420m). We train both models on sequences of 256 tokens and report a AR NLL of 4.20 for SED-S compared to 3.68 for SED-L. This improvement translates to improved sample quality, as is confirmed by our human preference scores, which are much higher for the larger model ( $63 \\%$, see Table 4 ). Table 5: Ablation of the proposed SED approach on unconditional generation. Both selfconditioning and embeddings pretraining play a key role in the model performance. | Diffusion space | Self-conditioning | Unigram entropy | AR NLL |\n| :--- | :---: | :---: | :---: |\n| Bits (Chen et al., 2022) | $\\boldsymbol{x}$ | 6.97 | 7.01 |\n|  | $\\boldsymbol{\\checkmark}$ | 7.47 | 6.05 |\n| Random embeddings | $\\boldsymbol{x}$ | 6.90 | 6.80 |\n|  | $\\boldsymbol{\\checkmark}$ | 6.86 | 5.31 |\n| Pretrained embeddings | $\\boldsymbol{x}$ | 6.75 | 5.66 |\n|  | $\\boldsymbol{\\checkmark}$ | 6.77 | $\\mathbf{4 . 5 7}$ |\n| Data | - | $6.70 \\pm 0.04$ | $1.81 \\pm 0.15$ |\n\nTable 6: On unconditional generation, self-conditioning results in better sentence modelling, pretrained embeddings enhances topic modelling. | Random embeddings | Random embeddings <br> with self-conditioning | Pretrained embeddings <br> with self-conditioning |\n| :---: | :---: | :---: |\n| did the buildingroom granted a <br> lighter distance. On it though, <br> salaries about clients that a child, <br> which dispersed gluc so many <br> events and certainly wanted the <br> Mother's project, discovered by <br> their child would keep | Tree brings the sound, bearing <br> features and capabilities that we <br> are set in. For the first time, she <br> uses a customizable framework <br> to use that we help students pub- <br> licly solve the weather conditions <br> that we only offer students | almost six decades ago - 72 per- <br> cent of Americans didn't feel <br> they'd actually rent their own <br> cars this year. Conversely, 90 per- <br> cent of Americans feel that the <br> decision to rent a car is some- <br> thing they feel it's impossible |\n\n## 5 LIMITATIONS\n\nWhile our results are promising and show that continuous diffusion for text can be an exciting alternative to AR models, the current approach does present some significant limitations. First, much more could be done in terms of model tuning, including scaling to much bigger models to better understand SED's limits, and to be able to compare it with state-of-the-art AR models. Our training regime in particular would certainly benefit from more hyperparameter optimisation. Second, one compelling reason we chose to explore continuous diffusion for text is to leverage the improvements produced by the literature on image generation. While we have ported some (e.g. selfconditioning), a lot more remains unexplored. The most obvious example is the sampling process itself, where the number of required steps has been considerably reduced for images (e.g. Karras et al. (2022) goes from 1000 to 35, and Salimans \\& Ho (2022) all the way down to 4 on simple images). Our current sampling is very inefficient, and this direction is one of the first improvements to make over SED. Third, SED crucially relies on diffusing in a pretrained embedding space. This means relying on a second model, and using embeddings that may not be optimal for diffusion. Ideally, we'd train the full model end-to-end, which could yield even better results. While Li et al. (2022) found some success with this approach, it was in a specific setting at a small scale; in practice we found it difficult to avoid competition between the diffusion and reconstruction loss. Finally, our work would benefit from improved metrics in the experimental section. Because the current state of the art involves AR models, the field lacks established benchmarks for tasks diffusion models are potentially better suited for, such as text in-filling. We opted for a reasonable mix, evaluating the negative log-likelihood of generated samples according to a very strong AR model as well as their token entropy and complementing it with a human evaluation. However, both NLL and unigram entropy are gameable (e.g. AR models assign very low NLL to repetitive snippets, and long enough repetitions can fool even entropy). Further, our NLL is inherently tied to its AR model and could thus be providing an unfair advantage to AR models. All told, we still found both metrics quite useful for measuring research progress, and our human evaluation confirmed our results. Moving forward, defining a clean in-filling benchmark would help produce even more convincing results. Table 7: Word embeddings with small dimension have higher AR likelihood.",
    "selfcondembdiffu-8": "| Embed. dim. | 16 | 32 | 64 | 128 | 256 | 896 |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| AR NLL | 4.65 | 4.57 | 4.71 | 4.61 | 4.77 | 4.92 |\n\nTable 8: Span masking tasks improves unconditional text generation. | Max span count | 1 | 3 | 5 | 7 |  | 11 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| AR NLL | 4.99 | 4.82 | 4.82 | 4.67 | 4.48 | 4.56 |\n\n## 6 CONCLUSION\n\nWe propose SED, the first generally-capable continuous diffusion model for text generation.",
    "selfcondembdiffu-9": "SED models can perform both conditional and unconditional generation, and their performance rivals AR models while being more flexible in their use (e.g.",
    "selfcondembdiffu-10": "enabling in-filling). We demonstrate their performance and study the impact of the main design choices. Despite its limitations, this work lays the foundation for more exciting research. Promising directions include speeding up the sampling following the lessons learnt in the image domain, devising better embedding spaces for diffusion and investigating new in-filling capabilities. ## REFERENCES\n\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In NeurIPS, pp. 17981-17993, 2021. Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short. arXiv preprint arXiv:1811.02549, 2018. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer.",
    "selfcondembdiffu-11": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315-11325, 2022. Ting Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. CoRR, abs/2208.04202, 2022. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019.",
    "selfcondembdiffu-12": "URL https: //arxiv.org/abs/1901.02860. Cyprien De Masson d'Autume, Shakir Mohamed, Mihaela Rosca, and Jack Rae. Training language gans from scratch. Advances in Neural Information Processing Systems, 32, 2019. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis.",
    "selfcondembdiffu-13": "In NeurIPS, pp. 8780-8794, 2021. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models.",
    "selfcondembdiffu-14": "In EMNLP/IJCNLP (1), pp. 6111-6120. Association for Computational Linguistics, 2019. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis.",
    "selfcondembdiffu-15": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696-10706, 2022. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016.",
    "selfcondembdiffu-16": "URL https: / / arxiv.org/abs/1606.08415. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
    "selfcondembdiffu-17": "In NIPS, pp. 6626-6637, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR. OpenReview.net, 2020. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454-12465, 2021. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. arXiv preprint arXiv:2206.00364, 2022. Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.",
    "selfcondembdiffu-18": "CoRR, abs/2107.00630, 2021. Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
    "selfcondembdiffu-19": "In EMNLP (Demonstration), pp. 66-71. Association for Computational Linguistics, 2018. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. Diffusion-lm improves controllable text generation. CoRR, abs/2205.14217, 2022.",
    "selfcondembdiffu-20": "Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. In ISMIR, pp. 468-475, 2021. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 8162-8171. PMLR, 2021. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models.",
    "selfcondembdiffu-21": "In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 16784-16804. PMLR, 2022. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "selfcondembdiffu-22": "J. Mach. Learn. Res., 21:140:1-140:67, 2020. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.",
    "selfcondembdiffu-23": "arXiv preprint arXiv:2202.00512, 2022. URL https://arxiv.org/abs/2202.00512. Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and A\u00e4ron van den Oord. Step-unrolled denoising autoencoders for text generation. In ICLR. OpenReview.net, 2022. Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936, 2018. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 2256-2265. JMLR.org, 2015. Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In ICML, 2011. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. ## A MODEL ARCHITECTURE\n\nFor both the AR and the SED models, we use the same transformer (Vaswani et al., 2017) architecture, which are similar to those described in (Hoffmann et al., 2022), with relative positional encoding as described in (Dai et al., 2019) in the attention blocks, and with a $4 x$ expansion and a Gelu (Hendrycks \\& Gimpel, 2016) non-linearity in the feed-forward blocks. The architecture hyper-parameters are detailed in table 9 . Noised word embeddings, $\\boldsymbol{x} \\in \\mathbb{R}^{N \\times D}$, are first passed through a linear projection that operates on each embedding independently to get a projected embedding whose feature dimension matches the width of the transformer, $d_{\\text {model }}$. At diffusion step $t$, we compute a time embedding as a sinusoidal position embedding (Vaswani et al., 2017) of size $d_{\\text {model }}$, which is then passed into a $d_{\\text {model }} \\times d_{\\text {model }}$ linear layer and added to the projected embedding. We add a linear output projection layer $\\boldsymbol{E}^{\\prime}$ which takes the output of the transformer $y \\in \\mathbb{R}^{N \\times d_{\\text {model }}}$ and projects each element $\\left(y_{i}\\right)_{1 \\leq i \\leq N}$ back to the same size as the word embeddings. When using self-conditionning, we modify the input to the model by concatenating $\\boldsymbol{x}$ and $\\hat{\\boldsymbol{x}}_{0}$ along the feature axis before passing them to the input projection layer. Table 9: Model hyper parameters. | Model | number of layers | $d_{\\text {model }}$ | number of heads | head size |\n| :--- | :---: | :---: | :---: | :---: |\n| S | 12 | 896 | 16 | 64 |\n| L | 12 | 1536 | 16 | 128 |\n\n## B FORWARD DIFFUSION PROCESS VISUALIZATION\n\nTo support the discussion on word embeddings dimension from Section 4.4, we present a visualization of the forward diffusion process. Given starting tokens $\\boldsymbol{x}_{0}$, we project the noised tokens $\\boldsymbol{x}_{t}$ of the forward process at step $t$ to their nearest neighbor among word embeddings $\\boldsymbol{E}$ to obtain $\\boldsymbol{w}_{t}$. We then store the 128 nearest neighbors $\\mathcal{N}\\left(\\boldsymbol{w}_{0}\\right)$ of starting tokens $\\boldsymbol{w}_{0}=\\boldsymbol{x}_{0}$ and define the rank $r_{t}$ of $\\boldsymbol{w}_{t}$ at its index in $\\mathcal{N}\\left(\\boldsymbol{w}_{0}\\right)$. We display $\\boldsymbol{w}_{t}$ and highlight it in green if $r_{t}$ is close to zero (meaning $\\boldsymbol{w}_{t}$ is a close neighbor of $\\boldsymbol{w}_{0}$ ) and in increasingly red colors otherwise. We present the first 16 nearest neighbors of $\\boldsymbol{w}_{0}$ in Figure 2 and provide an illustration of the color code used for highlighting. Figure 3 shows an instance of the forward diffusion process while diffusing on embeddings of with a high dimension of 896 and Figure 4 shows diffusion on embeddings with a lower dimension of 32 . We observe that in high dimension, the noised token's closest neighbour remains the original token up until the point where any token could be its closest neighbour. The diffusion random walk does not seem to pass through the neighbourhoods of semantically-related tokens. We hypothesize that the root cause of this issue is that the embedding space is mostly empty (with only 32000 points in $\\mathbb{R}^{896}$, as $d_{\\text {embed }}=896$ ); and that embeddings are potentially concentrating in a lower-dimensional space. In contrast, in lower dimension we see meaningfully-related tokens appear as the corruption progresses ('brown' becomes 'grey', 'quick' becomes 'swift', 'over' becomes 'underneath' etc). We believe this more gradual information destruction is beneficial for the diffusion model. | 00 | the | quick | brown | fox | jumps | over | the | lazy | dog |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 01 | The | fast | gray | wolf | swings | around | The | hungry | puppy |\n| 02 | the | swift | grey | rabbit | moves | under | their | tired | pet |\n| 03 | The | speedy | pink | spider | kicks | on | a | boring | horse |\n| 04 | their | easy | yellow | deer | spins | about | our | silly | pup |\n| 05 | his | simple | orange | lion | rides | through the | lonely | dogs |  |\n| 06 | their | slow | black | bull | hikes | in | its | bored | cat |\n| 07 | your | quicker | purple | tiger | blows | after | your | impatient | animal |\n| 08 | those straightforward red | frog | pulls | within | an | weary | Dog |  |  |\n| 09 | these faster | blue | superhero knots | for | his | dumb | pig |  |  |\n| 10 | its | rapid | straw | shark | loops | across | those | noisy | breed |\n| 11 | my | instant | dark | bird | curves | off | these | crazy | pony |\n| 12 | put | nice | coloured | crow | catches | up | this | stupid | goat |\n| 13 | our | brief | redd | fur | pushes | out | my | anxious | kid |\n| 14 | this | short | white | duck | draws | behind | The | awkward | cow |\n| 15 | Our | prompt | cream | elephant | gears | against | any | miserable monkey |  |\n\nFigure 2: Nearest neighbors of tokens from the sentence \"the quick brown fox jumps over the lazy dog\"\n\n| 0 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 25 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 50 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 75 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 100 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 125 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 150 the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 175 the | quick | brown | fox | jumps | over | treatment | lazy | dog |\n| 200 thel | quick | brown | fox | flights | over | army | lazy | ole |\n| 225 stated | quick | brown | 25- | flights | over | army | lazy | Rams |\n| 250 makers | quick | $\\tan$ | 25- | flights | over | army | Philip | Rams |\n| 275 furt | quick | $\\tan$ | herical | job | vol | third | Rays | ball |\n| 300 Search | quick | abdome | char | job | vol | third | pathway | thal |\n\nFigure 3: Visualization of the forward diffusion process up to 300 steps when diffusing on embeddings with dimension 896. ## C ADDITIONAL SAMPLES\n\n| 0 | the | quick | brown | fox | jumps | over | the | lazy | dog |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 25 | the | quick | brown | fox | jumps | over | the | lazy | dog |\n| 50 the | quick | brown | fox | jumps | over | the | lazy | dog |  |\n| 75 the | quick | brown | fox | jumps | over | the | lazy | dog |  |\n| 100 the | quick | grey | fox | jumps | over | the | lazy | dog |  |\n| 125 the | Pinterest gray | abduct | jumps | underneath | changed | lazy | animal |  |  |\n| 150 vivo | swift | gray | fox | hikes | over | No | bingo | dog |  |\n| 175 rapid | eager | grey | frog | umbled | over | ired | usi | collision |  |\n| 200 rapid | ggy | boy | mosqu | uttering | underneath | proud | casters | diesel |  |\n| 225 inter | ggy | blonde | mosqu | enced | underneath | Nor | gambling | tow |  |\n| 250 inter | tend | pink | illeg | acious | elsewhere | Yo | casino | cargo |  |\n| 275 un | $!\"$ | gray | erectile | dreaming | here | Nor | casino | bay |  |\n| 300 Al | !\" | gray | thir | dreaming | here | Nor | rare |  |  |\n\nFigure 4: Visualization of the forward diffusion process up to 300 steps when diffusing on embeddings with dimension 32 .",
    "selfcondembdiffu-24": "| 1000 | The be aver is an interesting animal that lives in rivers and decides | 25 | 143 | ouses | Wednesday | 2 - | $\\$ 4$ | 7) | 13. | 2,000 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 975 | The be aver is an interesting animal that lives in rivers and 115 | 20. | 2. | 1991 | 1989, | great | thirty | 14, | Thursday | $\\$ 1,000$ |\n| 950 | The be aver is an interesting animal that lives in rivers and toward | $7 \\%$ | 1,000 | 21 | 14. | 4 | 2009, | 57 | 1987, | Indiana |\n| 925 | The be aver is an interesting animal that lives in rivers and toward | takes | to | Wednesday | 1989 | ... | honored | 57 | 2 , | until |\n| 900 | The be aver is an interesting animal that lives in rivers and 91 | 2007, | -10 | Wednesday | 2010). | P | 9), | November | 2006, | yesterday |\n| 875 | The be aver is an interesting animal that lives in rivers and their | 2007, | $90 \\%$ | 1992. | northern | remarkable | 9). | November | rambia | since |\n| 850 | The be aver is an interesting animal that lives in rivers and traveled | 2007, | 90 | 16. | 1997. | 2010, | 47 | larger | 18, | since |\n| 825 | The be aver is an interesting animal that lives in rivers and toward | 2013 | 1990. | 16. | 1997. | 2010, | 47 | because | 18 , | successfully |\n| 800 | The be aver is an interesting animal that lives in rivers and toward | (10 | 2010 | 16. | \\$4. | 15 , | 2010. | because | many | 2, |\n| 775 | The be aver is an interesting animal that lives in rivers and osl | (10 | 2010 | $3 \\%$ | Nov | needs | attractive | larger | 2, | 2, |\n| 750 | The be aver is an interesting animal that lives in rivers and traveled | $(10$ | 1982 | $3 \\%$ | 40 | needs | !!! | 2015 | sites | completely |\n| 725 | The be aver is an interesting animal that lives in rivers and 20- | $5 \\%$ | really | $3 \\%$ | 135 | needs | towards | larger | 118 | 1998, |\n| 700 | The be aver is an interesting animal that lives in rivers and 20 - | $(10$ | really | $3 \\%$ | 95 | needs | towards | larger | 118 | $2 \\%$ |\n| 675 | The be aver is an interesting animal that lives in rivers and 2018 | (10 | 1982 | $3 \\%$ | 95 | needs | 75 , | larger | medieval | 2, |\n| 650 | The be aver is an interesting animal that lives in rivers and 51 | (1980 | 17 | $3 \\%$ | 95 | needs | 75 , | 1997, | October | 2 , |\n| 625 | The be aver is an interesting animal that lives in rivers and 1990, | 13, | 1982 | 31 | \\$75 | bathroom | 75 | 1997, | historic | 2 , |\n| 600 | The be aver is an interesting animal that lives in rivers and 1990, | 2000. | literally | $3 \\%$ | 38 | needs | 75 , | 1997, | historic | 2, |\n| 575 | The be aver is an interesting animal that lives in rivers and 1990, | realize | 3 | continues | 95 | needs | towards | 1997, | historic | 2, |\n| 550 | The be aver is an interesting animal that lives in rivers and 300 | realize | 3 | Pers | additionally | needs | 75 , | 27 | waterfront | rarely |\n| 525 | The be aver is an interesting animal that lives in rivers and schools | realize | 3 | continues | additionally | needs | 75 , | 27 | waterfront | rarely |\n| 500 | The be aver is an interesting animal that lives in rivers and schools | realize | 3 | Pers | additionally | relatively | 75 , | 27 | waterfront | rarely |\n| 475 | The be aver is an interesting animal that lives in rivers and schools | 6 | 3 | ates | primarily | 6. | 75 , | 1890 | central | rarely |\n| 450 | The be aver is an interesting animal that lives in rivers and 55 | usually | 3 | ates | primarily | -28 | !!! | 1890 | -28 | rarely |\n| 425 | The be aver is an interesting animal that lives in rivers and roads | often | literally | Pers | seventh | -28 | \\$300 | 27 | coastal | rarely |\n| 400 | The be aver is an interesting animal that lives in rivers and lakes | \u30fb | literally | ates | seventh | -28 | \\$300 | modern | coastal | rarely |\n| 375 | The be aver is an interesting animal that lives in rivers and lakes | amazing | literally | ates | primarily | southern | 25 , | 1947 | coastal | rarely |\n| 350 | The be aver is an interesting animal that lives in rivers and lakes | 2015. | certainly | ates | seventh | -28 | 25 , | modern | coastal | rarely |\n| 325 | The be aver is an interesting animal that lives in rivers and lakes | often | certainly | ates | seventh | southern | 25. | 1947 | coastal | primarily |\n| 300 | The be aver is an interesting animal that lives in rivers and lakes |  | certainly | ates | seventh | southern | 25 , | modern | coastal | works |\n| 275 | The be aver is an interesting animal that lives in rivers and lakes | often | certainly | ates | primarily | southern | 25 , | modern | coastal | rarely |\n| 250 | The be aver is an interesting animal that lives in rivers and lakes | often | certainly | resides | primarily | southern | \\$300 | western | picturesque | out |\n| 225 | The be aver is an interesting animal that lives in rivers and lakes | 2015. | certainly | ates | primarily | central | 25 , | western | metropolitan | out |\n| 200 | The be aver is an interesting animal that lives in rivers and lakes | 2015.",
    "selfcondembdiffu-25": "| certainly | floods | primarily | central | provided | western | northern | disappeared |\n| 175 | The be aver is an interesting animal that lives in rivers and lakes | 2015. | It | ates | primarily | central | and | western | coastal | parts |\n| 150 | The be aver is an interesting animal that lives in rivers and lakes | . | It | ates | primarily | central | and | western | coastal | rarely |\n| 125 | The be aver is an interesting animal that lives in rivers and lakes | I | it | resides | primarily | central | and | western | coastal | parts |\n| 100 | The be aver is an interesting animal that lives in rivers and lakes | ( | it | resides | in | central | and | western | coastal | parts |\n| 75 | The be aver is an interesting animal that lives in rivers and lakes | ( | It | resides | in | central | and | western | coastal | parts |\n| 50 | The be aver is an interesting animal that lives in rivers and lakes | I | It | resides | in | central | and | western | coastal | parts |\n| 25 | The be aver is an interesting animal that lives in rivers and lakes | I | It | resides | in | central | and | western | coastal | parts |\n\nFigure 5: Reverse diffusion process of SED-L with guidance scale 2.5.",
    "selfcondembdiffu-26": "Table 10: Samples from SED-L. This course is essential for the environment in which students choose the curriculum option study at JHD. Geographical Integration is at the heart of this department. Along with the urban infrastructural innovations of the mid-1990's and social issues in the 21st Century. People within the department regularly exemplify the concept of real integration. That did trigger some rewarding words or insights to begin preparing their kid for their future. Luckily, Mikaela has been nice enough to tolerate my questions about what parents can do to help, even during her summer vacation. I went to college at Boston University. After getting my degree, I decided to make a change! I enrolled in outdoor schools. After getting my degree, I loved jet skiing, offshore fishing, and Kitesurfing. The list became growing. More importantly, I started a career by fishing at sea. Now, I can't get enough of the Pacific Ocean! The beaver is an interesting animal that lives in rivers and waterfalls across Puerto Rico. It loves kayaking, fishing, and swimming. But, you want to know what are the animals behind the beaver? A year ago in Paris, I had the opportunity to take a field trip to La Rite-en-Laurences International de France where I met David Nigel Johnson, a professor of social studies. What a great trip and what a great day! A year ago in Paris, my friends and I went on a dirt road trip to the city. I remember walking through the church, its beautiful square, narrow streets lit with memorials and passing a terrible Catholic Bishop I am used to - what a sad day... There was no evidence, only fleeting glimpses of the existence of cognitive disability. There was no luck and no solid science. It was all guesswork, the blinding prospect of pneumonia could spur imagination at the possibility of brain damage. [^0]:    ${ }^{1}$ INRIA, D\u00e9partement d'informatique, \u00c9cole normale sup\u00e9rieure, CNRS, PSL Research University\n    ${ }^{*}$ Work done while interning at DeepMind\n    ${ }^{2}$ DeepMind\n    ${ }^{3}$ Massachusetts Institute of Technology\n\n"
}