{
    "selfcondembdiffu-0": "Self-conditioned Embedding Diffusion for Text Generation\n\nRobin Strudel &Corentin Tallec &Florent Altch\u00e9 33footnotemark: 3 &Yilun Du 22footnotemark: 2 \\ANDYaroslav Ganin 33footnotemark: 3 &Arthur Mensch 33footnotemark: 3 &Will Grathwohl 33footnotemark: 3 &Nikolay Savinov 33footnotemark: 3 &Sander Dieleman 33footnotemark: 3 &Laurent Sifre 33footnotemark: 3 &R\u00e9mi Leblond 33footnotemark: 3 INRIA, D\u00e9partement d\u2019informatique, \u00c9cole normale sup\u00e9rieure, CNRS, PSL Research UniversityWork done while interning at DeepMindDeepMindMassachusetts Institute of Technology\n\nAbstract\n\nCan continuous diffusion models bring the same performance breakthrough on natural language they did for image generation? To circumvent the discrete nature of text data, we can simply project tokens in a continuous space of embeddings, as is standard in language modeling. We propose Self-conditioned Embedding Diffusion (Sed), a continuous diffusion mechanism that operates on token embeddings and allows to learn flexible and scalable diffusion models for both conditional and unconditional text generation. Through qualitative and quantitative evaluation, we show that our text diffusion models generate samples comparable with those produced by standard autoregressive language models \u2014 while being in theory more efficient on accelerator hardware at inference time. Our work paves the way for scaling up diffusion models for text, similarly to autoregressive models, and for improving performance with recent refinements to continuous diffusion. 1 Introduction\n\nContinuous diffusion models (Sohl-Dickstein et al., 2015) have taken the world of image generation by storm, advancing the state of the art further than ever before (Rombach et al., 2021; Ramesh et al., 2022). Can the same framework encounter as much success on the text modality? Diffusion for language is indeed an attractive prospect. Compared to autoregressive (AR) models (Bengio et al., 2000; Sutskever et al., 2011; Austin et al., 2021; Hoffmann et al., 2022), diffusion models can predict all tokens in a sequence at once. This allows for bidirectional, rather than causal attention\u2014increasing interactions between tokens, potentially leading to more coherent samples. Diffusion models can make a better usage of hardware accelerators during inference than AR models, since computations are parallelizable over the sequence axis. Yet AR models remain the mainstream approach for modelling text. A major obstacle to text diffusion is that diffusion processes typically operate in continuous space. While this naturally handle images, text is inherently discrete. Consequently, most previous attempts to apply diffusion to text have focused on discrete diffusion-like approaches. These methods do not benefit from the refinements made to continuous diffusion in the image domain. Crucially, they cannot make use of guidance (Dhariwal & Nichol, 2021), which drastically improves diffusion models sample quality. We address this gap by making a simple observation: language models operate mostly in continuous space, with discrete tokens only as inputs and outputs. A natural idea is then to conduct diffusion directly in a continuous token embedding space. For simplicity, we use a fixed embedding space, either random or stemming from a trained language model. Combined with the \u201cself-conditioning\u201d (Chen et al., 2022) refinement, this forms the basis of the method we propose, Self-conditioned Embedding Diffusion (Sed). Sed models rival mainstream AR models in both conditional and unconditional text generation. We make the following contributions:\n\n\u2022\n\nIn section 3, we introduce Sed, the first continuous diffusion approach for text with good scaling properties (testing models up to 420M parameters). We analyze several continuous text diffusion settings, and identify self-conditioning and diffusion on small fixed embeddings as key factors to make continuous text diffusion work. \u2022\n\nIn section 4, we apply classifier-free guidance (Ho & Salimans, 2022) to text data\u2014an original achievement. We show that Sed can rival AR models on generic language tasks, for similar models sizes. Sed samples achieve a better likelihood-entropy trade-off compared to these models, and are deemed comparable (if slightly worse) by human raters. 2 Related work\n\nWe provide an overview of diffusion models with a focus on modeling discrete data, as well as AR models and sample-based metrics for evaluating text generation. Continuous diffusion on continuous image data. Continuous diffusion has recently established itself as the method of choice for modeling continuous data such as images. While our main focus in this paper is on discrete data, we review some key works in continuous data modeling as this literature was the major source of inspiration for Sed. The first continuous diffusion formulation was introduced in the seminal work by Sohl-Dickstein et al.",
    "selfcondembdiffu-1": "(2015). Ho et al. (2020) improved and simplified this formulation, relating it to denoising score matching, and creating a new method called DDPM. Nichol & Dhariwal (2021) further improved upon DDPM, showcasing impressive diffusion results compared to GANs. Rombach et al. (2021, Stable Diffusion) introduced diffusion in latent space. Conceptually similar to Sed, it was specifically targeted at image modeling. Classifier-free guidance was proposed by Ho & Salimans (2022) as a mean to improve image fidelity at the cost of reduced diversity. GLIDE (Nichol et al., 2022) scaled up the ideas of guided diffusion, while DALL-E 2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) are the latest, most advanced image generation systems to date, combining most of the improvements proposed in previous works. Discrete diffusion on discrete data. One cannot simply reuse the methods that are successful on continuous image data in the discrete text domain. A number of bespoke methods have been explored instead, forming the family of discrete diffusion approaches. In discrete diffusion, the data is corrupted by switching from one discrete value to another.",
    "selfcondembdiffu-2": "This was first proposed in the seminal work by Sohl-Dickstein et al. (2015), where it was tested on simplistic binary heartbeat data. It was extended to multinomial text modeling (Hoogeboom et al., 2021) and further scaled up in the D3PM work (Austin et al., 2021). Most recently, a similar discrete diffusion approach was applied to image modeling in VQ-Diffusion (Gu et al., 2022). In parallel, a few diffusion-like approaches were proposed in the denoising autoencoders literature. CMLM (Ghazvininejad et al., 2019) tackled machine translation. SUNDAE (Savinov et al., 2022) was the first non-AR method to show strong results both in machine translation and unconditional text generation. MaskGIT (Chang et al., 2022) demonstrated excellent results in modeling VQ-discretized images. These approaches rely on training models to predict masked tokens from their context, and iterating this reconstruction step multiple times at sampling time. Despite those positive developments, the samples from discrete diffusion methods for text modeling remains less coherent than those produced by AR methods. Continuous diffusion on discrete data. Fewer works try to tackle diffusion on discrete data from the same angle as Sed \u2013 starting by turning the data into continuous representations before modeling it with continuous diffusion formulations. Mittal et al. (2021) used a VAE to generate such representations for discrete music modeling, with exciting results. Closest to Sed, Diffusion-LM (Li et al., 2022) trains a token embedding together with the diffusion model itself. Diffusion-LM meets success on specific language applications, in low data regime and on constrained, very formatted textual data. Most recently, Analog Bits (Chen et al., 2022) introduced self-conditioning, closely related to step-unrolls in SUNDAE (Savinov et al., 2022), together with bit-level modeling to improve the generation of discretized images. While the qualitative results of those continuous methods on text modeling show promise, they have not been shown to scale to large realistic text datasets like C4 (Raffel et al., 2020) yet, or to compare with AR approaches on generic language tasks. Auto-regressive modelling on discrete data. AR models remain the method of choice for modeling discrete data. In combination with neural networks, they were first explored by Bengio et al. (2000) and later combined with RNNs (Sutskever et al., 2011). Their breakthrough moment came with the advent of the Transformer architecture, introduced by Vaswani et al. (2017) for machine translation. Even more impressive results were shown with GPT-3 (Brown et al., 2020), which trained a large AR language model unconditionally, and used few-shot prompting to adapt it to new tasks. A few works later improved upon the results of GPT-3, including Hoffmann et al.",
    "selfcondembdiffu-3": "(2022). Sample-based evaluation of text generative models. There are traditionally two classes of metrics for generative modeling: likelihood-based and sample-based. While the likelihood-based way is mathematically appealing, its usefulness for measuring progress is reduced by the fact that not all models readily provide likelihood computation. Just like the sampled-based FID metric was important for driving the progress of diffusion in image modeling, there is a need for a sample-based metric which would be universally accepted for text modeling. Caccia et al. (2018) investigated fidelity/variance metrics for evaluating text GANs. Semeniuta et al. (2018) suggested using FID for texts. De Masson d\u2019Autume et al. (2019) later used those previously proposed metrics to iterate on ScratchGAN but did not provide conclusive guidance on which metric a practitioner should choose \u2013 essentially finding serious vulnerabilities in all investigated metrics. We opted for a middle ground, reporting both sample likelihood according to a strong AR model and human preferences. 3 Method\n\nIn this section, we outline the different components of Sed: continuous diffusion in the space of token embeddings and self-conditioning, which form the basis of our approach for unconditional text generation; span masking and guided diffusion to enable conditional generation. 3.1 Diffusion models for unconditional text generation\n\nDiffusion models in continuous space. We consider diffusion models as introduced by Sohl-Dickstein et al. (2015) and improved by Ho et al. (2020). A diffusion model aims at modelling a data distribution by estimating a sequence of latent variables , \u2026, of the same dimensionality as the data . Starting from , the latent variables are generated with a Markov chain called the forward process: . It is defined by gradually interpolating the iterate with Gaussian noise according to noise levels defined by a schedule :\n\n\ud835\udc99 t \u223c q ( \u22c5 | \ud835\udc99 t \u2212 1 , t ) = \ud835\udca9 ( 1 \u2212 \u03b2 t \ud835\udc99 t \u2212 1 , \u03b2 t \ud835\udc70 ) . {\\bm{x}}_{t}\\sim q(\\cdotp|\\,{\\bm{x}}_{t-1},t)=\\mathcal{N}(\\sqrt{1-\\beta_{t}}{\\bm{x}}_{t-1},\\beta_{t}{\\bm{I}}). (1)\n\nThis parametrization gives us a closed form to sample for any arbitrary , given :\n\n\ud835\udc99 t = \u03b1 t \u200b \ud835\udc99 t \u2212 1 + 1 \u2212 \u03b1 t \u200b \u03f5 t = \u03b1 \u00af t \u200b \ud835\udc99 0 + 1 \u2212 \u03b1 \u00af t \u200b \u03f5 , subscript \ud835\udc99 \ud835\udc61 subscript \ud835\udefc \ud835\udc61 subscript \ud835\udc99 \ud835\udc61 1 1 subscript \ud835\udefc \ud835\udc61 subscript italic-\u03f5 \ud835\udc61 subscript \u00af \ud835\udefc \ud835\udc61 subscript \ud835\udc99 0 1 subscript \u00af \ud835\udefc \ud835\udc61 italic-\u03f5 {\\bm{x}}_{t}=\\sqrt{\\alpha_{t}}{\\bm{x}}_{t-1}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t}=\\sqrt{\\overline{\\alpha}_{t}}{\\bm{x}}_{0}+\\sqrt{1-\\overline{\\alpha}_{t}}\\epsilon, (2)\n\nwhere , , and . We define our generative model by approximately inverting the diffusion process of Eq. 1 to obtain a reverse process. The reverse process starts from and is defined as a Markov chain with learned Gaussian transitions (parameterized by , the weights of a neural network): . We train a neural network to predict an estimate of the data and approximate the reverse process by using the following parametrization, with learnable means but fixed variances, and a fixed schedule :\n\n\ud835\udf41 \u03b8 ( \ud835\udc99 t , t ) = \u03b1 \u00af t \u2212 1 \u200b \u03b2 t 1 \u2212 \u03b1 \u00af t \ud835\udc99 ^ 0 ( \ud835\udc99 t , t , \u03b8 ) + \u03b1 t \u200b ( 1 \u2212 \u03b1 \u00af t \u2212 1 ) 1 \u2212 \u03b1 \u00af t \ud835\udc99 t , \u03c3 ( t ) 2 = 1 \u2212 \u03b1 \u00af t \u2212 1 1 \u2212 \u03b1 \u00af t \u03b2 t \u22c5 {\\bm{\\mu}}_{\\theta}({\\bm{x}}_{t},t)=\\frac{\\sqrt{\\overline{\\alpha}_{t-1}}\\beta_{t}}{1-\\overline{\\alpha}_{t}}\\hat{{\\bm{x}}}_{0}({\\bm{x}}_{t},t,\\theta)+\\frac{\\sqrt{\\alpha_{t}}(1-\\overline{\\alpha}_{t-1})}{1-\\overline{\\alpha}_{t}}{\\bm{x}}_{t},\\qquad\\sigma(t)^{2}=\\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_{t}}\\beta_{t}\\cdot (3)\n\nWhile there exists a tractable variational lower-bound (VLB) on , Ho et al. (2020) showed that better results are obtained by optimizing a simplified objective that re-weights the terms in the VLB. We follow this approach, which simplifies the loss to a sum of mean-squared errors between the ground truth data and its estimates :\n\n\u2112 diffusion = \ud835\udd3c \ud835\udc99 0 \u223c q \u200b ( \ud835\udc99 0 ) , t \u223c \ud835\udcb0 \u200b ( 1 , T ) \u2225 \ud835\udc99 0 \u2212 \ud835\udc99 ^ 0 ( \ud835\udc99 t , t , \u03b8 ) \u2225 2 \u22c5 \\mathcal{L}_{\\text{diffusion}}=\\mathbb{E}_{{\\bm{x}}_{0}\\sim q({\\bm{x}}_{0}),\\,t\\sim\\mathcal{U}(1,T)}\\|{\\bm{x}}_{0}-\\hat{{\\bm{x}}}_{0}({\\bm{x}}_{t},t,\\theta)\\|^{2}\\cdot (4)\n\nThough this framework works out of the box on images, which are close to continuous, we cannot apply it directly to the discrete tokens of the text modality. To resolve this issue, we perform continuous diffusion in a continuous space in which we embed text tokens. Diffusion on word embeddings. We consider textual data , where each is a one-hot representation in of a discrete token in . Each token has an associated embedding , with fixed norm to match the norm of a random gaussian sample in dimension used to noise clean data. We denote by the matrix of all embeddings. We define our diffusion process in embedding space, rather than in token space. To that end, we define a forward discrete-to-continuous step , where is a constant scale factor with a similar order of magnitude as . Conversely, we define a reverse continuous-to-discrete step , where is a learnable readout matrix initialized to and is the softmax probability of token with logits . To train the readout step, we add a reconstruction loss to during training. Conveniently, it naturally arises when deriving the VLB of with this discretization step (Li et al., 2022), introducing a simple cross-entropy loss to maximise :\n\n\u2112 recon = \ud835\udd3c \ud835\udc98 \u223c \ud835\udc9f , \ud835\udc99 0 \u223c q \ud835\udc7d \u200b ( \ud835\udc98 ) \u200b [ \u2212 log \u2061 p \ud835\udc79 \u200b ( \ud835\udc98 | \ud835\udc99 0 ) ] , with \u2112 total = \u2112 diffusion + \u2112 recon . formulae-sequence subscript \u2112 recon subscript \ud835\udd3c formulae-sequence similar-to \ud835\udc98 \ud835\udc9f similar-to subscript \ud835\udc99 0 subscript \ud835\udc5e \ud835\udc7d \ud835\udc98 delimited-[] subscript \ud835\udc5d \ud835\udc79 conditional \ud835\udc98 subscript \ud835\udc99 0 with subscript \u2112 total subscript \u2112 diffusion subscript \u2112 recon \\mathcal{L}_{\\text{recon}}=\\mathbb{E}_{{\\bm{w}}\\sim\\mathcal{D},{\\bm{x}}_{0}\\sim q_{{\\bm{V}}}({\\bm{w}})}[-\\log p_{\\bm{R}}({\\bm{w}}|{\\bm{x}}_{0})],\\qquad\\text{with}\\qquad\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{diffusion}}+\\mathcal{L}_{\\text{recon}}. (5)\n\nContrary to what is done in Li et al. (2022), we do not learn the embedding matrix , as we identified that it was empirically unstable and could lead to drops in unigram entropy. The reconstruction loss therefore only depends on the trainable readout weights . At sampling time, we run the reverse process for steps, ultimately yielding a continuous embedding of size . We multiply it by to obtain logits in , and then use the index of the maximum component to convert it to a token , with . This entails running full forward passes which is quite expensive compared to cached AR sampling; however each forward pass computes all timesteps at once which is naturally parallelisable. Further, we hope to benefit from many diffusion sampling improvements to get down to low double-digits. Self-conditioning (Chen et al., 2022). In standard diffusion sampling, at each timestep the denoising network generates an estimate of given only as input. Self-conditioning progressively refines estimates by passing the estimate obtained at the previous sampling step as input to the denoising network; the self-conditioned estimate is then defined as , and sets the diffusion direction. In practice conditioning is performed by concatenating and on the feature axis. To approximate the inference behavior at train time while remaining computationally efficient, we compute a first estimate with the self-conditioning set to zero, then perform a second forward pass using a stop gradient on to obtain . The denoising network is then optimized using the output from the two forward passes in order to estimate accurately with and without self-conditioning. Equipped with these 3 components we can train models to generate text, though only unconditionally. To add conditional generation to our system\u2019s capabilities, we use two additional methods. 3.2 Span masking and guidance for conditional text generation\n\nBy design diffusion models for text generation are flexible and can handle a wide variety of infilling tasks. This is a key advantage over the predominant auto-regressive language models that typically generate text in a left-to-right fashion. Span masking. We train our model on a rich set of infilling tasks with the following method. We split between two set of tokens, diffusion tokens over which we apply diffusion and optimize the diffusion loss from Eq. 4, and conditioning tokens that remain fixed. Conditioning tokens are defined by a binary conditioning mask set to one on conditioning positions and zero on positions to be infilled. We sample conditioning mask randomly as follows. Given a sequence of length and a maximum number of spans , we sample a number of spans uniformly in . Span starting positions are defined by integers sampled uniformly without replacement and sorted in increasing order to satisfy . The tuple partitions the sequence of tokens in spans satisfying . The conditioning mask is defined using even spans for conditioning and odd spans for infilling, and then is flipped with a probability. The case corresponds to unconditional generation; we then set to 0 everywhere. This span masking strategy defines a collection of text generation tasks with a large variety of conditioning which on average evenly splits the sequence between conditioning and infilling spans. It enables conditional generation, and opens the door for additional diffusion improvements. Guided diffusion. Guidance (Dhariwal & Nichol, 2021) often improves the sample quality of conditional diffusion models. We use classifier-free guidance (Ho & Salimans, 2022), which alleviates the need for a separately-trained guide model. In the conditional case, our estimator is now a function , where are fixed conditioning tokens. During training, with fixed probability the conditioning tokens used in the estimator are dropped and set to a null label equal to zero. During sampling, the model prediction is extrapolated in the direction of and away from as follows:\n\n\ud835\udc99 ~ 0 , s t = \ud835\udc99 ^ 0 \u200b ( \ud835\udc99 t , 0 , 0 , t , \u03b8 ) + s \u22c5 ( \ud835\udc99 ^ 0 \u200b ( \ud835\udc99 t , \ud835\udc84 , \ud835\udc99 ~ 0 t + 1 , t , \u03b8 ) \u2212 \ud835\udc99 ^ 0 \u200b ( \ud835\udc99 t , 0 , 0 , t , \u03b8 ) ) , superscript subscript ~ \ud835\udc99 0 \ud835\udc60 \ud835\udc61 subscript ^ \ud835\udc99 0 subscript \ud835\udc99 \ud835\udc61 0 0 \ud835\udc61 \ud835\udf03 \u22c5 \ud835\udc60 subscript ^ \ud835\udc99 0 subscript \ud835\udc99 \ud835\udc61 \ud835\udc84 superscript subscript ~ \ud835\udc99 0 \ud835\udc61 1 \ud835\udc61 \ud835\udf03 subscript ^ \ud835\udc99 0 subscript \ud835\udc99 \ud835\udc61 0 0 \ud835\udc61 \ud835\udf03 \\tilde{{\\bm{x}}}_{0,s}^{t}=\\hat{{\\bm{x}}}_{0}\\big{(}{\\bm{x}}_{t},0,0,t,\\theta\\big{)}+s\\,\\cdot\\,\\Big{(}\\hat{{\\bm{x}}}_{0}\\big{(}{\\bm{x}}_{t},{\\bm{c}},\\tilde{{\\bm{x}}}_{0}^{t+1},t,\\theta\\big{)}-\\hat{{\\bm{x}}}_{0}\\big{(}{\\bm{x}}_{t},0,0,t,\\theta\\big{)}\\Big{)}, (6)\n\nwhere is the guidance scale.",
    "selfcondembdiffu-4": "Remark that we jointly drop conditioning and the self-conditioning , concretely setting both values to zero. Classifier-free guidance allows leveraging both the unconditional and conditional abilities of a model to improve its conditional generations. 4 Experiments\n\n4.1 Training details\n\nWe train all our models on the C4 dataset (Raffel et al., 2020), using a SentencePiece tokenizer (Kudo & Richardson, 2018) composed of 32000 words. We use a non-causal transformer model (Vaswani et al., 2017) as our diffusion model (see Appendix A for details). Sed models are trained with sequence length 256, while for ablations models are trained with sequence length 128. We insert uniformly, i.e. not necessarily at the end of the sequence, 10% of padding tokens in the training set to allow Sed models to generate samples of varying size and provide more flexibility. To generate word embeddings, we train a BERT model of fixed size (m parameters) and feature dimension . The diffusion space is defined by the initial lookup table of this BERT model. We bottleneck the dimension of the word embeddings and add a linear projection layer from to at the beginning of the model. We found this helped diffusion (see section 4.4). Sed models are trained with a cosine noise schedule (Dhariwal & Nichol, 2021), with , and . We use batches of 65.536 tokens, thus for sequence length 256 the batch size is set to 256. We use a maximum span count of 5 for all runs except for its specific ablation. We train Sed models at two different scales: Sed-S (m parameters, training steps) and Sed-L (m, steps). Their detailed architectures can be found in Appendix A. 4.2 Validation\n\nWhile optimizing the perplexity of AR models for text leads to improved language models, directly optimizing the ELBO of diffusion models for images does not correlate strongly with sample quality as observed by Nichol & Dhariwal (2021); Kingma et al. (2021); Ho & Salimans (2022). For images, the sample based metric FID (Heusel et al., 2017) has been introduced as a measure of sample quality and is now widely adopted. Similarly, we need a sample-based metric for text generation that is reliable and allows comparison between a large variety of generative models. To provide a fair comparison to AR models, we rely on three metrics. The first metric measures how likely the samples produced by a model are according to an AR language model with 70B parameters, trained on 1.4B tokens (Hoffmann et al., 2022); we denote this metric AR NLL for auto-regressive negative log-likelihood. It provides a continuous measure of sample quality that has proven useful when combined with a measure of sample diversity, e.g. in the development of nucleus sampling (Holtzman et al., 2020) for improved AR model decoding. To measure diversity we rely on a second metric, the unigram entropy of samples, which helps balance the AR NLL that can be gamed by unnatural repetitive samples. For both these metrics, our target is the score of the validation set data. Deviating from the data unigram entropy in particular is a sign of degenerate modeling. Though this initial combination has provided us with a reliable signal to iterate over our model design, it remains imperfect; it too can be gamed, though it is harder to do so. To address this limitation, we also report human preferences. We presented 6 colleagues with 20 pairs of samples for each comparison, asking them to pick the best one. For all three metrics, we report results on two tasks: unconditional language modeling and suffix in-filling, the later a heavily conditioned task. 4.3 Results\n\nSamples. We present samples generated with our Sed models in Table 10. We use a single model to perform a wide variety of text generation tasks, such as unconditional generation, filling-in-the-middle or filling several spans of text. We show strong performance in the unconditional case, with samples that are syntactically correct and stay coherent on long sequences. In the conditioned case, Sed models are able to infill spans with coherent transitions and links to the conditioning but also exhibit a rich diversity. By design, Sed yields flexible bi-directional masking models that can perform text generation on a diverse set of conditioned task. To compare Sed with AR baselines we next restrict conditioning to a prefix and consider a task of suffix in-filling. Comparison to AR models. To assess the generation ability of Sed, we compare against AR baselines of similar capacity and trained following optimal scaling laws from Hoffmann et al. (2022) on suffix in-filling. We sample a batch of sequences from C4 and use the first 128 tokens as conditioning given to the model to generate a suffix of 128 tokens. Figure 1 reports AR NLL and unigram entropy of the generated suffixes for AR and Sed models. As a reference point, we compute the AR NLL and unigram entropy of the ground truth C4 suffixes and report it on the plot. Several methods can be used to improve sampling quality at the cost of samples diversity; we use nucleus sampling (Holtzman et al., 2020) for AR models and guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) for Sed models. We show the impact of guidance on samples quality in Table 3. To our knowledge, we are the first to show sample quality improvement when using guidance for text generation. As shown in Figure 1, both Sed-S and Sed-L perform strongly when compared against AR baselines \u2013 even though we report a metric favoring AR models on a task AR models have been designed to optimize. Similar to nucleus sampling for AR models, guidance has a strong positive impact on sample quality that is both observed quantitatively with improved AR NLL in Figure 1 and qualitatively in Table 3. We observe that using a top- nucleus sampling below for AR models or a guidance scale above for Sed models leads to samples exhibiting a lot of repetitions, a degenerate case reflected by a lower entropy of samples even though sample AR NLL improves. Our human preference scores temper our observations in Table 4. They show that our NLL and entropy metrics do not tell the whole story, as humans still prefer AR models at equivalent size. While Sed-L performs slightly worse than AR-L (38% preference in suffix in-filling, 44% on unconditional generation), its scores remain comparable. Sed-L is roughly on par with AR-S. Finally, we compare Sed and AR models\u2019 qualitative examples with short prompts in Table 2. 4.4 Ablations\n\nSelf-conditioning and embedding pretraining. Results from Table 5 and samples from Table 6 show the influence of both the diffusion space and self-conditioning. AR NLL decreases very significantly when using self-conditioning, regardless of the rest of the setup. Diffusing at the bit-level (Chen et al., 2022) yields very high NLLs. While using random embeddings performs markedly better, using pretrained embeddings results in further improved numbers. Samples from Table 6 highlight that models trained on random word embeddings exhibit topic modelling abilities with the co-occurrence of words like child and mother even though the paragraph remains globally incoherent and meaningless tokens like gluc are generated. Self-conditioning dramatically improves sample quality; the diffusion model gets the low-level structure right and generates syntactically correct sentences, even though the global text is not intelligible. Combining self-conditioning and pretrained embeddings leads to globally coherent paragraphs that stay on topic with proper sentence structure. Embedding dimension. An important design choice for SED is the word embeddings space. We study the influence of pretrained embedding size in Table 8. Surprisingly, there is a threshold after which performance degrades when increasing the dimension of embeddings. We visualize the forward process for different embedding sizes by displaying the nearest neighbor of a noised token while running the forward process. In high dimension we observe that the nearest neighbor of a noised token remains the starting token itself until it switches to a completely random, unrelated token. In low dimension, we often observe that the closest neighbor of a noised token goes through several semantically related tokens (nearest neighbor of the starting token) before ultimately becoming random. We hypothesize that the random walks defined by diffusion are more likely to drift towards neighbors of the starting token in low dimension. As a result, when diffusing in low dimension information is destroyed in a more semantically meaningful fashion, which leads to an easier learning problem for the denoising function. Number of spans. In order to enable in-filling, we train the model not only to do unconditional generation but also to conditionally fill spans of tokens. For each data point we sample a span number uniformly at random and span delimiters to generate the span mask. Picking the maximum allowable number of spans has a significant effect on model performance, as we can see in Table 8. Somewhat counter-intuitively, adding span masking improves even unconditional generation NLLs. It also appears that using a relatively high maximum span number is optimal. We hypothesize that this results in a varied mix of task difficulty at training time, between \u201deasy\u201d, very conditioned problems on the one hand and \u201dharder\u201d, unconditional ones on the other. Scaling. We show encouraging results when scaling from Sed-S (150m) to Sed-L (m). We train both models on sequences of tokens and report a AR NLL of for Sed-S compared to for Sed-L. This improvement translates to improved sample quality, as is confirmed by our human preference scores, which are much higher for the larger model (63%, see Table 4). 5 Limitations\n\nWhile our results are promising and show that continuous diffusion for text can be an exciting alternative to AR models, the current approach does present some significant limitations. First, much more could be done in terms of model tuning, including scaling to much bigger models to better understand Sed\u2019s limits, and to be able to compare it with state-of-the-art AR models. Our training regime in particular would certainly benefit from more hyperparameter optimisation. Second, one compelling reason we chose to explore continuous diffusion for text is to leverage the improvements produced by the literature on image generation. While we have ported some (e.g. self-conditioning), a lot more remains unexplored. The most obvious example is the sampling process itself, where the number of required steps has been considerably reduced for images (e.g. Karras et al. (2022) goes from 1000 to 35, and Salimans & Ho (2022) all the way down to 4 on simple images). Our current sampling is very inefficient, and this direction is one of the first improvements to make over Sed. Third, Sed crucially relies on diffusing in a pretrained embedding space. This means relying on a second model, and using embeddings that may not be optimal for diffusion. Ideally, we\u2019d train the full model end-to-end, which could yield even better results. While Li et al. (2022) found some success with this approach, it was in a specific setting at a small scale; in practice we found it difficult to avoid competition between the diffusion and reconstruction loss. Finally, our work would benefit from improved metrics in the experimental section. Because the current state of the art involves AR models, the field lacks established benchmarks for tasks diffusion models are potentially better suited for, such as text in-filling. We opted for a reasonable mix, evaluating the negative log-likelihood of generated samples according to a very strong AR model as well as their token entropy and complementing it with a human evaluation. However, both NLL and unigram entropy are gameable (e.g. AR models assign very low NLL to repetitive snippets, and long enough repetitions can fool even entropy). Further, our NLL is inherently tied to its AR model and could thus be providing an unfair advantage to AR models. All told, we still found both metrics quite useful for measuring research progress, and our human evaluation confirmed our results. Moving forward, defining a clean in-filling benchmark would help produce even more convincing results. 6 Conclusion\n\nWe propose Sed, the first generally-capable continuous diffusion model for text generation. Sed models can perform both conditional and unconditional generation, and their performance rivals AR models while being more flexible in their use (e.g.",
    "selfcondembdiffu-5": "enabling in-filling). We demonstrate their performance and study the impact of the main design choices. Despite its limitations, this work lays the foundation for more exciting research. Promising directions include speeding up the sampling following the lessons learnt in the image domain, devising better embedding spaces for diffusion and investigating new in-filling capabilities.",
    "selfcondembdiffu-6": "References\n\nAustin et al. (2021) Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In NeurIPS, pp. 17981\u201317993, 2021. Bengio et al. (2000) Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000. Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Caccia et al. (2018) Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short.",
    "selfcondembdiffu-7": "arXiv preprint arXiv:1811.02549, 2018. Chang et al. (2022) Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315\u201311325, 2022. Chen et al. (2022) Ting Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning.",
    "selfcondembdiffu-8": "CoRR, abs/2208.04202, 2022. Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019.",
    "selfcondembdiffu-9": "URL https://arxiv.org/abs/1901.02860. De Masson d\u2019Autume et al. (2019) Cyprien De Masson d\u2019Autume, Shakir Mohamed, Mihaela Rosca, and Jack Rae. Training language gans from scratch. Advances in Neural Information Processing Systems, 32, 2019. Dhariwal & Nichol (2021) Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, pp. 8780\u20138794, 2021. Ghazvininejad et al. (2019) Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models.",
    "selfcondembdiffu-10": "In EMNLP/IJCNLP (1), pp. 6111\u20136120. Association for Computational Linguistics, 2019. Gu et al. (2022) Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis.",
    "selfcondembdiffu-11": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696\u201310706, 2022. Hendrycks & Gimpel (2016) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016. URL https://arxiv.org/abs/1606.08415.",
    "selfcondembdiffu-12": "Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
    "selfcondembdiffu-13": "In NIPS, pp. 6626\u20136637, 2017. Ho & Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance.",
    "selfcondembdiffu-14": "CoRR, abs/2207.12598, 2022. Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR. OpenReview.net, 2020. Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions.",
    "selfcondembdiffu-15": "Advances in Neural Information Processing Systems, 34:12454\u201312465, 2021. Karras et al. (2022) Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022. Kingma et al. (2021) Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.",
    "selfcondembdiffu-16": "CoRR, abs/2107.00630, 2021. Kudo & Richardson (2018) Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
    "selfcondembdiffu-17": "In EMNLP (Demonstration), pp. 66\u201371. Association for Computational Linguistics, 2018. Li et al. (2022) Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. Diffusion-lm improves controllable text generation. CoRR, abs/2205.14217, 2022. Mittal et al. (2021) Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. In ISMIR, pp. 468\u2013475, 2021. Nichol & Dhariwal (2021) Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 8162\u20138171. PMLR, 2021. Nichol et al. (2022) Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models.",
    "selfcondembdiffu-18": "In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 16784\u201316804. PMLR, 2022. Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "selfcondembdiffu-19": "J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020. Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models.",
    "selfcondembdiffu-20": "CoRR, abs/2112.10752, 2021. Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487, 2022. Salimans & Ho (2022) Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.",
    "selfcondembdiffu-21": "arXiv preprint arXiv:2202.00512, 2022. URL https://arxiv.org/abs/2202.00512. Savinov et al. (2022) Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and A\u00e4ron van den Oord. Step-unrolled denoising autoencoders for text generation. In ICLR. OpenReview.net, 2022. Semeniuta et al. (2018) Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936, 2018. Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 2256\u20132265. JMLR.org, 2015. Sutskever et al. (2011) Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In ICML, 2011. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Appendix A Model architecture\n\nFor both the AR and the Sed models, we use the same transformer (Vaswani et al., 2017) architecture, which are similar to those described in (Hoffmann et al., 2022), with relative positional encoding as described in (Dai et al., 2019) in the attention blocks, and with a 4x expansion and a Gelu (Hendrycks & Gimpel, 2016) non-linearity in the feed-forward blocks.",
    "selfcondembdiffu-22": "The architecture hyper-parameters are detailed in table 9. Noised word embeddings, , are first passed through a linear projection that operates on each embedding independently to get a projected embedding whose feature dimension matches the width of the transformer, . At diffusion step , we compute a time embedding as a sinusoidal position embedding (Vaswani et al., 2017) of size , which is then passed into a linear layer and added to the projected embedding. We add a linear output projection layer which takes the output of the transformer and projects each element back to the same size as the word embeddings. When using self-conditionning, we modify the input to the model by concatenating and along the feature axis before passing them to the input projection layer. Appendix B Forward diffusion process visualization\n\nTo support the discussion on word embeddings dimension from Section 4.4, we present a visualization of the forward diffusion process. Given starting tokens , we project the noised tokens of the forward process at step to their nearest neighbor among word embeddings to obtain . We then store the 128 nearest neighbors of starting tokens and define the rank of at its index in . We display and highlight it in green if is close to zero (meaning is a close neighbor of ) and in increasingly red colors otherwise. We present the first 16 nearest neighbors of in Figure 2 and provide an illustration of the color code used for highlighting. Figure 3 shows an instance of the forward diffusion process while diffusing on embeddings of with a high dimension of 896 and Figure 4 shows diffusion on embeddings with a lower dimension of 32. We observe that in high dimension, the noised token\u2019s closest neighbour remains the original token up until the point where any token could be its closest neighbour. The diffusion random walk does not seem to pass through the neighbourhoods of semantically-related tokens. We hypothesize that the root cause of this issue is that the embedding space is mostly empty (with only 32000 points in , as ); and that embeddings are potentially concentrating in a lower-dimensional space. In contrast, in lower dimension we see meaningfully-related tokens appear as the corruption progresses (\u2018brown\u2019 becomes \u2018grey\u2019, \u2018quick\u2019 becomes \u2018swift\u2019, \u2018over\u2019 becomes \u2018underneath\u2019 etc).",
    "selfcondembdiffu-23": "We believe this more gradual information destruction is beneficial for the diffusion model. Appendix C Additional samples\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Mar 14 06:34:45 2024 by LaTeXML"
}