{
    "latentattn-0": "# Latent Attention for Linear Time Transformers \n\nRares Dolga ${ }^{\\dagger}$ Marius Cobzarenco ${ }^{\\ddagger}$ David Barber ${ }^{\\dagger \\ddagger}$\n\n\n#### Abstract\n\nThe time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence.",
    "latentattn-1": "We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our \"Latte Transformer\" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention. ## 1. Introduction\n\nTransformers (Vaswani et al., 2017) are extensively used in sequence modelling, with widespread applications in natural language processing (NLP) (Devlin et al., 2018; Radford et al., 2019; Touvron et al., 2023), and computer vision (Khan et al., 2022). The transformer is based on an attention mechanism that compares each element of a sequence with every other. Specifically, the attention mechanism takes an input sequence of vectors $x_{1}, \\ldots, x_{T}$; $x_{s} \\in \\mathbb{R}^{D}$ and transforms this to a new sequence\n\n$$\n\\tilde{x}_{t}=\\sum_{s=1}^{T} a_{t s} v_{s}\n$$\n\nwhere $a_{t s}$ are the attention weights and $v_{t}=W_{v} x_{t}$ is a linear transformation of the input $x_{t}$. The standard attention\n\n[^0]weights are defined using ${ }^{1}$\n$$\na_{t s}=\\frac{\\exp \\left(q_{t}^{\\top} k_{s}\\right)}{\\sum_{s=1}^{T} \\exp \\left(q_{t}^{\\top} k_{s}\\right)}\n$$\nwhere $k_{t}=W_{k} x_{t}, q_{t}=W_{q} x_{t}$. This ensures normalisation $\\sum_{s=1}^{T} a_{t s}=1$. Here $W_{k}, W_{q}, W_{v}$ are the key, query and value parameter matrices. It is customary to write this in row-vector form by stacking the inputs as row vectors and forming the $T \\times D$ matrix $X=\\operatorname{vcat}\\left(x_{1}^{\\top}, \\ldots, x_{T}^{\\top}\\right)$ (Vaswani et al., 2017). Similarly writing the keys and queries into rowvector stacked matrices, we can write attention in matrix notation as\n$$\n\\tilde{X}=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V\n$$\n\nThe attention mechanism means that the attention weights $a$ are dependent on the sequence values $x_{1}, \\ldots, x_{T}$. Intuitively, the attention mechanism replaces the input $x_{t}$ with a transformed version $\\tilde{x}_{t}$ that is based on how similar $x_{t}$ is to each other element in the sequence. This forms the attention layer of a transformer block, which consists of an additional non-linear and feedforward layer. These blocks are repeated multiple times. The above is written for a single head, with the extension to multiple heads being straightforward (Vaswani et al., 2017). In the causal variant, we define\n\n$$\n\\tilde{x}_{t}=\\sum_{s=1}^{t} a_{t s} v_{s}\n$$\n\nand the normalisation in equation(2) is replaced with $\\sum_{s=1}^{t} \\exp \\left(k_{t}^{\\top} q_{s}\\right)$ to ensure $\\sum_{s=1}^{t} a_{t s}=1$. The attention mechanism has been extremely useful but suffers from $O\\left(T^{2}\\right)$ time and space complexity. While this can be easily reduced to linear memory complexity in the sequence length by using slower non-vectorised operations, the time complexity required is still quadratic (Rabe \\& Staats, 2021), limiting the application of transformers to only modest-length sequences. To address this, a large number of works have discussed ways to approximate the attention mechanism (see section(4)) to reduce the time (and space in a vectorised implementation) complexity. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_54b8ceffd7ad5eb5439dg-02.jpg?height=237&width=788&top_left_y=234&top_left_x=214)\n\nFigure 1. Non causal attention. The time complexity of each algorithm is $O(E D)$ where $E$ is the number of edges in each graph and $D$ is the dimension of each $x$. (Left) Standard Attention computes all pairwise similarities between the elements of the sequence. (Right) Latte computes only the pairwise similarities between each element of the sequence and each latent embedding. In this work we introduce Latte Transformer - a latent attention mechanism that scales linearly with sequence length $T$ and can be used as a drop-in replacement for standard attention. Intuitively, instead of comparing the similarity between each token $x_{s}$ and $x_{t}$, Latte compares how similar each $x_{s}$ is with learned latent tokens, see figure(1).",
    "latentattn-2": "In certain sequence modelling tasks, such as natural language, one might argue that word similarity should be better measured as similarity between words and a learned concept (for example, an abstract noun such as colour).",
    "latentattn-3": "This has both intuitive appeal and practical benefit in reducing the computational complexity of implementation. ## 2. Latte Attention\n\nLatte (Latent Attention) allows for efficient computation of the attention layer in a transformer.",
    "latentattn-4": "First, we consider a noncausal model before presenting the causal approach which is needed for autoregressive models (Radford et al., 2019). We describe the approach for a single head throughout, with the generalisation to multiple heads being straightforward. ### 2.1. Non-causal (bidirectional) Latte\n\nLet us consider a sequence $x_{1}, \\ldots, x_{T}$ and note that the non-causal attention layer can be written using probability notation\n\n$$\n\\tilde{x}_{t}=\\sum_{s=1}^{T} p(s \\mid t) v_{s}\n$$\n\nin which information from the past, present and future $x_{t}$ is used to define $\\tilde{x}_{t}$. We introduce a hidden variable with $L$ states and model the attention using\n\n$$\np(s \\mid t)=\\sum_{l=1}^{L} p(s \\mid l) p(l \\mid t)\n$$\n\nwhere we set\n\n$$\np(l \\mid t)=\\frac{e^{x_{t}^{\\top} w_{l}^{q}}}{\\sum_{j=1}^{L} e^{x_{t}^{\\top} w_{j}^{q}}}\n$$\n\nand\n\n$$\np(s \\mid l)=\\frac{e^{x_{s}^{\\top} w_{l}^{k}}}{\\sum_{s=1}^{T} e^{x_{s}^{\\top} w_{l}^{k}}}\n$$\n\nAnalogous to standard attention, we call $w_{l}^{q}$ the $l^{t h}$ query vector and $w_{l}^{k}$ is the $l^{t h}$ key vector; these are learnable parameters of the model. Diagrammatically, Latte replaces the full attention between the input vectors by attention between input vectors and latent embeddings, see figure(1). In practice, we compute the latent attention function on a set of queries and keys simultaneously. The queries, keys and values are packed into matrices $Q=X W_{q}, K=X W_{k}$ and $V=X W_{v}$. Similar to equation(3) we then define the Latte attention as\n\n$$\n\\text { Latte }(Q, K, V)=\\underbrace{\\operatorname{softmax}_{L}(Q)}_{T \\times L} \\underbrace{\\operatorname{softmax}_{T}(K)^{\\top}}_{L \\times T} \\underbrace{V}_{T \\times D}\n$$\n\nwhere $\\operatorname{softmax}_{L}(\\cdot)$ denotes normalisation over the latent dimension and $\\operatorname{softmax}_{T}(\\cdot)$ denotes normalisation over the temporal dimension. Latte can therefore be thought of as a rank $L$ parameterisation of the attention matrix, see section(4). Non-causal Latte requires $O(T L+L D)$ space complexity and $O(T L D)$ time. See Table 1 for more details and section(4) for a description of the compared methods. ### 2.2. Causal Latte\n\nCausal attention can be written using probability notation as\n\n$$\n\\tilde{x}_{t}=\\sum_{s=1}^{t} p(s \\mid t) v_{s}\n$$\n\nwhere the distribution $p(s \\mid t)$ is defined by causal attention such that $\\sum_{s=1}^{t} p(s \\mid t)=1$. We introduce a hidden variable with $L$ states and model the attention using\n\n$$\np(s \\mid t)=\\sum_{l=1}^{L} p(s \\mid l, t) p(l \\mid t)\n$$\n\nwhere we again set\n\n$$\np(l \\mid t)=\\frac{e^{x_{t}^{\\top} w_{l}^{q}}}{\\sum_{j=1}^{L} e^{x_{t}^{\\top} w_{j}^{q}}}\n$$\n\nand\n\n$$\np(s \\mid l, t)=\\frac{e^{x_{s}^{\\top} w_{l}^{k}}}{\\sum_{s=1}^{t} e^{x_{s}^{\\top} w_{l}^{k}}}\n$$\n\nUnlike non-causal Latte, note here the dependence on $t$ in $p(s \\mid l, t)$. This is due to the causal mechanism requiring normalisation up to time $t$ for each temporal distribution. However, this new dependency does not introduce a quadratic time complexity like the standard attention, since $t$ is used\nonly for normalisation which can be computed sequentially. To see this we define the normalisation terms\n\n$$\n\\beta_{t}=\\sum_{j=1}^{L} e^{x_{t}^{\\top} w_{j}^{q}}, \\quad \\alpha_{t, l}=\\sum_{s=1}^{t} e^{x_{s}^{\\top} w_{l}^{k}}\n$$\n\nWe can now write the new representation at time $t$ as\n\n$$\n\\begin{aligned}\n\\tilde{x}_{t} & =\\sum_{s=1}^{t} p(s \\mid t) v_{s}=\\sum_{l=1}^{L} p(l \\mid t) \\sum_{s=1}^{t} p(s \\mid l, t) v_{s} \\\\\n& =\\sum_{l=1}^{L} \\frac{e^{x_{t}^{\\top} w_{l}^{q}}}{\\beta_{t} \\alpha_{t, l}} \\sum_{s=1}^{t} e^{x_{s}^{\\top} w_{l}^{k}} v_{s}=\\sum_{l=1}^{L} \\gamma_{t, l} \\tilde{v}_{t, l}\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\gamma_{t, l}=\\frac{e^{x_{t}^{\\top} w_{l}^{q}}}{\\beta_{t} \\alpha_{t, l}}, \\quad \\tilde{v}_{t, l}=\\sum_{s=1}^{t} e^{x_{s}^{\\top} w_{l}^{k}} v_{s}\n$$\n\nCausal Latte is therefore a rank $L$ parameterisation of the attention matrix, see section(4). ### 2.3. Computational Complexity\n\nWe note that $\\tilde{v}_{t, l}$ and $\\alpha_{t, l}$ are cumulative sums which can be written recursively\n\n$$\n\\begin{aligned}\n\\alpha_{t, l} & =\\alpha_{t-1, l}+e^{x_{t}^{\\top} w_{l}^{k}} \\\\\n\\tilde{v}_{t, l} & =\\tilde{v}_{t-1, l}+e^{x_{t}^{\\top} w_{l}^{k}} v_{t}\n\\end{aligned}\n$$\n\nWe consider a sequence of length $T$, hidden vectors of dimension $D$ and $L$ latent variables. Then, given $\\alpha_{t-1, l}$, we need $O(L D)$ time and $O(L)$ memory to compute $\\alpha_{t, l}$. Since each $\\beta_{t}$ requires only $O(L D)$ time and $O(1)$ space, then all the terms $\\gamma_{t, l}$ can be precomputed using matrix multiplication in time $O(T L D)$ and $O(T L)$ storage $^{2}$. Similarly, given $\\tilde{v}_{t-1, l}$ one needs $O(L D)$ time to calculate the update to $\\tilde{v}_{t, l}$ and $O(L D)$ space to (in-place) store the update $\\tilde{v}_{t, l}$. Equation(16) therefore allows us to calculate all terms $\\tilde{x}_{1}, \\ldots, \\tilde{x}_{T}$ in total time $O(T L D)$ and total space $O(T L+L D)$. This is compared to $O\\left(T^{2} D\\right)$ time and $O(T D)$ space complexity for self attention (Rabe \\& Staats, 2021). The above calculations are based on a theoretically optimal sequential implementation - using a naive vectorised GPU implementation retains the time complexity but increases the Latte space requirement to $O(T L D)$ and standard attention to $O\\left(T^{2} D\\right)$. Table 1 summarises the time and memory complexities of our algorithms, comparing them to the attention mechanism and other related efficient approximations. In figure(2) we show the computational and memory costs between standard causal attention and different versions of\n\n[^2]Table 1. Time and memory complexity comparison for different models at inference. Only dominating terms are kept. $T$ - sequence length, $L$ - latent dimension, $D$ - hidden dimension, $S$ - number of random samples. | Model | Time | Memory |\n| :--- | :---: | :---: |\n| Latte Causal | $O(T L D)$ | $O(T L D)$ |\n| Latte Causal Seq. | $O(T L D)$ | $O(T L+L D)$ |\n| Latte Bidirectional | $O(T L D)$ | $O(T L+L D)$ |\n| Attention | $O\\left(T^{2} D\\right)$ | $O\\left(T^{2} D\\right)$ |\n| Seq. Attention | $O\\left(T^{2} D\\right)$ | $O(T D)$ |\n| Linformer | $O(T L D)$ | $O(T L D)$ |\n| Perceiver | $O(T L D)$ | $O(T L D)$ |\n| Performer | $O(T S D)$ | $O(T D+T S)$ |\n\nour algorithm for various sequence lengths. Bidirectional standard attention has the same time and space complexity as the causal version, so we do not include it in the plots. To avoid overheads we do not benchmark the entire layer, but only calculate the transform $\\tilde{x}_{t}$ of the input. It is also important to note that GPU parallelisation makes the less optimal algorithms faster up until a certain sequence length. Since Latte time complexity is $O(T L D)$, the point where it becomes faster than the standard attention, i.e. the crossover point, will increase with the number of latent variables $L$, see figure(4). Hence, there is a tradeoff between the speed of the method and model capacity. ### 2.4. Numerical Stability\n\nThe term $p(l \\mid t)$ in equation(12) can be calculated in a numerically stable way using exponential-max-normalisation ${ }^{3}$. However, we cannot directly apply the same approach to stabilize $\\alpha_{t, l}$ since we require $p(s \\mid l, t)$ in equation(13) to normalise for each $t$; furthermore, this must be computed sequentially to retain the optimal computational complexity. To exemplify this difference, consider a sequence $y=[1,10,1000]$, for which we require normalised distributions $e^{y_{i}} / \\sum_{j=1}^{i} e^{y_{j}}$ for each element $i \\in\\{1,2,3\\}$. In this case, subtracting the maximum value 1000 from each element $[-999,-990,0]$ will result in underflow for the first two elements when exponentiated, giving numerically meaningless results, except for the third distribution. We address this using a running maximum approach (Rabe \\& Staats, 2021). Let $\\theta_{t, l}=x_{t}^{\\top} w_{l}^{k}$ and $\\theta_{t, l}^{*}=\\max _{s \\in\\{0, \\ldots, t\\}} \\theta_{s, l}$. We then use a sequential computation:\n\n$$\n\\begin{aligned}\n\\alpha_{t, l} & =\\alpha_{t-1, l} e^{\\theta_{t-1, l}^{*}-\\theta_{t, l}^{*}}+e^{\\theta_{t, l}-\\theta_{t, l}^{*}} \\\\\n\\tilde{v}_{t, l} & =\\tilde{v}_{t-1, l} e^{\\theta_{t-1, l}^{*}-\\theta_{t, l}^{*}}+e^{\\theta_{t, l}-\\theta_{t, l}^{*}} v_{t}\n\\end{aligned}\n$$\n\nThis value for $\\alpha$ is then used to define $\\gamma$ in equation(16), which is in turn used with $\\tilde{v}$ above to compute $\\tilde{x}_{t}$. [^3]![](https://cdn.mathpix.com/cropped/2024_09_12_54b8ceffd7ad5eb5439dg-04.jpg?height=600&width=1568&top_left_y=314&top_left_x=224)\n\nFigure 2. Runtime comparison of Causal and Bidirectional Latte with both the Standard Causal Attention and the sequential implementation. Here we use batch size $B=2$, transformer hidden dimension of $D=128$, latent dimension $L=128$ and number of heads $H=4$. For all the sequential implementations, we unroll the loop 256 times ( appendix(B.1)) and for the sequential standard attention, we group the operations into chunk sizes of 1000 . Results are means over 100 runs in Jax (Bradbury et al., 2018) using one A10 GPU. ### 2.5. Efficient Inference\n\nFrom equation(19) it immediately follows that we can calculate $\\tilde{x}_{t+1}$ (i.e. infer the future token) directly from the stored $\\alpha_{t, l}, \\beta_{t}, \\tilde{v}_{t, l}$, without needing to refer to the previous data $x_{1}, \\ldots, x_{t}$. In other words, unlike standard attention which requires the original sequence, the only relevant quantities Latte needs to predict the future are stored in the current $\\alpha$, $\\beta, \\tilde{v}$. In this sense, Causal Latte shares some similarities with recurrent prediction approaches, such as SSMs (Gu et al., 2021; Smith et al., 2022; Fu et al., 2022). Next token inference in Latte requires $O(L D)$ memory, compared to $O(T D)$ in standard attention. Similarly, the Latte time complexity for next token inference is $O(L D)$, compared to $O(T D)$ in standard attention. This means that next token inference is potentially significantly faster than standard attention, assuming $L \\ll T$. ## 3. Experiments\n\nWe apply Latte on both bidirectional and unidirectional tasks, such as classification problems and language generation. For the bidirectional experiments we choose Long Range Arena(LRA) (Tay et al., 2020a), a corpus with longrange dependencies, while for causal tasks we use language generation with byte-pair and character-level tokenization. To run the experiments we use two A10 GPUs and implement the model using Jax (Bradbury et al., 2018). Jax allows us to easily and efficiently implement the sequential operation with the scan operator ${ }^{4}$. [^4]\n### 3.1. Bidirectional Tasks\n\nTo test the bidirectional case we experiment with LongRange-Arena (LRA) (Tay et al., 2020a), a classification corpus which tests the long-range capabilities of the model. This corpus consists of multiple datasets which we describe in more detail in appendix (A). We note that for LRA, statebased models currently outperform transformers for some tasks (Gu et al., 2021). However, our interest is to compare against other transformer models, rather than state-of-the art alternatives to the transformer. For a fair comparison, we keep the same architecture as the vanilla transformer, only replacing the standard bidirectional self-attention with bidirectional Latte. The hyperparameters used are described in Table 4. Table 2 compares Latte on the LRA benchmark to standard bidirectional attention and existing attention approximation approaches. For tasks like ListOps and Sequential Image prediction, we considerably outperform the best models, as reported in (Tay et al., 2020a). Indeed, we still achieve comparable scores for most benchmarks even when we use fewer latent variables, as shown by setting $L=40$. ### 3.2. Language Generation\n\nOpenWebText. For language generation, we use OpenWebText (Gokaslan \\& Cohen, 2019) an open-source version of the corpus used to train GPT (Radford et al., 2019).",
    "latentattn-5": "The corpus has a size of 41 GB and was extracted from $8,013,769$ documents. We tokenize it using the pre-trained Byte-Pair-\n\n[^5]Table 2. Classification accuracies for LRA dataset. We evaluate the model after each epoch and report the best test score. Best-LRA* means the best score reported in the LRA benchmark for each task and does not represent a single model. \"Bid.\" stands for bidirectional. | Model | ListOps | Text | Retrieval | Image | Pathfinder |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Latte Bid. $L=40$ | 40.18 | 64.51 | 73.39 | 47.55 | 75.61 |\n| Latte Bid. $L=D$ | $\\mathbf{4 1 . 7 4}$ | 65.15 | 72.39 | $\\mathbf{4 8 . 0 5}$ | 73.68 |\n| Luna Bid. $L=128$ | 38.01 | 65.74 | $\\mathbf{7 9 .",
    "latentattn-6": "5 5}$ | 47.47 | $\\mathbf{7 8 . 8 9}$ |\n| Bid. Attention | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 |\n| Linformer | 35.70 | 53.94 | 52.27 | 38.56 | 76.34 |\n| Performer | 18.01 | 65.40 | 53.82 | 42.77 | 77.05 |\n| Best-LRA* | 37.27 | $\\mathbf{6 5 . 9 0}$ | 59.59 | 44.24 | 77.05 |\n\nEncoding GPT2 tokenizer (Radford et al., 2019) and use the obtained data to train small models on the next-word prediction language modelling task. For a fair comparison, we train using both Causal Latte and Standard Causal Attention under the same settings: \\#Layer $=12, H=12$, $D=512, T=1024$ and a latent dimension $L=512$. We choose the latent dimension to be equal to the embedding size such that the number of parameters is the same across all models. For more details on hyperparameters see Table 5. As figure(3) shows, our Latte model reaches a similar performance to the Standard Causal Attention model while only using a linear time attention mechanism. Enwik8. We also experiment with character-level language modelling using Enwik8 (Hutter, 2002) which allows us to easily increase the context length. Like in the previous setting, we use small models and make a fair comparison with causal self-attention by using the same hyperparameters. Here we increased the context length to 2000 while keeping the number of latent variables fixed, to see how the model behaves when the number of latent variables is small compared to the sequence length. We summarise the results in Table 3. As with the OpenWebText dataset, the performance of Latte is close to that of Standard Causal Attention, even for an increased context length. To attain state-of-the-art results in terms of next token generation ability, we would need to train very large models, which our very limited hardware resources do not allow. However, for the given hardware limitation, Latte has comparable performance to full attention and we would expect this to carry over to much larger models and allow for implementation on significantly longer contexts. Table 3. Language generation comparison between Causal Latte and Standard Causal Attention. The metrics used are bits per character ( bpc) and perplexity ( ppl ).",
    "latentattn-7": "The lower the better. |  | OpenWeb |  | enwik8 |  |\n| :--- | :--- | :--- | :--- | :--- |\n| Model | bpc | ppl | bpc | ppl |\n| Standard Causal Attention | 10.80 | 25.53 | 1.28 | 2.45 |\n| Causal Latte | 11.06 | 27.60 | 1.40 | 2.66 |\n\n## 4. Related Work\n\nThe literature surrounding efficient attention is extensive, but it can be broadly classified into six overlapping classes (Tay et al., 2020b): Downsampling (Jaegle et al., 2021), Random Patterns (Zaheer et al., 2020), Learnable Patterns (Wang et al., 2022; Kitaev et al., 2020), Sparse Transformers (Ainslie et al., 2020; Beltagy et al., 2020), Recurrence (Dai et al., 2019) and Low-Rank (Wang et al., 2020; Katharopoulos et al., 2020). Here we provide an overview of the most related work on addressing attention in transformers; we do not discuss structured state models (SSMs) since they are a complete departure from the transformer space. Efficient Transformer. This is a linear time and space method for bidirectional attention (Shen et al., 2021). While the Efficient Transformer reduces to the same formulation as bidirectional Latte equation(9), they do not provide an extension to the causal setting. In contrast, our work provides a probabilistic framework from which the bidirectional and causal formulations arise naturally. Furthermore, they experiment with vision tasks such as image segmentation, while our work focuses on language problems. Luna. Linear Unified Nested Attention (Ma et al., 2021) performs attention between $T$ input tokens and a sequence of $L$ latent variables. For the bidirectional case, Luna uses nested attention\n\n$$\n\\begin{aligned}\n& Y=\\operatorname{softmax}(\\underbrace{Q}_{L \\times D} \\underbrace{K^{\\top}}_{D \\times T} \\underbrace{V}_{T \\times D} \\\\\n& \\tilde{X}=\\operatorname{softmax}(\\underbrace{Q^{\\prime}}_{T \\times D} \\underbrace{K^{\\prime \\top}}_{D \\times L}) \\underbrace{V^{\\prime}}_{L \\times D}\n\\end{aligned}\n$$\n\nwhere $Q=W_{q} P, K=W_{k} X, V=W_{v} X, Q^{\\prime}=W_{q}^{\\prime} X$, $K^{\\prime}=W_{k}^{\\prime} Y, V^{\\prime}=W_{v}^{\\prime} Y$ and we define $P$ to be a model parameter representing the sequence of $L$ latent variables. This differs substantially from our bidirectional formulation in equation(9). For causal attention one needs to modify the approach such that for each token $x_{t}$ the latent variables only interact with\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_54b8ceffd7ad5eb5439dg-06.jpg?height=633&width=1711&top_left_y=315&top_left_x=181)\n\nFigure 3. Performance comparison under the same settings for Causal Latte and Standard Causal Attention model. $x_{\\leq t}$. The Luna causal layer can be written as:\n\n$$\n\\tilde{x}_{t}=\\frac{1}{t} \\sum_{l=1}^{L} B_{t, l} \\sum_{s=1}^{t} A_{l, s} x_{s}\n$$\n\nWhilst this matrix factorisation looks similar to causal Latte equation(15), $B_{t, l}$ and $A_{l, s}$ are parameterised differently. For $A$ Luna uses softplus (Glorot et al., 2011) and elu (Clevert et al., 2015) non-linearities and $B$ depends on $A$ :\n\n$$\n\\begin{aligned}\nA & =\\operatorname{softplus}\\left(\\operatorname{elu}\\left(P X^{\\top}\\right)+1\\right) \\in \\mathbb{R}^{L \\times T} \\\\\nB_{t,:} & \\propto \\exp \\left(x_{t}^{\\top} \\frac{1}{t} \\sum_{s=1}^{t} x_{s} A_{:, s}^{\\top}\\right) \\in \\mathbb{R}^{T \\times L}\n\\end{aligned}\n$$\n\nIn equation(26) each $B_{t}$ is normalised by summing over the $L$ latent variables. Whilst Luna therefore shares a similar inspiration to Latte in terms of latent variables, its implementation differs significantly. Linformer. Another closely related work to ours is the Linformer (Wang et al., 2020). At the heart of the Linformer is a low-rank approximation of the attention matrix. This is motivated by the fact that most of the singular values of the attention matrix are small; hence using only a few of the largest singular values should result in a good low-rank approximation. Rather than using SVD directly, Linformer learns the low rank approximation. Writing for a single head, one can form the projections\n\n$$\n\\hat{K}=E K, \\quad \\hat{V}=F V\n$$\n\nwhere $E, F \\in \\mathbb{R}^{L \\times T}$. This leads to the attention approximation:\n\n$$\n\\tilde{X}=\\operatorname{softmax}\\left(Q \\hat{K}^{\\top}\\right) \\hat{V}\n$$\n\nThis also maintains a probabilistic interpretation: $\\tilde{x}_{t}=$ $\\sum_{l=1}^{L} \\hat{p}(l \\mid t) \\hat{v}_{l}$, but has many significant differences to our work. While this is also a rank $L$ parameterisation, it cannot be used for unidirectional problems and requires the input to be projected to latent space. That is, if the Linformer is trained with a context of $T$, it cannot be applied to a longer context. In contrast, Latte can be applied to any context size, independent of how long the context was on which it was trained ${ }^{5}$. Perceiver. The Perceiver (Jaegle et al., 2021) model performs cross-attention between a set of latent variables and the entire input. Cross-attention formulation is similar to standard attention, but they differ by making queries $\\hat{Q}$ parameters of the network, which represent the latent variables. The keys and values are still functions of the input, $K=X W_{k}, V=X W_{v}$, resulting in the following transform:\n\n$$\n\\tilde{X}=\\operatorname{softmax}(\\underbrace{\\hat{Q}}_{L \\times D} \\underbrace{K^{\\top}}_{D \\times T}) \\underbrace{V}_{T \\times D}\n$$\n\nThis formulation reduces the computational complexity of attention to $O(T L D)$ where $L$ is a chosen latent dimension. The newly obtained $\\tilde{X} \\in \\mathbb{R}^{L \\times D}$ is passed to multiple transformer blocks which use the standard bidirectional attention. The time complexity of those blocks is $O\\left(L^{2} D\\right)$ which is fast for $L \\ll T$. Equation(29) can be re-expressed as $\\tilde{x}_{l}=\\sum_{t=1}^{T} p(t \\mid l) \\hat{v}_{t}$ and can be interpreted as a weighted sum of the input, where weights are interactions between the input sequence and some global learnable patterns. Compared to our approach, this model is non-causal and cannot be applied to language generation. [^6]Performer. Similar to the linear transformer described by Katharopoulos et al. (2020), the Performer (Choromanski et al., 2020) is based on the parameterisation of the unnormalised attention matrix\n\n$$\n\\hat{a}_{t, j}=\\mathbb{E}\\left[\\phi^{\\top}\\left(q_{t}\\right) \\phi\\left(k_{j}\\right)\\right]\n$$\n\nfor some $L$-dimensional random feature map $\\phi$. The expectation is carried out numerically using $S$ samples The unnormalised attention can then be written as\n\n$$\n\\begin{aligned}\n\\sum_{j} \\hat{a}_{t, j} v_{j} & =\\sum_{j} \\mathbb{E}\\left[\\phi^{\\top}\\left(q_{t}\\right) \\phi\\left(k_{j}\\right)\\right] v_{j} \\\\\n& =\\mathbb{E}\\left[\\phi^{\\top}\\left(q_{t}\\right) \\sum_{j} \\phi\\left(k_{j}\\right) v_{j}\\right]\n\\end{aligned}\n$$\n\nThe normalisation is straightforward to include - see (Choromanski et al., 2020). The feature map $\\phi$ is carefully chosen to approximate the softmax function and is based on orthogonal features. Unlike the Linformer and Perceiver, the Performer can be applied in both a causal and non-causal model. However, it does not have the same interpretation as Latte but is rather an approximation of the original standard attention mechanism. In practice, it requires repeated resampling of the features to match the perplexity of the standard transformer. ## 5. Conclusion\n\nWe introduced a latent attention mechanism which scales linearly with sequence length and acts as a drop-in replacement for standard attention. Whilst previous approaches have discussed the use of latent variables, to our knowledge, none have the interpretation of latent variables in a probabilistic setting, from which a simple formulation for both the bidirectional and causal variants can be readily derived, leading also to a simple and numerically stable implementation. In our experiments, Latte provides comparable results to standard attention yet can be scaled to significantly longer contexts. In the causal setting, inference is fixed time rather than scaling with context. This rapid inference provides a bridge between standard attention and fast recurrent predictors based on state-space models. Future work could consider seamlessly integrating Latte with standard attention and also retrofitting Latte to pretrained standard attention models, thus allowing the application of pretrained models to much larger contexts. Code is available at https: //github.com/raresdolga/latte_transformer. ## Acknowledgements\n\nThis work was supported in part by Oracle Cloud credits and related resources provided by Oracle for Research. The authors would like to thank Ahmed Shahin, Kai Biegun, Giorgio Giannone, Jake Cunningham, Martin Schoernig, Philip Treleaven, Songling Yang and \u00a9sheggle_for valuable conversations and insights. ## References\n\nAinslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: Encoding Long and Structured Inputs in Transformers. arXiv preprint arXiv:2004.08483, 2020. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The Long-Document Transformer. arXiv preprint arXiv:2004.05150, 2020. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: Composable Transformations of Python+NumPy Programs, 2018. URL http://github.com/google/jax. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking Attention with Performers. arXiv preprint arXiv:2009.14794, 2020. Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus). arXiv preprint arXiv:1511.07289, 2015. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. arXiv preprint arXiv:1901.02860, 2019. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
    "latentattn-8": "arXiv preprint arXiv:1810.04805, 2018. Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and R\u00e9, C. Hungry Hungry Hippos: Towards Language Modeling with State Space Models.",
    "latentattn-9": "arXiv preprint arXiv:2212.14052, 2022. Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectifier Neural Networks. In JMLR Workshop and Conference Proceedings, pp. 315-323, 2011. Gokaslan, A. and Cohen, V. OpenWebText Corpus, 2019. URL http://Skylion007.github.io/ OpenWebTextCorpus. Gu, A., Goel, K., and R\u00e9, C. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Hutter, M. The Human Knowledge Compression Prize, 2002. URL https:// www.kurzweilai.net/hutter-prizefor-lossless-compression-of-humanknowledge. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning, pp. 4651-4664. PMLR, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Khan, S., Naseer, M., Hayat, M., Zamir, S.",
    "latentattn-10": "W., Khan, F. S., and Shah, M. Transformers in Vision: A Survey. ACM Computing Surveys (CSUR), 54(10s):1-41, 2022. Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The Efficient Transformer. arXiv preprint arXiv:2001.04451, 2020. Krizhevsky, A., Hinton, G., et al. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto, 2009. Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and Zettlemoyer, L. Luna: Linear Unified Nested Attention. Advances in Neural Information Processing Systems, 34: 2441-2453, 2021. Maas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150, 2011. Rabe, M. N. and Staats, C. Self-attention Does Not Need $O\\left(n^{2}\\right)$ Memory.",
    "latentattn-11": "arXiv preprint arXiv:2112.05682, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient Attention: Attention with Linear Complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3531-3539, 2021. Smith, J. T., Warrington, A., and Linderman, S. W. Simplified State Space Layers for Sequence Modeling. arXiv preprint arXiv:2208.04933, 2022. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006, 2020a. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient Transformers: A Survey. arXiv preprint arXiv:2009.06732, 2020 b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention Is All You Need. Advances In Neural Information Processing Systems, 30, 2017. Wang, N., Gan, G., Zhang, P., Zhang, S., Wei, J., Liu, Q., and Jiang, X. ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer.",
    "latentattn-12": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. $2390-2402,2022$. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-Attention with Linear Complexity.",
    "latentattn-13": "arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big Bird: Transformers for Longer Sequences.",
    "latentattn-14": "Advances In Neural Information Processing Systems, 33:17283-17297, 2020. ## A. LRA Dataset\n\nThis section displays the hyperparameters employed in the bidirectional experiments and provides a brief description of the synthetic datasets utilized in the LRA corpus. A more comprehensive account can be found in the original paper (Tay et al., 2020a). Table 4. Hyperprameters used for training on LRA. Number of latent dimensions $L$ specified in the result table. $H=$ number heads, $D=$ hidden dimension, $L R=$ learning rate, $B$ =batch size, $W D=$ weight decay. \\#Layers denotes the number of layers which include attention/approximation of attention and non-linear projections. | Dataset | \\#Layers | $H$ | $D$ | $L R$ | $B$ | $W D$ | Dropout | Epochs |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | ---: |\n| ListOps | 6 | 4 | 128 | $1 \\mathrm{e}-3$ | 64 | 0.01 | 0.1 | 50 |\n| Text | 6 | 4 | 256 | $1 \\mathrm{e}-3$ | 32 | 0.05 | 0.1 | 32 |\n| Retrieval | 6 | 4 | 128 | $1 \\mathrm{e}-4$ | 32 | 0.01 | 0.1 | 20 |\n| Image | 6 | 4 | 512 | $1 \\mathrm{e}-3$ | 32 | 0.05 | 0.1 | 200 |\n| Pathfinder | 6 | 4 | 256 | $1 \\mathrm{e}-3$ | 64 | 0.03 | 0.2 | 200 |\n\nIn the experiments, one layer consists of a standard transformer block where the transformation operation that gives $\\tilde{x}_{t}$ is Latte or Standard Attention.",
    "latentattn-15": "For positional encoding, we use the classic transformer sinusoidal embeddings.",
    "latentattn-16": "This convention holds for both bidirectional and unidirectional problems. A complete implementation can be found in our code repository: https://github.com/raresdolga/latte_transformer. ## A.1. ListOps\n\nThe dataset contains sequences up to length 2048 of numbers from 0 to 9 and four operators: MAX, MEAN, MEDIAN and $S U M A O D$. Parentheses are used to delimit the reach of each operator. The answer is also a digit from 0 to 9 , which allows us to easily transform the problem into a ten-way classification task. ## A.2. Text\n\nThe Text corpus is represented by a binary classification task with long text sequences. One can easily obtain large contexts from existent datasets by tokenizing at the character level. This part of the benchmark is derived from the IMDb (Maas et al., 2011) movie review corpus, resulting in 4 K character sequences. ## A.3. Retrieval\n\nThis dataset tests the ability of a model to predict the similarity between two long documents. Similarly to the previous corpus, it ensures long contexts through character-level tokenization, resulting in 4 K tokens per document. Using a \"twotower model setup\" (Tay et al., 2020a) the total sequence length becomes 8 K . This is a binary classification problem, which uses accuracy as a metric. ## A.4. Image\n\nAlongside text, images can also exhibit long-range dependencies by flattening the original image into a sequence. The Image dataset is the sequential version of Cifar10 (Krizhevsky et al., 2009), which contains images of 10 different entities: \"airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\". To obtain a sequence with one input channel we apply a grayscale transformation. The model needs to predict the correct entity class, given the flattened image represented as a sequence of tokens. ## A.5. PathFinder\n\nThis part of the benchmark is also represented by images where the task is to predict whether there is a path between two points in a black-and-white image. This dataset consists of $32 \\times 32$ images which after flattening result in sequences of length 1024. In general larger sequences can be created by increasing the resolution. Data is tokenized similarly to the image dataset in section(A.4). ![](https://cdn.mathpix.com/cropped/2024_09_12_54b8ceffd7ad5eb5439dg-10.jpg?height=609&width=811&top_left_y=300&top_left_x=603)\n\nFigure 4. Ablation study of time performance on the number of latent variables L. SCA: Standard Causal Attention, LMCA: Standard Causal Attention (Sequential), CL: Causal Latte, BL: Bidirectional Latte. Experiment settings: $T=5000, D=512, H=4, B=4$\n\n## A.6. PathX\n\nThis dataset is a version of PathFinder where the image size is increased to $128 \\times 128$, resulting in flattened sequences of 16384 tokens.",
    "latentattn-17": "Since all the transform architectures fail on this dataset, we do not add it to the benchmark. ## B. Latte\n\nWe present the hyperparameters used for the language generation tasks (Table 5) and also perform an ablation study of the GPU runtime required by different numbers of latent variables. ## B.1. Language generation Hyperparameters. Table 5. Hyperprameters for the language generation task. $L R$ is the learning rate and \"\\#\" denotes \"the number of\". | HyperParam. | OpenWebText | Enwiki |\n| :--- | :---: | :---: |\n| \\#Layers | 12 | 12 |\n| \\#Heads | 12 | 12 |\n| Hidden Dim $(D)$ | 516 | 516 |\n| Latent Dim $(L)$ | 516 | 516 |\n| Dropout | 0.0 | 0.0 |\n| LR | 0.0006 | 0.0006 |\n| LR-Warmup | 2000 | 2000 |\n| LR-Decay | Cosine | Cosine |\n| \\#Iters. | 60000 | 60000 |\n| Weight Decay | 0.01 | 0.01 |\n| Seq. Len. (T) | 1024 | 2000 |\n| Batch Size (B) | 8 | 8 |\n| Tokenizer | Byte-Pair | Character |\n| Positional Encoding | Sinusoidal | Sinusoidal |\n| Unroll Factor | 256 | 256 |\n| \\#Params. | 90.31 M | 38.61 M |\n\nThe Unroll Factor hyperparameter is the number of times the compiler copies the body of a loop to avoid checking the bounds at each iteration.",
    "latentattn-18": "This is used in the Jax sequential implementation to optimise the run-time. ## B.2. Latent Ablation Study\n\nWe also do an ablation study on the runtime cost for different numbers of latent variables. We fix the sequence length $T$ to 5000 , the hidden dimension $D$ to 512 , the number of heads $H$ to 4 and batch size $B$ to 4 , while we vary the latent dimension from 64 to 512 .",
    "latentattn-19": "The loop unroll factor can be considered a hyperparameter concerning the computation time. In this case, we choose an unroll factor of 10. Figure(4) shows that both bidirectional and causal Latte is faster than the linear memory causal attention implemented with a scan; however, it is slightly slower than the standard causal attention when $L=D=512$. This is explained by the fact that standard causal attention exploits parallelized matrix operations on GPU. Furthermore, as we previously showed in section(2.4), Latte is still faster when the sequence length increases. ## B.3. Causal Latte Implementation\n\n```\n@partial(jax.jit, static_argnums=(3, 5))\ndef causal_latte(Wq, Wk, Wv, H, X, unroll=100):\n    \"\"\"\n    Scan implementation of latte. B: batch size H: nr heads, T: seq_len, D: hidden_dim. L: latent dimension\n    Args:\n        Wq: jnp.array(DL), Wk:jnp.array(DL), Wv:jnp.array(DM) - parameter matrices\n        H: int - nr heads\n        X: jnp.array(BTD) - input\n        unroll: int - unroll of the loop\n    Returns:\n    \" \"\"\n            y: jnp.array(BTD) - transformed output sequence\n    def accumulate(carry, args):\n        csum, norm_cumsum, prev_mx = carry\n        Qs_t, curr_alph, V_t, c_mx = args\n        revert_maxi = jnp.exp(-c_mx + prev_mx)\n        add_maxi = jnp.exp(curr_alph - c_mx)\n        norm_cumsum = jnp.einsum(\"BHL,BHL->BHL\", norm_cumsum, revert_maxi)\n        norm_cumsum += add_maxi\n        carry = jnp.einsum(\"BHLD,BHL->BHLD\", csum, revert_maxi)\n        carry += jnp.einsum(\"BHL,BHD->BHLD\", add_maxi, V_t)\n        y = jnp.einsum(\"BHL,BHLD->BHD\", Qs_t / norm_cumsum, carry)\n        return ((carry, norm_cumsum, c_mx), y)\n    B, T, D = X.shape\n    L = Wk.shape [-1]\n    V = jnp.einsum(\"DM, BTD->TBM\", Wv, X).reshape(T, B, H, -1)\n    Q = jnp.einsum(\"DL,BTD->TBL\", Wq, X).reshape(T, B, H, -1)\n    K = jnp.einsum(\"DL,BTD->TBL\", Wk, X).reshape(T, B, H, -1)\n    maxi = jax.lax.cummax(K, axis=0)\n    init_alpha = jnp.zeros(shape=(B, H, L // H))\n    init_carry = jnp.zeros((B, H, L // H, D // H))\n    Qs = jax.nn.softmax(Q, axis=-1)\n    ,, y = jax.lax.scan\n        accumulate,\n        unroll=unroll\n        init=(\n            init_carry,\n            init_alpha,\n            K[0],\n        ),\n        xs=[Qs, K, V, maxi],\n    )\n    # TBHD -> BTHD\n    y = y.transpose(1, 0, 2, 3)\n    return y.reshape(B, T, D)\n```\n\nListing 1. Scan version of the Latte.",
    "latentattn-20": "[^0]:    ${ }^{\\dagger}$ Centre for Artificial Intelligence, University College London, London, WC1V 6LJ, UK *UiPath, 10 York Rd, London, SE1 7ND, UK. Correspondence to: Rares Dolga $<$ rares.dolga.16@ucl.ac.uk $>$. [^1]:    ${ }^{1}$ Attention is usually defined to include a division by a constant factor in the exponent $a_{t s}=\\frac{\\exp \\left(q_{t}^{\\top} k_{s} / \\sqrt{d_{k}}\\right)}{\\sum_{s=1}^{T} \\exp \\left(q_{t}^{\\top} k_{s} / \\sqrt{d_{k}}\\right)}$ to improve numerical stability. For presentation clarity we omit these factors throughout. [^2]:    ${ }^{2} \\gamma_{t, l}$ can be computed on the fly for each $\\tilde{x}_{t}$ reducing the memory complexity to $O(L)$. However, we precompute it to take advantage of matrix multiplication on GPU. [^3]:    ${ }^{3} e^{x_{i}} / \\sum_{i} e^{x_{i}}=e^{x_{i}-x^{*}} / \\sum_{i} e^{x_{i}-x^{*}}$, where $x^{*}$ is the maximum of the $x_{i}$ values. [^4]:    ${ }^{4}$ Implementation in PyTorch would require a custom CUDA\n\n[^5]:    kernel, as a simple for loop is not efficient for long sequences, even when the model is compiled. [^6]:    ${ }^{5}$ Provided that proper positional embeddings are used. "
}