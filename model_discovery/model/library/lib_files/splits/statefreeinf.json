{
    "statefreeinf-0": "State-Free Inference of State-Space Models: The Transfer Function Approach\n\nRom N. Parnichkun Stefano Massaroli Alessandro Moro Jimmy T.H. Smith Ramin Hasani Mathias Lechner Qi An Christopher R\u00e9 Hajime Asama Stefano Ermon Taiji Suzuki Atsushi Yamashita Michael Poli\n\nAbstract\n\nWe approach designing a state-space model for deep learning applications through its dual representation, the transfer function, and uncover a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel\u2019s spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers \u2013 parametrized in time-domain \u2013 on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF. State-space model, transfer function, signal processing, control theory, language modeling\n\n1 Introduction\n\nCentral to the success of a certain class of sequence modeling layers are linear recurrences, which unlike the nonlinear case (Hochreiter & Schmidhuber, 1997; Chung et al., 2014; Kidger et al., 2020; Massaroli et al., 2021), are compatible with exact sequence parallel algorithms i.e., parallel scans (Blelloch, 1990; Martin & Cundy, 2018; Smith et al., 2023; Gu & Dao, 2023; Katsch, 2023), or (with time-invariance) the Fast Fourier Transform (FFT) (Gu et al., 2022b, a; Zhang et al., 2023). Such recurrent layers, often referred to in deep learning simply as state-space models, depending on their parametrization, also boast efficient constant time and memory autoregressive inference, lowering latency and memory costs. Despite recent advancements, current SSMs exhibit certain limitations that this paper aims to address. With the goal of enabling parallel inference, many algorithms such as S5 (Smith et al., 2023), LRU (Orvieto et al., 2023) S4 (Gu et al., 2022b) and DSS (Gupta et al., 2022) employ a modal (diagonal) SSM representation, wherein the state transition matrix is diagonal, potentially limiting the model\u2019s expressive capacity for a given state dimension. Additionally, along with Mamba (Gu & Dao, 2023), S5 and LRU rely on the parallel scan directive (Martin & Cundy, 2018; Blelloch, 1990) which incurs considerable memory costs at large state sizes111Even when the states are only materialized in SRAM (Gu & Dao, 2023), as SRAMs are limited in size., due to the materialization of states over the sequence length, as made evident in Figure 1. The expensive space requirement is alleviated with S4 (Gu et al., 2022b), S4D (Gu et al., 2022a), and SpaceTime (Zhang et al., 2023) by an algorithm that admits what we denote as state-additive space complexities, in which the parallel inference algorithm collapses the state dimension onto the sequence length dimension , enabling space complexities of in place of the much greater state-multiplicative complexity of scan-based algorithms. To realize the aforementioned state-additive space complexity, S4 and S4D leverage fast Cauchy and Vandermonde matrix-vector product algorithms (Pan, 2001). These algorithms used in computing the convolutional kernel for S4 and S4D scale as , bottlenecking the faster required to execute the downstream convolution. We approach solving these issues through a thorough frequency analysis of state-space models and unveil a parallel inference algorithm that admits state-free space and time complexities of and respectively. Additionally, the proposed algorithm operates over a complete representation, the Rational Transfer Function (RTF) representation, which unlike diagonal SSMs (Gu et al., 2022a; Gupta et al., 2022; Smith et al., 2023), fully encapsulates the functional space of any linear time-invariant state-space model, including ones parameterized with dense matrices. Parallel inference with RTF solely relies on the Fast Fourier Transform () algorithm \u2013 a widely used and optimized algorithm, alleviating the need for additional custom low-level optimizations to obtain efficient subquadratic complexities. Figure 2 illustrates an overview of the parametrization, parallel inference, and sequential inference algorithms of our proposed SSM. In order to validate the proposed parametrization, we conducted experiments across a range of tasks, models, and importantly state sizes, including Long Range Arena (LRA), language modeling, and synthetic tasks. Notably, in LRA our proposed model obtained state-of-the-art accuracy (Table 1) among other attention-free models, and faster training speeds in comparison to S4 and S4D across state sizes (Figure 3). We approached language modeling by embedding RTF into a Hyena model (Poli et al., 2023a), effectively replacing the original convolutional filter parameterized with MLPs with transfer functions, and observed improved perplexity over the Hyena Filter baseline when trained on WikiText103 (Table 4). 2 Preliminaries and Related Work\n\nWe discuss sequence modeling, convolution-based sequence processing units and their state-space realization. 2.1 Sequence Modeling with Convolutions\n\nLet denote the space of length- vector-valued sequences, . We denote the time index with a subscript roman letter and additional dimensions with greek superscripts, e.g. for and . Any map from into itself is herein referred to as a sequence processor. Complex deep learning architectures tailored for sequence modeling typically involve the composition of simpler, parametric sequence processors in a multi-layer fashion. In this work, we focus on causal sequence processors , where the output at any given time is a function of solely the preceding inputs, i.e. for all and . This constraint is crucial, for instance, in auto-regressive training of decoder-only language models (Radford et al., 2018) or analogous modeling tasks of temporal dynamics (see e.g. Chen et al., 2021). The ideal sequence processing layer is expected to fulfill several design criteria, balancing factors such as expressivity, computational and memory efficiency, favorable training dynamics, and parametric efficiency. Of particular interest in this work are those sequence processors that utilize single-input single-output (SISO) discrete convolutions as their fundamental components, a.k.a. linear time invariant (LTI) systems, with convolutional filters being implicitly parameterized. SISO convolution operators can be represented by structured (Toeplitz) matrices that admit a fast multiplication algorithm with efficient sub-quadratic complexity . They serve as the fundamental building blocks on various classical signal processing pipelines such as audio systems (Oppenheim et al., 1999) and visual systems (Gonzalez & Woods, 2008). A notable modern example of sequence processors that make use of implicit convolutions as their core operation on the temporal dimension is the Hyena architecture (Poli et al., 2023a). Given three sequences obtained from the input through three dense linear projections followed by three short convolutions, Hyena realizes a map , defined element-wise for all and as\n\n( \u210b \u200b u ) t \u03b1 superscript subscript \u210b \ud835\udc62 \ud835\udc61 \ud835\udefc \\displaystyle{(\\mathcal{H}u)}_{t}^{\\alpha} = u t \u03b1 + \u2211 \u03b2 = 0 d \u2212 1 \u2211 j = 0 t \ud835\uddb3 \u03b1 \u200b \u03b2 \u200b q t \u03b2 \u200b h t \u2212 j \u03b2 \u200b k j \u03b2 \u200b v j \u03b2 absent superscript subscript \ud835\udc62 \ud835\udc61 \ud835\udefc superscript subscript \ud835\udefd 0 \ud835\udc51 1 superscript subscript \ud835\udc57 0 \ud835\udc61 superscript \ud835\uddb3 \ud835\udefc \ud835\udefd superscript subscript \ud835\udc5e \ud835\udc61 \ud835\udefd subscript superscript \u210e \ud835\udefd \ud835\udc61 \ud835\udc57 superscript subscript \ud835\udc58 \ud835\udc57 \ud835\udefd superscript subscript \ud835\udc63 \ud835\udc57 \ud835\udefd \\displaystyle=u_{t}^{\\alpha}+\\sum_{\\beta=0}^{d-1}\\sum_{j=0}^{t}\\mathsf{T}^{\\alpha\\beta}q_{t}^{\\beta}{\\color[rgb]{0.0,0.08,0.66}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.08,0.66}h^{\\beta}_{t-j}}k_{j}^{\\beta}v_{j}^{\\beta} (2)\n\nwhere is a collection of implicit long convolution filters and is an output projection that mixes channels across the sequence length. Hyena applies SISO convolutions, independently on each channel. This multi SISO approach has been successful in other convolution-based sequence processors such as S4 (Gu et al., 2022b, a) or H3 (Fu et al., 2023) (as well as linear input-varying models (Gu & Dao, 2023)). 2.2 State-Space Realization of Convolutions\n\nThis work delves deep into the design of the individual SISO filters , tailored for sequence processing architectures leveraging classical frequency-domain analysis techniques from signal processing and control theory. More specifically, we specialize on those filters that admit a finite-dimensional state-space (lumped) realization, i.e. the input-output relation of their induced convolution operator can be expressed as:\n\nx t + 1 = \ud835\udda0 \u200b x t + \ud835\udda1 \u200b u t y t = \ud835\udda2 \u200b x t + h 0 \u200b u t , t \u21a6 h t = { h 0 t = 0 \ud835\udda2\ud835\udda0 t \u2212 1 \u200b \ud835\udda1 t > 0 maps-to subscript \ud835\udc65 \ud835\udc61 1 absent \ud835\udda0 subscript \ud835\udc65 \ud835\udc61 \ud835\udda1 subscript \ud835\udc62 \ud835\udc61 subscript \ud835\udc66 \ud835\udc61 absent \ud835\udda2 subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 \ud835\udc61 subscript \u210e \ud835\udc61 cases subscript \u210e 0 \ud835\udc61 0 superscript \ud835\udda2\ud835\udda0 \ud835\udc61 1 \ud835\udda1 \ud835\udc61 0 \\begin{aligned} x_{t+1}&=\\mathsf{A}x_{t}+\\mathsf{B}u_{t}\\\\\ny_{t}&=\\mathsf{C}x_{t}+h_{0}u_{t}\\end{aligned}~{},\\quad t\\mapsto h_{t}=\\begin{cases}h_{0}&t=0\\\\\n\\mathsf{C}\\mathsf{A}^{t-1}\\mathsf{B}&t>0\\end{cases} (3)\n\nwith a finite-dimensional state (), input , and output . Our trainable degrees of freedom are the matrices , , , and . The initial condition is usually set to zero such that is a pure convolution. A major advantage of having a state-space realization is the possibility to switch between its convolution mode, for training, and recurrent mode, for efficient auto-regressive generation (see Massaroli et al., 2023 and Section A for further details and denominations). State-space representations\n\nParametrization of lumped convolutional filters with temporal dynamics, i.e., state-space parametrization present several challenges. Firstly, recurrence with dense transition matrices are computationally expensive, amounting to a computational complexity of . To make such systems feasible various recent works proposing efficient state-space models have resorted to diagonalization (Gu et al., 2022a; Smith et al., 2023; Orvieto et al., 2023) and low-rank add-ons (Gu et al., 2022b) of . As will be further uncovered when analyzing the dual representation, transfer functions, these restrictions impose a constraint on the expressivity of its convolutional filter , given a fixed state-size . Moreover, despite various works on optimizing parallel inference efficiency, associative scans utilized in (Martin & Cundy, 2018; Smith et al., 2023; Orvieto et al., 2023; Gu & Dao, 2023) still incur considerable memory costs due to its state-multiplicative complexity of , whereas fast Cauchy and Vandermonde matrix-vector products (Pan, 2001) utilized in (Gu et al., 2022b, a) present an improved state-additive space complexity of , but heavily rely on custom platform specific low-level optimizations. 3 Training SSMs in the frequency domain\n\nLinear time-invariant dynamical systems (1) are completely characterized by their impulse response , and in the case they admit a state-space realization (3), their system matrices . 3.1 Transfer Function Representation\n\nAn alternative complete representation of (3) is its transfer function , defined as the -transform of the impulse response for all where the sum converges. The transfer function of a state-space model is a proper222i.e. such that the denominator\u2019s order is not less than the numerator\u2019s one. rational function of ,\n\nH \u200b ( z ) \ud835\udc3b \ud835\udc67 \\displaystyle H(z) = h 0 + \ud835\udda2 \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 absent subscript \u210e 0 \ud835\udda2 superscript \ud835\udc67 \ud835\udda8 \ud835\udda0 1 \ud835\udda1 \\displaystyle=h_{0}+\\mathsf{C}(z\\mathsf{I}-\\mathsf{A})^{-1}\\mathsf{B} (4) = h 0 + b 1 \u200b z \u2212 1 + \u22ef + b n \u200b z \u2212 n 1 + a 1 \u200b z \u2212 1 + \u22ef + a n \u200b z \u2212 n . absent subscript \u210e 0 subscript \ud835\udc4f 1 superscript \ud835\udc67 1 \u22ef subscript \ud835\udc4f \ud835\udc5b superscript \ud835\udc67 \ud835\udc5b 1 subscript \ud835\udc4e 1 superscript \ud835\udc67 1 \u22ef subscript \ud835\udc4e \ud835\udc5b superscript \ud835\udc67 \ud835\udc5b \\displaystyle=h_{0}+\\frac{b_{1}z^{-1}+~{}\\cdots~{}+b_{n}z^{-n}}{1+a_{1}z^{-1}+~{}\\cdots~{}+a_{n}z^{-n}}. Refer to A.2 for complete derivations. As discrete convolutions are the dual operation to element-wise multiplication under -transform, the input-output relation of any LTI system can be equivalently characterized by ,\n\ny t = ( h \u2217 u ) t \u21d4 Y \u200b ( z ) = H \u200b ( z ) \u200b U \u200b ( z ) \u21d4 subscript \ud835\udc66 \ud835\udc61 subscript \u210e \ud835\udc62 \ud835\udc61 \ud835\udc4c \ud835\udc67 \ud835\udc3b \ud835\udc67 \ud835\udc48 \ud835\udc67 y_{t}=(h*u)_{t}~{}~{}\\Leftrightarrow~{}~{}Y(z)=H(z)U(z)\n\nwhere is defined outside the circle in the complex plane whose radius is the amplitude of the largest eigenvalue of the state transition matrix .",
    "statefreeinf-1": "The -transform is a projection of the sequence onto a power basis for . This basis is not orthogonal unless . That is the basis of the discrete-time Fourier transform . Hence, the discrete-time Fourier transform of the signal is defined as , i.e. it is the transfer function evaluated at . We say that sequences live in the time domain and their (or ) transforms in the frequency domain. We argue that parametrizing state-space models via their transfer function (i.e. making the learnable parameters), encompasses previous representations of SSMs such as using structured matrices (Fu et al., 2023; Gu et al., 2022b) or modal canonical forms (Gu et al., 2022a; Orvieto et al., 2023; Smith et al., 2023; Fu et al., 2023). Coordinate invariance of the transfer function\n\nNotably, the transfer function is an invariant of the system: if an invertible change of variables is applied to the state-space representation, the transfer function parameters remain unchanged.",
    "statefreeinf-2": "Without loss of generality let . Proof. The proof is classic and can be found in (Chen, 1998) and follows from the definition of equivalence transformation. Consider the state-space matrices under a change of variables , for some invertible\n\n\ud835\udda0 ^ = \ud835\uddaa\ud835\udda0\ud835\uddaa \u2212 1 , \ud835\udda1 ^ = \ud835\uddaa\ud835\udda1 , \ud835\udda2 ^ = \ud835\udda2\ud835\uddaa \u2212 1 . formulae-sequence ^ \ud835\udda0 superscript \ud835\uddaa\ud835\udda0\ud835\uddaa 1 formulae-sequence ^ \ud835\udda1 \ud835\uddaa\ud835\udda1 ^ \ud835\udda2 superscript \ud835\udda2\ud835\uddaa 1 \\hat{\\mathsf{A}}=\\mathsf{K}\\mathsf{A}\\mathsf{K}^{-1},~{}~{}\\hat{\\mathsf{B}}=\\mathsf{K}\\mathsf{B},~{}~{}\\hat{\\mathsf{C}}=\\mathsf{C}\\mathsf{K}^{-1}. The transformed transfer function is given by\n\nH ^ \u200b ( z ) = \ud835\udda2\ud835\uddaa \u2212 1 \u200b [ \ud835\uddaa \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u200b \ud835\uddaa \u2212 1 ] \u2212 1 \u200b \ud835\uddaa\ud835\udda1 = H \u200b ( z ) ^ \ud835\udc3b \ud835\udc67 superscript \ud835\udda2\ud835\uddaa 1 superscript delimited-[] \ud835\uddaa \ud835\udc67 \ud835\udda8 \ud835\udda0 superscript \ud835\uddaa 1 1 \ud835\uddaa\ud835\udda1 \ud835\udc3b \ud835\udc67 \\hat{H}(z)=\\mathsf{C}\\mathsf{K}^{-1}[\\mathsf{K}(z\\mathsf{I}-\\mathsf{A})\\mathsf{K}^{-1}]^{-1}\\mathsf{K}\\mathsf{B}=H(z)\n\nThis emergent coordinate invariance should be of warning to most attempts at modeling filters by directly learning either dense or structured state-space matrices as such: there are infinitely many equivalent state-space realizations that map to the same system. This also demonstrates that dense SSM parametrizations are inefficient in their use of parameters with respect to its expressivity. Expressivity of the transfer function\n\nAny impulse response that can be represented using dense matrices\u2014of parameters with stable dynamics\u2014can also be described using rational transfer functions with just parameters. This is demonstrated in the derivations presented in Section A.3. It illustrates that one can calculate the parameters of the transfer functions , given any state-space parameterization , through the following method:\n\na \ud835\udc4e \\displaystyle a = \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \u200b ( \ud835\uddbe\ud835\uddc2\ud835\uddc0 \u200b ( \ud835\udda0 ) ) , absent \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \ud835\uddbe\ud835\uddc2\ud835\uddc0 \ud835\udda0 \\displaystyle={\\sf poly}({\\sf eig}(\\mathsf{A})), (5) b \ud835\udc4f \\displaystyle b = \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \u200b ( \ud835\uddbe\ud835\uddc2\ud835\uddc0 \u200b ( \ud835\udda0 \u2212 \ud835\udda1\ud835\udda2 ) ) + \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \u200b ( \ud835\uddbe\ud835\uddc2\ud835\uddc0 \u200b ( \ud835\udda0 ) ) \u200b ( h 0 \u2212 1 ) , absent \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \ud835\uddbe\ud835\uddc2\ud835\uddc0 \ud835\udda0 \ud835\udda1\ud835\udda2 \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \ud835\uddbe\ud835\uddc2\ud835\uddc0 \ud835\udda0 subscript \u210e 0 1 \\displaystyle={\\sf poly}({\\sf eig}(\\mathsf{A}-\\mathsf{B}\\mathsf{C}))+{\\sf poly}({\\sf eig}(\\mathsf{A}))(h_{0}-1),\n\nin which computes the coefficients of a polynomial given its roots . Parallel to change of variable techniques such as diagonalization of employed in time-domain state-space realizations, partial fraction decomposition of transfer functions can not only provide alternative representations of state-space models, but also intuitive insights on the expressivity of these models. As an example, by simply taking the first order partial fraction decomposition of a rational transfer function , i.e.,\n\nH \u200b ( z ) = \u2211 i = 1 n r i z \u2212 \u03bb i + h 0 \ud835\udc3b \ud835\udc67 superscript subscript \ud835\udc56 1 \ud835\udc5b subscript \ud835\udc5f \ud835\udc56 \ud835\udc67 subscript \ud835\udf06 \ud835\udc56 subscript \u210e 0 H(z)=\\sum_{i=1}^{n}{\\frac{r_{i}}{z-\\lambda_{i}}}+h_{0} (6)\n\nin which , we obtain the diagonal time-domain parameterization. Its equivalence can be shown by simply breaking down the geometric series , and applying the inverse -transform ( is an impulse at time-step ), resulting in the diagonal SSM convolutional kernel for . Looking further, we observe that, like (4), it contains trainable parameters, but does not permit repeated roots, i.e. , thereby demonstrating its limited expressivity. 3.2 State-Free Parallel Inference\n\nFor attaining sub-quadratic parallel inference speeds, the approach taken by S4, S4D, and SpaceTime predominantly hinges on the efficient computation of its length- truncated impulse response :\n\nh t = { h 0 t = 0 \ud835\udda2\ud835\udda0 t \u2212 1 \u200b \ud835\udda1 0 < t \u2264 \u2113 0 t > \u2113 , h_{t}=\\left\\{\\begin{matrix}[l]h_{0}&t=0\\\\\n\\mathsf{C}\\mathsf{A}^{t-1}\\mathsf{B}&0<t\\leq\\ell\\\\\n0&t>\\ell\\end{matrix}\\right., (7)\n\nor its corresponding spectrum for downstream integration with the sub-quadratic convolution algorithm, , described in (Burrus & Parks, 1985; Selesnick & Burrus, 2017; Fu et al., 2024). Adopting a parallel approach for rational transfer function, we reveal that can be computed in a state-free manner, incurring space and time complexities of and , respectively. This is achieved through the evaluation of the truncated transfer function across the roots of unity, as delineated below. Firstly, we demonstrate that an impulse response of length-, when expressed in the -domain as , can be efficiently transformed into its time-domain representation in the following manner. Proof. \ud835\uddc2\ud835\udda5\ud835\udda5\ud835\uddb3 m \u200b ( ( H \u2113 \u200b ( z ) ) z \u2208 \ud835\udd4b m ) t subscript \ud835\uddc2\ud835\udda5\ud835\udda5\ud835\uddb3 \ud835\udc5a subscript subscript subscript \ud835\udc3b \u2113 \ud835\udc67 \ud835\udc67 subscript \ud835\udd4b \ud835\udc5a \ud835\udc61 \\displaystyle\\mathsf{iFFT}_{m}\\big{(}(H_{\\ell}(z))_{z\\in\\mathbb{T}_{m}}\\big{)}_{t} = 1 m \u200b \u2211 z \u2208 \ud835\udd4b m H \u2113 \u200b ( z ) \u200b z t absent 1 \ud835\udc5a subscript \ud835\udc67 subscript \ud835\udd4b \ud835\udc5a subscript \ud835\udc3b \u2113 \ud835\udc67 superscript \ud835\udc67 \ud835\udc61 \\displaystyle=\\frac{1}{m}\\sum_{z\\in\\mathbb{T}_{m}}H_{\\ell}(z)z^{t} (9) = 1 m \u200b \u2211 z \u2208 \ud835\udd4b m \u2211 j = 0 \u2113 \u2212 1 h j \u200b z t \u2212 j absent 1 \ud835\udc5a subscript \ud835\udc67 subscript \ud835\udd4b \ud835\udc5a superscript subscript \ud835\udc57 0 \u2113 1 subscript \u210e \ud835\udc57 superscript \ud835\udc67 \ud835\udc61 \ud835\udc57 \\displaystyle=\\frac{1}{m}\\sum_{z\\in\\mathbb{T}_{m}}\\sum_{j=0}^{\\ell-1}{h_{j}z^{t-j}} = 1 m \u200b \u2211 j = 0 \u2113 \u2212 1 h j \u200b { m t \u2212 j = 0 0 otherwise absent 1 \ud835\udc5a superscript subscript \ud835\udc57 0 \u2113 1 subscript \u210e \ud835\udc57 cases \ud835\udc5a \ud835\udc61 \ud835\udc57 0 0 otherwise \\displaystyle=\\frac{1}{m}\\sum_{j=0}^{\\ell-1}h_{j}\\begin{cases}m&t-j=0\\\\\n0&\\text{otherwise}\\end{cases} = h t . absent subscript \u210e \ud835\udc61 \\displaystyle=h_{t}. Additionally, observe that the inverse application of Lemma 3.2 results in the following insight. In order to truncate the rational transfer function, we devise a \u201ctail\u201d , such that , as follows. Proof. \u2211 t = \u2113 + 1 \u221e \ud835\udda2\ud835\udda0 t \u2212 1 \u200b \ud835\udda1 \u200b z \u2212 t superscript subscript \ud835\udc61 \u2113 1 superscript \ud835\udda2\ud835\udda0 \ud835\udc61 1 \ud835\udda1 superscript \ud835\udc67 \ud835\udc61 \\displaystyle\\sum_{t=\\ell+1}^{\\infty}\\mathsf{C}\\mathsf{A}^{t-1}\\mathsf{B}z^{-t} = \ud835\udda2\ud835\udda0 \u2212 1 \u200b [ \u2211 t = \u2113 + 1 \u221e \ud835\udda0 t \u200b z \u2212 t ] \u200b \ud835\udda1 absent superscript \ud835\udda2\ud835\udda0 1 delimited-[] superscript subscript \ud835\udc61 \u2113 1 superscript \ud835\udda0 \ud835\udc61 superscript \ud835\udc67 \ud835\udc61 \ud835\udda1 \\displaystyle=\\mathsf{C}\\mathsf{A}^{-1}\\left[\\sum_{t=\\ell+1}^{\\infty}\\mathsf{A}^{t}z^{-t}\\right]\\mathsf{B} (12) = \ud835\udda2\ud835\udda0 \u2212 1 \u200b [ \ud835\udda0 \u2113 + 1 \u200b z \u2212 \u2113 \u2212 1 \u200b ( \ud835\udda8 \u2212 \ud835\udda0 \u200b z \u2212 1 ) \u2212 1 ] \u200b \ud835\udda1 absent superscript \ud835\udda2\ud835\udda0 1 delimited-[] superscript \ud835\udda0 \u2113 1 superscript \ud835\udc67 \u2113 1 superscript \ud835\udda8 \ud835\udda0 superscript \ud835\udc67 1 1 \ud835\udda1 \\displaystyle=\\mathsf{C}\\mathsf{A}^{-1}\\left[\\mathsf{A}^{\\ell+1}z^{-\\ell-1}(\\mathsf{I}-\\mathsf{A}z^{-1})^{-1}\\right]\\mathsf{B} = \ud835\udda2\ud835\udda0 \u2113 \u200b z \u2212 \u2113 \u2212 1 \u200b ( \ud835\udda8 \u2212 \ud835\udda0 \u200b z \u2212 1 ) \u2212 1 \u200b \ud835\udda1 absent superscript \ud835\udda2\ud835\udda0 \u2113 superscript \ud835\udc67 \u2113 1 superscript \ud835\udda8 \ud835\udda0 superscript \ud835\udc67 1 1 \ud835\udda1 \\displaystyle=\\mathsf{C}\\mathsf{A}^{\\ell}z^{-\\ell-1}(\\mathsf{I}-\\mathsf{A}z^{-1})^{-1}\\mathsf{B} = \ud835\udda2\ud835\udda0 \u2113 \u200b z \u2212 \u2113 \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 . absent superscript \ud835\udda2\ud835\udda0 \u2113 superscript \ud835\udc67 \u2113 superscript \ud835\udc67 \ud835\udda8 \ud835\udda0 1 \ud835\udda1 \\displaystyle=\\mathsf{C}\\mathsf{A}^{\\ell}z^{-\\ell}(z\\mathsf{I}-\\mathsf{A})^{-1}\\mathsf{B}. Since , we can derive the length- truncated transfer function in the following manner,\n\nH \u2113 \u200b ( z ) = H \u200b ( z ) \u2212 H ~ \u2113 \u200b ( z ) = \ud835\udda2 ~ \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 , \ud835\udda2 ~ = \ud835\udda2 \u200b ( \ud835\udda8 \u2212 \ud835\udda0 \u2113 ) \u2254 b ~ , formulae-sequence subscript \ud835\udc3b \u2113 \ud835\udc67 \ud835\udc3b \ud835\udc67 subscript ~ \ud835\udc3b \u2113 \ud835\udc67 ~ \ud835\udda2 superscript \ud835\udc67 \ud835\udda8 \ud835\udda0 1 \ud835\udda1 ~ \ud835\udda2 \ud835\udda2 \ud835\udda8 superscript \ud835\udda0 \u2113 \u2254 ~ \ud835\udc4f \\begin{gathered}H_{\\ell}(z)=H(z)-\\tilde{H}_{\\ell}(z)=\\tilde{\\mathsf{C}}(z\\mathsf{I}-\\mathsf{A})^{-1}\\mathsf{B},\\\\\n\\tilde{\\mathsf{C}}=\\mathsf{C}(\\mathsf{I}-\\mathsf{A}^{\\ell})\\coloneqq\\tilde{b},\\end{gathered} (13)\n\nNonetheless, in practice, we circumvent the computation of , by directly optimizing during the training phase, and only apply the inverse correction , upon deployment, i.e.",
    "statefreeinf-3": "autoregressive inference. This is equivalent to the approach taken by (Gu et al., 2022b, a; Zhang et al., 2023), on the \u201ctruncated SSM generating function\u201d. To evaluate the truncated rational function, we recognize that:\n\n1. Rational functions are composed of polynomials. 2. Evaluating polynomials on the roots of unity, is equivalent to applying a fast Fourier transform over its coefficients. Proof. By definition of the Fourier Transform. \u220e\n\nIn light of Lemma 3.4, it becomes evident that for any -th order truncated rational transfer function parameterized by , by setting , and for (zero padding of polynomial coefficients), the spectrum of the impulse response can be computed with:\n\nas demonstrated in Algorithm 1.",
    "statefreeinf-4": "Finally to obtain , we simply apply Equation 8. Importantly, the proposed parallel inference algorithm relies solely on the algorithms, which have space and time complexities of and , respectively. The ubiquitous algorithm is widely used and already have low-level optimizations applied across several platforms, subsequently optimizing RTF across those platforms. 3.3 Fast Companion Recurrence\n\nRational transfer functions could directly be translated into a structured state-space model of the following form:\n\nx t + 1 = [ \u2212 a 1 \u2212 a 2 \u22ef \u2212 a n 1 0 \u22ef 0 0 1 \u22ef 0 \u22ee \u22ee \u22f1 \u22ee 0 0 \u22ef 0 ] \u200b x t + [ 1 0 0 \u22ee 0 ] \u200b u t y t = [ b 1 b 2 \u22ef b n ] \u200b x t + h 0 \u200b u t . subscript \ud835\udc65 \ud835\udc61 1 matrix subscript \ud835\udc4e 1 subscript \ud835\udc4e 2 \u22ef subscript \ud835\udc4e \ud835\udc5b 1 0 \u22ef 0 0 1 \u22ef 0 \u22ee \u22ee \u22f1 \u22ee 0 0 \u22ef 0 subscript \ud835\udc65 \ud835\udc61 matrix 1 0 0 \u22ee 0 subscript \ud835\udc62 \ud835\udc61 subscript \ud835\udc66 \ud835\udc61 matrix subscript \ud835\udc4f 1 subscript \ud835\udc4f 2 \u22ef subscript \ud835\udc4f \ud835\udc5b subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 \\begin{gathered}x_{t+1}=\\begin{bmatrix}-a_{1}&-a_{2}&\\cdots&-a_{n}\\\\\n1&0&\\cdots&0\\\\\n0&1&\\cdots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\cdots&0\\end{bmatrix}x_{t}+\\begin{bmatrix}1\\\\\n0\\\\\n0\\\\\n\\vdots\\\\\n0\\end{bmatrix}u_{t}\\\\\ny_{t}=\\begin{bmatrix}b_{1}&b_{2}&\\cdots&b_{n}\\end{bmatrix}x_{t}+h_{0}u_{t}.\\end{gathered} (16)\n\nThe structure (companion form) permits fast companion recurrence via the combination of shift operations, dot products resulting in single time-step space and time complexities of . Refer to Section B.1 for the full derivation. Moreover, as discussed in (Massaroli et al., 2023), the companion realization of a state-space model can be leveraged to perform fast prefilling, in which the state can be obtained from with computation complexity of . Fast prefilling is applicable in extensive language modeling applications, where the model, upon receiving a length- prompt from the user, autoregressively generates subsequent prompts using a constant-time recurrent algorithm as described above. For state-space realizations that are not in companion form, they must first be transformed into the companion form using Equation (5) to perform fast prefilling. Unlike SpaceTime (Zhang et al., 2023) that shares the same matrix but trains both and , we adhere to the true companion form during training, in which the matrix is a constant as shown in Equation (16), while ( matrix) is trained. 3.4 Stable Parametrization\n\nTo prevent numerical instabilities, it is important to configure SSMs to exhibit stable dynamics. The choice of parameters for the state-transition matrix significantly influences their stability. For rational transfer functions, the roots of the denominator polynomial (the pole) must lie within the complex unit circle, i.e. to prevent unstable dynamics (Chen, 1998). Unlike diagonal SSMs, with first order roots (Equation (6)), ensuring that the coefficients of a high order polynomial are such that its roots remain within the complex unit circle presents a complex challenge, as highlighted in (Alomari & Chesneau, 2022). SpaceTime (Zhang et al., 2023) adopts Montel\u2019s method (Horn & Johnson, 1985; Alomari & Chesneau, 2022), a technique that, for a Monic polynomial (where ), constrains the remaining coefficients in a manner described by:\n\n\u2211 i = 1 n \u2212 1 | a i | \u2264 1 . superscript subscript \ud835\udc56 1 \ud835\udc5b 1 subscript \ud835\udc4e \ud835\udc56 1 \\sum_{i=1}^{n-1}{|a_{i}|}\\leq 1. (17)\n\nHowever, as depicted in Figure 4, the application of Montel\u2019s method not only ensures that the roots are confined within the unit circle but also limits them to a specific subset of the stable region. This limitation could potentially diminish performance, a phenomenon supported by the findings in Table 5. To mitigate this, we propose an alternative initialization strategy for the SSM coefficients, aiming to position them as far as possible from violating Montel\u2019s constraint:\n\nargmin a \u2061 ( \u2211 i = 1 n \u2212 1 | a i | ) = \ud835\udfce , subscript argmin \ud835\udc4e superscript subscript \ud835\udc56 1 \ud835\udc5b 1 subscript \ud835\udc4e \ud835\udc56 0 \\operatorname{argmin}_{a}(\\sum_{i=1}^{n-1}{|a_{i}|})=\\mathbf{0}, (18)\n\nwhere . We denote this initialization scheme as the zero initialization. Our ablation tests (Table 5) and comparisons against SpaceTime on the Long Range Arena (Tay et al., 2021) benchmark (Table 1) show enhanced training stability and consequently, improved performance when adopting the zero initialization scheme.333Unless explicitly stated otherwise, all results presented in this paper adopts the zero initialization scheme with . 4 Experimental Results\n\nIn this section, we conduct an empirical evaluation of RTF in comparison to other state-space models and sequence models. Section 4.1 is dedicated to assessing memory usage and processing speed. Sections 4.2 and 4.3 examine the ability for SSMs to memorize and model long-range dependencies. Finally, their ability to model language is assessed in sections 4.4 and 4.5. 4.1 Efficiency Profiling\n\nWe profiled GPU memory usage between a parallel scan-based S5 model and RTF across different sequence lengths and state sizes at channel dimensions of . The results depicted in Figure 1 reveal a consistent trend, wherein the memory consumption for the scan operation rises in conjunction with state size and sequence length, while it solely escalates with sequence length for RTF. This phenomenon can be attributed to the aforementioned state-free characteristic of RTF\u2019s inference algorithm, which casts its parameters with size of the state dimension onto the sequence length for parallel inference. We also observed a similar trend for the inference latency which is further detailed in Appendix C.1. Next, we profiled inference latency across different SSMs of varying state-sizes over a suite of six LRA tasks, facilitating speed comparisons across a wide range of model architectures. Figure 3 reports the median inference latency per SSM layer across 75 training iterations. The results show a recurring trend, wherein RTF\u2019s inference latency remained consistent regardless of state size and conversely, S4D and S4 experienced slower speeds particularly at higher orders, due to the utilization of the slower Vandermonde or Cauchy matrix-vector product algorithms respectively, which have computational complexity of as opposed to RTF\u2019s . 4.2 Modeling Long Range Dependencies\n\nThe Long Range Arena (LRA) benchmark has become a common ground for testing various sequence models including SSMs (Gu et al., 2022b, a; Smith et al., 2023; Hasani et al., 2023) and Transformers (Vaswani et al., 2017; Choromanski et al., 2021). It is composed of six classification tasks with long range input sequences of lengths ranging from to . We conducted these experiments on RTF along with S4, S4D, and SpaceTime (Zhang et al., 2023) as presented in Table 1. RTF obtained strong results in several LRA tasks, including attaining state-of-the-art performance on Retrieval, and among attention-free approaches, the average score. However for Path-X, RTF was unable to learn a policy beyond random guessing when the state-size was fixed to 64, prompting an increase to 2048. Nevertheless, due to RTF\u2019s state-free parallel inference algorithm, this increase in state-size did not impact GPU memory consumption nor training speed as evidenced in Figure 3. 4.3 Synthetic Memorization Tasks\n\nRecurrences have traditionally struggled with vanishing and exploding gradients, making memorization tasks challenging (Bengio et al., 1994; Pascanu et al., 2013). To evaluate the memorization capabilities of our state-space model, we benchmark them against two synthetic memorization tasks: Copying and Delay. The Copying task, akin to (Arjovsky et al., 2016), presents SSMs with 1024 length sequences of 64 discrete states sampled uniformly, which the model is then tasked to recall all 1024 tokens in order. Each model was given 10k training samples for 50 epochs, and was tested with 1000 unseen samples. The Delay task, which was also used to ablate HiPPO SSM initialization schemes (Gu et al., 2023), simply tests the model\u2019s ability to delay a continuous white noise by 1000 time steps. As reported by Gu et al., LSTMs and Transformers struggle on this seemingly simple task, and are unable to improve beyond a random guessing policy. The primary distinction between Copying and Delay is whether the input data is discrete or continuous. More detailed experimental setup could be found in C.3. From the results reported in Table 2, we observed that at higher state-sizes, RTF could more accurately copy and delay data. S4 on the other hand struggled on Copying, showing no improvements beyond the state-size of 256. It is also worth noting that on both synthetic tasks, unlike the discrete-time RTF SSM, S4, being continuous-time required careful consideration of the initialization and interplay between the time-constant and the transition matrix for reasonable performance. 4.4 Laughing Hyena Distillation\n\nHyena (Poli et al., 2023a) and MultiHyena (Massaroli et al., 2023) operators utilize a diverse array of filters, encompassing short convolutional filters - filters implicitly parameterized by multi-layer perceptrons (MLP) (Poli et al., 2023a; Sitzmann et al., 2020; Romero et al., 2022), and diagonal SSMs (Massaroli et al., 2023). Notably, Hyena operators with MLP-parameterized filters have demonstrated superior performance compared to other convolutional and recurrent methods, as highlighted in (Aky\u00fcrek et al., 2024; Bhattamishra et al., 2024). Despite their effectiveness, these filters lack constant-time autoregressive inference speeds desired in applications such as language modeling. This limitation has led to the investigation of distilling MLP-based filters into SSMs, a process detailed in Laughing Hyena (Massaroli et al., 2023). Here, we look into distillation of MLP-based filters, using a 160M parameter multi-head StripedHyena (Poli et al., 2023b) language model, trained on The Pile (Gao et al., 2021), and compare distillation performances between RTF and a diagonal SSM employed in Laughing Hyena (LH), both of which boast highly efficient autoregressive algorithms. Table 3 reports distillation errors and downstream LM-Evaluation-Harness scores (Gao et al., 2023). Interestingly, despite the theoretically superior expressiveness of RTF models, we observed that the modal representation employed in LH exhibits more favorable training dynamics for distillation at state-sizes 16 and 64, as evidenced by the distillation MSE. However with , RTF outperforms LH while maintaining comparable downstream evaluation performances to the baseline model, making it a good candidate for unlocking efficient constant-speed autoregressive inference on Hyena language models. 4.5 WikiText103 Language Modeling\n\nIn addition to evaluating the language modeling capabilities of state space models through distillation techniques, their performance when directly trained on autoregressive cross-entropy loss (Radford et al., 2018) was investigated on the well-established WikiText-103 dataset. We used a Hyena operator and replaced its filters with RTF, which we refer to as Hyena-RTF. As shown in Table 4, Hyena-RTF outperforms both the Transformer and Hyena baselines on WikiText103. Additionally, RTF without the Hyena operator structure was compared against S4 and S4D on a pilot experiment further described in Appendix C.5.1, which similarly indicated relatively strong language modeling capability among other LTI SSMs. These results signal a promising potential for further scaling RTF on larger models and datasets. 5 Conclusion\n\nIn this study, we explore state-space model (SSM) parametrization via their dual representation, transfer functions. We systematically unveiled the realization of SSMs through rational transfer functions (RTF), demonstrating state-of-the-art efficiency through a state-free parallel inference algorithm, while maintaining the expressiveness of a dense SSM. Our experiments revealed that RTFs are effective for modeling long-range dependencies and processing language, and also exhibits improvements in comparison to the S4 model across synthetic memorization tasks with higher state-sizes. The results of our investigation suggest that RTFs hold significant potential for modeling signals across a variety of other domains. 6 Acknowledgements\n\nT.S. was partially supported by JSPS KAKENHI (20H00576) and JST CREST (JPMJCR2015). References\n\nAky\u00fcrek et al. (2024) Aky\u00fcrek, E., Wang, B., Kim, Y., and Andreas, J. In-context language learning: Architectures and algorithms, 2024. Alomari & Chesneau (2022) Alomari, M.",
    "statefreeinf-5": "W. and Chesneau, C. Bounding the zeros of polynomials using the frobenius companion matrix partitioned by the cartesian decomposition.",
    "statefreeinf-6": "Algorithms, 15(6), 2022. ISSN 1999-4893. doi: 10.3390/a15060184. URL https://www.mdpi.com/1999-4893/15/6/184. Amos et al. (2024) Amos, I., Berant, J., and Gupta, A. Never train from scratch: Fair comparison of long-sequence models requires data-driven priors. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=PdaPky8MUn. Arjovsky et al. (2016) Arjovsky, M., Shah, A., and Bengio, Y. Unitary evolution recurrent neural networks. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pp. 1120\u20131128. JMLR.org, 2016. Baevski & Auli (2019) Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ByxZX20qFQ. Bengio et al. (1994) Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is difficult.",
    "statefreeinf-7": "IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994. doi: 10.1109/72.279181. Bhattamishra et al. (2024) Bhattamishra, S., Patel, A., Blunsom, P., and Kanade, V. Understanding in-context learning in transformers and LLMs by learning to learn discrete functions.",
    "statefreeinf-8": "In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ekeyCgeRfC. Blelloch (1990) Blelloch, G. E. Prefix sums and their applications. In Sythesis of parallel algorithms, pp. 35\u201460. Morgan Kaufmann Publishers Inc., 1990. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.6430. Bradbury et al. (2018) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Burrus & Parks (1985) Burrus, C. S. and Parks, T. Convolution algorithms. Citeseer: New York, NY, USA, 6:15, 1985. Chen (1998) Chen, C.-T. Linear System Theory and Design. Oxford University Press, Inc., USA, 3rd edition, 1998.",
    "statefreeinf-9": "ISBN 0195117778. Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling.",
    "statefreeinf-10": "In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=a7APmM4B9d. Choromanski et al. (2021) Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D.",
    "statefreeinf-11": "B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH. Chung et al. (2014) Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014 Workshop on Deep Learning, December 2014, 2014. Dauphin et al. (2017) Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, pp. 933\u2013941. JMLR.org, 2017. Fu et al. (2023) Fu, D. Y., Dao, T., Saab, K.",
    "statefreeinf-12": "K., Thomas, A. W., Rudra, A., and R\u00e9, C. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations, 2023. Fu et al. (2024) Fu, D. Y., Kumbong, H., Nguyen, E., and R\u00e9, C. FlashFFTConv: Efficient convolutions for long sequences with tensor cores. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gPKTTAfYBp. Gao et al. (2021) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling.",
    "statefreeinf-13": "CoRR, abs/2101.00027, 2021. URL https://arxiv.org/abs/2101.00027. Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Glorot & Bengio (2010) Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks.",
    "statefreeinf-14": "In Teh, Y. W. and Titterington, M. (eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May 2010. PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html. Gonzalez & Woods (2008) Gonzalez, R. C. and Woods, R. E. Digital image processing. Prentice Hall, Upper Saddle River, N.J., 2008. ISBN 9780131687288 013168728X 9780135052679 013505267X. Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023.",
    "statefreeinf-15": "Gu et al. (2022a) Gu, A., Goel, K., Gupta, A., and R\u00e9, C. On the parameterization and initialization of diagonal state space models.",
    "statefreeinf-16": "In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 35971\u201335983. Curran Associates, Inc., 2022a. Gu et al. (2022b) Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=uYLFoz1vlAC. Gu et al. (2023) Gu, A., Johnson, I., Timalsina, A., Rudra, A., and Re, C. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=klK17OQ3KB. Gupta et al. (2022) Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35 - 36th Conference on Neural Information Processing Systems, NeurIPS 2022, Advances in Neural Information Processing Systems. Neural information processing systems foundation, 2022. Publisher Copyright: \u00a9 2022 Neural information processing systems foundation. All rights reserved.; 36th Conference on Neural Information Processing Systems, NeurIPS 2022 ; Conference date: 28-11-2022 Through 09-12-2022. Hasani et al. (2023) Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g4OTKRKfS7R. He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.",
    "statefreeinf-17": "In 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1026\u20131034, 2015. doi: 10.1109/ICCV.2015.123. He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90. Hendrycks & Gimpel (2023) Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus), 2023. Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Horn & Johnson (1985) Horn, R. A. and Johnson, C. R. Matrix Analysis. Cambridge University Press, 1985. Katsch (2023) Katsch, T. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023. Kidger et al. (2020) Kidger, P., Morrill, J., Foster, J., and Lyons, T. Neural controlled differential equations for irregular time series.",
    "statefreeinf-18": "Advances in Neural Information Processing Systems, 33:6696\u20136707, 2020. Krizhevsky (2009) Krizhevsky, A. Learning multiple layers of features from tiny images.",
    "statefreeinf-19": "2009. URL https://api.semanticscholar.org/CorpusID:18268744. Linsley et al. (2018) Linsley, D., Kim, J., Veerabadran, V., Windolf, C., and Serre, T. Learning long-range spatial dependencies with horizontal gated recurrent units. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf. Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Maas et al. (2011) Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Lin, D., Matsumoto, Y., and Mihalcea, R. (eds.), Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1015. Martin & Cundy (2018) Martin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HyUNwulC-. Massaroli et al. (2021) Massaroli, S., Poli, M., Sonoda, S., Suzuki, T., Park, J., Yamashita, A., and Asama, H. Differentiable multiple shooting layers. Advances in Neural Information Processing Systems, 34:16532\u201316544, 2021. Massaroli et al. (2023) Massaroli, S., Poli, M., Fu, D. Y., Kumbong, H., Parnichkun, R. N., Romero, D. W., Timalsina, A., McIntyre, Q., Chen, B., Rudra, A., Zhang, C., Re, C., Ermon, S., and Bengio, Y. Laughing hyena distillery: Extracting compact recurrences from convolutions.",
    "statefreeinf-20": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=OWELckerm6. Nangia & Bowman (2018) Nangia, N. and Bowman, S. ListOps: A diagnostic dataset for latent tree learning.",
    "statefreeinf-21": "In Cordeiro, S. R., Oraby, S., Pavalanathan, U., and Rim, K. (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 92\u201399, New Orleans, Louisiana, USA, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-4013. URL https://aclanthology.org/N18-4013. Oppenheim et al. (1999) Oppenheim, A. V., Schafer, R. W., and Buck, J. R. Discrete-Time Signal Processing. Prentice-hall Englewood Cliffs, second edition, 1999.",
    "statefreeinf-22": "Orvieto et al. (2023) Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023. Pan (2001) Pan, V. Y. Structured Matrices and Polynomials: Unified Superfast Algorithms. Springer-Verlag, Berlin, Heidelberg, 2001. ISBN 0817642404. Pascanu et al. (2013) Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks.",
    "statefreeinf-23": "In Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 1310\u20131318, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/pascanu13.html. Poli et al. (2023a) Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00e9, C. Hyena hierarchy: towards larger convolutional language models. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023a. Poli et al. (2023b) Poli, M., Wang, J., Massaroli, S., Quesnelle, J., Nguyen, E., and Thomas, A. Stripedhyena: Moving beyond transformers with hybrid signal processing models.",
    "statefreeinf-24": "2023b. Radev et al. (2009) Radev, D. R., Muthukrishnan, P., and Qazvinian, V. The ACL Anthology network corpus. In Kan, M.-Y. and Teufel, S. (eds.), Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries (NLPIR4DL), pp. 54\u201361, Suntec City, Singapore, August 2009. Association for Computational Linguistics. URL https://aclanthology.org/W09-3607. Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training.",
    "statefreeinf-25": "2018. Ren et al. (2023) Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., and Zhai, C. Sparse modular activation for efficient sequence modeling, 2023.",
    "statefreeinf-26": "Romero et al. (2022) Romero, D. W., Kuzina, A., Bekkers, E.",
    "statefreeinf-27": "J., Tomczak, J. M., and Hoogendoorn, M. Ckconv: Continuous kernel convolution for sequential data. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=8FhxBtXSl0. Sandberg (1963) Sandberg, I. W. On the theory of linear multi-loop feedback systems. Bell System Technical Journal, 42(2):355\u2013382, 1963. Selesnick & Burrus (2017) Selesnick, I. W. and Burrus, C. S. Fast convolution and filtering. In Digital Signal Processing Fundamentals, pp.",
    "statefreeinf-28": "185\u2013208. CRC Press, 2017. Sitzmann et al. (2020) Sitzmann, V., Martel, J. N., Bergman, A.",
    "statefreeinf-29": "W., Lindell, D. B., and Wetzstein, G. Implicit neural representations with periodic activation functions.",
    "statefreeinf-30": "In Proc. NeurIPS, 2020. Smith et al. (2023) Smith, J. T., Warrington, A., and Linderman, S. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Ai8Hw3AXqks. Tay et al. (2021) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Zhang et al. (2023) Zhang, M., Saab, K., Poli, M., Dao, T., Goel, K., and R\u00e9, C. Effectively modeling time series with simple discrete state spaces. International Conference on Learning Representations, 2023. Supplementary Material\n\nAuthor Contribution\n\nR.N.P. Developed the algorithm, theory, code base, and manuscript. Managed and conducted experiments. S.M. Developed the algorithm, theory, and manuscript. Supervised research. A.M. Developed the code base and manuscript. Conducted experiments and secured compute. J.S. Reviewed manuscript and assisted in writing. R.H., M.L. Reviewed manuscript and secured compute. Q.A., C.R., H.A., S.E., T.S. Supervised research. A.Y. Supervised research and secured compute. M.P. Developed the algorithm, theory and manuscript. Supervised research. Appendix A Linear System Theory\n\nThis section delves into linear system theory, including denomination of various characteristics such as lumpedness, time-invariance, etc., and also includes the analysis and derivation of -domain transfer functions. A.1 Overview and Basics\n\nLinear Systems:\n\nLinear systems consist of a series of linear equations generally expressed as:\n\ny = \ud835\udda6 \u200b u , \ud835\udc66 \ud835\udda6 \ud835\udc62 y=\\mathsf{G}u, (A.1.1)\n\nin which , , and are the input, output, and the transformation matrix, respectively. These systems adhere to the principles of linearity, including additivity and homogeneity. For the purpose of processing sequences, they can also written as:\n\ny t = \u2211 j = t 0 t \ud835\udda6 t , t \u2212 j \u200b u j , subscript \ud835\udc66 \ud835\udc61 superscript subscript \ud835\udc57 subscript \ud835\udc61 0 \ud835\udc61 subscript \ud835\udda6 \ud835\udc61 \ud835\udc61 \ud835\udc57 subscript \ud835\udc62 \ud835\udc57 y_{t}=\\sum_{j=t_{0}}^{t}\\mathsf{G}_{t,t-j}u_{j}, (A.1.2)\n\nin which, scales the input signal for the output, based on the absolute time and the relative time . Time-Invariance\n\nA linear time-invariant (LTI) system simply discards the absolute time dependence in (A.1.2) as follows:\n\ny t = \u2211 j = t 0 t h t \u2212 j \u200b u j . subscript \ud835\udc66 \ud835\udc61 superscript subscript \ud835\udc57 subscript \ud835\udc61 0 \ud835\udc61 subscript \u210e \ud835\udc61 \ud835\udc57 subscript \ud835\udc62 \ud835\udc57 y_{t}=\\sum_{j=t_{0}}^{t}h_{t-j}u_{j}. (A.1.3)\n\nThese systems are equivalent to convolutions characterized by , with a shorthand notation . is also known as the system\u2019s impulse response. As , in which is the Kronecker delta (impulse) function. Lumped Systems:\n\nLumped LTI systems (Chen, 1998) are LTI systems that can be characterized with a finite and discrete (lumped) set of states. They can be formulated as a state-space model:\n\nx t + 1 subscript \ud835\udc65 \ud835\udc61 1 \\displaystyle x_{t+1} = \ud835\udda0 \u200b x t + \ud835\udda1 \u200b u t absent \ud835\udda0 subscript \ud835\udc65 \ud835\udc61 \ud835\udda1 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\mathsf{A}x_{t}+\\mathsf{B}u_{t} (A.1.4) y t subscript \ud835\udc66 \ud835\udc61 \\displaystyle y_{t} = \ud835\udda2 \u200b x t + h 0 \u200b u t , absent \ud835\udda2 subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\mathsf{C}x_{t}+h_{0}u_{t},\n\nwhere , , , and . Unrolling the recurrence, its connection to the convolutional operation could be made clear:\n\ny 0 subscript \ud835\udc66 0 \\displaystyle y_{0} = \ud835\udda2 \u200b x 0 + h 0 \u200b u 0 absent \ud835\udda2 subscript \ud835\udc65 0 subscript \u210e 0 subscript \ud835\udc62 0 \\displaystyle=\\mathsf{C}x_{0}+h_{0}u_{0} (A.1.5) y 1 subscript \ud835\udc66 1 \\displaystyle y_{1} = \ud835\udda2 \u200b ( \ud835\udda0 \u200b x 0 + \ud835\udda1 \u200b u 0 ) + h 0 \u200b u 1 absent \ud835\udda2 \ud835\udda0 subscript \ud835\udc65 0 \ud835\udda1 subscript \ud835\udc62 0 subscript \u210e 0 subscript \ud835\udc62 1 \\displaystyle=\\mathsf{C}(\\mathsf{A}x_{0}+\\mathsf{B}u_{0})+h_{0}u_{1} y 2 subscript \ud835\udc66 2 \\displaystyle y_{2} = \ud835\udda2 \u200b ( \ud835\udda0 \u200b ( \ud835\udda0 \u200b x 0 + \ud835\udda1 \u200b u 0 ) + \ud835\udda1 \u200b u 1 ) + h 0 \u200b u 2 absent \ud835\udda2 \ud835\udda0 \ud835\udda0 subscript \ud835\udc65 0 \ud835\udda1 subscript \ud835\udc62 0 \ud835\udda1 subscript \ud835\udc62 1 subscript \u210e 0 subscript \ud835\udc62 2 \\displaystyle=\\mathsf{C}(\\mathsf{A}(\\mathsf{A}x_{0}+\\mathsf{B}u_{0})+\\mathsf{B}u_{1})+h_{0}u_{2} \u22ee \u22ee \\displaystyle\\vdots y t subscript \ud835\udc66 \ud835\udc61 \\displaystyle y_{t} = h 0 \u200b u t + \u2211 j = 1 t \ud835\udda2\ud835\udda0 j \u2212 1 \u200b \ud835\udda1 \u200b u t \u2212 j + \ud835\udda2\ud835\udda0 t \u200b x 0 absent subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 superscript subscript \ud835\udc57 1 \ud835\udc61 superscript \ud835\udda2\ud835\udda0 \ud835\udc57 1 \ud835\udda1 subscript \ud835\udc62 \ud835\udc61 \ud835\udc57 superscript \ud835\udda2\ud835\udda0 \ud835\udc61 subscript \ud835\udc65 0 \\displaystyle=h_{0}u_{t}+\\sum_{j=1}^{t}{\\mathsf{C}\\mathsf{A}^{j-1}\\mathsf{B}u_{t-j}}+\\mathsf{C}\\mathsf{A}^{t}x_{0} y t subscript \ud835\udc66 \ud835\udc61 \\displaystyle y_{t} = ( h \u2217 u ) t + \ud835\udda2\ud835\udda0 t \u200b x 0 , where \u200b h t = { h 0 t = 0 \ud835\udda2\ud835\udda0 t \u2212 1 \u200b \ud835\udda1 t > 0 . formulae-sequence absent subscript \u2217 \u210e \ud835\udc62 \ud835\udc61 superscript \ud835\udda2\ud835\udda0 \ud835\udc61 subscript \ud835\udc65 0 where subscript \u210e \ud835\udc61 cases subscript \u210e 0 \ud835\udc61 0 superscript \ud835\udda2\ud835\udda0 \ud835\udc61 1 \ud835\udda1 \ud835\udc61 0 \\displaystyle=(h\\ast u)_{t}+\\mathsf{C}\\mathsf{A}^{t}x_{0},\\;\\text{where }h_{t}=\\begin{cases}h_{0}&t=0\\\\\n\\mathsf{C}\\mathsf{A}^{t-1}\\mathsf{B}&t>0\\end{cases}. Note that all lumped LTI systems have complex exponential convolutional kernels. Non-lumped systems are not restricted to exponential convolutional kernels but cannot be directly expressed using a fixed and finite state-space, i.e. they have a non-constant time autoregressive inference complexity. Convolutional filters implicitly parameterized by MLPs such as CKConv (Romero et al., 2022) and (Poli et al., 2023a) are examples of non-lumped linear time-invariant systems. A.2 Transfer Function Realization of Lumped LTI Systems\n\nControl Theorists Derivation:\n\nBy applying the shift forward operator () in -domain to the state-space equations, we can obtain its transfer function as follows. x k + 1 subscript \ud835\udc65 \ud835\udc58 1 \\displaystyle x_{k+1} = \ud835\udda0 \u200b x k + \ud835\udda1 \u200b u k absent \ud835\udda0 subscript \ud835\udc65 \ud835\udc58 \ud835\udda1 subscript \ud835\udc62 \ud835\udc58 \\displaystyle=\\mathsf{A}x_{k}+\\mathsf{B}u_{k} state dynamics (A.2.1) X \u200b ( z ) \u200b z \ud835\udc4b \ud835\udc67 \ud835\udc67 \\displaystyle X(z)z = \ud835\udda0 \u200b X \u200b ( z ) + \ud835\udda1 \u200b U \u200b ( z ) absent \ud835\udda0 \ud835\udc4b \ud835\udc67 \ud835\udda1 \ud835\udc48 \ud835\udc67 \\displaystyle=\\mathsf{A}X(z)+\\mathsf{B}U(z) \ud835\udcb5 \ud835\udcb5 \\mathcal{Z} -transform ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u200b X \u200b ( z ) \ud835\udc67 \ud835\udda8 \ud835\udda0 \ud835\udc4b \ud835\udc67 \\displaystyle(z\\mathsf{I}-\\mathsf{A})X(z) = \ud835\udda1 \u200b U \u200b ( z ) absent \ud835\udda1 \ud835\udc48 \ud835\udc67 \\displaystyle=\\mathsf{B}U(z) ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u200b is also known as the resolvent matrix \ud835\udc67 \ud835\udda8 \ud835\udda0 is also known as the resolvent matrix \\displaystyle(z\\mathsf{I}-\\mathsf{A})\\text{ is also known as the resolvent matrix} X \u200b ( z ) \ud835\udc4b \ud835\udc67 \\displaystyle X(z) = ( I \u200b z \u2212 \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 \u200b U \u200b ( z ) absent superscript \ud835\udc3c \ud835\udc67 \ud835\udda0 1 \ud835\udda1 \ud835\udc48 \ud835\udc67 \\displaystyle=(Iz-\\mathsf{A})^{-1}\\mathsf{B}U(z)\\quad H \u200b ( z ) \ud835\udc3b \ud835\udc67 \\displaystyle H(z) = Y \u200b ( z ) U \u200b ( z ) = \ud835\udda2 \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 + h 0 absent \ud835\udc4c \ud835\udc67 \ud835\udc48 \ud835\udc67 \ud835\udda2 superscript \ud835\udc67 \ud835\udda8 \ud835\udda0 1 \ud835\udda1 subscript \u210e 0 \\displaystyle=\\frac{Y(z)}{U(z)}=\\mathsf{C}(z\\mathsf{I}-\\mathsf{A})^{-1}\\mathsf{B}+h_{0}\\quad substituted X \u200b ( z ) \ud835\udc4b \ud835\udc67 X(z) into Y \u200b ( z ) = \ud835\udda2 \u200b X \u200b ( z ) + h 0 \u200b U \u200b ( z ) \ud835\udc4c \ud835\udc67 \ud835\udda2 \ud835\udc4b \ud835\udc67 subscript \u210e 0 \ud835\udc48 \ud835\udc67 Y(z)=\\mathsf{C}X(z)+h_{0}U(z)\n\nAlternative Derivation: (Massaroli et al., 2023)\n\nThe transfer function can also be derived by direct -transform of the impulse response of the system. This derivation is useful to highlight the region of convergence of the transfer function. H \u200b ( z ) \ud835\udc3b \ud835\udc67 \\displaystyle H(z) = h 0 + \u2211 t = 1 \u221e z \u2212 t \u200b \ud835\udda2\ud835\udda0 t \u2212 1 \u200b \ud835\udda1 absent subscript \u210e 0 superscript subscript \ud835\udc61 1 superscript \ud835\udc67 \ud835\udc61 superscript \ud835\udda2\ud835\udda0 \ud835\udc61 1 \ud835\udda1 \\displaystyle=h_{0}+\\sum_{t=1}^{\\infty}z^{-t}\\mathsf{C}\\mathsf{A}^{t-1}\\mathsf{B} h 0 subscript \u210e 0 h_{0} is pulled out via h 0 \u200b z 0 = h 0 subscript \u210e 0 superscript \ud835\udc67 0 subscript \u210e 0 h_{0}z^{0}=h_{0} (A.2.2) = h 0 + \ud835\udda2 \u200b [ \u2211 t = 1 \u221e z \u2212 t \u200b \ud835\udda0 t \u2212 1 ] \u200b \ud835\udda1 absent subscript \u210e 0 \ud835\udda2 delimited-[] superscript subscript \ud835\udc61 1 superscript \ud835\udc67 \ud835\udc61 superscript \ud835\udda0 \ud835\udc61 1 \ud835\udda1 \\displaystyle=h_{0}+\\mathsf{C}\\left[\\sum_{t=1}^{\\infty}z^{-t}\\mathsf{A}^{t-1}\\right]\\mathsf{B} multiplication distributes over sum. = h 0 + z \u2212 1 \u200b \ud835\udda2 \u200b [ \u2211 t = 1 \u221e z \u2212 ( t \u2212 1 ) \u200b \ud835\udda0 t \u2212 1 ] \u200b \ud835\udda1 absent subscript \u210e 0 superscript \ud835\udc67 1 \ud835\udda2 delimited-[] superscript subscript \ud835\udc61 1 superscript \ud835\udc67 \ud835\udc61 1 superscript \ud835\udda0 \ud835\udc61 1 \ud835\udda1 \\displaystyle=h_{0}+z^{-1}\\mathsf{C}\\left[\\sum_{t=1}^{\\infty}z^{-(t-1)}\\mathsf{A}^{t-1}\\right]\\mathsf{B} multiply by z / z \ud835\udc67 \ud835\udc67 z/z = h 0 + z \u2212 1 \u200b \ud835\udda2 \u200b [ \u2211 t = 0 \u221e ( z \u2212 1 \u200b \ud835\udda0 ) t ] \u200b \ud835\udda1 absent subscript \u210e 0 superscript \ud835\udc67 1 \ud835\udda2 delimited-[] superscript subscript \ud835\udc61 0 superscript superscript \ud835\udc67 1 \ud835\udda0 \ud835\udc61 \ud835\udda1 \\displaystyle=h_{0}+z^{-1}\\mathsf{C}\\left[\\sum_{t=0}^{\\infty}(z^{-1}\\mathsf{A})^{t}\\right]\\mathsf{B} change of index and collect like terms\n\nWe look at the convergence of the series . We have\n\n\u2016 z \u2212 1 \u200b \ud835\udda0 \u2016 2 subscript norm superscript \ud835\udc67 1 \ud835\udda0 2 \\displaystyle\\|z^{-1}\\mathsf{A}\\|_{2} \u2264 \u2016 z \u2212 1 \u2016 2 \u200b \u2016 \ud835\udda0 \u2016 2 absent subscript norm superscript \ud835\udc67 1 2 subscript norm \ud835\udda0 2 \\displaystyle\\leq\\|z^{-1}\\|_{2}\\|\\mathsf{A}\\|_{2} = \u2016 r \u2212 1 \u200b e \u2212 i \u200b \u03c9 \u2016 2 \u200b \u2016 \ud835\udda0 \u2016 2 absent subscript norm superscript \ud835\udc5f 1 superscript \ud835\udc52 \ud835\udc56 \ud835\udf14 2 subscript norm \ud835\udda0 2 \\displaystyle=\\|r^{-1}e^{-i\\omega}\\|_{2}\\|\\mathsf{A}\\|_{2} using z \u2254 r \u200b e i \u200b \u03c9 \u2208 \u2102 , r , \u03c9 \u2208 \u211d formulae-sequence \u2254 \ud835\udc67 \ud835\udc5f superscript \ud835\udc52 \ud835\udc56 \ud835\udf14 \u2102 \ud835\udc5f \ud835\udf14 \u211d z\\coloneqq re^{i\\omega}\\in\\mathbb{C},~{}r,\\omega\\in\\mathbb{R} \u2264 r \u2212 1 \u200b \u2016 \ud835\udda0 \u2016 2 = r \u2212 1 \u200b \u03c1 \u200b ( \ud835\udda0 ) absent superscript \ud835\udc5f 1 subscript norm \ud835\udda0 2 superscript \ud835\udc5f 1 \ud835\udf0c \ud835\udda0 \\displaystyle\\leq r^{-1}\\|\\mathsf{A}\\|_{2}=r^{-1}\\rho(\\mathsf{A})\n\nThe series converges to if and only if i.e. for . Thus, in the exterior of the disk with radius , , converges to and\n\nz \u2208 \ud835\udd3b \u03c1 \u200b ( \ud835\udda0 ) \u21d2 H \u200b ( z ) = h 0 + z \u2212 1 \u200b \ud835\udda2 \u200b ( \ud835\udda8 \u2212 z \u2212 1 \u200b \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 = h 0 + \ud835\udda2 \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u2212 1 \u200b \ud835\udda1 \ud835\udc67 subscript \ud835\udd3b \ud835\udf0c \ud835\udda0 \u21d2 \ud835\udc3b \ud835\udc67 subscript \u210e 0 superscript \ud835\udc67 1 \ud835\udda2 superscript \ud835\udda8 superscript \ud835\udc67 1 \ud835\udda0 1 \ud835\udda1 subscript \u210e 0 \ud835\udda2 superscript \ud835\udc67 \ud835\udda8 \ud835\udda0 1 \ud835\udda1 z\\in\\mathbb{D}_{\\rho(\\mathsf{A})}~{}\\Rightarrow~{}H(z)=h_{0}+z^{-1}\\mathsf{C}(\\mathsf{I}-z^{-1}\\mathsf{A})^{-1}\\mathsf{B}=h_{0}+\\mathsf{C}(z\\mathsf{I}-\\mathsf{A})^{-1}\\mathsf{B}\n\nThe transfer function of a stable lumped discrete-time system is defined outside the disc in the complex plane that encloses all the eigenvalues of . Further dissecting , note that to compute the inverse, is a th order Monic polynomial, and is a order polynomial (for the SISO case), hence the general form of a transfer function can be written in the form of the following rational function (this is discussed in greater detail in A.3):\n\nH ( z ) = b 1 \u200b z \u2212 1 + b 2 \u200b z \u2212 2 + \u22ef + b n \u200b z \u2212 n 1 + a 1 \u200b z \u2212 1 + a 2 \u200b z \u2212 2 + \u22ef + a n \u200b z \u2212 n + h 0 \u2192 Rational function form . H(z)=\\frac{b_{1}z^{-1}+b_{2}z^{-2}+\\dots+b_{n}z^{-n}}{1+a_{1}z^{-1}+a_{2}z^{-2}+\\dots+a_{n}z^{-n}}+h_{0}\\quad\\rightarrow\\text{Rational function form}. (A.2.3)\n\nThe SISO rational coefficient form has parameters. With partial fraction decomposition, the rational function can be broken down into its first order partial decomposition, resulting in a modal representation:\n\nH ( z ) = \u2211 i = 1 n r i z \u2212 \u03bb i + h 0 \u2192 Modal form , H(z)=\\sum_{i=1}^{n}{\\frac{r_{i}}{z-\\lambda_{i}}}+h_{0}\\quad\\rightarrow\\text{Modal form}, (A.2.4)\n\nin which . This form parameterizes the poles () and its associated magnitude (). The modal form has trainable parameters. It is worth noting that the first order partial fraction decomposition does not permit any form of repeated roots, for this reason, it is not a complete representation of a lumped LTI systems. Another way in which rational functions can be structured is called the zero-pole-gain (ZPK) representation:\n\nH ( z ) = k \u220f i = 1 n \u2212 1 ( z \u2212 z i ) \u220f i = 1 n ( z \u2212 \u03bb i ) + h 0 \u2192 Zero-Pole-Gain form , H(z)=k\\frac{\\prod_{i=1}^{n-1}(z-z_{i})}{\\prod_{i=1}^{n}(z-\\lambda_{i})}+h_{0}\\quad\\rightarrow\\text{Zero-Pole-Gain form}, (A.2.5)\n\nin which, , , and are the gain, zeros, and poles respectively. The ZPK form has trainable parameters. A.3 From State-Space to Transfer Function (Massaroli et al., 2023)\n\nWe detail an implementation oriented method to compute the coefficients of a SSM\u2019s transfer function. Expanding the inverse of the resolvent matrix, recall that\n\nH \u200b ( z ) = \ud835\udda2 \u200b [ z \u200b \ud835\udda8 \u2212 \ud835\udda0 ] \u2212 1 \u200b \ud835\udda1 + h 0 = \ud835\udda2 \u200b \ud835\udda0\ud835\uddbd\ud835\uddc3 \u2061 ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u200b \ud835\udda1 + \ud835\uddbd\ud835\uddbe\ud835\uddcd \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u200b h 0 \ud835\uddbd\ud835\uddbe\ud835\uddcd \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \ud835\udc3b \ud835\udc67 \ud835\udda2 superscript delimited-[] \ud835\udc67 \ud835\udda8 \ud835\udda0 1 \ud835\udda1 subscript \u210e 0 \ud835\udda2 \ud835\udda0\ud835\uddbd\ud835\uddc3 \ud835\udc67 \ud835\udda8 \ud835\udda0 \ud835\udda1 \ud835\uddbd\ud835\uddbe\ud835\uddcd \ud835\udc67 \ud835\udda8 \ud835\udda0 subscript \u210e 0 \ud835\uddbd\ud835\uddbe\ud835\uddcd \ud835\udc67 \ud835\udda8 \ud835\udda0 H(z)=\\mathsf{C}[z\\mathsf{I}-\\mathsf{A}]^{-1}\\mathsf{B}+h_{0}=\\frac{\\mathsf{C}\\operatorname{\\mathsf{Adj}}(z\\mathsf{I}-\\mathsf{A})\\mathsf{B}+{\\sf det}(z\\mathsf{I}-\\mathsf{A})h_{0}}{{\\sf det}(z\\mathsf{I}-\\mathsf{A})} (A.3.1)\n\nThis shows that the denominator coefficients are simply the coefficients of the characteristic polynomial of matrix .",
    "statefreeinf-31": "They can be easily obtained by 1. computing the eigenvalues of and 2. calculating the coefficients of the polynomial whose roots are such eigenvalues. On the other hand, the numerator apparently involves more complex symbolic manipulation. This can be simplified recalling a classic matrix-determinant identity:\n\nApplying Lemma A.1 to (A.3.1) we obtain\n\nH \u200b ( z ) = \ud835\uddbd\ud835\uddbe\ud835\uddcd \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 + \ud835\udda1\ud835\udda2 ) + \ud835\uddbd\ud835\uddbe\ud835\uddcd \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) \u200b ( h 0 \u2212 1 ) \ud835\uddbd\ud835\uddbe\ud835\uddcd \u200b ( z \u200b \ud835\udda8 \u2212 \ud835\udda0 ) . \ud835\udc3b \ud835\udc67 \ud835\uddbd\ud835\uddbe\ud835\uddcd \ud835\udc67 \ud835\udda8 \ud835\udda0 \ud835\udda1\ud835\udda2 \ud835\uddbd\ud835\uddbe\ud835\uddcd \ud835\udc67 \ud835\udda8 \ud835\udda0 subscript \u210e 0 1 \ud835\uddbd\ud835\uddbe\ud835\uddcd \ud835\udc67 \ud835\udda8 \ud835\udda0 H(z)=\\frac{{\\sf det}(z\\mathsf{I}-\\mathsf{A}+\\mathsf{B}\\mathsf{C})+{\\sf det}(z\\mathsf{I}-\\mathsf{A})(h_{0}-1)}{{\\sf det}(z\\mathsf{I}-\\mathsf{A})}. Let denote the coefficients of the polynomials with roots . Then . Since and are of equal dimension, their characteristic polynomials have equal order and therefore\n\nb = \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \u200b ( \ud835\uddbe\ud835\uddc2\ud835\uddc0 \u200b ( \ud835\udda0 \u2212 \ud835\udda1\ud835\udda2 ) ) + \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \u200b ( \ud835\uddbe\ud835\uddc2\ud835\uddc0 \u200b ( \ud835\udda0 ) ) \u200b ( h 0 \u2212 1 ) \ud835\udc4f \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \ud835\uddbe\ud835\uddc2\ud835\uddc0 \ud835\udda0 \ud835\udda1\ud835\udda2 \ud835\uddc9\ud835\uddc8\ud835\uddc5\ud835\uddd2 \ud835\uddbe\ud835\uddc2\ud835\uddc0 \ud835\udda0 subscript \u210e 0 1 b={\\sf poly}({\\sf eig}(\\mathsf{A}-\\mathsf{B}\\mathsf{C}))+{\\sf poly}({\\sf eig}(\\mathsf{A}))(h_{0}-1)\n\nA.4 From Transfer Function to State-Space (Massaroli et al., 2023)\n\nChen\u2019s derivation\n\nThe derivation is based on the steps reported for the continuous-time multi-input multi-output case in (Chen, 1998) adapted to single-input single-output Transfer Functions.",
    "statefreeinf-32": "Let , we define a pseudo-state such that\n\np \u200b ( z ) \u200b V \u200b ( z ) = U \u200b ( z ) \u21d4 V \u200b ( z ) = 1 p \u200b ( z ) \u200b U \u200b ( z ) . formulae-sequence \ud835\udc5d \ud835\udc67 \ud835\udc49 \ud835\udc67 \ud835\udc48 \ud835\udc67 \u21d4 \ud835\udc49 \ud835\udc67 1 \ud835\udc5d \ud835\udc67 \ud835\udc48 \ud835\udc67 p(z)V(z)=U(z)\\quad\\Leftrightarrow\\quad V(z)=\\frac{1}{p(z)}U(z). (A.4.1)\n\nThen, we define the state as\n\nx t = ( v t \u2212 1 , v t \u2212 2 , \u22ef , v t \u2212 n ) \u21d4 \ud835\udcb5 \u200b { x } \u200b ( z ) = X \u200b ( z ) = [ z \u2212 1 \u22ee z \u2212 n ] \u200b V \u200b ( z ) . formulae-sequence subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc63 \ud835\udc61 1 subscript \ud835\udc63 \ud835\udc61 2 \u22ef subscript \ud835\udc63 \ud835\udc61 \ud835\udc5b \u21d4 \ud835\udcb5 \ud835\udc65 \ud835\udc67 \ud835\udc4b \ud835\udc67 matrix superscript \ud835\udc67 1 \u22ee superscript \ud835\udc67 \ud835\udc5b \ud835\udc49 \ud835\udc67 x_{t}=(v_{t-1},v_{t-2},\\cdots,v_{t-n})\\quad\\Leftrightarrow\\quad\\mathcal{Z}\\{x\\}(z)=X(z)=\\begin{bmatrix}z^{-1}\\\\\n\\vdots\\\\\nz^{-n}\\end{bmatrix}V(z). (A.4.2)\n\nFrom (A.4.1) we have\n\nV \u200b ( z ) + a 1 \u200b z \u2212 1 \u200b V \u200b ( z ) + \u22ef + a n \u200b z \u2212 n \u200b V \u200b ( z ) = U \u200b ( z ) \ud835\udc49 \ud835\udc67 subscript \ud835\udc4e 1 superscript \ud835\udc67 1 \ud835\udc49 \ud835\udc67 \u22ef subscript \ud835\udc4e \ud835\udc5b superscript \ud835\udc67 \ud835\udc5b \ud835\udc49 \ud835\udc67 \ud835\udc48 \ud835\udc67 \\displaystyle V(z)+a_{1}z^{-1}V(z)+\\cdots+a_{n}z^{-n}V(z)=U(z) \u21d4 \u21d4 \\displaystyle~{}~{}\\Leftrightarrow V \u200b ( z ) = \u2212 a 1 \u200b z \u2212 1 \u200b V \u200b ( z ) \u2212 \u22ef \u2212 a n \u200b z \u2212 n \u200b V \u200b ( z ) + U \u200b ( z ) \ud835\udc49 \ud835\udc67 subscript \ud835\udc4e 1 superscript \ud835\udc67 1 \ud835\udc49 \ud835\udc67 \u22ef subscript \ud835\udc4e \ud835\udc5b superscript \ud835\udc67 \ud835\udc5b \ud835\udc49 \ud835\udc67 \ud835\udc48 \ud835\udc67 \\displaystyle V(z)=-a_{1}z^{-1}V(z)-\\cdots-a_{n}z^{-n}V(z)+U(z) \u21d4 \u21d4 \\displaystyle~{}~{}\\Leftrightarrow v t = \u2212 a 1 \u200b v t \u2212 1 \u2212 \u22ef \u2212 a n \u200b v t \u2212 n + u t subscript \ud835\udc63 \ud835\udc61 subscript \ud835\udc4e 1 subscript \ud835\udc63 \ud835\udc61 1 \u22ef subscript \ud835\udc4e \ud835\udc5b subscript \ud835\udc63 \ud835\udc61 \ud835\udc5b subscript \ud835\udc62 \ud835\udc61 \\displaystyle v_{t}=-a_{1}v_{t-1}-\\cdots-a_{n}v_{t-n}+u_{t} \u21d4 \u21d4 \\displaystyle~{}~{}\\Leftrightarrow time-delay prop. of \ud835\udcb5 \ud835\udcb5 \\mathcal{Z} -transform x t + 1 1 = \u2212 a 1 \u200b x t 1 \u2212 \u22ef \u2212 a n \u200b x t n + u t subscript superscript \ud835\udc65 1 \ud835\udc61 1 subscript \ud835\udc4e 1 subscript superscript \ud835\udc65 1 \ud835\udc61 \u22ef subscript \ud835\udc4e \ud835\udc5b subscript superscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript \ud835\udc62 \ud835\udc61 \\displaystyle x^{1}_{t+1}=-a_{1}x^{1}_{t}-\\cdots-a_{n}x^{n}_{t}+u_{t} \u21d4 \u21d4 \\displaystyle~{}~{}\\Leftrightarrow by def.",
    "statefreeinf-33": "of state ( A.4.2 ) . by def. of state ( A.4.2 ) \\displaystyle\\text{by def. of state \\eqref{eq:chen_state}}. Thus, we have the overall recurrence\n\nx t + 1 1 subscript superscript \ud835\udc65 1 \ud835\udc61 1 \\displaystyle x^{1}_{t+1} = \u2212 a 1 \u200b x t 1 \u2212 \u22ef \u2212 a n \u200b x t n + u t absent subscript \ud835\udc4e 1 subscript superscript \ud835\udc65 1 \ud835\udc61 \u22ef subscript \ud835\udc4e \ud835\udc5b subscript superscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=-a_{1}x^{1}_{t}-\\cdots-a_{n}x^{n}_{t}+u_{t} x t + 1 2 subscript superscript \ud835\udc65 2 \ud835\udc61 1 \\displaystyle x^{2}_{t+1} = x t 1 absent subscript superscript \ud835\udc65 1 \ud835\udc61 \\displaystyle=x^{1}_{t} \u22ee \u22ee \\displaystyle\\vdots x t + 1 n subscript superscript \ud835\udc65 \ud835\udc5b \ud835\udc61 1 \\displaystyle x^{n}_{t+1} = x t n \u2212 1 absent subscript superscript \ud835\udc65 \ud835\udc5b 1 \ud835\udc61 \\displaystyle=x^{n-1}_{t}\n\nwhich can be written in matrix form as\n\nx t + 1 subscript \ud835\udc65 \ud835\udc61 1 \\displaystyle x_{t+1} = [ \u2212 a 1 \u2212 a 2 \u22ef \u2212 a n 1 0 \u22ef 0 0 1 \u22ef 0 \u22ee \u22ee \u22f1 \u22ee 0 0 \u22ef 0 ] \u200b x t + [ 1 0 0 \u22ee 0 ] \u200b u t absent matrix subscript \ud835\udc4e 1 subscript \ud835\udc4e 2 \u22ef subscript \ud835\udc4e \ud835\udc5b 1 0 \u22ef 0 0 1 \u22ef 0 \u22ee \u22ee \u22f1 \u22ee 0 0 \u22ef 0 subscript \ud835\udc65 \ud835\udc61 matrix 1 0 0 \u22ee 0 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\begin{bmatrix}-a_{1}&-a_{2}&\\cdots&-a_{n}\\\\\n1&0&\\cdots&0\\\\\n0&1&\\cdots&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n0&0&\\cdots&0\\end{bmatrix}x_{t}+\\begin{bmatrix}1\\\\\n0\\\\\n0\\\\\n\\vdots\\\\\n0\\end{bmatrix}u_{t}\n\nThe output spectrum is then given by\n\nY \u200b ( z ) \ud835\udc4c \ud835\udc67 \\displaystyle Y(z) = H \u200b ( z ) \u200b U \u200b ( z ) = q \u200b ( z ) p \u200b ( z ) \u200b U \u200b ( z ) + h 0 \u200b U \u200b ( z ) absent \ud835\udc3b \ud835\udc67 \ud835\udc48 \ud835\udc67 \ud835\udc5e \ud835\udc67 \ud835\udc5d \ud835\udc67 \ud835\udc48 \ud835\udc67 subscript \u210e 0 \ud835\udc48 \ud835\udc67 \\displaystyle=H(z)U(z)=\\frac{q(z)}{p(z)}U(z)+h_{0}U(z) = q \u200b ( z ) \u200b V \u200b ( z ) + h 0 \u200b U \u200b ( z ) absent \ud835\udc5e \ud835\udc67 \ud835\udc49 \ud835\udc67 subscript \u210e 0 \ud835\udc48 \ud835\udc67 \\displaystyle=q(z)V(z)+h_{0}U(z) by def.",
    "statefreeinf-34": "of V \u200b ( z ) . by def. of V ( z ) \\displaystyle\\text{by def. of $V(z)$}. Therefore,\n\nY \u200b ( z ) \ud835\udc4c \ud835\udc67 \\displaystyle Y(z) = q \u200b ( z ) \u200b V \u200b ( z ) + h 0 \u200b U \u200b ( z ) = [ b 1 b 2 \u22ef b N ] \u200b [ z \u2212 1 z \u2212 2 \u22ee z \u2212 n ] \u200b V \u200b ( z ) + h 0 \u200b U \u200b ( z ) absent \ud835\udc5e \ud835\udc67 \ud835\udc49 \ud835\udc67 subscript \u210e 0 \ud835\udc48 \ud835\udc67 matrix subscript \ud835\udc4f 1 subscript \ud835\udc4f 2 \u22ef subscript \ud835\udc4f \ud835\udc41 matrix superscript \ud835\udc67 1 superscript \ud835\udc67 2 \u22ee superscript \ud835\udc67 \ud835\udc5b \ud835\udc49 \ud835\udc67 subscript \u210e 0 \ud835\udc48 \ud835\udc67 \\displaystyle=q(z)V(z)+h_{0}U(z)=\\begin{bmatrix}b_{1}&b_{2}&\\cdots&b_{N}\\end{bmatrix}\\begin{bmatrix}z^{-1}\\\\\nz^{-2}\\\\\n\\vdots\\\\\nz^{-n}\\end{bmatrix}V(z)+h_{0}U(z) = [ b 1 b 2 \u22ef b n ] \u200b X \u200b ( z ) + h 0 \u200b U \u200b ( z ) absent matrix subscript \ud835\udc4f 1 subscript \ud835\udc4f 2 \u22ef subscript \ud835\udc4f \ud835\udc5b \ud835\udc4b \ud835\udc67 subscript \u210e 0 \ud835\udc48 \ud835\udc67 \\displaystyle=\\begin{bmatrix}b_{1}&b_{2}&\\cdots&b_{n}\\end{bmatrix}X(z)+h_{0}U(z)\n\nand the output equation in time-domain is given by\n\ny t = [ b 1 b 2 \u22ef b n ] \u200b x t + h 0 \u200b u t . subscript \ud835\udc66 \ud835\udc61 matrix subscript \ud835\udc4f 1 subscript \ud835\udc4f 2 \u22ef subscript \ud835\udc4f \ud835\udc5b subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 y_{t}=\\begin{bmatrix}b_{1}&b_{2}&\\cdots&b_{n}\\end{bmatrix}x_{t}+h_{0}u_{t}. yielding state-space matrices (A.4.3). [ \ud835\udda0 \ud835\udda1 \ud835\udda2 h 0 ] = [ \u2212 a 1 \u2212 a 2 \u22ef \u2212 a n \u2212 1 \u2212 a n 1 0 \u22ef 0 0 0 1 \u22ef 0 0 \u22ee \u22ee \u22f1 \u22ee \u22ee 0 0 \u22ef 1 0 1 0 0 \u22ee 0 b 1 b 2 \u22ef b n \u2212 1 b n h 0 ] . delimited-[] \ud835\udda0 \ud835\udda1 missing-subexpression missing-subexpression \ud835\udda2 subscript \u210e 0 delimited-[] matrix subscript \ud835\udc4e 1 subscript \ud835\udc4e 2 \u22ef subscript \ud835\udc4e \ud835\udc5b 1 subscript \ud835\udc4e \ud835\udc5b 1 0 \u22ef 0 0 0 1 \u22ef 0 0 \u22ee \u22ee \u22f1 \u22ee \u22ee 0 0 \u22ef 1 0 matrix 1 0 0 \u22ee 0 missing-subexpression missing-subexpression matrix subscript \ud835\udc4f 1 subscript \ud835\udc4f 2 \u22ef subscript \ud835\udc4f \ud835\udc5b 1 subscript \ud835\udc4f \ud835\udc5b subscript \u210e 0 \\left[\\begin{array}[]{c|c}\\mathsf{A}&\\mathsf{B}\\\\\n\\hline\\cr\\mathsf{C}&h_{0}\\end{array}\\right]=\\left[\\begin{array}[]{c|c}\\begin{matrix}-a_{1}&-a_{2}&\\cdots&-a_{n-1}&-a_{n}\\\\\n1&0&\\cdots&0&0\\\\\n0&1&\\cdots&0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots&1&0\\end{matrix}&\\begin{matrix}1\\\\\n0\\\\\n0\\\\\n\\vdots\\\\\n0\\end{matrix}\\\\\n\\hline\\cr\\begin{matrix}~{}~{}b_{1}&~{}~{}b_{2}&~{}\\cdots~{}&~{}~{}b_{n-1}&~{}~{}b_{n}\\end{matrix}&h_{0}\\end{array}\\right]. (A.4.3)\n\nAppendix B RTF: Further Details\n\nB.1 Fast Companion Recurrence\n\nThe recurrent step of a generic SSM (3) with dense system matrices usually requires operations due to the matrix-vector product . We show how the recurrence of SSMs in companion canonical form, i.e. with system\u2019s matrices (A.4.3), requires only operations. Proof. The companion state matrix can be broken down into a lower shift matrix and a low-rank term. Particularly, with the first element of the canonical basis of and , we have\n\n\ud835\udda0 = \ud835\uddab n \u2212 e 1 \u2297 a . \ud835\udda0 subscript \ud835\uddab \ud835\udc5b tensor-product subscript \ud835\udc52 1 \ud835\udc4e \\mathsf{A}=\\mathsf{L}_{n}-e_{1}\\otimes a. It follows that the recurrent update can be simplified to\n\nx t + 1 subscript \ud835\udc65 \ud835\udc61 1 \\displaystyle x_{t+1} = ( \ud835\uddab n \u2212 e 1 \u2297 a ) \u200b x t + \ud835\udda1 \u200b u t absent subscript \ud835\uddab \ud835\udc5b tensor-product subscript \ud835\udc52 1 \ud835\udc4e subscript \ud835\udc65 \ud835\udc61 \ud835\udda1 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\left(\\mathsf{L}_{n}-e_{1}\\otimes a\\right)x_{t}+\\mathsf{B}u_{t} y t subscript \ud835\udc66 \ud835\udc61 \\displaystyle y_{t} = \ud835\udda2 \u200b x t + h 0 \u200b u t absent \ud835\udda2 subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\mathsf{C}x_{t}+h_{0}u_{t}\n\nThe peculiarity of this formulation is that we never need to construct the full transition matrix to perform the recurrence. In particular we have:\n\nx t + 1 1 subscript superscript \ud835\udc65 1 \ud835\udc61 1 \\displaystyle x^{1}_{t+1} = u t \u2212 a \u22a4 \u200b x t absent subscript \ud835\udc62 \ud835\udc61 superscript \ud835\udc4e top subscript \ud835\udc65 \ud835\udc61 \\displaystyle=u_{t}-a^{\\top}x_{t} x t + 1 2 : n subscript superscript \ud835\udc65 : 2 \ud835\udc5b \ud835\udc61 1 \\displaystyle x^{2:n}_{t+1} = \ud835\uddcc\ud835\uddc1\ud835\uddc2\ud835\uddbf\ud835\uddcd \u200b ( x t ) absent \ud835\uddcc\ud835\uddc1\ud835\uddc2\ud835\uddbf\ud835\uddcd subscript \ud835\udc65 \ud835\udc61 \\displaystyle={\\sf shift}(x_{t}) y t subscript \ud835\udc66 \ud835\udc61 \\displaystyle y_{t} = b \u22a4 \u200b x t + h 0 \u200b u t absent superscript \ud835\udc4f top subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=b^{\\top}x_{t}+h_{0}u_{t}\n\nThus, each step only requires two inner products ( multiplications and sums each) and one shift operation, totaling operations. \u220e\n\nB.2 Initialization and Stability\n\nInitialization schemes can significantly impact the performance of SSMs, as explored in (Gu et al., 2022b), (Gu et al., 2023), (Orvieto et al., 2023), and (Zhang et al., 2023). Intriguingly, rational transfer functions allow for initialization schemes that can be directly translated from explicitly parameterized convolutional kernels, as demonstrated below:\n\nK \ud835\udda5\ud835\udda8\ud835\uddb1 \u200b ( z ) = k 0 + k 1 \u200b z \u2212 1 + k 2 \u200b z \u2212 2 + \u22ef + k m \u2212 1 \u200b z \u2212 ( m \u2212 1 ) subscript \ud835\udc3e \ud835\udda5\ud835\udda8\ud835\uddb1 \ud835\udc67 subscript \ud835\udc58 0 subscript \ud835\udc58 1 superscript \ud835\udc67 1 subscript \ud835\udc58 2 superscript \ud835\udc67 2 \u22ef subscript \ud835\udc58 \ud835\udc5a 1 superscript \ud835\udc67 \ud835\udc5a 1 K_{\\sf{FIR}}(z)=k_{0}+k_{1}z^{-1}+k_{2}z^{-2}+\\dots+k_{m-1}z^{-(m-1)} (B.2.1)\n\nwhere represents the z-domain representation of an -length finite impulse response (a convolutional kernel of size ). It could easily be seen that by simply setting , and for , the rational transfer function would represent the convolutional kernel. This implies that initialization approaches developed for explicitly parameterized convolutional models, such as (He et al., 2015) and (Glorot & Bengio, 2010), can be directly applied to the rational transfer function representation. Besides initialization, it is generally desirable for SSMs to be stable, meaning that the roots of the rational transfer function denominator (poles) should reside within a complex unit circle in the z-domain (Chen, 1998). When employing a polar representation of the kernel eigenvalues (poles), in which the roots are parameterized by , the roots can easily be restricted to in various ways such as , where as described in (Orvieto et al., 2023). However, for rational transfer functions, where the denominator is represented as a polynomial, ensuring the stability of the SSM is more challenging. Alomari & Chesneau presents several methods for constraining polynomial coefficients, for their roots to lay within the complex unit circle. One such method, Montel\u2019s method (Horn & Johnson, 1985), constrains the polynomial roots as follows:\n\n\u2211 i = 0 n \u2212 1 | \u03b1 i | \u2264 1 , superscript subscript \ud835\udc56 0 \ud835\udc5b 1 subscript \ud835\udefc \ud835\udc56 1 \\sum_{i=0}^{n-1}{|\\alpha_{i}|}\\leq 1, (B.2.2)\n\nThis can be implemented straightforwardly using a softmax or an norm over parameters, and then selecting parameters from this set, as shown in the following code snippet:\n\n1def get_constrained_coefs(coefs_plus_scalar):\n\n2 \"\"\"\n\n3 coefs_plus_scalar: torch.Tensor of shape [n+1]\n\n4 \"\"\"\n\n5 return (coefs/sum(coefs.abs()))[:n] # returns n coefficients that are constrained according to Montel\u2019s method. Spacetime (Zhang et al., 2023) also utilizes this approach to bound the gradients of their SSMs during training. However, we have found that Montel\u2019s method could excessively constrain the SSMs, potentially leading to diminished performance, as shown in Table 5. Next, we use a 2nd order polynomial case, as a visual illustration of the over-constraining occurring with Montel\u2019s method over the parameter space. Given a polynomial , its roots can be analytically computed with:\n\nr = \u2212 \u03b1 1 \u00b1 \u03b1 1 2 \u2212 4 \u200b \u03b1 0 2 . \ud835\udc5f plus-or-minus subscript \ud835\udefc 1 superscript subscript \ud835\udefc 1 2 4 subscript \ud835\udefc 0 2 r=\\frac{-\\alpha_{1}\\pm\\sqrt{\\alpha_{1}^{2}-4\\alpha_{0}}}{2}. (B.2.3)\n\nIn the case that , the quadratic equation becomes a summation of a real term and an imaginary term, therefore we can constrain the root to be within the unit circle by computing its norm as follows:\n\n( \u03b1 1 2 ) 2 \u2212 ( \u03b1 1 2 \u2212 4 \u200b \u03b1 0 2 ) 2 superscript subscript \ud835\udefc 1 2 2 superscript superscript subscript \ud835\udefc 1 2 4 subscript \ud835\udefc 0 2 2 \\displaystyle\\sqrt{\\left(\\frac{\\alpha_{1}}{2}\\right)^{2}-\\left(\\frac{\\sqrt{\\alpha_{1}^{2}-4\\alpha_{0}}}{2}\\right)^{2}} \u2264 1 absent 1 \\displaystyle\\leq 1 (B.2.4) \u03b1 1 2 \u2212 \u03b1 1 2 + 4 \u200b \u03b1 0 4 superscript subscript \ud835\udefc 1 2 superscript subscript \ud835\udefc 1 2 4 subscript \ud835\udefc 0 4 \\displaystyle\\frac{\\alpha_{1}^{2}-\\alpha_{1}^{2}+4\\alpha_{0}}{4} \u2264 1 absent 1 \\displaystyle\\leq 1 (B.2.5) \u03b1 0 \u2264 1 . subscript \ud835\udefc 0 1 \\displaystyle\\alpha_{0}\\leq 1. (B.2.6)\n\nThis shows that the two equations that govern the possible stable regions (for pairs of conjugate roots) are, and . Figure 4 illustrate the space of stable coefficients with a green-blue colormap along with the space of coefficients that obey Montel\u2019s constraints in pink. Notice that a sizable portion of the coefficient space that represents a stable SSM with low decay rates is not accessible with the constraint, which hurts SpaceTime\u2019s expressivity and enforces a short term bias to the model. We observed empirically (i.e., Table 5) that setting both numerator and denominator parameters to zeros, and setting , as formulated below,\n\nH \u03b4 \u200b ( z ) = 1 + 0 z n , subscript \ud835\udc3b \ud835\udeff \ud835\udc67 1 0 superscript \ud835\udc67 \ud835\udc5b H_{\\delta}(z)=1+\\frac{0}{z^{n}}, (B.2.7)\n\ngenerally resulted in RTF having faster training convergence, while simultaneously avoiding instability issues that may be caused via other initialization schemes. The improved stability of this initialization scheme is likely due to it being optimal with respect to satisfying the Montel constraint as follows:\n\nargmin \u03b1 \u2061 ( \u2211 i = 0 n \u2212 1 | \u03b1 i | ) = \ud835\udfce .",
    "statefreeinf-35": "subscript argmin \ud835\udefc superscript subscript \ud835\udc56 0 \ud835\udc5b 1 subscript \ud835\udefc \ud835\udc56 0 \\operatorname{argmin}_{\\mathbf{\\alpha}}(\\sum_{i=0}^{n-1}{|\\alpha_{i}|})=\\mathbf{0}. (B.2.8)\n\nWe denote this as the zero initialization scheme, and use it throughout all our experiments unless stated otherwise. B.3 Alternative Inference Algorithms\n\nB.3.1 RTF Kernel Generation via Long Polynomial Division\n\nGiven a rational transfer function (TF) representing an infinite length convolutional kernel:\n\nH \u200b ( z ) = h 0 + N \u200b ( z ) D \u200b ( z ) = h 0 + \u2211 0 n \u2212 1 b i \u200b z i \u2211 0 n a i \u200b z i = h 0 + h 1 \u200b z \u2212 1 + h 2 \u200b z \u2212 2 + \u2026 , \ud835\udc3b \ud835\udc67 subscript \u210e 0 \ud835\udc41 \ud835\udc67 \ud835\udc37 \ud835\udc67 subscript \u210e 0 subscript superscript \ud835\udc5b 1 0 subscript \ud835\udc4f \ud835\udc56 superscript \ud835\udc67 \ud835\udc56 subscript superscript \ud835\udc5b 0 subscript \ud835\udc4e \ud835\udc56 superscript \ud835\udc67 \ud835\udc56 subscript \u210e 0 subscript \u210e 1 superscript \ud835\udc67 1 subscript \u210e 2 superscript \ud835\udc67 2 \u2026 H(z)=h_{0}+\\frac{N(z)}{D(z)}=h_{0}+\\frac{\\sum^{n-1}_{0}{b_{i}z^{i}}}{\\sum^{n}_{0}{a_{i}z^{i}}}=h_{0}+h_{1}z^{-1}+h_{2}z^{-2}+\\dots, (B.3.1)\n\nwe would like to directly obtain the truncated (finite length) representation of such a kernel, in order to 1. train RTF numerators that directly correspond to the recurrent form without the need to correct for truncation (which could offer significant speedups in online learning tasks such as reinforcement learning), 2. directly evaluate the truncated transfer function at points, avoiding the need to convert the frequency domain kernel into time domain for causal padding. We could take the approach of constructing an infinite length tail function, which upon being subtracted from the original TF, results in truncation as follows:\n\nH \u2113 \u200b ( z ) = H \u200b ( z ) \u2212 H ~ \u2113 \u200b ( z ) = h 0 + h 1 \u200b z \u2212 1 + h 2 \u200b z \u2212 2 + \u22ef + h \u2113 \u2212 1 \u200b z \u2212 \u2113 + 1 . subscript \ud835\udc3b \u2113 \ud835\udc67 \ud835\udc3b \ud835\udc67 subscript ~ \ud835\udc3b \u2113 \ud835\udc67 subscript \u210e 0 subscript \u210e 1 superscript \ud835\udc67 1 subscript \u210e 2 superscript \ud835\udc67 2 \u22ef subscript \u210e \u2113 1 superscript \ud835\udc67 \u2113 1 H_{\\ell}(z)=H(z)-\\tilde{H}_{\\ell}(z)=h_{0}+h_{1}z^{-1}+h_{2}z^{-2}+\\dots+h_{\\ell-1}z^{-\\ell+1}. (B.3.2)\n\nTo satisfy such an equation, we observe that , which could be obtained from the original rational transfer function via long division of against as shown below:\n\nN \u200b ( z ) \u200b z \u2113 \u2212 1 D \u200b ( z ) \ud835\udc41 \ud835\udc67 superscript \ud835\udc67 \u2113 1 \ud835\udc37 \ud835\udc67 \\displaystyle\\frac{N(z)z^{\\ell-1}}{D(z)} = h 0 \u200b z \u2113 \u2212 1 + h 1 \u200b z \u2113 \u2212 2 + h 2 \u200b z \u2113 \u2212 3 + \u22ef + h \u2113 \u2212 1 \u23df C \u200b ( z ) + h \u2113 \u200b z \u2212 1 + h \u2113 + 1 \u200b z \u2212 2 + \u2026 \u23df H ~ \u2113 \u200b ( z ) \u200b z \u2212 \u2113 + 1 absent subscript \u23df subscript \u210e 0 superscript \ud835\udc67 \u2113 1 subscript \u210e 1 superscript \ud835\udc67 \u2113 2 subscript \u210e 2 superscript \ud835\udc67 \u2113 3 \u22ef subscript \u210e \u2113 1 \ud835\udc36 \ud835\udc67 subscript \u23df subscript \u210e \u2113 superscript \ud835\udc67 1 subscript \u210e \u2113 1 superscript \ud835\udc67 2 \u2026 subscript ~ \ud835\udc3b \u2113 \ud835\udc67 superscript \ud835\udc67 \u2113 1 \\displaystyle=\\underbrace{h_{0}z^{\\ell-1}+h_{1}z^{\\ell-2}+h_{2}z^{\\ell-3}+\\dots+h_{\\ell-1}}_{C(z)}+\\underbrace{h_{\\ell}z^{-1}+h_{\\ell+1}z^{-2}+\\dots}_{\\tilde{H}_{\\ell}(z)z^{-\\ell+1}} (B.3.3) = C \u200b ( z ) + H ~ \u2113 \u200b ( z ) \u200b z \u2212 \u2113 + 1 = C \u200b ( z ) + R \u200b ( z ) D \u200b ( z ) , absent \ud835\udc36 \ud835\udc67 subscript ~ \ud835\udc3b \u2113 \ud835\udc67 superscript \ud835\udc67 \u2113 1 \ud835\udc36 \ud835\udc67 \ud835\udc45 \ud835\udc67 \ud835\udc37 \ud835\udc67 \\displaystyle=C(z)+\\tilde{H}_{\\ell}(z)z^{-\\ell+1}=C(z)+\\frac{R(z)}{D(z)}, (B.3.4)\n\nH ~ \u2113 \u200b ( z ) = R \u200b ( z ) D \u200b ( z ) \u200b z \u2113 \u2212 1 . subscript ~ \ud835\udc3b \u2113 \ud835\udc67 \ud835\udc45 \ud835\udc67 \ud835\udc37 \ud835\udc67 superscript \ud835\udc67 \u2113 1 \\tilde{H}_{\\ell}(z)=\\frac{R(z)}{D(z)z^{\\ell-1}}. (B.3.5)\n\nThe naive long division algorithm takes operations, in which , however with fast Toeplitz matrix inversion algorithms described in (Pan, 2001), such an algorithm could operate with complexity of , assuming . Next, by simply constructing the truncated transfer function via Equation (B.3.2), the padded convolutional kernel in frequency domain can be obtained via transfer function evaluation at points of unity. B.3.2 Multi-Input Multi-Output RTF\n\nA multi-input multi-output (MIMO) LTI SSM could be represented using a matrix of numerator polynomials, that shares a denominator polynomial, forming a rational function for each input to output pair. Chen shows that such a system could be converted back into an SSM realizing the companion form (16) as follows:\n\nx k + 1 subscript \ud835\udc65 \ud835\udc58 1 \\displaystyle x_{k+1} = [ \u2212 a 0 \u200b \ud835\udda8 d \u2212 a 1 \u200b \ud835\udda8 d \u2026 \u2212 a n \u2212 2 \u200b \ud835\udda8 d \u2212 a n \u2212 1 \u200b \ud835\udda8 d \ud835\udda8 d \ud835\udfe2 \u2026 \ud835\udfe2 \ud835\udfe2 \ud835\udfe2 \ud835\udda8 d \u2026 \ud835\udfe2 \ud835\udfe2 \u22ee \u22ee \u22ee \u22ee \ud835\udfe2 \ud835\udfe2 \u2026 \ud835\udda8 d \ud835\udfe2 ] \u200b x k + [ \ud835\udda8 d \ud835\udfe2 \ud835\udfe2 \u22ee \ud835\udfe2 ] \u200b u absent matrix subscript \ud835\udc4e 0 subscript \ud835\udda8 \ud835\udc51 subscript \ud835\udc4e 1 subscript \ud835\udda8 \ud835\udc51 \u2026 subscript \ud835\udc4e \ud835\udc5b 2 subscript \ud835\udda8 \ud835\udc51 subscript \ud835\udc4e \ud835\udc5b 1 subscript \ud835\udda8 \ud835\udc51 subscript \ud835\udda8 \ud835\udc51 0 \u2026 0 0 0 subscript \ud835\udda8 \ud835\udc51 \u2026 0 0 \u22ee \u22ee missing-subexpression \u22ee \u22ee 0 0 \u2026 subscript \ud835\udda8 \ud835\udc51 0 subscript \ud835\udc65 \ud835\udc58 matrix subscript \ud835\udda8 \ud835\udc51 0 0 \u22ee 0 \ud835\udc62 \\displaystyle=\\begin{bmatrix}-a_{0}\\mathsf{I}_{d}&-a_{1}\\mathsf{I}_{d}&\\dots&-a_{n-2}\\mathsf{I}_{d}&-a_{n-1}\\mathsf{I}_{d}\\\\\n\\mathsf{I}_{d}&\\mathsf{0}&\\dots&\\mathsf{0}&\\mathsf{0}\\\\\n\\mathsf{0}&\\mathsf{I}_{d}&\\dots&\\mathsf{0}&\\mathsf{0}\\\\\n\\vdots&\\vdots&&\\vdots&\\vdots\\\\\n\\mathsf{0}&\\mathsf{0}&\\dots&\\mathsf{I}_{d}&\\mathsf{0}\\end{bmatrix}x_{k}+\\begin{bmatrix}\\mathsf{I}_{d}\\\\\n\\mathsf{0}\\\\\n\\mathsf{0}\\\\\n\\vdots\\\\\n\\mathsf{0}\\end{bmatrix}u (B.3.6) y \ud835\udc66 \\displaystyle y = \ud835\udda2 \u200b x k + \ud835\udda3 \u200b u , absent \ud835\udda2 subscript \ud835\udc65 \ud835\udc58 \ud835\udda3 \ud835\udc62 \\displaystyle=\\mathsf{C}x_{k}+\\mathsf{D}u,\n\nin which is a rank identity matrix, corresponds to the matrix of numerator coefficients and . is the denominator polynomial coefficient at order . We can observe that such a system\u2019s matrix becomes excessively large, making it not competitive in terms of both parallel inference and autoregressive inference speeds against other MIMO systems. For this reason, we focus on the multi SISO (2) companion realization, in which the SSMs are independent across the channel dimension, channel mixing is only done afterwards with a linear projection. Appendix C Experiments\n\nC.1 Memory and Latency Profiling Experiments\n\n\u2022\n\nExperiments were conducted using JAX (Bradbury et al., 2018) on a single A100 80GB GPU for the memory profiling experiments, and on a single H100 80GB GPU for the latency profiling experiments. \u2022\n\nS5 implementation was taken directly from (Smith et al., 2023). \u2022\n\nThe memory profiling was done on a single SSM layer with channel size , whereas the latency profiling was done using . \u2022\n\nDue to S5 being a Multi-Input Multi-Output (MIMO) SSM and RTF being a Single-Input Single-Output SSM, there are few additional points to note on interpreting the results:\n\n\u2013\n\nFor fairness we considered a RTF layer with channel mixing, which includes an additional output linear projection layer that mixes the channel dimensions. \u2013\n\nThe RTF layer with channel mixing is equivalent to a block diagonal MIMO SSM with a combined state size of . Mamba (Gu & Dao, 2023) makes use of the term state expansion factor (), which describes the state size per channel. For a multi-SISO SSM such as RTF, , whereas for a MIMO SSM such as S5, . \u2013\n\nFigure 1 and Table 6 compare each SSM layer\u2019s memory usage across multiple state sizes (), whereas Figure 5 and Table 7 compare SSM layer\u2019s parallel inference latency across multiple the expansion factors (). \u2022\n\nFor each sequence length , we collected profiling speeds for RTF and S5 with state-sizes ranging from 256 up to . \u2022\n\nTable 6 lists the exact peak memory usage in MB. Runs which ran out of the 80GB GPU memory is denoted as OOM (Out Of Memory). \u2022\n\nFigure 5 and Table 7 illustrates the median parallel inference latencies (across 100 iterations) in milliseconds. C.2 Long Range Arena Benchmark\n\nC.2.1 Model Architecture Details\n\nFor fair comparisons with S4 (Gu et al., 2022b) and S4D (Gu et al., 2022a), we employed the same model backbone, block design, and architectural hyperparameters as employed by S4. Each model contains a linear encoder and decoder that projects the inputs and outputs to an appropriate channel dimension. Simply put, each layer is a combination of a SSM layer, an activation function (GELU (Hendrycks & Gimpel, 2023)), followed by an output linear projection layer, and another activation function (GLU (Dauphin et al., 2017)), with skip connections (He et al., 2016) and normalization applied before each every SSM and linear layer. Each channel in a SSM layer comprises of a SISO SSM with the ability to share the the transition matrix across channels through the number of SSMs hyperparameter (Num. SSM). This sets the number of unique matrices (or rational function denominator) which are then equally dispersed across the channel dimensions. Additional hyperparameter details are outlined in Tables 8 and 9. Experiments using S4 and S4D models used the PyKeops implementation, available in the official S4 github repository (Gu et al., 2022b). Fused FFTConv (Fu et al., 2024) algorithms were not used for the RTF implementation. C.2.2 Long Range Arena Benchmark Details\n\nThe long range arena (LRA) benchmark (Tay et al., 2021) features 6 unique tasks within lengths of 1K-16K steps. These tasks involve diverse modalities and objectives, pushing models to reason about similarity, structure, and visuospatial relationships. We offer additional context and specifics for each dataset from the LRA (Tay et al., 2021) that we examine, following the identical data pre-processing procedures as those used by (Gu et al., 2022b). \u2022\n\nListOps An extended dataset introduced by (Nangia & Bowman, 2018). This task involves calculating the integer outcome of mathematical expressions encoded in prefix notation with brackets. Nested operations (min, max, etc.) and operands (0-9) are represented as one-hot vectors (17 unique values, brackets and operators combined). Sequence lengths vary, with max length of 2048. The dataset contains 10 distinct classes, each representing a possible integer outcome, with 96,000 training, 2,000 validation, and 2,000 test sequences. \u2022\n\nIMDB Sentiment dataset from (Maas et al., 2011). This task involves classifying movie reviews into positive or negative sentiment categories based on sequences of integer tokens (encoded as one-hot vectors, 129 unique values).",
    "statefreeinf-36": "Sequence length varies, with a maximum length of 4,096. The dataset consists of 25,000 training and 25,000 test examples. \u2022\n\nRetrieval This is derived from the ACL Anthology network corpus introduced by (Radev et al., 2009). The datasets requires determining if two provided textual citations, encoded as a sequence of integer tokens, are the same. Characters are converted into a one-hot vector with 97 unique values. The two paired sequences can have different lengths, with a maximum sequence length of 4,000. There are two categories, signifying whether the citations are equivalent or not. The dataset comprises 147,086 training pairs, 18,090 validation pairs, and 17,437 test pairs. \u2022\n\nImage The task utilizes the CIFAR-10 dataset introduced by (Krizhevsky, 2009). It involves classifying a 32 \u00d7 32 grayscale CIFAR-10 image, presented as a one-dimensional raster scan, into one of ten categories. All sequences have the same length (1,024). The dataset comprises 45,000 training examples, 5,000 validation examples, and 10,000 test examples. \u2022\n\nPathfinder This is derived from the Pathfinder challenge, as presented by (Linsley et al., 2018). It involves a 32 \u00d7 32 grayscale image that displays a start and an end point, each represented by a small circle. The image contains several dashed lines. The objective is to determine whether a dashed line (or path) connects the start and end points. There are two classes, signifying whether a valid path exists or not. All sequences have the same length (1,024). The dataset includes 160,000 training examples, 20,000 validation examples, and 20,000 test examples. \u2022\n\nPath-X This is a variant of the Pathfinder challenge. With a longer sequence and more complex, in this version, the images are 128 \u00d7 128 pixels, leading to sequences that are sixteen times longer. C.3 Synthetic Memorization Tasks\n\nBoth implementations of Copying (Arjovsky et al., 2016) and Delay (Gu et al., 2023) were taken directly from the official S4 repository (Gu et al., 2022b), and was modified to enable drop in replacements of our RTF SSMs under identical conditions. C.3.1 Copying Task\n\nEach model is first fed a length sequence of integer tokens randomly sampled from , and then fed a length sequence of token number to recall the initial sequence. Table 10 lists the task hyperameters. The overall model architecture is identical to that described in Section C.2.1. Each model was trained with 4 layers, 1024 channel dimensions, and the number of SSM was set to 1 (for weight sharing). Additionally, we initialized the RTF parameters by uniformly sampling from a range of 0 to 1, and applying the Montel constraint to limit the poles to a stable location. C.3.2 Delay Task\n\nThe models are given a signal of length and are tasked to output the original signal shifted by timesteps. The input is a white noise signal bandlimited to 1000 Hz. A single layer SSM with channel dimensions of 4 without a non-linear activation function was used for this experiment. Table 11 lists the task hyperameters. C.4 Laughing Hyena Distillation Task\n\n\u2022\n\nThe baseline 160M parameter MultiHyena-Attention hybrid model consists of 6 Attention layers and 6 MultiHyena layers. \u2022\n\nThe distillation task aims to replace the Hyena filters in the 6 MultiHyena layers with an RTF or a modal SSM. \u2022\n\nEach MultiHyena layer consist of 256 independent SISO convolutional filters, which are projected to 768 dimensions as described in (Massaroli et al., 2023). \u2022\n\nBoth LH and RTF were trained for iterations, on the AdamW (Loshchilov & Hutter, 2019) optimizer with learning rates set to . C.5 WikiText103 Language Modeling\n\nC.5.1 Pilot Experiments\n\nWe additionally compared S4, S4D, and RTF on WikiText103 under the modified Transformer backbone (Baevski & Auli, 2019), from the official S4 repository (Gu et al., 2022b), via drop-in replacements of S4 with S4D and RTF, while keeping the original hyperparameters. Table 13 shows perplexity scores for the models across multiple state-sizes, trained for 25 epochs on two 40GB A100 GPUs. The results show a consistent trend of RTF outperforming S4 and S4D across multiple state-sizes. C.5.2 Model Architecture Details\n\nFor our main WikiText103 experiment, we constructed Hyena-RTF by simply replacing the Hyena Filters in the Hyena Hierarchy model (Poli et al., 2023a) implemented in the HazyResearch/safari Github repository, with our RTF SSM. We also made slight modifications to the Hyena operator\u2019s output linear projection, by inserting an additional low-rank linear layer and a GELU (Hendrycks & Gimpel, 2023) activation, before the final output linear projection. This is to functionally mimic the low-rank MIMO SSM + non-linear activation function that Hyena-S5 (Smith et al., 2023) employs. It is worth noting that the additional low-rank layer does not increase parameter count since the original output linear projection also loses rank for compatibility of dimensions. We observed that the zero-initialization alone was not enough for the model to stay within the stable region across training \u2013 an important property for extrapolative tasks such as language generation. Therefore, we instead adopt the Xavier initialization (Glorot & Bengio, 2010) over the rational function coefficients and apply the Montel constraint via an penalization as shown in Section B.2. Table 13 lists the hyperparameters used to train our Hyena-RTF model.",
    "statefreeinf-37": "\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 13:43:23 2024 by LaTeXML"
}