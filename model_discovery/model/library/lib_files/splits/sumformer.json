{
    "sumformer-0": "Sumformer: Universal Approximation for Efficient Transformers\n\nSilas Alberti Niclas Dern Laura Thesing Gitta Kutyniok\n\nAbstract\n\nNatural language processing (NLP) made an impressive jump with the introduction of Transformers.",
    "sumformer-1": "ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation. 1 Introduction\n\nThe introduction of the Transformer architecture in 2017 (Vaswani et al., 2017) commenced a new revolution in the field of deep learning. It not only revolutionized Natural Language Processing with famous models like BERT (Devlin et al., 2018) and GPT-3 (Brown et al., 2020) but also other areas like computer vision (Dosovitskiy et al., 2020) and biology (Jumper et al., 2021). However, Transformers can become computationally expensive at scale. In many cases, the primary performance bottleneck is the attention mechanism that needs to compute a -Matrix, where is the length of the input sequence. Therefore, the computational complexity of a forward pass grows with the sequence length. This establishes the sequence length as one of the major bottlenecks when using of Transformers for long sequences, which are encountered in many fields, such as NLP for processing longer documents like books, time series (Wen et al., 2022), genomics (Eraslan et al., 2019), and reinforcement learning (Chen et al., 2021). To address this problem, many new architectures have been proposed (Child et al., 2019; Wang et al., 2020; Choromanski et al., 2020; Katharopoulos et al., 2020; Kitaev et al., 2020; Zaheer et al., 2020; Tay et al., 2022; Beltagy et al., 2020). These can be roughly divided into sparse Transformers and efficient Transformers (Tay et al., 2022). In some cases, the complexity can be reduced to as low as . While, in practice, these new architectures do not match the performance of Transformers, the relative performance to the decrease in computational cost makes them promising. Besides their empirical performance, little is known about the theoretical properties of these new architectures. Particularly, they have not yet been studied from the perspective of expressivity. This paper shows that the efficient Transformers, Linformer and Performer, are universal approximators of equivariant continuous sequence-to-sequence functions on compact sets. 1.1 Summary of contributions\n\nIn this paper, we introduce the Sumformer architecture. This architecture serves as a simple tool that we can use to investigate the expressive power of Transformers and two selected efficient Transformer architectures: Linformer (Wang et al., 2020) and Performer (Choromanski et al., 2020). We chose the latter two architectures since they performed best in the Long Range Arena benchmark (Tay et al., 2020). First, we show that the Sumformer architecture is able to approximate all continuous equivariant sequence-to-sequence functions on compact sets (Sec. 3). We give two different proofs: A continuous proof based on the algebra of multisymmetric polynomials, and a discrete proof based on a piecewise constant approximation. Using this result, we give a new proof of the universal approximation theorem for Transformers (Sec. 4.2). This proof improves significantly upon the previous result from (Yun et al., 2019), by reducing the number of necessary attention layers. Our proof only needs one attention layer, whereas the number of attention layers in (Yun et al., 2019) grows exponentially with the token dimension. Based on this proof, we give the first proof that Linformer and Performer are universal approximators (Sec. 4.3). This is the first universal approximation theorem for efficient Transformers, showing that despite using the efficient attention mechanisms we do not suffer from a loss in expressivity. Our numerical experiments (Sec. 5) using the Sumformer architecture show that the Sumformer architecture is not only theoretically useful but can indeed be used to learn functions using gradient descent. Furthermore, we find an exponential relation between the token dimension and the necessary latent dimension. 1.2 Related work\n\nThis paper analyses the expressive power of newly evolving efficient Transformer architectures. Expressivity is a natural first question when investigating the possibilities and limitations of network architectures. Therefore, the question of which functions can be approximated (uniformly) with neural networks and their variance is of great interest. The publications mentioned in the following are by no means exhaustive but rather a selection: The first universal approximation result for neural networks dates back to 1989 with the universal approximation theorem in (Hornik et al., 1989). Further investigations also for deeper networks were made in (Barron, 1994; Mhaskar, 1996; Shaham et al., 2018). These results were extended to functions with the rectified linear unit (ReLU) activation function in (Petersen & Voigtlaender, 2018; Lu et al., 2017; Yarotsky, 2017; G\u00fchring et al., 2020) and convolutional neural networks in (Yarotsky, 2022). Feed forward neural networks with fewer non-zero coefficients and values that can be stored with fewer bits and therefore improve memory efficiency are investigated in (Bolcskei et al., 2019). The Transformer architecture has not been explored as much in the literature. We know from (Yun et al., 2019) that Transformers are universal approximators in , for for continuous sequence-to-sequence functions. Moreover, it has been shown in (Yun et al., 2020) that under certain assumptions on the sparsity pattern, sparse Transformers form universal approximators in the same setting. The expressivity of the self-attention mechanism has also been examined from a complexity theory perspective in (Likhosherstov et al., 2021). For efficient Transformer architectures, no such universal approximation results exist to our knowledge. The main inspiration for this work is the Deep Sets architecture which shows a universal approximation theorem for invariant functions on sets (Zaheer et al., 2017; Wagstaff et al., 2019). We expand on their theorems in the continuous case (Theorem 7 & 9) and expand the theory from invariant functions on sets to equivariant functions on sequences. A similar model to Sumformer was proposed, and universality was proven in (Hutter, 2020). However, the connection to (efficient) Transformers was not made. We build upon their proof and propose an alternative discontinuous version. Concurrent work has given the continuous proof in higher dimension, but neither considers the expansion to equivariant sequence-to-sequence functions nor to Transformers (Chen et al., 2022). 2 Preliminaries\n\nThis section describes the setting and states helpful theorems for our proofs and experiments. We first recall the definition of attention heads and the Transformer block from (Vaswani et al., 2017). Afterwards, we describe how they can be changed to be more efficient with Linformer and Performer. Furthermore, we define equivariant, semi-invariant functions, multisymmetric polynomials, and multisymmetric power sums (Briand, 2004). We also state important theorems about the relations between these concepts from (Hutter, 2020) and (Briand, 2004). Lastly, we recall an important theorem from (Zaheer et al., 2017). 2.1 Transformer\n\nThe central part of the Transformer is the (self-)attention layer, which is able to connect every element in a sequence with every other element. Definition 2.1 (Attention Head (Vaswani et al., 2017)). Let be weight matrices and let be the softmax function. A (self-)attention head is a function with\n\nAttHead \u200b ( X ) := \u03c1 \u200b ( ( X \u200b W Q ) \u200b ( X \u200b W K ) \u22a4 / d ) \u23df A \u200b X \u200b W V assign AttHead \ud835\udc4b subscript \u23df \ud835\udf0c \ud835\udc4b subscript \ud835\udc4a \ud835\udc44 superscript \ud835\udc4b subscript \ud835\udc4a \ud835\udc3e top \ud835\udc51 \ud835\udc34 \ud835\udc4b subscript \ud835\udc4a \ud835\udc49 \\mathrm{AttHead}(X):=\\underbrace{\\rho\\Big{(}(XW_{Q})(XW_{K})^{\\top}/\\sqrt{d}\\Big{)}}_{A}XW_{V} (1)\n\nwhere is applied row-wise. We call the attention matrix. Computing the attention matrix has a computational complexity of , thereby forming the highest cost in evaluating the Transformer. In the next step, we combine the attention heads to an attention layer by concatenating attention heads and multiplying them with another weight matrix . Definition 2.2 (Attention Layer (Vaswani et al., 2017)). Let , let be attention heads and let . A (multi-head) (self-)attention layer is defined as\n\nAtt \u200b ( X ) := [ AttHead 1 \u200b ( X ) , \u2026 , AttHead h \u200b ( X ) ] \u200b W O . assign Att \ud835\udc4b subscript AttHead 1 \ud835\udc4b \u2026 subscript AttHead \u210e \ud835\udc4b subscript \ud835\udc4a \ud835\udc42 \\displaystyle\\begin{split}\\mathrm{Att}(X)&:=[\\mathrm{AttHead}_{1}(X),\\dots,\\mathrm{AttHead}_{h}(X)]W_{O}.\\end{split} (2)\n\nFor the Transformer architecture the attention layer is combined with fully-connected layers that are applied token-wise.",
    "sumformer-2": "Moreover, there are residual connections between all the layers (He et al., 2016). Those three components together yield the Transformer block. Definition 2.3 (Transformer Block (Vaswani et al., 2017)). A Transformer block is an attention layer followed by a fully-connected feed forward layer with residual connections\n\nBlock \u200b ( X ) := X + FC \u200b ( X + Att \u200b ( X ) ) assign Block \ud835\udc4b \ud835\udc4b FC \ud835\udc4b Att \ud835\udc4b \\mathrm{Block}(X):=X+\\mathrm{FC}(X+\\mathrm{Att}(X)) (3)\n\nwhere the fully-connected feed-forward layer is applied row-wise. Similar to the concept of feed-forward neural networks, we stack several Transformer blocks after each other by concatenation. The Transformer architecture is then defined as follows. Definition 2.4 (Transformer Network (Vaswani et al., 2017)). Let and be Transformer blocks. A Transformer network is a composition of Transformer blocks:\n\n\ud835\udcaf \u200b ( X ) := ( Block \u2113 \u2218 Block \u2113 \u2212 1 \u2218 \u22ef \u2218 Block 1 ) \u200b ( X ) . assign \ud835\udcaf \ud835\udc4b subscript Block \u2113 subscript Block \u2113 1 \u22ef subscript Block 1 \ud835\udc4b \\mathcal{T}(X):=(\\mathrm{Block}_{\\ell}\\circ\\mathrm{Block}_{\\ell-1}\\circ\\dots\\circ\\mathrm{Block}_{1})(X). (4)\n\n2.2 Efficient Transformer\n\nTo address the bottleneck of computing the attention matrix , various efficient Transformers were introduced. We chose to investigate Linformer and Performer since they stood out in the Long Range Arena benchmark (Tay et al., 2020). Both architectures only replace the attention mechanism and do not change the rest of the architecture. 2.2.1 Linformer\n\nThe Linformer architecture is motivated by the observation that the attention matrix is effectively low rank. This is supported by empirical evidence in actual language models and theoretical results in (Wang et al., 2020). The Linformer architecture utilizes the Johnson-Lindenstrauss Lemma by using linear projections to project the key and value matrix and from to . The entries of and are sampled from a normal distribution. The precise definition of a Linformer Attention Head is as follows:\n\nDefinition 2.5 (Linformer Attention Head (Wang et al., 2020)). Let with and let be linear projection matrices. Furthermore, let , be as in the Definition of a Transformer attention head 2.1. A Linformer attention head is a function with\n\nLinAttHead \u200b ( X ) := \u03c1 \u200b ( ( X \u200b W Q ) \u200b ( E \u200b X \u200b W K ) \u22a4 / d ) \u200b F \u200b X \u200b W V assign LinAttHead \ud835\udc4b \ud835\udf0c \ud835\udc4b subscript \ud835\udc4a \ud835\udc44 superscript \ud835\udc38 \ud835\udc4b subscript \ud835\udc4a \ud835\udc3e top \ud835\udc51 \ud835\udc39 \ud835\udc4b subscript \ud835\udc4a \ud835\udc49 \\mathrm{LinAttHead}(X):=\\rho\\Big{(}(XW_{Q})(EXW_{K})^{\\top}/\\sqrt{d}\\Big{)}FXW_{V} (5)\n\nwhere is applied row-wise. Then, the new attention matrix will be in , giving a computational complexity of instead of Using the Johnson-Lindenstrauss Lemma it is shown that when is chosen on the order of , the attention mechanism is approximated with error. Since is independent of , the complexity of Linformer Attention is as increases. 2.2.2 Performer\n\nThe key insight that motivates the Performer architecture is the fact that the attention mechanism could be more efficient if the attention matrix had no non-linearity:\n\n( Q \u200b K T ) \u200b V = Q \u200b ( K T \u200b V ) \ud835\udc44 superscript \ud835\udc3e \ud835\udc47 \ud835\udc49 \ud835\udc44 superscript \ud835\udc3e \ud835\udc47 \ud835\udc49 (QK^{T})V=Q(K^{T}V) (6)\n\nThis reduces the computational complexity from to . By interpreting the attention matrix as a kernel matrix, this non-linearity can be replaced by a dot product in a kernel space, enabling the following efficient attention algorithm:\n\nDefinition 2.6 (Performer Attention Head (Choromanski et al., 2020)).",
    "sumformer-3": "Let with , let and define as\n\na \u200b ( x ) := 1 k \u200b exp \u2061 ( \u2212 \u2225 x \u2225 2 2 ) \u200b [ exp \u2061 ( \u03c9 1 \u22a4 \u200b x ) , \u2026 , exp \u2061 ( \u03c9 k \u22a4 \u200b x ) ] . assign \ud835\udc4e \ud835\udc65 1 \ud835\udc58 superscript delimited-\u2225\u2225 \ud835\udc65 2 2 superscript subscript \ud835\udf14 1 top \ud835\udc65 \u2026 superscript subscript \ud835\udf14 \ud835\udc58 top \ud835\udc65 a(x):=\\frac{1}{\\sqrt{k}}\\exp\\bigg{(}-\\frac{\\left\\lVert x\\right\\rVert^{2}}{2}\\bigg{)}\\big{[}\\exp(\\omega_{1}^{\\top}x),\\dots,\\exp(\\omega_{k}^{\\top}x)\\big{]}. (7)\n\nFurthermore, let be weight matrices. A Performer attention head is a function with\n\nPerAttHead \u200b ( X ) := a \u200b ( X \u200b W Q ) \u200b ( a \u200b ( X \u200b W K ) \u22a4 \u200b ( X \u200b W V ) ) assign PerAttHead \ud835\udc4b \ud835\udc4e \ud835\udc4b subscript \ud835\udc4a \ud835\udc44 \ud835\udc4e superscript \ud835\udc4b subscript \ud835\udc4a \ud835\udc3e top \ud835\udc4b subscript \ud835\udc4a \ud835\udc49 \\mathrm{PerAttHead}(X):=a(XW_{Q})\\Big{(}a(XW_{K})^{\\top}(XW_{V})\\Big{)} (8)\n\nwhere is applied row-wise. With this definition, we avoid the computation of the full attention matrix, which reduces the computational complexity from to\n\n2.3 Equivariant and Semi-Equivariant Functions\n\nLet and be the domain and range of a function, e.g., or in the compact case.",
    "sumformer-4": "We call an element a sequence of elements and denote . Often, we refer to the elements of the sequence as points. In the canonical case , we can represent sequences as matrices. We call functions of type sequence-to-point functions. Definition 2.7 (Equivariance). A sequence-to-point function , with is equivariant to the order of elements in a sequence if for each permutation :\n\nf \u200b ( [ x \u03c0 \u200b ( 1 ) , \u2026 , x \u03c0 \u200b ( n ) ] ) = [ f \u03c0 \u200b ( 1 ) \u200b ( X ) , \u2026 , f \u03c0 \u200b ( n ) \u200b ( X ) ] . \ud835\udc53 subscript \ud835\udc65 \ud835\udf0b 1 \u2026 subscript \ud835\udc65 \ud835\udf0b \ud835\udc5b subscript \ud835\udc53 \ud835\udf0b 1 \ud835\udc4b \u2026 subscript \ud835\udc53 \ud835\udf0b \ud835\udc5b \ud835\udc4b f([x_{\\pi(1)},\\dots,x_{\\pi(n)}])=[f_{\\pi(1)}(X),\\dots,f_{\\pi(n)}(X)]. (9)\n\nWe write that . Transformers represent sequence-to-sequence functions, but sometimes it is more convenient to work with sequence-to-point functions. To facilitate that, we recall the concept of a semi-invariant function (see: (Hutter, 2020)). Definition 2.8 (Semi-invariance). A sequence-to-point function is semi-invariant if for each permutation :\n\ng \u200b ( [ x 1 , x 2 , \u2026 , x n ] ) = g \u200b ( [ x 1 , x \u03c0 \u200b ( 2 ) , \u2026 , x \u03c0 \u200b ( n ) ] ) . \ud835\udc54 subscript \ud835\udc65 1 subscript \ud835\udc65 2 \u2026 subscript \ud835\udc65 \ud835\udc5b \ud835\udc54 subscript \ud835\udc65 1 subscript \ud835\udc65 \ud835\udf0b 2 \u2026 subscript \ud835\udc65 \ud835\udf0b \ud835\udc5b g([x_{1},x_{2},\\dots,x_{n}])=g([x_{1},x_{\\pi(2)},\\dots,x_{\\pi(n)}]). (10)\n\nIn this context, the following insight from [(Hutter, 2020), Lemma 10] is important because it enables us to deal with equivariant sequence-to-sequence functions by looking at semi-invariant sequence-to-point functions instead:\n\nLemma 2.9 (Equivalence of Equivariance and Semi-invariance (Hutter, 2020)). A sequence-to-sequence function is equivariant if and only if there exists a semi-invariant sequence-to-point function such that\n\nf ( [ x 1 , \u2026 , x n ] ) = [ g \u200b ( x 1 , { x 2 , x 3 \u200b \u2026 } ) , g \u200b ( x 2 , { x 1 , x 3 , \u2026 } ) , \u2026 ] . \ud835\udc53 subscript \ud835\udc65 1 \u2026 subscript \ud835\udc65 \ud835\udc5b \ud835\udc54 subscript \ud835\udc65 1 subscript \ud835\udc65 2 subscript \ud835\udc65 3 \u2026 \ud835\udc54 subscript \ud835\udc65 2 subscript \ud835\udc65 1 subscript \ud835\udc65 3 \u2026 \u2026 \\displaystyle\\begin{split}f&([x_{1},\\dots,x_{n}])\\\\\n&=[g(x_{1},\\{x_{2},x_{3}\\dots\\}),g(x_{2},\\{x_{1},x_{3},\\dots\\}),\\dots].\\end{split} (11)\n\n2.4 Multisymmetric Polynomials\n\nWe discuss two different proofs for the universality of Sumformer. For the continuous proof, we use multisymmetric polynomials, which we introduce now. Our definitions are based on (Briand, 2004). Definition 2.10 (Multisymmetric Polynomial). Let . A (real) multisymmetric polynomial in a sequence of length is a polynomial in the variables which is invariant in permutations of . Definition 2.11 (Multisymmetric Power Sum). A multisymmetric power sum of multidegree is a multisymmetric polynomial of the form:\n\np \u03b1 : \ud835\udcb3 n \u2192 \u211d , [ x ( 1 ) , \u2026 , x ( n ) ] \u21a6 \u2211 i = 1 n ( x ( i ) ) \u03b1 : subscript \ud835\udc5d \ud835\udefc formulae-sequence \u2192 superscript \ud835\udcb3 \ud835\udc5b \u211d maps-to superscript \ud835\udc65 1 \u2026 superscript \ud835\udc65 \ud835\udc5b superscript subscript \ud835\udc56 1 \ud835\udc5b superscript superscript \ud835\udc65 \ud835\udc56 \ud835\udefc p_{\\alpha}:\\mathcal{X}^{n}\\rightarrow\\mathbb{R},[x^{(1)},...,x^{(n)}]\\mapsto\\sum_{i=1}^{n}(x^{(i)})^{\\alpha} (12)\n\nwhere . The multisymmetric power sums are of interest because they can generate any multisymmetric polynomial. The following theorem which follows directly from [(Briand, 2004), Theorem 3 & Corollary 5] shows this relationship:\n\nTheorem 2.12 (Multisymmetric Power Sums generate Multisymmetric Polynomials). The real multisymmetric power sums in a sequence of length with multidegree generate all real multisymmetric polynomials (in a sequence of length n), i.e. every multisymmetric polynomial p can be represented by\n\np = \u03c3 \u200b ( p \u03b1 ( 1 ) , \u2026 , p \u03b1 ( z ) ) \ud835\udc5d \ud835\udf0e subscript \ud835\udc5d superscript \ud835\udefc 1 \u2026 subscript \ud835\udc5d superscript \ud835\udefc \ud835\udc67 p=\\sigma(p_{\\alpha^{(1)}},...,p_{\\alpha^{(z)}}) (13)\n\nwith a (real) polynomial and the multisymmetric power sums . 2.5 Deep sets\n\nAs discussed in Section 1.2, the concept of a Sumformer, which we introduce in section 3, is related to the concept of deep sets introduced in (Zaheer et al., 2017). We also utilize the following theorem for the discontinuous proof:\n\nTheorem 2.13 ((Zaheer et al., 2017), Theorem 2). Let , countable and be the power set of . A function operating on can be permutation invariant to the elements in , if and only if it can be decomposed in the form , for suitable transformations and . 3 Sumformer\n\nWe now introduce the new architecture Sumformer. The name stems from the inherent dependence on the sum of a function evaluation of every token separately. Definition 3.1 (Sumformer). Let and let there be two functions . A Sumformer is a sequence-to-sequence function which is evaluated by first computing\n\n\u03a3 := \u2211 k = 1 n \u03d5 \u200b ( x k ) , assign \u03a3 superscript subscript \ud835\udc58 1 \ud835\udc5b italic-\u03d5 subscript \ud835\udc65 \ud835\udc58 \\Sigma:=\\sum_{k=1}^{n}\\phi(x_{k}), (14)\n\nand then\n\n\ud835\udcae \u200b ( [ x 1 , \u2026 , x n ] ) := [ \u03c8 \u200b ( x 1 , \u03a3 ) , \u2026 , \u03c8 \u200b ( x n , \u03a3 ) ] . assign \ud835\udcae subscript \ud835\udc65 1 \u2026 subscript \ud835\udc65 \ud835\udc5b \ud835\udf13 subscript \ud835\udc65 1 \u03a3 \u2026 \ud835\udf13 subscript \ud835\udc65 \ud835\udc5b \u03a3 \\mathcal{S}([x_{1},\\dots,x_{n}]):=[\\psi(x_{1},\\Sigma),\\dots,\\psi(x_{n},\\Sigma)]. (15)\n\nThe Sumformer architecture is simple and can be approximated with Transformers, Linformers, and Performers. The simplicity of the architecture and the ability to prove the universality of multiple architectures using it suggests that Sumformers can also be approximated by other architectures and thereby give universal approximation theorems for them. 4 Universal approximation\n\nIn this section, we give the main theorems of this paper. We first show that Sumformers are universal approximators for continuous sequence-to-sequence functions. This result can be used to give a new proof for the universality of Transformers and the first universal approximation results for Linformer and Performer.",
    "sumformer-5": "Before continuing, we make an important assumption: For the rest of this paper, let and let be a compact set. Note that and do not need to have the same dimensionality in the following theorems. This only simplifies our notation. 4.1 Sumformer\n\nWe show two different proof ideas for the universal approximation by Sumformer.",
    "sumformer-6": "The second relies on a local approximation with a piecewise constant function. This approximation allows us to choose the inherent dimension . Hence, we are able to choose a very small attention matrix. However, due to the discontinuous structure, we need exponentially many feed-forward layers in the sequence lengths and the token size . This problem can be circumvented with an approximation with continuous and using multisymmetric power sums from Definition 2.11. In this case, four feed-forward layers and one attention or summing layer are sufficient. However, the inherent dimension scales with - for a fixed d - in this case. Therefore, the related attention matrices also scale with . We investigate this trade-off further in Section 5 with numerical experiments. Theorem 4.1 (Universal Approximation by Sumformer). For each function and for each there exists a Sumformer such that\n\nsup X \u2208 \ud835\udcb3 n \u2016 f \u200b ( X ) \u2212 \ud835\udcae \u200b ( X ) \u2016 \u221e < \u03b5 . subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc53 \ud835\udc4b \ud835\udcae \ud835\udc4b \ud835\udf00 \\sup_{X\\in\\mathcal{X}^{n}}\\|f(X)-\\mathcal{S}(X)\\|_{\\infty}<\\varepsilon. (16)\n\nProof sketch continuous. We aim to use Theorem 2.12. Therefore, for every , we approximate coordinate of with an equivariant vector of polynomials with an accuracy of (as done in (Hutter, 2020)). This is possible using a version of the Stone-Weierstrass theorem from (Hutter, 2020). Because is equivariant we can use Theorem 2.9 to represent by a semi-invariant polynomial , such that . Now, we use Theorem 2.12 and a representation similar to (Hutter, 2020) to represent using multisymmetric monomials and polynomials of multisymmetric power sums. For this, we define a function mapping to the power sums: Let be the map to all multisymmetric monomials with order . The sum in the Sumformer is then represented as . We represent by\n\n\u03c8 i \u200b ( x ( j ) , \u03a3 ) = \u2211 \u03b1 \u2208 P ( x ( j ) ) \u03b1 \u22c5 \u03c3 \u03b1 \u200b ( \u03a3 \u2212 \u03d5 \u200b ( x ( j ) ) ) subscript \ud835\udf13 \ud835\udc56 superscript \ud835\udc65 \ud835\udc57 \u03a3 subscript \ud835\udefc \ud835\udc43 \u22c5 superscript superscript \ud835\udc65 \ud835\udc57 \ud835\udefc subscript \ud835\udf0e \ud835\udefc \u03a3 italic-\u03d5 superscript \ud835\udc65 \ud835\udc57 \\psi_{i}(x^{(j)},\\Sigma)=\\sum_{\\alpha\\in P}(x^{(j)})^{\\alpha}\\cdot\\sigma_{\\alpha}(\\Sigma-\\phi(x^{(j)})) (17)\n\nwith and are polynomials. Finally, by setting , we obtain a Sumformer with which therefore also fulfills the required goodness of fit. \u220e\n\nProof sketch discontinuous. Instead of approximating the equivariant function , we approximate the semi-invariant and uniformly continuous (since is compact) function , which represents every component as described in Theorem 2.9. To be able to use Theorem 2.13 with a countable input, we approximate with a locally constant function . The used grid is of size for some , which depends on . The new function is also semi-invariant. Now, we can assign every grid point a coordinate where . Furthermore, we can find a function with a finite range which yields the same output if and only if the input sequences are permutations of each other. In the next step, we can use Theorem 2.13 to find and so that . Let be the function mapping tokens to the corresponding cube-coordinate. Then by defining\n\n\u03a3 = \u2211 i = 1 n \u03d5 \u2217 \u200b ( q \u200b ( x ) ) \u03a3 superscript subscript \ud835\udc56 1 \ud835\udc5b superscript italic-\u03d5 \ud835\udc5e \ud835\udc65 \\Sigma=\\sum_{i=1}^{n}\\phi^{*}(q(x)) (18)\n\nand\n\n\u03c8 \u200b ( x 1 , \u03a3 ) := g \u00af \u200b ( \u03c7 \u2212 1 \u200b ( q \u200b ( x 1 ) , \u03bb \u2212 1 \u200b ( \u03c8 \u2217 \u200b ( \u03a3 \u2212 \u03d5 \u200b ( x 1 ) ) ) ) ) assign \ud835\udf13 subscript \ud835\udc65 1 \u03a3 \u00af \ud835\udc54 superscript \ud835\udf12 1 \ud835\udc5e subscript \ud835\udc65 1 superscript \ud835\udf06 1 superscript \ud835\udf13 \u03a3 italic-\u03d5 subscript \ud835\udc65 1 \\displaystyle\\begin{split}&\\psi(x_{1},\\Sigma)\\\\\n&:=\\overline{g}\\bigg{(}\\chi^{-1}\\bigg{(}q(x_{1}),\\;\\lambda^{-1}\\Big{(}\\psi^{*}\\big{(}\\Sigma-\\phi(x_{1})\\big{)}\\Big{)}\\bigg{)}\\bigg{)}\\end{split} (19)\n\nwe yield a Sumformer with the required goodness of fit. Note that even though , in general, might not be invertible, we can find an inverse of a restriction of to a subset of the domain such that properties necessary for our proof are given. \u220e\n\n4.2 Transformer\n\nWith the approximation result for Sumformers, we can now present two new proofs for the universality of Transformers. Before we give these proofs, we want to highlight the first universality theorem for Transformers from (Yun et al., 2019) and discuss the similiarities and differences. Theorem 4.2 (Universal Approximation by Transformer (Yun et al., 2019)). Let and let . Then, for any continuous, permutation-equivariant function with compact support, there exists a Transformer Network such that\n\n( \u222b \u2016 \ud835\udcaf \u200b ( X ) \u2212 f \u200b ( X ) \u2016 p p \u200b \ud835\udc51 X ) 1 / p \u2264 \u03b5 . superscript superscript subscript norm \ud835\udcaf \ud835\udc4b \ud835\udc53 \ud835\udc4b \ud835\udc5d \ud835\udc5d differential-d \ud835\udc4b 1 \ud835\udc5d \ud835\udf00 \\left(\\int\\|\\mathcal{T}(X)-f(X)\\|_{p}^{p}dX\\right)^{1/p}\\leq\\varepsilon. (20)\n\nThe first noticeable difference is the fact that (Yun et al., 2019) uses the norm to measure the accuracy. In our setting, we aim to understand the worst-case behavior and therefore use the supremum norm. Furthermore, (Yun et al., 2019) also gives proofs for functions that are not equivariant by using positional encoding. Because the positional encoding is added only to the input and does not change any further points about the architecture, this can probably be applied also in our case. Beyond the difference in the theorem setup, we also have a very different proof strategy. The proof in (Yun et al., 2019) relies on the concept of contextual mappings. To implement these mappings, the Transformer needs many attention layers, where is the token size and is the desired approximation accuracy. With our proof, we improve upon this result by showing that we only need one attention layer, which is used to represent the sum in the Sumformer. With this information, we can now state our theorem for the universal approximation by Transformers. Theorem 4.3 (Universal Approximation by Transformer). For each function and for each there exists a Transformer such that\n\nsup X \u2208 \ud835\udcb3 n \u2016 f \u200b ( X ) \u2212 \ud835\udcaf \u200b ( X ) \u2016 \u221e < \u03b5 . subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc53 \ud835\udc4b \ud835\udcaf \ud835\udc4b \ud835\udf00 \\sup_{X\\in\\mathcal{X}^{n}}\\|f(X)-\\mathcal{T}(X)\\|_{\\infty}<\\varepsilon. (21)\n\nProof Sketch. First, note that the weights in the attention matrix can be set to zero; this way, we can get feed-forward networks only. In the continuous case, is also continuous and can therefore be approximated with a 2-layer network by (Hornik et al., 1989). For the discontinuous proof, we know from (Yun et al., 2019) that we need many layers for the approximation. In the following steps, we approximate the sum with an attention head. This step is equal for the continuous and discontinuous settings. However, in the discontinuous case, we can set . This step is also the only step we need to investigate for the Linformer and Performer proof. We first use a feed-forward neural network to have as input the matrix:\n\n[ 1 x 1 \u03d5 \u200b ( x 1 ) \ud835\udfce d \u2032 \u2026 1 x n \u03d5 \u200b ( x n ) \ud835\udfce d \u2032 ] \u2208 \u211d n \u00d7 1 + d + 2 \u200b d \u2032 matrix 1 subscript \ud835\udc65 1 italic-\u03d5 subscript \ud835\udc65 1 subscript 0 superscript \ud835\udc51 \u2032 missing-subexpression \u2026 missing-subexpression missing-subexpression 1 subscript \ud835\udc65 \ud835\udc5b italic-\u03d5 subscript \ud835\udc65 \ud835\udc5b subscript 0 superscript \ud835\udc51 \u2032 superscript \u211d \ud835\udc5b 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 \\begin{bmatrix}1&x_{1}&\\phi(x_{1})&\\bm{0}_{d^{\\prime}}\\\\\n&\\ldots&&\\\\\n1&x_{n}&\\phi(x_{n})&\\bm{0}_{d^{\\prime}}\\end{bmatrix}\\in\\mathbb{R}^{n\\times 1+d+2d^{\\prime}} (22)\n\nThen, we choose\n\nW Q = W K = [ e 1 , \ud835\udfce ( 1 + d + 2 \u200b d \u2032 ) \u00d7 ( 1 + d + 2 \u200b d \u2032 ) ] subscript \ud835\udc4a \ud835\udc44 subscript \ud835\udc4a \ud835\udc3e subscript \ud835\udc52 1 subscript 0 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 W_{Q}=W_{K}=[e_{1},\\bm{0}_{(1+d+2d^{\\prime})\\times(1+d+2d^{\\prime})}] (23)\n\nwith such that and such that we get together with the skip connection:\n\n[ 1 x 1 \u03d5 \u200b ( x 1 ) \u03a3 \u2026 1 x n \u03d5 \u200b ( x n ) \u03a3 ] \u2208 \u211d n \u00d7 1 + d + 2 \u200b d \u2032 matrix 1 subscript \ud835\udc65 1 italic-\u03d5 subscript \ud835\udc65 1 \u03a3 missing-subexpression \u2026 missing-subexpression missing-subexpression 1 subscript \ud835\udc65 \ud835\udc5b italic-\u03d5 subscript \ud835\udc65 \ud835\udc5b \u03a3 superscript \u211d \ud835\udc5b 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 \\begin{bmatrix}1&x_{1}&\\phi(x_{1})&\\Sigma\\\\\n&\\ldots&&\\\\\n1&x_{n}&\\phi(x_{n})&\\Sigma\\end{bmatrix}\\in\\mathbb{R}^{n\\times 1+d+2d^{\\prime}} (24)\n\nWe can then, in the continuous case, apply another two layers for the approximation of the continuous , or we need another many feed-forward layers to approximate the build in the discontinuous case. \u220e\n\n4.2.1 Network size\n\nUsing Sumformer, we were able to give two different constructions for the Transformer as universal approximators. We note that the construction of the attention head remains the same except for the possible choice of . When we approximate and with smooth functions, we need a larger latent dimension . In the discontinuous construction, we need more layers to approximate and but can approximate the function of interest using only . The same situation can be observed for the efficient Transformers as we only replace the attention heads but keep the functions and from the proof of the Transformer. There might be another way of representing functions with Sumformers. However, the current proofs suggest a trade-off between the size of the latent dimension and the number of necessary layers. In Section 5, we test the dependence of the validation loss on the relationship of to the sequence length and the token size . 4.3 Efficient Transformers are Universal Approximators\n\nUsing the concept of Sumformer, we can show that Linformer and Performer are universal approximators for continuous functions on a compact support. We are able to utilize the proof for Transformers as the architecture is only changed in the attention head, which forms the main computational cost of Transformer. As the rest of the architecture stays the same, this part of the proof does not need to be adapted. We start with Linformer as introduced in Definition 2.5. Theorem 4.4 (Universal Approximation by Linformer). For each function and for each there exist and there exist matrices and a Linformer such that\n\nsup X \u2208 \ud835\udcb3 n \u2016 f \u200b ( X ) \u2212 \ud835\udcaf Lin \u200b ( X ) \u2016 \u221e < \u03b5 . subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc53 \ud835\udc4b subscript \ud835\udcaf Lin \ud835\udc4b \ud835\udf00 \\sup_{X\\in\\mathcal{X}^{n}}\\|f(X)-\\mathcal{T}_{\\mathrm{Lin}}(X)\\|_{\\infty}<\\varepsilon. (25)\n\nProof. By Definition 2.5, Linformer have the same architecture as Transformer except for the attention head. Therefore, we can use the same construction for and as in the proof of Theorem 4.2. It remains to show that we can represent the sum in the Sumformer with the linear attention head as well. We now discuss how the weight and projection matrices are chosen for the approximation. Let and , as in Equation (23) and we get that the Linformer attention layers maps to\n\n\u03c1 \u200b ( ( X \u200b W Q ) \u200b ( E \u200b X \u200b W K ) T ) \u22c5 ( F \u200b X \u200b W V ) = [ \ud835\udfce n \u00d7 1 + d + d \u2032 , \u03a3 ] \u22c5 \ud835\udf0c \ud835\udc4b subscript \ud835\udc4a \ud835\udc44 superscript \ud835\udc38 \ud835\udc4b subscript \ud835\udc4a \ud835\udc3e \ud835\udc47 \ud835\udc39 \ud835\udc4b subscript \ud835\udc4a \ud835\udc49 subscript 0 \ud835\udc5b 1 \ud835\udc51 superscript \ud835\udc51 \u2032 \u03a3 \\displaystyle\\begin{split}&\\rho((XW_{Q})(EXW_{K})^{T})\\cdot(FXW_{V})=[\\bm{0}_{n\\times 1+d+d^{\\prime}},\\Sigma]\\end{split} (26)\n\nAfter applying the skip connection, we get the same output as in Equation (24) in Theorem 4.2. Therefore, we can apply the same representation for and get the desired approximation. \u220e\n\nNow, even though the structure and idea of Performer differ a lot from Linformer, we can use a similar strategy to show the universal approximation. Theorem 4.5 (Universal Approximation by Performer). Let with . For each function and for each there exists a Performer such that\n\nsup X \u2208 \ud835\udcb3 n \u2016 f \u200b ( X ) \u2212 \ud835\udcaf Per \u200b ( X ) \u2016 \u221e < \u03b5 . subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc53 \ud835\udc4b subscript \ud835\udcaf Per \ud835\udc4b \ud835\udf00 \\sup_{X\\in\\mathcal{X}^{n}}\\|f(X)-\\mathcal{T}_{\\mathrm{Per}}(X)\\|_{\\infty}<\\varepsilon. (27)\n\nProof. As in the proof for the Linformer attention layer we use the fact that the Performer only differs from a Transformer by the choice of the attention head. Therefore, we now build a Performer attention head which is able to approximate the sum for the Sumformer. We choose the same and as in Equation (23). Next, we fix the vectors in in the Performer Definition 2.6. Then, because all rows are the same and is applied row-wise, for some\n\nIn contrast, to the previous proof, we need to add another feed-forward layer after the attention layer. We choose the weight matrix to be and the bias . Then, we get an output of\n\nW \u200b a \u200b ( X \u200b W Q ) \u200b a \u200b ( X \u200b W K ) \u22a4 \u200b ( X \u200b W V ) + b = [ \ud835\udfce n \u00d7 1 + d + d \u2032 , \u03a3 ] T . \ud835\udc4a \ud835\udc4e \ud835\udc4b subscript \ud835\udc4a \ud835\udc44 \ud835\udc4e superscript \ud835\udc4b subscript \ud835\udc4a \ud835\udc3e top \ud835\udc4b subscript \ud835\udc4a \ud835\udc49 \ud835\udc4f superscript subscript 0 \ud835\udc5b 1 \ud835\udc51 superscript \ud835\udc51 \u2032 \u03a3 \ud835\udc47 \\displaystyle\\begin{split}&Wa(XW_{Q})a(XW_{K})^{\\top}(XW_{V})+b\\\\\n&=[\\bm{0}_{n\\times 1+d+d^{\\prime}},\\Sigma]^{T}.\\end{split} (28)\n\nWith the skip connection we get the desired input for and are able to use the same approximation for as in Theorem 4.3. \u220e\n\n5 Numerical Experiments\n\nWe implemented two different Sumformer architectures and tested them on approximating analytically given (i.e., non-real-world) functions. Both architectures consist of three components: one representing , one representing , and the last combining the two as described in Definition 3.1. The function is represented by a Multi-layer perceptron (MLP) in both architectures. The representation of differs: The first model uses the we constructed in the proof of Theorem 4.1 (Polynomial Sumformer), whereas the second one uses an MLP again (MLP Sumformer). Each MLP we used consisted of five hidden layers of 50 nodes. We use the ReLU activation function. We trained our two models (using the same latent dimension ) on approximating multiple equivariant functions (assuming ): two polynomial-type and two non-polynomial-type functions. The results (Fig.1) show that the previous results are not just theoretical: Sumformer architectures can approximate a variety of functions. It is interesting to note that the two Sumformers perform approximately equally well on most functions we approximated (polynomial & non-polynomial type). Based on this, we observe that the construction used in the continuous proof of Theorem 4.1 is indeed able to learn our benchmark functions using gradient descent. Furthermore, we observe that the validation loss of the Polynomial Sumformer is smoother and decreases in a more stable way than that of the MLP Sumformer. In contrast, the validation loss of the MLP Sumformer often jumps to drastically lower levels over just a few epochs and is relatively flat apart from that. This phenomenon could be explained by the interaction of the two disjoint trainable components (MLPs). We also tested how changing the dimension (see Definition 3.1) in the MLP Sumformer impacts the best validation loss over a fixed number of epochs while holding and the function to approximate constant. The results (Fig. 2) show - as expected - that higher dimensions generally lead to better approximation. Furthermore, when changing linearly, we have to make non-linear - presumably exponential - changes to the size of to achieve significantly diminishing returns on further increasing .",
    "sumformer-7": "This finding is particularly interesting as the continuous proof of Theorem 4.1 needs in for a fixed . This suggests that the empirical performance aligns with the theory. 6 Conclusion\n\nWe have seen that the efficient Transformers, Linformer, and Performer, are able to represent all equivariant continuous sequence-to-sequence functions on compact sets arbitrarily well. Due to the simplicity of the Sumformer architecture on which the proofs are based, it seems likely that further research can use similar techniques to show that other Transformer architectures and state space models are also universal approximators. In addition, we offered a new proof for universal approximation by Transformer and were able to reduce the necessary number of non-zero attention layers to only one. In our experiments, we showed that the construction from our continuous proof of universal approximation by Sumformer is tractable and indeed able to approximate given functions using gradient descent. Furthermore, our numerical results about the impact of the latent dimension of a Sumformer in relation to the token size nicely relate to the required size of the latent dimension in our continuous proof. Lastly, we note that a significant limitation of our continuous proof is that (for a fixed token size ) the size of the attention matrix scales with . In other words: Although for a fixed model dimension the computational cost scales linearly in , for achieving universal approximation the required dimension grows polynomially in and correspondingly the overall computational cost. In the discontinuous setting, we were able to keep the latent dimension small but had to scale the number of feed-forward layers accordingly. It would be interesting to improve on this result and analyze the trade-off further in future research. Acknowledgements\n\nLT and GK acknowledge support from the German Research Foundation in the frame of the priority programme SPP 2298. SA appreciates the support by the Stanford Graduate Fellowship. GK is also grateful for partial support by the Konrad Zuse School of Excellence in Reliable AI (DAAD), the Munich Center for Machine Learning (BMBF) as well as the German Research Foundation under Grants KU 1446/31-1 and KU 1446/32-1 and under Grant DFG-SFB/TR 109, Project C09 and the Federal Ministry of Education and Research under Grant MaGriDo. References\n\nBarron (1994) Barron, A. R. Approximation and estimation bounds for artificial neural networks. Machine learning, 14:115\u2013133, 1994. Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Bolcskei et al. (2019) Bolcskei, H., Grohs, P., Kutyniok, G., and Petersen, P. Optimal approximation with sparsely connected deep neural networks.",
    "sumformer-8": "SIAM Journal on Mathematics of Data Science, 1(1):8\u201345, 2019. Briand (2004) Briand, E. When is the algebra of multisymmetric polynomials generated by the elementary multisymmetric polynomials? Beitr\u00e4ge zur Algebra und Geometrie: Contributions to Algebra and Geometry, 45 (2), 353-368., 2004.",
    "sumformer-9": "Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.",
    "sumformer-10": "Chen et al. (2022) Chen, C., Chen, Z., and Lu, J. Representation theorem for multivariable totally symmetric functions.",
    "sumformer-11": "arXiv preprint arXiv:2211.15958, 2022. Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021. Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "sumformer-12": "arXiv preprint arXiv:1904.10509, 2019. Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers.",
    "sumformer-13": "arXiv preprint arXiv:2009.14794, 2020. Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.",
    "sumformer-14": "arXiv preprint arXiv:2010.11929, 2020. Eraslan et al. (2019) Eraslan, G., Avsec, \u017d., Gagneur, J., and Theis, F. J. Deep learning: new computational modelling techniques for genomics. Nature Reviews Genetics, 20(7):389\u2013403, 2019.",
    "sumformer-15": "G\u00fchring et al. (2020) G\u00fchring, I., Kutyniok, G., and Petersen, P. Error bounds for approximations with deep relu neural networks in w s, p norms.",
    "sumformer-16": "Analysis and Applications, 18(05):803\u2013859, 2020. He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016. Hornik et al. (1989) Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approximators.",
    "sumformer-17": "Neural networks, 2(5):359\u2013366, 1989. Hutter (2020) Hutter, M. On representing (anti) symmetric functions. arXiv preprint arXiv:2007.15298, 2020. Jumper et al. (2021) Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., \u017d\u00eddek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold.",
    "sumformer-18": "Nature, 596(7873):583\u2013589, 2021. Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "sumformer-19": "In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020. Kitaev et al. (2020) Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer.",
    "sumformer-20": "arXiv preprint arXiv:2001.04451, 2020. Likhosherstov et al. (2021) Likhosherstov, V., Choromanski, K., and Weller, A. On the expressive power of self-attention matrices.",
    "sumformer-21": "arXiv preprint arXiv:2106.03764, 2021. Lu et al. (2017) Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expressive power of neural networks: A view from the width. Advances in neural information processing systems, 30, 2017. Mhaskar (1996) Mhaskar, H. N. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164\u2013177, 1996. Petersen & Voigtlaender (2018) Petersen, P. and Voigtlaender, F. Optimal approximation of piecewise smooth functions using deep relu neural networks. Neural Networks, 108:296\u2013330, 2018. Shaham et al. (2018) Shaham, U., Cloninger, A., and Coifman, R. R. Provable approximation properties for deep neural networks. Applied and Computational Harmonic Analysis, 44(3):537\u2013557, 2018. Tay et al. (2020) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers, 2020. Tay et al. (2022) Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1\u201328, 2022. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wagstaff et al. (2019) Wagstaff, E., Fuchs, F., Engelcke, M., Posner, I., and Osborne, M. A. On the limitations of representing functions on sets. In International Conference on Machine Learning, pp. 6487\u20136494. PMLR, 2019. Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity.",
    "sumformer-22": "arXiv preprint arXiv:2006.04768, 2020. Wen et al. (2022) Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. Transformers in time series: A survey.",
    "sumformer-23": "arXiv preprint arXiv:2202.07125, 2022. Yarotsky (2017) Yarotsky, D. Error bounds for approximations with deep relu networks. Neural Networks, 94:103\u2013114, 2017. Yarotsky (2022) Yarotsky, D. Universal approximations of invariant maps by neural networks. Constructive Approximation, 55(1):407\u2013474, 2022. Yun et al. (2019) Yun, C., Bhojanapalli, S., Rawat, A.",
    "sumformer-24": "S., Reddi, S. J., and Kumar, S. Are transformers universal approximators of sequence-to-sequence functions?",
    "sumformer-25": "arXiv preprint arXiv:1912.10077, 2019. Yun et al. (2020) Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. connections are expressive enough: Universal approximability of sparse transformers. Advances in Neural Information Processing Systems, 33:13783\u201313794, 2020. Zaheer et al. (2017) Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. Advances in neural information processing systems, 30, 2017. Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020. Appendix A Proofs of the universal approximation results for Sumformer\n\nIn this section we give the details of the continuous and discontinous proofs of Theorem 4.1. Discontinuous case. By Lemma 2.9, there exists a semi-invariant function such that . Since is continuous, the component functions are also continuous and thus also . The compactness of implies that is compact and therefore is uniformly continuous. Without loss of generality, let the compact support of be contained in . Then, we define a piece-wise constant function by\n\ng \u00af \u200b ( X ) = \u2211 \ud835\udc91 \u2208 \ud835\udca2 g \u200b ( \ud835\udc91 ) \u200b \ud835\udfcf \u200b { X \u2208 C \ud835\udc91 } , \u00af \ud835\udc54 \ud835\udc4b subscript \ud835\udc91 \ud835\udca2 \ud835\udc54 \ud835\udc91 1 \ud835\udc4b subscript \ud835\udc36 \ud835\udc91 \\overline{g}(X)=\\sum_{\\bm{p}\\in\\mathcal{G}}g(\\bm{p})\\bm{1}\\{X\\in C_{\\bm{p}}\\}, (29)\n\nwhere the grid for some with consists of cubes with corresponding values for each . Because is uniformly continuous, there exists for each a such that\n\nsup X \u2208 \ud835\udcb3 n \u2016 g \u200b ( X ) \u2212 g \u00af \u200b ( X ) \u2016 \u221e < \u03b5 . subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc54 \ud835\udc4b \u00af \ud835\udc54 \ud835\udc4b \ud835\udf00 \\sup_{X\\in\\mathcal{X}^{n}}\\|g(X)-\\overline{g}(X)\\|_{\\infty}<\\varepsilon. (30)\n\nWe next show that that is semi-invariant. Since is semi-invariant, we have for any permutation . With , we can write and get . Moreover, we get Hence, for any , we get\n\ng \u00af \u200b ( X ) = g \u200b ( \ud835\udc91 ) = g \u200b ( \u03c0 \u200b ( \ud835\udc91 ) ) = g \u00af \u200b ( \u03c0 \u200b ( X ) ) . \u00af \ud835\udc54 \ud835\udc4b \ud835\udc54 \ud835\udc91 \ud835\udc54 \ud835\udf0b \ud835\udc91 \u00af \ud835\udc54 \ud835\udf0b \ud835\udc4b \\overline{g}(X)=g(\\bm{p})=g(\\pi(\\bm{p}))=\\overline{g}(\\pi(X)). (31)\n\nNow, we want to to represent using an appropriate . While it is trivial to match each to its corresponding such that , it is more difficult to find the corresponding cube of when only being able to use and the aggregated . To achieve this, we will use the following strategy: Recall that is the number of cubes in each dimension. We can assign each grid point a coordinate The map is bijective and the first part of the coordinate can be constructed from by quantizing it in each dimension. Let be this quantization function such that . Let us now find a way to choose and such that we can reconstruct from . We can treat as a sequence of length and write with . Since there are finitely many , we can enumerate all using a function . Moreover, let us choose to be invariant to permutations of , i.e. for all permutations we have but we let always assign different values to if they are not a permutation of each other. Although this prevents from being injective, all cubes with the same value under have the same value under , due to semi-invariance, i.e. for a fixed and for all in the range of the inverse is well defined and we can evaluate\n\ng \u00af \u200b ( \u03c7 \u2212 1 \u200b ( a , \u03bb \u2212 1 \u200b ( n ) ) ) . \u00af \ud835\udc54 superscript \ud835\udf12 1 \ud835\udc4e superscript \ud835\udf06 1 \ud835\udc5b \\overline{g}\\Big{(}\\chi^{-1}\\big{(}a,\\lambda^{-1}(n)\\big{)}\\Big{)}. (32)\n\nNow, is an invariant sequence-to-point function and since is countable, we can utilize Theorem 2.13 (note that we use multisets of a fixed size here, to which the proof in (Zaheer et al., 2017) can be easily extended) to find and such that\n\n\u03bb \u200b ( \ud835\udc83 ) = \u03c8 \u2217 \u200b ( \u2211 i = 1 n \u2212 1 \u03d5 \u2217 \u200b ( b i ) ) \ud835\udf06 \ud835\udc83 superscript \ud835\udf13 superscript subscript \ud835\udc56 1 \ud835\udc5b 1 superscript italic-\u03d5 subscript \ud835\udc4f \ud835\udc56 \\lambda(\\bm{b})=\\psi^{*}\\left(\\sum_{i=1}^{n-1}\\phi^{*}(b_{i})\\right)\n\nWith the quantization function we set and define\n\n\u03a3 = \u2211 i = 1 n \u03d5 \u2217 \u200b ( q \u200b ( x ) ) . \u03a3 superscript subscript \ud835\udc56 1 \ud835\udc5b superscript italic-\u03d5 \ud835\udc5e \ud835\udc65 \\Sigma=\\sum_{i=1}^{n}\\phi^{*}(q(x)). (33)\n\nWe can than recover by\n\n\u03bb \u200b ( \ud835\udc83 ) = \u03c8 \u2217 \u200b ( \u03a3 \u2212 \u03d5 \u200b ( x 1 ) ) . \ud835\udf06 \ud835\udc83 superscript \ud835\udf13 \u03a3 italic-\u03d5 subscript \ud835\udc65 1 \\lambda(\\bm{b})=\\psi^{*}\\left(\\Sigma-\\phi(x_{1})\\right). (34)\n\nNow, we can define such that the related is equal to\n\n\u03c8 \u200b ( x 1 , \u03a3 ) := g \u00af \u200b ( \u03c7 \u2212 1 \u200b ( q \u200b ( x 1 ) , \u03bb \u2212 1 \u200b ( \u03c8 \u2217 \u200b ( \u03a3 \u2212 \u03d5 \u200b ( x 1 ) ) ) ) ) . assign \ud835\udf13 subscript \ud835\udc65 1 \u03a3 \u00af \ud835\udc54 superscript \ud835\udf12 1 \ud835\udc5e subscript \ud835\udc65 1 superscript \ud835\udf06 1 superscript \ud835\udf13 \u03a3 italic-\u03d5 subscript \ud835\udc65 1 \\psi(x_{1},\\Sigma):=\\overline{g}\\bigg{(}\\chi^{-1}\\bigg{(}q(x_{1}),\\;\\lambda^{-1}\\Big{(}\\psi^{*}\\big{(}\\Sigma-\\phi(x_{1})\\big{)}\\Big{)}\\bigg{)}\\bigg{)}. (35)\n\nSince we chose to uniformly approximate and thereby each component of up to error, this implies that uniformly approximates up to error. \u220e\n\nContinuous case. As before we have that the compactness of implies that is compact and without loss of generality, we can assume that the compact support of is contained in . Now, for every , we approximate coordinate of with an equivariant vector of polynomials with an accuracy of (as done in (Hutter, 2020)). This is possible using a version of the Stone-Weierstrass theorem from (Hutter, 2020). Because is equivariant we can use Theorem 2.9 to represent by a semi-invariant polynomial , such that . Now, we use Theorem 2.12 and a representation similar to (Hutter, 2020) to represent using multisymmetric monomials and polynomials of multisymmetric power sums. For this, we define a function mapping to the power sums: Let\n\n\u03d5 : [ 0 , 1 ] d \u2192 \u211d d \u2032 , x \u21a6 ( x 1 1 \u200b x 2 0 \u200b \u22ef \u200b x d 0 x 1 2 \u200b x 2 0 \u200b \u22ef \u200b x d 0 \u22ee x 1 \u03b1 1 \u200b x 2 \u03b1 2 \u200b \u22ef \u200b x d \u03b1 d \u22ee x 1 0 \u200b x 2 0 \u200b \u22ef \u200b x d n ) : italic-\u03d5 formulae-sequence \u2192 superscript 0 1 \ud835\udc51 superscript \u211d superscript \ud835\udc51 \u2032 maps-to \ud835\udc65 matrix superscript subscript \ud835\udc65 1 1 superscript subscript \ud835\udc65 2 0 \u22ef superscript subscript \ud835\udc65 \ud835\udc51 0 superscript subscript \ud835\udc65 1 2 superscript subscript \ud835\udc65 2 0 \u22ef superscript subscript \ud835\udc65 \ud835\udc51 0 \u22ee superscript subscript \ud835\udc65 1 subscript \ud835\udefc 1 superscript subscript \ud835\udc65 2 subscript \ud835\udefc 2 \u22ef superscript subscript \ud835\udc65 \ud835\udc51 subscript \ud835\udefc \ud835\udc51 \u22ee superscript subscript \ud835\udc65 1 0 superscript subscript \ud835\udc65 2 0 \u22ef superscript subscript \ud835\udc65 \ud835\udc51 \ud835\udc5b \\phi:[0,1]^{d}\\rightarrow\\mathbb{R}^{d^{\\prime}},x\\mapsto\\begin{pmatrix}x_{1}^{1}x_{2}^{0}\\cdots x_{d}^{0}\\\\\nx_{1}^{2}x_{2}^{0}\\cdots x_{d}^{0}\\\\\n\\vdots\\\\\nx_{1}^{\\alpha_{1}}x_{2}^{\\alpha_{2}}\\cdots x_{d}^{\\alpha_{d}}\\\\\n\\vdots\\\\\nx_{1}^{0}x_{2}^{0}\\cdots x_{d}^{n}\\\\\n\\end{pmatrix} (36)\n\nwhere runs over all multidegrees with order . The sum in the Sumformer is then represented as . By Theorem 2.12 the function\n\ns j \u200b ( x ( i \u2260 j ) ) = \u03c3 \u200b ( \u2211 i \u2260 j \u03d5 \u200b ( x ( i ) ) ) subscript \ud835\udc60 \ud835\udc57 superscript \ud835\udc65 \ud835\udc56 \ud835\udc57 \ud835\udf0e subscript \ud835\udc56 \ud835\udc57 italic-\u03d5 superscript \ud835\udc65 \ud835\udc56 s_{j}(x^{(i\\neq j)})=\\sigma\\left(\\sum_{i\\neq j}\\phi(x^{(i)})\\right) (37)\n\nwith being a polynomial function can fit any multisymmetric polynomial in the variables perfectly. We can therefore represent by\n\n\u03c8 i \u200b ( x ( j ) , \u03a3 ) = \u2211 \u03b1 \u2208 P ( x ( j ) ) \u03b1 \u22c5 \u03c3 \u03b1 \u200b ( \u03a3 \u2212 \u03d5 \u200b ( x ( j ) ) ) subscript \ud835\udf13 \ud835\udc56 superscript \ud835\udc65 \ud835\udc57 \u03a3 subscript \ud835\udefc \ud835\udc43 \u22c5 superscript superscript \ud835\udc65 \ud835\udc57 \ud835\udefc subscript \ud835\udf0e \ud835\udefc \u03a3 italic-\u03d5 superscript \ud835\udc65 \ud835\udc57 \\psi_{i}(x^{(j)},\\Sigma)=\\sum_{\\alpha\\in P}(x^{(j)})^{\\alpha}\\cdot\\sigma_{\\alpha}(\\Sigma-\\phi(x^{(j)})) (38)\n\nwith and are polynomials. By setting , we obtain a Sumformer with which is able to approximate sufficiently well. \u220e\n\nAppendix B Proofs of the universal approximation results for Transformer\n\nNow we give the detailed proof of the universality of Transformers from Theorem 4.3. Proof. We use the triangular inequality to divide the approximation in two steps. We first approximate by a Sumformer and then show that the Sumformer can be approximated by a Transformer , i.e. sup X \u2208 \ud835\udcb3 n \u2016 f \u200b ( X ) \u2212 \ud835\udcaf \u200b ( X ) \u2016 \u221e \u2264 sup X \u2208 \ud835\udcb3 n \u2016 f \u200b ( X ) \u2212 \ud835\udcae \u200b ( X ) \u2016 \u221e + sup X \u2208 \ud835\udcb3 n \u2016 \ud835\udcae \u200b ( X ) \u2212 \ud835\udcaf \u200b ( X ) \u2016 \u221e subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc53 \ud835\udc4b \ud835\udcaf \ud835\udc4b subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udc53 \ud835\udc4b \ud835\udcae \ud835\udc4b subscript supremum \ud835\udc4b superscript \ud835\udcb3 \ud835\udc5b subscript norm \ud835\udcae \ud835\udc4b \ud835\udcaf \ud835\udc4b \\sup_{X\\in\\mathcal{X}^{n}}\\|f(X)-\\mathcal{T}(X)\\|_{\\infty}\\leq\\sup_{X\\in\\mathcal{X}^{n}}\\|f(X)-\\mathcal{S}(X)\\|_{\\infty}+\\sup_{X\\in\\mathcal{X}^{n}}\\|\\mathcal{S}(X)-\\mathcal{T}(X)\\|_{\\infty} (39)\n\nFor the first summand we have from Theorem 4.1 that there is a Sumformer which approximates to an accuracy of . The Sumformer has the inherent latent dimension . We now turn to the second summand and construct a Transformer that is able to approximate the Sumformer to accuracy. Transformers are constructed as described in Definition 2.4. Because of the structure with , we can set the attention for the first layers to zero. Thereby, we obtain feed-forward layers without attention. The Transformer is then constructed as follows. We have the input with and map it with a feed-forward from the right to\n\n[ x 1 , x 1 \u22ef x n , x n ] \u2208 \u211d n \u00d7 2 \u200b d . matrix subscript \ud835\udc65 1 subscript \ud835\udc65 1 \u22ef subscript \ud835\udc65 \ud835\udc5b subscript \ud835\udc65 \ud835\udc5b superscript \u211d \ud835\udc5b 2 \ud835\udc51 \\begin{bmatrix}x_{1},x_{1}\\\\\n\\cdots\\\\\nx_{n},x_{n}\\end{bmatrix}\\in\\mathbb{R}^{n\\times 2d}. (40)\n\nWe can then find a two layer feed-forward network such that it acts as the identity on the first components and approximates the function . The approximation with two feed forward layers of is possible because of the universal approximation theorem (Hornik et al., 1989). In the discontinuous setting we need more layers to approximate . Therefore, after three feed-forward layers we get\n\n[ x 1 , \u03d5 \u200b ( x 1 ) \u22ef x n , \u03d5 \u200b ( x n ) ] \u2208 \u211d n \u00d7 ( d + d \u2032 ) . matrix subscript \ud835\udc65 1 italic-\u03d5 subscript \ud835\udc65 1 \u22ef subscript \ud835\udc65 \ud835\udc5b italic-\u03d5 subscript \ud835\udc65 \ud835\udc5b superscript \u211d \ud835\udc5b \ud835\udc51 superscript \ud835\udc51 \u2032 \\begin{bmatrix}x_{1},\\phi(x_{1})\\\\\n\\cdots\\\\\nx_{n},\\phi(x_{n})\\end{bmatrix}\\in\\mathbb{R}^{n\\times(d+d^{\\prime})}. (41)\n\nBefore, we get to the attention layer we add one more layer from the right with\n\nW = [ \ud835\udfce d \u00d7 1 I d \ud835\udfce d \u00d7 d \u2032 \ud835\udfce d \u00d7 d \u2032 \ud835\udfce d \u2032 \u00d7 1 \ud835\udfce d \u2032 \u00d7 d I d \u2032 \ud835\udfce d \u2032 \u00d7 d \u2032 ] \u2208 \u211d ( d + d \u2032 ) \u00d7 ( 1 + d + 2 \u200b d \u2032 ) \ud835\udc4a matrix subscript 0 \ud835\udc51 1 subscript \ud835\udc3c \ud835\udc51 subscript 0 \ud835\udc51 superscript \ud835\udc51 \u2032 subscript 0 \ud835\udc51 superscript \ud835\udc51 \u2032 subscript 0 superscript \ud835\udc51 \u2032 1 subscript 0 superscript \ud835\udc51 \u2032 \ud835\udc51 subscript \ud835\udc3c superscript \ud835\udc51 \u2032 subscript 0 superscript \ud835\udc51 \u2032 superscript \ud835\udc51 \u2032 superscript \u211d \ud835\udc51 superscript \ud835\udc51 \u2032 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 W=\\begin{bmatrix}\\bm{0}_{d\\times 1}&I_{d}&\\bm{0}_{d\\times d^{\\prime}}&\\bm{0}_{d\\times d^{\\prime}}\\\\\n\\bm{0}_{d^{\\prime}\\times 1}&\\bm{0}_{d^{\\prime}\\times d}&I_{d^{\\prime}}&\\bm{0}_{d^{\\prime}\\times d^{\\prime}}\\end{bmatrix}\\in\\mathbb{R}^{(d+d^{\\prime})\\times(1+d+2d^{\\prime})} (42)\n\nand . Using these transformations, we get as output after the first step:\n\nX 1 = [ 1 x 1 \u03d5 \u200b ( x 1 ) \ud835\udfce d \u2032 \u22ef 1 x n \u03d5 \u200b ( x n ) \ud835\udfce d ] \u2208 \u211d n \u00d7 1 + d + 2 \u200b d \u2032 subscript \ud835\udc4b 1 matrix 1 subscript \ud835\udc65 1 italic-\u03d5 subscript \ud835\udc65 1 subscript 0 superscript \ud835\udc51 \u2032 missing-subexpression \u22ef missing-subexpression missing-subexpression 1 subscript \ud835\udc65 \ud835\udc5b italic-\u03d5 subscript \ud835\udc65 \ud835\udc5b subscript 0 \ud835\udc51 superscript \u211d \ud835\udc5b 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 X_{1}=\\begin{bmatrix}1&x_{1}&\\phi(x_{1})&\\mathbf{0}_{d^{\\prime}}\\\\\n&\\cdots&&\\\\\n1&x_{n}&\\phi(x_{n})&\\mathbf{0}_{d}\\end{bmatrix}\\in\\mathbb{R}^{n\\times 1+d+2d^{\\prime}} (43)\n\nNote that these steps are the same for the efficient Transformers. Now, we turn to the attention head to represent the sum . First we choose for , such that\n\nA = \u03c1 \u200b ( ( X 1 \u200b W Q ) \u200b ( X 1 \u200b W K ) \u22a4 ) = 1 n \u200b \ud835\udfcf n \u00d7 n . \ud835\udc34 \ud835\udf0c subscript \ud835\udc4b 1 subscript \ud835\udc4a \ud835\udc44 superscript subscript \ud835\udc4b 1 subscript \ud835\udc4a \ud835\udc3e top 1 \ud835\udc5b subscript 1 \ud835\udc5b \ud835\udc5b A=\\rho((X_{1}W_{Q})(X_{1}W_{K})^{\\top})=\\frac{1}{n}\\mathbf{1}_{n\\times n}. (44)\n\nThe matrix will then be multiplied with . We can choose\n\nW V = [ \ud835\udfce ( 1 + d ) \u00d7 ( 1 + d + d \u2032 ) \ud835\udfce ( 1 + d ) \u00d7 d \u2032 \ud835\udfce d \u2032 \u00d7 ( 1 + d + d \u2032 ) n \u22c5 I d \u2032 \ud835\udfce d \u2032 \u00d7 ( 1 + d + d \u2032 ) \ud835\udfce d \u2032 \u00d7 d \u2032 ] \u2208 \u211d ( 1 + d + 2 \u200b d \u2032 ) \u00d7 ( 1 + d + 2 \u200b d \u2032 ) . subscript \ud835\udc4a \ud835\udc49 delimited-[] subscript 0 1 \ud835\udc51 1 \ud835\udc51 superscript \ud835\udc51 \u2032 subscript 0 1 \ud835\udc51 superscript \ud835\udc51 \u2032 subscript 0 superscript \ud835\udc51 \u2032 1 \ud835\udc51 superscript \ud835\udc51 \u2032 \u22c5 \ud835\udc5b subscript \ud835\udc3c superscript \ud835\udc51 \u2032 subscript 0 superscript \ud835\udc51 \u2032 1 \ud835\udc51 superscript \ud835\udc51 \u2032 subscript 0 superscript \ud835\udc51 \u2032 superscript \ud835\udc51 \u2032 superscript \u211d 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 1 \ud835\udc51 2 superscript \ud835\udc51 \u2032 W_{V}=\\left[\\begin{array}[]{cc}\\bm{0}_{(1+d)\\times(1+d+d^{\\prime})}&\\bm{0}_{(1+d)\\times d^{\\prime}}\\\\\n\\bm{0}_{d^{\\prime}\\times(1+d+d^{\\prime})}&n\\cdot I_{d^{\\prime}}\\\\\n\\bm{0}_{d^{\\prime}\\times(1+d+d^{\\prime})}&\\bm{0}_{d^{\\prime}\\times d^{\\prime}}\\end{array}\\right]\\in\\mathbb{R}^{(1+d+2d^{\\prime})\\times(1+d+2d^{\\prime})}. (45)\n\nThe output of this attention layer is\n\n[ \ud835\udfce 1 + d + d \u2032 , \u03a3 ] \u22a4 . superscript subscript 0 1 \ud835\udc51 superscript \ud835\udc51 \u2032 \u03a3 top [\\bm{0}_{1+d+d^{\\prime}},\\Sigma]^{\\top}. (46)\n\nThen, we apply a residual connection and obtain\n\n[ 1 , x i , \u03d5 \u200b ( x i ) , \u03a3 ] \u22a4 . superscript 1 subscript \ud835\udc65 \ud835\udc56 italic-\u03d5 subscript \ud835\udc65 \ud835\udc56 \u03a3 top [1,x_{i},\\phi(x_{i}),\\Sigma]^{\\top}. (47)\n\nLast, we implement . For the discontinuous case, we first compute . Then, we map a finite set of values to another finite set of values for which we can use Lemma 7 in (Yun et al., 2019). Hence, we need to add another feed-forward layers for the approximation of . In the continuous case this can be avoided because of the continuity of , we can approximate it with the universal approximation theorem (Hornik et al., 1989) with feed-forward layers. \u220e\n\nAppendix C Deep Sets\n\nSumformers are related to the concept of deep sets introduced in (Zaheer et al., 2017). For the discrete proof we use Theorem 2.13. However, there is also a version for uncountable inputs which we highlight here:\n\nTheorem C.1 ((Zaheer et al., 2017), Theorem 9). Assume the elements are from a compact set in , i.e. possibly uncountable, and the set size is fixed to . Then any continuous function operating on a set , i.e. which is permutation invariant to the elements in can be approximated arbitrarily close in the form of , for suitable transformations and . The fundamental differences of the previous theorem to our work are that we consider equivariant, continuous sequence-to-sequence functions.",
    "sumformer-26": "This difference is the reason why we need a second parameter in . \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Feb 28 18:21:33 2024 by LaTeXML"
}