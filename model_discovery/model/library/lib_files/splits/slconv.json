{
    "slconv-0": "Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\nZicheng Liu Siyuan Li Li Wang Zedong Wang Yunfan Liu Stan Z.",
    "slconv-1": "Li\n\nAbstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. Machine Learning, ICML\n\n1 Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (Devlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021, 2020a, 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.",
    "slconv-2": "It is worth noting that their complexity is essentially quadratic, and the corresponding linear versions both suffer performance degradation (see Fig. 1 left). While it is true that the simple use of a chunk linearization strategy can rival the speed of SSMs, the drop in performance is significant. We, therefore, abandon this strategy and the softmax function to accelerate the linear attention operations of the kernel-based approach. In addition, long convolution (Fu et al., 2023b) is more efficient and easier to implement than the traditional SSMs. We believe combining these two modules could release the true power of the hybrid model with faster speed and better performance. However, the existence of such a dilemma is worth considering: (1) the linear attention in hybrid models is a speed bottleneck for long convolution, the comparison is shown in Fig. 1 right; (2) the instability of long convolution makes attention hard to optimize. Specifically, due to the intense memory access (I/O) on the GPU and cumulative summation (cumsum) operation in a casual setting, the notable reduction in complexity from to in linear attention is only theoretical (Qin et al., 2024); On the other hand, the long convolution needs hand-crafted regularizations to prevent over-fitting high-frequency noise. To escape the dilemma, we have developed a novel model called CHELA, which stands for short-long Convolutional with Hardware-Efficient Linear Attention. This model is designed to effectively and efficiently capture complex dependencies in long inputs. Inspired by FlashAttention (Dao et al., 2022; Dao, 2023), the model comprises multiple layers of real linear token mixers with hardware-friendly implementation with a gating mechanism that achieves comparable performance to full attention. We have added a stabilizer, reparameterizable (Ding et al., 2022, 2023) short-long convolutions, to the bottom layer of the long convolution to integrate multiple frequencies with inputs. As a result, the top layers of CHELA capture more refined data-dependent information using linear attention, while the convolutions add a structural bias that enhances global abstraction in multi-level frequencies. We demonstrate the efficiency and effectiveness of CHELA on various datasets and tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (LRA) benchmark (Tay et al., 2020b), which is designed to test models\u2019 ability in modeling long sequences. Second, we show that in autoregressive language modeling, CHELA is not only significantly faster than the vanilla Transformer but also yields better performance. In all the settings, CHELA outperforms the baselines. Finally, we provide further analysis and ablation experiments to demonstrate the effectiveness. Our contribution can be summarized in three folds:\n\n\u2022\n\nEfficient Implementation: We release the potential speed of linear attention in the attention-ssm hybrid model, maintaining a constant rate regardless of sequence length with fixed memory. \u2022\n\nNew Design: We improve multi-frequency learning with multi-level convolutional hierarchies, leading to better performance and stability for long convolution. \u2022\n\nPromising Results: By connecting the above two basic modules, we propose CHELA for efficient long sequence learners are capable of both global information abstraction and fine-grained data-dependent selection. 2 Background\n\n2.1 Self Attention\n\nIf we have an input that belongs to , where represents sequence length and represents the embedding dimension, the attention mechanism produces pair-wise scores denoted as :\n\n\ud835\udc0c = Attn \u200b ( \ud835\udc10 , \ud835\udc0a , \ud835\udc15 ) = \u03c3 \u200b ( \ud835\udc10\ud835\udc0a \u22a4 d ) \u200b \ud835\udc15 , \ud835\udc0c Attn \ud835\udc10 \ud835\udc0a \ud835\udc15 \ud835\udf0e superscript \ud835\udc10\ud835\udc0a top \ud835\udc51 \ud835\udc15 \\displaystyle\\mathbf{M}=\\mathrm{Attn}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\sigma\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d}}\\right)\\mathbf{V}, (1) where \u200b \ud835\udc10 = \ud835\udc17\ud835\udc16 q , \ud835\udc0a = \ud835\udc17\ud835\udc16 k , \ud835\udc15 = \ud835\udc17\ud835\udc16 v . formulae-sequence where \ud835\udc10 subscript \ud835\udc17\ud835\udc16 \ud835\udc5e formulae-sequence \ud835\udc0a subscript \ud835\udc17\ud835\udc16 \ud835\udc58 \ud835\udc15 subscript \ud835\udc17\ud835\udc16 \ud835\udc63 \\displaystyle\\text{where }\\mathbf{Q}=\\mathbf{X}\\mathbf{W}_{q},\\ \\mathbf{K}=\\mathbf{X}\\mathbf{W}_{k},\\ \\mathbf{V}=\\mathbf{X}\\mathbf{W}_{v}. Here are learnable weights, and is the attention function. Denote the attention matrix . captures the relations between the -th and the -th input tokens. Linear attention\n\nTaking Norm as (Qin et al., 2024), to take advantage of the computational efficiency inherent in right matrix multiplication, the attention score can be transformed into its linear variant, which is mathematically equivalent due to the properties of matrix multiplication. \ud835\udc0c linear = Norm \u200b ( \ud835\udc10 \u200b ( \ud835\udc0a \u22a4 \u200b \ud835\udc15 ) ) subscript \ud835\udc0c linear Norm \ud835\udc10 superscript \ud835\udc0a top \ud835\udc15 \\mathbf{M}_{\\texttt{linear}}=\\texttt{Norm}(\\mathbf{Q}(\\mathbf{K}^{\\top}\\mathbf{V})) (2)\n\n2.2 Linear Recurrent Models\n\nContinuous formulation. A model with a continuous time latent space transforms a one-dimensional input signal into a latent state that has dimensions, and then this is transformed into a one-dimensional output signal . Clearly, we can define the process:\n\nx \u2032 \u200b ( t ) = \ud835\udc00 \u200b x \u200b ( t ) + \ud835\udc01 \u200b u \u200b ( t ) , y \u200b ( t ) = \ud835\udc02 \u200b x \u200b ( t ) . formulae-sequence superscript \ud835\udc65 \u2032 \ud835\udc61 \ud835\udc00 \ud835\udc65 \ud835\udc61 \ud835\udc01 \ud835\udc62 \ud835\udc61 \ud835\udc66 \ud835\udc61 \ud835\udc02 \ud835\udc65 \ud835\udc61 \\displaystyle x^{\\prime}(t)=\\mathbf{A}x(t)+\\mathbf{B}u(t),\\quad y(t)=\\mathbf{C}x(t). (3)\n\nwhere , and . Eq. 3 is utilized in prior research to model long sequences. For instance, Gu et al. (2020a) suggests a set of matrices called HiPPO (high-order polynomial projection operators) to initialize . The HiPPO matrices are constructed to enable the state at time to remember the history of the input up to the present.",
    "slconv-3": "Discrete formulation. Discrete sequences, like natural language inputs with representing sequence length, are frequently encountered. The model described in Eq. 3 can be discretized (bilinear method, for example) with a step size of to model discrete data. x k = \ud835\udc00 \u00af \u200b x k \u2212 1 + \ud835\udc01 \u00af \u200b u k , y k = \ud835\udc02 \u00af \u200b x k , formulae-sequence subscript \ud835\udc65 \ud835\udc58 \u00af \ud835\udc00 subscript \ud835\udc65 \ud835\udc58 1 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 subscript \ud835\udc66 \ud835\udc58 \u00af \ud835\udc02 subscript \ud835\udc65 \ud835\udc58 \\displaystyle x_{k}=\\overline{\\mathbf{A}}x_{k-1}+\\overline{\\mathbf{B}}u_{k},\\quad y_{k}=\\overline{\\mathbf{C}}x_{k}, (4) where \u200b \ud835\udc00 \u00af = ( \ud835\udc08 \u2212 \u0394 / 2 \u22c5 \ud835\udc00 ) \u2212 1 \u200b ( \ud835\udc08 + \u0394 / 2 \u22c5 \ud835\udc00 ) , where \u00af \ud835\udc00 superscript \ud835\udc08 \u22c5 \u0394 2 \ud835\udc00 1 \ud835\udc08 \u22c5 \u0394 2 \ud835\udc00 \\displaystyle\\text{where }\\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta/2\\cdot\\mathbf{A})^{-1}(\\mathbf{I}+\\Delta/2\\cdot\\mathbf{A}), \ud835\udc01 \u00af = ( \ud835\udc08 \u2212 \u0394 / 2 \u22c5 \ud835\udc00 ) \u2212 1 \u200b \u0394 \u200b \ud835\udc01 , \ud835\udc02 \u00af = \ud835\udc02 . formulae-sequence \u00af \ud835\udc01 superscript \ud835\udc08 \u22c5 \u0394 2 \ud835\udc00 1 \u0394 \ud835\udc01 \u00af \ud835\udc02 \ud835\udc02 \\displaystyle\\quad\\quad\\ \\ \\ \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta/2\\cdot\\mathbf{A})^{-1}\\Delta\\mathbf{B},\\quad\\overline{\\mathbf{C}}=\\mathbf{C}. We can expand the recurrent representation above to obtain:\n\ny k = \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af k \u200b \ud835\udc01 \u00af \u200b u 0 + \u22ef + \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u k \u2212 1 + \ud835\udc02 \u00af \u200b \ud835\udc01 \u00af \u200b u k . subscript \ud835\udc66 \ud835\udc58 \u00af \ud835\udc02 superscript \u00af \ud835\udc00 \ud835\udc58 \u00af \ud835\udc01 subscript \ud835\udc62 0 \u22ef \u00af \ud835\udc02 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 1 \u00af \ud835\udc02 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 \\displaystyle y_{k}=\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}^{k}\\overline{\\mathbf{B}}u_{0}+\\cdots+\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{k-1}+\\overline{\\mathbf{C}}\\overline{\\mathbf{B}}u_{k}. Convolution formulation. This can be written as a convolutional representation , where the convolution kernel\n\n\ud835\udc0a \u00af \u2208 \u211d L = ( \ud835\udc02 \u00af \u200b \ud835\udc01 \u00af , \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af , \u22ef , \ud835\udc02 \u00af \u200b \ud835\udc00 \u00af L \u2212 1 \u200b \ud835\udc01 \u00af ) . \u00af \ud835\udc0a superscript \u211d \ud835\udc3f \u00af \ud835\udc02 \u00af \ud835\udc01 \u00af \ud835\udc02 \u00af \ud835\udc00 \u00af \ud835\udc01 \u22ef \u00af \ud835\udc02 superscript \u00af \ud835\udc00 \ud835\udc3f 1 \u00af \ud835\udc01 \\displaystyle\\overline{\\mathbf{K}}\\in\\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C}}\\overline{\\mathbf{B}},\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}\\overline{\\mathbf{B}},\\cdots,\\overline{\\mathbf{C}}\\overline{\\mathbf{A}}^{L-1}\\overline{\\mathbf{B}}\\right). (5)\n\nEfficient calculation of the output in Eq. 5 can be achieved when the convolution kernel is known. However, determining the kernel is a difficult undertaking, and the majority of existing algorithms necessitate a time and space complexity of . S4 model. Gu et al. (2021) devised the S4 model to effectively calculate Eq.",
    "slconv-4": "5. In particular, the initialization of in Eq. 3 is random, and both and are initialized as\n\n\ud835\udc00 = \ud835\udc00 ( d s ) \u2212 \ud835\udc0f\ud835\udc0f \u22a4 , \ud835\udc01 i = ( 2 \u200b i + 1 ) 1 2 , formulae-sequence \ud835\udc00 superscript \ud835\udc00 subscript \ud835\udc51 \ud835\udc60 superscript \ud835\udc0f\ud835\udc0f top subscript \ud835\udc01 \ud835\udc56 superscript 2 \ud835\udc56 1 1 2 \\displaystyle\\mathbf{A}=\\mathbf{A}^{(d_{s})}-\\mathbf{P}\\mathbf{P}^{\\top},\\quad\\mathbf{B}_{i}=(2i+1)^{\\frac{1}{2}}, (6) where \u200b \ud835\udc0f i = ( i + 1 / 2 ) 1 / 2 , where subscript \ud835\udc0f \ud835\udc56 superscript \ud835\udc56 1 2 1 2 \\displaystyle\\text{where }\\mathbf{P}_{i}=\\left(i+1/2\\right)^{1/2}, \ud835\udc00 i \u200b j ( d s ) = \u2212 { ( i + 1 2 ) 1 2 \u200b ( j + 1 2 ) 1 2 , i > j , 1 2 , i = j , \u2212 ( i + 1 2 ) 1 2 \u200b ( j + 1 2 ) 1 2 , i < j . subscript superscript \ud835\udc00 subscript \ud835\udc51 \ud835\udc60 \ud835\udc56 \ud835\udc57 cases superscript \ud835\udc56 1 2 1 2 superscript \ud835\udc57 1 2 1 2 \ud835\udc56 \ud835\udc57 1 2 \ud835\udc56 \ud835\udc57 superscript \ud835\udc56 1 2 1 2 superscript \ud835\udc57 1 2 1 2 \ud835\udc56 \ud835\udc57 \\displaystyle\\mathbf{A}^{(d_{s})}_{ij}=-\\begin{cases}(i+\\frac{1}{2})^{\\frac{1}{2}}(j+\\frac{1}{2})^{\\frac{1}{2}},&i>j,\\\\\n\\frac{1}{2},&i=j,\\\\\n-(i+\\frac{1}{2})^{\\frac{1}{2}}(j+\\frac{1}{2})^{\\frac{1}{2}},&i<j.\\\\\n\\end{cases}\n\nThe convolution kernel in Eq.",
    "slconv-5": "5 can be efficiently computed using time and space complexity. After that, the efficient computation of for an input can be performed. Long convolution. If we parameterize the kernel with the same length as the sequence in Eq. 5 (Fu et al., 2023b), then we can replace the SSM layer with a learned convolution kernel as a drop-in replacement. The computation can be efficiently done by using the FFT theorem:\n\ny = u \u2217 \ud835\udc0a = \ud835\udc05 L \u2212 1 \u200b \ud835\udc03 \ud835\udc0a \u200b \ud835\udc05 L \u200b u \ud835\udc66 \ud835\udc62 \ud835\udc0a superscript subscript \ud835\udc05 \ud835\udc3f 1 subscript \ud835\udc03 \ud835\udc0a subscript \ud835\udc05 \ud835\udc3f \ud835\udc62 y=u*\\mathbf{K}=\\mathbf{F}_{L}^{-1}\\mathbf{D}_{\\mathbf{K}}\\mathbf{F}_{L}u (7)\n\nwhere denotes the DFT matrix of size , and . This so-called FFT convolution scales in linear complexity when dealing with sequences with length . 3 Why Combining Long Conv and Attention? As mentioned in Section 1, both long convolution and linear attention mechanisms have their limitations in spite of their widespread use and impressive accomplishments in sequence modeling. By bridging their benefits together, we push the limitation of the hybrid model, which allows us to benefit from the strong inductive bias of structured memory while still having the ability to learn complex data-dependent patterns in true linear time with respect to length. The following two issues are the two main obstacles to extreme performance in long-sequence modeling. Linear Attention is the Efficiency Bottleneck. We can see from Fig. 1 right that as the input sequence grows, the actual efficiency of linear attention is much lower than that of S4, despite the fact that the complexity of the S4 model is as for long convolution and for linear attention. The two obstacles to achieving a practical wall-clock speedup are intensive GPU memory access and cumulative summation (cumsum) in a casual setting. Inspired by Flashattention (Dao et al., 2022; Dao, 2023), we effectively doubled the speed of linear attention. Long Convolution Needs Short Convolutions. To match the efficiency of hardware-efficient linear attention, the long convolution is considered as a simpler and faster SSM model for long sequence abstraction. However, it requires additional constraint regularizations for stable performance on different data types. This is because it is hard for a single long convolutional kernel to learn multiple frequencies simultaneously, i.e., token patterns that repeat a lot versus those that occur more sparsely. We thereby introduce multi-resolution short convolutional kernels to share the learning pressure of long convolution. They can also be reparameterized by simple linear transformation into a single kernel to speed up the inference phase. 4 CHELA: Short-Long Convolutions with Hardware-Efficient Linear Attention\n\nIn this section, motivated by the above issues, we propose CHELA, short-long convolutions with hardware-efficient linear attention, which fully enjoys the benefits of each. We first describe how the tiling method is used in linear attention to reach the theoretical performance. We then introduce a simple and effective module named short-long convolutions to enhance the ability of global abstraction in this hybrid design. The blocks of CHELA are demonstrated in detailed architecture, including feed-forward and normalization layers. Moreover, we also discuss the relationship between CHELA and two closely related hybrid models: MEGA (Ma et al., 2022), SPADE (Zuo et al., 2023). 4.1 Hardware-Efficient Linear Attention\n\nIn response to the first question, we follow a tiling approach from GLA (Yang et al., 2024), which employs the classic divide-and-conquer strategy to take advantage of the significant difference in memory bandwidth between HBM and SRAM within GPU. Given the inputs , we aim to compute the linear attention in SRAM and finally output . The right-hand multiplication of linear attention is used for the loops between and in the data blocks; noticeably, the left multiplication is used inside the block when an attention mask exists. The intermediate variable is iteratively saved and accumulated within SRAM. Subsequently, the outputs of outer and inner loops are summed within SRAM, and the results are returned to HBM. This method aims to capitalize on the distinct advantages of each memory component, optimizing the computational workflow of linear attention to reach its theoretical speed as much as possible.",
    "slconv-6": "The graphic demonstration is visualized in Fig. 2 (b). 4.2 Short-Long Convolutions\n\nCHELA introduces a modification of the long convolution mentioned in Sec. 2.2 by adding short convolutions before the long kernel, named short-long convolutions, to learn multi-frequencies information and improve its stability and capacity. Structural Reparameterization (SR) can fuse these short kernels into a single kernel in the inference phase. Short convolution improves long convolution. The long convolutions have shown success when dealing with long sequences and are much more efficient than conventional SSMs. Empirical results tell us how the weights are initialized and how the regular terms are designed, all of which greatly impact the final performance on specific data types. One important finding is that long convolutions trying to learn both low and high frequencies in the input signal are the central cause of their instability because their kernel mixes various high and low-frequency patterns compared to S4 (Fu et al., 2023b). Inspired by this, we try adding parallel short kernels in long convolution. As shown in Fig 3, we simply put the short kernels and a SiLU activation function (Ramachandran et al., 2017) before the long convolution. Formally, the computation can be written as:\n\n\ud835\udc19 = \ud835\udc0a \u00af l \u200b ( \u03d5 silu \u200b ( \ud835\udc0a \u00af s \u200b ( \ud835\udc17 ) ) ) \ud835\udc19 subscript \u00af \ud835\udc0a \ud835\udc59 subscript italic-\u03d5 silu subscript \u00af \ud835\udc0a \ud835\udc60 \ud835\udc17 \\mathbf{Z}=\\overline{\\mathbf{K}}_{l}(\\phi_{\\mathrm{silu}}(\\overline{\\mathbf{K}}_{s}(\\mathbf{X}))) (8)\n\nwhere and denote for long and short convolutions. The Fig. 3 right also demonstrates the performance of this design on the ListOps dataset: better stabilization and accuracy on the task of long-sequence logic inference. In our experiments, we add two different sizes of short kernels. One is fixed the size at three, and the other varies with the sequence length, whose size equals . Structural reparamterization. Structural Reparameterization (Ding et al., 2022, 2023) is a methodology of equivalently converting model structures via linear parameter transformation. For example, RepVGG targeted a deep inference-time VGG-like (e.g., branch-free) model and constructed extra ResNet-style shortcuts parallel. After training, these residual kernels are absorbed into a single kernel of size 3. In this paper, we use this methodology to fuse short kernels after training. Consequently, we make the long convolution kernel capable of capturing flexible patterns, improving overall performance. 4.3 CHELA Model\n\nThe gating mechanism in CHELA is based on GRU (Cho et al., 2014) and GAU (Hua et al., 2022) as the backbone architectures, with the output of short-long convolutions layer embedded into the linear attention matrix. can be considered a contextual abstraction because it provides global information through multi-level structured patterns:\n\n\ud835\udc0c linear subscript \ud835\udc0c linear \\displaystyle\\mathbf{M}_{\\mathrm{linear}} = Norm \u200b ( \ud835\udc10 \u200b ( \ud835\udc0a \u22a4 \u200b \ud835\udc15 ) ) \u2299 \ud835\udc06 a absent direct-product Norm \ud835\udc10 superscript \ud835\udc0a top \ud835\udc15 subscript \ud835\udc06 \ud835\udc4e \\displaystyle=\\mathrm{Norm}(\\mathbf{Q}(\\mathbf{K}^{\\top}\\mathbf{V}))\\odot\\mathbf{G}_{a} (9) \ud835\udc06 a subscript \ud835\udc06 \ud835\udc4e \\displaystyle\\mathbf{G}_{a} = \u03d5 silu \u200b ( \ud835\udc19\ud835\udc16 v + b g ) absent subscript italic-\u03d5 silu subscript \ud835\udc19\ud835\udc16 \ud835\udc63 subscript \ud835\udc4f \ud835\udc54 \\displaystyle=\\phi_{\\mathrm{silu}}(\\mathbf{Z}\\mathbf{W}_{v}+b_{g}) (10) \ud835\udc10 \ud835\udc10 \\displaystyle\\mathbf{Q} = \u03b1 q \u2299 \ud835\udc19 + \u03b2 q absent direct-product subscript \ud835\udefc \ud835\udc5e \ud835\udc19 subscript \ud835\udefd \ud835\udc5e \\displaystyle=\\alpha_{q}\\odot\\mathbf{Z}+\\beta_{q} (11) \ud835\udc0a \ud835\udc0a \\displaystyle\\mathbf{K} = \u03b1 k \u2299 \ud835\udc19 + \u03b2 k absent direct-product subscript \ud835\udefc \ud835\udc58 \ud835\udc19 subscript \ud835\udefd \ud835\udc58 \\displaystyle=\\alpha_{k}\\odot\\mathbf{Z}+\\beta_{k} (12) \ud835\udc15 \ud835\udc15 \\displaystyle\\mathbf{V} = \u03d5 silu \u200b ( \ud835\udc17\ud835\udc16 v + b v ) absent subscript italic-\u03d5 silu subscript \ud835\udc17\ud835\udc16 \ud835\udc63 subscript \ud835\udc4f \ud835\udc63 \\displaystyle=\\phi_{\\mathrm{silu}}(\\mathbf{X}\\mathbf{W}_{v}+b_{v}) (13)\n\nwhere are the learnable scalers and offsets of queries and keys like GAU, respectively. The is the output of flash linear attention with gate . Moreover, we employ an output gate to update final activation output in a residual style:\n\n\ud835\udc06 o subscript \ud835\udc06 \ud835\udc5c \\displaystyle\\mathbf{G}_{o} = \u03d5 sigmoid \u200b ( \ud835\udc19\ud835\udc16 o + b o ) absent subscript italic-\u03d5 sigmoid subscript \ud835\udc19\ud835\udc16 \ud835\udc5c subscript \ud835\udc4f \ud835\udc5c \\displaystyle=\\phi_{\\mathrm{sigmoid}}(\\mathbf{Z}\\mathbf{W}_{o}+b_{o}) (14) \ud835\udc14 \ud835\udc14 \\displaystyle\\mathbf{U} = \ud835\udc0c linear \u2299 \ud835\udc06 o + \ud835\udc17 \u2299 ( 1 \u2212 \ud835\udc06 o ) absent direct-product subscript \ud835\udc0c linear subscript \ud835\udc06 \ud835\udc5c direct-product \ud835\udc17 1 subscript \ud835\udc06 \ud835\udc5c \\displaystyle=\\mathbf{M}_{\\mathrm{linear}}\\odot\\mathbf{G}_{o}+\\mathbf{X}\\odot(1-\\mathbf{G}_{o}) (15)\n\nthe architecture of CHELA is demonstrated in Fig. 2 (a). CHELA Block\n\nBesides, each standard CHELA block is also equipped with a two-layer feedforward neural network, layer normalization (Ba et al., 2016), and residual connections (He et al., 2016) as channel mixing. Concretely, given the input to the CHELA block, we have the output as:\n\n\ud835\udc17 a = CHELA \u200b ( LayerNorm \u200b ( \ud835\udc17 ) ) subscript \ud835\udc17 \ud835\udc4e CHELA LayerNorm \ud835\udc17 \\displaystyle\\mathbf{X}_{a}=\\mathrm{CHELA}\\left(\\mathrm{LayerNorm}(\\mathbf{X})\\right) \ud835\udc18 = FFN \u200b ( LayerNorm \u200b ( \ud835\udc17 a ) ) + \ud835\udc17 a . \ud835\udc18 FFN LayerNorm subscript \ud835\udc17 \ud835\udc4e subscript \ud835\udc17 \ud835\udc4e \\displaystyle\\mathbf{Y}=\\mathrm{FFN}\\left(\\mathrm{LayerNorm}(\\mathbf{X}_{a})\\right)+\\mathbf{X}_{a}. Note that here we apply pre-layer normalization, similar to Vaswani et al. (2017). There are other works (e.g., Devlin et al. 2019) that apply post-layer normalization, where the normalization is applied after the residual connection. 4.4 Relation to MEGA and SPADE. As hybrid models of SSM and attentions, we adhered to the idea of proposing an efficient variant to utilize both well-structured and fully data-dependent patterns on long-sequence modeling. However, it is undeniable that the linear versions of Mage-chunk and SPADE-chunk have caused significant performance degradation, which means they struggle to make good tradeoffs between complexity and performance. The EMA Mega uses is too simple to capture higher-order information, so the overall performance is heavily dependent on attention. SPADE uses a parallel structure that directly combines the SSM and Attention modules, which reduces the dependence on attention, but the linear strategy of dividing the chunks still limits the model\u2019s performance. CHELA maximizes efficiency by enhancing structured representations with short-length convolutions and lossless use of linear attention in a hardware-optimized implementation. Ultimately, CHELA boosting performance while also dramatically increasing speed in linear time. 5 Experiments\n\nTo assess CHELA, we performed tests on five standard sequence modeling tasks involving different types of data, and we compared them with the latest cutting-edge models for each task. All experiments were realized based on NVIDIA A100-80G and Pytorch. We used float32 parameters, with bfloat16 precision for most computations. 5.1 Long-Context Sequence Modeling\n\nWe conducted experiments to evaluate sequence models using the Long Range Arena (LRA) benchmark. This benchmark, introduced by (Tay et al., 2020b), is designed to assess the performance of sequence models in long-context learning. The benchmark includes six tasks, namely ListOps (Nangia & Bowman, 2018), byte-level Text classification (Maas et al., 2011), byte-level document Retrieval (Radev et al., 2013), Image classification on sequences of pixels (Krizhevsky et al., 2009), Pathfinder (Linsley et al., 2018), and its extreme long version (Path-X; Tay et al.",
    "slconv-7": "(2020b)). The input sequences for these tasks range from 1K to 16K tokens and cover various data modalities. In Table 1, CHELA is compared to various baselines, such as Transformer and its efficient versions, as well as the top-performing S4 models. In order to make a fair comparison, we make sure that Mega and S4 have a similar number of parameters by balancing the number of layers and model sizes for each task. The results are based on the average of five runs with different random seeds, and you can find the tuning information and model details in the Appendix. The performance of our model has been outstanding across all six tasks, achieving an average accuracy of 88.26% and surpassing all the other comparison methods. Additionally, we assessed the speed of our model when applied to the byte-level classification task with a 4K input. Our hardware-efficient linear mechanism has demonstrated remarkable efficiency, with a speed that is 5.8 times faster. It is important to highlight that our model, with its unique short-long convolutions hybrid design, exhibits even greater efficiency compared to a variety of linear Transformers, Structured State Space Models, and recent hybrid models. 5.2 Raw Speech Classification\n\nWe intend to evaluate the capability of CHELA in modeling lengthy speech signals by employing it for the classification of unaltered speech signals with a duration of 16000, instead of depending on traditional preprocessing techniques like converting them into MFCC features. As per Gu et al. (2021) approach, we classify speech on the Speech Commands dataset\u2019s SC10 subset, which was introduced by Warden (2018). As reported in (Ma et al., 2022), the Mega-chunk uses a chunk size of 1000 to enable processing the data. In Table 2, our model has 493K parameters and achieves a 97.98% accuracy, making it the leading method in this table. This result is primarily due to the suitability of long convolutions for processing the numerous continuous and low-frequency signals present in speech. Additionally, the ability of short convolutions to capture rich global information enables attention to focus on important aspects. 5.3 Auto-Regressive Language Modeling\n\nBy following Ma et al. (2022); Lingle (2023), we assess CHELA on two popular language modeling datasets, i.e., WikiText-103 (Merity et al., 2016) and enwik8 (Mahoney, 2011), which are next-token prediction tasks. WikiText-103 is a dataset for word-level language modeling with 103 million tokens from Wikipedia articles in its training set. In line with previous work (Baevski & Auli, 2019), our method involves using adaptive softmax and input embeddings, and we utilize a vocabulary of 260,000 tokens. Enwik8 is a commonly used benchmark for character-level language modeling, presenting a significant challenge to models. It comprises approximately 100 million unprocessed tokens from Wikipedia articles and has a vocabulary size of about 200. When evaluating language models, we segment the test data and process each segment sequentially during testing to assess their effectiveness. In Table 4, we compare with previous top-performing models that are designed to take advantage of longer context, including Transformers (Baevski & Auli, 2019), Transformer-XL and S4 (Gu et al., 2021). The model we developed demonstrated outstanding performance on both WikiText-103 and enwik8 datasets, outperforming the baseline models by a significant margin. Our model achieves an inference speed that is almost 10 times faster than the Pure Transformer model. The hybrid structure of the short-long convolutions layer plays a crucial role in enabling our model to manage length extrapolation during inference, allowing it to process longer sequences than those encountered during training. This distinctive characteristic of our model enhances its capability to naturally handle complex tasks, making it a valuable addition to any long-sequence project. 5.4 Pixel-Level Sequential Image Classification\n\nBegin by addressing tasks related to image classification, in which images are considered as a one-dimensional sequence of pixels. In these tasks, models cannot rely on preconceived two-dimensional structures within the image. Consequently, the model must possess the ability to recognize patterns at different temporal scales, including pixels that are close together in the original image but far apart in their sequential representation. We evaluate the performance of our model using the Sequential CIFAR-10 dataset, commonly used as a benchmark for capturing long-term dependencies in RNNs. The CIFAR-10 dataset is frequently employed in machine learning for tasks on image classification. Within this dataset, the typical training and testing split is maintained, reserving 10% of the training set for validation purposes. To categorize the images, the mean of all tokens in the output sequences is computed, and the resulting values are subjected to a fully connected layer to produce class logits. The Table 3 displays the results. CHELA has achieved state-of-the-art performance and the best test accuracy on the sequence classification task, surpassing multiple strong competitors such as Transformers (Vaswani et al., 2017), RNNs, state space models, and other convolutional models. In particular, the CHELA model has exceeded the performance of previous convolution-based models by more than ten percentage points. It is important to note that our model has delivered impressive results by surpassing the previously established performance standard, even though it uses a relatively simple architecture. The model primarily employs a hybrid method that compresses long historical information based on the output of short-long convolutions. Our most effective model consists of ten CHELA blocks, which significantly contribute to achieving exceptional performance. 6 Ablation Study\n\nOur ablation experiments focus on answering two key questions mostly related to our design: (1) Does the hardware-friendly implementation significantly improve the speed of linear attention? (2) The effectiveness of our proposed short-long convolutions module on long sequences. Q1: Benchmark hardware-efficient linear attention. To answer the first question, our Hardware-Efficient Linear Attention achieves almost real linear relationships with sequence lengths. We conducted an analysis on the WikiText-103 dataset with models with 200M parameters. As visualized in Fig. 4, we have more than doubled the speedup of the original Pytorch implementation of the linear attention. Q2: Analysis of short-long convolutions. To answer the second question, we further combined a variety of hybrid models following the modeling structure of CHELA. Specifically, we compared the representative SSM-like modules on the subset of the LRA (Tay et al., 2020b) dataset (Text, Image, and PathX). It is clear that the proposed Short-Long Convolutions are the best partner for linear attention. 7 Related Works\n\nEfficient transformer models\n\nA variety of efforts have been made to decrease the quadratic time and space complexity of standard attention mechanisms. One method is to utilize \u201csparse attention,\u201d where each token only attends to a subset of all the tokens based on predefined patterns, such as neighboring tokens within a fixed-size window. (Child et al., 2019) started the attempt to sparse the attention, and then there were a lot more followers, such as ETC (Ainslie et al., 2020), Longformer (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), Poolingformer (Zhang et al., 2021), and HEPOS (Huang et al., 2021) are some examples of this approach. Another option is to utilize \u201clow-rank projection,\u201d as mentioned in the work by (Wang et al., 2020). Similar techniques include Nystr\u00f6mformer (Xiong et al., 2021), Synthesizer (Tay et al., 2021), and Luna (Ma et al., 2021). However, these methods encounter challenges when dealing with causal tasks, such as auto-regressive language modeling. Another approach uses \u201cclustering method,\u201d where we partition or into multiple clusters and perform inter-cluster attention. Examples of such methods include Sinkhorn Transformer (Tay et al., 2020a), Reformer (Kitaev et al., 2020), Routing Transformer (Roy et al., 2021), and simplified FLASH (Hua et al., 2022), etc. \u201cMethods based on kernels\u201d can be utilized to approximate the complete attention . These methods replace the quadratic-time softmax attention with fast linear-time kernel approximations (such as Gaussian and arc-cosine kernels). Some instances of this approach include Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020), and FMMformer (Nguyen et al., 2021), etc. Both low-dimensional projection and methods based on kernels are employed to estimate full attention and, as a result, are vulnerable to significant approximation errors. State space models and long convolutions\n\nRecurrent neural networks and their linear counterparts such as state-space models are capable of retaining memory of the past. Among these models, S4 (Gu et al., 2021) is notable because it can be implemented through convolutions thanks to its linear recurrence. However, the long convolution kernel for this model is as long as the input sequence, and its efficient computation requires sophisticated parameterization and approximation techniques. Although recent advances have found solutions to this issue, initializing these models still requires special effort (Gupta et al., 2022; Gu et al., 2020b). Many of these models use the HiPPO (Gu et al., 2020a) initialization mechanism, which aims to memorize historical data through projection to orthogonal polynomials. Based on a structure similar to SSM, an increasing number of models focusing on either linear recurrence or global convolution have been developed recently (Fu et al., 2023b, a; Poli et al., 2023; Gu & Dao, 2023). Hardware-efficient implementation\n\nThe FlashAttention series (Dao et al., 2022; Dao, 2023) is dedicated to optimizing the standard attention operator for GPU platforms at the system level.",
    "slconv-8": "Its effectiveness has been extensively validated. The approach involves using tiling strategies to minimize the amount of memory reads/writes between the high bandwidth memory (HBM) and on-chip SRAM. 8 Conclusion and Limitations\n\nWe presented CHELA for robust and efficient modeling of long sequences. CHELA is an SSM-attention hybrid architecture that computes both structured and data-dependent patterns in linear time with respect to sequence length. Its superior performance is enabled by considering the global view of the Short-Long Convolutions and real linear Hardware-Efficient Linear Attention with gating mechanisms. Compared to other hybrid linear models, we are currently the first to achieve performance improvement while maintaining linear complexity. Our large-scale and diverse experiments demonstrate that CHELA is an efficient and flexible long sequence model with excellent performance on image, text, logical reasoning, and speech data. However, CHELA also has some limitations. The optimal combinations of short convolutions are not explored in this paper, which should be a future research direction of designing dynamic short-convolutional components according to the input data. Moreover, the time-varying SSM is a different idea to achieve this goal. By embracing CHELA as a starting point in the integration of hardware-efficient implementation into hybrid models, we are taking the initial step towards achieving greater efficiency gains in the future. Acknowledgements\n\nThis work was supported by Ministry of Science and Technology of the People\u2019s Republic of China (No. 2021YFA1301603), National Natural Science Foundation of China Project (No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University and Project (No. WU2023C019) from the Westlake University Industries of the Future Research Funding. This work was done when Li Wang and Zedong Wang interned at Westlake University. We thank the AI Station of Westlake University for the support of GPUs. Impact Statement\n\nThe goal of this paper is to advance research in long-sequence modeling by introducing an efficient model design CHELA. We have considered broader ethical impacts and do not foresee CHELA directly leading to negative societal consequences. All datasets and models used are existing public resources that do not contain private or sensitive information. Through discussing the hybrid design of linear attention and SSM models, we aim to make sequence mixers much more efficient by fully leveraging hardware and different model features. Besides, as the community proposes new methods, we encourage discussing any potential negative impacts early in the research process. Overall, we believe hardware-efficient style and the combination of structured and data-dependent patterns are the dominant trends of the future for efficient sequence modeling. References\n\nAinslie et al. (2020) Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268\u2013284, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.19. URL https://aclanthology.org/2020.emnlp-main.19. Ba et al. (2016) Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. ArXiv preprint, abs/1607.06450, 2016. URL https://arxiv.org/abs/1607.06450. Baevski & Auli (2019) Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=ByxZX20qFQ. Bai et al. (2018) Bai, S., Kolter, J. Z., and Koltun, V. Trellis networks for sequence modeling. arXiv preprint arXiv:1810.06682, 2018. Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. (6) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "slconv-9": "arxiv 2020. arXiv preprint arXiv:1904.10509. Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "slconv-10": "ArXiv preprint, abs/1904.10509, 2019. URL https://arxiv.org/abs/1904.10509. Cho et al. (2014) Cho, K., van Merrienboer, B., Bahdanau, D., and Bengio, Y. On the properties of neural machine translation: Encoder-decoder approaches, 2014.",
    "slconv-11": "Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dao (2023) Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.",
    "slconv-12": "Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. Devlin et al. (2019) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. Ding et al. (2022) Ding, X., Zhang, X., Zhou, Y., Han, J., Ding, G., and Sun, J. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns, 2022. Ding et al. (2023) Ding, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., and Shan, Y. Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition, 2023.",
    "slconv-13": "Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.",
    "slconv-14": "Erichson et al. (2020) Erichson, N. B., Azencot, O., Queiruga, A., Hodgkinson, L., and Mahoney, M. W. Lipschitz recurrent neural networks. arXiv preprint arXiv:2006.12070, 2020. Fu et al. (2023a) Fu, D. Y., Dao, T., Saab, K.",
    "slconv-15": "K., Thomas, A. W., Rudra, A., and R\u00e9, C. Hungry hungry hippos: Towards language modeling with state space models, 2023a. Fu et al. (2023b) Fu, D. Y., Epstein, E. L., Nguyen, E., Thomas, A. W., Zhang, M., Dao, T., Rudra, A., and R\u00e9, C. Simple hardware-efficient long convolutions for sequence modeling, 2023b. Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023.",
    "slconv-16": "Gu et al. (2020a) Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. Hippo: Recurrent memory with optimal polynomial projections.",
    "slconv-17": "Advances in neural information processing systems, 33:1474\u20131487, 2020a. Gu et al. (2020b) Gu, A., Gulcehre, C., Paine, T., Hoffman, M., and Pascanu, R. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800\u20133809. PMLR, 2020b. Gu et al. (2021) Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. ArXiv preprint, abs/2111.00396, 2021. URL https://arxiv.org/abs/2111.00396. Gu et al. (2022) Gu, A., Goel, K., Gupta, A., and R\u00e9, C. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971\u201335983, 2022. Gulati et al. (2020) Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., et al. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020. Gupta et al. (2022) Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982\u201322994, 2022. Hasani et al. (2022) Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space models.",
    "slconv-18": "arXiv preprint arXiv:2209.12951, 2022. He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90. Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Hua et al. (2022) Hua, W., Dai, Z., Liu, H., and Le, Q. Transformer quality in linear time. In International Conference on Machine Learning, pp. 9099\u20139117. PMLR, 2022. Huang et al. (2021) Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419\u20131436, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112. Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "slconv-19": "In International conference on machine learning, pp. 5156\u20135165. PMLR, 2020. Kitaev et al. (2020) Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images.",
    "slconv-20": "2009. Li et al. (2023) Li, S., Wang, Z., Liu, Z., Tan, C., Lin, H., Wu, D., Chen, Z., Zheng, J., and Li, S. Z. Efficient multi-order gated aggregation network, 2023. Li et al. (2022) Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. Lin et al. (2022) Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of transformers. AI Open, 2022. Lingle (2023) Lingle, L. D. Transformer-vq: Linear-time transformers via vector quantization, 2023. Linsley et al. (2018) Linsley, D., Kim, J., Veerabadran, V., Windolf, C., and Serre, T. Learning long-range spatial dependencies with horizontal gated recurrent units.",
    "slconv-21": "In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pp. 152\u2013164, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html. Liu et al. (2022) Liu, Z., Li, S., Wu, D., Liu, Z., Chen, Z., Wu, L., and Li, S. Z. Automix: Unveiling the power of mixup for stronger classifiers, 2022. Liu et al. (2023) Liu, Z., Li, S., Wang, G., Tan, C., Wu, L., and Li, S. Z. Harnessing hard mixed samples with decoupled regularizer, 2023. Ma et al. (2021) Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and Zettlemoyer, L. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441\u20132453, 2021. Ma et al. (2022) Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L. Mega: Moving average equipped gated attention. ArXiv preprint, abs/2209.10655, 2022. URL https://arxiv.org/abs/2209.10655. Maas et al. (2011) Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142\u2013150, Portland, Oregon, USA, 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1015. Mahoney (2011) Mahoney, M. Large text compression benchmark, 2011. URL http://www.mattmahoney.net/dc/text.html. Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016.",
    "slconv-22": "Nangia & Bowman (2018) Nangia, N. and Bowman, S. ListOps: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 92\u201399, New Orleans, Louisiana, USA, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-4013. URL https://aclanthology.org/N18-4013. Nguyen et al. (2021) Nguyen, T., Suliafu, V., Osher, S., Chen, L., and Wang, B. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention.",
    "slconv-23": "Advances in Neural Information Processing Systems, 34, 2021. Poli et al. (2023) Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00e9, C. Hyena hierarchy: Towards larger convolutional language models, 2023. Qin et al. (2024) Qin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y., Lv, B., Luo, X., Qiao, Y., and Zhong, Y. Transnormerllm: A faster and better large language model with improved transnormer, 2024.",
    "slconv-24": "Radev et al. (2013) Radev, D. R., Muthukrishnan, P., Qazvinian, V., and Abu-Jbara, A. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919\u2013944, 2013. Ramachandran et al. (2017) Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions, 2017. Romero et al. (2021) Romero, D. W., Bruintjes, R.-J., Tomczak, J. M., Bekkers, E. J., Hoogendoorn, M., and van Gemert, J. C. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. arXiv preprint arXiv:2110.08059, 2021. Roy et al. (2021) Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53\u201368, 2021. doi: 10.1162/tacl\u02d9a\u02d900353. URL https://aclanthology.org/2021.tacl-1.4. Shi et al. (2023) Shi, J., Wang, K. A., and Fox, E. B. Sequence modeling with multiresolution convolutional memory, 2023. Smith et al. (2023) Smith, J. T., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2023. Tay et al. (2020a) Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse sinkhorn attention. In International Conference on Machine Learning, pp. 9438\u20139447. PMLR, 2020a. Tay et al. (2020b) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020b. Tay et al. (2021) Tay, Y., Bahri, D., Metzler, D., Juan, D., Zhao, Z., and Zheng, C. Synthesizer: Rethinking self-attention for transformer models.",
    "slconv-25": "In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 10183\u201310192. PMLR, 2021. URL http://proceedings.mlr.press/v139/tay21a.html. Trinh et al. (2018) Trinh, T., Dai, A., Luong, T., and Le, Q. Learning longer-term dependencies in rnns with auxiliary losses. In International Conference on Machine Learning, pp. 4965\u20134974. PMLR, 2018. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity.",
    "slconv-26": "arXiv preprint arXiv:2006.04768, 2020. Warden (2018) Warden, P. Speech commands: A dataset for limited-vocabulary speech recognition, 2018.",
    "slconv-27": "Xiong et al. (2021) Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention.",
    "slconv-28": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14138\u201314148, 2021. Yang et al. (2024) Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2024.",
    "slconv-29": "Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020. Zhang et al. (2021) Zhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., and Chen, W. Poolingformer: Long document modeling with pooling attention.",
    "slconv-30": "In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 12437\u201312446. PMLR, 2021. URL http://proceedings.mlr.press/v139/zhang21h.html. Zuo et al. (2023) Zuo, S., Liu, X., Jiao, J., Charles, D., Manavoglu, E., Zhao, T., and Gao, J. Efficient long sequence modeling via state space augmented transformer, 2023. Appendix A Experimental Details\n\nA.1 Long Range Arena (LRA) and sCIFAR\n\nFor all tasks, we closely follow Tay et al. (2020b) for details such as data preprocessing, data split, etc. The hyper-parameters of CHELA models on these tasks are listed in Table 7. The experimental configuration of sCIFAR follows the parameter settings of the image in LRA. A.2 Language Modeling\n\nWe use the data of WikiText-103 and enwik8 and their splits provided by Ma et al. (2022). At training time, we split the training data into segments; each segment contains consecutive chunks, where the chunk size is the effective attention length. Other training hyperparameters, including optimizer, learning rate scheduler, and architecture, are presented in Table 8. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Jul 5 23:16:47 2024 by LaTeXML"
}