{
    "dijiang-0": "# DiJiang: Efficient Large Language Models through Compact Kernelization \n\nHanting Chen ${ }^{* 1}$ Zhicheng Liu*1 Xutao Wang ${ }^{1}$ Yuchuan Tian ${ }^{2}$ Yunhe Wang ${ }^{1}$<br>\\{chenhanting,yunhe.wang\\}@huawei.com;\n\n\n#### Abstract\n\nIn an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about $1 / 50$ training cost. Code is available at https: //github.com/YuchuanTian/DiJiang. ## 1. Introduction\n\nThe Transformer architecture (Vaswani et al., 2017) has revolutionized the field of Natural Language Processing (NLP), achieving outstanding results in various tasks such as speech recognition (Dong et al., 2018), machine translation (Wang et al., 2019), and document generation/summarization (Kim et al., 2022). This success has led to an era dominated by large language models (LLMs), where the Transformer\n\n[^0]Preprint. structure is scaled up to handle increasingly complex tasks. However, this scaling brings with it substantial computational demands, especially due to the attention mechanism which requires cross-correlation calculations between each token. These computational requirements, coupled with the significant inference costs and energy consumption, present considerable obstacles to deploying these models in resource-constrained environments like mobile devices and robotics. In response to the pressing need for more efficient Transformer models, the research community has directed its efforts towards optimizing the Transformer architecture. A myriad of strategies has been put forward, encompassing methods such as model pruning, quantization, and the development of more efficient attention mechanisms. Among these initiatives, simplifying the attention mechanism has emerged as a particularly promising avenue. This approach focuses on transforming the traditionally quadratic complexity of attention mechanisms into a more manageable linear scale. (Katharopoulos et al., 2020) introduces Linear Transformers, which leverage kernel feature maps to transform self-attention, reducing complexity from quadratic to linear while maintaining comparable results to traditional Transformers. (Kitaev et al., 2020) proposes replacies dotproduct attention with locality-sensitive hashing and using reversible residual layers to minimize memory usage in training. Performer (Choromanski et al., 2020) utilize positive orthogonal random features to approximate softmax-based self-attention in Transformers, achieving a transformative leap to linear complexity. However, the majority of existing methods for optimizing Transformers, particularly in relation to their attention mechanisms, necessitate comprehensive retraining. This retraining process presents a formidable challenge, especially for models with an immense array of parameters. It requires a significant investment in terms of computational resources and time. For instance, the training of a large model like LLaMA-7B (Touvron et al., 2023) demands approximately 82,432 GPU-hours and incurs a total power consumption of around 36 MWh . Undertaking such extensive retraining for models of this magnitude is not only economically taxing but also raises environmental concerns due to the substantial\nenergy expenditure involved. This underscores the need for more efficient approaches to adapt and optimize these largescale models. Undertaking such extensive retraining for models of this magnitude is not only economically taxing but also raises environmental concerns due to the substantial energy expenditure involved. Despite few research (Zheng et al., 2023; Choromanski et al., 2020) efforts focusing on finding fast approximations for attention mechanisms, these methods have not been thoroughly validated in large-scale language models. To address the issue of fast attention approximations in large language models, we conducted a thorough analysis of existing linear attention schemes. We discovered that the main source of approximation error in these methods is due to sampling based on the Monte Carlo method. Consequently, we propose the use of weighted Quasi-Monte Carlo sampling for mapping, specifically introducing Frequency Domain Kernelization. This approach efficiently and accurately maps the queries and keys of a Transformer to the frequency domain using Discrete Cosine Transform (DCT). This mapping allows us to effectively eliminate the softmax operation in the attention mechanism, rendering the attention computation linear in complexity, which is shown in Figure 1. We theoretically demonstrate that this frequency domain mapping is an approximate equivalent to the original attention mechanism. Our experiments show that our method achieves performance comparable to the original Transformer with a significantly smaller training cost $(<1 / 10)$, while also benefiting from faster inference speeds (up to about 10x). ## 2. Related Works\n\n### 2.1. Linear Transformers\n\nReducing the computational load of attention in Transformers remains a hot topic in research. (Child et al., 2019) achieved this by sparsifying attention, thereby reducing its computational cost. Similarly, (Kitaev et al., 2020) used locality-sensitive hashing to expedite the computation of attention. However, these methods are hard to apply in auto-regressive Transformer models. As a result, there has been a series of works focusing on removing or substituting the softmax in attention. Notably, the Linear Transformer, first introduced by (Katharopoulos et al., 2020), represents a significant stride in this direction. (Qin et al., 2022) approximated attention calculations using a linear operator and a cosine-based distance reweighting. (Zhai et al., 2021) achieved linear complexity in Transformers by preprocessing keys and values. (Lu et al., 2021) used Gaussian kernel functions in place of dot-product similarity, allowing for the approximation of the full self-attention matrix through low-rank matrix decomposition. (Bello, 2021) bypassed the need for attention calculations by capturing interactions through transforming available contexts into linear functions and applying them to each input, showcasing the variety of methods explored to optimize attention mechanisms in Transformer models. Additionally, recent proposals like RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), and Mamba (Gu \\& Dao, 2023) have introduced potential alternatives to the Transformer with linear complexity. However, these existing improvements typically require significant modifications to the model's architecture and often necessitate training a new model from scratch to achieve optimal performance. Given the substantial training costs associated with large language models, such retraining is not always feasible. While methods like StreamingLLM (Xiao et al., 2023) or Longformer (Beltagy et al., 2020) can be implemented through fine-tuning, their reliance on window attention compromises their ability to truly model long sequences, leading to a decrease in accuracy. This highlights the challenge of balancing model training efficiency with the ability to maintain high performance in handling long sequences. ### 2.2. Frequency-based Transformers\n\nA various of research has focused on applying the Transformer architecture in the frequency domain. For instance, FNet (Lee-Thorp et al., 2021) replaces the self-attention in BERT with Fourier Transform, significantly speeding up Transformer computations. A similar concept (Buchholz \\& Jug, 2022) has been adapted for image processing tasks. DCFormer (Li et al., 2023) proposes a Transformerbased network that learns semantic representations directly from frequency domain representations using Discrete Cosine Transform (DCT). In the realm of video prediction, ideas like the local frequency domain transformer (Farazi et al., 2021) have been introduced. However, applying these concepts to existing decoder-only large language models presents challenges. The auto-regressive inference style of these models makes token-level frequency domain transformations cumbersome. Each new token requires frequency domain transformation in conjunction with all previous tokens, which fails to reduce complexity and undermines the potential efficiency gains of frequency domain approaches in large-scale language models. ## 3. Kernelized Attention in Frequency Domain\n\nIn our study, we begin by revisiting the general form of selfattention (Vaswani et al., 2017). To simplify the notation and focus on the core aspects, we consider the single head form of self-attention and omit normalization factors. The self-attention mechanism is fundamentally composed of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_7edc096661daf8d487e6g-03.jpg?height=632&width=1676&top_left_y=210&top_left_x=214)\n\nFigure 1. Illustration of the proposed method, where the computation of queries and keys in the attention mechanism of a Transformer is efficiently mapped to the frequency domain using a fast Discrete Cosine Transform (DCT). This mapping effectively eliminates the softmax operation, thereby substantially reducing the computational complexity of the Transformer. queries $Q$, keys $K$, and values $V$, expressed in the formula:\n\n$$\n\\begin{aligned}\n& \\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V \\\\\n& \\text { where } Q, K, V \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\nwhere $n$ denotes the number of tokens and $d$ denotes the hidden dimension of the attention. Specifically, when we denote $Q$ as $\\left(q_{1}, q_{2}, \\ldots, q_{n}\\right), K$ as $\\left(k_{1}, k_{2}, \\ldots, k_{n}\\right), V$ as $\\left(v_{1}, v_{2}, \\ldots, v_{n}\\right)$, and output $O$ as $\\left(o_{1}, o_{2}, \\ldots, o_{n}\\right)$, Equation 1 can be reformulated as:\n\n$$\n\\begin{aligned}\n& o_{i}=\\sum_{j=1}^{n} \\frac{e^{q_{i} k_{j}^{\\top}}}{\\sum_{j^{\\prime}=1}^{n} e^{q_{i} k_{j^{\\prime}}^{\\top}}} v_{j} \\\\\n& \\text { where } q_{i}, k_{i}, v_{i} \\in \\mathbb{R}^{1 \\times d}, i=\\{1,2, \\ldots, n\\}\n\\end{aligned}\n$$\n\nIt can be observed that the computational and memory complexity for calculating each output in a Transformer model is $\\mathcal{O}(n d)$, where $n$ is the sequence length and $d$ is the dimensionality of the representation. Consequently, the time and memory complexity for processing a sentence of length $n$ scales quadratically, becoming $\\mathcal{O}\\left(n^{2} d\\right)$. This quadratic scaling poses a significant computational burden, particularly for longer sequences where $n$ is large, making processing resource-intensive and challenging. To mitigate this complexity, the concept of a kernel mechanism has been introduced as a means to reduce the computational demands of attention mechanisms, which has been introduced in (Tsai et al., 2019; Katharopoulos et al., 2020; Choromanski et al., 2020). Specifically, this involves the introduction of a kernel function $K(\\cdot, \\cdot)$, which acts as a positive-definite kernel capable of measuring similarity. By utilizing this kernel, the attention mechanism can be reformulated as:\n\n$$\no_{i}=\\sum_{j=1}^{n} \\frac{K\\left(q_{i}, k_{j}\\right)}{\\sum_{j^{\\prime}=1}^{n} \\mathcal{K}\\left(q_{i}, k_{j^{\\prime}}\\right)} v_{j}\n$$\n\nBy applying the kernel trick, it's possible to linearly decompose the attention mechanism:\n\n$$\no_{i}=\\sum_{j=1}^{n} \\frac{\\phi\\left(q_{i}\\right) \\phi\\left(k_{j}\\right)^{\\top}}{\\sum_{j^{\\prime}=1}^{n} \\phi\\left(q_{i}\\right) \\phi\\left(k_{j^{\\prime}}\\right)^{\\top}} v_{j}\n$$\n\nwhere $\\phi(\\cdot): \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{m}$ is a projection to map the inputs into $m$ dimension features. This decomposition benefits from the fact that the computational dimensions of the keys and values can be merged, effectively reducing the computational complexity from $\\mathcal{O}\\left(n^{2} d\\right)$ to $\\mathcal{O}(n m d)$. Given that the dimensionality $d$ and $m$ is typically much smaller than the sequence length $n$, this linearization of the attention mechanism results in a substantial decrease in computational intensity. In the context of large language models, the cost of retraining is prohibitively high. In such scenarios, it becomes imperative to find a kernel that can equivalently replace the vanilla attention mechanism without necessitating extensive retraining. Positive Random Features (PRF) (Choromanski et al., 2020) emerge as a viable candidate in this regard:\n\n$$\n\\phi_{\\mathrm{PRF}}(x)=e^{\\omega x^{\\top}-\\frac{\\|x\\|^{2}}{2}}\n$$\n\nwhere $\\omega \\in \\mathbb{R}^{m \\times d}$. Theoretical demonstrations have established that $e^{q k^{\\top}}=\\mathbb{E}_{\\omega \\sim \\mathcal{N}(0 . I)}\\left[e^{\\omega q^{\\top}-\\frac{\\|q\\|^{2}}{2}} e^{\\omega k^{\\top}-\\frac{\\|k\\|^{2}}{2}}\\right]$. It means that when $m$, the dimension of the feature space, is sufficiently large, Positive Random Features (PRF) mapping becomes an equivalent of the original attention mechanism. This equivalence suggests that, in theory, it is feasible to directly transform existing vanilla attention into linear attention using PRF mapping, thereby achieving an acceleration without loss of functionality. However, a notable challenge arises due to the need for $m$ to be set to a significantly large value to maintain the performance by reducing the approximation error. This requirement leads to a non-negligible increase in computational demand. For instance, in the case of the Performer (Choromanski et al., 2020), to achieve a lossless linear attention, $m$ often needs to be set to larger than $d$, diminishing the benefits of reduced computational load brought by linear attention. To address this issue, we first conduct a theoretical analysis of the kernel-based approach for approximating attention mechanisms. We begin with the application of Bochner's Theorem. This theorem allows us to equate the original attention computation involving queries $(\\mathrm{Q})$ and keys $(\\mathrm{K})-$ specifically the Gaussian kernel - to an integral computation akin to Equation 4. Theorem 3.1. (Bochner's Theorem) (Feller, 1966). A continuous shift invariant scaled kernel function $K(x, z)$ : $\\mathbb{R}^{d} \\rightarrow R$ is positive definite if and only if it is the Fourier Transform of a unique finite probability measure $p$ on $\\mathbb{R}^{d}$.",
    "dijiang-1": "$$\nK(x, z)=\\int_{\\mathbb{R}^{d}} e^{i(x-z)^{\\top} w} p(w) d w=E_{w \\sim p(\\cdot)}\\left[e^{i w^{\\top} x}\\left(e^{i w^{\\top} z}\\right)^{*}\\right]\n$$\n\nwhere the symbol $z^{*}$ denotes the complex conjugate of $z$. According to Bochner's theorem, there is a one-to-one correspondence between the kernel function $K(x, z)$ and the probability density $p(w)$ defined on $\\mathbb{R}^{d}$. Monte Carlo is equal weight approximation to kernel integrals. Taking $\\varphi_{p}(x):=\\frac{1}{\\sqrt{m}}\\left[e^{-i w_{1}^{\\top} x}, \\ldots, e^{-i w_{m}^{\\top} x}\\right]^{\\top}$, the feature maps can be constructed as:\n\n$$\nK(x, z)=E_{w \\sim p(\\cdot)}\\left[e^{i w^{\\top} x}\\left(e^{i w^{\\top} z}\\right)^{*}\\right] \\approx \\varphi_{p}(x)^{\\top} \\varphi_{p}^{*}(z)\n$$\n\nwhere $w_{i} \\sim p(\\cdot)$ are samples constructed by Monte Carlo methods. $\\varphi_{p}(\\cdot)$ is the explicit finite dimensional feature map, which depends on the kernel $K$. Moving forward, instead of employing the Monte Carlo method as suggested in (Choromanski et al., 2020), we utilize the Quasi-Monte Carlo method (Le et al., 2013). This shift enables the estimation of the integral using a specific uniform distribution as opposed to a randomly sampled distribution. Utilizing Bochner's theorem allows for a transformative interpretation of the attention mechanism in Transformer models. For the Gaussian Kernel:\n\n$$\nK_{G}(x, y):=e^{-\\frac{\\|x-y\\|^{2}}{2}}=e^{-\\frac{\\|x\\|^{2}+\\|y\\|^{2}}{2}} e^{x^{\\top} y}\n$$\n\nsince the $x$ and $y$ in attention mechanism is usually normalized, the Gaussian Kernel can be regarded as $e^{x^{\\top} y}$, which is the same as the calculation between the queries and keys. Theorem 3.2. The Positive Fixed Features (PFF) is formulated as:\n\n$$\n\\varphi_{P F F}(x):=\\frac{e^{-\\|x\\|^{2}}}{\\sqrt{m}}\\left[e^{\\Phi^{-1}\\left(t_{1}\\right) x^{\\top} v_{1}}, \\ldots, e^{\\Phi^{-1}\\left(t_{m}\\right) x^{\\top} v_{m}}\\right]^{\\top}\n$$\n\nwhere $V=\\left[v_{1}, \\ldots, v_{m}\\right] \\in \\mathbb{S}^{d \\times m}$ is asymptotically uniformly distributed and $t_{i} \\sim U(0,1)$. Then, $\\varphi_{P F F}(x)^{\\top} \\varphi_{P F F}(z)$ is an unbiased estimate of Gaussian kernel $K_{G}(x, y)$. The proof of this theorem involves a transformation to spherical coordinates, which can be found in the supplementary material. Through this transformation, we demonstrate that an approximation based on any asymptotically uniformly distribution can closely approximate the original Gaussian kernel. Furthermore, according to (Asmussen \\& Glynn, 2007), when utilizing uniform sequences, the Quasi-Monte Carlo method can offer superior approximation efficiency compared to the traditional Monte Carlo method. The approximation efficiency of Quasi-Monte Carlo is $\\mathcal{O}(1 / m)$, which is more favorable than the $\\mathcal{O}\\left(1 / m^{-0.5}\\right)$ efficiency of Monte Carlo. Consequently, this implies that using the PFF 9 kernel for approximating the Gaussian kernel is more advantageous than the PRF kernel in Equation 5. Theorem 3.3. The Weighted Positive Fixed Features (WPFF) is formulated as:\n\n$$\n\\varphi_{W P F F}(x):=\\frac{D e^{-\\|x\\|^{2}}}{\\sqrt{m}}\\left[e^{\\Phi^{-1}\\left(t_{1}\\right) x^{\\top} v_{1}}, \\ldots, e^{\\Phi^{-1}\\left(t_{m}\\right) x^{\\top} v_{m}}\\right]^{\\top}\n$$\n\nwhere $D$ is a learnable parameter which can be optimized by the input $x$. Then the upper bound of the integral estimation error of the objective function by WPFF (Weighted Positive Fixed Features) method is not greater than the upper bound of the integral estimation error of the objective function by PFF (Positive Fixed Features) method. Building upon the Quasi-Monte Carlo foundation, we further introduce the concept of weighted Quasi-Monte Carlo to enhance the efficiency of approximation. This advancement aims to leverage the strengths of the Quasi-Monte Carlo method, augmenting it with strategically weighted sampling to improve the precision and convergence rates of our approximations. The detailed proof is provided in the supplementary materials. To further accelerate the training speed, we propose the use of frequency domain transformations to reduce the required computational resources. Fast Fourier Transform (FFT) and Discrete Cosine Transform (DCT) are commonly used methods for such transformations. Compared to ordinary orthogonal transformations, frequency domain transformations have algorithms for rapid computation, significantly reducing the computational cost of our proposed mapping. Specifically, the complexity of $\\mathcal{O}(\\mathrm{m})$ can be reduced to $\\mathcal{O}(\\log (m))$. Additionally, since DCT operates in the real\n\n```\nAlgorithm 1 Frequency domain kernelization for efficient\nlanguage models. input A small amount of data \\(x_{i}\\), a pre-trained Transformer\n    model \\(M\\). 1. Initialization: the DCT coefficient \\(\\mathcal{C}\\), the weight \\(D\\),\n    the diagonal matrix \\(T\\) in Equation 12 for each layer in\n    \\(M\\). ```\n\n2. Transformation: transform the vanilla attention calculation Attention $(Q, K, V)=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V$ to $\\operatorname{FKA}(Q, K, V)=\\phi_{\\mathrm{WDCF}}(Q) \\phi_{\\mathrm{WDCF}}(K)^{\\top} V$ using the Weighted Discrete Cosine Features for each layer in $M$. 3. Get the transformed model $M_{\\mathrm{FKA}}$. repeat\n4. Randomly select a batch of data from $x_{i}$. 5. Employ the transformed model $M_{\\mathrm{FKA}}$ on the minibatch. 6. Update weights in $M_{\\mathrm{FKA}}$ according to the loss and gradient;\nuntil convergence. output An efficient language model $M_{\\text {FKA }}$. number domain, it demands even less computational resources and is more hardware-friendly. Therefore, we opt for the DCT to carry out our kernel mapping. Specifically, a DCT coefficient $\\mathcal{C} \\in \\mathbb{R}^{d \\times d}$ in the frequency domain is defined as:\n\n$$\n\\mathcal{C}_{j_{1} j_{2}}=s_{j_{1}} s_{j_{2}} \\sum_{i_{1}=0}^{n-1} \\sum_{i_{2}=0}^{d-1} \\cos \\left(\\frac{\\pi\\left(2 i_{1}+1\\right) j_{1}}{2 d}\\right) \\cos \\left(\\frac{\\pi\\left(2 i_{2}+1\\right) j_{2}}{2 d}\\right)\n$$\n\nwhere $s_{j}=\\sqrt{1 / d}$ if $j=0$ and $s_{j}=\\sqrt{2 / d}$ otherwise. The weighted mapping using DCT (which is called Weighted Discrete Cosine Features) can be reformulated as:\n\n$$\n\\phi_{\\mathrm{WDCF}}(x)=D e^{T \\mathcal{C} x^{\\top}}\n$$\n\nwhere $\\mathcal{C} \\in \\mathbb{R}^{m \\times d}$ is the DCT coefficient, $D \\in \\mathbb{R}^{m}$ is a learnable weight, and $T=\\operatorname{diag}\\left(t_{1}, \\ldots, t_{m}\\right)$ is a random diagonal matrix following the inverse cumulative distribution. Note that since the $x$ in attention mechanism is usually normalized, we ignore the term of $\\|x\\|^{2}$ in Equation 9 for efficiency. Therefore, using DCT as a kernel can closely approximate the original attention mechanism while have low computation complexity. For scenarios where $m>d$, more DCT transformations can be derived using different boundary conditions.",
    "dijiang-2": "Details can be referred to (Ahmed et al., 1974). It is noted that we set $m=d$ to avoid increasing computational complexity in the subsequent experiments. Therefore, the kernelized attention in frequency domain (FKA) is then reformulated as:\n\n$$\n\\begin{aligned}\n& \\operatorname{FKA}(Q, K, V)=\\phi_{\\mathrm{WDCF}}(Q) \\phi_{\\mathrm{WDCF}}(K)^{\\top} V \\\\\n& \\text { where } Q, K, V \\in \\mathbb{R}^{n \\times d}\n\\end{aligned}\n$$\n\nThis approach achieves a notable reduction in computational complexity by employing the Discrete Cosine Transform (DCT) to map the queries and keys within the Transformer's attention mechanism to a domain where operations are inherently more efficient. In summary, our method leverages frequency domain kernelization for Transformer attention mechanisms, significantly cutting computational costs while either preserving or enhancing model performance. The details are shown in Algorithm 1. Through the strategic use of the weighted Quasi-Monte Carlo method, which outperforms traditional Monte Carlo sampling in efficiency and accuracy, combined with DCT for efficient frequency domain transformations, we attain linear complexity in attention computation. This reformulation not only improves the scalability of Transformers, enabling them to handle larger datasets and extended sequences with ease, but also markedly accelerates the training and inference phases. ## 4. Experiments\n\nIn this section, we conduct extensive experimental validation of the proposed architecture, encompassing results across language models of varying scales. Additionally, we provide detailed analyses to substantiate the effectiveness of our approach. ### 4.1. Evaluation on Different Scales\n\nGiven the challenge of replicating the training processes of most language models, as only their checkpoints are openly available, we opted to validate our method using Pythia (Biderman et al., 2023), a model with a fully public dataset and training procedure, enabling fair comparisons. We adhered to the exact training settings employed by Pythia, including learning rates, optimizers, and other hyperparameters, and utilized the Pile dataset. The Pile (Gao et al., 2020) is an 825 GiB corpus of English text, specifically designed for training large-scale language models. This project is composed of 22 distinct, high-quality subsets, both pre-existing and newly constructed, many of which originate from academic or professional sources. This comprehensive and diverse dataset serves as a robust foundation for developing and fine-tuning language models Our DiJiang model was fine-tuned from the pre-trained Pythia model. We evaluated our approach on six public datasets used by Pythia: PIQA (Bisk et al., 2020), WinoGrande, WSC (Sakaguchi et al., 2021), ARC-E, ARC-C (Clark et al., 2018), and LogiQA (Liu et al., 2020). The Pythia model's checkpoint was obtained from HuggingFace ${ }^{1}$. We adapt the learned gating mechanism (Peng et al., 2021) similar with the RetNet (Sun et al., 2023) to augment our DiJiang. [^1]Table 1. The experimental results of the proposed method. Training time is measured using A800. Inference throughput is evaluated with token length of 2048. * denotes results from (He et al., 2024). | Model | PIQA | WinoGrande | WSC | ARC-E | ARC-C | LogiQA | Avg | Training <br> (day) | Inference <br> (tokens/s) |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Pythia-70M | 0.498 | 0.484 | 0.596 | 0.25 | 0.221 | 0.202 | 0.375 | 21.3 | 2037 |\n| DiJiang-70M | 0.587 | 0.511 | 0.365 | 0.403 | 0.213 | 0.253 | 0.389 | 1.3 | 2605 |\n| Pythia-160M | 0.532 | 0.484 | 0.634 | 0.265 | 0.227 | 0.202 | 0.391 | 42.9 | 622 |\n| DiJiang-160M | 0.618 | 0.490 | 0.384 | 0.439 | 0.217 | 0.239 | 0.398 | 2.7 | 1315 |\n| Pythia-410M | 0.668 | 0.537 | 0.567 | 0.521 | 0.213 | 0.22 | 0.454 | 105.8 | 203 |\n| DiJiang-410M | 0.663 | 0.524 | 0.567 | 0.492 | 0.244 | 0.247 | 0.456 | 6.6 | 787 |\n| Pythia-1B | 0.706 | 0.533 | 0.365 | 0.569 | 0.269 | 0.296 | 0.456 | 201.2 | 105 |\n| Mamba-1.3B* | 0.663 | 0.530 | 0.365 | 0.508 | 0.251 | 0.263 | 0.430 | - | - |\n| DiJiang-1B | 0.677 | 0.521 | 0.365 | 0.537 | 0.253 | 0.284 | 0.440 | 12.6 | 611 |\n| Pythia-2.8B | 0.737 | 0.596 | 0.384 | 0.640 | 0.295 | 0.215 | 0.478 | 593.3 | 34 |\n| DiJiang-2.8B | 0.713 | 0.545 | 0.413 | 0.597 | 0.289 | 0.279 | 0.473 | 37.1 | 284 |\n| OPT-350M | 0.645 | 0.524 | 0.365 | 0.441 | 0.208 | 0.210 | 0.399 | - | 201 |\n| DiJiang-350M | 0.550 | 0.507 | 0.635 | 0.286 | 0.227 | 0.223 | 0.404 | 5.6 | 820 |\n| TinyLLaMA-1.1B | 0.666 | 0.541 | 0.413 | 0.487 | 0.211 | 0.228 | 0.424 | - | 74 |\n| DiJiang-1.1B | 0.535 | 0.508 | 0.635 | 0.286 | 0.243 | 0.212 | 0.403 | 13.9 | 613 |\n\nThe experimental results, as shown in Table 1, indicate that our method achieved remarkable outcomes across different model sizes, ranging from 70 M to 2.8 B parameters. On average, the performance on the six datasets was nearly identical to that of the original Pythia, but with only $\\sim 1 / 16$ of the training cost. Furthermore, the inference speed of our DiJiang model was significantly faster than that of the original Pythia. These results substantiate the effectiveness of our approach, demonstrating its potential to enhance the efficiency of large language models without compromising performance. ### 4.2. Evaluation on Different Models\n\nTo evaluate the effectiveness of our method across different models, as shown in Table 1, we further applied our approach to the OPT-350M (Zhang et al., 2022) ${ }^{2}$ and TinyLLaMA-1.1B ${ }^{3}$ models. It's important to note that since their training data are not fully accessible, we continued to use the Pile dataset for fine-tuning them. Finally, we conducted further experiments on the wellknown publicly available large language model, LLaMA27B, fine-tuning it into the DiJiang-7B model. Table 3 reveal that the DiJiang-7B model achieves results that are virtually identical to the original LLaMA2-7B across various benchmarks. Remarkably, our model required only 40B\n\n[^2]training data, significantly less than the 2 T tokens used by LLaMA2-7B. This demonstrates the successful application of our method to large-scale models at the 7B parameter level, highlighting the efficiency and effectiveness of our fine-tuning approach even when scaling to vast model sizes. Interestingly, we found that despite using a limited dataset, our method achieved results similar to the original models with a significantly lower training cost and faster speed. This outcome further demonstrates the strong generalizability and flexibility of our approach, underscoring its potential applicability across a broad spectrum of language models, even in scenarios where the original training datasets are not available. ### 4.3. Comparison with Linear Transformers\n\nTo compare the superiority of our approach against other linear-complexity self-attention Transformer models, we validated the fine-tuning results on Pythia-400M for different models including Linformer, Performer, RetNet, and Cosformer. For a fair comparison, we employed the same training settings and data. Table 2 displays the comparative results, revealing that while existing methods can achieve good results through retraining, as evidenced by their original publications, most of them suffer from significant accuracy losses in scenarios where fine-tuning is done without retraining. This is largely because these methods struggle to accurately approximate the original attention mechanism, leading to an inability to restore the original accuracy with minimal training.",
    "dijiang-3": "Table 2. Comparison of different linear attention models on fine-tuning Pythoia-410M (Biderman et al., 2023). | Model | PIQA | WinoGrande | WSC | ARC-E | ARC-C | LogiQA | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Pythia-410M (Biderman et al., 2023) | 0.668 | 0.537 | 0.567 | 0.521 | 0.213 | 0.22 | 0.454 |\n| Linformer (Wang et al., 2020) | 0.5267 | 0.5114 | 0.6346 | 0.2656 | 0.244 | 0.2074 | 0.3982 |\n| Cosformer (Qin et al., 2022) | 0.5218 | 0.5059 | 0.6058 | 0.2673 | 0.2637 | 0.2642 | 0.4047 |\n| Performer (Choromanski et al., 2020) | 0.6431 | 0.4964 | 0.4327 | 0.4701 | 0.2312 | 0.2366 | 0.4183 |\n| RetNet (Sun et al., 2023) | 0.4951 | 0.4957 | 0.6346 | 0.2508 | 0.227 | 0.2028 | 0.3843 |\n| PFF (Equation 9) | 0.6453 | 0.4996 | 0.4712 | 0.4747 | 0.2295 | 0.2381 | 0.4264 |\n| DiJiang (Ours) | 0.6638 | 0.5241 | 0.5673 | 0.4928 | 0.2449 | 0.2473 | $\\mathbf{0 . 4 5 6 7}$ |\n\nTable 3. Comparison with LLaMA2-7B on various benchmarks. | Model | PIQA | SIQA | BoolQ | WSC | HellaSwag | ARC-E | ARC-C | MMLU | NQ | COPA | Race-Middle | Avg | Tokens |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| LLaMA2-7B | 0.782 | 0.485 | 0.749 | 0.663 | 0.740 | 0.561 | 0.403 | 0.468 | 0.192 | 0.670 | 0.402 | 0.565 | 2000 B |\n| DiJiang-7B | 0.775 | 0.346 | 0.626 | 0.683 | 0.694 | 0.626 | 0.427 | 0.407 | 0.194 | 0.730 | 0.618 | 0.557 | $40 B$ |\n\nAmong these comparison methods, Performer achieved the best results by approximating the original attention with Positive Random Features (PRF). However, as previously discussed, this Monte Carlo-based approximation method cannot achieve satisfactory outcomes, resulting in accuracy loss. By switching from Monte Carlo to the Quasi-Monte Carlo scheme using Positive Fixed Features (PFF) as described in Equation 9, we surpassed the accuracy of Performer but still fell short of the original vanilla Transformer's performance. Furthermore, by incorporating the Discrete Cosine Transform (DCT), our method achieves higher efficiency than approaches using PFF kernels. The DCT transformation enables a more compact and efficient representation of the frequency components of the attention mechanism. This efficiency stems from the DCT's ability to concentrate energy, allowing for a sparse representation that captures the most significant features of the data with fewer coefficients. Consequently, our approach not only closely approximates the original attention but also does so with improved computational performance compared to PFF-based methods. This advantage highlights the effectiveness of using DCT in optimizing the approximation of attention mechanisms, further underscoring the potential of our method in enhancing the efficiency of Transformer models. Further incorporating weighted Quasi-Monte Carlo, our DiJiang architecture ultimately achieved accuracy nearly identical to the original Pythia- 400 M , validating the efficacy of our approximation method. This demonstrates not only the potential of our approach for fine-tuning large-scale language models but also underscores the importance of choosing an efficient approximation strategy to maintain model performance. We further visualized the training curves to showcase the approximation efficiency of different linear Transformer models, as depicted in Figure 2. RetNet, as an emerging language model architecture, has shown its potential by achieving significantly low loss values, underscoring its\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_7edc096661daf8d487e6g-07.jpg?height=695&width=790&top_left_y=994&top_left_x=1058)\n\nFigure 2. Training Curve of different methods. The proposed method achieves the lowest PPL and the fastest converge speed. capability for language tasks. Despite its low loss, RetNet does not necessarily outperform on benchmark metrics and, in some cases, even falls short of the results achieved by the Performer. This discrepancy highlights the importance and advantages of employing kernel methods to approximate the original attention computation, particularly in fine-tuning scenarios. Our method demonstrates the fastest rate of loss reduction and ultimately achieves the lowest loss value. This rapid convergence indicates that our approach can quickly reach a performance level similar to that of the original Transformer. The visualization clearly underscores the superiority of our method in terms of both convergence speed and final model accuracy, validating our approach's effectiveness in efficiently approximating the attention mechanism\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_7edc096661daf8d487e6g-08.jpg?height=541&width=1605&top_left_y=251&top_left_x=227)\n\nFigure 3. Visualization of attention map of different architectures. The results are averaged by multiple heads. ![](https://cdn.mathpix.com/cropped/2024_09_12_7edc096661daf8d487e6g-08.jpg?height=1243&width=712&top_left_y=889&top_left_x=225)\n\nFigure 4. Comparison of inference memory and throughput between the proposed DiJIang and vanilla Transformer architecture. while maintaining high performance standards. This visual evidence further solidifies our claim that our method stands out among linear Transformer alternatives, offering a compelling solution for optimizing Transformer models without compromising on quality. ### 4.4. Comparison of Inference Cost\n\nFurthermore, we also evaluated the memory usage and throughput of our method in comparison to the original Transformer model under various conditions. We selected the Pythia-410M model as our primary subject for analysis. We follow the implementation of RetNet (Sun et al., 2023) to efficient inference. The specific results, as depicted in Figure 4, demonstrate that as the token length increases, the memory footprint and inference speed of our model do not escalate. This observation is attributed to the linear complexity characteristic of our approach, indicating that our method is more conducive to long-sequence inference. In contrast, due to the quadratic complexity of attention computations, the original Transformer model experiences a continuous increase in both inference time and required memory as the token length grows. This comparison highlights the efficiency and practicality of our solution, particularly in scenarios involving extensive sequences where computational resources are a critical concern. ### 4.5. Visualization\n\nTo further demonstrate the effectiveness of our model's approximation of the attention mechanism, we present attention maps generated by different methods in Figure 3. It is evident that the original Transformer's attention map (Figure 3 (a)) is rich in information, laying the foundation for its robust capabilities. In contrast, attention maps produced by other linear attention methods such as Performer (Figure 3 (b)) fail to adequately capture the relationships between tokens, resulting in maps that are dissimilar to those of the original Transformer and ultimately leading to decreased model accuracy, despite fine-tuning efforts. In contrast, our method (Figure 3 (c)), by employing the weighted Quasi-Monte Carlo scheme, closely approximates the original attention mechanism. This allows it to effectively model the relationships between different tokens, achieving results nearly identical to those of the original Transformer but\nwith significantly faster inference efficiency. This comparison not only highlights the inadequacies of other linear attention methods in capturing token interdependencies but also showcases the superiority of our approach in accurately approximating attention while enhancing computational efficiency. ## 5. Conclusion\n\nThis paper introduces DiJiang, a groundbreaking Frequency Domain Kernelization method designed to address the computational inefficiencies inherent in traditional Transformer models. By leveraging linear attention mechanisms and a novel application of the weighted Quasi-Monte Carlo method for efficient sampling, our approach significantly reduces the necessity for extensive retraining. This is particularly beneficial for large language models, where the cost and time associated with training are substantial barriers to progress. The kernelization process, underpinned by Discrete Cosine Transform (DCT), not only diminishes the computational complexity but also ensures that the adaptation from a vanilla Transformer to a linear attention model incurs minimal training costs. Our extensive experiments validate that DiJiang achieves performance on par with conventional Transformers while reducing training costs by about 10x and enhancing inference speeds. This method represents a significant advancement in the development of efficient and scalable Transformer models, promising wider applicability and facilitating advancements in various tasks within the realm of natural language processing and beyond. ## Broader Impact\n\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.",
    "dijiang-4": "## References\n\nAhmed, N., Natarajan, T., and Rao, K. R. Discrete cosine transform. IEEE transactions on Computers, 100(1):9093, 1974. Asmussen, S. and Glynn, P. W. Stochastic simulation: algorithms and analysis, volume 57.",
    "dijiang-5": "Springer, 2007. Bello, I. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint arXiv:2102.08602, 2021. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\nH., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling.",
    "dijiang-6": "In International Conference on Machine Learning, pp. 2397-2430. PMLR, 2023. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432-7439, 2020. Brauchart, J. S. and Grabner, P. J. Distributing many points on spheres: minimal energy and designs. Journal of Complexity, 31(3):293-326, 2015.",
    "dijiang-7": "Buchholz, T.-O. and Jug, F. Fourier image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1846-1854, 2022. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai 2 reasoning challenge.",
    "dijiang-8": "arXiv preprint arXiv:1803.05457, 2018. Dong, L., Xu, S., and Xu, B. Speech-transformer: a norecurrence sequence-to-sequence model for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 58845888. IEEE, 2018. Farazi, H., Nogga, J., and Behnke, S. Local frequency domain transformer networks for video prediction. In 2021 International Joint Conference on Neural Networks (IJCNN), pp.",
    "dijiang-9": "1-10. IEEE, 2021. Feller, W. Introduction to Probality Theory and Its Applications.",
    "dijiang-10": "Wiley Eastern, 1966. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800 gb dataset of diverse text for language modeling.",
    "dijiang-11": "arXiv preprint arXiv:2101.00027, 2020. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., and Wang, Y. Densemamba: State space models with dense hidden connection for efficient large language models.",
    "dijiang-12": "arXiv preprint arXiv:2403.00818, 2024. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "dijiang-13": "In International conference on machine learning, pp. 5156-5165. PMLR, 2020. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., and Park, S. Ocr-free document understanding transformer. In European Conference on Computer Vision, pp. 498-517. Springer, 2022. Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer.",
    "dijiang-14": "arXiv preprint arXiv:2001.04451, 2020. Le, Q., Sarl\u00f3s, T., Smola, A., et al. Fastfood-approximating kernel expansions in loglinear time.",
    "dijiang-15": "In Proceedings of the international conference on machine learning, volume 85, pp. 8, 2013. Lee-Thorp, J., Ainslie, J., Eckstein, I., and Ontanon, S. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021. Li, X., Zhang, Y., Yuan, J., Lu, H., and Zhu, Y. Discrete cosin transformer: Image modeling from frequency domain. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.",
    "dijiang-16": "5468-5478, 2023. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.",
    "dijiang-17": "arXiv preprint arXiv:2007.08124, 2020. Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., and Zhang, L. Soft: Softmax-free transformer with linear complexity.",
    "dijiang-18": "Advances in Neural Information Processing Systems, 34:21297-21309, 2021. Lyu, Y. Spherical structured feature maps for kernel approximation. In International Conference on Machine Learning, pp. 2256-2264. PMLR, 2017. Peloso, M. M. Classical spaces of holomorphic functions. Lecture notes available on http://www. mat. unimi. it/users/peloso, 2011. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y. cosformer: Rethinking softmax in attention. arXiv preprint arXiv:2202.08791, 2022. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R. Transformer dissection: a unified understanding of transformer's attention via the lens of kernel. arXiv preprint arXiv:1908.11775, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.",
    "dijiang-19": "F., and Chao, L. S. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks.",
    "dijiang-20": "arXiv preprint arXiv:2309.17453, 2023. Yang, J., Sindhwani, V., Avron, H., and Mahoney, M. Quasimonte carlo feature maps for shift-invariant kernels. In International Conference on Machine Learning, pp. 485493. PMLR, 2014. Zhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H., Zhang, R., and Susskind, J. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. DiJiang: Efficient Large Language Models through Compact Kernelization\nZheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates.",
    "dijiang-21": "arXiv preprint arXiv:2302.04542, 2023. ## A. Theoretical Proof. Theorem A.1. The Positive Fixed Features (PFF) is formulated as:\n\n$$\n\\varphi_{P F F}(x):=\\frac{e^{-\\|x\\|^{2}}}{\\sqrt{m}}\\left[e^{\\Phi^{-1}\\left(t_{1}\\right) x^{\\top} v_{1}}, \\ldots, e^{\\Phi^{-1}\\left(t_{m}\\right) x^{\\top} v_{m}}\\right]^{\\top}\n$$\n\nwhere $V=\\left[v_{1}, \\ldots, v_{m}\\right] \\in \\mathbb{S}^{d \\times m}$ is asymptotically uniformly distributed and $t_{i} \\sim U(0,1)$. Then, $\\varphi_{\\text {PFF }}(x)^{\\top} \\varphi_{\\text {PFF }}(z)$ is an unbiased estimate of Gaussian kernel $K_{G}(x, y)$.",
    "dijiang-22": "Proof. The proof is motivated by (Lyu, 2017). We also use spherical coordinate changes to get the following proof. The Gaussian kernel is real-valued and therefore the imaginary part in Eq.(6) can be discarded. $$\nK_{G}(x, y)=e^{-\\frac{\\|x-y\\|^{2}}{2}}=\\int_{\\mathbb{R}^{d}} e^{i(x-y)^{\\boldsymbol{\\top}} w} \\mu(w) d w=\\int_{\\mathbb{R}^{d}} \\cos \\left((x-y)^{\\boldsymbol{\\top}} w\\right) \\mu(w) d w=e^{-\\|x\\|^{2}-\\|y\\|^{2}} \\int_{\\mathbb{R}^{d}} e^{\\left((x+y)^{\\top} w\\right)} \\mu(w) d w\n$$\n\nwhere $\\mu(\\cdot)$ is the probability density function of d-dimensional standard normal distribution. The Gaussian kernel is a shift and rotation invariant kernel. Given any rotation $R \\in S O(d)$, where $S O(d)$ denotes rotation groups, the corresponding probability density is also Gaussian according to Bochner's theorem. For shift and rotation invariant kernels, we can convert the integral to spherical coordinates. $r=\\|w\\|_{2}$ and $p(r)$ be the density function of $r$, and $w=r v$. Because of the rotation invariant property of $K_{G}(x, y)$, we achieve:\n\n$$\nK_{G}(x, y)=\\int_{\\mathbb{R}^{d}} e^{i(x-y)^{\\boldsymbol{\\top}} w} p(w) d w=\\int_{R_{+}} \\int_{\\mathbb{S}^{d-1}} e^{i(x-y)^{\\top} r v} p(r) d r d \\sigma(v)=\\int_{[0,1]} \\int_{\\mathbb{S}^{d-1}} e^{i(x-y)^{\\boldsymbol{\\top}} \\Phi^{-1}(t) v} d t d \\sigma(v)\n$$\n\nwhere $\\sigma$ denotes the normalized surface area measure on $\\mathbb{S}^{d}:=\\left\\{x \\in \\mathbb{R}^{d} \\mid\\|x\\|_{2}=1\\right\\}$ and $\\Phi^{-1}(t)$ denotes the inverse cumulative distribution function w.r.t is a non-negative radial scale. For real valued continuous shift and rotation invariant scaled kernel $K_{G}(x, y)$, the imaginary parts of the integral vanish. We can achieve:\n\n$$\nK_{G}(x, y)=\\int_{\\mathbb{R}^{d}} e^{i(x-z)^{\\top} w} p(w) d w=\\int_{\\mathbb{R}^{d}} \\cos \\left((x-z)^{\\boldsymbol{\\top}} w\\right) p(w) d w=\\int_{[0,1]} \\int_{\\mathbb{S}^{d-1}} \\cos \\left((x-z)^{\\top} \\Phi^{-1}(t) v\\right) d t d \\sigma(v)\n$$\n\nFor Gaussian kernel, we can get another medium integral form:\n\n$$\nK_{G}(x, y)=\\int_{\\mathbb{R}^{d}} e^{i(x-y)^{\\top} w} \\mu(w) d w=e^{-\\|x\\|^{2}-\\|y\\|^{2}} \\int_{\\mathbb{R}^{d}} e^{\\left((x+y)^{\\top} w\\right)} \\mu(w) d w=e^{-\\|x\\|^{2}-\\|y\\|^{2}} \\int_{[0,1]} \\int_{\\mathbb{S}^{d-1}} e^{(x+y)^{\\top} \\Phi^{-1}(t) v} d t d \\sigma(v)\n$$\n\nAccording to (Brauchart \\& Grabner, 2015), if the point set $V=\\left[v_{1}, \\ldots, v_{m}\\right] \\in \\mathbb{S}^{d \\times m}$ is asymptotically uniformly distributed, the following equation holds true:\n\n$$\n\\lim _{m \\rightarrow \\infty} \\frac{1}{m} \\sum_{i=1}^{m} f\\left(v_{i}\\right)=\\int_{\\mathbb{S}^{d}} f(v) d \\sigma(v)\n$$\n\nThen, we have:\n\n$$\n\\begin{aligned}\n\\lim _{m \\rightarrow \\infty} E_{t_{i} \\sim U(0,1)}\\left[\\varphi_{\\mathrm{PFF}}(x)^{\\top} \\varphi_{\\mathrm{PFF}}(y)\\right] & =E_{t_{i} \\sim U(0,1)}\\left[\\lim _{m \\rightarrow \\infty} \\frac{e^{-\\|x\\|^{2}-\\|y\\|^{2}}}{m} \\sum_{i=1}^{m} e^{\\Phi^{-1}\\left(t_{j}\\right)(x+y)^{\\top} v_{i}}\\right] \\\\\n& =e^{-\\|x\\|^{2}-\\|y\\|^{2}} \\int_{[0,1]} \\int_{\\mathbb{S}^{d-1}} e^{(x+y)^{\\top} \\Phi^{-1}(t) v} d t d \\sigma(v) \\\\\n& =e^{-\\|x\\|^{2}-\\|y\\|^{2}}(2 \\pi)^{-\\frac{d}{2}} \\int_{\\mathbb{R}^{d}} e^{(x+y)^{\\top} w} e^{-\\frac{\\|w\\|^{2}}{2}} d w \\\\\n& =e^{-\\|x\\|^{2}-\\|y\\|^{2}}(2 \\pi)^{-\\frac{d}{2}} \\int_{\\mathbb{R}^{d}} e^{\\frac{\\|x+y\\|^{2}}{2}} e^{-\\frac{\\|w-(x+y)\\|^{2}}{2}} d w \\\\\n& =e^{-\\|x\\|^{2}-\\|y\\|^{2}} e^{\\frac{\\|x+y\\|^{2}}{2}} \\\\\n& =e^{-\\frac{\\|x-y\\|^{2}}{2}} \\\\\n& =K_{G}(x, y)\n\\end{aligned}\n$$\n\nTherefore, $\\varphi_{\\mathrm{PFF}}(x)^{\\boldsymbol{\\top}} \\varphi_{\\mathrm{PFF}}(z)$ is an unbiased estimate of Gaussian kernel $K_{G}(x, y)$. Theorem A.2. The Weighted Positive Fixed Features (WPFF) is formulated as:\n\n$$\n\\varphi_{W P F F}(x):=\\frac{D e^{-\\|x\\|^{2}}}{\\sqrt{m}}\\left[e^{\\Phi^{-1}\\left(t_{1}\\right) x^{\\top} v_{1}}, \\ldots, e^{\\Phi^{-1}\\left(t_{m}\\right) x^{\\top} v_{m}}\\right]^{\\top}\n$$\n\nwhere $D$ is a learnable parameter which can be optimized by the input $x$. Then the upper bound of the integral estimation error of the objective function by WPFF (Weighted Positive Fixed Features) method is not greater than the upper bound of the integral estimation error of the objective function by PFF (Positive Fixed Features) method. Proof. The proof is motivated by (Yang et al., 2014). We use some of the same mathematical definitions and similar proofs from this paper to show that the WPFF method has a smaller upper bound on the overall estimation error of the objective function.",
    "dijiang-23": "Theorem A. 2 Lemma A. 3 and Lemma A. 4 are all relevant to this paper. Consider the task of computing an approximation of the following integral $I_{d, p}[f]=\\int_{\\mathbb{R}^{d}} f(x) p(x) d x$, because of $I_{d, p}[f]=$ $E_{x \\sim p\\left(\\mathbb{R}^{d}\\right)}[f(x)]$, an empirical approximation called Monte Carlo (MC) to the integral can be computed by drawing a random point set $S=\\left\\{w_{1}, \\ldots, w_{s}\\right\\}$ independently from $p\\left(\\mathbb{R}^{d}\\right)$. When $S$ is a set of fixed points, the empirical approximation is a quasi-Monte Carlo (QMC) method. The purpose of the QMC method is to improve convergence speed by constructing $S$ using deterministic low-differential sequences instead of random sampling points. We have $I_{S}[f]=\\frac{1}{s} \\sum_{w \\in S} f(w)$. We define the integration error with respect to the point set S as $\\epsilon_{S}[f]=\\left|I_{d, p}(f)-I_{S}(f)\\right|$. The integration error for PFF is as follows:\n\n$$\n\\epsilon_{S, p}[f]=\\left|\\int_{\\mathbb{R}^{d}} f(x) p(x) d x-\\frac{1}{s} \\sum_{j=1}^{s} f\\left(w_{j}\\right)\\right|\n$$\n\nwhere $S$ is a set of fixed points. The classical Monte Carlo and quasi-Monte Carlo approximations of integrals have consistent weights. However, it makes sense to weight the approximations, approximate $I_{d, p}[f]=\\int_{\\mathbb{R}^{d}} f(x) p(x) d x$ using $I_{S, \\Xi}[f]=\\sum_{j=1}^{s} \\xi_{j} f\\left(w_{j}\\right)$, where $\\Xi=\\left\\{\\xi_{1}, \\xi_{2}, \\ldots, \\xi_{s}\\right\\}, \\xi_{i} \\geq 0$ for $i \\in\\{1,2, \\ldots, s\\}$, we do not need to normalize the weights, that is it is possible that $\\sum_{i=1}^{s} \\xi_{i} \\neq 1$\n\n$$\n\\begin{aligned}\n\\int_{\\mathbb{R}^{d}} e^{i(x-z)^{T} w} p(w) d w & \\approx \\sum_{j=1}^{s} \\zeta_{j}(x) \\zeta_{j}(z) f\\left(w_{j}\\right) \\\\\n& =\\sum_{j=1}^{s} \\zeta_{j}(x) \\zeta_{j}(z) e^{i x^{T} w_{j}} e^{-i z^{T} w_{j}} \\\\\n& =\\varphi_{\\mathrm{WPFF}, S}(x)^{T} \\varphi_{\\mathrm{WPFF}, S}(z)\n\\end{aligned}\n$$\n\nwhere $\\zeta_{j}(x) \\zeta_{j}(z)=\\xi_{j}, \\zeta_{j}(x), \\zeta_{j}(z) \\geq 0, e^{i x^{T} w_{j}} e^{-i z^{T} w_{j}}=f\\left(w_{j}\\right)$ for $j \\in\\{1,2, \\ldots, s\\}$, and $\\Psi_{S}(x)=$ $\\left[\\zeta_{1}(x) e^{i x^{T} w_{1}}, \\ldots, \\zeta_{s}(x) e^{i x^{T} w_{s}}\\right]^{T}$\n\nThe integration error for WPFF is as follows:\n\n$$\n\\epsilon_{S, p, \\Xi}[f]=\\left|\\int_{\\mathbb{R}^{d}} f(x) p(x) d x-\\sum_{j=1}^{s} \\xi_{j} f\\left(w_{j}\\right)\\right|\n$$\n\nFor a vector $b \\in R_{d}$, let us define $\\square b=\\left\\{u \\in \\mathbb{R}^{d}|| u_{j}|\\leq| b_{j} \\mid\\right\\}$. Let\n\n$$\nF_{\\square b}=\\left\\{f_{u}(x)=e^{i x^{T} u} \\mid u \\in \\square b\\right\\}\n$$\n\nand consider the space of functions that admit an integral representation over $F_{\\square b}$ of the form\n\n$$\nf(x)=\\int_{u \\in \\square b} \\hat{f}(u) e^{i x^{T} u} d u\n$$\n\nwhere $\\hat{f}(u) \\in \\ell_{2}(\\square b)$. This space is associated with the functions with compactly-supported inverse Fourier transforms called bandlimited functions, which play an important role in Shannon-Nyquist sampling theory. Under a natural choice of inner product, these spaces are called Paley-Wiener spaces and they constitute an RKHS. Lemma A.3. (The Kernel of Paley-Wiener RKHS) According to (Peloso, 2011), PW denotes the space of functions which are represented in the form of Eq.26, with the inner product $<f, g>_{P W_{b}}=(2 \\pi)^{2 d}<\\hat{f}, \\hat{g}>_{L_{2}(\\square b)} . P W_{b}$ is an RKHS with kernel function,\n\n$$\n\\sin c_{b}(u, v)=\\pi^{-d} \\prod_{i=1}^{d} \\frac{\\sin \\left(b_{j}\\left(u_{j}-v_{j}\\right)\\right)}{u_{j}-v_{j}}\n$$\n\nLemma A.4. According to (Yang et al., 2014), for $f \\in P W_{b}$ (Paley-Wiener spaces), we have\n\n$$\n\\epsilon_{S, p}[f] \\leq\\|f\\|_{P W_{b}} D_{p}^{\\square b}(S)\n$$\n\nwhere\n\n$$\nD_{p}^{\\square b}(S)^{2}=\\pi^{-d} \\int_{\\beta \\in \\square b}|\\Psi(\\beta)|^{2} d \\beta-\\frac{2(2 \\pi)^{-d}}{s} \\sum_{j=1}^{s} \\xi_{j} \\int_{\\beta \\in \\square b} \\Psi(\\beta) e^{i \\beta^{T} w_{j}} d \\beta+\\frac{1}{s^{2}} \\sum_{l=1}^{s} \\sum_{j=1}^{s} \\operatorname{sinc}_{b}\\left(w_{l}, w_{j}\\right)\n$$\n\nSuppose that $p(\\cdot)$ is a probability density function. Let $\\Psi(\\cdot)$ be the characteristic function associated with $p(\\cdot)$. Following (Yang et al., 2014), we can derive the following discrepancy measure that takes into account the weights:\n\n$$\n\\begin{aligned}\nD_{p}^{\\square b}(S, \\Xi)^{2} & =\\pi^{-d} \\int_{\\beta \\in \\square b}|\\Psi(\\beta)|^{2} d \\beta-2(2 \\pi)^{-d} \\sum_{j=1}^{s} \\xi_{j} \\int_{\\beta \\in \\square b} \\Psi(\\beta) e^{i \\beta^{T} w_{j}} d \\beta+\\sum_{l=1}^{s} \\sum_{j=1}^{s} \\xi_{l} \\xi_{j} \\sin c_{b}\\left(w_{l}, w_{j}\\right) \\\\\n& =\\pi^{-d} \\int_{\\beta \\in \\square b}|\\Psi(\\beta)|^{2} d \\beta-2(2 \\pi)^{-d} \\sum_{j=1}^{s} \\zeta_{j}(x) \\zeta_{j}(z) \\int_{\\beta \\in \\square b} \\Psi(\\beta) e^{i \\beta^{T} w_{j}} d \\beta+\\sum_{l=1}^{s} \\sum_{j=1}^{s} \\zeta_{l}(x) \\zeta_{l}(z) \\zeta_{j}(z) \\zeta_{j}(z) \\sin c_{b}\\left(w_{l}, w_{j}\\right) . \\end{aligned}\n$$\n\nWe note that if we fix the points, then optimizing just the weights is a optimization problem. The discrepancy measure can be written as\n\n$$\n\\begin{aligned}\nD_{p}^{\\square b}(S, \\xi)^{2} & =\\pi^{-d} \\int_{\\beta \\in \\square b}|\\Psi(\\beta)|^{2} d \\beta-2 v^{T} \\xi+\\xi^{T} H \\xi \\\\\n& =\\pi^{-d} \\int_{\\beta \\in \\square b}|\\Psi(\\beta)|^{2} d \\beta-2 v^{T}(\\zeta(x) \\otimes \\zeta(z))+(\\zeta(x) \\otimes \\zeta(z))^{T} H(\\zeta(x) \\otimes \\zeta(z))\n\\end{aligned}\n$$\n\nwhere $\\xi=\\left(\\xi_{1}, \\ldots, \\xi_{s}\\right)^{T}, \\zeta(x)=\\left(\\zeta_{1}(x), \\ldots, \\zeta_{s}(x)\\right)^{T}$, and $\\xi=\\zeta(x) \\otimes \\zeta(z)=\\left(\\zeta_{1}(x) \\zeta_{1}(z), \\ldots, \\zeta_{s}(x) \\zeta_{s}(z)\\right)^{T}$.",
    "dijiang-24": "$\\otimes$ is the Hadamard product. $v \\in R^{s}, v_{j}=(2 \\pi)^{-d} \\int_{\\beta \\in \\square b} \\Psi(\\beta) e^{i \\beta^{T} w_{j}} d \\beta$ and $H \\in R^{s \\times s}, H_{l j}=\\operatorname{sinc}_{b}\\left(w_{l}, w_{j}\\right)$. Thus, the optimal weights can be found by solving the following convex optimization problem\n\n$$\n\\begin{aligned}\n& \\arg \\min _{\\zeta(x), \\zeta(z) \\in R^{s}}-2 v^{T}(\\zeta(x) \\otimes \\zeta(z))+(\\zeta(x) \\otimes \\zeta(z))^{T} H(\\zeta(x) \\otimes \\zeta(z)) \\\\\n& \\text { s.t. } \\zeta(x) \\leq 0, \\zeta(z) \\leq 0\n\\end{aligned}\n$$\n\nSo obviously for a fixed sampling point, there is $D_{p}^{\\square b}(S, \\xi) \\leq D_{p}^{\\square b}(S)$. Then the upper bound of the integral estimation error of the objective function by WPFF (Weighted Positive Fixed Features) method is not greater than the upper bound of the integral estimation error of the objective function by PFF (Positive Fixed Features) method. ## A.1. Examples generated by DiJiang-7B\n\nPrompt: There is a time when I had to make a difficult decision. DiJiang-7B: I was in a situation where I could either stay or leave. It was a very difficult time for me. I was very happy in my job. But I felt that I needed to move on. So I decided to leave my current job and start my own business. This was not an easy decision to take. There were many things that needed my attention. And I didn't have enough time to do everything. LLaMA2-7B: I had to choose between my family and the people who were close with me. It was a hard decision, but it turned out that they weren't really friends after all! I am not sure if you have ever been in this situation before or what your thoughts are on making such an important choice like choosing one over another person when there is no clear answer as far back then goes.",
    "dijiang-25": ". . but here we go anyway: \"The best way forward would be for both parties involved (you included)to come together again soon so everyone can move past whatever happened. Prompt: The sun goes down, and finally Gauss and Curie find time to relax and discuss after an exhausting day of work. DiJiang-7B: \"So, what do you think about the newest research on the effects of the magnetic field on human brain?\" Gau asks, taking a sip of his coffee. He's been working on a new project for the last few weeks, trying to find a way to use the Earth's magnetic fields to help people with disabilities. It'd be a huge breakthrough for science, but it'll take a lot of time and money to get it to work properly. Gaug is a bit of a dreamer, so he'd rather not think too much about it. Curry, on other hand, is more pragmatic. She'll be the first to tell you that the world doesn't need more scientists, it needs more people who can actually do something. But she'l always been a realist, too. So, when Gaus asks her opinion, she doesn thesitate to give it, even though she knows it might not be what he wants to hear. '\"Well, I think it s a great idea, Gause. I mean, if we can find some way of using the earth's magnetic energy to improve people's lives, that would be amazing. And I know you're working hard on it.\" Currey says, sounding a little bit tired. They both know that it $1 l$ take some time, though. The project is still in its early stages, after all. Still, Curreys smile is enough to make Gauc smile back. After all, they're both scientist. That means they can talk about science all day long, right? LLaMA2-7B: Gauss: So, how did you like the new job? I heard that it was a bit of an adjustment. You're not used to working with people who are so... well-rounded and diverse in their interests! It must be quite different from your old life as one of Newton's apprentices at Cambridge University where all they cared about were numbers... or maybe just pi... but anyway - what do YOU think is more important for success these days - being able to calculate complex mathematical formulas quickly on paper using only pencil \\& eraser OR having good social skills when dealing w/ other humans??",
    "dijiang-26": "Hmmm.... let me know if u find out!! Bye now!!! [^0]:    ${ }^{\\text {*}}$ Equal contribution ${ }^{1}$ Huawei Noah's Ark Lab ${ }^{2}$ Peking University. Correspondence to: Yunhe Wang $<$ yunhe.wang@huawei.com $>$. [^1]:    ${ }^{1}$ https://huggingface.co/EleutherAI\n\n[^2]:    ${ }^{2}$ https://huggingface.co/facebook/opt-350m\n    ${ }^{3}$ https://huggingface.co/TinyLlama/ TinyLlama-1.1B-python-v0.1\n\n"
}