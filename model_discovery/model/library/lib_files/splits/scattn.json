{
    "scattn-0": "Sparse and Continuous Attention Mechanisms\n\nAndr\u00e9 F.",
    "scattn-1": "T. Martins\u264c,\u2644,\u264b &Ant\u00f3nio Farinhas\u264c &Marcos Treviso\u264c Vlad Niculae\u264d,\u264c&Pedro M. Q.",
    "scattn-2": "Aguiar\u2643,\u2644 &M\u00e1rio A. T. Figueiredo\u264c,\u2644 {andre.t.martins, marcos.treviso, antonio.farinhas, mario.figueiredo} @tecnico.ulisboa.pt, aguiar@isr.ist.utl.pt, vlad@vene.ro \u264cInstituto de Telecomunica\u00e7\u00f5es, Instituto Superior T\u00e9cnico, Lisbon, Portugal \u2643Instituto de Sistemas e Rob\u00f3tica, Instituto Superior T\u00e9cnico, Lisbon, Portugal \u2644LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal \u264dInformatics Institute, University of Amsterdam, The Netherlands \u264bUnbabel, Lisbon, Portugal\n\nAbstract\n\nExponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and -entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend -entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for . Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions. 1 Introduction\n\nExponential families are ubiquitous in statistics and machine learning [1, 2]. They enjoy many useful properties, such as the existence of conjugate priors (crucial in Bayesian inference) and the classical Pitman-Koopman-Darmois theorem [3, 4, 5], which states that, among families with fixed support (independent of the parameters), exponential families are the only having sufficient statistics of fixed dimension for any number of i.i.d. samples. Departing from exponential families, there has been recent work on discrete, finite-domain distributions with varying and sparse support, via the sparsemax and the entmax transformations [6, 7, 8]. Those approaches drop the link to exponential families of categorical distributions provided by the softmax transformation, which always yields dense probability mass functions. In contrast, sparsemax and entmax can lead to sparse distributions, whose support is not constant throughout the family. This property has been used to design sparse attention mechanisms with improved interpretability [8, 9]. However, sparsemax and entmax are so far limited to discrete domains. Can a similar approach be extended to continuous domains? This paper provides that extension and pinpoints a connection with \u201cdeformed exponential families\u201d [10, 11, 12] and Tsallis statistics [13], leading to -sparse families (\u00a72). We use this construction to obtain new density families with varying support, including the truncated parabola and paraboloid distributions (2-sparse counterpart of the Gaussian, \u00a72.4 and Fig. 1). Softmax and its variants are widely used in attention mechanisms, an important component of neural networks [14]. Attention-based neural networks can \u201cattend\u201d to finite sets of objects and identify relevant features. We use our extension above to devise new continuous attention mechanisms (\u00a73), which can attend to continuous data streams and to domains that are inherently continuous, such as images. Unlike traditional attention mechanisms, ours are suitable for selecting compact regions, such as 1D-segments or 2D-ellipses. We show that the Jacobian of these transformations are generalized covariances, and we use this fact to obtain efficient backpropagation algorithms (\u00a73.2). As a proof of concept, we apply our models with continuous attention to text classification, machine translation, and visual question answering tasks, with encouraging results (\u00a74). Notation. Let be a measure space, where is a set, is a -algebra, and is a measure. We denote by the set of -absolutely continuous probability measures. From the Radon-Nikodym theorem [15, \u00a731], each element of is identified (up to equivalence within measure zero) with a probability density function , with . For convenience, we often drop from the integral. We denote the measure of as , and the support of a density as . Given , we write expectations as . Finally, we define . 2 Sparse Families\n\nIn this section, we provide background on exponential families and its generalization through Tsallis statistics. We link these concepts, studied in statistical physics, to sparse alternatives to softmax recently proposed in the machine learning literature [6, 8], extending the latter to continuous domains. 2.1 Regularized prediction maps (-RPM)\n\nOur starting point is the notion of -regularized prediction maps, introduced by Blondel et al. [7] for finite domains . This is a general framework for mapping vectors in (e.g., label scores computed by a neural network) into probability vectors in (the simplex), with a regularizer encouraging uniform distributions. Particular choices of recover argmax, softmax [16], and sparsemax [6]. Our definition below extends this framework to arbitrary measure spaces , where we assume is a lower semi-continuous, proper, and strictly convex function. {definition}\n\nThe -regularized prediction map (-RPM) is defined as\n\np ^ \u03a9 \u200b [ f ] = arg \u200b max p \u2208 \u2133 + 1 \u200b ( S ) \u2061 \ud835\udd3c p \u200b [ f \u200b ( t ) ] \u2212 \u03a9 \u200b ( p ) , subscript ^ \ud835\udc5d \u03a9 delimited-[] \ud835\udc53 subscript arg max \ud835\udc5d superscript subscript \u2133 1 \ud835\udc46 subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 \u03a9 \ud835\udc5d \\hat{p}_{\\Omega}[f]=\\operatorname*{arg\\,max}_{p\\in\\mathcal{M}_{+}^{1}(S)}\\mathbb{E}_{p}[f(t)]-\\Omega(p), (1)\n\nwhere is the set of functions for which the maximizer above exists and is unique.",
    "scattn-3": "It is often convenient to consider a \u201ctemperature parameter\u201d , absorbed into via . If has a unique global maximizer , the low-temperature limit yields , a Dirac delta distribution at the maximizer of . For finite , this is the argmax transformation shown in [7]. Other interesting examples of regularization functionals are shown in the next subsections. 2.2 Shannon\u2019s negentropy and exponential families\n\nA natural choice of regularizer is the Shannon\u2019s negentropy, . In this case, if we interpret as an energy function, the -RPM corresponds to the well-known free energy variational principle, leading to Boltzmann-Gibbs distributions ([17]; see App. A):\n\np ^ \u03a9 \u200b [ f ] \u200b ( t ) = exp \u2061 ( f \u200b ( t ) ) \u222b S exp \u2061 ( f \u200b ( t \u2032 ) ) \u200b \ud835\udc51 \u03bd \u200b ( t \u2032 ) = exp \u2061 ( f \u200b ( t ) \u2212 A \u200b ( f ) ) , subscript ^ \ud835\udc5d \u03a9 delimited-[] \ud835\udc53 \ud835\udc61 \ud835\udc53 \ud835\udc61 subscript \ud835\udc46 \ud835\udc53 superscript \ud835\udc61 \u2032 differential-d \ud835\udf08 superscript \ud835\udc61 \u2032 \ud835\udc53 \ud835\udc61 \ud835\udc34 \ud835\udc53 \\hat{p}_{\\Omega}[f](t)=\\frac{\\exp(f(t))}{\\int_{S}\\exp(f(t^{\\prime}))d\\nu(t^{\\prime})}=\\exp\\bigl{(}f(t)-A(f)\\bigr{)}, (2)\n\nwhere is the log-partition function. If is finite and is the counting measure, the integral in (2) is a summation and we can write as a vector . In this case, the -RPM is the softmax transformation,\n\np ^ \u03a9 \u200b [ f ] = softmax \u200b ( f ) = exp \u2061 ( f ) \u2211 k = 1 | S | exp \u2061 ( f k ) \u2208 \u25b3 | S | . subscript ^ \ud835\udc5d \u03a9 delimited-[] \ud835\udc53 softmax \ud835\udc53 \ud835\udc53 superscript subscript \ud835\udc58 1 \ud835\udc46 subscript \ud835\udc53 \ud835\udc58 superscript \u25b3 \ud835\udc46 \\hat{p}_{\\Omega}[f]=\\mathrm{softmax}(f)=\\tfrac{\\exp(f)}{\\sum_{k=1}^{|S|}\\exp(f_{k})}\\in\\triangle^{|S|}. (3)\n\nIf , is the Lebesgue measure, and for and (i.e., is a positive definite matrix), we obtain a multivariate Gaussian, . This becomes a univariate Gaussian if . For and defining , with and , we get a Laplace density, . Exponential families. Let , where is a vector of statistics and is a vector of canonical parameters. A family of the form (2) parametrized by is called an exponential family [2]. Exponential families have many appealing properties, such as the existence of conjugate priors and sufficient statistics, and a dually flat geometric structure [18]. Many well-known distributions are exponential families, including the categorical and Gaussian distributions above, and Laplace distributions with a fixed . A key property of exponential families is that the support is constant within the same family and dictated by the base measure : this follows immediately from the positiveness of the function in (2). We abandon this property in the sequel. 2.3 Tsallis\u2019 entropies and -sparse families\n\nMotivated by applications in statistical physics, Tsallis [13] proposed a generalization of Shannon\u2019s negentropy. This generalization is rooted on the notions of -logarithm, (not to be confused with base- logarithm), and -exponential, :\n\nlog \u03b2 \u2061 ( u ) := { u 1 \u2212 \u03b2 \u2212 1 1 \u2212 \u03b2 , \u03b2 \u2260 1 log \u2061 u , \u03b2 = 1 ; exp \u03b2 \u2061 ( u ) := { [ 1 + ( 1 \u2212 \u03b2 ) \u200b u ] + 1 / ( 1 \u2212 \u03b2 ) , \u03b2 \u2260 1 exp \u2061 u , \u03b2 = 1 . formulae-sequence assign subscript \ud835\udefd \ud835\udc62 cases superscript \ud835\udc62 1 \ud835\udefd 1 1 \ud835\udefd \ud835\udefd 1 \ud835\udc62 \ud835\udefd 1 assign subscript \ud835\udefd \ud835\udc62 cases superscript subscript delimited-[] 1 1 \ud835\udefd \ud835\udc62 1 1 \ud835\udefd \ud835\udefd 1 \ud835\udc62 \ud835\udefd 1 \\log_{\\beta}(u):=\\left\\{\\begin{array}[]{ll}\\frac{u^{1-\\beta}-1}{1-\\beta},&\\beta\\neq 1\\\\\n\\log u,&\\beta=1;\\end{array}\\right.\\qquad\\exp_{\\beta}(u):=\\left\\{\\begin{array}[]{ll}[1+(1-\\beta)u]_{+}^{1/(1-\\beta)},&\\beta\\neq 1\\\\\n\\exp u,&\\beta=1.\\end{array}\\right. (4)\n\nNote that , , and for any . Another important concept is that of \u201c-escort distribution\u201d [13]: this is the distribution given by\n\np ~ \u03b2 \u200b ( t ) := p \u200b ( t ) \u03b2 \u2016 p \u2016 \u03b2 \u03b2 , where \u2016 p \u2016 \u03b2 \u03b2 = \u222b S p \u200b ( t \u2032 ) \u03b2 \u200b \ud835\udc51 \u03bd \u200b ( t \u2032 ) . assign superscript ~ \ud835\udc5d \ud835\udefd \ud835\udc61 \ud835\udc5d superscript \ud835\udc61 \ud835\udefd superscript subscript norm \ud835\udc5d \ud835\udefd \ud835\udefd where \u2016 p \u2016 \u03b2 \u03b2 = \u222b S p ( t \u2032 ) \u03b2 \ud835\udc51 \u03bd ( t \u2032 ) \\tilde{p}^{\\beta}(t):=\\frac{p(t)^{\\beta}}{\\|p\\|_{\\beta}^{\\beta}},\\quad\\text{where $\\|p\\|_{\\beta}^{\\beta}=\\int_{S}p(t^{\\prime})^{\\beta}d\\nu(t^{\\prime})$}. (5)\n\nNote that we have . The -Tsallis negentropy [19, 13] is defined as:111This entropy is normally defined up to a constant, often presented without the factor.",
    "scattn-4": "We use the same definition as Blondel et al. [7, \u00a74.3] for convenience. \u03a9 \u03b1 \u200b ( p ) := 1 \u03b1 \u200b \ud835\udd3c p \u200b [ log 2 \u2212 \u03b1 \u2061 ( p \u200b ( t ) ) ] = { 1 \u03b1 \u200b ( \u03b1 \u2212 1 ) \u200b ( \u222b S p \u200b ( t ) \u03b1 \u2212 1 ) , \u03b1 \u2260 1 , \u222b S p \u200b ( t ) \u200b log \u2061 p \u200b ( t ) , \u03b1 = 1 . assign subscript \u03a9 \ud835\udefc \ud835\udc5d 1 \ud835\udefc subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript 2 \ud835\udefc \ud835\udc5d \ud835\udc61 cases 1 \ud835\udefc \ud835\udefc 1 subscript \ud835\udc46 \ud835\udc5d superscript \ud835\udc61 \ud835\udefc 1 \ud835\udefc 1 subscript \ud835\udc46 \ud835\udc5d \ud835\udc61 \ud835\udc5d \ud835\udc61 \ud835\udefc 1 \\Omega_{\\alpha}(p):=\\tfrac{1}{\\alpha}\\mathbb{E}_{p}[\\log_{2-\\alpha}(p(t))]=\\begin{cases}\\frac{1}{\\alpha(\\alpha-1)}\\left(\\int_{S}p(t)^{\\alpha}-1\\right),&\\alpha\\neq 1,\\\\\n\\int_{S}p(t)\\log p(t),&\\alpha=1.\\end{cases} (6)\n\nNote that , for any , with recovering Shannon\u2019s negentropy (proof in App.",
    "scattn-5": "B). Another notable case is , the negative of which is called the Gini-Simpson index [20, 21]. We come back to the case in \u00a72.4. For , is strictly convex, hence it can be plugged in as the regularizer in Def. 2.1. The next proposition ([10]; proof in App. B) provides an expression for -RPM using the -exponential (4):\n\n{proposition}\n\nFor and ,\n\np ^ \u03a9 \u03b1 \u200b [ f ] \u200b ( t ) = exp 2 \u2212 \u03b1 \u2061 ( f \u200b ( t ) \u2212 A \u03b1 \u200b ( f ) ) , subscript ^ \ud835\udc5d subscript \u03a9 \ud835\udefc delimited-[] \ud835\udc53 \ud835\udc61 subscript 2 \ud835\udefc \ud835\udc53 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udc53 \\hat{p}_{\\Omega_{\\alpha}}[f](t)=\\exp_{2-\\alpha}(f(t)-A_{\\alpha}(f)), (7)\n\nwhere is a normalizing function:\n\nLet us contrast (7) with Boltzmann-Gibbs distributions (2), recovered with .",
    "scattn-6": "One key thing to note is that the -exponential, for , can return zero values. Therefore, the distribution in (7) may not have full support, i.e., we may have . We say that has sparse support if .222This should not be confused with sparsity-inducing distributions [22, 23]. This generalizes the notion of sparse vectors. Relation to sparsemax and entmax. Blondel et al. [7] showed that, for finite , -RPM is the sparsemax transformation, . Other values of were studied by Peters et al. [8], under the name -entmax transformation. For , these transformations have a propensity for returning sparse distributions, where several entries have zero probability. Proposition 2.3 shows that similar properties can be obtained when is continuous. Deformed exponential families. With a linear parametrization , distributions with the form (7) are called deformed exponential or -exponential families [10, 11, 12, 24]. The geometry of these families induced by the Tsallis -entropy was studied by Amari [18, \u00a74.3].333Unfortunately, the literature is inconsistent in defining these coefficients. Our matches that of Blondel et al. [7]; Tsallis\u2019 equals ; this family is also related to Amari\u2019s -divergences, but their . Inconsistent definitions have also been proposed for -exponential families regarding how they are normalized; for example, the Tsallis maxent principle leads to a different definition.",
    "scattn-7": "See App. C for a detailed discussion. Unlike those prior works, we are interested in the sparse, light tail scenario (), not in heavy tails. For , we call these -sparse families. When , -sparse families become exponential families and they cease to be \u201csparse\u201d, in the sense that all distributions in the same family have the same support. A relevant problem is that of characterizing . When , is the log-partition function (see (2)), and its first and higher order derivatives are equal to the moments of the sufficient statistics. The following proposition (stated as Amari and Ohara [25, Theorem 5], and proved in our App. D) characterizes for in terms of an expectation under the -escort distribution for (see (5)). This proposition will be used later to derive the Jacobian of entmax attention mechanisms. {proposition}\n\nis a convex function and its gradient is given by\n\n\u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) = \ud835\udd3c p ~ \u03b8 2 \u2212 \u03b1 \u200b [ \u03d5 \u200b ( t ) ] = \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b \u03d5 \u200b ( t ) \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 . subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 subscript \ud835\udd3c superscript subscript ~ \ud835\udc5d \ud835\udf03 2 \ud835\udefc delimited-[] italic-\u03d5 \ud835\udc61 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc \\nabla_{\\theta}A_{\\alpha}(\\theta)=\\mathbb{E}_{\\tilde{p}_{\\theta}^{2-\\alpha}}[\\phi(t)]=\\frac{\\int_{S}p_{\\theta}(t)^{2-\\alpha}\\phi(t)}{\\int_{S}p_{\\theta}(t)^{2-\\alpha}}. (8)\n\n2.4 The 2-Tsallis entropy: sparsemax\n\nIn this paper, we focus on the case . For finite , this corresponds to the sparsemax transfomation proposed by Martins and Astudillo [6], which has appealing theoretical and computational properties. In the general case, plugging in (7) leads to the -RPM,\n\np ^ \u03a9 2 \u200b [ f ] \u200b ( t ) = [ f \u200b ( t ) \u2212 \u03bb ] + , where \u03bb = A 2 \u200b ( f ) \u2212 1 , subscript ^ \ud835\udc5d subscript \u03a9 2 delimited-[] \ud835\udc53 \ud835\udc61 subscript delimited-[] \ud835\udc53 \ud835\udc61 \ud835\udf06 where \u03bb = A 2 ( f ) \u2212 1 , \\hat{p}_{\\Omega_{2}}[f](t)=[f(t)-\\lambda]_{+},\\qquad\\text{where $\\lambda=A_{2}(f)-1$,} (9)\n\ni.e., is obtained from by subtracting a constant (which may be negative) and truncating, where that constant must be such that . If is continuous and the Lebesgue measure, we call -RPM the continuous sparsemax transformation. Examples follow, some of which correspond to novel distributions. Truncated parabola. If , we obtain the continuous sparsemax counterpart of a Gaussian, which we dub a \u201ctruncated parabola\u201d:\n\np ^ \u03a9 2 \u200b [ f ] \u200b ( t ) = [ \u2212 ( t \u2212 \u03bc ) 2 2 \u200b \u03c3 2 \u2212 \u03bb ] + \u2255 TP \u200b ( t ; \u03bc , \u03c3 2 ) , subscript ^ \ud835\udc5d subscript \u03a9 2 delimited-[] \ud835\udc53 \ud835\udc61 subscript delimited-[] superscript \ud835\udc61 \ud835\udf07 2 2 superscript \ud835\udf0e 2 \ud835\udf06 \u2255 TP \ud835\udc61 \ud835\udf07 superscript \ud835\udf0e 2 \\hat{p}_{\\Omega_{2}}[f](t)=\\left[-\\tfrac{(t-\\mu)^{2}}{2\\sigma^{2}}-\\lambda\\right]_{+}\\eqqcolon\\mathrm{TP}(t;\\mu,\\sigma^{2}), (10)\n\nwhere (see App.",
    "scattn-8": "E.1). This function, depicted in Fig. 1 (top left), is widely used in density estimation. For and , it is known as the Epanechnikov kernel [26]. Truncated paraboloid. The previous example can be generalized to , with , where , leading to a \u201cmultivariate truncated paraboloid,\u201d the sparsemax counterpart of the multivariate Gaussian (see middle and rightmost plots in Fig. 1):\n\np ^ \u03a9 2 \u200b [ f ] \u200b ( t ) = [ \u2212 \u03bb \u2212 1 2 \u200b ( t \u2212 \u03bc ) \u200b \u03a3 \u2212 1 \u200b ( t \u2212 \u03bc ) ] + , where \u03bb = \u2212 ( \u0393 \u200b ( N 2 + 2 ) / det \u200b ( 2 \u200b \u03c0 \u200b \u03a3 ) ) 2 2 + N . subscript ^ \ud835\udc5d subscript \u03a9 2 delimited-[] \ud835\udc53 \ud835\udc61 subscript delimited-[] \ud835\udf06 1 2 \ud835\udc61 \ud835\udf07 superscript \u03a3 1 \ud835\udc61 \ud835\udf07 where \u03bb = \u2212 ( \u0393 ( N 2 + 2 ) / det ( 2 \u03c0 \u03a3 ) ) 2 2 + N \\hat{p}_{\\Omega_{2}}[f](t)=\\bigl{[}-\\lambda-\\tfrac{1}{2}(t-\\mu)\\Sigma^{-1}(t-\\mu)\\bigr{]}_{+},\\qquad\\text{where $\\lambda=-\\Bigl{(}\\Gamma\\bigl{(}\\tfrac{N}{2}+2\\bigr{)}/\\sqrt{\\mathrm{det}(2\\pi\\Sigma)}\\Bigr{)}^{\\frac{2}{2+N}}$}. (11)\n\nThe expression above, derived in App. E.2, reduces to (10) for . Notice that (unlike in the Gaussian case) a diagonal does not lead to a product of independent truncated parabolas. Triangular. Setting , with , yields the triangular distribution\n\np ^ \u03a9 2 \u200b [ f ] \u200b ( t ) = [ \u2212 \u03bb \u2212 | t \u2212 \u03bc | b ] + \u2255 Tri \u200b ( t ; \u03bc , b ) , subscript ^ \ud835\udc5d subscript \u03a9 2 delimited-[] \ud835\udc53 \ud835\udc61 subscript delimited-[] \ud835\udf06 \ud835\udc61 \ud835\udf07 \ud835\udc4f \u2255 Tri \ud835\udc61 \ud835\udf07 \ud835\udc4f \\hat{p}_{\\Omega_{2}}[f](t)=\\left[-\\lambda-\\tfrac{|t-\\mu|}{b}\\right]_{+}\\eqqcolon\\mathrm{Tri}(t;\\mu,b), (12)\n\nwhere (see App.",
    "scattn-9": "E.3). Fig. 1 (bottom left) depicts this distribution alongside Laplace. Location-scale families. More generally, let for a location and a scale , where is convex and continuously differentiable. Then, we have\n\np ^ \u03a9 2 \u200b [ f ] \u200b ( t ) = [ \u2212 \u03bb \u2212 1 \u03c3 \u200b g \u2032 \u200b ( | t \u2212 \u03bc | / \u03c3 ) ] + , subscript ^ \ud835\udc5d subscript \u03a9 2 delimited-[] \ud835\udc53 \ud835\udc61 subscript delimited-[] \ud835\udf06 1 \ud835\udf0e superscript \ud835\udc54 \u2032 \ud835\udc61 \ud835\udf07 \ud835\udf0e \\hat{p}_{\\Omega_{2}}[f](t)=\\left[-\\lambda-\\tfrac{1}{\\sigma}g^{\\prime}(|t-\\mu|/\\sigma)\\right]_{+}, (13)\n\nwhere and is the solution of the equation (a sufficient condition for such solution to exist is being strongly convex; see App.",
    "scattn-10": "E.4 for a proof). This example subsumes the truncated parabola () and the triangular distribution (). 2-sparse families. Truncated parabola and paraboloid distributions form a 2-sparse family, with statistics and canonical parameters . Gaussian distributions form an exponential family with the same sufficient statistics and canonical parameters. In 1D, truncated parabola and Gaussians are both particular cases of the so-called \u201c-Gaussian\u201d [10, \u00a74.1], for . Triangular distributions with a fixed location and varying scale also form a 2-sparse family (similarly to Laplace distributions with fixed location being exponential families). 3 Continuous Attention\n\nAttention mechanisms have become a key component of neural networks [14, 27, 28]. They dynamically detect and extract relevant input features (such as words in a text or regions of an image). So far, attention has only been applied to discrete domains; we generalize it to continuous spaces. Discrete attention. Assume an input object split in pieces, e.g., a document with words or an image with regions. A vanilla attention mechanism works as follows: each piece has a -dimensional representation (e.g., coming from an RNN or a CNN), yielding a matrix . These representations are compared against a query vector (e.g., using an additive model [14]), leading to a score vector . Intuitively, the relevant pieces that need attention should be assigned high scores. Then, a transformation (e.g., softmax or sparsemax) is applied to the score vector to produce a probability vector . We may see this as an -RPM. The probability vector is then used to compute a weighted average of the input representations, via . This context vector is finally used to produce the network\u2019s decision. To learn via the backpropagation algorithm, the Jacobian of the transformation , , is needed. Martins and Astudillo [6] gave expressions for softmax and sparsemax,\n\nJ softmax \u200b ( f ) = Diag \u200b ( p ) \u2212 p \u200b p \u22a4 , J sparsemax \u200b ( f ) = Diag \u200b ( s ) \u2212 s \u200b s \u22a4 / ( 1 \u22a4 \u200b s ) , formulae-sequence subscript \ud835\udc3d softmax \ud835\udc53 Diag \ud835\udc5d \ud835\udc5d superscript \ud835\udc5d top subscript \ud835\udc3d sparsemax \ud835\udc53 Diag \ud835\udc60 \ud835\udc60 superscript \ud835\udc60 top superscript 1 top \ud835\udc60 J_{\\mathrm{softmax}}(f)=\\mathrm{Diag}(p)-pp^{\\top},\\qquad J_{\\mathrm{sparsemax}}(f)=\\mathrm{Diag}(s)-ss^{\\top}/(1^{\\top}s), (14)\n\nwhere , and is a binary vector whose th entry is 1 iff . 3.1 The continuous case: score and value functions\n\nOur extension of -RPMs to arbitrary domains (Def. 2.1) opens the door for constructing continuous attention mechanisms. The idea is simple: instead of splitting the input object into a finite set of pieces, we assume an underlying continuous domain: e.g., text may be represented as a function that maps points in the real line (, continuous time) onto a -dimensional vector representation, representing the \u201csemantics\u201d of the text evolving over time; images may be regarded as a smooth function in 2D (), instead of being split into regions in a grid. Instead of scores , we now have a score function , which we map to a probability density . This density is used in tandem with the value mapping to obtain a context vector . Since may be infinite dimensional, we need to parametrize , , and to be able to compute in a finite-dimensional parametric space. Building attention mechanisms. We represent and using basis functions, and , defining and , where and . The score function is mapped into a probability density , from which we compute the context vector as , with . Summing up yields the following: {definition} Let be a tuple with , , and . An attention mechanism is a mapping , defined as:\n\n\u03c1 \u200b ( \u03b8 ) = \ud835\udd3c p \u200b [ \u03c8 \u200b ( t ) ] , \ud835\udf0c \ud835\udf03 subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udf13 \ud835\udc61 \\rho(\\theta)=\\mathbb{E}_{p}[\\psi(t)], (15)\n\nwith and . If , we call this entmax attention, denoted as . The values and lead to softmax and sparsemax attention, respectively. Note that, if and (Euclidean canonical basis), we recover the discrete attention of Bahdanau et al. [14]. Still in the finite case, if and are key and value vectors and is a query vector, this recovers the key-value attention of Vaswani et al. [28]. On the other hand, for and , we obtain new attention mechanisms (assessed experimentally for the 1D and 2D cases in \u00a74): for , the underlying density is Gaussian, and for , it is a truncated paraboloid (see \u00a72.4). In both cases, we show (App. G) that the expectation (15) is tractable (1D) or simple to approximate numerically (2D) if are Gaussian RBFs, and we use this fact in \u00a74 (see Alg. 1 for pseudo-code for the case ). Defining the value function . In many problems, the input is a discrete sequence of observations (e.g., text) or it was discretized (e.g., images), at locations . To turn it into a continuous signal, we need to smooth and interpolate these observations. If we start with a discrete encoder representing the input as a matrix , one way of obtaining a value mapping is by \u201capproximating\u201d with multivariate ridge regression. With , and packing the basis vectors as columns of matrix , we obtain:\n\nB \u22c6 = arg \u2061 min B \u2061 \u2016 B \u200b F \u2212 H \u2016 F 2 + \u03bb \u200b \u2016 B \u2016 F 2 = H \u200b F \u22a4 \u200b ( F \u200b F \u22a4 + \u03bb \u200b I N ) \u2212 1 = H \u200b G , superscript \ud835\udc35 \u22c6 subscript \ud835\udc35 superscript subscript norm \ud835\udc35 \ud835\udc39 \ud835\udc3b \ud835\udc39 2 \ud835\udf06 superscript subscript norm \ud835\udc35 \ud835\udc39 2 \ud835\udc3b superscript \ud835\udc39 top superscript \ud835\udc39 superscript \ud835\udc39 top \ud835\udf06 subscript \ud835\udc3c \ud835\udc41 1 \ud835\udc3b \ud835\udc3a B^{\\star}\\,\\,=\\,\\,\\arg\\min_{B}\\|BF-H\\|_{F}^{2}+\\lambda\\|B\\|_{F}^{2}\\,\\,=\\,\\,HF^{\\top}(FF^{\\top}+\\lambda I_{N})^{-1}\\,\\,=\\,\\,HG, (16)\n\nwhere is the Frobenius norm, and the matrix depends only on the values of the basis functions at discrete time steps and can be obtained off-line for different input lenghts . The result is an expression for with coefficients, cheaper than if . 3.2 Gradient backpropagation with continuous attention\n\nThe next proposition, based on Proposition 2.3 and proved in App. F, allows backpropagating over continuous entmax attention mechanisms. We define, for , a generalized -covariance,\n\ncov p , \u03b2 \u200b [ \u03d5 \u200b ( t ) , \u03c8 \u200b ( t ) ] = \u2016 p \u2016 \u03b2 \u03b2 \u00d7 ( \ud835\udd3c p ~ \u03b2 \u200b [ \u03d5 \u200b ( t ) \u200b \u03c8 \u200b ( t ) \u22a4 ] \u2212 \ud835\udd3c p ~ \u03b2 \u200b [ \u03d5 \u200b ( t ) ] \u200b \ud835\udd3c p ~ \u03b2 \u200b [ \u03c8 \u200b ( t ) ] \u22a4 ) , subscript cov \ud835\udc5d \ud835\udefd italic-\u03d5 \ud835\udc61 \ud835\udf13 \ud835\udc61 superscript subscript norm \ud835\udc5d \ud835\udefd \ud835\udefd subscript \ud835\udd3c subscript ~ \ud835\udc5d \ud835\udefd delimited-[] italic-\u03d5 \ud835\udc61 \ud835\udf13 superscript \ud835\udc61 top subscript \ud835\udd3c subscript ~ \ud835\udc5d \ud835\udefd delimited-[] italic-\u03d5 \ud835\udc61 subscript \ud835\udd3c subscript ~ \ud835\udc5d \ud835\udefd superscript delimited-[] \ud835\udf13 \ud835\udc61 top \\mathrm{cov}_{p,\\beta}[\\phi(t),\\psi(t)]\\,\\,=\\,\\,\\|p\\|_{\\beta}^{\\beta}\\times\\left(\\mathbb{E}_{\\tilde{p}_{\\beta}}\\big{[}\\phi(t)\\psi(t)^{\\top}\\big{]}-\\mathbb{E}_{\\tilde{p}_{\\beta}}[\\phi(t)]\\,\\mathbb{E}_{\\tilde{p}_{\\beta}}[\\psi(t)]^{\\top}\\right), (17)\n\nwhere is the -escort distribution in (5). For , we have the usual covariance; for , we get a covariance taken w.r.t. a uniform density on the support of , scaled by . {proposition}\n\nLet with . The Jacobian of the -entmax is:\n\nJ \u03c1 \u03b1 \u200b ( \u03b8 ) = \u2202 \u03c1 \u03b1 \u200b ( \u03b8 ) \u2202 \u03b8 = cov p , 2 \u2212 \u03b1 \u200b ( \u03d5 \u200b ( t ) , \u03c8 \u200b ( t ) ) . subscript \ud835\udc3d subscript \ud835\udf0c \ud835\udefc \ud835\udf03 subscript \ud835\udf0c \ud835\udefc \ud835\udf03 \ud835\udf03 subscript cov \ud835\udc5d 2 \ud835\udefc italic-\u03d5 \ud835\udc61 \ud835\udf13 \ud835\udc61 J_{\\rho_{\\alpha}}(\\theta)=\\frac{\\partial\\rho_{\\alpha}(\\theta)}{\\partial\\theta}=\\mathrm{cov}_{p,2-\\alpha}(\\phi(t),\\psi(t)). (18)\n\nNote that in the finite case, (18) reduces to the expressions in (14) for softmax and sparsemax. Example: Gaussian RBFs. As before, let , , and . For , we obtain closed-form expressions for the expectation (15) and the Jacobian (18), for any : is a Gaussian, the expectation (15) is the integral of a product of Gaussians, and the covariance (18) involves first- and second-order Gaussian moments. Pseudo-code for the case is shown as Alg. 1. For , is a truncated paraboloid. In the 1D case, both (15) and (18) can be expressed in closed form in terms of the function. In the 2D case, we can reduce the problem to 1D integration using the change of variable formula and working with polar coordinates.",
    "scattn-11": "See App. G for details. We use the facts above in the experimental section (\u00a74), where we experiment with continuous variants of softmax and sparsemax attentions in natural language processing and vision applications. 4 Experiments\n\nAs proof of concept, we test our continuous attention mechanisms on three tasks: document classification, machine translation, and visual question answering (more experimental details in App.",
    "scattn-12": "H). Document classification. Although textual data is fundamentally discrete, modeling long documents as a continuous signal may be advantageous, due to smoothness and independence of length. To test this hypothesis, we use the IMDB movie review dataset [29], whose inputs are documents (280 words on average) and outputs are sentiment labels (positive/negative). Our baseline is a biLSTM with discrete attention. For our continuous attention models, we normalize the document length into the unit interval , and use as the score function, leading to a 1D Gaussian () or truncated parabola () as the attention density. We compare three attention variants: discrete attention with softmax [14] and sparsemax [6]; continuous attention, where a CNN and max-pooling yield a document representation from which we compute and ; and combined attention, which obtains from discrete attention, computes and , applies the continuous attention, and sums the two context vectors (this model has the same number of parameters as the discrete attention baseline). Table 1 shows accuracies for different numbers of Gaussian RBFs. The accuracies of the individual models are similar, suggesting that continuous attention is as effective as its discrete counterpart, despite having fewer basis functions than words, i.e., . Among the continuous variants, the sparsemax outperforms the softmax, except for . We also see that a large is not necessary to obtain good results, which is encouraging for tasks with long sequences. Finally, combining discrete and continuous sparsemax produced the best results, without increasing the number of parameters. Machine translation. We use the DeEn IWSLT 2017 dataset [30], and a biLSTM model with discrete softmax attention as a baseline. For the continuous attention models, we use the combined attention setting described above, with 30 Gaussian RBFs and linearly spaced in and . The results (caption of Fig. 2) show a slight benefit in the combined attention over discrete attention only, without any additional parameters. Fig. 2 shows heatmaps for the different attention mechanisms on a DeEn sentence. The continuous mechanism tends to have attention means close to the diagonal, adjusting the variances based on alignment confidence or when a larger context is needed (e.g., a peaked density for the target word \u201csea\u201d, and a flat one for \u201cof\u201d). Visual QA. Finally, we report experiments with 2D continuous attention on visual question answering, using the VQA-v2 dataset [31] and a modular co-attention network as a baseline [32].444Software code is available at https://github.com/deep-spin/mcan-vqa-continuous-attention. The discrete attention model attends over a 1414 grid.555An alternative would be bounding box features from an external object detector [33]. We opted for grid regions to check if continuous attention has the ability to detect relevant objects on its own. However, our method can handle bounding boxes too, if the coordinates in the regression (16) are placed on those regions. For continuous attention, we normalize the image size into the unit square . We fit a 2D Gaussian () or truncated paraboloid () as the attention density; both correspond to , with . We use the mean and variance according to the discrete attention probabilities and obtain and with moment matching. We use Gaussian RBFs, with linearly spaced in and . Overall, the number of neural network parameters is the same as in discrete attention. The results in Table 2 show similar accuracies for all attention models, with a slight advantage for continuous softmax. Figure 3 shows an example (see App. H for more examples and some failure cases): in the baseline model, the discrete attention is too scattered, possibly mistaking the lamp with a TV screen. The continuous attention models focus on the right region and answer the question correctly, with continuous sparsemax enclosing all the relevant information in its supporting ellipse. 5 Related Work\n\nRelation to the Tsallis maxent principle. Our paper unifies two lines of work: deformed exponential families from statistical physics [13, 10, 25], and sparse alternatives to softmax recently proposed in the machine learning literature [6, 8, 7], herein extended to continuous domains. This link may be fruitful for future research in both fields. While most prior work is focused on heavy-tailed distributions (), we focus instead on light-tailed, sparse distributions, the other side of the spectrum (). See App. C for the relation to the Tsallis maxent principle. Continuity in other architectures and dimensions. In our paper, we consider attention networks exhibiting temporal/spatial continuity in the input data, be it text (1D) or images (2D). Recent work propose continuous-domain CNNs for 3D structures like point clouds and molecules [34, 35]. The dynamics of continuous-time RNNs have been studied in [36], and similar ideas have been applied to irregularly sampled time series [37]. Other recently proposed frameworks produce continuous variants in other dimensions, such as network depth [38], or in the target domain for machine translation tasks [39]. Our continuous attention networks can be used in tandem with these frameworks. Gaussian attention probabilities. Cordonnier et al. [40] analyze the relationship between (discrete) attention and convolutional layers, and consider spherical Gaussian attention probabilities as relative positional encodings. By contrast, our approach removes the need for positional encodings: by converting the input to a function on a predefined continuous space, positions are encoded implicitly, not requiring explicit positional encoding. Gaussian attention has also been hard-coded as input-agnostic self-attention layers in transformers for machine translation tasks by You et al. [41]. Finally, in their DRAW architecture for image generation, Gregor et al. [42, \u00a73.1] propose a selective attention component which is parametrized by a spherical Gaussian distribution. 6 Conclusions and Future Work\n\nWe proposed extensions to regularized prediction maps, originally defined on finite domains, to arbitrary measure spaces (\u00a72). With Tsallis -entropies for , we obtain sparse families, whose members can have zero tails, such as triangular or truncated parabola distributions. We then used these distributions to construct continuous attention mechanisms (\u00a73). We derived their Jacobians in terms of generalized covariances (Proposition 3.2), allowing for efficient forward and backward propagation. Experiments for 1D and 2D cases were shown on attention-based text classification, machine translation, and visual question answering (\u00a74), with encouraging results. There are many avenues for future work. As a first step, we considered unimodal distributions only (Gaussian, truncated paraboloid), for which we show that the forward and backpropagation steps have closed form or can be reduced to 1D integration. However, there are applications in which multiple attention modes are desirable. This can be done by considering mixtures of distributions, multiple attention heads, or sequential attention steps. Another direction concerns combining our continuous attention models with other spatial/temporal continuous architectures for CNNs and RNNs [34, 35, 36] or with continuity in other dimensions, such as depth [38] or output space [39]. Broader Impact\n\nWe discuss the broader impact of our work, including ethical aspects and future societal consequences. Given the early stage of our work and its predominantly theoretical nature, the discussion is mostly speculative. The continuous attention models developed in our work can be used in a very wide range of applications, including natural language processing, computer vision, and others. For many of these applications, current state-of-the-art models use discrete softmax attention, whose interpretation capabilities have been questioned in prior work [43, 44, 45]. Our models can potentially lead to more interpretable decisions, since they lead to less scattered attention maps (as shown in our Figures 2\u20133) and are able to select contiguous text segments or image regions. As such, they may provide better inductive bias for interpretation. In addition, our attention densities using Gaussian and truncated paraboloids include a variance term, being potentially useful as a measure of confidence\u2014for example, a large ellipse in an image may indicate that the model had little confidence about where it should attend to answer a question, while a small ellipse may denote high confidence on a particular object. We also see opportunities for research connecting our work with other continuous models [34, 35, 38] leading to end-to-end continuous models which, by avoiding discretization, have the potential to be less susceptible to adversarial attacks via input perturbations. Outside the machine learning field, the links drawn in \u00a72 between sparse alternatives to softmax and models used in non-extensive (Tsallis) statistical physics suggest a potential benefit in that field too. Note, however, that our work is a first step into all these directions, and as such further investigation will be needed to better understand the potential benefits. We strongly recommend carrying out user studies before deploying any such system, to better understand the benefits and risks. Some of the examples in App. H may help understand potential failure modes. We should also take into account that, for any computer vision model, there are important societal risks related to privacy-violating surveillance applications. Continuous attention holds the promise to scale to larger and multi-resolution images, which may, in the longer term, be deployed in such undesirable domains. Ethical concerns hold for natural language applications such as machine translation, where biases present in data can be arbitrarily augmented or hidden by machine learning systems. For example, our natural language processing experiments mostly use English datasets (as a target language in machine translation, and in document classification). Further work is needed to understand if our findings generalize to other languages. Likewise, in the vision experiments, the VQA-v2 dataset uses COCO images, which have documented biases [46]. In line with the fundamental scope and early stage of this line of research, we deliberately choose applications on standard benchmark datasets, in an attempt to put as much distance as possible from malevolent applications. Finally, although we chose the most widely used evaluation metrics for each task (accuracy for document classification and visual question answering, BLEU for machine translation), these metrics do not always capture performance quality\u2014for example, BLEU in machine translation is far from being a perfect metric. The data, memory, and computation requirements for training systems with continuous attention do not seem considerably higher than the ones which use discrete attention. On the other hand, for NLP applications, our approach has the potential to better compress sequential data, by using fewer basis functions than the sequence length (as suggested by our document classification experiments). While there is nothing specific about our research that poses environmental concerns or that promises to alleviate such concerns, our models share the same problematic property as other neural network models in terms of their energy consumption to train models and tune hyperparameters [47]. Acknowledgments and Disclosure of Funding\n\nThis work was supported by the European Research Council (ERC StG DeepSPIN 758969), by the P2020 program MAIA (contract 045909), and by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through contract UIDB/50008/2020. We would like to thank Pedro Martins, Zita Marinho, and the anonymous reviewers for their helpful feedback. References\n\nBrown [1986] Lawrence D Brown. Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory. Institute of Mathematical Statistics, 1986. Barndorff-Nielsen [2014] Ole Barndorff-Nielsen. Information and Exponential Families in Statistical Theory. John Wiley & Sons, 2014. Pitman [1936] Edwin James George Pitman. Sufficient statistics and intrinsic accuracy.",
    "scattn-13": "In Mathematical Proceedings of the Cambridge Philosophical Society, volume 32, pages 567\u2013579. Cambridge University Press, 1936. Darmois [1935] Georges Darmois. Sur les lois de probabilit\u00e9a estimation exhaustive.",
    "scattn-14": "CR Acad. Sci. Paris, 260(1265):85, 1935. Koopman [1936] Bernard Osgood Koopman. On distributions admitting a sufficient statistic. Transactions of the American Mathematical society, 39(3):399\u2013409, 1936. Martins and Astudillo [2016] Andr\u00e9 FT Martins and Ram\u00f3n Fernandez Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification.",
    "scattn-15": "In Proc. of ICML, 2016. Blondel et al. [2020] Mathieu Blondel, Andr\u00e9 FT Martins, and Vlad Niculae. Learning with fenchel-young losses. Journal of Machine Learning Research, 21(35):1\u201369, 2020. Peters et al. [2019] Ben Peters, Vlad Niculae, and Andr\u00e9 F.T. Martins. Sparse sequence-to-sequence models. In Proc. of ACL, 2019. Correia et al. [2019] Gon\u00e7alo M Correia, Vlad Niculae, and Andr\u00e9 FT Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174\u20132184, 2019. Naudts [2009] Jan Naudts. The q-exponential family in statistical physics. Central European Journal of Physics, 7(3):405\u2013413, 2009. Sears [2008] Timothy Sears. Generalized Maximum Entropy, Convexity and Machine Learning. PhD thesis, The Australian National University, 2008. Ding and Vishwanathan [2010] Nan Ding and S.V.N. Vishwanathan. t-logistic regression. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.",
    "scattn-16": "S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 514\u2013522. Curran Associates, Inc., 2010. Tsallis [1988] Constantino Tsallis. Possible generalization of Boltzmann-Gibbs statistics. Journal of Statistical Physics, 52:479\u2013487, 1988. Bahdanau et al. [2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate.",
    "scattn-17": "In Proc. of ICLR, 2015. Halmos [2013] Paul R Halmos. Measure Theory, volume 18. Springer, 2013. Bridle [1990] John S. Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Fran\u00e7oise Fogelman-Souli\u00e9 and Jeanny H\u00e9rault, editors, Neurocomputing, pages 227\u2013236. Springer, 1990. Cover and Thomas [2012] Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012. Amari [2016] Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016. Havrda and Charv\u00e1t [1967] Jan Havrda and Franti\u0161ek Charv\u00e1t. Quantification method of classification processes. concept of structural -entropy. Kybernetika, 3(1):30\u201335, 1967. Jost [2006] L. Jost. Entropy and diversity. Oikos, 113:363\u2013\u2013375, 2006. Rao [1982] R.A. Rao. Gini-Simpson index of diversity: a characterization, generalization, and applications. Utilitas Mathematics, 21:273\u2013282, 1982. Figueiredo [2001] M. Figueiredo. Adaptive sparseness using Jeffreys prior. In Proc. of NeurIPS, pages 697\u2013704, 2001. Tipping [2001] M. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211\u2013244, 2001. Matsuzoe and Ohara [2012] Hiroshi Matsuzoe and Atsumi Ohara. Geometry for q-exponential families. In Recent Progress in Differential Geometry and its Related Fields, pages 55\u201371. World Scientific, 2012. Amari and Ohara [2011] Shun-ichi Amari and Atsumi Ohara. Geometry of q-exponential family of probability distributions. Entropy, 13(6):1170\u20131185, 2011. Epanechnikov [1969] Vassiliy A Epanechnikov. Non-parametric estimation of a multivariate probability density. Theory of Probability & Its Applications, 14(1):153\u2013158, 1969. Sukhbaatar et al. [2015] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, pages 2440\u20132448, 2015. Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "scattn-18": "In Proc. of NeurIPS, 2017. Maas et al. [2011] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis.",
    "scattn-19": "In Proc. of NAACL-HLT, 2011. Cettolo et al. [2017] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, St\u00fcker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, and Federmann Christian. Overview of the IWSLT 2017 evaluation campaign. In Proc. of IWSLT, pages 2\u201314, 2017. Goyal et al. [2019] Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. International Journal of Computer Vision, 127(4):398\u2013414, 2019. Yu et al. [2019] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 6274\u20136283, 2019. Anderson et al. [2018a] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018a. Wang et al. [2018] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun. Deep parametric continuous convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2589\u20132597, 2018. Sch\u00fctt et al. [2017] Kristof Sch\u00fctt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In Advances in neural information processing systems, pages 991\u20131001, 2017. Funahashi and Nakamura [1993] Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural networks, 6(6):801\u2013806, 1993. Rubanova et al. [2019] Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, pages 5321\u20135331, 2019. Chen et al. [2018] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations.",
    "scattn-20": "In Advances in neural information processing systems, pages 6571\u20136583, 2018. Kumar and Tsvetkov [2018] Sachin Kumar and Yulia Tsvetkov. Von mises-fisher loss for training sequence to sequence models with continuous outputs.",
    "scattn-21": "In Proc. of ICLR, 2018. Cordonnier et al. [2019] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. In International Conference on Learning Representations, 2019. You et al. [2020] Weiqiu You, Simeng Sun, and Mohit Iyyer. Hard-coded Gaussian attention for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7689\u20137700, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.687. URL https://www.aclweb.org/anthology/2020.acl-main.687. Gregor et al. [2015] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation.",
    "scattn-22": "In International Conference on Machine Learning, pages 1462\u20131471, 2015. Jain and Wallace [2019] Sarthak Jain and Byron C Wallace. Attention is not explanation.",
    "scattn-23": "In Proc. NAACL-HLT, 2019. Serrano and Smith [2019] Sofia Serrano and Noah A Smith. Is attention interpretable? In Proc. ACL, 2019. Wiegreffe and Pinter [2019] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation.",
    "scattn-24": "In Proc. EMNLP-IJCNLP, 2019. Wang et al. [2019] Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations. In Proceedings of the IEEE International Conference on Computer Vision, pages 5310\u20135319, 2019. Strubell et al. [2019] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650, 2019. Bauschke and Combettes [2011] Heinz Bauschke and Patrick Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2011. Bregman [1967] Lev M Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics, 7(3):200\u2013217, 1967. Abe [2003] Sumiyoshi Abe. Geometry of escort distributions. Physical Review E, 68(3):031101, 2003. Jaynes [1957] Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957. Sennrich et al. [2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units.",
    "scattn-25": "In Proc. of ACL, 2016. Kreutzer et al. [2019] Julia Kreutzer, Joost Bastings, and Stefan Riezler. Joey nmt: A minimalist nmt toolkit for novices.",
    "scattn-26": "In Proc. of EMNLP-IJCNLP: System Demonstrations, pages 109\u2013114, 2019. He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-Decem:770\u2013778, 2016. Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. Anderson et al. [2018b] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 6077\u20136086, 2018b. Pennington et al. [2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation.",
    "scattn-27": "In Proc. of EMNLP, pages 1532\u20131543, 2014. Supplemental Material\n\nAppendix A Differential Negentropy and Boltzmann-Gibbs distributions\n\nWe adapt a proof from Cover and Thomas [17]. Let be the Shannon negentropy, which is proper, lower semi-continuous, and strictly convex [48, example 9.41], and let\n\nKL \u200b ( p \u2225 q ) := \u222b S p \u200b ( t ) \u200b log \u2061 p \u200b ( t ) q \u200b ( t ) assign KL conditional \ud835\udc5d \ud835\udc5e subscript \ud835\udc46 \ud835\udc5d \ud835\udc61 \ud835\udc5d \ud835\udc61 \ud835\udc5e \ud835\udc61 \\mathrm{KL}(p\\|q):=\\int_{S}p(t)\\log\\frac{p(t)}{q(t)}\n\nbe the Kullback-Leibler divergence between distributions and (which is always non-negative and equals iff ). Take as in (2), where is the log-partition function. We have, for any :\n\n0 0 \\displaystyle 0 \u2264 \\displaystyle\\leq KL \u200b ( p \u2225 q ) = \u222b S p \u200b ( t ) \u200b log \u2061 p \u200b ( t ) q \u200b ( t ) = \u03a9 \u200b ( p ) \u2212 \u222b S p \u200b ( t ) \u200b log \u2061 q \u200b ( t ) = \u03a9 \u200b ( p ) \u2212 \u222b S p \u200b ( t ) \u200b ( f \u200b ( t ) \u2212 A \u200b ( f ) ) KL conditional \ud835\udc5d \ud835\udc5e subscript \ud835\udc46 \ud835\udc5d \ud835\udc61 \ud835\udc5d \ud835\udc61 \ud835\udc5e \ud835\udc61 \u03a9 \ud835\udc5d subscript \ud835\udc46 \ud835\udc5d \ud835\udc61 \ud835\udc5e \ud835\udc61 \u03a9 \ud835\udc5d subscript \ud835\udc46 \ud835\udc5d \ud835\udc61 \ud835\udc53 \ud835\udc61 \ud835\udc34 \ud835\udc53 \\displaystyle\\mathrm{KL}(p\\|q)=\\int_{S}p(t)\\log\\frac{p(t)}{q(t)}=\\Omega(p)-\\int_{S}p(t)\\log q(t)=\\Omega(p)-\\int_{S}p(t)(f(t)-A(f)) (19) = \\displaystyle= \u03a9 \u200b ( p ) \u2212 \ud835\udd3c p \u200b [ f \u200b ( t ) ] + A \u200b ( f ) . \u03a9 \ud835\udc5d subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 \ud835\udc34 \ud835\udc53 \\displaystyle\\Omega(p)-\\mathbb{E}_{p}[f(t)]+A(f). Therefore, we have, for any , that\n\n\ud835\udd3c p \u200b [ f \u200b ( t ) ] \u2212 \u03a9 \u200b ( p ) \u2264 A \u200b ( f ) , subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 \u03a9 \ud835\udc5d \ud835\udc34 \ud835\udc53 \\mathbb{E}_{p}[f(t)]-\\Omega(p)\\leq A(f), (20)\n\nwith equality if and only if . Since the right hand side is constant with respect to , we have that the posited must be the maximizer of (1). Appendix B Tsallis Negentropy and Sparse Distributions\n\nB.1 Shannon as a limit case of Tsallis when\n\nWe show that for any .",
    "scattn-28": "From (6), it suffices to show that for any . Let , and . Observe that\n\nlim \u03b2 \u2192 1 log \u03b2 \u2061 ( u ) = lim \u03b2 \u2192 1 g \u200b ( \u03b2 ) h \u200b ( \u03b2 ) = g \u200b ( 1 ) h \u200b ( 1 ) = 0 0 , subscript \u2192 \ud835\udefd 1 subscript \ud835\udefd \ud835\udc62 subscript \u2192 \ud835\udefd 1 \ud835\udc54 \ud835\udefd \u210e \ud835\udefd \ud835\udc54 1 \u210e 1 0 0 \\lim_{\\beta\\rightarrow 1}\\log_{\\beta}(u)=\\lim_{\\beta\\rightarrow 1}\\frac{g(\\beta)}{h(\\beta)}=\\frac{g(1)}{h(1)}=\\frac{0}{0},\n\nso we are in an indeterminate case. We take the derivatives of and :\n\ng \u2032 \u200b ( \u03b2 ) = ( exp \u2061 ( log \u2061 u 1 \u2212 \u03b2 ) ) \u2032 = exp \u2061 ( log \u2061 u 1 \u2212 \u03b2 ) \u22c5 ( ( 1 \u2212 \u03b2 ) \u200b log \u2061 u ) \u2032 = \u2212 u 1 \u2212 \u03b2 \u200b log \u2061 u , superscript \ud835\udc54 \u2032 \ud835\udefd superscript superscript \ud835\udc62 1 \ud835\udefd \u2032 \u22c5 superscript \ud835\udc62 1 \ud835\udefd superscript 1 \ud835\udefd \ud835\udc62 \u2032 superscript \ud835\udc62 1 \ud835\udefd \ud835\udc62 g^{\\prime}(\\beta)=\\left(\\exp(\\log u^{1-\\beta})\\right)^{\\prime}=\\exp(\\log u^{1-\\beta})\\cdot((1-\\beta)\\log u)^{\\prime}=-u^{1-\\beta}\\log u, (21)\n\nand . From l\u2019H\u00f4pital\u2019s rule,\n\nlim \u03b2 \u2192 1 g \u200b ( \u03b2 ) h \u200b ( \u03b2 ) = lim \u03b2 \u2192 1 g \u2032 \u200b ( \u03b2 ) h \u2032 \u200b ( \u03b2 ) = log \u2061 u . subscript \u2192 \ud835\udefd 1 \ud835\udc54 \ud835\udefd \u210e \ud835\udefd subscript \u2192 \ud835\udefd 1 superscript \ud835\udc54 \u2032 \ud835\udefd superscript \u210e \u2032 \ud835\udefd \ud835\udc62 \\lim_{\\beta\\rightarrow 1}\\frac{g(\\beta)}{h(\\beta)}=\\lim_{\\beta\\rightarrow 1}\\frac{g^{\\prime}(\\beta)}{h^{\\prime}(\\beta)}=\\log u. (22)\n\nB.2 Proof of Proposition 2.3\n\nThe proof of Proposition 2.3 is similar to the one in \u00a7A, replacing the KL divergence by the Bregman divergence induced by , and using an additional bound. Let\n\nB \u03a9 \u03b1 \u200b ( p , q ) := \u03a9 \u03b1 \u200b ( p ) \u2212 \u03a9 \u03b1 \u200b ( q ) \u2212 \u27e8 \u2207 \u03a9 \u03b1 \u200b ( q ) , p \u2212 q \u27e9 assign subscript \ud835\udc35 subscript \u03a9 \ud835\udefc \ud835\udc5d \ud835\udc5e subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \u03a9 \ud835\udefc \ud835\udc5e \u2207 subscript \u03a9 \ud835\udefc \ud835\udc5e \ud835\udc5d \ud835\udc5e B_{\\Omega_{\\alpha}}(p,q):=\\Omega_{\\alpha}(p)-\\Omega_{\\alpha}(q)-\\langle\\nabla\\Omega_{\\alpha}(q),p-q\\rangle\n\nbe the (functional) Bregman divergence between distributions and induced by , and let\n\nq \u200b ( t ) = exp 2 \u2212 \u03b1 \u2061 ( f \u200b ( t ) \u2212 A \u03b1 \u200b ( f ) ) = [ 1 + ( \u03b1 \u2212 1 ) \u200b ( f \u200b ( t ) \u2212 A \u03b1 \u200b ( f ) ) ] + 1 \u03b1 \u2212 1 . \ud835\udc5e \ud835\udc61 subscript 2 \ud835\udefc \ud835\udc53 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udc53 superscript subscript delimited-[] 1 \ud835\udefc 1 \ud835\udc53 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udc53 1 \ud835\udefc 1 q(t)=\\exp_{2-\\alpha}(f(t)-A_{\\alpha}(f))=[1+(\\alpha-1)(f(t)-A_{\\alpha}(f))]_{+}^{\\frac{1}{\\alpha-1}}. Note that, from (6),\n\n( \u2207 q \u03a9 \u03b1 \u200b ( q ) ) \u200b ( t ) = q \u200b ( t ) \u03b1 \u2212 1 \u03b1 \u2212 1 . subscript \u2207 \ud835\udc5e subscript \u03a9 \ud835\udefc \ud835\udc5e \ud835\udc61 \ud835\udc5e superscript \ud835\udc61 \ud835\udefc 1 \ud835\udefc 1 \\left(\\nabla_{q}\\Omega_{\\alpha}(q)\\right)(t)=\\frac{q(t)^{\\alpha-1}}{\\alpha-1}. From the non-negativity of the Bregman divergence [49], we have, for any :\n\n0 0 \\displaystyle 0 \u2264 ( a ) superscript \ud835\udc4e \\displaystyle\\leq^{(a)} B \u03a9 \u03b1 \u200b ( p , q ) subscript \ud835\udc35 subscript \u03a9 \ud835\udefc \ud835\udc5d \ud835\udc5e \\displaystyle B_{\\Omega_{\\alpha}}(p,q) (23) = \\displaystyle= \u03a9 \u03b1 \u200b ( p ) \u2212 \u03a9 \u03b1 \u200b ( q ) \u2212 \u27e8 \u2207 \u03a9 \u03b1 \u200b ( q ) , p \u2212 q \u27e9 subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \u03a9 \ud835\udefc \ud835\udc5e \u2207 subscript \u03a9 \ud835\udefc \ud835\udc5e \ud835\udc5d \ud835\udc5e \\displaystyle\\Omega_{\\alpha}(p)-\\Omega_{\\alpha}(q)-\\langle\\nabla\\Omega_{\\alpha}(q),p-q\\rangle = \\displaystyle= \u03a9 \u03b1 \u200b ( p ) \u2212 \u03a9 \u03b1 \u200b ( q ) \u2212 \u222b S q \u200b ( t ) \u03b1 \u2212 1 \u03b1 \u2212 1 \u200b ( p \u200b ( t ) \u2212 q \u200b ( t ) ) subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \ud835\udc46 \ud835\udc5e superscript \ud835\udc61 \ud835\udefc 1 \ud835\udefc 1 \ud835\udc5d \ud835\udc61 \ud835\udc5e \ud835\udc61 \\displaystyle\\Omega_{\\alpha}(p)-\\Omega_{\\alpha}(q)-\\int_{S}\\frac{q(t)^{\\alpha-1}}{\\alpha-1}(p(t)-q(t)) = \\displaystyle= \u03a9 \u03b1 \u200b ( p ) \u2212 \u03a9 \u03b1 \u200b ( q ) \u2212 \ud835\udd3c p \u200b [ [ f \u200b ( t ) \u2212 A \u03b1 \u200b ( f ) + ( \u03b1 \u2212 1 ) \u2212 1 ] + ] \u23df \u2265 \ud835\udd3c p \u200b [ f \u200b ( t ) \u2212 A \u03b1 \u200b ( f ) + ( \u03b1 \u2212 1 ) \u2212 1 ] + 1 \u03b1 \u2212 1 \u200b \u222b S q \u200b ( t ) \u03b1 subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \u23df subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript delimited-[] \ud835\udc53 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udc53 superscript \ud835\udefc 1 1 absent subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udc53 superscript \ud835\udefc 1 1 1 \ud835\udefc 1 subscript \ud835\udc46 \ud835\udc5e superscript \ud835\udc61 \ud835\udefc \\displaystyle\\Omega_{\\alpha}(p)-\\Omega_{\\alpha}(q)-\\underbrace{\\mathbb{E}_{p}[[f(t)-A_{\\alpha}(f)+(\\alpha-1)^{-1}]_{+}]}_{\\geq\\mathbb{E}_{p}[f(t)-A_{\\alpha}(f)+(\\alpha-1)^{-1}]}+\\frac{1}{\\alpha-1}\\int_{S}q(t)^{\\alpha} \u2264 ( b ) superscript \ud835\udc4f \\displaystyle\\leq^{(b)} \u03a9 \u03b1 \u200b ( p ) \u2212 \u03a9 \u03b1 \u200b ( q ) \u2212 \ud835\udd3c p \u200b [ f \u200b ( t ) \u2212 A \u03b1 \u200b ( f ) + ( \u03b1 \u2212 1 ) \u2212 1 ] + 1 \u03b1 \u2212 1 \u200b \u222b S q \u200b ( t ) \u03b1 subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udc53 superscript \ud835\udefc 1 1 1 \ud835\udefc 1 subscript \ud835\udc46 \ud835\udc5e superscript \ud835\udc61 \ud835\udefc \\displaystyle\\Omega_{\\alpha}(p)-\\Omega_{\\alpha}(q)-\\mathbb{E}_{p}[f(t)-A_{\\alpha}(f)+(\\alpha-1)^{-1}]+\\frac{1}{\\alpha-1}\\int_{S}q(t)^{\\alpha} = \\displaystyle= \u03a9 \u03b1 \u200b ( p ) \u2212 \ud835\udd3c p \u200b [ f \u200b ( t ) ] \u2212 \u03a9 \u03b1 \u200b ( q ) + 1 \u03b1 \u2212 1 \u200b ( \u222b S q \u200b ( t ) \u03b1 \u2212 1 ) \u23df = \u03b1 \u200b \u03a9 \u03b1 \u200b ( q ) + A \u03b1 \u200b ( f ) subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \u23df 1 \ud835\udefc 1 subscript \ud835\udc46 \ud835\udc5e superscript \ud835\udc61 \ud835\udefc 1 absent \ud835\udefc subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \ud835\udc34 \ud835\udefc \ud835\udc53 \\displaystyle\\Omega_{\\alpha}(p)-\\mathbb{E}_{p}[f(t)]-\\Omega_{\\alpha}(q)+\\underbrace{\\frac{1}{\\alpha-1}\\left(\\int_{S}q(t)^{\\alpha}-1\\right)}_{=\\alpha\\Omega_{\\alpha}(q)}+A_{\\alpha}(f) = \\displaystyle= \u03a9 \u03b1 \u200b ( p ) \u2212 \ud835\udd3c p \u200b [ f \u200b ( t ) ] + ( \u03b1 \u2212 1 ) \u200b \u03a9 \u03b1 \u200b ( q ) + A \u03b1 \u200b ( f ) . subscript \u03a9 \ud835\udefc \ud835\udc5d subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 \ud835\udefc 1 subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \ud835\udc34 \ud835\udefc \ud835\udc53 \\displaystyle\\Omega_{\\alpha}(p)-\\mathbb{E}_{p}[f(t)]+(\\alpha-1)\\Omega_{\\alpha}(q)+A_{\\alpha}(f). Therefore, we have, for any ,\n\n\ud835\udd3c p \u200b [ f \u200b ( t ) ] \u2212 \u03a9 \u03b1 \u200b ( p ) \u2264 ( \u03b1 \u2212 1 ) \u200b \u03a9 \u03b1 \u200b ( q ) + A \u03b1 \u200b ( f ) , subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc53 \ud835\udc61 subscript \u03a9 \ud835\udefc \ud835\udc5d \ud835\udefc 1 subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \ud835\udc34 \ud835\udefc \ud835\udc53 \\mathbb{E}_{p}[f(t)]-\\Omega_{\\alpha}(p)\\leq(\\alpha-1)\\Omega_{\\alpha}(q)+A_{\\alpha}(f), (24)\n\nwith equality iff , which leads to zero Bregman divergence (i.e., a tight inequality ) and to (i.e., a tight inequality ). We can use the equality above to obtain an expression for the Fenchel conjugate (i.e., the value of the maximum in (1) and the right hand side in (24)):\n\n\u03a9 \u03b1 \u2217 \u200b ( f ) = ( \u03b1 \u2212 1 ) \u200b \u03a9 \u03b1 \u200b ( q ) + A \u03b1 \u200b ( f ) . superscript subscript \u03a9 \ud835\udefc \ud835\udc53 \ud835\udefc 1 subscript \u03a9 \ud835\udefc \ud835\udc5e subscript \ud835\udc34 \ud835\udefc \ud835\udc53 \\Omega_{\\alpha}^{*}(f)=(\\alpha-1)\\Omega_{\\alpha}(q)+A_{\\alpha}(f). (25)\n\nB.3 Normalizing function\n\nLet . The expression for in Prop. 2.3 is obtained by inverting (7), yielding , and integrating with respect to , leading to:\n\n\u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b A \u03b1 \u200b ( f ) subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc subscript \ud835\udc34 \ud835\udefc \ud835\udc53 \\displaystyle\\int_{S}p_{\\theta}(t)^{2-\\alpha}A_{\\alpha}(f) = \\displaystyle= \u222b S p \u200b ( t ) 2 \u2212 \u03b1 \u200b f \u200b ( t ) \u2212 \u222b S p \u200b ( t ) 2 \u2212 \u03b1 \u200b log 2 \u2212 \u03b1 \u2061 ( p \u200b ( t ) ) subscript \ud835\udc46 \ud835\udc5d superscript \ud835\udc61 2 \ud835\udefc \ud835\udc53 \ud835\udc61 subscript \ud835\udc46 \ud835\udc5d superscript \ud835\udc61 2 \ud835\udefc subscript 2 \ud835\udefc \ud835\udc5d \ud835\udc61 \\displaystyle\\int_{S}p(t)^{2-\\alpha}f(t)-\\int_{S}p(t)^{2-\\alpha}\\log_{2-\\alpha}(p(t)) (26) = \\displaystyle= \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b f \u200b ( t ) \u2212 \u222b S ( p \u200b ( t ) \u2212 p \u200b ( t ) 2 \u2212 \u03b1 ) \u03b1 \u2212 1 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc \ud835\udc53 \ud835\udc61 subscript \ud835\udc46 \ud835\udc5d \ud835\udc61 \ud835\udc5d superscript \ud835\udc61 2 \ud835\udefc \ud835\udefc 1 \\displaystyle\\int_{S}p_{\\theta}(t)^{2-\\alpha}f(t)-\\frac{\\int_{S}(p(t)-p(t)^{2-\\alpha})}{\\alpha-1} = \\displaystyle= \u222b S p \u200b ( t ) 2 \u2212 \u03b1 \u200b f \u200b ( t ) \u2212 1 \u03b1 \u2212 1 + \u222b S p \u200b ( t ) 2 \u2212 \u03b1 \u03b1 \u2212 1 , subscript \ud835\udc46 \ud835\udc5d superscript \ud835\udc61 2 \ud835\udefc \ud835\udc53 \ud835\udc61 1 \ud835\udefc 1 subscript \ud835\udc46 \ud835\udc5d superscript \ud835\udc61 2 \ud835\udefc \ud835\udefc 1 \\displaystyle\\int_{S}p(t)^{2-\\alpha}f(t)-\\frac{1}{\\alpha-1}+\\frac{\\int_{S}p(t)^{2-\\alpha}}{\\alpha-1},\n\nfrom which the desired expression follows. Appendix C Relation to the Tsallis Maxent Principle\n\nWe discuss here the relation between the -exponential family of distributions as presented in Prop. 2.3 and the distributions arising from the Tsallis maxent principle [13]. We put in perspective the related work in statistical physics [50, 10], information geometry [25, 18], and the discrete case presented in the machine learning literature [7, 8]. We start by noting that our parameter matches the used in prior machine learning literature related to the \u201c-entmax transformation\u201d [7, 8]. In the definition of Tsallis entropies (6), our corresponds to the entropic index defined by Tsallis [13]. However, our -exponential families correspond to the -exponential families as defined by Naudts [10], and to the -exponential families described by Ding and Vishwanathan [12] (which include the -Student distribution). The family of Amari\u2019s -divergences relates to this as [18, \u00a74.3]. These differences in notation have historical reasons, and they are explained by the different ways in which Tsallis entropies relate to -exponential families. In fact, the physics literature has defined -exponential distributions in two distinct ways, as we next describe. Note first that the -RPM in our Def. 2.1 is a generalization of the free energy variational principle, if we see as an energy function and the entropy scaled by a temperature. Let be the Tsallis -entropy. An equivalent constrained version of this problem is the maximum entropy (maxent) principle [51]:\n\nmax p \u2208 \u2133 + 1 \u200b ( S ) \u2212 \u03a9 \u03b1 ( p ) , s . t . \ud835\udd3c p [ \u03d5 ( t ) ] = b . \\max_{p\\in\\mathcal{M}_{+}^{1}(S)}-\\Omega_{\\alpha}(p),\\hskip 14.22636pt\\mathrm{s.t.}\\hskip 9.95863pt\\mathbb{E}_{p}[\\phi(t)]=b. (27)\n\nThe solution of this problem corresponds to a distribution in the -exponential family (7):\n\np \u22c6 \u200b ( t ) = exp 2 \u2212 \u03b1 \u2061 ( \u03b8 \u22a4 \u200b \u03d5 \u200b ( t ) \u2212 A \u03b1 \u200b ( \u03b8 ) ) , superscript \ud835\udc5d \u22c6 \ud835\udc61 subscript 2 \ud835\udefc superscript \ud835\udf03 top italic-\u03d5 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 p^{\\star}(t)=\\exp_{2-\\alpha}(\\theta^{\\top}\\phi(t)-A_{\\alpha}(\\theta)), (28)\n\nfor some Lagrange multiplier . However, this construction differs from the one by Tsallis [13] and others, who use escort distributions (Eq. 5) in the expectation constraints. Namely, instead of (27), they consider the problem:\n\nmax p \u2208 \u2133 + 1 \u200b ( S ) \u2212 \u03a9 \u03b1 ( p ) , s . t . \ud835\udd3c p ~ \u03b1 [ \u03d5 ( t ) ] = b . \\max_{p\\in\\mathcal{M}_{+}^{1}(S)}-\\Omega_{\\alpha}(p),\\hskip 14.22636pt\\mathrm{s.t.}\\hskip 9.95863pt\\mathbb{E}_{{\\color[rgb]{0,0,1}\\tilde{p}^{\\alpha}}}[\\phi(t)]=b. (29)\n\nThe solution of (29) is of the form\n\np \u22c6 \u200b ( t ) = B \u03b1 \u200b ( \u03b8 ) \u200b exp \u03b1 \u2061 ( \u03b8 \u22a4 \u200b ( \u03d5 \u200b ( t ) \u2212 b ) ) , superscript \ud835\udc5d \u22c6 \ud835\udc61 subscript \ud835\udc35 \ud835\udefc \ud835\udf03 subscript \ud835\udefc superscript \ud835\udf03 top italic-\u03d5 \ud835\udc61 \ud835\udc4f p^{\\star}(t)=B_{\\alpha}(\\theta)\\exp_{\\alpha}(\\theta^{\\top}(\\phi(t)-b)), (30)\n\nwhere is again a Lagrange multiplier.",
    "scattn-29": "This is derived, for example, in [50, Eq. 15]. There are two main differences between (28) and (30):\n\n\u2022\n\nWhile (28) involves the -exponential, (30) involves the -exponential. \u2022\n\nIn (28), the normalizing term is inside the -exponential. In (30), there is an normalizing factor outside the -exponential. Naturally, when , these two problems become equivalent, since an additive term inside the exponential is equivalent to a multiplicative term outside. However, this does not happen with -exponentials ( in general, for ), and therefore these two alternative paths lead to two different definitions of -exponential families. Unfortunately, both have been considered in the physics literature, under the same name, and this has been subject of debate. Quoting Naudts [10, \u00a71]:\n\n\u201cAn important question is then whether in the modification the normalization should stand in front of the deformed exponential function, or whether it should be included as inside. From the general formalism mentioned above it follows that the latter is the right way to go.\u201d\n\nThroughout our paper, we use the definition of [10, 25], equivalent to the maxent problem (27). Appendix D Proof of Proposition 2.3\n\nWe adapt the proof from Amari and Ohara [25, Theorem 5]. Note first that, for ,\n\n\u2207 \u03b8 p \u03b8 \u200b ( t ) subscript \u2207 \ud835\udf03 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 \\displaystyle\\nabla_{\\theta}p_{\\theta}(t) = \\displaystyle= \u2207 \u03b8 [ ( \u03b1 \u2212 1 ) ( \u03b8 \u22a4 \u03d5 ( t ) \u2212 A \u03b1 ( \u03b8 ) ) + 1 ] 1 / ( \u03b1 \u2212 1 ) \\displaystyle\\nabla_{\\theta}[(\\alpha-1)(\\theta^{\\top}\\phi(t)-A_{\\alpha}(\\theta))+1]^{1/(\\alpha-1)} (31) = \\displaystyle= [ ( \u03b1 \u2212 1 ) \u200b ( \u03b8 \u22a4 \u200b \u03d5 \u200b ( t ) \u2212 A \u03b1 \u200b ( \u03b8 ) ) + 1 ] ( 2 \u2212 \u03b1 ) / ( \u03b1 \u2212 1 ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) superscript delimited-[] \ud835\udefc 1 superscript \ud835\udf03 top italic-\u03d5 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 1 2 \ud835\udefc \ud835\udefc 1 italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 \\displaystyle[(\\alpha-1)(\\theta^{\\top}\\phi(t)-A_{\\alpha}(\\theta))+1]^{(2-\\alpha)/(\\alpha-1)}(\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)) = \\displaystyle= p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) , subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 \\displaystyle p_{\\theta}(t)^{2-\\alpha}(\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)),\n\nand\n\n\u2207 \u03b8 2 p \u03b8 \u200b ( t ) subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 \\displaystyle\\nabla^{2}_{\\theta}p_{\\theta}(t) = \\displaystyle= \u2207 \u03b8 p \u03b8 2 \u2212 \u03b1 \u200b ( t ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u22a4 \u2212 p \u03b8 2 \u2212 \u03b1 \u200b ( t ) \u200b \u2207 \u03b8 2 A \u03b1 \u200b ( \u03b8 ) subscript \u2207 \ud835\udf03 superscript subscript \ud835\udc5d \ud835\udf03 2 \ud835\udefc \ud835\udc61 superscript italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 top superscript subscript \ud835\udc5d \ud835\udf03 2 \ud835\udefc \ud835\udc61 subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 \\displaystyle\\nabla_{\\theta}p_{\\theta}^{2-\\alpha}(t)(\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta))^{\\top}-p_{\\theta}^{2-\\alpha}(t)\\nabla^{2}_{\\theta}A_{\\alpha}(\\theta) (32) = \\displaystyle= ( 2 \u2212 \u03b1 ) \u200b p \u03b8 1 \u2212 \u03b1 \u200b ( t ) \u200b \u2207 \u03b8 p \u03b8 \u200b ( t ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u22a4 \u2212 p \u03b8 2 \u2212 \u03b1 \u200b ( t ) \u200b \u2207 \u03b8 2 A \u03b1 \u200b ( \u03b8 ) 2 \ud835\udefc superscript subscript \ud835\udc5d \ud835\udf03 1 \ud835\udefc \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 superscript italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 top superscript subscript \ud835\udc5d \ud835\udf03 2 \ud835\udefc \ud835\udc61 subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 \\displaystyle(2-\\alpha)p_{\\theta}^{1-\\alpha}(t)\\nabla_{\\theta}p_{\\theta}(t)(\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta))^{\\top}-p_{\\theta}^{2-\\alpha}(t)\\nabla^{2}_{\\theta}A_{\\alpha}(\\theta) = \\displaystyle= ( 2 \u2212 \u03b1 ) \u200b p \u03b8 \u200b ( t ) 3 \u2212 2 \u200b \u03b1 \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u22a4 2 \ud835\udefc subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 3 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 superscript italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 top \\displaystyle(2-\\alpha)p_{\\theta}(t)^{3-2\\alpha}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}^{\\top} \u2212 p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b \u2207 \u03b8 2 A \u03b1 \u200b ( \u03b8 ) . subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 \\displaystyle-p_{\\theta}(t)^{2-\\alpha}\\nabla^{2}_{\\theta}A_{\\alpha}(\\theta). Therefore we have:\n\n0 = \u2207 \u03b8 \u222b S p \u03b8 \u200b ( t ) \u23df = 1 = \u222b S \u2207 \u03b8 p \u03b8 \u200b ( t ) = \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) , 0 subscript \u2207 \ud835\udf03 subscript \u23df subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 absent 1 subscript \ud835\udc46 subscript \u2207 \ud835\udf03 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 0=\\nabla_{\\theta}\\underbrace{\\int_{S}p_{\\theta}(t)}_{=1}=\\int_{S}\\nabla_{\\theta}p_{\\theta}(t)=\\int_{S}p_{\\theta}(t)^{2-\\alpha}(\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)), (33)\n\nfrom which we obtain\n\n\u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) = \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b \u03d5 \u200b ( t ) \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 . subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc \\nabla_{\\theta}A_{\\alpha}(\\theta)=\\frac{\\int_{S}p_{\\theta}(t)^{2-\\alpha}\\phi(t)}{\\int_{S}p_{\\theta}(t)^{2-\\alpha}}. (34)\n\nTo prove that is convex, we will show that its Hessian is positive semidefinite. Note that\n\n0 0 \\displaystyle 0 = \\displaystyle= \u2207 \u03b8 2 \u222b S p \u03b8 \u200b ( t ) \u23df = 1 = \u222b S \u2207 \u03b8 2 p \u03b8 \u200b ( t ) subscript superscript \u2207 2 \ud835\udf03 subscript \u23df subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 absent 1 subscript \ud835\udc46 subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 \\displaystyle\\nabla^{2}_{\\theta}\\underbrace{\\int_{S}p_{\\theta}(t)}_{=1}=\\int_{S}\\nabla^{2}_{\\theta}p_{\\theta}(t) (35) = \\displaystyle= \u222b S ( 2 \u2212 \u03b1 ) \u200b p \u03b8 \u200b ( t ) 3 \u2212 2 \u200b \u03b1 \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u22a4 \u2212 p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u200b \u2207 \u03b8 2 A \u03b1 \u200b ( \u03b8 ) subscript \ud835\udc46 2 \ud835\udefc subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 3 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 superscript italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 top subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 \\displaystyle\\int_{S}(2-\\alpha)p_{\\theta}(t)^{3-2\\alpha}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}^{\\top}-p_{\\theta}(t)^{2-\\alpha}\\nabla^{2}_{\\theta}A_{\\alpha}(\\theta) = \\displaystyle= ( 2 \u2212 \u03b1 ) \u200b \u222b S p \u03b8 \u200b ( t ) 3 \u2212 2 \u200b \u03b1 \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u22a4 2 \ud835\udefc subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 3 2 \ud835\udefc italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 superscript italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 top \\displaystyle(2-\\alpha)\\int_{S}p_{\\theta}(t)^{3-2\\alpha}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}^{\\top} \u2212 \u2207 \u03b8 2 A \u03b1 \u200b ( \u03b8 ) \u200b \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 , subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc \\displaystyle-\\nabla^{2}_{\\theta}A_{\\alpha}(\\theta)\\int_{S}p_{\\theta}(t)^{2-\\alpha},\n\nhence, for ,\n\n\u2207 \u03b8 2 A \u03b1 \u200b ( \u03b8 ) = ( 2 \u2212 \u03b1 ) \u200b \u222b S p \u03b8 \u200b ( t ) 3 \u2212 2 \u200b \u03b1 \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u22a4 \u23de \u2ab0 0 \u222b S p \u03b8 \u200b ( t ) 2 \u2212 \u03b1 \u2ab0 0 , subscript superscript \u2207 2 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 2 \ud835\udefc subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 3 2 \ud835\udefc superscript \u23de italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 superscript italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 top succeeds-or-equals absent 0 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc61 2 \ud835\udefc succeeds-or-equals 0 \\nabla^{2}_{\\theta}A_{\\alpha}(\\theta)=\\frac{(2-\\alpha)\\int_{S}p_{\\theta}(t)^{3-2\\alpha}\\overbrace{\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}\\bigl{(}\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta)\\bigr{)}^{\\top}}^{\\succeq 0}}{\\int_{S}p_{\\theta}(t)^{2-\\alpha}}\\succeq 0, (36)\n\nwhere we used the fact that for and that integrals of positive semidefinite functions and positive semidefinite. Appendix E Normalization Constants for Continuous Sparsemax Distributions\n\nE.1 Truncated parabola\n\nLet as in (10).",
    "scattn-30": "Let us determine the constant that ensures this distribution normalizes to 1. Note that does not depend on the location parameter , hence we can assume without loss of generality. We must have and , hence , which finally gives:\n\n\u03bb = \u2212 1 2 \u200b ( 3 2 \u200b \u03c3 ) 2 / 3 . \ud835\udf06 1 2 superscript 3 2 \ud835\udf0e 2 3 \\lambda=-\\frac{1}{2}\\left(\\frac{3}{2\\sigma}\\right)^{2/3}. (37)\n\nE.2 Multivariate truncated paraboloid\n\nLet as in (11). Let us determine the constant that ensures this distribution normalizes to 1, where we assume again without loss of generality. To obtain , we start by invoking the formula for computing the volume of an ellipsoid defined by the equation :\n\nV ell \u200b ( \u03a3 ) = \u03c0 n / 2 \u0393 \u200b ( n / 2 + 1 ) \u200b det \u200b ( \u03a3 ) 1 / 2 , subscript \ud835\udc49 ell \u03a3 superscript \ud835\udf0b \ud835\udc5b 2 \u0393 \ud835\udc5b 2 1 det superscript \u03a3 1 2 V_{\\mathrm{ell}}(\\Sigma)=\\frac{\\pi^{n/2}}{\\Gamma(n/2+1)}\\mathrm{det}(\\Sigma)^{1/2}, (38)\n\nwhere is the Gamma function. Since each slice of a paraboloid is an ellipsoid, we can apply Cavalieri\u2019s principle to obtain the volume of a paraboloid of height as follows:\n\nV par \u200b ( h ) subscript \ud835\udc49 par \u210e \\displaystyle V_{\\mathrm{par}}(h) = \\displaystyle= \u222b 0 h V ell \u200b ( 2 \u200b \u03a3 \u200b y ) \u200b \ud835\udc51 y = ( 2 \u200b \u03c0 ) n / 2 \u200b det \u200b ( \u03a3 ) 1 / 2 \u0393 \u200b ( n 2 + 1 ) \u200b \u222b 0 h y n 2 \u200b \ud835\udc51 y superscript subscript 0 \u210e subscript \ud835\udc49 ell 2 \u03a3 \ud835\udc66 differential-d \ud835\udc66 superscript 2 \ud835\udf0b \ud835\udc5b 2 det superscript \u03a3 1 2 \u0393 \ud835\udc5b 2 1 superscript subscript 0 \u210e superscript \ud835\udc66 \ud835\udc5b 2 differential-d \ud835\udc66 \\displaystyle\\int_{0}^{h}V_{\\mathrm{ell}}(2\\Sigma y)dy\\,\\,=\\,\\,\\frac{(2\\pi)^{n/2}\\mathrm{det}(\\Sigma)^{1/2}}{\\Gamma(\\frac{n}{2}+1)}\\int_{0}^{h}y^{\\frac{n}{2}}dy (39) = \\displaystyle= ( 2 \u200b \u03c0 ) n / 2 \u200b det \u200b ( \u03a3 ) 1 / 2 ( n 2 + 1 ) \u200b \u0393 \u200b ( n 2 + 1 ) \u200b h n 2 + 1 superscript 2 \ud835\udf0b \ud835\udc5b 2 det superscript \u03a3 1 2 \ud835\udc5b 2 1 \u0393 \ud835\udc5b 2 1 superscript \u210e \ud835\udc5b 2 1 \\displaystyle\\frac{(2\\pi)^{n/2}\\mathrm{det}(\\Sigma)^{1/2}}{(\\frac{n}{2}+1)\\Gamma(\\frac{n}{2}+1)}h^{\\frac{n}{2}+1} = \\displaystyle= ( 2 \u200b \u03c0 ) n \u200b det \u200b ( \u03a3 ) \u0393 \u200b ( n 2 + 2 ) \u200b h n 2 + 1 . superscript 2 \ud835\udf0b \ud835\udc5b det \u03a3 \u0393 \ud835\udc5b 2 2 superscript \u210e \ud835\udc5b 2 1 \\displaystyle\\frac{\\sqrt{(2\\pi)^{n}\\mathrm{det}(\\Sigma)}}{\\Gamma(\\frac{n}{2}+2)}h^{\\frac{n}{2}+1}. Equating the volume to 1, we obtain as:\n\n\u03bb = \u2212 ( \u0393 \u200b ( n 2 + 2 ) ( 2 \u200b \u03c0 ) n \u200b det \u200b ( \u03a3 ) ) 2 2 + n . \ud835\udf06 superscript \u0393 \ud835\udc5b 2 2 superscript 2 \ud835\udf0b \ud835\udc5b det \u03a3 2 2 \ud835\udc5b \\lambda=-\\left(\\frac{\\Gamma(\\frac{n}{2}+2)}{\\sqrt{(2\\pi)^{n}\\mathrm{det}(\\Sigma)}}\\right)^{\\frac{2}{2+n}}. (40)\n\nE.3 Triangular\n\nLet as in (12). Let us determine the constant that ensures this distribution normalizes to 1. Assuming again without loss of generality, we must have and , hence , which finally gives . E.4 Location-scale families\n\nWe first show that is the solution of the equation . From symmetry around , we must have\n\n1 2 = \u222b \u03bc \u03bc + a \u200b \u03c3 ( 1 \u03c3 \u200b g \u2032 \u200b ( a ) \u2212 1 \u03c3 \u200b g \u2032 \u200b ( t \u2212 \u03bc \u03c3 ) ) \u200b \ud835\udc51 t = \u222b 0 a ( g \u2032 \u200b ( a ) \u2212 g \u2032 \u200b ( s ) ) \u200b \ud835\udc51 s = a \u200b g \u2032 \u200b ( a ) \u2212 g \u200b ( a ) + g \u200b ( 0 ) , 1 2 superscript subscript \ud835\udf07 \ud835\udf07 \ud835\udc4e \ud835\udf0e 1 \ud835\udf0e superscript \ud835\udc54 \u2032 \ud835\udc4e 1 \ud835\udf0e superscript \ud835\udc54 \u2032 \ud835\udc61 \ud835\udf07 \ud835\udf0e differential-d \ud835\udc61 superscript subscript 0 \ud835\udc4e superscript \ud835\udc54 \u2032 \ud835\udc4e superscript \ud835\udc54 \u2032 \ud835\udc60 differential-d \ud835\udc60 \ud835\udc4e superscript \ud835\udc54 \u2032 \ud835\udc4e \ud835\udc54 \ud835\udc4e \ud835\udc54 0 \\displaystyle\\frac{1}{2}=\\int_{\\mu}^{\\mu+a\\sigma}\\left(\\frac{1}{\\sigma}g^{\\prime}(a)-\\frac{1}{\\sigma}g^{\\prime}\\left(\\frac{t-\\mu}{\\sigma}\\right)\\right)dt=\\int_{0}^{a}\\left(g^{\\prime}(a)-g^{\\prime}(s)\\right)ds=ag^{\\prime}(a)-g(a)+g(0), (41)\n\nwhere we made a variable substitution , which proves the desired result. Now we show that a solution always exists if is strongly convex, i.e., if there is some such that for any . Let . We want to show that the equation has a solution. Since is continuously differentiable, is continuous.",
    "scattn-31": "From the strong convexity of , we have that for any , which implies that . Therefore, since , we have by the intermediate value theorem that there must be some such that . Appendix F Proof of Proposition 3.2\n\nWe have\n\n\u2207 \u03b8 \ud835\udd3c p \u200b [ \u03c8 i \u200b ( t ) ] subscript \u2207 \ud835\udf03 subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc56 \ud835\udc61 \\displaystyle\\nabla_{\\theta}\\mathbb{E}_{p}[\\psi_{i}(t)] = \\displaystyle= \u2207 \u03b8 \u200b \u222b S p \u03b8 \u200b ( t ) \u200b \u03c8 i \u200b ( t ) = \u222b S \u2207 \u03b8 p \u03b8 \u200b ( t ) \u200b \u03c8 i \u200b ( t ) subscript \u2207 \ud835\udf03 subscript \ud835\udc46 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 subscript \ud835\udf13 \ud835\udc56 \ud835\udc61 subscript \ud835\udc46 subscript \u2207 \ud835\udf03 subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 subscript \ud835\udf13 \ud835\udc56 \ud835\udc61 \\displaystyle\\nabla_{\\theta}\\int_{S}p_{\\theta}(t)\\psi_{i}(t)=\\int_{S}\\nabla_{\\theta}p_{\\theta}(t)\\psi_{i}(t) (42) = \\displaystyle= \u222b S p \u03b8 2 \u2212 \u03b1 \u200b ( t ) \u200b \u2207 \u03b8 log 2 \u2212 \u03b1 \u2061 ( p \u03b8 \u200b ( t ) ) \u200b \u03c8 i \u200b ( t ) subscript \ud835\udc46 superscript subscript \ud835\udc5d \ud835\udf03 2 \ud835\udefc \ud835\udc61 subscript \u2207 \ud835\udf03 subscript 2 \ud835\udefc subscript \ud835\udc5d \ud835\udf03 \ud835\udc61 subscript \ud835\udf13 \ud835\udc56 \ud835\udc61 \\displaystyle\\int_{S}p_{\\theta}^{2-\\alpha}(t)\\nabla_{\\theta}\\log_{2-\\alpha}(p_{\\theta}(t))\\psi_{i}(t) = \\displaystyle= \u222b S p \u03b8 2 \u2212 \u03b1 \u200b ( t ) \u200b \u2207 \u03b8 ( \u03b8 \u22a4 \u200b \u03d5 \u200b ( t ) \u2212 A \u03b1 \u200b ( \u03b8 ) ) \u2061 \u03c8 i \u200b ( t ) subscript \ud835\udc46 superscript subscript \ud835\udc5d \ud835\udf03 2 \ud835\udefc \ud835\udc61 subscript \u2207 \ud835\udf03 superscript \ud835\udf03 top italic-\u03d5 \ud835\udc61 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 subscript \ud835\udf13 \ud835\udc56 \ud835\udc61 \\displaystyle\\int_{S}p_{\\theta}^{2-\\alpha}(t)\\nabla_{\\theta}(\\theta^{\\top}\\phi(t)-A_{\\alpha}(\\theta))\\psi_{i}(t) = \\displaystyle= \u222b S p \u03b8 2 \u2212 \u03b1 \u200b ( t ) \u200b ( \u03d5 \u200b ( t ) \u2212 \u2207 \u03b8 A \u03b1 \u200b ( \u03b8 ) ) \u200b \u03c8 i \u200b ( t ) .",
    "scattn-32": "subscript \ud835\udc46 superscript subscript \ud835\udc5d \ud835\udf03 2 \ud835\udefc \ud835\udc61 italic-\u03d5 \ud835\udc61 subscript \u2207 \ud835\udf03 subscript \ud835\udc34 \ud835\udefc \ud835\udf03 subscript \ud835\udf13 \ud835\udc56 \ud835\udc61 \\displaystyle\\int_{S}p_{\\theta}^{2-\\alpha}(t)(\\phi(t)-\\nabla_{\\theta}A_{\\alpha}(\\theta))\\psi_{i}(t). Using the expression for from Proposition 2.3 yields the desired result. Appendix G Continuous Attention with Gaussian RBFs\n\nWe derive expressions for the evaluation and gradient computation of continuous attention mechanisms where are Gaussian radial basis functions, both for the softmax () and sparsemax () cases. For softmax, we show closed-form expressions for any number of dimensions (including the 1D and 2D cases). For sparsemax, we derive closed-form expressions for the 1D case, and we reduce the 2D case to a univariate integral on an interval, easy to compute numerically. This makes it possible to plug both continuous attention mechanisms in neural networks and learn them end-to-end with the gradient backpropagation algorithm. G.1 Continuous softmax ()\n\nWe derive expressions for continuous softmax for multivariate Gaussians in . This includes the 1D and 2D cases, where . If , for , the distribution , with , is a multivariate Gaussian where the mean and the covariance matrix are related to the canonical parameters as . We derive closed form expressions for the attention mechanism output in (15) and for its Jacobian in (18), when are Gaussian RBFs, i.e., each is of the form . Forward pass. Each coordinate of the attention mechanism output becomes the integral of a product of Gaussians,\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] = \u222b \u211d D \ud835\udca9 \u200b ( t ; \u03bc , \u03a3 ) \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) . subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript superscript \u211d \ud835\udc37 \ud835\udca9 \ud835\udc61 \ud835\udf07 \u03a3 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\mathbb{E}_{p}[\\psi_{j}(t)]=\\int_{\\mathbb{R}^{D}}\\mathcal{N}(t;\\mu,\\Sigma)\\mathcal{N}(t;\\mu_{j},\\Sigma_{j}). (43)\n\nWe use the fact that the product of two Gaussians is a scaled Gaussian:\n\n\ud835\udca9 \u200b ( t ; \u03bc , \u03a3 ) \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) = s ~ \u200b \ud835\udca9 \u200b ( t ; \u03bc ~ , \u03a3 ~ ) , \ud835\udca9 \ud835\udc61 \ud835\udf07 \u03a3 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 ~ \ud835\udc60 \ud835\udca9 \ud835\udc61 ~ \ud835\udf07 ~ \u03a3 \\mathcal{N}(t;\\mu,\\Sigma)\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})=\\tilde{s}\\mathcal{N}(t;\\tilde{\\mu},\\tilde{\\Sigma}), (44)\n\nwhere\n\ns ~ = \ud835\udca9 \u200b ( \u03bc ; \u03bc j , \u03a3 + \u03a3 j ) , \u03a3 ~ = ( \u03a3 \u2212 1 + \u03a3 j \u2212 1 ) \u2212 1 , \u03bc ~ = \u03a3 ~ \u200b ( \u03a3 \u2212 1 \u200b \u03bc + \u03a3 j \u2212 1 \u200b \u03bc j ) . formulae-sequence ~ \ud835\udc60 \ud835\udca9 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \u03a3 subscript \u03a3 \ud835\udc57 formulae-sequence ~ \u03a3 superscript superscript \u03a3 1 superscript subscript \u03a3 \ud835\udc57 1 1 ~ \ud835\udf07 ~ \u03a3 superscript \u03a3 1 \ud835\udf07 superscript subscript \u03a3 \ud835\udc57 1 subscript \ud835\udf07 \ud835\udc57 \\tilde{s}=\\mathcal{N}(\\mu;\\mu_{j},\\Sigma+\\Sigma_{j}),\\qquad\\tilde{\\Sigma}=(\\Sigma^{-1}+\\Sigma_{j}^{-1})^{-1},\\qquad\\tilde{\\mu}=\\tilde{\\Sigma}(\\Sigma^{-1}\\mu+\\Sigma_{j}^{-1}\\mu_{j}). (45)\n\nTherefore, the forward pass can be computed as:\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] = s ~ \u200b \u222b \u211d D \ud835\udca9 \u200b ( t ; \u03bc ~ , \u03a3 ~ ) = s ~ = \ud835\udca9 \u200b ( \u03bc ; \u03bc j , \u03a3 + \u03a3 j ) . subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 ~ \ud835\udc60 subscript superscript \u211d \ud835\udc37 \ud835\udca9 \ud835\udc61 ~ \ud835\udf07 ~ \u03a3 ~ \ud835\udc60 \ud835\udca9 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \u03a3 subscript \u03a3 \ud835\udc57 \\begin{split}\\mathbb{E}_{p}[\\psi_{j}(t)]&=\\tilde{s}\\int_{\\mathbb{R}^{D}}\\mathcal{N}(t;\\tilde{\\mu},\\tilde{\\Sigma})=\\tilde{s}\\\\\n&=\\mathcal{N}(\\mu;\\mu_{j},\\Sigma+\\Sigma_{j}).\\end{split} (46)\n\nBackward pass. To compute the backward pass, we have that each row of the Jacobian becomes a first or second moment under the resulting Gaussian:\n\ncov p , 1 \u200b ( t , \u03c8 j \u200b ( t ) ) = \ud835\udd3c p \u200b [ t \u200b \u03c8 j \u200b ( t ) ] \u2212 \ud835\udd3c p \u200b [ t ] \u200b \ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] = \u222b \u211d D t \u200b \ud835\udca9 \u200b ( t ; \u03bc , \u03a3 ) \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) \u2212 s ~ \u200b \u03bc = s ~ \u200b \u222b \u211d D t \u200b \ud835\udca9 \u200b ( t ; \u03bc ~ , \u03a3 ~ ) \u2212 s ~ \u200b \u03bc = s ~ \u200b ( \u03bc ~ \u2212 \u03bc ) , subscript cov \ud835\udc5d 1 \ud835\udc61 subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc61 subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc61 subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript superscript \u211d \ud835\udc37 \ud835\udc61 \ud835\udca9 \ud835\udc61 \ud835\udf07 \u03a3 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 ~ \ud835\udc60 \ud835\udf07 ~ \ud835\udc60 subscript superscript \u211d \ud835\udc37 \ud835\udc61 \ud835\udca9 \ud835\udc61 ~ \ud835\udf07 ~ \u03a3 ~ \ud835\udc60 \ud835\udf07 ~ \ud835\udc60 ~ \ud835\udf07 \ud835\udf07 \\begin{split}\\mathrm{cov}_{p,1}(t,\\psi_{j}(t))&=\\mathbb{E}_{p}[t\\psi_{j}(t)]-\\mathbb{E}_{p}[t]\\mathbb{E}_{p}[\\psi_{j}(t)]\\\\\n&=\\int_{\\mathbb{R}^{D}}t\\mathcal{N}(t;\\mu,\\Sigma)\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})-\\tilde{s}\\mu\\\\\n&=\\tilde{s}\\int_{\\mathbb{R}^{D}}t\\mathcal{N}(t;\\tilde{\\mu},\\tilde{\\Sigma})-\\tilde{s}\\mu\\\\\n&=\\tilde{s}(\\tilde{\\mu}-\\mu),\\end{split} (47)\n\nand, noting that ,\n\ncov p , 1 \u200b ( t \u200b t \u22a4 , \u03c8 j \u200b ( t ) ) = \ud835\udd3c p \u200b [ t \u200b t \u22a4 \u200b \u03c8 j \u200b ( t ) ] \u2212 \ud835\udd3c p \u200b [ t \u200b t \u22a4 ] \u200b \ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] = \u222b \u211d D t \u200b t \u22a4 \u200b \ud835\udca9 \u200b ( t ; \u03bc , \u03a3 ) \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) \u2212 s ~ \u200b ( \u03a3 + \u03bc \u200b \u03bc \u22a4 ) = s ~ \u200b \u222b \u211d D t \u200b t \u22a4 \u200b \ud835\udca9 \u200b ( t ; \u03bc ~ , \u03a3 ~ ) \u2212 s ~ \u200b ( \u03a3 + \u03bc \u200b \u03bc \u22a4 ) = s ~ \u200b ( \u03a3 ~ + \u03bc ~ \u200b \u03bc ~ \u22a4 ) \u2212 s ~ \u200b ( \u03a3 + \u03bc \u200b \u03bc \u22a4 ) = s ~ \u200b ( \u03a3 ~ + \u03bc ~ \u200b \u03bc ~ \u22a4 \u2212 \u03a3 \u2212 \u03bc \u200b \u03bc \u22a4 ) . subscript cov \ud835\udc5d 1 \ud835\udc61 superscript \ud835\udc61 top subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc61 superscript \ud835\udc61 top subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript \ud835\udd3c \ud835\udc5d delimited-[] \ud835\udc61 superscript \ud835\udc61 top subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript superscript \u211d \ud835\udc37 \ud835\udc61 superscript \ud835\udc61 top \ud835\udca9 \ud835\udc61 \ud835\udf07 \u03a3 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 ~ \ud835\udc60 \u03a3 \ud835\udf07 superscript \ud835\udf07 top ~ \ud835\udc60 subscript superscript \u211d \ud835\udc37 \ud835\udc61 superscript \ud835\udc61 top \ud835\udca9 \ud835\udc61 ~ \ud835\udf07 ~ \u03a3 ~ \ud835\udc60 \u03a3 \ud835\udf07 superscript \ud835\udf07 top ~ \ud835\udc60 ~ \u03a3 ~ \ud835\udf07 superscript ~ \ud835\udf07 top ~ \ud835\udc60 \u03a3 \ud835\udf07 superscript \ud835\udf07 top ~ \ud835\udc60 ~ \u03a3 ~ \ud835\udf07 superscript ~ \ud835\udf07 top \u03a3 \ud835\udf07 superscript \ud835\udf07 top \\begin{split}\\mathrm{cov}_{p,1}(tt^{\\top},\\psi_{j}(t))&=\\mathbb{E}_{p}[tt^{\\top}\\psi_{j}(t)]-\\mathbb{E}_{p}[tt^{\\top}]\\mathbb{E}_{p}[\\psi_{j}(t)]\\\\\n&=\\int_{\\mathbb{R}^{D}}tt^{\\top}\\mathcal{N}(t;\\mu,\\Sigma)\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})-\\tilde{s}(\\Sigma+\\mu\\mu^{\\top})\\\\\n&=\\tilde{s}\\int_{\\mathbb{R}^{D}}tt^{\\top}\\mathcal{N}(t;\\tilde{\\mu},\\tilde{\\Sigma})-\\tilde{s}(\\Sigma+\\mu\\mu^{\\top})\\\\\n&=\\tilde{s}(\\tilde{\\Sigma}+\\tilde{\\mu}\\tilde{\\mu}^{\\top})-\\tilde{s}(\\Sigma+\\mu\\mu^{\\top})\\\\\n&=\\tilde{s}(\\tilde{\\Sigma}+\\tilde{\\mu}\\tilde{\\mu}^{\\top}-\\Sigma-\\mu\\mu^{\\top}).\\end{split} (48)\n\nG.2 Continuous sparsemax in 1D (, )\n\nWith , the distribution , with , becomes a truncated parabola where and are related to the canonical parameters as above, i.e., . We derive closed form expressions for the attention mechanism output in (15) and its Jacobian in (18) when and Gaussian RBFs, i.e., each is of the form . Forward pass. Each coordinate of the attention mechanism output becomes:\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 \\displaystyle\\mathbb{E}_{p}[\\psi_{j}(t)] = \\displaystyle= \u222b \u03bc \u2212 a \u03bc + a ( \u2212 \u03bb \u2212 ( t \u2212 \u03bc ) 2 2 \u200b \u03c3 2 ) \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03c3 j 2 ) superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e \ud835\udf06 superscript \ud835\udc61 \ud835\udf07 2 2 superscript \ud835\udf0e 2 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 superscript subscript \ud835\udf0e \ud835\udc57 2 \\displaystyle\\int_{\\mu-a}^{\\mu+a}\\left(-\\lambda-\\frac{(t-\\mu)^{2}}{2\\sigma^{2}}\\right)\\mathcal{N}(t;\\mu_{j},\\sigma_{j}^{2}) (49) = \\displaystyle= \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j 1 \u03c3 j \u200b ( \u2212 \u03bb \u2212 ( \u03c3 j \u200b t + \u03bc j \u2212 \u03bc ) 2 2 \u200b \u03c3 2 ) \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) \u200b \ud835\udc51 s , superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 1 subscript \ud835\udf0e \ud835\udc57 \ud835\udf06 superscript subscript \ud835\udf0e \ud835\udc57 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 \ud835\udf07 2 2 superscript \ud835\udf0e 2 \ud835\udca9 \ud835\udc60 0 1 differential-d \ud835\udc60 \\displaystyle\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}\\frac{1}{\\sigma_{j}}\\left(-\\lambda-\\frac{(\\sigma_{j}t+\\mu_{j}-\\mu)^{2}}{2\\sigma^{2}}\\right)\\mathcal{N}(s;0,1)ds,\n\nwhere and , as stated in (37), and we made the substitution . We use the fact that, for any such that :\n\n\u222b u v \ud835\udca9 \u200b ( t ; 0 , 1 ) superscript subscript \ud835\udc62 \ud835\udc63 \ud835\udca9 \ud835\udc61 0 1 \\displaystyle\\int_{u}^{v}\\mathcal{N}(t;0,1) = \\displaystyle= 1 2 \u200b ( erf \u200b ( v 2 ) \u2212 erf \u200b ( u 2 ) ) , 1 2 erf \ud835\udc63 2 erf \ud835\udc62 2 \\displaystyle\\frac{1}{2}\\left(\\mathrm{erf}\\left(\\frac{v}{\\sqrt{2}}\\right)-\\mathrm{erf}\\left(\\frac{u}{\\sqrt{2}}\\right)\\right), \u222b u v t \u200b \ud835\udca9 \u200b ( t ; 0 , 1 ) superscript subscript \ud835\udc62 \ud835\udc63 \ud835\udc61 \ud835\udca9 \ud835\udc61 0 1 \\displaystyle\\int_{u}^{v}t\\mathcal{N}(t;0,1) = \\displaystyle= \u2212 \ud835\udca9 \u200b ( v ; 0 , 1 ) + \ud835\udca9 \u200b ( u ; 0 , 1 ) , \ud835\udca9 \ud835\udc63 0 1 \ud835\udca9 \ud835\udc62 0 1 \\displaystyle-\\mathcal{N}(v;0,1)+\\mathcal{N}(u;0,1), \u222b u v t 2 \u200b \ud835\udca9 \u200b ( t ; 0 , 1 ) superscript subscript \ud835\udc62 \ud835\udc63 superscript \ud835\udc61 2 \ud835\udca9 \ud835\udc61 0 1 \\displaystyle\\int_{u}^{v}t^{2}\\mathcal{N}(t;0,1) = \\displaystyle= 1 2 \u200b ( erf \u200b ( v 2 ) \u2212 erf \u200b ( u 2 ) ) \u2212 v \u200b \ud835\udca9 \u200b ( v ; 0 , 1 ) + u \u200b \ud835\udca9 \u200b ( u ; 0 , 1 ) , 1 2 erf \ud835\udc63 2 erf \ud835\udc62 2 \ud835\udc63 \ud835\udca9 \ud835\udc63 0 1 \ud835\udc62 \ud835\udca9 \ud835\udc62 0 1 \\displaystyle\\frac{1}{2}\\left(\\mathrm{erf}\\left(\\frac{v}{\\sqrt{2}}\\right)-\\mathrm{erf}\\left(\\frac{u}{\\sqrt{2}}\\right)\\right)-v\\mathcal{N}(v;0,1)+u\\mathcal{N}(u;0,1), (50)\n\nfrom which the expectation (49) can be computed directly. Backward pass. Since , we have from (17) and (G.2) that each row of the Jacobian becomes:\n\ncov p , 2 \u200b ( t , \u03c8 j \u200b ( t ) ) = subscript cov \ud835\udc5d 2 \ud835\udc61 subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 absent \\displaystyle\\mathrm{cov}_{p,2}(t,\\psi_{j}(t))= (51) \u222b \u03bc \u2212 a \u03bc + a t \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03c3 j 2 ) \u2212 1 2 \u200b a \u200b ( \u222b \u03bc \u2212 a \u03bc + a t ) \u200b ( \u222b \u03bc \u2212 a \u03bc + a \ud835\udca9 \u200b ( t ; \u03bc j , \u03c3 j 2 ) ) superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e \ud835\udc61 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 superscript subscript \ud835\udf0e \ud835\udc57 2 1 2 \ud835\udc4e superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e \ud835\udc61 superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 superscript subscript \ud835\udf0e \ud835\udc57 2 \\displaystyle\\int_{\\mu-a}^{\\mu+a}t\\mathcal{N}(t;\\mu_{j},\\sigma_{j}^{2})-\\frac{1}{2a}\\left(\\int_{\\mu-a}^{\\mu+a}t\\right)\\left(\\int_{\\mu-a}^{\\mu+a}\\mathcal{N}(t;\\mu_{j},\\sigma_{j}^{2})\\right) = \\displaystyle= \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j ( \u03bc j + \u03c3 j \u200b s ) \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) \u2212 1 2 \u200b a \u200b ( ( \u03bc + a ) 2 2 \u2212 ( \u03bc \u2212 a ) 2 2 ) \u23df = \u03bc \u200b ( \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j \ud835\udca9 \u200b ( s ; 0 , 1 ) ) superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 subscript \ud835\udf07 \ud835\udc57 subscript \ud835\udf0e \ud835\udc57 \ud835\udc60 \ud835\udca9 \ud835\udc60 0 1 subscript \u23df 1 2 \ud835\udc4e superscript \ud835\udf07 \ud835\udc4e 2 2 superscript \ud835\udf07 \ud835\udc4e 2 2 absent \ud835\udf07 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udca9 \ud835\udc60 0 1 \\displaystyle\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}(\\mu_{j}+\\sigma_{j}s)\\mathcal{N}(s;0,1)-\\underbrace{\\frac{1}{2a}\\left(\\frac{(\\mu+a)^{2}}{2}-\\frac{(\\mu-a)^{2}}{2}\\right)}_{=\\mu}\\left(\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}\\mathcal{N}(s;0,1)\\right) = \\displaystyle= ( \u03bc j \u2212 \u03bc ) \u200b \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j \ud835\udca9 \u200b ( s ; 0 , 1 ) + \u03c3 j \u200b \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j s \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) subscript \ud835\udf07 \ud835\udc57 \ud835\udf07 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udca9 \ud835\udc60 0 1 subscript \ud835\udf0e \ud835\udc57 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udc60 \ud835\udca9 \ud835\udc60 0 1 \\displaystyle(\\mu_{j}-\\mu)\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}\\mathcal{N}(s;0,1)+\\sigma_{j}\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}s\\mathcal{N}(s;0,1) = \\displaystyle= \u03bc j \u2212 \u03bc 2 \u200b ( erf \u200b ( \u03bc \u2212 \u03bc j + a 2 \u200b \u03c3 j ) \u2212 erf \u200b ( \u03bc \u2212 \u03bc j \u2212 a 2 \u200b \u03c3 j ) ) subscript \ud835\udf07 \ud835\udc57 \ud835\udf07 2 erf \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e 2 subscript \ud835\udf0e \ud835\udc57 erf \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e 2 subscript \ud835\udf0e \ud835\udc57 \\displaystyle\\frac{\\mu_{j}-\\mu}{2}\\left(\\mathrm{erf}\\left(\\frac{\\mu-\\mu_{j}+a}{\\sqrt{2}\\sigma_{j}}\\right)-\\mathrm{erf}\\left(\\frac{\\mu-\\mu_{j}-a}{\\sqrt{2}\\sigma_{j}}\\right)\\right) \u2212 \u03c3 j \u200b ( \ud835\udca9 \u200b ( \u03bc \u2212 \u03bc j + a \u03c3 j ; 0 , 1 ) \u2212 \ud835\udca9 \u200b ( \u03bc \u2212 \u03bc j \u2212 a \u03c3 j ; 0 , 1 ) ) , subscript \ud835\udf0e \ud835\udc57 \ud835\udca9 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 0 1 \ud835\udca9 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 0 1 \\displaystyle-\\sigma_{j}\\left(\\mathcal{N}\\left(\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}};0,1\\right)-\\mathcal{N}\\left(\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}};0,1\\right)\\right),\n\nand\n\ncov p , 2 \u200b ( t 2 , \u03c8 j \u200b ( t ) ) = subscript cov \ud835\udc5d 2 superscript \ud835\udc61 2 subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 absent \\displaystyle\\mathrm{cov}_{p,2}(t^{2},\\psi_{j}(t))= (52) \u222b \u03bc \u2212 a \u03bc + a t 2 \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03c3 j 2 ) \u2212 1 2 \u200b a \u200b ( \u222b \u03bc \u2212 a \u03bc + a t 2 ) \u200b ( \u222b \u03bc \u2212 a \u03bc + a \ud835\udca9 \u200b ( t ; \u03bc j , \u03c3 j 2 ) ) superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e superscript \ud835\udc61 2 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 superscript subscript \ud835\udf0e \ud835\udc57 2 1 2 \ud835\udc4e superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e superscript \ud835\udc61 2 superscript subscript \ud835\udf07 \ud835\udc4e \ud835\udf07 \ud835\udc4e \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 superscript subscript \ud835\udf0e \ud835\udc57 2 \\displaystyle\\int_{\\mu-a}^{\\mu+a}t^{2}\\mathcal{N}(t;\\mu_{j},\\sigma_{j}^{2})-\\frac{1}{2a}\\left(\\int_{\\mu-a}^{\\mu+a}t^{2}\\right)\\left(\\int_{\\mu-a}^{\\mu+a}\\mathcal{N}(t;\\mu_{j},\\sigma_{j}^{2})\\right) = \\displaystyle= \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j ( \u03bc j + \u03c3 j \u200b s ) 2 \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) \u2212 1 2 \u200b a \u200b ( ( \u03bc + a ) 3 3 \u2212 ( \u03bc \u2212 a ) 3 3 ) \u23df = a 2 3 + \u03bc 2 \u200b ( \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j \ud835\udca9 \u200b ( s ; 0 , 1 ) ) superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 superscript subscript \ud835\udf07 \ud835\udc57 subscript \ud835\udf0e \ud835\udc57 \ud835\udc60 2 \ud835\udca9 \ud835\udc60 0 1 subscript \u23df 1 2 \ud835\udc4e superscript \ud835\udf07 \ud835\udc4e 3 3 superscript \ud835\udf07 \ud835\udc4e 3 3 absent superscript \ud835\udc4e 2 3 superscript \ud835\udf07 2 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udca9 \ud835\udc60 0 1 \\displaystyle\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}(\\mu_{j}+\\sigma_{j}s)^{2}\\mathcal{N}(s;0,1)-\\underbrace{\\frac{1}{2a}\\left(\\frac{(\\mu+a)^{3}}{3}-\\frac{(\\mu-a)^{3}}{3}\\right)}_{=\\frac{a^{2}}{3}+\\mu^{2}}\\left(\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}\\mathcal{N}(s;0,1)\\right) = \\displaystyle= ( \u03bc j 2 \u2212 \u03bc 2 \u2212 a 2 3 ) \u200b \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j \ud835\udca9 \u200b ( s ; 0 , 1 ) + 2 \u200b \u03bc j \u200b \u03c3 j \u200b \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j s \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) + \u03c3 j 2 \u200b \u222b \u03bc \u2212 \u03bc j \u2212 a \u03c3 j \u03bc \u2212 \u03bc j + a \u03c3 j s 2 \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) superscript subscript \ud835\udf07 \ud835\udc57 2 superscript \ud835\udf07 2 superscript \ud835\udc4e 2 3 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udca9 \ud835\udc60 0 1 2 subscript \ud835\udf07 \ud835\udc57 subscript \ud835\udf0e \ud835\udc57 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udc60 \ud835\udca9 \ud835\udc60 0 1 superscript subscript \ud835\udf0e \ud835\udc57 2 superscript subscript \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 superscript \ud835\udc60 2 \ud835\udca9 \ud835\udc60 0 1 \\displaystyle\\left(\\mu_{j}^{2}-\\mu^{2}-\\frac{a^{2}}{3}\\right)\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}\\mathcal{N}(s;0,1)+2\\mu_{j}\\sigma_{j}\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}s\\mathcal{N}(s;0,1)+\\sigma_{j}^{2}\\int_{\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}}}^{\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}}}s^{2}\\mathcal{N}(s;0,1) = \\displaystyle= ( \u03bc j 2 \u2212 \u03bc 2 + \u03c3 j 2 \u2212 a 2 3 ) \u200b ( erf \u200b ( \u03bc \u2212 \u03bc j + a 2 \u200b \u03c3 j ) \u2212 erf \u200b ( \u03bc \u2212 \u03bc j \u2212 a 2 \u200b \u03c3 j ) ) superscript subscript \ud835\udf07 \ud835\udc57 2 superscript \ud835\udf07 2 superscript subscript \ud835\udf0e \ud835\udc57 2 superscript \ud835\udc4e 2 3 erf \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e 2 subscript \ud835\udf0e \ud835\udc57 erf \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e 2 subscript \ud835\udf0e \ud835\udc57 \\displaystyle\\left(\\mu_{j}^{2}-\\mu^{2}+\\sigma_{j}^{2}-\\frac{a^{2}}{3}\\right)\\left(\\mathrm{erf}\\left(\\frac{\\mu-\\mu_{j}+a}{\\sqrt{2}\\sigma_{j}}\\right)-\\mathrm{erf}\\left(\\frac{\\mu-\\mu_{j}-a}{\\sqrt{2}\\sigma_{j}}\\right)\\right) \u2212 \u03c3 j \u200b ( \u03bc + \u03bc j + a ) \u200b \ud835\udca9 \u200b ( \u03bc \u2212 \u03bc j + a \u03c3 j ; 0 , 1 ) + \u03c3 j \u200b ( \u03bc + \u03bc j \u2212 a ) \u200b \ud835\udca9 \u200b ( \u03bc \u2212 \u03bc j \u2212 a \u03c3 j ; 0 , 1 ) . subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e \ud835\udca9 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 0 1 subscript \ud835\udf0e \ud835\udc57 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e \ud835\udca9 \ud835\udf07 subscript \ud835\udf07 \ud835\udc57 \ud835\udc4e subscript \ud835\udf0e \ud835\udc57 0 1 \\displaystyle-\\sigma_{j}(\\mu+\\mu_{j}+a)\\mathcal{N}\\left(\\frac{\\mu-\\mu_{j}+a}{\\sigma_{j}};0,1\\right)+\\sigma_{j}(\\mu+\\mu_{j}-a)\\mathcal{N}\\left(\\frac{\\mu-\\mu_{j}-a}{\\sigma_{j}};0,1\\right). G.3 Continuous sparsemax in 2D (, )\n\nLet us now consider the case where . For , the distribution , with , becomes a bivariate truncated paraboloid where and are related to the canonical parameters as before, . We obtain expressions for the attention mechanism output and its Jacobian that include 1D integrals (simple to integrate numerically), when are Gaussian RBFs, i.e., when each is of the form . We start with the following lemma:\n\n{lemma}\n\nLet be a -dimensional multivariate Gaussian, Let be a full column rank matrix (with ), and . Then we have with:\n\n\u03a3 ~ ~ \u03a3 \\displaystyle\\tilde{\\Sigma} = \\displaystyle= ( A \u22a4 \u200b \u03a3 \u2212 1 \u200b A ) \u2212 1 superscript superscript \ud835\udc34 top superscript \u03a3 1 \ud835\udc34 1 \\displaystyle(A^{\\top}\\Sigma^{-1}A)^{-1} \u03bc ~ ~ \ud835\udf07 \\displaystyle\\tilde{\\mu} = \\displaystyle= \u03a3 ~ \u200b A \u22a4 \u200b \u03a3 \u2212 1 \u200b ( \u03bc \u2212 b ) ~ \u03a3 superscript \ud835\udc34 top superscript \u03a3 1 \ud835\udf07 \ud835\udc4f \\displaystyle\\tilde{\\Sigma}A^{\\top}\\Sigma^{-1}(\\mu-b) s ~ ~ \ud835\udc60 \\displaystyle\\tilde{s} = \\displaystyle= ( 2 \u200b \u03c0 ) R \u2212 D 2 \u200b | \u03a3 ~ | 1 / 2 | \u03a3 | 1 / 2 \u200b exp \u2061 ( \u2212 1 2 \u200b ( \u03bc \u2212 b ) \u22a4 \u200b P \u200b ( \u03bc \u2212 b ) ) , P = \u03a3 \u2212 1 \u2212 \u03a3 \u2212 1 \u200b A \u200b \u03a3 ~ \u200b A \u22a4 \u200b \u03a3 \u2212 1 . superscript 2 \ud835\udf0b \ud835\udc45 \ud835\udc37 2 superscript ~ \u03a3 1 2 superscript \u03a3 1 2 1 2 superscript \ud835\udf07 \ud835\udc4f top \ud835\udc43 \ud835\udf07 \ud835\udc4f \ud835\udc43 superscript \u03a3 1 superscript \u03a3 1 \ud835\udc34 ~ \u03a3 superscript \ud835\udc34 top superscript \u03a3 1 \\displaystyle(2\\pi)^{\\frac{R-D}{2}}\\frac{|\\tilde{\\Sigma}|^{1/2}}{|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\mu-b)^{\\top}P(\\mu-b)\\right),\\quad P=\\Sigma^{-1}-\\Sigma^{-1}A\\tilde{\\Sigma}A^{\\top}\\Sigma^{-1}. If , then is invertible and the expressions above can be simplified to:\n\n\u03a3 ~ ~ \u03a3 \\displaystyle\\tilde{\\Sigma} = \\displaystyle= A \u2212 1 \u200b \u03a3 \u200b A \u2212 \u22a4 superscript \ud835\udc34 1 \u03a3 superscript \ud835\udc34 absent top \\displaystyle A^{-1}\\Sigma A^{-\\top} \u03bc ~ ~ \ud835\udf07 \\displaystyle\\tilde{\\mu} = \\displaystyle= A \u2212 1 \u200b ( \u03bc \u2212 b ) superscript \ud835\udc34 1 \ud835\udf07 \ud835\udc4f \\displaystyle A^{-1}(\\mu-b) s ~ ~ \ud835\udc60 \\displaystyle\\tilde{s} = \\displaystyle= | A | \u2212 1 .",
    "scattn-33": "superscript \ud835\udc34 1 \\displaystyle|A|^{-1}. Proof. The result can be derived by writing and splitting the exponential of the sum as a product of exponentials. \u220e\n\nForward pass. For the forward pass, we need to compute\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] = \u222c \u211d 2 [ \u2212 \u03bb \u2212 1 2 \u200b ( t \u2212 \u03bc ) \u22a4 \u200b \u03a3 \u2212 1 \u200b ( t \u2212 \u03bc ) ] + \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) \u200b \ud835\udc51 t , subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript double-integral superscript \u211d 2 subscript delimited-[] \ud835\udf06 1 2 superscript \ud835\udc61 \ud835\udf07 top superscript \u03a3 1 \ud835\udc61 \ud835\udf07 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 differential-d \ud835\udc61 \\mathbb{E}_{p}[\\psi_{j}(t)]=\\iint_{\\mathbb{R}^{2}}\\left[-\\lambda-\\frac{1}{2}(t-\\mu)^{\\top}\\Sigma^{-1}(t-\\mu)\\right]_{+}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})dt, (53)\n\nwith\n\n\ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) = 1 2 \u200b \u03c0 \u200b | \u03a3 j | 1 2 \u200b exp \u2061 ( \u2212 1 2 \u200b ( t \u2212 \u03bc j ) \u22a4 \u200b \u03a3 j \u2212 1 \u200b ( t \u2212 \u03bc j ) ) , \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 1 2 \ud835\udf0b superscript subscript \u03a3 \ud835\udc57 1 2 1 2 superscript \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 top superscript subscript \u03a3 \ud835\udc57 1 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 \\mathcal{N}(t;\\mu_{j},\\Sigma_{j})=\\frac{1}{2\\pi\\left|\\Sigma_{j}\\right|^{\\frac{1}{2}}}\\exp{\\left(-\\frac{1}{2}(t-\\mu_{j})^{\\top}\\Sigma_{j}^{-1}(t-\\mu_{j})\\right)}, (54)\n\nand (from (11))\n\n\u03bb = \u2212 ( 1 \u03c0 \u200b det ( \u03a3 ) ) 1 2 . \ud835\udf06 superscript 1 \ud835\udf0b \u03a3 1 2 \\lambda=-\\left(\\frac{1}{\\pi\\sqrt{\\det(\\Sigma)}}\\right)^{\\frac{1}{2}}. (55)\n\nUsing Lemma G.3 and the change of variable formula (which makes the determinants cancel), we can reparametrize and write this as an integral over the unit circle:\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] = \u222c \u2016 u \u2016 \u2264 1 \u2212 \u03bb \u200b ( 1 \u2212 \u2016 u \u2016 2 ) \u200b \ud835\udca9 \u200b ( u ; \u03bc ~ , \u03a3 ~ ) \u200b d \u200b u , subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript double-integral norm \ud835\udc62 1 \ud835\udf06 1 superscript norm \ud835\udc62 2 \ud835\udca9 \ud835\udc62 ~ \ud835\udf07 ~ \u03a3 \ud835\udc51 \ud835\udc62 \\mathbb{E}_{p}[\\psi_{j}(t)]=\\iint_{\\|u\\|\\leq 1}-\\lambda(1-\\|u\\|^{2})\\mathcal{N}(u;\\tilde{\\mu},\\tilde{\\Sigma})du, (56)\n\nwith , . We now do a change to polar coordinates, , where . The integral becomes:\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 \\displaystyle\\mathbb{E}_{p}[\\psi_{j}(t)] = \\displaystyle= \u222b 0 2 \u200b \u03c0 \u222b 0 1 \u2212 \u03bb \u200b ( 1 \u2212 r 2 ) \u200b \ud835\udca9 \u200b ( a \u200b r ; \u03bc ~ , \u03a3 ~ ) \u200b r \u200b d \u200b r \u200b d \u200b \u03b8 superscript subscript 0 2 \ud835\udf0b superscript subscript 0 1 \ud835\udf06 1 superscript \ud835\udc5f 2 \ud835\udca9 \ud835\udc4e \ud835\udc5f ~ \ud835\udf07 ~ \u03a3 \ud835\udc5f \ud835\udc51 \ud835\udc5f \ud835\udc51 \ud835\udf03 \\displaystyle\\int_{0}^{2\\pi}\\int_{0}^{1}-\\lambda(1-r^{2})\\mathcal{N}(ar;\\tilde{\\mu},\\tilde{\\Sigma})r\\,dr\\,d\\theta (57) = \\displaystyle= \u222b 0 2 \u200b \u03c0 \u222b 0 1 \u2212 \u03bb \u200b r \u200b ( 1 \u2212 r 2 ) \u200b s ~ \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b d \u200b r \u200b d \u200b \u03b8 , superscript subscript 0 2 \ud835\udf0b superscript subscript 0 1 \ud835\udf06 \ud835\udc5f 1 superscript \ud835\udc5f 2 ~ \ud835\udc60 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 \ud835\udc51 \ud835\udc5f \ud835\udc51 \ud835\udf03 \\displaystyle\\int_{0}^{2\\pi}\\int_{0}^{1}-\\lambda r(1-r^{2})\\tilde{s}\\mathcal{N}(r;r_{0},\\sigma^{2})\\,dr\\,d\\theta,\n\nwhere in the second line we applied again Lemma G.3, resulting in\n\n\u03c3 2 \u200b ( \u03b8 ) \u2261 \u03c3 2 superscript \ud835\udf0e 2 \ud835\udf03 superscript \ud835\udf0e 2 \\displaystyle\\sigma^{2}(\\theta)\\equiv\\sigma^{2} = \\displaystyle= ( a \u22a4 \u200b \u03a3 ~ \u2212 1 \u200b a ) \u2212 1 superscript superscript \ud835\udc4e top superscript ~ \u03a3 1 \ud835\udc4e 1 \\displaystyle(a^{\\top}\\tilde{\\Sigma}^{-1}a)^{-1} r 0 \u200b ( \u03b8 ) \u2261 r 0 subscript \ud835\udc5f 0 \ud835\udf03 subscript \ud835\udc5f 0 \\displaystyle r_{0}(\\theta)\\equiv r_{0} = \\displaystyle= \u03c3 2 \u200b a \u22a4 \u200b \u03a3 ~ \u2212 1 \u200b \u03bc ~ superscript \ud835\udf0e 2 superscript \ud835\udc4e top superscript ~ \u03a3 1 ~ \ud835\udf07 \\displaystyle\\sigma^{2}a^{\\top}\\tilde{\\Sigma}^{-1}\\tilde{\\mu} s ~ \u200b ( \u03b8 ) \u2261 s ~ ~ \ud835\udc60 \ud835\udf03 ~ \ud835\udc60 \\displaystyle\\tilde{s}(\\theta)\\equiv\\tilde{s} = \\displaystyle= 1 2 \u200b \u03c0 \u200b \u03c3 | \u03a3 ~ | 1 / 2 \u200b exp \u2061 ( \u2212 1 2 \u200b \u03bc ~ \u22a4 \u200b P \u200b \u03bc ~ ) , P = \u03a3 ~ \u2212 1 \u2212 \u03c3 2 \u200b \u03a3 ~ \u2212 1 \u200b a \u200b a \u22a4 \u200b \u03a3 ~ \u2212 1 . 1 2 \ud835\udf0b \ud835\udf0e superscript ~ \u03a3 1 2 1 2 superscript ~ \ud835\udf07 top \ud835\udc43 ~ \ud835\udf07 \ud835\udc43 superscript ~ \u03a3 1 superscript \ud835\udf0e 2 superscript ~ \u03a3 1 \ud835\udc4e superscript \ud835\udc4e top superscript ~ \u03a3 1 \\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\frac{\\sigma}{|\\tilde{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}\\tilde{\\mu}^{\\top}P\\tilde{\\mu}\\right),\\quad P=\\tilde{\\Sigma}^{-1}-\\sigma^{2}\\tilde{\\Sigma}^{-1}aa^{\\top}\\tilde{\\Sigma}^{-1}. Applying Fubini\u2019s theorem, we fix and integrate with respect to . We use the formulas (G.2) and the fact that, for any such that :\n\n\u222b u v t 3 \u200b \ud835\udca9 \u200b ( t ; 0 , 1 ) = \u2212 \ud835\udca9 \u200b ( v ; 0 , 1 ) \u200b ( 2 + v 2 ) + \ud835\udca9 \u200b ( u ; 0 , 1 ) \u200b ( 2 + u 2 ) . superscript subscript \ud835\udc62 \ud835\udc63 superscript \ud835\udc61 3 \ud835\udca9 \ud835\udc61 0 1 \ud835\udca9 \ud835\udc63 0 1 2 superscript \ud835\udc63 2 \ud835\udca9 \ud835\udc62 0 1 2 superscript \ud835\udc62 2 \\int_{u}^{v}t^{3}\\mathcal{N}(t;0,1)=-\\mathcal{N}(v;0,1)(2+v^{2})+\\mathcal{N}(u;0,1)(2+u^{2}). (58)\n\nWe obtain a closed from expression for the inner integral:\n\nF \u200b ( \u03b8 ) \ud835\udc39 \ud835\udf03 \\displaystyle F(\\theta) = \\displaystyle= \u222b 0 1 r \u200b ( 1 \u2212 r 2 ) \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r superscript subscript 0 1 \ud835\udc5f 1 superscript \ud835\udc5f 2 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f \\displaystyle\\int_{0}^{1}r(1-r^{2})\\mathcal{N}(r;r_{0},\\sigma^{2})\\,dr (59) = \\displaystyle= ( 2 \u200b \u03c3 3 + r 0 2 \u200b \u03c3 + r 0 \u200b \u03c3 ) \u200b \ud835\udca9 \u200b ( 1 \u2212 r 0 \u03c3 ; 0 , 1 ) \u2212 ( 2 \u200b \u03c3 3 + r 0 2 \u200b \u03c3 \u2212 \u03c3 ) \u200b \ud835\udca9 \u200b ( \u2212 r 0 \u03c3 ; 0 , 1 ) 2 superscript \ud835\udf0e 3 superscript subscript \ud835\udc5f 0 2 \ud835\udf0e subscript \ud835\udc5f 0 \ud835\udf0e \ud835\udca9 1 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 2 superscript \ud835\udf0e 3 superscript subscript \ud835\udc5f 0 2 \ud835\udf0e \ud835\udf0e \ud835\udca9 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 \\displaystyle(2\\sigma^{3}+r_{0}^{2}\\sigma+r_{0}\\sigma)\\mathcal{N}\\left(\\frac{1-r_{0}}{\\sigma};0,1\\right)-(2\\sigma^{3}+r_{0}^{2}\\sigma-\\sigma)\\mathcal{N}\\left(-\\frac{r_{0}}{\\sigma};0,1\\right) \u2212 r 0 3 + ( 3 \u200b \u03c3 2 \u2212 1 ) \u200b r 0 2 \u200b [ erf \u200b ( 1 \u2212 r 0 2 \u200b \u03c3 ) \u2212 erf \u200b ( \u2212 r 0 2 \u200b \u03c3 ) ] . superscript subscript \ud835\udc5f 0 3 3 superscript \ud835\udf0e 2 1 subscript \ud835\udc5f 0 2 delimited-[] erf 1 subscript \ud835\udc5f 0 2 \ud835\udf0e erf subscript \ud835\udc5f 0 2 \ud835\udf0e \\displaystyle-\\frac{r_{0}^{3}+(3\\sigma^{2}-1)r_{0}}{2}\\left[\\mathrm{erf}\\left(\\frac{1-r_{0}}{\\sqrt{2}\\sigma}\\right)-\\mathrm{erf}\\left(-\\frac{r_{0}}{\\sqrt{2}\\sigma}\\right)\\right]. The desired integral can then be expressed in a single dimension as\n\n\ud835\udd3c p \u200b [ \u03c8 j \u200b ( t ) ] subscript \ud835\udd3c \ud835\udc5d delimited-[] subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 \\displaystyle\\mathbb{E}_{p}[\\psi_{j}(t)] = \\displaystyle= \u2212 \u03bb \u200b \u222b 0 2 \u200b \u03c0 s ~ \u200b ( \u03b8 ) \u200b F \u200b ( \u03b8 ) , \ud835\udf06 superscript subscript 0 2 \ud835\udf0b ~ \ud835\udc60 \ud835\udf03 \ud835\udc39 \ud835\udf03 \\displaystyle-\\lambda\\int_{0}^{2\\pi}\\tilde{s}(\\theta)F(\\theta), (60)\n\nwhich may be integrated numerically. Backward pass. For the backward pass we need to solve\n\ncov p , 2 \u200b ( t , \u03c8 j \u200b ( t ) ) = \u222c E t \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) \u2212 1 | E | \u200b ( \u222c E t ) \u200b ( \u222c E \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) ) subscript cov \ud835\udc5d 2 \ud835\udc61 subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript double-integral \ud835\udc38 \ud835\udc61 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 1 \ud835\udc38 subscript double-integral \ud835\udc38 \ud835\udc61 subscript double-integral \ud835\udc38 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\mathrm{cov}_{p,2}(t,\\psi_{j}(t))=\\iint_{E}t\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})-\\frac{1}{|E|}\\left(\\iint_{E}t\\right)\\left(\\iint_{E}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})\\right) (61)\n\nand\n\ncov p , 2 \u200b ( t \u200b t \u22a4 , \u03c8 j \u200b ( t ) ) = \u222c E t \u200b t \u22a4 \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) \u2212 1 | E | \u200b ( \u222c E t \u200b t \u22a4 ) \u200b ( \u222c E \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) ) subscript cov \ud835\udc5d 2 \ud835\udc61 superscript \ud835\udc61 top subscript \ud835\udf13 \ud835\udc57 \ud835\udc61 subscript double-integral \ud835\udc38 \ud835\udc61 superscript \ud835\udc61 top \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 1 \ud835\udc38 subscript double-integral \ud835\udc38 \ud835\udc61 superscript \ud835\udc61 top subscript double-integral \ud835\udc38 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\mathrm{cov}_{p,2}(tt^{\\top},\\psi_{j}(t))=\\iint_{E}tt^{\\top}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})-\\frac{1}{|E|}\\left(\\iint_{E}tt^{\\top}\\right)\\left(\\iint_{E}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})\\right) (62)\n\nwhere denotes the support of the density , a region bounded by an ellipse. Note that these expressions include integrals of vector-valued functions and that (61) and (62) correspond to the first to second and the third to sixth row of the Jacobian, respectively. The integrals that do not include Gaussians have closed form expressions and can be computed as\n\n1 | E | \u200b ( \u222c E t ) = \u03bc 1 \ud835\udc38 subscript double-integral \ud835\udc38 \ud835\udc61 \ud835\udf07 \\frac{1}{|E|}\\left(\\iint_{E}t\\right)=\\mu (63)\n\nand\n\n1 | E | \u200b ( \u222c E t \u200b t \u22a4 ) = \u03bc \u200b \u03bc \u22a4 + \u03a3 | E | , 1 \ud835\udc38 subscript double-integral \ud835\udc38 \ud835\udc61 superscript \ud835\udc61 top \ud835\udf07 superscript \ud835\udf07 top \u03a3 \ud835\udc38 \\frac{1}{|E|}\\left(\\iint_{E}tt^{\\top}\\right)=\\mu\\mu^{\\top}+\\frac{\\Sigma}{|E|}, (64)\n\nwhere is the area of the region given by\n\n| E | = \u03c0 det ( 1 \u2212 2 \u200b \u03bb \u200b \u03a3 \u2212 1 ) . \ud835\udc38 \ud835\udf0b 1 2 \ud835\udf06 superscript \u03a3 1 |E|=\\frac{\\pi}{\\sqrt{\\det\\left(\\frac{1}{-2\\lambda}\\,\\Sigma^{-1}\\right)}}. (65)\n\nAll the other integrals are solved using the same affine transformation and change to polar coordinates as in the forward pass. Given this, , , , and are the same as before. To solve (61) we write\n\n\u222c E t \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) = \u222c \u2016 u \u2016 \u2264 1 ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b u + \u03bc ) \u200b \ud835\udca9 \u200b ( u ; \u03bc ~ , \u03a3 ~ ) \u200b \ud835\udc51 u subscript double-integral \ud835\udc38 \ud835\udc61 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 subscript double-integral norm \ud835\udc62 1 superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc62 \ud835\udf07 \ud835\udca9 \ud835\udc62 ~ \ud835\udf07 ~ \u03a3 differential-d \ud835\udc62 \\iint_{E}t\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})=\\iint_{\\|u\\|\\leq 1}\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}u+\\mu\\right)\\mathcal{N}(u;\\tilde{\\mu},\\tilde{\\Sigma})du (66)\n\nin polar coordinates,\n\n\u222b 0 2 \u200b \u03c0 \u222b 0 1 r \u200b ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b a \u200b r + \u03bc ) \u200b s ~ \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r \u200b \ud835\udc51 \u03b8 , superscript subscript 0 2 \ud835\udf0b superscript subscript 0 1 \ud835\udc5f superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e \ud835\udc5f \ud835\udf07 ~ \ud835\udc60 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f differential-d \ud835\udf03 \\int_{0}^{2\\pi}\\int_{0}^{1}r\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}ar+\\mu\\right)\\tilde{s}\\,\\mathcal{N}(r;r_{0},\\sigma^{2})dr\\,d\\theta, (67)\n\nwhich can be then expressed in a single dimension as\n\n\u222c E t \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) subscript double-integral \ud835\udc38 \ud835\udc61 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\displaystyle\\iint_{E}t\\mathcal{N}(t;\\mu_{j},\\Sigma_{j}) = \\displaystyle= \u222b 0 2 \u200b \u03c0 s ~ \u200b ( \u03b8 ) \u200b G \u200b ( \u03b8 ) \u200b \ud835\udc51 \u03b8 , superscript subscript 0 2 \ud835\udf0b ~ \ud835\udc60 \ud835\udf03 \ud835\udc3a \ud835\udf03 differential-d \ud835\udf03 \\displaystyle\\int_{0}^{2\\pi}\\tilde{s}(\\theta)G(\\theta)d\\theta, (68)\n\nwith\n\nG \u200b ( \u03b8 ) \ud835\udc3a \ud835\udf03 \\displaystyle G(\\theta) = \\displaystyle= \u222b 0 1 r \u200b ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b a \u200b r + \u03bc ) \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r superscript subscript 0 1 \ud835\udc5f superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e \ud835\udc5f \ud835\udf07 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f \\displaystyle\\int_{0}^{1}r\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}ar+\\mu\\right)\\mathcal{N}(r;r_{0},\\sigma^{2})\\,dr (69) = \\displaystyle= \u222b \u2212 r 0 \u03c3 1 \u2212 r 0 \u03c3 ( s \u200b \u03c3 + r 0 ) \u200b ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b a \u200b ( s \u200b \u03c3 + r 0 ) + \u03bc ) \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 s superscript subscript subscript \ud835\udc5f 0 \ud835\udf0e 1 subscript \ud835\udc5f 0 \ud835\udf0e \ud835\udc60 \ud835\udf0e subscript \ud835\udc5f 0 superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e \ud835\udc60 \ud835\udf0e subscript \ud835\udc5f 0 \ud835\udf07 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc60 \\displaystyle\\int_{-\\frac{r_{0}}{\\sigma}}^{\\frac{1-r_{0}}{\\sigma}}(s\\sigma+r_{0})\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}a(s\\sigma+r_{0})+\\mu\\right)\\mathcal{N}(r;r_{0},\\sigma^{2})\\,ds = \\displaystyle= ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b a \u200b \u03c3 \u200b ( r 0 ) + \u03bc \u200b \u03c3 ) \u200b \ud835\udca9 \u200b ( \u2212 r 0 \u03c3 ; 0 , 1 ) superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e \ud835\udf0e subscript \ud835\udc5f 0 \ud835\udf07 \ud835\udf0e \ud835\udca9 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 \\displaystyle\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}a\\sigma(r_{0})+\\mu\\sigma\\right)\\mathcal{N}\\left(-\\frac{r_{0}}{\\sigma};0,1\\right) \u2212 ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b a \u200b \u03c3 \u200b ( 1 + r 0 ) + \u03bc \u200b \u03c3 ) \u200b \ud835\udca9 \u200b ( 1 \u2212 r 0 \u03c3 ; 0 , 1 ) superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e \ud835\udf0e 1 subscript \ud835\udc5f 0 \ud835\udf07 \ud835\udf0e \ud835\udca9 1 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 \\displaystyle-\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}a\\sigma(1+r_{0})+\\mu\\sigma\\right)\\mathcal{N}\\left(\\frac{1-r_{0}}{\\sigma};0,1\\right) + 1 2 \u200b ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b a \u200b ( \u03c3 2 + r 0 2 ) + \u03bc \u200b r 0 ) \u200b [ erf \u200b ( 1 \u2212 r 0 2 \u200b \u03c3 ) \u2212 erf \u200b ( \u2212 r 0 2 \u200b \u03c3 ) ] . 1 2 superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e superscript \ud835\udf0e 2 superscript subscript \ud835\udc5f 0 2 \ud835\udf07 subscript \ud835\udc5f 0 delimited-[] erf 1 subscript \ud835\udc5f 0 2 \ud835\udf0e erf subscript \ud835\udc5f 0 2 \ud835\udf0e \\displaystyle+\\frac{1}{2}\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}a(\\sigma^{2}+r_{0}^{2})+\\mu r_{0}\\right)\\left[\\mathrm{erf}\\left(\\frac{1-r_{0}}{\\sqrt{2}\\sigma}\\right)-\\mathrm{erf}\\left(-\\frac{r_{0}}{\\sqrt{2}\\sigma}\\right)\\right]. We do the same for\n\n\u222c E \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) = \u222c \u2016 u \u2016 \u2264 1 \ud835\udca9 \u200b ( u ; \u03bc ~ , \u03a3 ~ ) \u200b \ud835\udc51 u = \u222b 0 2 \u200b \u03c0 \u222b 0 1 r \u200b s ~ \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r \u200b \ud835\udc51 \u03b8 , subscript double-integral \ud835\udc38 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 subscript double-integral norm \ud835\udc62 1 \ud835\udca9 \ud835\udc62 ~ \ud835\udf07 ~ \u03a3 differential-d \ud835\udc62 superscript subscript 0 2 \ud835\udf0b superscript subscript 0 1 \ud835\udc5f ~ \ud835\udc60 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f differential-d \ud835\udf03 \\iint_{E}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j})=\\iint_{\\|u\\|\\leq 1}\\mathcal{N}(u;\\tilde{\\mu},\\tilde{\\Sigma})du=\\int_{0}^{2\\pi}\\int_{0}^{1}r\\tilde{s}\\,\\mathcal{N}(r;r_{0},\\sigma^{2})dr\\,d\\theta, (70)\n\nwhich can then be expressed in a single dimension as\n\n\u222c E \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) subscript double-integral \ud835\udc38 \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\displaystyle\\iint_{E}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j}) = \\displaystyle= \u222b 0 2 \u200b \u03c0 s ~ \u200b ( \u03b8 ) \u200b H \u200b ( \u03b8 ) \u200b \ud835\udc51 \u03b8 , superscript subscript 0 2 \ud835\udf0b ~ \ud835\udc60 \ud835\udf03 \ud835\udc3b \ud835\udf03 differential-d \ud835\udf03 \\displaystyle\\int_{0}^{2\\pi}\\tilde{s}(\\theta)H(\\theta)d\\theta, (71)\n\nwith\n\nH \u200b ( \u03b8 ) \ud835\udc3b \ud835\udf03 \\displaystyle H(\\theta) = \\displaystyle= \u222b 0 1 r \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r = \u222b \u2212 r 0 \u03c3 1 \u2212 r 0 \u03c3 ( s \u200b \u03c3 + r 0 ) \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 s superscript subscript 0 1 \ud835\udc5f \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f superscript subscript subscript \ud835\udc5f 0 \ud835\udf0e 1 subscript \ud835\udc5f 0 \ud835\udf0e \ud835\udc60 \ud835\udf0e subscript \ud835\udc5f 0 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc60 \\displaystyle\\int_{0}^{1}r\\mathcal{N}(r;r_{0},\\sigma^{2})\\,dr=\\int_{-\\frac{r_{0}}{\\sigma}}^{\\frac{1-r_{0}}{\\sigma}}(s\\sigma+r_{0})\\mathcal{N}(r;r_{0},\\sigma^{2})\\,ds = \\displaystyle= \u03c3 \u200b [ \ud835\udca9 \u200b ( \u2212 r 0 \u03c3 ; 0 , 1 ) \u2212 \ud835\udca9 \u200b ( 1 \u2212 r 0 \u03c3 ; 0 , 1 ) ] + r 0 2 \u200b [ erf \u200b ( 1 \u2212 r 0 2 \u200b \u03c3 ) \u2212 erf \u200b ( \u2212 r 0 2 \u200b \u03c3 ) ] . \ud835\udf0e delimited-[] \ud835\udca9 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 \ud835\udca9 1 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 subscript \ud835\udc5f 0 2 delimited-[] erf 1 subscript \ud835\udc5f 0 2 \ud835\udf0e erf subscript \ud835\udc5f 0 2 \ud835\udf0e \\displaystyle\\sigma\\left[\\mathcal{N}\\left(-\\frac{r_{0}}{\\sigma};0,1\\right)-\\mathcal{N}\\left(\\frac{1-r_{0}}{\\sigma};0,1\\right)\\right]+\\frac{r_{0}}{2}\\left[\\mathrm{erf}\\left(\\frac{1-r_{0}}{\\sqrt{2}\\sigma}\\right)-\\mathrm{erf}\\left(-\\frac{r_{0}}{\\sqrt{2}\\sigma}\\right)\\right]. Finally, to solve (62) we simplify the integral\n\n\u222c E t \u200b t \u22a4 \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) subscript double-integral \ud835\udc38 \ud835\udc61 superscript \ud835\udc61 top \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\displaystyle\\iint_{E}tt^{\\top}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j}) = \\displaystyle= \u222c \u2016 u \u2016 \u2264 1 ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b u + \u03bc ) \u200b ( ( \u2212 2 \u200b \u03bb ) 1 2 \u200b \u03a3 1 2 \u200b u + \u03bc ) \u22a4 \u200b \ud835\udca9 \u200b ( u ; \u03bc ~ , \u03a3 ~ ) \u200b \ud835\udc51 u subscript double-integral norm \ud835\udc62 1 superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc62 \ud835\udf07 superscript superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc62 \ud835\udf07 top \ud835\udca9 \ud835\udc62 ~ \ud835\udf07 ~ \u03a3 differential-d \ud835\udc62 \\displaystyle\\iint_{\\|u\\|\\leq 1}\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}u+\\mu\\right)\\left((-2\\lambda)^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}u+\\mu\\right)^{\\top}\\mathcal{N}(u;\\tilde{\\mu},\\tilde{\\Sigma})du (72) = \\displaystyle= \u222b 0 2 \u200b \u03c0 \u222b 0 1 r \u200b ( r 2 \u200b A + r \u200b B + C ) \u200b s ~ \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r \u200b \ud835\udc51 \u03b8 superscript subscript 0 2 \ud835\udf0b superscript subscript 0 1 \ud835\udc5f superscript \ud835\udc5f 2 \ud835\udc34 \ud835\udc5f \ud835\udc35 \ud835\udc36 ~ \ud835\udc60 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f differential-d \ud835\udf03 \\displaystyle\\int_{0}^{2\\pi}\\int_{0}^{1}r(r^{2}A+rB+C)\\tilde{s}\\,\\mathcal{N}(r;r_{0},\\sigma^{2})dr\\,d\\theta\n\nwith\n\nA = ( \u2212 2 \u200b \u03bb ) \u200b \u03a3 1 2 \u200b a \u200b a \u22a4 \u200b ( \u03a3 1 2 ) \u22a4 \ud835\udc34 2 \ud835\udf06 superscript \u03a3 1 2 \ud835\udc4e superscript \ud835\udc4e top superscript superscript \u03a3 1 2 top A=(-2\\lambda)\\Sigma^{\\frac{1}{2}}aa^{\\top}(\\Sigma^{\\frac{1}{2}})^{\\top} (73)\n\nB = ( \u2212 2 \u200b \u03bb ) 1 2 \u200b ( \u03a3 1 2 \u200b a \u200b \u03bc \u22a4 + \u03bc \u200b a \u22a4 \u200b ( \u03a3 1 2 ) \u22a4 ) \ud835\udc35 superscript 2 \ud835\udf06 1 2 superscript \u03a3 1 2 \ud835\udc4e superscript \ud835\udf07 top \ud835\udf07 superscript \ud835\udc4e top superscript superscript \u03a3 1 2 top B=(-2\\lambda)^{\\frac{1}{2}}\\left(\\Sigma^{\\frac{1}{2}}a\\mu^{\\top}+\\mu a^{\\top}(\\Sigma^{\\frac{1}{2}})^{\\top}\\right) (74)\n\nC = \u03bc \u200b \u03bc \u22a4 . \ud835\udc36 \ud835\udf07 superscript \ud835\udf07 top C=\\mu\\mu^{\\top}.",
    "scattn-34": "(75)\n\nThe integral can then be expressed in a single dimension as\n\n\u222c E t \u200b t \u22a4 \u200b \ud835\udca9 \u200b ( t ; \u03bc j , \u03a3 j ) subscript double-integral \ud835\udc38 \ud835\udc61 superscript \ud835\udc61 top \ud835\udca9 \ud835\udc61 subscript \ud835\udf07 \ud835\udc57 subscript \u03a3 \ud835\udc57 \\displaystyle\\iint_{E}tt^{\\top}\\mathcal{N}(t;\\mu_{j},\\Sigma_{j}) = \\displaystyle= \u222b 0 2 \u200b \u03c0 s ~ \u200b ( \u03b8 ) \u200b M \u200b ( \u03b8 ) \u200b \ud835\udc51 \u03b8 , superscript subscript 0 2 \ud835\udf0b ~ \ud835\udc60 \ud835\udf03 \ud835\udc40 \ud835\udf03 differential-d \ud835\udf03 \\displaystyle\\int_{0}^{2\\pi}\\tilde{s}(\\theta)M(\\theta)d\\theta, (76)\n\nwith\n\nM \u200b ( \u03b8 ) \ud835\udc40 \ud835\udf03 \\displaystyle M(\\theta) = \\displaystyle= \u222b 0 1 ( r 3 \u200b A + r 2 \u200b B + r \u200b C ) \u200b \ud835\udca9 \u200b ( r ; r 0 , \u03c3 2 ) \u200b \ud835\udc51 r superscript subscript 0 1 superscript \ud835\udc5f 3 \ud835\udc34 superscript \ud835\udc5f 2 \ud835\udc35 \ud835\udc5f \ud835\udc36 \ud835\udca9 \ud835\udc5f subscript \ud835\udc5f 0 superscript \ud835\udf0e 2 differential-d \ud835\udc5f \\displaystyle\\int_{0}^{1}(r^{3}A+r^{2}B+rC)\\,\\mathcal{N}(r;r_{0},\\sigma^{2})dr (77) = \\displaystyle= \u222b \u2212 r 0 \u03c3 1 \u2212 r 0 \u03c3 ( s 3 \u200b A ~ + s 2 \u200b B ~ + s \u200b C ~ + D ~ ) \u200b \ud835\udca9 \u200b ( s ; 0 , 1 ) \u200b \ud835\udc51 s superscript subscript subscript \ud835\udc5f 0 \ud835\udf0e 1 subscript \ud835\udc5f 0 \ud835\udf0e superscript \ud835\udc60 3 ~ \ud835\udc34 superscript \ud835\udc60 2 ~ \ud835\udc35 \ud835\udc60 ~ \ud835\udc36 ~ \ud835\udc37 \ud835\udca9 \ud835\udc60 0 1 differential-d \ud835\udc60 \\displaystyle\\int_{-\\frac{r_{0}}{\\sigma}}^{\\frac{1-r_{0}}{\\sigma}}(s^{3}\\tilde{A}+s^{2}\\tilde{B}+s\\,\\tilde{C}+\\tilde{D})\\mathcal{N}(s;0,1)\\,ds = \\displaystyle= [ ( 2 + ( \u2212 r 0 \u03c3 ) 2 ) \u200b A ~ \u2212 r 0 \u03c3 \u200b B ~ + C ~ ] \u200b \ud835\udca9 \u200b ( \u2212 r 0 \u03c3 ; 0 , 1 ) delimited-[] 2 superscript subscript \ud835\udc5f 0 \ud835\udf0e 2 ~ \ud835\udc34 subscript \ud835\udc5f 0 \ud835\udf0e ~ \ud835\udc35 ~ \ud835\udc36 \ud835\udca9 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 \\displaystyle\\left[\\left(2+\\left(-\\frac{r_{0}}{\\sigma}\\right)^{2}\\right)\\tilde{A}-\\frac{r_{0}}{\\sigma}\\tilde{B}+\\tilde{C}\\right]\\mathcal{N}\\left(-\\frac{r_{0}}{\\sigma};0,1\\right) \u2212 [ ( 2 + ( 1 \u2212 r 0 \u03c3 ) 2 ) \u200b A ~ + 1 \u2212 r 0 \u03c3 \u200b B ~ + C ~ ] \u200b \ud835\udca9 \u200b ( 1 \u2212 r 0 \u03c3 ; 0 , 1 ) delimited-[] 2 superscript 1 subscript \ud835\udc5f 0 \ud835\udf0e 2 ~ \ud835\udc34 1 subscript \ud835\udc5f 0 \ud835\udf0e ~ \ud835\udc35 ~ \ud835\udc36 \ud835\udca9 1 subscript \ud835\udc5f 0 \ud835\udf0e 0 1 \\displaystyle-\\left[\\left(2+\\left(\\frac{1-r_{0}}{\\sigma}\\right)^{2}\\right)\\tilde{A}+\\frac{1-r_{0}}{\\sigma}\\tilde{B}+\\tilde{C}\\right]\\mathcal{N}\\left(\\frac{1-r_{0}}{\\sigma};0,1\\right) + 1 2 \u200b ( B ~ + D ~ ) \u200b [ erf \u200b ( 1 \u2212 r 0 2 \u200b \u03c3 ) \u2212 erf \u200b ( \u2212 r 0 2 \u200b \u03c3 ) ] 1 2 ~ \ud835\udc35 ~ \ud835\udc37 delimited-[] erf 1 subscript \ud835\udc5f 0 2 \ud835\udf0e erf subscript \ud835\udc5f 0 2 \ud835\udf0e \\displaystyle+\\frac{1}{2}\\left(\\tilde{B}+\\tilde{D}\\right)\\left[\\mathrm{erf}\\left(\\frac{1-r_{0}}{\\sqrt{2}\\sigma}\\right)-\\mathrm{erf}\\left(-\\frac{r_{0}}{\\sqrt{2}\\sigma}\\right)\\right]\n\nwhere\n\nA ~ = \u03c3 3 \u200b A ~ \ud835\udc34 superscript \ud835\udf0e 3 \ud835\udc34 \\tilde{A}=\\sigma^{3}A (78)\n\nB ~ = \u03c3 2 \u200b ( 3 \u200b r 0 \u200b A + B ) ~ \ud835\udc35 superscript \ud835\udf0e 2 3 subscript \ud835\udc5f 0 \ud835\udc34 \ud835\udc35 \\tilde{B}=\\sigma^{2}(3r_{0}\\,A+B) (79)\n\nC ~ = \u03c3 \u200b ( 3 \u200b r 0 2 \u200b A + 2 \u200b r 0 \u200b B + C ) ~ \ud835\udc36 \ud835\udf0e 3 superscript subscript \ud835\udc5f 0 2 \ud835\udc34 2 subscript \ud835\udc5f 0 \ud835\udc35 \ud835\udc36 \\tilde{C}=\\sigma(3r_{0}^{2}\\,A+2r_{0}\\,B+C) (80)\n\nD ~ = r 0 3 \u200b A + r 0 2 \u200b B + r 0 \u200b C . ~ \ud835\udc37 superscript subscript \ud835\udc5f 0 3 \ud835\udc34 superscript subscript \ud835\udc5f 0 2 \ud835\udc35 subscript \ud835\udc5f 0 \ud835\udc36 \\tilde{D}=r_{0}^{3}\\,A+r_{0}^{2}\\,B+r_{0}\\,C. (81)\n\nAppendix H Experimental Details and Model Hyperparameters\n\nH.1 Document classification\n\nWe used the IMDB movie review dataset [29],666https://ai.stanford.edu/~amaas/data/sentiment which consist of user-written text reviews with binary labels (positive/negative). Following [43], we used 25K training documents, 10% of which for validation, and 25K for testing. The training and test sets are perfectly balanced: 12.5K negative and 12.5K positive examples. The documents have 280 words on average. Our architecture is the same as [29], a BiLSTM with attention. We used pretrained GloVe embeddings from the 840B release,777http://nlp.stanford.edu/data/glove.840B.300d.zip kept frozen. We tuned three hyperparameters using the discrete softmax attention baseline: learning rate within ; within ; number of epochs within . We picked the best configuration by doing a grid search and by taking into consideration the accuracy on the validation set (selected values in bold). Table 3 shows the hyperparameters and model configurations used for all document classification experiments. H.2 Machine translation\n\nWe used the DeEn dataset from the IWSLT 2017 evaluation campaign [30], with the standard splits (206K, 9K, and 2K sentence pairs for train/dev/test).888https://wit3.fbk.eu/mt.php?release=2017-01-trnted We used BPE [52] with 32K merges to reduce the vocabulary size. Our implementation is based on Joey-NMT [53] and we used the provided configuration script for the baseline, a BiLSTM model with discrete softmax attention999https://github.com/joeynmt/joeynmt/blob/master/configs/iwslt14_deen_bpe.yaml with the hyperpameters in Table 4. H.3 Visual question answering\n\nWe used the VQA-v2 dataset [31] with the standard splits (443K, 214K, and 453K question-image pairs for train/dev/test, the latter subdivided into test-dev, test-standard, test-challenge and test-reserve). We adapted the implementation of [32],101010https://github.com/MILVLG/mcan-vqa consisting of a Modular Co-Attention Network (MCAN). Our architecture is the same as [32] except that we represent the image input with grid features generated by a ResNet [54] pretrained on ImageNet [55], instead of bounding-box features [56]. The images are resized to before going through the ResNet that outputs a feature map of size . To represent the input question words we use 300-dimensional GloVe word embeddings [57], yielding a question feature matrix representation. Table 5 shows the hyperparameters used for all the VQA experiments presented.",
    "scattn-35": "All the models we experimented with use the same features and were trained only on the train set without data augmentation. Examples. Figure 4 illustrates the difficulties that continuous attention models may face when trying to focus on objects that are too far from each other or that seem to have different relative importance to answer the question. Intuitively, in VQA, this becomes a problem when counting objects in those conditions. On the other side, in counting questions that require the understanding of a contiguous region of the image only, continuous attention may perform better (see Figure 5). Figures 6 and 7 show other examples where continuous attention focus on the right region of the image and answers the question correctly. For these cases, discrete attention is more diffuse than its continuous counterpart: in both examples, it attends to two different regions in the image, leading to incorrect answers. \u25c4 Feeling lucky?",
    "scattn-36": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Mar 6 18:27:39 2024 by LaTeXML"
}