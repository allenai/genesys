{
    "grootvl-0": "# GrootVL: Tree Topology is All You Need in State Space Model \n\nYicheng Xiao ${ }^{1+*}, \\quad$ Lin Song ${ }^{2,3 \\bowtie *}$,<br>Shaoli Huang ${ }^{3}$, Jiangshan Wang ${ }^{1}$, Siyu Song ${ }^{4}$, Yixiao $\\mathbf{G e}^{2,3}$, Xiu Li ${ }^{1 \\varpi}$, Ying Shan ${ }^{2,3}$<br>${ }^{1}$ Tsinghua Shenzhen International Graduate School, Tsinghua University<br>${ }^{2}$ ARC Lab, Tencent PCG $\\quad{ }^{3}$ Tencent AI Lab $\\quad{ }^{4}$ South China Normal University<br>xiaoyc23@mails.tsinghua.edu.cn ronnysong@tencent.com\n\n\n#### Abstract\n\nThe state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency.",
    "grootvl-1": "However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost. Code is available at https://github.com/EasonXiao-888/GrootVL. ## 1 Introduction\n\nMainstream fundamental models are primarily based on CNN [27, 57, 41, 29, 13] and Transformer architectures [15, 40, 39, 54, 14], which dominate in visual and language tasks. However, the small receptive field of CNNs and the high complexity of Transformers make it challenging to strike a good balance between effectiveness and efficiency. The state space models (SSMs) [21, 23, 48] attempt to disrupt this impasse, which model sequences in a recurrent form. Different from the previous recurrent neural networks [28, 7], these approaches draw inspiration from control systems, leveraging structural parameter initialization to attain stable optimization and superior computing performance. Nevertheless, it remains susceptible to the intrinsic flaw shared by recurrent neural networks, i.e., a deficiency in capturing long-range dependencies. Recently, an improved selection mechanism known as Mamba [18] is proposed to mitigate the challenges of SSMs. This approach introduces weight modulation during the propagation process, which substantially enlarges the effective receptive field and achieves impressive performance in NLP tasks. Besides, numerous studies aim to extend Mamba into computer vision, by employing various pre-defined strategies to map 2D image features into 1D sequences. ViM [70] and VMamba [38] utilize a multi-directional raster-scanning strategy, while LocalMamba [31] further confines its\n\n[^0]propagation range within a local window. They have successfully adapted Mamba to image inputs. Nevertheless, as shown in Fig. 1(a), both raster-scanning and local-scanning strategies introduce spatial discontinuities between adjacent pixels, and feature transformations in Mamba rely on the feature relationships, thereby impeding the effective information flow in a sequence. Additionally, PlainMamba [62] introduces a continuous scanning strategy, aiming to alleviate this issue by simply adjusting the propagation direction at discontinuous positions. However, all these methods rely on fixed propagation trajectories, which ignore the inherent spatial structure and cannot dynamically adjust the topology based on input. Therefore, this paper endeavors to explore a new perspective: introducing an input-aware topological network for feature propagation in state space models. To achieve it, we develop a tree state space model and propose a new framework, termed GrootVL, which adaptively generates a tree topology based on the input feature and then performs feature propagation on it. Specifically, two sub-networks, GrootV and GrootL, are designed for visual and language tasks respectively, which are illustrated in Fig.",
    "grootvl-2": "1(b) and Fig. 1(d). For visual tasks, motivated by $[64,50]$, we first utilize the dissimilarity between adjacent features to construct a minimum spanning tree on a four-connected planner graph. This process can adaptively encode the spatial and semantic information into a tree graph $[64,50]$. Then, we iteratively traverse each pixel, considering it as the root vertex, and aggregate the features of other pixels using the state transition function of Mamba. Intuitively, this operation requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels. However, given that the tree graph is acyclic, we propose a dynamic programming algorithm to achieve linear complexity propagation. With such an input-aware tree topology, our approach enables more effective long-range interactions while maintaining consistent linear complexity with Mamba. Furthermore, our method can also be applied to language tasks by constructing a tree typology based on the dissimilarity between token features, which overcomes the geometrical constraints of the text sequence. Using a similar aggregation process as GrootV, GrootL can significantly enhance the language representation of a pre-trained Large Language Model [18]. We conduct extensive experiments to validate the effectiveness of GrootV on multiple visual benchmarks, i.e. image classification on ImageNet [12], object detection and instance segmentation on MSCOCO [36] as well as semantic segmentation on ADE20K [68]. Results show that our method notably outperforms existing SSM-based methods for all benchmarks and achieves competitive performance with CNN and Transformer-based approaches. Moreover, with LoRA finetuning [30], GrootL demonstrates consistent improvements for a pre-trained large language model at minor training cost. ## 2 Related Work\n\n### 2.1 Conventional Vision Foundation Models\n\nThe evolution of deep neural networks has been a significant catalyst in machine vision perception. CNN-based models [27, 47, 32, 24, 56, 65, 35, 51, 66] firstly emerge as pivotal landmarks, with ResNet [27] notably standing out for its inventive residual connection module, garnering widespread adoption across diverse domains of visual recognition. Furthermore, more efficient convolution operations are formulated, such as depth-wise convolutions introduced by MobileNet [29], paving the way for lightweight models. Additionally, deformable convolution [10] has been proposed to enhance the receptive field. Subsequently, ViT [15] has significantly improved the vision recognition paradigm. It reformulates the architecture design and training mechanism by combining transformer architecture in natural language processing, aiming to improve computational efficiency and broaden the scope of applications. After research discourse is centred on hierarchical ViTs [40, 39, 11, 58, 14, 52, 5] which design networks by decreasing feature resolution across the backbone gradually. Furthermore, recent research built on CNN serves to re-emphasize the capabilities of convolutional networks. For example, InternImage [57] presents a large model based on deformable CNN, while UniRepLKNet [13] exhibits significant performance through large kernel convolution. ### 2.2 Explorations about State Space Models\n\nState space models (SSMs) have emerged as a novel class of models within the deep learning paradigm, showing significant potential for sequence transforming [22, 21, 48]. These methods have attracted significant attention due to their linear scalability with sequence length. The early method, LSSL [22], draws inspiration from continuous state space models in control systems and attempts to\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-03.jpg?height=583&width=1221&top_left_y=239&top_left_x=406)\n\nFigure 1: Comparison of different propagation strategies for multi-modal tasks. For visual tasks, the previous strategies (a) are based on fixed patterns, while our method can adaptively generate the propagation topology according to input features. For textual tasks, compared to previous methods (c), our approach (d) can break the inherent constraints of text sequences, facilitating the effective transmission of long-range information. address the long-range dependency problem through a combination with HIPPO [19] initialization. S4 [21] proposes to normalize the parameters into a diagonal matrix, prompting a subsequent series of research on structured SSMs [23, 20, 25, 18]. Recently, the Selective State Space Model [18], known as Mamba, strikes a balance between effectiveness and efficiency through the design of an input-dependent parameter initialization strategy, which has emerged as a formidable competitor to both transformer and CNN structures. In addition to showcasing superior outcomes in sequence modeling, Mamba has been seamlessly incorporated into the visual domain [70, 38, 31, 62]. These studies often rely on handcrafted fixed scanning mechanisms to mitigate the execution bias of the selective state space model on 2D non-causal images. However, such simplistic approaches cannot effectively capture spatial relationships in an input-dependent paradigm. To address this limitation, we propose an effective framework GrootVL in this work to enhance long-range modeling for both vision and language tasks by introducing an input-aware tree-based topological structure. ## 3 Method\n\nIn this section, we first revisit the selective state space model [18] and then elaborate on our inputaware topology scanning algorithm for state space modeling. With this superior algorithm, we develop a tree SSM and propose a novel framework called GrootVL, which consists of two sub-networks: GrootV for visual tasks and GrootL for fine-tuning a pre-trained language model [18]. ### 3.1 Revisiting Selective State Space Model\n\nState Space Models (SSMs) are commonly regarded as continuous linear time-invariant systems [59] that map input stimulation $x(t) \\in \\mathbb{R}^{1 \\times D}$ to output signal $y(t) \\in \\mathbb{R}^{1 \\times D}$ through a state vector $h(t) \\in \\mathbb{R}^{1 \\times N}$, where $t, D$ and $N$ indicate the time step, channel number of the signal and state size, respectively. These models can be formulated as the following linear ordinary differential equations:\n\n$$\nh^{\\prime}(t)=\\mathbf{A} h(t)+\\mathbf{B} x(t), \\quad y(t)=\\mathbf{C} h(t)+\\mathbf{D} x(t)\n$$\n\nwhere $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}, \\mathbf{B} \\in \\mathbb{R}^{N \\times D}, \\mathbf{C} \\in \\mathbb{R}^{N \\times D}$ and feedthrough coefficient $\\mathbf{D} \\in \\mathbb{R}^{D}$. Discretization. Although SSM serves as a powerful tool in systems and control engineering, its time-continuous nature poses challenges for integration into deep learning architectures. To alleviate this issue, most methods utilize the zero-order hold rule [18] to discretize the continuous system described by Eq. (1) and convert continuous variables (A, B, C, D) into corresponding discrete parameters $(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}, \\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})$ over the specified sampling time-scale $\\Delta \\in \\mathbb{R}^{D}$ :\n\n$$\n\\overline{\\mathbf{A}}=e^{\\Delta \\mathbf{A}}, \\quad \\overline{\\mathbf{B}}=\\left(e^{\\Delta \\mathbf{A}}-I\\right) \\mathbf{A}^{-1} \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}, \\quad \\overline{\\mathbf{D}}=\\mathbf{D}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-04.jpg?height=633&width=1405&top_left_y=231&top_left_x=358)\n\nFigure 2: Illustration of Tree State Space Model. With an image feature map $x$, we perform Tree Scanning Algorithm (TSA) to construct a 4 -connected graph with edge weights measured by dissimilarity between pixels. Then, we obtain an MST with vertices set $\\Omega$ through a pruning algorithm and perform the state transition for each vertex in this topology (detailed in Sec. 3.2). Red arrows describe the propagation source of vertex $i$. In addition, many improved methods $[38,18]$ use an approximation of $\\overline{\\mathbf{B}}$ based on the first-order Taylor Series:\n\n$$\n\\overline{\\mathbf{B}}=\\left(e^{\\Delta \\mathbf{A}}-I\\right) \\mathbf{A}^{-1} \\mathbf{B} \\approx(\\Delta \\mathbf{A})(\\Delta \\mathbf{A})^{-1} \\Delta \\mathbf{B}=\\Delta \\mathbf{B}\n$$\n\nSelective Mechanism . Previous SSMs store information through finite states and inherent timeinvariance, which limits their effectiveness. Therefore, Mamba [18] introduces a dynamic mechanism to selectively filter out input into a sequential state. Specifically, it utilizes Linear Projection to calculate the parameters $\\left\\{\\mathbf{B}_{i}\\right\\}_{i=1}^{L},\\left\\{\\mathbf{C}_{i}\\right\\}_{i=1}^{L}$ and $\\left\\{\\boldsymbol{\\Delta}_{i}\\right\\}_{i=1}^{L}$ from the input sequence $\\left\\{x_{i}\\right\\}_{i=1}^{L}$ with $x_{i} \\in \\mathbb{R}^{1 \\times D}$ directly to improve the context-aware ability. Then the output sequence $\\left\\{y_{i}\\right\\}_{i=1}^{L}$ can be computed with those input-adaptive discretized parameters as follows:\n\n$$\nh_{i}=\\overline{\\mathbf{A}}_{i} h_{i-1}+\\overline{\\mathbf{B}}_{i} x_{i}, \\quad y_{i}=\\mathbf{C}_{i} h_{i}+\\mathbf{D} x_{i}\n$$\n\n### 3.2 Tree State Space Model\n\nMamba [18] has showcased remarkable performance in modeling the dependencies of consecutive words in a sequence.",
    "grootvl-3": "However, its applicability in long-context tasks, especially visual modeling, still poses certain challenges. For visual tasks, many methods attempt to address this problem by employing fixed scanning strategies, such as multi-directional raster scan [38, 70], local scan [31], and continuous scan [62]. However, these handcrafted scanning methods fail to effectively preserve the 2D structural information of images. Following the design in Mamba [18], we construct a transform block as a tree state space model, which is presented in Fig. 2. The only difference between our block and Mamba lies in the replacement of the structured state space block with the proposed tree scanning algorithm. In the tree scanning algorithm, we generate a tree typology and then propagate the state of each vertex along the topological path to obtain strong feature representations. In addition, our algorithm can effectively enhance language representations by incorporating such a tree topology during text processing, which overcomes the geometrical constraints of text sequences. In the following, we elaborate on the proposed tree scanning algorithm and its applications for multi-modal tasks. Tree Scanning Algorithm. Given an input feature $X=\\left\\{x_{i}\\right\\}_{i=1}^{L}$ where $L$ is the sequence length (or the number of input pixels), we construct an undirected $m$-connected graph $G=(V, E)$ for the feature. $m$ is a hyper-parameter that indicates the number of adjacent tokens. Following [64, 50], we set $m=4$ for visual tasks, meaning each pixel is connected to its four neighboring pixels. For language tasks, we set $m=3$ by default, meaning each token is connected to the previous three tokens. In addition, the vertices $V$ represent the pixel (or token) embeddings, and the $E$ indicates\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-05.jpg?height=489&width=1397&top_left_y=230&top_left_x=359)\n\nFigure 3: Overview of GrootV.",
    "grootvl-4": "LN means LayerNorm and FFN is a feed-forward network in the basic block. S2 and P1 denote stride of 2 and padding size of 1 in convolution, respectively. the edges of the graph. The edge weight is calculated by the feature dissimilarity between adjacent vertices. Besides, the metric of dissimilarity uses cosine distance by default, and the comparison with other metrics refers to Table 5. We use the Contractive Boruvka algorithm [2] to prune the edges with significant dissimilarity, which generates a minimum spanning tree (MST) $\\mathcal{G}_{T}$ whose sum of dissimilarity weights is minimum out of all spanning trees. In the propagation process, we iteratively traverse each vertex, treating it as the root, and aggregate the features of the remaining vertices. Intuitively, applying state propagation within such a geometric configuration makes its preferential interactions among vertices with small spatial and feature distances. Following the Mamba, we employ the data-dependent transition matrix for state propagation. For a vertex $k$, we denote the transition matrix with its parent as $\\overline{\\mathbf{A}}_{k}$. Furthermore, following the Eq. (4), the state aggregation process for the $i$-th vertex can be formulated as:\n\n$$\nh_{i}=\\sum_{\\forall j \\in \\Omega} S\\left(E_{i j}\\right) \\overline{\\mathbf{B}}_{j} x_{j}, \\quad S\\left(E_{i j}\\right)=\\prod_{k \\in N_{i j}} \\overline{\\mathbf{A}}_{k}\n$$\n\nwhere $\\Omega$ denotes the index set of all vertices in the tree. $S\\left(E_{i j}\\right)$ represents the path weight of hyperedge $E_{i j}$ traced from $j$-th vertex to $i$-th vertex in the tree $\\mathcal{G}_{T}$, and $N_{i j}$ indicates the index set of all vertices on this hyperedge. For visual tasks, we iterate over each vertex, treating it as the root of the spanning tree $\\mathcal{G}_{T}$, and aggregate the states from the other vertices, thereby obtaining the transformed states $\\left\\{h_{i}\\right\\}_{i=1}^{L}$. For textual tasks, because of the causal prediction manner in large language models, we only take the last token as root and aggregate from other tokens. To achieve end-to-end training, we derive the derivative of the output hidden state $h_{i}$ to the input variables $\\overline{\\mathbf{A}}_{k}$, $\\overline{\\mathbf{B}}_{j}$ and $x_{j}$ as follows:\n\n$$\n\\begin{gathered}\n\\frac{\\partial h_{i}}{\\partial x_{j}}=S\\left(E_{i j}\\right) \\overline{\\mathbf{B}}_{j}, \\quad \\frac{\\partial h_{i}}{\\partial \\overline{\\mathbf{B}}_{j}}=S\\left(E_{i j}\\right) x_{j} \\\\\n\\frac{\\partial h_{i}}{\\partial \\overline{\\mathbf{A}}_{k}}=\\sum_{\\forall j \\in C_{k}^{i}} \\overline{\\mathbf{B}}_{j} x_{j} S\\left(E_{k j}\\right) S\\left(E_{i n}\\right)\n\\end{gathered}\n$$\n\nwhere $C_{k}^{i}$ indicates the children of vertex $k$ in tree $\\mathcal{G}_{T}$ whose root is the vertex $i$, and $n$ denotes the parent of vertex $k$ in Eq. (7). Finally, the output feature $Y$ can be formulated as:\n\n$$\nY=\\mathbf{C} \\odot N \\operatorname{Norm}(H)+\\mathbf{D} \\odot X\n$$\n\nwhere $Y, H$ and $X$ indicate the stack of $\\left\\{y_{i}\\right\\}_{i=1}^{L},\\left\\{h_{i}\\right\\}_{i=1}^{L}$ and $\\{x\\}_{i=1}^{L}$ respectively. $\\odot$ denotes the element-wise multiplication. Efficient Implementation for Multi-Modality. For visual tasks, the tree scanning algorithm requires two levels of traversal across the entire pixel set, resulting in an unacceptable quadratic complexity relative to the number of pixels $\\mathcal{O}\\left(L^{2}\\right)$. To alleviate this issue, we utilize a dynamic programming procedure to accelerate the inference and training processes as elaborated in Algorithm 1, which results in linear complexity $\\mathcal{O}(L)$. For textual tasks, we perform a unidirectional aggregation approach (shown in Algorithm 2 of Appendix B) in adherence to the causal nature of language. Moreover, we provide the back-propagation process for both Vision Tree Scanning and Language Tree Scanning processes, whose detailed proofs refer to Appendix C. ```\nAlgorithm 1 Vision Tree Scanning\nInput: Input feature \\(\\left\\{x_{i}\\right\\}_{i=1}^{L}\\); Input matrix \\(\\left\\{\\overline{\\mathbf{B}}_{i}\\right\\}_{i=1}^{L}\\); State matrix \\(\\left\\{\\overline{\\mathbf{A}}_{i}\\right\\}_{i=1}^{L}\\); Gradient of loss to\n    hidden states \\(\\left\\{\\frac{\\partial \\text { Loss }}{\\partial h_{i}}\\right\\}_{i=1}^{L}\\); Minimum Spanning Tree \\(\\mathcal{G}_{T}\\). Traverse Path: Root, ..., Leaf \\(\\leftarrow B F S\\left(\\mathcal{G}_{T}\\right) \\triangleright\\) Breadth-first topological order of \\(\\mathcal{G}_{T}\\)\nForward:\n    Initialization: \\(\\left\\{\\xi_{i}\\right\\}_{i=1}^{L} \\leftarrow\\left\\{x_{i}\\right\\}_{i=1}^{L}\\)\n    : for \\(i \\leftarrow\\) Leaf to Root do\n        \\(\\xi_{i}=\\overline{\\mathbf{B}}_{i} x_{i}+\\sum_{\\forall j \\in\\{t \\mid \\operatorname{Par}(t)=i\\}} \\xi_{j} \\overline{\\mathbf{A}}_{j}\\)\n    4: end for\n    for \\(i \\leftarrow\\) Root to Leaf do\n    6: \\(\\quad\\) if \\(i\\) is Root then\n                \\(h_{i}=\\xi_{i}\\)\n        else\n            \\(h_{i}=\\overline{\\mathbf{A}}_{i}\\left(h_{\\operatorname{Par}(i)}-\\overline{\\mathbf{A}}_{i} \\xi_{i}\\right)+\\xi_{i}=\\left(1-\\overline{\\mathbf{A}}_{i}^{2}\\right) \\xi_{i}+\\overline{\\mathbf{A}}_{i} h_{\\operatorname{Par}(i)}\\)\n        end if\n    end for\nBackward:\n12: Initialization: \\(\\left\\{\\eta_{i}\\right\\}_{i=1}^{L} \\leftarrow\\left\\{\\frac{\\partial \\text { Loss }}{\\partial h_{i}}\\right\\}_{i=1}^{L}\\)\n    for \\(i \\leftarrow\\) Leaf to Root do\n14: \\(\\quad \\eta_{i}=\\overline{\\mathbf{B}}_{i} \\frac{\\partial \\text { Loss }}{\\partial h_{i}}+\\sum_{\\forall j \\in\\{t \\mid \\operatorname{Par}(t)=i\\}} \\eta_{j} \\overline{\\mathbf{A}}_{j}\\)\n    end for\n16: for \\(i \\leftarrow\\) Root to Leaf do\n        if \\(i\\) is Root then\n            \\(\\frac{\\partial \\text { Loss }}{\\partial x_{i}}=\\eta_{i} \\overline{\\mathbf{B}}_{i}, \\quad \\frac{\\partial L \\text { oss }}{\\partial \\mathbf{B}_{i}}=\\eta_{i} x_{i}, \\quad \\frac{\\partial \\text { Loss }}{\\partial \\mathbf{A}_{i}}=0\\)\n        else\n            \\(\\frac{\\partial L o s s}{\\partial x_{i}}=\\left(1-\\overline{\\mathbf{A}}_{i}^{2}\\right) \\eta_{i} \\overline{\\mathbf{B}}_{i}+\\overline{\\mathbf{A}}_{i} \\frac{\\partial \\text { Loss }}{\\partial x_{\\operatorname{Par}(i)}} \\overline{\\mathbf{B}}_{i}, \\quad \\frac{\\partial L o s s}{\\partial \\overline{\\mathbf{B}}_{i}}=\\left(1-\\overline{\\mathbf{A}}_{i}^{2}\\right) \\eta_{i} x_{i}+\\overline{\\mathbf{A}}_{i} \\frac{\\partial L o s s}{\\partial \\mathbf{B}_{\\operatorname{Par}(i)}} x_{i}\\)\n            \\(\\frac{\\partial L o s s}{\\partial \\mathbf{A}_{i}}=\\eta_{i} *\\left(h_{i}-\\overline{\\mathbf{A}}_{i} \\xi_{i}\\right)+\\xi_{i} *\\left(\\frac{\\partial \\text { Loss }}{\\partial x_{i}}-\\overline{\\mathbf{A}}_{i} \\eta_{i}\\right)=\\eta_{i} h_{i}+\\xi_{i} \\frac{\\partial L o s s}{\\partial x_{i}}-2 \\eta_{i} \\xi_{i} \\overline{\\mathbf{A}}_{i}\\)\n        end if\n    end for\n```\n\nOutput: Hidden states $\\left\\{h_{i}\\right\\}_{i=1}^{L}$; Grad. of loss to input feature $\\left\\{\\frac{\\partial L o s s}{\\partial x_{i}}\\right\\}_{i=1}^{L}$; Grad. of loss to input matrix $\\left\\{\\frac{\\partial L o s s}{\\partial \\mathbf{B}_{i}}\\right\\}_{i=1}^{L}$; Grad. of loss to state matrix $\\left\\{\\frac{\\partial L o s s}{\\partial \\mathbf{A}_{i}}\\right\\}_{i=1}^{L}$. ### 3.3 Application for Vision and Language\n\nGrootV Given an image with a shape of $H \\times W \\times 3$, our goal is to obtain high-quality visual features for downstream tasks. To this end, we propose an effective vision architecture GrootV which consists of a stem module, several basic blocks and downsampling layers to generate hierarchical representations illustrated in Fig. 3. Overall, our GrootV comprises four stages similar to previous general vision backbones [41, 40, 57, 38]. We integrate the stem module before the first stage to decrease the resolution of the input image signal by a factor of 4 , resulting in a feature map with a shape of $\\frac{H}{4} \\times \\frac{W}{4} \\times C$. It includes two convolutions, two Layer Normalization (LN) layers and one GELU activation function. The kernel size for both convolutions is 3 with a stride of 2 and padding of 1. Similarly, a downsampling layer consists of a $3 \\times 3$ convolution with a stride of 2 and padding of 1 and an LN layer. Positioned between two stages, it serves to downsample the input feature map by a factor of 2 . Motivated by $[57,38]$, we devise a residual block with skip connections to integrate our fundamental Tree State Space Model in Sec. 3.2. In detail, we first normalize the input features with LN layer. Spatial priors and long-range dependencies are then obtained through our tree scanning algorithm with residual connections established alongside the input features.",
    "grootvl-5": "Finally, a feedforward neural network is utilized to project the normalized features to output signals as shown in Fig. 3. Based on the above origin components, we develop our GrootV in three scales, i.e., GrootV-Tiny, GrootV-Small and GrootV-Base. GrootL Recurrent neural networks rely on fixed memory to preserve past information, which poses limitations when handling long contexts where relevant words are distant from the current moment. While Mamba [18] employs a selection mechanism to enhance context awareness, its fixed memory\n\n| Method | Type \\#Param. \\#FLOPs |  |  | Top-1 <br> Acc. |\n| :---: | :---: | :---: | :---: | :---: |\n| Deit-S [54] | T | 22 M | 4.6G | 79.9 |\n| Swin-T [40] | T | 28 M | 4.6 G | 81.3 |\n| CoAtNet-0 [11] | T | 25 M | 4.0 G | 81.6 |\n| SG-Former-S [46] | T | 23 M | 4.8 G | 83.2 |\n| ConvNeXt-T [41] | C | 29 M | 4.5 G | 82.1 |\n| SLaK-T [37] | C | 30 M | 5.0G | 82.5 |\n| UniRepLKNet-T [13] | C | 31 M | 4.9 G | 83.2 |\n| InternImage-T [57] | C | 30 M | 5.0G | 83.5 |\n| ViM-S [70] | S | 26 M | 5.1G | 80.5 |\n| LocalViM-S [31] | S | 28 M | 4.8 G | 81.2 |\n| PlainMamba-L2 [62] | S | 25 M | 8.1G | 81.6 |\n| Mamba-2D-S [34] | S | 24 M | - | 81.7 |\n| S4ND-ConvNeXt-T [44] | S | 30 M | ${ }^{-}$ | 82.2 |\n| VMamba-T [38] | S | 31 M | 4.9 G | 82.5 |\n| LocalVMamba-T [31] | S | 26 M | 5.7 G | 82.7 |\n| GrootV-T (Ours) | S | 30 M | 4.8 G | 83.4 |\n| Swin-S [40] | T | 50M | 8.7G | 83.0 |\n| CoAtNet-1 [11] | T | 42M | 8.0G | 83.3 |\n\n\n| Method | Type \\#Param. \\#FLOPs | Top-1 <br> Acc. |  |  |\n| :--- | :---: | :---: | :---: | :---: |\n| ConvNeXt-S [41] | C | 50 M | 8.7 G | 83.1 |\n| SLaK-S [37] | C | 55 M | 9.8 G | 83.8 |\n| UniRepLKNet-S [13] | C | 56 M | 9.1 G | 83.9 |\n| InternImage-S [57] | C | 50 M | 8.0 G | 84.2 |\n| HyenaViT-B [16] | S | 88 M | - | 78.5 |\n| S4ND-ViT-B [44] | S | 89 M | - | 80.4 |\n| PlainMamba-L3 [62] | S | 50 M | 14.4 G | 82.3 |\n| VMamba-S [38] | S | 50 M | 8.7 G | 83.6 |\n| LocalVMamba-S [31] | S | 50 M | 11.4 G | 83.7 |\n| GrootV-S (Ours) | S | 51 M | 8.5 G | $\\mathbf{8 4 . 2}$ |\n| Deit-B [54] | T | 86 M | 55.4 G | 83.1 |\n| Swin-B [40] | T | 88 M | 15.4 G | 83.5 |\n| CoAtNet-2 [11] | T | 75 M | 16.0 G | 84.1 |\n| ConvNeXt-B [41] | C | 89 M | 15.4 G | 83.8 |\n| SLaK-B [37] | C | 95 M | 17.0 G | 84.0 |\n| Mamba-2D-B [34] | S | 92 M | - | 83.0 |\n| VMamba-B [38] | S | 89 M | 15.4 G | 83.9 |\n| GrootV-B (Ours) | S | 91 M | 15.1 G | $\\mathbf{8 4 . 8}$ |\n\nTable 1: Image classification performance on the ImageNet-1K validation set. T, C and S indicate the model type of Transformer, CNN and SSM, respectively. All models take a scale of $224^{2}$ as input. size cannot expand over time, resulting in restricted state space. Therefore, the ability to extrapolate decreases during scrolling as the prompt extends. To mitigate this issue, we propose an effective fine-tuning paradigm. Specifically, the tree-based topology branch is built upon one-way scrolling with a scaling factor, enabling state transitions within such a structure. This arrangement facilitates the preferential interaction of semantically related tokens. It is noteworthy that this paradigm does not introduce any additional training parameters. Instead, it utilizes pretrained state transformation parameters to conduct semantic aggregation by incorporating topological structures. Experimental results demonstrate the effectiveness of our approach. ## 4 Experiments\n\nWe conduct extensive experiments to evaluate the effectiveness of GrootV and compare it with advanced CNN-based, Transformer-based, and SSM-based models covering various downstream tasks, including image classification, object detection and semantic segmentation. Furthermore, we validate the capability of GrootL in the field of natural language understanding. ### 4.1 Image Classification\n\nSettings. We assess the classification performance of GrootV on the ImageNet-1k dataset [12]. Following previous practices [40, 41, 57, 38], all GrootV models are trained for 300 epochs from scratch using AdamW optimizer with a warm-up strategy of 20 epochs. During training, we utilize a Cosine Scheduler with an initial learning rate of $1 \\times 10^{-3}$ and weight decay of 0.05 .",
    "grootvl-6": "In addition, the exponential moving average (EMA) is also applied. Results. The comparison results summarized in Table 1 show GrootV leading all SSM-based models and competitive with advanced CNNs and Transformers across tiny, small, and base scales. Specifically, GrootV-T achieves $83.4 \\%$ Top-1 Acc. boosting ViM-S by $2.9 \\%$, LocalVim-S by $2.2 \\%$, PlainMamba-L2 by $1.8 \\%$ and VMamba-T by $0.9 \\%$ with similar FLOPs. Additionally, it surpasses ConvNeXt-T by $1.3 \\%$ and Swin-T by $2.2 \\%$, demonstrating the effectiveness of our method. ### 4.2 Object Detection\n\nSettings. We verify the detection performance of GrootV on the MSCOCO 2017 dataset [36] with MMDetection library [3]. We follow previous works [38, 57, 40, 31, 49, 51, 67, 63, 6] to validate object detection and instance segmentation tasks with Mask-RCNN [26]. Specifically, We adopt the\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-08.jpg?height=786&width=713&top_left_y=236&top_left_x=362)\n\nFigure 4: Visualization of affinity maps in the specific position. The Location is marked by the red cross in each input (a). TP is our tree topology scanning algorithm (b), which captures more detailed structural information and has a larger receptive field compared to raster scanning (c). | Method | Type \\#FLOPs |  | mIoU mIoU |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  |  |  | SS |  |\n| Swin-T [40] | T | 945G | 44.5 | 45.8 |\n| ConvNeXt-T [41] | C | 939G | 46.0 | 46.7 |\n| SLaK-T [37] | C | 936G | 47.6 |  |\n| InternImage-T [57] | C | 944G | 47.9 | 48.1 |\n| UniRepLKNet-T [13] | C | 946G | 48.6 | 49.1 |\n| ViM-S [70] | S |  | 44.9 |  |\n| LocalViM-S [31] | S | 297G | 46.4 | 47.5 |\n| PlainMamba-L2 [62] | S | 285G | 46.8 | - $\\square$ |\n| VMamba-T [38] | S | 964G | 47.3 | 48.3 |\n| LocalVMamba-T [3 | S | 970G | 47.9 | 49.1 |\n| GrootV-T (Ours) | S | 941G | 48.5 | 49.4 |\n| Swi | T | 1038G | 47.6 | 49.5 |\n| ConvNeXt-S [41] | C | 1027G | 48.7 | 49.6 |\n| SLaK-S [37] | C | 1028G | 49.4 | - |\n| InternImage-S [57] | C | 1017G | 50.1 | 50.9 |\n| UniRepLKNet-S [13] | C | 1036G | 50.5 | 51.0 |\n| PlainMamba-L3 [62] | S | 419G | 49.1 | - |\n| VMamba-S [38] | S | 1081G | 49.5 | 50.5 |\n| LocalVMamba-S [31] | S | 1095G | 50.0 | 51.0 |\n| GrootV-S (Ours) | S | 1019G | 50.7 | 51.7 |\n\nTable 2: Semantic segmentation performance on ADE20K val set.",
    "grootvl-7": "The crop size is all set to $512^{2}$. SS and MS denote singlescale and multi-scale testing, respectively. AdamW optimizer with a learning rate of $1 \\times 10^{-4}$ and batch size of 16 to optimize the model built upon our pre-trained classification backbones on ImageNet-1K. The training schedules include $1 \\times$ ( 12 epochs) and $3 \\times$ ( 36 epochs) with multi-scale data augmentation. Results. As depicted in Table 7 (in Appendix A.), our method outperforms existing methods on most evaluation metrics, especially for instance segmentation. Under $1 \\times$ schedule, GrootV-T achieves 47.0 in box mAP $\\left(\\mathrm{AP}^{b}\\right)$, which is 1.1 points higher than ViM-S and 0.5 points higher than VMamba-T. It is worth noting that GrootV-T outperforms ViM-S by 1.7 points with $1 \\times$ schedule and LocalVMamba-T by 0.4 points with $3 \\times$ schedule in mask $\\mathrm{mAP}\\left(\\mathrm{AP}^{m}\\right)$. Moreover, the best $\\mathrm{AP}^{b} 50.1$ and $\\mathrm{AP}^{m} 44.6$ are obtained by GrootV-S in $3 \\times$ schedule with multi-scale training. ### 4.3 Semantic Segmentation\n\nSettings. To evaluate the semantic segmentation performance of our GrootV series, we train our models with UperNet [60] initialized by pre-trained classification weights on ADE20K[68] for 160k iterations, following common practices without additional augmentations for fair comparison. Results. Our method performs exceptionally well on segmentation tasks shown in Table 2. GrootVT yields a clear improvement of +3.6 in single-scale mIoU compared to ViM-S and +1.9 in multiscale mIoU compared to LocalViM-S. Furthermore, GrootV-S boosts InterImage-S by 0.6 and 0.8 in single-scale and multi-scale respectively. We consider the preservation of intricate structural details through tree topology scanning to be particularly advantageous for segmentation tasks that require pixel-level perception. ### 4.4 Language Understanding\n\nWe regard Mamba [18] with 130M parameters as the base model. To verify the effectiveness of our GrootL in nature language understanding, we first fine-tune pre-trained Mamba via LoRA [30] and GrootL under the same setting with the Alpaca data [53], which contains 52000 instruction tuning data for supervised fine-tuning. Then we utilize popular language benchmarks provided in the open-sourced lm-evaluation-harness project [17] for evaluation, including PIQA [1], AI2-ARC [8],\n\n| Method | PIQA $\\uparrow$ | Arc-E $\\uparrow$ | SST $\\uparrow$ | WG $\\uparrow$ | L-ppl $\\downarrow$ | Race $\\uparrow$ | BQA $\\uparrow$ | Average <br> $\\uparrow$ <br> Acc. |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Mamba [18] | 64.5 | 48.0 | 65.6 | 51.8 | 16.1 | 27.4 | 16.8 | 45.7 |\n| + LoRA [30] | 64.7 | 48.3 | 65.1 | $\\mathbf{5 2 . 2}$ | 17.7 | 28.6 | 17.8 | 46.1 |\n| + GrootL (Ours) | $\\mathbf{6 5 . 0}$ | $\\mathbf{4 9 . 8}$ | $\\mathbf{6 9 . 5}$ | 51.1 | $\\mathbf{1 5 . 9}$ | $\\mathbf{2 8 .",
    "grootvl-8": "9}$ | $\\mathbf{1 9 . 2}$ | $\\mathbf{4 7 . 2}$ |\n\nTable 3: Evaluation on language model benchmarks. Arc-E, WG, L-ppl and BQA indicate Arceasy [8], WinoGrande, LAMBADA [45] and Openbookqa [43] benchmark, respectively. ![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-09.jpg?height=304&width=1399&top_left_y=588&top_left_x=363)\n\nTable 4: Effectiveness of our distance Metrics. traversing all vertices. SST [55], WinoGrande, LAMBADA [45], Race [33] and Openbookqa [43]. The results in Table 3 demonstrate that our GrootL provides a benefit of $+1.1 \\%$ in average Acc. compared to LoRA. Since the short prompt length of WinoGrande dataset, the performance degrades with a marginal gap. ### 4.5 Ablation Study \\& Qualitative Results\n\nIn this section, we conduct analysis experiments on ImageNet-1K dataset and present some visual results to illustrate the effectiveness of our algorithm. Scanning Strategy. We conduct a head-to-head comparison of different scanning strategies, as shown in Table 4. The tree topology scanning outperforms previous strategies by $0.8 \\%$ and $0.3 \\%$, highlighting the superiority of our algorithm in vision recognition. Distance Metric. Before generating a minimum spanning tree from a connected graph, it is important to measure the edge weights between vertices. Therefore, we validate several distance metrics as illustrated in Table 5. The results indicate that Cosine distance most effectively represents the relationship between vertices, performing $0.5 \\%$ better than Manhattan and $0.2 \\%$ better than Euclidean. Root Setting. We traverse all vertices, treating each as a root, and perform state transitions along the topological path from the other vertices toward the root. This traversal ensures that each vertex captures long-range dependencies. To verify the effectiveness of this operation, we consider only the first and last vertices as the root in Table 6. The results show reductions of $0.5 \\%$ and $0.4 \\%$, respectively. Qualitative Results. To better illustrate the superiority of our scanning strategy, we visualize the affinity maps of different positions marked by the red cross in each input image. For example, we set the anchor point in the upper left corner of the sky as shown in the second row of in Fig. 4(a). Our method can easily identify white houses, flagpoles, and the sky, which raster scanning fails to achieve. This demonstrates the capability of our algorithm to preserve detailed structural information. More comparisons can be seen in Fig. 6 (in Appendix D.)\n\n## 5 Conclusion \\& Limitations\n\nIn this paper, we propose a tree state space model to perform feature propagation on an input-aware topology. Besides, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. With the proposed techniques, we establish the general multi-modal networks to break the original sequence constraints and achieve stronger representation capabilities. Extensive experiments demonstrate the effectiveness of our method in both visual and language tasks. The limitation of our method is that the tree structure is not a common paradigm, and it needs to be specifically optimized according to the hardware device. ## References\n\n[1] Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al.: Piqa: Reasoning about physical commonsense in natural language.",
    "grootvl-9": "In: AAAI. pp. 7432-7439 (2020) 8\n[2] Bor\u016fvka, O.: O jist\u00e9m probl\u00e9mu minim\u00e1ln\u00edm (1926) 5\n[3] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019) 7\n[4] Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., Qiao, Y.: Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534 (2022) 15\n[5] Cheng, C., Song, L., Xue, R., Wang, H., Sun, H., Ge, Y., Shan, Y.: Meta-adapter: An online few-shot learner for vision-language model. arXiv preprint arXiv:2311.03774 (2023) 2\n[6] Cheng, T., Song, L., Ge, Y., Liu, W., Wang, X., Shan, Y.: Yolo-world: Real-time openvocabulary object detection. arXiv preprint arXiv:2401.17270 (2024) 7\n[7] Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014) 1\n[8] Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have solved question answering? try arc, the ai 2 reasoning challenge. arXiv preprint arXiv: 1803.05457 (2018) 8, 9\n[9] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: CVPR. pp. 113-123 (2019) 14\n[10] Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks. In: ICCV. pp. 764-773 (2017) 2\n[11] Dai, Z., Liu, H., Le, Q.V., Tan, M.: Coatnet: Marrying convolution and attention for all data sizes. NeurIPS 34, 3965-3977 (2021) 2, 7\n[12] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database.",
    "grootvl-10": "In: CVPR. pp. 248-255. Ieee (2009) 2, 7\n[13] Ding, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. CVPR (2023) 1, 2, 7, 8\n[14] Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: CVPR. pp. 12124-12134 (2022) $1,2,15$\n[15] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 1, 2\n[16] Fu, D., Arora, S., Grogan, J., Johnson, I., Eyuboglu, E.S., Thomas, A., Spector, B., Poli, M., Rudra, A., R\u00e9, C.: Monarch mixer: A simple sub-quadratic gemm-based architecture. NeurIPS 36 (2023) 7\n[17] Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., Zou, A.: A framework for few-shot language model evaluation (12 2023) 8\n[18] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023) 1, 2, 3, 4, 6, 8, 9\n[19] Gu, A., Dao, T., Ermon, S., Rudra, A., R\u00e9, C.: Hippo: Recurrent memory with optimal polynomial projections. NeurIPS 33, 1474-1487 (2020) 3\n[20] Gu, A., Goel, K., Gupta, A., R\u00e9, C.: On the parameterization and initialization of diagonal state space models. NeurIPS 35, 35971-35983 (2022) 3\n[21] Gu, A., Goel, K., R\u00e9, C.: Efficiently modeling long sequences with structured state spaces. In: $\\operatorname{ICLR}(2022) 1,2,3$\n[22] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., R\u00e9, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. NeurIPS 34, 572-585 (2021) 2\n[23] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. NeurIPS 35, 22982-22994 (2022) 1, 3\n[24] Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Wu, E., Tian, Q.: Ghostnets on heterogeneous devices via cheap operations. IJCV 130(4), 1050-1069 (2022) 2\n[25] Hasani, R., Lechner, M., Wang, T.H., Chahine, M., Amini, A., Rus, D.: Liquid structural state-space models. arXiv preprint arXiv:2209.12951 (2022) 3\n[26] He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: ICCV. pp. 2961-2969 (2017) 7\n[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.",
    "grootvl-11": "In: CVPR. pp. 770-778 (2016) 1, 2\n[28] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735-1780 (1997) 1\n[29] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017) 1, 2\n[30] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. In: ICLR (2022) 2, 8, 9\n[31] Huang, T., Pei, X., You, S., Wang, F., Qian, C., Xu, C.: Localmamba: Visual state space model with windowed selective scan. arXiv preprint arXiv:2403.09338 (2024) 1, 3, 4, 7, 8, 14, 15\n[32] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. NeurIPS 25 (2012) 2\n[33] Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.H.: RACE: large-scale reading comprehension dataset from examinations. In: EMNLP. pp. 785-794. Association for Computational Linguistics (2017) 9\n[34] Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multi-dimensional data. arXiv preprint arXiv:2402.05892 (2024) 7\n[35] Li, Y., Song, L., Chen, Y., Li, Z., Zhang, X., Wang, X., Sun, J.: Learning dynamic routing for semantic segmentation. In: CVPR (2020) 2\n[36] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740-755. Springer (2014) 2, 7\n[37] Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., K\u00e4rkk\u00e4inen, T., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51 x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022) 7, 8\n[38] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024) 1, 3, 4, 6, 7, 8, 14, 15\n[39] Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al.: Swin transformer v2: Scaling up capacity and resolution. In: CVPR. pp. 12009-12019 (2022) 1,2\n[40] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV. pp. 10012-10022 (2021) 1, 2, $6,7,8,14,15$\n[41] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s.",
    "grootvl-12": "In: CVPR. pp. 11976-11986 (2022) 1, 6, 7, 8, 15\n[42] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv: 1711.05101 (2017) 14\n[43] Mihaylov, T., Clark, P., Khot, T., Sabharwal, A.: Can a suit of armor conduct electricity? A new dataset for open book question answering. In: EMNLP. pp. 2381-2391. Association for Computational Linguistics (2018) 9\n[44] Nguyen, E., Goel, K., Gu, A., Downs, G.W., Shah, P., Dao, T., Baccus, S.A., R\u00e9, C.: S4nd: Modeling images and videos as multidimensional signals using state spaces. arXiv preprint arXiv:2210.06583(2022) 7\n[45] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 9,18\n[46] Ren, S., Yang, X., Liu, S., Wang, X.: Sg-former: Self-guided transformer with evolving token reallocation.",
    "grootvl-13": "In: ICCV. pp. 6003-6014 (2023) 7\n[47] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: Bengio, Y., LeCun, Y. (eds.) ICLR (2015) 2\n[48] Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933 (2022) 1, 2\n[49] Song, L., Li, Y., Jiang, Z., Li, Z., Sun, H., Sun, J., Zheng, N.: Fine-grained dynamic head for object detection. NIPS (2020) 7\n[50] Song, L., Li, Y., Li, Z., Yu, G., Sun, H., Sun, J., Zheng, N.: Learnable tree filter for structurepreserving feature transform. NeurIPS 32 (2019) 2, 4\n[51] Song, L., Zhang, S., Yu, G., Sun, H.: Tacnet: Transition-aware context network for spatiotemporal action detection. In: CVPR (2019) 2, 7\n[52] Song, L., Zhang, S., Liu, S., Li, Z., He, X., Sun, H., Sun, J., Zheng, N.: Dynamic grained encoder for vision transformers. NIPS (2021) 2\n[53] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.: Stanford alpaca: An instruction-following llama model (2023) 8\n[54] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers \\& distillation through attention.",
    "grootvl-14": "In: ICML. pp. 10347-10357. PMLR (2021) 1,7\n[55] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-task benchmark and analysis platform for natural language understanding. In: ICLR (2019) 9\n[56] Wang, J., Song, L., Li, Z., Sun, H., Sun, J., Zheng, N.: End-to-end object detection with fully convolutional network. In: CVPR (2021) 2\n[57] Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. In: CVPR. pp. 14408-14419 (2023) 1, 2, 6, 7, 8, 14, 15\n[58] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.",
    "grootvl-15": "In: ICCV. pp. 568-578 (2021) 2\n[59] Williams, R.L., Lawrence, D.A., et al.: Linear state-space control systems. John Wiley \\& Sons (2007) 3\n[60] Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding.",
    "grootvl-16": "In: ECCV. pp. 418-434 (2018) 8\n[61] Xiao, Y., Luo, Z., Liu, Y., Ma, Y., Bian, H., Ji, Y., Yang, Y., Li, X.: Bridging the gap: A unified video comprehension framework for moment retrieval and highlight detection. CVPR (2024) 14\n[62] Yang, C., Chen, Z., Espinosa, M., Ericsson, L., Wang, Z., Liu, J., Crowley, E.J.: Plainmamba: Improving non-hierarchical mamba in visual recognition. arXiv preprint arXiv:2403.17695 (2024) $2,3,4,7,8$\n[63] Yang, J., Song, L., Liu, S., Li, Z., Li, X., Sun, H., Sun, J., Zheng, N.: Dbq-ssd: Dynamic ball query for efficient 3d object detection. arXiv preprint arXiv:2207.10909 (2022) 7\n[64] Yang, Q.: Stereo matching using tree filtering. IEEE TPAMI 37(4), 834-846 (2014) 2, 4\n[65] Yang, R., Song, L., Ge, Y., Li, X.: Boxsnake: Polygonal instance segmentation with box supervision. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2023) 2\n[66] Zhang, S., Song, L., Gao, C., Sang, N.: Glnet: Global local network for weakly supervised action localization. IEEE Transactions on Multimedia 22(10), 2610-2622 (2019) 2\n[67] Zhang, S., Song, L., Liu, S., Ge, Z., Li, Z., He, X., Sun, J.: Workshop on autonomous driving at cvpr 2021: Technical report for streaming perception challenge. arXiv preprint arXiv:2108.04230 (2021) 7\n[68] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset.",
    "grootvl-17": "In: CVPR. pp. 633-641 (2017) 2, 8\n[69] Zhou, H., Yang, R., Zhang, Y., Duan, H., Huang, Y., Hu, R., Li, X., Zheng, Y.: Unihead: unifying multi-perception for detection heads. TNNLS (2023) 14\n[70] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024) $1,3,4,7,8,15$\n\n## Appendix\n\n## A Detailed Training Settings and Results\n\n## A. 1 Image Classification. We follow the previous works $[57,38,40]$ to conduct the experiments. The models are trained with thirty-two 32GB V100 GPUs by default. We set betas and momentum of the AdamW [42, 69, 61] optimizer with $(0.9,0.999)$ and 0.9 , respectively. During training, we utilize a Cosine Scheduler with an initial learning rate of $1 \\times 10^{-3}$ and weight decay of 0.05 . We adopt the common training data augmentation strategies following [31, 57], including AutoAugment [9] with rand-m9-mstd0.5-inc1. A MixUp strategy with a ratio of 0.8 is also adopted in each batch. Horizontal flip and Random resized crop strategy are both used in the process of training. ![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-14.jpg?height=589&width=706&top_left_y=817&top_left_x=707)\n\nFigure 5: Classification performance comparison among SSM-based vision foundation models. Performance Comparison. We compare various SSM-based visual foundation models as shown in Fig. 5, with different colors representing different models and different shapes indicating different model scales. The size of each shape indicates the number of model parameters. The horizontal axis denotes FLOPs and the vertical axis represents the Top-1 accuracy of the corresponding method on ImageNet-1K val dataset. Fig. 5 demonstrates that GrootV is the best choice in terms of efficiency and effectiveness. ## A. 2 Object Detection. For a fair comparison, we conduct the evaluation following common practice [57, 38, 40]. The models are trained with eight 32GB V100 GPUs by default. The input image is resized so that the shorter side is 800 pixels, while the longer side does not exceed 1333 pixels during the $1 \\times$ schedule. The number of warmup steps is set to 500 in the $1 \\times$ schedule. For $3 \\times$ schedule, the shorter side is resized to $480-800$ pixels and the longer side does not exceed 1333 pixels. The number of warmup steps is set to 1000 in $3 \\times$ schedule. Results shown in Table 7 demonstrate the effectiveness of GrootV in object detection and instance segmentation on COCO val2017. ## A. 3 Semantic Segmentation. We optimize our GrootV-T/S using AdamW optimizer with an initial learning rate of $6 \\times 10^{-5}$ which is decayed by a rate of 1.0 with the polynomial decay schedule following [57]. The number of warmup iters is set to 1600 with an initial learning rate of $1 \\times 10^{-6}$. The default input resolution is $512 \\times 512$ as well as FLOPs are calculated with an input size of $512 \\times 2048$. The models are trained with eight 32 GB V100 GPUs by default. | Method | \\#FLOPs. | Mask R-CNN $1 \\times$ Schedule |  |  |  |  |  | Mask R-CNN $3 \\times$ MS Schedule |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | $\\mathrm{AP}^{b}$ | $\\mathrm{AP}_{50}^{b}$ | $\\mathrm{AP}_{75}^{b}$ | $\\mathrm{AP}^{m}$ | $\\mathrm{AP}_{50}^{m}$ | $\\mathrm{AP}_{75}^{m}$ | $\\mathrm{AP}^{b}$ | $\\mathrm{AP}_{50}^{b}$ | $\\mathrm{AP}_{75}^{b}$ | $\\mathrm{AP}^{m}$ | $\\mathrm{AP}_{50}^{m}$ | $\\mathrm{AP}_{75}^{m}$ |\n| Swin-T [40] | 267G | 42.7 | 65.2 | 46.8 | 39.3 | 62.2 | 42.2 | 46.0 | 68.1 | 50.3 | 41.6 | 65.1 | 44.9 |\n| ConvNeXt-T [41] | $262 G$ | 44.2 | 66.6 | 48.3 | 40.1 | 63.3 | 42.8 | 46.2 | 67.9 | 50.8 | 41.7 | 65.0 | 44.9 |\n| CSWin-T [14] | 279 G | 46.7 | 68.6 | 51.3 | 42.2 | 65.6 | 45.4 | 49.0 | 70.7 | 53.7 | 43.6 | 67.9 | 46.6 |\n| ViM-S [70] | 218 G | 44.9 | 67.1 | 49.3 | 41.0 | 64.2 | 44.1 |  | - | - | - | - | - |\n| VMamba-T [38] | 286G | 46.5 | 68.5 | 50.7 | 42.1 | 65.5 | 45.3 | 48.5 | 69.9 | 52.9 | 43.2 | 66.8 | 46.3 |\n| L-Vmamba-T [31] | 291G | 46.7 | 68.7 | 50.8 | 42.2 | 65.7 | 45.5 | 48.7 | 70.1 | 53.0 | 43.4 | 67.0 | 46.4 |\n| GrootV-T (Ours) | 265G | 47.0 | 69.4 | 51.5 | 42.7 | 66.4 | 46.0 | 49.0 | 70.8 | 54.0 | 43.8 | 67.6 | 47.1 |\n| Vit-Adapter-S [4] | 403G | 44.7 | 65.8 | 48.3 | 39.9 | 62.5 | 42.8 | 48.2 | 69.7 | 52.5 | 42.8 | 66.4 | 45.9 |\n| Swin-S [40] | 354 G | 44.8 | 66.6 | 48.9 | 40.9 | 63.4 | 44.2 | 48.2 | 69.8 | 52.8 | 43.2 | 67.0 | 46.1 |\n| ConvNeXt-T [41] | 348 G | 45.4 | 67.9 | 50.0 | 41.8 | 65.2 | 45.1 | 47.9 | 70.0 | 52.7 | 42.9 | 66.9 | 46.2 |\n| InternImage-S [57] | 340G | 47.8 | 69.8 | 52.8 | 43.3 | 67.1 | 46.7 | 49.7 | 71.1 | 54.5 | 44.5 | 68.5 | 47.8 |\n| VMamba-S [38] | 400 G | 48.2 | 69.7 | 52.5 | 43.0 | 66.6 | 46.4 | 49.7 | 70.4 | 54.2 | 44.0 | 67.6 | 47.3 |\n| L-Vmamba-S [31] | 414G | 48.4 | 69.9 | 52.7 | 43.2 | 66.7 | 46.5 | 49.9 | 70.5 | 54.4 | 44.1 | 67.8 | 47.4 |\n| GrootV-S (Ours) | 341G | 48.6 | 70.3 | 53.5 | 43.6 | 67.5 | 47.1 | 50.1 | 71.2 | 54.9 | 44.6 | 68.7 | 47.8 |\n\nTable 7: Object detection and instance segmentation performance on COCO val2017.",
    "grootvl-18": "$\\mathrm{AP}^{b}$ and $\\mathrm{AP}^{m}$ indicate the mAP of detection and segmentation, respectively. MS indicates the multi-scale training strategy. ## B Language Tree Topology Scanning Operator\n\n```\nAlgorithm 2 Language Tree Scanning\nInput: Input feature \\(\\left\\{x_{i}\\right\\}_{i=1}^{L}\\); Input matrix \\(\\left\\{\\overline{\\mathbf{B}}_{i}\\right\\}_{i=1}^{L}\\); State matrix \\(\\left\\{\\overline{\\mathbf{A}}_{i}\\right\\}_{i=1}^{L}\\); Gradient of loss to\n        hidden states \\(\\left\\{\\frac{\\partial \\text { Loss }}{\\partial h_{i}}\\right\\}_{i=1}^{L}\\); Minimum Spanning Tree \\(\\mathcal{G}_{T}\\). Traverse Path: Root, ..., Leaf \\(\\leftarrow B F S\\left(\\mathcal{G}_{T}\\right) \\triangleright\\) Breadth-first topological order of \\(\\mathcal{G}_{T}\\)\nForward:\n        Initialization: \\(\\left\\{\\xi_{i}\\right\\}_{i=1}^{L} \\leftarrow\\left\\{x_{i}\\right\\}_{i=1}^{L}\\)\n    2: for \\(i \\leftarrow\\) Leaf to Root do\n        \\(\\xi_{i}=\\overline{\\mathbf{B}}_{i} x_{i}+\\sum_{\\forall j \\in\\{t \\mid \\operatorname{Par}(t)=i\\}} \\xi_{j} \\overline{\\mathbf{A}}_{j}\\)\n    4: end for\nBackward:\n    for \\(i \\leftarrow\\) Root to Leaf do\n    6: \\(\\quad\\) if \\(i\\) is Root then\n        \\(\\frac{\\partial \\text { Loss }}{\\partial x_{i}}=\\eta_{i} \\overline{\\mathbf{B}}_{i}, \\quad \\frac{\\partial \\text { Loss }}{\\partial \\mathbf{B}_{i}}=\\eta_{i} x_{i}, \\quad \\frac{\\partial \\text { Loss }}{\\partial \\mathbf{A}_{i}}=0\\)\n    : else\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-15.jpg?height=72&width=1002&top_left_y=1837&top_left_x=491)\n\n```\n10: \\(\\quad \\frac{\\partial \\text { Loss }}{\\partial \\mathbf{A}_{i}}=\\frac{\\partial \\text { Loss }}{\\partial x_{\\operatorname{Par}(i)}^{\\prime}} h_{i}\\)\n        end if\n12: end for\nOutput: Hidden states \\(\\left\\{h_{i}\\right\\}_{i=1}^{L}\\); Grad.",
    "grootvl-19": "of loss to input feature \\(\\left\\{\\frac{\\partial L o s s}{\\partial x_{i}}\\right\\}_{i=1}^{L}\\); Grad. of loss to input\n        matrix \\(\\left\\{\\frac{\\partial \\text { Loss }}{\\partial \\mathbf{B}_{i}}\\right\\}_{i=1}^{L} ;\\) Grad. of loss to state matrix \\(\\left\\{\\frac{\\partial \\text { Loss }}{\\partial \\mathbf{A}_{i}}\\right\\}_{i=1}^{L}\\). ```\n\n\n## C Algorithm Proof\n\nIn this section, we present detailed proofs for our tree scanning algorithm. The definitions of symbols are consistent with those in the main paper. ## C. 1 Proof for Algorithm 1. We randomly take a vertex in the MST $\\mathcal{G}_{T}$ as the root. According to the definition of the tree scanning algorithm introduced in Sec. 3.2, we can derive $h_{\\text {root }}$ as follows:\n\n$$\nh_{\\text {root }}=\\sum_{\\forall j \\in C_{\\text {root }}} S\\left(E_{\\text {root }, j}\\right) \\overline{\\mathbf{B}}_{j} x_{j}, \\quad S\\left(E_{\\text {root }, j}\\right)=\\prod_{k \\in N_{\\text {root }, j}} \\overline{\\mathbf{A}}_{k}\n$$\n\nwhich shows a process of aggregation from all leaf vertices to the root. Therefore, each vertex is only related to its child in this period. Taking vertex $m$ as an example, the $\\mathrm{Aggr}_{m}$ can be derived as:\n\n$$\n\\operatorname{Aggr}_{m}(x)=\\overline{\\mathbf{B}}_{m} x_{m}+\\sum_{\\forall k \\in\\{t \\mid \\operatorname{Par}(t)=i\\}} \\operatorname{Aggr}_{k}(x) \\overline{\\mathbf{A}}_{k}\n$$\n\nWe assume that one of the child of $m$ is $n$ and $h_{n}$ can be derived as following:\n\n$$\nh_{n}=\\operatorname{Aggr}_{n}(x)+\\overline{\\mathbf{A}}_{n} \\widetilde{\\operatorname{Aggr}}_{m}(x)\n$$\n\nwhere $\\widetilde{\\operatorname{Aggr}}_{m}(x)$ indicates the aggregation value from the vertices $\\in \\Omega \\backslash C_{m}^{r o o t}$ to vertex $m$. Therefore, we can obtain the propagation relationship between the hidden state of parent $m$ and child $n$ :\n\n$$\n\\begin{aligned}\nh_{n} & =\\operatorname{Aggr}_{n}(x)+\\overline{\\mathbf{A}}_{n} \\widetilde{\\operatorname{Aggr}}_{m}(x) \\\\\n& =\\operatorname{Aggr}_{n}(x)+\\overline{\\mathbf{A}}_{n}\\left(h_{m}-\\overline{\\mathbf{A}}_{n} \\operatorname{Aggr}_{n}(x)\\right) \\\\\n& =\\overline{\\mathbf{A}}_{n} h_{m}+\\left(1-\\overline{\\mathbf{A}}_{n}^{2}\\right) \\operatorname{Aggr}_{n}(x)\n\\end{aligned}\n$$\n\nThrough the above derivation, we can calculate $\\left\\{h_{i}\\right\\}_{i=1}^{L}$ with only two traversals (i.e., the aggregation from leaf to root and the propagation from root to leaf) in the forward process as shown in Algorithm 1, thereby reducing the computational complexity from $\\mathcal{O}\\left(L^{2}\\right)$ to $\\mathcal{O}(L)$. Next, we analyze the backpropagation process in Algorithm 1. According to the chain rule, we can easily calculate the derivative of loss with respect to $x_{i}$ :\n\n$$\n\\begin{aligned}\n\\frac{\\partial l o s s}{\\partial x_{i}} & =\\sum_{j \\in \\Omega} \\frac{\\partial \\operatorname{loss}}{\\partial h_{j}} \\frac{\\partial h_{j}}{\\partial x_{i}} \\\\\n& =\\overline{\\mathbf{B}}_{i} \\sum_{j \\in \\Omega} S\\left(E_{j i}\\right) \\frac{\\partial l o s s}{\\partial h_{j}}\n\\end{aligned}\n$$\n\nSimilarly, the derivative of loss with respect to $\\overline{\\mathbf{B}}_{i}$ is:\n\n$$\n\\begin{aligned}\n\\frac{\\partial l o s s}{\\partial \\overline{\\mathbf{B}}_{i}} & =\\sum_{j \\in \\Omega} \\frac{\\partial \\operatorname{loss}}{\\partial h_{j}} \\frac{\\partial h_{j}}{\\partial \\overline{\\mathbf{B}}_{i}} \\\\\n& =x_{i} \\sum_{j \\in \\Omega} S\\left(E_{j i}\\right) \\frac{\\partial l o s s}{\\partial h_{j}}\n\\end{aligned}\n$$\n\nThe above formulas are equivalent to replacing the input $x$ with $\\frac{\\partial l o s s}{\\partial h}$ during the forward process. Subsequently, we assume that the vertex $k$ is the child of vertex $l$ and define $C_{l}^{k}$ indicates the children of vertex $l$ with the root of vertex $k . \\frac{\\partial l o s s}{\\partial \\mathbf{A}_{k}}$ is formulated as follows:\n\n$$\n\\begin{aligned}\n\\frac{\\partial l o s s}{\\partial \\overline{\\mathbf{A}}_{k}} & =\\sum_{j \\in \\Omega} \\frac{\\partial l o s s}{\\partial h_{j}} \\frac{\\partial h_{j}}{\\partial \\overline{\\mathbf{A}}_{k}} \\\\\n& =\\sum_{j \\in \\Omega} \\frac{\\partial l o s s}{\\partial h_{j}} \\sum_{p \\in \\Omega} \\frac{\\partial S\\left(E_{j p}\\right) \\overline{\\mathbf{B}}_{p} x_{p}^{\\prime}}{\\partial \\overline{\\mathbf{A}}_{k}} \\\\\n& =\\sum_{j \\in C_{l}^{k}} \\frac{\\partial l o s s}{\\partial h_{j}} \\sum_{p \\in C_{k}^{l}} S\\left(E_{k p}\\right) S\\left(E_{j l}\\right) \\overline{\\mathbf{B}}_{p} x_{p}^{\\prime}+\\sum_{j \\in C_{k}^{l}} \\frac{\\partial l o s s}{\\partial h_{j}} \\sum_{p \\in C_{l}^{k}} S\\left(E_{k j}\\right) S\\left(E_{p l}\\right) \\overline{\\mathbf{B}}_{p} x_{p}^{\\prime} \\\\\n& =\\sum_{j \\in C_{l}^{k}} S\\left(E_{j l}\\right) \\frac{\\partial l o s s}{\\partial h_{j}} \\sum_{p \\in C_{k}^{l}} S\\left(E_{k p}\\right) \\overline{\\mathbf{B}}_{p} x_{p}^{\\prime}+\\sum_{j \\in C_{k}^{l}} S\\left(E_{k j}\\right) \\frac{\\partial l o s s}{\\partial h_{j}} \\sum_{p \\in C_{l}^{k}} S\\left(E_{p l}\\right) \\overline{\\mathbf{B}}_{p} x_{p}^{\\prime} \\\\\n& =\\left(\\frac{\\partial L o s s}{\\partial x_{k}}-\\overline{\\mathbf{A}}_{k} \\operatorname{Aggr}_{k}\\left(\\frac{\\partial l o s s}{\\partial h}\\right)\\right) * \\operatorname{Aggr}_{k}(x)+\\operatorname{Aggr}_{k}\\left(\\frac{\\partial l o s s}{\\partial h}\\right) *\\left(h_{k}-\\overline{\\mathbf{A}}_{k} \\operatorname{Aggr}_{k}(x)\\right) \\\\\n& =\\frac{\\partial L o s s}{\\partial x_{k}} \\operatorname{Aggr}_{k}(x)+\\operatorname{Aggr}_{k}\\left(\\frac{\\partial l o s s}{\\partial h}\\right) h_{k}-2 \\overline{\\mathbf{A}}_{k} \\operatorname{Aggr}_{k}\\left(\\frac{\\partial l o s s}{\\partial h}\\right) \\operatorname{Aggr}_{k}(x) \\\\\n& \\triangleq \\frac{\\partial L o s s}{\\partial x_{k}} \\xi_{k}+\\eta_{k} h_{k}-2 \\overline{\\mathbf{A}}_{k} \\eta_{k} \\xi_{k} \\quad(\\text { definition in Algorithm } 1)\n\\end{aligned}\n$$\n\nSo far we have completed the proof of forward and back-propagation of Algorithm 1.",
    "grootvl-20": "## C. 2 Proof for Algorithm 2. We only take the last token as root and replace the transition source from $\\Omega$ to $C_{i}$ in sequence modeling tasks like nature language understanding to ensure causality. Therefore, only one traversal (from leaf to root) is required for the forward process, and another traversal (from root to leaf) is needed for the backpropagation process. The proof is similar to the Algorithm 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_5fcae90888b2966fccd2g-17.jpg?height=824&width=1399&top_left_y=1542&top_left_x=363)\n\nFigure 6: Visualization of affinity maps in the specific position. The Location is marked by the red cross in each affinity map. TP represents our Tree Scanning Algorithm. ## D More Qualitative Results\n\nFig. 6 displays additional qualitative comparisons between our algorithm and previous scanning strategies (e.g., cross-scanning and raster-scanning), which shows our advanced capability to perceive detailed structural information and capture long-range dependencies. ## E Statistical Significance\n\n| Method | PIQA | Arc-Easy | SST | WinoGrande | LAM-ppl | Race | Openbookqa |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| GrootL (Ours) | 0.011 | 0.010 | 0.016 | 0.014 | 0.553 | 0.014 | 0.018 |\n\nTable 8: Standard error on language model benchmarks. LAM-ppl indicates LAMBADA [45]. We calculate the standard deviation of our GrootL on language model benchmarks in the open-sourced lm-evaluation-harness project as shown in Table 8. [^0]:    ${ }^{*}$ Equal contribution. ${ }^{\\dagger}$ Work done during an internship at Tencent. ${ }^{\\boxtimes}$ Corresponding author.",
    "grootvl-21": ""
}