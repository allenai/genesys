{
    "kerple-0": "# KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation \n\nTa-Chung Chi*<br>Carnegie Mellon University<br>tachungc@andrew.cmu.edu\n\nTing-Han Fan*<br>Princeton University<br>tinghanf@princeton.edu\n\nPeter J. Ramadge<br>Princeton University<br>ramadge@princeton.edu\n\nAlexander I. Rudnicky<br>Carnegie Mellon University<br>air@cs.cmu.edu\n\n\n#### Abstract\n\nRelative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at https://github.com/chijames/KERPLE.git. ## 1 Introduction\n\nTransformer-based models have excelled in various natural language processing tasks such as chatbot [Roller et al., 2021], code completion [Chen et al., 2021a], and paper abstract summarization [Zhang et al., 2020]. These sequence modeling tasks often require the model to operate well on significantly longer text sequences than the fixed maximum length $L$ used at training time. Training (or retraining) the model using a substantially larger value of $L$ is often infeasible since the transformer training cost is $O\\left(L^{2}\\right)$. Hence, one desires a transformer that continues to perform well on longer sequences than those used during training; i.e., perform length extrapolation at inference time. Most transformer designs do not have this property [Press et al., 2022]. While recent work on absolute positional embeddings demonstrated the extrapolation ability Kiyono et al. 2021, Likhomanenko et al., 2021], it is believed that relative positional embeddings are more robust to input length change [Likhomanenko et al., 2021], for example, ALiBi [Press et al., 2022] and T5 [Raffel et al., 2020]. Hence, we are motivated to study the inner workings of relative positional embeddings. Relative positional embeddings (RPE) encode the idea of shift-invariance: for any shift $p,(m+p)-$ $(n+p)=m-n$. It is often added directly to the self-attention matrix before Softmax normalization [hen et al. 2021b]. Inspired by shift-invariance and the ability of a kernel to define a similarity function, there have been studies on shift-invariant kernels for RPE [Wennberg and Henter, 2021] with a focus on Gaussian kernel. However, in our preliminary experiments, the Gaussian kernel\n\n[^0]Figure 1: The 3-Para-Log Variant of Our KERPLE Framework.",
    "kerple-1": "$a, b$, and $p$ are learnable parameters in each attention head shared across layers. Since \\# of heads is $H$, there are $3 \\cdot H$ learnable parameters. The learnable parameters are trained with length-3 sequences. At the inference time, the last row (in dashed squares) becomes active, and the model extrapolates to length- 4 sequences. Note we focus on causal language modeling following ALiBi , so the matrices are triangular. ![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-02.jpg?height=326&width=855&top_left_y=455&top_left_x=624)\ndemonstrates limited length extrapolation ability (see Appendix A.3). Hence, a distinct class of shift-invariant kernels is needed to achieve adequate length extrapolation. To this end, we note a set of well-established conditionally positive definite (CPD) kernels suitable for modeling distance metrics [Sch\u00f6lkopf, 2000]. However, CPD kernels do not conform to an inner product. We can remedy this issue by transforming a CPD kernel into a PD kernel by adding a sufficiently large constant. This constant offset is subsequently absorbed implicitly in the Softmax normalization (see the discussion below Eq. (2)). For example, ALiBi implicitly admits a PD kernel of the form $c-|m-n|$ (see the end of section $4 \\mid$, which is reduced to a CPD kernel $-|m-n|$. The CPD kernel and Softmax normalization combination opens the door to a sea of possible CPD kernels. We investigate structures from this class that exhibit a strong length extrapolation ability, like ALiBi. Our main result is a framework for KErnelize Relative Positional Embedding for Length Extrapolation (KERPLE). The framework elucidates key principles that encourage the length extrapolation property. We show that ALiBi is a particular instance within our framework. Our subsequent experiments suggest that the proposed method yields better length extrapolation on large datasets such as OpenWebText2, GitHub, and ArXiv. ## 2 Background and Related Work\n\n### 2.1 Preliminary\n\nLet $\\left\\{w_{m}\\right\\}_{m=1}^{L}$ be the input tokens to a transformer model, where $L$ is the total number of tokens. Each $w_{m}$ is a scalar and is used to index the embedding vector $\\boldsymbol{e}_{m} \\in \\mathbb{R}^{d}$ as the input to the transformer. A transformer converts each $\\boldsymbol{e}_{m}$ into query, key, and value vectors in $\\mathbb{R}^{d}: \\boldsymbol{q}_{m}=\\boldsymbol{W}_{q} \\boldsymbol{e}_{m}, \\boldsymbol{k}_{m}=\\boldsymbol{W}_{k} \\boldsymbol{e}_{m}$, $\\boldsymbol{v}_{m}=\\boldsymbol{W}_{v} \\boldsymbol{e}_{m}$, where $\\boldsymbol{W}_{q}, \\boldsymbol{W}_{k}, \\boldsymbol{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable matrices. Then, the self-attention module computes the scaled attention scores and generates the output vector $\\boldsymbol{o}_{m}$ at position $m$ as:\n\n$$\na_{m, n}=\\frac{\\exp \\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n} / \\sqrt{d}\\right)}{\\sum_{i=1}^{L} \\exp \\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{i} / \\sqrt{d}\\right)}, \\quad \\boldsymbol{o}_{m}=\\sum_{n=1}^{L} a_{m, n} \\boldsymbol{v}_{n}\n$$\n\nSince the operation is position-agnostic, it is believed that positional information helps model token interactions Vaswani et al.",
    "kerple-2": "2017], which we survey in the next subsection. ### 2.2 Positional Embedding\n\nAbsolute. Absolute positional embeddings assign a positional vector $\\boldsymbol{p}_{m}$ to each position $m$ and adds $\\boldsymbol{p}_{m}$ to the embedding vector $\\boldsymbol{e}_{m}$. The very first version of which is the predefined sinusoidal function Vaswani et al., 2017]. Followed by the success of BERT [Devlin et al., 2019], learnable absolute positional embeddings have been applied to the task of masked language modeling [Devlin et al., 2019, Liu et al., 2019. Clark et al., 2020, Lan et al., 2020], Autoregressive-decoding Radford et al., 2018, 2019], and sequence-to-sequence [Gehring et al., 2017, Lewis et al., 2019] settings. Recent work studied ways to extrapolate sinusoidal positional embeddings to longer sequences by randomly shifting absolute positions during training [Kiyono et al. 2021] or augmenting with continuous signals [Likhomanenko et al., 2021]. Relative. As opposed to the modeling of absolute position $m$, relative positional embeddings (RPE) that model the positional difference $m-n$ has become popular in the literature [Shaw et al, , 2018, Huang et al., 2019, Dai et al., 2019, Yang et al., 2019, Huang et al., 2020, He et al., 2021, Ke et al., 2021, Chen et al.| 2021b|. In particular, the T5 model that considers bucketed relative distances and log-binning has been shown to perform well on various transformer architectures [Raffel et al. 2020]. Rotary positional embedding [Su et al., 2021] encodes the position with rotations: $f\\left(\\boldsymbol{q}_{m}, m\\right)=$ $R_{m} \\boldsymbol{q}_{m}$ where $R_{m}$ is a rotation matrix with angles proportional to $m$. With the rotation's property, the query-key product exhibits a positional difference: $f\\left(\\boldsymbol{q}_{m}, m\\right)^{\\top} f\\left(\\boldsymbol{k}_{n}, n\\right)=\\boldsymbol{q}_{m}^{\\top} R_{n-m} \\boldsymbol{k}_{n}$. We note that the overview above focuses on the NLP domain. Recent work has applied positional embeddings to other domains such as vision [Wu et al., 2021a] and speech [Likhomanenko et al., 2021]. A survey can be found in [Dufter et al., 2022]. ### 2.3 Kernel and its Application in Transformer\n\nThe kernel trick is a classic approach to generalize the inner product to high dimensional spaces Mika et al., 1998, Sch\u00f6lkopf, 2000, Leslie et al., 2001, Dhillon et al., 2004, Takeda et al., 2007]. In the context of transformers, there has been interest in applying kernels to the self-attention structure to enhance the performance. Examples of such work include kernel for positional embeddings [Tsai et al. 2019. Wu et al., 2021b, Wennberg and Henter, 2021, Luo et al., 2021]. Another line of research leverages the kernel's feature map [Rahimi and Recht, 2007| to linearize the self-attention module and reduce the computational cost [Katharopoulos et al., 2020, Chen et al., 2021c, Xiong et al., 2021, Peng et al., 2021, Choromanski et al., 2021, Qin et al., 2022]. ## 3 Theoretical Foundations of CPD Kernels\n\n### 3.1 PD and CPD Kernels\n\nIn this work, we use shift-invariant conditionally positive definite (CPD) kernels to model the effect of relative positional differences. We propose this formulation because the notion of relative is modeled by a shift-invariant function: a bivariate function $k$ over two positions $(m, n)$ such that $k(m, n)=f(m-n)$ for some univariate $f$. The notion of positional difference $m-n$ is generalized by the CPD kernel. We review the definitions of PD and CPD kernels below. Definition 1 (PD Kernel). A (real) symmetric function $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is a positive definite kernel if for any integer $N$ and any $\\left\\{x_{i} \\in \\mathcal{X}\\right\\}_{i=1}^{N},\\left\\{c_{i} \\in \\mathbb{R}\\right\\}_{i=1}^{N}$, the quadratic form is nonnegative: $\\sum_{i=1}^{N} \\sum_{j=1}^{N} c_{i} c_{j} k\\left(x_{i}, x_{j}\\right) \\geq 0$\n\nDefinition 2 (CPD Kernel). A (real) symmetric function $\\tilde{k}: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is a conditionally positive definite kernel if for any integer $N$ and any $\\left\\{x_{i} \\in \\mathcal{X}\\right\\}_{i=1}^{N}$, the quadratic form is conditionally nonnegative: $\\sum_{i=1}^{N} \\sum_{j=1}^{N} c_{i} c_{j} \\tilde{k}\\left(x_{i}, x_{j}\\right) \\geq 0$ for $\\left\\{c_{i} \\in \\mathbb{R}\\right\\}_{i=1}^{N}$ with $\\sum_{i=1}^{N} c_{i}=0$.",
    "kerple-3": "Fact 1 Berg et al.",
    "kerple-4": "[1984] and Prop. 5 of Sch\u00f6lkopf 2000]). Let $\\tilde{k}: \\mathcal{X} \\times \\mathcal{X} \\rightarrow(-\\infty, 0$ be be a $C P D$ kernel with $\\dot{k}(x, x)=0 \\forall x \\in \\mathcal{X}$. Then, there exists a Hilbert space $\\mathcal{H}$ and a mapping $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ such that $\\left\\|\\phi(x)-\\phi\\left(x^{\\prime}\\right)\\right\\|^{2}=-\\tilde{k}\\left(x, x^{\\prime}\\right)$. Fact 1 suggests that CPD kernels generalize distance metrics to high dimensional spaces. Since we are interested in positional differences, we examine modeling the distance between positions using CPD kernels. However, Fact 1 also implies that CPD kernels do not encode inner products as required by selfattention for the computation of pairwise relations. PD kernels represent inner products. To better understand the effect of CPD kernels on self-attention, we need to establish relations between CPD and PD kernels. As noted in Sch\u00f6lkopf [2000], if one takes any PD kernel and offsets it by a constant, the result is at least a CPD kernel. In the next subsection, we show that the converse is nearly true: if $\\tilde{k}$ is CPD, so is $c+\\tilde{k}$ for large enough $c \\in \\mathbb{R}$ (Lemma 1). Therefore, we may generate the CPD kernels of interest and transform them into PD kernels if needed. ### 3.2 Constructing PD Kernels From CPD Kernels via Constant Shifts\n\nIn this subsection, we review a few properties of CPD kernels and use these to generate a variety of CPD kernels. Then, we present a lemma that transforms CPD kernels into PD kernels via constant shifts. This enables the production of a family of PD kernels from CPD kernels. Finally, we present our critical observation that the exact value of the constant shift is not needed, thanks to a nice property of Softmax normalization. Below are some important facts about CPD kernels. Fact 2 (Scaling and Summation). If $\\tilde{k}_{1}$ and $\\tilde{k}_{2}$ are $C P D$, then so are $a \\cdot \\tilde{k}_{1}($ for $a>0)$ and $\\tilde{k}_{1}+\\tilde{k}_{2}$. Fact 3 Berg et al.",
    "kerple-5": "[1984] and Prop. 4 of Sch\u00f6lkopf 2000]). If $\\tilde{k}: \\mathcal{X} \\times \\mathcal{X} \\rightarrow(-\\infty, 0]$ is $C P D$, then so are $-(-\\dot{k})^{\\alpha}$ for $0<\\alpha<1$ and $-\\log (1-k)$. Fact 4 (Page 3 of Sch\u00f6lkopf 2000]). The negative squared distance $-\\left\\|x-x^{\\prime}\\right\\|^{2}$ is CPD. The three Facts above jointly yield a rich family of CPD kernels as shown below. Corollary 1. The following are CPD kernels. (a) $\\tilde{k}\\left(x, x^{\\prime}\\right)=-a\\left\\|x-x^{\\prime}\\right\\|^{p}$ with $0<p \\leq 2$ and $a>0$. (b) $\\tilde{k}\\left(x, x^{\\prime}\\right)=-b \\cdot \\log \\left(1+a\\left\\|x-x^{\\prime}\\right\\|^{p}\\right)$ with $0<p \\leq 2$ and $a, b>0$. We note that it is possible to keep iterating between Fact 2 and 3 and generate more complicated examples, e.g., $-a\\left\\|x-x^{\\prime}\\right\\|^{p}-b \\cdot \\log \\left(1+a\\left\\|x-x^{\\prime}\\right\\|^{p}\\right)$ or $-b \\cdot \\log \\left(1+a\\left\\|x-x^{\\prime}\\right\\|^{p}\\right)^{c}$ for $0<c<1$. However, since relative positional embeddings are of our interest, we only consider simple CPD kernels. Those with complicated forms are deferred to future work. Now that Corollary 1 has presented a few class of CPD kernels, we prove a lemma (in Appendix A.1) that constructs PD kernels from CPD kernels through shifting. Later in Eq. (2), we will see that the shifting construction is combined neatly with the Softmax normalization of self-attention. Lemma 1 (CPD Shift Lemma. Proof in Appendix A.1. Let $\\tilde{k}: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ be a CPD kernel. There exists $c \\geq 0$ such that $c+\\tilde{k}$ is a PD kernel. Lemma 1 implies the CPD kernels in Corollary 1 can be made PD if a large enough constant is added. For example, $c-\\left\\|x-x^{\\prime}\\right\\|^{p}$ for large enough $c$. Although Lemma 1 does not have an explicit construction of $c$, thanks to the shift-invariant property of the Softmax normalization, we can leave it as an under-determined constant in our positional embedding design (Eq. (1) in section 4). Given a set of test points $\\left\\{x_{i}\\right\\}_{i=1}^{N}$, one can do a geometric sequence search to search for a $c$ such that the $N \\times N$ matrix $\\left[c+\\tilde{k}\\left(x_{i}, x_{j}\\right)\\right]_{i, j=1}^{N} \\succeq 0$. Hence, we do not need the value of $c$, but we can compute it if needed, e.g., deriving the feature map of $c+\\tilde{k}$. Alternative Proof of $c-\\left\\|x-x^{\\prime}\\right\\|^{p}$. While the CPD shift lemma is convenient, one can prove $c-\\left\\|x-x^{\\prime}\\right\\|^{p}$ is PD for large enough $c$ using a kernel representation theorem in Schoenberg [1938].",
    "kerple-6": "See Appendix A. 2 for details. ## 4 Kernelized Relative Positional Embedding\n\nLet $\\left\\{\\boldsymbol{q}_{m}\\right\\}_{m=1}^{L}$ and $\\left\\{\\boldsymbol{k}_{n}\\right\\}_{n=1}^{L}$ be the input queries and keys. Let $\\left(r_{1}, \\ldots, r_{\\ell}\\right)$ be learnable parameters. We propose a kernelized relative positional embedding as follows. $$\na_{m, n}=\\frac{\\exp \\left(\\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+\\tilde{k}_{r_{1}, \\ldots, r_{\\ell}}(m, n)\\right) / \\sqrt{d}\\right)}{\\sum_{i=1}^{L} \\exp \\left(\\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{i}+\\tilde{k}_{r_{1}, \\ldots, r_{\\ell}}(m, i)\\right) / \\sqrt{d}\\right)}\n$$\n\n[^1]where $\\tilde{k}_{r_{1}, \\ldots, r_{\\ell}}(m, n)$ is any shift-invariant CPD kernel with $\\ell$ parameters.",
    "kerple-7": "Due to Lemma 1. Eq. (1) can be reformulated into its kernel form as follows. \\[\n\n$$\n\\begin{aligned}\n& a_{m, n} \\stackrel{(*)}{=} \\frac{\\exp \\left(\\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+c+\\tilde{k}_{r_{1}, \\ldots, r_{\\ell}}(m, n)\\right) / \\sqrt{d}\\right)}{\\sum_{i=1}^{L} \\exp \\left(\\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{i}+c+\\tilde{k}_{r_{1}, \\ldots, r_{\\ell}}(m, i)\\right) / \\sqrt{d}\\right)} \\\\\n& \\stackrel{\\text { Lemma }}{=} \\frac{\\left.\\exp \\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+k_{r_{1}, \\ldots, r_{\\ell}}(m, n)\\right) / \\sqrt{d}\\right)}{\\left.\\sum_{i=1}^{L} \\exp \\left(\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{i}+k_{r_{1}, \\ldots, r_{\\ell}}(m, i)\\right) / \\sqrt{d}\\right)}=\\frac{\\exp \\left(k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right) / \\sqrt{d}\\right)}{\\sum_{i=1}^{L} \\exp \\left(k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{i}, i\\right]\\right) / \\sqrt{d}\\right)} \\text {. }\n\\end{aligned}\n$$\n\\]\n\n(*) is due to the shift-invariant property of the Softmax normalization: $\\frac{\\exp \\left(x_{i}\\right)}{\\sum_{j} \\exp \\left(x_{j}\\right)}=\\frac{\\exp \\left(x_{i}+c\\right)}{\\sum_{j} \\exp \\left(x_{j}+c\\right)}$ for any $c \\in \\mathbb{R}$. The second equality defines a bias kernel which is positive definite using Lemma 1 . $$\nk_{r_{1}, \\ldots, r_{\\ell}}=c+\\tilde{k}_{r_{1}, \\ldots, r_{\\ell}}\n$$\n\nThe last equality introduces a composite kernel $k^{\\text {comp }}: \\mathbb{R}^{d+1} \\times \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}$ as\n\n$$\nk^{\\mathrm{comp}}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+k_{r_{1}, \\ldots, r_{\\ell}}(m, n)\n$$\n\nInterpretation. The proposed method can be interpreted as applying a composite kernel to selfattention. The composite kernel combines the information from query $\\boldsymbol{q}_{m}$, key $\\boldsymbol{k}_{n}$, and positions $(m, n)$ in a way that augments the original self-attention structure by multiplicative and additive position embeddings. The augmentation allows $k^{\\text {comp }}$ to not only retain the original $\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}$ but also include positional information from the bias kernel $k_{r_{1}, \\ldots, r_{\\ell}}$. Practical Choice. In section 5.2, we fix $\\ell=2$ and experiment on two variants of the composite kernel, Eq. (4), where we call these the power variant and the logarithmic variant of our proposed KERPLE framework, Eq.",
    "kerple-8": "24. These are from a combination of Corollary 1.and Eq. (3). (power) $k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+c-r_{1}|m-n|^{r_{2}}$ with $r_{1}>0$ and $0<r_{2} \\leq 2$. (logarithmic) $k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+c-r_{1} \\cdot \\log \\left(1+r_{2}|m-n|\\right)$ with $r_{1}, r_{2}>0$. We note that these are not the only variants of the composite kernel. In section 5.3 , we experiment with two more complicated variants, but only find lower training speeds and marginal improvement in perplexities (e.g., logarithmic variant vs. 3-para-log). Thus, based on our study, the choices above hold advantages in both performance and speed. Connection to Prior Work. When the bias kernel, Eq. (3), is a triangle kernel: $c-|m-n|$, our model reduces to ALiBi [Press et al., 2022]. Wennberg and Henter [2021] discuss the situation where the bias kernel is a Gaussian kernel. Tsai et al. [2019] is the case where there is no bias kernel and the attention product $\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}$ is multiplied by an exponentiated inner product kernel, $\\exp \\left(\\boldsymbol{x}^{\\top} \\boldsymbol{y}\\right)$. Since ALiBi is the state-of-the-art and has great input length extrapolation, we will focus on comparison with ALiBi in our experiments.",
    "kerple-9": "The logarithmic variant has an implicit connection to T 5 positional bias [Raffel et al. 2020]. According to the official GitHub repository https://github.com/google-research/ text-to-text-transfer-transformer and the HuggingFace Transformer [Wolf et al., 2020], T5 bias is implemented with a log-binning strategy. For each head of the transformer, they maintain a bucket of 32 learnable parameters and assign the relative positional bias $b_{m-n}$ to these parameters as\n\n$$\nb_{m-n}= \\begin{cases}\\operatorname{bucket}[0] & \\text { if } m-n<0 \\\\ \\operatorname{bucket}[m-n] & \\text { if } 0 \\leq m-n<16 \\\\ \\operatorname{bucket}\\left[\\min \\left(31,\\left\\lfloor\\frac{\\log ((m-n) / 16)}{\\log (128 / 16)}\\right) \\cdot 16\\right]\\right] & \\text { if } m-n \\geq 16\\end{cases}\n$$\n\nwhere $\\lfloor\\cdot\\rfloor$ is the floor function. Note that the $\\log$ factor is approximately $7.7 \\log \\frac{m-n}{16}$. Therefore, T5 is using a logarithmic bucket assignment, which turns out to extrapolate to different input lengths. Compared with T5, our logarithmic variant uses less parameters ( $2 \\times 12$ vs. $32 \\times 12$ ) but cannot learn non-monotonic relations (the log function is monotonic). We will conduct more comparisons with T5 bias in our experiments. ## 5 Experiments\n\n### 5.1 Dataset and Implementation Description\n\nDataset. We conduct experiments on OpenWebText2, GitHub, and ArXiv datasets gathered in Gao et al. [2020]. OpenWebText2 includes recent content from Reddit submissions until 2020, content from multiple languages, document metadata, multiple dataset versions, and open-source replication code. GitHub includes open-source repositories written in primary coding languages such as Java, C/C++, Python, and Go. ArXiv includes papers written in LaTex in Math, Computer Science, Physics, and some related fields. These tasks are motivated by the downstream applications such as online chatting [Roller et al., 2021], code completion [Chen et al, 2021a], and academic paper summarization [Zhang et al., 2020]. Table 1: Dataset Overview. Raw Size is the size before any up- or down-sampling. |  | OpenWebText2 | GitHub | ArXiv |\n| :--- | :---: | :---: | :---: |\n| Raw Size | 66.77 GB | 95.16 GB | 56.21 GB |\n| Type | Internet | Coding | Academic |\n\nImplementation. We adapt our model from GPT-NeoX Black et al. 2021], a transformer implementation by the EleutherAI team. The codebase is based on NVIDIA Megatron Language Model [Shoeybi et al. 2019] and further accelerated using Microsoft DeepSpeed library [Rasley et al., 2020]. Our model is trained on a machine with one NVIDIA A100 GPU with 40 GB of memory. We adopt almost all configurations of small GPT-NeoX ${ }^{2}$, except that we change the train-micro-batch-size to 32 , seq-length to 512 , and max-position-embeddings to 512 . Table 2 summarizes the important configurations fixed throughout our experiments. In particular, the floating-point encoding is set as\n\nTable 2: 162M Model Configurations. | \\# Layers | Hidden Size | \\# Attention Heads | Train Seq. Len. | \\# Trainable Params. |\n| :---: | :---: | :---: | :---: | :---: |\n| 12 | 64 | 12 | 512 | 162 M |\n| Optimizer | Batch Size | Train Steps | Precision | \\# Trainable Params. for RPEs |\n| Adam $(\\operatorname{lr} 6 \\mathrm{e}-4)$ | 32 | 50,000 | bfloat16 | at most 36 |\n\nbfloat16 (Brain Floating Point, developed by Google Brain) so that the training can be accelerated by half-precision computation with reliable stability [Kalamkar et al., 2019].",
    "kerple-10": "Hidden size 64 means that $d=64$ in Eq. 11. ### 5.2 Experimental Results (Also c.f. Appendix A.4 to A.7\n\nWe conduct experiments to cover aspects such as input length extrapolation, application on different domains, and comparison with the prior work. These are elaborated on below. (i) Motivated by the input length extrapolation demonstrated in [Press et al., 2022], we train our model with length 512 and test on lengths ranging from 512 to 16384 . We hope that the emphasis on extrapolation enables the application of transformers to longer sequences. (ii) To evaluate the applicability of the model in different domains, we conduct experiments on OpenWebText2, GitHub, and ArXiv datasets. (iii) To validate the effectiveness of our method, we compare KERPLE with Sinusoidal [Vaswani et al. 2017], Rotary [Su et al., 2021], T5 [Raffel et al., 2020], and ALiBi [Press et al., 2022]. Table 3 reports the perplexities at different extrapolation lengths. We perform non-overlapping evaluation: Suppose text is segmented in a different manner for 512 and 1024 tokens, we have N sentences and N/2 correspondingly to evaluate. We also perform a paired two-sided t-test to validate the statistical significance (significance level=0.05). We compare each candidate RPE with our proposed logarithmic variant and mark the candidate with a ${ }^{\\dagger}$ if the log variant is statistically significantly better. Table 4 reports the training speeds. These tables yield three conclusions. First, within the KERPLE framework, the logarithmic variant is better than the power variant. Secondly, the logarithmic variant is $9.7 \\%$ faster than T5. In terms of extrapolation, the logarithmic variant generally\n\n[^2]does better than T 5 but could be slightly worse than T 5 at shorter lengths. Third, the logarithmic variant is slightly slower than some prior work (ALiBi, Rotary, and Sinusoidal) but consistently outperform these methods at all extrapolation lengths.",
    "kerple-11": "More details are given below. Logarithmic Variant vs. Power Variant. In our proposed KERPLE framework, the logarithmic variant is better than the power variant. Precisely, the logarithmic variant is $4.4 \\%$ faster and has lower perplexities across all extrapolation lengths and all tasks. Logarithmic Variant vs. T5. In terms of speed, the logarithmic variant is $9.7 \\%$ faster than T5. In terms of extrapolation perplexity, the logarithmic variant is close to or slightly worse than T5 when the extrapolation length is shorter than 2048, and consistently excels T5 at longer extrapolation lengths.",
    "kerple-12": "The tendency of extrapolation holds for all datasets evaluated in this work. Logarithmic Variant vs. ALiBi, Rotary, and Sinusoidal. The logarithmic variant is $1.6 \\%$ slower, $7.5 \\%$ faster, and $3.0 \\%$ slower than ALiBi, Rotary, and Sinusoidal. The speed comparison makes sense because we require only a limited amount of learnable parameters for RPEs (at most $3 \\cdot H$ ). Also, the logarithmic variant consistently outperforms prior work at all extrapolation lengths and tasks. Table 3: Perplexity Comparison on OpenWebText2, GitHub, and ArXiv. All models are trained for 50 k steps with training length 512 and five random seeds. $x^{\\dagger}$ means our log variant is statistically significantly better than $x$. The test used is paired two-sided t-test with $\\alpha=0.05$. | OpenWebText2 |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Extrp. | KER | RPLE | ALiBi | T5 | Rotary | Sinusoidal |\n|  | $(\\log )$ | (power) |  |  |  |  |\n| 512 | $23.9 \\pm 0.6$ | $23.9 \\pm 0.6$ | $23.9 \\pm 0.6$ | $23.7 \\pm 0.6$ | $24.2 \\pm 0.6^{\\dagger}$ | $33 \\pm 1^{\\dagger}$ |\n| 1024 | $22.0 \\pm 0.6$ | $22.1 \\pm 0.7$ | $22.4 \\pm 0.5^{\\dagger}$ | $21.9 \\pm 0.6$ | $32.8 \\pm 1.7^{\\dagger}$ | $750 \\pm 346^{\\dagger}$ |\n| 2048 | $21.6 \\pm 0.3$ | $21.9 \\pm 0.2^{\\dagger}$ | $22.5 \\pm 0.2^{\\dagger}$ | $21.7 \\pm 0.2$ | $62.4 \\pm 6.1^{\\dagger}$ | $5507 \\pm 2607^{\\dagger}$ |\n| 4096 | $21.2 \\pm 0.4$ | $21.5 \\pm 0.5^{\\dagger}$ | $22.2 \\pm 0.4^{\\dagger}$ | $22.5 \\pm 0.6^{\\dagger}$ | $111 \\pm 13.8^{\\dagger}$ | $14039 \\pm 2325^{\\dagger}$ |\n| 8192 | $21.3 \\pm 0.4$ | $21.6 \\pm 0.4^{\\dagger}$ | $22.3 \\pm 0.3^{\\dagger}$ | $25.5 \\pm 1.3^{\\dagger}$ | $185 \\pm 18.9^{\\dagger}$ | $22621 \\pm 1927^{\\dagger}$ |\n| 16384 | $21.4 \\pm 0.6$ | $21.6 \\pm 0.6$ | $22.5 \\pm 0.5^{\\dagger}$ | $31.4 \\pm 3.1^{\\dagger}$ | $269 \\pm 33.0^{\\dagger}$ | $30046 \\pm 4824^{\\dagger}$ |\n| GitHub |  |  |  |  |  |  |\n| Extrp. | KER | PLE | ALiBi | T5 | Rotary | Sinusoidal |\n|  | $(\\log )$ | (power) |  |  |  |  |\n| 512 | $3.40 \\pm 0.20$ | $3.42 \\pm 0.20$ | $3.42 \\pm 0.21$ | $3.38 \\pm 0.21$ | $3.44 \\pm 0.20^{\\dagger}$ | $4 \\pm 0.2^{\\dagger}$ |\n| 1024 | $3.04 \\pm 0.14$ | $3.07 \\pm 0.16$ | $3.15 \\pm 0.17^{\\dagger}$ | $3.02 \\pm 0.14$ | $3.86 \\pm 0.25^{\\dagger}$ | $105 \\pm 39^{\\dagger}$ |\n| 2048 | $2.86 \\pm 0.10$ | $2.90 \\pm 0.08^{\\dagger}$ | $3.13 \\pm 0.10^{\\dagger}$ | $2.84 \\pm 0.09$ | $5.94 \\pm 0.64^{\\dagger}$ | $1380 \\pm 404^{\\dagger}$ |\n| 4096 | $2.74 \\pm 0.05$ | $2.79 \\pm 0.06$ | $3.04 \\pm 0.08^{\\dagger}$ | $2.78 \\pm 0.04^{\\dagger}$ | $11.1 \\pm 1.55^{\\dagger}$ | $5217 \\pm 1118^{\\dagger}$ |\n| 8192 | $2.71 \\pm 0.05$ | $2.76 \\pm 0.05$ | $3.04 \\pm 0.03^{\\dagger}$ | $2.95 \\pm 0.13^{\\dagger}$ | $20.2 \\pm 2.75^{\\dagger}$ | $10081 \\pm 3583^{\\dagger}$ |\n| 16384 | $2.75 \\pm 0.16$ | $2.76 \\pm 0.13$ | $3.02 \\pm 0.13^{\\dagger}$ | $3.35 \\pm 0.27^{\\dagger}$ | $31.3 \\pm 5.20^{\\dagger}$ | $16443 \\pm 8503^{\\dagger}$ |\n| ArXiv |  |  |  |  |  |  |\n| Extrp. | KER | LE | ALiBi | T5 | Rotary | Sinusoidal |\n|  | $(\\log )$ | (power) |  |  |  |  |\n| 512 | $6.07 \\pm 0.26$ | $6.10 \\pm 0.26$ | $6.12 \\pm 0.26^{\\dagger}$ | $6.03 \\pm 0.26$ | $6.07 \\pm 0.27$ | $43 \\pm 44$ |\n| 1024 | $5.61 \\pm 0.10$ | $5.65 \\pm 0.10^{\\dagger}$ | $5.82 \\pm 0.09^{\\dagger}$ | $5.58 \\pm 0.09$ | $7.49 \\pm 0.34^{\\dagger}$ | $221 \\pm 136^{\\dagger}$ |\n| 2048 | $5.22 \\pm 0.12$ | $5.26 \\pm 0.13^{\\dagger}$ | $5.71 \\pm 0.14^{\\dagger}$ | $5.21 \\pm 0.14$ | $14.2 \\pm 1.81^{\\dagger}$ | $730 \\pm 343^{\\dagger}$ |\n| 4096 | $5.20 \\pm 0.10$ | $5.25 \\pm 0.09$ | $5.87 \\pm 0.08^{\\dagger}$ | $5.32 \\pm 0.16^{\\dagger}$ | $30.1 \\pm 4.32^{\\dagger}$ | $1998 \\pm 497^{\\dagger}$ |\n| 8192 | $5.01 \\pm 0.10$ | $5.06 \\pm 0.15$ | $5.74 \\pm 0.13^{\\dagger}$ | $5.54 \\pm 0.39^{\\dagger}$ | $54.3 \\pm 6.22^{\\dagger}$ | $4228 \\pm 2645^{\\dagger}$ |\n| 16384 | $5.07 \\pm 0.16$ | $5.07 \\pm 0.19$ | $5.78 \\pm 0.15^{\\dagger}$ | $6.25 \\pm 0.61^{\\dagger}$ | $85.4 \\pm 7.40^{\\dagger}$ | $6674 \\pm 5696$ |\n\nTable 4: Training Time Comparison on GitHub\n\n|  | KERPLE |  | ALiBi | T5 | Rotary | Sinusoidal |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | (log) | (power) |  |  |  |  |\n| sec/step | 0.307 | 0.321 | 0.302 | 0.340 | 0.332 | 0.298 |\n\n### 5.3 Experiments on Complicated Kernels\n\nIn addition to the practical variants (power \\& logarithmic) in section 4, we consider two complicated versions of the composite kernel, Eq. (4), as follows. (bias+wht) bias + weight:\n\n$$\n\\begin{aligned}\n& k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n} \\cdot \\exp \\left(-r_{3}|m-n|^{r_{4}}\\right)+c-r_{1}|m-n|^{r_{2}} \\\\\n& \\text { with } r_{1}, r_{3}>0 \\text { and } 0<r_{2}, r_{4} \\leq 2\n\\end{aligned}\n$$\n\n(3-para-log) 3-parameter-logarithmic:\n\n$$\n\\begin{aligned}\n& k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+c-r_{1} \\cdot \\log \\left(1+r_{2}|m-n|^{r_{3}}\\right) \\\\\n& \\text { with } r_{1}, r_{2}>0 \\text { and } 0<r_{3} \\leq 2\n\\end{aligned}\n$$\n\nRecall the tensor product property of a kernel: if $k_{1}$ is a kernel on $\\mathcal{X}$ and $k_{2}$ is a kernel on $\\mathcal{Y}$, then $k\\left((x, y),\\left(x^{\\prime}, y^{\\prime}\\right)\\right)=k_{1}\\left(x, x^{\\prime}\\right) k_{2}\\left(y, y^{\\prime}\\right)$ is a kernel on $\\mathcal{X} \\times \\mathcal{Y}$. Therefore, (bias+wht) is the setting where we train a weight $\\exp \\left(-r_{3}|m-n|^{r_{4}}\\right)$ and a bias kernel $c-r_{1}|m-n|^{r_{2}} \\cdot \\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}$ is multiplied by the weight kernel and then added with the bias kernel. (3-para-log) is the setting where we consider $|m-n|^{r_{3}}$ in the log. When $r_{3}=1$, it is reduced to the logarithmic variant proposed in section 4\nWe plug in these composite kernel $k^{\\text {comp }}$ into our KERPLE framework, Eq. (2), and test the performance of these RPE. Compared with section 5.2, Table 5 suggests that these variants do not have clear advantage in extrapolation performance, e.g., 3-para-log is slightly better in perplexity than the (two-parameter) logarithmic variant. Thus, enlarging the complexity of kernels does not necessarily give better performance in the context of RPE. Table 5: Perplexity Comparison for KERPLE with Complicated Kernels on OpenWebText2, GitHub, and ArXiv. All models are trained for 50k steps with training length 512 and five seeds random.",
    "kerple-13": "OOM means out of memory. | Extrp. | OpenWebText2 |  | GitHub |  | ArXiv |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | (bias+wht) | (3-para-log) | (bias+wht) | (3-para-log) | (bias+wht) | (3-para-log) |\n| 512 | $24.1 \\pm 0.6$ | $23.8 \\pm 0.6$ | $3.44 \\pm 0.21$ | $3.40 \\pm 0.20$ | $6.11 \\pm 0.27$ | $6.06 \\pm 0.27$ |\n| 1024 | $22.2 \\pm 0.6$ | $22.0 \\pm 0.7$ | $3.08 \\pm 0.15$ | $3.04 \\pm 0.13$ | $5.66 \\pm 0.09$ | $5.61 \\pm 0.10$ |\n| 2048 | $21.9 \\pm 0.4$ | $21.6 \\pm 0.2$ | $2.90 \\pm 0.12$ | $2.85 \\pm 0.10$ | $5.28 \\pm 0.12$ | $5.21 \\pm 0.12$ |\n| 4096 | $21.5 \\pm 0.5$ | $21.2+0.4$ | $2.79 \\pm 0.06$ | $2.73 \\pm 0.05$ | $5.31 \\pm 0.08$ | $5.18 \\pm 0.09$ |\n| 8192 | $21.4 \\pm 0.5$ | $21.3 \\pm 0.4$ | $2.76 \\pm 0.03$ | $2.68 \\pm 0.04$ | $5.16 \\pm 0.18$ | $5.00 \\pm 0.11$ |\n| 16384 | OOM | OOM | OOM | OOM | OOM | OOM |\n\n### 5.4 Plots of Kernel Functions\n\nWe plot kernel functions including the power, log variants, and ALiBi for different heads to see their contributions to softmax. We use the GitHub dataset for demonstration. Please see Figure 2,3 and 4 Both ALiBi and its generalized power variant quickly reach a very negative value. In contrast, the log variant successfully discovers several flat kernels, effectively extending the window attention. This corroborates our previous observation that KERPLE-log can utilize more distant token information. Figure 2: Kernel Functions of Learned by the Log Variant. ![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-08.jpg?height=497&width=985&top_left_y=1985&top_left_x=559)\n\nFigure 3: Kernel Functions Learned by the Power Variant. Note the y-axis should be multiplied by $1 e 8$, which is a very negative value. ![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-09.jpg?height=500&width=1001&top_left_y=341&top_left_x=562)\n\nFigure 4: Kernel Functions Learned by ALiBi. ![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-09.jpg?height=506&width=987&top_left_y=926&top_left_x=558)\n\n### 5.5 Position-wise Perplexity Evaluation\n\nWe plot the position-wise perplexity with evaluation length=4096 in Figure 5.",
    "kerple-14": "Please see Appendix A. 6 for similar length $=16384$ result. The evaluation is done by measuring the loss at each position in each sequence and averaging over the sequences. We note that PPL@512 of KERPLE-log is the lowest among all model variants. We can derive several critical observations for evaluation length=4096 in Figure 5. First, KERPLE-log lies below KERPLElog-windowed@512, indicating its usage of more distant information than window attention: If our model does not use more information other than a fixed-window=512, the $y$-values after position=512 should overlap with the line windowed at 512. This is clearly not the case. In addition, the PPL of KERPLE-log continues to decrease till the end of 4096 positions (Not plateauing). Second, T5 lies below KERPLE-log-windowed@512 most of the time and fluctuates around KERPLE-logwindowed@512 after length=3000. It is still worse than KERPLE-log. Third, ALiBi lies above KERPLE-log-windowed@512 for almost all the positions, indicating that window attention might be a better choice than ALiBi. Although window attention is a strong baseline, our KERPLE-log is almost like a free lunch compared to window attention: With only 24 additional learnable parameters ( 2 para. for each head), the almost same training speed, and the same train length=512 as window attention, it is able to achieve lower PPLs across different positions. ## 6 Conclusion and Future Work\n\nA general framework, KERPLE, is proposed to kernelize relative positional embeddings for length extrapolation. At the core of this framework is the application of CPD kernels and the derivation of practical variants. We show that these CPD kernels can be implicitly converted to PD kernels, which\n\nFigure 5: Position-wise Perplexity on GitHub at Evaluation Length=4096 Compared to Window Attention@512. ![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-10.jpg?height=1044&width=1395&top_left_y=340&top_left_x=365)\nkeep the inner product interpretation of self-attention. We also demonstrate that the logarithmic variant achieves exceptional extrapolation performance on three large language modeling datasets. We believe our work paves the way for some interesting future directions that resolve our limitations. For instance, we can consider general kernel families and model non-monotonic effects due to positional differences. In addition, the use of learnable parameters in KERPLE might enable better generalization to inputs higher than one-dimensional. Last but not least, there is always room for improving memory efficiency by adjusting the model architecture and training procedure. ## 7 Broader Impact\n\nOur work develops a better understanding of relative positional embedding for transformers based on expressive kernel classes that adapt well to various datasets. The results apply to domains where the positional information is helpful in the modeling, e.g., natural language, programming language, and DNA/protein sequences for biology/medicine. The studies of transformers may have positive economic effects by enabling new tasks which cannot be done by humans or enhancing accuracy and efficiency. But inappropriate use can have negative societal impacts. These include job loss due to automation, the ethical challenges from improper text generation, and the privacy issues in the data collection process. These implications apply to any research on natural language processing and are not associated with any specific work. ## 8 Acknowledgement\n\nWe thank the anonymous reviewers for their insightful feedback and suggestions. We thank Princeton Research Computing for the technical support on the Della and the Adroit clusters. The third author acknowledges support from NSF MRI Award: 1919452. ## References\n\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 300-325, Online, April 2021. Association for Computational Linguistics. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.",
    "kerple-15": "In International Conference on Machine Learning, pages 11328 -11339. PMLR, 2020. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. SHAPE: Shifted absolute position embedding for transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309-3321, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. Cape: Encoding relative positions with continuous augmented positional embeddings.",
    "kerple-16": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 16079-16092. Curran Associates, Inc., 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung, Yin-Wen Chang, and ChunSung Ferng. A simple and effective positional encoding for transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2974-2988, $2021 b$. Ulme Wennberg and Gustav Eje Henter. The case for translation-invariant self-attention in transformerbased language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 130-140, Online, August 2021. Association for Computational Linguistics. Bernhard Sch\u00f6lkopf. The kernel trick for distances.",
    "kerple-17": "In T. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2020. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI blog, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 1243-1252. JMLR.org, 2017. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
    "kerple-18": "arXiv preprint arXiv:1910.13461, 2019. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M.",
    "kerple-19": "Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer. In International Conference on Learning Representations, 2019. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy, July 2019. Association for Computational Linguistics. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3327-3335, Online, November 2020. Association for Computational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. \\{DEBERTA\\}: \\{DECODING\\}\\{enhanced \\} \\{bert\\} \\{with\\} \\{disentangled\\} \\{attention\\}. In International Conference on Learning Representations, 2021. Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations, 2021. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative position encoding for vision transformer. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10013-10021, 2021a. Philipp Dufter, Martin Schmitt, and Hinrich Sch\u00fctze. Position Information in Transformers: An Overview. Computational Linguistics, pages 1-31, 072022. Sebastian Mika, Bernhard Sch\u00f6lkopf, Alex Smola, Klaus-Robert M\u00fcller, Matthias Scholz, and Gunnar R\u00e4tsch. Kernel pca and de-noising in feature spaces. Advances in neural information processing systems, 11, 1998. Christina Leslie, Eleazar Eskin, and William Stafford Noble. The spectrum kernel: A string kernel for svm protein classification.",
    "kerple-20": "In Biocomputing 2002, pages 564-575. World Scientific, 2001. Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral clustering and normalized cuts. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551-556, 2004. Hiroyuki Takeda, Sina Farsiu, and Peyman Milanfar. Kernel regression for image processing and reconstruction. IEEE Transactions on image processing, 16(2):349-366, 2007. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 4344-4353, Hong Kong, China, November 2019. Association for Computational Linguistics. Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. DA-transformer: Distance-aware transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059-2068, Online, June 2021b. Association for Computational Linguistics. Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. Advances in Neural Information Processing Systems, 34, 2021. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20.",
    "kerple-21": "Curran Associates, Inc., 2007. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.",
    "kerple-22": "In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5156-5165. PMLR, 13-18 Jul 2020. Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr \\\" om method. Advances in Neural Information Processing Systems, 34, 2021c. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nyst\u00f6m-based algorithm for approximating self-attention.",
    "kerple-23": "In Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 35, page 14138. NIH Public Access, 2021. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2021. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention.",
    "kerple-24": "In International Conference on Learning Representations, 2022. Christian Berg, Jens Peter Reus Christensen, and Paul Ressel. Harmonic analysis on semigroups: theory of positive definite and related functions, volume 100. Springer, 1984. Isaac J Schoenberg. Metric spaces and positive definite functions. Transactions of the American Mathematical Society, 44(3):522-536, 1938. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.",
    "kerple-25": "arXiv preprint arXiv:2101.00027, 2020. Sid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Jason Phang, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in pytorch, 2021. URL http://github.com/eleutherai/ gpt-neox\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages $3505-3506,2020$. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training.",
    "kerple-26": "arXiv preprint arXiv:1905.12322, 2019. ## A Appendix\n\n## A. 1 Proof of CPD Shift Lemma\n\nLemma 1 (CPD Shift Lemma). Let $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ be a conditionally positive definite ( $C P D$ ) kernel. Then, there exists $c \\geq 0$ such that $c+k(x, y)$ is a positive definite kernel. Proof. Let $K=\\left[k\\left(x_{i}, x_{j}\\right)\\right]_{i, j=1}^{N}$ be the matrix generated by $\\left\\{x_{1}, \\ldots, x_{N}\\right\\}$ with $N \\in \\mathbb{N}$. Consider\n\n$$\nf_{c}(v)=v^{\\top}\\left(c \\mathbb{1} \\mathbb{1}^{\\top}+K\\right) v=c\\left(v^{\\top} \\mathbb{1}\\right)^{2}+v^{\\top} K v\n$$\n\nWe want to show there exists a large enough $c$ such that $f_{c}(v) \\geq 0$ for all $v \\in\\{v:\\|v\\|=1\\}$. (i) It is sufficient to consider $a^{*}=\\min _{v:\\|v\\|=1} v^{\\top} K v<0$. Let $a^{*}$ be the solution to the minimization:\n\n$$\na^{*}=\\min _{v:\\|v\\|=1} v^{\\top} K v\n$$\n\nSince $v^{\\top} K v$ is continuous in $v$ and $\\{v:\\|v\\|=1\\}$ is compact (i.e., closed and bounded), $a^{*}$ must exist. If $a^{*} \\geq 0, K$ is positive semidefinite and $f_{c}(v) \\geq 0$ for $c \\geq 0$. Thus, without loss of generality, we assume $a^{*}<0$. ## (ii) It is sufficient to consider $K$ without zero eigenvalues (i.e., full rank). If there exists $v_{0}$ such that $K v_{0}=0$, then $c \\geq 0$ is enough to satisfy $f_{c}\\left(v_{0}\\right) \\geq 0$. For any $v_{1}$ satisfying $v_{1}^{\\top} v_{0}=0$, we have $\\left(v_{1}+v_{0}\\right)^{\\top} K\\left(v_{1}+v_{0}\\right)=v_{1}^{\\top} K v_{1}$. Therefore, whether there exists $c$ to have $f_{c}(v) \\geq 0$ doesn't depend on the eigenvector corresponding to zero eigenvalue (if there is such a vector). This means it is enough to consider $K$ without zero eigenvalues. ## (iii) It is sufficient to consider strict CPD. By definition of conditional positive definiteness (CPD), we know $v^{\\top} K v \\geq 0$ when $v^{\\top} \\mathbb{1}=0$. Since $K$ has no zero eigenvalue, we cannot have $v^{\\top} K v=0$ when $v^{\\top} \\mathbb{1}=q^{3}$. This means the inequality is strict here: $v^{\\top} K v>0$ when $v^{\\top} \\mathbb{1}=0$, and it is enough to consider strict CPD. (iv) It is sufficient to show there exists (small enough) $\\delta>0$ such that $\\boldsymbol{v}^{\\prime} \\in \\boldsymbol{T}_{\\delta} \\Rightarrow \\boldsymbol{v}^{\\boldsymbol{\\top}} \\boldsymbol{K} \\boldsymbol{v}^{\\prime}>0$. Note $T_{\\delta}$ is defined as\n\n$$\nT_{\\delta}=\\left\\{v^{\\prime}:\\left|v^{\\prime \\top} \\mathbb{1}\\right|<\\delta,\\left\\|v^{\\prime}\\right\\|=1\\right\\}\n$$\n\nFor any $v^{\\prime} \\in T_{\\delta}$, if $v^{\\prime}$ satisfies $v^{\\prime \\top} K v^{\\prime}>0$, then $c \\geq 0$ is enough to have $f_{c}\\left(v^{\\prime}\\right) \\geq 0$. Conversely, when $\\left|v^{\\top} \\mathbb{1}\\right| \\geq \\delta$ and $\\|v\\|=1$, observe that\n\n$$\nf_{c}(v)=c\\left(v^{\\top} \\mathbb{1}\\right)^{2}+v^{\\top} K v \\geq c\\left(v^{\\top} \\mathbb{1}\\right)^{2}+a^{*} \\geq c \\delta^{2}+a^{*}\n$$\n\nThen, $f_{c}(v) \\geq 0$ when $c \\geq \\frac{-a^{*}}{\\delta^{2}}$. Therefore, we need to prove $v^{\\prime} \\in T_{\\delta} \\Rightarrow v^{\\prime \\top} K v^{\\prime}>0$ for small enough $\\delta$. (v) Leveraging Continuity. Consider $v^{\\prime}$ satisfying $\\left\\|v^{\\prime}-v\\right\\|<\\delta_{2}$ with $v \\in S=\\{v:\\|v\\|=$ $\\left.1, v^{\\top} \\mathbb{1}=0\\right\\}$. Since $v^{\\top} K v$ is continuous in $v$ and $\\|K\\|<\\infty$, for any $\\epsilon>0$ and any $v \\in S$, a small enough $\\delta_{2}>0$ gives $\\left|v^{\\prime \\top} K v^{\\prime}-v^{\\top} K v\\right|<\\epsilon$. To see this, taking $v^{\\prime}=v+p$ with $\\|p\\|<\\delta_{2}$, we have\n$\\left|v^{\\prime \\top} K v^{\\prime}-v^{\\top} K v\\right|=\\left|p^{\\top} K v+v^{\\top} K p+p^{\\top} K p\\right| \\underset{\\|v\\|=1}{\\leq}\\|K\\|\\left(2\\|p\\|+\\|p\\|^{2}\\right) \\leq\\|K\\|\\left(2 \\delta_{2}+\\delta_{2}^{2}\\right)$. Therefore, $0<\\delta_{2}<\\sqrt{1+\\epsilon /\\|K\\|}-1$ is enough to have $\\left|v^{\\prime \\top} K v^{\\prime}-v^{\\top} K v\\right|<\\epsilon$. By definition of strict CPD, we know $\\min _{v \\in S} v^{\\top} K v=\\lambda>0$. Thus, take $\\epsilon<\\lambda$, a small enough $\\delta_{2}$ gives $v^{\\prime \\top} K v^{\\prime}>v^{\\top} K v-\\epsilon>=\\lambda-\\epsilon>0$. In other words, there exists a small enough $\\delta_{2}$ such that $v^{\\prime \\top} K v^{\\prime}>0$ for $v^{\\prime} \\in S_{\\delta_{2}}=\\left\\{v^{\\prime}:\\left\\|v^{\\prime}-v\\right\\|<\\delta_{2}, v \\in S\\right\\}$. (vi) Proving $\\exists \\delta>0$ s.t. $v^{\\prime} \\in T_{\\delta} \\Rightarrow v^{\\prime \\top} K v^{\\prime}>0$. Due to (iv), we want to show $\\exists \\delta>0$ s.t. $v^{\\prime} \\in T_{\\delta} \\Rightarrow v^{\\prime \\top} K v^{\\prime}>0$. We will prove by the conclusion of (v). Let $\\left\\|v^{\\prime}\\right\\|=1, v^{\\prime \\top} \\mathbb{1}=r$ with $|r|<\\delta$ and $v^{\\prime \\prime}=v^{\\prime}-\\frac{r}{n} \\mathbb{1}$. We have\n\n$$\nv^{\\prime \\prime \\top} \\mathbb{1}=0, \\quad\\left\\|v^{\\prime}-v^{\\prime \\prime}\\right\\|=\\frac{r}{\\sqrt{n}}, \\quad\\left\\|v^{\\prime \\prime}\\right\\|=\\sqrt{\\left\\|v^{\\prime}-\\frac{r}{n} \\mathbb{1}\\right\\|^{2}}=\\sqrt{1-\\frac{r^{2}}{n}}\n$$\n\nTake $v=\\frac{v^{\\prime \\prime}}{\\left\\|v^{\\prime \\prime}\\right\\|}=\\frac{v^{\\prime \\prime}}{q}$, where $q=\\sqrt{1-\\frac{r^{2}}{n}}$. We have $v^{\\top} \\mathbb{1}=0, \\quad\\|v\\|=1$ and\n\n$$\n\\begin{aligned}\n\\left\\|v^{\\prime}-v\\right\\|^{2} & =\\left\\|\\left(1-\\frac{1}{q}\\right) v^{\\prime}+\\frac{1}{q} \\frac{r}{n} \\mathbb{1}\\right\\|^{2}=\\left(1-\\frac{1}{q}\\right)^{2}+\\frac{r^{2}}{n q^{2}}+2\\left(1-\\frac{1}{q}\\right) \\frac{1}{q} \\frac{r^{2}}{n} \\\\\n& =\\left(1-\\frac{1}{q}\\right)^{2}+\\frac{r^{2}}{n}\\left(\\frac{2}{q}-\\frac{1}{q^{2}}\\right)=\\frac{1}{q^{2}}\\left((q-1)^{2}+\\frac{r^{2}}{n}(2 q-1)\\right) \\\\\n& =\\frac{1}{1-\\frac{r^{2}}{n}}\\left(1-\\frac{r^{2}}{n}-2 q+1+\\frac{r^{2}}{n} 2 q-\\frac{r^{2}}{n}\\right) \\\\\n& =\\frac{1}{1-\\frac{r^{2}}{n}}\\left(1-\\frac{r^{2}}{n}-2 q\\left(1-\\frac{r^{2}}{n}\\right)+1-\\frac{r^{2}}{n}\\right)=2-2 q \\\\\n& =2\\left(1-\\sqrt{1-\\frac{r^{2}}{n}}\\right) \\approx 2\\left(1-1+\\frac{1}{2} \\frac{r^{2}}{n}\\right)=\\frac{r^{2}}{n} \\quad\\left(\\sqrt{1-x} \\approx 1-\\frac{x}{2} \\text { when }|x| \\ll 1\\right)\n\\end{aligned}\n$$\n\nThus, $\\left\\|v^{\\prime}-v\\right\\|=O\\left(\\frac{|r|}{\\sqrt{n}}\\right) \\leq O\\left(\\frac{\\delta}{\\sqrt{n}}\\right)<\\delta_{2}$ for small enough $\\delta$. This implies that, with a small enough $\\delta$, for any $v^{\\prime} \\in T_{\\delta}$, we can find $v \\in S$ such that $\\left\\|v^{\\prime}-v\\right\\|<\\delta_{2}$. Thus, $v^{\\prime} \\in S_{\\delta_{2}}$, and by (v), we arrive at $v^{\\prime \\top} K v^{\\prime}>0$. [^3]\n## A. 2 Shift-invariant Kernels with Bounded and Unbounded Ranges\n\nDefinition 1 implies a shift-invariant kernel is generated by a univariate function $f: \\mathcal{X} \\rightarrow \\mathbb{R}$. To characterize the set of valid univariate functions, we introduce the positive definite functions as below. Definition 3 (Positive definite function). A (real) positive definite function is a function $f: \\mathcal{X} \\rightarrow \\mathbb{R}$ such that for any integer $N$ and any set of $N$ points $\\left\\{x_{i} \\in \\mathcal{X}\\right\\}_{i=1}^{N}$, the $N \\times N$ matrix $\\boldsymbol{A}=$ $\\left[f\\left(x_{i}-x_{j}\\right)\\right]_{i, j=1}^{N}$ is positive semidefinite. We will interchange the ideas of shift-invariant kernels and positive definite functions because they are equivalent by definition. Any statement in positive definite functions can be translated into shift-invariant kernels, and vice versa. Because of this, we will use some facts about the positive definite functions to derive the shift-invariant kernels of our interest. Generalizing Classical Bounded Shift-invariant Kernel. In the literature, there have been studies on applying kernels in the attention mechanism [Tsai et al. 2019.",
    "kerple-27": "Choromanski et al., 2021, Peng et al. 2021]. One of the most common approaches is to consider the Gaussian kernel:\n\n$$\nk(m, n)=\\exp \\left(-\\gamma(m-n)^{2}\\right), \\quad \\gamma>0\n$$\n\nNote the Gaussian kernel is bounded $(k(m, n) \\in(0,1]$ for the case above). To generalize it to a broader class of bounded shift-invariant kernels, observe that the Gaussian kernel generated by a positive definite function of the form $f(x)=\\exp \\left(-|x|^{2}\\right)$. Since there is no strong reason to stick to the power of 2, one may generalize it to a broader class of positive definite functions as below. Fact 5 (Corollary 3 of Schoenberg [1938]). $\\exp \\left(-|x|^{p}\\right)$ is positive definite if $0<p \\leq 2$ and not positive definite if $p>2$. Fact 5 implies that, if one wants to find a class of bounded shift-invariant kernel (i.e., $k(m, n)$ is within some fixed interval for any $m, n)$, then $k(m, n)=\\exp \\left(-a|m-n|^{p}\\right)$ with $a>0$ and $p \\in(0,2]$ may be of interest. Constructing Unbounded Shift-invariant Kernels. A limitation of Fact 5 is that it only generates kernels with a bounded range (here, the range is bounded in $(0,1])$. In situations where there are no explicit bounds, one might want to consider kernels with unbounded range. To construct such kernels, we utilize a kernel representation theorem presented in Schoenberg [1938]:\nFact 6 (Theorem 4 of Schoenberg [1938]). $f(x)$ is bounded away from zero $(f(x)>0)$ and its positive powers $f(x)^{\\lambda}(\\lambda>0)$ are all positive definite if and only if $f(x)$ is of the form\n\n$$\nf(x)=\\exp (c+\\psi(x))\n$$\n\nwhere $\\psi(x)$ is positive definite and $c$ is a real constant. Since Fact 6 works for non-negative kernels, we combine it with Fact 5 and show the following class of shift-invariant kernel with an unbounded range. Proposition 1 (Kernel from Distance Powers). For any $p \\in(0,2]$, there exists $c_{\\min } \\in \\mathbb{R}$ such that for any $c \\geq c_{\\min }$,\n\n$$\nk(m, n)=c-|m-n|^{p}\n$$\n\nis a positive definite kernel. When $p>2$, there is no c to make $k(m, n)$ positive definite. Proof. Due to Fact 55, we know $\\exp \\left(-|x|^{p}\\right)$ is positive definite when $p \\in(0,2]$. Since $\\exp \\left(-|x|^{p}\\right)>$ 0 and $\\exp \\left(-\\lambda|x|^{p}\\right)$ is positive definite for any $\\lambda>q^{4}$. Fact 6 implies there exists a $c^{\\prime} \\in \\mathbb{R}$ and a positive definite $\\psi(x)$ such that\n\n$$\n\\exp \\left(-|x|^{p}\\right)=\\exp \\left(c^{\\prime}+\\psi(x)\\right)\n$$\n\nIn other words, $-c^{\\prime}-|x|^{p}$ is a positive definite function. Take $c_{\\min }=-c^{\\prime}$. We see that $c_{\\min }-|m-n|^{p}$ is a shift-invariant positive definite kernel. Finally, let $k(m, n)=c-|m-n|^{p}$ with $c \\geq c_{\\min }$. The $N \\times N$ matrix $\\left[k\\left(x_{i}, x_{j}\\right)\\right]_{i, j=1}^{N}$ generated by $k(m, n)$ on points $\\left\\{x_{i} \\in \\mathbb{R}\\right\\}_{i=1}^{N}$ obeys\n\n$$\n\\left[k\\left(x_{i}, x_{j}\\right)\\right]_{i, j=1}^{N}=\\left[c_{\\min }-\\left|x_{i}-x_{j}\\right|^{p}\\right]_{i, j=1}^{N}+\\left(c-c_{\\min }\\right) \\mathbb{1} \\mathbb{1}^{\\top} \\succeq\\left(c-c_{\\min }\\right) \\mathbb{1} \\mathbb{1}^{\\top} \\succeq 0\n$$\n\n[^4]where $\\succeq$ is the Loewner order and $\\mathbb{1}=[1, \\ldots, 1]^{\\top}$ in the $N$-dimensional vector with all ones. This shows $\\bar{k}(m, n)$ is a shift-invariant positive definite kernel when $0<p \\leq 2$. The conclusion on $p>2$ is proved by contradiction. When $p>2$, if there exists a $c$ such that $k(m, n)$ is positive definite, then $\\exp (k(m, n))$ is positive definite, which contradicts to the case of $p>2$ in Fact 5\n\nProp. 1 introduces a kernel with unbounded range $(k(m, n) \\in(\\infty, c])$ and is adapted from the p-th power of the distance $|m-n|$. Since the distance is a notion of \"dissimilarity\", $-|m-n|^{p}$ becomes a notion of \"similarity\", which gives a sense of kernel. Thereby, we can interpret the constant $c$ as the required value to shift $-|m-n|^{p}$ such that $c-|m-n|^{p}$ becomes a positive definite kernel. In fact, $-|m-n|^{p}$ is a conditional positive definite kernel for $p \\in(0,2]$ Sch\u00f6lkopf [2000]. Therefore, the fact that $-|m-n|^{p}$ can become a positive definite kernel by shifting is not a coincidence, as it has already had an intimate relation to positive definite kernels. ## A. 3 Experiments on Gaussian-like Kernels\n\nSince the prior work on shift-invariant kernels mainly focuses on Gaussian kernels, we present preliminary experiments on Gaussian-like kernels. Compared with section 5.2 , the perplexities of these kernels are large at every extrapolation length. This verifies our previous assertion that the Gaussian-like kernels have limited extrapolation ability. Because the kernel can be used as a weight or a bias, we consider four kinds of the composite kernel (see section 4) as follows. ```\n(2-para-bias) \\(r_{1}, r_{2}>0\\). \\(k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+r_{1} \\exp \\left(-r_{2}|m-n|^{2}\\right)\\). (3-para-bias) \\(r_{1}, r_{2}>0\\) and \\(0<r_{3} \\leq 2\\). \\(k^{\\operatorname{comp}}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+r_{1} \\exp \\left(-r_{2}|m-n|^{r_{3}}\\right)\\)\n(1-para-wht) \\(r_{1}>0\\). \\(k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n} \\cdot \\exp \\left(-r_{1}|m-n|^{2}\\right)\\). (2-para-wht) \\(r_{1}>0\\) and \\(0<r_{2} \\leq 2\\). \\(k^{\\text {comp }}\\left(\\left[\\boldsymbol{q}_{m}, m\\right],\\left[\\boldsymbol{k}_{n}, n\\right]\\right)=\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n} \\cdot \\exp \\left(-r_{1}|m-n|^{r_{2}}\\right)\\). ```\n\n(2-para-bias) and (1-para-wht) are the settings where we put the Gaussian kernel as a bias and a weight, respectively. (3-para-bias) and (2-para-wht) generalize these settings by considering a learnable power between 0 and 2. Note we must constrain the power in $(0,2]$; otherwise, the function is not positive definite. See Fact 5 for details. These composite kernel $k^{\\mathrm{comp}}$ are plugged into the KERPLE framework, Eq. (2), and are evaluated on OpenWebText2, GitHub, and ArXiv datasets. Table 6 shows the Gaussian-like kernel is better to be a weight instead of a bias. As discussed in Appendix A. 2 the Gaussian-like kernels are bounded. To some extent, this implies that the bounded positive kernel can model a weight. However, compared with section 5.2 the Gaussian-like kernels have limited advantages in extrapolation. Although the performance might be improved if the power of $\\exp \\left(-|x|^{p}\\right)$ is relaxed from $p=2$ to $p \\in(0,2]$, still it cannot be as good as the logarithmic variant as we demonstrate in section 5.2. Therefore, while the Gaussian kernel is frequently used in the literature, we need a better class of shift-invariant kernels to tackle the length extrapolation challenge. ## A. 4 Experiments on Large Model, Longer Training Length, and Wikitext-103\n\nIn this subsection, we present additional experiments on (a) large models, (b) longer training length, and (c) Wikitext-103. Below is the summary of the experiments. (a) The 1.3B large model is trained on a machine with two NVIDIA A100 GPU with 40 GB of memory. We adopt almost all configurations of XL GPT-NeoX ${ }^{5}$, except that we change the trainmicro-batch-size to 16 , model-parallel-size to 2 , seq-length to 512 , and max-position-embeddings to 512 . Table 8 summarizes the configurations of the 1.3 B model. (b) The 162 M Model with training sequence length=1024 follows the same configurations as the ones in Table 2 except that the train seq. length is changed to 1024. [^5]Table 6: Extrapolation of Gaussian-like kernels on OpenWebText2, GitHub, and ArXiv. All models are trained for 50 k steps with training length 512 and five random seeds. | Extrp. | OpenWebText2 |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n|  | 1-para-wht | 2-para-wht | 2-para-bias | 3-para-bias |  |\n| 512 | $33.8 \\pm 1.1$ | $24.8 \\pm 0.9$ | $58.4 \\pm 71.6$ | $26.4 \\pm 0.5$ |  |\n| 1024 | $32.5 \\pm 0.8$ | $23.0 \\pm 0.8$ | $88.7 \\pm 62.6$ | $75.3 \\pm 37.8$ |  |\n| 2048 | $34.1 \\pm 0.6$ | $22.7 \\pm 0.4$ | $406 \\pm 101$ | $2629 \\pm 4024$ |  |\n| 4096 | $35.6 \\pm 0.9$ | $22.6 \\pm 0.6$ | $2590 \\pm 3211$ | $37557 \\pm 67936$ |  |\n| 8192 | $39.2 \\pm 1.1$ | $23.2 \\pm 0.3$ | $10829 \\pm 18855$ | $189216 \\pm 369499$ |  |\n|  |  | GitHub |  |  |  |\n| Extrp. | 1-para-wht | 2-para-wht | 2-para-bias |  |  |\n| 512 | $7.78 \\pm 0.48$ | $3.56 \\pm 0.23$ | $4.08 \\pm 0.85$ | 3-para-bias |  |\n| 1024 | $7.85 \\pm 0.40$ | $3.19 \\pm 0.17$ | $4.63 \\pm 0.59$ | $4.67 \\pm 0.23 \\pm 0.57$ |  |\n| 2048 | $8.08 \\pm 0.21$ | $3.01 \\pm 0.09$ | $18.8 \\pm 6.8$ | $20.0 \\pm 4.8$ |  |\n| 4096 | $8.47 \\pm 0.43$ | $2.93 \\pm 0.09$ | $75.8 \\pm 32.2$ | $94.0 \\pm 24.7$ |  |\n| 8192 | $9.41 \\pm 0.75$ | $3.05 \\pm 0.20$ | $207 \\pm 110$ | $261 \\pm 86$ |  |\n|  |  |  | ArXiv |  |  |\n| Extrp. | 1-para-wht | 2-para-wht | 2-para-bias |  |  |\n| 512 | $10.6 \\pm 0.4$ | $6.18 \\pm 0.25$ | $6.73 \\pm 0.30$ | 3-para-bias |  |\n| 1024 | $10.7 \\pm 0.2$ | $5.73 \\pm 0.11$ | $7.07 \\pm 0.63$ | $7.12 \\pm 1.43$ |  |\n| 2048 | $10.8 \\pm 0.3$ | $5.35 \\pm 0.15$ | $20.4 \\pm 9.3$ | $23.5 \\pm 0.69$ |  |\n| 4096 | $11.6 \\pm 0.3$ | $5.44 \\pm 0.14$ | $80.6 \\pm 49.4$ | $131 \\pm 140$ |  |\n| 8192 | $12.1 \\pm 0.2$ | $5.50 \\pm 0.27$ | $220 \\pm 138$ | $437 \\pm 591$ |  |\n\nTable 7: Training Time Comparison for Gaussian-like Kernels on GitHub. |  | 1-para-wht | 2-para-wht | 2-para-bias | 3-para-bias |\n| :---: | :---: | :---: | :---: | :---: |\n| sec/step | 0.326 | 0.327 | 0.324 | 0.351 |\n\n(c) The Wikitext-103 model is implemented on ALiBi's GitHuk ${ }^{6}$ with exactly the same configurations ( 247 M parameters), except that the function buffered_future_mask() at line 1011 of attention_with_linear_biases/fairseq/models/transformer.py is adapted to our KERPLE-log.",
    "kerple-28": "Table 8: 1.3B Model Configurations. | \\# Layers | Hidden Size | \\# Attention Heads | Train Seq. Len. | \\# Trainable Params. |\n| :---: | :---: | :---: | :---: | :---: |\n| 24 | 128 | 16 | 512 | 1.3 B |\n| Optimizer | Batch Size | Train Steps | Precision | \\# Trainable Params. for RPEs |\n| Adam (lr 2e-4) | 32 | 150,000 | float16 | 48 |\n\nTable 9 shows the results on the large model (1.3B). Compared with the small model results in Table 3, we see that T5 bias becomes weaker than KERPLE-log and ALiBi, and KERPLE-log remains stronger than ALiBi on GitHub and ArXiv datasets. This is explained by the tendency of overfitting. Observe that both T5 and KERPLE learn the positional embeddings while ALiBi uses fixed ones. T5 and KERPLE have a higher tendency of overfitting. A larger model (1.3B > 162M) or a noisy dataset (OpenWebText2 $>$ GitHub, ArXiv) posits a higher risk of overfitting. Hence, we see that T5 bias is weak on a large model, and KERPLE-log only extrapolates well on GitHub and ArXiv. Again, Table 9 shows the results on long training length (1024). compared with the short training length (512) in Table 3. KERPLE-log remains better than ALiBi and T5 bias, especially on longer evaluation length. This shows the robustness of KERPLE-log over different training lengths. Table 10 compares KERPLE-log with ALiBi using ALiBi's implementation and configurations. The results show that KERPLE-log is superior to ALiBi on Wikitext-103. [^6]Table 9: Perplexity Comparison for Large Models (1.3B) and Long Training Length (1024) on GitHub, ArXiv, OpenWebText2. Due to the time constraint and limited computing resources, we are not able to obtain the numbers for the large model (1.3B) on OpenWebText2 for now. All models are trained with five random seeds. $x^{\\dagger}$ means our log variant is statistically significantly better than $x$. The test used is paired two-sided t-test with $\\alpha=0.05$. | 162M Model. Train length, steps=1024, 50k. |  |  |  | 1.3B Model. Train length, steps=512, 150k. |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Extrp. | GitHub |  |  | GitHub |  |  |\n|  | KERPLE-log | ALiBi | T5 bias | KERPLE-log | ALiBi | T5 bias |\n| 512 | - | - | - | $\\mathbf{2 .",
    "kerple-29": "8 8} \\pm \\mathbf{0 .",
    "kerple-30": "1 1}$ | $\\mathbf{2 . 8 8} \\pm \\mathbf{0 . 1 1}$ | $2.93 \\pm 0.11^{\\dagger}$ |\n| 1024 | $2.83 \\pm 0.16$ | $2.84 \\pm 0.16^{\\dagger}$ | $2.81 \\pm 0.16$ | $2.60 \\pm 0.12$ | $2.62 \\pm 0.11^{\\dagger}$ | $2.64 \\pm 0.11^{\\dagger}$ |\n| 2048 | $2.70 \\pm 0.07$ | $2.82 \\pm 0.07^{\\dagger}$ | $2.68 \\pm 0.07$ | $2.44 \\pm 0.05$ | $2.58 \\pm 0.05^{\\dagger}$ | $2.47 \\pm 0.07^{\\dagger}$ |\n| 4096 | $2.53 \\pm 0.04$ | $2.77 \\pm 0.06^{\\dagger}$ | $2.54 \\pm 0.04$ | $2.46 \\pm 0.11$ | $2.65 \\pm 0.12^{\\dagger}$ | $2.49 \\pm 0.12$ |\n| 8192 | $2.42 \\pm 0.03$ | $2.74 \\pm 0.02^{\\dagger}$ | $2.57 \\pm 0.06^{\\dagger}$ | $2.44 \\pm 0.13$ | $2.57 \\pm 0.13^{\\dagger}$ | $2.57 \\pm 0.13^{\\dagger}$ |\n| 16384 | $2.48 \\pm 0.11$ | $2.80 \\pm 0.11^{\\dagger}$ | $3.10 \\pm 0.34^{\\dagger}$ | $\\mathbf{2 . 6 0} \\pm \\mathbf{0 . 0 7}$ | $2.61 \\pm 0.07$ | $3.16 \\pm 0.35^{\\dagger}$ |\n| Extrp. | ArXiv |  |  | ArXiv |  |  |\n|  | KERPLE-log | ALiBi | T5 bias | KERPLE-log | ALiBi | T5 bias |\n| 512 | - | - | - | $5.56 \\pm 0.15$ | $5.58 \\pm 0.16$ | $5.62 \\pm 0.15^{\\dagger}$ |\n| 1024 | $5.23 \\pm 0.09$ | $5.26 \\pm 0.09$ | $5.20 \\pm 0.10$ | $4.87 \\pm 0.07$ | $4.94 \\pm 0.07^{\\dagger}$ | $4.92 \\pm 0.06^{\\dagger}$ |\n| 2048 | $4.76 \\pm 0.12$ | $4.98 \\pm 0.18^{\\dagger}$ | $4.74 \\pm 0.12$ | $4.50 \\pm 0.16$ | $4.87 \\pm 0.17^{\\dagger}$ | $4.55 \\pm 0.16^{\\dagger}$ |\n| 4096 | $4.75 \\pm 0.10$ | $5.31 \\pm 0.13^{\\dagger}$ | $4.97 \\pm 0.27$ | $4.45 \\pm 0.06$ | $4.97 \\pm 0.13^{\\dagger}$ | $4.53 \\pm 0.08^{\\dagger}$ |\n| 8192 | $4.54 \\pm 0.10$ | $5.25 \\pm 0.15^{\\dagger}$ | $6.55 \\pm 0.97^{\\dagger}$ | $4.47 \\pm 0.20$ | $4.94 \\pm 0.16^{\\dagger}$ | $4.65 \\pm 0.15^{\\dagger}$ |\n| $\\underline{ } 16384$ | $4.62 \\pm 0.15$ | $5.35 \\pm 0.19^{\\dagger}$ | $16.0 \\pm 4.77^{\\dagger}$ | $4.65 \\pm 0.24$ | $4.94 \\pm 0.07$ | $5.25 \\pm 0.26^{\\dagger}$ |\n| Extrp. | OpenWebText2 |  |  | OpenWebText2 |  |  |\n|  | KERPLE-log | ALiBi | T5 bias | KERPLE-log | ALiBi | T5 bias |\n| 512 | 1 | 2 |  | $17.5 \\pm 0.3$ | $17.5 \\pm 0.4$ | $17.8 \\pm 0.3^{\\dagger}$ |\n| 1024 | $19.2 \\pm 0.1$ | $19.3 \\pm 0.2$ | $19.1 \\pm 0.1$ | $16.6 \\pm 0.6$ | $16.7 \\pm 0.6$ | $16.9 \\pm 0.6^{\\dagger}$ |\n| 2048 | $19.3 \\pm 0.2$ | $19.5 \\pm 0.1$ | $19.2 \\pm 0.2$ | $16.2 \\pm 0.4$ | $16.4 \\pm 0.4^{\\dagger}$ | $16.7 \\pm 0.4^{\\dagger}$ |\n| 4096 | $18.6 \\pm 0.3$ | $19.0 \\pm 0.3^{\\dagger}$ | $19.2 \\pm 0.4^{\\dagger}$ | $16.4 \\pm 0.8$ | $16.5 \\pm 0.5$ | $18.0 \\pm 0.9^{\\dagger}$ |\n| 8192 | $18.7 \\pm 0.5$ | $19.3 \\pm 0.4^{\\dagger}$ | $24.0 \\pm 1.1^{\\dagger}$ | $16.9 \\pm 0.7$ | $16.5 \\pm 0.1$ | $22.7 \\pm 3.7^{\\dagger}$ |\n| 16384 | $\\mathbf{1 8 . 8 \\pm 0 . 5}$ | $19.2 \\pm 0.3^{\\dagger}$ | $50.8 \\pm 6.5^{\\dagger}$ | $17.8 \\pm 1.2$ | $16.5 \\pm 0.3$ | $37.1 \\pm 13.1^{\\dagger}$ |\n\nTable 10: Perplexity Comparison on Wikitext-103. To ensure a fair comparison, the model (247M) is trained on ALiBi's codebase with exactly the same configurations except for the positional embeddings. The results show that KERPLE-log is superior to ALiBi on Wikitext-103. |  |  | train length 512 |  |  | train length 2048 |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Extrp. length | 512 | 1024 | 1536 | 2048 | 3072 | 2048 | 3072 |\n| ALiBi | 19.73 | 18.81 | 18.50 | 18.48 | 18.40 | 17.91 | 17.64 |\n| KERPLE-log | $\\mathbf{1 9 .",
    "kerple-31": "6 9}$ | $\\mathbf{1 8 . 7 6}$ | $\\mathbf{1 8 . 3 7}$ | $\\mathbf{1 8 . 2 9}$ | $\\mathbf{1 8 . 2 4}$ | $\\mathbf{1 7 .",
    "kerple-32": "8 4}$ | $\\mathbf{1 7 . 5 6}$ |\n\n## A. 5 Additional Analyses\n\nSince the power and logarithmic variants derived from KERPLE achieve superior performance on length extrapolation across various datasets, we investigate the underlying reason by visualizing the effective length as shown in Figure 6.",
    "kerple-33": "The visualization works in the following procedure. 1. For each training dataset, the learnable parameters $\\left(r_{1}^{(h)}, \\ldots, r_{\\ell}^{(h)}\\right)$ associated with each head $h(12$ in total) are extracted from the model checkpoint. The CPD kernel at head $h$ is $\\tilde{k}^{(h)}=\\tilde{k}_{r_{1}^{(h)}, \\ldots, r_{\\ell}^{(h)}}$. Both the power and the logarithmic variants in corollary 1 undergo a similar procedure. The only difference is that their $\\tilde{k}$ 's are different. 2. For each head $h$, we compute the effective length of $\\tilde{k}^{(h)}$ as $\\operatorname{eff}^{(h)}=\\min _{\\tilde{k}^{(h)}(0,|m-n|)<-2}|m-n|$. That is, the relative positional difference $|m-n|$ such that $\\tilde{k}(m, n) \\stackrel{\\text { shift-inv. }}{=} \\tilde{k}(0,|m-n|)$ just becomes smaller than -2 . Note $\\tilde{k}(0,|m-n|)$ strictly decreases in $|m-n|$, so there is only one\n\nFigure 6: Number of Heads with Effective Lengths $\\leq|m-n|$ for different choices of CPD kernels and datasets.",
    "kerple-34": "See section A. 5 for details. (a) Power Variant: $-a|m-n|^{p}$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-20.jpg?height=434&width=573&top_left_y=420&top_left_x=424)\n(b) Logarithmic Variant:\n$-a \\log (1+b|m-n|)$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-20.jpg?height=424&width=570&top_left_y=425&top_left_x=1122)\npossible value. We pick -2 here because $\\tilde{k}$ is a bias and is followed by the Softmax normalization. A bias of -2 or smaller can make a great impact on the output of Softmax $\\left.{ }^{7}\\right] \\mathrm{eff}^{(h)}$ is interpreted as the effective length because, when $|m-n|<\\mathrm{eff}^{(h)}$, the attenuation due to $\\tilde{k}^{(h)}$ is not strong. When $|m-n|>\\operatorname{eff}{ }^{(h)}$, the attenuation is strong and has a large impact on $\\boldsymbol{q}_{m}^{\\top} \\boldsymbol{k}_{n}+\\tilde{k}^{(h)}(m, n)$. 3. Then, for each $|m-n| \\in[0, \\ldots, 20480]$, we count the number of heads that satisfies $\\mathrm{eff}^{(h)} \\leq$ $|m-n|$. This gives a cumulative plot as shown in Figure 6, where the x-axis is $|m-n|$ and the y -axis is $\\operatorname{Count}\\left(\\left\\{h: h \\in[1, \\ldots, 12]\\right.\\right.$, eff $\\left.\\left.{ }^{(h)} \\leq|m-n|\\right\\}\\right)$.",
    "kerple-35": "4. Repeat the above steps for other datasets and kernels. Interpretation of Curves. For a point $(x, y)$ on a curve, it means that there are $y$ heads with at least -2 bias when the token distance $|m-n|$ is greater than $x$. In other words, the slower the $y$ converges to 12 , the longer the inter-token range that the model focuses on. The Advantage of Learnable Parameters. We observe that ALiBi [Press et al. 2022] produces the same curve no matter which dataset is used. The reason is that ALiBi selects a fixed parameter $r=2^{\\frac{-8 h}{H}}$ at head $h$ for its linear bias $-r|m-n|$ ( $H$ heads in total) regardless of the dataset. While this strategy is useful for extrapolation, we hypothesize that different datasets might have different characteristics, e.g., the average distance of highly related tokens should differ among the datasets as shown in Figure 6 . These characteristics are easier adapted by learnable parameters. Thus, we believe that learnable parameters have more advantages in capturing the dataset-dependent characteristics. Trends Across Datasets. We notice that both kernels trained on OpenWebText2 tend to focus more on distant relations. This makes sense because OpenWebText2 has the highest perplexity scores among all datasets, implying that more context is needed to disambiguate the next predicted token. The opposite trend holds for Arxiv and GitHub datasets, which is reasonable considering their lower perplexity scores. Characteristics Learned by Kernels. Under any dataset, the logarithmic variant tends to focus more on distant relations than the power variant does. We can explain it through their functional forms. Because logarithm $(-a \\log (1+b|m-n|))$ decays much slower than power $\\left(-a|m-n|^{p}\\right)$ does, the log variant might encourage the focus on distant relations. ## A. 6 Position-wise Perplexity for Length=16384\n\nWe can draw similar conclusions from Figure 78\n\n1. KERPLE-log lies below KERPLE-log-windowed@512 most of the time, indicating its usage of more distant information than window attention.",
    "kerple-36": "[^7]2. The PPL of T5 explodes. 3. The PPL of ALiBi does not explode, but it is still worse than window attention, i.e. lies above KERPLE-log-windowed@512. Figure 7: Position-wise Perplexity on GitHub at Evaluation Length=16384 Compared to Window Attention@512. ![](https://cdn.mathpix.com/cropped/2024_09_12_0bd192cf53977f5d3564g-21.jpg?height=1039&width=1397&top_left_y=524&top_left_x=359)\n\n## A. 7 The Choice of codebase and Hyperparameters\n\nWe adopt almost all the hyperparameters (except batch size to fit in our GPU) and all implementations of the T5 bias, ALiBi, Rotary, and Sinusoidal baselines from the GPT-NeoX codebase. To ensure fair comparisons, we did not fine-tune hyper-parameters for KERPLE. The datasets we used are exactly the same as the ones released with the GPT-NeoX codebase. We just ran their prepare_data.py script to automatically download and parse the datasets. All our code was uploaded with the submission on openreview, and https://github.com/EleutherAI/gpt-neox is the original GitHub repository. As a side note, we chose this codebase and adopted their parameter settings because it is built by EleutherAI, which is a well-known and truly non-profit group of researchers publishing various famous pretrained models for academia including GPT-J-6B and GPT-NeoX-20B. [^0]:    *Equal contribution\n\n[^1]:    ${ }^{1}$ By geometric sequence search, we can enlarge $c$ by $2,4,8,16$, and so on until we find the required large enough constant. [^2]:    $\\sqrt[2]{ }$ https://github.com/EleutherAI/gpt-neox/blob/main/configs/small_bf16.yml\n\n[^3]:    ${ }^{3}$ By spectral decomposition, $v^{\\top} K v=\\sum_{i} \\lambda_{i}\\left(v^{\\top} u_{i}\\right)^{2} \\geq 0$. Since there is no $\\lambda_{i}=0$, the inequality is strict. [^4]:    ${ }^{4} \\exp \\left(-\\lambda|x|^{p}\\right)=\\exp \\left(-\\left|\\lambda^{1 / p} x\\right|^{p}\\right)$ is a constant rescaling of $\\exp \\left(-|x|^{p}\\right)$ and therefore is positive definite. [^5]:    5 https://github.com/EleutherAI/gpt-neox/blob/main/configs/XL.yml\n\n[^6]:    ${ }^{6}$ https://github.com/ofirpress/attention_with_linear_biases\n\n[^7]:    ${ }^{7}$ Since Softmax is an exponentiated function, a - 2 bias in the Softmax's argument roughly gives an attenuation of $\\exp (-2) \\approx 0.135$\n\n"
}