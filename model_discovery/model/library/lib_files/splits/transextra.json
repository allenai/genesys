{
    "transextra-0": "# Exploring Transformer Extrapolation \n\n${ }^{1}$ Zhen Qin ${ }^{*},{ }^{1}$ Yiran Zhong ${ }^{* \\dagger},{ }^{2}$ Hui Deng<br>${ }^{1}$ OpenNLPLab, Shanghai Artificial Intelligence Laboratory, ${ }^{2}$ Northwestern Polytechnical University<br>https://github.com/OpenNLPLab/Rpe\n\n\n#### Abstract\n\nLength extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training.",
    "transextra-1": "Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on the Wikitext-103, Books, Github, and WikiBook datasets to demonstrate the viability of our discovered conditions. We also compare TRF to Empirical Receptive Field (ERF) across different models, showing consistently matched trends on these datasets. ## Introduction\n\nTransformer (Vaswani et al. 2017) is advancing steadily in the areas of natural language processing (Qin et al. 2023b; Devlin et al. 2019; Liu et al. 2019; Qin et al. 2022b,a; Liu et al. 2022; Qin and Zhong 2023), computer vision (Dosovitskiy et al. 2020; Sun et al. 2022b; Lu et al. 2022; Hao et al. 2024), and audio processing (Gong, Chung, and Glass 2021; Akbari et al. 2021; Gulati et al. 2020; Sun et al. 2022a). Although it outperforms other architectures such as RNNs (Cho et al. 2014; Qin, Yang, and Zhong 2023) and CNNs (Kim 2014; Hershey et al. 2016; Gehring et al. 2017) in many sequence modeling tasks, its lack of length extrapolation capability limits its ability to handle a wide range of sequence lengths, i.e., inference sequences need to be equal to or shorter than training sequences. Increasing the training sequence length is only a temporary solution because the space-time complexity grows quadratically with\n\n[^0]the sequence length. Another option is to extend the inference sequence length by converting the trained full attention blocks to sliding window attention blocks (Beltagy, Peters, and Cohan 2020), but this will result in significantly worse efficiency than the full attention speed (Press, Smith, and Lewis 2022). How to permanently resolve this issue without incurring additional costs has emerged as a new topic. A mainstream solution for length extrapolation is to design a Relative Positional Encoding (RPE) (Qin et al. 2023c) that concentrates attention on neighboring tokens. For example, ALiBi (Press, Smith, and Lewis 2022) applies linear decay biases to the attention to reduce the contribution from distant tokens. Kerple (Chi et al. 2022) investigates shiftinvariant conditionally positive definite kernels in RPEs and proposes a collection of kernels that promote the length extrapolation property. It also shows that ALiBi is one of its instances. Sandwich (Chi, Fan, and Rudnicky 2022) proposes a hypothesis to explain the secret behind ALiBi and empirically proves it by integrating the hypothesis into sinusoidal positional embeddings. In order to investigate transformer extrapolation, we first establish a hypothesis regarding why existing RPE-based length extrapolation methods (Qin et al. 2023a) have this capacity to extrapolate sequences in inference based on empirical analysis. Then we identify the conditions of RPEs that satisfy the hypothesis through mathematical analysis. Finally, the discovered conditions are empirically validated on a variety of corpora. Specifically, we assume that due to decay biases, existing RPE-based length extrapolation methods behave similarly to sliding window attention, i.e., only tokens within a certain range can influence the attention scores. A transformer can extrapolate for certain in this scenario since the out-of-range tokens have no effect on the attention outcomes. We derive that a transformer is guaranteed to satisfy this hypothesis if the series corresponding to the exponential of its RPE converges. Based on the observation, we show that previous RPE-based methods (Press, Smith, and Lewis 2022; Chi et al. 2022) can be seen as particular instances under the conditions. Two new practices from the conditions are derived and evaluated in language modeling. The observed conditions not only shed light on the secret of length extrapolation but also offer a new perspective on computing the Theoretical Receptive Fields (TRF) of RPEs. In contrast to prior approaches that require training gradi-\nents to compute TRF, we propose a new way to calculate TRF that is solely based on the formulation of RPEs. Extensive experiments on Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020), and WikiBook (Wettig et al. 2022) validate the conditions. TRF calculated by our method substantially matches the trend of the Empirical Receptive Field (ERF) in real-world scenarios. ## Preliminary\n\nBefore embarking on the journey of exploring, we introduce several preliminary concepts that will be used throughout the paper, such as softmax attention, relative positional encoding, length extrapolation, and sliding window attention.",
    "transextra-2": "We also provide the necessary notations for the subsequent analysis, i.e., we use $\\mathbf{M}$ to denote a matrix and $\\mathbf{m}_{i}^{\\top}$ to represent the $i$ th row of $\\mathbf{M}$. The complete math notations can be found in Appendix. Following previous work (Press, Smith, and Lewis 2022), we restrict our analysis to causal language models and assume that the max sequence length during training is $m$. ## Softmax attention\n\nSoftmax attention is a key component of transformers which operates on query $\\mathbf{Q}$, key $\\mathbf{K}$ and value $\\mathbf{V}$ matrices. Each matrix is a linear map that takes $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ as input:\n\n$$\n\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{Q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{K}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{V} \\in \\mathbb{R}^{n \\times d}\n$$\n\nwhere $n$ is the sequence length and $d$ is the dimension of the hidden feature. The output attention matrix $\\mathbf{O} \\in \\mathbb{R}^{n \\times d}$ can be formulated as:\n\n$$\n\\mathbf{O}=\\operatorname{Softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\mathbf{V}\n$$\n\nTo prevent information leakage in causal language modeling, a mask matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$ is used to ensure that current tokens can only see previous tokens and themselves. The lower triangular elements of $\\mathbf{M}$ are 0 , and the upper triangular ones, except for the diagonal, are $-\\infty$. Then the output attention matrix $\\mathbf{O}$ for causal language models will be:\n\n$$\n\\mathbf{O}=\\operatorname{Softmax}\\left(\\mathbf{Q} \\mathbf{K}^{\\top} / \\sqrt{d}+\\mathbf{M}\\right) \\mathbf{V}\n$$\n\nNote that Eq.",
    "transextra-3": "3 can be seen as a general form of attention, i.e., when the elements of M are all 0, Eq.",
    "transextra-4": "3 is degenerated to Eq. 2. For ease of discussion, we use Eq. 3 to represent attention computation. ## Relative positional encoding\n\nPositional encoding is designed to inject positional bias into transformers. Absolute Positional Encoding (APE) (Vaswani et al. 2017; Gehring et al. 2017) and Relative Positional Encoding (RPE) (Su et al. 2021; Liutkus et al. 2021; Press, Smith, and Lewis 2022; Chi et al. 2022) are the two most common types of positional encoding. In this paper, we focus on RPE because it is the key for length extrapolation, as shown in (Press, Smith, and Lewis 2022). An attention with RPE can be written as:\n\n$$\n\\mathbf{O}=\\operatorname{Softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}+\\mathbf{M}+\\mathbf{P}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ is a Toeplitz matrix that encodes relative positional information, i.e., $p_{i j}=p_{i-j}$. It is worth noting that $\\mathbf{M}$ and $\\mathbf{P}$ can be merged, and the merged matrix is still a Toeplitz matrix. We use $\\mathbf{R}$ to represent the merged matrix and rewrite Eq. 4 as:\n\n$$\n\\mathbf{O}=\\operatorname{Softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}+\\mathbf{R}\\right) \\mathbf{V}\n$$\n\n## Definition of length extrapolation\n\nThe property of length extrapolation allows a model to be tested on longer sequences than those used in training. Previous sequence modeling structures such as RNNs (Hochreiter and Schmidhuber 1997) and CNNs (Gehring et al. 2017) often naturally possess this property, but it is a difficult task for transformers. This property is only present in sliding window transformers and a few transformer variants with specifically designed RPEs (Chi et al. 2022; Press, Smith, and Lewis 2022; Chi, Fan, and Rudnicky 2022). In language modeling, one token can only see itself and previous tokens. Therefore, regardless the sequence length, the performance should be stable for the neighboring tokens that are within the training sequence length (Beltagy, Peters, and Cohan 2020). For the tokens that are out of range, the performance will degrade if the model does not support length extrapolation (Press, Smith, and Lewis 2022). Based on the observation above, we give a definition of length extrapolation:\nDefinition 0.1. For a language model $\\mathcal{F}$, given dataset $\\mathcal{X}$, if for any $n$, there is,\n\n$$\n\\left|p p l_{n}(\\mathcal{X}, \\mathcal{F})-p p l_{m}(\\mathcal{X}, \\mathcal{F})\\right| / p p l_{m}(\\mathcal{X}, \\mathcal{F})<\\delta\n$$\n\nthen $\\mathcal{F}$ is considered to have the extrapolation property.",
    "transextra-5": "Here $\\delta>0$ is a small constant, $\\operatorname{ppl}_{n}(\\mathcal{X}, \\mathcal{F})$ means that $\\mathcal{F}$ calculates perplexity with a max sequence length of $n$ on the data set $\\mathcal{X}$. Empirically, if $\\mid p p l_{n}(\\mathcal{X}, \\mathcal{F})-$ $p p l_{m}(\\mathcal{X}, \\mathcal{F}) \\mid / p p l_{m}(\\mathcal{X}, \\mathcal{F})$ becomes very large $(\\gg 1)$ as $n$ increases, we consider that $\\mathcal{F}$ does not have extrapolation property. ## Sliding window attention\n\nFor the convenience of subsequent discussions, we define a window attention at position $i$ and window size $j$ as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{o}_{i}^{j} & =\\frac{\\sum_{i-j+1 \\leq s \\leq i} \\exp \\left(\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{s} / \\sqrt{d}\\right) \\exp \\left(r_{i s}\\right) \\mathbf{v}_{s}}{\\sum_{i-j+1 \\leq t \\leq i} \\exp \\left(\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{t} / \\sqrt{d}\\right) \\exp \\left(r_{i t}\\right)} \\\\\n& \\triangleq \\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} \\mathbf{v}_{s}}{C_{i j}}\n\\end{aligned}\n$$\n\nwhere $C_{i j}=\\sum_{i-j+1 \\leq t \\leq i} c_{i t}, c_{i j}=a_{i j} b_{i j}, a_{i j}=$ $\\exp \\left(\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{j} / \\sqrt{d}\\right), b_{i j}=\\exp \\left(r_{i j}\\right), j \\leq i$. We further assume $\\left\\|\\mathbf{x}_{i}\\right\\| \\leq l, \\mathbf{x} \\in\\{\\mathbf{q}, \\mathbf{k}, \\mathbf{v}\\}$, where $l>0$ is a constant. The $\\mathbf{o}_{i}^{j}$ represents the attention output of the $i$-th token, which interacts with the $j$ tokens preceding it. Note that window attention naturally possesses the length extrapolation ability. There are two ways to infer window attention: nonoverlapping inference and sliding window inference as shown on the right of Figure 1. In sliding window inference, the tokens within each sliding window must be re-encoded multiple times, making it substantially slower than the nonoverlapping one. In table 1 we compare the average inference time over a group of window sizes between the sliding window inference and nonoverlapping window inference. The sliding window one is more than 44 times slower than the nonoverlapping one. However, as shown on the left of Figure 1 , the sliding window inference has much lower ppl than the nonoverlapping one. ![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-03.jpg?height=462&width=1701&top_left_y=151&top_left_x=209)\n\nFigure 1: Sliding window inference $\\boldsymbol{v} \\boldsymbol{s}$ Nonoverlapping inference. We illustrate the difference between sliding window inference and nonoverlapping inference in the right figure. The left figure shows the curves of \"Sliding Window\" and \"Nonoverlapping Window\"corresponding to the ppls calculated by a language model at different inference window sizes. As a reference, we also plot the average ppl of Alibi extrapolated from 512 to 9216 . The sliding window inference has much lower ppl than the nonoverlapping one and is close to the Alibi's performance. Table 1: Relative average inference time. We compute the relative average inference time of sliding window inference and nonoverlapping inference over a set of window sizes $\\{16,32,64,128,258,512\\}$. We also include the Alibi inference time as a reference. | Method | Rel Avg infer time |\n| :--- | :---: |\n| Sliding Window | 44.35 |\n| Nonoverlapping Window | 1.00 |\n| Alibi | 1.00 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-03.jpg?height=421&width=399&top_left_y=1177&top_left_x=191)\n(a) Sliding window\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-03.jpg?height=410&width=399&top_left_y=1188&top_left_x=603)\n(b) Sliding Window\nFigure 2: Visualization of attention reweighting. We plot the reweighting schema of sliding window attention and Alibi linear decay bias. They share a similar behavior in that only neighboring tokens can influence the attention results. ## Transformer Extrapolation Exploration\n\nIn this section, we first describe the hypothesis about why existing RPE-based length extrapolation methods can extrapolate sequences in inference and provide empirical evidence for it. Then we derive the conditions for length extrapolation in detail and demonstrate that recent RPE-based length extrapolation methods (Chi et al. 2022; Press, Smith, and Lewis 2022) satisfy the conditions. ## The hypothesis\n\nA sliding window attention with window size $w$ is equivalent to the following RPE on full attention:\n\n$$\n\\mathrm{m}_{i j}= \\begin{cases}0, & i-j \\leq w \\\\ -\\infty, & \\text { others }\\end{cases}\n$$\n\nBy comparing Eq. 8 and the corresponding RPE of Alibi (Press, Smith, and Lewis 2022) in Figure 2, we can see that they both have the same behavior in that they both concentrate tokens inside a specified range. Also, in Figure 1, we show that the performance of Alibi is similar to the sliding window attention when the window size is sufficiently large. Based on these two observations, we make the following hypothesis:\nHypothesis 0.1. A RPE that makes a transformer extrapolatable needs to have similar behavior to sliding window attention, i.e., $\\delta(i, j)$ should satisfy:\n\n$$\n\\forall \\epsilon>0, \\exists j_{0}, \\text { s.t }, j>j_{0}, \\delta(i, j)<\\epsilon\n$$\n\nwhere $\\delta(i, j) \\triangleq\\left\\|\\mathbf{o}_{i}^{i}-\\mathbf{o}_{i}^{j}\\right\\|$, and the window length $j$ needs to be sufficiently large. In the following sections, we will derive the conditions for RPEs that satisfy Eq. 9. ## The conditions\n\nLet us introduce the first lemma:\nLemma 0.2. When the following condition is satisfied, Eq. 9 holds. $$\n\\lim _{i \\rightarrow \\infty} C_{i i} \\triangleq C<\\infty\n$$\n\nProof. When $i \\leq m$, the test sequence length is smaller than the max sequence length $m$ during training, take $j=i$, we get $\\left\\|\\mathbf{o}_{i}^{i}-\\mathbf{o}_{i}^{j}\\right\\|=\\left\\|\\mathbf{o}_{i}^{i}-\\mathbf{o}_{i}^{i}\\right\\|=0$. When $i>m$, we can reformulate Eq. 7 as:\n\n$$\n\\begin{aligned}\n\\mathbf{o}_{i}^{i} & =\\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} \\mathbf{v}_{s}+\\sum_{1 \\leq s \\leq i-j} c_{i s} \\mathbf{v}_{s}}{C_{i i}} \\\\\n& =\\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} \\mathbf{v}_{s}}{C_{i j}} \\frac{C_{i j}}{C_{i i}}+\\frac{\\sum_{1 \\leq s \\leq i-j} c_{i s} \\mathbf{v}_{s}}{C_{i i}-C_{i j}} \\frac{C_{i i}-C_{i j}}{C_{i i}} \\\\\n& =\\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} \\mathbf{v}_{s}}{C_{i j}} \\frac{C_{i j}}{C_{i i}}+\\frac{\\sum_{1 \\leq s \\leq i-j} c_{i s} \\mathbf{v}_{s}}{C_{i i}-C_{i j}}\\left(1-\\frac{C_{i j}}{C_{i i}}\\right) . \\end{aligned}\n$$\n\nTherefore we have:\n$\\mathbf{o}_{i}^{i}-\\mathbf{o}_{i}^{j}=\\left(1-\\frac{C_{i j}}{C_{i i}}\\right)\\left(\\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} \\mathbf{v}_{s}}{C_{i j}}-\\frac{\\sum_{1 \\leq s \\leq i-j} c_{i s} \\mathbf{v}_{s}}{C_{i i}-C_{i j}}\\right)$. For the second part:\n\n$$\n\\begin{aligned}\n& \\left\\|\\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} \\mathbf{v}_{s}}{C_{i j}}-\\frac{\\sum_{1 \\leq s \\leq i-j} c_{i s} \\mathbf{v}_{s}}{C_{i i}-C_{i j}}\\right\\| \\\\\n& \\leq \\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s}\\left\\|\\mathbf{v}_{s}\\right\\|}{C_{i j}}+\\frac{\\sum_{1 \\leq s \\leq i-j} c_{i s}\\left\\|\\mathbf{v}_{s}\\right\\|}{C_{i i}-C_{i j}} \\\\\n& \\leq \\frac{\\sum_{i-j+1 \\leq s \\leq i} c_{i s} l}{C_{i j}}+\\frac{\\sum_{1 \\leq s \\leq i-j} c_{i s} l}{C_{i i}-C_{i j}}=2 l\n\\end{aligned}\n$$\n\nWe have\n\n$$\n\\delta(i, j) \\leq 2\\left(1-\\frac{C_{i j}}{C_{i i}}\\right) l\n$$\n\nAccording to Eq 10 and the tail of convergence series is arbitrarily small. $\\forall C / 2>\\epsilon>0$, we can find a $j_{0}$, s.t. if $i \\geq j>j_{0}, C_{i i}-C_{i j}<\\epsilon$. We can also find a $j_{1}$, s.t. if $i \\geq j>j_{1}, C-\\epsilon<C_{i i}<C+\\epsilon$. If we take $j_{2}=\\max \\left(j_{0}, j_{1}\\right)$, so if $i \\geq j \\geq j_{2}$, we have:\n\n$$\nC_{i i}-C_{i j}<\\epsilon, C-\\epsilon<C_{i i}<C+\\epsilon\n$$\n\nSo when $i \\geq j \\geq j_{2}$, we have\n\n$$\n\\begin{aligned}\n\\delta(i, j) \\leq 2\\left(1-\\frac{C_{i j}}{C_{i i}}\\right) l & =2 \\frac{C_{i i}-C_{i j}}{C_{i i}} l \\leq 2 \\frac{\\epsilon}{C-\\epsilon} l \\\\\n& \\leq \\frac{2 l \\epsilon}{C-C / 2}=\\frac{4 l \\epsilon}{C}\n\\end{aligned}\n$$\n\nAccording to the definition of limitation, Eq. 10 holds. This lemma implies that for any token if the attention of the model focuses on its neighboring $j\\left(j \\geq j_{2}\\right)$ tokens, the model has length extrapolation property. The lemma accompanies our intuitions. Does it mean that as long as a RPE follows the same principle, i.e., places more weights on neighboring $j$ tokens, the model is guaranteed to have the length extrapolation property? In the following sections, we will demonstrate that concentrating more weights on neighboring tokens does not guarantee the transformer has the length extrapolation property. Specifically, we will provide a mathematical proof of the sufficient conditions for RPE to have the length extrapolation property.",
    "transextra-6": "Theorem 0.3. When the following condition is satisfied, Eq. 9 holds. $$\n\\lim _{i \\rightarrow \\infty} B_{i i}<\\infty, B_{i i}=\\sum_{1 \\leq t \\leq i} b_{i t}<\\infty\n$$\n\nProof. Since we assume $\\left\\|\\mathbf{q}_{i}\\right\\| \\leq l,\\left\\|\\mathbf{k}_{i}\\right\\| \\leq l$, then:\n\n$$\n\\begin{gathered}\na_{i j}=\\exp \\left(\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{j}\\right) \\leq \\exp \\left(l^{2}\\right) \\\\\nc_{i j}=a_{i j} b_{i j} \\leq \\exp \\left(l^{2}\\right) b_{i j}, C_{i i} \\leq \\exp \\left(l^{2}\\right) B_{i i}\n\\end{gathered}\n$$\n\nTherefore, Eq.",
    "transextra-7": "10 can be derived from Eq. 16. Combine with Lemma 0.2 , the proof is concluded. By leveraging the property of RPE, Theorem 0.3 can be further simplified as:\nTheorem 0.4.",
    "transextra-8": "When the following condition is satisfied, Eq. 9 holds. $$\n\\lim _{i \\rightarrow \\infty} \\sum_{t=1}^{i} b_{i-t}=\\lim _{i \\rightarrow \\infty} \\sum_{t=0}^{i-1} b_{t}<\\infty\n$$\n\nProof. According to the definition of RPE:\n\n$$\nB_{i i}=\\sum_{1 \\leq t \\leq i} b_{i t}=\\sum_{t=1}^{i} b_{i-t}=\\sum_{t=0}^{i-1} b_{t}\n$$\n\nThis means that Eq. 16 is equivalent to:\n\n$$\n\\lim _{i \\rightarrow \\infty} B_{i i}=\\lim _{i \\rightarrow \\infty} \\sum_{t=0}^{i-1} b_{t}<\\infty\n$$\n\nTheorem 0.4 indicates that as long as the series of $\\exp (\\mathrm{RPE})$ converges, the model is guaranteed to have length extrapolation property. Based on this principle, we can mathematically determine whether an RPE allows for length extrapolation before conducting experiments or designing a variety of RPEs that can do length extrapolation. In Appendix, we show that previous methods such as Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), and Sandwich (Chi, Fan, and Rudnicky 2022) satisfy our derived conditions for length extrapolation. ## Theoretical receptive field\n\nIn the previous section, we established the conditions for length extrapolation. As an extra bonus, we can derive Theoretical Receptive Fields (TRF) for any RPE-based length extrapolation method. Let us start with the definition of Empirical Receptive Field (ERF). ERF can be viewed as a window containing the vast majority of the information contained within the attention. Recall Eq. 13, by setting $1-\\frac{C_{i j}}{C_{i i}}=\\epsilon$, we can define:\n\n$$\nC_{i j}=C_{i i}(1-\\epsilon), n_{\\mathrm{emp}}(\\epsilon)=\\inf _{j}\\left(C_{i j}>C_{i i}(1-\\epsilon)\\right)\n$$\n\n$n_{\\text {emp }}(\\epsilon)$ is the ERF that represents the minimal sequence length required to maintain the performance within a gap of $\\epsilon$. Intuitively, ERF can be viewed as the smallest window that contains the majority of the information within an attention. Since it is related to both $a_{i j}$ and $b_{i j}$, it can only be calculated after training. Now we define TRF, which allows us to estimate the receptive field without training. To accomplish this, we consider the upper bound of $C_{i j}$.",
    "transextra-9": "From the definition of $C_{i j}$ and Eq. 17, $C_{i j}$ is upper bounded by $B_{i j}$. Therefore, we can define the TRF $n_{\\text {the }}^{b}(\\epsilon)$ respect to series $b_{t}$ as:\n\n$$\n\\begin{aligned}\nn_{\\text {the }}(\\epsilon) & =\\inf _{j}\\left(B_{i j}>B(1-\\epsilon)\\right) \\\\\n& =\\inf _{j}\\left(\\sum_{t=0}^{j-1} b_{t}>B(1-\\epsilon)\\right) \\\\\n& =\\inf _{j}\\left(\\sum_{t \\geq j} b_{t}<B \\epsilon\\right)\n\\end{aligned}\n$$\n\nwhere $B=\\lim _{j \\rightarrow \\infty} \\sum_{t=0}^{j-1} b_{t}$. We may find it difficult to give the analytical form of the partial sum of the series at times, but we can still compute the TRF numerically or compare the TRFs of different RPEs using the theorem below:\n\nTheorem 0.5. If the following conditions hold:\n\n$$\n\\frac{\\alpha_{t}}{\\alpha} \\leq \\frac{\\beta_{t}}{\\beta}, t \\rightarrow \\infty, \\alpha \\triangleq \\lim _{j \\rightarrow \\infty} \\sum_{t=0}^{j-1} \\alpha_{t}, \\beta \\triangleq \\lim _{j \\rightarrow \\infty} \\sum_{t=0}^{j-1} \\beta_{t}\n$$\n\nThen:\n\n$$\nn_{\\text {the }}^{\\alpha}(\\epsilon) \\leq n_{\\text {the }}^{\\beta}(\\epsilon), \\epsilon \\rightarrow 0\n$$\n\nProof. According to Eq.23, there exists $t_{0}>0$, such that, when $t>t_{0}$, we have:\n\n$$\n\\frac{\\alpha_{t}}{\\alpha} \\leq \\frac{\\beta_{t}}{\\beta}\n$$\n\nLet $\\epsilon<\\epsilon_{0}$, where\n\n$$\nn_{\\text {the }}^{\\beta}\\left(\\epsilon_{0}\\right)=t_{0}\n$$\n\nthen we get:\n\n$$\n\\sum_{t \\geq n_{\\text {the }}^{\\beta}(\\epsilon)} \\beta_{t} \\leq \\beta \\epsilon, n_{\\text {the }}^{\\beta}(\\epsilon)>t_{0}\n$$\n\nFinally:\n\n$$\n\\sum_{t \\geq n_{\\mathrm{the}}^{\\beta}(\\epsilon)} \\alpha_{t} \\leq \\sum_{t \\geq n_{\\mathrm{the}}^{\\beta}(\\epsilon)} \\frac{\\alpha \\beta_{t}}{\\beta} \\leq \\frac{\\alpha \\beta \\epsilon}{\\beta}=\\alpha \\epsilon\n$$\n\nAccording to Eq. 22, we have:\n\n$$\nn_{\\text {the }}^{a}(\\epsilon) \\leq n_{\\text {the }}^{b}(\\epsilon)\n$$\n\nThe $\\exp (\\mathrm{RPE})$ series follows the same trend as TRF, the smaller the series, the smaller the TRF. We provide several examples of how to compute TRF in the Appendix. ## Two new RPEs\n\nBased on the proven conditions of length extrapolation, we can design infinite kinds of RPEs with the length extrapolation property. Here, we propose two new RPEs to empirically prove the conditions and hypothesis, namely:\n\n$$\n\\begin{aligned}\n& \\text { Type } 1: b_{n}=\\frac{1}{n^{2}}=\\exp (-2 \\ln n) \\\\\n& \\text { Type } 2: b_{n}=\\exp \\left(-\\ln ^{2} n\\right)\n\\end{aligned}\n$$\n\nThe corresponding TRF of Type 1 is:\n\n$$\n\\begin{aligned}\nB_{i j} & =\\sum_{i=0}^{j-1} \\frac{1}{(i+1)^{2}} \\approx \\int_{1}^{j} \\frac{1}{x^{2}} d x=1-\\frac{1}{j}, B=1 \\\\\nn_{\\mathrm{the}}(\\epsilon) & =\\inf _{j}\\left(B_{i j}>B(1-\\epsilon)\\right) \\\\\n& =\\inf _{j}\\left(1-\\frac{1}{j}>1-\\epsilon\\right)=\\Theta\\left(\\frac{1}{\\epsilon}\\right)\n\\end{aligned}\n$$\n\nFor Type 2, it is difficult to provide the analytical form of its TRF. However, we can prove that the TRF of Type 2 is smaller than the TRF of Type 1 using Theorem 0.5 and the inequality below:\n\n$$\n\\forall c_{1}, c_{2}>0, \\frac{\\exp \\left(-\\ln ^{2} n\\right)}{c_{1}}<\\frac{1 / n^{2}}{c_{2}}, n \\rightarrow \\infty\n$$\n\n## Empirical Validation\n\nSetting All models are implemented in Fairseq (Ott et al.",
    "transextra-10": "2019) and trained on 8 V100 GPUs. We use the same model architecture and training configuration for all RPE variants to ensure fairness. For Wikitext-103 (Merity et al. 2016), since it is a relatively small dataset, we use a 6-layer transformer decoder structure with an embedding size of 512 . For other datasets, in particular, we used a 12-layer transformer decoder structure with an embedding size of 768. The evaluation metric is perplexity (PPL) and the max training length during training is 512. The detailed hyper-parameter settings are listed in Appendix. Dataset We conduct experiments on Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020) and WikiBook (Wettig et al. 2022). Wikitext-103 is a small dataset containing a preprocessed version of the Wikipedia dataset. It is widely used in many NLP papers. Books has a large number of novels, making it a good corpus for long sequence processing. Github consists of a sizable amount of open-source repositories, the majority of which are written in coding languages. WikiBook is a 22-gigabyte corpus of Wikipedia articles and books curated by (Wettig et al. 2022). This corpus is used to validate the performance of various models on large datasets. Validating the sufficiency. To empirically validate the sufficiency of our discovered conditions, we integrate the two RPEs that were proposed in the previous section into transformers and test their length extrapolation capability on Wikitext-103, Books, Github, and WikiBook datasets. We increase the length of the inference sequence from 512 to 9216 tokens and plot the testing PPLs of our proposed RPEs as well as those of existing methods such as Alibi, Kerple, and Sandwich in Figure 3. Detailed numerical results can be found in Table 4 and Table 5 from Appendix. All these methods demonstrate good length extrapolation capability. However, the stabilized PPL may vary due to the effectiveness of different positional encoding strategies, which are not considered in this paper. We include the Sinusoidal (Vaswani et al. 2017) positional encoding as a reference method that cannot extrapolate, which grows rapidly as the inference sequence length increases. Validating the necessity. Although we only provide mathematical proof for the sufficiency of our discovered conditions, we also attempt to verify their necessity empirically in this section. Specifically, we pick two RPEs that are very close to satisfying Theorem 0.4 as follows. Note that both of them concentrate their weight on neighboring tokens. $$\n\\text { Example1 }: b_{n}=\\frac{1}{n}, \\text { Example } 2: b_{n}=\\frac{1}{n \\ln n}\n$$\n\nBelow is a brief mathematical proof that the above RPEs do not satisfy Theorem 0.4 . $$\n\\begin{gathered}\n\\sum_{n=1}^{k} \\frac{1}{n}>\\int_{1}^{k+1} \\frac{1}{x} d x=\\ln (k+1) \\\\\n\\sum_{n=3}^{k} \\frac{1}{n \\ln n}>\\int_{3}^{k+1} \\frac{1}{x \\ln x} d x=\\ln \\ln (k+1)-\\ln \\ln 3\n\\end{gathered}\n$$\n\nWe then empirically test their length extrapolation capability on Wikitext-103, Books, Github, and WikiBook datasets by\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-06.jpg?height=1842&width=770&top_left_y=178&top_left_x=211)\n\nFigure 3: Sufficiency validation on Wikitext-103, Books, Github, WikiBook datasets. To test length extrapolation capability, we lengthen inference sequences from 512 to 9216 tokens and plot the testing PPLs of our proposed Type 1 and Type 2 RPEs, as well as Alibi, Kerple, and Sandwich. All these methods are stable in PPL. For methods that cannot extrapolate, e.g., Sinusoidal, its PPL grows rapidly. scaling the inference sequence length from 512 to 9216 tokens. As shown in Figure 4, the PPLs of both RPEs grow rapidly as the length of the testing sequence increases. It de-\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-06.jpg?height=1839&width=762&top_left_y=182&top_left_x=1135)\n\nFigure 4: Necessity validation on Wikitext-103, Books, Github, WikiBook datasets. We select two RPEs that do not satisfying Theorem 0.4 , e.g., $b_{n}=\\frac{1}{n}$ and $b_{n}=\\frac{1}{n \\ln n}$. We increase the inference sequence length from 512 to $\\frac{n}{} \\ln n 216$ tokens and plot the testing PPLs for them. Their PPLs increase rapidly as the inference sequence lengthens, whereas Type 1 remains stable. monstrates that both of them cannot extrapolate. We also include Type 1 RPE in Figure 4 as a reference that can extrapolate. Detailed numerical results can be found in Table 6\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-07.jpg?height=1839&width=772&top_left_y=181&top_left_x=209)\n\nFigure 5: Visualization of ERF We plot the ERF for Alibi, Kerple, Sandwich and our proposed Type 1 and Type 2 methods on Wikitext-103, Books, Github, and WikiBook datasets using trained models. By comparing with Figure 6, we can find that the overall trend of ERF matches the trend of TRF. ERF is normalized for better visualization. from Appendix. Validating TRF We validate our proposed TRF by comparing the trend between the TRF and ERF. We plot the TRFs and ERFs of the Alibi, Kerple, Sandwich, and our pro-\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-07.jpg?height=399&width=752&top_left_y=191&top_left_x=1142)\n\nFigure 6: Visualization of TRF. We numerically plot TRFs for existing methods and our proposed method. TRF is normalized for visualization. The TRF of Type 1 is larger than Type 2, which matches the Theorem 0.5 and our analysis. ![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-07.jpg?height=409&width=770&top_left_y=766&top_left_x=1133)\nFigure 7: Visualization of RPE. We plot the heatmap of $\\exp ($ RPE ) for Type 1 and Type 2. Type 2 concentrates weights on closer neighboring tokens than Type 1, indicating a smaller TRF. We also visualize other methods in Appendix. posed RPEs on the aforementioned datasets. As observed in Figure 6 and Figure 5, while the curves vary across datasets, TRF estimates a similar overall trend of ERFs. Visualizing RPE We visualize the weighting schemes of Type 1 and Type 2 in Figure 7, i.e., the heatmap of $\\exp (\\mathrm{RPE})$. Type 2 concentrates weights on closer neighboring tokens than Type 1, indicating a smaller TRF and ERF as shown in Figure 6 and Figure 5. We also visualize other methods in Appendix. ## Conclusion\n\nIn this paper, we explore the secrets of transformer length extrapolation in language modeling. We first make a hypothesis about extrapolation and then derived the sufficient conditions for RPE to have the length extrapolation property. A thorough mathematical analysis reveals that a transformer model is certain to be capable of length extrapolation if the series that corresponds to the exponential of its RPE converges. This observation brings an extra bonus: we can estimate TRFs of RPEs solely based on their formulations. We chose two new RPEs that satisfy the conditions and two that do not to empirically prove the sufficiency of the conditions on four widely used datasets. We also validated our TRFs by comparing them with ERFs on these datasets as well. The results show that our TRFs can accurately reflect the actual receptive fields of RPEs before training. ## Acknowledgement\n\nThis work is partially supported by the National Key R\\&D Program of China (NO.2022ZD0160100).",
    "transextra-11": "## References\n\nAkbari, H.; Yuan, L.; Qian, R.; Chuang, W.-H.; Chang, S.-F.; Cui, Y.; and Gong, B. 2021. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. In arXiv preprint arXiv:2104.11178. Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The Long-Document Transformer. In arXiv:2004.05150. Chi, T.-C.; Fan, T.-H.; Ramadge, P. J.; and Rudnicky, A.",
    "transextra-12": "I. 2022. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation.",
    "transextra-13": "ArXiv, abs/2205.09921. Chi, T.-C.; Fan, T.-H.; and Rudnicky, A. I. 2022. Receptive Field Alignment Enables Transformer Length Extrapolation.",
    "transextra-14": "ArXiv, abs/2212.10356. Cho, K.; van Merri\u00ebnboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1724-1734. Doha, Qatar: Association for Computational Linguistics. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171-4186. Minneapolis, Minnesota: Association for Computational Linguistics. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; Presser, S.; and Leahy, C. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. In arXiv preprint arXiv:2101.00027. Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y. N. 2017. Convolutional sequence to sequence learning. In International Conference on Machine Learning, 12431252. PMLR. Gong, Y.; Chung, Y.-A.; and Glass, J. 2021. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, 571575. Gulati, A.; Chiu, C.-C.; Qin, J.; Yu, J.; Parmar, N.; Pang, R.; Wang, S.; Han, W.; Wu, Y.; Zhang, Y.; and Zhang, Z., eds. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. Hao, D.; Mao, Y.; He, B.; Han, X.; Dai, Y.; and Zhong, Y. 2024. Improving Audio-Visual Segmentation with Bidirectional Generation. In Proceedings of the AAAI Conference on Artificial Intelligence. Hershey, S.; Chaudhuri, S.; Ellis, D. P. W.; Gemmeke, J. F.; Jansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous, R. A.; Seybold, B.; Slaney, M.; Weiss, R. J.; and Wilson, K.",
    "transextra-15": "W. 2016. CNN architectures for large-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 131-135. Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation, 9(8): 1735-1780. Kim, Y. 2014. Convolutional Neural Networks for Sentence Classification. In Conference on Empirical Methods in Natural Language Processing. Knopp, K. 1956. Infinite sequences and series. Courier Corporation. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Liu, Z.; Li, D.; Lu, K.; Qin, Z.; Sun, W.; Xu, J.; and Zhong, Y. 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955. Liutkus, A.; C\u00edfka, O.; Wu, S.-L.; Simsekli, U.; Yang, Y.H.; and Richard, G. 2021. Relative positional encoding for transformers with linear complexity.",
    "transextra-16": "In International Conference on Machine Learning, 7067-7079. PMLR. Lu, K.; Liu, Z.; Wang, J.; Sun, W.; Qin, Z.; Li, D.; Shen, X.; Deng, H.; Han, X.; Dai, Y.; and Zhong, Y. 2022. Linear video transformer with feature fixation. arXiv preprint arXiv:2210.08164. Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016. Pointer Sentinel Mixture Models. In arXiv:1609.07843. Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grangier, D.; and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038. Press, O.; Smith, N.; and Lewis, M. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations. Qin, Z.; Han, X.; Sun, W.; He, B.; Li, D.; Li, D.; Dai, Y.; Kong, L.; and Zhong, Y. 2023a. Toeplitz Neural Network for Sequence Modeling. In The Eleventh International Conference on Learning Representations. Qin, Z.; Han, X.; Sun, W.; Li, D.; Kong, L.; Barnes, N.; and Zhong, Y. 2022a. The Devil in Linear Transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 7025-7041. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Qin, Z.; Li, D.; Sun, W.; Sun, W.; Shen, X.; Han, X.; Wei, Y.; Lv, B.; Yuan, F.; Luo, X.; Qiao, Y.; and Zhong, Y. 2023b. Scaling TransNormer to 175 Billion Parameters. In arXiv preprint 2307.14995. Qin, Z.; Sun, W.; Deng, H.; Li, D.; Wei, Y.; Lv, B.; Yan, J.; Kong, L.; and Zhong, Y. 2022b. cosFormer: Rethinking Softmax In Attention. In International Conference on Learning Representations. Qin, Z.; Sun, W.; Lu, K.; Deng, H.; Li, D.; Han, X.; Dai, Y.; Kong, L.; and Zhong, Y. 2023c. Linearized Relative Positional Encoding. arXiv preprint arXiv:2307.09270. Qin, Z.; Yang, S.; and Zhong, Y. 2023. Hierarchically gated recurrent neural network for sequence modeling. NeurIPS. Qin, Z.; and Zhong, Y. 2023. Accelerating Toeplitz Neural Network with Constant-time Inference Complexity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Su, J.; Lu, Y.; Pan, S.; Wen, B.; and Liu, Y. 2021. Roformer: Enhanced transformer with rotary position embedding.",
    "transextra-17": "arXiv preprint arXiv:2104.09864. Sun, J.; Zhong, G.; Zhou, D.; Li, B.; and Zhong, Y. 2022a. Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition. arXiv preprint arXiv:2203.15609. Sun, W.; Qin, Z.; Deng, H.; Wang, J.; Zhang, Y.; Zhang, K.; Barnes, N.; Birchfield, S.; Kong, L.; and Zhong, Y. 2022b. Vicinity Vision Transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence, (01): 1-14. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.",
    "transextra-18": "Wettig, A.; Gao, T.; Zhong, Z.; and Chen, D. 2022. Should You Mask $15 \\%$ in Masked Language Modeling? In arXiv:2202.08005. Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV). ## Supplementary Material\n\n## Mathematical notations\n\n| Notation | Meaning |\n| :---: | :---: |\n| $\\mathbf{X}$ | Hidden state. |\n| $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ | Query, key, value. |\n| $\\mathbf{O}$ | Attention output. |\n| $d$ | Feature dimension. |\n| $\\mathbf{m}_{s}^{\\top}$ | $s$-th row of matrix $M$. |\n\nTable 2: Mathematical notations used in the paper. ## Examples\n\nIn this section, we use Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), and Sandwich (Chi, Fan, and Rudnicky 2022) as examples to support the discovered sufficient conditions 16 for length extrapolation. Alibi The form of Alibi can be written as:\n\n$$\nb_{t}=\\exp (-k t), k>0\n$$\n\nAccording to the convergence of geometric series:\n\n$$\n\\lim _{i \\rightarrow \\infty} B_{i i}=\\sum_{t=0}^{i-1} \\exp (-k t)<\\infty\n$$\n\nwhich satisfies our observed conditions. Kerple Kerple proposes two forms of RPEs: $\\log$ and power. We discuss them separately. The formulation of the Log variant can be expressed as:\n\n$$\nb_{t}=\\exp (-r \\log (1+k t))=\\frac{1}{(1+k t)^{r}}\n$$\n\nwhere $\\mathrm{r}_{1} \\mathrm{k}_{i} 0 \\mathrm{r}, \\mathrm{k}_{i} 0$. In Kerple, $\\mathrm{r}, \\mathrm{kr}$, $\\mathrm{k}_{\\text {are }}$ learnable. Based on Theorem 0.4 , to enable the model to extrapolate, we must add the restriction that $\\mathrm{r}_{i} 1 \\mathrm{r}_{i} 1$ because:\n\n$$\n\\frac{1}{(1+k t)^{r}} \\sim \\frac{1}{k^{r} t^{r}}\n$$\n\nIn empirical analysis, we will show that when $\\mathrm{r}=1 \\mathrm{r}=1$, the model cannot extrapolate. We also checked the trained model from Kerple and found this condition is met. The Ploy variant can be written as:\n\n$$\nb_{t}=\\exp \\left(-k t^{r}\\right), 0<r \\leq 2\n$$\n\nSince\n\n$$\nb_{t} \\leq \\exp (-k t), t \\rightarrow \\infty\n$$\n\naccording to the convergence of geometric series, we have:\n\n$$\n\\lim _{i \\rightarrow \\infty} B_{i i}=\\lim _{i \\rightarrow \\infty} \\sum_{t=0}^{i-1} \\exp \\left(-k t^{r}\\right)<\\infty\n$$\n\nwhich satisfies our observed conditions. Sandwich Given the formulation of Sandwich:\n\n$$\nb_{t}=\\exp \\left(k\\left(\\sum_{j=1}^{d / 2} \\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-\\frac{d}{2}\\right)\\right), k>0, r>0\n$$\n\nWe first do the following transformations:\n\n$$\n\\begin{aligned}\nb_{t} & =\\exp \\left(k\\left(\\sum_{j=1}^{d / 2}\\left(\\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-1\\right)\\right)\\right) \\\\\n& =\\prod_{j=1}^{d / 2} \\exp \\left(k\\left(\\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-1\\right)\\right)\n\\end{aligned}\n$$\n\nthen make a partition over $j$ :\n\n$$\n\\frac{t}{r^{2 j / d}} \\geq \\frac{\\pi}{2}\n$$\n\nwhich is equivalent to:\n\n$$\n\\begin{aligned}\n2 t & \\geq \\pi r^{2 j / d}, \\frac{2 t}{\\pi} \\geq r^{2 j / d} \\\\\n\\log _{r}\\left(\\frac{2 t}{\\pi}\\right) & \\geq 2 j / d, j \\leq \\frac{d \\log _{r}\\left(\\frac{2 t}{\\pi}\\right)}{2} \\triangleq f(t)\n\\end{aligned}\n$$\n\nTherefore we have:\n\n$$\n\\begin{aligned}\nb_{t} & =\\prod_{1 \\leq j \\leq f(t)} \\exp \\left(k\\left(\\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-1\\right)\\right) \\\\\n& \\times \\prod_{f(t)<j \\leq d / 2} \\exp \\left(k\\left(\\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-1\\right)\\right)\n\\end{aligned}\n$$\n\nFor the first part:\n\n$$\n\\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-1<-1\n$$\n\nFor the second part:\n\n$$\n\\cos \\left(\\frac{t}{r^{2 j / d}}\\right)-1<0\n$$\n\nThen:\n\n$$\n\\begin{aligned}\n\\beta_{t} & \\leq \\prod_{1 \\leq j \\leq f(t)} \\exp (-k) \\\\\n& =\\exp (-k\\lfloor f(t)\\rfloor) \\\\\n& \\leq \\exp (k) \\exp (-k f(t)) \\\\\n& =\\exp (k) \\exp \\left(-\\frac{k d \\log _{r}\\left(\\frac{2 t}{\\pi}\\right)}{2}\\right) \\triangleq g(t)\n\\end{aligned}\n$$\n\nAccording to Rabbe's test (Knopp 1956):\n\n$$\n\\begin{aligned}\nt\\left(\\frac{g(t)}{g(t+1)}-1\\right) & =t\\left(\\exp \\left(\\frac{k d}{2} \\log _{r} \\frac{2 t+2}{2 t}\\right)-1\\right) \\\\\n& =t\\left(\\exp \\left(\\frac{k d}{2} \\log _{r}\\left(1+\\frac{1}{t}\\right)\\right)-1\\right) \\\\\n& \\sim t\\left(\\frac{k d}{2} \\log _{r}\\left(1+\\frac{1}{t}\\right)+O\\left(\\frac{1}{t^{2}}\\right)\\right) \\\\\n& \\sim t\\left(\\frac{k d}{2 \\ln r}\\left(\\frac{1}{t}-\\frac{1}{2 t^{2}}\\right)+O\\left(\\frac{1}{t^{2}}\\right)\\right) \\\\\n& \\rightarrow \\frac{k d}{2 \\ln r}\n\\end{aligned}\n$$\n\nif:\n\n$$\n\\frac{k d}{2 \\ln r}<1, \\quad d<\\frac{2 \\ln r}{k}\n$$\n\nthen the series converges ${ }^{1}$. ## TRF example\n\nWe use Alibi as an example to show the calculation of TRF. The $B_{i j}$ of Alibi can be written as:\n\n$$\nB_{i j}=\\sum_{i=0}^{j-1} \\exp (-i)=\\frac{1-\\exp (-j)}{1-\\exp (-1)}, B=\\frac{1}{1-\\exp (-1)}\n$$\n\nThe TRF of Alibi can be calculated as:\n\n$$\n\\begin{aligned}\nn_{\\text {the }}(\\epsilon) & =\\inf _{j}\\left(B_{i j}>B(1-\\epsilon)\\right) \\\\\n& =\\inf _{j}(1-\\exp (-j)>1-\\epsilon)=\\Theta(-\\log \\epsilon)\n\\end{aligned}\n$$\n\nwhere $\\Theta$ represents the upper and lower asymptotic bound. [^1]\n## Configurations\n\n| Data | WikiText-103 | Others |\n| :--- | :--- | :--- |\n| Decoder layers | 6 | 12 |\n| Hidden dimensions | 512 | 768 |\n| Number of heads | 8 | 12 |\n| FFN dimensions | 2048 | 3072 |\n| Tokenizer method | BPE | BPE |\n| Src Vocab size | 50265 | 50265 |\n| Sequence length | 512 | 512 |\n| Total batch size | 128 | 128 |\n| Number of updates/epochs | 50 k updates | 50 k updates |\n| Warmup steps/epochs | 4 k steps | 4 k steps |\n| Peak learning rate | $5 \\mathrm{e}-4$ | $5 \\mathrm{e}-4$ |\n| Learning rate scheduler | Inverse sqrt | Inverse sqrt |\n| Optimizer | Adam | Adam |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-8$ | $1 \\mathrm{e}-8$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.98)$ | $(0.9,0.98)$ |\n| Weight decay | 0.01 | 0.01 |\n\nTable 3: Detailed training configurations used in our experiments. \"Total batch size\" means batch_per_gpu $\\times$ update_freq $\\times$ num_gpus. ## Pseudocode for TRF and ERF visualization\n\n```\nimport torch\ndef draw(array, n=50):\n    epsilon = torch.flip(torch.linspace(0, 1, n), dims=[0])\n    index = torch.zeros(n)\n    cusum = torch.sum(array)\n    m = len(array)\n    s = 0\n    i = 0\n    for j in range(m):\n        eps = epsilon[i]\n        while s >= cusum * (1 - eps) and i < n:\n            index[i] = j\n            if i < n - 1:\n                i += 1\n            else:\n                        break\n            eps = epsilon[i]\n        s += array[j]\n    while i < n:\n        index[i] = m\n        i += 1\n    return index / m, epsilon\n```\n\n\n## Detailed Experimental Results <br> Heatmap\n\n|  | Wikitext-103 |  |  |  |  |  |  |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Length | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2 |\n| 512 | 24.73 | 24.22 | 24.12 | 24.18 | 24.76 | 24.25 | 24.29 |\n| 768 | 41.08 | 23.45 | 23.36 | 23.42 | 24.04 | 23.43 | 23.51 |\n| 1024 | 62.71 | 23.06 | 22.93 | 22.98 | 23.63 | 23.03 | 23.09 |\n| 1280 | 83.81 | 22.83 | 22.67 | 22.73 | 23.39 | 22.77 | 22.83 |\n| 1536 | 102.28 | 22.66 | 22.45 | 22.54 | 23.21 | 22.55 | 22.63 |\n| 1792 | 121.98 | 22.60 | 22.41 | 22.47 | 23.18 | 22.50 | 22.59 |\n| 2048 | 138.17 | 22.52 | 22.28 | 22.37 | 23.08 | 22.38 | 22.48 |\n| 3072 | 194.43 | 22.33 | 22.02 | 22.14 | 22.91 | 22.15 | 22.27 |\n| 4096 | 259.55 | 22.26 | 21.97 | 22.08 | 22.96 | 22.09 | 22.21 |\n| 5120 | 289.79 | 22.20 | 21.86 | 22.00 | 22.93 | 21.99 | 22.14 |\n| 6144 | 337.46 | 22.17 | 21.87 | 21.96 | 23.06 | 21.97 | 22.11 |\n| 7168 | 376.41 | 22.16 | 21.84 | 21.95 | 23.13 | 21.96 | 22.10 |\n| 8192 | 406.95 | 22.14 | 21.82 | 21.94 | 23.20 | 21.95 | 22.08 |\n| 9216 | 423.92 | 22.12 | 21.80 | 21.90 | 23.26 | 21.90 | 22.06 |\n|  |  |  |  | Books |  |  |  |\n| Length | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2 |\n| 512 | 7.49 | 7.28 | 7.34 | 7.31 | 7.64 | 7.30 | 7.35 |\n| 768 | 10.43 | 7.15 | 7.21 | 7.18 | 7.55 | 7.18 | 7.22 |\n| 1024 | 13.32 | 7.09 | 7.15 | 7.11 | 7.49 | 7.11 | 7.15 |\n| 1280 | 15.53 | 7.06 | 7.11 | 7.08 | 7.47 | 7.08 | 7.12 |\n| 1536 | 17.47 | 7.04 | 7.08 | 7.05 | 7.44 | 7.05 | 7.09 |\n| 1792 | 19.02 | 7.03 | 7.06 | 7.03 | 7.42 | 7.03 | 7.06 |\n| 2048 | 20.55 | 7.02 | 7.05 | 7.02 | 7.41 | 7.03 | 7.05 |\n| 3072 | 24.70 | 7.00 | 7.02 | 7.00 | 7.38 | 7.00 | 7.03 |\n| 4096 | 27.57 | 6.99 | 7.00 | 6.99 | 7.37 | 6.99 | 7.03 |\n| 5120 | 29.54 | 6.99 | 7.00 | 6.99 | 7.36 | 6.99 | 7.03 |\n| 6144 | 31.59 | 6.99 | 7.00 | 6.98 | 7.35 | 6.99 | 7.02 |\n| 7168 | 32.41 | 6.98 | 7.00 | 6.98 | 7.35 | 6.99 | 7.02 |\n| 8192 | 34.35 | 6.98 | 7.00 | 6.98 | 7.35 | 6.99 | 7.02 |\n| 9216 | 34.70 | 6.98 | 7.01 | 6.98 | 7.35 | 6.99 | 7.02 |\n\nTable 4: Sufficiency validation on Wikitext-103 and Books datasets. To test the length extrapolation capability, we lengthen the inference sequence from 512 to 9216 and compute PPLs of our proposed Type 1 and Type 2 RPEs, as well as Alibi, Kerple, and Sandwich.",
    "transextra-19": "All these methods are stable in PPL. For methods that cannot extrapolate, e.g., Sinusoidal, its PPL grows rapidly. |  |  | Github |  |  |  |  |  |  |  |  |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :---: | :---: | :---: |\n| Length | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2 |  |  |  |\n| 512 | 2.29 | 2.25 | 2.24 | 2.25 | 2.29 | 2.25 | 2.25 |  |  |  |\n| 768 | 3.98 | 2.16 | 2.16 | 2.16 | 2.20 | 2.16 | 2.16 |  |  |  |\n| 1024 | 7.91 | 2.12 | 2.12 | 2.12 | 2.16 | 2.12 | 2.12 |  |  |  |\n| 1280 | 12.97 | 2.10 | 2.09 | 2.09 | 2.14 | 2.09 | 2.09 |  |  |  |\n| 1536 | 18.66 | 2.09 | 2.07 | 2.08 | 2.12 | 2.08 | 2.08 |  |  |  |\n| 1792 | 24.08 | 2.08 | 2.06 | 2.07 | 2.11 | 2.07 | 2.07 |  |  |  |\n| 2048 | 30.02 | 2.07 | 2.05 | 2.07 | 2.10 | 2.06 | 2.06 |  |  |  |\n| 3072 | 51.64 | 2.06 | 2.03 | 2.05 | 2.08 | 2.04 | 2.04 |  |  |  |\n| 4096 | 70.62 | 2.05 | 2.02 | 2.04 | 2.07 | 2.03 | 2.03 |  |  |  |\n| 5120 | 89.78 | 2.05 | 2.02 | 2.04 | 2.07 | 2.03 | 2.03 |  |  |  |\n| 6144 | 101.28 | 2.05 | 2.01 | 2.04 | 2.06 | 2.03 | 2.03 |  |  |  |\n| 7168 | 117.21 | 2.05 | 2.01 | 2.04 | 2.06 | 2.03 | 2.03 |  |  |  |\n| 8192 | 130.15 | 2.05 | 2.01 | 2.03 | 2.06 | 2.02 | 2.03 |  |  |  |\n| 9216 | 143.17 | 2.05 | 2.01 | 2.03 | 2.05 | 2.02 | 2.03 |  |  |  |\n|  |  |  |  | Wikibook |  |  |  |  |  |  |\n| Length | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2 |  |  |  |\n| 512 | 17.98 | 17.64 | 17.65 | 17.62 | 18.21 | 17.68 | 17.70 |  |  |  |\n| 768 | 29.66 | 17.04 | 17.03 | 17.03 | 17.63 | 17.07 | 17.08 |  |  |  |\n| 1024 | 47.31 | 16.76 | 16.73 | 16.73 | 17.35 | 16.76 | 16.80 |  |  |  |\n| 1280 | 65.13 | 16.62 | 16.54 | 16.56 | 17.16 | 16.58 | 16.61 |  |  |  |\n| 1536 | 83.16 | 16.51 | 16.41 | 16.44 | 17.04 | 16.44 | 16.49 |  |  |  |\n| 1792 | 100.46 | 16.49 | 16.33 | 16.41 | 16.95 | 16.39 | 16.44 |  |  |  |\n| 2048 | 116.94 | 16.43 | 16.26 | 16.36 | 16.89 | 16.31 | 16.39 |  |  |  |\n| 3072 | 172.09 | 16.39 | 16.13 | 16.31 | 16.75 | 16.23 | 16.33 |  |  |  |\n| 4096 | 231.86 | 16.39 | 16.14 | 16.29 | 16.73 | 16.24 | 16.36 |  |  |  |\n| 5120 | 277.59 | 16.35 | 16.10 | 16.26 | 16.68 | 16.21 | 16.32 |  |  |  |\n| 6144 | 312.17 | 16.35 | 16.12 | 16.25 | 16.66 | 16.22 | 16.32 |  |  |  |\n| 7168 | 349.08 | 16.34 | 16.19 | 16.24 | 16.71 | 16.26 | 16.33 |  |  |  |\n| 8192 | 390.81 | 16.35 | 16.22 | 16.25 | 16.71 | 16.30 | 16.36 |  |  |  |\n| 9216 | 412.06 | 16.33 | 16.23 | 16.24 | 16.70 | 16.28 | 16.33 |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |\n\nTable 5: Sufficiency validation on Github, WikiBook datasets. To test the length extrapolation capability, we lengthen the inference sequence from 512 to 9216 and compute PPLs of our proposed Type 1 and Type 2 RPEs, as well as Alibi, Kerple, and Sandwich.",
    "transextra-20": "All these methods are stable in PPL. For methods that cannot extrapolate, e.g., Sinusoidal, its PPL grows rapidly. |  | Wikitext-103 |  | Books |  | Github |  | Wikibook |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Length | $\\frac{1}{n}$ | 1 | $\\frac{1}{n}$ | 1 | $\\frac{1}{n}$ | 1 | $\\frac{1}{n}$ | 1 |\n| 512 | ${ }_{2}^{n} 4.67$ | ![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-14.jpg?height=48&width=100&top_left_y=1728&top_left_x=764) | ${ }^{n} 7.40$ | 7.35 | ${ }^{n} 2.28$ | ${ }^{n / 27}{ }^{n / 2} n$ | ${ }^{n} 17.91$ | $n \\ln n$ <br> 17.90 |\n| 768 | 23.87 | 23.81 | 7.28 | 7.24 | 2.19 | 2.18 | 17.28 | 17.28 |\n| 1024 | 23.53 | 23.44 | 7.27 | 7.19 | 2.17 | 2.15 | 17.21 | 17.12 |\n| 1280 | 23.50 | 23.25 | 7.41 | 7.20 | 2.19 | 2.15 | 17.70 | 17.28 |\n| 1536 | 23.66 | 23.13 | 7.65 | 7.24 | 2.31 | 2.19 | 18.86 | 17.72 |\n| 1792 | 24.20 | 23.22 | 7.97 | 7.30 | 2.52 | 2.28 | 20.74 | 18.65 |\n| 2048 | 24.80 | 23.26 | 8.34 | 7.39 | 2.85 | 2.39 | 23.31 | 19.82 |\n| 3072 | 28.31 | 23.65 | 9.97 | 7.82 | 5.05 | 3.15 | 38.46 | 27.94 |\n| 4096 | 33.18 | 24.41 | 11.84 | 8.29 | 9.13 | 4.35 | 59.51 | 40.06 |\n| 5120 | 37.65 | 25.01 | 14.15 | 8.80 | 15.79 | 5.93 | 84.60 | 55.12 |\n| 6144 | 43.20 | 25.80 | 16.64 | 9.27 | 25.00 | 8.03 | 112.30 | 71.08 |\n| 7168 | 48.07 | 26.39 | 18.99 | 9.73 | 36.01 | 10.26 | 140.34 | 89.79 |\n| 8192 | 52.85 | 27.00 | 22.06 | 10.06 | 49.20 | 12.80 | 173.55 | 108.85 |\n| 9216 | 58.46 | 27.63 | 26.34 | 10.65 | 76.78 | 16.35 | 204.10 | 133.36 |\n\nTable 6: Necessity validation on Wikitext-103, Books, Github, WikiBook datasets. We select two RPEs that do not satisfying Theorem 0.4 , e.g., $b_{n}=\\frac{1}{n}$ and $b_{n}=\\frac{1}{n \\ln n}$. We increase the inference sequence length from 512 to 9216 and compute the testing PPLs of them. Their PPLs increase rapidly as the inference sequence lengthens. ![](https://cdn.mathpix.com/cropped/2024_09_17_251ab249cc3cd696927cg-15.jpg?height=1117&width=1758&top_left_y=729&top_left_x=185)\n\nFigure 8: Visualization of RPE. We additionally plot the heatmap of $\\exp ($ RPE $)$ for all methods. Note that all methods are concentrated near the diagonal, including $\\frac{1}{n}, \\frac{1}{n \\ln n}$, which shows that the concentration is not enough to guarantee extrapolation.",
    "transextra-21": "[^0]:    ${ }^{*}$ These authors contributed equally. ${ }^{\\dagger}$ Corresponding author. Email: zhongy iran@gmail . com. Copyright (c) 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. [^1]:    ${ }^{1}$ Note that here we only show that the series corresponding to Sandwich is convergent under certain conditions.",
    "transextra-22": "The upper bound here is relatively loose, and the conditions used in practice are broader.",
    "transextra-23": ""
}