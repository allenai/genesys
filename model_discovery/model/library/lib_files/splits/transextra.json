{
    "transextra-0": "Exploring Transformer Extrapolation\n\n1Zhen Qin\\equalcontrib, 1Yiran Zhong\\equalcontrib, 2Hui Deng Corresponding author. Email: zhongyiran@gmail.com. Abstract\n\nLength extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training. Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE\u2019s exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on the Wikitext-103, Books, Github, and WikiBook datasets to demonstrate the viability of our discovered conditions. We also compare TRF to Empirical Receptive Field (ERF) across different models, showing consistently matched trends on these datasets. Introduction\n\nTransformer (Vaswani et al. 2017) is advancing steadily in the areas of natural language processing (Qin et al. 2023b; Devlin et al. 2019; Liu et al. 2019; Qin et al. 2022b, a; Liu et al. 2022; Qin and Zhong 2023), computer vision (Dosovitskiy et al. 2020; Sun et al. 2022b; Lu et al. 2022; Hao et al. 2024), and audio processing (Gong, Chung, and Glass 2021; Akbari et al.",
    "transextra-1": "2021; Gulati et al. 2020; Sun et al. 2022a). Although it outperforms other architectures such as RNNs (Cho et al. 2014; Qin, Yang, and Zhong 2023) and CNNs (Kim 2014; Hershey et al. 2016; Gehring et al. 2017) in many sequence modeling tasks, its lack of length extrapolation capability limits its ability to handle a wide range of sequence lengths, i.e., inference sequences need to be equal to or shorter than training sequences. Increasing the training sequence length is only a temporary solution because the space-time complexity grows quadratically with the sequence length. Another option is to extend the inference sequence length by converting the trained full attention blocks to sliding window attention blocks (Beltagy, Peters, and Cohan 2020), but this will result in significantly worse efficiency than the full attention speed (Press, Smith, and Lewis 2022). How to permanently resolve this issue without incurring additional costs has emerged as a new topic. A mainstream solution for length extrapolation is to design a Relative Positional Encoding (RPE) (Qin et al. 2023c) that concentrates attention on neighboring tokens. For example, ALiBi (Press, Smith, and Lewis 2022) applies linear decay biases to the attention to reduce the contribution from distant tokens. Kerple (Chi et al. 2022) investigates shift-invariant conditionally positive definite kernels in RPEs and proposes a collection of kernels that promote the length extrapolation property. It also shows that ALiBi is one of its instances. Sandwich (Chi, Fan, and Rudnicky 2022) proposes a hypothesis to explain the secret behind ALiBi and empirically proves it by integrating the hypothesis into sinusoidal positional embeddings. In order to investigate transformer extrapolation, we first establish a hypothesis regarding why existing RPE-based length extrapolation methods (Qin et al. 2023a) have this capacity to extrapolate sequences in inference based on empirical analysis. Then we identify the conditions of RPEs that satisfy the hypothesis through mathematical analysis. Finally, the discovered conditions are empirically validated on a variety of corpora. Specifically, we assume that due to decay biases, existing RPE-based length extrapolation methods behave similarly to sliding window attention, i.e., only tokens within a certain range can influence the attention scores. A transformer can extrapolate for certain in this scenario since the out-of-range tokens have no effect on the attention outcomes. We derive that a transformer is guaranteed to satisfy this hypothesis if the series corresponding to the exponential of its RPE converges.",
    "transextra-2": "Based on the observation, we show that previous RPE-based methods (Press, Smith, and Lewis 2022; Chi et al. 2022) can be seen as particular instances under the conditions. Two new practices from the conditions are derived and evaluated in language modeling. The observed conditions not only shed light on the secret of length extrapolation but also offer a new perspective on computing the Theoretical Receptive Fields (TRF) of RPEs. In contrast to prior approaches that require training gradients to compute TRF, we propose a new way to calculate TRF that is solely based on the formulation of RPEs.",
    "transextra-3": "Extensive experiments on Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020), and WikiBook (Wettig et al. 2022) validate the conditions. TRF calculated by our method substantially matches the trend of the Empirical Receptive Field (ERF) in real-world scenarios. Preliminary\n\nBefore embarking on the journey of exploring, we introduce several preliminary concepts that will be used throughout the paper, such as softmax attention, relative positional encoding, length extrapolation, and sliding window attention.",
    "transextra-4": "We also provide the necessary notations for the subsequent analysis, i.e., we use to denote a matrix and to represent the th row of . The complete math notations can be found in Appendix. Following previous work (Press, Smith, and Lewis 2022), we restrict our analysis to causal language models and assume that the max sequence length during training is . Softmax attention\n\nSoftmax attention is a key component of transformers which operates on query , key and value matrices. Each matrix is a linear map that takes as input:\n\n\ud835\udc10 = \ud835\udc17\ud835\udc16 Q , \ud835\udc0a = \ud835\udc17\ud835\udc16 K , \ud835\udc15 = \ud835\udc17\ud835\udc16 V \u2208 \u211d n \u00d7 d , formulae-sequence \ud835\udc10 subscript \ud835\udc17\ud835\udc16 \ud835\udc44 formulae-sequence \ud835\udc0a subscript \ud835\udc17\ud835\udc16 \ud835\udc3e \ud835\udc15 subscript \ud835\udc17\ud835\udc16 \ud835\udc49 superscript \u211d \ud835\udc5b \ud835\udc51 \\mathbf{Q}=\\mathbf{X}\\mathbf{W}_{Q},\\ \\mathbf{K}=\\mathbf{X}\\mathbf{W}_{K},\\ \\mathbf{V}=\\mathbf{X}\\mathbf{W}_{V}\\in\\mathbb{R}^{n\\times d}, (1)\n\nwhere is the sequence length and is the dimension of the hidden feature. The output attention matrix can be formulated as:\n\n\ud835\udc0e = Softmax \u200b ( \ud835\udc10\ud835\udc0a \ud835\uddb3 / d ) \u200b \ud835\udc15 . \ud835\udc0e Softmax superscript \ud835\udc10\ud835\udc0a \ud835\uddb3 \ud835\udc51 \ud835\udc15 \\mathbf{O}=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\mkern-1.5mu\\mathsf{T}}/\\sqrt{d})\\mathbf{V}. (2)\n\nTo prevent information leakage in causal language modeling, a mask matrix is used to ensure that current tokens can only see previous tokens and themselves. The lower triangular elements of are , and the upper triangular ones, except for the diagonal, are . Then the output attention matrix for causal language models will be:\n\n\ud835\udc0e = Softmax \u200b ( \ud835\udc10\ud835\udc0a \u22a4 / d + \ud835\udc0c ) \u200b \ud835\udc15 . \ud835\udc0e Softmax superscript \ud835\udc10\ud835\udc0a top \ud835\udc51 \ud835\udc0c \ud835\udc15 \\mathbf{O}=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\top}/\\sqrt{d}+\\mathbf{M})\\mathbf{V}. (3)\n\nNote that Eq. 3 can be seen as a general form of attention, i.e., when the elements of are all 0, Eq. 3 is degenerated to Eq.",
    "transextra-5": "2. For ease of discussion, we use Eq. 3 to represent attention computation. Relative positional encoding\n\nPositional encoding is designed to inject positional bias into transformers. Absolute Positional Encoding (APE) (Vaswani et al. 2017; Gehring et al. 2017) and Relative Positional Encoding (RPE) (Su et al. 2021; Liutkus et al. 2021; Press, Smith, and Lewis 2022; Chi et al. 2022) are the two most common types of positional encoding. In this paper, we focus on RPE because it is the key for length extrapolation, as shown in (Press, Smith, and Lewis 2022). An attention with RPE can be written as:\n\n\ud835\udc0e = Softmax \u200b ( \ud835\udc10\ud835\udc0a \u22a4 / d + \ud835\udc0c + \ud835\udc0f ) \u200b \ud835\udc15 , \ud835\udc0e Softmax superscript \ud835\udc10\ud835\udc0a top \ud835\udc51 \ud835\udc0c \ud835\udc0f \ud835\udc15 \\mathbf{O}=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\top}/\\sqrt{d}+\\mathbf{M}+\\mathbf{P})\\mathbf{V}, (4)\n\nwhere is a Toeplitz matrix that encodes relative positional information, i.e., . It is worth noting that and can be merged, and the merged matrix is still a Toeplitz matrix. We use to represent the merged matrix and rewrite Eq. 4 as:\n\n\ud835\udc0e = Softmax \u200b ( \ud835\udc10\ud835\udc0a \u22a4 / d + \ud835\udc11 ) \u200b \ud835\udc15 . \ud835\udc0e Softmax superscript \ud835\udc10\ud835\udc0a top \ud835\udc51 \ud835\udc11 \ud835\udc15 \\mathbf{O}=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\top}/\\sqrt{d}+\\mathbf{R})\\mathbf{V}. (5)\n\nDefinition of length extrapolation\n\nThe property of length extrapolation allows a model to be tested on longer sequences than those used in training. Previous sequence modeling structures such as RNNs (Hochreiter and Schmidhuber 1997) and CNNs (Gehring et al. 2017) often naturally possess this property, but it is a difficult task for transformers. This property is only present in sliding window transformers and a few transformer variants with specifically designed RPEs (Chi et al. 2022; Press, Smith, and Lewis 2022; Chi, Fan, and Rudnicky 2022). In language modeling, one token can only see itself and previous tokens. Therefore, regardless the sequence length, the performance should be stable for the neighboring tokens that are within the training sequence length (Beltagy, Peters, and Cohan 2020). For the tokens that are out of range, the performance will degrade if the model does not support length extrapolation (Press, Smith, and Lewis 2022). Based on the observation above, we give a definition of length extrapolation:\n\nDefinition 0.1. For a language model , given dataset , if for any , there is,\n\n| p \u200b p \u200b l n \u200b ( \ud835\udcb3 , \u2131 ) \u2212 p \u200b p \u200b l m \u200b ( \ud835\udcb3 , \u2131 ) | / p \u200b p \u200b l m \u200b ( \ud835\udcb3 , \u2131 ) < \u03b4 , \ud835\udc5d \ud835\udc5d subscript \ud835\udc59 \ud835\udc5b \ud835\udcb3 \u2131 \ud835\udc5d \ud835\udc5d subscript \ud835\udc59 \ud835\udc5a \ud835\udcb3 \u2131 \ud835\udc5d \ud835\udc5d subscript \ud835\udc59 \ud835\udc5a \ud835\udcb3 \u2131 \ud835\udeff |ppl_{n}(\\mathcal{X},\\mathcal{F})-ppl_{m}(\\mathcal{X},\\mathcal{F})|/ppl_{m}(\\mathcal{X},\\mathcal{F})<\\delta, (6)\n\nthen is considered to have the extrapolation property.",
    "transextra-6": "Here is a small constant, means that calculates perplexity with a max sequence length of on the data set . Empirically, if becomes very large() as increases, we consider that does not have extrapolation property. Sliding window attention\n\nFor the convenience of subsequent discussions, we define a window attention at position and window size as follows:\n\n\ud835\udc28 i j superscript subscript \ud835\udc28 \ud835\udc56 \ud835\udc57 \\displaystyle\\mathbf{o}_{i}^{j} = \u2211 i \u2212 j + 1 \u2264 s \u2264 i exp \u2061 ( \ud835\udc2a i \u22a4 \u200b \ud835\udc24 s / d ) \u200b exp \u2061 ( r i \u200b s ) \u200b \ud835\udc2f s \u2211 i \u2212 j + 1 \u2264 t \u2264 i exp \u2061 ( \ud835\udc2a i \u22a4 \u200b \ud835\udc24 t / d ) \u200b exp \u2061 ( r i \u200b t ) absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc60 \ud835\udc51 subscript \ud835\udc5f \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc61 \ud835\udc51 subscript \ud835\udc5f \ud835\udc56 \ud835\udc61 \\displaystyle=\\frac{\\sum_{i-j+1\\leq s\\leq i}\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{s}/\\sqrt{d})\\exp(r_{is})\\mathbf{v}_{s}}{\\sum_{i-j+1\\leq t\\leq i}\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{t}/\\sqrt{d})\\exp(r_{it})} (7) \u225c \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \ud835\udc2f s C i \u200b j , \u225c absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 \\displaystyle\\triangleq\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\mathbf{v}_{s}}{C_{ij}},\n\nwhere\n\nWe further assume , where is a constant. The represents the attention output of the -th token, which interacts with the tokens preceding it. Note that window attention naturally possesses the length extrapolation ability. There are two ways to infer window attention: nonoverlapping inference and sliding window inference as shown on the right of Figure 1. In sliding window inference, the tokens within each sliding window must be re-encoded multiple times, making it substantially slower than the nonoverlapping one. In table 1 we compare the average inference time over a group of window sizes between the sliding window inference and nonoverlapping window inference. The sliding window one is more than 44 times slower than the nonoverlapping one. However, as shown on the left of Figure 1, the sliding window inference has much lower ppl than the nonoverlapping one. Transformer Extrapolation Exploration\n\nIn this section, we first describe the hypothesis about why existing RPE-based length extrapolation methods can extrapolate sequences in inference and provide empirical evidence for it. Then we derive the conditions for length extrapolation in detail and demonstrate that recent RPE-based length extrapolation methods (Chi et al. 2022; Press, Smith, and Lewis 2022) satisfy the conditions. The hypothesis\n\nA sliding window attention with window size is equivalent to the following RPE on full attention:\n\nm i \u200b j = { 0 , i \u2212 j \u2264 w . \u2212 \u221e , others . subscript m \ud835\udc56 \ud835\udc57 cases 0 \ud835\udc56 \ud835\udc57 \ud835\udc64 others \\mathrm{m}_{ij}=\\begin{cases}0,&i-j\\leq w.\\\\\n-\\infty,&\\mathrm{others}.\\end{cases} (8)\n\nBy comparing Eq. 8 and the corresponding RPE of Alibi (Press, Smith, and Lewis 2022) in Figure 2, we can see that they both have the same behavior in that they both concentrate tokens inside a specified range. Also, in Figure 1, we show that the performance of Alibi is similar to the sliding window attention when the window size is sufficiently large. Based on these two observations, we make the following hypothesis:\n\nHypothesis 0.1. A RPE that makes a transformer extrapolatable needs to have similar behavior to sliding window attention, i.e., should satisfy:\n\n\u2200 \u03f5 > 0 , \u2203 j 0 , s . t , j > j 0 , \u03b4 \u200b ( i , j ) < \u03f5 , formulae-sequence for-all italic-\u03f5 0 subscript \ud835\udc57 0 \ud835\udc60 \ud835\udc61 \ud835\udc57 subscript \ud835\udc57 0 \ud835\udeff \ud835\udc56 \ud835\udc57 italic-\u03f5 \\forall\\epsilon>0,\\exists j_{0},s.t,j>j_{0},\\delta(i,j)<\\epsilon, (9)\n\nwhere , and the window length needs to be sufficiently large. In the following sections, we will derive the conditions for RPEs that satisfy Eq. 9. The conditions\n\nLet us introduce the first lemma:\n\nLemma 0.2. When the following condition is satisfied, Eq. 9 holds. lim i \u2192 \u221e C i \u200b i \u225c C < \u221e . \u225c subscript \u2192 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \ud835\udc36 {\\lim}_{i\\to\\infty}C_{ii}\\triangleq C<\\infty. (10)\n\nProof. When , the test sequence length is smaller than the max sequence length during training, take , we get When , we can reformulate Eq. 7 as:\n\n\ud835\udc28 i i superscript subscript \ud835\udc28 \ud835\udc56 \ud835\udc56 \\displaystyle\\mathbf{o}_{i}^{i} = \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \ud835\udc2f s + \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b \ud835\udc2f s C i \u200b i absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \\displaystyle=\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\mathbf{v}_{s}+\\sum_{1\\leq s\\leq i-j}c_{is}\\mathbf{v}_{s}}{C_{ii}} = \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \ud835\udc2f s C i \u200b j \u200b C i \u200b j C i \u200b i + \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b \ud835\udc2f s C i \u200b i \u2212 C i \u200b j \u200b C i \u200b i \u2212 C i \u200b j C i \u200b i absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \\displaystyle=\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\mathbf{v}_{s}}{C_{ij}}\\frac{{C_{ij}}}{{C_{ii}}}+\\frac{\\sum_{1\\leq s\\leq i-j}c_{is}\\mathbf{v}_{s}}{C_{ii}-C_{ij}}\\frac{C_{ii}-C_{ij}}{C_{ii}} = \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \ud835\udc2f s C i \u200b j \u200b C i \u200b j C i \u200b i + \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b \ud835\udc2f s C i \u200b i \u2212 C i \u200b j \u200b ( 1 \u2212 C i \u200b j C i \u200b i ) . absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 1 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \\displaystyle=\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\mathbf{v}_{s}}{C_{ij}}\\frac{{C_{ij}}}{{C_{ii}}}+\\frac{\\sum_{1\\leq s\\leq i-j}c_{is}\\mathbf{v}_{s}}{C_{ii}-C_{ij}}\\left(1-\\frac{{C_{ij}}}{{C_{ii}}}\\right). Therefore we have:\n\n\ud835\udc28 i i \u2212 \ud835\udc28 i j superscript subscript \ud835\udc28 \ud835\udc56 \ud835\udc56 superscript subscript \ud835\udc28 \ud835\udc56 \ud835\udc57 \\displaystyle\\mathbf{o}_{i}^{i}-\\mathbf{o}_{i}^{j} = ( 1 \u2212 C i \u200b j C i \u200b i ) \u200b ( \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \ud835\udc2f s C i \u200b j \u2212 \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b \ud835\udc2f s C i \u200b i \u2212 C i \u200b j ) . absent 1 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 \\displaystyle=\\left(1-\\frac{{C_{ij}}}{{C_{ii}}}\\right)\\left(\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\mathbf{v}_{s}}{C_{ij}}-\\frac{\\sum_{1\\leq s\\leq i-j}c_{is}\\mathbf{v}_{s}}{C_{ii}-C_{ij}}\\right). (11)\n\nFor the second part:\n\n\u2016 \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \ud835\udc2f s C i \u200b j \u2212 \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b \ud835\udc2f s C i \u200b i \u2212 C i \u200b j \u2016 norm subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 \\displaystyle\\left\\|\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\mathbf{v}_{s}}{C_{ij}}-\\frac{\\sum_{1\\leq s\\leq i-j}c_{is}\\mathbf{v}_{s}}{C_{ii}-C_{ij}}\\right\\| (12) \u2264 \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b \u2016 \ud835\udc2f s \u2016 C i \u200b j + \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b \u2016 \ud835\udc2f s \u2016 C i \u200b i \u2212 C i \u200b j absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 norm subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 norm subscript \ud835\udc2f \ud835\udc60 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 \\displaystyle\\leq\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}\\|\\mathbf{v}_{s}\\|}{C_{ij}}+\\frac{\\sum_{1\\leq s\\leq i-j}c_{is}\\|\\mathbf{v}_{s}\\|}{C_{ii}-C_{ij}} \u2264 \u2211 i \u2212 j + 1 \u2264 s \u2264 i c i \u200b s \u200b l C i \u200b j + \u2211 1 \u2264 s \u2264 i \u2212 j c i \u200b s \u200b l C i \u200b i \u2212 C i \u200b j = 2 \u200b l absent subscript \ud835\udc56 \ud835\udc57 1 \ud835\udc60 \ud835\udc56 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 \ud835\udc59 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript 1 \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc50 \ud835\udc56 \ud835\udc60 \ud835\udc59 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 2 \ud835\udc59 \\displaystyle\\leq\\frac{\\sum_{i-j+1\\leq s\\leq i}c_{is}l}{C_{ij}}+\\frac{\\sum_{1\\leq s\\leq i-j}c_{is}l}{C_{ii}-C_{ij}}=2l\n\nWe have\n\n\u03b4 \u200b ( i , j ) \u2264 2 \u200b ( 1 \u2212 C i \u200b j C i \u200b i ) \u200b l . \ud835\udeff \ud835\udc56 \ud835\udc57 2 1 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \ud835\udc59 \\delta(i,j)\\leq 2\\left(1-\\frac{{C_{ij}}}{{C_{ii}}}\\right)l. (13)\n\nAccording to Eq 10 and the tail of convergence series is arbitrarily small.",
    "transextra-7": ", we can find a , s.t. if , . We can also find a , s.t. if , . If we take , so if , we have:\n\nC i \u200b i \u2212 C i \u200b j < \u03f5 , C \u2212 \u03f5 < C i \u200b i < C + \u03f5 formulae-sequence subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 italic-\u03f5 \ud835\udc36 italic-\u03f5 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \ud835\udc36 italic-\u03f5 C_{ii}-C_{ij}<\\epsilon,C-\\epsilon<C_{ii}<C+\\epsilon (14)\n\nSo when , we have\n\n\u03b4 \u200b ( i , j ) \u2264 2 \u200b ( 1 \u2212 C i \u200b j C i \u200b i ) \u200b l \ud835\udeff \ud835\udc56 \ud835\udc57 2 1 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \ud835\udc59 \\displaystyle\\delta(i,j)\\leq 2\\left(1-\\frac{{C_{ij}}}{{C_{ii}}}\\right)l = 2 \u200b C i \u200b i \u2212 C i \u200b j C i \u200b i \u200b l \u2264 2 \u200b \u03f5 C \u2212 \u03f5 \u200b l absent 2 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 \ud835\udc59 2 italic-\u03f5 \ud835\udc36 italic-\u03f5 \ud835\udc59 \\displaystyle=2\\frac{C_{ii}-C_{ij}}{C_{ii}}l\\leq 2\\frac{\\epsilon}{C-\\epsilon}l (15) \u2264 2 \u200b l \u200b \u03f5 C \u2212 C / 2 = 4 \u200b l \u200b \u03f5 C absent 2 \ud835\udc59 italic-\u03f5 \ud835\udc36 \ud835\udc36 2 4 \ud835\udc59 italic-\u03f5 \ud835\udc36 \\displaystyle\\leq\\frac{2l\\epsilon}{C-C/2}=\\frac{4l\\epsilon}{C}\n\nAccording to the definition of limitation, Eq. 10 holds. \u220e\n\nThis lemma implies that for any token if the attention of the model focuses on its neighboring tokens, the model has length extrapolation property. The lemma accompanies our intuitions. Does it mean that as long as a RPE follows the same principle, i.e., places more weights on neighboring tokens, the model is guaranteed to have the length extrapolation property? In the following sections, we will demonstrate that concentrating more weights on neighboring tokens does not guarantee the transformer has the length extrapolation property. Specifically, we will provide a mathematical proof of the sufficient conditions for RPE to have the length extrapolation property.",
    "transextra-8": "Theorem 0.3. When the following condition is satisfied, Eq. 9 holds. lim i \u2192 \u221e B i \u200b i < \u221e , B i \u200b i = \u2211 1 \u2264 t \u2264 i b i \u200b t < \u221e . formulae-sequence subscript \u2192 \ud835\udc56 subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 subscript 1 \ud835\udc61 \ud835\udc56 subscript \ud835\udc4f \ud835\udc56 \ud835\udc61 {\\lim}_{i\\to\\infty}B_{ii}<\\infty,B_{ii}=\\sum_{1\\leq t\\leq i}b_{it}<\\infty. (16)\n\nProof. Since we assume , then:\n\na i \u200b j = exp \u2061 ( \ud835\udc2a i \u22a4 \u200b \ud835\udc24 j ) \u2264 exp \u2061 ( l 2 ) , subscript \ud835\udc4e \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc57 superscript \ud835\udc59 2 \\displaystyle a_{ij}=\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j})\\leq\\exp(l^{2}), (17)\n\nc i \u200b j = a i \u200b j \u200b b i \u200b j \u2264 exp \u2061 ( l 2 ) \u200b b i \u200b j , C i \u200b i \u2264 exp \u2061 ( l 2 ) \u200b B i \u200b i . formulae-sequence subscript \ud835\udc50 \ud835\udc56 \ud835\udc57 subscript \ud835\udc4e \ud835\udc56 \ud835\udc57 subscript \ud835\udc4f \ud835\udc56 \ud835\udc57 superscript \ud835\udc59 2 subscript \ud835\udc4f \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 superscript \ud835\udc59 2 subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 \\displaystyle c_{ij}=a_{ij}b_{ij}\\leq\\exp(l^{2})b_{ij},C_{ii}\\leq\\exp(l^{2})B_{ii}.",
    "transextra-9": "(18)\n\nTherefore, Eq. 10 can be derived from Eq. 16. Combine with Lemma 0.2, the proof is concluded. \u220e\n\nBy leveraging the property of RPE, Theorem 0.3 can be further simplified as:\n\nTheorem 0.4.",
    "transextra-10": "When the following condition is satisfied, Eq. 9 holds. lim i \u2192 \u221e \u2211 t = 1 i b i \u2212 t = lim i \u2192 \u221e \u2211 t = 0 i \u2212 1 b t < \u221e . subscript \u2192 \ud835\udc56 superscript subscript \ud835\udc61 1 \ud835\udc56 subscript \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \u2192 \ud835\udc56 superscript subscript \ud835\udc61 0 \ud835\udc56 1 subscript \ud835\udc4f \ud835\udc61 \\lim_{i\\to\\infty}\\sum_{t=1}^{i}b_{i-t}=\\lim_{i\\to\\infty}\\sum_{t=0}^{i-1}b_{t}<\\infty. (19)\n\nProof. According to the definition of RPE:\n\nB i \u200b i = \u2211 1 \u2264 t \u2264 i b i \u200b t = \u2211 t = 1 i b i \u2212 t = \u2211 t = 0 i \u2212 1 b t . subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 subscript 1 \ud835\udc61 \ud835\udc56 subscript \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript subscript \ud835\udc61 1 \ud835\udc56 subscript \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript subscript \ud835\udc61 0 \ud835\udc56 1 subscript \ud835\udc4f \ud835\udc61 B_{ii}=\\sum_{1\\leq t\\leq i}b_{it}=\\sum_{t=1}^{i}b_{i-t}=\\sum_{t=0}^{i-1}b_{t}. (20)\n\nThis means that Eq. 16 is equivalent to:\n\nlim i \u2192 \u221e B i \u200b i = lim i \u2192 \u221e \u2211 t = 0 i \u2212 1 b t < \u221e . \u220e subscript \u2192 \ud835\udc56 subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 subscript \u2192 \ud835\udc56 superscript subscript \ud835\udc61 0 \ud835\udc56 1 subscript \ud835\udc4f \ud835\udc61 {\\lim_{i\\to\\infty}}B_{ii}={\\lim_{i\\to\\infty}}\\sum_{t=0}^{i-1}b_{t}<\\infty.\\qed (21)\n\nTheorem 0.4 indicates that as long as the series of converges, the model is guaranteed to have length extrapolation property. Based on this principle, we can mathematically determine whether an RPE allows for length extrapolation before conducting experiments or designing a variety of RPEs that can do length extrapolation. In Appendix, we show that previous methods such as Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), and Sandwich (Chi, Fan, and Rudnicky 2022) satisfy our derived conditions for length extrapolation. Theoretical receptive field\n\nIn the previous section, we established the conditions for length extrapolation. As an extra bonus, we can derive Theoretical Receptive Fields (TRF) for any RPE-based length extrapolation method. Let us start with the definition of Empirical Receptive Field (ERF). ERF can be viewed as a window containing the vast majority of the information contained within the attention. Recall Eq. 13, by setting , we can define:\n\nC i \u200b j = C i \u200b i \u200b ( 1 \u2212 \u03f5 ) , n emp \u200b ( \u03f5 ) = inf j ( C i \u200b j > C i \u200b i \u200b ( 1 \u2212 \u03f5 ) ) , formulae-sequence subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 1 italic-\u03f5 subscript \ud835\udc5b emp italic-\u03f5 subscript infimum \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc57 subscript \ud835\udc36 \ud835\udc56 \ud835\udc56 1 italic-\u03f5 C_{ij}=C_{ii}(1-\\epsilon),\\ n_{\\text{emp}}(\\epsilon)=\\inf_{j}(C_{ij}>C_{ii}(1-\\epsilon)),\n\nis the ERF that represents the minimal sequence length required to maintain the performance within a gap of . Intuitively, ERF can be viewed as the smallest window that contains the majority of the information within an attention. Since it is related to both and , it can only be calculated after training. Now we define TRF, which allows us to estimate the receptive field without training. To accomplish this, we consider the upper bound of .",
    "transextra-11": "From the definition of and Eq. 17, is upper bounded by . Therefore, we can define the TRF respect to series as:\n\nn the \u200b ( \u03f5 ) subscript \ud835\udc5b the italic-\u03f5 \\displaystyle n_{\\mathrm{the}}(\\epsilon) = inf j ( B i \u200b j > B \u200b ( 1 \u2212 \u03f5 ) ) absent subscript infimum \ud835\udc57 subscript \ud835\udc35 \ud835\udc56 \ud835\udc57 \ud835\udc35 1 italic-\u03f5 \\displaystyle=\\inf_{j}(B_{ij}>B(1-\\epsilon)) (22) = inf j ( \u2211 t = 0 j \u2212 1 b t > B \u200b ( 1 \u2212 \u03f5 ) ) absent subscript infimum \ud835\udc57 superscript subscript \ud835\udc61 0 \ud835\udc57 1 subscript \ud835\udc4f \ud835\udc61 \ud835\udc35 1 italic-\u03f5 \\displaystyle=\\inf_{j}\\left(\\sum_{t=0}^{j-1}b_{t}>B(1-\\epsilon)\\right) = inf j ( \u2211 t \u2265 j b t < B \u200b \u03f5 ) absent subscript infimum \ud835\udc57 subscript \ud835\udc61 \ud835\udc57 subscript \ud835\udc4f \ud835\udc61 \ud835\udc35 italic-\u03f5 \\displaystyle=\\inf_{j}\\left(\\sum_{t\\geq j}b_{t}<B\\epsilon\\right)\n\nwhere . We may find it difficult to give the analytical form of the partial sum of the series at times, but we can still compute the TRF numerically or compare the TRFs of different RPEs using the theorem below:\n\nTheorem 0.5.",
    "transextra-12": "If the following conditions hold:\n\n\u03b1 t \u03b1 subscript \ud835\udefc \ud835\udc61 \ud835\udefc \\displaystyle\\frac{\\alpha_{t}}{\\alpha} \u2264 \u03b2 t \u03b2 , t \u2192 \u221e , \u03b1 \u225c lim j \u2192 \u221e \u2211 t = 0 j \u2212 1 \u03b1 t , \u03b2 \u225c lim j \u2192 \u221e \u2211 t = 0 j \u2212 1 \u03b2 t . formulae-sequence absent subscript \ud835\udefd \ud835\udc61 \ud835\udefd formulae-sequence \u2192 \ud835\udc61 formulae-sequence \u225c \ud835\udefc subscript \u2192 \ud835\udc57 superscript subscript \ud835\udc61 0 \ud835\udc57 1 subscript \ud835\udefc \ud835\udc61 \u225c \ud835\udefd subscript \u2192 \ud835\udc57 superscript subscript \ud835\udc61 0 \ud835\udc57 1 subscript \ud835\udefd \ud835\udc61 \\displaystyle\\leq\\frac{\\beta_{t}}{\\beta},t\\to\\infty,\\ \\alpha\\triangleq\\lim_{j\\to\\infty}\\sum_{t=0}^{j-1}\\alpha_{t},\\ \\beta\\triangleq\\lim_{j\\to\\infty}\\sum_{t=0}^{j-1}\\beta_{t}. (23)\n\nThen:\n\nn the \u03b1 \u200b ( \u03f5 ) \u2264 n the \u03b2 \u200b ( \u03f5 ) , \u03f5 \u2192 0 . formulae-sequence superscript subscript \ud835\udc5b the \ud835\udefc italic-\u03f5 superscript subscript \ud835\udc5b the \ud835\udefd italic-\u03f5 \u2192 italic-\u03f5 0 n_{\\mathrm{the}}^{\\alpha}(\\epsilon)\\leq n_{\\mathrm{the}}^{\\beta}(\\epsilon),\\epsilon\\to 0. (24)\n\nProof. According to Eq.23, there exists , such that, when , we have:\n\n\u03b1 t \u03b1 \u2264 \u03b2 t \u03b2 . subscript \ud835\udefc \ud835\udc61 \ud835\udefc subscript \ud835\udefd \ud835\udc61 \ud835\udefd \\frac{\\alpha_{t}}{\\alpha}\\leq\\frac{\\beta_{t}}{\\beta}. (25)\n\nLet , where\n\nn the \u03b2 \u200b ( \u03f5 0 ) = t 0 , superscript subscript \ud835\udc5b the \ud835\udefd subscript italic-\u03f5 0 subscript \ud835\udc61 0 n_{\\mathrm{the}}^{\\beta}(\\epsilon_{0})=t_{0}, (26)\n\nthen we get:\n\n\u2211 t \u2265 n the \u03b2 \u200b ( \u03f5 ) \u03b2 t \u2264 \u03b2 \u200b \u03f5 , n the \u03b2 \u200b ( \u03f5 ) > t 0 . formulae-sequence subscript \ud835\udc61 superscript subscript \ud835\udc5b the \ud835\udefd italic-\u03f5 subscript \ud835\udefd \ud835\udc61 \ud835\udefd italic-\u03f5 superscript subscript \ud835\udc5b the \ud835\udefd italic-\u03f5 subscript \ud835\udc61 0 \\sum_{t\\geq n_{\\mathrm{the}}^{\\beta}(\\epsilon)}\\beta_{t}\\leq\\beta\\epsilon,n_{\\mathrm{the}}^{\\beta}(\\epsilon)>t_{0}. (27)\n\nFinally:\n\n\u2211 t \u2265 n the \u03b2 \u200b ( \u03f5 ) \u03b1 t \u2264 \u2211 t \u2265 n the \u03b2 \u200b ( \u03f5 ) \u03b1 \u200b \u03b2 t \u03b2 \u2264 \u03b1 \u200b \u03b2 \u200b \u03f5 \u03b2 = \u03b1 \u200b \u03f5 . subscript \ud835\udc61 superscript subscript \ud835\udc5b the \ud835\udefd italic-\u03f5 subscript \ud835\udefc \ud835\udc61 subscript \ud835\udc61 superscript subscript \ud835\udc5b the \ud835\udefd italic-\u03f5 \ud835\udefc subscript \ud835\udefd \ud835\udc61 \ud835\udefd \ud835\udefc \ud835\udefd italic-\u03f5 \ud835\udefd \ud835\udefc italic-\u03f5 \\displaystyle\\sum_{t\\geq n_{\\mathrm{the}}^{\\beta}(\\epsilon)}\\alpha_{t}\\leq\\sum_{t\\geq n_{\\mathrm{the}}^{\\beta}(\\epsilon)}\\frac{\\alpha\\beta_{t}}{\\beta}\\leq\\frac{\\alpha\\beta\\epsilon}{\\beta}=\\alpha\\epsilon. According to Eq. 22, we have:\n\nn the a \u200b ( \u03f5 ) \u2264 n the b \u200b ( \u03f5 ) . superscript subscript \ud835\udc5b the \ud835\udc4e italic-\u03f5 superscript subscript \ud835\udc5b the \ud835\udc4f italic-\u03f5 n_{\\mathrm{the}}^{a}(\\epsilon)\\leq n_{\\mathrm{the}}^{b}(\\epsilon). (28)\n\nThe series follows the same trend as TRF, the smaller the series, the smaller the TRF. \u220e\n\nWe provide several examples of how to compute TRF in the Appendix. Two new RPEs\n\nBased on the proven conditions of length extrapolation, we can design infinite kinds of RPEs with the length extrapolation property. Here, we propose two new RPEs to empirically prove the conditions and hypothesis, namely:\n\nType1 : b n : Type1 subscript \ud835\udc4f \ud835\udc5b \\displaystyle\\mathrm{Type1}:b_{n} = 1 n 2 = exp \u2061 ( \u2212 2 \u200b ln \u2061 n ) , absent 1 superscript \ud835\udc5b 2 2 \ud835\udc5b \\displaystyle=\\frac{1}{n^{2}}=\\exp(-2\\ln n), Type2 : b n : Type2 subscript \ud835\udc4f \ud835\udc5b \\displaystyle\\mathrm{Type2}:b_{n} = exp \u2061 ( \u2212 ln 2 \u2061 n ) ; absent superscript 2 \ud835\udc5b \\displaystyle=\\exp(-\\ln^{2}n);\n\nThe corresponding TRF of Type 1 is:\n\nB i \u200b j subscript \ud835\udc35 \ud835\udc56 \ud835\udc57 \\displaystyle B_{ij} = \u2211 i = 0 j \u2212 1 1 ( i + 1 ) 2 \u2248 \u222b 1 j 1 x 2 \u200b \ud835\udc51 x = 1 \u2212 1 j , B = 1 . formulae-sequence absent superscript subscript \ud835\udc56 0 \ud835\udc57 1 1 superscript \ud835\udc56 1 2 superscript subscript 1 \ud835\udc57 1 superscript \ud835\udc65 2 differential-d \ud835\udc65 1 1 \ud835\udc57 \ud835\udc35 1 \\displaystyle=\\sum_{i=0}^{j-1}\\frac{1}{(i+1)^{2}}\\approx\\int_{1}^{j}\\frac{1}{x^{2}}dx=1-\\frac{1}{j},B=1. (29) n the \u200b ( \u03f5 ) subscript \ud835\udc5b the italic-\u03f5 \\displaystyle n_{\\mathrm{the}}(\\epsilon) = inf j ( B i \u200b j > B \u200b ( 1 \u2212 \u03f5 ) ) absent subscript infimum \ud835\udc57 subscript \ud835\udc35 \ud835\udc56 \ud835\udc57 \ud835\udc35 1 italic-\u03f5 \\displaystyle=\\inf_{j}\\left(B_{ij}>B(1-\\epsilon)\\right) = inf j ( 1 \u2212 1 j > 1 \u2212 \u03f5 ) = \u0398 \u200b ( 1 \u03f5 ) absent subscript infimum \ud835\udc57 1 1 \ud835\udc57 1 italic-\u03f5 \u0398 1 italic-\u03f5 \\displaystyle=\\inf_{j}\\left(1-\\frac{1}{j}>1-\\epsilon\\right)=\\Theta\\left(\\frac{1}{\\epsilon}\\right)\n\nFor Type 2, it is difficult to provide the analytical form of its TRF. However, we can prove that the TRF of Type 2 is smaller than the TRF of Type 1 using Theorem 0.5 and the inequality below:\n\n\u2200 c 1 , c 2 > 0 , exp \u2061 ( \u2212 ln 2 \u2061 n ) c 1 < 1 / n 2 c 2 , n \u2192 \u221e .",
    "transextra-13": "formulae-sequence for-all subscript \ud835\udc50 1 subscript \ud835\udc50 2 0 formulae-sequence superscript 2 \ud835\udc5b subscript \ud835\udc50 1 1 superscript \ud835\udc5b 2 subscript \ud835\udc50 2 \u2192 \ud835\udc5b \\forall c_{1},c_{2}>0,\\frac{\\exp(-\\ln^{2}n)}{c_{1}}<\\frac{1/n^{2}}{c_{2}},n\\to\\infty. Empirical Validation\n\nSetting\n\nAll models are implemented in Fairseq (Ott et al. 2019) and trained on 8 V100 GPUs. We use the same model architecture and training configuration for all RPE variants to ensure fairness. For Wikitext-103 (Merity et al. 2016), since it is a relatively small dataset, we use a 6-layer transformer decoder structure with an embedding size of 512. For other datasets, in particular, we used a 12-layer transformer decoder structure with an embedding size of 768. The evaluation metric is perplexity (PPL) and the max training length during training is 512. The detailed hyper-parameter settings are listed in Appendix. Dataset\n\nWe conduct experiments on Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020) and WikiBook (Wettig et al. 2022). Wikitext-103 is a small dataset containing a preprocessed version of the Wikipedia dataset. It is widely used in many NLP papers. Books has a large number of novels, making it a good corpus for long sequence processing. Github consists of a sizable amount of open-source repositories, the majority of which are written in coding languages. WikiBook is a 22-gigabyte corpus of Wikipedia articles and books curated by (Wettig et al. 2022). This corpus is used to validate the performance of various models on large datasets. Validating the sufficiency. To empirically validate the sufficiency of our discovered conditions, we integrate the two RPEs that were proposed in the previous section into transformers and test their length extrapolation capability on Wikitext-103, Books, Github, and WikiBook datasets. We increase the length of the inference sequence from 512 to 9216 tokens and plot the testing PPLs of our proposed RPEs as well as those of existing methods such as Alibi, Kerple, and Sandwich in Figure 3. Detailed numerical results can be found in Table 4 and Table 5 from Appendix. All these methods demonstrate good length extrapolation capability. However, the stabilized PPL may vary due to the effectiveness of different positional encoding strategies, which are not considered in this paper. We include the Sinusoidal (Vaswani et al. 2017) positional encoding as a reference method that cannot extrapolate, which grows rapidly as the inference sequence length increases. Validating the necessity. Although we only provide mathematical proof for the sufficiency of our discovered conditions, we also attempt to verify their necessity empirically in this section. Specifically, we pick two RPEs that are very close to satisfying Theorem 0.4 as follows. Note that both of them concentrate their weight on neighboring tokens. Example1 : b n = 1 n , Example2 : b n = 1 n \u200b ln \u2061 n : Example1 subscript \ud835\udc4f \ud835\udc5b 1 \ud835\udc5b Example2 : subscript \ud835\udc4f \ud835\udc5b 1 \ud835\udc5b \ud835\udc5b \\mathrm{Example1}:b_{n}=\\frac{1}{n},\\ \\mathrm{Example2}:b_{n}=\\frac{1}{n\\ln n}\n\nBelow is a brief mathematical proof that the above RPEs do not satisfy Theorem 0.4.",
    "transextra-14": "\u2211 n = 1 k 1 n superscript subscript \ud835\udc5b 1 \ud835\udc58 1 \ud835\udc5b \\displaystyle\\sum_{n=1}^{k}\\frac{1}{n} > \u222b 1 k + 1 1 x \u200b \ud835\udc51 x = ln \u2061 ( k + 1 ) , absent superscript subscript 1 \ud835\udc58 1 1 \ud835\udc65 differential-d \ud835\udc65 \ud835\udc58 1 \\displaystyle>\\int_{1}^{k+1}\\frac{1}{x}dx=\\ln(k+1), \u2211 n = 3 k 1 n \u200b ln \u2061 n superscript subscript \ud835\udc5b 3 \ud835\udc58 1 \ud835\udc5b \ud835\udc5b \\displaystyle\\sum_{n=3}^{k}\\frac{1}{n\\ln n} > \u222b 3 k + 1 1 x \u200b ln \u2061 x \u200b \ud835\udc51 x = ln \u2061 ln \u2061 ( k + 1 ) \u2212 ln \u2061 ln \u2061 3 . absent superscript subscript 3 \ud835\udc58 1 1 \ud835\udc65 \ud835\udc65 differential-d \ud835\udc65 \ud835\udc58 1 3 \\displaystyle>\\int_{3}^{k+1}\\frac{1}{x\\ln x}dx=\\ln\\ln(k+1)-\\ln\\ln 3. We then empirically test their length extrapolation capability on Wikitext-103, Books, Github, and WikiBook datasets by\n\nscaling the inference sequence length from 512 to 9216 tokens. As shown in Figure 4, the PPLs of both RPEs grow rapidly as the length of the testing sequence increases. It de-\n\nmonstrates that both of them cannot extrapolate. We also include Type 1 RPE in Figure 4 as a reference that can extrapolate. Detailed numerical results can be found in Table 6\n\nfrom Appendix. Validating TRF\n\nWe validate our proposed TRF by comparing the trend between the TRF and ERF. We plot the TRFs and ERFs of the Alibi, Kerple, Sandwich, and our proposed RPEs on the aforementioned datasets. As observed in Figure 6 and Figure 5, while the curves vary across datasets, TRF estimates a similar overall trend of ERFs. Visualizing RPE\n\nWe visualize the weighting schemes of Type 1 and Type 2 in Figure 7, i.e., the heatmap of . Type 2 concentrates weights on closer neighboring tokens than Type 1, indicating a smaller TRF and ERF as shown in Figure 6 and Figure 5. We also visualize other methods in Appendix. Conclusion\n\nIn this paper, we explore the secrets of transformer length extrapolation in language modeling. We first make a hypothesis about extrapolation and then derived the sufficient conditions for RPE to have the length extrapolation property. A thorough mathematical analysis reveals that a transformer model is certain to be capable of length extrapolation if the series that corresponds to the exponential of its RPE converges. This observation brings an extra bonus: we can estimate TRFs of RPEs solely based on their formulations. We chose two new RPEs that satisfy the conditions and two that do not to empirically prove the sufficiency of the conditions on four widely used datasets. We also validated our TRFs by comparing them with ERFs on these datasets as well. The results show that our TRFs can accurately reflect the actual receptive fields of RPEs before training. Acknowledgement\n\nThis work is partially supported by the National Key R&D Program of China (NO.2022ZD0160100). References\n\nAkbari et al. (2021) Akbari, H.; Yuan, L.; Qian, R.; Chuang, W.-H.; Chang, S.-F.; Cui, Y.; and Gong, B. 2021. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. In arXiv preprint arXiv:2104.11178. Beltagy, Peters, and Cohan (2020) Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The Long-Document Transformer.",
    "transextra-15": "In arXiv:2004.05150. Chi et al. (2022) Chi, T.-C.; Fan, T.-H.; Ramadge, P. J.; and Rudnicky, A.",
    "transextra-16": "I. 2022. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation.",
    "transextra-17": "ArXiv, abs/2205.09921. Chi, Fan, and Rudnicky (2022) Chi, T.-C.; Fan, T.-H.; and Rudnicky, A. I. 2022. Receptive Field Alignment Enables Transformer Length Extrapolation.",
    "transextra-18": "ArXiv, abs/2212.10356. Cho et al. (2014) Cho, K.; van Merri\u00ebnboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1724\u20131734. Doha, Qatar: Association for Computational Linguistics. Devlin et al. (2019) Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171\u20134186. Minneapolis, Minnesota: Association for Computational Linguistics. Dosovitskiy et al. (2020) Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale.",
    "transextra-19": "arXiv preprint arXiv:2010.11929. Gao et al. (2020) Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; Presser, S.; and Leahy, C. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. In arXiv preprint arXiv:2101.00027. Gehring et al. (2017) Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y. N. 2017. Convolutional sequence to sequence learning.",
    "transextra-20": "In International Conference on Machine Learning, 1243\u20131252. PMLR. Gong, Chung, and Glass (2021) Gong, Y.; Chung, Y.-A.; and Glass, J. 2021. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, 571\u2013575. Gulati et al. (2020) Gulati, A.; Chiu, C.-C.; Qin, J.; Yu, J.; Parmar, N.; Pang, R.; Wang, S.; Han, W.; Wu, Y.; Zhang, Y.; and Zhang, Z., eds. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. Hao et al. (2024) Hao, D.; Mao, Y.; He, B.; Han, X.; Dai, Y.; and Zhong, Y. 2024. Improving Audio-Visual Segmentation with Bidirectional Generation. In Proceedings of the AAAI Conference on Artificial Intelligence. Hershey et al. (2016) Hershey, S.; Chaudhuri, S.; Ellis, D. P. W.; Gemmeke, J. F.; Jansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous, R. A.; Seybold, B.; Slaney, M.; Weiss, R. J.; and Wilson, K.",
    "transextra-21": "W. 2016. CNN architectures for large-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 131\u2013135. Hochreiter and Schmidhuber (1997) Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation, 9(8): 1735\u20131780. Kim (2014) Kim, Y. 2014. Convolutional Neural Networks for Sentence Classification. In Conference on Empirical Methods in Natural Language Processing. Knopp (1956) Knopp, K. 1956. Infinite sequences and series. Courier Corporation. Liu et al. (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Liu et al. (2022) Liu, Z.; Li, D.; Lu, K.; Qin, Z.; Sun, W.; Xu, J.; and Zhong, Y. 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955. Liutkus et al. (2021) Liutkus, A.; C\u00edfka, O.; Wu, S.-L.; Simsekli, U.; Yang, Y.-H.; and Richard, G. 2021. Relative positional encoding for transformers with linear complexity.",
    "transextra-22": "In International Conference on Machine Learning, 7067\u20137079. PMLR. Lu et al. (2022) Lu, K.; Liu, Z.; Wang, J.; Sun, W.; Qin, Z.; Li, D.; Shen, X.; Deng, H.; Han, X.; Dai, Y.; and Zhong, Y. 2022. Linear video transformer with feature fixation. arXiv preprint arXiv:2210.08164. Merity et al. (2016) Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016. Pointer Sentinel Mixture Models. In arXiv:1609.07843. Ott et al. (2019) Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grangier, D.; and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038. Press, Smith, and Lewis (2022) Press, O.; Smith, N.; and Lewis, M. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations. Qin et al. (2023a) Qin, Z.; Han, X.; Sun, W.; He, B.; Li, D.; Li, D.; Dai, Y.; Kong, L.; and Zhong, Y. 2023a. Toeplitz Neural Network for Sequence Modeling. In The Eleventh International Conference on Learning Representations. Qin et al. (2022a) Qin, Z.; Han, X.; Sun, W.; Li, D.; Kong, L.; Barnes, N.; and Zhong, Y. 2022a. The Devil in Linear Transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 7025\u20137041. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. Qin et al. (2023b) Qin, Z.; Li, D.; Sun, W.; Sun, W.; Shen, X.; Han, X.; Wei, Y.; Lv, B.; Yuan, F.; Luo, X.; Qiao, Y.; and Zhong, Y. 2023b. Scaling TransNormer to 175 Billion Parameters. In arXiv preprint 2307.14995. Qin et al. (2022b) Qin, Z.; Sun, W.; Deng, H.; Li, D.; Wei, Y.; Lv, B.; Yan, J.; Kong, L.; and Zhong, Y. 2022b. cosFormer: Rethinking Softmax In Attention. In International Conference on Learning Representations. Qin et al. (2023c) Qin, Z.; Sun, W.; Lu, K.; Deng, H.; Li, D.; Han, X.; Dai, Y.; Kong, L.; and Zhong, Y. 2023c. Linearized Relative Positional Encoding. arXiv preprint arXiv:2307.09270. Qin, Yang, and Zhong (2023) Qin, Z.; Yang, S.; and Zhong, Y. 2023. Hierarchically gated recurrent neural network for sequence modeling. NeurIPS. Qin and Zhong (2023) Qin, Z.; and Zhong, Y. 2023. Accelerating Toeplitz Neural Network with Constant-time Inference Complexity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Su et al. (2021) Su, J.; Lu, Y.; Pan, S.; Wen, B.; and Liu, Y. 2021. Roformer: Enhanced transformer with rotary position embedding.",
    "transextra-23": "arXiv preprint arXiv:2104.09864. Sun et al. (2022a) Sun, J.; Zhong, G.; Zhou, D.; Li, B.; and Zhong, Y. 2022a. Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition. arXiv preprint arXiv:2203.15609. Sun et al. (2022b) Sun, W.; Qin, Z.; Deng, H.; Wang, J.; Zhang, Y.; Zhang, K.; Barnes, N.; Birchfield, S.; Kong, L.; and Zhong, Y. 2022b. Vicinity Vision Transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence, (01): 1\u201314. Vaswani et al. (2017) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Wettig et al. (2022) Wettig, A.; Gao, T.; Zhong, Z.; and Chen, D. 2022. Should You Mask 15% in Masked Language Modeling?",
    "transextra-24": "In arXiv:2202.08005. Zhu et al. (2015) Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV). Supplementary Material\n\nAppendix A Mathematical notations\n\nAppendix B Examples\n\nIn this section, we use Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), and Sandwich (Chi, Fan, and Rudnicky 2022) as examples to support the discovered sufficient conditions 16 for length extrapolation. Alibi The form of Alibi can be written as:\n\nb t = exp \u2061 ( \u2212 k \u200b t ) , k > 0 . formulae-sequence subscript \ud835\udc4f \ud835\udc61 \ud835\udc58 \ud835\udc61 \ud835\udc58 0 b_{t}=\\exp(-kt),k>0. (30)\n\nAccording to the convergence of geometric series:\n\nlim i \u2192 \u221e B i \u200b i = \u2211 t = 0 i \u2212 1 exp \u2061 ( \u2212 k \u200b t ) < \u221e , subscript \u2192 \ud835\udc56 subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 superscript subscript \ud835\udc61 0 \ud835\udc56 1 \ud835\udc58 \ud835\udc61 \\lim_{i\\to\\infty}B_{ii}=\\sum_{t=0}^{i-1}\\exp(-kt)<\\infty, (31)\n\nwhich satisfies our observed conditions. Kerple Kerple proposes two forms of RPEs: log and power. We discuss them separately. The formulation of the Log variant can be expressed as:\n\nb t = exp \u2061 ( \u2212 r \u200b log \u2061 ( 1 + k \u200b t ) ) = 1 ( 1 + k \u200b t ) r subscript \ud835\udc4f \ud835\udc61 \ud835\udc5f 1 \ud835\udc58 \ud835\udc61 1 superscript 1 \ud835\udc58 \ud835\udc61 \ud835\udc5f b_{t}=\\exp(-r\\log\\left(1+kt\\right))=\\frac{1}{(1+kt)^{r}} (32)\n\nwhere r,k\u00bf0r, k \u00bf 0. In Kerple, r,kr, k are learnable. Based on Theorem 0.4, to enable the model to extrapolate, we must add the restriction that r\u00bf1r \u00bf 1 because:\n\n1 ( 1 + k \u200b t ) r \u223c 1 k r \u200b t r , similar-to 1 superscript 1 \ud835\udc58 \ud835\udc61 \ud835\udc5f 1 superscript \ud835\udc58 \ud835\udc5f superscript \ud835\udc61 \ud835\udc5f \\frac{1}{(1+kt)^{r}}\\sim\\frac{1}{k^{r}t^{r}}, (33)\n\nIn empirical analysis, we will show that when r=1r= 1, the model cannot extrapolate. We also checked the trained model from Kerple and found this condition is met. The Ploy variant can be written as:\n\nb t = exp \u2061 ( \u2212 k \u200b t r ) , 0 < r \u2264 2 . formulae-sequence subscript \ud835\udc4f \ud835\udc61 \ud835\udc58 superscript \ud835\udc61 \ud835\udc5f 0 \ud835\udc5f 2 b_{t}=\\exp(-kt^{r}),0<r\\leq 2. (34)\n\nSince\n\nb t \u2264 exp \u2061 ( \u2212 k \u200b t ) , t \u2192 \u221e . formulae-sequence subscript \ud835\udc4f \ud835\udc61 \ud835\udc58 \ud835\udc61 \u2192 \ud835\udc61 b_{t}\\leq\\exp(-kt),t\\to\\infty. (35)\n\naccording to the convergence of geometric series, we have:\n\nlim i \u2192 \u221e B i \u200b i = lim i \u2192 \u221e \u2211 t = 0 i \u2212 1 exp \u2061 ( \u2212 k \u200b t r ) < \u221e . subscript \u2192 \ud835\udc56 subscript \ud835\udc35 \ud835\udc56 \ud835\udc56 subscript \u2192 \ud835\udc56 superscript subscript \ud835\udc61 0 \ud835\udc56 1 \ud835\udc58 superscript \ud835\udc61 \ud835\udc5f \\lim_{i\\to\\infty}B_{ii}=\\lim_{i\\to\\infty}\\sum_{t=0}^{i-1}\\exp(-kt^{r})<\\infty. (36)\n\nwhich satisfies our observed conditions. Sandwich Given the formulation of Sandwich:\n\nb t = exp \u2061 ( k \u200b ( \u2211 j = 1 d / 2 cos \u2061 ( t r 2 \u200b j / d ) \u2212 d 2 ) ) , k > 0 , r > 0 . formulae-sequence subscript \ud835\udc4f \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udc57 1 \ud835\udc51 2 \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 \ud835\udc51 2 formulae-sequence \ud835\udc58 0 \ud835\udc5f 0 b_{t}=\\exp\\left(k\\left(\\sum_{j=1}^{d/2}\\cos\\left(\\frac{t}{r^{2j/d}}\\right)-\\frac{d}{2}\\right)\\right),k>0,r>0. We first do the following transformations:\n\nb t subscript \ud835\udc4f \ud835\udc61 \\displaystyle b_{t} = exp \u2061 ( k \u200b ( \u2211 j = 1 d / 2 ( cos \u2061 ( t r 2 \u200b j / d ) \u2212 1 ) ) ) absent \ud835\udc58 superscript subscript \ud835\udc57 1 \ud835\udc51 2 \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 1 \\displaystyle=\\exp\\left(k\\left(\\sum_{j=1}^{d/2}\\left(\\cos\\left(\\frac{t}{r^{2j/d}}\\right)-1\\right)\\right)\\right) (37) = \u220f j = 1 d / 2 exp \u2061 ( k \u200b ( cos \u2061 ( t r 2 \u200b j / d ) \u2212 1 ) ) , absent superscript subscript product \ud835\udc57 1 \ud835\udc51 2 \ud835\udc58 \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 1 \\displaystyle=\\prod_{j=1}^{d/2}\\exp\\left(k\\left(\\cos\\left(\\frac{t}{r^{2j/d}}\\right)-1\\right)\\right),\n\nthen make a partition over :\n\nt r 2 \u200b j / d \u2265 \u03c0 2 , \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 \ud835\udf0b 2 \\frac{t}{r^{2j/d}}\\geq\\frac{\\pi}{2}, (38)\n\nwhich is equivalent to:\n\n2 \u200b t 2 \ud835\udc61 \\displaystyle 2t \u2265 \u03c0 \u200b r 2 \u200b j / d , 2 \u200b t \u03c0 \u2265 r 2 \u200b j / d , formulae-sequence absent \ud835\udf0b superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 2 \ud835\udc61 \ud835\udf0b superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 \\displaystyle\\geq\\pi r^{2j/d},\\frac{2t}{\\pi}\\geq r^{2j/d}, (39) log r \u2061 ( 2 \u200b t \u03c0 ) subscript \ud835\udc5f 2 \ud835\udc61 \ud835\udf0b \\displaystyle\\log_{r}\\left(\\frac{2t}{\\pi}\\right) \u2265 2 \u200b j / d , j \u2264 d \u200b log r \u2061 ( 2 \u200b t \u03c0 ) 2 \u225c f \u200b ( t ) . formulae-sequence absent 2 \ud835\udc57 \ud835\udc51 \ud835\udc57 \ud835\udc51 subscript \ud835\udc5f 2 \ud835\udc61 \ud835\udf0b 2 \u225c \ud835\udc53 \ud835\udc61 \\displaystyle\\geq 2j/d,j\\leq\\frac{d\\log_{r}\\left(\\frac{2t}{\\pi}\\right)}{2}\\triangleq f(t). Therefore we have:\n\nb t subscript \ud835\udc4f \ud835\udc61 \\displaystyle b_{t} = \u220f 1 \u2264 j \u2264 f \u200b ( t ) exp \u2061 ( k \u200b ( cos \u2061 ( t r 2 \u200b j / d ) \u2212 1 ) ) absent subscript product 1 \ud835\udc57 \ud835\udc53 \ud835\udc61 \ud835\udc58 \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 1 \\displaystyle=\\prod_{1\\leq j\\leq f(t)}\\exp\\left(k\\left(\\cos\\left(\\frac{t}{r^{2j/d}}\\right)-1\\right)\\right) (40) \u00d7 \u220f f \u200b ( t ) < j \u2264 d / 2 exp ( k ( cos ( t r 2 \u200b j / d ) \u2212 1 ) ) . \\displaystyle\\times\\prod_{f(t)<j\\leq d/2}\\exp\\left(k\\left(\\cos\\left(\\frac{t}{r^{2j/d}}\\right)-1\\right)\\right). For the first part:\n\ncos \u2061 ( t r 2 \u200b j / d ) \u2212 1 < \u2212 1 . \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 1 1 \\cos\\left(\\frac{t}{r^{2j/d}}\\right)-1<-1. (41)\n\nFor the second part:\n\ncos \u2061 ( t r 2 \u200b j / d ) \u2212 1 < 0 . \ud835\udc61 superscript \ud835\udc5f 2 \ud835\udc57 \ud835\udc51 1 0 \\cos\\left(\\frac{t}{r^{2j/d}}\\right)-1<0. (42)\n\nThen:\n\n\u03b2 t subscript \ud835\udefd \ud835\udc61 \\displaystyle\\beta_{t} \u2264 \u220f 1 \u2264 j \u2264 f \u200b ( t ) exp \u2061 ( \u2212 k ) absent subscript product 1 \ud835\udc57 \ud835\udc53 \ud835\udc61 \ud835\udc58 \\displaystyle\\leq\\prod_{1\\leq j\\leq f(t)}\\exp(-k) (43) = exp \u2061 ( \u2212 k \u200b \u230a f \u200b ( t ) \u230b ) absent \ud835\udc58 \ud835\udc53 \ud835\udc61 \\displaystyle=\\exp(-k\\lfloor f(t)\\rfloor) \u2264 exp \u2061 ( k ) \u200b exp \u2061 ( \u2212 k \u200b f \u200b ( t ) ) absent \ud835\udc58 \ud835\udc58 \ud835\udc53 \ud835\udc61 \\displaystyle\\leq\\exp(k)\\exp(-kf(t)) = exp \u2061 ( k ) \u200b exp \u2061 ( \u2212 k \u200b d \u200b log r \u2061 ( 2 \u200b t \u03c0 ) 2 ) \u225c g \u200b ( t ) . absent \ud835\udc58 \ud835\udc58 \ud835\udc51 subscript \ud835\udc5f 2 \ud835\udc61 \ud835\udf0b 2 \u225c \ud835\udc54 \ud835\udc61 \\displaystyle=\\exp(k)\\exp\\left(-\\frac{kd\\log_{r}\\left(\\frac{2t}{\\pi}\\right)}{2}\\right)\\triangleq g(t). According to Rabbe\u2019s test (Knopp 1956):\n\nt \u200b ( g \u200b ( t ) g \u200b ( t + 1 ) \u2212 1 ) \ud835\udc61 \ud835\udc54 \ud835\udc61 \ud835\udc54 \ud835\udc61 1 1 \\displaystyle t\\left(\\frac{g(t)}{g({t+1})}-1\\right) = t \u200b ( exp \u2061 ( k \u200b d 2 \u200b log r \u2061 2 \u200b t + 2 2 \u200b t ) \u2212 1 ) absent \ud835\udc61 \ud835\udc58 \ud835\udc51 2 subscript \ud835\udc5f 2 \ud835\udc61 2 2 \ud835\udc61 1 \\displaystyle=t\\left(\\exp\\left(\\frac{kd}{2}\\log_{r}\\frac{2t+2}{2t}\\right)-1\\right) (44) = t \u200b ( exp \u2061 ( k \u200b d 2 \u200b log r \u2061 ( 1 + 1 t ) ) \u2212 1 ) absent \ud835\udc61 \ud835\udc58 \ud835\udc51 2 subscript \ud835\udc5f 1 1 \ud835\udc61 1 \\displaystyle=t\\left(\\exp\\left(\\frac{kd}{2}\\log_{r}\\left(1+\\frac{1}{t}\\right)\\right)-1\\right) \u223c t \u200b ( k \u200b d 2 \u200b log r \u2061 ( 1 + 1 t ) + O \u200b ( 1 t 2 ) ) similar-to absent \ud835\udc61 \ud835\udc58 \ud835\udc51 2 subscript \ud835\udc5f 1 1 \ud835\udc61 \ud835\udc42 1 superscript \ud835\udc61 2 \\displaystyle\\sim t\\left(\\frac{kd}{2}\\log_{r}\\left(1+\\frac{1}{t}\\right)+O\\left(\\frac{1}{t^{2}}\\right)\\right) \u223c t \u200b ( k \u200b d 2 \u200b ln \u2061 r \u200b ( 1 t \u2212 1 2 \u200b t 2 ) + O \u200b ( 1 t 2 ) ) similar-to absent \ud835\udc61 \ud835\udc58 \ud835\udc51 2 \ud835\udc5f 1 \ud835\udc61 1 2 superscript \ud835\udc61 2 \ud835\udc42 1 superscript \ud835\udc61 2 \\displaystyle\\sim t\\left(\\frac{kd}{2\\ln r}\\left(\\frac{1}{t}-\\frac{1}{2t^{2}}\\right)+O\\left(\\frac{1}{t^{2}}\\right)\\right) \u2192 k \u200b d 2 \u200b ln \u2061 r , \u2192 absent \ud835\udc58 \ud835\udc51 2 \ud835\udc5f \\displaystyle\\to\\frac{kd}{2\\ln r},\n\nif:\n\nk \u200b d 2 \u200b ln \u2061 r < 1 , d < 2 \u200b ln \u2061 r k , formulae-sequence \ud835\udc58 \ud835\udc51 2 \ud835\udc5f 1 \ud835\udc51 2 \ud835\udc5f \ud835\udc58 \\displaystyle\\frac{kd}{2\\ln r}<1,\\quad d<\\frac{2\\ln r}{k}, (45)\n\nthen the series converges111Note that here we only show that the series corresponding to Sandwich is convergent under certain conditions. The upper bound here is relatively loose, and the conditions used in practice are broader.. Appendix C TRF example\n\nWe use Alibi as an example to show the calculation of TRF. The of Alibi can be written as:\n\nB i \u200b j = \u2211 i = 0 j \u2212 1 exp \u2061 ( \u2212 i ) = 1 \u2212 exp \u2061 ( \u2212 j ) 1 \u2212 exp \u2061 ( \u2212 1 ) , B = 1 1 \u2212 exp \u2061 ( \u2212 1 ) formulae-sequence subscript \ud835\udc35 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc56 0 \ud835\udc57 1 \ud835\udc56 1 \ud835\udc57 1 1 \ud835\udc35 1 1 1 B_{ij}=\\sum_{i=0}^{j-1}\\exp(-i)=\\frac{1-\\exp(-j)}{1-\\exp(-1)},B=\\frac{1}{1-\\exp(-1)} (46)\n\nThe TRF of Alibi can be calculated as:\n\nn the \u200b ( \u03f5 ) subscript \ud835\udc5b the italic-\u03f5 \\displaystyle n_{\\mathrm{the}}(\\epsilon) = inf j ( B i \u200b j > B \u200b ( 1 \u2212 \u03f5 ) ) absent subscript infimum \ud835\udc57 subscript \ud835\udc35 \ud835\udc56 \ud835\udc57 \ud835\udc35 1 italic-\u03f5 \\displaystyle=\\inf_{j}\\left(B_{ij}>B(1-\\epsilon)\\right) (47) = inf j ( 1 \u2212 exp \u2061 ( \u2212 j ) > 1 \u2212 \u03f5 ) = \u0398 \u200b ( \u2212 log \u2061 \u03f5 ) .",
    "transextra-25": "absent subscript infimum \ud835\udc57 1 \ud835\udc57 1 italic-\u03f5 \u0398 italic-\u03f5 \\displaystyle=\\inf_{j}\\left(1-\\exp(-j)>1-\\epsilon\\right)=\\Theta(-\\log\\epsilon). where represents the upper and lower asymptotic bound. Appendix D Configurations\n\nAppendix E Pseudocode for TRF and ERF visualization\n\n1import torch\n\n3def draw(array, n=50):\n\n4 epsilon = torch.flip(torch.linspace(0, 1, n), dims=[0])\n\n5 index = torch.zeros(n)\n\n6 cusum = torch.sum(array)\n\n7 m = len(array)\n\n8 s = 0\n\n9 i = 0\n\n10 for j in range(m):\n\n11 eps = epsilon[i]\n\n12 while s >= cusum * (1 - eps) and i < n:\n\n13 index[i] = j\n\n14 if i < n - 1:\n\n15 i += 1\n\n16 else:\n\n17 break\n\n18 eps = epsilon[i]\n\n19 s += array[j]\n\n20 while i < n:\n\n21 index[i] = m\n\n22 i += 1\n\n23\n\n24 return index / m, epsilon\n\nAppendix F Detailed Experimental Results\n\nAppendix G Heatmap\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Feb 28 17:44:44 2024 by LaTeXML"
}