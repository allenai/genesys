{
    "maxmargin-0": "# Max-Margin Token Selection in Attention Mechanism \n\nDavoud Ataee Tarzanagh<br>University of Pennsylvania<br>tarzanaq@upenn.edu\n\nYingcong Li Xuechen Zhang<br>University of California, Riverside<br>\\{yli692, xzhan394\\}@ucr.edu\n\nSamet Oymak<br>University of Michigan<br>UC Riverside<br>oymak@umich.edu\n\n\n#### Abstract\n\nAttention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models.",
    "maxmargin-1": "However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmaxattention model $f(\\boldsymbol{X})=\\langle\\boldsymbol{X} \\boldsymbol{v}$, softmax $(\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{p})\\rangle$, where $\\boldsymbol{X}$ is the token sequence and $(\\boldsymbol{v}, \\boldsymbol{W}, \\boldsymbol{p})$ are trainable parameters. We prove that running gradient descent on $\\boldsymbol{p}$, or equivalently $\\boldsymbol{W}$, converges in direction to a max-margin solution that separates locally-optimal tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize optimality of tokens in terms of the value embeddings $\\boldsymbol{X} \\boldsymbol{v}$ and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$ and $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where $v$ separates the input features based on their labels.",
    "maxmargin-2": "Interestingly, the SVM formulation of $\\boldsymbol{p}$ is influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we verify our theoretical findings via numerical experiments and provide insights. ## 1 Introduction\n\nSince its introduction in the seminal work [1], attention mechanism has played an influential role in advancing natural language processing, and more recently, large language models $[2,3,4,5]$. Initially introduced for encoder-decoder RNN architectures, attention allows the decoder to focus on the most relevant parts of the input sequence, instead of relying solely on a fixed-length hidden state. Attention mechanism has taken the center stage in the transformers [6], where the self-attention layer - which calculates softmax similarities between input tokens - serves as the backbone of the architecture. Since their inception, transformers have revolutionized natural language processing, from models like BERT [7] to ChatGPT [8], and have also become the architecture of choice for foundation models [9] addressing diverse challenges in generative modeling [3, 10], computer vision $[11,12]$, and reinforcement learning $[13,14,15]$. The prominence of the attention mechanism motivates a fundamental theoretical understanding of its role in optimization and learning. While it is well-known that attention enables the model to focus on the relevant parts of the input sequence, the precise mechanism by which this is achieved is far from clear. To this end, we ask\n\nQ: What are the optimization dynamics and inductive biases of the attention mechanism? We study this question using the fundamental attention model $f(\\boldsymbol{X})=\\left\\langle\\boldsymbol{X} \\boldsymbol{v}, \\mathbb{S}\\left(\\boldsymbol{X} \\boldsymbol{W}^{\\top} \\boldsymbol{p}\\right)\\right\\rangle$. Here, $\\boldsymbol{X}$ is the sequence of input tokens, $v$ is the prediction head, $\\boldsymbol{W}$ is the trainable key-query weights, and $\\mathbb{S}$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-02.jpg?height=410&width=1362&top_left_y=251&top_left_x=361)\n\nFigure 1: The convergence behavior of the gradient descent on the attention weights $\\boldsymbol{p}$ using the logistic loss in (ERM).",
    "maxmargin-3": "The arrows ( $\\rightarrow-$ ) represent trajectories from different initializations. Here, $(---)$ and (- - $)$ denote the globally- and locally-optimal max-margin directions (GMM, LMM). $\\gamma$ denotes the score of a token per Definition 1. Discussion is provided under Theorems 2 and 3. denotes the softmax nonlinearity. For transformers, $\\boldsymbol{p}$ corresponds to the [CLS] token or tunable prompt [16,17,18], whereas for RNN architectures [1], $\\boldsymbol{p}$ corresponds to the hidden state. Given training data $\\left(Y_{i}, \\boldsymbol{X}_{i}\\right)_{i=1}^{n}$ with labels $Y_{i} \\in\\{-1,1\\}$ and inputs $\\boldsymbol{X}_{i} \\in \\mathbb{R}^{T \\times d}$, we consider the empirical risk minimization with a decreasing loss function $\\ell(\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}$\n\n$$\n\\mathcal{L}(\\boldsymbol{v}, \\boldsymbol{p}, \\boldsymbol{W})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot f\\left(\\boldsymbol{X}_{i}\\right)\\right), \\text { where } f\\left(\\boldsymbol{X}_{i}\\right)=\\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}^{\\top} \\boldsymbol{p}\\right)\n$$\n\nAt a high-level, this work establishes fundamental equivalences between the optimization trajectories of (1) and hard-margin SVM problems. Our main contributions are as follows:\n\n- Optimization geometry of attention (Sec 2): We first show that gradient iterations of $\\boldsymbol{p}$ and $\\boldsymbol{W}$ admit a one-to-one mapping, thus we focus on optimizing $\\boldsymbol{p}$ without losing generality. In Theorem 3, we prove that, under proper initialization:\n\nGradient descent on $\\boldsymbol{p}$ converges in direction to a max-margin solution - namely\n(ATT-SVM) - that separates locally-optimal tokens from non-optimal ones. We call these Locally-optimal Max-Margin (LMM) directions and show that these thoroughly characterize the viable convergence directions of attention when the norm of its weights grows to infinity. We also identify conditions under which (algorithm-independent) regularization path and gradient descent path converge to Globally-optimal Max-Margin (GMM) direction in Theorems 1 and 2, respectively. A central feature of our results is precisely quantifying optimality in terms of token scores $\\gamma_{t}=Y \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{x}_{t}$ where $\\boldsymbol{x}_{t}$ is the $t^{\\text {th }}$ token of the input sequence $\\boldsymbol{X}$. Locally-optimal tokens are those with higher scores than their nearest neighbors determined by the SVM solution. These are illustrated in Figure 1. - Optimize attention $\\boldsymbol{p}$ and prediction-head $\\boldsymbol{v}$ jointly (Sec 3): We study the joint problem under logistic loss function. We use regularization path analysis where (ERM) is solved under ridge constraints and we study the solution trajectory as the constraints are relaxed. Since the problem is linear in $\\boldsymbol{v}$, if the attention features $\\boldsymbol{x}_{i}^{\\text {att }}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}^{\\top} \\boldsymbol{p}\\right)$ are separable based on their labels $Y_{i}, \\boldsymbol{v}$ would implement a max-margin classifier. Building on this, we prove that $\\boldsymbol{p}$ and $\\boldsymbol{v}$ converges to their respective max-margin solutions under proper geometric conditions (Theorem 5). Relaxing these conditions, we obtain a more general solution where margin constraints on $\\boldsymbol{p}$ are relaxed on the inputs whose attention features are not support vectors of $\\boldsymbol{v}$ (Theorem 6). Figure 3 illustrates these outcomes. The next section introduces the preliminary concepts, Section 4 presents numerical experiments ${ }^{1}$, Section 5 discusses related literature, and Section 6 highlights limitations and future work. ### 1.1 Preliminaries\n\nNotations. For any integer $N \\geq 1$, let $[N]:=\\{1, \\ldots, N\\}$. We use lower-case and upper-case bold letters (e.g. $\\boldsymbol{a}$ and $\\boldsymbol{A}$ ) to represent vectors and matrices, respectively. The entries of $\\boldsymbol{a}$ are denoted as $\\boldsymbol{a}_{i}$. We use $\\bar{\\sigma}(\\boldsymbol{A})$ to denote the maximum singular value of $\\boldsymbol{A}$. We denote the minimum of two numbers $a, b$ as $a \\wedge b$, and the maximum as $a \\vee b$. Big-O notation $O(\\cdot)$ hides the universal constants. Throughout, we will use $\\mathcal{L}(\\boldsymbol{p})$ and $\\mathcal{L}(\\boldsymbol{v}, \\boldsymbol{p})$ to denote Objective (1) with fixed ( $\\boldsymbol{v}, \\boldsymbol{W}$ ) and $\\boldsymbol{W}$, respectively. Optimization. Given an objective function $\\mathcal{L}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ and an $\\ell_{2}$-norm bound $R$, define the regularized solution as\n\n[^0]$$\n\\overline{\\boldsymbol{p}}(R):=\\arg \\min _{\\|\\boldsymbol{p}\\| \\leq R} \\mathcal{L}(\\boldsymbol{p}) . $$\n\nRegularization path - the evolution of $\\overline{\\boldsymbol{p}}(R)$ as $R$ grows - is known to capture the spirit of gradient descent as the ridge constraint $R$ provides a proxy for the number of gradient descent iterations. For instance, [19, 20, 21] study the implicit bias of logistic regression and rigorously connect the directional convergence of regularization path (i.e. $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) / R$ ) and gradient descent. For gradient descent, we assume the objective $\\mathcal{L}(\\boldsymbol{p})$ is smooth and describe the gradient descent process as\n\n$$\n\\boldsymbol{p}(t+1)=\\boldsymbol{p}(t)-\\eta(t) \\nabla \\mathcal{L}(\\boldsymbol{p}(t))\n$$\n\nwhere $\\eta(t)$ is the stepsize at time $t$ and $\\nabla \\mathcal{L}(\\boldsymbol{p}(t))$ is the gradient of $\\mathcal{L}$ at $\\boldsymbol{p}(t)$. Attention in Transformers. Next, we will discuss the connection between our model and the attention mechanism used in transformers. Our exposition borrows from [17], where the authors analyze the same attention model using gradient-based techniques on specific contextual datasets. - Self-attention is the core building block of transformers [6]. Given an input consisting of $T$ tokens $\\boldsymbol{X}=\\left[\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{T}\\right]^{\\top} \\in \\mathbb{R}^{T \\times d}$, self-attention with key-query matrix $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times d}$, and value matrix $\\boldsymbol{V} \\in \\mathbb{R}^{d \\times v}$, the self-attention model is defined as follows:\n\n$$\nf_{s a}(\\boldsymbol{X})=\\mathbb{S}\\left(\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{X}^{\\top}\\right) \\boldsymbol{X} \\boldsymbol{V}\n$$\n\nHere, $\\mathbb{S}(\\cdot)$ is the softmax nonlinearity that applies row-wise on the similarity matrix $\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{X}^{\\top}$. - Tunable tokens: [CLS] and prompt-tuning. In practice, we append additional tokens to the raw input features $\\boldsymbol{X}$ : For instance, a [CLS] token is used for classification purposes [7] and prompt vectors can be appended for adapting a pretrained model to new tasks $[16,18]$. Let $\\boldsymbol{p} \\in \\mathbb{R}^{d}$ be the tunable token ([CLS] or prompt vector) and concatenate it to $\\boldsymbol{X}$ to obtain $\\boldsymbol{X}_{\\boldsymbol{p}}:=\\left[\\boldsymbol{p} \\boldsymbol{X}^{\\boldsymbol{\\top}}\\right]^{\\top} \\in \\mathbb{R}^{(T+1) \\times d}$. Consider the cross-attention features obtained from $\\boldsymbol{X}_{p}$ and $\\boldsymbol{X}$ given by\n\n$$\n\\left[\\begin{array}{l}\nf_{c l s}^{\\top}(\\boldsymbol{X}) \\\\\nf_{s a}(\\boldsymbol{X})\n\\end{array}\\right]=\\mathbb{S}\\left(\\boldsymbol{X}_{\\boldsymbol{p}} \\boldsymbol{W} \\boldsymbol{X}^{\\top}\\right) \\boldsymbol{X} \\boldsymbol{V}=\\left[\\begin{array}{c}\n\\mathbb{S}\\left(\\boldsymbol{p}^{\\top} \\boldsymbol{W} \\boldsymbol{X}^{\\top}\\right) \\\\\n\\mathbb{S}\\left(\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{X}^{\\top}\\right)\n\\end{array}\\right] \\boldsymbol{X} \\boldsymbol{V}\n$$\n\nThe beauty of cross-attention is that it isolates the contribution of $\\boldsymbol{p}$ under the upper term $f_{\\text {cls }}(\\boldsymbol{X})=$ $\\boldsymbol{V}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X} \\boldsymbol{W}^{\\top} \\boldsymbol{p}\\right) \\in \\mathbb{R}^{v}$. In this work, we use the value weights for classification, thus we set $v=1$, and denote $v=V \\in \\mathbb{R}^{d}$. This brings us to our attention model of interest:\n\n$$\nf(\\boldsymbol{X})=\\boldsymbol{v}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p}), \\quad \\text { where } \\quad \\boldsymbol{K}=\\boldsymbol{X} \\boldsymbol{W}^{\\top}\n$$\n\nHere, $(\\boldsymbol{v}, \\boldsymbol{W}, \\boldsymbol{p})$ are the tunable model parameters and $\\boldsymbol{K}$ is the key embeddings. Note that $\\boldsymbol{W}$ and $\\boldsymbol{p}$ are playing the same role within softmax, thus, it is intuitive that they exhibit similar optimization dynamics. Confirming this, the next lemma shows that gradient iterations of $\\boldsymbol{p}$ (after setting $\\boldsymbol{W} \\leftarrow$ Identity) and $\\boldsymbol{W}$ admit a one-to-one mapping. Lemma 1 Fix $\\boldsymbol{u} \\in \\mathbb{R}^{d} \\backslash\\{\\mathbf{0}\\}$. Let $\\psi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ and $\\ell: \\mathbb{R} \\rightarrow \\mathbb{R}$ be differentiable functions. On the same training data $\\left(Y_{i}, \\boldsymbol{X}_{i}\\right)_{i=1}^{n}$, define $\\tilde{\\mathcal{L}}(\\boldsymbol{p}):=1 / n \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)\\right)\\right)$ and $\\mathcal{L}(\\boldsymbol{W}):=$ $1 / n \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}^{\\top} \\boldsymbol{u}\\right)\\right)\\right)$. Consider the gradient descent iterations on $\\boldsymbol{p}$ and $\\boldsymbol{W}$ with initial values $\\boldsymbol{p}(0)$ and $\\boldsymbol{W}(0)=\\boldsymbol{u} \\boldsymbol{p}(0)^{\\top} /\\|\\boldsymbol{u}\\|^{2}$ and stepsizes $\\eta$ and $\\eta /\\|\\boldsymbol{u}\\|^{2}$, respectively:\n\n$$\n\\begin{aligned}\n\\boldsymbol{p}(t+1) & =\\boldsymbol{p}(t)-\\eta \\nabla \\tilde{\\mathcal{L}}(\\boldsymbol{p}(t)) \\\\\n\\boldsymbol{W}(t+1) & =\\boldsymbol{W}(t)-\\frac{\\eta}{\\|\\boldsymbol{u}\\|^{2}} \\nabla \\mathcal{L}(\\boldsymbol{W}(t))\n\\end{aligned}\n$$\n\nWe have that $\\boldsymbol{W}(t)=\\boldsymbol{u} \\boldsymbol{p}(t)^{\\top} /\\|\\boldsymbol{u}\\|^{2}$ for all $t \\geq 0$. This lemma directly characterizes the optimization dynamics of $\\boldsymbol{W}$ through the dynamics of $\\boldsymbol{p}$, allowing us to reconstruct $\\boldsymbol{W}$ from $\\boldsymbol{p}$ using their gradient iterations. Therefore, we will fix $\\boldsymbol{W}$ and concentrate on optimizing $\\boldsymbol{p}$ in Section 2 and the joint optimization of $(\\boldsymbol{v}, \\boldsymbol{p})$ in Section 3. Problem definition: Throughout, $\\left(Y_{i}, \\boldsymbol{X}_{i}\\right)_{i=1}^{n}$ denotes training dataset where $Y_{i} \\in\\{-1,1\\}$ and $\\boldsymbol{X}_{i} \\in \\mathbb{R}^{T \\times d}$. We denote the key embeddings of $\\boldsymbol{X}_{i}$ via $\\boldsymbol{K}_{i}=\\boldsymbol{X}_{i} \\boldsymbol{W}^{\\top}$ and explore the training risk\n\n$$\n\\mathcal{L}(\\boldsymbol{v}, \\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)\n$$\n\nImportantly, our results apply to general tuples $\\left(Y_{i}, \\boldsymbol{X}_{i}, \\boldsymbol{K}_{i}\\right)$ and do not assume that $\\left(\\boldsymbol{X}_{i}, \\boldsymbol{K}_{i}\\right)$ are tied via $\\boldsymbol{W}$. Finally, the $t^{t h}$ tokens of $\\boldsymbol{X}_{i}, \\boldsymbol{K}_{i}$ are denoted by $\\boldsymbol{x}_{i t}, \\boldsymbol{k}_{i t} \\in \\mathbb{R}^{d}$, respectively, for $t \\in[T]$. The highly nonlinear and nonconvex nature of the softmax operation makes the training problem in (ERM) a challenging nonconvex optimization problem for $\\boldsymbol{p}$, even with a fixed $\\boldsymbol{v}$. In the next section, we will introduce a set of assumptions to demonstrate the global and local convergence of gradient descent for margin maximization in the attention mechanism. ## 2 Global and Local Margin Maximization with Attention\n\nIn this section, we present the main results of this paper (Theorems 2 and 3) by examining the implicit bias of gradient descent on learning $\\boldsymbol{p} \\in \\mathbb{R}^{d}$ given a fixed choice of $\\boldsymbol{v} \\in \\mathbb{R}^{d}$. Notably, our results apply to general decreasing loss functions without requiring convexity. This generality is attributed to margin maximization arising from the exponentially-tailed nature of softmax within attention, rather than $\\ell$. We maintain the following assumption on the loss function throughout this section. Assumption A (Well-behaved Loss) Over any bounded interval: (1) $\\ell: \\mathbb{R} \\rightarrow \\mathbb{R}$ is strictly decreasing. (2) $\\ell^{\\prime}$ is $M_{0}$-Lipschitz continuous and $\\left|\\ell^{\\prime}(u)\\right| \\leq M_{1}$. Assumption A includes many common loss functions, including the logistic loss $\\ell(u)=\\log \\left(1+e^{-u}\\right)$, exponential loss $\\ell(u)=e^{-u}$, and correlation loss $\\ell(u)=-u$. Assumption A implies that $\\mathcal{L}(\\boldsymbol{p})$ is $L_{p}$-smooth (see Lemma 6 in Supplementary), where\n\n$$\nL_{p}:=\\frac{1}{n} \\sum_{i=1}^{n}\\left(M_{0}\\|\\boldsymbol{v}\\|^{2}\\|\\boldsymbol{W}\\|^{2}\\left\\|\\boldsymbol{X}_{i}\\right\\|^{4}+3 M_{1}\\|\\boldsymbol{v}\\|\\|\\boldsymbol{W}\\|^{2}\\left\\|\\boldsymbol{X}_{i}\\right\\|^{3}\\right)\n$$\n\nWe now introduce a convex hard-margin SVM problem that separates one token of the input sequence from the rest, jointly solved over all inputs. We will show that this problem captures the optimization properties of softmax-attention. Fix indices $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ and consider\n\n$$\n\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})=\\arg \\min _{\\boldsymbol{p}}\\|\\boldsymbol{p}\\| \\quad \\text { subject to } \\min _{t \\neq \\alpha_{i}} \\boldsymbol{p}^{\\top}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right) \\geq 1, \\text { for all } \\quad 1 \\leq i \\leq n . \\text { (ATT-SVM) }\n$$\n\nNote that existence of $\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$ implies the separability of tokens $\\boldsymbol{\\alpha}$ from the others. Specifically, choosing direction $\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$ will exactly select tokens $\\left(\\boldsymbol{x}_{i \\alpha_{i}}\\right)_{i=1}^{n}$ at the attention output for each input sequence, that is, $\\lim _{R \\rightarrow \\infty} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(R \\cdot \\boldsymbol{K}_{i} \\boldsymbol{p}^{\\mathrm{mm}}(\\boldsymbol{\\alpha})\\right)=\\boldsymbol{x}_{i \\alpha_{i}}$. We are now ready to introduce our main results that characterize the global and local convergence of the attention weights $\\boldsymbol{p}$ via (ATT-SVM). ### 2.1 Global convergence of the attention weights $p$\n\nWe first identify the conditions that guarantee the global convergence of gradient descent for $\\boldsymbol{p}$. The intuition is that, in order for attention to exhibit implicit bias, the softmax nonlinearity should be forced to select the optimal token within each input sequence. Fortunately, the optimal tokens that achieve the smallest training objective under decreasing loss function $\\ell(\\cdot)$ have a clear definition. Definition 1 (Token Scores, Optimality \\& GMM) The score of token $\\boldsymbol{x}_{i t}$ of input $\\boldsymbol{X}_{i}$ is defined as $\\boldsymbol{\\gamma}_{i t}:=Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{x}_{i t}$. The optimal tokens for input $\\boldsymbol{X}_{i}$ are those tokens with highest scores given by\n\n$$\no p t_{i} \\in \\arg \\max _{t \\in[T]} \\gamma_{i t}\n$$\n\nGlobally-optimal max-margin (GMM) direction is defined as the solution of (ATT-SVM) with optimal indices $\\left(\\text { opt }_{i}\\right)_{i=1}^{n}$ by $\\boldsymbol{p}^{m m \\star}$. It is worth noting that score definition simply uses the value embeddings $\\boldsymbol{v}^{\\top} \\boldsymbol{x}_{i t}$ of the tokens. Note that multiple tokens within an input might attain the same score, thus opt ${ }_{i}$ or $\\boldsymbol{p}^{m m \\star}$ may not be unique. The theorem below provides our regularization path guarantee on the global convergence of attention. Theorem 1 (Regularization Path) Suppose Assumption A on the loss function holds, and for all $i \\in$ $[n]$ and $t \\neq$ opt $i_{i}$, the scores obey $\\boldsymbol{\\gamma}_{i t}<\\boldsymbol{\\gamma}_{\\text {iopt }_{i}}$. Then, the regularization path $\\overline{\\boldsymbol{p}}(R)=\\arg \\min _{\\|\\boldsymbol{p}\\| \\leq R} \\mathcal{L}(\\boldsymbol{p})$ converges to the GMM direction i.e. $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) / R=\\boldsymbol{p}^{m m \\star} /\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|$. Theorem 1 shows that as the regularization strength $R$ increases towards the ridgeless problem $\\min _{p} \\mathcal{L}(\\boldsymbol{p})$, the optimal direction $\\overline{\\boldsymbol{p}}(R)$ aligns more closely with the max-margin solution $\\boldsymbol{p}^{m m \\star}$. Since this theorem allows for arbitrary token scores, it demonstrates that max-margin token separation is an essential feature of the attention mechanism. In fact, it is a corollary of Theorem 8, which applies to\nthe generalized model $f(\\boldsymbol{X})=\\psi\\left(\\boldsymbol{X}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X} \\boldsymbol{W}^{\\top} \\boldsymbol{p}\\right)\\right)$ and accommodates multiple optimal tokens per input. However, while regularization path analysis captures the global behavior, gradient descent lacks general global convergence guarantees. In Section 2.2, we show that due to the nonconvex landscape and softmax nonlinearity, gradient descent often converges to local optima. We first establish that when (ERM) is trained with gradient descent, the norm of the parameters will diverge. For the restrictive setting of $n=1$, gradient descent also exhibits a global convergence guarantee. Assumption B For all $i \\in[n]$ and $t, \\tau \\neq$ opt $_{i}$, the scores per Definition 1 obey $\\gamma_{i t}=\\gamma_{i \\tau}<\\boldsymbol{\\gamma}_{\\text {iopt }_{i}}$. Theorem 2 (Global Convergence of Gradient Descent) Suppose Assumption A on the loss function $\\ell$ and Assumption B on the tokens' score hold. Then, the gradient descent iterates $\\boldsymbol{p}(t+1)=$ $\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t))$ on $(\\mathrm{ERM})$, with the stepsize $\\eta \\leq 1 / L_{p}$ and any starting point $\\boldsymbol{p}(0)$ satisfy $\\lim _{t \\rightarrow \\infty}\\|\\boldsymbol{p}(t)\\|=\\infty$. If $n=1$, we also have $\\lim _{t \\rightarrow \\infty} \\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\|=\\boldsymbol{p}^{m m \\star} /\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|$. Theorem 2 shows that gradient descent will diverge in norm, and when $n=1$, the normalized predictor $\\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\|$ converges towards $\\boldsymbol{p}^{m m \\star}$, the separator of the globally optimal token.",
    "maxmargin-4": "While $n=1$ is a stringent condition, this requirement is in fact tight as discussed in Appendix E.",
    "maxmargin-5": "To illustrate this theorem, we have conducted synthetic experiments. Let us first explain the setup used in Figure 1. We set $d=3$ as the dimension, with each token having three entries $\\boldsymbol{x}=\\left[x_{1}, x_{2}, x_{3}\\right]$. We reserve the first two coordinates as key embeddings $\\boldsymbol{k}=\\left[x_{1}, x_{2}, 0\\right]$ by setting $\\boldsymbol{W}=\\operatorname{diag}([1,1,0])$. This is what we display in our figures as token positions. Finally, in order to assign scores to the tokens we use the last coordinate by setting $\\boldsymbol{v}=[0,0,1]$. This way score becomes $Y \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{x}=Y \\cdot x_{3}$, allowing us to assign any score (regardless of key embedding). In Figure 1(a), the gray paths represent gradient descent trajectories from different initializations. The points $(0,0)$ and $(1,0)$ correspond to non-optimal tokens, while the point $(-0.1,1)$ represents the optimal token. Notably, gradient descent iterates with various starting points converge towards the direction of the max-margin solution $\\boldsymbol{p}^{m m \\star}$ (depicted by --- ). Moreover, as the iteration count $t$ increases, the inner product $\\left\\langle\\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\|, \\boldsymbol{p}^{m m \\star} /\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|\\right\\rangle$ consistently increases. Figure 1(c) also depicts the directional convergence of gradient descent from various initializations on multiple inputs, with the gray dotted line representing the separating hyperplane. These emphasize the gradual alignment between the evolving predictor and the max-margin solution throughout the optimization. ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-05.jpg?height=50&width=1389&top_left_y=1447&top_left_x=365) full-rank. Then $\\boldsymbol{p}^{m m \\star}$ exists - i.e. (ATT-SVM) is feasible for optimal indices $\\alpha_{i} \\leftarrow o p t_{i}$. ### 2.2 Local convergence of the attention weights $p$\n\nTheorem 2 on the global convergence of gradient descent serves as a prelude to the general behavior of the optimization. Once we relax Assumption B by allowing for arbitrary token scores, we will show that $\\boldsymbol{p}$ can converge (in direction) to a locally-optimal solution. However, this locally-optimal solution is still characterized in terms of (ATT-SVM) which separates locally-optimal tokens from the rest. Our theory builds on two new concepts: locally-optimal tokens and neighbors of these tokens. Definition 2 (SVM-Neighbor and Locally-Optimal Tokens) Fix token indices $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ for which (ATT-SVM) is feasible to obtain $\\boldsymbol{p}^{m m}=\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$. Consider tokens $\\mathcal{T}_{i} \\subset[T]$ such that $\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}^{\\mathrm{mm}}=1$ for all $t \\in \\mathcal{T}_{i}$. We refer to $\\mathcal{T}_{i}$ as SVM-neighbors of $\\boldsymbol{k}_{i \\alpha_{i}}$. Additionally, tokens with indices $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ are called locally-optimal iffor all $i \\in[n]$ and $t \\in \\mathcal{T}_{i}$ scores per Definition 1 obey $\\gamma_{i \\alpha_{i}}>\\gamma_{i t}$. Associated $\\boldsymbol{p}^{\\text {mm }}$ is called a locally-optimal max-margin (LMM) direction. To provide a basis for discussing local convergence, we provide some preliminary definitions regarding cones. For a given $\\boldsymbol{q}$ and a scalar $\\mu>0$, we define $\\operatorname{cone}_{\\mu}(\\boldsymbol{q})$ as the set of vectors $\\boldsymbol{p} \\in \\mathbb{R}^{d}$ such that the correlation coefficient between $\\boldsymbol{p}$ and $\\boldsymbol{q}$ is at least $1-\\mu$ :\n\n$$\n\\operatorname{cone}_{\\mu}(\\boldsymbol{q}):=\\left\\{\\boldsymbol{p} \\in \\mathbb{R}^{d} \\left\\lvert\\,\\left\\langle\\frac{\\boldsymbol{p}}{\\|\\boldsymbol{p}\\|}, \\frac{\\boldsymbol{q}}{\\|\\boldsymbol{q}\\|}\\right\\rangle \\geq 1-\\mu\\right.\\right\\}\n$$\n\nGiven $R>0$, the intersection of $\\operatorname{cone}_{\\mu}(\\boldsymbol{q})$ and the set $\\left\\{\\boldsymbol{p} \\in \\mathbb{R}^{d} \\mid\\|\\boldsymbol{p}\\| \\geq R\\right\\}$ is denoted as $C_{\\mu, R}(\\boldsymbol{q})$ :\n\n$$\n\\mathcal{C}_{\\mu, R}(\\boldsymbol{q}):=\\operatorname{cone}_{\\mu}(\\boldsymbol{q}) \\cap\\left\\{\\boldsymbol{p} \\in \\mathbb{R}^{d} \\mid\\|\\boldsymbol{p}\\| \\geq R\\right\\}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-05.jpg?height=283&width=557&top_left_y=2076&top_left_x=1185)\n\nFigure 2: Gradient descent initialization $\\boldsymbol{p}(0)$ inside the cone containing the locally-optimal solution $\\boldsymbol{p}^{m m}$. Next, we demonstrate the existence of parameters $\\mu=\\mu(\\boldsymbol{\\alpha})>0$ and $R>0$ such that when $R$ is sufficiently large, there are no stationary points within $C_{\\mu, R}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. Further, the gradient descent initialized within $C_{\\mu, R}\\left(\\boldsymbol{p}^{m m}\\right)$ converges in direction to $\\boldsymbol{p}^{m m} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$; refer to Figure 2 for a visualization. Theorem 3 (Local Convergence of Gradient Descent) Suppose Assumption A on the loss function $\\ell$ holds and assume $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ are indices of locally-optimal tokens per Definition 2. Then, there is a constant $\\mu=\\mu(\\boldsymbol{\\alpha}) \\in(0,1)$ and $R>0$ such that $\\mathcal{C}_{\\mu, R}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$ does not contain any stationary points. Further, for any starting point $\\boldsymbol{p}(0) \\in C_{\\mu, R}\\left(\\boldsymbol{p}^{m m}\\right)$, gradient descent iterates $\\boldsymbol{p}(t+1)=\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t))$ on $(\\mathrm{ERM})$ with stepsize $\\eta \\leq 1 / L_{p}$ satisfies $\\lim _{t \\rightarrow \\infty}\\|\\boldsymbol{p}(t)\\|=\\infty$ and $\\lim _{t \\rightarrow \\infty} \\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\|=\\boldsymbol{p}^{m m} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. To further illustrate Theorem 3, we can consider Figure 1(b) where $n=1$ and $T=3$. In this figure, the point $(0,0)$ represents the non-optimal tokens, while $(1,0)$ represents the locally optimal token. Additionally, the gray paths represent the trajectories of gradient descent initiated from different points. By observing the figure, we can see that gradient descent, when properly initialized, converges towards the direction of $\\boldsymbol{p}^{\\mathrm{mm}}$ (depicted by - - ). This direction of convergence effectively separates the locally optimal tokens $(1,0)$ from the non-optimal token $(0,0)$. ### 2.3 Regularization paths can only converge to locally-optimal max-margin directions\n\nAn important question arises regarding whether our definition of LMM (Definition 2) encompasses all possible convergence paths of the attention mechanism when $\\|\\boldsymbol{p}\\| \\rightarrow \\infty$. To address this, we introduce the set of LMM directions as follows:\n\n$$\n\\mathcal{P}^{m m}:=\\left\\{\\left.\\frac{\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})}{\\left\\|\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})\\right\\|} \\right\\rvert\\, \\boldsymbol{\\alpha} \\text { is locally-optimal per Definition } 2\\right\\}\n$$\n\nThe following theorem establishes the tightness of these directions: It demonstrates that for any candidate $\\boldsymbol{q} \\notin \\mathcal{P}^{m m}$, its local regularization path within an arbitrarily small neighborhood will provably not converge in the direction of $\\boldsymbol{q}$. Theorem 4 Fix $\\boldsymbol{q} \\notin \\mathcal{P}^{m m}$ with unit $\\ell_{2}$ norm. Assume that token scores are distinct (namely $\\boldsymbol{\\gamma}_{i t} \\neq \\boldsymbol{\\gamma}_{i \\tau}$ for $t \\neq \\tau$ ) and key embeddings $\\boldsymbol{k}_{\\text {it }}$ are in general position (see Theorem 7). Fix arbitrary $\\epsilon>0, R_{0}>0$. Define the local regularization path of $\\boldsymbol{q}$ as its $\\left(\\epsilon, R_{0}\\right)$-conic neighborhood:\n\n$$\n\\overline{\\boldsymbol{p}}(R)=\\underset{\\boldsymbol{p} \\in \\mathcal{C}_{\\epsilon, R_{0}}(\\boldsymbol{q})\\|\\boldsymbol{p}\\| \\leq R}{\\arg \\min } \\mathcal{L}(\\boldsymbol{p}), \\text { where } C_{\\epsilon, R_{0}}(\\boldsymbol{q})=\\operatorname{cone}_{\\epsilon}(\\boldsymbol{q}) \\cap\\left\\{\\boldsymbol{p} \\in \\mathbb{R}^{d} \\mid\\|\\boldsymbol{p}\\| \\geq R_{0}\\right\\}\n$$\n\nThen, either $\\lim _{R \\rightarrow \\infty}\\|\\overline{\\boldsymbol{p}}(R)\\|<\\infty$ or $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) /\\|\\overline{\\boldsymbol{p}}(R)\\| \\neq \\boldsymbol{q}$. In both scenarios $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) / R \\neq \\boldsymbol{q}$. The result above nicely complements Theorem 3 , which states that when gradient descent is initialized above a threshold $\\left(\\|\\boldsymbol{p}(0)\\| \\geq R_{0}\\right)$ in an LMM direction, $\\|\\boldsymbol{p}(t)\\|$ diverges but the direction converges to LMM. In contrast, Theorem 4 shows that regardless of how small the cone is (in terms of angle and norm lower bound $\\|\\boldsymbol{p}\\| \\geq R_{0}$ ), the optimal solution path will not converge along $\\boldsymbol{q} \\notin \\mathcal{P}^{\\mathrm{mm}}$. ## 3 Joint Convergence of Head $\\boldsymbol{v}$ and Attention Weights $p$\n\nIn this section, we extend the preceding results to the general case of joint optimization of head $v$ and attention weights $\\boldsymbol{p}$ using a logistic loss function. To this aim, we focus on regularization path analysis, which involves solving (ERM) under ridge constraints and examining the solution trajectory as the constraints are relaxed. High-level intuition. Since the prediction is linear as a function of $\\boldsymbol{v}$, logistic regression in $v$ can exhibit its own implicit bias to a max-margin solution. Concretely, define the attention features $\\boldsymbol{x}_{i}^{p}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$ and define the dataset $\\mathcal{D}^{p}=\\left(Y_{i}, \\boldsymbol{x}_{i}^{p}\\right)_{i=1}^{n}$. If this dataset $\\mathcal{D}^{p}$ is linearly separable, then fixing $\\boldsymbol{p}$ and optimizing only $\\boldsymbol{v}$ will converge in the direction of the standard max-margin classifier\n\n$$\n\\boldsymbol{v}^{\\mathrm{mm}}=\\arg \\min _{\\boldsymbol{v} \\in \\mathbb{R}^{d}}\\|\\boldsymbol{v}\\| \\quad \\text { subject to } \\quad Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{r}_{i} \\geq 1, \\text { for all } 1 \\leq i \\leq n\n$$\n\nafter setting inputs to the attention features $\\boldsymbol{r}_{i} \\leftarrow \\boldsymbol{x}_{i}^{\\boldsymbol{p}}$ [22]. This motivates a clear question:\nUnder what conditions, optimizing $\\boldsymbol{v}, \\boldsymbol{p}$ jointly will converge to their respective max-margin solutions? We study this question in two steps. Loosely speaking: (1) We will first assume that, at the optimal\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-07.jpg?height=410&width=1384&top_left_y=253&top_left_x=339)\n\nFigure 3: (a) and (b) Joint convergence of attention weights $\\boldsymbol{p}(\\longrightarrow-)$ and classifier head $\\boldsymbol{v}(->)$ to max-margin directions. (c) Averaged softmax probability evolution of optimal tokens and logistic probability evolution of output in (a). tokens $\\boldsymbol{x}_{i \\alpha_{i}}, i \\in[n]$ selected by $\\boldsymbol{p}$, when solving (SVM) with $\\boldsymbol{r}_{i} \\leftarrow \\boldsymbol{x}_{i \\alpha_{i}}$, all of these tokens become support vectors of (SVM). (2) We will then relax this condition to uncover a more general implicit bias for $\\boldsymbol{p}$ that distinguish support vs non-support vectors. Throughout, we assume that the joint problem is separable and there exists $(\\boldsymbol{v}, \\boldsymbol{p})$ asymptotically achieving zero training loss. ### 3.1 When all attention features are support vectors\n\nIn (SVM), define label margin to be $1 /\\left\\|\\boldsymbol{v}^{\\mathrm{mm}}\\right\\|$. Our first insight in quantifying the joint implicit bias is that, optimal tokens admit a natural definition: Those that maximize the downstream label margin when selected. This is formalized below where we assume that: (1) Selecting the token indices $\\alpha=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ from each input data achieves the largest label margin. (2) The optimality of the $\\boldsymbol{\\alpha}$ choice is strict in the sense that mixing other tokens will shrink the label margin in (SVM). Assumption C (Optimal Tokens) Let $\\Gamma>0$ be the label margin when solving (SVM) with $\\boldsymbol{r}_{i} \\leftarrow \\boldsymbol{x}_{i \\alpha_{i}}$. There exists $v>0$ such that for all $\\boldsymbol{p}$, solving (SVM) with $\\boldsymbol{r}_{i} \\leftarrow \\boldsymbol{x}_{i}^{p}$ results in a label margin of at most $\\Gamma-v \\cdot \\max _{i \\in[n]}\\left(1-\\boldsymbol{s}_{i \\alpha_{i}}\\right)$ where $\\boldsymbol{s}_{i}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$. Example: To gain intuition, let us fix $\\boldsymbol{a} \\in \\mathbb{R}^{d}$ and consider the dataset obeying $\\boldsymbol{x}_{i 1}=Y_{i} \\cdot \\boldsymbol{a}$ and $\\left\\|\\boldsymbol{x}_{i t}\\right\\|<\\|\\boldsymbol{a}\\|$ for all $t \\geq 2$ and all $i \\in[n]$. For this dataset, we can choose $\\alpha_{i}=1, \\boldsymbol{v}^{\\mathrm{mm}}=\\boldsymbol{a} /\\|\\boldsymbol{a}\\|^{2}$, $\\Gamma=1 /\\left\\|\\boldsymbol{v}^{\\mathrm{mm}}\\right\\|=\\|\\boldsymbol{a}\\|$ and $v=\\|\\boldsymbol{a}\\|-\\sup _{i \\in[n], t \\geq 2}\\left\\|\\boldsymbol{x}_{i t}\\right\\|$. Theorem 5 Consider the ridge-constrained solutions $\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right.$ ) of (ERM) defined as\n\n$$\n\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right)=\\underset{\\|\\boldsymbol{v}\\| \\leq r,\\|\\boldsymbol{p}\\| \\leq R}{\\arg \\min } \\mathcal{L}(\\boldsymbol{v}, \\boldsymbol{p})\n$$\n\nSuppose there are token indices $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ for which $\\left\\|\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})\\right\\|$ exists (ATT-SVM is feasible) and Assumption C holds for some $\\Gamma, v>0$. Then, $\\lim _{R \\rightarrow \\infty} \\boldsymbol{p}_{R} / R=\\boldsymbol{p}^{m m} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$, where $\\boldsymbol{p}^{m m}$ is the solution of (ATT-SVM); and $\\lim _{r \\rightarrow \\infty} \\boldsymbol{v}_{r} / r=\\boldsymbol{v}^{\\mathrm{mm}} /\\left\\|\\boldsymbol{v}^{\\mathrm{mm}}\\right\\|$, where $\\boldsymbol{v}^{\\mathrm{mm}}$ is the solution of (SVM) with $\\boldsymbol{r}_{i}=\\boldsymbol{x}_{i \\alpha_{i}}$. As further discussion, consider Figure 3(a) where we set $n=3, T=d=2$ and $\\boldsymbol{W}=$ Identity. All three inputs share the point $(0,0)$ which corresponds to their non-optimal tokens. The optimal tokens (denoted by $\\star$ ) are all support vectors of the (SVM) since $\\boldsymbol{v}^{\\mathrm{mm}}=[0,1]$ is the optimal classifier direction (depicted by -- ). Because of this, $\\boldsymbol{p}^{\\mathrm{mm}}$ will separate optimal $\\star$ tokens from tokens at the $(0,0)$ coordinate via (ATT-SVM) and its direction is dictated by yellow and teal colored $\\star$ s which are the support vectors. ### 3.2 General solution when selecting one token per input\n\nCan we relax Assumption C, and if so, what is the resulting behavior? Consider the scenario where the optimal $\\boldsymbol{p}$ diverges to $\\infty$ and ends up selecting one token per input. Suppose this $\\boldsymbol{p}$ selects some coordinates $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$. Let $\\mathcal{S} \\subset[n]$ be the set of indices where the associated token $\\boldsymbol{x}_{i \\alpha_{i}}$ is a support vector when solving (SVM). Set $\\overline{\\mathcal{S}}=[n]-\\mathcal{S}$. Our intuition is as follows: Even if we slightly perturb this $\\boldsymbol{p}$ choice and mix other tokens $t \\neq \\alpha_{i}$ over the input set $\\overline{\\mathcal{S}} \\subset[n]$, since $\\overline{\\mathcal{S}}$ is not support vector for (SVM), we can preserve the label margin (by only preserving the support vectors $\\mathcal{S}$ ). This means that $\\boldsymbol{p}$ may not have to enforce max-margin constraint over inputs $i \\in \\mathcal{S}$, instead, it suffices to just select\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-08.jpg?height=407&width=972&top_left_y=252&top_left_x=317)\n\nFigure 4: Evolution of softmax probability and attention weights when training with normalized gradient descent or constant step size $\\eta$ respectively. ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-08.jpg?height=348&width=465&top_left_y=262&top_left_x=1296)\n\nFigure 5: Trajectories of $\\boldsymbol{p}$ with different loss functions and scores in Theorem 2. these tokens (asymptotically). This results in the following relaxed SVM problem:\n\n$$\n\\boldsymbol{p}^{\\text {relax }}=\\arg \\min _{\\boldsymbol{p}}\\|\\boldsymbol{p}\\| \\quad \\text { such that } \\quad \\boldsymbol{p}^{\\top}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right) \\geq\\left\\{\\begin{array}{lll}\n1 & \\text { for all } \\quad t \\neq \\alpha_{i}, i \\in \\mathcal{S} \\\\\n0 & \\text { for all } & t \\neq \\alpha_{i}, i \\in \\overline{\\mathcal{S}}\n\\end{array}\\right. $$\n\nHere, $\\boldsymbol{p}^{\\top}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right) \\geq 0$ corresponds to the selection idea. Building on this intuition, the following theorem captures the generalized behavior of the joint regularization path. Theorem 6 Consider the same (ERM) problem as discussed in Theorem 5. Suppose $\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)_{\\alpha_{i}} \\rightarrow 1$, i.e., the tokens $\\left(\\alpha_{i}\\right)_{i=1}^{n}$ are asymptotically selected. Let $\\boldsymbol{v}^{\\mathrm{mm}}$ be the solution of (SVM) with $\\boldsymbol{r}_{i}=\\boldsymbol{x}_{i \\alpha_{i}}$ and $\\mathcal{S}$ be its set of support vector indices. Suppose Assumption $C$ holds over $\\mathcal{S}$ i.e. having $\\boldsymbol{s}_{i \\alpha_{i}}<1$ shrinks the margin when (SVM) is only solved over $\\mathcal{S} \\subset[n]$. Then, $\\lim _{r \\rightarrow \\infty} \\boldsymbol{v}_{r} / r=v^{\\mathrm{mm}} /\\left\\|v^{m \\mathrm{~mm}}\\right\\|$ and $\\lim _{R \\rightarrow \\infty} \\boldsymbol{p}_{R} / R=\\boldsymbol{p}^{\\text {relax }} /\\left\\|\\boldsymbol{p}^{\\text {relax }}\\right\\|$, where $\\boldsymbol{p}^{\\text {relax }}$ is the solution of (10) with $\\left(\\alpha_{i}\\right)_{i=1}^{n}$ choices. To illustrate this numerically, consider Figure 3(b) which modifies Figure 3(a) by pushing the yellow $\\star$ to the northern position $(0.5,1.5)$. We still have $\\boldsymbol{v}^{\\mathrm{mm}}=[0,1]$ however the yellow $\\star$ is no longer a support vector of (SVM). Thus, $\\boldsymbol{p}$ solves the relaxed problem (10) which separates green and teal $\\star$ 's by enforcing the max-margin constraint on $\\boldsymbol{p}$ (which is the red direction). Instead, yellow $\\star$ only needs to achieve positive correlation with $\\boldsymbol{p}$ (unlike Figure 3(a) where it dictates the direction). We also display the direction of $\\boldsymbol{p}^{\\mathrm{mm}}$ using a gray dashed line. We further investigate the evolution of softmax and logistic output probabilities throughout the training process of Figure 3(a), and the results are illustrated in Figure 3(c). The averaged softmax probability of optimal tokens is represented by the red curve and is calculated as $\\frac{1}{n} \\sum_{i=1}^{n} \\max _{t \\in[T]} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)_{t}$. An achievement of 1 for this probability indicates that the attention mechanism successfully selects the optimal tokens. On the other hand, the logistic probability of the output is represented by the blue curve and is determined by $1 / n \\sum_{i=1}^{n} 1 /\\left(1+e^{-Y_{i} f\\left(X_{i}\\right)}\\right)$. This probability also reaches a value of 1 , suggesting that the inputs are correctly classified. ## 4 Experiments\n\nSparsity of softmax and evolution of attention weights. It is well known that, in practice, attention maps often exhibit sparsity and highlight salient tokens that aid inference. Our results provide a formal explanation of this when tokens are separable: Since attention selects a locally-optimal token within the input sequence and suppresses the rest, the associated attention map $\\mathbb{S}(\\boldsymbol{X} \\boldsymbol{p})$ will (eventually) be a sparse vector. Additionally, the sparsity should arise in tandem with the increasing norm of attention weights.",
    "maxmargin-6": "We provide empirical evidence to support these findings. Synthetic experiments. Figures 4(a) and 4(b) show the evolution of the largest softmax probability and attention weights over time when using either normalized gradient or a fixed stepsize $\\eta$ for training. The dataset model follows Figure 1(c). The softmax probability shown in Figure 4(a) is defined as $\\frac{1}{n} \\sum_{i=1}^{n} \\max _{t \\in[T]} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)_{t}$. When this average probability reaches the value of 1 , it means attention selects only a single token per input. The attention norm in Figure 4(b), is simply equal to $\\|\\boldsymbol{p}\\|$. The red curves in both figures represent the normalized gradient method, which updates the model parameters $\\boldsymbol{p}$ using $\\boldsymbol{p}(t+1)=\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t)) /\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|$ with $\\eta=0.1$. The blue curves correspond\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-09.jpg?height=574&width=711&top_left_y=248&top_left_x=359)\nFigure 6: Illustration of the progressive change in attention weights of the [CLS] token during training in the transformer model, using a specific input image shown in Figure 6(a). ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-09.jpg?height=397&width=583&top_left_y=257&top_left_x=1118)\n\nFigure 7: Red curve is the sparsity level $\\widehat{n n z}(s) /$ $T$ of the average attention map which takes values on $[0,1]$. A sparser vector implies that few key tokens receive significantly higher attention, while the majority of the tokens receive minimal attention. Blue curve is the Frobenius norm of attention weights $\\|W\\|_{F}$ of the final layer. We display their evolutions over epochs. to gradient descent with constant learning rate given by $\\boldsymbol{p}(t+1)=\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t))$ with $\\eta=1$. Observe that the normalized gradient method achieves a softmax probability of 1 quicker as vanilla GD suffers from vanishing gradients.",
    "maxmargin-7": "This is visible in Figure 4(b) where blue norm curve levels off. Real experiments. To study softmax sparsity and the evolution of attention weights throughout training, we train a vision transformer (ViT-base) model [23] from scratch, utilizing the CIFAR10 dataset [24] for 400 epochs with fixed learning rate $3 \\times 10^{-3}$. ViT tokenizes an image into $16 \\times 16$ patches, thus, its softmax attention maps can be easily visualized. We examine the average attention map - associated with the [CLS] token - computed from all 12 attention heads within the model. Figure 6 provides a visual representation of the resulting attention weights $(16 \\times 16$ grids) corresponding to the original patch locations within the image. During the initial epochs of training, the attention weights are randomly distributed and exhibit a dense pattern. However, as the training progresses, the attention map gradually becomes sparser and the attention mechanism begins to concentrate on fewer salient patches within the image that possess distinct features that aid classification. This illustrates the evolution of attention from a random initial state to a more focused and sparse representation. These salient patches highlighted by attention conceptually corresponds to the optimal tokens within our theory. We quantify the sparsity of the attention map via a soft-sparsity measure, denoted by $\\widehat{\\operatorname{nnz}}(\\boldsymbol{s})$ where $\\boldsymbol{s}$ is the softmax probability vector. The soft-sparsity is computed as the ratio of the $\\ell_{1}-$ norm to the squared $\\ell_{2}-$ norm, defined as $\\widehat{\\mathrm{nnz}}(\\boldsymbol{s})=\\|\\boldsymbol{s}\\|_{1} /\\|\\boldsymbol{s}\\|^{2}$. $\\widehat{\\mathrm{nnz}}(s)$ takes values between 1 to $T=256$ and a smaller value indicates a sparser vector. Also note that $\\|s\\|_{1}=\\sum_{t=1}^{T} s_{t}=1$. Together with sparsity, Figure 7 also displays the Frobenius norm of the combined key-query matrix $\\boldsymbol{W}$ of the last attention layer over epochs. The theory suggests that the increase in sparsity is associated with the growth of attention weights - which converge directionally. The results in Figure 7 align with the theory, demonstrating the progressive sparsification of the attention map as $\\|\\boldsymbol{W}\\|_{F}$ grows. Transient optimization dynamics and the influence of the loss function. Theorem 2 shows that the asymptotic direction of gradient descent is determined by $\\boldsymbol{p}^{m m \\star}$. However, it is worth noting that transient dynamics can exhibit bias towards certain input examples and their associated optimal tokens. We illustrate this idea in Fig 5(a), which displays the trajectories of the gradients for different scores and loss functions. We consider two optimal tokens ( $\\star$ ) with scores $\\gamma_{1}=1$ and $\\gamma_{2}=C$, where $C$ varies. For our analysis, we examine the correlation loss $\\ell(x)=-x$ and the logistic loss $\\ell(x)=\\log \\left(1+e^{-x}\\right)$. In essence, as $C$ increases, we can observe that the correlation loss $\\ell(x)=-x$ exhibits a bias towards the token with a high score, while the logistic loss is biased towards the token with a low score. The underlying reason for this behavior can be observed from the gradients of individual inputs: $\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})=\\ell_{i}^{\\prime} \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{X} \\boldsymbol{p}) \\boldsymbol{X} \\boldsymbol{v}$, where $\\mathbb{S}^{\\prime}(\\cdot)$ represents the derivative of the softmax function and $\\ell_{i}^{\\prime}:=\\ell^{\\prime}\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)\\right)$. Assuming that $\\boldsymbol{p}$ (approximately) selects the optimal tokens, this\nsimplifies to $\\ell_{i}^{\\prime} \\approx \\ell^{\\prime}\\left(\\gamma_{i}\\right)$ and $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto\\left|\\ell^{\\prime}\\left(\\gamma_{i}\\right)\\right| \\cdot \\gamma_{i}$. With the correlation loss, $\\left|\\ell^{\\prime}\\right|=1$, resulting in $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto \\boldsymbol{\\gamma}_{i}$, meaning that a larger score induces a larger gradient. On the other hand, the logistic loss behaves similarly to the exponential loss under separable data, i.e., $\\left|\\ell^{\\prime}\\right|=e^{-x} /\\left(1+e^{-x}\\right) \\approx e^{-x}$. Consequently, $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto \\gamma_{i} e^{-\\gamma_{i}} \\approx e^{-\\gamma_{i}}$, indicating that a smaller score leads to a larger gradient. These observations explain the empirical behavior we observe. ## 5 Related Work\n\nImplicit Regularization. The implicit bias of gradient descent in classification tasks involving separable data has been extensively examined by [22, 25, 26, 27, 28, 29]. These works typically use logistic loss or, more generally, exponentially-tailed losses to make connections to margin maximization. These results are also extended to non-separable data by [30, 31, 21]. Furthermore, there have been notable investigations into the implicit bias in regression problems/losses utilizing techniques such as mirror descent $[32,25,33,34,35,36]$. In addition, several papers have explored the implicit bias of stochastic gradient descent [37,38, 39, 40, 41, 42], as well as adaptive and momentum-based methods [43, 44, 45, 46]. Although there are similarities between our optimization approach for $\\boldsymbol{v}$ and existing works, the optimization of $\\boldsymbol{p}$ stands out as significantly different. Firstly, our optimization problem is nonconvex, introducing new challenges and complexities. Secondly, it necessitates the introduction of novel concepts such as locally-optimal tokens and requires a fresh analysis specifically tailored to the cones surrounding them. Attention Mechanism. Transformers, introduced by [6], revolutionized the field of NLP and machine translation, with earlier works on self-attention by [47, 48, 49, 50]. Self-attention differs from traditional models like MLPs and CNNs by leveraging global interactions for feature representations, showing exceptional empirical performance. However, the underlying mechanisms and learning processes of the attention layer remain unknown. Recent studies such as [51, 52, 53, 54, 23] have focused on specific aspects like representing sparse functions, convex-relaxations, and expressive power. In contrast to our nonconvex (ERM), [52] studies self-attention with linear activation instead of softmax, while [53] approximates softmax using a linear operation with unit simplex constraints. Their main objective is to derive convex reformulations for ERM-based training problem. [55, 56] have developed initial results to characterize the optimization and generalization dynamics of attention. [17] is another closely related work where the authors analyze the same attention model (ERM) as us. Specifically, they jointly optimize $\\boldsymbol{v}, \\boldsymbol{p}$ for three gradient iterations for a contextual dataset model. However, all of these works make stringent assumptions on the data, namely, tokens are tightly clusterable or can be clearly split into clear relevant and irrelevant sets. Additionally [56] requires assumptions on initialization and [55] considers a simplified attention structure where the attention matrix is not directly parameterized with respect to the input. Our work links attention models to hard-margin SVM problems and pioneers the study of gradient descent's implicit bias in these models. ## 6 Discussion\n\nWe have provided a thorough optimization-theoretic characterization of the fundamental attention model $f(\\boldsymbol{X})=\\boldsymbol{v}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{p})$ by formally connecting it to max-margin problems. We first established the convergence of gradient descent on $\\boldsymbol{p}$ (or equivalently $\\boldsymbol{W}$ ) in isolation. We also explored joint convergence of $(\\boldsymbol{v}, \\boldsymbol{p})$ via regularization path which revealed surprising implicit biases such as (10). These findings motivate several exciting avenues for future research. An immediate open problem is characterizing the (local) convergence of gradient descent for joint optimization of ( $\\boldsymbol{v}, \\boldsymbol{p}$ ). Another major direction is to extend similar analysis to study self-attention layer (4) or to allow for multiple tunable tokens (where $\\boldsymbol{p}$ becomes a matrix). Either setting will enrich the problem by allowing the attention to discover multiple hyperplanes to separate tokens. While our convergence guarantees apply when tokens are separable, it would be interesting to characterize the non-separable geometry by leveraging results developed for logistic regression analysis [31, 22]. Ideas from such earlier results can also be useful for characterizing the non-asymptotic/transient dynamics of how gradient descent aligns with the max-margin direction. Overall, we believe that max-margin token selection is a fundamental characteristic of attention mechanism and the theory developed in this work lays the groundwork of these future extensions. ## Acknowledgements\n\nThis work was supported by the NSF grants CCF-2046816 and CCF-2212426, Google Research Scholar award, and Army Research Office grant W911NF2110312. The authors express their gratitude for the valuable feedback provided by the anonymous reviewers and Christos Thrampoulidis, which has significantly improved this paper. ## References\n\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. The International Conference on Learning Representations, 2015. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. In Advances in neural information processing systems, volume 33, pages 1877 -1901, 2020. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [8] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.",
    "maxmargin-8": "arXiv preprint arXiv:2108.07258, 2021. [10] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR, 2021. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision.",
    "maxmargin-9": "In International conference on machine learning, pages 8748-8763. PMLR, 2021. [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model.",
    "maxmargin-10": "arXiv preprint arXiv:2303.03378, 2023. [14] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.",
    "maxmargin-11": "In Advances in Neural Information Processing Systems, volume 34, pages 15084-15097, 2021. [15] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.",
    "maxmargin-12": "arXiv preprint arXiv:2205.06175, 2022. [16] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, 2021. [17] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. In International Conference on Machine Learning, 2023. [18] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.",
    "maxmargin-13": "arXiv preprint arXiv:2101.00190, 2021. [19] Saharon Rosset, Ji Zhu, and Trevor Hastie. Margin maximizing loss functions. Advances in neural information processing systems, 16, 2003. [20] Arun Suggala, Adarsh Prasad, and Pradeep K Ravikumar. Connecting optimization and regularization paths. Advances in Neural Information Processing Systems, 31, 2018. [21] Ziwei Ji, Miroslav Dud\u00edk, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses.",
    "maxmargin-14": "In Conference on Learning Theory, pages 2109-2136. PMLR, 2020. [22] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data.",
    "maxmargin-15": "The Journal of Machine Learning Research, 19(1):2822-2878, 2018. [23] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth.",
    "maxmargin-16": "In International Conference on Machine Learning, pages 2793-2803. PMLR, 2021. [24] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset.",
    "maxmargin-17": "online: http://www. cs. toronto. edu/kriz/cifar. html, 55(5), 2014. [25] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry.",
    "maxmargin-18": "In International Conference on Machine Learning, pages 1832-1841. PMLR, 2018. [26] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data.",
    "maxmargin-19": "In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420-3428. PMLR, 2019. [27] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, pages 772-804. PMLR, 2021. [28] Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy.",
    "maxmargin-20": "Advances in neural information processing systems, 33:22182-22193, 2020. [29] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.",
    "maxmargin-21": "F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17176-17186. Curran Associates, Inc., 2020\n[30] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018. [31] Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In Conference on Learning Theory, pages 1772-1798. PMLR, 2019. [32] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.",
    "maxmargin-22": "In Conference on Learning Theory, pages 3635-3673. PMLR, 2020. [33] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural networks.",
    "maxmargin-23": "arXiv preprint arXiv:2010.02501, 2020. [34] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. Advances in Neural Information Processing Systems, 32:2972-2983, 2019. [35] Ehsan Amid and Manfred K Warmuth. Winnowing with gradient descent. In Conference on Learning Theory, pages 163-182. PMLR, 2020. [36] Ehsan Amid and Manfred KK Warmuth. Reparameterizing mirror descent as gradient descent.",
    "maxmargin-24": "Advances in Neural Information Processing Systems, 33:8430-8439, 2020. [37] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019. [38] Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process.",
    "maxmargin-25": "In Conference on learning theory, pages 483-513. PMLR, 2020. [39] Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the noise covariance.",
    "maxmargin-26": "arXiv preprint arXiv:2006.08680, 2020. [40] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after SGD reaches zero loss? -a mathematical framework. In International Conference on Learning Representations, 2022. [41] Alex Damian, Tengyu Ma, and Jason Lee. Label noise sgd provably prefers flat global minimizers. arXiv preprint arXiv:2106.06530, 2021. [42] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster, and Sham Kakade. The benefits of implicit regularization from sgd in least squares problems. Advances in Neural Information Processing Systems, 34:5456-5468, 2021. [43] Qian Qian and Xiaoyuan Qian. The implicit bias of adagrad on separable data. Advances in Neural Information Processing Systems, 32, 2019. [44] Bohan Wang, Qi Meng, Huishuai Zhang, Ruoyu Sun, Wei Chen, and Zhi-Ming Ma. Momentum doesn't change the implicit bias. arXiv preprint arXiv:2110.03891, 2021. [45] Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. The implicit bias for adaptive optimization algorithms on homogeneous neural networks. In International Conference on Machine Learning, pages 10849-10858. PMLR, 2021. [46] Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration.",
    "maxmargin-27": "In International Conference on Machine Learning, pages 4860-4869. PMLR, 2021. [47] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading.",
    "maxmargin-28": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 551-561, Austin, Texas, November 2016. Association for Computational Linguistics. [48] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249-2255, Austin, Texas, November 2016. Association for Computational Linguistics. [49] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. In International Conference on Learning Representations, 2018. [50] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In International Conference on Learning Representations, 2017. [51] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms.",
    "maxmargin-29": "In International Conference on Machine Learning, pages 5793-5831. PMLR, 2022. [52] Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers.",
    "maxmargin-30": "In International Conference on Machine Learning, pages 19050-19088. PMLR, 2022. [53] Tolga Ergen, Behnam Neyshabur, and Harsh Mehta. Convexifying transformers: Improving optimization and understanding of transformer networks.",
    "maxmargin-31": "arXiv:2211.11052, 2022. [54] Pierre Baldi and Roman Vershynin. The quarks of attention.",
    "maxmargin-32": "arXiv:2202.08371, 2022. [55] Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [56] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015, 2023. [57] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks.",
    "maxmargin-33": "In International Conference on Machine Learning, pages $1675-1685$. PMLR, 2019. [58] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018. [59] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1(1):84-105, 2020. [60] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization.",
    "maxmargin-34": "In International Conference on Machine Learning, pages 242-252. PMLR, 2019. [61] Vladimir Vapnik. Estimation of dependences based on empirical data. Springer Science \\& Business Media, 2006. [62] Peter Bartlett. For valid generalization the size of the weights is more important than the size of the network. Advances in neural information processing systems, 9, 1996. [63] Albert B Novikoff. On convergence proofs for perceptrons. Technical report, STANFORD RESEARCH INST MENLO PARK CA, 1963. [64] Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E Schapire. Boosting the margin: A new explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651-1686, 1998. [65] Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, page 1538, 2005. [66] Matus Telgarsky. Margins, shrinkage, and boosting. In International Conference on Machine Learning, pages 307-315. PMLR, 2013. [67] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization. Advances in Neural Information Processing Systems, 34:18970-18983, 2021. [68] Mahdi Soltanolkotabi, Dominik St\u00f6ger, and Changzhi Xie. Implicit balancing and regularization: Generalization and convergence guarantees for overparameterized asymmetric matrix sensing. arXiv:2303.14244, 2023. [69] Hossein Taheri and Christos Thrampoulidis. On generalization of decentralized learning with separable data. In International Conference on Artificial Intelligence and Statistics, pages 4917-4945. PMLR, 2023. [70] Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the shortest path?",
    "maxmargin-35": "In International Conference on Machine Learning, pages 4951-4960. PMLR, 2019. [71] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks.",
    "maxmargin-36": "arXiv preprint arXiv:1810.02032, 2018. [72] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization.",
    "maxmargin-37": "Advances in Neural Information Processing Systems, 32, 2019. [73] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv preprint arXiv:1906.05890, 2019. [74] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on Learning Theory, pages 1305-1338. PMLR, 2020. [75] Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. arXiv e-prints, pages arXiv-2303, 2023. [76] Gal Vardi, Ohad Shamir, and Nati Srebro. On margin maximization in linear and relu networks. Advances in Neural Information Processing Systems, 35:37024-37036, 2022. [77] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized nonlinear models. IEEE Transactions on Neural Networks and Learning Systems, 33(12):77177727, 2021. [78] Navid Azizan and Babak Hassibi. Stochastic gradient/mirror descent: Minimax optimality and implicit regularization.",
    "maxmargin-38": "In International Conference on Learning Representations. [79] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction.",
    "maxmargin-39": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery $\\mathcal{E}$ Data Mining, pages 1059-1068, 2018. [80] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data, pages 1-4, 2019. [81] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 1441-1450, 2019. [82] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combining recent advances in neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76-86, Melbourne, Australia, July 2018. Association for Computational Linguistics. [83] Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. In ICML 2021 Workshop on Unsupervised Reinforcement Learning, 2021. [84] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In Proceedings of the 39th International Conference on Machine Learning, 2022. [85] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. [86] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers \\& distillation through attention. In International Conference on Machine Learning, pages 10347-10357. PMLR, 2021. [87] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter: Token labeling for training better vision transformers.",
    "maxmargin-40": "In Advances in Neural Information Processing Systems, volume 34, pages 18590-18602, 2021. [88] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention.",
    "maxmargin-41": "In International Conference on Machine Learning, pages 5562-5571. PMLR, 2021. [89] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks.",
    "maxmargin-42": "In International Conference on Machine Learning, pages 4376-4386. PMLR, 2020. [90] Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture.",
    "maxmargin-43": "arXiv preprint arXiv:2006.14548, 2020. [91] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2018. [92] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2019. [93] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv:2301.13196, 2023. [94] Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. Limits to depth efficiencies of self-attention. In Advances in Neural Information Processing Systems, volume 33, pages 22640-22651, 2020. [95] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. arXiv preprint arXiv:2103.07601, 2021. [96] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners.",
    "maxmargin-44": "arXiv preprint arXiv:2109.01652, 2021. [97] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv:2211.15661, 2022. [98] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022. [99] Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, 2023. [100] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. [101] Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. arXiv preprint arXiv:2305.15408, 2023. [102] Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak. Dissecting chain-of-thought: A study on compositional in-context learning of mlps. arXiv preprint arXiv:2305.18869, 2023. [103] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv:2305.16380, 2023. [104] Tan Minh Nguyen, Tam Minh Nguyen, Nhat Ho, Andrea L Bertozzi, Richard Baraniuk, and Stanley Osher. A primal-dual framework for transformers and neural networks. In The Eleventh International Conference on Learning Representations, 2023. [105] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines.",
    "maxmargin-45": "arXiv preprint arXiv:2308.16898, 2023. Roadmap. The appendix is organized as follows: Section A provides basic facts about the training risk. Section B presents the proof of local and global gradient descent and regularized path for learning $\\boldsymbol{p} \\in \\mathbb{R}^{d}$ with a fixed $\\boldsymbol{v} \\in \\mathbb{R}^{d}$ choice. Section $C$ provides the proof of regularized path applied to the general case of joint optimization of head $v$ and attention weights $p$ using a logistic loss function. Section D presents the regularized path applied to a more general model $f(\\boldsymbol{X})=\\psi\\left(\\boldsymbol{X}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X} \\boldsymbol{W}^{\\top} \\boldsymbol{p}\\right)\\right)$ with a nonlinear head $\\psi$. Section E provides implementation details. Finally, Section F discusses additional related work on implicit bias and self-attention. ## Table of Contents\n\nA Addendum to Section 1 ..... 17\nA. 1 Preliminaries on the Training Risk ..... 17\nA. 2 Proof of Lemma 1 ..... 19\nB Addendum to Section 2 ..... 20\nB. 1 Descent and Gradient Correlation Conditions ..... 20\nB. 2 Proof of Theorem 1 ..... 28\nB. 3 Proof of Theorem 2 ..... 29\nB. 4 Proof of Theorem 3 ..... 29\nB. 5 Proof of Theorem 4 ..... 32\nB. 6 Proof of Lemma 2 ..... 38\nC Addendum to Section 3 ..... 38\nC. 1 Proof of Theorem 5 ..... 38\nC. 2 Proof of Theorem 6 ..... 40\nD Regularization Path of Attention with Nonlinear Head .....",
    "maxmargin-46": "42\nD. 1 Proof of Theorem 8 ..... 43\nD. 2 Application to Linearly-mixed Labels ..... 44\nE Implementation Details and Additional Experiments ..... 45\nF Addendum to Section 5 ..... 47\nF. 1 Related Work on Implicit Regularization ..... 47\nF. 2 Related Work on Attention Mechanism ..... 47\n\n## A Addendum to Section 1\n\n## A. 1 Preliminaries on the Training Risk\n\nBy our assumption $\\psi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ and $\\ell: \\mathbb{R} \\rightarrow \\mathbb{R}$ are differentiable functions. Recall the objective\n\n$$\n\\mathcal{L}(\\boldsymbol{p}, \\boldsymbol{W})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)\\right)\n$$\n\nwith the generic prediction model $\\psi\\left(\\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p})\\right)$ and $\\boldsymbol{K}=\\boldsymbol{X} \\boldsymbol{W}^{\\top}$. Here, we write down the gradients of $\\boldsymbol{W}$ and $\\boldsymbol{p}$ in (11) to highlight the connection. Set $\\boldsymbol{q}:=\\boldsymbol{W}^{\\top} \\boldsymbol{p}$, $\\boldsymbol{z}\\{\\boldsymbol{X}\\}:=\\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p})$, and $\\boldsymbol{a}\\{\\boldsymbol{X}\\}:=\\boldsymbol{K} \\boldsymbol{p}$. Given $\\boldsymbol{X}$ and using $\\boldsymbol{K}=\\boldsymbol{X} \\boldsymbol{W}^{\\top}$, we have that\n\n$$\n\\begin{aligned}\n\\nabla_{\\boldsymbol{q}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) & =\\boldsymbol{X}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{a}\\{\\boldsymbol{X}\\}) \\boldsymbol{X} \\cdot \\psi^{\\prime}(\\boldsymbol{z}\\{\\boldsymbol{X}\\}) \\\\\n\\nabla_{\\boldsymbol{p}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) & =\\boldsymbol{W} \\nabla_{\\boldsymbol{q}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) \\\\\n\\nabla_{\\boldsymbol{W}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) & =\\boldsymbol{p} \\nabla_{\\boldsymbol{q}}^{\\top} \\psi(\\boldsymbol{p}, \\boldsymbol{W})\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\mathbb{S}^{\\prime}(\\boldsymbol{a}\\{\\boldsymbol{X}\\})=\\operatorname{diag}(\\mathbb{S}(\\boldsymbol{a}\\{\\boldsymbol{X}\\}))-\\mathbb{S}(\\boldsymbol{a}\\{\\boldsymbol{X}\\}) \\mathbb{S}(\\boldsymbol{a}\\{\\boldsymbol{X}\\})^{\\top} \\in \\mathbb{R}^{T \\times T}\n$$\n\nSetting $\\psi(z)=\\boldsymbol{v}^{\\top} z$ for linear head, we obtain\n\n$$\n\\begin{aligned}\n\\nabla_{q} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) & =\\boldsymbol{X}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{a}\\{\\boldsymbol{X}\\}) \\gamma \\\\\n\\nabla_{\\boldsymbol{p}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) & =\\boldsymbol{W} \\nabla_{\\boldsymbol{q}} \\psi(\\boldsymbol{p}, \\boldsymbol{W})=\\boldsymbol{K}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{a}\\{\\boldsymbol{X}\\}) \\gamma \\\\\n\\nabla_{\\boldsymbol{W}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) & =\\boldsymbol{p} \\nabla_{\\boldsymbol{q}}^{\\top} \\psi(\\boldsymbol{p}, \\boldsymbol{W})=\\boldsymbol{p} \\boldsymbol{v}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{a}\\{\\boldsymbol{X}\\}) \\boldsymbol{X}\n\\end{aligned}\n$$\n\nRecalling (12b) and (12c), and defining $\\ell_{i}^{\\prime}:=\\ell^{\\prime}\\left(Y_{i} \\cdot \\psi\\left(z\\left\\{\\boldsymbol{X}_{i}\\right\\}\\right)\\right) \\in \\mathbb{R}$, we have that\n\n$$\n\\begin{aligned}\n& \\nabla_{\\boldsymbol{p}} \\mathcal{L}(\\boldsymbol{p}, \\boldsymbol{W})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot Y_{i} \\cdot \\boldsymbol{W} \\nabla_{\\boldsymbol{q}} \\psi(\\boldsymbol{p}, \\boldsymbol{W}) \\\\\n& \\nabla_{\\boldsymbol{W}} \\mathcal{L}(\\boldsymbol{p}, \\boldsymbol{W})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot Y_{i} \\cdot \\boldsymbol{p} \\nabla_{\\boldsymbol{q}}^{\\top} \\psi(\\boldsymbol{p}, \\boldsymbol{W})\n\\end{aligned}\n$$\n\nSetting $\\psi(z)=\\boldsymbol{v}^{\\top} \\boldsymbol{z}$ for linear head and $\\boldsymbol{\\gamma}_{i}=Y_{i} \\cdot \\boldsymbol{X}_{i} \\boldsymbol{v}$, we obtain\n\n$$\n\\begin{aligned}\n& \\nabla_{p} \\mathcal{L}(\\boldsymbol{p}, \\boldsymbol{W})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}\\left\\{\\boldsymbol{X}_{i}\\right\\}\\right) \\boldsymbol{\\gamma}_{i} \\\\\n& \\nabla_{\\boldsymbol{W}} \\mathcal{L}(\\boldsymbol{p}, \\boldsymbol{W})=\\boldsymbol{p}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}\\left\\{\\boldsymbol{X}_{i}\\right\\}\\right) \\boldsymbol{X}_{i}\\right)\n\\end{aligned}\n$$\n\nLemma 3 (Key Lemma) For any $\\boldsymbol{p}, \\boldsymbol{q} \\in \\mathbb{R}^{d}$, let $\\boldsymbol{a}=\\boldsymbol{K q}, \\boldsymbol{s}=\\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p})$, and $\\boldsymbol{\\gamma}=$ Xv.",
    "maxmargin-47": "Set\n\n$$\n\\Gamma=\\sup _{t, \\tau \\in[T]}\\left|\\boldsymbol{\\gamma}_{t}-\\boldsymbol{\\gamma}_{\\tau}\\right| \\quad \\text { and } \\quad A=\\sup _{t \\in[T]}\\left\\|\\boldsymbol{k}_{t}\\right\\| \\cdot\\|\\boldsymbol{q}\\| \\text {. }\n$$\n\nWe have that\n\n$$\n\\left|\\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma}-\\sum_{t \\geq 2}^{T}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}\\left(\\boldsymbol{\\gamma}_{1}-\\boldsymbol{\\gamma}_{t}\\right)\\right| \\leq 2 \\Gamma A\\left(1-\\boldsymbol{s}_{1}\\right)^{2}\n$$\n\nProof. Set $\\bar{\\gamma}=\\sum_{t=1}^{T} \\gamma_{t} s_{t}$. We have\n\n$$\n\\gamma_{1}-\\bar{\\gamma}=\\sum_{t \\geq 2}^{T}\\left(\\gamma_{1}-\\gamma_{t}\\right) s_{t}, \\text { and }\\left|\\gamma_{1}-\\bar{\\gamma}\\right| \\leq \\Gamma\\left(1-s_{1}\\right)\n$$\n\nThen,\n\n$$\n\\begin{aligned}\n\\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma} & =\\sum_{t=1}^{T} \\boldsymbol{a}_{t} \\boldsymbol{\\gamma}_{t} \\boldsymbol{s}_{t}-\\sum_{t=1}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t} \\sum_{t=1}^{T} \\boldsymbol{\\gamma}_{t} \\boldsymbol{s}_{t} \\\\\n& =\\boldsymbol{a}_{1} \\boldsymbol{s}_{1}\\left(\\gamma_{1}-\\bar{\\gamma}\\right)-\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}\\left(\\bar{\\gamma}-\\gamma_{t}\\right)\n\\end{aligned}\n$$\n\nSince\n\n$$\n\\left|\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}\\left(\\bar{\\gamma}-\\boldsymbol{\\gamma}_{t}\\right)-\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}\\left(\\boldsymbol{\\gamma}_{1}-\\boldsymbol{\\gamma}_{t}\\right)\\right| \\leq A \\Gamma\\left(1-\\boldsymbol{s}_{1}\\right)^{2}\n$$\n\nwe obtain ${ }^{2}$\n\n$$\n\\begin{aligned}\n\\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma} & =\\boldsymbol{a}_{1} \\boldsymbol{s}_{1}\\left(\\gamma_{1}-\\bar{\\gamma}\\right)-\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right) \\pm A \\Gamma\\left(1-\\boldsymbol{s}_{1}\\right)^{2} \\\\\n& =\\boldsymbol{a}_{1} \\boldsymbol{s}_{1} \\sum_{t \\geq 2}^{T}\\left(\\gamma_{1}-\\gamma_{t}\\right) \\boldsymbol{s}_{t}-\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right) \\pm A \\Gamma\\left(1-\\boldsymbol{s}_{1}\\right)^{2} \\\\\n& =\\sum_{t \\geq 2}^{T}\\left(\\boldsymbol{a}_{1} \\boldsymbol{s}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right) \\pm A \\Gamma\\left(1-\\boldsymbol{s}_{1}\\right)^{2} \\\\\n& =\\sum_{t \\geq 2}^{T}\\left(a_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right) \\pm 2 A \\Gamma\\left(1-\\boldsymbol{s}_{1}\\right)^{2}\n\\end{aligned}\n$$\n\nHere, $\\pm$ on the right handside uses the fact that\n\n$$\n\\left|\\sum_{t \\geq 2}^{T}\\left(a_{1} s_{1}-a_{1}\\right) s_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right)\\right| \\leq\\left(1-s_{1}\\right) A \\Gamma \\sum_{t \\geq 2}^{T} s_{t}=\\left(1-s_{1}\\right)^{2} A \\Gamma\n$$\n\n## A.",
    "maxmargin-48": "2 Proof of Lemma 1\n\nProof. Let us prove the result for a general step size sequence $\\left(\\eta_{t}\\right)_{t \\geq 0}$. On the same training data $\\left(Y_{i}, \\boldsymbol{X}_{i}\\right)_{i=1}^{n}$, recall the objectives $\\tilde{\\mathcal{L}}(\\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)\\right)\\right)$ and $\\mathcal{L}(\\boldsymbol{W})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i}\\right.$. $\\left.\\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}^{\\top} \\boldsymbol{u}\\right)\\right)\\right)$. Suppose claim is true till iteration $t$. For iteration $t+1$, using $\\boldsymbol{W}(t)^{\\top} \\boldsymbol{u}=\\boldsymbol{p}(t)$, define and observe that\n\n$$\n\\begin{aligned}\n& \\boldsymbol{S}_{i}=\\mathbb{S}^{\\prime}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}(t)^{\\top} \\boldsymbol{u}\\right)=\\mathbb{S}^{\\prime}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}(t)\\right) \\\\\n& \\boldsymbol{s}_{i}=\\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}(t)^{\\top} \\boldsymbol{u}\\right)=\\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}(t)\\right) \\\\\n& \\boldsymbol{z}\\left\\{\\boldsymbol{X}_{i}\\right\\}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}(t)\\right)=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{W}(t)^{\\top} \\boldsymbol{u}\\right)\n\\end{aligned}\n$$\n\nfor all $i \\in[n]$. Thus, using (14), we have that\n\n$$\n\\begin{aligned}\n\\nabla \\tilde{\\mathcal{L}}(\\boldsymbol{p}(t)) & =\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot Y_{i} \\cdot \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{S}_{i} \\boldsymbol{X}_{i} \\cdot \\psi^{\\prime}\\left(z\\left\\{\\boldsymbol{X}_{i}\\right\\}\\right) \\\\\n\\nabla \\mathcal{L}(\\boldsymbol{W}(t)) & =\\boldsymbol{u}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot Y_{i} \\cdot \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{S}_{i} \\boldsymbol{X}_{i} \\cdot \\psi^{\\prime}\\left(z\\left\\{\\boldsymbol{X}_{i}\\right\\}\\right)\\right)^{\\top}\n\\end{aligned}\n$$\n\nConsequently, we found that gradient is rank-1 with left singular space equal to $\\boldsymbol{u}$, i.e.,\n\n$$\n\\nabla \\mathcal{L}(\\boldsymbol{W}(t))=\\boldsymbol{u} \\nabla^{\\top} \\tilde{\\mathcal{L}}(\\boldsymbol{p}(t))\n$$\n\nSince $\\boldsymbol{W}(t)$ 's left singular space is guaranteed to be in $\\boldsymbol{u}$ (including $\\boldsymbol{W}(0)$ by initialization), we only need to study the right singular vector. Using the induction till $t$, this yields\n\n$$\n\\begin{aligned}\n\\boldsymbol{W}(t+1)^{\\top} \\boldsymbol{u} & =\\boldsymbol{W}(t)^{\\top} \\boldsymbol{u}-\\eta_{t}\\|\\boldsymbol{u}\\|^{-2} \\nabla^{\\top} \\mathcal{L}(\\boldsymbol{W}(t)) \\boldsymbol{u} \\\\\n& =\\boldsymbol{p}(t)-\\eta_{t}\\|\\boldsymbol{u}\\|^{-2} \\boldsymbol{u}^{\\top} \\boldsymbol{u} \\nabla \\tilde{\\mathcal{L}}(\\boldsymbol{p}(t)) \\\\\n& =\\boldsymbol{p}(t+1)\n\\end{aligned}\n$$\n\nThis concludes the induction. [^1]\n## B Addendum to Section 2\n\n## B. 1 Descent and Gradient Correlation Conditions\n\nThe lemma below identifies conditions under which $\\boldsymbol{p}^{\\text {mm\u592b }}$ is a global descent direction for $\\mathcal{L}(\\boldsymbol{p})$. Lemma 4 Suppose $\\ell(\\cdot)$ is a strictly decreasing differentiate loss function and Assumption B holds. Then, for all $\\boldsymbol{p} \\in \\mathbb{R}^{d}$, the training loss (ERM) obeys $\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{p}^{\\text {mm }}\\right\\rangle<0$. Proof. Set\n\n$$\n\\boldsymbol{\\gamma}_{i}=Y_{i} \\cdot \\boldsymbol{X}_{i} \\boldsymbol{v}, \\quad \\boldsymbol{a}_{i}=\\boldsymbol{K}_{i} \\boldsymbol{p}, \\quad \\overline{\\boldsymbol{a}}_{i}=\\boldsymbol{K}_{i} \\boldsymbol{p}^{m m \\star}, \\text { and } \\ell_{i}^{\\prime}=\\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)\n$$\n\nLet us recall the gradient evaluated at $\\boldsymbol{p}$ which is given by\n\n$$\n\\nabla \\mathcal{L}(\\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}_{i}\\right) \\gamma_{i}\n$$\n\nThis implies that\n\n$$\n\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{p}^{m m \\star}\\right\\rangle=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot\\left\\langle\\overline{\\boldsymbol{a}}_{i}, \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}_{i}\\right) \\gamma_{i}\\right\\rangle\n$$\n\nTo proceed, we will prove that individual summands are all strictly negative. To show that, without losing generality, let us focus on the first input and drop the subscript $i$ for cleaner notation. This yields\n\n$$\n\\left\\langle\\overline{\\boldsymbol{a}}, \\mathbb{S}^{\\prime}(\\boldsymbol{a}) \\gamma\\right\\rangle=\\overline{\\boldsymbol{a}}^{\\top} \\operatorname{diag}(\\mathbb{S}(\\boldsymbol{a})) \\gamma-\\overline{\\boldsymbol{a}}^{\\top} \\mathbb{S}(\\boldsymbol{a}) \\mathbb{S}(\\boldsymbol{a})^{\\top} \\gamma\n$$\n\nWithout losing generality, assume optimal token is the first one and $\\gamma_{t}$ is a constant for all $t \\geq 2$. To proceed, we will prove the following: Suppose $\\gamma=\\gamma_{t \\geq 2}$ is constant, $\\gamma_{1}, \\overline{\\boldsymbol{a}}_{1}$ are the largest indices of $\\boldsymbol{\\gamma}, \\overline{\\boldsymbol{a}}$. Then, for any $\\boldsymbol{s}$ obeying $\\sum_{t \\in[T]} \\boldsymbol{s}_{t}=1, \\boldsymbol{s}_{t} \\geq 0$, we have that $\\overline{\\boldsymbol{a}}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\overline{\\boldsymbol{a}}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma}>0$. To see this, we write\n\n$$\n\\begin{aligned}\n\\overline{\\boldsymbol{a}}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\overline{\\boldsymbol{a}}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma} & =\\sum_{t=1}^{T} \\overline{\\boldsymbol{a}}_{t} \\gamma_{t} \\boldsymbol{s}_{t}-\\sum_{t=1}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t} \\sum_{t=1}^{T} \\boldsymbol{\\gamma}_{t} \\boldsymbol{s}_{t} \\\\\n& =\\left(\\overline{\\boldsymbol{a}}_{1} \\gamma_{1} \\boldsymbol{s}_{1}+\\gamma \\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t}\\right)-\\left(\\gamma_{1} \\boldsymbol{s}_{1}+\\gamma\\left(1-\\boldsymbol{s}_{1}\\right)\\right)\\left(\\overline{\\boldsymbol{a}}_{1} \\boldsymbol{s}_{1}+\\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t}\\right) \\\\\n& =\\overline{\\boldsymbol{a}}_{1}\\left(\\gamma_{1}-\\gamma\\right) \\boldsymbol{s}_{1}\\left(1-\\boldsymbol{s}_{1}\\right)+\\left(\\gamma-\\left(\\gamma_{1} \\boldsymbol{s}_{1}+\\gamma\\left(1-\\boldsymbol{s}_{1}\\right)\\right)\\right) \\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t} \\\\\n& =\\overline{\\boldsymbol{a}}_{1}\\left(\\gamma_{1}-\\gamma\\right) \\boldsymbol{s}_{1}\\left(1-\\boldsymbol{s}_{1}\\right)-\\left(\\gamma_{1}-\\gamma\\right) \\boldsymbol{s}_{1} \\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t} \\\\\n& =\\left(\\gamma_{1}-\\gamma\\right)\\left(1-\\boldsymbol{s}_{1}\\right) \\boldsymbol{s}_{1}\\left[\\overline{\\boldsymbol{a}}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}\\right]\n\\end{aligned}\n$$\n\nTo proceed, let $\\boldsymbol{\\gamma}_{\\text {gap }}=\\boldsymbol{\\gamma}_{1}-\\gamma$ and $\\boldsymbol{a}_{\\text {gap }}=\\overline{\\boldsymbol{a}}_{1}-\\max _{t \\geq 2} \\boldsymbol{a}_{t}$. With these, we obtain\n\n$$\n\\overline{\\boldsymbol{a}}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\overline{\\boldsymbol{a}}^{\\top} \\boldsymbol{s s}^{\\top} \\boldsymbol{\\gamma} \\geq \\boldsymbol{a}_{\\text {gap }} \\boldsymbol{\\gamma}_{\\text {gap }} \\boldsymbol{s}_{1}\\left(1-\\boldsymbol{s}_{1}\\right)\n$$\n\nNote that\n\n$$\n\\begin{aligned}\n& \\boldsymbol{a}_{\\text {gap }}^{i} \\geq \\inf _{t \\neq \\text { opt }_{i}}\\left(\\boldsymbol{k}_{i \\mathrm{opt}_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}^{m m \\star} \\geq 1, \\\\\n& \\gamma_{\\text {gap }}^{i}=\\inf _{t \\neq \\mathrm{opt}_{i}} \\gamma_{\\text {opt }_{i}}-\\gamma_{i t}>0, \\\\\n& s_{i 1}\\left(1-s_{i 1}\\right)>0 .",
    "maxmargin-49": "\\end{aligned}\n$$\n\nOn the other hand, by our assumption $\\ell_{i}^{\\prime}<0$. Hence, infimum'ing (22) over all inputs, multiplying by $\\ell_{i}^{\\prime}$ and using (19) give the desired result. ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-21.jpg?height=45&width=1387&top_left_y=254&top_left_x=369) solution separating $\\alpha=$ opt from remaining tokens of input $\\boldsymbol{X}$. Suppose $\\ell(\\cdot)$ is a strictly decreasing differentiate loss function and Assumption B holds. For any choice of $\\pi>0$, there exists $R:=R_{\\pi}$ such that, for any $\\boldsymbol{p}$ with $\\|\\boldsymbol{p}\\| \\geq R$, we have\n\n$$\n\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}}{\\|\\boldsymbol{p}\\|}\\right\\rangle \\geq(1+\\pi)\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle\n$$\n\nAbove, observe that as $R \\rightarrow \\infty$, we eventually get to set $\\pi=0$.",
    "maxmargin-50": "Proof. The proof is similar to Lemma 4 at a high-level. However, we also need to account for the impact of $\\boldsymbol{p}$ besides $\\boldsymbol{p}^{\\mathrm{mm}}$ in the gradient correlation. The main goal is showing that $\\boldsymbol{p}^{\\mathrm{mm}}$ is the near-optimal descent direction, thus, $\\boldsymbol{p}$ cannot significantly outperform it. To proceed, let $\\overline{\\boldsymbol{p}}=\\left\\|\\boldsymbol{p}^{m m}\\right\\| \\boldsymbol{p} /\\|\\boldsymbol{p}\\|, M=\\sup _{t}\\left\\|\\boldsymbol{k}_{t}\\right\\|, \\Theta=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|, \\boldsymbol{s}=\\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p}), \\boldsymbol{a}=\\boldsymbol{K} \\overline{\\boldsymbol{p}}, \\overline{\\boldsymbol{a}}=\\boldsymbol{K} \\boldsymbol{p}^{m m}$. Without losing generality assume opt $=1$. Set $\\gamma=\\gamma_{t \\geq 2}$. Repeating the proof of Lemma 4 yields\n\n$$\n\\begin{aligned}\n\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{p}^{m m}\\right\\rangle & =\\ell^{\\prime} \\cdot\\left(\\gamma_{1}-\\gamma\\right)\\left(1-\\boldsymbol{s}_{1}\\right) \\boldsymbol{s}_{1}\\left[\\overline{\\boldsymbol{a}}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}\\right] \\\\\n\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\overline{\\boldsymbol{p}}\\rangle & =\\ell^{\\prime} \\cdot\\left(\\gamma_{1}-\\gamma\\right)\\left(1-\\boldsymbol{s}_{1}\\right) \\boldsymbol{s}_{1}\\left[\\boldsymbol{a}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}\\right]\n\\end{aligned}\n$$\n\nGiven $\\pi$, for sufficiently large $R$, we wish to show that\n\n$$\n\\boldsymbol{a}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}} \\leq(1+\\pi) \\cdot\\left[\\overline{\\boldsymbol{a}}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}\\right]\n$$\n\nWe consider two scenarios. Scenario 1: $\\left\\|\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{m m}\\right\\| \\leq \\epsilon:=\\pi /(2 M)$. In this scenario, for any token, we find that\n\n$$\n\\left|\\boldsymbol{a}_{t}-\\overline{\\boldsymbol{a}}_{t}\\right|=\\left|\\boldsymbol{k}_{t}^{\\top}\\left(\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{m m}\\right)\\right| \\leq M\\left\\|\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{m m}\\right\\| \\leq M \\epsilon\n$$\n\nConsequently, we obtain\n\n$$\n\\overline{\\boldsymbol{a}}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\overline{\\boldsymbol{a}}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}} \\geq \\boldsymbol{a}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}-2 M \\epsilon=\\boldsymbol{a}_{1}-\\frac{\\sum_{t \\geq 2}^{T} \\boldsymbol{a}_{t} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}-\\pi\n$$\n\nAlso noticing $\\overline{\\boldsymbol{a}}_{1}-\\frac{\\sum_{t 2}^{T} \\overline{\\bar{t}}_{\\boldsymbol{t}} s_{t}}{\\sum_{t 22}^{T} s_{t}} \\geq 1$ (thanks to $\\boldsymbol{p}^{m m}$ satisfying $\\geq 1$ margin), this implies (23). Scenario 2: $\\left\\|\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{m m}\\right\\| \\geq \\epsilon:=\\pi /(2 M)$. In this scenario, for some $v=v(\\epsilon)$ and $\\tau \\geq 2$, we have that\n\n$$\n\\overline{\\boldsymbol{p}}^{\\top}\\left(\\boldsymbol{k}_{1}-\\boldsymbol{k}_{\\tau}\\right)=\\boldsymbol{a}_{1}-\\boldsymbol{a}_{\\tau} \\leq 1-2 v\n$$\n\nHere $\\tau=\\arg \\max _{t \\geq 2} \\overline{\\boldsymbol{p}}^{\\top} \\boldsymbol{k}_{t}$ denotes the nearest point to $\\boldsymbol{k}_{1}$. Recall that $\\boldsymbol{s}=\\mathbb{S}(\\bar{R} \\boldsymbol{a})$ where $\\bar{R}=\\|\\boldsymbol{p}\\| /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. To proceed, split the tokens into two groups: Let $\\mathcal{N}$ be the group of tokens obeying $\\overline{\\boldsymbol{p}}^{\\top}\\left(\\boldsymbol{k}_{1}-\\boldsymbol{k}_{t}\\right) \\leq 1-v$ for $t \\in \\mathcal{N}$ and $[T]-\\mathcal{N}$ be the rest. Observe that\n\n$$\n\\frac{\\sum_{t \\in[T]-\\mathcal{N}} \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}} \\leq \\frac{\\sum_{t \\in[T]-\\mathcal{N}} s_{t}}{\\boldsymbol{s}_{\\tau}} \\leq T \\frac{e^{v \\bar{R}}}{e^{2 v \\bar{R}}}=T e^{-\\bar{R} v}\n$$\n\nSet $\\bar{M}=M / \\Theta$ and note that $\\left\\|\\boldsymbol{a}_{t}\\right\\| \\leq\\left\\|\\boldsymbol{p}^{m m}\\right\\| \\cdot\\left\\|\\boldsymbol{k}_{t}\\right\\| \\leq \\bar{M}$. Using $\\overline{\\boldsymbol{p}}^{\\top}\\left(\\boldsymbol{k}_{1}-\\boldsymbol{k}_{t}\\right) \\leq 1-v$ over $t \\in \\mathcal{N}$ and plugging in the above bound, we obtain\n\n$$\n\\begin{aligned}\n\\frac{\\sum_{t \\geq 2}^{T}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}} & =\\frac{\\sum_{t \\in \\mathcal{N}}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}}+\\frac{\\sum_{t \\in[T]-\\mathcal{N}}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}}{\\sum_{t \\geq 2}^{T} \\boldsymbol{s}_{t}} \\\\\n& \\leq 1-v+2 \\bar{M} T e^{-\\bar{R} v}\n\\end{aligned}\n$$\n\nUsing the fact that $\\overline{\\boldsymbol{a}}_{1}-\\frac{\\sum_{t 22}^{T} \\bar{a}_{t} s_{t}}{\\sum_{t 22}^{T} s_{t}} \\geq 1$, the above implies (23) with $\\pi^{\\prime}=2 \\bar{M} T e^{-\\bar{R} v}-v$. To proceed, choose $R_{\\pi}=v^{-1} \\Theta^{-1} \\log (2 \\bar{M} T / \\pi)$ to ensure $\\pi^{\\prime} \\leq \\pi$\n\nThe following lemma states the descent property of gradient descent for $\\mathcal{L}(\\boldsymbol{p})$ under Assumption A. It is important to note that although the infimum of the optimization problem is $\\mathcal{L}^{*}$, it is not achieved at any finite $\\boldsymbol{p}$. Additionally, there are no finite critical points $\\boldsymbol{p}$. Lemma 6 Under Assumption $A$, the function $\\mathcal{L}(\\boldsymbol{p})$ is $L_{p}$-smooth, where\n\n$$\nL_{p}:=\\frac{1}{n} \\sum_{i=1}^{n}\\left(M_{0}\\|\\boldsymbol{v}\\|^{2}\\|\\boldsymbol{W}\\|^{2}\\left\\|\\boldsymbol{X}_{i}\\right\\|^{4}+3 M_{1}\\|\\boldsymbol{v}\\|\\|\\boldsymbol{W}\\|^{2}\\left\\|\\boldsymbol{X}_{i}\\right\\|^{3}\\right)\n$$\n\nFurthermore, if $\\eta \\leq 1 / L_{p}$, then, for any initialization $\\boldsymbol{p}(0)$, with the $G D$ sequence $\\boldsymbol{p}(t+1)=$ $\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t))$, we have\n\n$$\n\\mathcal{L}(\\boldsymbol{p}(t+1))-\\mathcal{L}(\\boldsymbol{p}(t)) \\leq-\\frac{\\eta}{2}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}\n$$\n\nfor all $t \\geq 0$. This implies that\n\n$$\n\\sum_{t=0}^{\\infty}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}<\\infty, \\text { and } \\lim _{t \\rightarrow \\infty}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}=0\n$$\n\nProof. Recall that we defined $\\boldsymbol{\\gamma}_{i}=Y_{i} \\cdot \\boldsymbol{X}_{i} \\boldsymbol{v}$ and $\\boldsymbol{a}_{i}=\\boldsymbol{K}_{i} \\boldsymbol{p}$. The gradient of $\\mathcal{L}(\\boldsymbol{p})$ is given by\n\n$$\n\\nabla \\mathcal{L}(\\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right) \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}_{i}\\right) \\boldsymbol{\\gamma}_{i}\n$$\n\nNote that for any $\\boldsymbol{p} \\in \\mathbb{R}^{d}$, the Jacobian of $\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$ is given by\n\n$$\n\\frac{\\partial \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)}{\\partial \\boldsymbol{p}}=\\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\boldsymbol{K}_{i}=\\left(\\operatorname{diag}\\left(\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)^{\\top}\\right) \\boldsymbol{K}_{i}\n$$\n\nThe Jacobian (27) together with the definition of the softmax function $\\mathbb{S}(\\cdot)$ implies that $\\left\\|\\partial \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) / \\partial \\boldsymbol{p}\\right\\| \\leq\\left\\|\\boldsymbol{K}_{i}\\right\\|$. Hence, for any $\\boldsymbol{p}, \\dot{\\boldsymbol{p}} \\in \\mathbb{R}^{d}$, we have\n\n$$\n\\left\\|\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right\\| \\leq\\left\\|\\boldsymbol{K}_{i}\\right\\|\\|\\boldsymbol{p}-\\dot{\\boldsymbol{p}}\\|\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\left\\|\\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)-\\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right\\| & \\leq\\left\\|\\operatorname{diag}\\left(\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)-\\operatorname{diag}\\left(\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right)\\right\\| \\\\\n& +\\left\\|\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)^{\\top}-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right) \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)^{\\top}\\right\\| \\\\\n& \\leq 3\\left\\|\\boldsymbol{K}_{i}\\right\\|\\|\\boldsymbol{p}-\\dot{\\boldsymbol{p}}\\|\n\\end{aligned}\n$$\n\nHere, the last inequality uses the fact that $|a b-c d| \\leq|d||a-c|+|a||b-d|$. Next, for any $\\boldsymbol{p}, \\dot{\\boldsymbol{p}} \\in \\mathbb{R}^{d}$, we have\n\n$$\n\\begin{aligned}\n\\|\\nabla \\mathcal{L}(\\boldsymbol{p})-\\nabla \\mathcal{L}(\\dot{\\boldsymbol{p}})\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|\\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right) \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\boldsymbol{\\gamma}_{i}-\\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right) \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right) \\boldsymbol{\\gamma}_{i}\\right\\| \\\\\n& \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|\\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right) \\boldsymbol{\\gamma}_{i}\\right\\|\\left|\\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)-\\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right)\\right| \\\\\n& \\left.+\\frac{1}{n} \\sum_{i=1}^{n} \\right\\rvert\\, \\ell^{\\prime}\\left(\\boldsymbol{\\gamma}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right)\\left\\|\\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\boldsymbol{\\gamma}_{i}-\\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right) \\boldsymbol{\\gamma}_{i}\\right\\| \\\\\n& \\leq \\frac{1}{n} \\sum_{i=1}^{n} M_{0}\\left\\|\\boldsymbol{\\gamma}_{i}\\right\\|^{2}\\left\\|\\boldsymbol{K}_{i}\\right\\|\\left\\|\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right\\| \\\\\n& +\\frac{1}{n} \\sum_{i=1}^{n} M_{1}\\left\\|\\boldsymbol{\\gamma}_{i}\\right\\|\\left\\|\\boldsymbol{K}_{i}\\right\\|\\left\\|\\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)-\\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\dot{\\boldsymbol{p}}\\right)\\right\\|\n\\end{aligned}\n$$\n\nwhere the second inequality follows from the fact that $|a b-c d| \\leq|d||a-c|+|a||b-d|$ and the third inequality uses Assumption A. Substituting (28a) and (28b) into (29), we get\n\n$$\n\\begin{aligned}\n\\|\\nabla \\mathcal{L}(\\boldsymbol{p})-\\nabla \\mathcal{L}(\\dot{\\boldsymbol{p}})\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left(M_{0}\\left\\|\\boldsymbol{\\gamma}_{i}\\right\\|^{2}\\left\\|\\boldsymbol{K}_{i}\\right\\|^{2}+3 M_{1}\\left\\|\\boldsymbol{K}_{i}\\right\\|^{2}\\left\\|\\boldsymbol{\\gamma}_{i}\\right\\|\\right)\\|\\boldsymbol{p}-\\dot{\\boldsymbol{p}}\\| \\\\\n& \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left(M_{0}\\|\\boldsymbol{v}\\|^{2}\\|\\boldsymbol{W}\\|^{2}\\left\\|\\boldsymbol{X}_{i}\\right\\|^{4}+3 M_{1}\\|\\boldsymbol{v}\\| \\boldsymbol{W}\\left\\|^{2}\\right\\| \\boldsymbol{X}_{i} \\|^{3}\\right)\\|\\boldsymbol{p}-\\dot{\\boldsymbol{p}}\\| \\\\\n& \\leq L_{p}\\|\\boldsymbol{p}-\\dot{\\boldsymbol{p}}\\|\n\\end{aligned}\n$$\n\nwhere $L_{p}$ is defined in (24). The remaining proof follows standard gradient descent analysis (see e.g. [22, Lemma 10]). Since $\\mathcal{L}(\\boldsymbol{p})$ is $L_{p}$-smooth, we get\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{p}(t+1)) & \\leq \\mathcal{L}(\\boldsymbol{p}(t))+\\nabla \\mathcal{L}(\\boldsymbol{p}(t))^{\\top}(\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t))+\\frac{L_{p}}{2}\\|\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t)\\|^{2} \\\\\n& =\\mathcal{L}(\\boldsymbol{p}(t))-\\eta\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}+\\frac{L_{p} \\eta^{2}}{2}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2} \\\\\n& =\\mathcal{L}(\\boldsymbol{p}(t))-\\eta\\left(1-\\frac{L_{p} \\eta}{2}\\right)\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2} \\\\\n& \\leq \\mathcal{L}(\\boldsymbol{p}(t))-\\frac{\\eta}{2}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}\n\\end{aligned}\n$$\n\nwhere the last inequality follows from our assumption on the stepsize. The above inequality implies that\n\n$$\n\\sum_{t=0}^{\\infty}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2} \\leq \\frac{2}{\\eta}\\left(\\mathcal{L}(\\boldsymbol{p}(0))-\\mathcal{L}^{*}\\right)\n$$\n\nwhere the right hand side is upper bounded by a finite constant. This is because, by Assumption A, $\\mathcal{L}(\\boldsymbol{p}(0))<\\infty$ and $\\mathcal{L}^{*} \\leq \\mathcal{L}(\\boldsymbol{p}(t))$, where $\\mathcal{L}^{*}$ denotes the minimum objective. Finally, (30) yields the expression (26). In the following lemma, we demonstrate the existence of parameters $\\mu=\\mu(\\boldsymbol{\\alpha})>0$ and $R_{\\mu}>0$ such that when $R_{\\mu}$ is sufficiently large, there are no stationary points within $C_{\\mu, R_{\\mu}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. Additionally, we provide the local gradient correlation condition. Lemma 7 (Local Gradient Condition) Suppose Assumption A on the loss function $\\ell$ holds. Let $\\alpha=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ be indices of locally-optimal tokens per Definition 2. L1. There exists a positive scalar $\\mu=\\mu(\\boldsymbol{\\alpha})>0$ such that for sufficiently large $\\bar{R}_{\\mu}$, no stationary point exists within $C_{\\mu, \\bar{R}_{\\mu}}\\left(\\boldsymbol{p}^{m m}\\right)$, where $C_{\\mu, \\bar{R}_{\\mu}}$ is defined in (8). L2. For all $\\boldsymbol{q}, \\boldsymbol{p} \\in$ cone $_{\\mu}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$ with $\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|$ and $\\|\\boldsymbol{p}\\| \\geq \\bar{R}_{\\mu}$ with same $\\bar{R}_{\\mu}$ choice as (L1.), there exist dataset dependent constants $C, c>0$ such that\n\n$$\n\\begin{aligned}\n& C \\cdot \\frac{1}{n} \\sum_{i \\in[n]}\\left\\{1-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)_{\\alpha_{i}}\\right\\} \\geq-\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{q}\\rangle \\geq c \\cdot \\frac{1}{n} \\sum_{i \\in[n]}\\left\\{1-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)_{\\alpha_{i}}\\right\\}>0 \\\\\n& \\|\\nabla \\mathcal{L}(\\boldsymbol{p})\\| \\leq \\bar{A} C \\cdot \\frac{1}{n} \\sum_{i \\in[n]}\\left\\{1-\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)_{\\alpha_{i}}\\right\\} \\leq \\bar{A} C T e^{-\\bar{R}_{\\mu} \\Theta / 2} \\\\\n& -\\left\\langle\\frac{\\boldsymbol{q}}{\\|\\boldsymbol{q}\\|}, \\frac{\\nabla \\mathcal{L}(\\boldsymbol{p})}{\\|\\nabla \\mathcal{L}(\\boldsymbol{p})\\|}\\right\\rangle \\geq \\frac{c \\Theta}{C \\bar{A}}>0\n\\end{aligned}\n$$\n\nHere, $\\bar{A}=\\max _{i \\in[n], t, \\tau \\in[T]}\\left\\|\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\tau}\\right\\|$ and $\\Theta=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. L3. For any $\\pi>0$, there exists $R_{\\pi}$ such that $R_{\\pi} \\geq \\bar{R}_{\\mu}$ and all $\\boldsymbol{p} \\in C_{\\mu, R_{\\pi}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$ obeys\n\n$$\n\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}}{\\|\\boldsymbol{p}\\|}\\right\\rangle \\geq(1+\\pi)\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle\n$$\n\nProof. Let $\\boldsymbol{p}^{m m}=\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$ be the solution of (ATT-SVM). Recall\n\n$$\nC_{\\mu, \\bar{R}_{\\mu}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)=\\operatorname{cone}_{\\mu}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right) \\bigcap\\left\\{\\boldsymbol{p} \\mid\\|\\boldsymbol{p}\\| \\geq \\bar{R}_{\\mu}\\right\\}\n$$\n\nLet $\\left(\\mathcal{T}_{i}\\right)_{i=1}^{n}$ be the sets of all SVM-neighbors per Definition 2. Let $\\overline{\\mathcal{T}}_{i}=[T]-\\mathcal{T}_{i}-\\left\\{\\alpha_{i}\\right\\}$ be the set of non-SVM-neighbor tokens, $i \\in[n]$. Let\n\n$$\n\\begin{aligned}\n& \\Theta=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\| \\\\\n& \\delta=0.5 \\min _{i \\in[n]} \\min _{t \\in \\mathcal{T}_{i}, \\tau \\in \\overline{\\mathcal{T}}}\\left(\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\tau}\\right)^{\\top} \\boldsymbol{p}^{\\mathrm{mm}} \\\\\n& A=\\max _{i \\in[n], t \\in[T]}\\left\\|\\boldsymbol{k}_{i t}\\right\\| / \\Theta \\\\\n& \\mu \\leq \\mu(\\delta)=\\frac{1}{8}\\left(\\frac{\\min (0.5, \\delta)}{A}\\right)^{2}\n\\end{aligned}\n$$\n\nWhen $\\overline{\\mathcal{T}}_{i}=\\emptyset$ for all $i \\in[n]$ (i.e. globally-optimal indices), we set $\\delta=\\infty$ as all non-neighbor related terms will disappear. Since $\\boldsymbol{p}^{\\mathrm{mm}}$ is the max-margin model ensuring $\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}^{\\mathrm{mm}} \\geq 1$ for all $i \\in[n]$, the following inequalities hold for all $\\boldsymbol{q} \\in \\operatorname{cone}_{\\mu}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right),\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|$ and all $i \\in[n], t \\in \\mathcal{T}_{i}, \\tau \\in \\overline{\\mathcal{T}}_{i}$ :\n\n$$\n\\begin{aligned}\n\\left(\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\tau}\\right)^{\\top} \\boldsymbol{q} & \\geq \\delta>0 \\\\\n\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\tau}\\right)^{\\top} \\boldsymbol{q} & \\geq 1+\\delta \\\\\n3 / 2 \\geq\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q} & \\geq 1 / 2\n\\end{aligned}\n$$\n\nHere, we used $\\left\\|\\boldsymbol{q}-\\boldsymbol{p}^{m m}\\right\\|^{2} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|^{2} \\leq 2 \\mu$ which implies $\\left\\|\\boldsymbol{q}-\\boldsymbol{p}^{m m}\\right\\| \\leq \\sqrt{2 \\mu} / \\Theta$.",
    "maxmargin-51": "L1. and L2.. Now that the choice of local cone is determined, we need to prove the main claims. We will lower bound $-\\boldsymbol{q}^{\\top} \\nabla \\mathcal{L}(\\boldsymbol{p})$ and establish its strict positivity for $\\|\\boldsymbol{p}\\| \\geq R$, where $R=\\bar{R}_{\\mu}$. This will show that there is no stationary point as a by product. Consider any $\\boldsymbol{q} \\in \\mathbb{R}^{d}$ satisfying $\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. To proceed, we write the gradient correlation following (18) and (21)\n\n$$\n\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{q}\\rangle=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot\\left\\langle\\boldsymbol{a}_{i}, \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}_{i}^{\\prime}\\right) \\gamma_{i}\\right\\rangle\n$$\n\nwhere we denoted $\\ell_{i}^{\\prime}=\\ell^{\\prime}\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right), \\boldsymbol{a}_{i}=\\boldsymbol{K}_{i} \\boldsymbol{q}, \\boldsymbol{a}_{i}^{\\prime}=\\boldsymbol{K}_{i} \\boldsymbol{p}, \\boldsymbol{s}_{i}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$. Using (33), for all $t \\in \\mathcal{T}_{i}, \\tau \\in \\overline{\\mathcal{T}}_{i}$, for all $\\boldsymbol{p} \\in \\mathcal{C}_{\\mu, R}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$, we have that\n\n$$\n\\boldsymbol{a}_{i \\alpha_{i}}^{\\prime}-\\boldsymbol{a}_{i \\tau}^{\\prime} \\geq R \\Theta(1+\\delta), \\quad \\text { and } \\quad \\boldsymbol{a}_{i t}^{\\prime}-\\boldsymbol{a}_{i \\tau}^{\\prime} \\geq R \\Theta \\delta\n$$\n\nConsequently, we can bound the softmax probabilities $\\boldsymbol{s}_{i}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$ as follows: For all $i \\in[n]$,\n\n$$\n\\begin{aligned}\n& S_{i}:=\\sum_{\\tau \\in \\mathcal{T}_{i}} s_{i \\tau} \\leq 1-s_{i \\alpha_{i}}=\\sum_{\\tau \\neq \\alpha_{i}} s_{i \\tau} \\leq T e^{-R \\Theta / 2} s_{i \\alpha_{i}} \\leq T e^{-R \\Theta / 2} \\\\\n& Q_{i}:=\\sum_{\\tau \\in \\overline{\\mathcal{T}}_{i}} s_{i \\tau} \\leq T e^{-R \\Theta \\delta} s_{i t_{i}} \\leq T e^{-R \\Theta \\delta} S_{i}, \\quad \\forall t_{i} \\in \\mathcal{T}_{i}\n\\end{aligned}\n$$\n\nRecall scores $\\boldsymbol{\\gamma}_{i t}=Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{x}_{i t}$. Define the score gaps over neighbors:\n\n$$\n\\gamma_{i}^{\\text {gap }}=\\gamma_{i \\alpha_{i}}-\\max _{t \\in \\mathcal{T}_{i}} \\gamma_{i t}, \\quad \\text { and } \\quad \\bar{\\gamma}_{i}^{g a p}=\\gamma_{i \\alpha_{i}}-\\min _{t \\in \\mathcal{T}_{i}} \\gamma_{i t}\n$$\n\nIt follows from (32) that\n\n$$\nA=\\max _{i \\in[n], t \\in[T]}\\left\\|\\boldsymbol{k}_{i t}\\right\\| / \\Theta \\geq \\max _{i \\in[n], t \\in[T]}\\left\\|\\boldsymbol{a}_{i t}\\right\\|=\\left\\|\\boldsymbol{k}_{i t} \\boldsymbol{q}\\right\\|\n$$\n\nDefine the $\\alpha$-dependent global scalar $\\Gamma=\\sup _{i \\in[n], t, \\tau \\in[T]}\\left|\\gamma_{i t}-\\gamma_{i \\tau}\\right|$. Let us focus on a fixed datapoint $i \\in[n]$, assume (without losing generality) $\\alpha_{i}=1$, and drop subscripts $i$, that is, $\\alpha:=\\alpha_{i}=1, \\boldsymbol{X}:=\\boldsymbol{X}_{i}$, $Y:=Y_{i}, \\boldsymbol{K}:=\\boldsymbol{K}_{i}, \\boldsymbol{a}^{\\prime}=\\boldsymbol{K} \\boldsymbol{p}, \\boldsymbol{a}=\\boldsymbol{K} \\boldsymbol{q}, \\boldsymbol{s}=\\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p}), \\boldsymbol{\\gamma}=Y \\cdot \\boldsymbol{X} \\boldsymbol{v}, \\gamma^{\\text {gap }}:=\\gamma_{i}^{\\text {gap }}, \\bar{\\gamma}^{\\text {gap }}:=\\bar{\\gamma}_{i}^{\\text {gap }}, Q:=Q_{i}$, and $S:=S_{i}$. Directly applying Lemma 3, we obtain\n\n$$\n\\left|\\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma}-\\sum_{t \\geq 2}^{T}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}\\left(\\boldsymbol{\\gamma}_{1}-\\boldsymbol{\\gamma}_{t}\\right)\\right| \\leq 2 \\Gamma A\\left(1-\\boldsymbol{s}_{1}\\right)^{2}\n$$\n\nTo proceed, let us decouple the non-neighbors within $\\sum_{t \\geq 2}^{T}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}\\left(\\boldsymbol{\\gamma}_{1}-\\boldsymbol{\\gamma}_{t}\\right)$ via\n\n$$\n\\left|\\sum_{t \\in \\mathcal{T}}\\left(a_{1}-a_{t}\\right) s_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right)\\right| \\leq 2 Q \\Gamma A\n$$\n\nAggregating these, we found\n\n$$\n\\left|\\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma}-\\sum_{t \\in \\mathcal{T}_{i}}\\left(\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right) \\boldsymbol{s}_{t}\\left(\\gamma_{1}-\\boldsymbol{\\gamma}_{t}\\right)\\right| \\leq 2 \\Gamma A\\left(\\left(1-\\boldsymbol{s}_{1}\\right)^{2}+Q\\right)\n$$\n\nTo proceed, let us upper/lower bound the gradient correlation. We use two bounds depending on $\\boldsymbol{q} \\in$ cone $_{\\mu}\\left(\\boldsymbol{p}^{m m}\\right)$ (Case 1) or general $\\boldsymbol{q} \\in \\mathbb{R}^{d}$ (Case 2). - Case 1: $\\boldsymbol{q} \\in \\operatorname{cone}_{\\mu}\\left(\\boldsymbol{p}^{m m}\\right)$. Since $1.5 \\geq \\boldsymbol{a}_{1}-\\boldsymbol{a}_{t} \\geq 0.5$ following (33), we find\n\n$$\n1.5 \\cdot S \\cdot \\bar{\\gamma}^{\\text {gap }} \\geq \\sum_{t \\in \\mathcal{T}_{i}}\\left(a_{1}-a_{t}\\right) s_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right) \\geq 0.5 \\cdot S \\cdot \\gamma^{\\text {gap }}\n$$\n\nNext we claim that $S$ dominates $\\left(\\left(1-s_{1}\\right)^{2}+Q\\right)$ for large $R$. Specifically, we wish for\n\n$$\nS \\cdot \\gamma^{\\text {gap }} / 4 \\geq 4 \\Gamma A \\max \\left(\\left(1-s_{1}\\right)^{2}, Q\\right) \\Longleftrightarrow S \\geq 16 \\frac{\\Gamma A}{\\gamma^{g a p}} \\max \\left(\\left(1-s_{1}\\right)^{2}, Q\\right)\n$$\n\nNow choose $R \\geq \\delta^{-1} \\log (T) / \\Theta$ to ensure $Q \\leq S$ since $Q \\leq T e^{-R \\Theta \\delta} S$. Consequently\n\n$$\n\\left(1-s_{1}\\right)^{2}=(Q+S)^{2} \\leq 4 S^{2} \\leq 4 S T e^{-R \\Theta / 2}\n$$\n\nCombining these, what we wish is ensured by guaranteeing\n\n$$\nS \\geq 16 \\frac{\\Gamma A}{\\gamma^{g a p}} \\max \\left(4 S T e^{-R \\Theta / 2}, T e^{-R \\Theta \\delta} S\\right)\n$$\n\nThis in turn is ensured for all inputs $i \\in[n]$ by choosing\n\n$$\nR=\\frac{\\max \\left(2, \\delta^{-1}\\right)}{\\Theta} \\log \\left(\\frac{64 T \\Gamma A}{\\gamma_{\\min }^{g a p}}\\right)\n$$\n\nwhere $\\gamma_{\\min }^{\\text {gap }}=\\min _{i \\in[n]} \\gamma_{i}^{g a p}$ is the global scalar which is the worst case score gap over all inputs. With the above choice of $R$ we guaranteed\n\n$$\n2\\left(1-\\boldsymbol{s}_{1}\\right) \\cdot \\bar{\\gamma}^{\\text {gap }} \\geq 2 \\cdot S \\cdot \\bar{\\gamma}^{\\text {gap }} \\geq \\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma} \\geq \\frac{S \\cdot \\gamma^{\\text {gap }}}{4} \\geq \\frac{\\left(1-\\boldsymbol{s}_{1}\\right) \\gamma^{\\text {gap }}}{8}\n$$\n\nSince this holds over all inputs, going back to the gradient correlation (34) and averaging above over all inputs $i \\in[n]$ and plugging back the indices $i$, we obtain the advertised bound by setting $q_{i}=1-\\boldsymbol{s}_{i \\alpha_{i}}$ (where we set $\\alpha_{i}=1$ above without losing generality)\n\n$$\n\\frac{2}{n} \\sum_{i \\in[n]}-\\ell_{i}^{\\prime} \\cdot q_{i} \\cdot \\bar{\\gamma}_{i}^{\\text {gap }} \\geq-\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{q}\\rangle \\geq \\frac{1}{8 n} \\sum_{i \\in[n]}-\\ell_{i}^{\\prime} \\cdot q_{i} \\cdot \\gamma_{i}^{g a p}\n$$\n\nLet $-\\ell_{\\min / \\max }^{\\prime}$ be the $\\min / \\max$ values negative loss derivative admits over the ball $[-B, B]$ for $B=$ $\\|\\boldsymbol{v}\\| \\cdot \\max _{i, t}\\left\\|\\boldsymbol{x}_{i t}\\right\\|$ and note that $\\max _{i \\in[n]} \\bar{\\gamma}_{i}^{\\text {gap }}>0$ and $\\min _{i \\in[n]} \\gamma_{i}^{\\text {gap }}>0$ are dataset dependent constants. Then, we declare the constants $C=-2 \\ell_{\\max }^{\\prime} \\cdot \\max _{i \\in[n]} \\bar{\\gamma}_{i}^{\\text {gap }}>0, c=-(1 / 8) \\ell_{\\min }^{\\prime} \\cdot \\min _{i \\in[n]} \\gamma_{i}^{\\text {gap }}>0$ to obtain the bound\n\n$$\n\\frac{C}{n} \\sum_{i \\in[n]} q_{i} \\geq-\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{q}\\rangle \\geq \\frac{c}{n} \\sum_{i \\in[n]} q_{i}\n$$\n\nwhich is the desired statement in (31a). - Case 2: $\\boldsymbol{q} \\in \\mathbb{R}^{d}$ and $\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. Define $\\bar{A}=\\max _{i \\in[n], t \\tau \\in[T]}\\left\\|\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\tau}\\right\\|$. For any $\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|$, we use the fact that\n\n$$\n\\left\\|\\boldsymbol{a}_{1}-\\boldsymbol{a}_{t}\\right\\| \\leq\\left\\|\\boldsymbol{k}_{1}-\\boldsymbol{k}_{t}\\right\\| \\cdot\\|\\boldsymbol{q}\\| \\leq \\frac{\\bar{A}}{\\Theta}\n$$\n\nNote that by definition $\\frac{\\bar{A}}{\\Theta} \\geq 1$. To proceed, we can upper bound\n\n$$\n\\frac{\\bar{A}}{\\Theta} \\cdot S \\cdot \\bar{\\gamma}^{\\text {gap }} \\geq \\sum_{t \\in \\mathcal{T}}\\left(a_{1}-a_{t}\\right) \\boldsymbol{s}_{t}\\left(\\gamma_{1}-\\gamma_{t}\\right)\n$$\n\nBy choosing the same $R$ as in (39) to ensure $S$ dominates $\\left(\\left(1-s_{1}\\right)^{2}+Q\\right)$ and since $\\frac{\\bar{A}}{\\Theta} \\geq 1$, we guaranteed\n\n$$\n\\frac{2 \\bar{A}}{\\Theta} \\cdot S \\cdot \\bar{\\gamma}^{g a p} \\geq \\boldsymbol{a}^{\\top} \\operatorname{diag}(\\boldsymbol{s}) \\boldsymbol{\\gamma}-\\boldsymbol{a}^{\\top} \\boldsymbol{s} \\boldsymbol{s}^{\\top} \\boldsymbol{\\gamma}\n$$\n\nGoing back to the gradient correlation (34) and averaging above over all inputs $i \\in[n]$, with the same definition of $C>0$, we obtain\n\n$$\n\\frac{\\bar{A} C}{\\Theta n} \\sum_{i \\in[n]} q_{i} \\geq-\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{q}\\rangle\n$$\n\nTo proceed, since (43) holds for any $\\boldsymbol{q} \\in \\mathbb{R}^{d}$ and $\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|$, we observe that when choosing $\\boldsymbol{q}=\\frac{\\left\\|p^{m m}\\right\\|}{\\|\\nabla \\mathcal{L}(p)\\|} \\cdot \\nabla \\mathcal{L}(\\boldsymbol{p})$, this implies that\n\n$$\n\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{q}\\rangle=\\|\\nabla \\mathcal{L}(\\boldsymbol{p})\\| \\cdot\\left\\|\\boldsymbol{p}^{m m}\\right\\| \\leq \\frac{\\bar{A} C}{\\Theta n} \\sum_{i \\in[n]} q_{i}\n$$\n\nSimplifying $\\Theta=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$ on both sides yields (31b). Incorporating (35) in the bound above provides the exponential upper bound that decay with $R$. Combining this with (41), we obtain that for all $\\boldsymbol{q}, \\boldsymbol{p} \\in \\operatorname{cone}_{\\mu}\\left(\\boldsymbol{p}^{m m}\\right)$ and $\\|\\boldsymbol{q}\\| \\geq \\bar{R}_{\\mu}$\n\n$$\n-\\left\\langle\\frac{\\boldsymbol{q}}{\\|\\boldsymbol{q}\\|}, \\frac{\\nabla \\mathcal{L}(\\boldsymbol{p})}{\\|\\nabla \\mathcal{L}(\\boldsymbol{p})\\|}\\right\\rangle \\geq \\frac{c \\Theta}{C \\bar{A}}\n$$\n\nThis gives the desired result in (31c). ## L3.: Establishing gradient correlation. Our final goal is establishing gradient comparison between $\\boldsymbol{p}, \\boldsymbol{p}^{\\mathrm{mm}}$ for the same choice of $\\mu>0$ provided in (32). Define $\\overline{\\boldsymbol{p}}=\\left\\|\\boldsymbol{p}^{\\mathrm{mm}}\\right\\| \\boldsymbol{p} /\\|\\boldsymbol{p}\\|$ to be the normalized vector. Set notations $\\boldsymbol{a}_{i}=\\boldsymbol{K}_{i} \\overline{\\boldsymbol{p}}$, $\\overline{\\boldsymbol{a}}_{i}=\\boldsymbol{K}_{i} \\boldsymbol{p}^{\\mathrm{mm}}$, and $\\boldsymbol{\\gamma}_{i}=Y_{i} \\cdot \\boldsymbol{X}_{i} \\boldsymbol{v}$. To establish the result, using (34), we will prove that, for any $\\pi>0$, there is sufficiently large $R=R_{\\pi}$ such that for any $\\boldsymbol{p} \\in C_{\\mu, R}\\left(\\boldsymbol{p}^{m m}\\right)$ :\n\n$$\n\\begin{aligned}\n\\left\\langle-\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}}{\\|\\boldsymbol{p}\\|}\\right\\rangle & =-\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot\\left\\langle\\boldsymbol{a}_{i}, \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\boldsymbol{\\gamma}_{i}\\right\\rangle \\\\\n& \\leq-\\frac{1+\\pi}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot\\left\\langle\\overline{\\boldsymbol{a}}_{i}, \\mathbb{S}^{\\prime}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\boldsymbol{\\gamma}_{i}\\right\\rangle=(1+\\pi)\\left\\langle-\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle\n\\end{aligned}\n$$\n\nFollowing (36), for all $i \\in[n]$, for all $\\boldsymbol{q} \\in$ cone $_{\\mu}\\left(\\boldsymbol{p}^{m m}\\right)$ with $\\|\\boldsymbol{q}\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|, \\boldsymbol{a}^{\\prime}=\\boldsymbol{K} \\boldsymbol{q}$ and $\\boldsymbol{s}=\\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p})$, we have found\n\n$$\n\\left|\\boldsymbol{a}_{i}^{\\prime \\top} \\operatorname{diag}\\left(\\boldsymbol{s}_{i}\\right) \\gamma-\\boldsymbol{a}_{i}^{\\prime \\top} \\boldsymbol{s}_{i} \\boldsymbol{s}_{i}^{\\top} \\boldsymbol{\\gamma}_{i}-\\sum_{t \\in \\mathcal{T}_{i}}\\left(\\boldsymbol{a}_{i 1}^{\\prime}-\\boldsymbol{a}_{i t}^{\\prime}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\\right| \\leq 2 \\Gamma A\\left(\\left(1-\\boldsymbol{s}_{i 1}\\right)^{2}+Q_{i}\\right)\n$$\n\nPlugging in $\\boldsymbol{a}_{i}, \\overline{\\boldsymbol{a}}_{i}$ in the bound above and assuming $\\pi \\leq 1$ (w.l.o.g.), (44) is implied by the following stronger inequality\n\n$$\n\\begin{aligned}\n-\\frac{1}{n} & \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot\\left(6 \\Gamma A\\left(\\left(1-\\boldsymbol{s}_{i 1}\\right)^{2}+Q_{i}\\right)+\\sum_{t \\in \\mathcal{T}_{i}}\\left(\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\\right) \\\\\n& \\leq-\\frac{1+\\pi}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\sum_{t \\in \\mathcal{T}_{i}}\\left(\\overline{\\boldsymbol{a}}_{i 1}-\\overline{\\boldsymbol{a}}_{i t}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& =-\\frac{1+\\pi}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\sum_{t \\in \\mathcal{T}_{i}} \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\n\\end{aligned}\n$$\n\nFirst, we claim that $0.5 \\pi \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\geq 6 \\Gamma A\\left(\\left(1-s_{i 1}\\right)^{2}+Q_{i}\\right)$ for all $i \\in[n]$. The proof of this claim directly follows the earlier argument, namely, following (37), (38) and (39) which leads to the choice\n\n$$\nR \\geq \\frac{\\max \\left(2, \\delta^{-1}\\right)}{\\Theta} \\log \\left(\\frac{C_{0} \\cdot T \\Gamma A}{\\pi \\gamma_{\\min }^{g a p}}\\right)\n$$\n\nfor some constant $C_{0}>0$. Here, we choose sufficiently large $C_{0} \\geq 64 \\pi$ to ensure $R=R_{\\pi} \\geq \\bar{R}_{\\mu}$. Following this control over the perturbation term $6 \\Gamma A\\left(\\left(1-s_{i 1}\\right)^{2}+Q_{i}\\right)$, to conclude with the result, what remains is proving the comparison\n\n$$\n-\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\sum_{t \\in \\mathcal{T}_{i}}\\left(a_{i 1}-a_{i t}\\right) s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\leq-\\frac{1+0.5 \\pi}{n} \\sum_{i=1}^{n} \\ell_{i}^{\\prime} \\cdot \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\n$$\n\nTo proceed, we split the problem into two scenarios. Scenario 1: $\\left\\|\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{m m}\\right\\| \\leq \\epsilon=\\frac{\\pi}{4 A \\Theta}$ for some $\\epsilon>0$. In this scenario, for any token, we find that\n\n$$\n\\left|\\boldsymbol{a}_{i t}-\\overline{\\boldsymbol{a}}_{t}\\right|=\\left|\\boldsymbol{k}_{i t}^{\\top}\\left(\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{m m}\\right)\\right| \\leq A \\Theta \\epsilon=\\pi / 4\n$$\n\nConsequently, we obtain\n\n$$\n\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t} \\leq \\overline{\\boldsymbol{a}}_{i 1}-\\overline{\\boldsymbol{a}}_{i t}+2 A \\Theta \\epsilon=1+0.5 \\pi\n$$\n\nSimilarly, $\\boldsymbol{a}_{i 1}-a_{i t} \\geq 1-0.5 \\pi \\geq 0.5$. Since all terms $a_{i 1}-a_{i t}, s_{i t}, \\gamma_{i 1}-\\gamma_{i t}$ in (47) are nonnegative and $\\left(a_{i 1}-a_{i t}\\right) s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\leq(1+0.5 \\pi) s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)$, above implies the desired result (47). Scenario 2: $\\left\\|\\overline{\\boldsymbol{p}}-\\boldsymbol{p}^{\\text {mm }}\\right\\| \\geq \\epsilon=\\frac{\\pi}{4 A \\Theta}$. Since $\\overline{\\boldsymbol{p}}$ is not (locally) max-margin, in this scenario, for some $i \\in[n], v=v(\\epsilon)>0$, and $\\tau \\in \\mathcal{T}_{i}$, we have that\n\n$$\n\\overline{\\boldsymbol{p}}^{\\top}\\left(\\boldsymbol{k}_{i 1}-\\boldsymbol{k}_{i \\tau}\\right)=\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i \\tau} \\leq 1-2 v\n$$\n\nHere $\\tau=\\arg \\max _{t \\in \\mathcal{T}_{i}} \\overline{\\boldsymbol{p}}^{\\top} \\boldsymbol{k}_{i t}$ denotes the nearest point to $\\boldsymbol{k}_{i 1}$ (along the $\\overline{\\boldsymbol{p}}$ direction). Note that a nonneighbor $t \\in \\overline{\\mathcal{T}}_{i}$ cannot be nearest because $\\overline{\\boldsymbol{p}} \\in \\operatorname{cone}_{\\mu}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$ and (33) holds. Recall that $\\boldsymbol{s}_{i}=\\mathbb{S}\\left(\\bar{R} \\boldsymbol{a}_{i}\\right)$ where $\\bar{R}=\\|\\boldsymbol{p}\\| \\Theta \\geq R \\Theta$. To proceed, let $\\underline{\\boldsymbol{a}}_{i}:=\\min _{t \\in \\mathcal{T}_{i}} \\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t}$,\n\n$$\n\\mathcal{I}:=\\left\\{i \\in[n]: \\underline{\\boldsymbol{a}}_{i} \\leq 1-2 v\\right\\}, \\quad[n]-\\mathcal{I}:=\\left\\{i \\in[n]: 1-2 v<\\underline{\\boldsymbol{a}}_{i}\\right\\}\n$$\n\nFor all $i \\in[n]-\\mathcal{I}$,\n\n$$\n\\begin{aligned}\n\\sum_{t \\in \\mathcal{T}_{i}}\\left(a_{i 1}-a_{i t}\\right) s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) & -(1+0.5 \\pi) \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq(2 A-(1+0.5 \\pi)) \\Gamma \\sum_{t \\in \\mathcal{T}_{i}, a_{i 1}-a_{i t} \\geq 1+\\frac{\\pi}{2}} s_{i t} \\\\\n& \\leq(2 A-(1+0.5 \\pi)) \\Gamma T e^{-\\bar{R}\\left(1+\\frac{\\pi}{2}\\right)} \\\\\n& \\leq 2 A \\Gamma T e^{-\\bar{R}\\left(1+\\frac{\\pi}{2}\\right)}\n\\end{aligned}\n$$\n\nFor all $i \\in \\mathcal{I}$, split the tokens into two groups: Let $\\mathcal{N}_{i}$ be the group of tokens obeying $\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t} \\leq 1-v$ and $\\mathcal{T}_{i}-\\mathcal{N}_{i}$ be the rest of the neighbors. Observe that\n\n$$\n\\frac{\\sum_{t \\in \\mathcal{T}_{i}-\\mathcal{N}_{i}} \\boldsymbol{s}_{i t}}{\\sum_{t \\in \\mathcal{T}_{i}} \\boldsymbol{s}_{i t}} \\leq T \\frac{e^{v \\bar{R}}}{e^{2 v \\bar{R}}}=T e^{-\\bar{R} v}\n$$\n\nUsing $\\left|\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t}\\right| \\leq 2 A=2 \\max _{i \\in[n], t \\in[T]}\\left\\|\\boldsymbol{k}_{i t}\\right\\| / \\Theta$ and $\\gamma_{\\min }^{g a p}=\\min _{i \\in[n]} \\gamma_{i}^{g a p}=\\min _{i \\in[n]}\\left(\\gamma_{i 1}-\\max _{t \\in \\mathcal{T}_{i}} \\gamma_{i t}\\right)$, observe that\n\n$$\n\\sum_{t \\in \\mathcal{T}_{i}-\\mathcal{N}_{i}}\\left(a_{i 1}-a_{i t}\\right) s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\leq \\frac{2 \\Gamma A T e^{-\\bar{R} v}}{\\gamma_{\\min }^{g a p}} \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\n$$\n\nThus,\n\n$$\n\\begin{aligned}\n\\sum_{t \\in \\mathcal{T}_{i}}\\left(\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) & =\\sum_{t \\in \\mathcal{N}_{i}}\\left(\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)+\\sum_{t \\in \\mathcal{T}_{i}-\\mathcal{N}_{i}}\\left(a_{i 1}-a_{i t}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq \\sum_{t \\in \\mathcal{N}_{i}}(1-v) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)+\\frac{2 \\Gamma A T e^{-\\bar{R} v}}{\\gamma_{\\min }^{g a p}} \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq\\left(1-v+\\frac{2 \\Gamma A T e^{-\\bar{R} v}}{\\gamma_{\\min }^{g a p}}\\right) \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq\\left(1+\\frac{2 \\Gamma A T e^{-\\bar{R} v}}{\\gamma_{\\min }^{g a p}}\\right) \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\n\\end{aligned}\n$$\n\nHence, choosing\n\n$$\nR \\geq \\frac{1}{\\nu \\Theta} \\log \\left(\\frac{8 \\Gamma A T}{\\gamma_{\\min }^{g a p} \\pi}\\right)\n$$\n\nresults in that\n\n$$\n\\begin{aligned}\n& \\sum_{t \\in \\mathcal{T}_{i}}\\left(a_{i 1}-a_{i t}\\right) s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)-\\left(1+\\frac{\\pi}{2}\\right) \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq\\left(\\frac{2 \\Gamma A T e^{-\\bar{R} v}}{\\gamma_{\\min }^{g a p}}-\\frac{\\pi}{2}\\right) \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq-\\frac{\\pi}{4} \\sum_{t \\in \\mathcal{T}_{i}} s_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right) \\\\\n& \\leq-\\frac{\\pi}{4 T} \\gamma_{\\min }^{\\text {gap }} e^{-\\bar{R}(1-2 v)}\n\\end{aligned}\n$$\n\nHere, the last inequality follows from the fact that $\\sum_{t \\in \\mathcal{T}_{i}} s_{i t} \\geq \\max _{t \\in \\mathcal{T}_{i}} s_{i t} \\geq \\frac{e^{-\\bar{R}(1-2 v)}}{\\sum_{t=1}^{T} e^{-\\bar{R}\\left(a_{i}-a_{i t}\\right)}} \\geq e^{-\\bar{R}(1-2 v)} / T$. From Assumption A, we have $c_{\\min } \\leq-\\ell^{\\prime} \\leq c_{\\max }$ for some positive constants $c_{\\min }$ and $c_{\\max }$. It follows from (48) and (50) that\n\n$$\n\\begin{aligned}\n-\\frac{1}{n} \\sum_{i}^{n} \\ell_{i}^{\\prime} \\cdot & \\left(\\sum_{t \\in \\mathcal{T}_{i}}\\left(\\boldsymbol{a}_{i 1}-\\boldsymbol{a}_{i t}\\right) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)-\\sum_{t \\in \\mathcal{T}_{i}}(1+0.5 \\pi) \\boldsymbol{s}_{i t}\\left(\\gamma_{i 1}-\\gamma_{i t}\\right)\\right. \\\\\n& \\leq c_{\\max } 2 A \\Gamma T \\Gamma e^{-\\bar{R}\\left(1+\\frac{\\pi}{2}\\right)}-\\frac{c_{\\min }}{n T} \\cdot \\frac{\\pi \\gamma_{\\min }^{g a p}}{4} e^{-\\bar{R}(1-2 v)} \\\\\n& \\leq 0\n\\end{aligned}\n$$\n\nCombing with (49), this is guaranteed by choosing\n\n$$\nR \\geq \\max \\left\\{\\frac{1}{v \\Theta} \\log \\left(\\frac{8 \\Gamma A T}{\\gamma_{\\min }^{\\text {gap }} \\pi}\\right), \\frac{1}{(2 v+\\pi / 2) \\Theta} \\log \\left(\\frac{8 n \\Gamma A T^{2} c_{\\max }}{c_{\\min } \\gamma_{\\min }^{\\text {gap }} \\pi}\\right)\\right\\}\n$$\n\nwhere $v=v\\left(\\frac{\\pi}{4 A \\Theta}\\right)$ depends only on $\\pi$ and global problem variables.",
    "maxmargin-52": "Combining this with the prior $R$ choice (46) (by taking maximum), we conclude with the statement.",
    "maxmargin-53": "## B. 2 Proof of Theorem 1\n\nProof. This proof is a direct corollary of Lemma 14 which itself is a special case of the nonlinear head Theorem 8. Let us verify that $f(\\boldsymbol{X})=\\boldsymbol{v}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{X} \\boldsymbol{p})$ satisfies the assumptions of Lemma 14 where we replace the nonlinear head with linear $v$.",
    "maxmargin-54": "To see this, set the optimal sets to be the singletons $O_{i}=\\left\\{\\mathrm{opt}_{i}\\right\\}$. Given $\\left(\\boldsymbol{X}_{i}, Y_{i}\\right)$, let $\\boldsymbol{s}_{i}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$ and $q_{i}=q_{i}^{\\boldsymbol{p}}=\\sum_{t \\neq \\mathrm{opt}_{i}} \\boldsymbol{s}_{i t}$. Recalling score definition $\\gamma_{i}=Y_{i} \\cdot \\boldsymbol{X}_{i} \\boldsymbol{v}$ and setting $v_{i}:=\\boldsymbol{\\gamma}_{\\text {optt }_{i}}$ and $Z_{i}:=\\sum_{t \\neq \\mathrm{opt}_{i}} \\gamma_{i t} \\boldsymbol{s}_{i t}$, a particular prediction can be written as\n\n$$\n\\begin{aligned}\nY_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right) & =\\boldsymbol{\\gamma}_{i}^{\\top} \\boldsymbol{s}_{i}=\\gamma_{i \\mathrm{opt}_{i}}\\left(1-q_{i}\\right)+\\sum_{t \\neq \\mathrm{opt}_{i}} \\boldsymbol{\\gamma}_{i t} \\boldsymbol{s}_{i t} \\\\\n& =v_{i}\\left(1-q_{i}\\right)+Z_{i}\n\\end{aligned}\n$$\n\nTo proceed, we demonstrate the choices for $C, \\epsilon>0$. Let $C:=-\\min _{i \\in[n], t[T]]} \\gamma_{i t} \\wedge 0$ and $q_{\\max }=$ $\\max _{i \\in[n]} q_{i}$. Note that $Z_{i} \\geq \\sum_{t \\neq \\mathrm{opt}_{i}} \\gamma_{i t} \\boldsymbol{s}_{i t} \\geq q_{i} \\gamma_{\\min } \\geq-C q_{\\max }$. Now, using strict score optimality of opt $_{i}$ 's for all $i \\in[n]$, we set\n\n$$\n\\epsilon:=1-\\sup _{i \\in[n]} \\frac{\\sum_{t \\neq \\mathrm{opt}_{i}} \\gamma_{i t} s_{i t}}{v_{i} q_{i}} \\geq 1-\\sup _{i \\in[n]} \\frac{\\sup _{t \\neq \\mathrm{opt}_{i}} \\gamma_{i t}}{\\gamma_{i \\mathrm{opt}_{i}}}>0\n$$\n\nWe conclude by observing $Z_{i} \\leq v_{i} q_{i} \\frac{\\sum_{l \\neq \\text { opt }} \\gamma_{i t} s_{i t}}{v_{i} q_{i}} \\leq v_{i} q_{i} \\epsilon$ as desired.",
    "maxmargin-55": "## B. 3 Proof of Theorem 2\n\nProof. We first show that $\\lim _{t \\rightarrow \\infty}\\|\\boldsymbol{p}(t)\\|=\\infty$. From Lemma 4, we have\n\n$$\n\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{p}^{m m \\star}\\right\\rangle=\\frac{1}{n} \\sum_{i=1}^{n} \\ell^{\\prime}\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)\\right) \\cdot\\left\\langle\\boldsymbol{K}_{i} \\boldsymbol{p}^{m m \\star}, \\mathbb{S}^{\\prime}\\left(\\boldsymbol{a}_{i}\\right) \\gamma_{i}\\right\\rangle\n$$\n\nwhere $\\boldsymbol{\\gamma}_{i}=Y_{i} \\cdot \\boldsymbol{X}_{i} \\boldsymbol{v}$ and $\\boldsymbol{a}_{i}=\\boldsymbol{K}_{i} \\boldsymbol{p}$. ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-29.jpg?height=45&width=1389&top_left_y=563&top_left_x=368) $\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{p}^{m m \\star}\\right\\rangle$ cannot be equal to zero, as a sum of negative terms. Therefore, there are no finite critical points $\\boldsymbol{p}$, for which $\\nabla \\mathcal{L}(\\boldsymbol{p})=0$. On the other hand, Lemma 6 states $\\nabla \\mathcal{L}(\\boldsymbol{p}(t)) \\rightarrow 0$ which implies that $\\|\\boldsymbol{p}(t)\\| \\rightarrow \\infty$. Next, we provide the directional convergence for the setting $n=1$. Let us consider an arbitrary value of $\\epsilon \\in(0,1)$ and set $\\pi=\\epsilon /(1-\\epsilon)$. As $\\lim _{t \\rightarrow \\infty}\\|\\boldsymbol{p}(t)\\|=\\infty$, we can select a specific $t_{\\epsilon}$ such that for all $t \\geq t_{\\epsilon}$, it holds that $\\|\\boldsymbol{p}(t)\\| \\geq R_{\\epsilon} \\vee 1 / 2$ for any choice of $R_{\\epsilon}$. To proceed, we choose $R_{\\epsilon}$ based on Lemma 5 so that for any $t \\geq t_{\\epsilon}$, we have that\n\n$$\n\\left\\langle-\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}^{m m \\star}}{\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|}\\right\\rangle \\geq(1-\\epsilon)\\left\\langle-\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}\\right\\rangle\n$$\n\nMultiplying both sides by the stepsize $\\eta$ and using the gradient descent update, we get\n\n$$\n\\begin{aligned}\n\\left\\langle\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t), \\frac{\\boldsymbol{p}^{m m \\star}}{\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|}\\right\\rangle & \\geq(1-\\epsilon)\\left\\langle\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t), \\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}\\right\\rangle \\\\\n& =\\frac{(1-\\epsilon)}{2\\|\\boldsymbol{p}(t)\\|}\\left(\\|\\boldsymbol{p}(t+1)\\|^{2}-\\|\\boldsymbol{p}(t)\\|^{2}-\\|\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t)\\|^{2}\\right) \\\\\n& \\geq(1-\\epsilon)\\left(\\frac{1}{2\\|\\boldsymbol{p}(t)\\|}\\left(\\|\\boldsymbol{p}(t+1)\\|^{2}-\\|\\boldsymbol{p}(t)\\|^{2}\\right)-\\|\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t)\\|^{2}\\right) \\\\\n& \\geq(1-\\epsilon)\\left(\\|\\boldsymbol{p}(t+1)\\|-\\|\\boldsymbol{p}(t)\\|-\\|\\boldsymbol{p}(t+1)-\\boldsymbol{p}(t)\\|^{2}\\right) \\\\\n& \\geq(1-\\epsilon)(\\|\\boldsymbol{p}(t+1)\\|-\\|\\boldsymbol{p}(t)\\|-2 \\eta(\\mathcal{L}(\\boldsymbol{p}(t))-\\mathcal{L}(\\boldsymbol{p}(t+1))))\n\\end{aligned}\n$$\n\nHere, the second inequality is obtained from $\\|\\boldsymbol{p}(t)\\| \\geq 1 / 2$; the third inequality follows since for any $a, b>0$, we have $\\left(a^{2}-b^{2}\\right) /(2 b)-(a-b) \\geq 0$; and the last inequality uses Lemma 6 . Summing the above inequality over $t \\geq t_{\\epsilon}$ gives\n\n$$\n\\left\\langle\\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}, \\frac{\\boldsymbol{p}^{m m \\star}}{\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|}\\right\\rangle \\geq 1-\\epsilon+\\frac{C(\\epsilon, \\eta)}{\\|\\boldsymbol{p}(t)\\|}\n$$\n\nfor some finite constant $C(\\epsilon, \\eta)$ defined as\n\n$$\nC(\\epsilon, \\eta):=\\left\\langle\\boldsymbol{p}\\left(t_{\\epsilon}\\right), \\frac{\\boldsymbol{p}^{m m \\star}}{\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|}\\right\\rangle-(1-\\epsilon)\\left\\|\\boldsymbol{p}\\left(t_{\\epsilon}\\right)\\right\\|-2 \\eta(1-\\epsilon)\\left(\\mathcal{L}\\left(\\boldsymbol{p}\\left(t_{\\epsilon}\\right)\\right)-\\mathcal{L}^{*}\\right)\n$$\n\nwhere $\\mathcal{L}^{*}$ denotes the minimum objective. Since $\\|\\boldsymbol{p}(t)\\| \\rightarrow \\infty$, we get\n\n$$\n\\liminf _{t \\rightarrow \\infty}\\left\\langle\\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}, \\frac{\\boldsymbol{p}^{m m \\star}}{\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|}\\right\\rangle \\geq 1-\\epsilon\n$$\n\nGiven that we can choose any value of $\\epsilon \\in(0,1)$, we have $\\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\| \\rightarrow \\boldsymbol{p}^{m m \\star} /\\left\\|\\boldsymbol{p}^{m m \\star}\\right\\|$.",
    "maxmargin-56": "## B. 4 Proof of Theorem 3\n\nProof. Following the proof of Lemma 7, let $\\left(\\mathcal{T}_{i}\\right)_{i=1}^{n}$ denote the sets of SVM-neighbors as defined in Definition 2. We define $\\overline{\\mathcal{T}}_{i}=[T]-\\mathcal{T}_{i}-\\left\\{\\alpha_{i}\\right\\}$ as the tokens that are non-SVM neighbors. Additionally, let $\\mu$ be defined as in (32). Let us denote the initialization lower bound as $R_{\\mu}^{0}:=R$, where $R$ is given in the Theorem 3's statement. Consider an arbitrary value of $\\epsilon \\in(0, \\mu / 2)$ and let $1 /(1+\\pi)=1-\\epsilon$. We additionally denote $R_{\\epsilon} \\leftarrow R_{\\pi} \\vee 1 / 2$ where $R_{\\pi}$ was defined in Lemma 7(L3.). At initialization $\\boldsymbol{p}(0)$, we set $\\epsilon=\\mu / 2$ to obtain $R_{\\mu}^{0}=R_{\\mu / 2}$ and provide the proof in four steps:\nStep 1: There are no stationary points within $C_{\\mu, R_{\\mu}^{0}}\\left(\\boldsymbol{p}^{\\mathbf{m m}}\\right)$. We begin by proving that there are no stationary points within $\\mathcal{C}_{\\mu, R_{\\mu}^{0}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. Then, since $R_{\\mu}^{0} \\geq \\bar{R}_{\\mu}$ per Lemma 7, we can apply (L2.) to find that: For all $\\boldsymbol{q}, \\boldsymbol{p} \\in \\operatorname{cone}_{\\mu}\\left(\\boldsymbol{p}^{m m}\\right)$ with $\\boldsymbol{q} \\neq 0$ and $\\|\\boldsymbol{p}\\| \\geq R_{\\mu}^{0}$, we have that $-\\boldsymbol{q}^{\\top} \\nabla \\mathcal{L}(\\boldsymbol{p})$ is strictly positive. Step 2: It follows from Lemma 7(L3.) that, for all $\\epsilon \\in(0, \\mu / 2)$, all $\\boldsymbol{p} \\in \\mathcal{C}_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)$ satisfy\n\n$$\n\\left\\langle-\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\geq(1-\\epsilon)\\left\\langle-\\nabla \\mathcal{L}(\\boldsymbol{p}), \\frac{\\boldsymbol{p}}{\\|\\boldsymbol{p}\\|}\\right\\rangle\n$$\n\nThe argument above applies to a general $\\epsilon \\in(0, \\mu / 2)$. However, at initialization $\\boldsymbol{p}(0)$, we set $\\epsilon=\\mu / 2$ to obtain our earlier $R_{\\mu}^{0}$ choice. To proceed, for any $\\epsilon \\in(0, \\mu / 2)$, we will show that after gradient descent enters the conic set $C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)$ for the first time, it will never leave the set. Let $t_{\\epsilon}$ be the first time gradient descent enters $\\mathcal{C}_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. In Step 4 , we will prove that such $t_{\\epsilon}$ is guaranteed to exist. Additionally, for $\\epsilon \\leftarrow \\mu / 2$, note that $t_{\\epsilon}=0$ i.e. the point of initialization. Step 3: Updates remain inside the cone $C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{\\boldsymbol{m m}}\\right)$. By leveraging the results from Step 1 and Step 2, we demonstrate that the gradient iterates, with an appropriate constant step size, starting from $\\boldsymbol{p}\\left(t_{\\epsilon}\\right) \\in C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)$, remain within this cone. We proceed by induction. Suppose that the claim holds up to iteration $t \\geq t_{\\epsilon}$. This implies that $\\boldsymbol{p}(t) \\in C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)$. Hence, recalling cone definition, for $\\mu>0$ and $R_{\\epsilon}$, we have $\\left\\langle\\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}, \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\geq 1-\\mu$ and $\\|\\boldsymbol{p}(t)\\| \\geq R_{\\epsilon}$. Let\n\n$$\n\\rho(t):=-\\frac{1}{1-\\epsilon}\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle\n$$\n\nNote that $\\rho(t)>0$ due to Step 1. This together with the gradient descent update rule gives\n\n$$\n\\begin{aligned}\n\\left\\langle\\frac{\\boldsymbol{p}(t+1)}{\\|\\boldsymbol{p}(t)\\|}, \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle & =\\left\\langle\\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}-\\frac{\\eta}{\\|\\boldsymbol{p}(t)\\|} \\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\\\\n& \\geq 1-\\mu-\\frac{\\eta}{\\|\\boldsymbol{p}(t)\\|}\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\\\\n& =1-\\mu+\\frac{\\eta \\rho(t)(1-\\epsilon)}{\\|\\boldsymbol{p}(t)\\|}\n\\end{aligned}\n$$\n\nNote that from Lemma 7, we have $\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\boldsymbol{p}(t)\\rangle<0$ which implies that $\\|\\boldsymbol{p}(t+1)\\| \\geq\\|\\boldsymbol{p}(t)\\|$. This together with $R_{\\epsilon}$ definition and $\\|\\boldsymbol{p}(t)\\| \\geq 1 / 2$ implies that\n\n$$\n\\begin{aligned}\n\\|\\boldsymbol{p}(t+1)\\| & \\leq \\frac{1}{2\\|\\boldsymbol{p}(t)\\|}\\left(\\|\\boldsymbol{p}(t+1)\\|^{2}+\\|\\boldsymbol{p}(t)\\|^{2}\\right) \\\\\n& =\\frac{1}{2\\|\\boldsymbol{p}(t)\\|}\\left(2\\|\\boldsymbol{p}(t)\\|^{2}-2 \\eta\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\boldsymbol{p}(t)\\rangle+\\eta^{2}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}\\right) \\\\\n& \\leq\\|\\boldsymbol{p}(t)\\|-\\frac{\\eta}{\\|\\boldsymbol{p}(t)\\|}\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\boldsymbol{p}(t)\\rangle+\\eta^{2}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}\n\\end{aligned}\n$$\n\nHence, using (53)\n\n$$\n\\begin{aligned}\n\\frac{\\|\\boldsymbol{p}(t+1)\\|}{\\|\\boldsymbol{p}(t)\\|} & \\leq 1-\\frac{\\eta}{\\|\\boldsymbol{p}(t)\\|}\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}\\right\\rangle+\\eta^{2} \\frac{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}}{\\|\\boldsymbol{p}(t)\\|} \\\\\n& \\leq 1-\\frac{\\eta}{(1-\\epsilon)\\|\\boldsymbol{p}(t)\\|}\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}(t)), \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle+\\eta^{2} \\frac{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}}{\\|\\boldsymbol{p}(t)\\|} \\\\\n& =1+\\frac{\\eta \\rho(t)}{\\|\\boldsymbol{p}(t)\\|}+\\frac{\\eta^{2}\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}}{\\|\\boldsymbol{p}(t)\\|}=: C_{1}(\\rho(t), \\eta)\n\\end{aligned}\n$$\n\nHere, the second inequality follows from (53). Now, it follows from (54a) and (54c) that\n\n$$\n\\begin{aligned}\n\\left\\langle\\frac{\\boldsymbol{p}(t+1)}{\\|\\boldsymbol{p}(t+1)\\|}, \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle & \\geq \\frac{1}{C_{1}(\\rho(t), \\eta)}\\left(1-\\mu+\\frac{\\eta \\rho(t)(1-\\epsilon)}{\\|\\boldsymbol{p}(t)\\|}\\right) \\\\\n& =1-\\mu+\\frac{1}{C_{1}(\\rho(t), \\eta)}\\left((1-\\mu)\\left(1-C_{1}(\\rho(t), \\eta)\\right)+\\frac{\\eta \\rho(t)(1-\\epsilon)}{\\|\\boldsymbol{p}(t)\\|}\\right) \\\\\n& =1-\\mu+\\frac{\\eta}{C_{1}(\\rho(t), \\eta)}\\left((\\mu-1)\\left(\\frac{\\rho(t)}{\\|\\boldsymbol{p}(t)\\|}+\\frac{\\eta\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}}{\\|\\boldsymbol{p}(t)\\|}\\right)+\\frac{\\rho(t)(1-\\epsilon)}{\\|\\boldsymbol{p}(t)\\|}\\right) \\\\\n& =1-\\mu+\\frac{\\eta}{C_{1}(\\rho(t), \\eta)}\\left(\\frac{\\rho(t)(\\mu-\\epsilon)}{\\|\\boldsymbol{p}(t)\\|}-\\eta(1-\\mu) \\frac{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}}{\\|\\boldsymbol{p}(t)\\|}\\right) \\\\\n& \\geq 1-\\mu\n\\end{aligned}\n$$\n\nwhere the last inequality uses our choice of stepsize $\\eta \\leq 1 / L_{p}$ in Theorem 3's statement.",
    "maxmargin-57": "Specifically, we need $\\eta$ to be small to ensure the last inequality. We will guarantee this by choosing a proper $R_{\\epsilon}$ in Lemma 7. Specifically, Lemma 7 leaves the choice of $C_{0}$ in $R_{\\epsilon}$ lower bound of (46) open (it can always be chosen larger). Here, by choosing $C_{0} \\gtrsim 1 / L_{p}$ will ensure $\\eta \\leq 1 / L_{p}$ works well. To proceed, we have that\n\n$$\n\\begin{aligned}\n\\frac{(\\mu-\\epsilon)}{1-\\mu} \\frac{\\rho(t)}{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|^{2}} & \\geq \\frac{\\mu-\\epsilon}{1-\\mu} \\frac{1}{1-\\epsilon} \\frac{c}{C} \\frac{\\Theta}{\\bar{A}} \\frac{1}{\\bar{A} C T} e^{R_{\\mu}^{0} \\Theta / 2} \\\\\n& \\geq \\frac{\\mu}{2(1-\\mu)\\left(1-\\frac{\\mu}{2}\\right)} \\frac{c}{C} \\frac{\\Theta}{\\bar{A}} \\frac{1}{\\bar{A} C T} e^{R_{\\mu}^{0} \\Theta / 2} \\geq \\eta\n\\end{aligned}\n$$\n\nHere, the second inequality uses our choice of $\\epsilon \\in(0, \\mu / 2)$ (see Step 2), and the first inequality is obtained from Lemma 7 since\n\n$$\n\\begin{aligned}\n& \\frac{\\rho(t)}{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|}=-\\frac{1}{1-\\epsilon}\\left\\langle\\frac{\\nabla \\mathcal{L}(\\boldsymbol{p}(t))}{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|}, \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\geq \\frac{1}{1-\\epsilon} \\frac{c}{C} \\frac{\\Theta}{\\bar{A}} \\\\\n& \\frac{1}{\\|\\nabla \\mathcal{L}(\\boldsymbol{p}(t))\\|} \\geq \\frac{1}{\\bar{A} C \\frac{1}{n} \\sum_{i=1}^{n}\\left(1-\\boldsymbol{s}_{i \\alpha_{i}}\\right)} \\geq \\frac{1}{\\bar{A} C T e^{-R_{\\mu}^{0} \\Theta / 2}}\n\\end{aligned}\n$$\n\nfor some data dependent constants $c$ and $C, \\bar{A}=\\max _{i \\in[n], t, \\tau[T]}\\left\\|\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\tau}\\right\\|$, and $\\Theta=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. Next, we will demonstrate that the choice of $\\eta$ in (56) does indeed meet our step size condition as stated in the theorem, i.e., $\\eta \\leq 1 / L_{p}$. Recall that $1 /(1+\\pi)=1-\\epsilon$, which implies that $\\pi=\\epsilon /(1-\\epsilon)$. Combining this with (46), we obtain:\n\n$$\n\\begin{aligned}\nR_{\\pi} & \\geq \\frac{\\max \\left(2, \\delta^{-1}\\right)}{\\Theta} \\log \\left(\\frac{C_{0} T \\Gamma A}{\\pi \\gamma_{\\min }^{\\text {gap }}}\\right), \\quad \\text { where } \\quad C_{0} \\geq 64 \\pi \\\\\n& \\Rightarrow R_{\\epsilon} \\geq \\frac{\\max \\left(2, \\delta^{-1}\\right)}{\\Theta} \\log \\left(\\frac{(1-\\epsilon) C_{0} T \\Gamma A}{\\epsilon \\gamma_{\\min }^{\\text {gap }}}\\right), \\quad \\text { where } \\quad C_{0} \\geq 64 \\frac{\\epsilon}{1-\\epsilon}\n\\end{aligned}\n$$\n\nOn the other hand, at the initialization, we have $\\epsilon=\\mu / 2$ which implies that\n\n$$\nR_{\\mu}^{0} \\geq \\frac{\\max \\left(2, \\delta^{-1}\\right)}{\\Theta} \\log \\left(\\frac{(2-\\mu) C_{0} T \\Gamma A}{\\mu \\gamma_{\\min }^{\\operatorname{gap}}}\\right), \\quad \\text { where } \\quad C_{0} \\geq 64 \\frac{\\mu}{(2-\\mu)}\n$$\n\nIn the following, we will determine a lower bound on $C_{0}$ such that our step size condition in Theorem 3's statement, i.e., $\\eta \\leq 1 / L_{p}$, is satisfied. Note that for the choice of $\\eta$ in (56) to meet the condition $\\eta \\leq 1 / L_{p}$, the following condition must hold:\n\n$$\n\\frac{1}{L_{p}} \\leq \\frac{\\mu}{(2-\\mu)} \\frac{1}{C_{2} T} e^{R_{\\mu}^{0} \\Theta / 2} \\Rightarrow R_{\\mu}^{0} \\geq \\frac{2}{\\Theta} \\log \\left(\\frac{1}{L_{p}} \\frac{(2-\\mu)}{\\mu} C_{2} T\\right)\n$$\n\nwhere $C_{2}=(1-\\mu) \\frac{\\bar{A}^{2} C^{2}}{\\Theta c}$. This together with (57) implies that for sufficiently large\n\n$$\nR_{\\mu}^{0} \\geq \\frac{\\max \\left(2, \\delta^{-1}\\right)}{\\Theta} \\log \\left(\\frac{(2-\\mu) C_{3} T}{\\mu}\\right), \\quad \\text { where } \\quad C_{3}=\\frac{C_{0} \\Gamma A}{\\gamma_{\\min }^{\\text {gap }}} \\vee \\frac{C_{2}}{L_{p}}\n$$\n\nthe step size bound in (56) ensures that $\\eta \\leq 1 / L_{p}$ guarantees (55). Hence, $\\boldsymbol{p}(t+1$ ) remains within the cone, i.e., $\\boldsymbol{p}(t+1) \\in C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. Step 4: The correlation of $\\boldsymbol{p}(t)$ and $\\boldsymbol{p}^{\\boldsymbol{m m}}$ increases over $t$. The remainder is similar to the proof of Theorem 2. From Step 3, we have that all iterates remain within the initial conic set i.e. $\\boldsymbol{p}(t) \\in$ $C_{\\mu, R_{\\mu}^{0}}\\left(\\boldsymbol{p}^{m m}\\right)$ for all $t \\geq 0$. Note that it follows from Lemma 7 that $\\left\\langle\\nabla \\mathcal{L}(\\boldsymbol{p}), \\boldsymbol{p}^{m m} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|\\right\\rangle<0$, for any finite $\\boldsymbol{p} \\in \\mathcal{C}_{\\mu, R_{\\mu}^{0}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. Hence, there are no finite critical points $\\boldsymbol{p} \\in \\mathcal{C}_{\\mu, R_{\\mu}^{0}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$, for which $\\nabla \\mathcal{L}(\\boldsymbol{p})=0$. Now, based on Lemma 6 , which guarantees that $\\nabla \\mathcal{L}(\\boldsymbol{p}(t)) \\rightarrow 0$, this implies that $\\|\\boldsymbol{p}(t)\\| \\rightarrow \\infty$. Consequently, for any choice of $\\epsilon \\in(0, \\mu / 2)$ there is a time $t_{\\epsilon}$ such that, for all $t \\geq t_{\\epsilon}$, $\\boldsymbol{p}(t) \\in C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$. Once within $C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)$, following similar steps in (51) and (52), for any $t \\geq t_{\\epsilon}$,\n\n$$\n\\left\\langle\\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}, \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\geq 1-\\epsilon+\\frac{C_{2}(\\epsilon, \\eta)}{\\|\\boldsymbol{p}(t)\\|}, \\quad \\boldsymbol{p}(t) \\in C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)\n$$\n\nfor some finite constant $C_{2}(\\epsilon, \\eta)$. Consequently,\n\n$$\n\\liminf _{t \\rightarrow \\infty}\\left\\langle\\frac{\\boldsymbol{p}(t)}{\\|\\boldsymbol{p}(t)\\|}, \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}\\right\\rangle \\geq 1-\\epsilon, \\text { where } \\boldsymbol{p}(t) \\in C_{\\mu, R_{\\epsilon}}\\left(\\boldsymbol{p}^{m m}\\right)\n$$\n\nSince the choice of $\\epsilon \\in(0, \\mu / 2)$ is arbitrary, we obtain $\\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\| \\rightarrow \\boldsymbol{p}^{m m} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$.",
    "maxmargin-58": "## B. 5 Proof of Theorem 4\n\n## B.5.1 Supporting Lemma\n\nWe present a lemma that will aid in simplifying our analysis. We begin with a definition. ## Definition 3 (Selected-tokens, Neighbors, Margins, and Neighbor-optimality of a direction)\n\nLet $\\boldsymbol{q} \\in \\mathbb{R}^{d}-\\{\\boldsymbol{0}\\}$ and $\\left(Y_{i}, \\boldsymbol{K}_{i}, \\boldsymbol{X}_{i}\\right)_{i=1}^{n}$ be our dataset. We define the (possibly non-unique) selected-tokens of $\\boldsymbol{q}$ as follows. ${ }^{3}$\n\n$$\n\\alpha_{i} \\in \\arg \\max _{t \\in[T]} \\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{q}\n$$\n\nNext, we define the margin and directional-neighbors for $\\boldsymbol{q}$ as the minimum margin tokens to the selected-tokens, i.e.,\n\n$$\n\\begin{aligned}\n\\Gamma_{\\boldsymbol{q}} & =\\min _{i \\in[n], t \\neq \\alpha_{i}}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q} \\\\\n\\mathcal{M}_{\\boldsymbol{q}} & =\\left\\{(i, t) \\mid\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q}=\\Gamma_{q}\\right\\}\n\\end{aligned}\n$$\n\nFinally, we say that $\\boldsymbol{q}$ is neighbor-optimal if the scores of its directional-neighbors are strictly less than the corresponding selected-token. Concretely, for all $(i, t) \\in \\mathcal{M}_{q}$, we require that\n\n$$\n\\boldsymbol{\\gamma}_{i t}=Y_{i} \\cdot \\boldsymbol{x}_{i t}^{\\top} \\boldsymbol{v}<\\boldsymbol{\\gamma}_{i \\alpha_{i}}=Y_{i} \\cdot \\boldsymbol{x}_{i \\alpha_{i}}^{\\top} \\boldsymbol{v}\n$$\n\nLemma 8 (When does one direction dominate another?) Suppose $\\boldsymbol{q}, \\boldsymbol{p} \\in \\mathbb{R}^{d}$ be two unit Euclidean norm vectors with identical selected tokens. Specifically, for each $i \\in[n]$, there exists unique $\\alpha_{i} \\in[T]$ such that $\\alpha_{i}=\\arg \\max _{t \\in[T]} \\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{q}=\\arg \\max _{t \\in[T]} \\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{p}$. Suppose directional margins obey $\\Gamma_{q}<\\Gamma_{p}$ and set $\\delta_{\\Gamma}=\\Gamma_{p}-\\Gamma_{q}$. - Suppose $\\boldsymbol{q}$ and $\\boldsymbol{p}$ are both neighbor-optimal. Then, for some $R\\left(\\delta_{\\Gamma}\\right)$ and all $R>R\\left(\\delta_{\\Gamma}\\right)$, we have that $\\mathcal{L}(R \\cdot \\boldsymbol{p})<\\mathcal{L}(R \\cdot \\boldsymbol{q})$. - Suppose $\\boldsymbol{q}$ has a unique directional-neighbor and is not neighbor-optimal (i.e. this neighbor has higher score). Let $\\delta_{q}$ be the margin difference between unique directional-neighbor and the second-most minimum-margin neighbor (i.e. the one after the unique one, see (65)) of $\\boldsymbol{q}$. Then, for some $R\\left(\\delta_{\\Gamma} \\wedge \\delta_{q}\\right)$ and all $R>R\\left(\\delta_{\\Gamma} \\wedge \\delta_{q}\\right)$, we have that $\\mathcal{L}(R \\cdot \\boldsymbol{q})<\\mathcal{L}(R \\cdot \\boldsymbol{p})$. Proof. We prove these two statements in order. First define the directional risk baseline induced by letting $R \\rightarrow \\infty$ and purely selecting the tokens $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$. This is given by\n\n$$\n\\mathcal{L}_{\\star}:=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{x}_{i \\alpha_{i}}\\right)\n$$\n\n[^2]We evaluate $\\boldsymbol{q}, \\boldsymbol{p}$ with respect to $\\mathcal{L}_{\\star}$.",
    "maxmargin-59": "To proceed, let $\\boldsymbol{s}_{i}=\\mathbb{S}\\left(R \\boldsymbol{K}_{i} \\boldsymbol{q}\\right)$. Define $\\Gamma_{\\boldsymbol{q}}^{i t}=\\boldsymbol{k}_{i \\alpha_{i}}^{\\top} \\boldsymbol{q}-\\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{q}$. Note that, the smallest value for $t \\neq \\alpha_{i}$ is achieved for $\\Gamma_{q}$. For sufficiently large $R \\gtrsim O\\left(\\log (T) / \\Gamma_{q}\\right)$, observe that, for $t \\neq \\alpha_{i}$\n\n$$\ne^{-R R_{q}^{i t}} \\geq \\boldsymbol{s}_{i t}=\\frac{e^{R k_{i t}^{\\top} \\boldsymbol{q}}}{\\sum_{t \\in[T]} e^{R k_{i t}^{\\top} \\boldsymbol{q}}} \\geq 0.5 e^{-R R_{q}^{i t}}\n$$\n\nRecalling the score definition and let $M_{+}, M_{-}$be the upper and lower bounds on $-\\ell^{\\prime}$ over its bounded domain that scores fall on, respectively. Note that, for some intermediate $M_{+} \\geq M_{i} \\geq M_{-}$values, we have\n\n$$\n\\begin{aligned}\n\\mathcal{L}(R \\boldsymbol{q})-\\mathcal{L}_{\\star} & =\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(\\sum_{t \\in[T]} s_{i t} \\gamma_{i t}\\right)-\\ell\\left(\\gamma_{i \\alpha_{i}}\\right) \\\\\n& =\\frac{1}{n} \\sum_{i=1}^{n} M_{i} \\sum_{t \\neq \\alpha_{i}} s_{i t}\\left(\\gamma_{i \\alpha_{i}}-\\gamma_{i t}\\right)\n\\end{aligned}\n$$\n\nNow, using (62) for a refreshed $M_{+} \\geq M_{i t} \\geq 0.5 M_{-}$values, we can write\n\n$$\n\\mathcal{L}(R \\boldsymbol{q})-\\mathcal{L}_{\\star}=\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{t \\neq \\alpha_{i}} M_{i t} e^{-R \\Gamma_{q}^{i t}}\\left(\\gamma_{i \\alpha_{i}}-\\gamma_{i t}\\right)\n$$\n\nThe same bound also applies to $\\boldsymbol{p}$ with some $M_{i t}^{\\prime}$, multipliers\n\n$$\n\\mathcal{L}(R \\boldsymbol{p})-\\mathcal{L}_{\\star}=\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{t \\neq \\alpha_{i}} M_{i t}^{\\prime} e^{-R \\Gamma_{p}^{i t}}\\left(\\gamma_{i \\alpha_{i}}-\\gamma_{i t}\\right)\n$$\n\nWe can now proceed with the proof. Case 1: $\\boldsymbol{q}$ and $\\boldsymbol{p}$ are both neighbor-optimal. This means that $\\gamma_{i \\alpha_{i}}-\\gamma_{i t}>0$ for all $i \\in[n], t \\neq \\alpha_{i}$. Let $K_{+}>K_{-}>0$ be upper and lower bounds on $\\gamma_{i \\alpha_{i}}-\\gamma_{i t}$ values. We can now upper bound the right hand side of (63) via\n\n$$\nM_{+} K_{+} T e^{-R \\Gamma_{q}} \\geq \\mathcal{L}(R \\boldsymbol{q})-\\mathcal{L}_{\\star} \\geq \\frac{1}{2 n} M_{-} K_{-} e^{-R \\Gamma_{q}}\n$$\n\nConsequently, $\\mathcal{L}(R \\boldsymbol{q})>\\mathcal{L}(R \\boldsymbol{p})$ as soon as $\\frac{1}{2 n} M_{-} K_{-} e^{-R \\Gamma_{q}}>M_{+} K_{+} T e^{-R \\Gamma_{p}}$. Since $M_{+}, K_{+}, n, T$ are global constants, this happens under the stated condition on the margin gap $\\Gamma_{p}-\\Gamma_{q}$. Case 2: $q$ has a unique directional-neighbor and is not neighbor-optimal. In this scenario, $\\mathcal{L}(R \\boldsymbol{q})-\\mathcal{L}_{\\star}$ is actually negative for large $R$. To proceed, define the maximum score difference $K_{+}=\\sup _{i, t \\neq \\alpha_{i}}\\left|\\gamma_{i \\alpha_{i}}-\\gamma_{i t}\\right|$. Also let $(j, \\beta)$ be the unique directional neighbor achieving the minimum margin $\\Gamma_{q}$. Then, $\\delta_{q}$ - the margin difference between unique directional-neighbor and the second minimum-margin neighbor (i.e. the one after the unique one) of $\\boldsymbol{q}$ - is defined as\n\n$$\n\\delta_{\\boldsymbol{q}}=\\min _{i \\in[n], t \\neq \\alpha_{i},(i, t) \\neq(j, \\beta)} \\Gamma_{q}^{i t}-\\Gamma_{q}\n$$\n\nTo proceed, we can write\n\n$$\n\\mathcal{L}(R \\boldsymbol{p})-\\mathcal{L}_{\\star} \\geq-M_{+} K_{+} T e^{-R \\Gamma_{p}}\n$$\n\nOn the other hand, setting $\\kappa=\\gamma_{j \\beta}-\\gamma_{j \\alpha_{j}}>0$, we can bound\n\n$$\n\\begin{aligned}\n\\mathcal{L}(R \\boldsymbol{q})-\\mathcal{L}_{\\star} & =-\\frac{1}{n} M_{j \\beta} e^{-R \\Gamma_{q}} K+\\frac{1}{n} \\sum_{i \\in[n], t \\neq \\alpha_{i},(i, t) \\neq(j, \\beta)} M_{i t} e^{-R \\Gamma_{q}^{i t}}\\left(\\gamma_{i \\alpha_{i}}-\\gamma_{i t}\\right) \\\\\n& \\leq-\\frac{1}{n} M_{-} e^{-R \\Gamma_{q}} K+M_{+} K_{+} T e^{-R\\left(\\Gamma_{q}+\\delta_{q}\\right)}\n\\end{aligned}\n$$\n\nConsequently, we have found that $\\mathcal{L}(R \\boldsymbol{p})>\\mathcal{L}(R \\boldsymbol{q})$ as soon as\n\n$$\n\\frac{1}{n} M_{-} e^{-R \\Gamma_{q}} K \\geq M_{+} K_{+} T\\left(e^{-R\\left(\\Gamma_{q}+\\delta_{q}\\right)}+e^{-R \\Gamma_{p}}\\right)\n$$\n\nThis happens when $R \\gtrsim \\frac{1}{\\delta_{q} \\wedge\\left(\\Gamma_{p}-\\Gamma_{q}\\right)}$ (up to logarithmic terms) establishing the desired statement. ## B.5.2 Proof of Theorem 4\n\nDefine the locally-optimal unit directions\n\n$$\n\\mathcal{P}^{m m}=\\left\\{\\left.\\frac{\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})}{\\left\\|\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})\\right\\|} \\right\\rvert\\, \\boldsymbol{\\alpha} \\text { is a locally-optimal set of indices }\\right\\}\n$$\n\nThe theorem below shows that cone-restricted regularization paths can only directionally converge to an element of this set. Theorem 7 (Non-LOMM Regularization Paths Fail) Fix a unit Euclidean norm vector $\\boldsymbol{q} \\in \\mathbb{R}^{\\text {d }}$ such that $\\boldsymbol{q} \\notin \\mathcal{P}^{\\mathrm{mm}}$. Assume that the token scores are distinct (i.e., $\\gamma_{i t} \\neq \\gamma_{i \\tau}$ for $t \\neq \\tau$ ) and the key embeddings $\\boldsymbol{k}_{\\text {it }}$ are in general position. Specifically, we require the following conditions to hold ${ }^{4}$ :\n\n- When $m=d$, all matrices $\\overline{\\boldsymbol{K}} \\in \\mathbb{R}^{m \\times d}$ where each row of $\\overline{\\boldsymbol{K}}$ has the form $\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\alpha_{i}}$ for a unique $\\left(i, \\alpha_{i}, t \\neq \\alpha_{i}\\right)$ tuple, are full-rank. - When $m=d+1$, the vector of all ones is not in the range space of any such $\\overline{\\boldsymbol{K}}$ matrix. Fix arbitrary $\\epsilon>0, R_{0}>0$. Define the local regularization path of $\\boldsymbol{q}$ as its $\\left(\\epsilon, R_{0}\\right)$-conic neighborhood:\n\n$$\n\\overline{\\boldsymbol{p}}(R)=\\underset{\\boldsymbol{p} \\in C_{\\epsilon, R_{0}}(\\boldsymbol{q})\\|\\boldsymbol{p}\\| \\leq R}{\\arg \\min } \\mathcal{L}(\\boldsymbol{p}), \\text { where } \\mathcal{C}_{\\epsilon, R_{0}}(\\boldsymbol{q})=\\operatorname{cone}_{\\epsilon}(\\boldsymbol{q}) \\cap\\left\\{\\boldsymbol{p} \\in \\mathbb{R}^{d} \\mid\\|\\boldsymbol{p}\\| \\geq R_{0}\\right\\}\n$$\n\nThen, either $\\lim _{R \\rightarrow \\infty}\\|\\overline{\\boldsymbol{p}}(R)\\|<\\infty$ or $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) /\\|\\overline{\\boldsymbol{p}}(R)\\| \\neq \\boldsymbol{q}$.",
    "maxmargin-60": "In both scenarios $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) / R \\neq \\boldsymbol{q}$.",
    "maxmargin-61": "Proof. We will prove the result by dividing the problem into distinct cases. In each case, we will construct an alternative direction that achieves a strictly better objective than some $\\delta=\\delta(\\epsilon)>0$ neighborhood of $\\boldsymbol{q}$, thereby demonstrating the suboptimality of the $\\boldsymbol{q}$ direction. Let's define the $\\delta$ neighborhood as follows:\n\n$$\n\\mathcal{N}_{\\delta}=\\left\\{\\boldsymbol{p} \\left\\lvert\\,\\left\\|\\frac{\\boldsymbol{p}}{\\|\\boldsymbol{p}\\|}-\\boldsymbol{q}\\right\\| \\leq \\delta \\quad\\right. \\text { and } \\quad\\|\\boldsymbol{p}\\| \\geq R_{0}\\right\\}\n$$\n\nNow, let's recall a few more definitions based on Definition 3. First, the tokens selected by $\\boldsymbol{q}$ are given by (59). To proceed, let's initially consider the scenario where $\\alpha_{i}$ is unique for all $i \\in[n]$, meaning that as we let $c \\rightarrow \\infty, c \\cdot \\boldsymbol{q}$ will choose a single token per input. Later, we will revisit the setting when arg max is not a singleton, and $\\boldsymbol{q}$ is allowed to select multiple tokens. Additionally, it's important to note that $\\|\\overline{\\boldsymbol{p}}(R)\\|$ is non-decreasing by definition. Suppose it has a finite upper bound $\\|\\overline{\\boldsymbol{p}}(R)\\| \\leq M$ for all $R<\\infty$. In that scenario, we have $\\lim _{R \\rightarrow \\infty} \\frac{\\overline{\\boldsymbol{p}}(R)}{R}=0 \\neq \\boldsymbol{q}$. - (A) $\\boldsymbol{q}$ selects a single token per input: Given that the indices $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ defined in (59) are uniquely determined, we can conclude that the $\\boldsymbol{q}$ direction eventually selects tokens $\\boldsymbol{k}_{i \\alpha_{i}}$. Recall the definition of the margin $\\Gamma_{q}$ from (60) and the set of directional neighbors, which is defined as the indices that achieve $\\Gamma_{\\boldsymbol{q}}$, as shown in (61). Let us refer to $\\boldsymbol{q}$ as neighbor-optimal if $\\boldsymbol{\\gamma}_{i t}<\\boldsymbol{\\gamma}_{i \\alpha_{i}}$ for all $(i, t) \\in \\mathcal{M}_{\\boldsymbol{q}}$. We will consider two cases for this scenario: when $\\boldsymbol{q}$ is neighbor-optimal and when $\\boldsymbol{q}$ is not neighboroptimal. $\\diamond(\\mathbf{A 1}) \\boldsymbol{q}$ is neighbor-optimal. In this case, we will argue that max-margin direction $\\overline{\\boldsymbol{p}}^{\\mathrm{mm}}:=$ $\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha}) /\\left\\|\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})\\right\\|$ can be used to construct a strictly better objective than $\\boldsymbol{q}$. Note that $\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$ exists because $\\boldsymbol{q}$ is already a viable separating direction for tokens $\\boldsymbol{\\alpha}$. Specifically, consider the direction\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-34.jpg?height=61&width=760&top_left_y=2097&top_left_x=368)\n\n$$\n\\boldsymbol{q}^{\\top} \\boldsymbol{q}^{\\prime} \\geq \\frac{1-\\epsilon}{1+\\epsilon} \\geq 1-2 \\epsilon\n$$\n\nWe now argue that, there exists $\\delta=\\delta_{\\epsilon}>0$ such that for all $R>R_{\\epsilon}$\n\n$$\n\\min _{R_{\\epsilon} \\leq r \\leq R} \\mathcal{L}\\left(r \\cdot \\boldsymbol{q}^{\\prime}\\right)<\\min _{\\boldsymbol{p} \\in \\mathcal{N}_{\\delta}, R_{\\epsilon} \\leq\\|p\\| \\leq R} \\mathcal{L}(\\boldsymbol{p})\n$$\n\n[^3]To prove this, we study the margin $\\Gamma_{\\boldsymbol{q}^{\\prime}}$ induced by $\\boldsymbol{q}^{\\prime}$ and the maximum margin $\\Gamma_{\\delta}$ induced within $\\boldsymbol{p} \\in \\mathcal{N}_{\\delta}$. Concretely, we will show that $\\Gamma_{\\boldsymbol{q}^{\\prime}}>\\Gamma_{\\delta}$ and directly apply the first statement of Lemma 8 to conclude with (68). Let $\\Gamma=1 /\\left\\|\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})\\right\\|$ be the margin induced by $\\overline{\\boldsymbol{p}}^{m m}$. Note that $\\Gamma>\\Gamma_{\\boldsymbol{q}}$ by the optimality of $\\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$ and the fact that $\\boldsymbol{q} \\neq \\boldsymbol{p}^{m m}(\\boldsymbol{\\alpha})$. Consequently, we can lower and upper bound the margins via\n\n$$\n\\begin{aligned}\n\\Gamma_{q^{\\prime}} & =\\min _{i \\in[n]} \\min _{i \\neq \\alpha_{i}}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q}^{\\prime} \\geq \\frac{\\Gamma_{q}+\\epsilon \\Gamma}{1+\\epsilon} \\geq \\Gamma_{q}+\\frac{\\epsilon}{2}\\left(\\Gamma-\\Gamma_{q}\\right), \\\\\n\\Gamma_{\\delta} & =\\max _{p \\in \\mathcal{N}_{\\delta}} \\min _{i \\in[n]} \\min _{l \\neq \\alpha_{i}}\\left(\\boldsymbol{k}_{i_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p} /\\|\\boldsymbol{p}\\| \\\\\n& \\leq \\max _{\\|\\| \\| \\leq 1 \\leq 1} \\min _{i \\in[n]} \\min _{t \\neq \\alpha_{i}}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top}(\\boldsymbol{q}+\\delta \\boldsymbol{r}) \\\\\n& \\leq \\Gamma_{q}+M \\delta,\n\\end{aligned}\n$$\n\nwhere $M=\\max _{i, t, \\tau}\\left\\|\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\tau}\\right\\|$. Consequently, setting $\\delta=\\frac{\\epsilon}{4 M}\\left(\\Gamma-\\Gamma_{q}\\right)$, we find that\n\n$$\n\\Gamma_{q^{\\prime}} \\geq \\Gamma_{\\delta}+\\frac{\\epsilon}{4}\\left(\\Gamma-\\Gamma_{q}\\right)\n$$\n\nEquipped with this inequality, we apply the first statement of Lemma 8 which concludes that ${ }^{6}$ for some $R_{\\epsilon}=R\\left(\\frac{\\epsilon}{4}\\left(\\Gamma-\\Gamma_{q}\\right)\\right)$ and all $R>R_{\\epsilon},(68)$ holds. This in turn implies that, within $C_{\\epsilon}$, the optimal solution is\n\n- either upper bounded by $R_{\\epsilon}$ in $\\ell_{2}$ norm (i.e. $\\lim _{R \\rightarrow \\infty}\\|\\overline{\\boldsymbol{p}}(R)\\|<\\infty$ ) or\n- at least $\\delta=\\delta(\\epsilon)>0$ away from $\\boldsymbol{q}$ after $\\ell_{2}$-normalization i.e. $\\left\\|\\frac{\\bar{p}(R)}{\\|\\overline{\\boldsymbol{p}}(R)\\|}-\\boldsymbol{q}\\right\\| \\geq \\delta$. In either scenario, we have proven that $\\lim _{R \\rightarrow \\infty} \\frac{\\overline{\\boldsymbol{p}}(R)}{R} \\neq \\boldsymbol{q}$. $\\diamond$ (A2) $\\boldsymbol{q}$ is not neighbor-optimal. In this scenario, we will prove that $\\overline{\\boldsymbol{p}}(R)$ is finite to obtain $\\lim _{R \\rightarrow \\infty} \\overline{\\boldsymbol{p}}(R) / R=0 \\neq \\boldsymbol{q}$. To start, assume that conic neighborhood $\\epsilon$ of $\\boldsymbol{q}$ is small enough so that selected-tokens $\\alpha$ remain unchanged within $C_{\\epsilon}$. This is without generality because if directional convergence fails in a small neighborhood of $\\boldsymbol{q}$, it will fail in the larger neighborhood as well. Secondly, if $\\lim _{R \\rightarrow \\infty}\\|\\overline{\\boldsymbol{p}}(R)\\| \\rightarrow \\infty$ and $\\overline{\\boldsymbol{p}}(R) \\in \\mathcal{C}_{\\epsilon}$, since softmax will eventually perfectly select $\\boldsymbol{\\alpha}$ (i.e. assigning probability 1 on token indices $\\left(i, \\alpha_{i}\\right)$ ), we would have\n\n$$\n\\lim _{R \\rightarrow \\infty} \\mathcal{L}(\\overline{\\boldsymbol{p}}(R))=\\mathcal{L}_{\\star}=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(\\gamma_{i \\alpha_{i}}\\right)\n$$\n\nNote that, this is simply by selection of $\\boldsymbol{\\alpha}$ and regardless of $\\overline{\\boldsymbol{p}}(R)$ directionally converges to $\\boldsymbol{q}$. This means that, if there exists a finite $\\boldsymbol{p} \\in C_{\\epsilon}$ such that $\\mathcal{L}(\\boldsymbol{p})<\\mathcal{L}_{\\star}$ (i.e. outperforming the training loss of $\\|\\overline{\\boldsymbol{p}}(R)\\| \\rightarrow \\infty)$, then $\\|\\overline{\\boldsymbol{p}}(R)\\|<\\infty$. This would conclude the proof. Thus, we will simply find such a $\\boldsymbol{p}$ obeying $\\mathcal{L}(\\boldsymbol{p})<\\mathcal{L}_{\\star}$. To this aim, we first prove the following lemma. Lemma 9 Given a fixed unit Euclidean norm vector $\\boldsymbol{p}$, if all directional neighbors of $\\boldsymbol{p}$ consistently have higher scores for their associated selected tokens, i.e., $\\gamma_{i \\alpha_{i}}<\\gamma_{i \\beta}$ for all ( $\\left.i, \\alpha_{i}\\right)$ and directional neighbor $(i, \\beta)$, then there exists $\\bar{R}$ such that for all $R>\\bar{R}$,\n\n$$\n\\mathcal{L}(R \\cdot \\boldsymbol{p})<\\mathcal{L}_{\\star}=\\lim _{R \\rightarrow \\infty} \\mathcal{L}(R \\cdot \\boldsymbol{p})\n$$\n\nProof. Define the maximum score difference $K_{+}=\\sup _{i \\in[n], t \\neq \\alpha_{i}}\\left|\\boldsymbol{\\gamma}_{i \\alpha_{i}}-\\gamma_{i t}\\right|$. Also let $\\mathcal{M}_{p}$ be the set of directional neighbors achieving the minimum margin $\\Gamma_{\\boldsymbol{p}}$; see (61). Define $\\Gamma^{i t}=\\boldsymbol{k}_{i \\alpha_{i}}^{\\top} \\boldsymbol{p}-\\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{p}$. Define $\\delta_{\\boldsymbol{p}}$ to be the margin difference between the directional-neighbors and the second-most minimum-margin neighbors defined as\n\n$$\n\\delta_{\\boldsymbol{p}}=\\min _{i \\in[n], t \\neq \\alpha_{i}(i, t) \\notin \\mathcal{M}_{p}} \\Gamma_{\\boldsymbol{p}}^{i t}-\\Gamma_{\\boldsymbol{p}}\n$$\n\n[^4]To proceed, setting $\\kappa=\\min _{(j, \\beta) \\in \\mathcal{M}_{p}} \\gamma_{j \\beta}-\\gamma_{j \\alpha_{j}}>0$ and using (63), we can bound\n\n$$\n\\begin{aligned}\n\\mathcal{L}(R \\boldsymbol{p})-\\mathcal{L}_{\\star} & \\leq-\\frac{1}{n} \\sum_{(j, \\beta) \\in \\mathcal{M}_{p}} M_{j \\beta} e^{-R \\Gamma_{p}} K+\\frac{1}{n} \\sum_{i \\in[n], t \\neq \\alpha_{i}(i, t) \\notin \\mathcal{M}_{p}} M_{i t} e^{-R \\Gamma_{p}^{i t}}\\left(\\gamma_{i \\alpha_{i}}-\\gamma_{i t}\\right) \\\\\n& \\leq-\\frac{1}{n} M_{-} e^{-R \\Gamma_{p}}{ }^{\\prime}+M_{+} K_{+} T e^{-R\\left(\\Gamma_{p}+\\delta_{p}\\right)}\n\\end{aligned}\n$$\n\nConsequently, we have found that $\\mathcal{L}(R \\boldsymbol{p})<\\mathcal{L}_{\\star}$ as soon as\n\n$$\n\\frac{1}{n} M_{-} e^{-R \\Gamma_{q}} K \\geq M_{+} K_{+} T e^{-R\\left(\\Gamma_{q}+\\delta_{q}\\right)}\n$$\n\nThis happens when $R \\gtrsim 1 / \\delta_{q}$ (up to logarithmic terms) establishing the desired statement. Based on this lemma, what we need is constructing a perturbation to modify $\\boldsymbol{q}$ 's directional neighbors and make sure all of them have strictly better scores than their associated selected-tokens. Note that, once we construct a new candidate (say $\\boldsymbol{q}_{0}=\\boldsymbol{q}+$ perturbation), all sufficiently large scalings of $\\boldsymbol{q}_{0}$ will achieve $\\mathcal{L}\\left(R \\cdot \\boldsymbol{q}_{0}\\right)<\\mathcal{L}_{\\star}$. Thus, we can find a strictly better solution than $\\mathcal{L}_{\\star}$ for any norm lower bound $R_{0}$ - which is enforced within the definition of $C_{\\epsilon}$. Lemma 10 There are at most $d$ directional neighbors i.e. $\\left|\\mathcal{M}_{q}\\right| \\leq d$. Proof. Directional neighbors are indices $(i, t)$ obeying the inequality\n\n$$\n\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q}=\\Gamma_{\\boldsymbol{q}}\n$$\n\nDeclare $\\boldsymbol{D}$ to be the matrix with rows obtained by these key differences $\\boldsymbol{k}_{i t}-\\boldsymbol{k}_{i \\alpha_{i}} . \\boldsymbol{D} \\in \\mathbb{R}^{M \\times d}$ where $M=\\left|\\mathcal{M}_{q}\\right|$. We then obtain $D q=-\\Gamma_{q} \\mathbf{1}_{M}$ where $\\mathbf{1}_{M}$ is the all ones vector. If $M>d$, the equality $\\boldsymbol{D q}=-\\Gamma_{\\boldsymbol{q}} \\mathbf{1}_{M}$ cannot be satisfied because $\\mathbf{1}_{M}$ is not in the range space of $\\boldsymbol{D}$ by our assumption of general key embedding positions. To proceed, $\\left|\\mathcal{M}_{\\boldsymbol{q}}\\right| \\leq d$ and let $\\boldsymbol{D}$ be as defined in the lemma above. $\\boldsymbol{D}$ is also full-rank by our assumption of general key positions. We use $\\boldsymbol{D}$ to construct a perturbation as follows. Let $\\mathcal{M}_{q}^{+} \\subset \\mathcal{M}_{q}$ be the set of directional neighbor with strictly higher scores than their associated selected-tokens. In other words, all $(j, \\beta) \\in \\mathcal{M}_{q}^{+}$obeys\n\n$$\n\\gamma_{j \\beta}>\\gamma_{j \\alpha_{j}}\n$$\n\nDefine the score difference $\\kappa=\\min _{(j, \\beta) \\in \\mathcal{M}_{q}^{+}} \\boldsymbol{\\gamma}_{j \\beta}-\\boldsymbol{\\gamma}_{j \\alpha_{j}}>0$. We know $\\kappa>0$ because $\\boldsymbol{\\alpha}$ is not neighboroptimal. Finally, define the indicator vector of $\\mathbf{1}_{+}$with same dimension as cardinality $\\left|\\mathcal{M}_{\\boldsymbol{q}}\\right| . \\mathbf{1}_{+}$is 1 for the rows of $\\boldsymbol{D}$ corresponding to $\\mathcal{M}_{q}^{+}$and is 0 otherwise. Finally, set the perturbation as\n\n$$\n\\boldsymbol{q}^{\\perp}=\\boldsymbol{D}^{\\dagger} \\mathbf{1}_{+}\n$$\n\nwhere we used the full-rankness of $\\boldsymbol{D}$ during pseudo-inversion. To proceed, for a small $\\epsilon_{0}>0$, consider the candidate direction $\\boldsymbol{q}_{0}=\\boldsymbol{q}+\\epsilon_{0} \\boldsymbol{q}^{\\perp}$. We pick $\\epsilon_{0}=O(\\epsilon)$ sufficiently small to ensure $\\boldsymbol{q}_{0} \\in C_{\\epsilon}$. To finalize, let us consider the margins of the tokens within $\\boldsymbol{q}_{0}$. Similar to Lemma 9 , set $\\delta_{\\boldsymbol{q}}=\\min _{i \\in[n], t \\neq \\alpha_{i},(i, t) \\notin \\mathcal{M}_{q}} \\Gamma_{\\boldsymbol{q}}^{i t}-\\Gamma_{\\boldsymbol{q}}>0$. Let $\\bar{\\epsilon}_{0}=\\left\\|\\boldsymbol{q}_{0}\\right\\|-1=\\left\\|\\boldsymbol{q}+\\epsilon_{0} \\boldsymbol{q}^{\\perp}\\right\\|-1$. Using definition of $\\boldsymbol{q}^{\\perp}$, we have that\n\n- $\\operatorname{For}(i, t) \\in \\mathcal{M}_{q}^{+}$, we achieve a margin of\n\n$$\n\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top}\\left(\\boldsymbol{q}+\\epsilon_{0} \\boldsymbol{q}^{\\perp}\\right) /\\left(1+\\bar{\\epsilon}_{0}\\right)=\\frac{\\Gamma_{\\boldsymbol{q}}-\\epsilon_{0}}{1+\\bar{\\epsilon}_{0}}\n$$\n\n- For $(i, t) \\in \\mathcal{M}_{q}^{+}$, we achieve a margin of $\\frac{\\Gamma_{q}}{1+\\bar{\\epsilon}_{0}}$. - For $(i, t) \\notin \\mathcal{M}_{q}$, setting $K=\\left\\|\\boldsymbol{q}^{\\perp}\\right\\| \\cdot \\sup _{i, t, \\tau}\\left\\|\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right\\|$, we achieve a margin of at most\n\n$$\n\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top}\\left(\\boldsymbol{q}+\\epsilon_{0} \\boldsymbol{q}^{\\perp}\\right) /\\left(1+\\bar{\\epsilon}_{0}\\right) \\geq \\frac{\\Gamma_{\\boldsymbol{q}}+\\delta_{\\boldsymbol{q}}-\\epsilon_{0} K}{1+\\bar{\\epsilon}_{0}}\n$$\n\nIn short, since $\\bar{\\epsilon}_{0}=O\\left(\\epsilon_{0}\\right)$, setting $\\epsilon_{0}$ sufficiently small guarantees that $\\mathcal{M}_{q}^{+}$is the set of directional neighbors of $\\boldsymbol{q}_{0}$. Since $\\mathcal{M}_{q}^{+}$has strictly higher scores than their associated selected-tokens, applying Lemma 9 on $\\boldsymbol{q}_{0}$ shows that, $\\mathcal{L}\\left(R \\cdot \\boldsymbol{q}_{0}\\right)<\\mathcal{L}_{\\star}$ for sufficiently large $R$ implying $\\|\\overline{\\boldsymbol{p}}(R)\\|<\\infty$. - (B) $\\boldsymbol{q}$ selects multiple tokens for some inputs $i \\in[n]$ : In this setting, we will again construct a perturbation to create a scenario where $\\boldsymbol{q}_{0}=\\boldsymbol{q}+\\epsilon \\boldsymbol{q}^{\\perp}$ selects a single token for each input $i \\in[n]$. We will then employ margin analysis (first statement of Lemma 8) to conclude that $\\boldsymbol{q}_{0}$ outperforms a $\\delta \\ll \\epsilon$ neighborhood of $\\boldsymbol{q}$. Let $I \\subset[n]$ be the set of inputs for which $\\boldsymbol{q}$ selects multiple tokens. Specifically, for each $i \\in I$, there is $\\mathcal{T}_{i} \\subset[T]$ such that $\\left|\\mathcal{T}_{i}\\right| \\geq 2$ and for any $i \\in I$ and $\\theta \\in \\mathcal{T}_{i}$,\n\n$$\n\\boldsymbol{k}_{i \\theta}^{\\top} \\boldsymbol{q}=\\arg \\max _{t \\in[T]} \\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{q}\n$$\n\nFrom these multiply-selected token indices let us select the highest score one, namely, $\\beta_{i}=$ $\\arg \\max _{\\theta \\in \\mathcal{T}_{i}} \\gamma_{i \\theta}$ for $i \\in \\mathcal{I}$. Now, define the unique optimal tokens for each input as $\\alpha \\in \\mathbb{R}^{n}$ where $\\alpha_{i}:=\\beta_{i}$ for $i \\in \\mathcal{I}$ and $\\alpha_{i}=\\arg \\max _{t \\in[T]} \\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{q}$ for $i \\notin \\mathcal{I}$. Define $\\mathcal{L}_{\\star}=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(\\gamma_{i \\alpha_{i}}\\right)$ as earlier. Secondly, we construct a perturbation $\\boldsymbol{q}^{\\perp}$ to show that $\\boldsymbol{q}_{0}=\\boldsymbol{q}+\\epsilon_{0} \\boldsymbol{q}^{\\perp}$ can select tokens $\\boldsymbol{\\alpha}$ asymptotically. To see this, define the matrix $\\boldsymbol{D}$ where each (unique) row is given by $\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\theta}$ where $\\theta \\in \\mathcal{T}_{i}, \\theta \\neq \\alpha_{i}$, $i \\in \\mathcal{I}$. Now note that, $\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\theta}\\right)^{\\top} \\boldsymbol{q}=0$ for all $\\theta \\in \\mathcal{T}_{i}, \\theta \\neq \\alpha_{i}, i \\in \\mathcal{I}$. Since keys are in general positions, this implies that $\\boldsymbol{D}$ has at most $d-1$ rows and, thus, its rows are linearly independent. Consequently, choose $\\boldsymbol{q}^{\\perp}=\\boldsymbol{D}^{\\dagger} \\mathbf{1}$ where $\\dagger$ denotes pseudo-inverse and $\\mathbf{1}$ is the all ones vector. Also let $\\Gamma_{\\boldsymbol{q}}$ be the margin of directional margin of $\\boldsymbol{q}$ that is\n\n$$\n\\Gamma_{\\boldsymbol{q}}=\\min _{i \\in[n], t \\notin \\mathcal{T}_{i}}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q}\n$$\n\nWith this choice and setting $K=\\sup _{i, t, \\tau}\\left\\|\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right\\|$, we have that\n\n- For $\\theta \\in \\mathcal{T}_{i}, \\theta \\neq \\alpha_{i}:\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\theta}\\right)^{\\top} \\boldsymbol{q}_{0}=\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\theta}\\right)^{\\top} \\boldsymbol{q}^{\\perp}=\\epsilon_{0}$. - For all other $(i, t)$ with $t \\neq \\alpha_{i}:\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{q}_{0} \\geq \\Gamma_{\\boldsymbol{q}}-K\\left\\|\\boldsymbol{q}^{\\perp}\\right\\| \\epsilon_{0}$. Choosing $\\epsilon_{0}<\\Gamma_{\\boldsymbol{q}} /\\left(1+K\\left\\|\\boldsymbol{q}^{\\perp}\\right\\|\\right)$, together, these imply that,\n\n- $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ is the selected-tokens of $\\boldsymbol{q}_{0}$,\n- $\\boldsymbol{q}_{0}$ achieves a directional margin of\n\n$$\n\\Gamma_{\\boldsymbol{q}_{0}}=\\frac{\\epsilon_{0}}{\\left\\|\\boldsymbol{q}_{0}\\right\\|} \\geq \\frac{\\epsilon_{0}}{1+\\epsilon_{0}\\left\\|\\boldsymbol{q}^{\\perp}\\right\\|}\n$$\n\n- $\\boldsymbol{q}_{0}$ is neighbor-optimal because directional neighbors are $\\theta \\in \\mathcal{T}_{i}, \\theta \\neq \\alpha_{i}$ and $\\boldsymbol{\\gamma}_{i \\theta}<\\boldsymbol{\\gamma}_{i \\alpha_{i}}$. Note that, these conditions lay the groundwork for applying the first statement of Lemma 8 with $\\boldsymbol{p} \\leftarrow \\boldsymbol{q}_{0}$. We next explore the optimal directions within $\\mathcal{N}_{\\delta}$ and show that $\\boldsymbol{q}_{0}$ strictly outperform them in terms of training loss. To proceed, given small $\\delta \\ll \\epsilon_{0}$, let us study $\\mathcal{L}_{R}=\\min _{p \\in \\mathcal{N}_{\\delta}, R \\leq \\| \\boldsymbol{p} \\mid<\\infty} \\mathcal{L}(\\boldsymbol{p})$. Here, recall that $\\boldsymbol{p}$ has a $\\delta$-small directional perturbation around $\\boldsymbol{q}$ which can modify the token selections by breaking the ties between the multiply-selected token indices by $\\boldsymbol{q}$. However, thanks to the distinct token score assumption, for large $R \\leq\\|\\boldsymbol{p}\\|$, the optimal $\\boldsymbol{p} \\in \\mathcal{N}_{\\delta}$ is guaranteed to (uniquely) select indices $\\alpha_{i} \\in \\mathcal{T}_{i}$. Because all other $\\boldsymbol{p}$ directions - which, asymptotically, either select other tokens $\\theta \\in \\mathcal{T}_{i}, \\theta \\neq \\alpha_{i}$ or split the probabilities equally across a subset of $\\mathcal{T}_{i}-$ achieve a larger loss. For instance, $\\boldsymbol{q}$ will split the probabilities equally across $\\mathcal{T}_{i}$ to achieve an asymptotic loss of\n\n$$\n\\mathcal{L}_{\\star}^{\\boldsymbol{q}}:=\\lim _{R \\rightarrow \\infty} \\mathcal{L}(R \\cdot \\boldsymbol{q})=\\frac{1}{n} \\sum_{i \\notin \\mathcal{I}} \\ell\\left(\\gamma_{i \\alpha_{i}}\\right)+\\frac{1}{n} \\sum_{i \\in I} \\ell\\left(\\frac{1}{\\left|\\mathcal{T}_{i}\\right|} \\sum_{\\theta \\in \\mathcal{T}_{i}} \\gamma_{i \\theta}\\right)\n$$\n\nThus, $\\mathcal{L}_{\\star}^{q}>\\mathcal{L}_{\\star}$ because $\\ell\\left(\\frac{1}{\\left|\\mathcal{T}_{i}\\right|} \\sum_{\\theta \\in \\mathcal{T}_{i}} \\boldsymbol{\\gamma}_{i \\theta}\\right)>\\ell\\left(\\gamma_{i \\alpha_{i}}\\right)=\\ell\\left(\\gamma_{i \\alpha_{i}}\\right)$ where $\\alpha_{i}$ has the highest score i.e. $\\boldsymbol{\\gamma}_{i \\alpha_{i}}>$ $\\frac{1}{\\left|\\mathcal{T}_{i}\\right|} \\sum_{\\theta \\in \\mathcal{T}_{i}} \\boldsymbol{\\gamma}_{i \\theta}$. Set $\\tilde{\\boldsymbol{p}}(R)=\\arg \\min _{p \\in \\mathcal{N}_{\\delta}, \\| p \\mid \\leq R} \\mathcal{L}(\\boldsymbol{p})$. Consequently, there are two scenarios are:\n\n- $\\lim _{R \\rightarrow \\infty}\\|\\tilde{\\boldsymbol{p}}(R)\\|$ is finite. This already proves the statement of the theorem as $\\tilde{\\boldsymbol{p}}(R) / R \\rightarrow 0$ within $\\delta<\\epsilon$ neighborhood of $\\boldsymbol{q}$. - For sufficiently large $R$, the selected-tokens of $\\tilde{\\boldsymbol{p}}(R)$ are $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$. Proceeding with the second (remaining scenario), we study the directional margin of $\\tilde{\\boldsymbol{p}}(R)$. More broadly, for any $\\boldsymbol{p} \\in \\mathcal{N}_{\\delta}$ and $\\overline{\\boldsymbol{p}}=\\boldsymbol{p} /\\|\\boldsymbol{p}\\|$ with selected-tokens $\\boldsymbol{\\alpha}$, using the fact that $\\|\\overline{\\boldsymbol{p}}-\\boldsymbol{q}\\| \\leq \\delta \\ll \\epsilon_{0}$, we can bound the directional margin as\n\n- For $\\theta \\in \\mathcal{T}_{i}, \\theta \\neq \\alpha_{i}:\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\theta}\\right)^{\\top} \\overline{\\boldsymbol{p}}=\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i \\theta}\\right)^{\\top}(\\overline{\\boldsymbol{p}}-\\boldsymbol{q}) \\leq K \\delta$. - For all other $(i, t)$ with $t \\neq \\alpha_{i}:\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\overline{\\boldsymbol{p}} \\geq \\Gamma_{\\boldsymbol{q}}-K \\delta$. This means that, any such $\\overline{\\boldsymbol{p}} \\in \\mathcal{N}_{\\delta}$ achieves a directional margin of at most\n\n$$\n\\Gamma_{\\bar{p}} \\leq K \\delta\n$$\n\nApplying Lemma 8 and setting $\\delta=O\\left(\\epsilon_{0}\\right)$, this implies that for\n\n$$\nR \\gtrsim \\frac{1}{\\Gamma_{q_{0}}-\\Gamma_{\\bar{p}}}=\\frac{1}{\\frac{\\epsilon_{0}}{1+\\epsilon_{0}\\left\\|q^{\\perp}\\right\\|}-K \\delta}=O\\left(\\frac{1}{\\epsilon_{0}}\\right)\n$$\n\nwe have that $\\mathcal{L}\\left(R \\cdot \\boldsymbol{q}_{0}\\right)<\\min _{\\|p\\|=R, p \\in \\mathcal{N}_{\\delta}} \\mathcal{L}(\\boldsymbol{p})$. Since this holds for all $R$, (68) holds (similar to Case (A1)) and we conclude that whenever $\\|\\overline{\\boldsymbol{p}}(R)\\| \\rightarrow \\infty$, it doesn't directionally converge within $\\mathcal{N}_{\\delta}$ (i.e.",
    "maxmargin-62": "$\\delta>0$ neighborhood of $\\boldsymbol{q}$ ) proving the advertised result. ## B. 6 Proof of Lemma 2\n\nWe prove a slightly general restatement where we require $\\boldsymbol{v} \\in \\operatorname{range}\\left(\\boldsymbol{W}^{\\top}\\right)-$ instead of full-rank $\\boldsymbol{W}$. Lemma 11 Suppose for all $i \\in[n]$ and $t \\neq$ opt $_{i}, Y_{i}=1$ and $\\gamma_{i t}<\\gamma_{\\text {iopt }_{i}}$.",
    "maxmargin-63": "Also suppose $\\boldsymbol{v} \\in \\operatorname{range}\\left(\\boldsymbol{W}^{\\boldsymbol{\\top}}\\right)$. ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-38.jpg?height=50&width=1088&top_left_y=1276&top_left_x=367)\n\nProof. To establish the existence of $\\boldsymbol{p}^{m m \\star}$, we only need to find a direction that demonstrates the feasibility of (ATT-SVM), i.e. we need to find $\\boldsymbol{p}$ that satisfies the margin constraints. To begin, let's define the minimum score difference:\n\n$$\n\\underline{\\gamma}=\\min _{i \\in[n], t \\neq \\mathrm{opt}_{i}} \\gamma_{i \\text { opt }_{i}}-\\gamma_{i t} . $$\n\nWe then set $\\boldsymbol{p}=\\underline{\\gamma}^{-1}\\left(\\boldsymbol{W}^{\\top}\\right)^{\\dagger} \\boldsymbol{v}$ where $\\dagger$ denotes pseudo-inverse. By assumption $\\boldsymbol{W}^{\\top} \\boldsymbol{p}=\\underline{\\gamma}^{-1} \\boldsymbol{v}$. To conclude, observe that $\\boldsymbol{p}$ is a feasible solution since $\\boldsymbol{k}_{i t}=\\boldsymbol{W} \\boldsymbol{x}_{i t}$ and for all $i \\in[n]$ and $t \\neq \\bar{o}^{-} \\mathrm{pt}_{i}$, we have that\n\n$$\n\\begin{aligned}\n& \\left(\\boldsymbol{k}_{\\text {oopt }_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}=\\left(\\boldsymbol{x}_{\\text {iopt }_{i}}-\\boldsymbol{x}_{i t}\\right)^{\\top} \\boldsymbol{W}^{\\top} \\boldsymbol{p}=\\underline{\\gamma}^{-1}\\left(\\boldsymbol{x}_{\\text {iopt }_{i}}-\\boldsymbol{x}_{i t}\\right)^{\\top} \\boldsymbol{W}^{\\top}\\left(\\boldsymbol{W}^{\\top}\\right)^{\\dagger} \\boldsymbol{v}\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-38.jpg?height=58&width=354&top_left_y=1787&top_left_x=1105)\nwhich together with the constraints in (ATT-SVM) completes the proof.",
    "maxmargin-64": "## C Addendum to Section 3\n\n## C. 1 Proof of Theorem 5\n\nProof. Suppose the claim is incorrect and either $\\boldsymbol{p}_{R} / R$ or $\\boldsymbol{v}_{r} / r$ fails to converge as $R, r$ grows. Set $\\Xi=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|, \\Gamma=1 /\\left\\|\\boldsymbol{v}^{m m}\\right\\|, \\tilde{\\boldsymbol{p}}^{m m}=R \\Xi \\boldsymbol{p}^{m m}$ and $\\tilde{\\boldsymbol{v}}^{m m}=r \\Gamma \\boldsymbol{v}^{m m}$. The proof strategy is obtaining a contradiction by proving that $\\left(\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{m m}\\right)$ is a strictly better solution compared to $\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right)$ for large $R, r$. Without losing generality, we will set $\\alpha_{i}=1$ for all $i \\in[n]$ as the problem is invariant to tokens' permutation. Define $q_{i}^{p}=1-s_{i 1}^{p}$ to be the amount of non-optimality (cumulative probability of non-first tokens) where $\\boldsymbol{s}_{i}^{\\boldsymbol{p}}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right)$ is the softmax probabilities. - Case 1: $\\boldsymbol{p}_{R} / R$ does not converge. Under this scenario there exists $\\delta, \\gamma=\\gamma(\\delta)>0$ such that we can find arbitrarily large $R$ with $\\left\\|\\boldsymbol{p}_{R} / R-\\tilde{\\boldsymbol{p}}^{m m} / R\\right\\| \\geq \\delta$ and margin induced by $\\boldsymbol{p}_{R} / R$ is at most $\\Xi(1-\\gamma)$ (from strong convexity of (ATT-SVM)). Following $q_{i}^{p}$ definition above, set $\\hat{q}_{\\max }=\\sup _{i \\in[n]} q_{i}^{p_{R}}$ to be\nworst non-optimality in $\\boldsymbol{p}_{R}$ and $q_{\\max }^{\\star}=\\sup _{i \\in[n]} q_{i}^{\\tilde{p}^{m m}}$ to be the same for $\\tilde{\\boldsymbol{p}}^{\\mathrm{mm}}$. Repeating the identical argument in Theorem 8 (specifically (84)), we can bound the non-optimality amount $q_{i}^{\\tilde{p}^{m m}}$ of $\\tilde{\\boldsymbol{p}}^{\\mathrm{mm}}$ as\n\n$$\nq_{i}^{\\tilde{\\boldsymbol{p}}^{m m}}=\\frac{\\sum_{t \\neq \\alpha_{i}} \\exp \\left(\\boldsymbol{k}_{i t}^{\\top} \\tilde{\\boldsymbol{p}}^{m m}\\right)}{\\sum_{t \\in[T]} \\exp \\left(\\boldsymbol{k}_{i t}^{\\top} \\tilde{\\boldsymbol{p}}^{m m}\\right)} \\leq \\frac{\\sum_{t \\neq \\alpha_{i}} \\exp \\left(\\boldsymbol{k}_{i t}^{\\top} \\tilde{\\boldsymbol{p}}^{m m}\\right)}{\\exp \\left(\\boldsymbol{k}_{i \\alpha_{i}}^{\\top} \\tilde{\\boldsymbol{p}}^{m m}\\right)} \\leq T \\exp (-R \\Xi)\n$$\n\nThus, $q_{\\text {max }}^{\\star}=\\max _{i \\in[n]} q_{i}^{\\tilde{p}_{m m}^{m m}} \\leq T \\exp (-R \\Xi)$. Next without losing generality, assume first margin constraint is $\\gamma$-violated by $\\boldsymbol{p}_{R}$ and $\\min _{t \\neq \\alpha_{1}}\\left(\\boldsymbol{k}_{1 \\alpha_{1}}-\\boldsymbol{k}_{1 t}\\right)^{\\top} \\boldsymbol{p}_{R} \\leq \\Xi R(1-\\gamma)$. Denoting the amount of non-optimality of the first input as $q_{1}^{p_{R}}$, we find\n\n$$\nq_{1}^{\\boldsymbol{p}_{R}}=\\frac{\\sum_{t \\neq \\alpha_{1}} \\exp \\left(\\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}\\right)}{\\sum_{t \\in[T]} \\exp \\left(\\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}\\right)} \\geq \\frac{1}{T} \\frac{\\sum_{t \\neq \\alpha_{1}} \\exp \\left(\\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}\\right)}{\\exp \\left(\\boldsymbol{k}_{1 \\alpha_{1}}^{\\top} \\boldsymbol{p}_{R}\\right)} \\geq T^{-1} \\exp (-(1-\\gamma) R \\Xi)\n$$\n\nWe similarly have $q_{\\max }^{\\star} \\geq T^{-1} \\exp (-R \\Xi)$ to find that\n\n$$\n\\begin{aligned}\n\\log \\left(\\hat{q}_{\\max }\\right) & \\geq-(1-\\gamma) \\Xi R-\\log T \\\\\n-\\Xi R-\\log T \\leq \\log \\left(q_{\\max }^{\\star}\\right) & \\leq-\\Xi R+\\log T . \\end{aligned}\n$$\n\nIn words, $\\tilde{\\boldsymbol{p}}^{\\text {mm }}$ contains exponentially less non-optimality compared to $\\boldsymbol{p}_{R}$ as $R$ grows. The remainder of the proof differs from Theorem 8 as we need to upper/lower bound the logistic loss of ( $\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{m m}$ ) and $\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right)$ respectively to conclude with the contradiction. First, let us upper bound the logistic loss of $\\left(\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{m m}\\right)$. Set $\\boldsymbol{r}_{i}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\tilde{\\boldsymbol{p}}^{m m}\\right)$. Observe that if $\\left\\|\\boldsymbol{r}_{i}-\\boldsymbol{x}_{i 1}\\right\\| \\leq \\epsilon_{i}$, we have that $\\boldsymbol{v}^{\\mathrm{mm}}$ satisfies the SVM constraints on $\\boldsymbol{r}_{i}$ with $Y_{i} \\cdot \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}^{\\mathrm{mm}} \\geq 1-$ $\\epsilon_{i} / \\Gamma$. Consequently, setting $\\epsilon_{\\max }=\\sup _{i \\in[n]} \\epsilon_{i}, v^{\\mathrm{mm}}$ achieves a label-margin of $\\Gamma-\\epsilon_{\\max }$ on the dataset $\\left(Y_{i}, \\boldsymbol{r}_{i}\\right)_{i \\in[n]}$. With this, we upper bound the logistic loss of $\\left(\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{m m}\\right)$ as follows. Let $M=\\sup _{i \\in[n], t, \\tau \\in[T]}\\left\\|\\boldsymbol{x}_{i t}-\\boldsymbol{x}_{i \\tau}\\right\\|$. Let us recall the fact (73) that worst-case perturbation is\n\n$$\n\\epsilon_{\\max } \\leq M \\exp (-\\Xi R+\\log T)=M T \\exp (-\\Xi R)\n$$\n\nThis implies that\n\n$$\n\\begin{aligned}\n\\mathcal{L}\\left(\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{m m}\\right) & \\leq \\max _{i \\in[n]} \\log \\left(1+\\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\tilde{\\boldsymbol{v}}^{m m}\\right)\\right) \\\\\n& \\leq \\max _{i \\in[n]} \\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\tilde{\\boldsymbol{v}}^{m m}\\right) \\\\\n& \\leq \\exp \\left(-r \\Gamma+r \\epsilon_{\\max }\\right) \\\\\n& \\leq e^{r M T \\exp (-\\Xi R)} e^{-r \\Gamma}\n\\end{aligned}\n$$\n\nConversely, we obtain a lower bound for $\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right)$. Set $\\boldsymbol{r}_{i}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)$. Using Assumption C, we find that solving $(\\mathrm{SVM})$ on $\\left(Y_{i}, \\boldsymbol{r}_{i}\\right)_{i \\in[n]}$ achieves at most $\\Gamma-v e^{-(1-\\gamma) \\Xi R} / T$ margin. Consequently, we have\n\n$$\n\\begin{aligned}\n\\mathcal{L}\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right) & \\geq \\frac{1}{n} \\max _{i \\in[n]} \\log \\left(1+\\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}_{r}\\right)\\right) \\\\\n& \\geq \\frac{1}{2 n} \\max _{i \\in[n]} \\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}_{r}\\right) \\wedge \\log 2 \\\\\n& \\geq \\frac{1}{2 n} \\exp \\left(-r\\left(\\Gamma-v e^{-(1-\\gamma) \\Xi R} / T\\right)\\right) \\wedge \\log 2 \\\\\n& \\geq \\frac{1}{2 n} e^{r(v / T) \\exp (-(1-\\gamma) \\Xi R)} e^{-r \\Gamma} \\wedge \\log 2\n\\end{aligned}\n$$\n\nObserve that, this lower bound dominates the previous upper bound when $R$ is large, namely, when (ignoring the multiplier $1 / 2 n$ for brevity)\n\n$$\n(v / T) e^{-(1-\\gamma) \\Xi R} \\geq M T e^{-\\Xi R} \\Longleftrightarrow R \\geq R_{0}:=\\frac{1}{\\gamma \\Xi} \\log \\left(\\frac{M T^{2}}{v}\\right)\n$$\n\nThus, we indeed obtain the desired contradiction since such large $R$ is guaranteed to exist when $\\boldsymbol{p}_{R} / R \\nrightarrow \\boldsymbol{p}^{\\mathrm{mm}}$. - Case 2: $\\boldsymbol{v}_{r} / r$ does not converge. This is the simpler scenario: There exists $\\delta>0$ such that we can find arbitrarily large $r$ obeying $\\left\\|\\boldsymbol{v}_{r} / r-\\boldsymbol{v}^{\\mathrm{mm}} /\\right\\| \\boldsymbol{v}^{\\mathrm{mm}}\\|\\| \\geq \\delta$. If $\\left\\|\\boldsymbol{p}_{R} / R-\\Xi \\boldsymbol{p}^{m m}\\right\\| \\nrightarrow 0$, then \"Case\n1\" applies. Otherwise, we have $\\left\\|\\boldsymbol{p}_{R} / R-\\Xi \\boldsymbol{p}^{m m}\\right\\| \\rightarrow 0$, thus we can assume $\\left\\|\\boldsymbol{p}_{R} / R-\\Xi \\boldsymbol{p}^{m m}\\right\\| \\leq \\epsilon$ for arbitrary choice of $\\epsilon>0$. On the other hand, due to the strong convexity of (SVM), for some $\\gamma:=\\gamma(\\delta)>0, \\boldsymbol{v}_{r}$ achieves a margin of at most $(1-\\gamma) \\Gamma r$ on the dataset $\\left(Y_{i}, \\boldsymbol{x}_{i 1}\\right)_{i \\in[n]}$. Additionally, since $\\left\\|\\boldsymbol{p}_{R} / R-\\Xi \\boldsymbol{p}^{\\mathrm{mm}}\\right\\| \\leq \\epsilon, \\boldsymbol{p}_{R}$ strictly separates all optimal tokens (for small enough $\\epsilon>0$ ) and $\\hat{q}_{\\text {max }}:=f(\\epsilon) \\rightarrow 0$ as $R \\rightarrow \\infty$. Consequently, setting $\\boldsymbol{r}_{i}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)$, for sufficiently large $R>0$ setting $M=\\sup _{i \\in[n], t \\in[T]}\\left\\|\\boldsymbol{x}_{i t}\\right\\|$, we have that\n\n$$\n\\begin{aligned}\n\\min _{i \\in[n]} Y_{i} \\boldsymbol{v}_{r}^{\\top} \\boldsymbol{r}_{i} & \\leq \\min _{i \\in[n]} Y_{i} \\boldsymbol{v}_{r}^{\\top} \\boldsymbol{x}_{i 1}+\\sup _{i \\in[n]}\\left|\\boldsymbol{v}_{r}^{\\top}\\left(\\boldsymbol{r}_{i}-\\boldsymbol{x}_{i 1}\\right)\\right| \\\\\n& \\leq(1-\\gamma) \\Gamma r+M f(\\epsilon) r \\\\\n& \\leq(1-\\gamma / 2) \\Gamma r . \\end{aligned}\n$$\n\nThis in turn implies that logistic loss is lower bounded by (following (75)),\n\n$$\n\\mathcal{L}\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right) \\geq \\frac{1}{2 n} e^{\\gamma \\Gamma r / 2} e^{-\\Gamma r} \\wedge \\log 2\n$$\n\nGoing back to (74), this exponentially dominates the upper bound of ( $\\tilde{\\boldsymbol{p}}^{\\mathrm{mm}}, \\tilde{\\boldsymbol{v}}^{\\mathrm{mm}}$ ) whenever $r M T \\exp (-\\Xi R)<r \\gamma \\Gamma / 2$, (that is, whenever $R, r$ are sufficiently large), again concluding the proof.",
    "maxmargin-65": "## C. 2 Proof of Theorem 6\n\nWe will prove this result in two steps. Our first claim restricts the optimization to the particular quadrant induced by $\\min _{t \\neq \\alpha_{i}}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}_{R} \\geq 0$ under the theorem's condition $\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)_{\\alpha_{i}} \\rightarrow 1$. Lemma 12 Suppose $\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)_{\\alpha_{i}} \\rightarrow 1$. Then, there exists $R_{0}$ such that for all $R \\geq R_{0}$, we have that,\n\n$$\n\\min _{t \\neq \\alpha_{i}}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}_{R} \\geq 0, \\quad \\text { for all } \\quad i \\in[n]\n$$\n\nProof. Suppose the claim does not hold. Set $s_{i}^{R}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)$. Fix $R_{0}$ such that $\\boldsymbol{s}_{i \\alpha_{i}}^{R} \\geq 0.9$ for all $R \\geq R_{0}$. On the other hand, there exists arbitrarily large $R$ for which $\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right)^{\\top} \\boldsymbol{p}_{R}<0$ for some $t \\neq \\alpha_{i} \\in[T]$ and $i \\in[n]$. At this $(R, i, t)$ choices, we have that $s_{i t}^{R} \\geq s_{i \\alpha_{i}}^{R}$. Since $s_{i t}^{R}+s_{i \\alpha_{i}}^{R} \\leq 1$, we find $s_{i \\alpha_{i}}^{R}<0.5$ which contradicts with $s_{i \\alpha_{i}}^{R} \\geq 0.9$. Let $\\boldsymbol{Q}$ be the set of $\\boldsymbol{p}$ satisfying the quadrant constraint (77) - i.e. indices $\\left(\\alpha_{i}\\right)_{i=1}^{n}$ are selected. Let $\\boldsymbol{h}_{R}$ be the solution of regularization path of $(\\boldsymbol{v}, \\boldsymbol{p})$ subject to the constraint $\\boldsymbol{p} \\in \\boldsymbol{Q}$. From Lemma 12, we know that, for some $R_{0}$ and all $R \\geq R_{0}, \\boldsymbol{h}_{R}=\\boldsymbol{p}_{R}$. Thus, if the limit exists, we have that $\\lim _{R \\rightarrow \\infty} \\boldsymbol{h}_{R} / R=\\lim _{R \\rightarrow \\infty} \\boldsymbol{p}_{R} / R$. To proceed, we will prove that $\\lim _{R \\rightarrow \\infty} \\boldsymbol{h}_{R} / R$ exists and is equal to $\\boldsymbol{p}^{\\text {relax }} /\\left\\|\\boldsymbol{p}^{\\text {relax }}\\right\\|$ and simultaneously establish $\\boldsymbol{v}_{r} / r \\rightarrow \\boldsymbol{v}^{\\mathrm{mm}} /\\left\\|\\boldsymbol{v}^{m m}\\right\\|$. Lemma $13 \\lim _{R \\rightarrow \\infty} \\boldsymbol{h}_{R} / R=\\boldsymbol{p}^{\\text {relax }} /\\left\\|\\boldsymbol{p}^{\\text {relax }}\\right\\|$ and $\\lim _{r \\rightarrow \\infty} \\boldsymbol{v}_{r} / r=\\boldsymbol{v}^{\\mathrm{mm}} /\\left\\|\\boldsymbol{\\nu}^{\\mathrm{mm}}\\right\\|$.",
    "maxmargin-66": "Proof. The proof will be similar to that of Theorem 5. As usual, we aim to show that SVM-solutions constitute the most competitive direction. Set $\\Xi=1 /\\left\\|\\boldsymbol{p}^{\\text {relax }}\\right\\|$. - Case 1: $\\boldsymbol{h}_{R} / R$ does not converge. Under this scenario there exists $\\delta, \\gamma=\\gamma(\\delta)>0$ such that we can find arbitrarily large $R$ with $\\left\\|\\boldsymbol{h}_{R} / R-\\Xi \\boldsymbol{p}^{\\text {relax }}\\right\\| \\geq \\delta$. This implies that margin induced by $\\boldsymbol{h}_{R} / R$ is at most $\\Xi(1-\\gamma)$ over the support vectors $S$ (from strong convexity of (10)). The reason is that, $\\boldsymbol{h}_{R}$ satisfies $\\boldsymbol{h}_{R}^{\\top}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right) \\geq 0$ for all $t \\neq \\alpha_{i}$ by construction as $\\boldsymbol{h}_{R} \\in \\boldsymbol{Q}$. Thus, a constraint over the support vectors have to be violated (when normalized to the same $\\ell_{2}$ norm as $\\left\\|\\boldsymbol{p}^{\\text {relax }}\\right\\|=1 / \\Xi$ ). As usual, we will construct a solution strictly superior to $\\boldsymbol{h}_{R}$ and contradicts with its optimality. Construction of competitor: Rather than using $\\boldsymbol{p}^{\\text {relax }}$ direction, we will choose a slightly deviating direction that ensures the selection of the correct tokens over non-supports $\\overline{\\mathcal{S}}$. Specifically, consider the solution of (10) where we tighten the non-support constraints by arbitrarily small $\\epsilon>0$. $$\n\\boldsymbol{p}^{\\epsilon-\\mathrm{rlx}}=\\arg \\min _{\\boldsymbol{p}}\\|\\boldsymbol{p}\\| \\quad \\text { such that } \\quad \\boldsymbol{p}^{\\top}\\left(\\boldsymbol{k}_{i \\alpha_{i}}-\\boldsymbol{k}_{i t}\\right) \\geq\\left\\{\\begin{array}{lll}\n1 & \\text { for all } & t \\neq \\alpha_{i}, i \\in \\mathcal{S} \\\\\n\\epsilon & \\text { for all } & t \\neq \\alpha_{i}, i \\in \\overline{\\mathcal{S}}\n\\end{array}\\right. $$\n\nLet $\\boldsymbol{p}^{m m}$ be the solution of (ATT-SVM) with $\\boldsymbol{\\alpha}=\\left(\\alpha_{i}\\right)_{i=1}^{n}$ (which was assumed to be separable). Observe that $\\boldsymbol{p}_{\\epsilon}^{m m}=\\epsilon \\boldsymbol{p}^{m m}+(1-\\epsilon) \\boldsymbol{p}^{\\text {relax }}$ satisfies the constraints of (78). Additionally, $\\boldsymbol{p}_{\\epsilon}^{m m}$ would achieve a margin of $\\frac{1}{(1-\\epsilon) / \\Xi+\\epsilon / \\Delta}=\\frac{\\Delta \\Xi}{\\Delta+\\epsilon(\\Xi-\\Delta)}$ where $\\Delta=1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. Using optimality of $\\boldsymbol{p}^{\\epsilon-r I x}$, this implies that the reduced margin $\\Xi_{\\epsilon}=1 /\\left\\|\\boldsymbol{p}^{\\epsilon-I X}\\right\\|$ (by enforcing $\\epsilon$ over non-support) over the support vectors is a Lipschitz function of $\\epsilon$. That is $\\Xi_{\\epsilon} \\geq \\Xi-\\epsilon M$ for some $M \\geq 0$. To proceed, choose an $\\epsilon>0$ such that, it is strictly superior to margin induced by $\\boldsymbol{h}_{R}$, that is,\n\n$$\n\\Xi_{\\epsilon} \\geq \\Xi\\left(1-\\frac{\\gamma}{2}\\right)\n$$\n\nTo proceed, set $\\tilde{\\boldsymbol{p}}^{\\epsilon-r l x}=R \\Xi_{\\epsilon} \\boldsymbol{p}^{\\epsilon-r l x}$. Let us recall the following notation from the proof of Theorem 5: $\\boldsymbol{s}_{i}^{\\boldsymbol{p}}=\\mathbb{S}\\left(\\boldsymbol{K}_{\\boldsymbol{i}} \\boldsymbol{p}\\right)$ and $q_{i}^{\\boldsymbol{p}}=1-\\boldsymbol{s}_{i \\alpha_{i}}$. Set $\\hat{q}_{\\max }=\\max _{i \\in \\mathcal{S}} q_{i}^{\\boldsymbol{h}_{\\boldsymbol{R}}}$ to be worst non-optimality of $\\boldsymbol{h}_{R}$ over support set. Similarly, define $q_{\\max }^{\\star}=\\max _{i \\in \\mathcal{S}} q_{i}^{\\tilde{p}_{i}^{\\epsilon-I x}}$ to be the same for $\\tilde{\\boldsymbol{p}}^{\\epsilon-I X}$. Repeating the identical arguments to (71), (72), (73), and using the fact that $\\boldsymbol{p}^{\\epsilon-\\mathrm{rlx}}$ achieves a margin $\\Xi\\left(1-\\frac{\\gamma}{2}\\right) \\leq \\Xi_{\\epsilon} \\leq \\Xi$, we end up with the lines\n\n$$\n\\begin{aligned}\n\\log \\left(\\hat{q}_{\\max }\\right) & \\geq-(1-\\gamma) \\Xi R-\\log T \\\\\n-\\Xi R-\\log T \\leq \\log \\left(q_{\\max }^{\\star}\\right) & \\leq-\\Xi(1-0.5 \\gamma) R+\\log T\n\\end{aligned}\n$$\n\nIn what follows, we will prove that $\\tilde{\\boldsymbol{p}}^{\\epsilon-\\mathrm{rlx}}$ achieves a strictly smaller logistic loss contradicting with the optimality of $\\boldsymbol{p}_{R}$ (whenever $\\left\\|\\boldsymbol{h}_{R} / R-\\Xi \\boldsymbol{p}^{\\text {relax }}\\right\\| \\geq \\delta$ ). Upper bounding logistic loss. Let us now upper bound the logistic loss of ( $\\left.\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{\\epsilon-I l x}\\right)$ where $\\tilde{\\boldsymbol{v}}^{m m}=$ $r \\Gamma \\boldsymbol{v}^{m m}$ with $\\boldsymbol{v}^{m m}$ being the solution of (SVM) with $\\boldsymbol{r}_{i} \\leftarrow \\boldsymbol{x}_{i \\alpha_{i}}$ and $\\Gamma=1 /\\left\\|\\boldsymbol{v}^{m m}\\right\\|$. Set $\\boldsymbol{r}_{i}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\tilde{\\boldsymbol{p}}^{\\epsilon-r I x}\\right)$. Set $v=\\min _{i \\in \\overline{\\mathcal{S}}} Y_{i} \\cdot \\boldsymbol{x}_{i \\alpha_{i}}^{\\top} v^{\\mathrm{mm}}-1$ to be the additional margin buffer that non-support vectors have access to. Also set $M=\\sup _{i \\in[n], t, \\tau \\in[T]}\\left\\|\\boldsymbol{x}_{i t}-\\boldsymbol{x}_{i \\tau}\\right\\|$. Observe that we can write\n\n$$\n\\boldsymbol{x}_{i \\alpha_{i}}-\\boldsymbol{r}_{i}=\\sum_{t \\neq \\alpha_{i}} \\boldsymbol{s}_{i t}\\left(\\boldsymbol{x}_{i \\alpha_{i}}-\\boldsymbol{x}_{i t}\\right) \\Longrightarrow\\left\\|\\boldsymbol{x}_{i \\alpha_{i}}-\\boldsymbol{r}_{i}\\right\\| \\leq q_{i} M\n$$\n\nNon-supports achieve strong label-margin: Using above and (78) for all $i \\in \\overline{\\mathcal{S}}$ and $t \\neq \\alpha_{i}$, we have that $\\boldsymbol{s}_{i t} \\leq e^{-\\epsilon \\Xi_{\\epsilon} R} \\boldsymbol{s}_{i \\alpha_{i}} \\leq e^{-\\epsilon \\Xi(1-\\gamma / 2) R} \\boldsymbol{s}_{i \\alpha_{i}}$. Consequently, whenever $R \\geq \\bar{R}_{0}:=(\\epsilon \\Xi(1-\\gamma / 2))^{-1} \\log \\left(\\frac{T M}{\\Gamma \\nu}\\right)$,\n\n$$\nq_{i}^{\\tilde{p}_{i}^{\\epsilon-I x}} \\leq \\frac{\\sum_{t \\neq \\alpha_{i}} s_{i t}}{s_{i \\alpha_{i}}} \\leq T e^{-\\epsilon \\Xi(1-\\gamma / 2) R} \\leq \\frac{\\Gamma v}{M}\n$$\n\nThis implies that, on $i \\in \\overline{\\mathcal{S}}$\n\n$$\nY_{i} \\cdot \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}^{m m} \\geq 1+v+Y_{i} \\cdot\\left(\\boldsymbol{r}_{i}-\\boldsymbol{x}_{i \\alpha_{i}}\\right)^{\\top} \\boldsymbol{v}^{m m} \\geq 1+v-q_{i} M\\left\\|\\boldsymbol{v}^{m m}\\right\\| \\geq 1\n$$\n\nIn words: Above a fixed $\\bar{R}_{0}$ that only depends on $\\gamma=\\gamma(\\delta)$, features $\\boldsymbol{r}_{i}$ induced by all non-support indices $i \\in \\overline{\\mathcal{S}}$ achieve margin at least 1 . What remains is analyzing the margin shrinkage over the support vectors as in Theorem 5. Controlling support margin and combining bounds: Over $\\mathcal{S}$, suppose $\\boldsymbol{v}^{\\mathrm{mm}}$ satisfies the SVM constraints on $\\boldsymbol{r}_{i}$ with $Y_{i} \\cdot \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}^{m m} \\geq 1-\\epsilon_{i} / \\Gamma$. Consequently, setting $\\epsilon_{\\max }=\\sup _{i \\in[n]} \\epsilon_{i}, \\boldsymbol{v}^{m m}$ achieves a label-margin of $\\Gamma-\\epsilon_{\\max }$ on the dataset $\\left(Y_{i}, \\boldsymbol{r}_{i}\\right)_{i \\in[n]}$. Next, we recall the fact ( 79 b ) that worst-case perturbation is $\\epsilon_{\\max } \\leq M \\exp (-\\Xi(1-0.5 \\gamma) R+\\log T)=M T \\exp (-\\Xi(1-0.5 \\gamma) R)$. With this and (80), we upper bound the logistic loss of $\\left(\\tilde{\\boldsymbol{v}}^{\\text {mm }}, \\tilde{\\boldsymbol{p}}^{\\epsilon-I x}\\right)$ as follows. $$\n\\begin{aligned}\n\\mathcal{L}\\left(\\tilde{\\boldsymbol{v}}^{m m}, \\tilde{\\boldsymbol{p}}^{m m}\\right) & \\leq \\max _{i \\in[n]} \\log \\left(1+\\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\tilde{\\boldsymbol{v}}^{m m}\\right)\\right) \\\\\n& \\leq \\max _{i \\in[n]} \\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\tilde{\\boldsymbol{v}}^{m m}\\right) \\\\\n& \\leq \\exp \\left(-r \\Gamma+r \\epsilon_{\\max }\\right) \\\\\n& \\leq e^{r M T \\exp (-\\Xi(1-0.5 \\gamma) R)} e^{-r \\Gamma}\n\\end{aligned}\n$$\n\nConversely, we obtain a lower bound for $\\left(\\boldsymbol{v}_{r}, \\boldsymbol{h}_{R}\\right)$. Set $\\boldsymbol{r}_{i}=\\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{h}_{R}\\right)$. Recall the lower bound (79a) over the support vector set $\\mathcal{S}$. Combining this with our Assumption C over the support vectors\nof (SVM) implies that, solving (SVM) on $\\left(Y_{i}, r_{i}\\right)_{i \\in[n]}$ achieves at most $\\Gamma-v e^{-(1-\\gamma) \\Xi R} / T$ margin. Consequently, we have\n\n$$\n\\begin{aligned}\n\\mathcal{L}\\left(\\boldsymbol{v}_{r}, \\boldsymbol{h}_{R}\\right) & \\geq \\frac{1}{n} \\max _{i \\in[n]} \\log \\left(1+\\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}_{r}\\right)\\right) \\\\\n& \\geq \\frac{1}{2 n} \\max _{i \\in[n]} \\exp \\left(-Y_{i} \\boldsymbol{r}_{i}^{\\top} \\boldsymbol{v}_{r}\\right) \\wedge \\log 2 \\\\\n& \\geq \\frac{1}{2 n} \\exp \\left(-r\\left(\\Gamma-v e^{-(1-\\gamma) \\Xi R} / T\\right)\\right) \\wedge \\log 2 \\\\\n& \\geq \\frac{1}{2 n} e^{r(v / T) \\exp (-(1-\\gamma) \\Xi R)} e^{-r \\Gamma} \\wedge \\log 2\n\\end{aligned}\n$$\n\nObserve that, this lower bound dominates the previous upper bound when $R$ is large, namely, when (ignoring the multiplier $1 / 2 n$ for brevity)\n\n$$\n(v / T) e^{-(1-\\gamma) \\Xi R} \\geq M T e^{-\\Xi(1-0.5 \\gamma) R} \\Longleftrightarrow R \\geq R_{0}:=\\frac{2}{\\gamma \\Xi} \\log \\left(\\frac{M T^{2}}{v}\\right)\n$$\n\nThus, we obtain the desired contradiction since $\\tilde{\\boldsymbol{p}}^{\\epsilon-\\mathrm{rlx}}$ is a strictly better solution compared to $\\boldsymbol{p}_{R}=\\boldsymbol{h}_{R}$ (once $R$ is sufficiently large). - Case 2: $\\boldsymbol{v}_{r} / r$ does not converge. This is the simpler scenario: There exists $\\delta>0$ such that we can find arbitrarily large $r$ obeying $\\left\\|\\boldsymbol{v}_{r} / r-\\boldsymbol{v}^{\\mathrm{mm}} /\\right\\| \\boldsymbol{v}^{\\mathrm{mm}} \\mid\\|\\| \\geq \\delta$. First, note that, due to the strong convexity of (SVM), for some $\\gamma:=\\gamma(\\delta)>0, \\boldsymbol{v}_{r}$ achieves a margin of at most $(\\Gamma-\\gamma) r$ on the dataset $\\left(Y_{i}, \\boldsymbol{x}_{i 1}\\right)_{i \\in[n]}$. By theorem's condition, we are provided that $\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}\\right)_{\\alpha_{i}} \\rightarrow 1$. This immediately implies that, for any choice of $\\epsilon=\\gamma / 3>0$, above some sufficiently large $\\left(r_{0}, R_{0}\\right)$, we have that $\\left\\|x_{i}^{p_{R}}-\\boldsymbol{r}_{i}\\right\\| \\leq \\epsilon$. Following (81), this implies that, choosing $\\tilde{\\boldsymbol{v}}^{\\mathrm{mm}}=r \\boldsymbol{v}^{\\mathrm{mm}} /\\left\\|\\boldsymbol{v}^{\\mathrm{mm}}\\right\\|$ achieves a logistic loss of at most $e^{r \\gamma / 3} e^{-r \\Gamma}$. Again using $\\left\\|x_{i}^{p_{R}}-\\boldsymbol{r}_{i}\\right\\| \\leq \\epsilon$, for sufficiently large $(r, R)$ we have that\n\n$$\n\\begin{aligned}\n\\min _{i \\in[n]} Y_{i} \\boldsymbol{v}_{r}^{\\top} \\boldsymbol{r}_{i} & \\leq \\min _{i \\in[n]} Y_{i} \\boldsymbol{v}_{r}^{\\top} \\boldsymbol{x}_{i 1}+\\sup _{i \\in[n]}\\left|\\boldsymbol{v}_{r}^{\\top}\\left(\\boldsymbol{r}_{i}-\\boldsymbol{x}_{i 1}\\right)\\right| \\\\\n& \\leq(\\Gamma-\\gamma) r+\\epsilon r \\\\\n& \\leq(\\Gamma-2 \\gamma / 3) r . \\end{aligned}\n$$\n\nThis in turn implies that logistic loss is lower bounded by (following (82)),\n\n$$\n\\mathcal{L}\\left(\\boldsymbol{v}_{r}, \\boldsymbol{p}_{R}\\right) \\geq \\frac{1}{2 n} e^{2 \\gamma r / 3} e^{-r \\Gamma} \\wedge \\log 2\n$$\n\nThis dominates the above upper bound $e^{r \\gamma / 3} e^{-r \\Gamma}$ of $\\tilde{v}^{m m}$ whenever $\\frac{1}{2 n} e^{\\gamma r / 3}>1 \\Longleftrightarrow r>\\frac{3}{\\gamma} \\log (2 n)$, (that is, when $r$ is sufficiently large), again concluding the proof. ## D Regularization Path of Attention with Nonlinear Head\n\nSo far our discussion has focused on the attention model with linear head. However, the conceptual ideas on optimal token selection via margin maximization also extends to a general nonlinear model under mild assumptions. The aim of this section is showcasing this generalization. Specifically, we consider the prediction model $f(\\boldsymbol{X})=\\psi\\left(\\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{K} \\boldsymbol{p})\\right)$ where $\\psi(\\cdot): \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ generalizes the linear head $\\boldsymbol{v}$ of our attention model. For instance, following exposition in Section 1.1, $\\psi(\\cdot)$ can represent a multilayer transformer with $\\boldsymbol{p}$ being a tunable prompt at the input layer. Recall that $\\left(\\boldsymbol{X}_{i}, \\boldsymbol{K}_{i}, Y_{i}\\right)_{i=1}^{n}$ is the dataset of the input-key-label tuples. We consider the training risk\n\n$$\n\\mathcal{L}(\\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i}, \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\boldsymbol{s}_{i}^{\\boldsymbol{p}}\\right)\\right), \\quad \\text { where } \\quad \\boldsymbol{s}_{i}^{\\boldsymbol{p}}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}\\right) \\in \\mathbb{R}^{T}\n$$\n\nThe challenge with nonlinear $\\psi(\\cdot)$ is that, we lack a clear score function (Def. 1) unlike the previous sections. The assumption below introduces a generic condition that splits the tokens of each $X_{i}$ into an optimal set $O_{i}$ and non-optimal set $\\bar{O}_{i}=[T]-O_{i}$. In words, non-optimal tokens are those that strictly increase the training risk $\\mathcal{L}(\\boldsymbol{p})$ if they are not fully suppressed by attention probabilities $s_{i}^{p}$. Assumption D (Mixing non-optimal tokens hurt) There exists sets $\\left(O_{i}\\right)_{i=1}^{n} \\subset[T]$ as follows. Let $q_{i}^{\\boldsymbol{p}}=\\sum_{t \\in \\bar{O}_{i}} \\boldsymbol{s}_{i t}^{\\boldsymbol{p}}$ be the sum of softmax similarities over the non-optimal set for $\\boldsymbol{p}$. Set $q_{\\max }^{\\boldsymbol{p}}=\\max _{i \\in[n]} q_{i}^{\\boldsymbol{p}}$. For any $\\Delta>0$, there exists $\\rho<0$ such that:\n\n$$\n\\text { For all } \\boldsymbol{p}, \\boldsymbol{p}^{\\prime} \\in \\mathbb{R}^{d}, \\text { if } \\log \\left(q_{\\max }^{p}\\right) \\leq(1+\\Delta) \\log \\left(q_{\\max }^{p^{\\prime}}\\right) \\wedge \\rho, \\text { then } \\mathcal{L}(\\boldsymbol{p})<\\mathcal{L}\\left(\\boldsymbol{p}^{\\prime}\\right)\n$$\n\nThis assumption is titled mixing hurts because the attention output $\\boldsymbol{X}_{i}^{\\top} \\boldsymbol{s}_{i}^{p}$ is mixing the tokens of $\\boldsymbol{X}_{i}$ and our condition is that, to achieve optimal risk, this mixture should not contain any non-optimal tokens. In particular, we require that, a model $\\boldsymbol{p}$ that contains exponentially less non-optimality (quantified via $\\left.\\log \\left(q_{\\max }\\right)\\right)$ compared to $\\boldsymbol{p}^{\\prime}$ is strictly preferable. As we discuss in the supplementary material, Theorem 1 is in fact a concrete instance (with linear head $v$ ) satisfying this condition. Before stating our generic theorem, we need to introduce the max-margin separator towards which regularization path of attention will converge. This is a slightly general version of Section 2's (ATT-SVM) problem where we allow for a set of optimal tokens $O_{i}$ for each input. $$\n\\boldsymbol{p}^{m m}=\\arg \\min _{\\boldsymbol{p}}\\|\\boldsymbol{p}\\| \\quad \\text { subject to } \\quad \\max _{\\alpha \\in O_{i}} \\min _{\\beta \\in \\bar{O}_{i}} \\boldsymbol{p}^{\\top}\\left(\\boldsymbol{k}_{i \\alpha}-\\boldsymbol{k}_{i \\beta}\\right) \\geq 1, \\quad \\text { for all } \\quad i \\in[n] . \\quad \\text { (ATT-SVM') }\n$$\n\nUnlike (ATT-SVM), this problem is not necessarily convex when the optimal set $O_{i}$ is not a singleton. To see this, imagine $n=d=1$ and $T=3$ : Set the two optimal tokens as $\\boldsymbol{k}_{1}=1$ and $\\boldsymbol{k}_{2}=-1$ and the non-optimal token as $\\boldsymbol{k}_{3}=0$. The solution set of (ATT-SVM') is $\\boldsymbol{p}^{\\mathrm{mm}} \\in\\{-1,1\\}$ whereas their convex combination $\\boldsymbol{p}=0$ violates the constraints. To proceed, our final result establishes the convergence of regularization path to the solution set of (ATT-SVM') under Assumption D. Theorem 8 Let $\\mathcal{G}^{m m}$ be the set of global minima of (ATT-SVM'). Suppose its margin $\\Xi:=$ $1 /\\left\\|\\boldsymbol{p}^{m m}\\right\\|>0$ and Assumption D holds. Let dist $(\\cdot, \\cdot)$ denote the $\\ell_{2}$-distance between a vector and a set. Following (83), define $\\overline{\\boldsymbol{p}}(R)=\\arg \\min _{\\|p\\| \\leq R} \\mathcal{L}(\\boldsymbol{p})$. We have that $\\lim _{R \\rightarrow \\infty} \\operatorname{dist}\\left(\\frac{\\bar{p}(R)}{\\Xi R}, \\mathcal{G}^{\\mathrm{mm}}\\right)=0$. We note that Theorem 1 is a corollary of this result where $O_{i}$ 's and $\\mathcal{G}^{m m}$ are singleton. Based on this result, with multiple optimal tokens, Theorem 1 would gracefully generalize to solve (ATT-SVM'). ## D. 1 Proof of Theorem 8\n\nProof. The key idea is showing that, thanks to the exponential tail of softmax-attention, (harmful) contribution of the non-optimal token with the minimum margin can dominate the contribution of all other tokens as $R \\rightarrow \\infty$. This high-level approach is similar to earlier works on implicit bias of gradient descent with logistic loss [31,22]. Pick $\\boldsymbol{p}^{m m} \\in \\mathcal{G}^{m m}$ and set $\\boldsymbol{p}_{R}^{\\star}=R \\frac{\\boldsymbol{p}^{m m}}{\\left\\|\\boldsymbol{p}^{m m}\\right\\|}$. This will be the baseline model that $\\boldsymbol{p}_{R}$ has to compete against. Also let $\\overline{\\boldsymbol{p}}_{R}=\\frac{\\boldsymbol{p}_{R}}{\\mathrm{E} R}$. Now suppose dist $\\left(\\overline{\\boldsymbol{p}}_{R}, \\mathcal{G}^{\\mathrm{mm}}\\right) \\nrightarrow 0$ as $R \\rightarrow \\infty$. Then, there exists $\\delta>0$ such that, we can always find arbitrarily large $R$ obeying $\\operatorname{dist}\\left(\\overline{\\boldsymbol{p}}_{R}, \\mathcal{G}^{m m}\\right) \\geq \\delta$. Since $\\overline{\\boldsymbol{p}}_{R}$ is $\\delta>0$ bounded away from $\\mathcal{G}^{m m}$, and $\\left\\|\\overline{\\boldsymbol{p}}_{R}\\right\\|=\\left\\|\\boldsymbol{p}^{m m}\\right\\|, \\overline{\\boldsymbol{p}}_{R}$ strictly violates at least one of the inequality constraints in (ATT-SVM'). Otherwise, we would have $\\bar{p}_{R} \\in \\mathcal{G}^{\\mathrm{mm}}$. Without losing generality, suppose $\\overline{\\boldsymbol{p}}_{R}$ violates the first margin constraint, that is, for some $\\gamma:=\\gamma(\\delta)>0$, $\\max _{\\alpha \\in O_{1}} \\min _{\\beta \\in \\bar{O}_{1}} \\overline{\\boldsymbol{p}}_{R}^{\\top}\\left(\\boldsymbol{k}_{1 \\alpha}-\\boldsymbol{k}_{1 \\beta}\\right) \\leq 1-\\gamma$. Now, we will argue that this will lead to a contradiction as $R \\rightarrow \\infty$ since we will show that $\\mathcal{L}\\left(\\boldsymbol{p}_{R}^{\\star}\\right)<\\mathcal{L}\\left(\\boldsymbol{p}_{R}\\right)$ for sufficiently large $R$. First, let us control $\\mathcal{L}\\left(\\boldsymbol{p}_{R}^{\\star}\\right)$. We study $\\boldsymbol{s}_{i}^{\\star}=\\mathbb{S}\\left(\\boldsymbol{K}_{i} \\boldsymbol{p}_{R}^{\\star}\\right)$ and let $\\alpha_{i} \\in O_{i}$ be the index $\\alpha$ in (ATT-SVM') for which $\\operatorname{margin}_{i}=\\max _{\\alpha \\in O_{i}} \\min _{\\beta \\in \\bar{O}_{i}}\\left(\\boldsymbol{k}_{i \\alpha}-\\boldsymbol{k}_{i \\beta}\\right)^{\\top} \\boldsymbol{p}^{\\text {mm }} \\geq 1$ is attained. Then, we bound the non-optimality amount $q_{i}^{\\star}$ of $\\boldsymbol{p}_{R}^{\\star}$ as\n\n$$\nq_{i}^{\\star}=\\frac{\\sum_{t \\epsilon \\bar{O}_{i}} \\exp \\left(\\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{p}_{R}^{\\star}\\right)}{\\sum_{t \\in[T]} \\exp \\left(\\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{p}_{R}^{\\star}\\right)} \\leq \\frac{\\sum_{t \\epsilon \\bar{O}_{i}} \\exp \\left(\\boldsymbol{k}_{i t}^{\\top} \\boldsymbol{p}_{R}^{\\star}\\right)}{\\exp \\left(\\boldsymbol{k}_{i \\alpha_{i}}^{\\top} \\boldsymbol{p}_{R}^{\\star}\\right)} \\leq T \\exp (-\\Xi R)\n$$\n\nThus, $q_{\\max }^{\\star}=\\max _{i \\in[n]} q_{i}^{\\star} \\leq T \\exp (-\\Xi R)$. Secondly, we wish to control $\\mathcal{L}\\left(\\boldsymbol{p}_{R}\\right)$ by lower bounding the non-optimality in $\\boldsymbol{p}_{R}$. Focusing on the first margin constraint, let $\\alpha \\in O_{1}$ be the index in (ATT-SVM') for which $\\operatorname{margin}_{1} \\leq 1-\\gamma$ is attained. Denoting the amount of non-optimality of the first input as $\\hat{q}_{1}$, we find ${ }^{7}$\n\n$$\n\\hat{q}_{1}=\\frac{\\sum_{t \\in \\bar{O}_{1}} \\exp \\left(\\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}\\right)}{\\sum_{t \\in[T]} \\exp \\left(\\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}\\right)} \\geq \\frac{1}{T} \\frac{\\sum_{t \\in \\bar{O}_{1}} \\exp \\left(\\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}\\right)}{\\exp \\left(\\boldsymbol{k}_{1 \\alpha}^{\\top} \\boldsymbol{p}_{R}\\right)} \\geq T^{-1} \\exp (-\\Xi R(1-\\gamma))\n$$\n\n[^5]We similarly have $q_{\\max }^{\\star} \\geq T^{-1} \\exp (-\\Xi R)$. In conclusion, for $\\boldsymbol{p}_{R}, \\boldsymbol{p}_{R}^{\\star}$, denoting maximum nonoptimality by $\\hat{q}_{\\max } \\geq \\hat{q}_{1}$ and $q_{\\max }^{\\star}$, we respectively obtained\n\n$$\n\\begin{aligned}\n\\log \\left(\\hat{q}_{\\max }\\right) & \\geq-(1-\\gamma)(\\Xi R)-\\log T \\\\\n-(\\Xi R)-\\log T \\leq \\log \\left(q_{\\max }^{\\star}\\right) & \\leq-(\\Xi R)+\\log T\n\\end{aligned}\n$$\n\nThe above inequalities satisfy Assumption D as follows where $\\boldsymbol{p} \\leftarrow \\boldsymbol{p}_{R}^{\\star}$ and $\\boldsymbol{p}^{\\prime} \\leftarrow \\boldsymbol{p}_{R}$ : Set $R_{0}=$ $3 \\gamma^{-1} \\Xi^{-1} \\log T$ so that $\\log T=\\frac{\\gamma \\Xi R_{0}}{3}$. Secondly, set $\\rho_{0}=-\\Xi R_{0}-\\log T$. This way, $\\rho_{0} \\geq \\log \\left(q_{\\max }^{\\star}\\right)$ implies $R \\geq R_{0}$ and $\\log T \\leq \\frac{\\gamma \\Xi R}{3}$. Using the latter inequality, we bound the $\\log T$ terms to obtain\n\n- $\\log \\left(\\hat{q}_{\\max }\\right) \\geq-(1-2 \\gamma / 3)(\\Xi R)$, and\n- $\\log \\left(q_{\\text {max }}^{\\star}\\right) \\leq-(1-\\gamma / 3)(\\Xi R)$. To proceed, we pick $1+\\Delta=\\frac{1-\\gamma / 3}{1-2 \\gamma / 3}$ implying $\\Delta:=\\frac{\\gamma}{3-2 \\gamma}$. Finally, for this $\\Delta$, there exists $\\rho(\\Delta)$ which we need to ensure $\\log \\left(\\hat{q}_{\\max }\\right) \\leq \\rho(\\Delta)$. This can be guaranteed by picking sufficiently large $R$ that ensures $\\log \\left(q_{\\max }^{\\star}\\right) \\leq-(1-\\gamma / 3)(\\Xi R) \\leq \\rho(\\Delta)$ to satisfy all conditions of Assumption D. Since such large $R$ exists by initial assumption $\\operatorname{dist}\\left(\\overline{\\boldsymbol{p}}_{R}, \\mathcal{G}^{\\text {mm }}\\right) \\nrightarrow 0$, Assumption D in turn implies that $\\mathcal{L}\\left(\\boldsymbol{p}_{R}^{\\star}\\right)<\\mathcal{L}\\left(\\boldsymbol{p}_{R}\\right)$ contradicting with the optimality of $\\boldsymbol{p}_{R}$ in (83). ## D. 2 Application to Linearly-mixed Labels\n\nThe following example shows that if non-optimal tokens result in reduced score (in terms of the alignment of prediction and label), Assumption D holds. The high-level idea behind this lemma is that, if the optimal risk is achieved by setting $q_{\\max }^{p}=0$, then, Assumption D will hold. Lemma 14 (Linear label mixing) Recall $q_{i}^{\\boldsymbol{p}}=\\sum_{t \\in \\bar{O}_{t}} \\boldsymbol{s}_{i t}^{\\boldsymbol{p}}$ from Assumption D. Suppose $Y_{i} \\in\\{-1,1\\}$ and\n\n$$\nY_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\boldsymbol{s}_{i}^{\\boldsymbol{p}}\\right)=v_{i}\\left(1-q_{i}^{\\boldsymbol{p}}\\right)+Z_{i}\n$$\n\nfor some $\\left(v_{i}\\right)_{i=1}^{n}>0$. Here $Z_{i}=Z_{i}(\\boldsymbol{p})$ is the contribution of non-optimal tokens to prediction. For some $C, \\epsilon>0$ and for all $\\boldsymbol{p} \\in \\mathbb{R}^{d}$, assume\n\n$$\n-C q_{\\max }^{p} \\leq Z_{i} \\leq(1-\\epsilon) v_{i} q_{i}^{p}\n$$\n\nThen, Assumption D holds for $\\mathcal{L}(\\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(Y_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\boldsymbol{s}_{i}^{\\boldsymbol{p}}\\right)\\right)$ when $\\ell(\\cdot)$ is a strictly decreasing loss function with continuous derivative. Proof. Recall the assumption $Y_{i} \\cdot \\psi\\left(\\boldsymbol{X}_{i}^{\\top} \\boldsymbol{s}_{i}^{\\boldsymbol{p}}\\right)=v_{i}\\left(1-q_{i}^{\\boldsymbol{p}}\\right)+Z_{i}$ with $Z_{i}$ obeying (85). Let us also write the loss function\n\n$$\n\\mathcal{L}(\\boldsymbol{p})=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(v_{i}\\left(1-s_{i}^{\\boldsymbol{p}}\\right)+Z_{i}\\right)\n$$\n\nDefine $q_{\\max }^{p}=\\sup _{i \\leq[n]} q_{i}^{p}$. Let $M$ be the maximum absolute value of score over tokens. Let\n\n$$\nB=\\max _{|x| \\leq M}-\\ell^{\\prime}(x) \\geq A=\\min _{|x| \\leq M}-\\ell^{\\prime}(x)>0\n$$\n\nThrough Taylor's Theorem (integral remainder), we have that\n\n$$\nB\\left(q_{i}^{p} v_{i}-Z_{i}\\right) \\geq \\ell\\left(v_{i}\\left(1-q_{i}^{p}\\right)+Z_{i}\\right)-\\ell\\left(v_{i}\\right) \\geq A\\left(q_{i}^{p} v_{i}-Z_{i}\\right) \\geq \\epsilon A v_{i} q_{i}^{p}\n$$\n\nSet $\\mathcal{L}_{\\star}=\\frac{1}{n} \\sum_{i=1}^{n} \\ell\\left(v_{i}\\right)$. Set $C_{+}=B\\left(C+\\max _{i \\in[n]} v_{i}\\right)$ and $C_{-}=n^{-1} A \\epsilon \\min _{i \\in[n]} v_{i}$. This also implies\n\n$$\n\\begin{aligned}\nC_{+} q_{\\max }^{p} \\geq \\frac{1}{n} \\sum_{i \\in[n]} B\\left(q_{i}^{p} v_{i}-Z_{i}\\right) \\geq \\mathcal{L}(\\boldsymbol{p})-\\mathcal{L}_{\\star} & \\geq \\frac{1}{n} \\sum_{i \\in[n]} A\\left(q_{i}^{p} v_{i}-Z_{i}\\right) \\\\\n& \\geq \\frac{1}{n} \\sum_{i \\in[n]} \\epsilon A v_{i} q_{i}^{p} \\geq C_{-} q_{\\max }^{p}\n\\end{aligned}\n$$\n\nThus, to prove $\\mathcal{L}\\left(\\boldsymbol{p}^{\\prime}\\right)>\\mathcal{L}(\\boldsymbol{p})$, we simply need to establish the stronger statement $C_{-} q_{\\text {max }}^{\\boldsymbol{p}^{\\prime}}>C_{+} q_{\\text {max }}^{\\boldsymbol{p}}$. Going back to the condition of Assumption D, any $\\log \\left(q_{\\max }^{p}\\right) \\leq(1+\\Delta) \\log \\left(q_{\\max }^{p^{\\prime}}\\right)$ obeys $q_{\\max }^{p} \\leq$ $\\left(q_{\\max }^{\\boldsymbol{p}^{\\prime}}\\right)^{1+\\Delta}$ i.e. $q_{\\max }^{\\boldsymbol{p}^{\\prime}} \\geq\\left(q_{\\max }^{\\boldsymbol{p}}\\right)^{(1+\\Delta)^{-1}}$. Following above, we wish to ensure $q_{\\max }^{\\boldsymbol{p}^{\\prime}}>\\Theta q_{\\max }^{\\boldsymbol{p}}$ for such $\\left(\\boldsymbol{p}, \\boldsymbol{p}^{\\prime}\\right)$ pairs where $\\Theta=C_{+} / C_{-}>1$. This is guaranteed by\n\n$$\n\\left(q_{\\max }^{p}\\right)^{(1+\\Delta)^{-1}-1}>\\Theta \\Longleftrightarrow \\frac{\\Delta}{1+\\Delta} \\log \\left(q_{\\max }^{p}\\right)<-\\log (\\Theta)\n$$\n\nThe above is satisfied by choosing a $\\rho(\\Delta):=-2\\left(1+\\Delta^{-1}\\right) \\log (\\Theta)$ in Assumption D. Thus, all $\\boldsymbol{p}, \\boldsymbol{p}^{\\prime}$ with $\\log \\left(q_{\\max }^{p}\\right) \\leq \\rho=\\rho(\\Delta)$ satisfies the condition of Assumption D finishing the proof. ## E Implementation Details and Additional Experiments\n\nIn this section, we provide implementation details and additional experiments. - We build one attention layer using PyTorch. During training, we use SGD optimizer with learning rate 0.1 and train the model for 1000 iterations. To better visualize the convergence path, we normalize the gradient of $\\boldsymbol{p}$ (and $\\boldsymbol{v}$ ) at each iteration. - Next, given the gradient solution $\\boldsymbol{p}$, we determine locally-optimal indices to be those with the highest softmax scores. Using these optimal indices, we utilize python package cvxopt to build and solve (ATT-SVM), and then get solution $\\boldsymbol{p}^{m m}$. After obtaining $\\boldsymbol{p}^{m m}$, we also verify that these indices satisfy our local-optimal definition. The examples we use in the paper are all trivial to verify (by construction). - In Figures 3(a) and 3(b), $\\boldsymbol{v}^{\\text {mm }}$ (blue dashed) is solved using python package sklearn. svm via (SVM) based on the given label information, and red dashed line represents $\\boldsymbol{p}^{\\text {relax }}$ direction instead, which is the solution of (10). Note that in both figures, $\\boldsymbol{v}^{\\mathrm{mm}} /\\left\\|\\boldsymbol{v}^{\\mathrm{mm}}\\right\\|=[0,1]$. Therefore, in Figure 3(a) all optimal tokens are support vectors and $\\boldsymbol{p}^{\\text {relax }}=\\boldsymbol{p}^{\\text {mm }}$. Whereas in Figure 3(b), yellow $\\star$ is not a support vector and only needs to satisfy positive correlation with $\\boldsymbol{p}$. Gray dashed line displays the $\\boldsymbol{p}^{\\mathrm{mm}}$ direction. Failure of gradient descent's global convergence when $n \\geq 2$ (refer to Theorem 2). Figure 8 provides a counter-example demonstrating that the $n=1$ restriction is indeed necessary and tight to guarantee global convergence of the gradient descent iterates $\\boldsymbol{p}(t+1)=\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t))$ on (ERM). For this example, we use logistic loss in (ERM). We set $n=T=d=2$, implying that there is only one nonoptimal token, thus Assumption B is satisfied. The red and blue lines represent GMM and LMM solutions, respectively. We note that the green star and teal square indicate the locally-optimal tokens. Specifically, referring to the local optimality definition (Definition 2), for LMM solution $\\left(\\boldsymbol{p}^{\\mathrm{mm}}\\right)$ represented by the blue line, the square teal token does not have any SVM-neighbors. The arrows indicate the two trajectories originating from different initializations. This demonstrates that the gradient descent iterates $\\boldsymbol{p}(t+1)=\\boldsymbol{p}(t)-\\eta \\nabla \\mathcal{L}(\\boldsymbol{p}(t))$ on (ERM) with two different initializations converge to two different SVM solutions (GMM and LMM). Results validate the necessity of $n=1$ in Theorem 2 to provide the gradient descent convergence to $\\boldsymbol{p}^{\\text {mm\u592b }}$ from any initialization. ![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-45.jpg?height=400&width=551&top_left_y=1513&top_left_x=1210)\n\nFigure 8: The convergence behavior of the gradient descent on the attention weights $\\boldsymbol{p}$ using the logistic loss in (ERM) with $n=T=d=2$. The convergence behavior of gradient descent under over-parameterization. To illustrate Theorems $3 \\& 4$, we have investigated the convergence behavior of $\\boldsymbol{p}(t)$ generated by gradient descent in Figure 9(a), using $n=4, T=6$, and conducted 1,000 random trials for varying $d \\in$ $\\{2,5,10,100,300,500\\}$. These experiments use normalized gradient descent with learning rate 1 for 1000 iterations. Inputs $\\boldsymbol{x}_{i t}$ and the linear head $\\boldsymbol{v}$ are uniformly sampled from the unit sphere, while $Y_{i}$ is uniformly $\\pm 1$, and $\\boldsymbol{W}$ is set to $\\boldsymbol{I}$. The bar plot in Figure 9(a) distinguishes between non-saturated softmax (red bars) and saturated softmax (other bars). Saturation is defined as average softmax probability over tokens selected by gradient descent are at least $1-10^{-5}$ and implies that attention selects one token per input. Note that, whenever the norm of gradient descent solution is finite, softmax will be non-saturated. For\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-46.jpg?height=499&width=1383&top_left_y=267&top_left_x=350)\n\nFigure 9: Convergence analysis of $\\boldsymbol{p}(t)$ trained with random data using gradient descent. (a) shows three scenarios: (1) attention failing to select one token per input (i.e. softmax is not saturated); (2) $\\boldsymbol{p}$ converging towards $\\boldsymbol{p}^{m m}$; and (3) $\\boldsymbol{p}^{m m}$ equating to $\\boldsymbol{p}^{m m \\star}$ with red, blue, and green bars, respectively. Considering saturated softmax instances where $\\boldsymbol{p}(t)$ selects one token $\\alpha_{i}$ per-input, (b) presents histogram of the minimal score gap between $\\alpha_{i}$ and its corresponding neighbors $\\mathcal{T}_{i}$\nsmall $d$ (e.g., $d=2$ ), problem has small degrees of freedom to separate optimal tokens from the rest (i.e. no SVM solution for LMM directions) - especially due to label randomness. This results in a tall red bar capturing the finite-norm solutions. However, for larger $d$, we observe that softmax saturates (i.e. $\\|\\boldsymbol{p}(t)\\| \\rightarrow \\infty$ ) and we observe that the selected tokens $\\boldsymbol{\\alpha}$ almost always converges to an LMM direction (blue bar) - this is in line with Theorems $3 \\& 4$. We also study the convergence to the globally-optimal GMM which is represented by the green bar: GMM is a strict subset of LMM however as $d$ increases, we observe that the probability of GMM convergence increases as well. This behavior is in line with what one would expect from over-parameterized deep learning theory [57, 58, 59, 60] and motivates future research. The average correlation coefficient between $\\boldsymbol{p}(t)$ and its associated LMM/GMM direction is 0.997 , suggesting that, whenever softmax saturates, gradient descent indeed directionally converges to a LMM solution $\\boldsymbol{p} \\in \\mathcal{P}^{m m}$, confirming Theorem 4. Furthermore, we found that there exist problem instances, with saturated softmax and $\\|\\boldsymbol{p}(t)\\| \\rightarrow \\infty$, that do not converge to either LMM or GMM. We analyzed this phenomenon using the minimum score gap, $\\underline{\\gamma}:=\\min _{i \\in[n], t \\in \\mathcal{T}_{i}} \\boldsymbol{\\gamma}_{i \\alpha_{i}}-\\gamma_{i t}$, where $\\mathcal{T}_{i}, i \\in[n]$, represents the sets of SVM-neighbor tokens. Figure 9 (b) provides the probability distribution of $\\underline{\\gamma}$ (with bins of width $<0.01$ ) and demonstrates the rarity of such cases. Specifically, we found this happens less than $1 \\%$ of the problems, that is, $\\operatorname{Prob}(\\underline{\\gamma}<0)<0.01$. Figure 9(b) also reveals that, in these scenarios, even if $\\underline{\\gamma}<0$, it is typically close to zero i.e. even if there exists a SVM-neighbor with a higher score, it is only slightly so. This is not surprising since when token scores are close, we need a large number of gradient iterations to distinguish them. For all practical purposes, the optimization will treat both tokens equally and rather than solving (ATT-SVM), the more refined formulation (ATT-SVM') developed in Section D will be a better proxy. Confirming this intuition, we have verified that, over the instances $\\underline{\\gamma}<0$, gradient descent solution is still $>0.99$ correlated with the max-margin solution in average.",
    "maxmargin-67": "- In Figure 9, we again applied normalized gradient descent with a learning rate equal to 1. Each trial involved randomly generated data and training for 1000 iterations as discussed in Theorem 4. The tokens selected by $\\boldsymbol{p}$ were denoted as $\\left(\\alpha_{i}\\right)_{i=1}^{n}$, where $\\alpha_{i}=\\arg \\max _{t \\in[T]} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)_{t}$. The averaged softmax probabilities were calculated as $\\bar{s}:=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)_{\\alpha_{i}}$ (same as Figure 3(c)). The red bars in Figure 9(b) represent the values of $\\mathbb{P}\\left(\\bar{s} \\leq 1-10^{-5}\\right)$ for each choice of $d$. Figure 10 displays the cumulative probability distribution of $\\gamma$ from Figure 9(b), with the gray dashed line indicating $\\bar{\\gamma}=0$. From this figure, we observe that the minimal score gap exhibit a sharp transition at zero ( $<1 \\%$ of the instances have $\\underline{\\gamma}<0$ ), demonstrating that, in most random problem instances with $\\|\\boldsymbol{p}\\| \\rightarrow \\infty(\\bar{s} \\rightarrow 1)$, problem directionally\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b0dd7275b5506dd10cd1g-46.jpg?height=408&width=538&top_left_y=2016&top_left_x=1203)\n\nFigure 10: Cumulative prob. of the gap $\\underline{\\gamma}:=\\min _{i \\in[n], t \\in \\mathcal{T}_{i}} \\boldsymbol{\\gamma}_{i \\alpha_{i}}-\\gamma_{i t}$ in Figure 9(b). converges to an LMM i.e. $\\boldsymbol{p}(t) /\\|\\boldsymbol{p}(t)\\| \\rightarrow \\boldsymbol{p}^{m m} /\\left\\|\\boldsymbol{p}^{m m}\\right\\|$. We believe the rare occurrence of a negative score gap is due to small score differences (so that optimal token is not clearly distinguished) and finite number of gradient iterations we run. Interestingly, even in the negative score gap scenarios, gradient descent is aligned with $\\boldsymbol{p}^{\\mathrm{mm}}(\\boldsymbol{\\alpha})$ (even if $\\boldsymbol{p}^{\\mathrm{mm}}(\\boldsymbol{\\alpha})$ is not LMM) which can be predicted from our Section D which handles the scenario where there are multiple optimal tokens per input. ## F Addendum to Section 5\n\nWe provide an overview of the current literature on implicit regularization and attention mechanism. ## F. 1 Related Work on Implicit Regularization\n\nThe introduction of Support Vector Machines (SVM), which utilize explicit regularization to choose maximum margin classifiers, represents one of the earliest relevant literature in this field [61]. The concept of maximizing the margin was later connected to generalization performance [62]. From a practical perspective, exponential losses with decaying regularization exhibit asymptotic behavior similar to SVMs, as demonstrated in [22]. While the analysis of the perceptron [63] originally introduced the concept of margins, the method itself does not possess an inherent bias as it terminates with zero classification error. However, establishing a meaningful lower bound for the attained margin is not possible. Initial empirical investigations highlighting the implicit bias of descent methods focused on $\\ell_{1}$-regularization, revealing that coordinate descent, when combined with the exponential loss, exhibits an inherent inclination towards $\\ell_{1}$-regularized solutions [64]. This work draws extensively from the literature on implicit bias and regularization, which has provided valuable techniques and inspiration. A common observation in these studies is the convergence to a specific optimal solution over the training set. This phenomenon has been observed in various approaches, including coordinate descent [65, 66], gradient descent [30, 67, 25, 68, 69, 22, 70], deep linear networks [71, 72], ReLU networks [73, 74, 29, 75, 76], mirror descent [77, 78, 33, 36], and many others. The implicit bias of gradient descent in classification tasks involving separable data has been extensively examined by [22,25,26,27,28,29]. The works on classification typically utilize logistic loss or exponentially-tailed losses to establish connections to margin maximization. The results have also been extended to non-separable data by [30,31, 21]. Additionally, several papers have explored the implicit bias of stochastic gradient descent [37, 38, 41, 42], as well as adaptive and momentum-based methods [43, 44, 45, 46]. While there are some similarities between our optimization approach for $v$ and existing works, the optimization of $\\boldsymbol{p}$ presents notable differences. Firstly, our optimization problem is nonconvex and involves a composition of loss and softmax, which introduces new challenges and complexities. The presence of softmax adds a nonlinearity to the problem, requiring specialized techniques for analysis and optimization. Secondly, our analysis introduces the concept of locally-optimal tokens, which refers to tokens that achieve locally optimal solutions in their respective attention cones. This concept is crucial for understanding the behavior of the attention mechanism and its convergence properties. By focusing on the cones surrounding locally-optimal tokens, we provide a tailored analysis that captures the unique characteristics of the attention model. Overall, our work offers novel insights into the optimization of attention-based models and sheds light on the behavior of the attention mechanism during training. ## F. 2 Related Work on Attention Mechanism\n\nAs the backbone of Transformers [6], the self-attention mechanism [47, 48, 49, 50] plays a crucial role in computing feature representations by globally modeling long-range interactions within the input. Transformers have achieved remarkable empirical success in various domains, including natural language processing [4, 2], recommendation systems [79, 80, 81], and reinforcement learning [82, 83, 84]. With the introduction of Vision Transformer (ViT) [85], Transformer-based models $[86,87]$ have become a strong alternative to convolutional neural networks (CNN) and become prevalent in vision tasks. However, the theoretical foundation of Transformers and self-attention mechanisms has remained largely unexplored. Some studies have established important results, including the Lipschitz constant of self-attention [88], properties of the neural tangent kernel [89, 90], and the expressive power and\n\nTuring-completeness of Transformers [91, 92, 93, 51, 23, 94] with statistical guarantees [95, 96]. There is also a growing effort towards a theoretical understanding of emergent abilities of language models - such as in-context learning [97, 98, 99] and chain-of-thought [100, 101, 102] - which are inherently related to the models ability to attend to the relevant information within the input sequence. Focusing on the self-attention component, Edelman et al. [51] theoretically shows that a single self-attention head can represent a sparse function of the input with a sample complexity for the generalization gap between the training loss and the test loss. However, they did not delve into the algorithmic aspects of training Transformers to achieve desirable loss.",
    "maxmargin-68": "Sahiner et al. [52] and Ergen et al. [53] further explored the analysis of convex relaxations for self-attention, investigating potential optimization techniques and properties. The former work applies to self-attention with linear activation (rather than softmax) whereas the latter work attempts to approximate softmax via a linear operation with unit simplex constraints. In contrast, we directly study softmax and characterize its non-convex geometry. In terms of expressive ability, Baldi and Vershynin [54] investigated the capacity of attention layers to capture complex patterns and information, while Dong et al. [23] illustrates the propensity of attention networks to degenerate during the training process, with the result often being an output that is approximately a rank-1 matrix. Recent works have made progress in characterizing the optimization and generalization dynamics of attention [55, 56, 103, 17, 104]. Jelassi et al. [55] studied gradient-based methods from random initialization and provided a theoretical analysis of the empirical finding that Vision Transformers learn position embeddings that recapitulate the spatial structure of the training data, even though this spatial structure is no longer explicitly represented after the image is split into patches. Li et al. [56] provided theoretical results on training three-layer ViTs for classification tasks. They quantified the importance of self-attention in terms of sample complexity for achieving zero generalization error, as well as the sparsity of attention maps when trained by stochastic gradient descent (SGD). In another related work, Nguyen et al. [104] proposed a primal-dual optimization framework that focuses on deriving attention as the dual expansion of a primal neural network layer. By solving a support vector regression problem, they gained a deeper understanding and explanation of various attention mechanisms. This framework also enables the creation of novel attention mechanisms, offering flexibility and customization in designing attention-based models. In another closely related work, Oymak et al. [17] analyzed the same attention model as ours, denoted by (ERM). Specifically, they jointly optimize $\\boldsymbol{v}, \\boldsymbol{p}$ for three gradient iterations for a contextual dataset model. This is in contrast to our emphasis on infinite-iteration behavior of $\\boldsymbol{p}$-only optimization. However, it is important to note that all of these works make certain assumptions about the data. Specifically, they assume that tokens are tightly clusterable or can be clearly split into relevant and irrelevant sets. Additionally, Li et al. [56] require specific assumptions on the initialization of the model, while Jelassi et al. [55] consider a simplified attention structure where the attention matrix is not directly parameterized with respect to the input. In contrast, our work offers a comprehensive optimization-theoretic analysis of the attention model, establishing a formal connection to max-margin problems. While comparable works make assumptions on the dataset model, our results apply under minimal assumptions for general data and realistic conditions. Our analysis based on max-margin-equivalence allows us to gain a deeper understanding of the optimization geometry of attention and its behavior during the training process. As articulated in our experiments, our results lead to novel insights even for $n=1,2$ samples, $T=2,3$ tokens and $d=2,3$ dimensions (in contrast to [55, 56, 103, 17]). Notably, our work also presents the first theoretical understanding of the implicit bias exhibited by gradient descent methods in the context of the attention model. We remark that recent work [105] expands the theory presented in this work to 1-layer transformers. By uncovering the underlying optimization principles and thoroughly characterizing the directional convergence of attention, we provide valuable insights into the dynamics and generalization properties of attention-based models opening the path for future research. [^0]:    ${ }^{1}$ The code for experiments can be found at https://github.com/ucr-optml/max_margin_attention. [^1]:    ${ }^{2}$ For simplicity, we use $\\pm$ on the right hand side to denote the upper and lower bounds. [^2]:    ${ }^{3}$ If $\\alpha_{i}$ is unique for all $i \\in[n]$, let us call it, unique selected tokens. [^3]:    ${ }^{4}$ This requirement holds for general data because it is guaranteed by adding arbitrarily small independent gaussian perturbations to keys $\\boldsymbol{k}_{i t}$. ${ }^{5}$ As a result, let us prove the result for $\\epsilon \\leftarrow 2 \\epsilon$ without losing generality. [^4]:    ${ }^{6}$ Here, we apply this lemma to compare $\\boldsymbol{q} \\boldsymbol{q}$ against all $\\boldsymbol{p} \\in \\boldsymbol{N}_{\\delta}$. We can do this uniform comparison because the $R$ requirement in Lemma 8 only depends on the margin difference and global problem variables and not the particular choice of $\\boldsymbol{p} \\in \\mathcal{N}_{\\delta}$. [^5]:    ${ }^{7}$ Here, we assumed margin is non-negative i.e. $\\boldsymbol{k}_{1 \\alpha}^{\\top} \\boldsymbol{p}_{R} \\geq \\sup _{t \\epsilon \\bar{O}_{1}} \\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}$. Otherwise, $\\sup _{t \\in[T]} \\boldsymbol{k}_{1 t}^{\\top} \\boldsymbol{p}_{R}$ is attained in $\\bar{O}_{1}$ which implies $\\hat{q}_{1} \\geq T^{-1}$. Thus, we can still use the identical inequality (84) with the choice $\\gamma=1$.",
    "maxmargin-69": ""
}