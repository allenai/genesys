{
    "hierarchinn-0": "# Hierarchical Neural Network Approaches for Long Document Classification \n\nSnehal Khandve ${ }^{1 \\star}$, Vedangi Wagh ${ }^{1 \\star}$, Apurva Wani ${ }^{1 \\star}$, Isha Joshi ${ }^{1 \\star}$, and<br>Raviraj Joshi ${ }^{2}$<br>${ }^{1}$ Pune Institute of Computer Technology, India<br>${ }^{2}$ Indian Institute of Technology Madras, India\n\n\n#### Abstract\n\nText classification algorithms investigate the intricate relationships between words or phrases and attempt to deduce the document's interpretation.",
    "hierarchinn-1": "In the last few years, these algorithms have progressed tremendously. Transformer architecture and sentence encoders have proven to give superior results on natural language processing tasks. But a major limitation of these architectures is their applicability for text no longer than a few hundred words. In this paper, we explore hierarchical transfer learning approaches for long document classification. We employ pre-trained Universal Sentence Encoder (USE) and Bidirectional Encoder Representations from Transformers (BERT) in a hierarchical setup to capture better representations efficiently. Our proposed models are conceptually simple where we divide the input data into chunks and then pass this through base models of BERT and USE. Then output representation for each chunk is then propagated through a shallow neural network comprising of LSTMs or CNNs for classifying the text data. These extensions are evaluated on 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its stand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its stand-alone counterpart. However, the hierarchical BERT models are still desirable as it avoids the quadratic complexity of the attention mechanism in BERT. Along with the hierarchical approaches, this work also provides a comparison of different deep learning algorithms like USE, BERT, HAN, Longformer, and BigBird for long document classification. The Longformer approach consistently performs well on most of the datasets. Keywords: Transformer, BERT, CNN, LSTM, Universal sentence encoder, Document Classification, Hierarchical Approaches\n\n## 1 Introduction\n\nAn enormous amount of data is generated every day in the form of text. This data is generally unstructured and hence it is necessary to classify it to extract meaningful information. Text classification involves labeling this raw input data into various classes based on their content. There are many machine learning\n\n[^0]models available that automate and simplify the text classification process 21 , 29, 30]. Processing the text in the document and allocating it to one of the predefined categories is called document classification. A major hurdle in document classification is the large length of the input text. It is difficult to comprehend the entire information in large news articles or movie reviews and categorize them. We begin with existing transformer-based models and then move towards hierarchical models which make use of the inherent structure of the document to perform classification seamlessly [35, 6, 17]. A document can be seen as a sequence of sentences. The sentences are in turn are a sequence of words. We exploit this structure of words $->$ sentences $->$ document and process it using hierarchical neural network models. The first level processes the sequence of words to generate sentence representation whereas the second level processes the sequence of sentence representation to generate document representation. Although such models have been previously studied in literature we present a more exhaustive analysis. In this paper, we make use of two sentence encoder architectures - USE and BERT - which are used to generate sentence embeddings in a hierarchical network. These sentence encoders are pre-trained models thus allowing us to leverage transfer learning with hierarchical models. Universal Sentence Encoder 77 encodes sentences into fixed-length embedding vectors for transfer learning tasks. This model is pre-trained on a general corpus and can be efficiently finetuned for diverse transfer tasks. The paper proposes two models - one uses transformers 28 and the other uses DAN(Deep Averaging Network) 16]. The DAN model is highly efficient with linear complexity and gives slightly lower accuracy than the transformer model. We make use of the DAN variation for all the experiments. Since it is difficult to encode long documents into fixed-size vectors we explore a two-level architecture for USE. The transformer architecture has shown superior results on almost all-natural language processing tasks. Transformers have surpassed the previous sequential models in both efficiency and accuracy. They have the advantage of processing all input tokens simultaneously thereby utilizing the hardware most optimally. This architecture is based on the self-attention mechanism [3] where each token is processed to generate a contextual embedding by taking every other token. BERT 12 is a novel architecture with a set of bidirectional transformers stacked on top of each other. This model achieved state-of-the-art results on various NLP tasks. Although transformer-based architectures like BERT have a decided advantage over sequential models, they are computationally expensive as the input length increases. The quadratic complexity of the self-attention mechanism is a major limitation for a task like long document classification. In this paper, we introduce 4 models that have been built upon USE and BERT architectures. We split the input text into segments and obtain USE and BERT representations of these segments using a shared encoder. This is followed by a shallow CNN or LSTM network to perform further classification. The six datasets that were employed in this work are standard text classification datasets viz., BBC News, AG News, 20 NewsGroup (20 NG), R8, BBC Sports, and IMDB. The novel contributions of this work are as follows:\n\n- To the best of our knowledge, this is the first work to use USE in a hierarchical setup along with CNN or LSTM and show its effectiveness. - Although hierarchical approaches have been explored for BERT, we provide a more exhaustive analysis on six benchmark datasets and different deep learning algorithms. We also contrast BERT + LSTM with BERT + CNN to show that the former works better in most of the cases. - We benchmark different deep learning algorithms like USE, BERT, HAN, Longformer, and BigBird on these datasets. The Longformer consistently performs well on most of the datasets. ## 2 Related Work\n\nIn the pre-Transformers era, the deep learning approaches for text classification typically made use of CNNs or RNNs. Text classification using CNNs with little hyperparameter tuning showed significant improvement over traditional approaches [9]. The performance was further improved with deeper CNNs which were implemented at the character level 10,34 instead of word level. 25, 26, make use of tree-structured LSTMs for text classification. Architectures with combinations of LSTM and CNN are used in 19] 36] 31. In 27] a hierarchical network is implemented using simple CNN and RNN for document classification. In the first stage, a sentence is represented using an LSTM or CNN. These sentence representations are passed to gated RNNs to form a vector representing the entire document. Hierarchical Attention Networks(HAN) were introduced in 32 which used self-attention [3] at the word-level and sentence-level. The model mirrored the inherent structure of documents in the form of paragraphs, sentences, and words. 13] implemented Hierarchical Convolutional Attention Networks (HCAN) which are like transformers 28, used convoluted self-attention layers on raw embeddings. This approach could be trained faster than RNNs and gave competitive results. One of the shortcomings of transformers is that they do not perform well when it comes to classifying long sequences of text. It was addressed by introducing two models viz. Tobert(Transformer over BERT) and RoBERT(Recurrence over BERT), built on the top of BERT [12]. The input text was divided into smaller chunks, passed on to the base model of BERT to obtain their word representations which are then given to the next layer(LSTM or BERT) 22]. 4] performed an empirical comparison of hierarchical models and transfer learning methods for document classification in five languages. Hierarchical networks are seen to outperform previous baselines and this effect is particularly stronger for longer documents. CNNs have also been explored extensively for text classification. Recording CNN results as baseline accuracies, 15 used RNNs to take into account the sequential nature of events occurring in the mental health-related text. They also studied attention mechanisms to focus only on the relevant text. Thus Hierarchical Recurrent Neural Network (RNN) architecture was built to classify the text with improved performance. This idea is further extended by [8, by proposing language-model-based pre-training methods for hierarchical document representations. The two methods proposed are: 1) Pre-training hierarchical left-to-right and right-to-left document representations and, 2) Masked language model technique to pre-train bidirectional hierarchical document-level representations. BERT [12] is a set of bidirectional stacked transformer 28 units that are trained to perform two tasks namely masked word prediction and next sentence prediction. BERT has been used for document classification (1) but the input length was truncated to 512 , the typical input length for BERT. To overcome the limitation of quadratic complexity posed by the self-attention mechanism, some variations of BERT have been implemented to classify longer documents without increasing the complexity 18]. 5, 33 use simultaneous global attention and local sliding windows thereby reducing complexity. 11 connects multiple segments of text by recurrence and uses relative position embeddings. ## 3 Dataset Details\n\nWe make use of text classification datasets with longer document lengths 29. The datasets used in this work are described below. - 20NG : The 20 Newsgroup dataset 23] contains a total of 18774 records. Using an $80 \\%$ split, train data contains 11314 records and test data contains 7532 records. Each record is a news article with an average size of 315 words. There are 20 distinct classes in this dataset. - BBC News : 14 dataset is one of the popular text classification datasets. It contains news articles classified into 5 different categories viz., Sport, Business, Politics, Tech, and Entertainment. There are 2225 records each with an average size of 389 words. - AG News : [2] dataset is divided into a training set with 120000 records and a test set with 7600 records. Each record is a news article collected by ComeToMyHead. There are 4 classes viz., Business, World, Sports, and $\\mathrm{Sci} /$ Tech. The average size of each record is 39 words. - BBC Sports : 14 dataset consists of 737 sports articles each with an average size of 337 words. These articles are collected from the BBC Sport website and classified into 5 categories viz., Athletics, Cricket, Football, Rugby and Tennis. Using an 80:20 split, train data contains 590 records and test data contains 147 records. - IMDB : 20 dataset is used for sentiment classification and contains 50000 movie reviews. It is a binary text analytics dataset with an average record size of 231 words. Each review is classified as either positive or negative. The standard train test split available for this dataset is $50 \\%$. - R8 : 24 dataset is a subset of the Reuters 21578 dataset. Each record is divided into 8 categories viz., Earn, Acq, Trade, Ship, Grain, Crude, Interest, and Money-fx. The average size of each record is 64 words. Training data and test data consist of 5485 and 2189 documents respectively. ![](https://cdn.mathpix.com/cropped/2024_09_12_5818b8c079feaef0f4feg-05.jpg?height=966&width=854&top_left_y=666&top_left_x=641)\n\nFig. 1. Block diagram of the proposed model architecture\n\n## 4 Experimental Setup\n\nThe problem is formulated as a standard multi-class text classification problem.",
    "hierarchinn-2": "The input is pre-processed and the unwanted tokens and characters are removed. The clean text is then passed to the model with appropriate tokenization. The non-BERT based models use word tokenization whereas the BERT uses subword tokenization. For hierarchical models, the text is segmented into fixed-size chunks and the document is represented as a series of chunks. The total number of chunks in each document or sentence may vary depending upon the total words in it. The size of each chunk ranges from $20-50$ words. The chunk size is chosen depending upon the average length of the dataset. The individual chunk\nis passed to base USE or BERT to get its sentence embedding which is further passed through a CNN or LSTM network. We have also experimented with other partitioning schemes like variable size chunks but the fixed-length chunks work optimally. ### 4.1 Preprocessing\n\nFollowing preprocessing steps are used on the input data:\n\n- Remove HTML tags: HTML tags in the text data are removed as they often don't convey useful information. - Normalization: Complete text in the document is converted into lowercase. - Remove accented characters: In this step accented characters are converted into ASCII characters to prevent the NLP model from handling accented terms and others differently than their normal spellings. - Expand Contractions: Words that make use of apostrophes used for shortening text are expanded. - Remove special characters: Special characters are neither alphabets nor numerals, they are unreadable and hence need to be removed. ## 5 Results\n\n### 5.1 Architecture Details\n\nThe USE, BERT, and HAN are the baseline architectures used in this work. We couple USE and BERT with CNN or LSTM to form a hierarchical model. The general setup of model is shown in Figure 1. The hierarchical variations of the model are described below. - USE + LSTM: For this experiment, input data is converted into 512dimensional vectors using the USE architecture. The pre-trained Universal sentence encoder from the TensorFlow library is used for generating high dimensional vectors which are then passed as input embeddings to an LSTM network. Initially, there are two Bi-LSTM layers with 256 and 128 units each, followed by a max-pooling layer. This is then linked to two series of dense, dropout, and batch normalization layers. Wherein the dense layers are of size 256 and 64 units with 'Relu' activation and the dropout rate is 0.4 . - USE + CNN: Similar to the previous architecture USE vectors are generated for the input data and this is connected as input embedding to the CNN network. This CNN network consists of 2 series of 512 units CNN layer with kernel size 1, max-pooling layer, and dropout layer of rate 0.5 . This is then connected to a global max-pooling layer followed by 2 series of dense layers and dropout layers. These dense layers are of 1024 and 128 units each with 'tanh' activation and a dropout rate of 0.5 is employed. - BERT + LSTM: In this architecture, we generate BERT embeddings for each dataset. The trained 'bert-base-uncased' model from the transformers library is used for generating embeddings. These embeddings are then connected to an LSTM network. This network firstly consists of 2 Bi-LSTM layers of 256 and 128 units respectively followed by a max-pooling layer. This is then connected to another dense layer with 'Relu' activation of 64 units. - BERT + CNN: Similar to the previous architecture BERT embeddings are generated and passed as input to a CNN network. This CNN network consists of 2 CNN layers of 512 and 256 units each and a kernel size of 3 , followed by a max-pooling layer and a dense layer of 64 units with 'Relu' activation. Table 1. Models and their accuracies on datasets\n\n| Model | 20NG | BBC News | AG News | BBC Sports | IMDB | R8 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| USE | 81.76 | 96.63 | 92.09 | 98.65 | 87.14 | 95.61 |\n| BERT | $\\mathbf{8 5 . 7 8}$ | 98.2 | $\\mathbf{9 4 . 0 4}$ | 98.65 | 89.58 | $\\mathbf{9 7 . 6 2}$ |\n| HAN | 85.01 | 97.75 | 92.11 | 96.24 | 88.94 | 94.47 |\n| USE+LSTM | 81.81 | 98.2 | 92.25 | 98.65 | 88.89 | 95.75 |\n| USE+CNN | 80.03 | 97.53 | 92.21 | 99.32 | 89.7 | 96.44 |\n| BERT+LSTM | 85.57 | $\\mathbf{9 8 . 4 3}$ | 94.01 | 99.32 | $\\mathbf{9 3 . 6 3}$ | 95.89 |\n| BERT+CNN | 83.79 | 98.2 | 92.4 | $\\mathbf{1 0 0}$ | $\\mathbf{9 3 . 6 3}$ | 96.35 |\n| BigBird | 85.14 | 97.97 | 92.3 | 99.32 | $\\mathbf{9 4 . 3 2}$ | $\\mathbf{9 8 . 0 3}$ |\n| Longformer | $\\mathbf{8 6 . 4 5}$ | $\\mathbf{9 8 . 6 5}$ | 93.4 | $\\mathbf{1 0 0}$ | 93.3 | 97.85 |\n\n## 6 Results\n\nIn this work, we have obtained results on 6 different benchmark datasets. The primary algorithms used are USE, BERT, and HAN. The USE and HAN are nontransformer based algorithms and are expected to perform poorer than BERTbased models. The primary focus of this work is hierarchical approaches based on USE and BERT. The results of these algorithms are shown in Table 1. These results were explicitly re-computed on similar hyper-parameters and hardware environments to ensure a fair comparison. We compare USE + LSTM and USE + CNN with USE and HAN. We see that USE + CNN/LSTM performs better than HAN on all the datasets except for 20NG. Similarly, they either perform better\nthan standalone USE or are on par with it. This shows that the hierarchical setup allows the model to capture better representations for classification. The BERT model performs better than the USE and its variations on most of the datasets. However, BERT + CNN/LSTM performs on par with standalone BERT, unlike USE where there is a clear improvement in performance. Although the hierarchical setup does not provide an improvement over BERT it is still desirable because of its higher efficiency. Since we run the base BERT on individual chunks the self-attention is now restricted to words in the individual chunk. The complexity of self-attention is no longer quadratic in nature and this allows us to process longer-length sentences. Specifically, the BERT + LSTM model performs better in most cases. Overall we observe that the hierarchical setup is beneficial with both transformer and non-transformer approaches. The results for Longformer and BigBird are also added for comparison. These transformer architectures use specialized attention mechanisms to improve selfattention efficiency. Although Longformer does provide the best results on most of the datasets its attention mechanism hurts the parallelization of Transformers and requires specialized handling. Hence the vanilla BERT-based architectures are still relevant in this context. ## 7 Conclusion\n\nIn this paper, we present four novel models of BERT + LSTM, USE + LSTM, USE + CNN, and BERT + CNN that were evaluated on 6 benchmark datasets. We observed that the BERT + LSTM model performs better than other extensions. The accuracies obtained from this are not only comparable to the baseline models for a few datasets like 20 newsgroups and AG news but it also outperforms the accuracies for BBC news and IMDB dataset. Similarly the USE + LSTM extension also either has improved results than baseline USE or has comparable results. Our results confirm that all four models can be efficiently used for the task of long document classification. We also provide a comparative analysis of deep learning algorithms like USE, BERT, HAN, Longformer, and BigBird for long document classification on these benchmark datasets. ## Acknowledgements\n\nThis research was conducted under the guidance of L3Cube, Pune. We would like to express our gratitude towards our mentors at L3Cube for their continuous support and encouragement. ## References\n\n[1] Ashutosh Adhikari et al. \"Docbert: Bert for document classification\". In: arXiv preprint arXiv:1904.08398 (2019). [2] AG News Dataset. http://groups.di.unipi.it/ gulli/AG_corpus_of_news_ articles.html. [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate\". In: arXiv preprint arXiv:1409.0473 (2014). [4] Jeremy Barnes et al. \"Hierarchical models vs. transfer learning for documentlevel sentiment classification\". In: arXiv preprint arXiv:2002.08131 (2020). [5] Iz Beltagy, Matthew E Peters, and Arman Cohan. \"Longformer: The longdocument transformer\".",
    "hierarchinn-3": "In: arXiv preprint arXiv:2004.05150 (2020). [6] Pranali Bora et al. \"ICodeNet-A Hierarchical Neural Network Approach For Source Code Author Identification\". In: 2021 13th International Conference on Machine Learning and Computing.",
    "hierarchinn-4": "2021, pp. 180-185. [7] Daniel Cer et al. \"Universal sentence encoder\". In: arXiv preprint arXiv:1803.11175 (2018). [8] Ming-Wei Chang et al. \"Language model pre-training for hierarchical document representations\". In: arXiv preprint arXiv:1901.09128 (2019). [9] Yahui Chen. \"Convolutional neural network for sentence classification\". MA thesis. University of Waterloo, 2015. [10] Alexis Conneau et al. \"Very deep convolutional networks for text classification\". In: arXiv preprint arXiv:1606.01781 (2016). [11] Zihang Dai et al. \"Transformer-xl: Attentive language models beyond a fixed-length context\". In: arXiv preprint arXiv:1901.02860 (2019). [12] Jacob Devlin et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding\". In: arXiv preprint arXiv:1810.04805 (2018). [13] Shang Gao, Arvind Ramanathan, and Georgia Tourassi. \"Hierarchical convolutional attention networks for text classification\". In: Proceedings of The Third Workshop on Representation Learning for NLP. 2018, pp. 1123 . [14] Derek Greene and P\u00e1draig Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\".",
    "hierarchinn-5": "In: Proc. 23rd International Conference on Machine learning (ICML'06). ACM Press, 2006, pp.",
    "hierarchinn-6": "377-384. [15] Julia Ive et al. \"Hierarchical neural model with attention mechanisms for the classification of social media text related to mental health\". In: Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic.",
    "hierarchinn-7": "2018, pp. 69-77. [16] Mohit Iyyer et al. \"Deep unordered composition rivals syntactic methods for text classification\". In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers).",
    "hierarchinn-8": "2015, pp. $1681-1691$. [17] Ramchandra Joshi et al. \"Evaluation of Deep Learning Models for Hostility Detection in Hindi Text\". In: 2021 6th International Conference for Convergence in Technology (I2CT). IEEE. 2021, pp. 1-5. [18] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. \"Reformer: The efficient transformer\".",
    "hierarchinn-9": "In: arXiv preprint arXiv:2001.04451 (2020). [19] Siwei Lai et al. \"Recurrent convolutional neural networks for text classification\". In: Twenty-ninth AAAI conference on artificial intelligence. 2015. [20] Andrew L. Maas et al. \"Learning Word Vectors for Sentiment Analysis\". In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: Association for Computational Linguistics, June 2011, pp. 142-150. URL: http://www.aclweb.org/anthology/P11-1015. [21] Shervin Minaee et al. \"Deep Learning-based Text Classification: A Comprehensive Review\". In: ACM Computing Surveys (CSUR) 54.3 (2021), pp.",
    "hierarchinn-10": "$1-40$. [22] Raghavendra Pappagari et al. \"Hierarchical transformers for long document classification\". In: 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).",
    "hierarchinn-11": "IEEE. 2019, pp. 838-844. [23] F. Pedregosa et al. \"Scikit-learn: Machine Learning in Python\". In: Journal of Machine Learning Research 12 (2011), pp. 2825-2830. [24] R8 Dataset. https://www.kaggle.com/weipengfei/ohr8r52/version/1. [25] Richard Socher et al. \"Recursive deep models for semantic compositionality over a sentiment treebank\".",
    "hierarchinn-12": "In: Proceedings of the 2013 conference on empirical methods in natural language processing. 2013, pp. 1631-1642. [26] Kai Sheng Tai, Richard Socher, and Christopher D Manning. \"Improved semantic representations from tree-structured long short-term memory networks\". In: arXiv preprint arXiv:1503.00075 (2015). [27] Duyu Tang, Bing Qin, and Ting Liu. \"Document modeling with gated recurrent neural network for sentiment classification\".",
    "hierarchinn-13": "In: Proceedings of the 2015 conference on empirical methods in natural language processing. 2015, pp. $1422-1432$. [28] Ashish Vaswani et al. \"Attention is all you need\". In: Advances in neural information processing systems. 2017, pp. 5998-6008. [29] Vedangi Wagh et al. \"Comparative Study of Long Document Classification\". In: arXiv preprint arXiv:2111.00702 (2021). [30] Apurva Wani et al. \"Evaluating deep learning approaches for covid19 fake news detection\". In: International Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situation.",
    "hierarchinn-14": "Springer. 2021, pp. 153-163. [31] Yijun Xiao and Kyunghyun Cho. \"Efficient character-level document classification by combining convolution and recurrent layers\". In: arXiv preprint arXiv:1602.00367 (2016). [32] Zichao Yang et al. \"Hierarchical attention networks for document classification\". In: Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies.",
    "hierarchinn-15": "2016, pp. 1480-1489. [33] Manzil Zaheer et al. \"Big Bird: Transformers for Longer Sequences.\" In: NeurIPS.",
    "hierarchinn-16": "2020. [34] Xiang Zhang, Junbo Zhao, and Yann LeCun. \"Character-level convolutional networks for text classification\".",
    "hierarchinn-17": "In: Advances in neural information processing systems 28 (2015), pp. 649-657. [35] Jianming Zheng et al. \"A hierarchical neural-network-based document representation approach for text classification\". In: Mathematical Problems in Engineering 2018 (2018). [36] Chunting Zhou et al. \"A C-LSTM neural network for text classification\". In: arXiv preprint arXiv:1511.08630 (2015). [^0]:    * Authors contributed equally\n\n"
}