{
    "transformerfam-0": "TransformerFAM: Feedback attention is working memory\n\nDongseong Hwang Weiran Wang Zhuoyuan Huo Khe Chai Sim Pedro Moreno Mengibar\n\nAbstract\n\nWhile Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length. 1 Introduction\n\nThe introduction of the Transformer architecture (Vaswani et al., 2017) has revolutionized deep learning by permeating diverse domains and enhancing performance due to its efficacy and scalability. This scalability fuels a trend analogous to Moore\u2019s law, which links increased model size to performance gains (Kaplan et al., 2020). The effectiveness of attention in text sequence processing was solidified through the Transformer paper. Models like BERT (Devlin et al., 2018) and GPT-3 (Brown et al., 2020) further showcased the scalability of Transformer and its tendency for improved performance with increased model size. Following the replacement of LSTM (Hochreiter & Schmidhuber, 1997b) by Transformer in most Natural Language Processing (NLP) domains, the Vision Transformer (ViT) (Dosovitskiy et al., 2020) replaced Convolutional Neural Network (CNN) (LeCun et al., 1995) with Transformers in the vision domain, and Conformer (Convolution-augmented Transformer) (Gulati et al., 2020) replaced LSTM in the speech domain. The Transformer has become the de facto architecture in various domains. Currently, attention serves as the leading architecture for extracting meaningful representations from homogeneous data. The logical progression points toward extending attention capabilities to heterogeneous data. This has enabled advances in multimodal fusion (text and vision), as seen in models like DALL\u00b7E 2 (Ramesh et al., 2022), Flamingo (Alayrac et al., 2022) and CoCa (Yu et al., 2022). AudioLM (Borsos et al., 2023) has shown that attention also excels at fusing audio and text. Consequently, Gemini (Team et al., 2023) integrates text, images, audio, and video into a single generative model. This was possible because attention to heterogeneous data works exceptionally well. Despite the impressive success of attention, it suffers from major drawbacks. Firstly, attention has quadratic complexity with respect to context length, which limits the capability of modeling long contexts. Secondly, it forgets information from context before attention window, unlike LSTM, which theoretically can propagate information indefinitely. We want the better architecture to be able to process arbitrarily long sequences efficiently, while preserving very long-term dependencies. Sliding window attention is introduced (Dai et al., 2019; Beltagy et al., 2020) to handle infinitely long sequences as input. However, it disregards information beyond the effective receptive field (approximately model depth window size in Figure 1(d)) during sequence generation. Various approaches have attempted to address the long-context problem by sparse attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Oren et al., 2024) and linear approximated attention (Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2020; Xiong et al., 2021), showing effectiveness below the 1B scale. Yet, scaling laws (Tay et al., 2022) suggest these approximations do not perform as well at GPT-3 level. Current SoTA large language model (LLM) leaderboards (LMSYS, 2023) do not feature architectures primarily relying on approximated attention. On the other hand, Neuroscience links attention to multisensory integration (Tang et al., 2016). Endogenous (goal-driven) and exogenous (stimulus-driven) attention are distributed throughout sensory processing regions in brain, and the brain areas correlated with the attention overlap substantially with the multisensory cortical regions involved in multisensory integration. Working memory exhibits similar distribution (Christophel et al., 2017), overlapping with attention-correlated areas. This indicates that attention is an important factor in not only multimodal fusion but also working memory. In the human brain, working memory (Wallace, 1960) provides a temporary memory for performing tasks. While working memory is stored in sustained activations, long-term memory is stored in weights (Fuster, 1973). LLMs has enormous long-term memory enough to store the entire internet (Villalobos et al., 2022), but they do not have working memory. The activation of working memory is sustained by prefrontal cortical-thalamic loops (Ashby et al., 2005), which means working memory is sustained by the continuous spiking of activation within a feedback loop. Attention has been shown to be effective for processing both homogeneous and heterogeneous data. The next natural step is to apply attention to its own latent representations through a feedback loop. We hypothesize that this next step will naturally lead to the emergence of working memory in Transformers. Assumption 1.1. The attention mechanism within the feedback loop functions as working memory. Feedback connections are prevalent in biological neural networks. Even organisms with simple neural structures, such as C. elegans (with only 302 neurons) (White et al., 1986), exhibit various feedback loops, like connections from higher-level interneurons to lower-level ones (Hasani et al., 2018). However, incorporating feedback loops in Transformer is challenging. There are two main approaches. The first approach is linking the topmost layer to the bottommost (Fan et al., 2020; Bulatov et al., 2022). However, this cannot model feedback between interneurons, and this has only one global working memory. The second approach is within-transformer-block feedback: the output activation of a Transformer layer is fed back as input to the same layer. This is the approach we propose, which enables each Transformer layer to have a distributed working memory that corresponds to its abstraction level. Recurrent Neural Networks (RNNs) have achieved great success in machine learning by introducing feedback loops (Hochreiter & Schmidhuber, 1997a; Cho et al., 2014). RNNs pass feedback between sequences through hidden states. Attention mechanisms can implement feedback loops by attending to both the input sequence and the feedback state simultaneously. We propose a novel Transformer architecture (TransformerFAM) that enables attention to both homogeneous sequence data and latent representations via a feedback loop. This architecture change fosters the natural emergence of working memory within Transformers. During inference, TransformerFAM has a computational complexity of and a memory complexity of , where is the length of the processed tokens. TransformerFAM can maintain past information for an indefinite horizon, making it a promising solution for LLMs to handle infinitely long input sequences. TransformerFAM does not introduce new weights to the Transformer, allowing the reuse of pre-trained checkpoints. Our experiments show that fine-tuning TransformerFAM with LoRA for just 50k steps significantly enhances performance on long-context tasks across 1B, 8B, and 24B Flan-PaLM LLMs (Chung et al., 2022). 2 TransformerFAM\n\n2.1 Block Sliding Window Attention (BSWA)\n\nIn general, when handling long context inputs, there are the two main approaches. A first approach is to increase the context length with increasing the computational resources (memory and processing power). A second approach is the implementation of Sliding Window Attention (SWA) (Beltagy et al., 2020; Jiang et al., 2023), as illustrated in Figure 1(a). During inference, the standard implementation allocates key and value cache twice the window length at the beginning, and then using a ring buffer to update the necessary components at each step, in order to avoid memory allocation and copying operations every step, which are computationally expensive. Longformer (Beltagy et al., 2020) introduced Sliding Window Attention, which caches on a block-by-block basis. We will refer to this as Block Sliding Window Attention (BSWA). BSWA does not mask out past keys and values in the ring buffer, but attends to all the information in the ring buffer. BSWA has two hyperparameters: block size and memory segment, as illustrated in Figure 1(b) and 1(c). Block size determines how many tokens are in each block and is also used as the stride for sliding.",
    "transformerfam-1": "Memory segment determines how many past blocks to cache. In our experiments, we set the default hyperparameters as follows: block size of and memory segment of (corresponding to a window size ranging from 3073 to 4096). Given a sequence input to a vanilla Transformer layer, Transformer transforms the input to a sequence output as follows (Vaswani et al., 2017):\n\nQ , K , V \ud835\udc44 \ud835\udc3e \ud835\udc49 \\displaystyle Q,K,V = QKV \u200b ( PreLN \u200b ( I ) ) absent QKV PreLN \ud835\udc3c \\displaystyle=\\text{QKV}(\\text{PreLN}(I)) a t subscript \ud835\udc4e \ud835\udc61 \\displaystyle a_{t} = SelfAttention \u200b ( q t , K , V ) + i t absent SelfAttention subscript \ud835\udc5e \ud835\udc61 \ud835\udc3e \ud835\udc49 subscript \ud835\udc56 \ud835\udc61 \\displaystyle=\\text{SelfAttention}(q_{t},K,V)+i_{t} O \ud835\udc42 \\displaystyle O = FF \u200b ( PreLN \u200b ( A ) ) + A absent FF PreLN \ud835\udc34 \ud835\udc34 \\displaystyle=\\text{FF}(\\text{PreLN}(A))+A\n\nWhile PreLN (Xiong et al., 2020) is used in the equation, it is not mandatory. Autoregressive Transformer changes the SelfAttention equation as follows:\n\na t subscript \ud835\udc4e \ud835\udc61 \\displaystyle a_{t} = SelfAttention \u200b ( q t , K : t , V : t ) + i t absent SelfAttention subscript \ud835\udc5e \ud835\udc61 subscript \ud835\udc3e : absent \ud835\udc61 subscript \ud835\udc49 : absent \ud835\udc61 subscript \ud835\udc56 \ud835\udc61 \\displaystyle=\\text{SelfAttention}(q_{t},K_{:t},V_{:t})+i_{t}\n\nIn a Transformer with SWA, the equation is modified to account for a window size of :\n\na t subscript \ud835\udc4e \ud835\udc61 \\displaystyle a_{t} = SelfAttention \u200b ( q t , K t \u2212 w : t , V t \u2212 w : t ) + i t absent SelfAttention subscript \ud835\udc5e \ud835\udc61 subscript \ud835\udc3e : \ud835\udc61 \ud835\udc64 \ud835\udc61 subscript \ud835\udc49 : \ud835\udc61 \ud835\udc64 \ud835\udc61 subscript \ud835\udc56 \ud835\udc61 \\displaystyle=\\text{SelfAttention}(q_{t},K_{t-w:t},V_{t-w:t})+i_{t}\n\nIn a Transformer with BSWA, denotes the block index.",
    "transformerfam-2": "Each block contains a set of keys and values, determined by the block size. The equation is modified to account for a block index and a memory segment in Equation 1. is the keys in the memory segments from the blocks before to the block before , and is the keys from the beginning of block up to . We will refer to Transformer with BSWA as TransformerBSWA. K ^ t subscript ^ \ud835\udc3e \ud835\udc61 \\displaystyle\\hat{K}_{t} = Concat \u200b ( K \u03c4 \u2212 m : \u03c4 \u2212 1 , K \u03c4 , : t ) absent Concat subscript \ud835\udc3e : \ud835\udf0f \ud835\udc5a \ud835\udf0f 1 subscript \ud835\udc3e \ud835\udf0f : absent \ud835\udc61 \\displaystyle=\\text{Concat}(K_{\\tau-m:\\tau-1},K_{\\tau,:t}) (1a) V ^ t subscript ^ \ud835\udc49 \ud835\udc61 \\displaystyle\\hat{V}_{t} = Concat \u200b ( V \u03c4 \u2212 m : \u03c4 \u2212 1 , V \u03c4 , : t ) absent Concat subscript \ud835\udc49 : \ud835\udf0f \ud835\udc5a \ud835\udf0f 1 subscript \ud835\udc49 \ud835\udf0f : absent \ud835\udc61 \\displaystyle=\\text{Concat}(V_{\\tau-m:\\tau-1},V_{\\tau,:t}) (1b) a t subscript \ud835\udc4e \ud835\udc61 \\displaystyle a_{t} = SelfAttention \u200b ( q t , K ^ t , V ^ t ) + i t absent SelfAttention subscript \ud835\udc5e \ud835\udc61 subscript ^ \ud835\udc3e \ud835\udc61 subscript ^ \ud835\udc49 \ud835\udc61 subscript \ud835\udc56 \ud835\udc61 \\displaystyle=\\text{SelfAttention}(q_{t},\\hat{K}_{t},\\hat{V}_{t})+i_{t} (1c)\n\nAlgorithm 1 presents TransformerBSWA (Equation 1), re-expressed from the perspective of a block index .",
    "transformerfam-3": "Algorithm 1 describes how to iteratively calculate and then concatenate the results into an sequence. Typically, standard implementations employ a causal attention mask to enable parallel computation of self-attention. TransformerXL (Dai et al., 2019) proposed to use a technique called \u201dstop gradient\u201d for the memory segment. However, we argue that this technique has a negative impact on ability of the model to attend to past information. Specifically, we show that using stop gradient results in a much shorter receptive field than the theoretical receptive field in Section 3.2. We believe that allowing gradient to flow to the memory segment is necessary for the model to learn to carry important information in the memory segment. While this modification might seem to burden training memory and computation, it does not significantly impact performance in practice. This is primarily due to the prevalence of gradient checkpointing (Chen et al., 2016) in LLM training on ML accelerators, as memory often presents the primary bottleneck. Gradient checkpointing recomputes attention during backpropagation, from later blocks to earlier blocks. Therefore, the presence or absence of stop gradients has little impact on the overall computational complexity, while still improving performance. In addition, when training an LLM with sliding window attention on long context inputs (more than k tokens), computing the attention over the entire input at once would require too much memory. As a result, the standard practice is to divide the attention into blocks and calculate it using a vectorized map (e.g., jax.vmap, torch.vmap). This reduces the peak memory usage to the amount required to calculate one block. Blocks are independent of each other, so they can be calculated in any order. Understanding this point is essential when evaluating the memory requirements and training efficiency of Feedback Attention Memory (FAM), as discussed in Section 2.2. Transformer has quadratic memory and computation complexity with respect to the length of the input sequence due to self-attention. It has complexity for input length . However, Transformer with Sliding Window Attention has linear complexity with respect to the input sequence. It has complexity for input length and window size . If the input length is about the same as the window size (k), the complexity difference is almost negligible, but if the input length is large like k in GPT-4 turbo (Achiam et al., 2023), there is a huge difference. In inference, Transformer with SWA or BSWA only needs to cache a fixed ring buffer (block size memory segment). Therefore, it only consumes constant memory regardless of the generated token length. Therefore, LLMs using SWA or BSWA can generate infinitely long output tokens. However, BSWA has a limited receptive field, approximately equal to model depth window size as illustrated in Figure 1(d). As a result, the later generated tokens are not related to tokens outside the receptive field (e.g., prompt). To address this limitation, we propose a novel architecture in the following Section 2.2. Our approach, Feedback Attention Memory (FAM), builds upon BSWA. This is because the block stride concept of BSWA is well-suited for blockwise feedback updates. 2.2 Feedback Attention Memory\n\nAs mentioned in Section 1, we hypothesized that attending to the feedback loop can give rise to working memory in 1.1. To implement the feedback loop, we add feedback activations that feed contextual representation back into each block of BSWA. We call these virtual activations as Feedback Attention Memory (FAM). FAM is designed to meet the following key requirements:\n\n\u2022\n\nIntegrated Attention: Self-attention should simultaneously process input context and FAM. \u2022\n\nBlock-Wise Updates: FAM should be updated when transitioning between blocks. \u2022\n\nInformation Compression: FAM updates should compress current block information, conditioned on previous FAM. \u2022\n\nGlobal Contextual Storage: FAM should store comprehensive contextual information indefinitely. The proposed architecture achieves this by appending FAM to block segments and incorporating it into self-attention processes. This enables richer representations and dynamic propagation of global contextual information across blocks, as illustrated in Figure 2(b). When self-attention occurs on the current block, the input query for the block attends to the input key for that block, the memory segment, and the previous FAM. The previous FAM provides global contextual information, allowing for a much richer representation than BSWA. In parallel, the FAM query attends to the current block and the FAM key. The FAM query compresses the current block, conditioned on the previous global contextual information. The FAM query is dynamically generated based on previous global contextual information, as it is copied from the previous FAM. Then, the newly updated FAM serves to propagate global contextual information to the next block recursively. This process is formally described in Algorithm 2. While Algorithm 2 might initially suggest a doubling of matrix operations compared to Algorithm 1, it performs the same number of matrix operations in the actual implementation, because it starts with the concatenation of block input and FAM . The attention mask within self-attention requires a minor modification to accurately represent FAM. The FAM is much shorter than the input , and in Section 3, we experimented with a block size of and a FAM length of . Transformers are much better at exploiting the parallelism of ML accelerators than Recurrent Neural Networks (RNNs). This is because RNNs have a causal relationship between input sequences, while Transformers only have a causal relationship between the inputs and the layer one depth below. It is possible to worry that the feedback mechanism of TransformerFAM will eliminate the advantages of Transformers and make training inefficient. As explained in the implementation of BSWA, memory-efficient implementations perform self-attention in blocks using vectorized maps. Otherwise, peak memory increases during LLM training, requiring more ML accelerators. The causal relationship of TransformerFAM only exists between blocks. Since vectorized maps are used to perform self-attention in blocks, the causal relationship between blocks does not affect training speed and memory. In addition, processing additional FAM when processing block input sequences has only a minor impact on performance. Therefore, the memory consumption and training speed of TransformerFAM are almost the same as those of TransformerBSWA. TransformerFAM requires additional considerations for FAM initialization and length extrapolation. These details are explained in Appendix B. An evaluation of multiple FAM variants was conducted, and the best-performing variant is presented in the main paper. Appendix C provides further details for the remaining variants. 3 Experiments\n\n3.1 Training\n\nPretraining an LLM from scratch requires a huge amount of resources. TransformerFAM can reuse existing LLM checkpoints because it does not add new weights to the Transformer layer. We reused 1B, 8B, and 24B Flan-PaLM LLMs (Chung et al., 2022) for our experiments. This is a large enough size to prove that TransformerFAM is a general solution for LLMs. The model sizes 1B, 8B, and 24B refer to the size of the plain Transformer, excluding the text embedding table. The models use a 256k sentence piece tokenizer (Kudo & Richardson, 2018), resulting in 400M text embedding table weights for the 1B model and 1B weights for the 8B and 24B models. The detailed model architecture is described in Table 5 in Section A.1. Flan-PaLM is a model that is fine-tuned on top of a pretrained PaLM model (Chowdhery et al., 2023) using instruction finetuning. The instruction data consists of few-shot instructions with to k tokens, which are packed into k tokens for training. This means that individual instruction data are concatenated until they reach k tokens. We applied both the TransformerBSWA and TransformerFAM architectures to Flan-PaLM and fine-tuned it for an additional 50k steps. We experimented with different memory segment for both architectures. The block size is set to and the FAM length is set to . During fine-tuning, we used the same Flan instruction data packed into k tokens. To maintain a minibatch size of 128 for all models, we used 32 TPUv5 (Jouppi et al., 2023) cores for the 1B model, 64 cores for the 8B model, and 128 cores for the 24B model. If we had used more resources and a larger minibatch size, we might have achieved better results than those reported in the paper. We performed LoRA finetuning by adding LoRA (Hu et al., 2021) to the Attention and FF layers of Transformer without training all the parameters. Full finetuning resulted in lower scores on various tasks reported by GPT-3 (Brown et al., 2020), because catastrophic forgetting (Kirkpatrick et al., 2017) occurred in domains that were not covered by the instruction data. In LoRA finetuning, the scores on GPT-3 tasks actually improved, and the performance on long context tasks was similar to that of full finetuning. The rank of LoRA was 64, and the weights of the original Attention and FF layers were merged with LoRA weights and used during inference. The Adafactor optimizer ( ) (Shazeer & Stern, 2018) was used with constant learning rate. The learning rates used were for 1B, and for both 8B and 24B. In addition, TransformerXL exhibits comparable performance to TransformerBSWA. The implementations are almost identical, with TransformerXL employing an additional QK attention mask to mask out keys beyond a predetermined window size. Section B.4.5 demonstrates that the performance difference between the two architectures is insignificant, and therefore, experimental results for TransformerBSWA are only included in the main paper. 3.1.1 Data\n\nThe ideal data for training TransformerFAM is a very long document with continuous context, such as textbooks and novels, and the data should be large enough to finetune an LLM. Additionally, the same very long document should be used continuously in each minibatch component, while maintaining FAM and memory segments between training steps. The loss function of an LLM is to minimize the difference between the parametric probabilistic model and the data-generating true distribution . The Kullback\u2013Leibler divergence (KL divergence) is used to measure this difference. To perform an unbiased estimation of KL divergence, we draw samples from the data-generating true distribution, which are assumed to be independent and identically distributed (IID). However, the ideal training scenario for the aforementioned memory training directly contradicts the IID assumption. We refer to this as the curse of IID. Due to the curse of IID, we could not find the training infrastructure or data suitable for training memory. So we used Flan instruction data as a last resort. We used Flan instruction data as training data, packed up to 8.5k tokens. In the Flan paper (Chung et al., 2022), a special attention mask was used to prevent attention between different segments during self-attention, by applying a separate mask to each packed segment. We did not use this special attention mask processing. Attention occurs causally within the window, regardless of the segment. We expected TransformerBSWA and TransformerFAM to learn to remember important information and forget outdated information by themselves. Each token in Flan data has a weight. Few shots examples and instructions have a weight of 0, and answers have a weight of 1. This means that the model only learns to generate the answer. To incentivize the model to remember long contexts, we randomly selected 256 consecutive tokens from 8.5k tokens and appended them to the end of the data with the prompt \u2019[repeat random segment]:\u2019. The repeated tokens were given a weight of . We hope that future studies can use more suitable data for training memory, such as long continuous documents, long-form speech, video or video game. 3.2 PassKey Retrieval\n\nThe PassKey retrieval task is a recent benchmark used in several long-context transformer papers (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023). In this task, a passkey is presented at the beginning, followed by a very long filler context. Finally, a question about the passkey is asked, as shown in Figure 11 in Section D.1. This task is a good smoke test to quickly check if information is transmitted in a long context. However, this task only checks if small and important information is transmitted, and does not check if large amounts of information can be efficiently compressed. We fine-tuned the Flan-PaLM 1B model for 5k steps with the PassKey format, which has a filler context of 2k to 18k randomly. We used a block size of 1024, TransformerBSWA with 0 to 12 memory segments, TransformerFam with 0 memory segments, and a FAM length of . When the number of memory segments is 3, the window size is 3k (i.e. memory segment block size). As shown in Figure 3, TransformerFAM was able to perfectly solve the task with a filler context of up to 260k tokens. In the figure, MX denotes the number of BSWA memory segments. The performance of TransformerBSWA improves significantly up to M2, after which it saturates. The performance of M12 also drops significantly after 20k tokens. The theoretical receptive field of M2 is 36k (i.e. depth(18) memory segment(2k)), but the effective receptive field is much shorter. In Figure 3, it is important to compare M1_SG and M1. M1_SG has a stop gradient applied to one memory segment, which limits the receptive field to the window size. This is because the model cannot learn which contextual information stored in the memory segment will have a good result later. SWA with back propagation through time (BPTT) functions similarly to a time-limited RNN. It is common to use stop gradients on the memory segments of TransformerXL (Dai et al., 2019; Chevalier et al., 2023).",
    "transformerfam-4": "However, we recommend against this practice. In Section B.4.6, we compare our work with recent Transformer with memory papers (Bulatov et al., 2022; Chevalier et al., 2023). 3.3 Long Context Tasks\n\nGemini (Team et al., 2023) evaluated long-context capabilities using the following tasks: NarrativeQA (Ko\u010disk\u1ef3 et al., 2018), Scrolls-Qasper, Scrolls-Quality (Shaham et al., 2022), and XLSum (Hasan et al., 2021). Additionally, PG-19 (Rae et al., 2019) and Isabelle (Wu et al., 2022) are another common evaluation tasks among long-context Transformer papers (Rae et al., 2019; Wu et al., 2022; Chen et al., 2023). Detailed information on the evaluation data is provided in Table 10 in Section D.2. We evaluated the long-context capabilities of the 1B TransformerBSWA model trained in Section 3.1 using memory segment sizes ranging from 0 to 8. As shown in Figure 4, TransformerFAM outperformed TransformerBSWA on all the long context tasks (LCT), regardless of the number of memory segments in BSWA. It shows a significant performance improvement on ScrollsQasper and NarrativeQA, where it has to understand 5k to 500k tokens of context before answering a question. The LCT results demonstrate that TransformerFAM can effectively compress and retain important contextual information within extremely long contexts. Above M1, the number of memory segments does not significantly impact LCT performance on TransformerBSWA, because the input sequences are much longer than the window size of all experiments. We observed the same phenomenon in TransformerFAM, and TransformerFAM uses memory segments in Figure 4. The figure shows the normalized scores of all tasks to view the scores on the same scale. The raw results are in Table 11 in Section D.2. We further evaluated TransformerFAM and TransformerBSWA on 8B and 24B models. As shown in Table 1, TransformerFAM demonstrates scalability as the model size increases. This suggests that self-attention can route local information relevant to each input sequence while simultaneously routing contextual information to the FAM. In addition, TransformerFAM marginally surpasses TransformerBSWA on GPT-3 tasks (Brown et al., 2020) (see Table 2). This is not expected because all tasks are shorter than 2k tokens. We hypothesize this improvement stems from the efficient contextual representation by TransformerFAM. By offloading contextual data to FAM, TransformerFAM reduces duplication within input activations, optimizing latent space usage. Thus, BSWA memory segments (local representation) and FAM (global representation) complement each other. For LLMs, we recommend using FAM for compressed contextual representation alongside BSWA memory segments up to inference budgets (e.g., 2k, 8k, 32k (Team et al., 2023), or 128k (Achiam et al., 2023)). 3.4 Ablation study\n\n3.4.1 FAM length\n\nIn the Flan 1B model, we observed performance saturation on ScrollsQasper, ScrollsQuality, and XLSum tasks when the FAM length reached 64. Interestingly, performance declined when FAM length exceeded 64. This suggests that information compression is more effective when space is limited. 3.4.2 The number of previous FAM blocks\n\nIn Figure 2(b), the input query attends to the FAM as denoted Attention to Feedback. The input query can attend to not only the immediately previous FAM, but also to more previous FAMs. Table 4 shows the XLSum scores for different numbers of previous FAM blocks. As the table shows, increasing the number of blocks did not have a significant effect, because the previous FAM already encapsulates all the previous information by a feedback copy. Therefore, the default setup attends to only the immediately previous FAM. In Section B.4, additional ablation studies are presented, due to the page limitation. 4 Related Work\n\nThere have been attempts to incorporate feedback mechanisms into the Transformer, but most of them involve feeding the output activations from the top layer to the bottom (Bulatov et al., 2022; Chevalier et al., 2023) or to intermediate layers (Fan et al., 2020). Since the top three layers in the Transformer are heavily focused on output reconstruction (Pasad et al., 2021), we hypothesize that there is a significant representational gap between the top and other layers. In this paper, we propose a feedback mechanism between intermediate layers. There were papers that compressed information blockwise (Rae et al., 2019; Guo et al., 2019; Gupta & Berant, 2020; Mohtashami & Jaggi, 2023; Mu et al., 2023). However, in those papers, the information was not propagated infinitely. Relevant prior work includes the use of recurrent cross-attention between blocks (Hutchins et al., 2022), enabling the propagation of compressed information to subsequent blocks. Additionally, incorporating feedback from a few upper layers has been used to integrate past information (Ju et al., 2022). We propose TransformerFAM under the assumption that the human brain processes homogenous, heterogeneous, and feedback data with the same attention mechanism across distributed brain areas. In Appendix E, additional related works are presented, due to the page limitation. 5 Conclusion\n\nIn the film \u2019Memento\u2019 (2000), the protagonist struggles with anterograde amnesia, which means he can not remember anything before happened in the last 10 minutes, but his long-term memory is intact, He has to tattoo important information on his body to remember it. This is similar to the current state of large language models (LLMs). LLMs memorize the entire internet thanks to scaling laws (Kaplan et al., 2020), which allow them to store an enormous amount of information in large weights (long-term memory). However, their short-term memory is limited by the attention window. As a result, the complex prompt engineering becomes necessary to help them recall important details. We propose a new architecture called TransformerFAM that could fix anterograde amnesia of LLMs. The rapid progress of machine learning is astonishing, but there are two key problems that we still do not know how to approach: reasoning and memory. In this paper, we provide a clue to the memory problem. Memory is a critical prerequisite for reasoning. It is hard to imagine how we can derive complex mathematical equations without working memory. Reasoning must be a phenomenon that occurs based on the current working memory. This paper explores the integration of attention-based working memory, a concept from neuroscience, into the field of deep learning. Our goal is to ignite further research within the community to address and solve the ongoing challenge of limited memory in deep learning. There is a significant set of problems to tackle here, ranging from refining feedback attention architecture to investigating the transfer of working memory to long-term memory. References\n\nAchiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning.",
    "transformerfam-5": "Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022. Ashby et al. (2005) Ashby, F. G., Ell, S. W., Valentin, V. V., and Casale, M. B. Frost: A distributed neurocomputational model of working memory maintenance. Journal of cognitive neuroscience, 17(11):1728\u20131743, 2005. Baars (2005) Baars, B. J. Global workspace theory of consciousness: toward a cognitive neuroscience of human experience.",
    "transformerfam-6": "Progress in brain research, 150:45\u201353, 2005. Baevski et al. (2020) Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations.",
    "transformerfam-7": "Advances in neural information processing systems, 33:12449\u201312460, 2020. Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Borsos et al. (2023) Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., et al. Audiolm: a language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Bulatov et al. (2022) Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079\u201311091, 2022. Chen et al. (2023) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation.",
    "transformerfam-8": "arXiv preprint arXiv:2306.15595, 2023. Chen et al. (2016) Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost.",
    "transformerfam-9": "arXiv preprint arXiv:1604.06174, 2016. Chevalier et al. (2023) Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788, 2023. Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "transformerfam-10": "arXiv preprint arXiv:1904.10509, 2019. Cho et al. (2014) Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation.",
    "transformerfam-11": "arXiv preprint arXiv:1406.1078, 2014. Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers.",
    "transformerfam-12": "arXiv preprint arXiv:2009.14794, 2020. Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023. Christophel et al. (2017) Christophel, T. B., Klink, P. C., Spitzer, B., Roelfsema, P. R., and Haynes, J.-D. The distributed nature of working memory. Trends in cognitive sciences, 21(2):111\u2013124, 2017. Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Dai et al. (2019) Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.",
    "transformerfam-13": "arXiv preprint arXiv:2010.11929, 2020. Fan et al. (2020) Fan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar, S. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. Fuster (1973) Fuster, J. M. Unit activity in prefrontal cortex during delayed-response performance: neuronal correlates of transient memory. Journal of neurophysiology, 36(1):61\u201378, 1973. Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Gu et al. (2021) Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces.",
    "transformerfam-14": "arXiv preprint arXiv:2111.00396, 2021. Gulati et al. (2020) Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., et al. Conformer: Convolution-augmented transformer for speech recognition.",
    "transformerfam-15": "arXiv preprint arXiv:2005.08100, 2020. Guo et al. (2019) Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., and Zhang, Z. Star-transformer. arXiv preprint arXiv:1902.09113, 2019. Gupta & Berant (2020) Gupta, A. and Berant, J. Gmat: Global memory augmentation for transformers.",
    "transformerfam-16": "arXiv preprint arXiv:2006.03274, 2020. Hasan et al. (2021) Hasan, T., Bhattacharjee, A., Islam, M.",
    "transformerfam-17": "S., Samin, K., Li, Y.-F., Kang, Y.-B., Rahman, M. S., and Shahriyar, R. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages.",
    "transformerfam-18": "arXiv preprint arXiv:2106.13822, 2021. Hasani et al. (2018) Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu, R. Can a compact neuronal circuit policy be re-purposed to learn simple robotic control? arXiv preprint arXiv:1809.04423, 2018. Hochreiter & Schmidhuber (1997a) Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997a. Hochreiter & Schmidhuber (1997b) Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735\u20131780, nov 1997b. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735. Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.",
    "transformerfam-19": "arXiv preprint arXiv:2106.09685, 2021. Hutchins et al. (2022) Hutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur, B. Block-recurrent transformers. Advances in Neural Information Processing Systems, 35:33248\u201333261, 2022. Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jouppi et al. (2023) Jouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil, N., Subramanian, S., Swing, A., Towles, B., et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pp.",
    "transformerfam-20": "1\u201314, 2023. Ju et al. (2022) Ju, D., Roller, S., Sukhbaatar, S., and Weston, J. E. Staircase attention for recurrent processing of sequences. Advances in Neural Information Processing Systems, 35:13203\u201313213, 2022. Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "transformerfam-21": "In International conference on machine learning, pp. 5156\u20135165. PMLR, 2020. Kirkpatrick et al. (2017) Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017. Kitaev et al. (2020) Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer.",
    "transformerfam-22": "arXiv preprint arXiv:2001.04451, 2020. Ko\u010disk\u1ef3 et al. (2018) Ko\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328, 2018. Kudo & Richardson (2018) Kudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
    "transformerfam-23": "arXiv preprint arXiv:1808.06226, 2018. LeCun et al. (1995) LeCun, Y., Bengio, Y., et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.",
    "transformerfam-24": "Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. LMSYS (2023) LMSYS. Lmsys chatbot arena leaderboard. https://chat.lmsys.org/?arena, 2023.",
    "transformerfam-25": "Mohtashami & Jaggi (2023) Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. Mu et al. (2023) Mu, J., Li, X. L., and Goodman, N. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467, 2023. Narayanan et al. (2019) Narayanan, A., Prabhavalkar, R., Chiu, C.-C., Rybach, D., Sainath, T. N., and Strohman, T. Recognizing long-form speech using streaming end-to-end models. In 2019 IEEE automatic speech recognition and understanding workshop (ASRU), pp. 920\u2013927. IEEE, 2019. Oren et al. (2024) Oren, M., Hassid, M., Adi, Y., and Schwartz, R. Transformers are multi-state rnns.",
    "transformerfam-26": "arXiv preprint arXiv:2401.06104, 2024. Pasad et al. (2021) Pasad, A., Chou, J.-C., and Livescu, K. Layer-wise analysis of a self-supervised speech representation model. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 914\u2013921. IEEE, 2021. Peng et al. (2023) Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S.",
    "transformerfam-27": "M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents.",
    "transformerfam-28": "arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rangapuram et al. (2018) Rangapuram, S. S., Seeger, M. W., Gasthaus, J., Stella, L., Wang, Y., and Januschowski, T. Deep state space models for time series forecasting. Advances in neural information processing systems, 31, 2018. Roy et al. (2021) Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53\u201368, 2021. Shaham et al. (2022) Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., et al. Scrolls: Standardized comparison over long language sequences.",
    "transformerfam-29": "arXiv preprint arXiv:2201.03533, 2022. Shazeer (2019) Shazeer, N. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Shazeer & Stern (2018) Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596\u20134604. PMLR, 2018. Su et al. (2024) Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding.",
    "transformerfam-30": "Neurocomputing, 568:127063, 2024. Tang et al. (2016) Tang, X., Wu, J., and Shen, Y. The interactions of multisensory integration with endogenous and exogenous attention. Neuroscience & Biobehavioral Reviews, 61:208\u2013224, 2016. Tay et al. (2022) Tay, Y., Dehghani, M., Abnar, S., Chung, H. W., Fedus, W., Rao, J., Narang, S., Tran, V. Q., Yogatama, D., and Metzler, D. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022. Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Tolstikhin et al. (2021) Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u201324272, 2021. Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling.",
    "transformerfam-31": "arXiv preprint arXiv:2307.03170, 2023. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Villalobos et al. (2022) Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325, 2022. Wallace (1960) Wallace, A. F. Plans and the structure of behavior, 1960. Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity.",
    "transformerfam-32": "arXiv preprint arXiv:2006.04768, 2020.",
    "transformerfam-33": "White et al. (1986) White, J. G., Southgate, E., and Thomson, J.",
    "transformerfam-34": "N. S. brenner (1986) the structure of the nervous system of the nematode caenorhabditis elegans.",
    "transformerfam-35": "Philosophical Transactions of the Royal Society B: Biological Sciences, 314:1\u2013340, 1986.",
    "transformerfam-36": "Wu et al. (2022) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Xiao et al. (2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks.",
    "transformerfam-37": "arXiv preprint arXiv:2309.17453, 2023. Xiong et al. (2020) Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 10524\u201310533. PMLR, 2020. Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Xiong et al. (2021) Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention.",
    "transformerfam-38": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14138\u201314148, 2021. Yu et al. (2022) Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are image-text foundation models.",
    "transformerfam-39": "arXiv preprint arXiv:2205.01917, 2022. Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020. Appendix A Architecture details\n\nA.1 Flan-PaLM architecture\n\nTable 5 provides detailed information on the architecture of Flan-PaLM for 1B, 8B, and 24B models. MQA (Multi-Query Attention) (Shazeer, 2019) is an attention mechanism that employs a single set of keys and values for all attention heads. A.2 FAM hyperparameters\n\nTable 6 presents the default settings for the hyperparameters added in TransformerFAM. Appendix B Additional details of TransformerFAM\n\nThe appendix describes additional details not covered in the main text. B.1 Initial FAM\n\nThe current FAM is the output activation of the previous FAM update. So, how do we initialize the FAM for the first block? When FAM are used as queries, they summarize the block context. Therefore, we learn by adding learnable summarization embeddings to the token embedding lookup level of the Transformer model. This is the same as prepending learnable embeddings in soft prompt tuning (Lester et al., 2021). The difference is that full attention is applied between the FAM prompt activations, and the updated FAM is used for the next block. The FAM prompt is passed to the next transformer layer through the forward propagation of the Transformer model, and it has a summary representation that is suitable for the level of the layer. Prefix tuning (Li & Liang, 2021) can also be used to train learnable initial FAM at each Transformer layer. However, we had difficulty matching the magnitude of the learnable prefix to the regular input sequence, and the results of prompt tuning were consistently better. Ablation study in Section B.4.4 shows prompt tuning outperforms. In addition, prefix tuning has the disadvantage of adding additional weights of FAM length to each layer. In addition, the first FAM update in self-attention should utilize a zero tensor in the residual connection rather than the initial FAM, because the initial FAM does not carry any contextual information. In summary, we learned the initial FAM using prompt tuning, which only adds a very small number of weights of FAM length to the entire model. We share the same initial FAM across all minibatches. B.2 Input Length Extrapolation\n\nB.2.1 FAM Position Encoding\n\nWe used rotary position embedding (RoPE) (Su et al., 2024) in all of our experiments. Each input sequence is assigned an integer position , which is converted into sinusoidal encoding in the form of where (Vaswani et al., 2017). FAM is inserted at each block boundary, but the problem is how to assign positions to FAM. We assigned positions to FAM in order from the last number of the compressed block. For example, if the block size is 4, the FAM length is 2, and the positions of the compressed blocks are , , , and , then the updated FAM positions are and . We tried other methods, but this method worked best according to the ablation results in Section B.4.1. B.2.2 Random Position Offset\n\nThe input length extrapolation problem of Transformer is well known (Chen et al., 2023; Xiong et al., 2023). For example, a Transformer LLM trained with 2k tokens experiences a severe performance drop when generating 8k tokens. This is because machine learning (ML) does not generalize well to situations that it has not seen during training. Transformer with SWA does not suffer from the position extrapolation problem when using relative positional embedding like RoPE. This is because the score value of becomes a function of in the form of . Because the range of is limited to the window size, independent of the input length, the model can handle long input sequences without facing novel scenarios during inference. The model can accurately determine the relative position, if the window size is smaller than the maximum wavelength of . If the typical is used for positional embedding, the working maximum window size is the maximum wavelength (k tokens ). However, FAM breaks the symmetry of relative position. Since the absolute position from the past to the present is recursively embedded in the FAM, the large absolute position value that the model encounters for the first time during inference creates a situation where the model needs to extrapolate. We propose Random Position Offset as a solution. At each training step, the Transformer model randomly samples a scalar value between 0 and the maximum wavelength. All Transformer layers add that random value to the absolute position at that training step. Therefore, the FAM experiences the entire range of absolute position embedding during training. This is a purely training technique. During inference, the default offset is . We used the below algorithm that generates by when sampling the offset, as is the default value. offset = np.uniform([b], maxval=wavelen)\n\noffset *= np.round(np.uniform([b]))\n\nB.2.3 Random State Passing\n\nDue to the recursive use of FAM, we need to determine the maximum number of updates for which the FAM remains valid. If it is updated up to 8 times during training, the model will have to extrapolate the situation where it is updated 100 times during inference. The same problem existed in RNNs, and Random State Passing (RSP) (Narayanan et al., 2019) was proposed to handle long-form speech in the speech domain. We also used RSP to generalize the number of updates of FAM. RSP saves FAM as weights at the end of each training step. Then, it loads the saved FAM at the next training step. When initializing FAM, it either uses randomly saved FAM or learned FAM. In our default setup, it used saved FAM with probability. To save FAM of all minibatch, weights are required as many as the number of minibatch. We save FAM of only the first batch and all minibatch share it in the next training step. On the other hand, saved FAM can be thought of as learnable prefix for prefix tuning (Li & Liang, 2021). It is also possible to train only the FAM while the model is frozen and use them for various downstream tasks or personalization. This part is left as a future research topic. B.3 FAM illustrated\n\nIn Section 2.2, we define the feedback loop as feedback activations that feed contextual representation back into each block of BSWA. This feedback loop is formally described in Algorithm 2. While Figure 2(b) illustrates Algorithm 2, the static image makes it challenging to fully grasp the dynamic nature of the decoding self-attention mechanism. To clarify this, we create a multi-frame animation in Figure 5 that demonstrates how the attention mechanism evolves over time (top to bottom). B.4 Ablation Studies\n\nAll ablation studies were conducted on a 1B model. B.4.1 FAM Position Encoding\n\nSection B.2.1 proposed assigning the last number of the compressed block as the FAM position. In addition, we also experimented with FAM having a float position between blocks. In the example of block size 4 in Section B.2.1, the FAM positions would be and . We also experimented with the case where the FAM position is always 0. As shown in Table 7, the last number showed the best accuracy. Figure 6 shows FAM (i.e. Last number in Table 7) outperforming FAM-POS (i.e. Zero) in PG-19 accuracy over most base frequencies. B.4.2 Random Position Offset\n\nAs mentioned in Section B.2.2, the input length extrapolation problem of the Transformer is well-known. The \u201dAttention is All You Need\u201d (Vaswani et al., 2017) introduced sinusoidal position encoding in the form of where . Popular solutions for full attention models include increasing the base frequency from 10k to 500k (Chen et al., 2023) or scaling down to (Xiong et al., 2023). Since the relative position of BSWA has a range of 0 to window size, it does not suffer from the input length extrapolation problem. However, TransformerFAM encodes absolute position into FAM, which requires a solution. Section B.2.2 proposes Random Position Offset (RPO) as a solution. In Figure 6, FAM shows better PG-19 accuracy than FAM-RPO at the 10k base frequency. As mentioned in Section D.2, the max length of PG-19 was truncated to 256k in the experiments. Furthermore, scaling up the base frequency or scaling down is only a remedy for pain, not a final solution. It reduces the resolution of the position, which negatively affects the overall Transformer performance, as shown in the figure. Interestingly, the originally proposed 10k is a very good value even in long contexts, until the window size reaches its wavelength (63k). In addition, we did not observe the attention sink phenomenon (Xiao et al., 2023) in our TransformerBSWA experiments. The paper proposes that Transformers with sliding window attention should store initial tokens in a KV cache for long-context inference. However, the relative position of SWA is restricted to a range of 0 to window size, independent of input sequence length. Our TransformerBSWA implementation, trained on 8.5k tokens, successfully operated for inference up to 256k tokens without any issues. B.4.3 Random State Passing\n\nTo extrapolate the number of FAM updates, Section B.2.3 proposes Random State Passing (RSP). In Figure 6, FAM shows significantly better PG-19 accuracy than FAM-RSP. This demonstrates that RSP plays a crucial role in training FAM. Figure 7 shows the best performance at a probability of 0.8. At 0.8, the half-life is 3 training steps (). This means that every 26k (8.5k x 3) tokens, FAM restarts from the beginning with a 50 probability. In other words, if FAM experiences 25 FAM updates during training, it can extrapolate to 256 FAM updates (256k tokens) during inference. B.4.4 Prompt vs. Prefix\n\nSection B.1 mentions that Prompt tuning outperforms Prefix tuning for training FAM in terms of performance, memory, and number of parameters. Figure 6 shows that Prefix tuning for FAM training leads to performance degradation in PG-19 (FAM vs.",
    "transformerfam-40": "FAM+Prefix). B.4.5 TransformerXL vs. TransformerBSWA\n\nTransformerXL is a modification of TransformerBSWA that incorporates an additional QK attention mask to mask out keys beyond a specified window size. Therefore, there is no reason to expect a significant difference in performance between the two. Table 8 shows the scores for the main tasks. TransformerBSWA used a single memory segment, and TransformerXL window size was equal to the block size of . B.4.6 Comparison with other methods\n\nWe compared FAM with Recurrent Memory Transformer (RMT) (Bulatov et al., 2022), because RMT also implements memory in Transformer using feedback mechanism from top layer to bottom layer. As shown in Figure 8, RMT showed worse performance than Block Sliding Window Attention (BSWA) with 1 memory segment, in the PassKey retrieval task. FAM solved the PassKey retrieval task, but RMT did not work at all with very long filler token lengths. RMT is implemented by feeding the output memory of the previous segment as the input memory to the next segment. In the constraint that the input is text embedding and the output is text reconstruction in LLM, RMT has an additional constraint that the latent space of the output memory and the input memory must match. In this situation, RMT fails to remember the PassKey for a very long context. On the other hand, FAM seems to compress, store, and propagate information more effectively by learning memory representation that matches the abstraction level of each layer through training. AutoCompressors (Chevalier et al., 2023) is an extension of RMT that continuously accumulates blockwise memory of RMT. AutoCompressors theoretically should be able to solve the PassKey task since it maintains all the memory tokens for all blocks.",
    "transformerfam-41": "However, as shown in Figure 8, its performance drops sharply after 18k tokens. This is because the model only saw up to 18k tokens during training. It fails to generalize to longer filler token lengths. The AutoCompressors in Figure 8 accumulates 260 memories to support up to 260k tokens. Appendix C Don\u2019t\n\nThere could be various ways to create feedback loops in Transformers. This appendix summarizes our attempts that did not work well. We hope that this will save other researchers time when improving the architecture of feedback loops in future studies. C.1 Beyond Block-Recurrent Transformers\n\nBlock-Recurrent Transformers (BRT) (Hutchins et al., 2022) have a recurrent state that connects each block, and the recurrent state and the block input sequence exchange information through cross-attention. In this architecture, the recurrent state plays a role similar to FAM. We started the project by removing the additional QKV projection and cross-attention for the recurrent state in BRT and integrating all computations into self-attention. Like BRT, we tried to compress the block input by attention to use it as the recurrent state of the next block, and it required additional projections like BRT. It was difficult to properly train the additional projections that were only used when striding blocks, and as a result, the performance of general tasks such as GPT-3 tasks was degraded. Finally, we found that the activations compressed by attention must go through the FF layer of the Transformer to be aligned as a latent representation that can be used again as the input of the next Transformer. The input of the Transformer and the output of self-attention are very different representations. However, the FF layer transforms it back into a representation similar to the input of the Transformer for the next Transformer layer. After the discovery of reusing the FF layer, we also found that separate QKV projection and additional projections are not required for FAM. That is how the TransformerFAM architecture was created. C.2 Feedback Memory Segment\n\nAs shown in Figure 2(b), TransformerFAM utilizes the updated FAM from the previous block as the input to the current block. Therefore, it is natural to consider using the Transformer outputs of the previous block directly as the input to the current block, instead of using a complex FAM mechanism. Figure 9 illustrates this modification, which we refer to as the Feedback Memory Segment (FM). FM achieves an infinite theoretical receptive field by performing self-attention at the output level. This is a specific variant of Staircase Attention (Ju et al., 2022). Staircase Attention proposes using activations from progressively higher Transformer layers as the memory segment goes further into the past. However, as shown in Figure 10, FM fails to retain PassKey information over a long context. The figure compares M1, 2, and 4 with FM1, 2, and 4 when the memory segment size is 1, 2, and 4 for BSWA and Feedback Memory Segment. TransformerFM outperforms TransformerBSWA, but still falls short of TransformerFAM by a significant margin. In TransformerFM, each activation must possess both local and global representation, similar to TransformerBSWA. However, the absence of an activation specifically responsible for the global representation appears to prevent the retention of critical information like PassKey over an extended context. Furthermore, TransformerFM exhibited lower performance than TransformerBSWA on GPT-3 tasks (Table 9). As TransformerFM introduced additional complexity without any discernible performance advantages, we discontinued this architecture. Table 9 presents the results after 25k steps of fine-tuning. C.3 Static Summary Tokens\n\nAs illustrated in Figure 2(b), \u201dFAM copy\u201d duplicates the previously updated FAM to the current FAM input. However, \u201dFAM copy\u201d is not an essential component in the feedback loop. When FAM compresses the current block, it can also attend to the previous FAM as both key and value, as shown in the experiment in Section 3.4.2. Therefore, \u201dFAM copy\u201d is not strictly necessary because past FAM information is propagated through self-attention. As mentioned in Section B.1, we learn the initial FAM using prompt tuning, and this token can be considered a \u201dSummary\u201d token. An alternative design could employ a persistent summary token for block compression, with the feedback loop enabled by attending to the key-value pairs of previous FAMs. As illustrated in Figure 10, SUM FAM (static summary token) prevents the model from successfully solving the PassKey task. This suggests that the query for summarization needs to be dynamically generated conditioned on the past FAM. A static query may not be able to cover all situations effectively. Additionally, simply attending to the key and value alone does not transmit sufficient information from the past. On the other hand, the \u201dFAM copy\u201d mechanism propagates information from the past FAM to the current FAM through the residual connection of the Transformer, which facilitates better performance on the PassKey task. C.4 Diversity Loss\n\nIn Section 3.4.1, we observed a performance drop when the FAM length exceeded 64. We hypothesized that FAM was underutilizing its capacity and experiencing mode collapse, where lengthy FAM either focused on a small number of inputs or prevented all inputs from obtaining sufficient information. As a remedy, we employed diversity loss as an auxiliary loss, similar to Wav2vec 2.0 (Baevski et al., 2020). Diversity loss aims to maximize the entropy of QK attention probability, as shown in Equation 2b. This loss encourages FAM to uniformly attend to all inputs, and all inputs equally attend to FAM. In Equation 2, denotes the batch size, represents the block index, indicates the sequence position within a block, and refers to multi heads. In Equation 2a, represents the average attention probability across all sequences and multi-heads for the self-attention of each block. The auxiliary loss functions to regularize this probability towards uniformity. p \u00af b \u200b \u03c4 subscript \u00af \ud835\udc5d \ud835\udc4f \ud835\udf0f \\displaystyle\\bar{p}_{b\\tau} = 1 H \u200b L \u200b \u2211 h = 1 H \u2211 l = 1 L p b \u200b \u03c4 \u200b h \u200b l absent 1 \ud835\udc3b \ud835\udc3f superscript subscript \u210e 1 \ud835\udc3b superscript subscript \ud835\udc59 1 \ud835\udc3f subscript \ud835\udc5d \ud835\udc4f \ud835\udf0f \u210e \ud835\udc59 \\displaystyle=\\frac{1}{HL}\\sum_{h=1}^{H}\\sum_{l=1}^{L}p_{b\\tau hl} (2a) \u2112 d subscript \u2112 \ud835\udc51 \\displaystyle\\mathcal{L}_{d} = 1 B \u200b T \u200b \u2211 b = 1 B \u2211 \u03c4 = 1 T \u2212 H \u200b ( p \u00af b \u200b \u03c4 ) = 1 B \u200b T \u200b \u2211 b = 1 B \u2211 \u03c4 = 1 T p \u00af b \u200b \u03c4 \u200b log \u2061 p \u00af b \u200b \u03c4 absent 1 \ud835\udc35 T superscript subscript \ud835\udc4f 1 \ud835\udc35 superscript subscript \ud835\udf0f 1 T \ud835\udc3b subscript \u00af \ud835\udc5d \ud835\udc4f \ud835\udf0f 1 \ud835\udc35 T superscript subscript \ud835\udc4f 1 \ud835\udc35 superscript subscript \ud835\udf0f 1 T subscript \u00af \ud835\udc5d \ud835\udc4f \ud835\udf0f subscript \u00af \ud835\udc5d \ud835\udc4f \ud835\udf0f \\displaystyle=\\frac{1}{B\\mathrm{T}}\\sum_{b=1}^{B}\\sum_{\\tau=1}^{\\mathrm{T}}-H(\\bar{p}_{b\\tau})=\\frac{1}{B\\mathrm{T}}\\sum_{b=1}^{B}\\sum_{\\tau=1}^{\\mathrm{T}}\\bar{p}_{b\\tau}\\log{\\bar{p}_{b\\tau}} (2b)\n\nWe trained the model with diversity loss with various weights, but the overall performance was always worse, regardless of the FAM length. It did not help at all even when the FAM length was 256 or longer. C.5 Reconstruction Loss\n\nCompressive transformers (Rae et al., 2019) compress memory segments before forwarding them to the next block. The key difference between our approach is that the compressed information is not recurrently connected. The paper proposes a reconstruction loss as an auxiliary loss, which aims to reconstruct the original activations from the compressed activations. We also experimented with the reconstruction loss. In details, we generated transformer outputs autoregressively from the updated FAM and the query sequence of the original block, and compared them to the original outputs using the MSE loss. However, this did not help to improve the performance. Appendix D Experiments details\n\nThis section provides additional details about the experiments that were not covered in Section 3. D.1 PassKey Retrieval\n\nThe format of the PassKey retrieval task is shown in Figure 11. The original paper (Mohtashami & Jaggi, 2023) also included a prefix filler, but we removed it in our paper. D.2 Long Context Tasks\n\nTable 10 provides detailed information about the long context tasks we used. Due to the limitations of TPU memory, PG-19, Isabelle, and NarrativeQA were truncated to 256k tokens, which is within two standard deviations. Table 11 presents the LCT results for TransformerBSWA and TransformerFAM. MX represents the number of memory segments for TransformerBSWA. D.3 GPT-3 Tasks\n\nWe evaluated all model sizes on the tasks reported by GPT-3 (Brown et al., 2020). The results are shown in Table 12. D.4 Complexity\n\nBSWA and FAM have memory and computational complexity of , where is the chunk size. Table 13 and Table 14 show memory and inference time through inference jobs on TPUv4. The most tokens in the experiment are pre-filled and generation is limited to 256 tokens. Appendix E Related Work\n\nThe Transformer architecture exhibits a quadratic complexity with respect to context length, a significant limitation. To address this, several research works have focused on approximating the attention mechanism. One approach involves sparse attention, where only a subset of important tokens are attended to, as seen in models like Sparse Transformer (Child et al., 2019), Big Bird (Zaheer et al., 2020), Reformer (Kitaev et al., 2020), Routing Transformer (Roy et al., 2021), and TOVA (Oren et al., 2024). Linear approximation methods offer an alternative, seeking to circumvent the quadratic complexity by altering attention calculations, as exemplified by Linformer (Wang et al., 2020), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020), and Nystr\u00f6mformer (Xiong et al., 2021). Finally, some research explores entirely different sequence-to-sequence architectures as replacements for attention-based Transformers, including MLP-mixer (Tolstikhin et al., 2021), State Space Models (Rangapuram et al., 2018), S4 (Gu et al., 2021), Mamba (Gu & Dao, 2023), and RWKV (Peng et al., 2023). Global Workspace Theory (GWT) (Baars, 2005) is a leading theory of consciousness. According to GWT, the human brain possesses a global workspace where various modules, such as sensory input, memories, and internal representations, converge. The attention mechanism in brain acts as a spotlight, focusing on specific inputs among the multitude, and transforming this unconscious activity into conscious awareness. These \u201dmomentarily active, subjectively experienced\u201d events are then stored in working memory. TransformerFAM draws inspiration from GWT, adopting its principle of a unified attention mechanism for processing homogenous, heterogeneous, and feedback data. Appendix F Attention Visualization\n\nFigure 12 depicts the attention map for each head in each layer of a 1B model. FAM is prepended and is located at the bottom left corner. The bright spots along the left edge represent the block inputs attending to FAM, while the bright spots along the bottom edge represent FAM compressing the corresponding block. Overall, the block inputs actively reference FAM, while FAM compresses only the selective inputs. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Sun May 5 21:19:36 2024 by LaTeXML"
}