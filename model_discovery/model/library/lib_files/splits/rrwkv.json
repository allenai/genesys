{
    "rrwkv-0": "# RRWKV: CAPTURING LONG-RANGE DEPENDENCIES IN RWKV \n\nINCOMPLETE PAPER<br>Leilei Wang*<br>leileiwang03@gmail.com\n\n\n#### Abstract\n\nOwing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dotproduct attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.",
    "rrwkv-1": "## 1 Introduction\n\nTransformers Vaswani et al. [2017], with dot-product attention as the fundamental component, have emerged as dominant architectures in various artificial intelligence tasks, especially in language modeling Brown et al. [2020], Devlin et al. [2019]. By enabling direct interaction between every pair of elements in a sequence, Transformers overcome the limitations of long-range dependencies and enable parallelization, which make them validly scale up to efficiently learn and exhibit emergent abilities Kaplan et al. [2020]. However, Transformers face the challenge of high computational costs in dot-product attention operations, which scale quadratically with the length of the sequence. In contrast, recurrent neural networks (RNNs) Hochreiter and Schmidhuber [1997], Chung et al. [2014] naturally offer low computational costs with linearly scaling in space and time requirements. The reason is that RNNs generate the currently hidden states $h_{t}$ giving the current input and previous hidden state $h_{t-1}$ at position $t$. Unfortunately, this sequential nature leads to RNNs being unable to parallel and bear the gradient vanishing problem, with the result that they lack scalability Hochreiter [1998], Le and Zuidema [2016]. To achieve parallelization and low computational costs, the architecture, Receptance Weighted Key Value (RWKV) Peng et al. [2023], redesigns the tensor-product attention Zhai et al. [2021] as a linear attention mechanism with a time-sequential mode similar to RNNs. Regretfully, the RWKV cannot afford to capture long-range dependencies over long sequences since the information would be lost in the sequential flow, in contrast to standard attentions Bahdanau et al. [2016]. In this paper, we propose the Retrospected Receptance Weighted Key Value (RRWKV) architecture based on RWKV for discharging long-range interactions and performing efficient information flow in which mediums are incorporated. Especially, the RRWKV model is to obtain the appropriate messages specifically addressing the issues of information redundancy that may occur with the attention mechanisms and information loss with the RWKV. ## 2 Background\n\nIn this section, we will briefly introduce the development history of RWKV from the standard transformer architectures and then analyse the reasons of lacking long-range dependencies in RWKV. On the one hand, we discuss dot-product\n\n[^0]attention and tensor-product attention in standard transformer architectures. On the other hand, we uncover the underlying ideas of two blocks in RWKV and the deficiencies they bring. ### 2.1 Standard Transformer Architecture\n\nHere we simply review the core components of Transformers, attention mechanisms. Attention mechanisms leverage query, key, and value components to facilitate information selection. The query encapsulates the context of a specific matter, while the keys and values correspond to the input elements. By evaluating compatibility between the query and each key, attention weights are assigned to the corresponding values. These weighted values are then summed, enabling the matter to incorporate pertinent information from values. ### 2.1.1 Dot-Product Attention\n\nIn dot-product attention Vaswani et al. [2017], attention weights are determined by taking the dot product between the query and key vectors. The dot-product serves as a measure of similarity and represents the compatibility between the query and key. Mathematically, for a given sequence of tokens, the dot-product attention can be expressed as follows:\n\n$$\n\\operatorname{Atten}(Q, K, V)_{t}=\\frac{\\sum_{i=1}^{t} \\exp \\left(q_{t}^{T} k_{i}\\right) \\cdot v_{i}}{\\sum_{i=1}^{t} \\exp \\left(q_{t}^{T} k_{i}\\right)}\n$$\n\nwhere $q$ represents the query vector, $k$ represents the key vector, $v$ represents the value vector, and scaling factor is neglected for convenience. Dot-product attention effectively implements the core idea of attention mechanisms and allows for parallel computation at each time step. However, it is constrained by the quadratic calculation and storage requirements of the similarity matrix, limiting its capabilities. ### 2.1.2 Tensor-Product Attention\n\nIn tensor-product attention Zhai et al. [2021], the determination of attention weights is based on the key vectors and position biases, replacing the conventional use of key and query vectors Bahdanau et al. [2016], Vaswani et al. [2017]. The tensor-product performs the role of feature interaction between the query and the weighted sum value. Correspondingly, the tensor-product attention can be expressed as follows:\n\n$$\n\\operatorname{Atten}(Q, K, V)_{t}=\\sigma\\left(q_{t}\\right) \\odot \\frac{\\sum_{i=1}^{t} \\exp \\left(k_{i}+w_{t, i}\\right) \\odot v_{i}}{\\sum_{i=1}^{t} \\exp \\left(k_{i}+w_{t, i}\\right)}\n$$\n\nwhere each $w_{t, i}$ is a learned scalar which introduces an offset in the key vector $i$. This implies that key vectors can be adjusted to be more suitable for the specific query at time $t$. Although the tensor-product attention still retains the same high computational costs as the dot-product attention, it can be transformed into a linear complexity operation by disregarding the position biases. ### 2.2 Receptance Weighted Key Value Architecture\n\nHere we briefly analyse the Receptance Weighted Key Value (RWKV) architecture which combines the sequential mode of RNNs and then achieves the linear calculation and storage costs.",
    "rrwkv-2": "### 2.2.1 Recurrent Neural Networks\n\nRecurrent Neural Networks (RNNs) Chung et al. [2014] are designed to handle sequential data by incorporating a mechanism that enables information to propagate from one step to the next. This process can be succinctly expressed using the following formula:\n\n$$\n\\begin{aligned}\n& h_{t}=g\\left(x_{t}, h_{t-1}\\right) \\\\\n& \\tilde{x}_{t}=f\\left(x_{t}, h_{t}\\right)\n\\end{aligned}\n$$\n\nIn this equation, the functions $g$ and $f$ refer to the reset gate and update gate, respectively. The reset gate function, $g$, determines the extent to which the previous state, $h_{t-1}$, is reset or forgotten based on the current input, $x_{t}$. Subsequently, the update gate function, $f$, governs the proportion of the current input $x_{t}$ and the hidden current state $h_{t}$ that contribute to the computation of the output, $\\tilde{x}_{t}$. By utilizing these reset and update gates, RNNs effectively capture the temporal dependencies inherent in sequential data. ### 2.2.2 Receptance Weighted Key Value Model\n\nReceptance Weighted Key Value (RWKV) Peng et al. [2023] model captures and propagates information in a sequential or auto-regressive mode Graves [2014] as integrating the capabilities of RNNs and attention mechanisms. This model consists of two core blocks, namely the time-mix block and the channel-mix block. Time-Mix Block This block aims to enhance the modeling of dependencies and patterns within a sequence by replacing the conventional weighted sum calculation in attention mechanisms with hidden states. By incorporating hidden states, the time-mix block can effectively propagate and update information across sequential steps. The calculations involved in this block can be expressed as follows:\n\n$$\n\\begin{aligned}\nq_{t} & =\\left(\\mu_{q} \\odot x_{t}+\\left(1-\\mu_{q}\\right) \\odot x_{t-1}\\right) \\cdot W_{q} \\\\\nk_{t} & =\\left(\\mu_{k} \\odot x_{t}+\\left(1-\\mu_{k}\\right) \\odot x_{t-1}\\right) \\cdot W_{k} \\\\\nv_{t} & =\\left(\\mu_{v} \\odot x_{t}+\\left(1-\\mu_{v}\\right) \\odot x_{t-1}\\right) \\cdot W_{v} \\\\\no_{t} & =\\left(\\sigma\\left(q_{t}\\right) \\odot h\\left(k_{t}, v_{t}\\right)\\right) \\cdot W_{o}\n\\end{aligned}\n$$\n\nIn these formulas, the representations of $q_{t}, k_{t}$, and $v_{t}$ are calculated by linearly interpolating between the current input and the input at the previous time for coherent and fluent token representations. The parameter $\\mu$ denotes the token shift, which determines the interpolation weight. Similar to tensor-product attention, this block applies a non-linear activation function $\\sigma$ to $q_{t}$ and combines it with the hidden states $h\\left(k_{t}, v_{t}\\right)$ using element-wise multiplication as an update gate. Furthermore, the hidden states $h\\left(k_{t}, v_{t}\\right)$ serve as both the reset gate and a replacement for the traditional weighted sum value. This process of calculating the hidden states can be described by the following equations:\n\n$$\n\\begin{aligned}\na_{0}, b_{0}, p_{0} & =0,0,0 \\\\\np_{t} & =\\max \\left(p_{t-1}, k_{t}\\right) \\\\\nh_{t} & =\\frac{\\exp \\left(p_{t-1}-p_{t}\\right) \\odot a_{t-1}+\\exp \\left(k_{t}-p_{t}\\right) \\odot v_{t}}{\\exp \\left(p_{t-1}-p_{t}\\right) \\odot b_{t-1}+\\exp \\left(k_{t}-p_{t}\\right)}\n\\end{aligned}\n$$\n\nNote that the position biases are omitted in these equations for convenience, and the division is element-wise division. Intuitively, the hidden states are computed recursively, and the vector $p$ serves as the reset gate in this process. Channel-Mix Block This block is to amplify the outputs of time-mix block, which is given by:\n\n$$\n\\begin{aligned}\nr_{t} & =\\left(\\mu_{r} \\odot o_{t}+\\left(1-\\mu_{r}\\right) \\odot o_{t-1}\\right) \\cdot W_{r} \\\\\nz_{t} & =\\left(\\mu_{z} \\odot o_{t}+\\left(1-\\mu_{z}\\right) \\odot o_{t-1}\\right) \\cdot W_{z} \\\\\n\\tilde{x}_{t} & =\\sigma\\left(r_{t}\\right) \\odot\\left(\\max \\left(z_{t}, 0\\right)^{2} \\cdot W_{v}\\right)\n\\end{aligned}\n$$\n\nwhere squared ReLU activation So et al. [2022] is adopted. In these equations, the output $o_{t}$ contains the history information up to time $t$, and the interpolation weight $\\mu$ is derived from both $o_{t}$ and $o_{t-1}$ same as in time-mix block. Intuitively, this amplification process enhances the representations of historical information. In sum, the RWKV model has achieved the parallelization in a time-parallel mode due to the tensor-product Lei et al. [2018] and contains the linear computational costs for discarding similarity matrix.",
    "rrwkv-3": "However, the calculations of hidden states may lead to information loss and fail to capture long-range dependencies Peng et al. [2023]. ## 3 The Retrospected Receptance Weighted Key Value (RRWKV) Model\n\nThe RRWKV model derives its name from the expanding the ability of \"looking back\" at previous tokens on the RWKV model, inspired by SENet Hu et al.",
    "rrwkv-4": "[2018]. This ability is achieved by inserting a few mediums into the sequential tokens to make information flow fluently and shorten the maximum path length for easily learning long-range dependencies Informatik et al. [2003]. The overview of RRWKV model can be found in Figure 1. ### 3.1 Interpolations of the Mediums\n\nGiven the sequential tokens $X=\\left\\{x_{1}, \\cdots, x_{t}\\right\\}$, we propose to introduce the mediums $M=\\left\\{m_{1}, \\cdots, m_{c}\\right\\}$ into the sequence at the interval of $s$ tokens for simplicity, where $m_{1}$ is set as zeros-like token vector and inserted in the beginning of $X$ and $t$ divided by $s$ using the floor division operation should yield $c$. This new sequence can be redefined as $X_{\\text {new }}=\\left\\{m_{1}, x_{1}, \\cdots, m_{c}, \\cdots, x_{t}\\right\\}$. Note that the mediums can be inserted in a more complex and adaptive way. ![](https://cdn.mathpix.com/cropped/2024_09_12_1686ce93732a3c705d16g-4.jpg?height=915&width=1288&top_left_y=372&top_left_x=402)\n\nFigure 1: The overview of RRWKV architecture\n\n### 3.2 Squeeze of the Mediums\n\nAs for the representations of mediums, each $s$ tokens are passed through a squeeze operation to generate the corresponding medium $m$. This squeeze operation can be regarded as a pooling method across the time dimension of $s$ tokens. For instance, medium $m_{2}$ obtains its representation from the pooling of the tokens $\\left\\{x_{1}, \\cdots, x_{s}\\right\\}$. Especially, the pooling method can be a linear operation due the existence of \"sentry\" $m_{1}$. In addition, each $m_{i}$ is the adaptive recalibration of the mediums $M_{i}=\\left\\{m_{1}, \\cdots, m_{i}\\right\\}$ to emphasise the beneficial history information. The recalibration is employed by a simple gating mechanism with a sigmoid activation same as SENet Hu et al. [2018]:\n\n$$\nm_{i}=\\sigma\\left(W_{s_{i}} \\cdot \\delta\\left(W_{m_{i}} \\cdot M_{i}\\right)\\right)\n$$\n\nwhere $W_{m_{i}} \\in R^{C \\times i}$ and $W_{s_{i}} \\in R^{1 \\times C}$. In this way, we could distinguish and contain the favourable information. ### 3.3 Excitation of the Mediums\n\nThe representations of mediums can afford to enhance the information flow in time-mix block and emphasize the context in channel-mix block, which we call the excitation of mediums. Excitation in Time-Mix Block This block in RWKV recognizes the dependencies and patterns in the sequential mode. Automatically, the calculations of information circulation $X_{n e w}$ follow the equations of time-mix block in RWKV. In particular, each medium play the role of the abstract of past information and powerful intermediary in the chain of sequential tokens. Excitation in Channel-Mix Block This block in RWKV strives to obtain smooth token representations within the context of sequence tokens. However, the backgrounds are established by linearly interpolating between the current input and the input at the previous time in a micro way. To possess a more macro backgrounds, the mediums are adopted\n\nTable 1: Comparisons of models for auto-regressive tasks. $n$ is the length of sequence, $d$ is the token dimension, $c$ is the number of mediums and $s$ is the interval of mediums. Especially, the product of $c$ and $s$ is approximately equal to $n$. | Model | Complexity | Parallelization | Information Redundancy | Maximum Path Length |\n| :---: | :---: | :---: | :---: | :---: |\n| Transformers | $O\\left(n^{2} \\cdot d\\right)$ | Yes | Noisy | $O(1)$ |\n| RNNs | $O(n \\cdot d)$ | No | Scarce | $O(n)$ |\n| RWKV | $O(n \\cdot d)$ | Yes | Scarce | $O(n)$ |\n| RRWKVours | $O\\left(\\left(n+c^{2}\\right) \\cdot d\\right)$ | Yes | Appropriate | $O(s)$ |\n\nto recalibrate the token representations:\n\n$$\n\\begin{aligned}\n& r_{t}=\\left(\\nu_{r} \\odot o_{t}+\\left(1-\\nu_{r}\\right) \\odot m_{t}\\right) \\cdot W_{r} \\\\\n& z_{t}=\\left(\\nu_{z} \\odot o_{t}+\\left(1-\\nu_{z}\\right) \\odot m_{t}\\right) \\cdot W_{z} \\\\\n& \\tilde{x}_{t}=\\sigma\\left(r_{t}\\right) \\odot\\left(\\max \\left(z_{t}, 0\\right)^{2} \\cdot W_{v}\\right)\n\\end{aligned}\n$$\n\nwhere $m_{t}$ denotes the corresponding medium of token $o_{t}$ such as $m_{2}$ corresponding to $o_{1}$, and we regulate the subsequent tokens of $m_{t}$ restricted to $m_{t}$. ### 3.4 Why Retrospected Receptance Weighted Key Value (RRWKV) Model\n\nIn this part, we compare various aspects of RRWKV to the Transformers, RNNs and RWKV commonly applied in the auto-regressive tasks, including computational complexity, parallelized computation, information redundancy, and the maximum path length.",
    "rrwkv-5": "As for the maximum length, it measures the long-range dependencies in the network. The longer these paths of interactions, the harder it is to capture long-range dependencies Informatik et al. [2003]. As noted in Table 1, our model RRWKV has demonstrated superior performances compared to other models based on these criteria. In terms of computational costs and parallelization, RRWKV has nearly achieved the linear calculations since $c$ will be small to be ignored and parallel same as RWKV. For the information redundancy, attention mechanisms in Transformers are more likely to introduce noises as associating unrelated information Zhou et al. [2021], while information loss may occur in RNNs and RWKV due to its sequential mode Peng et al. [2023]. In RRWKV, the pooled medium representations could sweep the noisy information and the excitation of mediums is supposed to retrospecting the history information. Moreover, the maximum path length has shorten to $O(s)$ to break the limitation of long-range dependencies in RWKV. ## 4 Future Work\n\nThere are two works in the future:\n\n- Designing a more adaptive method of mediums interpolating. - Experiments on benchmark datasets should be managed. - Exploring the possible benefits of $W_{s_{i}}$ in the squeeze of mediums such as interpretability. ## References\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6000-6010, Red Hook, NY, USA, 2017.",
    "rrwkv-6": "Curran Associates Inc. ISBN 9781510860964. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics, 2019. doi $10.18653 / \\mathrm{v} 1 / \\mathrm{n} 19-1423$. URL https://doi.org/10.18653/v1/n19-1423. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-1780, nov 1997. ISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014. Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions.",
    "rrwkv-7": "Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 6(2):107-116, apr 1998.",
    "rrwkv-8": "ISSN 0218-4885. doi 10.1142/S0218488598000094 URL https://doi.org/10.1142/S0218488598000094\nPhong Le and Willem Zuidema. Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs.",
    "rrwkv-9": "In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 87-93, Berlin, Germany, August 2016. Association for Computational Linguistics. doi 10.18653/v1/W16-1610 URL https://aclanthology.org/W16-1610\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023. Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer, 2021. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2016. Alex Graves. Generating sequences with recurrent neural networks, 2014. David R. So, Wojciech Ma\u0144ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer: Searching for efficient transformers for language modeling, 2022. Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4470-4481, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi 10.18653/v1/D18-1477. URL https://aclanthology.org/D18-1477\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks.",
    "rrwkv-10": "In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7132-7141, 2018. doi 10.1109/CVPR.2018.00745. Fakultit Informatik, Y. Bengio, Paolo Frasconi, and Jfirgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 032003. Yiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. Trar: Routing the attention spans in transformer for visual question answering. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2054-2064, 2021. doi 10.1109/ICCV48922.2021.00208. [^0]:    * the first author\n\n"
}