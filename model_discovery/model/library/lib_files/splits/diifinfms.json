{
    "diifinfms-0": "# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.",
    "diifinfms-1": "A visual representation of these dimensions is given in Appendix A.",
    "diifinfms-2": "We use superscripts, e.g. . ${ }^{d}$, to denote the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. ${ }_{i}$, to denote the time index (or input dependency). Specifically, $v_{i}$ represents the value of vector $v$ at time $i$. We use bold notation to indicate sequences, i.e., $\\mathbf{v}_{i}=\\left[v_{1}, \\ldots, v_{i}\\right]$. We use $\\sigma(\\cdot)$ to denote is the sigmoid function. The products $\\odot$ and $\\otimes$ denote the Hadamard (element-wise) product and the Kronecker (block-wise) product, respectively. $\\mathbb{I}_{n}$ denotes the identity matrix of size $\\mathbb{R}^{n \\times n}$. Generally, we omit stating the bias term for weight matrices unless stating the bias term helps with clarity. ## 2 Preliminaries\n\nIn this section, we introduce the key architectural components studied in this work: attention, SSMs, and RNNs. We remark that these components are often the central block - considered to be the backbone - within a complex architecture composed of other blocks and skip connections (see for instance Touvron et al., 2023]). In what follows, we review exclusively the backbone block, which\nwe denote as $f(\\cdot)$ in $\\mathbf{y}=f(\\mathbf{u})$, where $\\mathbf{u} \\in \\mathbb{R}^{L \\times d}$ and $\\mathbf{y} \\in \\mathbb{R}^{L \\times d}$ are the input and output sequences, respectively. ### 2.1 Attention\n\nThe standard self-attention block [Vaswani et al. 2017] consists of three matrices: $W_{Q}, W_{K}$, and $W_{V}$, which are the learnt parameters of the model. These matrices, when multiplied with the input $\\mathbf{u}$, yield the queries $\\mathbf{q} \\in \\mathbb{R}^{d_{k}}$, keys $\\mathbf{k} \\in \\mathbb{R}^{d_{k}}$, and values $\\mathbf{v} \\in \\mathbb{R}^{d_{v}}$, respectively:\n\n$$\n\\mathbf{q}=\\mathbf{u} W_{Q}, \\quad \\mathbf{k}=\\mathbf{u} W_{K}, \\quad \\mathbf{v}=\\mathbf{u} W_{V}\n$$\n\nKeys, queries, and values are then combined in the attention block to produce the output\n\n$$\n\\mathbf{y}=\\zeta\\left(\\frac{\\mathbf{q} \\mathbf{k}^{\\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{v}\n$$\n\nwhere $\\zeta(\\cdot)$ is a map $\\mathbb{R}^{L} \\rightarrow \\mathbb{R}^{L}$ and is applied row-wise. In the standard version of attention - softmax attention $-\\zeta(\\cdot):=\\operatorname{softmax}(\\cdot)$. However, given the limitations of the softmax function, alternative formulations have been proposed. In this work, we consider two formulations of attention: softmax attention (2) and linear attention [Katharopoulos et al. 2020]. We focus on the masked attention formulations, i.e., the attention matrix $\\zeta\\left(\\mathbf{q k}^{\\top}\\right)$ has a lower-triangular structure. Furthermore, in order to simplify the derivations, we drop the scaling factor $\\sqrt{d_{k}}$. ### 2.2 State Space Models\n\nArchitectures based on a state space parametrization compute the output y through a dynamic recurrence of input signals at each time step $i$,\n\n$$\n\\begin{aligned}\nh_{i} & =A_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i}$ is the hidden state of the system, and the dynamic matrices of appropriate dimensions $A_{i}, B_{i}, C_{i}, D_{i}$ are the learnt parameters of the model. Different time-varying and time-invariant parameterizations for $A_{i}, B_{i}, C_{i}, D_{i}$ have been proposed in the literature (an overview is given in (Amo Alonso et al., 2024]).",
    "diifinfms-3": "Here we discuss the most prominent one. S6. The first selective SSM parametrization (S6) was introduced together with the Mamba architecture Gu and Dao, 2023]. The S6 block parametrizes the recurrence as\n\n$$\nA_{i}=e^{-\\Delta_{i} A}, \\quad B_{i}=\\Delta_{i} W_{B} u_{i}, \\quad C_{i}=W_{C} u_{i}, \\quad D_{i}=W_{D} u_{i}\n$$\n\nwith $\\Delta_{i}=\\operatorname{softplus}\\left(W_{\\Delta}\\left(W_{u} u_{i}\\right)+b_{\\Delta}\\right)$ for every $i, W_{\\Delta}, W_{u}, W_{B}, W_{C}, W_{D}, A$ are learnt matrices of appropriate dimensions, and $b_{\\Delta}$ is a learnt bias. While SSM models allow for complex-valued matrices $A_{i}, B_{i}, C_{i}, D_{i}$, here we restrict ourselves to real-valued matrices as in Gu and Dao, 2023]. ### 2.3 Recurrent Neural Networks\n\nSimilar to SSMs, RNNs also parameterize the input-output relationship via a recurrent computation, commonly given by the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997], at each time step $i$\n\n$$\n\\begin{aligned}\nx_{i} & =f_{i} \\odot x_{i-1}+i_{i} \\odot \\bar{u}_{i} \\\\\ny_{i} & =o_{i} \\odot \\tanh \\left(x_{i}\\right)\n\\end{aligned}\n$$\n\nwhere $\\bar{u}_{i}$ represents the pre-processed raw input $u_{i}$, i.e.,\n\n$$\n\\bar{u}_{i}=\\tanh \\left(W_{u} u_{i}+U_{u} y_{i-1}\\right)\n$$\n\nand $f_{i}, i_{i}$, and $o_{i}$ are the forget gate, the input gate, and the output gate, respectively,\n\n$$\nf_{i}=\\sigma\\left(W_{f} u_{i}+U_{f} y_{i-1}\\right), \\quad i_{i}=\\sigma\\left(W_{i} u_{i}+U_{i} y_{i-1}\\right), \\quad o_{i}=\\sigma\\left(W_{o} u_{i}+U_{o} y_{i-1}\\right)\n$$\n\nwhere $W_{f}, W_{i}, W_{o}$ and $U_{f}, U_{i}, U_{o}$ are the learnt gate parameters. In this paper, we focus on two variants: quasi LSTMs (qLSTM) [Stani\u0107 et al. 2023], which removes the output dependence of the gates, and RG-LRU [De et al. 2024], which attempts to integrate ideas from SSMs into RNNs. qLSTM. The qLSTM model is parameterized by recurrence (5) with pre-processed input $\\bar{u}_{i}$ and gates $f_{i}, i_{i}, o_{i}$ :\n\n$$\n\\bar{u}_{i}=\\tanh \\left(W_{u} u_{i}\\right), \\quad f_{i}=\\sigma\\left(W_{f} u_{i}\\right), \\quad i_{i}=\\sigma\\left(W_{i} u_{i}\\right), \\quad o_{i}=\\sigma\\left(W_{o} u_{i}\\right)\n$$\n\nRG-LRU. The RG-LRU model presents a hybrid between a qLSTM and a SSM. The recurrence is given as\n\n$$\n\\begin{aligned}\n& x_{i}=a_{i} \\odot x_{i-1}+\\sqrt{1-a_{i}^{2}} \\odot\\left(i_{i} \\odot u_{i}\\right) \\\\\n& y_{i}=x_{i}\n\\end{aligned}\n$$\n\nwith the following gates and no pre-processing of $u_{i}$ :\n\n$$\nr_{i}=\\sigma\\left(W_{a} u_{i}\\right), \\quad i_{i}=\\sigma\\left(W_{u} u_{i}\\right), \\quad a_{i}=e^{-c r_{i} \\odot \\operatorname{softplus}(\\Lambda)}\n$$\n\n## 3 Dynamical Systems Framework for Architecture Comparison\n\nIn this section, we introduce the Dynamical Systems Framework (DSF) that allows in-depth analysis of the architectural features of attention, SSMs, and RNNs from a dynamical systems perspective. We use this to rewrite the parametrizations in a common framework and provide detailed comparisons. ### 3.1 Dynamical Systems Framework (DSF)\n\nThe DSF relies on a dynamical systems representation of the architectures. As is standard in RNN and SSM literature, we choose a recurrent state space representation. In particular, a linear structured time-varying (LTV) dynamical system is defined by the recurrence\n\n$$\n\\begin{aligned}\nh_{i} & =\\Lambda_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i} \\in \\mathbb{R}^{N}$ is the hidden state initialized with $h_{-1}=0, \\Lambda_{i} \\in \\mathbb{R}^{N \\times N}$ is the diagonal state transition matrix, $B_{i} \\in \\mathbb{R}^{N \\times d}$ and $C_{i} \\in \\mathbb{R}^{d \\times N}$ are the input and output matrices, respectively, and $D_{i} \\in \\mathbb{R}^{d \\times d}$ is a scaled skip connection. Dynamical system (11) can alternatively be written in its convolutional representation, i.e., $\\mathbf{y}=\\boldsymbol{\\Phi u}$, where the convolutional kernel $\\boldsymbol{\\Phi}$ is defined as\n\n$$\n\\boldsymbol{\\Phi}=\\left[\\begin{array}{cccc}\nC_{0} B_{0}+D_{0} & & & \\\\\nC_{1} \\Lambda_{1} B_{0} & C_{1} B_{1}+D_{1} & & \\\\\n\\vdots & \\ddots & \\ddots & \\\\\nC_{L} \\prod_{k=1}^{L} \\Lambda_{k} B_{0} & \\ldots & C_{L} \\Lambda_{L} B_{L-1} & C_{L} B_{L}+D_{L}\n\\end{array}\\right]\n$$\n\nNote that the convolution kernel $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{L \\times L}$ is of the same dimension as the attention matrix $\\zeta\\left(\\mathbf{q} \\mathbf{k}^{\\top}\\right)$ and that these matrices are equivalent, up to the scaling factor $W_{V}$ used in self-attention.",
    "diifinfms-4": "Remark 1. This recurrent view yields a causal convolution kernel by definition. However, certain models (e.g. non-masked attention) also use non-causal kernels. This can be incorporated in the DSF (11) by modifying the state update 11a). For the sake of simplicity and consistency with recent literature (RNNs, SSMs), we stick with causal models in the following. ### 3.2 Architecture Reformulation\n\nIn the following, we show how popular architectures based on attention, SSMs, and RNNs can be rewritten into the DSF. To do this, all models will be reformulated into recurrence (11, i.e., all resulting DSF representations will have hidden state dimension $N{ }^{2}$ Although the parametrization of models commonly found in the literature is conductive to efficient computation, here we depart from this convention. The goal of the DSF reformulation is to establish a theoretical framework that leads us to mathematical insights on the design of these models. The presented formulations are not intended to be computationally implemented in DSF form, however the framework can be used to identify computational algorithms for new architectures as mentioned in the introduction. [^1]\n### 3.2.1 Attention\n\nIn the following, we assume that we can separate the map in as\n\n$$\n\\zeta\\left(q_{i}^{\\top} k_{j}\\right)=\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\n$$\n\nwhere $\\phi(\\cdot): \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}, \\psi(\\cdot): \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$, and $\\eta(\\cdot, \\cdot): \\mathbb{R}^{m} \\times \\mathbb{R}^{m \\times(i+1)} \\rightarrow \\mathbb{R}$, which is the case for all the considered architectures in this paper. This allows us to write the self-attention input-output relationship as\n\n$$\ny_{i}=\\sum_{j=0}^{i} \\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} W_{V} u_{j}\n$$\n\nwith $q_{i}=W_{Q} u_{i} \\in \\mathbb{R}^{m}, k_{j}=W_{V} u_{j} \\in \\mathbb{R}^{m}$, and $W_{Q} \\in \\mathbb{R}^{m \\times d}, W_{K} \\in \\mathbb{R}^{m \\times d}, W_{V} \\in \\mathbb{R}^{d \\times d}$. Hence, equation 14) can be reformulated into the DSF 11) as a dynamical system of dimension $N=n d$, i.e., with hidden state $h_{i} \\in \\mathbb{R}^{n d}$, and dynamic matrices\n\n$$\n\\begin{aligned}\n\\Lambda_{i} & =\\frac{\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} \\mathbb{I}_{n d} \\in \\mathbb{R}^{n d \\times n d} \\\\\nB_{i} & =\\left(\\frac{1}{\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{j}\\right)\\right) W_{V} \\in \\mathbb{R}^{n d \\times d} \\\\\nC_{i} & =\\mathbb{I}_{d} \\otimes \\phi\\left(q_{i}\\right)^{\\top} \\in \\mathbb{R}^{d \\times n d}\n\\end{aligned}\n$$\n\nWe note that for the recurrence 11), the matrix $\\Lambda_{i}$ is given as an $n d \\times n d$ matrix, where $n$ is the number of features in $\\phi$ and $\\psi$, and $d$ is the input dimension. However, due to the structure of $\\Lambda_{i}$ as given in 15a, it can be implemented as a scalar multiplication $\\frac{\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} h_{i-1}$ in 11). Hence, the hidden state is never materialized as such in the computation of the attention scores. Interested readers are referred to Appendix B for a detailed derivation. Linear Attention. In the case of linear attention, both maps $\\phi(\\cdot)$ and $\\psi(\\cdot)$ in the DSF parametrization (15) are linear in their arguments and we use the kernel proposed in Katharopoulos et al. 2020, i.e.,\n\n$$\n\\phi\\left(q_{i}\\right)=\\operatorname{elu}\\left(q_{i}\\right)+1, \\quad \\psi\\left(k_{j}\\right)=\\operatorname{elu}\\left(k_{j}\\right)+1, \\quad \\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)=\\left(\\operatorname{elu}\\left(q_{i}\\right)+1\\right) \\sum_{j=0}^{i}\\left(\\operatorname{elu}\\left(k_{j}\\right)+1\\right)\n$$\n\nwhere elu stands for exponential linear unit. Generalized Linear Attention. We also study here the generalized linear attention, where we simply require that $\\phi(\\cdot)$ and $\\psi(\\cdot)$ be linear in their arguments; but we do not require that the normalization function $\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)$ is linear as well. Softmax Attention. Softmax attention also satisfies the assumption of separability (13). However, it holds that the feature vector representation of the transformed Gaussian kernel in the softmax function, i.e., $e^{q_{i}^{\\top} k_{j}}$, is infinite dimensional. Hence, the DSF representation (15) of softmax attention (2) and its corresponding hidden state dimension $N$ would also be infinite dimensional. This insight gives further motivation to approximations of the softmax function by using, e.g., a Taylor series approximation such as in [Nauen et al., 2024], to render the feature vector finite-dimensional. Lemma 1. Softmax attention (2) can be expressed by separable attention (13) with\n\n$$\n\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)=\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)=e^{q_{i}^{\\top} k_{j}}, \\quad \\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)=\\sum_{j=0}^{i} e^{q_{i}^{\\top} k_{j}}\n$$\n\nwhere $\\phi\\left(q_{i}\\right):=c \\cdot\\left[1, q_{i}, \\bigotimes_{j=1}^{2} q_{i}, \\bigotimes_{j=1}^{3} q_{i}, \\ldots\\right]$ is an infinite-dimensional feature vector and $c$ is a matrix of constant coefficients. Proof. The exponential in the softmax attention $e^{q_{i}^{\\top} k_{j}}$ can be expressed in terms of its Taylor expansion, which consists of an infinite sum of polynomial kernels of increasing degree $p$, decomposable through the vectors of monomials $\\bigotimes_{j=1}^{p} q_{i}$. See Appendix C for a complete proof. ### 3.2.2 State Space Models\n\nSSM models are straightforward to rewrite in the DSF given their intrinsic recurrent linear representation. However, similarly to attention, we slightly rewrite the standard representation introduced in the literature in order to reveal deeper insights obscured by the standard representation focused on computational efficiency. The detailed derivation can be found in Appendix D\n\nS6. The S6 parametrization can be written in the DSF 11) as\n\n$$\n\\begin{aligned}\n& \\Lambda_{i}=e^{-\\left(\\Delta_{i} \\otimes \\mathbb{I}_{n}\\right) \\odot A} \\in \\mathbb{R}^{n d \\times n d} \\\\\n& B_{i}=\\Delta_{i} \\otimes b_{i} \\in \\mathbb{R}^{n d \\times d} \\\\\n& C_{i}=\\mathbb{I}_{d} \\otimes c_{i}^{\\top} \\in \\mathbb{R}^{d \\times n d}\n\\end{aligned}\n$$\n\nwith $\\Delta_{i}=\\operatorname{diag}\\left(\\operatorname{softplus}\\left(W_{\\Delta}\\left(W_{u} u_{i}\\right)+b_{\\Delta}\\right)\\right) \\in \\mathbb{R}^{d \\times d}, b_{i}=W_{B} u_{i} \\in \\mathbb{R}^{n}, c_{i}=W_{C} u_{i} \\in \\mathbb{R}^{n}$, and $W_{u} \\in \\mathbb{R}^{p \\times d}$, $W_{\\Delta} \\in \\mathbb{R}^{d \\times p}$ are weight matrices with $p<d$, and $b_{\\Delta} \\in \\mathbb{R}^{d}$ is a bias. Note that in formulation (18) the dimensions of the matrices are $\\Lambda_{i} \\in \\mathbb{R}^{n d \\times n d}, B_{i} \\in \\mathbb{R}^{n d \\times d}$, $C_{i} \\in \\mathbb{R}^{d \\times n d}$, i.e., $n$ is the state dimension and $d$ is the input dimension in the original formulation (3). ### 3.2.3 Recurrent Neural Networks\n\nGiven their recurrent nature, one can express LSTMs (5) in the DSF with some basic algebraic manipulations (see Appendix Efor details). Once again, we slightly rewrite the standard representation since our goal is to obtain mathematical insights as opposed to computational efficiency. qLSTM. In order to write the qLSTM formulation (5) in the DSF (11), a small modification is needed. In particular, the tanh functions in the input pre-processing (8) and output gate (5b) need to be dropped. Hence, the reformulated qLSTM in the DSF 11) writes as\n\n$$\n\\begin{aligned}\n& \\Lambda_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{f} u_{i}\\right)\\right) \\in \\mathbb{R}^{d \\times d} \\\\\n& B_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{i} u_{i}\\right)\\right) \\odot W_{u} \\in \\mathbb{R}^{d \\times d} \\\\\n& C_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{o} u_{i}\\right)\\right) \\in \\mathbb{R}^{d \\times d}\n\\end{aligned}\n$$\n\nwhere $W_{f}, W_{i}, W_{o}, W_{u} \\in \\mathbb{R}^{d \\times d}$ are the learnt parameters in 8. It is important to note that here the dimension of the hidden state $h_{i}$ is equal to the number of input channels $d$. This is in contrast with attention and SSMs, where the dimension of the hidden state $h_{i}$ in the DSF 11] is $n d$. In the case of qLSTMs, $n=1$. This fact will become relevant in further discussions; we refer to the fact that $n>1$ as state expansion. RG-LRU. Given the similarities of the RG-LRU model [De et al. 2024] with a SSM, it is straightforward to reformulate it into the DSF (11) without the need for modifications besides simple algebraic manipulations. Hence, the RG-LRU can be expressed in the DSF as\n\n$$\n\\begin{aligned}\n\\Lambda_{i} & =e^{-c r_{i} \\odot \\text { softplus }(A)} \\in \\mathbb{R}^{d \\times d} \\\\\nB_{i} & =\\sqrt{1-\\Lambda_{i}^{2}} \\odot \\operatorname{diag}\\left(\\sigma\\left(W_{B} u_{i}\\right)\\right) \\in \\mathbb{R}^{d \\times d} \\\\\nC_{i} & =\\mathbb{I}_{d}\n\\end{aligned}\n$$\n\nwhere $r_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{R} u_{i}\\right)\\right)$, and the function $\\sqrt{1-\\Lambda_{i}^{2}}$ is applied elementwise to $\\Lambda_{i}$. Similar to the qLSTM and in contrast with the other models, RG-LRU does not have state expansion, i.e. $n=1$. ## 4 Architecture Comparison: Theoretical and Experimental Results\n\nIn this section, we use the DSF to explore some of the long-standing questions between SSMs, attention, and RNNs. We provide theoretical results and/or numerical experiments to substantiate our claims. All experiments are performed on the multi-query associate recall (MQAR) benchmark Arora et al. 2023] using the code base provided with the benchmark $\\sqrt[3]{3}$ We set the vocab size in all MQAR tasks to 8,192 and report the best result for a learning rate sweep in np.logspace $(-4,-2,4)$. Complete experimental setup and computational resources used are in Appendix Iand respectively. [^2]\n### 4.1 Softmax Attention vs. Separable Attention. Separable attention is used to avoid computation of the query-key matrix $\\mathbf{q} \\mathbf{k}^{\\top}$. It allows to compute $\\mathbf{k}^{\\top} \\mathbf{v}$ before multiplying the queries $\\mathbf{q}$, which reduces the computational complexity from quadratic to linear in sequence length. The DSF makes apparent how a necessary condition for separability is for the map $\\zeta(\\cdot)$ to be expressed by a finite-dimensional kernel. However, in the case of softmax $(\\cdot)$, an infinite-dimensional kernel is needed, i.e., in the DSF softmax attention requires $n=\\infty$. This insight can mathematically explain why the good performance observed in softmax attention can only be approximated by separable attention mechanisms, SSMs, or RNNs; but no other architecture is equivalent. The DSF predicts that softmax can be better approximated by growing $n$, which we show in the following theoretical result:\nLemma 2. For two dynamical systems (11) with hidden state dimensions $N$ and $\\bar{N}$ with $N \\leq \\bar{N}$, the dynamical system of state dimension $N$ can always recover the dynamical system with state dimension $N$. Proof. The result follows from the fact that the first $N$ states and the output in 11) can be chosen to be independent of the additional states. The proof is given in Appendix H. ![](https://cdn.mathpix.com/cropped/2024_09_12_5bc3ed65c8ad8740f705g-07.jpg?height=407&width=1399&top_left_y=939&top_left_x=360)\n\nFigure 1: Comparison of linear attention and softmax attention on two MQAR tasks $\\{(L=$ 256, KV-pairs $=16),(L=512$, KV-pairs $=64)\\}$, fixed model size $d=512$, and varying state expansion $n$. We report the best result from a learning rate sweep in $n \\mathrm{np}$.logspace $(-4,-2,4)$. Since $N=n d$, this means that the larger the state expansion $n$, the more expressivity the architecture has - in the case of softmax attention (2), $n=\\infty$ (Lemma 1). To show expressivity with increasing $n$ empirically, we compare linear attention (16, for which it holds that $n=m$, and softmax attention on the MQAR. Figure 1 shows that with larger state expansion linear attention converges to the performance of softmax attention, which achieves perfect accuracy throughout. ### 4.2 Generalized Linear Attention vs. S6. For this comparison, we assume a generalized linear attention with $\\phi\\left(q_{i}\\right)=q_{i}, \\psi\\left(k_{j}\\right)=k_{j}$, and general normalization function $\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)$. By comparing the DSF expressions for both Generalized Linear Attention (15) and S6 (18), one can immediately see that the S6 parameters $b_{i}=W_{B} u_{i} \\in$ $\\mathbb{R}^{n}, c_{i}=W_{C} u_{i} \\in \\mathbb{R}^{n}$ directly correspond with keys and queries in attention, i.e. $k_{i}=b_{i}$ and $q_{i}=c_{i}$. Moreover, the state expansion $n$ in S 6 is the same as the hidden dimension in attention. However, while this leads to an equivalent output matrix $C_{i}$ in the DSF parametrization for both architectures, there are remarkable differences between the two:\n\n- Number of parameters. The transition matrix $\\Lambda_{i}$ has $d$ parameters in S6 18) and only 1 in attention. In attention (15), $\\eta\\left(q_{i}, \\mathbf{k}\\right)$ is the only parameter in $\\Lambda_{i}$, and it is by definition a scalar. In S6, the parameters in $\\Lambda_{i}$ are determined by $\\Delta_{i} \\in \\mathbb{R}^{d}$, which has $d$ different parameters. However, it was shown in [Dao and Gu, 2024] that the number of parameters in $\\Lambda_{i}$ can be reduced to $1-$ similar to attention - without compromising performance. - Normalization strategy vs Discretization step. In attention (15), a normalization map $\\eta(\\cdot)$ is used. This map enters as a fraction in $\\Lambda_{i}$ and also as a denominator in $B_{i}$. Given that this map is a scalar, these two cancel out when computing the output, as one can see in the convolution representation (12). Hence, in attention $\\prod_{k=j}^{i} \\Lambda_{k} B_{j}$ evaluates to $\\frac{1}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}$. Notice that this does not occur in S6 (18), since the only shared parameter - discretization\nstep $\\Delta_{i}$ - does not cancel out in $\\Lambda_{i}$ and $B_{i}$ given their different structure. This impacts the selectivity of the matrices on the input, since some input-dependent features are normalized differently in the two architectures. We remark that multi-headed attention increases the number of parameters in $\\Lambda_{i}$ from 1 to the number of heads $s$; for more details see Appendix F. While the number of parameters in the state transition $\\Lambda_{i}$ does play a role in increasing performance (multi-headed attention typically performs better than single-headed attention [Vaswani et al., 2017]), the results in [Dao and Gu, 2024] suggest that this role is small. The larger influence thus lies in the recursive structure of $\\Lambda_{i}$ and $B_{i}$ and/or the parameterization of normalization $\\eta(\\cdot)$. To further investigate this and the role of normalization in attention, we compare S6 and softmax attention to SSD [Dao and Gu, 2024], linear attention Katharopoulos et al. 2020], and normalized attention on the multi-query associative recall (MQAR) benchmark [Arora et al., 2023] and train the three attention-based methods on WikiText-103. Inspired by S6, we define normalized attention as the attention function\n\n$$\n\\phi\\left(q_{i}\\right)=q_{i}, \\quad \\psi\\left(k_{j}\\right)=k_{j}, \\quad \\eta\\left(u_{i}\\right)=e^{W_{\\eta} u_{i}}\n$$\n\nwhere $W_{\\eta} \\in \\mathbb{R}^{1 \\times d}$ is an additional learnt parameter. The MQAR results are shown in Figure 2 and the training perplexity on WikiText-103 in Table 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_5bc3ed65c8ad8740f705g-08.jpg?height=424&width=787&top_left_y=956&top_left_x=363)\n\nFig. 2: Model accuracy with increasing model size $d$ for different models: softmax, linear, and normalized attention, S6, and SSD. The MQAR task is $(L=512$, KV-pairs $=64$ ), we fix $n=128$, and report the best performance of a learning rate sweep in np .logspace $(-4,-2,4)$. ![](https://cdn.mathpix.com/cropped/2024_09_12_5bc3ed65c8ad8740f705g-08.jpg?height=318&width=557&top_left_y=958&top_left_x=1185)\n\nTable 1: Training perplexity score for different Attention architectures ( 70 M params) on the WikiText-103 corpus. ### 4.3 RNNs vs. S6\n\nIn the comparison of RNNs and S6 it is immediate to observe several similarities. In particular, as shown in Appendix G, the state transition matrix $\\Lambda_{i}$ in S 6 18) can be rewritten when assuming $A=a \\cdot \\mathbb{I}_{n d}$ as\n\n$$\n\\Lambda_{i}=\\operatorname{diag}\\left(\\sigma_{\\mathrm{rev}}\\left(\\bar{W}_{\\Delta} u_{i}\\right)^{a}\\right) \\otimes \\mathbb{I}_{n}\n$$\n\nNotice that when no state expansion is considered, i.e., $n=1$ and $\\mathbb{I}_{n}=1$, this expression almost coincides with the qLSTM state transition 19a, with the only difference that (I) it uses the reversed sigmoid function instead of a sigmoid for the forget gate, and (II) there is an additional learnt parameters $a$ in the exponent. As shown in the recent xLSTM paper [Beck et al., 2024], state expanded LSTM $s^{4}$ can yield similar performance to $S 6$. This aligns with Lemma 2 , and further highlights the importance of state expansion for expressivity. In the case of qLSTM and RGLRU, state expansion can be easily incorporated by changing the dimensions of the projections $W_{f}, W_{i}, W_{o}$, where the $\\odot$ operation in RG-LRU would be replaced by blockwise operations $\\otimes$. Finally, the most apparent difference between the two RNN variants qLSTM and RG-LRU and S6 is the parameter coupling in $\\Lambda_{i}$ and $B_{i}$. While qLSTM does not use a coupling, the couplings in RG-LRU and S6 are performed with different nonlinearities, which is discussed in more detail in [De et al., 2024, Appendix A]. Inspired by the subtle difference in the state transition, we compare the original qLSTM state transition and the S6 state transition on the MQAR benchmark. Specifically, we compare qLSTM 85 and\n\n[^3]qLSTM with only the state transition replaced by 22. The performance of both models is shown in Figure 3 We note that the reversed sigmoid state transition outperforms the original state transition on all three benchmark tasks, i.e., the performance of qLSTMs can be improved by insights from S6. ![](https://cdn.mathpix.com/cropped/2024_09_12_5bc3ed65c8ad8740f705g-09.jpg?height=383&width=1412&top_left_y=408&top_left_x=355)\n\nFigure 3: Comparison of qLSTM (8) and a qLSTM variant where the original state transition $\\Lambda_{i}$ is replace by $(22)$. ## 5 Related Work\n\nState-space models emerged from the $S 4$ architecture by Gu et al. [2022a], who developed a new theoretically principled approach to sequence modeling rooted in polynomial approximation theory |Gu et al., 2020]. The result is a transformer-like architecture [Vaswani et al., 2017], where attention is replaced by a linear recurrent neural network with special reparametrization. The design of S4 got later simplified in [Gupta et al., 2022, Gu et al., 2022b], who achieve state-of-the-art on the long-range arena (LRA) [Tay et al., 2021] with a highly efficient recurrent mechanism leveraging convolutional views [Li et al., 2022], or parallel scans [Smith et al., 2023, Orvieto et al., 2023]. The high efficiency of SSMs (linear processing) makes them particularly appealing when compared to softmax attention-based transformers, where both inference time and memory suffer quadratically from sequence length. The S4 architecture found first successful applications in reinforcement learning [Lu et al., 2023], vision [Nguyen et al., 2022], audio [Goel et al., 2022] as well as online learning [Zucchet et al.| 2023b]. Initial attempts in language modeling [Fu et al., 2023, Wang et al., 2022], supported by theoretical investigations [Orvieto et al., 2024. Wang and Xue 2023] hint at some necessary architectural improvements to unlock the NLP domain. Leveraging in-context learning arguments, a few works [Peng et al., 2023, Sun et al., 2023, Katsch, 2023] started incorporating input selectivity mechanisms [Olsson et al., 2022] into SSMs. These efforts culminated in the Mamba architecture [Gu and Dao, 2023], which proposed an highly efficient and light (in terms of parameters) input selectivity strategy, with drastic improvements when comparing to earlier variants ( $\\mathrm{H} 3[\\mathrm{Fu}$ et al., 2023] and Hyena [Poli et al., 2023]) on text. This approach was also shown to be effective at byte level [Wang et al., 2024]. Beyond text, Mamba was recently applied to the vision domain [Liu et al. 2024b, Zhu et al., 2024] - with outstanding results compared to ViTs [Touvron et al., 2022] both in terms of performance and efficiency. Other applications include e.g. genetics [Schiff et al. 2024], and point clouds [Liu et al., 2024a]. Further, improvements on architectural aspects were proposed in Yang et al. 2023, Dao and Gu, 2024, Qin et al. 2024. The design of Mamba is also strongly supported by theoretical evidence showing precisely its superior expressive power compared to S 4 [Cirone et al., 2024]. This boost in computational power is due to Mamba's novel input selectivity mechanism resembling gating, which unlocks content-dependent reasoning [Gu and Dao, 2023, Olsson et al., 2022]. Interestingly, input selectivity brings SSMs closer to attention: in particular, Ali et al. [2024] showed that the particular parametrization of Mamba can be linked to a non-normalized softmax operator. This finding is also supported by evidence from language theory - Mamba and Attention can solve a similar class of problems Merrill et al., 2024]. Beyond Ali et al. [2024] the connection between linear attention and linear RNNs has been illustrated a few times in the literature Katharopoulos et al., 2020, Schlag et al., 2021, Zucchet et al. 2023a]. Compared to these works and to Ali et al.[[2024], we offer here a more careful comparison identifying some precise distinctions between Mamba, linear, and softmax attention - which play a nontrivial role in practice and can help bring to light interesting architectural improvements. ## 6 Conclusion\n\nIn this paper we presented the DSF, a framework based on dynamical-systems theory that allows analysis of different deep learning architectures by writing them as linear recurrences in state space. We first showed how to reformulate different architectures into the DSF, and then explored (theoretical and computational) insights resulting from this analysis. For instance, we showed that with proper normalization the performance of linear attention can be significantly increased (see Fig. 2). We also show, that the DSF allows to integrate insights from one architecture to another as exemplified by Section 4.2 Lastly, the DSF can provide a deeper theoretical understanding on the relationship between softmax attention and other models. We expect that the DSF can serve as a tool for principled analysis and design of deep learning architectures. Limitations. In terms of the limitations of the DSF, it is important to highlight that, while the DSF parametrization allows for a principled comparison between frameworks, architectures in DSF do not necessarily enjoy an efficient implementation unless their specific structure can leverage some of the existing algorithms (parallel scan, etc.).",
    "diifinfms-5": "In terms of the computational experiments, the insights mentioned above are only verified on a synthetic task (MQAR) and for some on a small size language task (WikiText-103). To strengthen the insights, a more in-depth analysis is needed on a larger and more complex language task. ## Acknowledgements\n\nWe would like to thank Imanol Schlag and Volodymyr Kyrylov for their valuable feedback and discussions on LSTMs and qLSTMs. We also thank Volodymyr Kyrylov for his help with the Hippogriff codebase ${ }^{5}$\n\n## References\n\nAmeen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models.",
    "diifinfms-6": "arXiv preprint arXiv:2403.01590, 2024. Carmen Amo Alonso, Jerome Sieber, and Melanie N Zeilinger. State space models as foundation models: A control theoretic overview.",
    "diifinfms-7": "arXiv preprint arXiv:2403.16899, 2024. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and Improving Recall in Efficient Language Models.",
    "diifinfms-8": "arXiv:2312.04927, 2023. Maximilian Beck, Korbinian P\u00f6ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xLSTM: Extended Long Short-Term Memory.",
    "diifinfms-9": "arXiv preprint arXiv:2405.04517, 2024. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR, 2023. Guy E Blelloch. Prefix sums and their applications.",
    "diifinfms-10": "1990. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\n\n[^4]Dario Amodei. Language Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical foundations of deep selective state-space models. arXiv preprint arXiv:2402.19047, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.",
    "diifinfms-11": "2023. Tri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms with Structured State Space Duality.",
    "diifinfms-12": "In ICML 2024, 2024. Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models, 2024.",
    "diifinfms-13": "URL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.",
    "diifinfms-14": "In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org, 2020. Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023. Volodymyr Kyrylov. Accelerated Scan, January 2024. URL https://github.com/proger/ accelerated-scan. Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. Jiuming Liu, Ruiji Yu, Yian Wang, Yu Zheng, Tianchen Deng, Weicai Ye, and Hesheng Wang. Point mamba: A novel point cloud backbone based on state space model with octree-based ordering strategy. arXiv preprint arXiv:2403.06467, 2024a. Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model.",
    "diifinfms-15": "arXiv preprint arXiv:2401.10166, 2024b. Daniel L\u00f3pez-S\u00e1nchez, Ang\u00e9lica Gonz\u00e1lez Arrieta, and Juan M Corchado. Data-independent random projections from the feature-space of the homogeneous polynomial kernel. Pattern Recognition, $82: 130-146,2018$. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 2023. William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. Tobias Christian Nauen, Sebastian Palacio, and Andreas Dengel. Taylorshift: Shifting the complexity of self-attention from squared to linear (and back) using taylor-softmax.",
    "diifinfms-16": "arXiv preprint arXiv:2403.02920, 2024. Eric Nguyen, Karan Goel, Albert Gu, Gordon W. Downs, Preey Shah, Tri Dao, Stephen A. Baccus, and Christopher R\u00e9. S4nd: Modeling images and videos as multidimensional signals using state spaces. Advances in Neural Information Processing Systems, 2022. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting Recurrent Neural Networks for Long Sequences. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 26670-26698. PMLR, 23-29 Jul 2023. Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. Universality of linear recurrences followed by non-linear projections: Finite-width guarantees and benefits of complex eigenvalues. International Conference on Machine Learning, 2024. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. HGRN2: Gated Linear RNNs with State Expansion.",
    "diifinfms-17": "arXiv preprint arXiv:2404.07904, 2024. Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling.",
    "diifinfms-18": "arXiv preprint arXiv:2403.03234, 2024. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers.",
    "diifinfms-19": "In International Conference on Machine Learning, pages 9355-9366. PMLR, 2021. Amnon Shashua. Introduction to machine learning: Class notes 67577. arXiv preprint arXiv:0904.3664, 2009. Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for Sequence Modeling. In The Eleventh International Conference on Learning Representations, 2023. Aleksandar Stani\u0107, Dylan Ashley, Oleg Serikov, Louis Kirsch, Francesco Faccio, J\u00fcrgen Schmidhuber, Thomas Hofmann, and Imanol Schlag. The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute, 2023. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A Benchmark for Efficient Transformers. In International Conference on Learning Representations (ICLR), 2021. Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit.",
    "diifinfms-20": "arXiv preprint arXiv:2204.07118, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush. Mambabyte: Token-free selective state space model.",
    "diifinfms-21": "arXiv preprint arXiv:2401.13660, 2024. Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory.",
    "diifinfms-22": "Advances in Neural Information Processing Systems, 2023. Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training.",
    "diifinfms-23": "arXiv preprint arXiv:2312.06635, 2023. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.",
    "diifinfms-24": "2024. Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes Von Oswald, Maxime Larcher, Angelika Steger, and Joao Sacramento. Gated recurrent neural networks discover attention. arXiv preprint arXiv:2309.01775, 2023a.",
    "diifinfms-25": "Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, and Jo\u00e3o Sacramento. Online learning of long-range dependencies, 2023b. ## A Visual Representation of the matrix dimensions\n\nFigure 4 represents the dimensions of the recurrence expressed by the linear structured time-varying (LTV) dynamical system described in (11):\n\n$$\nh_{i}=\\Lambda_{i} h_{i-1}+B_{i} u_{i}\n$$\n\nwhere $h_{i} \\in \\mathbb{R}^{N}$ is the hidden state, $\\Lambda_{i} \\in \\mathbb{R}^{N \\times N}$ is the diagonal state transition matrix, $B_{i} \\in \\mathbb{R}^{N \\times d}$ is the input matrix. We highlight the role of the state expansion, where $u \\in \\mathbb{R}^{d}$ and $h \\in \\mathbb{R}^{N}=\\mathbb{R}^{n d}$. ## B Derivation of Separable Attention into DSF\n\nWe consider the layer\n\n$$\ny_{i}=\\sum_{j=0}^{i} f\\left(q_{i}, k_{j}, \\mathbf{k}_{i}\\right) W_{V} u_{j}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5bc3ed65c8ad8740f705g-14.jpg?height=529&width=1131&top_left_y=234&top_left_x=497)\n\nFigure 4: Visual representation of the dimensions considered in the DSF. where we define the sequence of keys\n\n$$\n\\mathbf{k}_{i}=\\left\\{k_{0}, \\ldots, k_{i}\\right\\}\n$$\n\nWe show that it can be equivalently written as the LTV system 11, i.e.,\n\n$$\ny_{i}=\\sum_{j=0}^{i} C_{i}\\left(\\prod_{k=j+1}^{i} \\Lambda_{k}\\right) B_{j} u_{j}\n$$\n\nwith $\\Lambda_{i} \\in \\mathbb{R}^{n d \\times n d}, B_{i} \\in \\mathbb{R}^{n d \\times d}$ and $C_{i} \\in \\mathbb{R}^{d \\times n d}$, if the function $f(\\cdot, \\cdot)$ can be separated as follows\n\n$$\nf\\left(q_{i}, \\mathbf{k}_{i}\\right)=\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\n$$\n\nwhere $\\phi\\left(q_{i}\\right) \\in \\mathbb{R}^{n}, \\psi\\left(k_{j}\\right) \\in \\mathbb{R}^{n}$, and $\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right) \\in \\mathbb{R}$ can be considered to be a normalization. If the unnormalized part of $f(\\cdot, \\cdot)$ is a kernel, it holds that $\\psi\\left(k_{j}\\right)=\\phi\\left(k_{j}\\right)$ for some, possibly infinite dimensional, feature vector $\\phi$. We can compare the output formulations\n\n$$\ny_{0}=C_{0} B_{0} u_{0}\n$$\n\nand\n\n$$\n\\begin{aligned}\ny_{0} & =f\\left(q_{0}, k_{0}, \\mathbf{k}_{0}\\right) W_{V} u_{0} \\\\\n& =\\phi\\left(q_{0}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{0}, \\mathbf{k}_{0}\\right)} \\psi\\left(k_{0}\\right) W_{V} u_{0}\n\\end{aligned}\n$$\n\nresulting in\n\n$$\nB_{0}=\\left(\\frac{1}{\\eta\\left(q_{0}, \\mathbf{k}_{0}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{0}\\right)\\right) W_{V} \\in \\mathbb{R}^{n d \\times d}, \\quad C_{0}=\\mathbb{I}_{d} \\otimes \\phi\\left(q_{0}\\right)^{\\top} \\in \\mathbb{R}^{d \\times n d}\n$$\n\nSimlarly, we have\n\n$$\n\\begin{aligned}\ny_{1} & =C_{1} B_{1} u_{1}+C_{1} \\Lambda_{1} B_{0} u_{0} \\\\\ny_{1} & =f\\left(q_{1}, k_{1}, \\mathbf{k}_{1}\\right) W_{V} u_{1}+f\\left(q_{1}, k_{0}, \\mathbf{k}_{1}\\right) W_{V} u_{0} \\\\\n& =\\phi\\left(q_{1}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)} \\psi\\left(k_{1}\\right) W_{V} u_{1}+\\phi\\left(q_{1}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)} \\psi\\left(k_{0}\\right) W_{V} u_{0} \\\\\n\\Rightarrow B_{1} & =\\left(\\frac{1}{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{1}\\right)\\right) W_{V}, C_{1}=\\mathbb{I}_{d} \\otimes \\phi\\left(q_{1}\\right)^{\\top} \\text { and } \\Lambda_{1}=\\frac{\\eta\\left(q_{0}, \\mathbf{k}_{0}\\right)}{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)} \\mathbb{I}_{n d}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& y_{2}=C_{2} B_{2} u_{2}+C_{2} \\Lambda_{2} B_{1} u_{1}+C_{2} \\Lambda_{2} \\Lambda_{1} B_{0} u_{0} \\\\\n& y_{2}=f\\left(q_{2}, k_{2}, \\mathbf{k}_{2}\\right) W_{V} u_{2}+f\\left(q_{2}, k_{1}, \\mathbf{k}_{2}\\right) W_{V} u_{1}+f\\left(q_{2}, k_{0}, \\mathbf{k}_{2}\\right) W_{V} u_{0} \\\\\n& y_{2}=\\phi\\left(q_{2}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\psi\\left(k_{2}\\right) W_{V} u_{2}+\\phi\\left(q_{2}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\psi\\left(k_{1}\\right) W_{V} u_{1}+\\phi\\left(q_{2}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\psi\\left(k_{0}\\right) W_{V} u_{0} \\\\\n& \\Rightarrow B_{2}=\\left(\\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{2}\\right)\\right) W_{V}, C_{2}=\\mathbb{I}_{d} \\otimes \\phi\\left(q_{2}\\right)^{\\top} \\text { and } \\Lambda_{2}=\\frac{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\mathbb{I}_{n d}\n\\end{aligned}\n$$\n\nPlugging it back in we can see\n\n$$\n\\begin{aligned}\ny_{2} & =C_{2} B_{2} u_{2}+C_{2} \\Lambda_{2} B_{1} u_{1}+C_{2} \\Lambda_{2} \\Lambda_{1} B_{0} u_{0} \\\\\n& =\\mathbb{I}_{d} \\otimes \\phi\\left(q_{2}\\right)^{\\top}\\left(\\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{2}\\right)\\right) W_{V} u_{2} \\\\\n& +\\mathbb{I}_{d} \\otimes \\phi\\left(q_{2}\\right)^{\\top} \\frac{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\mathbb{I}_{n d}\\left(\\frac{1}{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{1}\\right)\\right) W_{V} u_{1} \\\\\n& +\\mathbb{I}_{d} \\otimes \\phi\\left(q_{2}\\right)^{\\top} \\frac{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\mathbb{I}_{n d} \\frac{\\eta\\left(q_{0}, \\mathbf{k}_{0}\\right)}{\\eta\\left(q_{1}, \\mathbf{k}_{1}\\right)} \\mathbb{I}_{n d}\\left(\\frac{1}{\\eta\\left(q_{0}, \\mathbf{k}_{0}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{0}\\right)\\right) W_{V} u_{0} \\\\\n& =\\phi\\left(q_{2}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\psi\\left(k_{2}\\right) W_{V} u_{2}+\\phi\\left(q_{2}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\psi\\left(k_{1}\\right) W_{V} u_{1}+\\phi\\left(q_{2}\\right)^{\\top} \\frac{1}{\\eta\\left(q_{2}, \\mathbf{k}_{2}\\right)} \\psi\\left(k_{0}\\right) W_{V} u_{0} \\\\\n& =f\\left(q_{2}, k_{2}, \\mathbf{k}_{2}\\right) W_{V} u_{2}+f\\left(q_{2}, k_{1}, \\mathbf{k}_{2}\\right) W_{V} u_{1}+f\\left(q_{2}, k_{0}, \\mathbf{k}_{2}\\right) W_{V} u_{0}\n\\end{aligned}\n$$\n\nThis generalizes the dynamical system matrices to\n\n$$\n\\begin{aligned}\nB_{i} & =\\left(\\frac{1}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} \\mathbb{I}_{d} \\otimes \\psi\\left(k_{i}\\right)\\right) W_{V} \\\\\nC_{i} & =\\mathbb{I}_{d} \\otimes \\phi\\left(q_{i}\\right)^{\\top} \\\\\n\\Lambda_{i} & =\\frac{\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} \\mathbb{I}_{n d}\n\\end{aligned}\n$$\n\n## C Proof of Lemma 1; Derivation of Softmax Attention into DSF\n\nThe function softmax : $\\mathbb{R}^{n \\times m} \\rightarrow(0,1]^{n \\times m}$ is defined through row-wise normalisation and is given by\n\n$$\n\\operatorname{softmax}(\\mathbf{z}):=\\left(\\left[\\begin{array}{ccc}\n\\frac{e^{z_{0}, 0}}{\\sum_{j=0}^{m-1} e^{z_{0, j}}} & \\cdots & \\frac{e^{z_{0}, m}}{\\sum_{j=0}^{m-1} e^{z_{0, j}}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{e^{z_{n, 0}}}{\\sum_{j=0}^{m-1} e^{z_{n, j}}} & \\cdots & \\frac{e^{z_{n, m}}}{\\sum_{j=0}^{m-1} e^{z_{n, j}}}\n\\end{array}\\right]\\right)\n$$\n\nThe attention block is given as\n\n$$\ny_{i}=\\sum_{j=0}^{i} \\zeta_{i, j}\\left(\\mathbf{q}^{\\top} \\mathbf{k}\\right) v_{j}=\\sum_{j=0}^{i} \\operatorname{softmax}_{i, j}\\left(\\mathbf{u} W_{Q} W_{K}^{\\top} \\mathbf{u}^{\\top}\\right) W_{V} u_{j}\n$$\n\nwhere $\\operatorname{softmax}_{i, j}$ refers to the element $(i, j)$ of the corresponding matrix. Note that $e^{q_{i}^{\\top} k_{j}}$ is a kernel, implying that the softmax attention can be brought into the separable form (13). In order to provide the separable form for $e^{q_{i}^{\\top} k_{j}}$, we first consider the Taylor expansion of the kernel, which is given by\n\n$$\ne^{q_{i}^{\\top} k_{j}}=\\sum_{p=0}^{\\infty} \\frac{\\left(q_{i}^{\\top} k_{j}\\right)^{p}}{p!}\n$$\n\nEach polynomial $\\left(q_{i}^{\\top} k_{j}\\right)^{p}$ represents itself a homogeneous polynomial kernel and its decomposition into a feature vector of $\\binom{n+p-1}{p}$ monomials, as shown in [Shashua 2009], is given by\n\n$$\n\\tilde{\\phi}_{p}(x)=\\left[\\sqrt{\\frac{p!}{n_{1}!n_{2}!\\cdots n_{n}!}} x_{1}^{n_{1}} \\cdots x_{n}^{n_{n}}\\right]_{n_{i} \\geq 0, \\sum_{i} n_{i}=p}\n$$\n\nThe feature representation of the exponential kernel is therefore\n\n$$\n\\begin{aligned}\ne^{q_{i}^{\\top} k_{j}} & =\\left[1, q_{i}, \\sqrt{\\frac{1}{2!}} \\tilde{\\phi}_{2}\\left(q_{i}\\right)^{\\top}, \\sqrt{\\frac{1}{3!}} \\tilde{\\phi}_{3}\\left(q_{i}\\right)^{\\top}, \\ldots\\right]\\left[1, k_{j}^{\\top}, \\sqrt{\\frac{1}{2!}} \\tilde{\\phi}_{2}\\left(k_{j}\\right)^{\\top}, \\sqrt{\\frac{1}{3!}} \\tilde{\\phi}_{3}\\left(k_{j}\\right)^{\\top}, \\ldots\\right]^{\\top} \\\\\n& :=\\phi\\left(q_{i}\\right)^{\\top} \\phi\\left(k_{j}\\right)\n\\end{aligned}\n$$\n\nNote that to attain the monomials in 23 for a given $p$, one can also use $\\bigotimes_{j=1}^{p} x$, as given in, e.g., [L\u00f3pez-S\u00e1nchez et al., 2018], which is equivalent up to the constant coefficients, such that we can use $\\sqrt{\\frac{1}{p!}} \\tilde{\\phi}_{p}(x)=c_{p} \\bigotimes_{j=1}^{p} x$ where $c_{p}$ is a matrix of the respective coefficients multiplying each monomial. ## D Derivation of S6 into DSF\n\nWhile $A$ in (4) is represented as a dense matrix of size $n \\times d$ purely for computational reasons, mathematically $A$ is a diagonal matrix of size $n d \\times n d$. This is evident from the fact that S6 parameterizes a different submatrix $A^{d} \\in \\mathbb{R}^{n \\times n}$ for each embedding dimension, leading to\n\n$$\nA=\\left[\\begin{array}{lll}\nA^{1} & & \\\\\n& \\ddots & \\\\\n& & A^{d}\n\\end{array}\\right]\n$$\n\nTo compute $\\Lambda_{i}$ in $\\mathbb{1 1}$, the matrix $A$ is multiplied with the selective discretization time $\\Delta_{i} \\in \\mathbb{R}^{d \\times d}$, which is computed as\n\n$$\n\\Delta_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{\\Delta}\\left(W_{u} u_{i}\\right)+b_{\\Delta}\\right)\\right)\n$$\n\nwhere $\\sigma(\\cdot)$ denotes the softplus function, $W_{u} \\in \\mathbb{R}^{p \\times d}, W_{\\Delta} \\in \\mathbb{R}^{d \\times p}$ are weight matrices with $p<d$, and $b_{\\Delta} \\in \\mathbb{R}^{d}$ is a bias. Note that we embed the computed discretization times in a diagonal $d \\times d$ matrix to simplify the next reformulations. The product of $\\Delta_{i}$ and $A$ is performed along the embedding dimension axis, i.e.,\n\n$$\n\\left[\\begin{array}{ccc}\n\\Delta_{i}^{1} A^{1} & & \\\\\n& \\ddots & \\\\\n& & \\Delta_{i}^{d} A^{d}\n\\end{array}\\right]=\\left(\\Delta_{i} \\otimes \\mathbb{I}_{n}\\right) \\odot A\n$$\n\nTo arrive at the DSF formulation (18), it only remains to take the exponential function of (26) and state $B_{i}, C_{i}$ as in 4 with the appropriate dimensions. ## E Derivation of LSTMs into DSF\n\n## E. 1 RG-LRU\n\nIn order to replace the abundant elementwise operations $\\odot$ in LSTMs with more suitable matrix-vector multiplications for SSMs, we rely on the following observation for $a_{i} \\in \\mathbb{R}^{d}, u_{i} \\in \\mathbb{R}^{d}$ :\n\n$$\n\\sigma\\left(a_{i}\\right) \\odot u_{i}=\\left[\\begin{array}{c}\n\\sigma\\left(a_{i}^{1}\\right) u_{i}^{1} \\\\\n\\vdots \\\\\n\\sigma\\left(a_{i}^{d}\\right) u_{i}^{d}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n\\sigma\\left(a_{i}^{1}\\right) & & \\\\\n& \\ddots & \\\\\n& & \\sigma\\left(a_{i}^{d}\\right)\n\\end{array}\\right]\\left[\\begin{array}{c}\nu_{i}^{1} \\\\\n\\vdots \\\\\nu_{i}^{d}\n\\end{array}\\right]=\\operatorname{diag}\\left(\\sigma\\left(a_{i}\\right)\\right) u_{i}\n$$\n\nAs with S 6 in Appendix D, we reformulate some quantities for easier presentation, namely we embed the input-dependent vectors $\\sigma\\left(W_{R} u_{i}\\right) \\in \\mathbb{R}^{d}, \\sigma\\left(W_{B} u_{i}\\right) \\in \\mathbb{R}^{d}$, where $\\sigma(\\cdot)$ denotes the sigmoid function, in a diagonal matrix, i.e.,\n\n$$\nr_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{R} u_{i}\\right)\\right)=\\left[\\begin{array}{lll}\nr_{i}^{1} & & \\\\\n& \\ddots & \\\\\n& & r_{i}^{d}\n\\end{array}\\right], \\quad b_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{B} u_{i}\\right)\\right)=\\left[\\begin{array}{lll}\nb_{i}^{1} & & \\\\\n& \\ddots & \\\\\n& & b_{i}^{d}\n\\end{array}\\right]\n$$\n\nThe DSF representation 20) is then obtained in a straightforward manner. ## E. 2 qLSTM\n\nWe start by using the same reformulation of the gates as in RG-LRU above:\n\n$$\nf_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{f} u_{i}\\right)\\right), \\quad i_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{i} u_{i}\\right)\\right), \\quad o_{i}=\\operatorname{diag}\\left(\\sigma\\left(W_{o} u_{i}\\right)\\right)\n$$\n\nwhere $f_{i}$ is commonly called the forget gate, $i_{i}$ and $o_{i}$ are called the input and output gates, respectively, and $W_{f}, W_{i}, W_{o} \\in \\mathbb{R}^{d \\times d}$. By removing the tanh activation function, we effectively eliminated the input activation gate, which now serves as a standard input to the recurrence 11), i.e., we reformulated the standard qLSTM (8) to the LTV\n\n$$\n\\begin{aligned}\nx_{i} & =f_{i} x_{t-1}+\\left(i_{i} \\odot W_{u}\\right) u_{i} \\\\\ny_{i} & =o_{i} h_{i}\n\\end{aligned}\n$$\n\n## F Dynamical System Derivation of Multi-headed Separable Attention\n\nAs in Appendix B, we assume a separable attention function, i.e.,\n\n$$\nf\\left(q_{i}, k_{j}, \\mathbf{k}_{i}\\right)=\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\n$$\n\nAdditionally, we consider the multi-headed setting introduced in [Vaswani et al., 2017, Section 3.2.2], i.e., $s$ different attention operations are performed in parallel. Due to the right multiplication of the input (instead of left multiplication as in the original paper), the output of the different heads is stacked row-wise instead of column-wise. Additionally, we assume there is no output mapping after the attention operation. This yields the simplified multi-headed layer\n\n$$\n\\begin{aligned}\n& y_{i}=\\sum_{j=0}^{i}\\left[\\begin{array}{c}\n{\\left[\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} v_{j}\\right]^{1}} \\\\\n\\vdots \\\\\n{\\left[\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)} v_{j}\\right]^{s}}\n\\end{array}\\right]=\\sum_{j=0}^{i}\\left[\\begin{array}{ccc}\n{\\left[\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\\right]^{1}} & & \\\\\n& \\ddots & \\\\\n& & {\\left[\\frac{\\phi\\left(q_{i}\\right)^{\\top} \\psi\\left(k_{j}\\right)}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\\right]^{s}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n{\\left[v_{j}\\right]^{1}} \\\\\n\\vdots \\\\\n{\\left[v_{j}\\right]^{s}}\n\\end{array}\\right] \\\\\n& =\\sum_{j=0}^{i}\\left[\\begin{array}{lll}\n{\\left[\\phi\\left(q_{i}\\right)^{\\top}\\right]^{1}} & & \\\\\n& \\ddots & \\\\\n& & {\\left[\\phi\\left(q_{i}\\right)^{\\top}\\right]^{s}}\n\\end{array}\\right]\\left[\\begin{array}{lll}\n{\\left[\\frac{1}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\\right]^{1} \\cdot \\mathbb{I}_{n / h}} & & \\\\\n& \\ddots & \\\\\n& & {\\left[\\frac{1}{\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)}\\right]^{s} \\cdot \\mathbb{I}_{n / h}}\n\\end{array}\\right] \\\\\n& {\\left[\\begin{array}{ccc}\n{\\left[\\psi\\left(k_{j}\\right)\\right]^{1}} & & \\\\\n& \\ddots & \\\\\n& & {\\left[\\psi\\left(k_{j}\\right)\\right]^{s}}\n\\end{array}\\right]\\left[\\begin{array}{c}\n{\\left[v_{j}\\right]^{1}} \\\\\n\\vdots \\\\\n{\\left[v_{j}\\right]^{s}}\n\\end{array}\\right],}\n\\end{aligned}\n$$\n\nwhere $\\cdot{ }^{r}$ denotes the head index. As is standard for multi-headed attention, we reduce the dimensions of the queries, keys, and values by the number of heads, i.e., $q_{i} \\in \\mathbb{R}^{m / h}, k_{j} \\in \\mathbb{R}^{m / h}$, and $v_{j} \\in \\mathbb{R}^{d / h}$. Since the $h$ different values $\\left[v_{j}\\right]^{s}$ are stacked, this is equivalent to the single headed version, i.e.,\n\n$$\n\\left[\\begin{array}{c}\n{\\left[v_{j}\\right]^{1}} \\\\\n\\vdots \\\\\n{\\left[v_{j}\\right]^{s}}\n\\end{array}\\right]=v_{j}=W_{V} u_{j}\n$$\n\nAbove observation is also valid for the queries and keys, i.e., we can e.g. write $\\left[\\phi\\left(q_{i}\\right)\\right]^{s}$ using an indicator function $\\mathcal{I}(\\cdot)$ on the single headed query $q_{i}$, i.e.,\n\n$$\n\\left[\\phi\\left(q_{i}\\right)\\right]^{s}=\\phi\\left(\\mathcal{I}_{h}\\left(q_{i}\\right)\\right)=\\phi\\left(\\mathcal{I}_{h}\\left(W_{Q} u_{i}\\right)\\right)\n$$\n\nThis shows the intuition behind multi-headed attention, which essentially compares parts of the single-headed queries and keys in parallel. Therefore, we can use Appendix B to write multi-headed separable attention in the DSF as\n\n$$\n\\begin{aligned}\n\\Lambda_{i} & =\\operatorname{diag}\\left(\\frac{\\left[\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)\\right]^{1}}{\\left[\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)\\right]^{1}} \\mathbb{I}_{d / h}, \\ldots, \\frac{\\left[\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)\\right]^{s}}{\\left[\\eta\\left(q_{i}, \\mathbf{k}_{i}\\right)\\right]^{s}} \\mathbb{I}_{d / h}\\right) \\otimes \\mathbb{I}_{n} \\in \\mathbb{R}^{n d \\times n d} \\\\\nB_{i} & =\\left[\\operatorname{diag}\\left(\\frac{1}{\\left[\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)\\right]^{1}} \\mathbb{I}_{d / h}, \\ldots, \\frac{1}{\\left[\\eta\\left(q_{i-1}, \\mathbf{k}_{i-1}\\right)\\right]^{s}} \\mathbb{I}_{d / h}\\right) \\otimes \\psi\\left(\\mathcal{I}_{h}\\left(k_{i}\\right)\\right)\\right] W_{V} \\in \\mathbb{R}^{n d \\times d} \\\\\nC_{i} & =\\mathbb{I}_{d} \\otimes \\phi\\left(\\mathcal{I}_{h}\\left(q_{i}\\right)\\right)^{\\top} \\in \\mathbb{R}^{d \\times n d}\n\\end{aligned}\n$$\n\nMultiple heads therefore extend the single scalar in $\\Lambda_{i}$ (in the single-headed case) to $s$ different scalars, however these only act upon a part of the queries $q_{i}$ and keys $k_{j}$ due to the indicator function. ## G S6 uses reversed sigmoid in state transition matrix\n\nIn the following, we show that the state transition matrix $\\Lambda_{i}$ in S6 is essentially a reversed sigmoid of the projected input. To show this, we assume for simplicity that $A$ in $\\Lambda_{i}=e^{-\\left(\\Delta_{i} \\otimes \\mathbb{I}_{n}\\right) \\odot A}$ is a scalar, i.e., $A=a \\cdot \\mathbb{I}_{n d}$. This assumption simplifies $\\Lambda_{i}$ to\n\n$$\n\\Lambda_{i}=e^{-a\\left(\\Delta_{i} \\otimes \\mathbb{I}_{n}\\right)}=\\left[\\begin{array}{lll}\ne^{-a \\Delta_{i}^{1} \\cdot \\mathbb{I}_{n}} & & \\\\\n& \\ddots & \\\\\n& & e^{-a \\Delta_{i}^{d} \\cdot \\mathbb{I}_{n}}\n\\end{array}\\right]\n$$\n\nwhere each $e^{-a \\Delta_{i}^{j} \\cdot \\mathbb{I}_{n}}$ itself is a diagonal matrix with $n$-times $e^{-a \\Delta_{i}^{j}}$ on its diagonal. In order to analyze this expression, we simplify the computation of $\\Delta_{i}$ by fusing the two projection matrices with out loss of generality, i.e.,\n\n$$\n\\begin{aligned}\n& \\Delta_{i}=\\operatorname{diag}\\left(\\operatorname{softplus}\\left(W_{\\Delta}\\left(W_{u} u_{i}\\right)+b_{\\Delta}\\right)\\right) \\\\\n& \\qquad=\\operatorname{diag}\\left(\\operatorname{softplus}\\left(\\bar{W}_{\\Delta} u_{i}\\right)\\right)=\\left[\\begin{array}{lll}\n\\operatorname{softplus}\\left(\\bar{W}_{\\Delta}^{1,:} u_{i}\\right) & & \\\\\n& \\ddots & \\\\\n& & \\operatorname{softplus}\\left(\\bar{W}_{\\Delta}^{d,:} u_{i}\\right)\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nwhere $\\bar{W}_{\\Delta}^{j,:}$ denotes the $j^{\\text {th }}$ row of $\\bar{W}_{\\Delta}$. Above reformulation is valid since the softplus $(\\cdot)$ function is applied elementwise and we note that $\\Delta_{i}^{j}=\\operatorname{softplus}\\left(\\bar{W}_{\\Delta}^{j,:} u_{i}\\right)$ in 28. Using the definition of the softplus $(\\cdot)$ function, we can show that\n\n$$\ne^{-a \\Delta_{i}^{1}}=e^{-a \\operatorname{softplus}\\left(\\bar{W}_{\\Delta}^{j,:} u_{i}\\right)}=\\left(1+e^{\\bar{W}_{\\Delta}^{j:} u_{i}}\\right)^{-a}=\\sigma_{\\operatorname{rev}}\\left(\\bar{W}_{\\Delta}^{j,:} u_{i}\\right)^{a}\n$$\n\nwhere $\\sigma_{\\mathrm{rev}}(\\cdot)$ is the reversed sigmoid, i.e., $\\sigma_{\\mathrm{rev}}(x)=\\frac{1}{1+e^{x}}$. Since the reversed sigmoid is again applied elementwise to a vector or a matrix, we can write the S6 state transition matrix as\n\n$$\n\\Lambda_{i}=\\operatorname{diag}\\left(\\sigma_{\\mathrm{rev}}\\left(\\bar{W}_{\\Delta} u_{i}\\right)^{a}\\right) \\otimes \\mathbb{I}_{n}\n$$\n\nwhere the power $a$ is applied elementwise. The assumption $A=a \\cdot \\mathbb{I}_{n d}$ we made in the beginning, can be relaxed to any diagonal matrix, however the resulting $\\Lambda_{i}$ will have a more complex representation. ## H Proof of Lemma 2\n\nGiven two dynamical systems of the form , we denote the system of hidden state dimension $N$ with the state $h_{i}^{N}$ and the matrices $\\Lambda_{i}^{N}, B_{i}^{N}, C_{i}^{N}$ and $D_{i}^{N}$ and correspondingly, the system of hidden state dimension $\\bar{N}$ using the state $h_{i}^{\\bar{N}}$ and the matrices $\\Lambda_{i}^{\\bar{N}}, B_{i}^{\\bar{N}}, C_{i}^{\\bar{N}}$ and $D_{i}^{\\bar{N}}$. We show that the system of dimension $\\bar{N} \\geq N$ can recover the system of dimension $N$ by selecting the following system matrices:\n\n$$\n\\begin{gathered}\n\\Lambda_{i}^{\\bar{N}}=\\left[\\begin{array}{cc}\n\\Lambda_{i}^{N} & 0 \\\\\n\\tilde{\\Lambda} & \\bar{\\Lambda}\n\\end{array}\\right], B_{i}^{\\bar{N}}=\\left[\\begin{array}{c}\nB_{i}^{N} \\\\\n\\tilde{B}\n\\end{array}\\right] \\\\\nC_{i}^{\\bar{N}}=\\left[\\begin{array}{ll}\nC_{i}^{N} & 0\n\\end{array}\\right], D_{i}^{\\bar{N}}=D_{i}^{N}\n\\end{gathered}\n$$\n\nIt can be seen that the $N$ first states of the system with dimension $\\bar{N}$ propagate equivalently to the states of the system of dimension $N$.",
    "diifinfms-26": "The additional states can evolve independently given any matrices $\\tilde{\\Lambda}, \\bar{\\Lambda}$ and $\\tilde{B}$ and do not affect the output, such that both systems are equivalent. The two outputs are then equivalent by setting the corresponding entries of the $C_{i}^{\\bar{N}}$ matrix to 0 . ## I Experimental Details\n\nThe experimental results provided in Section 4 are performed on the multi-query associative recall (MQAR) benchmark proposed in [Arora et al., 2023] and the WikiText-103 dataset. To obtain the MQAR results, we modified the Zoology code base ${ }^{\\circ}$ and added the missing models. We provide our code here: https://github.com/IntelligentControlSystems/dsf-mqar. ## I. 1 MQAR experiments\n\nTraining Details We evaluate the following three architecture classes:\n\n1. Attention: softmax attention [Vaswani et al. 2017], linear attention Katharopoulos et al. 2020, normalized attention (21). For all three attention functions, we use a standard GPT-2 style multi-headed Transformer architecture, where we replace the attention block with the respective attention function. The three attention functions are defined in Section 2.1. For all MQAR runs we use a single attention head. [^5]2. State space model: S6 [Gu and Dao, 2023], SSD [Dao and Gu, 2024]. For both SSM variants, we use a standard GPT-2 style single-headed Transformer architecture, where we replace the attention block with the respective SSM variant. This means for S6 and SSD, we do not implement the pre-convolution on the input or the SiLU activations; but just the S6 and SSD blocks. We do this to ensure a fair comparison of the backbones (sequence mixers) irrespective of the higher-level architecture. The S6 block is defined in Section 2.2 and we use the provided code base ${ }^{7}$ to implement it. For SSD there is no exact mathematical definition in the paper [Dao and Gu, 2024] and no code provided. Therefore, we modify the S6 architecture and code to incorporate the ideas presented in [Dao and Gu, 2024]. This means we replace the learnt parameter $A \\in \\mathbb{R}^{n \\times d}$ with a single learnt scalar $a$ and reduce the dimension of the input-dependent discretization time $\\Delta_{i}$ from $d$ to a scalar as well. We then implement this with the selective scan algorithm [Gu and Dao, 2023] instead of the newly proposed SSD algorithm. 3. RNN: qLSTM [Stani\u0107 et al. 2023], modified qLSTM. For implementation of the qLSTM, we build on the Hippogriff codebase ${ }^{8}$ We embed both qLSTM variants in a standard GPT-2 style single-headed Transformer architecture, where we replace the attention block with the qLSTM. We do this to ensure a fair comparison of the backbones (sequence mixers) irrespective of the higher-level architecture. The standard qLSTM is defined in Section 2.3 and the modified qLSTM is the same as the standard qLSTM but with modified state transition $[22$, i.e., a modified forget gate. For all MQAR runs, we use the following training protocol:\n\n- Optimizer and schedule: Weight decay of 0.1 , linear warmup with duration of $10 \\%$, AdamW optimizer [Loshchilov and Hutter, 2019]. For each run, we sweep the learning rates in np. logspace $(-4,-2,4)$ and train for 64 epochs. This is the same setup as in Arora et al. 2023]. - Training duration: We use a global batch size of 512 , which we reduce to 256 if sequence length $L \\geq 128$, to 128 if sequence length $L \\geq 256$, and to 64 if sequence length $L \\geq 512$. We do this to keep the memory consumption approximately constant over different tasks. - Width and depth: For all runs, we use two layers (each with a sequence model and a MLP, interleaved with layer normalization). The model dimensions $d$, state expansion $n$, sequence length $L$, and number of KV pairs are swept according to the experiment (see Section 4). This is the same setup as in Arora et al., 2023]. - Position information: Positional embeddings [Brown et al., 2020] are used for the attention and RNN architecture classes, but not for the SSM architecture classes.",
    "diifinfms-27": "This is the same setup as in [Arora et al. 2023]. - Data: Each model is trained on 100,000 datapoints and evaluated on 3,000 datapoints. The data and its order are constant for all runs. This is the same setup as in [Arora et al., 2023]. Performed Experiments We run the three attention models and the two state space models on four different MQAR tasks, i.e., $\\{L=64$, KV-pairs $=4\\},\\{L=128$, KV-pairs $=8\\},\\{L=$ 256, KV-pairs $=16\\}$, and $\\{L=512$, KV-pairs $=64\\}$, which progressively increase in complexity. For each model and task, we sweep both the model size $d=[64,128,256,512]$ and the state expansion $n=[32,64,128,256]]^{9}$ resulting in a total of 320 experiments. We only report the results of the best performing learning rate; the full results of all experiments are stated in Appendix K We run the two qLSTM variants on three different MQAR tasks, i.e., $\\{L=64$, KV-pairs $=4\\}$, $\\{L=128$, KV-pairs $=8\\}$, and $\\{L=256$, KV-pairs $=16\\}$. For both variants we sweep the model size $d=[64,128,256,512]$, resulting in a total of 24 experiments. We only report the results of the best performing learning rate; the full results are reported in Figure 3 . [^6]\n## I. 2 WikiText-103 experiments\n\nTraining Details We use the 70M parameter Pythia architecture (Pythia70M) Biderman et al. 2023| $\\left.\\right|^{10} \\mid$ For softmax attention we use the standard Pythia attention block, while for linear attention [Katharopoulos et al., 2020] and normalized attention (21] we replace the attention block in the Pythia architecture with the respective attention functions defined in Sections 2.1 and 4.2\n\nFor all training runs on WikiText-103, we use the following protocol:\n\n- Optimizer and schedule: Weight decay of 0.1 , linear warmup with duration of $10 \\%$, AdamW optimizer [Loshchilov and Hutter, 2019] with $\\beta=(0.9,0.95)$, and gradient clipping $=1$. For each run, we sweep the learning rates in $[0.0003,0.001,0.003,0.01]$ and train for 50 epochs. - Training duration: We use a batch size of 128 and train for 50 epochs. - Width and depth: We use a context length of 1024 and the standard Pythia70M configuration, i.e., model size of 512,8 heads, and 6 layers. - Position information: Positional embeddings [Brown et al, 2020] are used as in standard Pythia. Performed Experiments We train the three attention functions on WikiText-103 and sweep the learning rates $[0.0003,0.001,0.003,0.01]$. For all three attention functions learning rate 0.003 performed best and the corresponding results are reported in Table 1. ## J Computational Resources\n\nAll experiments (MQAR and WikiText-103) were run on a cluster with 11 nodes with the following GPU and CPU specifications:\n\n| GPU Model | Nr. of nodes | memory/GPU | GPUs/node | CPUs/node |\n| :--- | :---: | :---: | :---: | :---: |\n| NVIDIA GTX 1080 Ti | 1 | 11 GB | 8 | 20 |\n| NVIDIA GTX 2080 Ti | 2 | 11 GB | 8 | 64 |\n| NVIDIA GTX 3090 | 1 | 24 GB | 8 | 128 |\n| NVIDIA GTX 4090 | 1 | 24 GB | 8 | 128 |\n| NVIDIA TITAN RTX | 1 | 24 GB | 8 | 128 |\n| NVIDIA Quadro RTX 6000 | 1 | 24 GB | 8 | 128 |\n| NVIDIA V100 | 2 | 32 GB | 8 | 44 |\n| NVIDIA A100 | 1 | 40 GB | 8 | 48 |\n| NVIDIA A100 | 1 | 80 GB | 10 | 48 |\n\nThe MQAR training and test runs were parallelized and assigned to the best available GPU node, while the parallelized training on WikiText-103 was exclusively run on the NVIDIA A100 (80GB) node. For each learning rate sweep of the MQAR runs described in Appendix $\\square$ we estimate the average runtime to be $1 \\mathrm{~h}{ }^{11}$ leading to a total unparallelized runtime of 54 days for all MQAR tasks. There where approximately 20 runs for debugging and training purposes, which were terminated after a few minutes, thus we did not include them in the time estimate. For the training on WikiText-103, each learning rate sweep took approximately 14 h , leading to a total parallelized runtime of 42 h for all three attention models. We estimate a total of 1 h of runtime for tuning runs, bringing the total runtime to 43 h . [^7]\n## K Extended Results on MQAR\n\nIn Figure 5 we report the complete results of all MQAR experiments detailed in Appendix 1 A selected subset of these are already presented in Figure 1 and Figure 2 in the main text. ![](https://cdn.mathpix.com/cropped/2024_09_12_5bc3ed65c8ad8740f705g-21.jpg?height=1708&width=1410&top_left_y=511&top_left_x=357)\n\nFigure 5: Results for softmax attention [Vaswani et al., 2017], linear attention [Katharopoulos et al. 2020], normalized attention (21), S6 [Gu and Dao, 2023], and SSD [Dao and Gu, 2024] on four different, progressively harder MQAR tasks $\\{L=64$, KV-pairs $=4\\},\\{L=128$, KV-pairs $=8\\}$, $\\{L=256$, KV-pairs $=16\\}$, and $\\{L=512$, KV-pairs $=64\\}$. We sweep the model size $d=$ $[64,128,256,512]$ and the state expansion $n=[32,64,128,256]$ for each model and task. We only report the best performance from a learning rate sweep in np . logspace $(-4,-2,4)$ measured as accuracy on the MQAR task. The accuracy is denoted in \\% in the grid in the figure. The effect of state expansion can not only be observed for linear attention (Figure 11) but also for normalized attention (21), S6, and SSD for the task $\\{L=512$, KV-pairs $=64\\}$. Contrary to this, for small model sizes $d$ and larger tasks (e.g. normalized attention, task $\\{L=512$, KV-pairs $=64\\}$, $d=64$ ) the performance decreases with increased state expansion $n$ or shows erratic behaviour. Since this behavior only occurs for small $d$, we hypothesis that this effect is due to the model being to small to accurately learn the task. While Figure 2 shows that normalized attention outperforms standard linear attention Katharopoulos et al., 2020], Figure 5 shows an even more significant performance gap. Additionally, we note that SSD slightly outperforms S6, which was already hinted at in [Dao and Gu, 2024], and that normalized attention performs on par with S6. Together these results hint at the importance of normalization both for attention and SSMs. The comparison of S6 and SSD shows that reducing the number of parameters in the state transition $\\Lambda_{i}$ from $d$ to a scalar does not hurt performance, which is further supported by the findings in [Dao and Gu. 2024]. These experiments also suggest that the recursive structure in $\\Lambda_{i}$ and $B_{i}$ (present in S6 and SSD but not in normalized attention or linear attention) is less important than proper normalization of the attention scores. Additionally, the results on WikiText-103 (Table 1) show that better normalization can close $25 \\%$ of the perplexity gap between linear attention and softmax attention. Together, these results warrant more investigations into new and better normalization techniques for attention-based models. Finally, we remark that softmax attention performs perfectly accross all sweeps except $\\{L=$ 512 , KV-pairs $=64\\}, d=64$, and $n=32$, which is most likely due to a too small model or too small learning rate. [^0]:    *These authors contributed equally; ordered randomly. [^1]:    ${ }^{2}$ The dimensions used in the following are visualized in Appendix A for clarity. [^2]:    $\\sqrt[3]{\\text { https://github.com/HazyResearch/zoology }}$\n\n[^3]:    ${ }^{4}$ The presented state expanded LSTM versions, cannot be directly translated into the DSF framework, since the gates not only depend on the inputs $u_{i}$ but also on past outputs $y_{i-1}$. However, the used state expansion is essentially $n=d$, hence leading to a DSF system of size $d^{2}$. [^4]:    5 https://github.com/proger/hippogriff\n\n[^5]:    ${ }^{6}$ https://github.com/HazyResearch/zoology Arora et al. 2023\n\n[^6]:    https://github.com/state-spaces/mamba\n    ${ }^{8}$ https://github.com/proger/hippogriff\n    ${ }^{9}$ We did not increase $n$ further, since the selective scan CUDA kernel provided for S6 [Gu and Dao 2023] is capped at $n=256$; for more information see https://github.com/state-spaces/mamba. [^7]:    ${ }^{10}$ https://github.com/EleutherAI/pythia\n    ${ }^{11}$ Obviously, larger model sizes and MQAR tasks with larger sequence length took longer than smaller models and tasks with shorter sequence length. However, a more accurate time estimate is hard to obtain due to the cluster setup with multiple different GPU models and the fact that we terminated tasks early if the $99 \\%$ accuracy threshold was achieved.",
    "diifinfms-28": ""
}