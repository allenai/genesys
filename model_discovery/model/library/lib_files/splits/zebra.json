{
    "zebra-0": "# Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention \n\nKaiqiang Song*, Xiaoyang Wang*, Sangwoo Cho*, Xiaoman Pan, Dong Yu<br>Tencent AI Lab, Seattle<br>\\{riversong, shawnxywang, swcho, xiaomanpan, dyu\\}@global.tencent.com\n\n\n#### Abstract\n\nThis paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information.",
    "zebra-1": "Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency. ## 1 Introduction\n\nTo effectively leverage the power of Long Context in Large Language Models (LLMs), it is essential to develop and refine techniques that enable these models to process and interpret extensive text sequences accurately. This capability is particularly significant in applications that demand deep understanding and synthesis of large volumes of information, such as summarization (Huang et al., 2021; Hu et al., 2023; Song et al., 2022; Kry\u015bci\u0144ski et al., 2021), reading comprehension (Nguyen et al., 2016; Fan et al., 2019; Zhong et al., 2021; Yang et al., 2023), long-form generation (Guan et al., 2021; Deng et al., 2022; Roziere et al., 2023), and\n\n[^0]complex reasoning (Wei et al., 2022; Yao et al., 2023; Chen et al., 2023a). However, it is challenging to extend the context window from different viewpoints: First, the predominant LLM model uses Transformer architecture (Vaswani et al., 2017). Such models like BERT (Devlin et al., 2018), GPT (OpenAI, 2023), and T5 (Raffel et al., 2020) employ full attention in each layer which inherently incurs quadratic time and memory complexity. This may potentially diminish the efficiency of both the training and inference processes. Second, attention computation over an extremely long sequence might lead to an almost even distribution, potentially causing the omission of vital information (Han et al., 2023). This may further lead to the issue of being \"lost in the middle\" (Liu et al., 2023). Finally, the distribution of training signals for long and short sequences is imbalanced. It is evident that longer sequences are infrequent in both plain text and instructiontuning data. Consequently, this rarity poses a challenge in effectively capturing long-term dependencies during the training process. To tackle the above issues, we propose to group local-global attention layers into blocks during the training and inference phases. This strategy enhances efficiency while yielding results comparable to those of a global attention Transformer. Notably, it attains equivalent performance levels with merely half the computational effort required for training. Additionally, this approach significantly reduces memory consumption during inference by maintaining a local Key-Value (K-V) cache specifically for the local attention layers. In Section 2.1, we list the two critical components essential for a long-context model as well as the potential alternatives for consideration. These encompass diverse attention mechanisms and methodologies for positional embedding. Subsequently, in Section 2.2, we conduct a comparative analysis of these alternatives, presenting their em-\npirical outcomes for a comprehensive evaluation. Integrating these insights, we name our model Zebra, drawing an analogy to the alternating black and white stripes of a zebra, which resemble the grouped local and global layers in our model's architecture. To validate the proposed model at large scales, Section 3 details the continuation of training the Llama-2-7B model (Touvron et al., 2023) using long-context adaptation training through Zebra. This approach not only exhibits comparable performance on short-sequence benchmarks but also achieves superior perplexity results for longer sequences. Additionally, in Section 4, we conduct fine-tuning of Zebra using a combination of both short and long instruction-tuning datasets. This is followed by a systematic evaluation of the model's performance across a range of benchmark datasets. It demonstrates generally better performance on both long and short benchmarks compared to Llama-2-7b-chat. To Conclude, our contribution is 3 -fold:\n\n- We develop a novel architecture, referred to as Zebra, which incorporates grouped localglobal attention layers and rotary positional embedding. - We conduct comprehensive experiments and detailed analyses of the Zebra framework across various settings, including pretraining from scratch, continuation of training, and extensive instruction tuning. The findings from these results demonstrate the advantage of $\\mathbf{Z e}$ bra model architecture. - Additionally, we analyze the training and inference efficiency for Zebra and provide the pseudocode for implementation. ## 2 Zebra\n\n### 2.1 Model Architecture Design\n\nTo extend the context window for Transformer models, two critical elements must be addressed: First, the Attention mechanism that allows the model to efficiently focus on and process relevant parts of long sequences.",
    "zebra-2": "However, it is important to note that the computational of attention escalates quadratically, leading to a decrease in efficiency as the length of the sequence increases. Consequently, addressing this computational challenge is essential for maintaining effectiveness over longer sequences. Second, the Positional Embedding that imparts a structured signal indicative of the sequential order of tokens. It is vital to employ a positional embedding that is not only robust but also exhibits strong generalization capabilities, particularly for processing long sequences. This ensures the model's effectiveness in maintaining sequence integrity over longer spans of data. ### 2.1.1 Attention\n\nIn Figure (1a, 1b, 1c), we showcase three representative variants of single attention layers including global attention, local attention, and local attention with global approximations. Additional sparse attention models like blockwise attention(Qiu et al., 2019), dilated window attention (Beltagy et al., 2020), stride attention (Child et al., 2019), Sinkhorn Attention (Tay et al., 2020a), transient global attention (Guo et al., 2021) are considered potential alternatives for basic local attention. For the sake of clarity and focus in our research, we confine our analysis to two primary variants: local attention and local attention with global approximations. This decision allows for a more targeted exploration of these specific attention mechanisms within our work. Moreover, we also consider using different strategies among different layers. In Figure 1d, we combine several local layers with one global attention layer as a group and stack such groups for the model. Considering one head of the self-attention layer in a decoder-only transformer, the query, key, and value of $i$-th position and $l$-th layer are defined as projections of the last layer hidden states $h_{i}^{(l-1)}$ :\n\n$$\n\\begin{aligned}\n\\mathbf{q}_{i}^{(l)} & =W_{q}^{T} \\mathbf{h}^{(l-1)_{i}} \\\\\n\\mathbf{k}_{i}^{(l)} & =W_{k}^{T} \\mathbf{h}^{(l-1)_{i}} \\\\\n\\mathbf{v}_{i}^{(l)} & =W_{v}^{T} \\mathbf{h}^{(l-1)_{i}}\n\\end{aligned}\n$$\n\nWe denote the similarity between $i$-th query and $j$-th key as:\n\n$$\n\\operatorname{Sim}(i, j)=\\exp \\left(\\mathbf{q}_{i}^{T} \\mathbf{k}_{j} / \\sqrt{D}\\right)\n$$\n\nwhere $D$ is a normalized factor usually equal to the model dimension. Global Attention: It is the most common attention, where each token has attention to all the positions before it and itself:\n\n$$\n\\alpha_{i, j}=\\frac{\\operatorname{Sim}(i, j)}{\\sum_{t=0}^{i} \\operatorname{Sim}(i, t)}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-03.jpg?height=423&width=1600&top_left_y=224&top_left_x=236)\n\nFigure 1: Four different attention strategies to be compared in this work. (a) Global Attention, where each token has its attention to all previous tokens and itself; (b) Local Attention, where each token only has the attention within its local window; (c) Local Attention with Global Approximation is newly introduced in this work, where each token not only has attention to its local window but also has an approximated attention from the remaining non-local chunks; (d) Group Attention is our introduced layerwise grouped local-global attention strategy, where we group $L$ layers and apply the global attention at the first layer of each group (the remaining layers use local attention). where $\\alpha_{i, j}$ is the attention value of $i$-th query over $j$-th key. The context vector is then defined as a weighted sum of value vectors:\n\n$$\n\\text { context }_{i}=\\sum_{j=0}^{i} \\alpha_{i, j} \\mathbf{v}_{j}\n$$\n\nLocal Attention: Each query only considers the key-value pairs within its local window. $$\n\\alpha_{i, j}=\\frac{\\operatorname{Sim}(i, j)}{\\sum_{t=\\min (0, i-w)}^{i} \\operatorname{Sim}(i, t)}\n$$\n\nwhere $w$ is the window size of local attention. Local Attention w/ Global Approximation: Inspired by transient global attention (Guo et al., 2021), we approximate the global attention output by combining multiple non-local tokens as a chunk, and take the attention over local tokens and nonlocal chunks. Each non-local chunk's key-value pairs are estimated using the following equations:\n\n$$\n\\begin{aligned}\n& \\hat{\\mathbf{k}}_{j}=\\sum_{t=(j-1) * c}^{j * c-1} \\mathbf{k}_{t}+\\ln (c) \\\\\n& \\hat{\\mathbf{v}}_{j}=\\sum_{t=(j-1) * c}^{j * c-1} \\mathbf{v}_{t}+\\ln (c)\n\\end{aligned}\n$$\n\nwhere $c$ is the chunk size, and $\\ln (c)$ is a compensation term for each chunk. Layerwise Grouped Local-Global Attention: Instead of using identical layers for the entire network, we propose to use grouped local-global attention layers. In figure 1 d , we group every $L$ layer and use only one global attention layer at the first layer of each group. We apply local attention described in Equation (7) for the remaining layers. ![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-03.jpg?height=105&width=631&top_left_y=2489&top_left_x=267)\n\nTo simplify, we use Group Attention to denote the layerwise grouped local-global attention. ### 2.1.2 Positional Embedding\n\nIn the Transformer architecture, positional embeddings are commonly used to encode the sequence order information. In this study, we incorporate three widely recognized types of positional embeddings to facilitate a comprehensive analysis. Absolute Positional Embedding: The vanilla Transformer (Vaswani et al., 2017) advocates to use an absolute sinusoidal positional embedding:\n\n$$\n\\begin{aligned}\nP E(p o s, 2 i) & =\\sin \\left(p o s / 10000^{2 i / d)}\\right) \\\\\nP E(p o s, 2 i+1) & =\\cos \\left(p o s / 10000^{2 i / d}\\right)\n\\end{aligned}\n$$\n\nwhere pos is the position index, $d$ is the model dimension, and $i$ is the iterative variable for different dimensions. After the work of the vanilla Transformer, a trainable absolute positional embedding has been introduced (Devlin et al., 2018; Radford et al., 2018), serving as a replacement for the fixed sinusoidal pattern. Such positional embedding is directly added to the semantic embedding:\n\n$$\nE M B(\\text { word, pos })=W E(\\text { word })+P E(\\text { pos })\n$$\n\nwhere word is the input token index, and pos is the absolute position index. Most recently, the relative positional embeddings (Shaw et al., 2018; Yang et al., 2019) are introduced to eliminate the positional bias while improving the performance. These approaches also facilitate the model's ability to extend its contextual window, a process known as position extrapolation. Within this framework, two principal types of relative positional embeddings are taken into consideration. Alibi Positional Embedding (Press et al., 2022), which applies the relative positional embedding by directly adding a bias term to the attention matrix. $$\n\\alpha_{i, j}=\\operatorname{Softmax}_{j}^{i}(\\operatorname{Sim}(i, j)-(i-j) * m)\n$$\n\nwhere $m$ is a head-specific scalar and $(i-j)$ is the relative distance between query and key positions. By canceling out the $-i * m$ term, we have\n\n$$\n\\alpha_{i, j}=\\operatorname{Softmax}_{j}^{i}(\\operatorname{Sim}(i, j)+j * m)\n$$\n\nRotary Positional Embedding (Su et al., 2023) rotates the conjugate dimensions of query and key vectors, such that the relative distance is encoded during the attention calculation. $$\n\\begin{aligned}\n\\widetilde{\\mathbf{q}} & =\\left(W_{q}^{T} \\mathbf{h}_{i}\\right) e^{\\mathbf{i}(i \\theta)} \\\\\n\\widetilde{\\mathbf{k}} & =\\left(W_{k}^{T} \\mathbf{h}_{i}\\right) e^{\\mathbf{i}(i \\theta)}\n\\end{aligned}\n$$\n\nwhere $\\mathbf{i}$ denotes the imaginary unit, and $i$ is the positional index. For each pair of conjugate dimensions, the similarity between query and key can be written as:\n\n$$\n\\operatorname{Sim}(i, j)=R E\\left[\\left(W_{q}^{T} \\mathbf{h}_{i}\\right)^{T}\\left(W_{k}^{T} \\mathbf{h}_{j}\\right) e^{\\mathbf{i}(i-j) \\theta}\\right]\n$$\n\nwhere $R E$ takes the real value of the complex number. The overall similarity is consequently defined as the cumulative measure of similarities across all corresponding dimensions. ### 2.2 Experiments for Model Design\n\n| Model Size | 117 M | 345 M |\n| :---: | :---: | :---: |\n| Num Layers | 12 | 24 |\n| Hidden Size | 768 | 1024 |\n| Num Heads | 12 | 16 |\n| K-V Channel | 64 | 64 |\n| FF Layer Hidden Size | 3072 | 4096 |\n\nTable 1: Parameters of two models with different sizes. We conduct experiments with various attention strategies and positional embedding methods as described earlier. Two GPT models with 117M and 345M parameters as detailed in Table 1, are trained from scratch to assess different model architectures. The training sequence length used for these experiments ranges from 1024, 4096, to 16384. A $10 \\%$ of the training data from the Pile dataset (Gao et al., 2020) is utilized for model training. Its testing and validation data is used in experiments of this section for evaluation. We employ an Adam Optimizer (Kingma and Ba , 2014) for training with the beta values of 0.9 and 0.99 . The training process spans 20,000 steps with a batch size of 2 M tokens. The initial learning rate is set to $1 e^{-3}$ with a warmup step of 2,000 , followed by linear decay to $1 e^{-5}$. Weight decay is set to 0.01 , and the gradient clipping is set to 1.0 . For the local attention, a window size of $w=1,024$ is applied. For local attention with global approximation, we employ a chunk size of $c=16$. We group every three layers for local and global layers. For rotary embedding (Su et al., 2023), the RoPE theta is configured as 131,072 to enhance its generalization performance on longer sequences. All experiments are implemented using Megatron-LM ${ }^{1}$ (Shoeybi et al., 2019; Narayanan et al., 2021; Korthikanti et al., 2023). ### 2.2.1 Attention\n\nFigure 2 shows the testing perplexity (PPL) difference between each attention strategy and the global attention method on the 117 M model. From the figures, we have a few observations: First, global attention has the best overall performance; Second, the performance gap between group attention and global attention is small but stable when the training sequence length is getting longer; Third, as the training sequence length grows, the performance of local attention and global approximation attention drops a lot for longer sequences, though it may benefit the shorter ones. As group attention has less computation but achieves a similar performance compared to global attention, it has a high scalability potential. To better compare the global and group attention strategies, we take both performance and computation into consideration. In Figure 3, we draw the curve of the estimated TFLOPS and the validation PPL on three different training lengths with DeepSpeed FLOPS profiler ${ }^{2}$ for the 117 M model. We observe that group attention achieves a similar performance with less computation than global attention. When the local window is equal to the training sequence length (i.e., 1 k training length in Figure 3), the gain is negligible. However, as the training sequence gets longer (e.g., 4 k or 16 k training length in Figure 3), the gain becomes magnified. This verifies that group attention has better scalability compared to global attention. ### 2.2.2 Positional Embedding\n\nTable 2 shows the perplexity results comparing different positional embeddings with the 117 M and 345 M models. We find that no significant performance differences are observed among the three positional embeddings for sequence lengths within the training sequence 16,384 . This result is in line with (Taylor et al., 2022; Kazemnejad et al., 2023)\n\n[^1]![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-05.jpg?height=359&width=1523&top_left_y=258&top_left_x=245)\n\nFigure 2: The testing PPL gap between each method and the baseline system (global attention) on 1024, 4096, and 16384 training sequence length.",
    "zebra-3": "The smaller the better. In this experiment, we split the entire testing set into different splits according to their length. Each split contains the instances within the length range of $\\frac{x}{2}+1$ to $x$, except the first one (length $\\leq 128$ ). ![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-05.jpg?height=355&width=1504&top_left_y=802&top_left_x=265)\n\nFigure 3: The validation PPL vs TFLOPs for global attention(red) and group attention(blue) on 1024, 4096, and 16384 training sequence lengths. observation. While the absolute positional embedding encounters challenges in extrapolating to longer sequences, both Alibi and Rotary positional embeddings demonstrate similar capabilities for sequences exceeding the training sequence length of 16,384 . It is important to note that, in our experiments, the Alibi positional embedding requires full precision (fp32) computation to prevent position collision. Consequently, we opt for the Rotary positional embedding in the Zebra model. ### 2.2.3 Training Sequence length\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-05.jpg?height=509&width=690&top_left_y=1987&top_left_x=249)\n\nFigure 4: Perplexity on test sequences with $1 k, 4 k$, and $16 k$ training sequence lengths. In this experiment, we split the entire testing set into different splits according to their length. Each split contains the instances within the length range of $\\frac{x}{2}+1$ to $x$, except the first one (length $\\leq 128$ ). We experiment with training sequence lengths of 1024,4096 , and 16384 with a 117 M model. The corresponding validation perplexity with the three training sequence lengths is $14.29,12.98$, and 12.76, respectively. In Figure 4, we observe training with longer sequence length generally performs better than those training with shorter sequence length, especially on longer test splits. Meanwhile, the perplexity of the model with longer training length drops a little on short test splits. Interestingly, as we only train with 16 k length, the perplexity is still going down on the 32 k test split. The results suggest that training with a longer sequence length helps with performance. ### 2.3 Conclusion on Model Architecture\n\nBased on the experiments and analysis above, we decide to apply Rotary positional embedding and group attention strategy for extending LLM's context window. The model is denoted as Zebra due to the analogous arrangement of grouped local and global attention layers, resembling the black and white color stripes on a zebra. ## 3 Long Context Adaptation Training\n\nIn this section, we expand our examination of the Zebra architecture by applying it to a larger-scale model, utilizing Long Context Adaptation Training (LCAT).",
    "zebra-4": "LCAT is essential for handling large\n\n| Pos. Emb. |  | PPL on Pile Testset with Different Lengths |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Min Len | 1 | 129 | 257 | 513 | 1025 | 2049 | 4097 | 8193 | 16385 | 32789 | 65537 |\n| Max Len |  | 128 | 256 | 512 | 1024 | 2048 | 4096 | 8192 | 16384 | 32768 | 65536 | 131072 |\n| $\\sum$ | Absolute | 24.62 | 20.34 | 17.00 | 17.06 | 16.42 | 11.84 | 10.02 | 8.84 |  | - | - |\n|  | Alibi | 24.54 | 20.29 | 17.01 | 17.05 | 16.41 | 11.87 | 10.08 | 8.93 | 7.60 | 8.83 | 18.47 |\n|  | Rotary | 24.70 | 20.37 | 17.03 | 17.05 | 16.41 | 11.84 | 10.06 | 8.92 | 7.62 | 8.86 | 18.51 |\n| $\\triangle$ PPL (Max - Min) |  | 0.16 | 0.08 | 0.03 | 0.01 | 0.01 | 0.03 | 0.06 | 0.09 | 0.02 | 0.03 | 0.04 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-06.jpg?height=84&width=39&top_left_y=511&top_left_x=258) | Absolute | 19.41 | 16.12 | 13.57 | 13.61 | 13.19 | 9.86 | 8.37 | 7.46 | - | - | - |\n|  | Alibi | 19.27 | 16.02 | 13.51 | 13.55 | 13.11 | 9.78 | 8.36 | 7.44 | 6.44 | 7.38 | 14.84 |\n|  | Rotary | 19.25 | 16.01 | 13.48 | 13.51 | 13.08 | 9.74 | 8.32 | 7.42 | 6.43 | 7.37 | 14.77 |\n| $\\Delta$ PPL (Max - Min) |  | 0.16 | 0.11 | 0.09 | 0.10 | 0.11 | 0.12 | 0.05 | 0.04 | 0.01 | 0.01 | 0.07 |\n\nTable 2: Perplexity with different positional embeddings and model sizes on the Pile test set.",
    "zebra-5": "The minimum value of each model is indicated in bold. All the systems are trained with 16,384 sequence length. In this experiment, we split the entire testing set into different splits according to their length. Each split contains the instances within the length range of its minimum length to its maximum length. $\\Delta$ PPL is the gap between the best-performing system and the worst-performing system of each test split. contexts. This modification enables the model to adapt to an expanded context through Zebra architecture, while also providing extended signals during training. This approach allows the model to effectively learn and utilize long-range dependencies. Training such a model from scratch requires a lot of computing power, so we start with an already developed model called LLAMA-2 (Touvron et al., 2023) as our base. From there, we train the Zebra model with different volumes of data. All our experiments are conducted with the 7B-sized model. ### 3.1 Training Details\n\nZebra layers are organized into groups, each consisting of four layers $(L=4)$. The first layer within each group functions as the global layer, while the remaining three layers serve as local layers with an attention local window size of 512 . Each batch contains 1, 572, 864 tokens. As shown in Table 5, Zebra models are trained with 24,576 or 32,768 sequence lengths with different data sizes where multiple documents are packed with a BOS token and an EOS token. The Adam Optimizer (Kingma and $\\mathrm{Ba}, 2014)$ is utilized, with beta values of $(0.9,0.95)$ and an Adam epsilon of $1 e^{-5}$. Training incorporates a cosine learning schedule with a linear warmup of 100 steps. The maximum learning rate is $1 e^{-4}$, and the minimum learning rate is $1 e^{-5}$. Gradient clipping is applied with a 1.0 threshold and weight decay is set to 0.1 . ### 3.2 Data Recipes\n\nFigure 5 presents an analysis of the distribution imbalance in the Pile dataset (Gao et al., 2020), particularly highlighting the variations due to different sequence lengths. A notable observation is that doubling the sequence length, when measured in bytes, results in a reduction of the dataset's volume\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-06.jpg?height=506&width=672&top_left_y=912&top_left_x=1086)\n\nFigure 5: Sequence Length vs. Number of Instances on The Pile Dataset. to merely one-fourth of its initial size. In order to sufficiently capture longer text segments in our model, we have strategically integrated the LCAT data with sources that typically contain longer data, such as books. All data is extracted from diverse sources within the Pile dataset, followed by recombination with various domains.",
    "zebra-6": "Table 3 illustrates the detailed mapping of the original data source to the domain split. The data within each domain split is shuffled, and the specified data quantities, as outlined in Table 3, are selected. ### 3.3 Evaluation Results\n\nThe long context adaptation training is essential for Zebra to refine the initial parameters from the LlAMA-2 model. Due to the architectural disparities between the LlamA model and the Zebra model, our objective is to achieve comparable performance with LLAMA-2 on both the shortcontext tasks and the long-context tasks after training with long sequences. We first evaluate the pre-trained models in Table 4 on Gutenberg (PG19) (Rae et al., 2019) by computing the perplex-\n\n| Domain | Data Source | Data Version |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  |  | 100 GB | 50 GB | 100 GB |\n| Book | Bile-CC, OpenWebText2, HackerNews | 0 | 50 GB | 100 GB |\n| WebPage | Pils | 0 | 50 GB | 100 GB |\n| Knowledge | USPTO Backgrounds, PubMed Abstracts <br> Wikipedia(en), NIH ExPorter | 0 | 20 GB | 30 GB |\n| Q\\&A | Stack Exchange | 0 | 20 GB | 20 GB |\n| Code | Github | 0 | 10 GB | 10 GB |\n| Translation | EuroParl | 100 GB | 200 GB | 360 GB |\n| Total | - | 15.7 B | 50.3 B | 100.7 B |\n| \\# Token Used | - |  |  |  |\n\nTable 3: Data source for each domain and various combinations of data size employed for LCAT training. | Model Architecture | Training |  | RoPE Parameters |  |  |  | PPL with Different Test Length |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Len. | Data | Scale | theta | 4096 | 8192 | 16384 | 24576 | 32768 | Average |  |  |\n| LLAMA2 | - | - | 1 | 10,000 | 7.57 | 126.53 | 1008.50 | $2,037.35$ | 2814.72 | 1198.93 |  |  |\n|  |  |  | 4 | 10,000 | 15.27 | 15.31 | 18.35 | 64.12 | 178.97 | 58.40 |  |  |\n|  | - | - | 4 | 40,000 | 48.71 | 56.96 | 71.75 | 82.38 | 101.17 | 72.19 |  |  |\n| Llama2-PI |  |  | 16 | 160,000 | 195.06 | 202.61 | 212.50 | 220.79 | 240.52 | 214.30 |  |  |\n|  |  |  | 16 | 160,000 | 239.38 | 255.36 | 339.49 | 427.84 | 532.97 | 359.01 |  |  |\n| LLAMA2-LCAT | 32 k | v 1 | 16 | 160,000 | 7.13 | 6.85 | 6.75 | 6.63 | 6.55 | 6.78 |  |  |\n|  | 24 k | v 0 | 16 | 160,000 | 9.02 | 8.80 | 8.50 | 8.52 | 8.41 | 8.65 |  |  |\n| Zebra-LCAT | 24 k | v 1 | 16 | 160,000 | 7.32 | 7.06 | 6.92 | 6.85 | 6.91 | 7.02 |  |  |\n|  | 32 k | v 1 | 16 | 160,000 | 7.36 | 6.9 | 6.84 | 6.73 | 6.75 | 6.93 |  |  |\n|  | 32 k | v 2 | 16 | 160,000 | 7.14 | 6.90 | 6.71 | 6.62 | 6.57 | 6.79 |  |  |\n\nTable 4: PPL on Gutenberg (PG-19). The data for this evaluation is from the Pile training split but excluded in LCAT training. ity across various lengths. Notably, Zebra-LCAT trained with an extended context length of 32 k and larger data (v2) exhibits the most favorable perplexity results, comparable to the performance of the LLAMA-2-LCAT model. Additionally, we assess the performance of our models on a set of common pretraining benchmarks to ensure robust performance across standard short-context tasks (Touvron et al., 2023; Xiong et al., 2023). As shown in Table 5, the Llama2-LCAT 32k model continually trained from the original LLAMA-2 model using the 50B-token v1 data in Table 3 results in a slight degradation in performance compared to LLAMA-2. This can be attributed to the impact of long context adaptation training, as observed in previous work (Chen et al., 2023b), which results in degradation on shortcontext benchmarks. Given that global attention typically yields superior performance compared to local attention (Rae and Razavi, 2020; Beltagy et al., 2020; Sun et al., 2023), the LLAMA-2-LCAT performance indicates potential performance upper limit of models with local attention. Comparing LLAMA-2-LCAT to Zebra-LCAT trained with the same amount of data (50B), the results demonstrate similarities except for MMLU. We speculate the performance decrease in MMLU is originated from the architectural change of the model, potentially resulting in the forgetting of partial knowledge in\n\n| Model | Tks | MMLU | CS | OQA | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| LLAMA2 | 2T | 45.3 | 63.9 | 48.9 | 52.7 |\n| LLAMA2 LoNG | 2T+400B | 47.8 | 64.9 | 51.0 | 54.6 |\n| LLAMA2-LCAT | 2T+50B | 44.4 | 61.4 | 45.6 | 50.5 |\n| Zebra-LCAT | 2T+15B | 32.6 | 59.4 | 41.0 | 44.3 |\n| Zebra-LCAT | 2T+50B | 39.1 | 61.2 | 46.3 | 48.9 |\n| Zebra-LCAT | 2T+100B | 41.8 | 61.3 | 46.0 | 49.7 |\n\nTable 5: Perfomance of 7B models on short-context benchmarks. The scores in the first two rows are from (Xiong et al., 2023). Commonsense (CS) score as the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2019); OpenQA (OQA) score as the average of 5 -shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). LLAMA-2. Finally, as illustrated in Table 5, training Zebra models with varying token quantities indicates that more tokens contribute to narrowing the performance gaps relative to the LlAMA-2LCAT model. ### 3.4 Conclusion on LCAT\n\nOur model exhibits comparable performance to the full attention model (LLAMA-2-LCAT) in both perplexity and downstream tasks. This equivalence is achieved while ensuring faster training and higher GPU throughput. Based on the experimental findings, we choose Zebra-LCAT trained with a 32 k context window and 100B tokens for subsequent\ninstruction tuning (a.k.a. supervised fine-tuning) in long context window, as outlined in Section 4. ## 4 Long Instruction Tuning\n\nThe Long Context Adaptation Training (LCAT), as discussed in Section 3, facilitates the adaptation of the Zebra model architecture from its LlAMA-2 foundation model architecture. This adaptation involves the incorporation of grouped local-global layers and position interpolation by training the model with the pre-training learning objectives in long context. Furthermore, we hypothesize that LCAT contributes to enhancing the model by incorporating additional knowledge and capabilities that can only be acquired through training in longer context windows, as suggested by prior research (Xiong et al., 2023). To align the Zebra-LCAT model for comprehensive open-domain language understanding and generation tasks based on its acquired knowledge, we conduct supervised fine-tuning on the ZebraLCAT model with Long Instruction Tuning (LIT). The objective is to empower the model to proficiently handle both short and long-context tasks, adhering to the specifications outlined in human instructions. ### 4.1 Instruction Tuning Data Recipe\n\nTo align the model for both short and long-context tasks, our instruction tuning (IT) data comprises instances of both types. Table 6 shows the statistics on the average token number per instance. The LLAMA tokenizer is utilized for tokenizing the instances to calculate the token numbers. |  | \\# of Instances | Avg. Tokens \\# / Instance |\n| :---: | :---: | :---: |\n| Short IT Data | 344,818 | 208.3 |\n| Long IT Data | 108,963 | 6113.5 |\n| Overall | 453,781 | 1626.3 |\n\nTable 6: Statistics of our instruction tuning (IT) data for Zebra LIT. Our short instruction tuning data (Short IT Data) contains more instances than our long instruction tuning data (Long IT Data) but Long IT Data has a significantly larger average token number per instance. ### 4.1.1 Short Instruction Tuning Data\n\nOur short instruction tuning data primarily incorporates publicly available English instruction tuning datasets, including LIMA (Zhou et al., 2023), Alpaca (Taori et al., 2023), and ShareGPT (Tey). Additionally, we introduce samples from hh-rlhf (Ganguli et al., 2022) with \"selected\" responses for multi-turn dialogue. To customize the profile of our\nZebra assistant, our dataset further encompasses carefully curated short instances. ### 4.1.2 Long Instruction Tuning Data\n\nFor long instruction tuning, we address two cases: First case, where the user provides a lengthy document and instructs the system to process the substantial information and complete the specified task succinctly; Second case, where the input is relatively short, but the desired output needs to be more extensive. The former case encompass tasks of summarization, question-answering (QA), and machine reading comprehension (MRC), while the latter involves writing tasks. In writing tasks, the user provides key information, major characters, or an outline, tasking the AI assistant with composing a natural language document, such as a letter or a chapter in a novel. To systematically empower LLMs for long tasks in both scenarios, we carefully curate high-quality instruction tuning data for three specific tasks: summarization, long-MRC, and writing. Summarization: We select 3,000 news reports from CNN / Daily Mail (See et al., 2017) and 2,000 multi-turn long dialogue transcripts from MediaSum (Zhu et al., 2021) as documents to summarize. Given an document, we randomly choose one out of ten predefined prompts to instruct GPT-4 (OpenAI, 2023) to generate a summary. Our long instruction tuning data for summarization hence consists of 5,000 instances with a document and the randomly selected summarization prompt as input, and the GPT-4 generated summary as output. Long-MRC: We create synthetic long-MRC data utilizing the long documents from Guttenberg PG19 corpus (Rae et al., 2019). Given a long document (e.g., a chapter or a chapter chunk) in the corpus, we first divide the long document into text segments in approximately even length. For each text segment, we prompt GPT-4 to write one questionanswer pair grounding on the information from the text segment. Each long-MRC instruction tuning instance then consists of the long document and the generated question as input $x$, and the generated answer as output $y$.",
    "zebra-7": "We collect in total of 1,245 such instances. Writing: We further utilize texts from Guttenberg PG-19 corpus to generate the data for writing. Given a document (e.g., a chapter or a chapter chunk) in the corpus, we prompt ChatGPT (OpenAI, 2022) to extract its key elements including \"central idea\", \"outline\", \"keywords\", \"time\",\n\"place\", \"key figures\", \"cause\", \"process\", and \"result\".",
    "zebra-8": "We use predefined rules to randomly select a subset of these elements, dynamically fill them into the instruction template, and hence use the completed instruction containing selected key elements as our input.",
    "zebra-9": "The original document is then the corresponding output. We collect 328 such instances. Besides our curated data for tasks of summarization, long-MRC, and writing, we further incorporate 102 k training instances randomly selected from public datasets including BigPatent (Sharma et al., 2019), GovReport (Huang et al., 2021), GSM8k (Cobbe et al., 2021), CUAD (Hendrycks et al., 2021), MultiDoc2Dial (Feng et al., 2021), Multi-News (Fabbri et al., 2019), Natural Question (Kwiatkowski et al., 2019), Musique (Trivedi et al., 2022), NarrativeQA (Ko\u010disk\u00fd et al., 2018), Qasper (Dasigi et al., 2021), QMSum (Zhong et al., 2021), QuALITY (Pang et al., 2022), SPACE (Angelidis et al., 2021), SQuALITY (Wang et al., 2022), SummScreen (Chen et al., 2022), and TOEFL-QA (Tseng et al., 2016; Chung et al., 2018). These datasets cover traditional long tasks like summarization, QA, and multi-choice QA. ### 4.2 LIT Training Details\n\nThe Zebra-LIT training uses the Zebra-LCAT model as backbone and thus inherits ZebraLCAT's model structure including the grouped attention setting. Zebra-LIT is trained with $l_{s}=$ 16,384 sequence length. We pack training instances with less than $l_{s}$ tokens into the $l_{s}$-token sequences. The padding tokens are added to the right. For long instruction instances with more than $l_{s}$ tokens, we truncate only its input document but keep all the instruction text. The batch size is $2,097,152$ tokens per batch. We use Adam Optimizer with beta values of $(0.9,0.95)$ and an Adam epsilon of $1 e^{-8}$. Training incorporates a cosine learning schedule with a linear warm-up of 32 steps. The maximum learning rate is $2 e^{-5}$, and the minimum learning rate is $6 e^{-6}$. We conduct 4-epoch training with our instruction tuning data. In LIT, we calculate the loss on the output tokens $y$ only. The rest hyper-parameters are the same as LCAT training. ### 4.3 Evaluation Results\n\nWe assess the performance of our instruction-tuned model on short and long-context benchmarks, as presented in Table 7 and 8, respectively. Finetuning models on long instruction data reveals some degradation in performance on short-context benchmarks, such as MMLU, as depicted in Table 7. However, the model consistently outperforms the LLAMA-2-chat model overall. Additionally, we evaluate the model on long-context benchmarks (Shaham et al., 2023). The results, as shown in Table 8, indicate that our model performs comparably or surpasses LLAMA2-chat in QA datasets but falls short in datasets employing Rouge (Lin, 2004) evaluation metrics. It is noteworthy that the long-context evaluation of LLMs poses a non-trivial challenge, as highlighted in (Xiong et al., 2023). Automatic metrics, such as Rouge, employed in benchmarks, only consider $n$-gram overlaps with a reference, which may not consistently align with human preferences. We anticipate that incorporating more substantial and diverse finetuning data can significantly contribute to improved model performance. Overall, our instruction-tuned model demonstrates better performance on both short and long-context benchmarks, affirming the effectiveness of the architecture. ## 5 Discussion\n\n### 5.1 Scalability\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1130b80ace813be43d3cg-09.jpg?height=521&width=664&top_left_y=1533&top_left_x=1093)\n\nFigure 6: Perplexity (PPL) on Pile validation set vs number of training tokens for different model sizes and training sequence lengths. Due to resource constraints, our scalability experiments are limited to models with 117 M and 345M parameters. As illustrated in Figure 6, employing a larger context window enhances model performance for the 117 M model. However, a notable observation is the difficulty in differentiating the performance curves when the sequence length is increased from 4,096 to 16,384 tokens. We attribute this marginal difference to two primary factors. Firstly, the amount of training data, capped at\n\n| Model (7B) | cot/gsm8k | cot/math | cot/bbh | cot/mmlu | human-eval-plus | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| LLAMA2-chat | 25.0 | 3.8 | 30.7 | 40.7 | 9.8 | 22.0 |\n| Zebra-LIT | 30.4 | 2.2 | 33.6 | 38.8 | 15.2 | 24.0 |\n\nTable 7: Performance of instruction tuned 7B models on short-context benchmarks. In this evaluation, we follow the setting of FastEval (tju01), which focuses mostly on the zero-shot setting. | Model (7B) | Summarization |  |  |  | QA |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | GR | SS | QM | SQAL | Qspr | Nrtv | QALT | MuSQ | Avg. |  |  |\n| LLAMA2-chat | 11.1 | 12.7 | 14.2 | 19.0 | 14.8 | 11.0 | 38.1 | 6.7 | 15.9 |  |  |\n| Zebra-LIT | 13.6 | 5.3 | 13.6 | 17.3 | 21.4 | 22.6 | 33.3 | 12.0 | 17.4 |  |  |\n\nTable 8: Performance of instruction tuned 7B models on long-context benchmarks, validation set on ZeroScrolls.",
    "zebra-10": "20 billion tokens, is likely insufficient to manifest a significant difference. Secondly, both the training and validation datasets predominantly comprise shorter sequences, thereby inherently constraining the potential improvement in PPL. Despite these limitations, we anticipate that the benefits of the increased model size will become more apparent with larger and more varied datasets. Evidently, as the model size increases from 117 M to 345 M , the performance gains significantly, notably from 12.87 to 10.34 . ### 5.2 Efficiency\n\n| Method | $\\mathcal{O}$ Complexity |  |  |\n| :---: | :---: | :---: | :---: |\n|  | Attention | Other | Total |\n| Global | $D N^{2}$ |  | $D N^{2}+D^{2} N$ |\n| Local | $D W N$ | $D^{2} N$ | $D(D+W) N$ |\n| GA | $\\frac{D}{C^{2}} N^{2}+D W N$ |  | $\\frac{D}{C^{2}} N^{2}+D(D+W) N$ |\n| Group | $\\frac{D}{L} N^{2}+D W N$ |  | $\\frac{D}{L} N^{2}+D(D+W) N$ |\n\nTable 9: Complexities of attention and other operations. Training Efficiency: In Table 9, we present a detailed comparison of computational complexities for different attention operations, as well as other relevant operations, across the four distinct attention strategies outlined in Section 2.1.1. This comparison indicates that with an increase in sequence length, the computational demand of global attention exhibits a quadratic growth pattern. On the other hand, both the Global Approximation and Group Attention strategies, though also following a quadratic growth trajectory, do so with significantly lower coefficients. Notably, Local Attention demonstrates the best computational efficiency, requiring the least amount of resources as the sequence length extends. Inference Efficiency: Because of the implementation of local attention in the majority of Transformer layers, Zebra does not require retaining all Key-Value (K-V) pairs in the cache. Consequently, this approach significantly reduces the GPU memory requirements during inference, thereby potentially increasing the inference throughput. The detailed pseudo-code for Zebra training and inference can be found in Appendix A.1. ## 6 Related Work\n\nAttention. The Transformer architecture has a self-attention component with $O\\left(N^{2}\\right)$ computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to $O(N \\log N)$ or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020b), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021). Positional Embedding. In Transformer models, positional embeddings can be primarily categorized into two types: absolute and relative. Earlier versions of Transformers utilize absolute positional encoding. For instance, the vanilla Transformer (Vaswani et al., 2017) model adds sinusoidal positional embeddings to the word embeddings, whereas GPT (Radford et al., 2018) and BERT (Devlin et al., 2018) introduce learnable positional embeddings. Currently, it has become more common to use relative positional embedding. For instance, Transformer-XL (Dai et al., 2019) and T5 (Raffel et al., 2020) adds learnable attention logit bias into attention layers. Alibi (Press et al., 2022) biases attention scores based on the distance between key and query elements. RoPE ( Su et al., 2023) multiplies the keys and queries of every attention layer by sinusoidal embeddings. The Alibi\nand RoPE methods are further improved through the incorporation of an additional bias term (Sun et al., 2022; Chi et al., 2023). LLM. In the early stages, open-source large language models such as OPT (Zhang et al., 2022), BLOOM (Workshop et al., 2023), and Llama1 (Touvron et al., 2023) have a context window length limited to 2048 tokens. Later, a smaller open-source model Starcoder (Li et al., 2023), with 15B parameters, manage to extend its context window length to 8000 by optimizing computational efficiency with Multi-Query Attention (Shazeer, 2019) and FlashAttention (Dao et al., 2022). Following this, LLAMA-2 (Touvron et al., 2023), another open-source model, expands its default length to 4096 tokens. The open-source community then discover that by interpolating Rotary Positional Embeddings ${ }^{3}$, the context window length could be further extended to 8192 tokens. Subsequently, (Chen et al., 2023b) expand and validate this approach, known as Position Interpolation, to further enhance the window length capability. LlamA-2 undergoes extended training with long-context continual pertaining, extending up to 32,768 tokens, the positional interpolation method, and FlashAttention (Dao et al., 2022) showing its enhanced effectiveness (Xiong et al., 2023). Similar approaches are employed to extend the context length (Peng et al., 2023; Du et al., 2022; Dacheng Li* and Zhang, 2023) by fine-tuning pretrained models with long documents. LongLoRA (Chen et al., 2023c) adopts a fine-tuning approach with shifted local attention for more efficient training to further extend context length. As of December 2023, closed-source large language models have significantly expanded their context window capabilities, reaching scales of up to 100,000 tokens. For instance, GPT-4-Turbo ${ }^{4}$ supports a context window of 128,000 tokens, while Claude-2. $1^{5}$ supports up to 200,000 tokens. The commercialization of these closed-source models is heavily reliant on long context understanding. For example, it allows users to upload long text files for queries or engage in extended dialogues with extensive historical records. Long Evaluation. The majority of benchmarks for evaluating large language models are primarily\n\n[^2]focused on tasks involving short context. However, the evaluation dimensions for long context and short context may differ significantly. For example, (Liu et al., 2023) develop a multi-document question answering task, which demonstrates the phenomenon of being lost in the middle, a challenge not present in short context. The foundation for comprehensive evaluation of long context understanding is currently still underdeveloped. Recently, there have been efforts to develop benchmarks specifically for long context analysis, such as (Shaham et al., 2023; Kwan et al., 2023; Dong et al., 2023; Bai et al., 2023; An et al., 2023). ## 7 Conclusion\n\nIn this work, we introduce Zebra, a novel architecture designed to enhance the capabilities of Large Language Models (LLMs) in processing and interpreting long text sequences. Through the innovative use of grouped local-global attention layers and rotary positional embedding, Zebra addresses critical challenges associated with extending the context window in LLMs. Our extensive experiments and analyses demonstrate that Zebra not only maintains comparable performance on shortsequence benchmarks but also excels in handling longer sequences, as evidenced by its superior long benchmark performances and perplexity results on Gutenberg (PG-19). This indicates that our approach effectively balances the need for efficiency with the demand for high performance in longcontext scenarios. The grouped local-global attention mechanism, in particular, proves to be a crucial component in achieving this balance, offering a significant reduction in computational and memory requirements while maintaining, and in some cases enhancing model performance. Moreover, the application of Zebra among diverse short and long downstream tasks showcases its versatility and robustness across various NLP tasks. In conclusion, Zebra represents a significant step forward in the realm of long-context language processing. Its ability to efficiently handle extensive text sequences without compromising on performance opens up new possibilities for the application of LLMs in a variety of complex and information-rich environments. We believe that Zebra sets a new standard for long-context modeling and will inspire further innovation in the field. ## Limitations\n\nWhile our work introduces a novel model architecture and exhibits promising accuracy and efficiency, it is not without limitations. Due to computation resource constraints, we have not yet evaluated the model architecture with a parameter size larger than 7B. Though a larger model typically brings stronger performance, it is still valuable to further verify with Zebra. Moreover, our current evaluation, especially for long-context alignment tasks, largely relies on automatic metrics like Rouge and F-1 employed by public benchmarks. Such metrics evaluating $n$ gram overlapping with a reference have been under debate before the era of LLMs. We anticipate a comprehensive evaluation strategy for long-context alignment to be proposed in the future. ## References\n\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268-284, Online. Association for Computational Linguistics. Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277-293. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.",
    "zebra-11": "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In ThirtyFourth AAAI Conference on Artificial Intelligence. Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. 2023a. Skills-in-context prompting: Unlocking compositionality in large language models. arXiv preprint arXiv:2308.00304. Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. SummScreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602-8615, Dublin, Ireland. Association for Computational Linguistics. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Longlora: Efficient fine-tuning of long-context large language models. Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13522-13537, Toronto, Canada. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2022. Rethinking attention with performers. Yu-An Chung, Hung-Yi Lee, and James Glass. 2018. Supervised and unsupervised transfer learning for question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1585-1594, New Orleans, Louisiana. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai 2 reasoning challenge.",
    "zebra-12": "arXiv:1803.05457v1. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Anze Xie Ying Sheng Lianmin Zheng Joseph E. Gonzalez Ion Stoica Xuezhe Ma Dacheng Li*, Rulin Shao* and Hao Zhang. 2023. How long can open-source llms truly promise on context length? Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy. Association for Computational Linguistics. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.",
    "zebra-13": "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A.",
    "zebra-14": "Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599-4610, Online. Association for Computational Linguistics. Yuntian Deng, Volodymyr Kuleshov, and Alexander M Rush. 2022. Model criticism for long-form text generation. arXiv preprint arXiv:2210.08444. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. arXiv preprint arXiv:2309.13345. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335. Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074-1084, Florence, Italy. Association for Computational Linguistics. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. Eli5: Long form question answering. arXiv preprint arXiv:1907.09190.",
    "zebra-15": "Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. Multidoc2dial: Modeling dialogues grounded in multiple documents. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.",
    "zebra-16": "Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. Openmeva: A benchmark for evaluating open-ended story generation metrics. arXiv preprint arXiv:2105.08920. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021. Longt5: Efficient text-to-text transformer for long sequences. arXiv preprint arXiv:2112.07916. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137.",
    "zebra-17": "Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review. Yebowen Hu, Tim Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023. Meetingbank: A benchmark dataset for meeting summarization. arXiv preprint arXiv:2305.17529. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419-1436, Online. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.",
    "zebra-18": "arXiv e-prints, page arXiv:1705.03551. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. The impact of positional encoding on length generalization in transformers.",
    "zebra-19": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Tom\u00e1\u015b Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328. Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5. Wojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2021. Booksum: A collection of datasets for longform narrative summarization.",
    "zebra-20": "arXiv preprint arXiv:2105.08209. Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong. 2023. M4le: A multi-ability multirange multi-task multi-domain long-context evaluation benchmark for large language models. arXiv preprint arXiv:2310.19240. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you! Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.",
    "zebra-21": "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 115 .",
    "zebra-22": "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human-generated machine reading comprehension dataset.",
    "zebra-23": "OpenAI. 2022. Introducing chatgpt.",
    "zebra-24": "OpenAI. 2023. Gpt-4 technical report. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336-5358, Seattle, United States. Association for Computational Linguistics. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A.",
    "zebra-25": "Smith, and Lingpeng Kong. 2021. Random feature attention. Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. 2019. Blockwise selfattention for long document understanding. arXiv preprint arXiv:1911.02972. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Jack Rae and Ali Razavi. 2020. Do transformers need deep long-range memory? In Proceedings of the 58th Annual Meeting of the Association for Computational\n\nLinguistics, pages 7524-7529, Online. Association for Computational Linguistics. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale.",
    "zebra-26": "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9 th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China. Association for Computational Linguistics. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada. Association for Computational Linguistics. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zeroshot benchmark for long text understanding. arXiv preprint arXiv:2305.14196. Eva Sharma, Chen Li, and Lu Wang. 2019. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204-2213, Florence, Italy. Association for Computational Linguistics. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need.",
    "zebra-27": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.",
    "zebra-28": "Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu, and Fei Liu. 2022. Towards abstractive grounded summarization of podcast transcripts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4407-4418, Dublin, Ireland. Association for Computational Linguistics. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2023. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590-14604, Toronto, Canada. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.",
    "zebra-29": "Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.",
    "zebra-30": "Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020a. Sparse sinkhorn attention. In International Conference on Machine Learning, pages $9438-9447$. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and DaCheng Juan. 2020b. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science.",
    "zebra-31": "Steven Tey. Sharegpt. tju01. Fasteval. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,\n\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.",
    "zebra-32": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition.",
    "zebra-33": "Transactions of the Association for Computational Linguistics, 10:539-554. Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016. Towards machine comprehension of spoken content: Initial toefl listening comprehension test by machine. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30 . Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. 2022. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139-1156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.",
    "zebra-34": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837. BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on,\nYacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u015far, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, ZhengXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, JanChristoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,\n\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su , Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023. Bloom: A 176b-parameter open-access multilingual language model. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,\nSergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. 2023. Effective long-context scaling of foundation models. Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Xiaoman Pan, Linda Petzold, and Dong Yu. 2023. OASum: Large-scale open domain aspectbased summarization. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4381-4401, Toronto, Canada. Association for Computational Linguistics. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.",
    "zebra-35": "Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for querybased multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905-5921, Online. Association for Computational Linguistics. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. ArXiv, abs/2305.11206.",
    "zebra-36": "Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. Mediasum: A large-scale media interview\ndataset for dialogue summarization.",
    "zebra-37": "arXiv preprint arXiv:2103.06410. ## A Appendix\n\n## A. 1 Group Attention Implementation\n\nAs detailed in Section 2.2, our architecture employs a layerwise grouped local-global attention approach, segregating the application of local and global attention across different layers. The methodology for local attention is outlined in Algorithm 1. In contrast, the global attention mechanism adheres to the conventional multi-head attention paradigm found in transformers. It is important to note that during inference when leveraging the Key-Value ( $\\mathrm{K}-\\mathrm{V}$ ) cache, the process for local attention layers deviates from Algorithm 1. Instead, we exclusively implement the standard global attention framework while maintaining the latest $w \\mathrm{~K}-\\mathrm{V}$ states.",
    "zebra-38": "This approach streamlines the attention process while ensuring efficiency and effectiveness in inference. ## A. 2 Case Study\n\nAs discussed in Section 4.3, evaluating long-context tasks presents challenges. Firstly, common evaluation metrics may misrepresent response quality, particularly in summarization datasets where the reference summary may not align with diverse reader interests or knowledge levels. Furthermore, the limited diversity in long-context tasks may result in inadequate training for instruction tuning, often stemming from summarization, QA, or information extraction datasets with long documents. A more varied set of tasks could potentially enhance the capability of instruction-tuned models. Lastly, as demonstrated in the subsequent section, instances are frequently constructed with inaccurate gold labels or information not covered in given documents, intensifying the difficulty of model evaluation and potentially yielding erroneous results. We present exemplars and results employing Zebra, LlAMA, and ChatGPT-3.5 on ZeroScrolls (Shaham et al., 2023).",
    "zebra-39": "Due to the limited space and long documents, summarization examples are not presented. ```\nAlgorithm 1: Local Attention\n    Data: \\(Q, K, V \\in \\mathcal{R}^{b s z \\times l e n \\times n \\_h e a d s \\times d i m \\_p e r \\_h e a d}\\)\n    Data: \\(M \\in[0,1]^{b s z \\times n \\_ \\text {heads } \\times \\text { len } \\times l e n}\\) is attention mask\n    Data: \\(w\\) is the window size\n    Data: \\(\\lambda\\) is the normalizing factor\n    // padding the sequence length to multiple of window size\n    // after padding shape:\n    // bsz \u00d7 padded_len \\(\\times\\) n_heads \\(\\times\\) dim_per_head\n    \\(Q \\leftarrow\\) pad_to_multiple \\((Q, w)\\)\n    \\(2 K \\leftarrow\\) pad_to_multiple \\((K, w)\\);\n    \\(V \\leftarrow\\) pad_to_multiple \\((V, w)\\)\n    // split \\(Q, K, V\\) into blocks\n    // after spliting shape:\n    // bsz \u00d7 n_blocks \\(\\times w \\times n \\_h e a d s \\times d i m \\_p e r \\_h e a d\\)\n    \\(4 Q_{\\text {local }} \\leftarrow\\) split_into_blocks \\((Q)\\);\n    \\(5 K_{\\text {local }} \\leftarrow\\) split_into_blocks \\((K)\\);\n    6 Vlocal \\(\\leftarrow\\) split_into_blocks \\((V)\\);\n    // for \\(K\\) and \\(V\\) merge each block and the blocks before it\n    // after merging shape:\n    // bsz \u00d7 n_blocks \\(\\times 2 * w \\times n \\_h e a d s \\times d i m \\_p e r \\_h e a d\\)\n    \\(7 K_{\\text {local }} \\leftarrow\\) concatenate_2_blocks \\((K)\\);\n    8 Vlocal \\(\\leftarrow\\) concatenate_2_blocks \\((V)\\);\n    // calculate attention score\n    // the attention score shape:\n    // bsz \\(\\times\\) n_heads \\(\\times\\) n_blocks \\(\\times w \\times 2 * w\\)\n    9 attn_score \\(\\leftarrow\\);\n10 torch.einsum ( \\(\\left.{ }^{\\prime} \\ldots q h d, \\ldots k h d->\\ldots h q k^{\\prime}, Q_{\\text {local }}, K_{\\text {local }}\\right)\\);\n\\(11 \\quad\\).transpose \\((1,2)\\)\n    // multiply with the normalizing factor\n12 attn_score \\(\\leftarrow \\lambda *\\) attn_score ;\n    // Extract the attention from the original attention mask\n\\(13 M_{\\text {original }} \\leftarrow\\) extract_original_blocks \\((M, w)\\);\n    // Prepare the attention for a single block \\((w \\times 2 * w\\)\n    \\(14 M_{\\text {block }} \\leftarrow\\) create_blockwise_mask \\((w)\\);\n    // Taking both attention mask into consideration\n    \\(15 M_{\\text {overall }} \\leftarrow M_{\\text {original }} \\cap M_{\\text {block }}\\)\n    // merge the sequence length dim\n    // after merging shape:\n    // bsz \\(\\times n_{\\_} h e a d s \\times\\) padded_len \\(\\times 2 * w\\)\n16 new_shape \\(\\leftarrow\\left(b s z, n \\_h e a d s\\right.\\), padded_len, \\(\\left.2 * w\\right)\\);\n17 attn_score \\(\\leftarrow\\) attn_score.reshape(new_shape) ;\n\\(18 M_{\\text {overall }} \\leftarrow M_{\\text {overall }}\\). reshape(new_shape);\n    // softmax\n    19 attn_pro \\(6 \\leftarrow \\operatorname{softmax}\\left(\\right.\\) attn_score,\\(\\left.M_{\\text {overall }}\\right)\\);\n    // reshape back in block format\n    // shape: bsz \\(\\times\\) n_blocks \\(\\times w \\times n \\_h e a d s \\times 2 * w\\)\n20 new_shape \\(\\leftarrow\\left(b s z, n \\_h e a d s, n \\_b l o c k s, w, 2 * w\\right)\\);\n21 attn_prob \\(\\leftarrow\\) attn_prob\u0304.reshape(new_shape).transpose \\((1,2)\\) \uff1b\n    // get context vector\n    // shape: bsz \\(\\times\\) n_blocks \\(\\times w \\times n \\_h e a d s \\times d i m \\_p e r \\_h e a d\\)\n22 attn_outputs \\(\\leftarrow\\);\n23 torch.einsum \\(\\left({ }^{\\prime} \\ldots h q d, \\ldots k h d->\\ldots q h d^{\\prime}\\right.\\), attn_prob,\\(\\left.V_{\\text {local }}\\right)\\) \uff1b\n    // reshape to output format\n24 new_shape \\(\\leftarrow\\left(\\right.\\) bsz, padded_len, \\(\\left.n \\_h e a d s, d i m \\_p e r \\_h e a d\\right)\\);\n    // Don't forget to remove the padding ones\n25 attn_outputs \\(\\leftarrow\\) attn_outputs.reshape(new_shape)[:, seq_len, :, :];\n    Result: attn_outputs\n```\n\n\n## NarrativeQA\n\nInstruction You are given a story, which can be either a novel or a movie script, and a question.",
    "zebra-40": "Answer the question as concisely as you can, using a single phrase if possible.",
    "zebra-41": "Do not provide any explanation. Document [...] INTRODUCTION. The Crito seems intended to exhibit the character of Socrates in one light only, not as the philosopher, fulfilling a divine mission and trusting in the will of heaven, but simply as the good citizen, who having been unjustly condemned is willing to give up his life in obedience to the laws of the state... The days of Socrates are drawing to a close; the fatal ship has been seen off Sunium, as he is informed by his aged friend and contemporary Crito, who visits him before the dawn has broken; he himself has been warned in a dream that on the third day he must depart. Time is precious, and Crito has come early in order to gain his consent to a plan of escape. This can be easily accomplished by his friends, who will incur no danger in making the attempt to save him, but will be disgraced for ever if they allow him to perish. He should think of his duty to his children, and not play into the hands of his enemies. Money is already provided by Crito as well as by Simmias and others, and he will have no difficulty in finding friends in Thessaly and other places. Socrates is afraid that Crito is but pressing upon him the opinions of the many: whereas, all his life long he has followed the dictates of reason only and the opinion of the one wise or skilled man. There was a time when Crito himself had allowed the propriety of this. And although some one will say 'the many can kill us,' that makes no difference; but a good life, in other words, a just and honourable life, is alone to be valued. All considerations of loss of reputation or injury to his children should be dismissed: the only question is whether he would be right in attempting to escape. Crito, who is a disinterested person not having the fear of death before his eyes, shall answer this for him. Before he was condemned they had often held discussions, in which they agreed that no man should either do evil, or return evil for evil, or betray the right. Are these principles to be altered because the circumstances of Socrates are altered? Crito admits that they remain the same. Then is his escape consistent with the maintenance of them? To this Crito is unable or unwilling to reply. Socrates proceeds:-Suppose the Laws of Athens to come and remonstrate with him: they will ask 'Why does he seek to overturn them?' and if he replies, 'they have injured him,' will not the Laws answer, 'Yes, but was that the agreement? Has he any objection to make to them which would justify him in overturning them? Was he not brought into the world and educated by their help, and are they not his parents? He might have left Athens and gone where he pleased, but he has lived there for seventy years more constantly than any other citizen.' Thus he has clearly shown that he acknowledged the agreement, which he cannot now break without dishonour to himself and danger to his friends. Even in the course of the trial he might have proposed exile as the penalty, but then he declared that he preferred death to exile. And whither will he direct his footsteps? In any well-ordered state the Laws will consider him as an enemy. Possibly in a land of misrule like Thessaly he may be welcomed at first, and the unseemly narrative of his escape will be regarded by the inhabitants as an amusing tale. But if he offends them he will have to learn another sort of lesson. Will he continue to give lectures in virtue? That would hardly be decent. [...]\nQuestion Who persuades Socrates to escape? Ground Truth Crito, Crito convinces Socrates to leave his cell. Zebra Output Crito $\\checkmark$\nLlama Output According to the text, it is Crito who persuades Socrates to escape. ChatGPT-3.5 Output Crito $\\checkmark$\n\nIn this example, while all models provide correct answers, LLAMA responds in a more verbose manner, deviating from the given instruction. Qasper\nInstruction You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation. Document [...] For all the features, we separated the words in the tweets based on the language annotation (Section 2.3) and prepared the feature vector for each tweet by combining the vectors for both the languages . Previous researches shows that letter n-grams are very efficient for classifying text. They are language independent and does not require expensive text pre-processing techniques like tokenization, stemming and stop words removal, hence in the case of code-mix texts, this could yield good results BIBREF16, BIBREF17. Since the number of n-grams can be very large we took trigrams which occur more than ten times in the corpus. For classifying humor in texts, it is important to understand the semantics of the sentence. Thus, we took a three word window as a feature to train our classification models to incorporate the contextual information. Many jokes and idioms sometimes have common words. We identified those words and took them as as a feature for classification. In the preprocessing step, we decomposed hashtags using camel cases and added them along with the words. Hence, common words in the hashtags were also included in the feature vector. Classification Approach and\nResultsWe experimented with four different classifiers, namely, support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19. Chi square feature selection algorithm is applied to reduces the size of our feature vector. For training our system classifier, we used Scikit-learn BIBREF19 . 10-fold cross validation on 3543 code-mixed tweets was carried out by dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing. Mean accuracy is calculated by taking the average of the accuracy obtained in each iteration of the testing process. Table 2 shows the accuracy for each feature when trained using mentioned classifiers along with the accuracy when all the features are used along with the overall accuracy. Support vector machine with radial basis function kernel and extra tree classifier performs better than other classifiers and yields $69.3 \\%$ and $67.8 \\%$ accuracy respectively. The reason kernel SVM yields the best result is because the number of observations is greator than the number of features BIBREF20 . N-grams proved to be the most efficient in all classification models followed by common words and hastags. Bag-of-words feature performed the worst in SVM, random forest and extra tree classifier but yielded better result in naive bayes classifiers. Accuracies mentioned in table 2 were calculated using fine tuning of model parameters using grid search. Conclusion and Future Work\nIn this paper, we describe a freely available corpus of 3453 English-Hindi code-mixed tweets. The tweets are annotated with humorous $(\\mathrm{H})$ and non-humorous $(\\mathrm{N})$ tags along with the language tags at the word level. The task of humor identification in social media texts is analyzed as a classification problem and several machine learning classification models are used. The features used in our classification system are n-grams, bag-of-words, common words and hashtags. N-grams when trained with support vector machines with radial basis function kernel performed better than other features and yielded an accuracy of $68.5 \\%$. The best accuracy ( $69.3 \\%$ ) was given by support vector machines with radial basis function kernel. This paper describes the initial efforts in automatic humor detection in code-mixed social media texts. Corpus can be annotated with part-of-speech tags at the word level which may yield better results in language detection. [...]\nQuestion What type of system does the baseline classification use? Ground Truth support vector machine BIBREF18, random forest, extra tree and naive bayes classifier BIBREF19, Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers. Zebra Output The baseline classification uses n-grams, bag-of-words, common words and hashtags. $\\checkmark$\nLlamA Output to the article, the baseline classification system uses a support vector machine (SVM) classifier. $X$\nChatGPT-3.5 Output Supervised classification system $X$\n\n## Musique\n\nInstruction You are given several paragraphs from Wikipedia and a question.",
    "zebra-42": "Answer the question as concisely as you can, using a single phrase if possible.",
    "zebra-43": "If the question cannot be answered based on the information in the paragraphs, write \"unanswerable\". Do not provide any explanation. Document $[\\ldots$.$] Title: Israel$\nIsrael, officially the State of Israel, is a country in the Middle East, on the southeastern shore of the Mediterranean Sea and the northern shore of the Red Sea. It has land borders with Lebanon to the north, Syria to the northeast, Jordan on the east, the Palestinian territories of the West Bank and Gaza Strip to the east and west, respectively, and Egypt to the southwest. The country contains geographically diverse features within its relatively small area. Israel's economy and technology center is Tel Aviv, while its seat of government and proclaimed capital is Jerusalem, although the state's sovereignty over East Jerusalem is not recognised internationally. The population of Israel was estimated in 2017 to be 8,777,580 people, of whom $74.7 \\%$ were Jewish, $20.8 \\%$ Arab and 4.5\\% others. Title: The World (archipelago)\nThe World or The World Islands, (\"Juzur al-\u0100lam\") is an artificial archipelago of various small islands constructed in the rough shape of a world map, located in the waters of the Persian Gulf, off the coast of Dubai, United Arab Emirates. The World islands are composed mainly of sand dredged from Dubai's shallow coastal waters, and are one of several artificial island developments in Dubai. The World's developer is Nakheel Properties, and the project was originally conceived by Sheikh Mohammed bin Rashid Al Maktoum, the ruler of Dubai. The actual construction was done by two Dutch (joint venture) specialist companies, Van Oord and Boskalis. The same companies also created the Palm Jumeirah. Title: Israel\nIsrael is considered the most advanced country in Southwest Asia and the Middle East in economic and industrial development. Israel's quality university education and the establishment of a highly motivated and educated populace is largely responsible for spurring the country's high technology boom and rapid economic development. In 2010, it joined the OECD. The country is ranked 3rd in the region and 38th worldwide on the World Bank's Ease of Doing Business Index as well as in the World Economic Forum's Global Competitiveness Report. It has the second-largest number of startup companies in the world (after the United States) and the largest number of NASDAQ-listed companies outside North America. Title: Israel\nIsrael is one of the world's technological leaders in water technology. In 2011, its water technology industry was worth around $\\$ 2$ billion a year with annual exports of products and services in the tens of millions of dollars. The ongoing shortage of water in the country has spurred innovation in water conservation techniques, and a substantial agricultural modernization, drip irrigation, was invented in Israel. [...]\nQuestion When was the region that lies to the north of the region Israel is located and where The World is located created? Ground Truth 1930\nZebra Output 1946 X\nLlama Output The region that lies to the north of Israel was created in the mid-18th century. $X$ ChatGPT-3.5 Output unanswerable $X$\n\nThis example underscores that the pertinent information for the given question cannot be discerned within the context, indicating that certain examples may not have been accurately constructed. The paragraphs do not provide information to answer when the region north of Israel and The World archipelago was created. There is information about Israel and The World islands located in Dubai, but no mention of the region to the north of Israel. The Zebra and Llama models manifest hallucinations, whereas GPT answers accurately following the given instruction. ## Musique\n\nInstruction You are given several paragraphs from Wikipedia and a question. Answer the question as concisely as you can, using a single phrase if possible. If the question cannot be answered based on the information in the paragraphs, write \"unanswerable\". Do not provide any explanation. Document Title: Jerome Quinn\nBorn in Green Bay, Wisconsin, Quinn was a realtor and served on the Green Bay Common Council, the Brown County, Wisconsin Board of Supervisors, the local Board of Education, and the Wisconsin State Assembly from 1955 until 1973. He was a Republican. Title: Max Rosenthal Tenement in Bydgoszcz\nMax Rosenthal House is a historical tenement located at Gda\u0144ska Street ${ }^{\\circ} 42$ in downtown Bydgoszcz, Poland, built when the city was part of the Kingdom of Prussia. It is registered on the Kuyavian-Pomeranian Voivodeship Heritage List. Title: Borders of China\nChina shares international borders with 14 sovereign states. In addition, there is a $30-\\mathrm{km}$ border with the special administrative region of Hong Kong, which was a British dependency before 1997, and a 3 km border with Macau, a Portuguese territory until 1999. With a land border of 22,117 kilometres $(13,743 \\mathrm{mi})$ in total it also has the longest land border of any country. Title: Notogawa, Shiga\nNotogawa Station (Location: N35.179899,E136.165913) is the only Japan Railway station in Higashiomi. The station is a rapid stop on the JR Biwako Line, located between stations in Omi-Hachiman to the east and Hikone to the west. The town shares a small border with Lake Biwa to the northwest. Title: Tenement at Gdanska street 75, Bydgoszcz\nTenement at Gdanska street 75 is a historical habitation building located at Gda\u0144ska Street N ${ }^{\\circ} 75$, in Bydgoszcz. It is registered on the Kuyavian-Pomeranian Voivodeship Heritage List. Title: Tenement (band)\nTenement is a three piece rock band from Appleton, Wisconsin formed in 2006. They are often associated with the American hardcore punk scene. Their recorded output has been described as everything from \"noise pop\" to \"fuzz punk\", while in a live setting they are often known for experimentation, improvisation, and high-energy performance. The visual art of singer/guitarist Amos Pitsch is associated with most of their records, as well as several records by other notable punk and hardcore bands. In 2016, they were included in \"Rolling Stone's\" \"10 Great Modern Punk Bands\". Title: Pulaski High School\nPulaski High School is a public high school in Pulaski, Wisconsin, in Brown County, Wisconsin (school district also serves parts of Shawano, Outagamie and Oconto counties), that serves students in grades 9 through 12. Its mascot is the Red Raider.[...]\nQuestion What is the capital of the county that shares a border with the county where the band Tenement was formed? Ground Truth Green Bay\nZebra Output Appleton $X$\nLlama Output Appleton. $X$\nChatGPT-3.5 Output Appleton, Wisconsin $X$\nThis example shows that the ground truth is not correct. The paragraph states that the band Tenement is \"a three piece rock band from Appleton, Wisconsin formed in 200\". There is no information about the capital of the county that shares a border with Appleton in the document but Appleton is the closest answer given the context. [^0]:    *Equal Contribution\n\n[^1]:    ${ }^{1}$ https://github.com/NVIDIA/Megatron-LM\n    ${ }^{2}$ https://www.deepspeed.ai/tutorials/ flops-profiler/\n\n[^2]:    ${ }^{3}$ https://www.reddit.com/r/LocalLLaMA/comments/ 14fgjqj/a_simple_way_to_extending_context_to_8k\n    ${ }^{4}$ https://platform.openai.com/docs/models/ gpt-4-and-gpt-4-turbo\n    ${ }^{5}$ https://docs.anthropic.com/claude/reference/ selecting-a-model\n\n"
}