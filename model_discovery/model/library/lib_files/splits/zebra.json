{
    "zebra-0": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention\n\nKaiqiang Song\u2217, Xiaoyang Wang\u2217, Sangwoo Cho\u2217, Xiaoman Pan, Dong Yu Tencent AI Lab, Seattle {riversong, shawnxywang, swcho, xiaomanpan, dyu}@global.tencent.com\n\nAbstract\n\nThis paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information.",
    "zebra-1": "Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra\u2019s alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra\u2019s performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency. 11footnotetext: Equal Contribution\n\n1 Introduction\n\nTo effectively leverage the power of Long Context in Large Language Models (LLMs), it is essential to develop and refine techniques that enable these models to process and interpret extensive text sequences accurately.",
    "zebra-2": "This capability is particularly significant in applications that demand deep understanding and synthesis of large volumes of information, such as summarization Huang et al.",
    "zebra-3": "(2021); Hu et al. (2023); Song et al. (2022); Kry\u015bci\u0144ski et al. (2021), reading comprehension Nguyen et al. (2016); Fan et al. (2019); Zhong et al. (2021); Yang et al. (2023), long-form generation Guan et al. (2021); Deng et al. (2022); Roziere et al. (2023), and complex reasoning Wei et al. (2022); Yao et al. (2023); Chen et al. (2023a). However, it is challenging to extend the context window from different viewpoints: First, the predominant LLM model uses Transformer architecture Vaswani et al.",
    "zebra-4": "(2017). Such models like BERT Devlin et al. (2018), GPT OpenAI (2023), and T5 Raffel et al. (2020) employ full attention in each layer which inherently incurs quadratic time and memory complexity. This may potentially diminish the efficiency of both the training and inference processes. Second, attention computation over an extremely long sequence might lead to an almost even distribution, potentially causing the omission of vital information Han et al.",
    "zebra-5": "(2023). This may further lead to the issue of being \u201clost in the middle\u201d Liu et al. (2023). Finally, the distribution of training signals for long and short sequences is imbalanced. It is evident that longer sequences are infrequent in both plain text and instruction-tuning data. Consequently, this rarity poses a challenge in effectively capturing long-term dependencies during the training process. To tackle the above issues, we propose to group local-global attention layers into blocks during the training and inference phases. This strategy enhances efficiency while yielding results comparable to those of a global attention Transformer. Notably, it attains equivalent performance levels with merely half the computational effort required for training. Additionally, this approach significantly reduces memory consumption during inference by maintaining a local Key-Value (K-V) cache specifically for the local attention layers. In Section 2.1, we list the two critical components essential for a long-context model as well as the potential alternatives for consideration. These encompass diverse attention mechanisms and methodologies for positional embedding. Subsequently, in Section 2.2, we conduct a comparative analysis of these alternatives, presenting their empirical outcomes for a comprehensive evaluation. Integrating these insights, we name our model Zebra, drawing an analogy to the alternating black and white stripes of a zebra, which resemble the grouped local and global layers in our model\u2019s architecture. To validate the proposed model at large scales, Section 3 details the continuation of training the Llama-2-7B model Touvron et al. (2023) using long-context adaptation training through Zebra. This approach not only exhibits comparable performance on short-sequence benchmarks but also achieves superior perplexity results for longer sequences. Additionally, in Section 4, we conduct fine-tuning of Zebra using a combination of both short and long instruction-tuning datasets. This is followed by a systematic evaluation of the model\u2019s performance across a range of benchmark datasets. It demonstrates generally better performance on both long and short benchmarks compared to Llama-2-7b-chat. To Conclude, our contribution is 3-fold:\n\n\u2022\n\nWe develop a novel architecture, referred to as Zebra, which incorporates grouped local-global attention layers and rotary positional embedding. \u2022\n\nWe conduct comprehensive experiments and detailed analyses of the Zebra framework across various settings, including pretraining from scratch, continuation of training, and extensive instruction tuning. The findings from these results demonstrate the advantage of Zebra model architecture. \u2022\n\nAdditionally, we analyze the training and inference efficiency for Zebra and provide the pseudocode for implementation. 2 Zebra\n\n2.1 Model Architecture Design\n\nTo extend the context window for Transformer models, two critical elements must be addressed: First, the Attention mechanism that allows the model to efficiently focus on and process relevant parts of long sequences.",
    "zebra-6": "However, it is important to note that the computational of attention escalates quadratically, leading to a decrease in efficiency as the length of the sequence increases. Consequently, addressing this computational challenge is essential for maintaining effectiveness over longer sequences. Second, the Positional Embedding that imparts a structured signal indicative of the sequential order of tokens. It is vital to employ a positional embedding that is not only robust but also exhibits strong generalization capabilities, particularly for processing long sequences. This ensures the model\u2019s effectiveness in maintaining sequence integrity over longer spans of data. 2.1.1 Attention\n\nIn Figure (1(a), 1(b), 1(c)), we showcase three representative variants of single attention layers including global attention, local attention, and local attention with global approximations. Additional sparse attention models like blockwise attentionQiu et al. (2019), dilated window attention Beltagy et al. (2020), stride attention Child et al. (2019), Sinkhorn Attention Tay et al. (2020a), transient global attention Guo et al. (2021) are considered potential alternatives for basic local attention. For the sake of clarity and focus in our research, we confine our analysis to two primary variants: local attention and local attention with global approximations. This decision allows for a more targeted exploration of these specific attention mechanisms within our work. Moreover, we also consider using different strategies among different layers. In Figure 1(d), we combine several local layers with one global attention layer as a group and stack such groups for the model. Considering one head of the self-attention layer in a decoder-only transformer, the query, key, and value of -th position and -th layer are defined as projections of the last layer hidden states :\n\n\ud835\udc2a i ( l ) = subscript superscript \ud835\udc2a \ud835\udc59 \ud835\udc56 absent \\displaystyle\\mathbf{q}^{(l)}_{i}= W q T \u200b \ud835\udc21 ( l \u2212 1 ) i superscript subscript \ud835\udc4a \ud835\udc5e \ud835\udc47 superscript \ud835\udc21 subscript \ud835\udc59 1 \ud835\udc56 \\displaystyle W_{q}^{T}\\mathbf{h}^{(l-1)_{i}} (1) \ud835\udc24 i ( l ) = subscript superscript \ud835\udc24 \ud835\udc59 \ud835\udc56 absent \\displaystyle\\mathbf{k}^{(l)}_{i}= W k T \u200b \ud835\udc21 ( l \u2212 1 ) i superscript subscript \ud835\udc4a \ud835\udc58 \ud835\udc47 superscript \ud835\udc21 subscript \ud835\udc59 1 \ud835\udc56 \\displaystyle W_{k}^{T}\\mathbf{h}^{(l-1)_{i}} (2) \ud835\udc2f i ( l ) = subscript superscript \ud835\udc2f \ud835\udc59 \ud835\udc56 absent \\displaystyle\\mathbf{v}^{(l)}_{i}= W v T \u200b \ud835\udc21 ( l \u2212 1 ) i superscript subscript \ud835\udc4a \ud835\udc63 \ud835\udc47 superscript \ud835\udc21 subscript \ud835\udc59 1 \ud835\udc56 \\displaystyle W_{v}^{T}\\mathbf{h}^{(l-1)_{i}} (3)\n\nWe denote the similarity between -th query and -th key as:\n\nSim \u200b ( i , j ) = e \u200b x \u200b p \u200b ( \ud835\udc2a i T \u200b \ud835\udc24 j / D ) Sim \ud835\udc56 \ud835\udc57 \ud835\udc52 \ud835\udc65 \ud835\udc5d superscript subscript \ud835\udc2a \ud835\udc56 \ud835\udc47 subscript \ud835\udc24 \ud835\udc57 \ud835\udc37 \\textit{Sim}(i,j)=exp(\\mathbf{q}_{i}^{T}\\mathbf{k}_{j}/\\sqrt{D}) (4)\n\nwhere is a normalized factor usually equal to the model dimension. Global Attention: It is the most common attention, where each token has attention to all the positions before it and itself:\n\n\u03b1 i , j = Sim \u200b ( i , j ) \u2211 t = 0 i Sim \u200b ( i , t ) subscript \ud835\udefc \ud835\udc56 \ud835\udc57 Sim \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc61 0 \ud835\udc56 Sim \ud835\udc56 \ud835\udc61 \\alpha_{i,j}=\\frac{\\textit{Sim}(i,j)}{\\sum_{t=0}^{i}\\textit{Sim}(i,t)} (5)\n\nwhere is the attention value of -th query over -th key. The context vector is then defined as a weighted sum of value vectors:\n\nc \u200b o \u200b n \u200b t \u200b e \u200b x \u200b t i = \u2211 j = 0 i \u03b1 i , j \u200b \ud835\udc2f j \ud835\udc50 \ud835\udc5c \ud835\udc5b \ud835\udc61 \ud835\udc52 \ud835\udc65 subscript \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc57 0 \ud835\udc56 subscript \ud835\udefc \ud835\udc56 \ud835\udc57 subscript \ud835\udc2f \ud835\udc57 context_{i}=\\sum_{j=0}^{i}\\alpha_{i,j}\\mathbf{v}_{j} (6)\n\nLocal Attention: Each query only considers the key-value pairs within its local window. \u03b1 i , j = Sim \u200b ( i , j ) \u2211 t = m \u200b i \u200b n \u200b ( 0 , i \u2212 w ) i Sim \u200b ( i , t ) subscript \ud835\udefc \ud835\udc56 \ud835\udc57 Sim \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc61 \ud835\udc5a \ud835\udc56 \ud835\udc5b 0 \ud835\udc56 \ud835\udc64 \ud835\udc56 Sim \ud835\udc56 \ud835\udc61 \\alpha_{i,j}=\\frac{\\textit{Sim}(i,j)}{\\sum_{t=min(0,i-w)}^{i}\\textit{Sim}(i,t)} (7)\n\nwhere is the window size of local attention. Local Attention w/ Global Approximation: Inspired by transient global attention Guo et al. (2021), we approximate the global attention output by combining multiple non-local tokens as a chunk, and take the attention over local tokens and non-local chunks. Each non-local chunk\u2019s key-value pairs are estimated using the following equations:\n\n\ud835\udc24 ^ j = subscript ^ \ud835\udc24 \ud835\udc57 absent \\displaystyle\\mathbf{\\hat{k}}_{j}= \u2211 t = ( j \u2212 1 ) \u2217 c j \u2217 c \u2212 1 \ud835\udc24 t + l \u200b n \u200b ( c ) superscript subscript \ud835\udc61 \ud835\udc57 1 \ud835\udc50 \ud835\udc57 \ud835\udc50 1 subscript \ud835\udc24 \ud835\udc61 \ud835\udc59 \ud835\udc5b \ud835\udc50 \\displaystyle\\sum_{t=(j-1)*c}^{j*c-1}\\mathbf{k}_{t}+ln(c) (8) \ud835\udc2f ^ j = subscript ^ \ud835\udc2f \ud835\udc57 absent \\displaystyle\\mathbf{\\hat{v}}_{j}= \u2211 t = ( j \u2212 1 ) \u2217 c j \u2217 c \u2212 1 \ud835\udc2f t + l \u200b n \u200b ( c ) superscript subscript \ud835\udc61 \ud835\udc57 1 \ud835\udc50 \ud835\udc57 \ud835\udc50 1 subscript \ud835\udc2f \ud835\udc61 \ud835\udc59 \ud835\udc5b \ud835\udc50 \\displaystyle\\sum_{t=(j-1)*c}^{j*c-1}\\mathbf{v}_{t}+ln(c) (9)\n\nwhere is the chunk size, and is a compensation term for each chunk. Layerwise Grouped Local-Global Attention: Instead of using identical layers for the entire network, we propose to use grouped local-global attention layers. In figure 1(d), we group every layer and use only one global attention layer at the first layer of each group. We apply local attention described in Equation (7) for the remaining layers. h ( l ) = { G-Block \u200b ( h ( l \u2212 1 ) ) l mod L = = 0 L-Block \u200b ( h ( l \u2212 1 ) ) otherwise h^{(l)}=\\left\\{\\begin{array}[]{lr}\\textit{G-Block}(h^{(l-1)})&l\\mod L==0\\\\\n\\textit{L-Block}(h^{(l-1)})&\\text{otherwise}\\\\\n\\end{array}\\right. (10)\n\nTo simplify, we use Group Attention to denote the layerwise grouped local-global attention. 2.1.2 Positional Embedding\n\nIn the Transformer architecture, positional embeddings are commonly used to encode the sequence order information. In this study, we incorporate three widely recognized types of positional embeddings to facilitate a comprehensive analysis. Absolute Positional Embedding: The vanilla Transformer Vaswani et al. (2017) advocates to use an absolute sinusoidal positional embedding:\n\nP \u200b E \u200b ( p \u200b o \u200b s , 2 \u200b i ) = \ud835\udc43 \ud835\udc38 \ud835\udc5d \ud835\udc5c \ud835\udc60 2 \ud835\udc56 absent \\displaystyle PE(pos,2i)= s \u200b i \u200b n \u200b ( p \u200b o \u200b s / 10000 2 i / d ) ) \\displaystyle sin(pos/10000^{2i/d)}) P \u200b E \u200b ( p \u200b o \u200b s , 2 \u200b i + 1 ) = \ud835\udc43 \ud835\udc38 \ud835\udc5d \ud835\udc5c \ud835\udc60 2 \ud835\udc56 1 absent \\displaystyle PE(pos,2i+1)= c \u200b o \u200b s \u200b ( p \u200b o \u200b s / 10000 2 \u200b i / d ) \ud835\udc50 \ud835\udc5c \ud835\udc60 \ud835\udc5d \ud835\udc5c \ud835\udc60 superscript 10000 2 \ud835\udc56 \ud835\udc51 \\displaystyle cos(pos/10000^{2i/d})\n\nwhere is the position index, is the model dimension, and is the iterative variable for different dimensions. After the work of the vanilla Transformer, a trainable absolute positional embedding has been introduced Devlin et al. (2018); Radford et al. (2018), serving as a replacement for the fixed sinusoidal pattern. Such positional embedding is directly added to the semantic embedding:\n\nEMB \u200b ( w \u200b o \u200b r \u200b d , p \u200b o \u200b s ) = WE \u200b ( w \u200b o \u200b r \u200b d ) + PE \u200b ( p \u200b o \u200b s ) EMB \ud835\udc64 \ud835\udc5c \ud835\udc5f \ud835\udc51 \ud835\udc5d \ud835\udc5c \ud835\udc60 WE \ud835\udc64 \ud835\udc5c \ud835\udc5f \ud835\udc51 PE \ud835\udc5d \ud835\udc5c \ud835\udc60 \\textit{EMB}(word,pos)=\\textit{WE}(word)+\\textit{PE}(pos)\\vspace{-0.1in} (11)\n\nwhere is the input token index, and is the absolute position index. Most recently, the relative positional embeddings Shaw et al. (2018); Yang et al. (2019) are introduced to eliminate the positional bias while improving the performance. These approaches also facilitate the model\u2019s ability to extend its contextual window, a process known as position extrapolation. Within this framework, two principal types of relative positional embeddings are taken into consideration. Alibi Positional Embedding Press et al. (2022), which applies the relative positional embedding by directly adding a bias term to the attention matrix. \u03b1 i , j = Softmax j i \u200b ( Sim \u200b ( i , j ) \u2212 ( i \u2212 j ) \u2217 m ) subscript \ud835\udefc \ud835\udc56 \ud835\udc57 subscript superscript Softmax \ud835\udc56 \ud835\udc57 Sim \ud835\udc56 \ud835\udc57 \ud835\udc56 \ud835\udc57 \ud835\udc5a \\alpha_{i,j}=\\textit{Softmax}^{i}_{j}(\\textit{Sim}(i,j)-(i-j)*m) (12)\n\nwhere is a head-specific scalar and is the relative distance between query and key positions. By canceling out the term, we have\n\n\u03b1 i , j = Softmax j i \u200b ( Sim \u200b ( i , j ) + j \u2217 m ) subscript \ud835\udefc \ud835\udc56 \ud835\udc57 subscript superscript Softmax \ud835\udc56 \ud835\udc57 Sim \ud835\udc56 \ud835\udc57 \ud835\udc57 \ud835\udc5a \\alpha_{i,j}=\\textit{Softmax}^{i}_{j}(\\textit{Sim}(i,j)+j*m) (13)\n\nRotary Positional Embedding Su et al. (2023) rotates the conjugate dimensions of query and key vectors, such that the relative distance is encoded during the attention calculation. \ud835\udc2a ~ = ~ \ud835\udc2a absent \\displaystyle\\mathbf{\\widetilde{q}}= ( W q T \u200b \ud835\udc21 i ) \u200b e \ud835\udc22 \u200b ( i \u200b \u03b8 ) superscript subscript \ud835\udc4a \ud835\udc5e \ud835\udc47 subscript \ud835\udc21 \ud835\udc56 superscript \ud835\udc52 \ud835\udc22 \ud835\udc56 \ud835\udf03 \\displaystyle(W_{q}^{T}\\mathbf{h}_{i})e^{\\mathbf{i}(i\\theta)} (14) \ud835\udc24 ~ = ~ \ud835\udc24 absent \\displaystyle\\mathbf{\\widetilde{k}}= ( W k T \u200b \ud835\udc21 i ) \u200b e \ud835\udc22 \u200b ( i \u200b \u03b8 ) superscript subscript \ud835\udc4a \ud835\udc58 \ud835\udc47 subscript \ud835\udc21 \ud835\udc56 superscript \ud835\udc52 \ud835\udc22 \ud835\udc56 \ud835\udf03 \\displaystyle(W_{k}^{T}\\mathbf{h}_{i})e^{\\mathbf{i}(i\\theta)}\\vspace{-0.1in} (15)\n\nwhere denotes the imaginary unit, and is the positional index. For each pair of conjugate dimensions, the similarity between query and key can be written as:\n\nSim \u200b ( i , j ) = RE \u200b [ ( W q T \u200b \ud835\udc21 i ) T \u200b ( W k T \u200b \ud835\udc21 j ) \u200b e i \u200b ( i \u2212 j ) \u200b \u03b8 ] Sim \ud835\udc56 \ud835\udc57 RE delimited-[] superscript superscript subscript \ud835\udc4a \ud835\udc5e \ud835\udc47 subscript \ud835\udc21 \ud835\udc56 \ud835\udc47 superscript subscript \ud835\udc4a \ud835\udc58 \ud835\udc47 subscript \ud835\udc21 \ud835\udc57 superscript \ud835\udc52 i \ud835\udc56 \ud835\udc57 \ud835\udf03 \\textit{Sim}(i,j)=\\textit{RE}[(W_{q}^{T}\\mathbf{h}_{i})^{T}(W_{k}^{T}\\mathbf{h}_{j})e^{\\textbf{i}(i-j)\\theta}]\\vspace{-0.1in} (16)\n\nwhere RE takes the real value of the complex number. The overall similarity is consequently defined as the cumulative measure of similarities across all corresponding dimensions. 2.2 Experiments for Model Design\n\nWe conduct experiments with various attention strategies and positional embedding methods as described earlier. Two GPT models with 117M and 345M parameters as detailed in Table 1, are trained from scratch to assess different model architectures. The training sequence length used for these experiments ranges from , , to . A 10% of the training data from the Pile dataset Gao et al. (2020) is utilized for model training. Its testing and validation data is used in experiments of this section for evaluation. We employ an Adam Optimizer Kingma and Ba (2014) for training with the beta values of and . The training process spans steps with a batch size of 2M tokens. The initial learning rate is set to with a warm-up step of , followed by linear decay to . Weight decay is set to , and the gradient clipping is set to . For the local attention, a window size of is applied. For local attention with global approximation, we employ a chunk size of . We group every three layers for local and global layers. For rotary embedding Su et al. (2023), the RoPE theta is configured as to enhance its generalization performance on longer sequences. All experiments are implemented using Megatron-LM111https://github.com/NVIDIA/Megatron-LM Shoeybi et al.",
    "zebra-7": "(2019); Narayanan et al. (2021); Korthikanti et al. (2023). 2.2.1 Attention\n\nFigure 2 shows the testing perplexity (PPL) difference between each attention strategy and the global attention method on the 117M model. From the figures, we have a few observations: First, global attention has the best overall performance; Second, the performance gap between group attention and global attention is small but stable when the training sequence length is getting longer; Third, as the training sequence length grows, the performance of local attention and global approximation attention drops a lot for longer sequences, though it may benefit the shorter ones. As group attention has less computation but achieves a similar performance compared to global attention, it has a high scalability potential. To better compare the global and group attention strategies, we take both performance and computation into consideration. In Figure 3, we draw the curve of the estimated TFLOPS and the validation PPL on three different training lengths with DeepSpeed FLOPS profiler222https://www.deepspeed.ai/tutorials/flops-profiler/ for the 117M model. We observe that group attention achieves a similar performance with less computation than global attention. When the local window is equal to the training sequence length (i.e., 1k training length in Figure 3), the gain is negligible. However, as the training sequence gets longer (e.g., 4k or 16k training length in Figure 3), the gain becomes magnified. This verifies that group attention has better scalability compared to global attention. 2.2.2 Positional Embedding\n\nTable 2 shows the perplexity results comparing different positional embeddings with the 117M and 345M models. We find that no significant performance differences are observed among the three positional embeddings for sequence lengths within the training sequence 16,384.",
    "zebra-8": "This result is in line with Taylor et al.",
    "zebra-9": "(2022); Kazemnejad et al. (2023) observation. While the absolute positional embedding encounters challenges in extrapolating to longer sequences, both Alibi and Rotary positional embeddings demonstrate similar capabilities for sequences exceeding the training sequence length of 16,384. It is important to note that, in our experiments, the Alibi positional embedding requires full precision (fp32) computation to prevent position collision. Consequently, we opt for the Rotary positional embedding in the Zebra model. 2.2.3 Training Sequence length\n\nWe experiment with training sequence lengths of 1024, 4096, and 16384 with a 117M model. The corresponding validation perplexity with the three training sequence lengths is , , and , respectively. In Figure 4, we observe training with longer sequence length generally performs better than those training with shorter sequence length, especially on longer test splits. Meanwhile, the perplexity of the model with longer training length drops a little on short test splits. Interestingly, as we only train with 16k length, the perplexity is still going down on the 32k test split. The results suggest that training with a longer sequence length helps with performance. 2.3 Conclusion on Model Architecture\n\nBased on the experiments and analysis above, we decide to apply Rotary positional embedding and group attention strategy for extending LLM\u2019s context window. The model is denoted as Zebra due to the analogous arrangement of grouped local and global attention layers, resembling the black and white color stripes on a zebra. 3 Long Context Adaptation Training\n\nIn this section, we expand our examination of the Zebra architecture by applying it to a larger-scale model, utilizing Long Context Adaptation Training (LCAT). LCAT is essential for handling large contexts. This modification enables the model to adapt to an expanded context through Zebra architecture, while also providing extended signals during training. This approach allows the model to effectively learn and utilize long-range dependencies. Training such a model from scratch requires a lot of computing power, so we start with an already developed model called Llama-2 Touvron et al. (2023) as our base. From there, we train the Zebra model with different volumes of data. All our experiments are conducted with the 7B-sized model. 3.1 Training Details\n\nZebra layers are organized into groups, each consisting of four layers (). The first layer within each group functions as the global layer, while the remaining three layers serve as local layers with an attention local window size of . Each batch contains tokens. As shown in Table 5, Zebra models are trained with or sequence lengths with different data sizes where multiple documents are packed with a BOS token and an EOS token. The Adam Optimizer Kingma and Ba (2014) is utilized, with beta values of and an Adam epsilon of .",
    "zebra-10": "Training incorporates a cosine learning schedule with a linear warm-up of 100 steps. The maximum learning rate is , and the minimum learning rate is . Gradient clipping is applied with a threshold and weight decay is set to . 3.2 Data Recipes\n\nFigure 5 presents an analysis of the distribution imbalance in the Pile dataset Gao et al. (2020), particularly highlighting the variations due to different sequence lengths. A notable observation is that doubling the sequence length, when measured in bytes, results in a reduction of the dataset\u2019s volume to merely one-fourth of its initial size. In order to sufficiently capture longer text segments in our model, we have strategically integrated the LCAT data with sources that typically contain longer data, such as books. All data is extracted from diverse sources within the Pile dataset, followed by recombination with various domains. Table 3 illustrates the detailed mapping of the original data source to the domain split. The data within each domain split is shuffled, and the specified data quantities, as outlined in Table 3, are selected. 3.3 Evaluation Results\n\nThe long context adaptation training is essential for Zebra to refine the initial parameters from the Llama-2 model. Due to the architectural disparities between the Llama model and the Zebra model, our objective is to achieve comparable performance with Llama-2 on both the short-context tasks and the long-context tasks after training with long sequences. We first evaluate the pre-trained models in Table 4 on Gutenberg (PG-19) Rae et al. (2019) by computing the perplexity across various lengths. Notably, Zebra-LCAT trained with an extended context length of 32k and larger data (v2) exhibits the most favorable perplexity results, comparable to the performance of the Llama-2-LCAT model. Additionally, we assess the performance of our models on a set of common pretraining benchmarks to ensure robust performance across standard short-context tasks Touvron et al.",
    "zebra-11": "(2023); Xiong et al. (2023). As shown in Table 5, the Llama-2-LCAT 32k model continually trained from the original Llama-2 model using the 50B-token v1 data in Table 3 results in a slight degradation in performance compared to Llama-2.",
    "zebra-12": "This can be attributed to the impact of long context adaptation training, as observed in previous work Chen et al. (2023b), which results in degradation on short-context benchmarks. Given that global attention typically yields superior performance compared to local attention Rae and Razavi (2020); Beltagy et al. (2020); Sun et al. (2023), the Llama-2-LCAT performance indicates potential performance upper limit of models with local attention. Comparing Llama-2-LCAT to Zebra-LCAT trained with the same amount of data (50B), the results demonstrate similarities except for MMLU. We speculate the performance decrease in MMLU is originated from the architectural change of the model, potentially resulting in the forgetting of partial knowledge in Llama-2. Finally, as illustrated in Table 5, training Zebra models with varying token quantities indicates that more tokens contribute to narrowing the performance gaps relative to the Llama-2-LCAT model. 3.4 Conclusion on LCAT\n\nOur model exhibits comparable performance to the full attention model (Llama-2-LCAT) in both perplexity and downstream tasks. This equivalence is achieved while ensuring faster training and higher GPU throughput. Based on the experimental findings, we choose Zebra-LCAT trained with a 32k context window and 100B tokens for subsequent instruction tuning (a.k.a. supervised fine-tuning) in long context window, as outlined in Section 4. 4 Long Instruction Tuning\n\nThe Long Context Adaptation Training (LCAT), as discussed in Section 3, facilitates the adaptation of the Zebra model architecture from its Llama-2 foundation model architecture. This adaptation involves the incorporation of grouped local-global layers and position interpolation by training the model with the pre-training learning objectives in long context. Furthermore, we hypothesize that LCAT contributes to enhancing the model by incorporating additional knowledge and capabilities that can only be acquired through training in longer context windows, as suggested by prior research Xiong et al. (2023). To align the Zebra-LCAT model for comprehensive open-domain language understanding and generation tasks based on its acquired knowledge, we conduct supervised fine-tuning on the Zebra-LCAT model with Long Instruction Tuning (LIT). The objective is to empower the model to proficiently handle both short and long-context tasks, adhering to the specifications outlined in human instructions. 4.1 Instruction Tuning Data Recipe\n\nTo align the model for both short and long-context tasks, our instruction tuning (IT) data comprises instances of both types. Table 6 shows the statistics on the average token number per instance. The Llama tokenizer is utilized for tokenizing the instances to calculate the token numbers. 4.1.1 Short Instruction Tuning Data\n\nOur short instruction tuning data primarily incorporates publicly available English instruction tuning datasets, including LIMA Zhou et al.",
    "zebra-13": "(2023), Alpaca Taori et al. (2023), and ShareGPT Tey . Additionally, we introduce samples from hh-rlhf Ganguli et al. (2022) with \u201cselected\u201d responses for multi-turn dialogue. To customize the profile of our Zebra assistant, our dataset further encompasses carefully curated short instances. 4.1.2 Long Instruction Tuning Data\n\nFor long instruction tuning, we address two cases: First case, where the user provides a lengthy document and instructs the system to process the substantial information and complete the specified task succinctly; Second case, where the input is relatively short, but the desired output needs to be more extensive. The former case encompass tasks of summarization, question-answering (QA), and machine reading comprehension (MRC), while the latter involves writing tasks. In writing tasks, the user provides key information, major characters, or an outline, tasking the AI assistant with composing a natural language document, such as a letter or a chapter in a novel. To systematically empower LLMs for long tasks in both scenarios, we carefully curate high-quality instruction tuning data for three specific tasks: summarization, long-MRC, and writing. Summarization: We select 3,000 news reports from CNN / Daily Mail See et al. (2017) and 2,000 multi-turn long dialogue transcripts from MediaSum Zhu et al. (2021) as documents to summarize. Given an document, we randomly choose one out of ten predefined prompts to instruct GPT-4 OpenAI (2023) to generate a summary. Our long instruction tuning data for summarization hence consists of instances with a document and the randomly selected summarization prompt as input, and the GPT-4 generated summary as output. Long-MRC: We create synthetic long-MRC data utilizing the long documents from Guttenberg PG-19 corpus Rae et al.",
    "zebra-14": "(2019). Given a long document (e.g., a chapter or a chapter chunk) in the corpus, we first divide the long document into text segments in approximately even length. For each text segment, we prompt GPT-4 to write one question-answer pair grounding on the information from the text segment. Each long-MRC instruction tuning instance then consists of the long document and the generated question as input , and the generated answer as output .",
    "zebra-15": "We collect in total of 1,245 such instances. Writing: We further utilize texts from Guttenberg PG-19 corpus to generate the data for writing. Given a document (e.g., a chapter or a chapter chunk) in the corpus, we prompt ChatGPT OpenAI (2022) to extract its key elements including \u201ccentral idea\u201d, \u201coutline\u201d, \u201ckeywords\u201d, \u201ctime\u201d, \u201cplace\u201d, \u201ckey figures\u201d, \u201ccause\u201d, \u201cprocess\u201d, and \u201cresult\u201d.",
    "zebra-16": "We use predefined rules to randomly select a subset of these elements, dynamically fill them into the instruction template, and hence use the completed instruction containing selected key elements as our input. The original document is then the corresponding output. We collect 328 such instances. Besides our curated data for tasks of summarization, long-MRC, and writing, we further incorporate 102k training instances randomly selected from public datasets including BigPatent Sharma et al.",
    "zebra-17": "(2019), GovReport Huang et al. (2021), GSM8k Cobbe et al. (2021), CUAD Hendrycks et al. (2021), MultiDoc2Dial Feng et al. (2021), Multi-News Fabbri et al. (2019), Natural Question Kwiatkowski et al. (2019), Musique Trivedi et al. (2022), NarrativeQA Ko\u010disk\u00fd et al. (2018), Qasper Dasigi et al. (2021), QMSum Zhong et al. (2021), QuALITY Pang et al. (2022), SPACE Angelidis et al. (2021), SQuALITY Wang et al. (2022), SummScreen Chen et al. (2022), and TOEFL-QA Tseng et al.",
    "zebra-18": "(2016); Chung et al. (2018). These datasets cover traditional long tasks like summarization, QA, and multi-choice QA. 4.2 LIT Training Details\n\nThe Zebra-LIT training uses the Zebra-LCAT model as backbone and thus inherits Zebra-LCAT\u2019s model structure including the grouped attention setting. Zebra-LIT is trained with sequence length. We pack training instances with less than tokens into the -token sequences. The padding tokens are added to the right. For long instruction instances with more than tokens, we truncate only its input document but keep all the instruction text. The batch size is tokens per batch. We use Adam Optimizer with beta values of and an Adam epsilon of . Training incorporates a cosine learning schedule with a linear warm-up of 32 steps. The maximum learning rate is , and the minimum learning rate is . We conduct 4-epoch training with our instruction tuning data. In LIT, we calculate the loss on the output tokens only. The rest hyper-parameters are the same as LCAT training. 4.3 Evaluation Results\n\nWe assess the performance of our instruction-tuned model on short and long-context benchmarks, as presented in Table 7 and 8, respectively. Fine-tuning models on long instruction data reveals some degradation in performance on short-context benchmarks, such as MMLU, as depicted in Table 7. However, the model consistently outperforms the Llama-2-chat model overall. Additionally, we evaluate the model on long-context benchmarks Shaham et al. (2023). The results, as shown in Table 8, indicate that our model performs comparably or surpasses Llama2-chat in QA datasets but falls short in datasets employing Rouge Lin (2004) evaluation metrics. It is noteworthy that the long-context evaluation of LLMs poses a non-trivial challenge, as highlighted in Xiong et al. (2023). Automatic metrics, such as Rouge, employed in benchmarks, only consider -gram overlaps with a reference, which may not consistently align with human preferences. We anticipate that incorporating more substantial and diverse fine-tuning data can significantly contribute to improved model performance. Overall, our instruction-tuned model demonstrates better performance on both short and long-context benchmarks, affirming the effectiveness of the architecture. 5 Discussion\n\n5.1 Scalability\n\nDue to resource constraints, our scalability experiments are limited to models with 117M and 345M parameters. As illustrated in Figure 6, employing a larger context window enhances model performance for the 117M model. However, a notable observation is the difficulty in differentiating the performance curves when the sequence length is increased from to tokens. We attribute this marginal difference to two primary factors. Firstly, the amount of training data, capped at billion tokens, is likely insufficient to manifest a significant difference. Secondly, both the training and validation datasets predominantly comprise shorter sequences, thereby inherently constraining the potential improvement in PPL.",
    "zebra-19": "Despite these limitations, we anticipate that the benefits of the increased model size will become more apparent with larger and more varied datasets. Evidently, as the model size increases from 117M to 345M, the performance gains significantly, notably from to . 5.2 Efficiency\n\nTraining Efficiency: In Table 9, we present a detailed comparison of computational complexities for different attention operations, as well as other relevant operations, across the four distinct attention strategies outlined in Section 2.1.1. This comparison indicates that with an increase in sequence length, the computational demand of global attention exhibits a quadratic growth pattern. On the other hand, both the Global Approximation and Group Attention strategies, though also following a quadratic growth trajectory, do so with significantly lower coefficients. Notably, Local Attention demonstrates the best computational efficiency, requiring the least amount of resources as the sequence length extends. Inference Efficiency: Because of the implementation of local attention in the majority of Transformer layers, Zebra does not require retaining all Key-Value (K-V) pairs in the cache. Consequently, this approach significantly reduces the GPU memory requirements during inference, thereby potentially increasing the inference throughput. The detailed pseudo-code for Zebra training and inference can be found in Appendix A.1. 6 Related Work\n\nAttention. The Transformer architecture has a self-attention component with computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn Tay et al. (2020b), Longformer Beltagy et al. (2020), ETC Ainslie et al. (2020), and BigBird Zaheer et al. (2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer Wang et al. (2020), Performer Choromanski et al. (2022), and Random Feature Attention Peng et al. (2021). Positional Embedding. In Transformer models, positional embeddings can be primarily categorized into two types: absolute and relative. Earlier versions of Transformers utilize absolute positional encoding. For instance, the vanilla Transformer Vaswani et al. (2017) model adds sinusoidal positional embeddings to the word embeddings, whereas GPT Radford et al. (2018) and BERT Devlin et al. (2018) introduce learnable positional embeddings. Currently, it has become more common to use relative positional embedding. For instance, Transformer-XL Dai et al. (2019) and T5 Raffel et al. (2020) adds learnable attention logit bias into attention layers. Alibi Press et al. (2022) biases attention scores based on the distance between key and query elements. RoPE Su et al. (2023) multiplies the keys and queries of every attention layer by sinusoidal embeddings. The Alibi and RoPE methods are further improved through the incorporation of an additional bias term Sun et al.",
    "zebra-20": "(2022); Chi et al. (2023). LLM. In the early stages, open-source large language models such as OPT Zhang et al. (2022), BLOOM Workshop et al. (2023), and Llama-1 Touvron et al. (2023) have a context window length limited to 2048 tokens. Later, a smaller open-source model Starcoder Li et al. (2023), with 15B parameters, manage to extend its context window length to 8000 by optimizing computational efficiency with Multi-Query Attention Shazeer (2019) and FlashAttention Dao et al. (2022). Following this, Llama-2 Touvron et al. (2023), another open-source model, expands its default length to 4096 tokens. The open-source community then discover that by interpolating Rotary Positional Embeddings333https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_extending_context_to_8k, the context window length could be further extended to 8192 tokens. Subsequently, (Chen et al., 2023b) expand and validate this approach, known as Position Interpolation, to further enhance the window length capability. Llama-2 undergoes extended training with long-context continual pertaining, extending up to 32,768 tokens, the positional interpolation method, and FlashAttention Dao et al. (2022) showing its enhanced effectiveness Xiong et al. (2023). Similar approaches are employed to extend the context length Peng et al. (2023); Du et al. (2022); Dacheng Li* and Zhang (2023) by fine-tuning pretrained models with long documents. LongLoRA Chen et al. (2023c) adopts a fine-tuning approach with shifted local attention for more efficient training to further extend context length. As of December 2023, closed-source large language models have significantly expanded their context window capabilities, reaching scales of up to 100,000 tokens. For instance, GPT-4-Turbo444https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo supports a context window of 128,000 tokens, while Claude-2.1555https://docs.anthropic.com/claude/reference/selecting-a-model supports up to 200,000 tokens.",
    "zebra-21": "The commercialization of these closed-source models is heavily reliant on long context understanding. For example, it allows users to upload long text files for queries or engage in extended dialogues with extensive historical records. Long Evaluation. The majority of benchmarks for evaluating large language models are primarily focused on tasks involving short context. However, the evaluation dimensions for long context and short context may differ significantly. For example, (Liu et al., 2023) develop a multi-document question answering task, which demonstrates the phenomenon of being lost in the middle, a challenge not present in short context. The foundation for comprehensive evaluation of long context understanding is currently still underdeveloped. Recently, there have been efforts to develop benchmarks specifically for long context analysis, such as Shaham et al.",
    "zebra-22": "(2023); Kwan et al. (2023); Dong et al. (2023); Bai et al.",
    "zebra-23": "(2023); An et al. (2023). 7 Conclusion\n\nIn this work, we introduce Zebra, a novel architecture designed to enhance the capabilities of Large Language Models (LLMs) in processing and interpreting long text sequences. Through the innovative use of grouped local-global attention layers and rotary positional embedding, Zebra addresses critical challenges associated with extending the context window in LLMs. Our extensive experiments and analyses demonstrate that Zebra not only maintains comparable performance on short-sequence benchmarks but also excels in handling longer sequences, as evidenced by its superior long benchmark performances and perplexity results on Gutenberg (PG-19). This indicates that our approach effectively balances the need for efficiency with the demand for high performance in long-context scenarios. The grouped local-global attention mechanism, in particular, proves to be a crucial component in achieving this balance, offering a significant reduction in computational and memory requirements while maintaining, and in some cases enhancing model performance. Moreover, the application of Zebra among diverse short and long downstream tasks showcases its versatility and robustness across various NLP tasks. In conclusion, Zebra represents a significant step forward in the realm of long-context language processing. Its ability to efficiently handle extensive text sequences without compromising on performance opens up new possibilities for the application of LLMs in a variety of complex and information-rich environments. We believe that Zebra sets a new standard for long-context modeling and will inspire further innovation in the field. Limitations\n\nWhile our work introduces a novel model architecture and exhibits promising accuracy and efficiency, it is not without limitations. Due to computation resource constraints, we have not yet evaluated the model architecture with a parameter size larger than 7B. Though a larger model typically brings stronger performance, it is still valuable to further verify with Zebra. Moreover, our current evaluation, especially for long-context alignment tasks, largely relies on automatic metrics like Rouge and F-1 employed by public benchmarks. Such metrics evaluating -gram overlapping with a reference have been under debate before the era of LLMs. We anticipate a comprehensive evaluation strategy for long-context alignment to be proposed in the future. References\n\nAinslie et al. (2020) Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268\u2013284, Online. Association for Computational Linguistics. An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. Angelidis et al. (2021) Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277\u2013293. Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language.",
    "zebra-24": "In Thirty-Fourth AAAI Conference on Artificial Intelligence. Chen et al. (2023a) Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. 2023a. Skills-in-context prompting: Unlocking compositionality in large language models. arXiv preprint arXiv:2308.00304. Chen et al. (2022) Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. SummScreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602\u20138615, Dublin, Ireland. Association for Computational Linguistics. Chen et al. (2023b) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation.",
    "zebra-25": "Chen et al. (2023c) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Longlora: Efficient fine-tuning of long-context large language models. Chi et al. (2023) Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13522\u201313537, Toronto, Canada. Association for Computational Linguistics. Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. Choromanski et al. (2022) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2022. Rethinking attention with performers. Chung et al. (2018) Yu-An Chung, Hung-Yi Lee, and James Glass. 2018. Supervised and unsupervised transfer learning for question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1585\u20131594, New Orleans, Louisiana. Association for Computational Linguistics. Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge.",
    "zebra-26": "arXiv:1803.05457v1. Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Dacheng Li* and Zhang (2023) Anze Xie Ying Sheng Lianmin Zheng Joseph E. Gonzalez Ion Stoica Xuezhe Ma Dacheng Li*, Rulin Shao* and Hao Zhang. 2023. How long can open-source llms truly promise on context length? Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978\u20132988, Florence, Italy. Association for Computational Linguistics. Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.",
    "zebra-27": "Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics. Deng et al. (2022) Yuntian Deng, Volodymyr Kuleshov, and Alexander M Rush. 2022. Model criticism for long-form text generation. arXiv preprint arXiv:2210.08444. Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Dong et al. (2023) Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. arXiv preprint arXiv:2309.13345. Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335. Fabbri et al. (2019) Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074\u20131084, Florence, Italy. Association for Computational Linguistics. Fan et al. (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. Eli5: Long form question answering. arXiv preprint arXiv:1907.09190. Feng et al. (2021) Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. Multidoc2dial: Modeling dialogues grounded in multiple documents. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Ganguli et al. (2022) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Guan et al. (2021) Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. Openmeva: A benchmark for evaluating open-ended story generation metrics.",
    "zebra-28": "arXiv preprint arXiv:2105.08920. Guo et al. (2021) Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021. Longt5: Efficient text-to-text transformer for long sequences. arXiv preprint arXiv:2112.07916. Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137. Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review.",
    "zebra-29": "Hu et al. (2023) Yebowen Hu, Tim Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023. Meetingbank: A benchmark dataset for meeting summarization. arXiv preprint arXiv:2305.17529. Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436, Online. Association for Computational Linguistics. Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.",
    "zebra-30": "arXiv e-prints, page arXiv:1705.03551. Kazemnejad et al. (2023) Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. The impact of positional encoding on length generalization in transformers. Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Ko\u010disk\u00fd et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328. Korthikanti et al. (2023) Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5. Kry\u015bci\u0144ski et al. (2021) Wojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2021. Booksum: A collection of datasets for long-form narrative summarization.",
    "zebra-31": "arXiv preprint arXiv:2105.08209. Kwan et al. (2023) Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong. 2023. M4le: A multi-ability multi-range multi-task multi-domain long-context evaluation benchmark for large language models.",
    "zebra-32": "arXiv preprint arXiv:2310.19240. Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you! Lin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain.",
    "zebra-33": "Association for Computational Linguistics. Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering.",
    "zebra-34": "In EMNLP. Narayanan et al. (2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201315. Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human-generated machine reading comprehension dataset. OpenAI (2022) OpenAI. 2022. Introducing chatgpt. OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. Pang et al. (2022) Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336\u20135358, Seattle, United States. Association for Computational Linguistics. Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. Peng et al. (2021) Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. 2021. Random feature attention. Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation.",
    "zebra-35": "In International Conference on Learning Representations. Qiu et al. (2019) Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. 2019. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972. Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Rae and Razavi (2020) Jack Rae and Ali Razavi. 2020. Do transformers need deep long-range memory? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7524\u20137529, Online. Association for Computational Linguistics. Rae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint. Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551. Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale.",
    "zebra-36": "Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463\u20134473, Hong Kong, China.",
    "zebra-37": "Association for Computational Linguistics. See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u20131083, Vancouver, Canada. Association for Computational Linguistics. Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding.",
    "zebra-38": "arXiv preprint arXiv:2305.14196. Sharma et al. (2019) Eva Sharma, Chen Li, and Lu Wang. 2019. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics. Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Shazeer (2019) Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053. Song et al. (2022) Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu, and Fei Liu. 2022. Towards abstractive grounded summarization of podcast transcripts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4407\u20134418, Dublin, Ireland. Association for Computational Linguistics. Su et al. (2023) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. Sun et al. (2023) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2023. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590\u201314604, Toronto, Canada. Association for Computational Linguistics. Talmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, Minneapolis, Minnesota. Association for Computational Linguistics. Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.",
    "zebra-39": "Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca. Tay et al. (2020a) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020a. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438\u20139447. PMLR. Tay et al. (2020b) Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020b. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920. JMLR.org. Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science.",
    "zebra-40": "(79) Steven Tey. Sharegpt. tju (01) tju01. Fasteval. Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition.",
    "zebra-41": "Transactions of the Association for Computational Linguistics, 10:539\u2013554. Tseng et al. (2016) Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016. Towards machine comprehension of spoken content: Initial toefl listening comprehension test by machine. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Wang et al. (2022) Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R.",
    "zebra-42": "Bowman. 2022. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139\u20131156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Wang et al. (2020) Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models.",
    "zebra-43": "Advances in Neural Information Processing Systems, 35:24824\u201324837. Workshop et al. (2023) BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u015far, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023. Bloom: A 176b-parameter open-access multilingual language model. Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. 2023. Effective long-context scaling of foundation models. Yang et al. (2023) Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Xiaoman Pan, Linda Petzold, and Dong Yu. 2023. OASum: Large-scale open domain aspect-based summarization. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4381\u20134401, Toronto, Canada. Association for Computational Linguistics. Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32. Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33. Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800, Florence, Italy. Association for Computational Linguistics. Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905\u20135921, Online. Association for Computational Linguistics. Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. ArXiv, abs/2305.11206. Zhu et al. (2021) Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. Mediasum: A large-scale media interview dataset for dialogue summarization. arXiv preprint arXiv:2103.06410. Appendix A Appendix\n\nA.1 Group Attention Implementation\n\nAs detailed in Section 2.2, our architecture employs a layerwise grouped local-global attention approach, segregating the application of local and global attention across different layers. The methodology for local attention is outlined in Algorithm 1. In contrast, the global attention mechanism adheres to the conventional multi-head attention paradigm found in transformers. It is important to note that during inference when leveraging the Key-Value (K-V) cache, the process for local attention layers deviates from Algorithm 1. Instead, we exclusively implement the standard global attention framework while maintaining the latest K-V states. This approach streamlines the attention process while ensuring efficiency and effectiveness in inference. A.2 Case Study\n\nAs discussed in Section 4.3, evaluating long-context tasks presents challenges. Firstly, common evaluation metrics may misrepresent response quality, particularly in summarization datasets where the reference summary may not align with diverse reader interests or knowledge levels. Furthermore, the limited diversity in long-context tasks may result in inadequate training for instruction tuning, often stemming from summarization, QA, or information extraction datasets with long documents. A more varied set of tasks could potentially enhance the capability of instruction-tuned models. Lastly, as demonstrated in the subsequent section, instances are frequently constructed with inaccurate gold labels or information not covered in given documents, intensifying the difficulty of model evaluation and potentially yielding erroneous results. We present exemplars and results employing Zebra, Llama, and ChatGPT-3.5 on ZeroScrolls Shaham et al. (2023). Due to the limited space and long documents, summarization examples are not presented. In this example, while all models provide correct answers, Llama responds in a more verbose manner, deviating from the given instruction. This example underscores that the pertinent information for the given question cannot be discerned within the context, indicating that certain examples may not have been accurately constructed. The paragraphs do not provide information to answer when the region north of Israel and The World archipelago was created. There is information about Israel and The World islands located in Dubai, but no mention of the region to the north of Israel. The Zebra and Llama models manifest hallucinations, whereas GPT answers accurately following the given instruction. This example shows that the ground truth is not correct. The paragraph states that the band Tenement is \u201ca three piece rock band from Appleton, Wisconsin formed in 200\u201d. There is no information about the capital of the county that shares a border with Appleton in the document but Appleton is the closest answer given the context.",
    "zebra-44": "\u25c4 Feeling lucky?",
    "zebra-45": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 13:58:53 2024 by LaTeXML"
}