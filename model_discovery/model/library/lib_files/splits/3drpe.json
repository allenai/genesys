{
    "3drpe-0": "# 3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding \n\nXindian Ma ${ }^{1}$, Wenyuan Liu ${ }^{1}$, Peng Zhang ${ }^{1 *}$, Nan $\\mathbf{X u}^{2}$<br>${ }^{1}$ College of Intelligence and Computing, Tianjin University, Tianjin, China<br>${ }^{2}$ Beijing Wenge Technology Co. Ltd.<br>\\{xindianma, lwy2020, pzhang\\} @tju.edu.cn<br>\\{xunan2015\\}@ia.ac.cn\n\n\n#### Abstract\n\nInspired by the Bloch Sphere representation, we propose a novel rotary position encoding on a three-dimensional sphere, named 3D Rotary Position Encoding (3D-RPE). 3D-RPE is an advanced version of the widely used 2D Rotary Position Encoding (RoPE), with two major advantages for modeling long contexts: controllable long-term decay and improved position resolution. For controllable long-term decay, 3D-RPE allows for the regulation of long-term decay within the chunk size, ensuring the modeling of relative positional information between tokens at a distant relative position. For enhanced position resolution, 3D-RPE can mitigate the degradation of position resolution caused by position interpolation on RoPE. We have conducted experiments on long-context Natural Language Understanding (NLU) and long-sequence Language Modeling (LM) tasks. From the experimental results, 3D-RPE achieved performance improvements over RoPE, especially in long-context NLU tasks. ## 1 Introduction\n\nRotary Position Encoding (RoPE) [23] is essential in Transformer-based Large Language Models (LLMs), such as the LLaMA models [24]. RoPE merges the advantages of absolute and relative positional encoding by using a rotation mechanism to represent each position. Despite its widespread use in LLMs [24, 27, 7], RoPE has notable limitations when extending LLMs with a predefined context window. The long-term decay problem of RoPE limits the model's ability to extend positions outward in long-context tasks. Although the long-context modeling capability of LLMs can be extended through position interpolation, as more positions are inserted, RoPE encounters the challenge of decreased position resolution. We propose a novel position encoding mechanism for transformer architecture, called 3D Rotary Position Encoding (3D-RPE), to address challenges in long-context modeling faced by LLMs using RoPE. Inspired by the Bloch Sphere representation, 3D-RPE applies rotary position encoding on a three-dimensional spherical surface, as illustrated in Figure 1(b). In contrast, RoPE employs rotation on a two-dimensional circular path, as depicted in Figure 1(a). RoPE suffers from long-term decay, as shown in Figure 1. (c), implying that as the relative distance increases, the relative upper bound on token correlations at modeled relative positions will continuously decrease. 3D-RPE addresses this issue by segmenting a long sequence into chunks and setting rotation angles within and between the chunks to construct position encoding. As shown in Figure 11(d), 3D-RPE is able to control this relative upper bound through two relative positional dimensions, namely within and between chunks. [^0]| Method | 2D Rotary Position Encoding (RoPE) | 3D Rotary Position Encoding(3D-RPE) |\n| :---: | :---: | :---: |\n| Schematic <br> Drawing | (a) | (b) |\n| Formula | $f_{\\{\\boldsymbol{q}, \\boldsymbol{k}\\}}(\\boldsymbol{h}, m)=e^{i m \\theta} \\boldsymbol{h}$ | $f_{\\{\\boldsymbol{q}, \\boldsymbol{k}\\}}(\\boldsymbol{h}, m, j)=e^{i m \\theta}\\left(\\cos \\varphi_{j} \\boldsymbol{h}^{\\perp}+\\sin \\varphi_{j} \\boldsymbol{h}\\right)$ |\n| Long-term <br> Decay | (c) relative upper bound | (d) relative upper bound <br> chunk size c $=1000$ |\n| Position <br> Resolution | $\\varepsilon_{\\text {rope }}=1 \\xrightarrow{\\mathrm{PI}} \\boldsymbol{\\varepsilon}_{\\text {rope }}^{\\prime}=\\frac{L_{p}}{L}$ | (f) $\\boldsymbol{\\varepsilon}_{3 \\boldsymbol{d}-\\boldsymbol{r p e}}=1 \\xrightarrow{\\mathrm{PI}} \\boldsymbol{\\varepsilon}_{\\boldsymbol{3} \\boldsymbol{d}-\\boldsymbol{p} \\boldsymbol{e}}^{\\prime}>\\frac{L_{p}}{L}$ |\n\nFigure 1: 2D Rotary Position Encoding (RoPE) vs. 3D Rotary Position Encoding (3D-RPE). Compared to Figure 11 c), this method improves the upper bound on correlations between long relative distances and alleviates the issue of long-term decay. Position Interpolation (PI) methods [4, 18] based on RoPE are often employed to extend LLMs for modeling contexts that exceed the pre-training length. These techniques scale the position encoding during inference, allowing the originally out-of-range position encoding to fall within the trained position interval after interpolation. However, as the interpolation factor increases, PI experiences a substantial decline in positional resolution among tokens, detrimentally affecting long-context modeling performance. As illustrated in Figure 1(e), extending the pre-training length $L_{p}$ to $L$ using linear PI [4] leads to reduced positional resolution with increasing $L$. 3D-RPE employs a 3D rotating sphere for position encoding, which supports higher positional resolution compared to the 2D circular rotation. Similarly, through linear PI extension, 3D-RPE achieves a positional resolution superior to $\\frac{L_{p}}{L}$ (See Figure 1 (f)). This benefit has been theoretically proven (Refer to Theorem 1 in Section 3.2.2) and corroborated by experimental results (Refer to Table 4 in Section 5.2). We conducted experiments on long-sequence Language Modeling (LM) and long-context Natural Language Understanding (NLU) tasks. Our experimental results highlight the promising performance of the 3D-RPE method, especially in tasks requiring long-context language understanding. Our major contributions of this paper are as follows:\n\n- A position encoding method on a 3D sphere, 3D-RPE, is provided, which can enhance the long-context modeling capability of LLMs by replacing RoPE. - It is proved that 3D-RPE has two benefits, controllable long-term decay and mitigating the reduction in positional resolution caused by position interpolation. - LLMs combine with 3D-RPE have achieved significant performance improvements in long-context NLU tasks. The structure of this paper is as follows. Section 2 covers the preliminaries of 3D-RPE, Bloch Sphere, and RoPE. Section 3 explains the construction of 3D-RPE on a 3D rotating sphere and highlights its benefits over RoPE. Section 4 reviews related work. In Section 5 , we validated the advantages of our method through experiments. Section 6 concludes with a discussion on 3D-RPE's impact. ## 2 Preliminaries\n\nThe analysis of 3D-RPE relies on these concepts and results from the filed of Bloch Sphere and RoPE. We offer an introduction to Bloch Sphere in Section 2.1] RoPE [23] in Section 2.2. ### 2.1 Bloch Sphere\n\nBloch Sphere (BS) offers a geometric depiction of a quantum mechanical system's pure state, limited to two levels. The state vector $|\\phi\\rangle$ is mathematically expressed as\n\n$$\n|\\phi\\rangle=e^{\\mathrm{i} \\theta}\\left(\\cos \\frac{\\varphi}{2}|0\\rangle+\\sin \\frac{\\varphi}{2} e^{i \\theta_{1}}|1\\rangle\\right)\n$$\n\nwhere $|0\\rangle$ and $|1\\rangle$ are Dirac's notations. $\\theta, \\theta_{1}$ and $\\varphi$ are rotation angles. In our work, $\\theta$ encodes the relative positions of tokens within chunks, $\\varphi$ encodes the relative positions of tokens across chunks, and $\\theta_{1}$ is equal to 0 . Some other concepts about BS are showed in Supplementary Materials A. ### 2.2 Rotary Position Embedding\n\nRotary Position Embedding (RoPE) is a commonly used relative position encoding technique in LLMs, such as LLaMA [24], GPT-J [27], Vicuna [7] and etc. RoPE is a 2-dimensional space rotary encoding, which is denoted as follows:\n\n$$\n\\operatorname{RoPE}\\left(\\boldsymbol{h}_{m}, m\\right)=e^{\\mathrm{i} m \\theta} \\boldsymbol{h}_{m}, \\operatorname{RoPE}\\left(\\boldsymbol{h}_{n}, n\\right)=e^{\\mathrm{i} n \\theta} \\boldsymbol{h}_{n}\n$$\n\n$\\boldsymbol{h}_{m}$ and $\\boldsymbol{h}_{n}$ are hidden vectors from the query and key for a specific attention head in transformer. For ease of differentiation, $\\boldsymbol{h}_{m}$ and $\\boldsymbol{h}_{n}$ can be refined later as $\\boldsymbol{q}_{m}$ and $\\boldsymbol{k}_{n}$, i is the imaginary unit, $\\theta$ is the rotary angle in RoPE. $m$ and $n$ are indexes about positions. Then, the inner product is employed to define the self-attention score before softmax computing:\n\n$$\n\\begin{aligned}\ns\\left(m-n, \\boldsymbol{q}_{m}, \\boldsymbol{k}_{n}\\right) & =\\left\\langle\\operatorname{RoPE}\\left(\\boldsymbol{q}_{m}, m\\right), \\operatorname{RoPE}\\left(\\boldsymbol{k}_{n}, n\\right)\\right\\rangle \\\\\n& =\\operatorname{Re}\\left[\\sum_{l=0}^{d / 2-1} \\boldsymbol{q}_{[2 l: 2 l+1]} \\boldsymbol{k}_{[2 l: 2 l+1]} e^{\\mathrm{i}(m-n) \\theta_{l}}\\right]\n\\end{aligned}\n$$\n\nEq (3) is unary function respect to the relative position $(m-n)$, representing the relative position between tokens and modeling the relative positional information. Here, $R e[\\cdot]$ denotes the calculation of the real part of a complex number. In our study, the 3D-RPE self-attention score is a binary function containing the relative position $(m-n)$. ## 3 Method\n\nSection 3.1 introduces the new position encoding on a 3D sphere, 3D-RPE. Section 3.2 focuses on analyzing two benefits of 3D-RPE, namely controllable long-term decay and enhanced position resolution. ### 3.1 3D Rotary Position Encoding\n\nFor a long sequence of length $L$ and a chunk size set to $c$, where $c$ is smaller than the pre-training length of LLM, the sequence can be divided into $\\lceil L / c\\rceil$ chunks. Here, $\\lceil$.$\\rceil represents the ceiling$ function, rounding up to the nearest integer (see Figure 2). The state vector $\\boldsymbol{h}_{j, m}$ comes from either Query or Key. Here, $j \\in[0,\\lceil L / c\\rceil-1]$ represents the positional index of the chunk, and $m \\in[0, c-1]$ indicates the positional index of the token within the chunk. This is used to calculate the new state vector $\\widetilde{\\boldsymbol{h}}_{j, m}$ by rotating the Bloch Sphere. Specifically, two rotation angles, $\\theta$ and $\\varphi$ are defined, with $\\theta$ governing the position encoding within the chunk's internal tokens, and $\\varphi$ governing the position encoding between the chunks. Our position encoding method is called 3D Rotary Position Encoding, or 3D-RPE. The formal definition of 3D-RPE is provided as follows. The computational process of 3D-RPE in practice is provided in Supplementary Materials B.1. Definition 1 (3D Rotary Position Encoding). Let $\\boldsymbol{h}_{j, m} \\in \\mathbb{R}^{d}$ be a state vector of an attention head without position encoding, where $d$ is the dimension of the vector, which is an even number. $3 D-R P E$ encodes $\\boldsymbol{h}_{j, m}$ into the vector $\\widetilde{\\boldsymbol{h}}_{j, m}$, which can be formalized as:\n\n$$\n\\widetilde{\\boldsymbol{h}}_{j, m}=e^{\\mathrm{i} m \\theta}\\left(\\cos \\varphi_{j} \\boldsymbol{h}_{j, m}^{\\perp}+\\sin \\varphi_{j} \\boldsymbol{h}_{j, m}\\right)\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d426bc005c38a790628bg-04.jpg?height=939&width=1137&top_left_y=273&top_left_x=494)\n\nFigure 2: Visualization of the 3D Rotary Position Encoding (3D-RPE).",
    "3drpe-1": "The context size is $L$, and the chunk size is $c$. The vectors $\\left[\\boldsymbol{h}_{j, m}^{1}, \\boldsymbol{h}_{j, m}^{2}\\right]^{T}$ and $\\left[-\\boldsymbol{h}_{j, m}^{2}, \\boldsymbol{h}_{j, m}^{1}\\right]^{T}$ form an orthogonal basis, corresponding to the $|1\\rangle$ and $|0\\rangle$ states in Eq. 11. The components $\\boldsymbol{h}_{j, m}^{1}$ and $\\boldsymbol{h}_{j, m}^{2}$ represent the first and second dimensions of the state vector $\\boldsymbol{h}_{j, m}$, which is the $m_{t h}$ token in the $j_{t h}$ chunk. i is the imaginary unit. $\\boldsymbol{h}_{j, m}^{\\perp}$ equals to $\\left[-\\boldsymbol{h}_{j, m}^{2}, \\boldsymbol{h}_{j, m}^{1}\\right]^{T}$, where $\\boldsymbol{h}_{j, m}^{1} \\in \\mathbb{R}^{d / 2}$ and $\\boldsymbol{h}_{j, m}^{2} \\in \\mathbb{R}^{d / 2}$ is the first and second halves of the state vector $\\boldsymbol{h}_{j, m}$. In transformer-based LLMs, after applying position encoding to the state vectors from Query and Key, it is essential to compute their attention scores. For the sake of clarity and formalization, we denote the position encoding of the state vector from Query as 3d-PE $(\\boldsymbol{q}, i, m)$ and from Key as $3 \\mathrm{~d}-\\mathrm{PE}(\\boldsymbol{k}, j, n)$, where $i$ and $j$ range from 0 to $\\lceil L / c\\rceil-1$, and $m$ and $n$ range from 0 to $c-1$. The self-attention score can be obtained through the conjugate symmetric inner product of $\\boldsymbol{q}_{i, m}$ and $\\boldsymbol{k}_{j, n}$, which are the state vectors from Query and Key,\n\n$$\ns\\left(\\boldsymbol{q}_{i, m}, \\boldsymbol{k}_{j, n}, \\varphi_{i}-\\varphi_{j}, m-n\\right)=\\operatorname{Re}\\left[e^{\\mathrm{i}\\left(\\varphi_{i}-\\varphi_{j}\\right)} \\sum_{l=0}^{d / 2-1} e^{\\mathrm{i}(m-n) \\theta_{l}}\\left(\\boldsymbol{q}_{l} \\boldsymbol{k}_{l}+\\boldsymbol{q}_{d / 2+l} \\boldsymbol{k}_{d / 2+l}\\right)\\right]\n$$\n\nwhere $l \\in\\left[0, \\frac{d}{2}-1\\right], \\varphi_{i}=b a s e^{-i}$ and $\\varphi_{j}=$ base ${ }^{-j}$. Let $\\{\\boldsymbol{q}, \\boldsymbol{k}\\}_{l}$ denote the $l$-th components of $\\{\\boldsymbol{q}, \\boldsymbol{k}\\}$. In experiments using the LLaMA2 models, the base is generally set to 10, 000. The self-attention score computed after applying 3d-PE is a function of both the relative position between chunks $\\left(\\varphi_{i}-\\varphi_{j}\\right)$ and the relative position $(m-n)$. Consequently, the self-attention score relying on 3d-PE is influenced by the relative positions at both the chunk and token levels. It is important to highlight that when $\\boldsymbol{q}_{i, m}$ and $\\boldsymbol{k}_{j, n}$ reside within the same chunk (i.e., $i=j$ ), Eq. (5) simplifies to the standard RoPE formulation as depicted in Eq. (3). For a detailed derivation and computation process of Eq. (5), as well as the complete formulation of Eq. 4, please refer to Supplementary Materials B.2. ### 3.2 Benefits of 3D-RPE\n\nIn this section, we delve into two benefits offered by 3D-RPE: the ability to control long-term decay and mitigate the reduction in positional resolution caused by position interpolation. ![](https://cdn.mathpix.com/cropped/2024_09_12_d426bc005c38a790628bg-05.jpg?height=709&width=858&top_left_y=296&top_left_x=517)\n\nFigure 3: Visualization of the Relative Position Matrix $\\boldsymbol{A}$ employing 3D-RPE, with chunk size $c=4$, and sequence size $L=12$. The matrix elements $A_{i, j}$ represents the relative position between the $i_{t h}$ query vector $\\boldsymbol{q}$ and the $j_{t h}$ key vector $\\boldsymbol{k}$. ### 3.2.1 Controllable Long-term Decay\n\n3D-RPE has the property of controllable long-term decay. Like RoPE, taking the absolute value $s$ in Eq (5) and applying the Abel transformation, we derive the upper bound of the correlation coefficients related to term dependencies as follows:\n\n$$\n\\begin{aligned}\n\\left|s\\left(\\boldsymbol{q}_{i, m}, \\boldsymbol{k}_{j, n}, \\varphi_{i}-\\varphi_{j}, m-n\\right)\\right| & \\leq\\left|e^{\\mathrm{i}\\left(\\varphi_{i}-\\varphi_{j}\\right)} \\| \\sum_{l=0}^{d / 2-1} E_{l+1}\\left(h_{l+1}-h_{l}\\right)\\right| \\\\\n& \\leq\\left(\\max _{l}\\left|h_{l+1}-h_{l}\\right|\\right) \\sum_{l=0}^{d / 2-1}\\left|E_{l+1}\\right|\n\\end{aligned}\n$$\n\nwhere $E_{l}=\\sum_{k=0}^{l-1} e^{\\mathrm{i}(m-n) \\theta_{k}}$ and $E_{0}=0$. For RoPE [23], the relative upper bound $E_{\\text {rope }}$ is given by $\\frac{1}{d / 2} \\sum_{j=1}^{d / 2}\\left|S_{j}\\right|$, where $S_{j}=\\sum_{t=0}^{j-1} e^{i(m-n) \\theta_{t}}$ (see the section 3.4.3 of RoPE [23]). By setting $\\theta_{t}=10000^{\\frac{-2 t}{d}}$, the value decays as the relative position $(m-n)$ increases. For the upper bound $E_{3 d-r p e}$ of 3D-RPE, it is formalized as follows:\n\n$$\nE_{3 d-r p e}=\\frac{1}{d / 2} \\sum_{j=1}^{d / 2}\\left|E_{l}\\right|\n$$\n\nThe domains of the relative position $(m-n)$ differ between $E_{3 d-r p e}$ and $E_{\\text {rope }}$. In $E_{\\text {rope }},(m-n)$ is in the range $[0, L-1]$, while in $E_{3 d-r p e}$, it is in $[0, c-1]$. The relative positions between tokens exceeding the chunk size $c$ are constructed collaboratively using positional encoding within and across chunks. The Relative Position Matrix $\\boldsymbol{A}$ using 3D-RPE is shown in Figure 3. To illustrate the advantage of controllable long-term decay, we present the results in Figure 1(c) and Figure 1/d). As shown in Figure 1 (c), when the relative position $(m-n)$ exceeds approximately 1000, $E_{\\text {rope }}$ begins to significantly decrease to below 5 . This limitation of $E_{\\text {rope }} \\leq 5$ poses challenges for RoPE in modeling attention scores between tokens with longer relative distances (greater than 4000). In contrast, as shown in Figure 11d), 3D-RPE employs both $(m-n)$ and $\\left(\\varphi_{i}-\\varphi_{j}\\right)$, setting $c=1000$ to keep $(m-n)$ within 1000, thereby preventing decay over longer distances. This method ensures $E_{3 d-r p e}$ stays at or above 5 for all relative positions. ### 3.2.2 Enhanced Positional Resolution\n\nPosition Interpolation (PI) [4] has been introduced to scale down the position indices to align with the original window size, resulting in enhanced outcomes for context extension. However, as the extension length and interpolation increase, PI can lead to a reduction in relative positional resolution. 3D-RPE can be used alongside PI for long-context extensions. Compared to RoPE combined with PI, 3D-RPE has the advantage of mitigating the reduction in positional resolution caused by positional interpolation, as demonstrated in Theorem 1 . Theorem 1 (Enhanced Position Resolution). For a pre-trained language model with a length of $L_{p}$ and an extension length requirement of L, employing linear position interpolation extension methods $\\mathcal{I}$ based on Rotary Position Encoding (RoPE) can elevate the relative positional resolution from $\\mathcal{E}_{\\text {rope }}$ to $\\mathcal{E}_{\\text {rope }}^{\\prime}$. Let $\\mathcal{E}_{3 d-r p e}^{\\prime}$ denote the relative positional encoding resolution achieved by the method $\\mathcal{I}$ based on $3 D-R P E$, with chunk size $c \\geq 3$, there is:\n\n$$\n\\mathcal{E}_{3 d-r p e}^{\\prime}>\\mathcal{E}_{\\text {rope }}^{\\prime}\n$$\n\nThe Proof of Theorem 1 is provided in Supplementary Materials C. To empirically validate the superior performance of this benefit in a training-free setting, it has been observed that methods combining RoPE with interpolation lead to a significant increase in Perplexity as the modeling length increases in language modeling tasks. Conversely, the increase in Perplexity is substantially smaller when employing 3D-RPE with linear interpolation (Refer to Table 4 in Section 5). This phenomenon indicates that this benefit has led to an improvement in the performance of long sequence language modeling. ## 4 Related Work\n\nThis section provides an overview of the extensive literature on position encoding in Transformers [26] and discusses context extending capabilities based on RoPE. Position Encoding (PE): PE is important for Transformer-based language models. Earlier studies [22, 21, 28, 23] have focused on enhancing the original absolute position encoding to develop better relative position encoding, thereby improving the text modeling capabilities of language models. These works [22, 21, 28] utilized trainable position vector encoding to directly incorporate positional information into context representations. Although effective, these methods typically add positional information to contextual representations, making them unsuitable for linear self-attention architectures. RoFormer [23] introduced relative position information by rotating context representations, known as RoPE. Transformers utilizing RoPE have become a prevalent backbone in various LLM designs [24, 8, 27, 16]. Our proposed 3D-RPE differs from the two-dimensional space of RoPE by modeling the relative position information of tokens through rotation on the Bloch Sphere. Long-context LLMs based on RoPE: To enhance the contextual capabilities of Large Language Models (LLMs) using RoPE, several positional encoding interpolation techniques have been developed. These include Linear Position Interpolation (LPI) [4], Neural Tangent Kernel (NTK) [17], and Yet Another Recurrent Network (YaRN) [18] interpolation. Position Sequence Tuning (PoSE) [31] has notably increased sequence lengths to $128 k$ by amalgamating these positional interpolation strategies. Additionally, LongLora [5] introduced the shift-short attention mechanism, allowing for effective emulation of full attention and extending sequences up to $100 k$, leveraging the LLMa-2-7B model and LoRA's fine-tuning approach [12]. 3D-RPE further strengthens the positional relationships between distant tokens by capturing inter-chunk positional information and is compatible with existing fine-tuning techniques like LoRA to bolster long-context representation. The Dual Chunk Attention (DCA) [2] method, which enhances the use of pre-trained integer-based parameters, splits query and key sequences into chunks and uses three specialized matrices to capture the relative positions within and between these chunks. This method enhances the model's ability to process longer sequences, but it is unable to model the relative positions within distant chunks. In our work, we employ rotating positional encoding to link attention across different chunks. ## 5 Experiments\n\nWe evaluate the method of position encoding, 3D-RPE, on LLaMA2 [24] models (specifically, LLaMA-2-7B and LLaMA-2-7B-chat), which have a $4 k$ pre-training context, and LLaMA-3-8B-\n\nInstruct ${ }^{2}$, which has an $8 k$ pre-training context. Our experiments aim to explore the following aspects: 1) The effect of 3D-RPE on long-context generation can be assessed using Perplexity. 2) The impact of 3D-RPE on long-context understanding and generation tasks, can be reflected by the accuracy of long sequence natural language tasks, e.g., multiply documents QA. 3) Ablation studies to confirm the advantages of 3D-RPE in position interpolation. ### 5.1 Experimental Settings\n\nIn this section, we elaborate on the experimental setup by introducing two types of tasks (i.e., longcontext language understanding and long sequence language modeling) and detailing three aspects of the configuration (i.e., training parameters, training and evaluation datasets, and baseline models). Training Setting: For long-context Natural Language Understanding (NLU) tasks, we have finetuned LLaMA-2-7B-chat and LLaMA-3-8B-Instruct. The context length for these models has been extended from $4 k$ to $16 k$ and from $8 k$ to $16 k$, respectively. The fine-tuning method follows the fine-tuning strategy of LongChat [13]. The training step is 3, 000. For the long-sequence Language Modeling (LM) tasks, we have fine-tuned LLaMA-2-7B to support extended context length of $32 k$ tokens. The training step is 1,000 . We set the per-device batch size as 1 , and gradient accumulation step as 8 , which means that the batch size is 8 . We train the model with the next token prediction objective with LoRA [12]. We employed the AdamW optimizer [15] with $\\beta_{1}=0.9$ and $\\beta_{2}=0.95$ for all fine-tuned models. Chunk size is set to $3 k$. The learning rate was set to $2 \\times 10^{-5}$, and a linear learning rate warmup was applied. Training was conducted on a single 4xA800 GPU machine using FlashAttention-2 [10]. Datasets: In the context of long-context NLU tasks, we employ the LongAlpaca-12k dataset, which contains 9,000 LongQA and 3,000 short QA entries [6], and the LongAlpace-16k-length datase ${ }^{3}$ To evaluate the performance of 3D-RPE for long-context extension, we use the LongBench [3], which includes 13 English tasks, 5 Chinese tasks and 2 code tasks, with most tasks having an average context length of $5 k$ to $15 k$ tokens. We focus on the English and code tasks to evaluate our method, 3D-RPE. Additionally, the LEval [1] evaluation set, which also consists of long-context datasets, is used to verify the effectiveness of 3D-RPE. The five datasets annotated from scratch in LEval, namely Coursera, QuALiTY, CodeU, GSM,and TOEFL, are utilized. For long-sequence LM tasks, we use the RedPajama-Data [9] for fine-tuning training. The dataset is a large-scale pre-training dataset (the size reaches 1.2 trillion tokens) designed to provide high-quality training data for language models, and contains multiple data sources (i.e., github, arxiv, book, c4 and Wikipedia, etc.). We sample 20,000 samples from these data sources for training. For evaluation, we utilize the PG19 book corpus dataset [20], which includes 100 documents, and the Arxiv Math Proof-pile dataset (test split). Additionally, all methods evaluate perplexity by using a sliding window following [19]. Baseline models: For long-context NLU tasks, the fine-tuned models, including LongAlpace-16k [5], LongChat-32k [14] LongLlama [25] and ChatGLM [11] are used as the baseline models. Models of fine-tuning free in language modeling tasks are also used in long-context NLU tasks. In long sequence LM tasks, the methods of LongLoRA [5], StreamingLLM [29], Positional Interpolation(PI) [4] and the NTK-Aware Scale RoPE(NTK) [17] are selected as the baselines, all based on the LLaMA-2-7B-base model. Among these baseline models, PI, NTK and StreamingLLM are fine-tuning-free methods. The fine-tuned models include LongLoRA and Activation Beacon [30]. In Ablation experiments, interpolation methods without training are used as baseline models, which are PI and NTK. ### 5.2 Long-Context Natural Language Understanding\n\nIn this task, the LongBench [3] evaluation set was initially utilized. Five categories of tasks were included: single-document QA (3 tasks), multi-document QA (3 tasks), summarization (3 tasks), few-shot learning ( 3 tasks), and code completion ( 2 tasks). The average score for each type is reported in Table 1]. The evaluation metrics followed those specified in LongBench [3], which differ across\n\n[^1]Table 1: Comparison between open-source based models on long-context NLU tasks. Our model, 3D-RPE-LlaMA2-7B-Chat is fine-tuning on LLaMA-2-7b-chat, which is extended from $4 k$ to $16 k$ context lengths. Baseline models can be categorized into two groups: those that necessitate finetuning during training (such as LongAlpaca [5] and LongLLaMA [25]), and those that do not require it (including PI, NTK, StreamingLLM [29], and ChunkLLaMA-16k [2]). The experimental results for each specific evaluation set in Supplementary Material D.2. | METHODS | Single-Doc QA | Multi-Doc QA | Summarization | Few-shot | Code |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| LLaMA-2-7B-chat | 24.90 | 22.60 | 24.70 | 60.01 | 48.10 |\n| LLaMA-2-7B-chat-PI | 18.98 | 17.16 | 25.03 | 49.43 | 52.73 |\n| LLaMA-2-7B-chat-NTK | 23.21 | 23.34 | 24.40 | 59.29 | 49.28 |\n| StreamingLLM | 21.47 | 22.22 | 22.20 | 50.05 | 48.00 |\n| ChunkLLaMA-16 $k$ | 24.04 | 22.98 | 21.52 | 46.31 | 49.73 |\n| LongChat-32 $k$ | 31.58 | 23.50 | 26.70 | 64.02 | 54.10 |\n| LongAlpaca-16 $k$ | 28.70 | 28.10 | 27.80 | 63.70 | 56.00 |\n| LongLLaMA | 30.12 | 16.37 | 24.19 | 60.31 | 66.05 |\n| Vicuna-v1.5-7B-16 $k$ | 28.01 | 18.63 | 26.01 | 66.20 | 47.30 |\n| ChatGLM3-6B-32 $k$ | 40.30 | 46.60 | $\\mathbf{2 9 . 5 0}$ | 68.10 | 56.20 |\n| 3D-RPE-LLaMA2-7B-Chat | $\\mathbf{4 7 . 4 0}$ | $\\mathbf{6 0 . 1 0}$ | 28.99 | $\\mathbf{7 3 . 1 6}$ | $\\mathbf{7 6 . 5 0}$ |\n\nTable 2: Comparison with open-source models, LLaMA-2-7B-chat, LLaMA3-8B-Instruct, on 5 closed-ended-ended tasks with various input length from LEval [1]. The evaluation metric \"EM,\" which represents the exact match score, is adopted. $*$ indicates the model is train-free. | MODELS | Coursera | QuALiTY | CodeU | GSM | TOEFL |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| LLaMA-2-7B-Chat | 29.21 | 37.62 | 1.11 | 19.00 | 51.67 |\n| LongChat-7B-16K | 29.74 | 33.66 | 3.33 | 10.00 | 47.95 |\n| LLaMA2-7B-NTK | 32.71 | 33.16 | 0.00 | 19.00 | 52.78 |\n| Vicuna1.5-7B-16k | 38.66 | $\\mathbf{3 9 . 6 0}$ | $\\mathbf{5 . 5 5}$ | 19.00 | 55.39 |\n| 3D-RPE-LLaMA2-7B-Chat(ours) | $\\mathbf{3 9 . 3 8}$ | 38.11 | 2.22 | $\\mathbf{2 1 . 0 1}$ | $\\mathbf{5 7 . 9 9}$ |\n| LLaMA3-8B-Instruct* | 51.45 | $\\mathbf{6 4 . 3 4}$ | 4.44 | 76.00 | 82.89 |\n| 3D-RPE-LLaMA3-8B-Instruct* | $\\mathbf{5 1 . 8 9}$ | 61.38 | 4.44 | $\\mathbf{8 0 . 0 0}$ | 82.89 |\n\ntasks and are detailed in Supplementary Material D.1. The results in Table 1 highlight our model's significant performance advantages over baseline models in four tasks, both for models without training and those with fine-tuning. In summarization tasks, our model also achieved performance comparable to ChatGLM3-6B-32k. These experimental outcomes indicate that our model enhances the correlation between tokens with distant relative positions in long contexts through 3D-RPE, resulting in improved performance. Subsequently, the LEval Benchmark [1] was employed. Table 2 reveals that our model, 3D-RPELLaMA2-7B-Chat, outperformed LLaMA2-7B-NTK and LongChat-7B-16K. Although it did not surpass Vicuna1.5-7B-16K in Quality and CodeU tasks, it excelled in the Coursera, GSM, and TOEFL tasks. Additionally, we conducted experiments on LLaMA3-8B-Instruct using a 16k context window with 3D-RPE. The 3D-RPE-LLaMA3-8B-Instruct* showed performance improvements in the Coursera and GSM tasks. While 3D-RPE did not enhance performance in the CodeU, TOEFL, and QuALiTY tasks, there was no significant performance decline either. These experimental results demonstrate the effectiveness of the 3D-RPE method. ### 5.3 Long-Sequence Language Modeling\n\nIn Table 3, we present the perplexity scores for our model, 3D-RPE-LLaMA-2-7B and baseline models on the proof-pile and PG19 test datasets. 3D-RPE-LLaMA-2-7B was fine-tuned from the LLaMA2-7B-Base model using a dataset with a $32 k$ context window. To evaluate performance, we set sequence lengths of $8 k, 16 k$, and $32 k$. We extended our model's sequence length from $32 k$ to $100 k$ using the position extending method from PoSE [31]. The results indicate that our method outperforms train-free sequence extending models. Compared to fine-tuned models, our model shows better performance at $8 k$ and $16 k$ sequence lengths. This suggests that the new positional encoding,\n\nTable 3: Perplexity evaluation on different extending methods. We conduct evaluation on the Proof-pile and PG-19 test datasets, varying evaluation context window size from $8 k$ to $100 k$. | METHODS | PG-19 |  |  |  | Proof-Pile |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $8 k$ | $16 k$ | $32 k$ | $100 k$ | $8 k$ | $16 k$ | $32 k$ | $100 k$ |\n| LLaMA2-7B-Base | 131.09 | $>10^{2}$ | $>10^{2}$ | OOM | 16.79 | $>10^{2}$ | $>10^{2}$ | OOM |\n| LLama2-7B-PI | 11.32 | 19.5 | $>10^{2}$ | OOM | 3.86 | 5.94 | 33.7 | OOM |\n| LLama2-7B-NTK | 10.28 | 11.5 | 37.8 | OOM | 3.98 | 5.94 | 33.7 | OOM |\n| StreamingLLM | 9.23 | 9.25 | 9.24 | 9.32 | 3.47 | 3.51 | 3.50 | 3.55 |\n| LongLoRA-32k | 7.33 | 7.16 | $\\mathbf{7 . 0 4}$ | - | 2.78 | 2.61 | $\\mathbf{2 . 5 0}$ | - |\n| LongLoRA-100k | 7.57 | 7.33 | 7.16 | $\\mathbf{7 . 0 4}$ | 2.78 | $\\mathbf{2 . 6 0}$ | 2.58 | $\\mathbf{2 . 5 2}$ |\n| LongChat-32k | 8.92 | 8.85 | 8.81 | OOM | 2.98 | 2.70 | 2.65 | OOM |\n| Activation Beacon | 8.52 | 8.54 | 8.56 | 8.68 | 3.45 | 3.42 | 3.39 | 3.35 |\n| 3D-RPE-LLaMA2-7B | $\\mathbf{7 .",
    "3drpe-2": "0 3}$ | $\\mathbf{7 . 1 0}$ | 8.09 | 8.12 | $\\mathbf{2 . 7 2}$ | 2.93 | 2.89 | 3.05 |\n\nTable 4: Results are evaluated in Perplexity on PG19 validation split. '*' denotes train-free, implementing 3D-RPE directly on the LLaMA2-7B-Base model without additional fine-tuning, utilizing a chunk size of $3 k$. The context length of $8 k$ is extended directly with 3D-RPE. Achieving $16 k$ and $32 k$ is accomplished through linear positional interpolation with chunks based on the $8 k$ context length. | MODELS | $4 k$ | $8 k$ | $16 k$ | $32 k$ |\n| :--- | :---: | :---: | :---: | :---: |\n| LLaMA2-7B-PI | 7.94 | 9.19 | 15.11 | $>10^{2}$ |\n| LLaMA2-7B-NTK | 7.87 | 11.98 | 26.12 | 58.91 |\n| LLaMA2-7B-Yarn | 7.87 | 8.06 | 9.82 | 11.74 |\n| 3D-RPE-LLaMA2-7B* | 7.87 | $\\mathbf{7 .",
    "3drpe-3": "9 0}$ | $\\mathbf{7 . 7 1}$ | $\\mathbf{9 . 3 4}$ |\n\n3D-RPE, improves or maintains modeling performance for larger context windows ( $32 k$ ) compared to smaller ones ( $8 k$ and $16 k$ ). For the $32 k$ and $100 k$ tasks, although our model did not surpass LongLoRA- $32 k$ and LongLoRA-100k, it did outperform LongChat- $32 k$ and Activation Beacon. Notably, our model can further extend from $32 k$ to $100 k$ without significantly increasing perplexity values, in combination with other train-free extension methods. However, due to its specific attention mechanism, the LongLoRA models cannot be extended beyond their predefined context windows in a train-free manner. For instance, LongLoRA- $32 k$ cannot be further extended to $100 k$. ### 5.4 Ablation Study\n\nIn this section, we conduct ablation studies in this section to explore how 3D-RPE affects the linear interpolation method. We compare position interpolation methods (PI, NTK, and Yarn) with the method that combines 3D-RPE with position interpolation on the LLaMA-2-7B-Base model in a train-free manner. The experimental results can be found in Table 2 . The 3D-RPE-LLaMA2-7B* model with linearly positional interpolation from $8 k$ to $16 k$ and $32 k$, the 3D-RPE approach yields improved results by mitigating the decrease in positional resolution caused by interpolation methods. These results are consistent with the findings of Theorem 1 in Section 3.2.2 presented in this paper. ## 6 Conclusion and Future Work\n\nIn this paper, we present a novel rotary position encoding method called 3D Rotary Position Encoding (3D-RPE). Compared to RoPE, we have theoretically proved that 3D-RPE possesses two key advantages: controllable long-term decay and enhanced interpolation resolution. Experimentally, 3D-RPE has demonstrated outstanding performance in long-context Natural Language Understanding. In the future, 3D-RPE holds promise as a foundational positional encoding strategy for LLMs, especially in the aspect of modeling long contexts. Moreover, given that 3D-RPE encapsulates positional encoding within a three-dimensional framework, it has the potential to integrate with visual data, thereby facilitating an in-depth exploration of its efficacy in synchronizing graphical and textual semantic information. ## References\n\n[1] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023. [2] Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models, 2024. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [5] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307, 2023. [6] Yukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Long alpaca: Long-context instruction-following models. https://github.com/dvlab-research/ LongLoRA 2023. [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.",
    "3drpe-4": "arXiv preprint arXiv:2204.02311, 2022. [9] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset. https: //github.com/togethercomputer/RedPajama-Data.",
    "3drpe-5": "2023. [10] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.",
    "3drpe-6": "CoRR, 2023. [11] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60 th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335, 2022. [12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [13] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.",
    "3drpe-7": "[14] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?",
    "3drpe-8": "arXiv preprint arXiv:2306.04537, June 2023. [15] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.",
    "3drpe-9": "In ICLR, 2019, 2019. [16] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis.",
    "3drpe-10": "arXiv preprint arXiv:2203.13474, 2022. [17] Bowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended $(8 \\mathrm{k}+)$ context size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_ allows_llama_models_to_have 2023. [18] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2203.13474, 2023. [19] Oriol Press, Noah A Smith, and Michael Lewis. Train short, test long: Attention with linear biases enables input length extrapolation.",
    "3drpe-11": "In ICLR, 2022, 2022. [20] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. [21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. [22] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 464-468, 2018. [23] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "3drpe-12": "Neurocomputing, 568:127063, 2024. [24] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.",
    "3drpe-13": "arXiv preprint arXiv:2307.09288, 2023. [25] Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142os. Focused transformer: Contrastive training for context scaling.",
    "3drpe-14": "arXiv preprint arXiv:2307.03170, 2023. [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "3drpe-15": "Advances in neural information processing systems, 30, 2017. [27] Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model.",
    "3drpe-16": "GitHub, 2021. [28] Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding word order in complex embeddings. In International Conference on Learning Representations, 2020 . [29] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv, 2023. [30] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Soaring from 4 k to 400k: Extending llm's context with activation beacon, 2024. [31] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. ## A Bloch Sphere\n\nBloch Sphere: 3D Rotary Position Encoding (3D-RPE), proposed by us, corresponds to a Bloch Sphere. In this section, we mainly introduce the basic concept of Bloch Sphere, which corresponds to Eq. (1) in this paper. The Bloch Sphere is a geometric tool to used to represent qubits, typically depicted in a three-dimensional polar coordinate system as a point on the Sphere (see Figure 4. A single quantum state is represented by the following equation in linear algebra:\n\n$$\n|\\phi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle\n$$\n\nwhere $\\alpha$ and $\\beta$ are complex numbers, i.e., $\\alpha, \\beta \\in \\mathbb{C}$. According to Euler's formula in complex analysis, the coefficients $\\alpha$ and $\\beta$ can be reexpressed as:\n\n$$\n\\begin{aligned}\n& \\alpha=a+b \\mathrm{i}=r_{0}\\left(\\cos \\left(\\theta_{\\alpha}\\right)+\\mathrm{i} \\sin \\left(\\theta_{\\alpha}\\right)\\right)=r_{0} e^{\\mathrm{i} \\theta_{\\alpha}} \\\\\n& \\beta=c+d \\mathrm{i}=r_{1}\\left(\\cos \\left(\\theta_{\\beta}\\right)+\\mathrm{i} \\sin \\left(\\theta_{\\beta}\\right)\\right)=r_{1} e^{\\mathrm{i} \\theta_{\\beta}}\n\\end{aligned}\n$$\n\nBy substituting Eq. (10) into Eq. (9), the quantum state representation is denoted as:\n\n$$\n\\begin{aligned}\n|\\phi\\rangle & =r_{0}\\left(\\cos \\left(\\theta_{\\alpha}\\right)+\\mathrm{i} \\sin \\left(\\theta_{\\alpha}\\right)\\right)|0\\rangle+r_{1}\\left(\\cos \\left(\\theta_{\\beta}\\right)+\\mathrm{i} \\sin \\left(\\theta_{\\beta}\\right)\\right)|1\\rangle \\\\\n& =r_{0} e^{\\mathrm{i} \\theta_{\\alpha}}|0\\rangle+r_{1} e^{\\mathrm{i} \\theta_{\\beta}}|1\\rangle \\\\\n& =e^{\\mathrm{i} \\theta_{\\alpha}}\\left(r_{0}|0\\rangle+r_{1} e^{\\mathrm{i}\\left(\\theta_{\\beta}-\\theta_{\\alpha}\\right)}|1\\rangle\\right)\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d426bc005c38a790628bg-12.jpg?height=751&width=744&top_left_y=275&top_left_x=688)\n\nFigure 4: A diagram of Bloch Sphere. $\\theta_{\\alpha}$ is the global phase. Considering the normalization condition $|\\alpha|^{2}+|\\beta|^{2}=1$, we have:\n\n$$\n\\left|r_{0}\\right|^{2}+\\left|r_{1} e^{\\mathrm{i}\\left(\\theta_{\\beta}-\\theta_{\\alpha}\\right)}\\right|^{2}=r_{0}^{2}+r_{1}^{2}\\left|e^{\\mathrm{i}\\left(\\theta_{\\beta}-\\theta_{\\alpha}\\right)}\\right|^{2}=r_{0}^{2}+r_{1}^{2}=1\n$$\n\nGiven $r_{0}=\\cos \\frac{\\varphi}{2}, r_{1}=\\sin \\frac{\\varphi}{2}, \\theta_{\\alpha}=\\theta$ and $\\theta_{\\beta}-\\theta_{\\alpha}=\\theta_{1}$, the state $|\\phi\\rangle$ can be expressed as:\n\n$$\n|\\phi\\rangle=e^{\\mathrm{i} \\theta}\\left(\\cos \\frac{\\varphi}{2}|0\\rangle+\\sin \\frac{\\varphi}{2} e^{\\mathrm{i} \\theta_{1}}|1\\rangle\\right)\n$$\n\nTherefore, the Eq. (1) of this paper is given out. To adapt to the original 2D rotation position encoding (RoPE) of pre-trained LLMs, such as LLaMA models, the global phase $\\theta$ is used to model the relative positions between tokens within a chunk, while the rotation angle $\\frac{\\varphi}{2}$ is used to model the relative positions between tokens across chunks. ## B Supplementary Material for the Method Section\n\nIn this section, we mainly introduce the specific implementation of our positional encoding method (3D-RPE), and the formula derivation details of attention score calculation (Eq.",
    "3drpe-17": "(5) of this paper) not detailed in this paper. ## B. 1 Implement of 3D-RPE\n\nIn Section 3.1, we give the general form of 3D Rotary Position Encoding (3D-RPE):\n\n$$\n\\widetilde{\\boldsymbol{h}}_{j, m}=e^{\\mathrm{i} m \\theta}\\left(\\cos \\varphi_{j} \\boldsymbol{h}_{j, m}^{\\perp}+\\sin \\varphi_{j} \\boldsymbol{h}_{j, m}\\right)\n$$\n\n$\\cos \\varphi_{j}$ and $\\sin \\varphi_{j}$ are scalar quantities in $\\mathbb{R}$. $\\boldsymbol{h}_{j, m}^{\\perp}$ and $\\boldsymbol{h}_{j, m}$ are shown below:\n\n$$\n\\boldsymbol{h}_{j, m}=\\left[\\begin{array}{c}\nh^{0} \\\\\nh^{1} \\\\\n\\vdots \\\\\nh^{d / 2-2} \\\\\nh^{d / 2-1} \\\\\nh^{d / 2} \\\\\nh^{d / 2+1} \\\\\n\\vdots \\\\\nh^{d-2} \\\\\nh^{d-1}\n\\end{array}\\right] \\quad \\boldsymbol{h}_{j, m}^{\\perp}=\\left[\\begin{array}{c}\n-h^{d / 2} \\\\\n-h^{d / 2+1} \\\\\n\\vdots \\\\\n-h^{d-2} \\\\\n-h^{d-1} \\\\\nh^{0} \\\\\nh^{1} \\\\\n\\vdots \\\\\nh^{d / 2-2} \\\\\nh^{d / 2-1}\n\\end{array}\\right]\n$$\n\nIn the concrete implementation, analogous to RoPE, for each two-dimensional subspace $\\mathbb{R}^{2}$ of $\\mathbb{R}^{d}$, we assign angles $\\theta_{l}=$ base $e^{-2 l / d}$ that vary from high to low frequencies. An equivalent rotation matrix $\\mathcal{R}_{m}^{\\theta}$ is utilized to substitute for $e^{\\mathrm{i} m \\theta}$ :\n\n$$\n\\mathcal{R}_{m}^{\\theta}=\\left[\\begin{array}{cccccccc}\n\\cos m \\theta_{0} & 0 & \\cdots & 0 & -\\sin m \\theta_{0} & 0 & \\cdots & 0 \\\\\n0 & \\cos m \\theta_{1} & \\cdots & 0 & 0 & -\\sin m \\theta_{1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\cos m \\theta_{d / 2-1} & 0 & 0 & \\cdots & -\\sin m \\theta_{d / 2-1} \\\\\n\\sin m \\theta_{0} & 0 & \\cdots & 0 & \\cos m \\theta_{0} & 0 & \\cdots & 0 \\\\\n0 & \\sin m \\theta_{1} & \\cdots & 0 & 0 & \\cos m \\theta_{1} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sin m \\theta_{d / 2-1} & 0 & 0 & \\cdots & \\cos m \\theta_{d / 2-1}\n\\end{array}\\right]\n$$\n\nTherefore, Eq.(4) of this paper can be transformed to\n\n$$\n\\widetilde{\\boldsymbol{h}}_{j, m}=\\mathcal{R}_{m}^{\\theta}\\left(\\cos \\varphi_{j} \\boldsymbol{h}_{j, m}^{\\perp}+\\sin \\varphi_{j} \\boldsymbol{h}_{j, m}\\right)\n$$\n\nwhere $\\mathcal{R}_{m}^{\\theta}$ is a design form equivalent to the rotation matrix in RoPE, mainly re-mapped to correspond to specific application implementations and calculation derivations in LLMs. In the specific implementation, after the rotary position encoding of LLMs, the long sequence is chunked based on the chunk size $c$.",
    "3drpe-18": "Then, the rotation $\\varphi_{j}$ is set on each chunk, $j$ is the position of chunk. ## B. 2 Derivation of Attention for 3D-RPE\n\nThe formula derivation details of attention score calculation(Eq. (5)) is as follows. Since $\\boldsymbol{h}^{\\perp}=e^{\\mathrm{i} \\frac{\\pi}{2}} \\boldsymbol{h}=\\mathrm{i} \\boldsymbol{h}$, we could get:\n\n$$\n\\begin{aligned}\n\\widetilde{\\boldsymbol{h}}_{j, m} & =e^{\\mathrm{i} m \\theta}\\left(\\mathrm{i} \\cos \\varphi_{j} \\boldsymbol{h}_{j, m}+\\sin \\varphi_{j} \\boldsymbol{h}_{j, m}\\right) \\\\\n& =e^{\\mathrm{i} m \\theta}\\left(\\mathrm{i} \\sin \\left(\\frac{\\pi}{2}-\\varphi_{j}\\right) \\boldsymbol{h}_{j, m}+\\cos \\left(\\frac{\\pi}{2}-\\varphi_{j}\\right) \\boldsymbol{h}_{j, m}\\right) \\\\\n& =e^{\\mathrm{i} m \\theta} e^{\\mathrm{i} \\frac{\\pi}{2}-\\varphi_{j}} \\boldsymbol{h}_{j, m}\n\\end{aligned}\n$$\n\nLet $\\boldsymbol{q}_{i, m}=3 \\mathrm{~d}-\\operatorname{PE}(\\boldsymbol{q}, i, m), \\boldsymbol{k}_{j, n}=3 \\mathrm{~d}-\\operatorname{PE}(\\boldsymbol{k}, j, n)$. Taking the real part of the inner product of $\\boldsymbol{q}_{i, m}$ and $\\boldsymbol{k}_{j, n}$ yields:\n\n$$\ns\\left(\\boldsymbol{q}_{i, m}, \\boldsymbol{k}_{j, n}\\right)=\\operatorname{Re}\\left[e^{\\mathrm{i}\\left(\\varphi_{i}-\\varphi_{j}\\right)} \\sum_{l=0}^{d / 2-1} e^{\\mathrm{i}(m-n) \\theta_{l}}\\left(\\boldsymbol{q}_{l} \\boldsymbol{k}_{l}+\\boldsymbol{q}_{d / 2+l} \\boldsymbol{k}_{d / 2+l}\\right)\\right]\n$$\n\nwhich is a function related to both $m-n$ and $\\varphi_{i}-\\varphi_{j}$. ## C 3D Rotary Position Encoding Resolution Enhancement\n\nIn this section, before proving Theorem 1, we first provide the definitions of positional resolution for RoPE, as well as the positional resolution after positional interpolation. Definition 2 (Positional Interpolation Resolution). Let $\\boldsymbol{q}_{m+1}$ and $\\boldsymbol{k}_{m}$ be query state and key state of the $m$-th and $(m+1)$-th hidden states after RoPE. Given a pre-training length $L_{p}$, the attention score a is:\n\n$$\na\\left(\\boldsymbol{q}_{m+1}, \\boldsymbol{k}_{m}\\right)=\\boldsymbol{q} \\boldsymbol{k}^{T} e^{\\mathrm{i} \\theta}\n$$\n\nThe Resolution $\\mathcal{E}_{\\text {rope }}$ corresponding to the initial length $L_{p}$ is $\\mathcal{E}_{\\text {rope }}=1$. After employing linear interpolation with length $L \\geq L_{p}$, the attention score is:\n\n$$\na\\left(\\boldsymbol{q}_{m+1}, \\boldsymbol{k}_{m}\\right)=\\boldsymbol{q} \\boldsymbol{k}^{T} e^{\\mathrm{i} \\frac{L_{p}}{L} \\theta}\n$$\n\nNote that the Resolution $\\mathcal{E}_{\\text {rope }}$ turns to $\\mathcal{E}_{\\text {rope }}^{\\prime}=L_{p} / L \\leq 1$ and decreases as $L$ increases. As the resolution decreases, the magnitude of the rotation of attention score becomes smaller, reflecting the extent of positional difference becomes smaller. Now we give the following theorem, explaining how 3D-RPE mitigates the resolution decreasing in detail. Theorem 2 (Chunk Position Encoding Resolution Enhancement). For a pre-trained language model with a pre-training length $L_{p}$ and an extension length requirement of $L$, employing linear position interpolation extension methods $\\mathcal{I}$ based on Rotary Position Encoding (RoPE) can elevate the relative positional resolution from $\\mathcal{E}_{\\text {rope }}$ to $\\mathcal{E}_{\\text {rope }}^{\\prime}$, Let $\\mathcal{E}_{3 d-r p e}^{\\prime}$ denote the relative positional encoding resolution achieved by the method $\\mathcal{I}$ based on $3 D-R P E$, with chunk size $c>=3$, there is:\n\n$$\n\\mathcal{E}_{3 d-r p e}^{\\prime}>\\mathcal{E}_{\\text {rope }}^{\\prime}\n$$\n\nProof.",
    "3drpe-19": "For 3D-RPE, let the chunk size and chunk number be denoted as $c$ and $n=\\left\\lceil L_{p} / c\\right\\rceil$ respectively. Prior to interpolation, the indices within a chunk range from $[0,1, \\cdots, c-1]$. Linear interpolation involves evenly distributing the excess $L-L_{p}$ tokens across $n$ chunks. This results in new indices within the chunk, range from $\\left[0,1,2, \\cdots, c^{\\prime}-1\\right]$, where $c^{\\prime}=\\lceil L / n\\rceil \\leq L_{p}$. So the attention score of $\\boldsymbol{q}_{i, m+1}$ and $\\boldsymbol{k}_{i, m}$ based on 3D-RPE after interpolation is:\n\n$$\n\\begin{aligned}\na_{3 d-r p e} & =\\boldsymbol{q} \\boldsymbol{k}^{T} e^{\\mathrm{i} \\theta} e^{\\mathrm{i}\\left(\\varphi_{i}-\\varphi_{i}\\right)} \\\\\n& =\\boldsymbol{q} \\boldsymbol{k}^{T} e^{\\mathrm{i} \\theta}\n\\end{aligned}\n$$\n\nThe resolution of relative position for 3D-RPE is:\n\n$$\n\\mathcal{E}_{3 d-r p e}^{\\prime}=1\n$$\n\nFor special cases $\\boldsymbol{q}_{(i+1,0)}$ and $\\boldsymbol{k}_{\\left(i, c^{\\prime}-1\\right)}$ :\n\n$$\n\\mathcal{E}_{3 d-r p e}^{\\prime} \\geq c^{\\prime}-1+\\frac{\\left(\\varphi_{i+1}-\\varphi_{i}\\right)}{\\theta}>c^{\\prime}-2 \\geq 1\n$$\n\nwhere $\\left(\\varphi_{i+1}-\\varphi_{i}\\right) / \\theta \\geq-1 / 10000>-1$. As long as $c^{\\prime} \\geq 3$, there is $\\mathcal{E}_{3 d-r p e}^{\\prime} \\geq 1>\\mathcal{E}_{\\text {rope }}^{\\prime}=L_{p} / L$. Under normal case, the chunk size $c$ is not set to a very small number, hence $c^{\\prime} \\geq 3$ is certainly established; moreover, for different interpolation lengths $L$, we need to configure a varying number of chunks $n$, such that $c^{\\prime}=\\lceil L / n\\rceil \\leq$ $L_{p}$. ## D Experimental Supplementary Materials\n\n## D. 1 Evaluation Metrics\n\nThis section mainly presents the utilization of evaluation metrics for a total of 16 tasks from the LongBench. | Dataset | Metric |\n| :---: | :---: |\n| Narrative QA | F1_Score |\n| Qsper | F1_Score |\n| MultiFieldQA-En | F1_Score |\n| Hotpot QA | F1_Score |\n| 2WikiM QA | F1_Score |\n| Musique | F1_Score |\n| GovReport | Rouge_Score |\n| QMSum | Rouge_Score |\n| MultiNews | Rouge_Score |\n| Trec | Classification_Score |\n| Trivia QA | F1_Score |\n| SAMsum | Rouge_Score |\n| PassageRetrieval-En | Retrieval_Score |\n| Passage Count | Count_Score |\n| Lcc | Code_Sim_Score |\n| RepoBench-P | Code_Sim_Score |\n\n## D. 2 Details of Experimental Results\n\nThis section mainly presents the performance of all tasks corresponding to each type of experiment in LongBench. These experimental results are reported in Table 5\n\nTable 5: Comparison of Experimental Performance on Different Datasets for Various Tasks in LongBench, Using Baseline Models Provided by LongBench. 3D-RPE-LLaMA2-7B is our model. | Single-Document QA | Narrative QA | Qasper | MultiFieldQA-En |\n| :---: | :---: | :---: | :---: |\n| LLaMA2-7B-Chat-4k | 18.7 | 19.2 | 36.8 |\n| LongChat-v1.5-7B-32k | 16.9 | 27.7 | 41.4 |\n| InternLM-7B-8k | 12.1 | 16.7 | 23.4 |\n| Vicuna-v1.5-7B-16k | 19.4 | 26.1 | 38.5 |\n| LongLora-16 $k$ | 19.8 | 29.1 | 37.1 |\n| 3D-RPE-LLaMA2-7B(our) | 40.56 | 41.35 | 60.3 |\n| Multi-Document QA | Hotpot QA | 2WikiM QA | Musique |\n| LLaMA2-7B-chat-4k | 25.4 | 32.8 | 9.4 |\n| LongChat-v1.5-7B-32k | 31.5 | 20.6 | 9.7 |\n| InternLM-7B-8k | 28.7 | 22.8 | 9.0 |\n| Vicuna-v1.5-7B-16 | 25.3 | 20.8 | 9.8 |\n| LongLora-16k | 37.01 | 30.26 | 17.14 |\n| 3D-RPE-LLaMA2-7B(our) | 62.49 | 58.80 | 59.01 |\n| Summarization | GovReport | QMSum | MultiNews |\n| LLaMA2-7B-chat-4 $k$ | 27.3 | 20.8 | 25.8 |\n| LongChat-v1.5-7B-32k | 30.8 | 22.7 | 26.4 |\n| InternLM-7B- $k$ | 9.7 | 15.9 | 22.8 |\n| Vicuna-v1.5-7B-16 | 27.9 | 22.8 | 27.2 |\n| LongLora-16 $k$ | 31.53 | 24.13 | 27.74 |\n| 3D-RPE-LLaMA2-7B(our) | 32.01 | 25.3 | 29.68 |\n| Few-shot Learning | Trec | Trivia QA | SAMSum |\n| LLaMA2-7B-chat-4k | 61.5 | 77.8 | 40.7 |\n| LongChat-v1.5-7B-32k | 63.5 | 82.3 | 34.2 |\n| InternLM-7B- $k$ | 52.0 | 77.8 | 21.2 |\n| Vicuna-v1.5-7B-16 $k$ | 71.5 | 86.2 | 40.8 |\n| LongLora-16k | 63.5 | 85.69 | 41.88 |\n| 3D-RPE-LLaMA2-7B-16 $k$ (our) | 89.50 | 90.00 | 40.00 |\n| Synthetic Tasks | Passage Count | PassageRetrival-En |  |\n| LLaMA2-7B-chat-4k | 2.1 | 9.8 |  |\n| LongChat-v1.5-7B-32k | 1.0 | 30.5 |  |\n| InternLM-7B- $8 k$ | 3.0 | 6.0 |  |\n| Vicuna-v1.5-7B-16k | 6.5 | 4.5 |  |\n| LongLora-16 $k$ | 3.61 | 29.75 |  |\n| 3D-RPE-LLaMA2-7B-16k(our) | 4.0 | 14.5 |  |\n| Code Completion | Lcc | RepoBench-P |  |\n| LLaMA2-7B-chat-4k | 52.4 | 43.8 |  |\n| LongChat-v1.5-7B-32k | 53.0 | 55.3 |  |\n| InternLM-7B-8 $k$ | 44.1 | 28.8 |  |\n| Vicuna-v1.5-7B-16k | 51.0 | 43.5 |  |\n| LongLora-16k | 57.61 | 54.45 |  |\n| 3D-RPE-LLaMA2-7B-16 $k$ (our) | 79.10 | 73.90 |  |\n\n\n[^0]:    ${ }^{*}$ Corresponding Author: Peng Zhang\n\n[^1]:    ${ }^{2}$ https://github.com/meta-llama/llama3\n    ${ }^{3}$ https://github.com/dvlab-research/LongLoRA/\n\n"
}