{
    "spectralssm-0": "# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.",
    "spectralssm-1": "We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.",
    "spectralssm-2": "We discuss the details of this ablation in the appendix (Section A.1). Spectral filtering. The technique of spectral filtering [HSZ17] was developed as a convex improper learning alternative to directly parameterizing an LDS (as in the case of SSMs) leading to an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods (eg. SSMs) that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in [HLS ${ }^{+}$18]. Convolutional Models for Sequence Modeling. Exploiting the connnection between LDS and convolutions [GGR21], various convolutional models have been proposed for sequence modelling. $\\left[\\mathrm{FEN}^{+}\\right.$23] employ direct learning of convolutional kernels but find that they underperform SSMs, identifying non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations. [LCZ $\\left.{ }^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left[\\mathrm{LCZ}^{+} 22\\right]$ and $\\left[\\mathrm{FEN}^{+} 23\\right]$. ## 2 Preliminaries\n\nSequence prediction. We treat sequence prediction as a game between a predictor/learner and nature in which iteratively at every time $t \\in[L]$, the learner is presented an input $u_{t} \\in \\mathbb{R}^{d_{\\text {in }}}$. The learner $A$ then produces a candidate output $\\hat{y}_{t}=\\hat{y}_{t}(A)$, and nature reveals the $t^{t h}$ element of a target sequence $y_{t} \\in \\mathbb{R}^{d_{\\text {out }}}$. The learner then suffers an instantaneous loss of $\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}$. The task of the learner is to minimize regret over a benchmark set of learning algorithms $\\mathcal{A}$, defined as follows\n\n$$\n\\text { Regret }=\\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}-\\min _{A \\in \\mathcal{A}} \\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}(A)\\right\\|^{2}\n$$\n\nLinear Dynamical Systems (LDS): An example benchmark set of methods is that of a linear dynamical system, which has four matrix parameters, $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\text {in }}}, C \\in \\mathbb{R}^{d_{\\text {out }} \\times N}, D \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The system evolves and generates outputs according to the following equations\n\n$$\nx_{t} \\triangleq A x_{t-1}+B u_{t}, \\quad \\hat{y}_{t} \\triangleq C x_{t}+D u_{t}\n$$\n\nThus, an example class of benchmark algorithms $\\mathcal{A}$ are all predictors that generate $\\hat{y}_{t}$ according to these rules, for a fixed set of matrices $A, B, C, D$.",
    "spectralssm-3": "Spectral Filtering: Another important set of predictors is one which is inspired by spectral filtering [HSZ17]. The spectral filtering theory builds an efficient representation for all vectors in the range of the function $\\mu:[0,1] \\rightarrow \\mathbb{R}^{L}$ defined as $\\mu(\\alpha) \\triangleq(\\alpha-1)\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$. To build this representation, for any $L$ define the following Hankel matrix $Z \\in \\mathbb{R}^{L \\times L}$ whose entries are given by\n\n$$\nZ[i, j] \\triangleq \\frac{2}{(i+j)^{3}-(i+j)}\n$$\n\nIt is shown in the appendix (see Lemma C.1p that $Z=\\int_{0}^{1} \\mu(\\alpha) \\mu(\\alpha)^{\\top} d \\alpha$. Thus it can be seen that $Z$ is a real PSD Hankel matrix. It is known (see LemmaC. 4 in the appendix) that real PSD Hankel matrices have an exponentially decaying spectrum. As a result, the crux of the spectral filtering theory, lies in showing that for all $\\alpha \\in[0,1]]^{1}$, the vector $\\mu(\\alpha)$ is approximately contained in the\n\n[^0]subspace spanned by the top eigenvectors of $Z$, making the subspace spanned by top-eigenvectors of Z a very efficient subspace to project the input into. This fact is formalized as Lemma C.3 in the appendix. We now use this intuition to describe the Spectral Filtering algorithm. Since Z is a real PSD matrix, it admits a real spectral decomposition, and the (non-negative) eigenvalues can be easily ordered naturally by their value. Let $\\left\\{\\left(\\sigma_{j} \\in \\mathbb{R}, \\phi_{j} \\in \\mathbb{R}^{T}\\right)\\right\\}_{j=1}^{L}$ be the eigenvalueeigenvector pairs of $Z$ ordered to satisfy $\\sigma_{1} \\geq \\sigma_{2} \\geq \\ldots \\geq \\sigma_{d}$. We consider a fixed number $K$ of the above eigenvectors. Algorithms in the spectral filtering class generate $\\hat{y}_{t}$ as follows. For each $k \\in K$, we first featurize the input sequence by projecting the input sequence until time $t$ on $\\phi_{k}$, leading to a sequence $U_{t, k} \\in \\mathbb{R}^{d_{\\text {in }}}$ defined as\n\n$$\nU_{t, k}=\\sum_{i=1}^{t} u_{t-i} \\cdot \\phi_{k}(i)\n$$\n\nThe spectral filtering class is further parameterized by matrices $M_{1}^{u} \\in \\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}, M_{2}^{u} \\in \\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$ and a set of matrices $M_{1}^{\\phi}, \\ldots, M_{K}^{\\phi} \\in \\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The output at time $t$ is then given by\n\n$$\n\\hat{y}_{t}=\\hat{y}_{t-1}+M_{1}^{u} u_{t}+M_{2}^{u} u_{t-1}+\\sum_{k=1}^{K} M_{k}^{\\phi} U_{t, k}\n$$\n\nNote that given an input sequence $u_{1: L}$ for any $k$, the $d_{\\text {in }} \\times T$ matrix $U_{1: L, k}$ can be efficiently computed via convolutions along the time dimension $L$ in total time $O\\left(d_{\\mathrm{in}} \\cdot L \\log (L)\\right)$. The following theorem (proved in [HSZ17]) establishes that the spectral filtering class of predictors approximately contains bounded linear dynamical systems with positive semi-definite $A$. Theorem 2.1. Given any $A, B, C, D$ such that $A$ is a $P S D$ matrix with $\\|A\\| \\leq 1$ and given any numbers $K \\in \\mathbb{I}^{+}, a \\in \\mathbb{R}^{+}$, there exists matrices $M_{1}^{u}, M_{2}^{u}, M_{1}^{\\phi}, \\ldots, M_{K}^{\\phi}$, such that for all $L$ and all sequences $u_{1: L}$ satisfying $\\left\\|u_{t}\\right\\| \\leq$ for all $t \\in[L]$ the following holds. Let $y_{1: L}^{\\mathrm{LDS}}$ be the sequence generated by execution of the LDS given by $A, B, C, D$ (via (1) and $y_{1: L}^{\\mathrm{SF}}$ be the sequence generated by Spectral Filtering (via (2)) using the matrices $M_{1}^{u}, M_{2}^{u}, M_{1}^{\\phi}, \\ldots, M_{K}^{\\phi}$. Then for all $t \\in[L]$,\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{SF}}\\right\\|^{2} \\leq c \\cdot\\|B\\|_{\\text {col }} \\cdot\\|C\\|_{\\text {col }} \\cdot L^{3} \\cdot a \\cdot e^{-\\left(\\frac{\\pi^{2}}{4} \\cdot \\frac{K}{\\log (L)}\\right)}\n$$\n\nwhere $c \\leq 10^{6}$ is a universal constant and $\\|B\\|_{\\text {col }},\\|C\\|_{\\text {col }}$ are the maximum column norm of the matrices $B$ and $C$ respectively. We do not provide a proof for this theorem which can be found in HSZ172 Instead, in the next section we provide a generalization of this theory to cover all symmetric matrices and not just PSD matrices and prove a more general theorem (Theorem 3.1). We further build upon this generalization to create a sequence to sequence prediction unit. ## 3 Spectral Transform Unit (STU)\n\nIn this section we use Spectral Filtering to create a sequence to sequence neural network layer, i.e. given an input sequence $\\left\\{u_{1} \\ldots u_{L}\\right\\} \\in \\mathbb{R}^{d_{\\text {in }}}$, it produces an output sequence $\\left\\{y_{1} \\ldots y_{L}\\right\\} \\in \\mathbb{R}^{d_{\\text {out }}}$. A single layer of STU (depicted in Figure 2) is parameterized by a number $K$, denoting the number of eigenfactors and matrices $M_{1}^{\\phi+} \\ldots M_{K}^{\\phi+}, M_{1}^{\\phi-} \\ldots M_{K}^{\\phi-} \\in \\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$, and $M_{1}^{u}, M_{2}^{u}, M_{3}^{u} \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The matrices form the params of the layer. Further recall the Hankel matrix $Z \\in \\mathbb{R}^{L \\times L}$ whose entries are given by\n\n$$\nZ[i, j] \\triangleq \\frac{2}{(i+j)^{3}-(i+j)}\n$$\n\nand let $\\left\\{\\left(\\sigma_{j} \\in \\mathbb{R}, \\phi_{j} \\in \\mathbb{R}^{L}\\right)\\right\\}_{j=1}^{L}$ be the eigenvalue-eigenvector pairs of $Z$ ordered to satisfy $\\sigma_{1} \\geq \\sigma_{2} \\geq \\ldots \\sigma_{d}$. Given an input sequence $\\left\\{u_{1} \\ldots u_{L}\\right\\} \\in \\mathbb{R}^{d_{\\text {in }}}$, we first featurize the input sequence\n\n[^1]![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-06.jpg?height=476&width=1006&top_left_y=250&top_left_x=554)\n\nFigure 2: Schematic showing the spectral projection of a 1-dimensional input sequence and how these features are used to produce the spectral component in the STU output (4). In the multi-dimensional case the operation is applied in parallel across every input dimension. as follows. For any $t, k$, we begin by projecting the input sequence till time $t$ on fixed filters $\\phi_{k}$, leading to two feature vectors $U_{t, k}^{+}, U_{t, k}^{-} \\in \\mathbb{R}^{d_{\\text {in }}}$ defined as\n\n$$\nU_{t, k}^{+}=\\sum_{i=0}^{t-1} u_{t-i} \\cdot \\phi_{k}(i) \\quad U_{t, k}^{-}=\\sum_{i=0}^{t-1} u_{t-i} \\cdot(-1)^{i} \\cdot \\phi_{k}(i)\n$$\n\nNote that for every $k$, the sequence of features $U_{1: L, k}$ can be computed efficiently via convolution. The output sequence $\\left\\{y_{1} \\cdots y_{L}\\right\\}$ is then given by\n\n$$\n\\hat{y}_{t}=\\underbrace{\\hat{y}_{t-2}+\\sum_{i=1}^{3} M_{i}^{u} u_{t+1-i}}_{\\text {Auto-regressive Component }}+\\underbrace{\\sum_{k=1}^{K} M_{k}^{\\phi+} \\sigma_{k}^{1 / 4} U_{t-2, k}^{+}+\\sum_{k=1}^{K} M_{k}^{\\phi-} \\sigma_{k}^{1 / 4} U_{t-2, k}^{-}}_{\\text {Spectral Component }}\n$$\n\nThe above output contains a small auto-regressive component that essentially allows for stable learning of the spectral component as the memory grows. The differences from the original spectral filtering class (2) are the introduction of a negative part in the spectral component and the slight change in the auto-regressive component. Both of these changes are necessitated by the requirement to capture negative eigenvalues of $A$. Note that (4) corresponds to the specification of the algorithm presented in $\\mathrm{HLS}^{+}$18], when the eigenvalues are known to be real numbers. For completeness and ease of discourse we prove the following representation theorem in the Appendix which shows that the above class approximately contains any marginally-stable LDS with symmetric $A^{3}$\nTheorem 3.1. Given any $A, B, C, D$ such that $A$ is a symmetric matrix with $\\|A\\| \\leq 1$ and given any numbers $K \\in \\mathbb{I}^{+}, a \\in \\mathbb{R}^{+}$, there exists matrices $M_{1}^{u}, M_{2}^{u}, M_{3}^{u}, M_{1}^{\\phi+} \\ldots M_{K}^{\\phi+}, M_{1}^{\\phi-} \\ldots M_{K}^{\\phi-} \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\mathrm{in}}}$, such that for all $L$ and all sequences $u_{1: L}$ satisfying $\\left\\|u_{t}\\right\\| \\leq$ a for all $t \\in[L]$ the following holds. Let $y_{1: L}^{\\mathrm{LDS}}$ be the sequence generated by execution of the LDS given by $A, B, C, D$ (via (1)) and $y_{1: L}^{\\mathrm{SF}}$ be the sequence generated by Spectral Filtering (via (4)) using the matrices $M_{1}^{u}, M_{2}^{u}, M_{3}^{u}, M_{1}^{\\phi+} \\ldots M_{K}^{\\phi+}, M_{1}^{\\phi-} \\ldots M_{K}^{\\phi-}$. Then for all $t \\in[T]$, we have that\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{SF}}\\right\\|^{2} \\leq c \\cdot\\|B\\|_{\\text {col }} \\cdot\\|C\\|_{c o l} \\cdot L^{3} \\cdot a \\cdot e^{-\\left(\\frac{\\pi^{2}}{4} \\cdot \\frac{K}{\\log (L)}\\right)}\n$$\n\nwhere $c \\leq 2 \\times 10^{6}$ is a universal constant and $\\|B\\|_{\\text {col }},\\|C\\|_{\\text {col }}$ are the maximum column norm of the matrices $B$ and $C$ respectively. The above theorem in particular ensures for any sequence length $L$ that setting $K=$ $O\\left(\\log (L) \\log \\left(\\frac{\\|B\\|_{\\text {col }} \\cdot\\|C\\|_{\\text {col }} \\cdot L \\cdot a}{\\epsilon}\\right)\\right)$ we get there exists a spectral filtering model with $K$ filters that can approximate any LDS upto an error of $\\epsilon$. Note that the requirement on the number of filters grows logarithmically in $L$, highlighting the efficiency of the representation. The proof of the above\n\n[^2]![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-07.jpg?height=494&width=1404&top_left_y=243&top_left_x=360)\n\nFigure 3: Learning dynamics for learning a marginally stable LDS. (a.)(Smoothed) Learning curves for a single STU layer (red) vs a single LRU layer (black). The learning rate was tuned for both models. See Appendix for a detailed discussion of the tuning and sensitivity to hyperparameters for both the models. Curiously at stable LRs we observe that LRUs show a plateauing of learning. (b.) Error (in log-scale) obtained by the single STU layer as a function of the model parameter ' K '. We observe an exponential drop in the reconstruction loss as predicted by the analysis. theorem is provided in the appendix along with an alternative version of spectral filtering using slightly modified filters which also provide the same guarantee. Remark 3.2. Comparing Theorem 3.1 (our contribution) and Theorem 2.1 (Theorem 1 from HSZ17]), we note firstly that our theorem holds for symmetric matrices and not just PSD matrices. [HSZ17] allude to a direct extension for the symmetric case which we believe is not fully correct. We use a similar idea to prove this theorem. Secondly a minor difference is that in the sequential prediction setting the prediction is auto-regressive, i.e. uses its own y to make the future predictions. Due to space limitations, we discuss the runtime scaling of our method and compare it with different methods in the appendix (Section A). ### 3.1 Experiment: Learning a marginally-stable LDS\n\nWe provide a simple synthetic evaluation of the stability and training efficiency afforded by the STU. We consider a low-dimensional linear system $A, B, C, D$ generated as follows. $B \\in \\mathbb{R}^{4 \\times 3}, C \\in \\mathbb{R}^{3 \\times 4}$ are matrices with iid unit Gaussian entries. $D$ is a diagonal matrix with iid unit Gaussian entries and $A$ is a diagonal matrix with $A_{i i} \\sim 0.9999 * Z$ where $Z$ is a random sign. By design this is a system with a very high stability constant $\\left(\\sim 10^{4}\\right)$. As a training dataset we generated $\\left\\{\\left(u_{i}, y_{i}\\right)\\right\\}$ where $u_{i}$ is a random input sequence and $y_{i}$ is the output generated by applying the linear dynamical system on $u_{i}$. We perform mini-batch (batch size 1) training with the 12 loss. As comparison we perform the same procedure with an LRU (Linear Recurrent Unit) layer as proposed by [OSG $\\left.{ }^{+} 23\\right]$ which directly parameterizes the linear system. The results of the training loss as seen by the two systems are presented in Figure 3 a\nWe use all the initialization/normalization techniques as recommended by $\\mathrm{OSG}^{+}$23 for LRU including the stable exponential parameterization, $\\gamma$-normalization and ring-initialization.",
    "spectralssm-4": "Indeed we find that all these tricks were necessary to learn this system at all. We provide more details about the ablations and other hyperparameter setups in the appendix. We observe that the STU is significantly more efficient at learning the LDS as opposed to the LRU. We further find that there is a wide range of LRs where the STU has a stable optimization trajectory and the loss decreases continuously highlighting the advantages of a convex parameterization. On the other hand, LRU is able to eventually learn the system at the right learning rates, it requires almost 8 x the number of samples to get to a system with non-trivial accuracy. More details can be found in the appendix. Curiously we observe that for the LRU training plateaus completely for the first $50 \\%$ of training highlighting the difficulty of optimization via a non-convex landscape. The STU layer in the previous experiment employs $K=25$. In Figure 3b we plot the performance of STU at various levels of $K$. As predicted by the theory we observe an exponential decay in the error as $K$ increases with the error effectively plateauing after $K \\geq 15$. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-08.jpg?height=321&width=659&top_left_y=243&top_left_x=386)\n(a) Schematic displaying a multi-layer STU model. | Model | Specification | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: |\n| STU | Eqn $4(\\mathrm{~K}=16)$ | 91.8 | 89.5 |\n| LRU | Dense A | $\\boldsymbol{x}$ | $\\boldsymbol{x}$ |\n|  | $\\Lambda$ Exp. Param. | 65.4 | $\\boldsymbol{x}$ |\n|  | $\\Lambda$ Stable Exp. | 93.5 | $\\boldsymbol{x}$ |\n|  | + Ring Init. | 94.4 | $\\boldsymbol{x}$ |\n|  | $+\\gamma$-Norm. | 95.1 | 94.2 |\n\n(b) Comparison of the basic stacked STU model against LRU ablations in $\\mathrm{OSG}^{+}$23\n\n## 4 Stacked STU\n\nTo increase the representation capacity and to maintain the efficiency of prediction through linear dynamical systems, proposed models in the SSM literature take the form of stacking these sequence to sequence transforms into multiple layers. Non-linearities in the model can then be introduced by sandwiching them as layers lying in between these sequence to sequence transforms. In this paper we closely follow the stacking approach followed by $\\mathrm{OSG}^{+}$23], replacing the LRU layers appropriately by STU layers. A schematic for the resultant multi-layer model is displayed in Figure 4 a . In a nutshell, the input sequence is first embedded via a time-invariant embedding function followed by multiple repetitions of alternating STU layers and non-linearities (in particular we use GLU). Finally the resulting output is time-pooled followed by a final readout layer according to the task at hand. This composite model can now be trained in a standard fashion via back-propagation and other commonly used deep-learning optimization techniques. ### 4.1 Experiments on Long Range Arena [TDA ${ }^{+}$21]\n\nWe evaluate the stacked STU model on the Long Range Arena (LRA) benchmark [TDA ${ }^{+}$21]. This benchmark aims to assess the performance of sequence prediction models in long-context scenarios and consists of six tasks of various modalities, including text and images. The context length for the tasks ranges from 1 K to 16 K , and the tasks require capabilities such as hierarchical reasoning, matching and retrieval, and visual-spatial understanding. SSMs [GGR21] have shown significantly superior performance on most of the tasks compared to Transformer architectures. In particular for the hardest task in the suite, PathX (image classification with context length of 16 K ), no transformer model has been able to achieve accuracy beyond random guessing. We provide the evaluation of the stacked STU model on the two hardest tasks namely PathFinder and PathX in Table 4b\nWe compare our performance against the ablation carried out by $\\left[\\mathrm{OSG}^{+} 23\\right]$ who find that ring initialization, stable exponential parameterization and $\\gamma$-normalization are all crucial towards learning these tasks. In particular as reported by $\\left[\\mathrm{OSG}^{+}\\right.$23] all three of the above interventions were necessary to learn on PathX to any non-trivial accuracy. This is a result of the much larger context length of 16K employed by the PathX task. On the other hand we find that the the stacked STU (with the STU component exactly as represented by (4)) is sufficient to learn on both these tasks to relatively high accuracies. Notably we do not require any other normalizations or initialization techniques We initialize all the parameters of the STU i.e. M matrices to 0 . Details about our implementation as well as details about the experiments including hyperparameters can be found in the appendix (Section E. This result in particular confirms and highlights the theoretical stability afforded by the STU even under learning tasks involving large sequence lengths. In the appendix (Table 2 we provide the performance evalaution of the stacked STU on all tasks of the LRA benchmark. In the next section we highlight a simple technique towards significantly improving the achieved accuracy for the stacked STU model. |  | CIFAR | ListOps | Text | Retrieval | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| S4 $[\\overline{\\text { GGR21] }}]$ | 88.65 | 59.60 | 86.82 | 90.90 | 94.20 | 96.35 |\n| LRU $\\left[\\right.$ OSG $\\left.^{+23}\\right]$ | 89 | 60.2 | 89.4 | 89.9 | 95.1 | 94.2 |\n| AR-STU | $\\mathbf{9 1 .",
    "spectralssm-5": "3 4}$ | $\\mathbf{6 1 . 1 4}$ | $\\mathbf{9 0 . 4 7}$ | 90.52 | $\\mathbf{9 5 . 4 5}$ | 93.24 |\n\nTable 1: Comparison of the STU model against various proposed SSM models on the LRA benchmark. We report the median over 5 trials for our experiments. ## 5 Hybrid Temporal and Spectral Units\n\nA simple extension to the STU model (Equation (4)) is to parameterize the dependence of $y_{t}$ on $y_{t-2}$ with a parameter $M_{y}$, leading to the following prediction model\n\n$$\n\\hat{y}_{t}=\\underbrace{\\boldsymbol{M}^{\\boldsymbol{y}} \\hat{y}_{t-2}+\\sum_{i=1}^{3} M_{i}^{u} u_{t+1-i}}_{\\text {Auto-regressive Component }}+\\underbrace{\\sum_{k=1}^{K} M_{k}^{\\phi+} \\sigma_{k}^{1 / 4} U_{t-2, k}^{+}+\\sum_{k=1}^{K} M_{k}^{\\phi-} \\sigma_{k}^{1 / 4} U_{t-2, k}^{-}}_{\\text {Spectral Component }}\n$$\n\nSetting $M^{y}=I$ we recover the guarantees afforded by Theorem 3.1 and thus the above model is strictly more powerful. We find that the above change leads to significant improvements over the accuracy achieved by the simple STU model. We can further extend the auto-regression to depend on multiple previous $y$ as opposed to just $y_{t-2}$. Indeed as the following theorem shows adding sufficiently long auto-regression is powerful enough to capture any LDS. Theorem 5.1. Given an LDS parameterized by $A \\in \\mathbb{R}^{d \\times d}, B, C, D$, there exist coefficients $\\alpha_{1: d}$ and matrices $\\Gamma_{0: d}$ such that given any input sequence $u_{1: L}$, the output sequence $y_{1: L}$ generated by the action of the LDS on the input satisfies for all $t$\n\n$$\ny_{t}=\\sum_{i=1}^{d} \\alpha_{i} y_{t-i}+\\sum_{i=0}^{d} \\Gamma_{i} u_{t-i}\n$$\n\nThis is a well-known observation and we provide a proof in the appendix (SectionF). Motivated by the above theorem we propose a generalization of STU, which we call AR-STU, to add auto-regression over the previously produced outputs. In particular given a parameter $k_{y}$ we define AR-STU as\n\n$$\n\\hat{y}_{t}=\\underbrace{\\sum_{i=1}^{k_{y}} \\boldsymbol{M}_{\\boldsymbol{i}}^{\\boldsymbol{y}} \\hat{y}_{t-i}+\\sum_{i=1}^{3} M_{i}^{u} u_{t+1-i}}_{\\text {Auto-regressive Component }}+\\underbrace{\\sum_{k=1}^{K} M_{k}^{\\phi+} \\sigma_{k}^{1 / 4} U_{t-2, k}^{+}+\\sum_{k=1}^{K} M_{k}^{\\phi-} \\sigma_{k}^{1 / 4} U_{t-2, k}^{-}}_{\\text {Spectral Component }}\n$$\n\nIn Table 1, we evaluate the performance of AR-STU on Long Range Arena.",
    "spectralssm-6": "In our experiments we search over two values of $k_{y}=\\{2,32\\}$. For non-image tasks, ListOps, Text and Retrieval, we find that setting $k_{y}=2$ is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that $k_{y}=32$ led to significant performance gains. A performance ablation over this parameter can be found in the appendix (Table 2]. Overall we find that the STU model provides improvements over baselines such as S4 and LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the STU layers come with provable guarantees and thus performs well out of the box without the need for specific initializations, discretizations or normalizations. We initialize all parameters $M_{i}^{y}, M_{i}^{u}, M_{k}^{\\phi+}, M_{k}^{\\phi-}$ with 0 . We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E). ## 6 Conclusion\n\nInsprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for\nlearning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially small in real world tasks. Indeed more recent SSM models like [GD23, $\\mathrm{DSF}^{+}$24] work with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help. Spectral filtering has been extended in certain settings to asymmetric A [HLS ${ }^{+}$18] and a similar extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work. ## References\n\n[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120-1128. PMLR, 2016. [Ble89] Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526-1538, 1989. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BSF94] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994. [CVMG ${ }^{+}$14] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. $\\left[\\mathrm{DBK}^{+}\\right.$20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.",
    "spectralssm-7": "arXiv preprint arXiv:2010.11929, 2020. [DFS ${ }^{+}$22] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models.",
    "spectralssm-8": "arXiv preprint arXiv:2212.14052, 2022. $\\left[\\mathrm{DSF}^{+}\\right.$24] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George CristianMuraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [Elm90] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990. $\\left[\\mathrm{FEN}^{+}\\right.$23] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. arXiv preprint arXiv:2302.06646, 2023. [GD23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "spectralssm-9": "arXiv preprint arXiv:2312.00752, 2023. [GDE ${ }^{+}$20] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections.",
    "spectralssm-10": "In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. [GGB22] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [GGDR22] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. In International Conference on Machine Learning, pages 7616-7633. PMLR, 2022. [GGR21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [GJG ${ }^{+}$21] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers.",
    "spectralssm-11": "Advances in neural information processing systems, 34:572-585, 2021. [GLS ${ }^{+}$20] Udaya Ghai, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. No-regret prediction in marginally stable systems. In Conference on Learning Theory, pages 1714-1757. PMLR, 2020. [HLS ${ }^{+}$18] Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi Zhang. Spectral filtering for general linear dynamical systems. In Advances in Neural Information Processing Systems, pages 4634-4643, 2018. [Hop82] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982. [HS97] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. [HS22] Elad Hazan and Karan Singh. Introduction to online nonstochastic control. arXiv preprint arXiv:2211.09619, 2022. [HSZ17] Elad Hazan, Karan Singh, and Cyril Zhang. Learning linear dynamical systems via spectral filtering. In Advances in Neural Information Processing Systems, pages $6702-6712,2017$. [JEP ${ }^{+}$21] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold.",
    "spectralssm-12": "Nature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.",
    "spectralssm-13": "We provide further details about this study after this section. Spectral filtering. The technique of spectral filtering for learning linear dynamical systems was put forth in [HSZ17]. This work studies online prediction of the sequence of observations $y_{t}$, and the goal is to predict as well as the best symmetric LDS using past inputs and observations. Directly\nlearning the dynamics is a non-convex optimization problem, and spectral filtering is developed as an improper learning technique with an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in $\\mathrm{HLS}^{+} 18$. Convolutional Models for Sequence Modeling Exploiting the connnection between Linear dynamical systems and convolutions (as highlighted by [GGR21]) various convolutional models have been proposed for sequence modelling. [FEN ${ }^{+}$23] employ direct learning of convolutional kernels directly to sequence modelling but find that they underperform SSMs. They find the non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations to the kernels to match performance on the Long Range Arena benchmark. The proposed model still contains significantly large number of parameters growing with the sequence length. [ $\\left.\\mathrm{LCZ}^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. They achieve this via a specific form of the kernel derived by repeating and scaling the kernel in a dyadic fashion. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1] by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left\\lfloor\\mathrm{LCZ}^{+} 22\\right\\rfloor$ and $\\left[\\mathrm{FEN}^{+}\\right.$23]. ## A. 1 Ablations performed by OSG $^{+}$23]\n\nMotivated by the success of SSMs, $\\left[\\mathrm{OSG}^{+} 23\\right]$ revisit the RNN model (under the same deep stacked structure as SSMs) to investigate their efficiency. They begin from a simple linear RNN (a directly parameterized LDS) and add multiple components inspired from the SSM literature to ensure numerical stability and trainability of the model especially as the sequences grow larger. Overall they demonstrate that carefully designed parameterizations and initializations of LDS parameters as well as specifically designed normalizations are all necessary for model to learn consistently over the LRA dataset and in particularly over the 16 K context length task PathX. These interventions are driven by specific intuitions such as an inductive bias towards larger memory or controlling the loss blowup at initialization under long contexts but as such come with no theoretical guarantees towards alleviating the problem. We provide some quick details towards what these interventions are and refer the reader to $\\left[\\mathrm{OSG}^{+} 23\\right]$ to understand the motivations behind them and comparisons with similar ideas existing in previous SSM literature. The LRU model considered by $\\left[\\mathrm{OSG}^{+} 23\\right]$ is given by\n\n$$\ny_{k}=\\operatorname{diag}(\\lambda) y_{k-1}+\\gamma \\odot B u_{k}\n$$\n\nIn the above the learned parameters are $\\lambda$ and $B$ and note that $\\operatorname{diag}(\\lambda)$ corresponds to a diagonal $A$. $\\gamma$ is a specific normalization technique they develop to control the loss blowup under long-context detailed below. They perform the following interventions towards stable training\n\n- Stable Exponential Parameterization: They parameterize $\\lambda$ as\n\n$$\n\\lambda_{j}=\\underbrace{\\exp \\left(-\\exp \\left(\\nu_{j}^{\\log }\\right)\\right.}_{\\text {magnitude }}+i \\underbrace{\\exp \\left(\\theta_{j}^{\\log }\\right)}_{\\text {phase }})\n$$\n\nThe above is done to ensure a bound on the magnitude of eigenvalues of the effective A matrix as well as to ensure more resolution in the parameter space closer to the value of 1 . - Ring Initialization: They initialize the $\\lambda_{j}$ in the complex annulus [min_rad, max_rad]. This ensures that at initialization the magnitude of $\\lambda_{j}$ chosen randomly lies in $\\in$ [min_rad, max_rad] and the phase is chosen randomly. When not applying this intervention min_rad and max_rad are chosen to be 0,1 respectively. When applying this intervention these values are chosen to be closer to 1 , e.g. $0.9,0.999$ respectively. - $\\gamma$-Normalization: They set $\\gamma_{j}=\\sqrt{1-\\left|\\lambda_{j}\\right|^{2}}$\n- Restricting Phase at initialization: Instead of drawing a random phase at initialization the authors recommend selecting the initial phase from $[0, \\pi / 10]$. The authors claim that uniform phase inherently biases the network towards learning spurious features in the input sequence. $\\left[\\mathrm{OSG}^{+} 23\\right]$ provide the following ablation in the paper. In particular we see that all the above interventions are necessary to make the model get to non-trivial accuracy on PathX. On the contrary, as we show the STU model achieves comparable accuracy without requiring any specific initialization or normalization. | Model | Specification | sCIFAR | ListOps | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| LRU | Dense A | 72.2 | 50.4 | $\\boldsymbol{X}$ | $\\boldsymbol{X}$ |\n|  | $\\Lambda$ Exp.",
    "spectralssm-14": "Param. | 85.4 | 60.5 | 65.4 | $\\boldsymbol{X}$ |\n|  | $\\Lambda$ Stable Exp. Param. | 87.2 | 59.4 | 93.5 | $\\boldsymbol{X}$ |\n|  | + Ring Init. | 88.1 | 59.4 | 94.4 | $\\boldsymbol{X}$ |\n|  | $+\\gamma$-Norm.",
    "spectralssm-15": "+ Phase Init. | 89.0 | 60.2 | 95.1 | 94.2 |\n\n## B Computational complexity and comparison to other methods. Using the STU method to make a sequence of $L$ predictions, the features $U^{+}, U^{-} \\in \\mathbb{R}^{L \\times d_{\\text {in }} \\times K}$ can be computed in time $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\log (L)\\right)$ using the Discrete Fast Fourier Transform, where $K$ is the number of filters and $L$ is the context length. The linear prediction part (i.e. spectral component) takes $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$ time, and the autoregressive part can be implemented in total time $O\\left(L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$. Therefore the overall runtime is $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot\\left(\\log (L)+d_{\\text {out }}\\right)\\right) \\cdot{ }^{4}$\nFor comparison, consider LRU and transformers. The same computation carried out by LRU w. diagonal system matrices is dominated by the hidden dimension, i.e. $O\\left(L \\cdot d_{\\text {hidden }} \\cdot\\left(d_{\\text {in }}+d_{\\text {out }}\\right)\\right)$. Thus, the number of filters is replaced by $d_{\\text {hiden }}$, which is usually an order of magnitude larger, although STU has another $O(\\log L)$ overhead. A transformer model with full attention runs in time $O\\left(L^{2} d_{\\text {in }} d_{\\text {out }}\\right)$, which is significantly more costly than both LRU and STU. This is consistent with the motivation of SSM as more efficient models for sequences. ## C Proof of Theorem 3.1\n\nWe begin by observing that without loss of generality we can assume that $A$ is a real-diagonal matrix. This can be ensured by performing a spectral decomposition of $A=U \\Sigma U^{\\top}$ and absorbing the $U, U^{\\top}$ by redefining the system. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any $\\alpha \\in \\mathbb{R}, \\mu(\\alpha) \\in \\mathbb{R}^{L}$, with $\\mu(\\alpha)(i)=(\\alpha-1) \\alpha^{i-1}$. Further define the Hankel matrix $H$ as\n\n$$\nZ \\triangleq \\int_{0}^{1} \\mu(\\alpha) \\mu(\\alpha)^{\\top} d \\alpha\n$$\n\nAs the following lemma shows the Hankel matrix $Z$ above is the same Hankel matrix defined in the definition of STU (3). Lemma C.1. $Z$ is a Hankel matrix with entries given as\n\n$$\nZ(i, j)=\\frac{2}{(i+j)^{3}-(i+j)}\n$$\n\n[^3]Lemma C.2. We have that the following statements hold regarding $\\mu(\\alpha)$ for any $\\alpha \\in[0,1]$,\n\n- $|\\mu(\\alpha)|^{2} \\leq 1$\n- For any $\\alpha \\in[0,1]$ and any unit vector $v$ we have that\n\n$$\n\\left(\\mu(\\alpha)^{\\top} v\\right)^{2} \\leq 12\\left(v^{\\top} H v\\right)\n$$\n\nLemma C.3. For any $\\alpha \\in[0,1]$, let $\\tilde{\\mu}(\\alpha)$ be the projection of $\\mu(\\alpha)$ on the subspace spanned by top $k$ eigenvectors of $Z$, then we have that\n\n$$\n\\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2} \\leq 12 \\sum_{i=k+1}^{L} \\sigma_{i}\n$$\n\nFinally the following lemma from HSZ17] shows that the spectrum of the matrix $Z$ decays exponentially.",
    "spectralssm-16": "Lemma C. 4 (Lemma E. 3 HSZ17]). Let $\\sigma_{j}$ be the top $j^{\\text {th }}$ eigenvalue of $Z$. Then we have that\n\n$$\n\\sigma_{j} \\leq \\Gamma c^{-j / \\log (L)}\n$$\n\nwhere $c=e^{\\pi^{2} / 4} \\sim 11.79$ and $\\Gamma=235200$ is an absolute constant. We now move towards proving Theorem 3.1. Consider the following calculation for the LDS sequence $y_{t}^{\\mathrm{LDS}}$\n\n$$\ny_{t}^{\\mathrm{LDS}}=\\sum_{i=0}^{T} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nand therefore we have that\n\n$$\ny_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}}=(C B+D) u_{t}+C A B u_{t-1}-D u_{t-2}+\\underbrace{\\sum_{i=0}^{T} C\\left(A^{i+2}-A^{i}\\right) B u_{t-2-i}}_{\\text {Term of Interest }}\n$$\n\nFor any $t_{1} \\geq t_{2}$ we define the matrix $\\bar{U}_{\\left\\{t_{1}: t_{2}\\right\\}} \\in \\mathbb{R}^{d_{\\text {out }} \\times t_{1}-t_{2}+1}$ whose $i^{t h}$ column is the input vector $u_{t_{1}-i+1}$.",
    "spectralssm-17": "We allow $t_{2}$ to be negative and by convention assume $u_{t}=0$ for any $t \\leq 0$. Denote the diagonal entries of $A$ by $\\left\\{\\alpha_{l}\\right\\}_{l=1}^{d_{h}}$, i.e. $\\alpha_{l}=A(l, l)$. Further let $b_{l}, c_{l}$ be the $l$-th column for the matrices $B, C$ respectively. The term of interest above can then be written as\n\n$$\n\\begin{aligned}\n& \\sum_{i=0}^{L} C\\left(A^{i+2}-A^{i}\\right) B u_{t-2-i} \\\\\n& =\\sum_{l=1}^{d_{h}}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\alpha_{l}^{i+2}-\\alpha_{l}^{i}\\right) u_{t-2-i}\\right) \\\\\n& =\\sum_{l: \\alpha_{l} \\geq 0}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\alpha_{l}^{2}-1\\right) \\alpha_{l}^{i} u_{t-2-i}\\right)+\\sum_{l: \\alpha_{l}<0}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\alpha_{l}^{2}-1\\right) \\alpha_{l}^{i} u_{t-2-i}\\right) \\\\\n& =\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\alpha_{l}-1\\right) \\alpha_{l}^{i} u_{t-2-i}\\right)+\\sum_{l: \\alpha_{l}<0}\\left(1+\\left|\\alpha_{l}\\right|\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\left|\\alpha_{l}\\right|-1\\right)\\left|\\alpha_{l}\\right|^{i}(-1)^{i} u_{t-2-i}\\right) \\\\\n& =\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\mu(\\alpha)\\right)+\\sum_{l: \\alpha_{l}<0}\\left(\\left|\\alpha_{l}\\right|+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\odot \\mathbf{1}^{ \\pm}\\right) \\mu\\left(\\left|\\alpha_{l}\\right|\\right)\n\\end{aligned}\n$$\n\nwhere $1^{ \\pm} \\in \\mathbb{R}^{d_{\\text {out }} \\times L}$ is defined as the matrix whose every row is the alternating sign vector $[1,-1,1,-1 \\ldots])$ and $\\odot$ is Hadamard product (i.e. entry-wise multiplication). $$\n\\begin{aligned}\n& y_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}}=(C B+D) u_{t}+C A B u_{t-1}-D u_{t-2}+\\underbrace{\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\mu(\\alpha)\\right)}_{\\text {PositivePart }} \\\\\n& +\\underbrace{\\sum_{l: \\alpha_{l}<0}\\left(\\left|\\alpha_{l}\\right|+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\odot \\mathbf{1}^{ \\pm}\\right) \\mu\\left(\\left|\\alpha_{l}\\right|\\right)}_{\\text {NegativePart }}\n\\end{aligned}\n$$\n\nRecall that we defined the sequence $\\left\\{\\sigma_{k}, \\phi_{k}\\right\\}_{k=1}^{L}$ to be the eigenvalue and eigenvector pairs for the Hankel matrix $Z$. For any $\\alpha$ we define the projection of $\\mu(\\alpha)$ on the top $k$ eigenvectors as $\\tilde{\\mu}(\\alpha)$, i.e. $\\tilde{\\mu}(\\alpha)=\\sum_{k=1}^{K}\\left(\\mu\\left(\\alpha_{l}\\right)^{\\top} \\phi_{k}\\right) \\phi_{k}$. Further define STU parameters as follows\n\n$$\n\\begin{aligned}\nM_{1}^{u} & =C B+D, M_{2}^{u}=C A B, M_{3}^{u}=-D \\\\\nM_{k}^{\\phi+} & =\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(\\mu\\left(\\alpha_{l}\\right)^{\\top} \\phi_{k}\\right) \\sigma_{k}^{-1 / 4}\\left(c_{l} \\otimes b_{l}\\right) \\\\\nM_{k}^{\\phi-} & =\\sum_{l: \\alpha_{l}<0}\\left(\\left|\\alpha_{l}\\right|+1\\right)\\left(\\mu\\left(\\left|\\alpha_{l}\\right|\\right)^{\\top} \\phi_{k}\\right) \\sigma_{k}^{-1 / 4}\\left(c_{l} \\otimes b_{l}\\right)\n\\end{aligned}\n$$\n\nBy the definition of STU prediction (4) we have that,\n\n$$\n\\begin{aligned}\ny_{t}^{\\mathrm{STU}} & =y_{t-2}^{\\mathrm{STU}}+\\sum_{i=1}^{3} M_{i}^{u} u_{t+1-i}+\\sum_{k=1}^{K} M_{k}^{\\phi+} \\sigma_{k}^{1 / 4}\\left(\\sum_{i=0}^{t-1} u_{t-i} \\cdot \\phi_{k}(i)\\right)+\\sum_{k=1}^{K} M_{k}^{\\phi-} \\sigma_{k}^{1 / 4}\\left(\\sum_{i=0}^{t-1} u_{t-i} \\cdot(-1)^{i} \\cdot \\phi_{k}(i)\\right) \\\\\n& =y_{t-2}^{\\mathrm{STU}}+\\sum_{i=1}^{3} M_{i}^{u} u_{t+1-i}+\\sum_{k=1}^{K} M_{k}^{\\phi+} \\sigma_{k}^{1 / 4}\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\phi_{k}\\right)+\\sum_{k=1}^{K} M_{k}^{\\phi-} \\sigma_{k}^{1 / 4}\\left(\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\odot 1^{ \\pm}\\right) \\phi_{k}\\right)\n\\end{aligned}\n$$\n\nUsing the parameters specified in 8 in the above we have that,\n\n$$\n\\begin{aligned}\ny_{t}^{\\mathrm{STU}}-y_{t-2}^{\\mathrm{STU}} & =(C B+D) u_{t}+C A B u_{t-1}-D u_{t-2}+\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}}\\right)(\\underbrace{\\sum_{k=1}^{K}\\left(\\mu\\left(\\alpha_{l}\\right)^{\\top} \\phi_{k}\\right) \\phi_{k}}_{=\\tilde{\\mu}(\\alpha)}) \\\\\n& +\\sum_{l: \\alpha_{l}<0}\\left(\\left|\\alpha_{l}\\right|+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\odot \\mathbf{1}^{ \\pm}\\right)(\\underbrace{\\sum_{k=1}^{K}\\left(\\mu\\left(\\left|\\alpha_{l}\\right|\\right)^{\\top} \\phi_{k}\\right) \\phi_{k}}_{=\\tilde{\\mu}\\left(\\left|\\alpha_{l}\\right|\\right)})\n\\end{aligned}\n$$\n\nCombining the above display with (7), we get that\n\n$$\n\\begin{aligned}\ny_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}=y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}} & +\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}}\\right)(\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)) \\\\\n& +\\sum_{l: \\alpha_{l}<0}\\left(\\left|\\alpha_{l}\\right|+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\odot \\mathbf{1}^{ \\pm}\\right)\\left(\\mu\\left(\\left|\\alpha_{l}\\right|\\right)-\\tilde{\\mu}\\left(\\left|\\alpha_{l}\\right|\\right)\\right)\n\\end{aligned}\n$$\n\nLet $\\|B\\|_{\\text {col }}=\\max _{l}\\left\\|b_{l}\\right\\|,\\|C\\|_{\\text {col }}=\\max _{l}\\left\\|c_{l}\\right\\|$ be the maximum column norms of $B$ and $C$ respectively.",
    "spectralssm-18": "Therefore we have that for all $l$, the spectral norm of the matrix $c_{l} \\otimes b_{l}$ is bounded as $\\|B\\|_{\\text {col }} \\cdot\\|C\\|_{\\text {col }}$. Further note that every column of $\\bar{U}$ is an input $u_{t}$ for some time $t$. Further we have\nassumed that $\\left\\|u_{t}\\right\\| \\leq a$ for all $t$. Therefore we have that the frobenius norm (and thus spectral norm) of $U_{t-2: t-1-L}$ is bounded as\n\n$$\n\\left\\|\\bar{U}_{t-2: t-1-L}\\right\\| \\leq\\left\\|\\bar{U}_{t-2: t-1-L}\\right\\|_{F} \\leq \\sqrt{L} \\cdot a\n$$\n\nPutting the above together we get that for all $l$,\n$\\left\\|\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{t-2: t-1-L}\\right)\\right\\| \\leq\\left|\\left(\\alpha_{l}+1\\right)\\right|\\left\\|\\left(c_{l} \\otimes b_{l}\\right)\\right\\|\\left\\|\\left(\\bar{U}_{t-2: t-1-L}\\right)\\right\\| \\leq 2 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot \\sqrt{L} \\cdot a$. Therefore we have (using C. 3 that,\n\n$$\n\\begin{aligned}\n& \\left\\|\\sum_{l: \\alpha_{l} \\geq 0}\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{t-2: t-1-L}\\right)(\\mu(\\alpha)-\\tilde{\\mu}(\\alpha))\\right\\| \\\\\n& \\leq \\sum_{l: \\alpha_{l} \\geq 0}\\left\\|\\left(\\alpha_{l}+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{t-2: t-1-L}\\right)\\right\\| \\cdot\\|(\\mu(\\alpha)-\\tilde{\\mu}(\\alpha))\\| \\\\\n& \\leq 5 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{1.5} \\cdot a \\cdot \\sqrt{\\sum_{i=K+1}^{L} \\sigma_{i}}\n\\end{aligned}\n$$\n\nSimilarly we have that\n\n$$\n\\left\\|\\sum_{l: \\alpha_{l}<0}\\left(\\left|\\alpha_{l}\\right|+1\\right)\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\odot \\mathbf{1}^{ \\pm}\\right)\\left(\\mu\\left(\\left|\\alpha_{l}\\right|\\right)-\\tilde{\\mu}\\left(\\left|\\alpha_{l}\\right|\\right)\\right)\\right\\| \\leq 5 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{1 \\cdot 5} \\cdot a \\cdot \\sqrt{\\sum_{i=K+1}^{L} \\sigma_{i}}\n$$\n\nPlugging the above into (9), we get that\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\right\\| \\leq\\left\\|y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}}\\right\\|+10 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{1.5} \\cdot a \\cdot \\sqrt{\\sum_{i=K+1}^{L} \\sigma_{i}}\n$$\n\nApplying the above equation recursively and Lemma C. 4 we get that for any $K \\geq \\log (L)$,\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\right\\| \\leq 5 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{2.5} \\cdot a \\cdot \\sqrt{\\sum_{i=K+1}^{L} \\sigma_{i}} \\leq c \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{3} \\cdot a \\cdot e^{\\left(-\\frac{\\pi^{2}}{4} \\cdot \\frac{K}{\\log (L)}\\right)}\n$$\n\nwhere $c=5 \\Gamma \\leq 2 \\times 10^{6}$ is an absolute constant.",
    "spectralssm-19": "This finishes the proof of the theorem. ## C. 1 Proofs of Lemmas\n\nProof of Lemma C.1. The lemma follows from the following simple calculations. $$\n\\begin{aligned}\nZ(i, j)=\\int_{0}^{1}(\\alpha-1)^{2} \\alpha^{i+j-2} d \\alpha & =\\int_{0}^{1}\\left(\\alpha^{i+j}+\\alpha^{i+j-2}-2 \\alpha^{i+j-1}\\right) d \\alpha \\\\\n& =\\frac{1}{(i+j+1)}+\\frac{1}{(i+j-1)}-\\frac{2}{(i+j)} \\\\\n& =\\frac{2}{(i+j)^{3}-(i+j)}\n\\end{aligned}\n$$\n\nLemma C. 3 is immediate from the second part of Lemma C. 2 . We show Lemma C. 2 below. Proof of Lemma C.2. By definition $\\mu(\\alpha)=0$ for $\\alpha \\in\\{0,1\\}$. Otherwise we have that for all $\\alpha \\in(0,1)$,\n\n$$\n|\\mu(\\alpha)|^{2}=\\sum_{i=1}^{T}(\\alpha-1)^{2} \\alpha^{2 i-2} \\leq \\frac{(\\alpha-1)^{2}}{\\left(1-\\alpha^{2}\\right)} \\leq \\frac{1-\\alpha}{1+\\alpha} \\leq 1-\\alpha\n$$\n\nTo prove the second part we consider drawing $\\alpha$ from the uniform distribution between $[0,1]$. We get that\n\n$$\nE\\left[\\left(\\mu(\\alpha)^{\\top} v\\right)^{2}\\right]=v^{\\top} Z v\n$$\n\nWe now show that the worst case value is not significantly larger than the expectation. To this end we consider the function $f(\\alpha)=\\left(\\mu(\\alpha)^{\\top} v\\right)^{2}$ and we show that this is a-Lipschitz function. To this end consider the following,\n\n$$\n\\begin{aligned}\n\\left\\|\\frac{\\partial \\mu(\\alpha)}{\\partial \\alpha}\\right\\|_{2}^{2} & =\\sum_{i=0}^{T-1}\\left\\{\\left|\\frac{\\partial}{\\partial \\alpha}(1-\\alpha) \\alpha^{i}\\right|^{2}\\right\\} \\\\\n& =\\sum_{i=0}^{T-1}\\left((1-\\alpha) i \\alpha^{i-1}-\\alpha^{i}\\right)^{2} \\\\\n& \\leq 2(1-\\alpha)^{2} \\sum_{i=1}^{T-1} i^{2} \\alpha^{2(i-1)}+2 \\sum_{i=0}^{T-1} \\alpha^{2 i} \\\\\n& \\leq 2(1-\\alpha)^{2}\\left(\\frac{1}{\\left(1-\\alpha^{2}\\right)^{2}}+\\frac{2 \\alpha^{2}}{\\left(1-\\alpha^{2}\\right)^{3}}\\right)+\\frac{2}{1-\\alpha^{2}} \\quad \\sum_{i=1}^{\\infty} i^{2} \\beta^{i-1}=\\frac{1}{(1-\\beta)^{2}}+\\frac{2 \\beta}{(1-\\beta)^{3}} \\\\\n& =\\frac{2}{(1+\\alpha)^{2}}+\\frac{4 \\alpha^{2}}{\\left(1-\\alpha^{2}\\right)(1+\\alpha)^{2}}+\\frac{2}{1-\\alpha^{2}}\n\\end{aligned}\n$$\n\nTherefore we have that for all $\\alpha \\in[0,1]$,\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(\\alpha)}{\\partial \\alpha} & =2\\left(\\mu(\\alpha)^{\\top} v\\right)\\left(\\frac{\\partial \\mu(\\alpha)^{\\top}}{\\partial \\alpha} v\\right) \\leq 2\\|\\mu(\\alpha)\\|\\|v\\|^{2}\\left\\|\\frac{\\partial \\mu(\\alpha)}{\\partial \\alpha}\\right\\| \\\\\n& \\leq 2 \\sqrt{(1-\\alpha) *\\left(\\frac{2}{(1+\\alpha)^{2}}+\\frac{4 \\alpha^{2}}{\\left(1-\\alpha^{2}\\right)(1+\\alpha)^{2}}+\\frac{2}{1-\\alpha^{2}}\\right)} \\\\\n& \\leq 2 \\sqrt{\\left(\\frac{2(1-\\alpha)}{(1+\\alpha)^{2}}+\\frac{4 \\alpha^{2}}{(1+\\alpha)^{3}}+\\frac{2}{1+\\alpha}\\right)} \\leq 6\n\\end{aligned}\n$$\n\nNow for the positive function $f(\\alpha)$ which is 6 -Lipschitz on $[0,1]$ let the maximum value be $R$. It can be seen the lowest expected value of $f(\\alpha)$ over the uniform distribution over $[0,1]$, one can achieve is $R^{2} / 2 * 6$ and therefore we have that\n\n$$\nR^{2} / 12 \\leq v^{\\top} Z v \\Rightarrow R \\leq \\sqrt{12 v^{\\top} H v}\n$$\n\nwhich finishes the proof. ## D Alternative Representation for capturing negative eigenvalues\n\nIn this section we setup an alternative version of STU wherein a different Hankel matrix is used but one can get a similar result. As before a single layer of STU (depicted in figure 2) is parameterized by a number $K$, denoting the number of eigenfactors and matrices $M_{1}^{\\phi} \\ldots M_{K}^{\\phi} \\in \\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$, and $M_{1}^{u}, M_{2}^{u}, M_{3}^{u} \\in \\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The matrices form the params of the layer. We use a different Hankel matrix $Z_{L} \\in \\mathbb{R}^{L \\times L}$ whose entries are given by\n\n$$\nZ_{L}[i, j] \\triangleq\\left((-1)^{i+j-2}+1\\right) \\cdot \\frac{8}{(i+j+3)(i+j-1)(i+j+1)}\n$$\n\nand let $\\left\\{\\left(\\sigma_{j} \\in \\mathbb{R}, \\phi_{j} \\in \\mathbb{R}^{T}\\right)\\right\\}_{j=1}^{T}$ be the eigenvalue-eigenvector pairs of $Z_{L}$ ordered to satisfy $\\sigma_{1} \\geq \\sigma_{2} \\ldots \\sigma_{d}$\nGiven an input sequence $\\left\\{u_{1} \\ldots u_{L}\\right\\} \\in \\mathbb{R}^{d_{\\text {in }}}$, as before we first featurize the input sequence by projecting the input sequence till time $t$ on fixed filters $\\phi_{k}$. The main difference is that we do not need\nto create a negative featurization now. We define\n\n$$\nU_{t, k}=\\sum_{i=0}^{t-1} u_{t-i} \\cdot \\phi_{k}(i)\n$$\n\nNote that for every $k$, the sequence of features $X_{1: T, k}$ can be computed efficiently via convolution. The output sequence $\\left\\{y_{1} \\cdots y_{T}\\right\\}$ is then given by\n\n$$\n\\hat{y}_{t}=\\underbrace{\\hat{y}_{t-2}+\\sum_{i=1}^{3} M_{i}^{u} u_{t+1-i}}_{\\text {Auto-regressive Component }}+\\underbrace{\\sum_{k=1}^{K} M_{k}^{\\phi} \\sigma_{k}^{1 / 4} X_{t-2, k}}_{\\text {Spectral Component }}\n$$\n\nWe prove the following representation theorem which shows that the above class approximately contains any marginally-stable LDS with symmetric $A$. Theorem D.1. Given any $A, B, C, D$ such that $A$ is a symmetric matrix with $\\|A\\| \\leq 1$ and given any numbers $K \\in \\mathbb{I}^{+}, a \\in \\mathbb{R}^{+}$, there exists matrices $M_{1}^{u}, M_{2}^{u}, M_{3}^{u}, M_{1}^{\\phi} \\ldots M_{K}^{\\phi} \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\mathrm{in}}}$ for all $L$ and all sequences $u_{1: L}$ satisfying $\\left\\|u_{t}\\right\\| \\leq$ for all $t \\in[L]$ the following holds. Let $y_{1: L}^{\\mathrm{LDS}}$ be the sequence generated by execution of the LDS given by $A, B, C, D$ (via (1)) and $y_{1: L}^{\\mathrm{SF}}$ be the sequence generated by Spectral Filtering (via (11) using the matrices $M_{1}^{u}, M_{2}^{u}, M_{3}^{u}, M_{1}^{\\phi+} \\ldots M_{K}^{\\phi+}, M_{1}^{\\phi-} \\ldots M_{K}^{\\phi-}$. Then for all $t \\in[T]$, we have that\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{SF}}\\right\\|^{2} \\leq c \\cdot\\|B\\|_{\\text {col }} \\cdot\\|C\\|_{\\text {col }} \\cdot L^{3} \\cdot a \\cdot e^{-\\left(\\frac{\\pi^{2}}{4} \\cdot \\frac{K}{\\log (L)}\\right)}\n$$\n\nwhere $c \\leq 10^{6}$ is a universal constant and $\\|B\\|_{\\text {col }},\\|C\\|_{\\text {col }}$ are the maximum column norm of the matrices $B$ and $C$ respectively.",
    "spectralssm-20": "In the following we prove the above theorem. ## D. 1 Proof of Theorem D. 1\n\nWithout loss of generality we assume that $A$ is a real-diagonal matrix. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any $\\alpha$, $\\mu(\\alpha) \\in \\mathbb{R}^{T}$, with $\\mu(\\alpha)(i)=\\left(\\alpha^{2}-1\\right) \\alpha^{i-1}$. Further define the Hankel matrix $H$ as\n\n$$\nZ \\triangleq \\int_{-1}^{1} \\mu(\\alpha) \\mu(\\alpha)^{\\top} d \\alpha\n$$\n\nAs the following lemma shows the Hankel matrix $Z$ above is the same Hankel matrix $Z_{L}$ defined in the definition of STU 10 . Lemma D.2. $Z$ is a Hankel matrix with entries given as\n\n$$\nZ(i, j)=\\left((-1)^{i+j-2}+1\\right) \\cdot \\frac{8}{(i+j+3)(i+j-1)(i+j+1)}\n$$\n\nProof. Consider the following simple computations\n\n$$\n\\begin{aligned}\nH(i, j) & =\\int_{-1}^{1}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{i+j-2} d \\alpha \\\\\n& =\\int_{-1}^{0}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{i+j-2} d \\alpha+\\int_{0}^{1}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{i+j-2} d \\alpha \\\\\n& =\\int_{-1}^{0}\\left(|\\alpha|^{2}-1\\right)^{2}(-1)^{i+j-2}|\\alpha|^{i+j-2} d \\alpha+\\int_{0}^{1}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{i+j-2} d \\alpha \\\\\n& =\\int_{0}^{1}\\left(\\alpha^{2}-1\\right)^{2}(-1)^{i+j-2} \\alpha^{i+j-2} d \\alpha+\\int_{0}^{1}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{i+j-2} d \\alpha \\\\\n& =\\left((-1)^{i+j-2}+1\\right) \\int_{0}^{1}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{i+j-2} d \\alpha \\\\\n& =\\left((-1)^{i+j-2}+1\\right) \\cdot \\frac{8}{(i+j+3)(i+j-1)(i+j+1)}\n\\end{aligned}\n$$\n\nLemma D.3. We have that the following statements hold regarding $\\mu(\\alpha)$ for any $\\alpha \\in[-1,1]$,\n\n- $|\\mu(\\alpha)|^{2} \\leq 1$\n- For any $\\alpha \\in[-1,1]$ and any unit vector $v$ we have that\n\n$$\n\\left(\\mu(\\alpha)^{\\top} v\\right)^{2} \\leq 6\\left(v^{\\top} Z v\\right)\n$$\n\nProof. By definition $\\mu(\\alpha)=0$ for $\\alpha \\in\\{-1,1\\}$. Otherwise we have that for all $\\alpha \\in(-1,1)$,\n\n$$\n|\\mu(\\alpha)|^{2}=\\sum_{i=1}^{T}\\left(\\alpha^{2}-1\\right)^{2} \\alpha^{2 i-2} \\leq \\frac{\\left(\\alpha^{2}-1\\right)^{2}}{\\left(1-\\alpha^{2}\\right)}=1-\\alpha^{2} \\leq 1\n$$\n\nTo prove the second part we consider drawing $\\alpha$ from the uniform distribution between $[-1,1]$. We get that\n\n$$\nE\\left[\\left(\\mu(\\alpha)^{\\top} v\\right)^{2}\\right]=\\frac{v^{\\top} Z v}{2}\n$$\n\nWe now show that the worst case value is not significantly larger than the expectation. To this end we consider the function $f(\\alpha)=\\left(\\mu(\\alpha)^{\\top} v\\right)^{2}$ and we show that this is a 6-Lipschitz function. To this end consider the following,\n\n$$\n\\begin{aligned}\n&\\left\\|\\frac{\\partial \\mu(\\alpha)}{\\partial \\alpha}\\right\\|_{2}^{2}=\\sum_{i=0}^{T-1}\\left\\{\\left|\\frac{\\partial}{\\partial \\alpha}\\left(1-\\alpha^{2}\\right) \\alpha^{i}\\right|^{2}\\right\\} \\\\\n&=\\sum_{i=0}^{T-1}\\left(\\left(1-\\alpha^{2}\\right) i \\alpha^{i-1}-2 \\alpha^{i+1}\\right)^{2} \\\\\n& \\leq 2\\left(1-\\alpha^{2}\\right)^{2} \\sum_{i=1}^{T-1} i^{2} \\alpha^{2(i-1)}+4 \\sum_{i=0}^{T-1} \\alpha^{2 i+2} \\\\\n& \\leq 2\\left(1-\\alpha^{2}\\right)^{2}\\left(\\frac{1}{\\left(1-\\alpha^{2}\\right)^{2}}+\\frac{2 \\alpha^{2}}{\\left(1-\\alpha^{2}\\right)^{3}}\\right)+\\frac{4 \\alpha^{2}}{1-\\alpha^{2}} \\quad(a+b)^{2} \\leq 2\\left(a^{2}+b^{2}\\right) \\\\\n& \\sum_{i=1}^{2} \\beta^{i-1}=\\frac{1}{(1-\\beta)^{2}}+\\frac{2 \\beta}{(1-\\beta)^{3}} \\\\\n&=2+\\frac{8 \\alpha^{2}}{\\left(1-\\alpha^{2}\\right)}\n\\end{aligned}\n$$\n\nTherefore we have that for all $\\alpha \\in[-1,1]$,\n\n$$\n\\begin{aligned}\n\\frac{\\partial f(\\alpha)}{\\partial \\alpha} & =2\\left(\\mu(\\alpha)^{\\top} v\\right)\\left(\\frac{\\partial \\mu(\\alpha)^{\\top}}{\\partial \\alpha} v\\right) \\leq 2\\|\\mu(\\alpha)\\|\\|v\\|^{2}\\left\\|\\frac{\\partial \\mu(\\alpha)}{\\partial \\alpha}\\right\\| \\\\\n& \\leq 2 \\sqrt{\\left(1-\\alpha^{2}\\right) *\\left(2+\\frac{8 \\alpha^{2}}{\\left(1-\\alpha^{2}\\right)}\\right)} \\\\\n& \\leq 2 \\sqrt{2+6 \\alpha^{2}} \\leq 6\n\\end{aligned}\n$$\n\nNow for the positive function $f(\\alpha)$ which is 6 -Lipschitz on $[-1,1]$ let the maximum value be $R$. It can be seen the lowest expected value of $f(\\alpha)$ over the uniform distribution over $[0,1]$, one can achieve is $R^{2} / 2 * 6$ and therefore we have that\n\n$$\nR^{2} / 12 \\leq \\frac{v^{\\top} Z v}{2} \\Rightarrow R \\leq \\sqrt{6 v^{\\top} Z v}\n$$\n\nwhich finishes the proof. A direct consequence of the above lemma is the following. Lemma D.4. For any $\\alpha \\in[0,1]$, let $\\tilde{\\mu}(\\alpha)$ be the projection of $\\mu(\\alpha)$ on the subspace spanned by top $k$ eigenvectors of $Z$, then we have that\n\n$$\n\\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2} \\leq 6 \\sum_{i=k+1}^{L} \\sigma_{i}\n$$\n\nFinally the following lemma with a proof similar to C. 3 shows that the spectrum of the matrix $Z$ decays exponentially. Lemma D.5. Let $\\sigma_{j}$ be the top $j^{\\text {th }}$ eigenvalue of $Z$. Then we have that\n\n$$\n\\sigma_{j} \\leq \\Gamma c^{-j / \\log (L)}\n$$\n\nwhere $c=e^{\\pi^{2} / 4} \\sim 11.79$ and $\\Gamma=235200$ is an absolute constant. We now move towards proving Theorem D.1. Consider the following calculation for the LDS sequence $y_{t}^{\\mathrm{LDS}}$\n\n$$\ny_{t}^{\\mathrm{LDS}}=\\sum_{i=0}^{T} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nand therefore we have that\n\n$$\ny_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}}=(C B+D) u_{t}+C A B u_{t-1}-D u_{t-2}+\\underbrace{\\sum_{i=0}^{T} C\\left(A^{i+2}-A^{i}\\right) B u_{t-2-i}}_{\\text {Term of Interest }}\n$$\n\nFor any $t_{1} \\geq t_{2}$ we define the matrix $\\bar{U}_{t_{1}: t_{2}} \\in \\mathbb{R}^{d_{\\text {out }} \\times t_{1}-t_{2}+1}$ whose $i^{\\text {th }}$ column is the input vector $u_{t_{1}-i+1}$.",
    "spectralssm-21": "We allow $t_{2}$ to be negative and by convention assume $u_{t}=0$ for any $t \\leq 0$.",
    "spectralssm-22": "Denote the diagonal entries of $A$ by $\\left\\{\\alpha_{l}\\right\\}_{l=1}^{d_{h}}$, i.e. $\\alpha_{l}=A(l, l)$. The term of interest above can then be written as\n\n$$\n\\begin{aligned}\n\\sum_{i=0}^{L} C\\left(A^{i+2}-A^{i}\\right) B u_{t-2-i} & =\\sum_{l=1}^{d_{h}}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\alpha_{l}^{i+2}-\\alpha_{l}^{i}\\right) u_{t-2-i}\\right) \\\\\n& =\\sum_{l=1}^{d_{h}}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\sum_{i=0}^{L}\\left(\\alpha_{l}^{2}-1\\right) \\alpha_{l}^{i} u_{t-2-i}\\right) \\\\\n& =\\sum_{l=1}^{d_{h}}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\mu(\\alpha)\\right)\n\\end{aligned}\n$$\n\nTherefore we get that\n\n$$\ny_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}}=(C B+D) u_{t}+C A B u_{t-1}-D u_{t-2}+\\sum_{l=1}^{d_{h}}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}} \\mu(\\alpha)\\right)\n$$\n\nRecall that we defined the sequence $\\left\\{\\sigma_{k}, \\phi_{k}\\right\\}_{k=1}^{L}$ to be the eigenvalue and eigenvector pairs for the Hankel matrix $Z$. For any $\\alpha$ we define the projection of $\\mu(\\alpha)$ on the top $k$ eigenvectors as $\\tilde{\\mu}(\\alpha)$, i.e. $\\tilde{\\mu}(\\alpha)=\\sum_{k=1}^{K}\\left(\\mu\\left(\\alpha_{l}\\right)^{\\top} \\phi_{k}\\right) \\phi_{k}$. Further define STU parameters as follows\n\n$$\n\\begin{gathered}\nM_{1}^{u}=C B+D, M_{2}^{u}=C A B, M_{3}^{u}=-D \\\\\nM_{k}^{\\phi}=\\sum_{l}\\left(\\mu\\left(\\alpha_{l}\\right)^{\\top} \\phi_{k}\\right) \\sigma_{k}^{-1 / 4}\\left(c_{l} \\otimes b_{l}\\right)\n\\end{gathered}\n$$\n\nThe definition of STU prediction (using the above parameters) implies that the predicted sequence satisfies\n\n$$\ny_{t}^{\\mathrm{STU}}-y_{t-2}^{\\mathrm{STU}}=(C B+D) u_{t}+C A B u_{t-1}-D u_{t-2}+\\sum_{l}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}}\\right)(\\underbrace{\\sum_{k=1}^{K}\\left(\\mu\\left(\\alpha_{l}\\right)^{\\top} \\phi_{k}\\right) \\phi_{k}}_{=\\tilde{\\mu}(\\alpha)})\n$$\n\nCombining the above displays we get that\n\n$$\ny_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}=y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}}+\\sum_{l}\\left(c_{l} \\otimes b_{l}\\right)\\left(\\bar{U}_{\\{t-2: t-1-L\\}}\\right)(\\mu(\\alpha)-\\tilde{\\mu}(\\alpha))\n$$\n\nUsing a similar derivation as in the proof of Theorem 3.1 we get that\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\right\\| \\leq\\left\\|y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}}\\right\\|+10 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{1.5} \\cdot a \\cdot \\sqrt{\\sum_{i=K+1}^{L} \\sigma_{i}}\n$$\n\nApplying the above equation recursively and Lemma D.5 we get that for any $K \\geq \\log (L)$\n\n$$\n\\left\\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\right\\| \\leq 5 \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{2.5} \\cdot a \\cdot \\sqrt{\\sum_{i=K+1}^{L} \\sigma_{i}} \\leq c \\cdot\\|B\\|_{\\mathrm{col}} \\cdot\\|C\\|_{\\mathrm{col}} \\cdot L^{3} \\cdot a \\cdot e^{\\left(-\\frac{\\pi^{2}}{4} \\cdot \\frac{K}{\\log (L)}\\right)}\n$$\n\nwhere $c=2.5 \\times \\Gamma \\leq 10^{6}$ is an absolute constant.",
    "spectralssm-23": "This finishes the proof of the theorem. ## E Experiment Details\n\n## E. 1 Synthetic Experiments with a marginally-stable LDS\n\nThe random system we generated for the experiments displayed in Figure 3 a is as follows -\n$A=\\left[\\begin{array}{cccc}-0.9999 & 0 .",
    "spectralssm-24": "& 0 . & 0 . \\\\ 0 . & 0.9999 & 0 . & 0 . \\\\ 0 . & 0 . & -0.9999 & 0 . \\\\ 0 .",
    "spectralssm-25": "& 0 . & 0 . & 0.9999\\end{array}\\right], \\quad B=\\left[\\begin{array}{cccc}0.36858183 & -0.34219486 & 0.1407376 \\\\ 0.18933886 & -0.1243964 & 0.21866894 \\\\ 0.14593862 & -0.5791096 & -0.06816235 \\\\ -0.3095346 & -0.21441863 & 0.08696061\\end{array}\\right]$\n\n$$\nC=\\left[\\begin{array}{cccc}\n0.5528727 & -0.51329225 & 0.21110639 & 0.2840083 \\\\\n-0.18659459 & 0.3280034 & 0.21890792 & -0.8686644 \\\\\n-0.10224352 & -0.46430188 & -0.32162794 & 0.1304409\n\\end{array}\\right], \\quad D=\\left[\\begin{array}{ccc}\n1.5905786 & 0 .",
    "spectralssm-26": "& 0 . \\\\\n0 . & -0.45901108 & 0 \\\\\n0 . & 0 . & 0.3238576\n\\end{array}\\right]\n$$\n\nHyperparameters for STU: We only tuned the learning rate in the set ( $[5 e-2,1 e-1,5 e-$ $1,1,5,10]$ ) for vanilla STU and used $K=25$. ## Hyperparameters for LRU:\n\n- Model Hyperparameters $\\mathrm{OSG}^{+}$23] provide a few recommendations for the LRU model. We tested exhaustively over the following hyperparameter choices:\n- Stable Exp-parameterization: We searched over [True, False]\n- Logarithmic Representation of Recurrent Parameters: We searched over [True, False]\n- $\\gamma$-Normalization: We searched over [True, False]\n- Ring Initialization: We searched over min_rad $\\in\\{0.0,0.9,0.99,0.999\\}$ and max_rad $\\in\\{0.9,0.99,0.999,1.0\\}$. - Setting the max_init_phase $\\in\\{1.57,3.14,6.28\\}$\n\nWe found the Stable Exp-parameterization, Logarithmic Representation of Recurrent Parameters and $\\gamma$-normalization to be essential for training in this problem. We did not observe any particular benefit of Ring Initialization or reducing the phase at initialization and we set them to defaults eventually. We provide the learning curves over our search space in Figure 6\n\n- Optimization Hyperparameters Given the comparatively higher sample complexity of the LRU model we employed standard deep-learning optimization tricks like tuning weightdecay as well as applying a cosine learning rate schedule with warmup. These optimization tricks did not lead to gains over standard training with Adam and a fixed learning rate in this problem. We tuned the learning rate in the set ( $[5 e-2,1 e-1,5 e-1,1,5,10]$ ). ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-23.jpg?height=445&width=964&top_left_y=249&top_left_x=575)\n\nFigure 5: (Smoothed) Learning curves for learning a marginally stable LDS for a single STU layer (dashed) vs a single LRU layer (solid). Different colors represent different learning rates highlighting that the training becomes unstable for LRUs quickly as LR increases while the STU trains at much higher learning rates. Curiously at stable LRs we observe that LRUs show a platea-ing of learning for a large fraction of the training time. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-23.jpg?height=500&width=953&top_left_y=994&top_left_x=575)\n\nFigure 6: LRU Hparam search vs STU. All the gray curves represent the hyperparameters for LRU we tried. The STU curve is the best taken from Figure 5]. For LRU we searched over choices of enabling stable exp-parameterization, gamma-normalization, ring-initialization, phase-initialization, learning rate, weight decay and constant vs warmup+cosine decay lr schedule. ## E. 2 Experimental setup for LRA experiments\n\nOur training setup closely follows the experimental setup used by $\\left[\\mathrm{OSG}^{+} 23\\right]$. We use the same batch sizes and training horizons for all the tasks as employed by $\\left[\\mathrm{OSG}^{+}\\right.$23]. Hyperparameter tuning For all of our experiments on the LRA benchmark for both the vanilla STU model and the auto-regressive AR-STU model we searched the learning rate in the set $\\{1 e-4,3 e-$ $4,5 e-4,1 e-3,2.5 e-3,5 e-3\\}$ and tune the weight decay in the set $\\{1 e-3,1 e-2,1 e-1,5 e-1,1.0\\}$.",
    "spectralssm-27": "We fix the number of filters $K$ to be 24 . We use Adam as the training algorithm with other optimization hyperparameters set to their default values. We use the same learning rate schedule as [OSG ${ }^{+}$23], i.e. $10 \\%$ warmup followed by cosine decay to 0 . For the AR-STU model we searched over two values of $k_{y} \\in\\{2,32\\}$. In Table 2 we present a comparison of vanilla STU with AR-STU with $k_{y}=2$ and AR-STU with $k_{y}=32$. We find that both vanilla STU and AR-STU $k_{y}=2$ reach comparable accuracy which is better than the baselines S4 and LRU on non-image datasets. On image datasets we found $k_{y}=32$ to be helpful in getting better test accuracies. Initialization For the STU model we initialized all the $M$ matrices at 0 . Finally while training the AR-STU model as employed by the training setup of $\\left[\\mathrm{OSG}^{+} 23\\right]$ and previous SSM implementations, we found that using a smaller value of LR specifically for $M^{y}$ matrices to be useful. We decreased the value of LR by a factor 0.1 or 0.05 and searched over this parameter. |  | CIFAR | ListOps | Text | Retrieval | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $\\mathrm{S} 4[$ GGR21] | 88.65 | 59.60 | 86.82 | 90.90 | 94.20 | 96.35 |\n| LRU $\\left[\\mathrm{OSG}^{+23}\\right]$ | 89 | 60.2 | 89.4 | 89.9 | 95.1 | 94.2 |\n| STU | 83.73 | 61.04 | $\\mathbf{9 0 . 4 8}$ | 90.40 | 91.70 | 89.71 |\n| AR-STU $\\left(k_{y}=2\\right)$ | 86.56 | $\\mathbf{6 1 . 1 4}$ | $\\mathbf{9 0 . 4 7}$ | 90.52 | 93.85 | 90.49 |\n| AR-STU $\\left(k_{y}=32\\right)$ | $\\mathbf{9 1 . 3 4}$ | 57.66 | 88.51 | 87.39 | $\\mathbf{9 5 . 4 5}$ | 93.24 |\n\nTable 2: Comparison of the STU model against various proposed SSM models on the LRA benchmark: Bold values indicate the best for that task.",
    "spectralssm-28": "We find that STU is competitive across all the workloads without the need for carefully designed initializations, discretizations or normalizations. We report the median over 5 trials for our experiments. ## F Power of Auto-regression: Dimension-dependent representation for LDS\n\nIn this section we give a short proof that any partially-observed LDS can be perfectly predicted via a linear predictor acting over at most $d$ of its past inputs and outputs where $d$ is the hidden-state dimensionality (i.e. $A \\in \\mathbb{R}^{d \\times d}$ ). In particular\nTheorem F.1. Given an LDS parameterized by $A \\in \\mathbb{R}^{d \\times d}, B, C, D$, there exist coefficients $\\alpha_{1: d}$ and matrices $\\Gamma_{0: d}$ such that given any input sequence $u_{1: L}$, the output sequence $y_{1: L}$ generated by the action of the LDS on the input satisfies for all $t$\n\n$$\ny_{t}=\\sum_{i=1}^{d} \\alpha_{i} y_{t-i}+\\sum_{i=0}^{d} \\Gamma_{i} u_{t-i}\n$$\n\nProof. By unrolling the LDS we have that $y_{t}=\\sum_{i=0}^{t} C A^{i} B u_{t-i}+D u_{t}$. . By the Cayley Hamilton theorem, the matrix $A$ has a characteristic polynomial $p$ of degree $d$, namely there exists $d$ numbers $c_{1: d}$ such that\n\n$$\np(z)=\\sum_{i=0}^{d} c_{i} z^{i}\n$$\n\nsatisfies $p(A)=0$. Without loss of generality we can assume the constant term in the polynomial is 1. We can now consider the series for $y_{t}, y_{t-1}, \\ldots$ as\n\n$$\n\\begin{array}{cccccc}\ny_{t}-D u_{t} & = & C B u_{t} & C A B u_{t-1} & \\ldots & C A^{t} B u_{1} \\\\\ny_{t-1}-D u_{t-1} & = & 0 & C B u_{t-1} & \\ldots & C A^{t-1} B u_{1} \\\\\n\\vdots & & & & & \\\\\ny_{t-d}-D u_{t-d} & = & 0 & 0 & \\ldots & C A^{t-d} B u_{1}\n\\end{array}\n$$\n\nNow, if we take the combination of the above rows according to the coefficients of the characteristic polynomial, we get that\n\n$$\n\\sum_{i=0}^{d} c_{i} y_{t-i}=\\sum_{j=0}^{t} R_{j}+\\sum_{i=0}^{d} D u_{t-i}\n$$\n\nwhere $R_{j}$ is the appropriate sum along the $j^{\\prime}$ th column of the matrix above. For all $j>d$, this amounts to an expression of the form:\n$j>d \\Rightarrow R_{j}=\\sum_{i=0}^{d} c_{i} C A^{i} \\cdot A^{t-j} B u_{t-j}=C\\left(\\sum_{i=0}^{d} c_{i} A^{i}\\right) \\cdot A^{t-j} B u_{t-j}=C \\cdot p(A) \\cdot A^{t-j} B u_{t-j}=0$. Since all but the first $d$ columns are zero, rearranging (12) and collecting terms, we get that there exists coefficients $\\alpha_{1: d}$ and matrices $\\Gamma_{0: d}$ such that\n\n$$\ny_{t}=\\sum_{i=1}^{d} \\alpha_{i} y_{t-i}+\\sum_{j=0}^{d} \\Gamma_{j} u_{t-j}\n$$\n\n\n[^0]:    ${ }^{1}$ in particular all $\\alpha$ close to 1 , representing marginally stable systems. [^1]:    ${ }^{2}$ Note that [HSZ17] consider a simpler setting where in the ground truth $y_{t}$ is available to the learner for all future time steps. We do not make such an assumption and theorems have been adjusted to suffer an additional $L$ factor in the error as a result. [^2]:    ${ }^{3}$ We discovered some small but easily fixable errors in the original proof of HSZ17] which we have corrected in our proof\n\n[^3]:    ${ }^{4}$ We shortly note that the $K$ filters can be distributed amongst $K$ machines and their computations done separately.",
    "spectralssm-29": "There are many other opportunities for distributed computing for all architectures which we will not survey here as it is out of scope.",
    "spectralssm-30": ""
}