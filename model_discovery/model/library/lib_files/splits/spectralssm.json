{
    "spectralssm-0": "Spectral State Space Models\n\nNaman Agarwal Daniel Suo Xinyi Chen Elad Hazan\n\nAbstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.",
    "spectralssm-1": "We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al., 2017). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problems in sequence prediction/modelling. Recurrent Neural Networks (Hopfield, 1982; Rumelhart et al., 1985; Elman, 1990) are a natural choice for sequence modelling, but are notoriously hard to train; they often suffer from vanishing and exploding gradients (Bengio et al., 1994; Pascanu et al., 2013) and despite techniques to mitigate the issue (Hochreiter & Schmidhuber, 1997; Cho et al., 2014; Arjovsky et al., 2016), they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models (Vaswani et al., 2017) have become the staple of sequence modelling, achieving remarkable success across multiple domains (Brown et al., 2020; Dosovitskiy et al., 2020; Jumper et al., 2021). Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, the attention layers have memory and computation requirements that scale quadratically with context length. Many approximations to the standard attention layers have been proposed (see (Tay et al., 2022) for a recent survey). Recurrent Neural Networks have seen a recent resurgence in the form of state space models (SSM) which have shown promise modelling long-range sequences across varied modalities (Gu et al., 2021a; Dao et al., 2022; Gupta et al., 2022; Orvieto et al., 2023; Poli et al., 2023; Gu & Dao, 2023). SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\nx t = A \u200b x t \u2212 1 + B \u200b u t subscript \ud835\udc65 \ud835\udc61 \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 1 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \\displaystyle x_{t}=Ax_{t-1}+Bu_{t} y t = C \u200b x t + D \u200b u t . subscript \ud835\udc66 \ud835\udc61 \ud835\udc36 subscript \ud835\udc65 \ud835\udc61 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \\displaystyle y_{t}=Cx_{t}+Du_{t}. Here is the hidden state of the dynamical system, is the input to the system, and are observations. The matrices govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems in engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans (Blelloch, 1989; Smith et al., 2023) or convolutions (Gu et al., 2021a). A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see Hazan & Singh (2022). These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models (Dao et al., 2022), modelling time series (Zhang et al., 2023), and audio generation (Goel et al., 2022). To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the \u2019th observation can be written as\n\ny t = C \u200b x t + D \u200b u t subscript \ud835\udc66 \ud835\udc61 \ud835\udc36 subscript \ud835\udc65 \ud835\udc61 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \\displaystyle y_{t}=Cx_{t}+Du_{t} = C \u200b ( A \u200b x t \u2212 1 + B \u200b u t ) + D \u200b u t = \u2026 absent \ud835\udc36 \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 1 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \u2026 \\displaystyle=C(Ax_{t-1}+Bu_{t})+Du_{t}=... = \u2211 i = 0 t \u2212 1 C \u200b A i \u200b B \u200b u t \u2212 i + D \u200b u t absent superscript subscript \ud835\udc56 0 \ud835\udc61 1 \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\sum_{i=0}^{t-1}CA^{i}Bu_{t-i}+Du_{t}\n\nThe matrix is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is larger than one, then the the observation can grow without bounds. This is called an \u201cexplosive\u201d system. In a well-behaved system, the eigenvalues of have magnitude smaller than one. If the magnitudes are bounded away from one, say , for some small , then we can write\n\ny t = \u2211 i = 0 k C A i B u t \u2212 i + \u03c9 k , \u2225 \u03c9 k \u2225 \u2264 \u03b5 y_{t}=\\sum_{i=0}^{k}CA^{i}Bu_{t-i}+\\omega_{k}\\ \\ ,\\ \\ \\|\\omega_{k}\\|\\leq\\varepsilon\n\nfor . This mathematical fact implies that the effective memory of the system is on the order of . In general, the parameter is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of Orvieto et al. (2023) who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature (Gu et al., 2021a). Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity (Ghai et al., 2020) or via control over the input sequence (Simchowitz et al., 2018). Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering (Hazan et al., 2017). The idea is to represent previous inputs to the system in a different basis that is inspired by the special structure of powers of the system matrices. In a nutshell, the basic idea is to represent the output as\n\ny t = \u2211 j = 1 k M j \u200b ( \u2211 i \u03d5 j \u200b ( i ) \u22c5 u t \u2212 i ) , subscript \ud835\udc66 \ud835\udc61 superscript subscript \ud835\udc57 1 \ud835\udc58 subscript \ud835\udc40 \ud835\udc57 subscript \ud835\udc56 \u22c5 subscript italic-\u03d5 \ud835\udc57 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 {y}_{t}=\\sum_{j=1}^{k}M_{j}\\left(\\sum_{i}\\phi_{j}(i)\\cdot{u}_{t-i}\\right),\n\nwhere are \u201cspectral filters\u201d which sequence-length sized vectors that can be computed offline, and are matrices parameterizing the model. Figure 1 depicts these filters, that are computed as the eigenvectors of a special matrix. For the details of how these filters are derived and their computation, see Section 2 (for further details and background we refer the reader to Hazan & Singh (2022)). The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices , the effective memory111measured by the number of features required to represent an observation at any point in the sequence in the spectral basis is independent of !. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that are capable of efficiently and stably representing systems with extremely long memory. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in Hazan et al. (2017) with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark (Tay et al., 2021). We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previous convolutional models for sequence modeling, surveyed in the related work section, learn the convolutions from the data. The filters used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably more expressive than Linear Dynamical Systems. The reason is that their expressiveness does not depend on the memory decay, nor on the dimension of the system, which are necessary in all other methods. 1.2 Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. Gu et al. (2020) propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, Gu et al. (2021b) propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes.",
    "spectralssm-2": "However, the proposed method is numerically unstable in practice as well as memory-intensive. As a result, Gu et al. (2021a) develop the S4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. Gupta et al. (2022) simplify the S4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. Smith et al. (2023) propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. Motivated by the similarities between SSMs and RNNs, Orvieto et al. (2023) investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks. Spectral filtering. The technique of spectral filtering for learning linear dynamical systems was put forth in Hazan et al. (2017). This work studies online prediction of the sequence of observations , and the goal is to predict as well as the best symmetric LDS using past inputs and observations. Directly learning the dynamics is a non-convex optimization problem, and spectral filtering is developed as an improper learning technique with an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods that aim to identify the system dynamics, spectral filtering\u2019s guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in Hazan et al. (2018). Convolutional Models for Sequence Modeling\n\nExploiting the connnection between Linear dynamical systems and convolutions (as highlighted by Gu et al. (2021a)) various convolutional models have been proposed for sequence modelling. Fu et al. (2023) employ direct learning of convolutional kernels directly to sequence modelling but find that they underperform SSMs. They find the non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations to the kernels to match performance on the Long Range Arena benchmark. The proposed still contains significantly large number of parameters growing with the sequence length. Li et al. (2022) identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. The achieve this via a specific form of the kernel derived by repeating and scaling the kernel in a dyadic fashion. Shi et al. (2023) propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by Li et al.",
    "spectralssm-3": "(2022) and Fu et al. (2023). 2 Preliminaries\n\nOnline sequential prediction. Online sequential prediction is a game between a predictor/learner and nature in which iteratively at every time , the learner is presented an input . The learner then produces a candidate output , and nature reveals the element of a target sequence . The learner then suffers an instantaneous loss\n\n\u2016 y t \u2212 y ^ t \u2016 2 . superscript norm subscript \ud835\udc66 \ud835\udc61 subscript ^ \ud835\udc66 \ud835\udc61 2 \\|y_{t}-\\hat{y}_{t}\\|^{2}. The task of the learner is to minimize regret which is defined as follows\n\nRegret = \u2211 t = 1 L \u2016 y t \u2212 y ^ t \u2016 2 \u2212 min A \u2208 \ud835\udc9c \u200b \u2211 t = 1 L \u2016 y t \u2212 y ^ t \u200b ( A ) \u2016 2 , Regret superscript subscript \ud835\udc61 1 \ud835\udc3f superscript norm subscript \ud835\udc66 \ud835\udc61 subscript ^ \ud835\udc66 \ud835\udc61 2 subscript \ud835\udc34 \ud835\udc9c superscript subscript \ud835\udc61 1 \ud835\udc3f superscript norm subscript \ud835\udc66 \ud835\udc61 subscript ^ \ud835\udc66 \ud835\udc61 \ud835\udc34 2 \\mbox{Regret}=\\sum_{t=1}^{L}\\|y_{t}-\\hat{y}_{t}\\|^{2}-\\min_{A\\in{\\mathcal{A}}}\\sum_{t=1}^{L}\\|y_{t}-\\hat{y}_{t}(A)\\|^{2},\n\nwhere is a benchmark set of learning algorithms. Linear Dynamical Systems:\n\nAn example benchmark set of methods is that of linear dynamical systems. Recall that a linear dymamical system (LDS) has four matrix parameters, . The system evolves and generates outputs according to the following equations\n\nx t subscript \ud835\udc65 \ud835\udc61 \\displaystyle x_{t} \u225c A \u200b x t \u2212 1 + B \u200b u t . \u225c absent \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 1 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \\displaystyle\\triangleq Ax_{t-1}+Bu_{t}. y ^ t subscript ^ \ud835\udc66 \ud835\udc61 \\displaystyle\\hat{y}_{t} \u225c C \u200b x t + D \u200b u t \u225c absent \ud835\udc36 subscript \ud835\udc65 \ud835\udc61 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \\displaystyle\\triangleq Cx_{t}+Du_{t} (1)\n\nThus, an example class of benchmark algorithms are all predictors that generate according to these rules, for a fixed set of matrices . Spectral Filtering:\n\nAnother important set of predictors is one which is inspired by spectral filtering. For any define the following Hankel matrix whose entries are given by\n\nZ \u200b [ i , j ] \u225c 2 ( i + j ) 3 \u2212 ( i + j ) \u225c \ud835\udc4d \ud835\udc56 \ud835\udc57 2 superscript \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 Z[i,j]\\triangleq\\frac{2}{(i+j)^{3}-(i+j)}\n\nIt can be shown that is a real PSD matrix and therefore admits a real spectral decomposition and the (non-negative) eigenvalues can be easily ordered naturally by their value. Let be the eigenvalue-eigenvector pairs of ordered to satisfy . We consider a fixed number of the above eigenvectors. Algorithms in the spectral filtering class generate as follows. For each , we first featurize the input sequence by projecting the input sequence until time on , leading to a sequence defined as\n\nU t , k = \u2211 i = 1 t u t \u2212 i \u22c5 \u03d5 k \u200b ( i ) . subscript \ud835\udc48 \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udc56 1 \ud835\udc61 \u22c5 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 subscript italic-\u03d5 \ud835\udc58 \ud835\udc56 U_{t,k}=\\sum_{i=1}^{t}u_{t-i}\\cdot\\phi_{k}(i). The spectral filtering class is further parameterized by matrices , and a set of matrices . The output at time is then given by\n\ny ^ t = y t \u2212 1 + M 1 u \u200b u t + M 2 u \u200b u t \u2212 1 + \u2211 k = 1 K M k \u03d5 \u200b U t , k . subscript ^ \ud835\udc66 \ud835\udc61 subscript \ud835\udc66 \ud835\udc61 1 subscript superscript \ud835\udc40 \ud835\udc62 1 subscript \ud835\udc62 \ud835\udc61 subscript superscript \ud835\udc40 \ud835\udc62 2 subscript \ud835\udc62 \ud835\udc61 1 superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 italic-\u03d5 \ud835\udc58 subscript \ud835\udc48 \ud835\udc61 \ud835\udc58 \\hat{y}_{t}=y_{t-1}+M^{u}_{1}u_{t}+M^{u}_{2}u_{t-1}+\\sum_{k=1}^{K}M^{\\phi}_{k}U_{t,k}. (2)\n\nNote that given an input sequence for any , the matrix can be efficiently computed via convolutions along the time dimension in total time . The following theorem (proved in Hazan et al. (2017)) establishes that the spectral filtering class of predictors approximately contains bounded linear dynamical systems with positive semi-definite . Theorem 2.1. Given any such that is a PSD matrix with and given any numbers , there exists matrices , such that for all and all sequences satisfying for all the following holds. Let be the sequence generated by execution of the LDS given by (via (2)) and be the sequence generated by Spectral Filtering (via (2)) using the matrices . Then for all , we have that\n\n\u2016 y t LDS \u2212 y t SF \u2016 2 \u2264 c 1 \u22c5 L 2.5 \u22c5 ( c 2 \u2212 log \u2061 ( L ) / K ) superscript norm subscript superscript \ud835\udc66 LDS \ud835\udc61 subscript superscript \ud835\udc66 SF \ud835\udc61 2 \u22c5 subscript \ud835\udc50 1 superscript \ud835\udc3f 2.5 superscript subscript \ud835\udc50 2 \ud835\udc3f \ud835\udc3e \\|y^{\\mathrm{LDS}}_{t}-y^{\\mathrm{SF}}_{t}\\|^{2}\\leq c_{1}\\cdot L^{2.5}\\cdot(c_{2}^{-\\log(L)/K})\n\nwhere depends upon and is a universal constant. In the next section we build upon this predictor class to create neural networks that are sequential predictors with a special structure. 3 Spectral Transform Unit(STU)\n\nIn this section we use the Spectral Filtering class to create a neural network layer which is a sequence to sequence map, i.e. given an input sequence , it produces an output sequence . A single layer of STU (depicted in Figure 2) is parameterized by a number , denoting the number of eigenfactors and matrices and . The matrices form the params of the layer. Further recall the Hankel matrix whose entries are given by\n\nZ \u200b [ i , j ] \u225c 2 ( i + j ) 3 \u2212 ( i + j ) . \u225c \ud835\udc4d \ud835\udc56 \ud835\udc57 2 superscript \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 Z[i,j]\\triangleq\\frac{2}{(i+j)^{3}-(i+j)}. (3)\n\nand let be the eigenvalue-eigenvector pairs of ordered to satisfy . Given an input sequence , we first featurize the input sequence as follows. For any , we begin by projecting the input sequence till time on fixed filters , leading to two feature vectors defined as\n\nU t , k + = \u2211 i = 0 t \u2212 1 u t \u2212 i \u22c5 \u03d5 k \u200b ( i ) U t , k \u2212 = \u2211 i = 0 t \u2212 1 u t \u2212 i \u22c5 ( \u2212 1 ) i \u22c5 \u03d5 k \u200b ( i ) . formulae-sequence superscript subscript \ud835\udc48 \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udc56 0 \ud835\udc61 1 \u22c5 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 subscript italic-\u03d5 \ud835\udc58 \ud835\udc56 superscript subscript \ud835\udc48 \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udc56 0 \ud835\udc61 1 \u22c5 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 superscript 1 \ud835\udc56 subscript italic-\u03d5 \ud835\udc58 \ud835\udc56 U_{t,k}^{+}=\\sum_{i=0}^{t-1}u_{t-i}\\cdot\\phi_{k}(i)\\qquad U_{t,k}^{-}=\\sum_{i=0}^{t-1}u_{t-i}\\cdot(-1)^{i}\\cdot\\phi_{k}(i). Note that for every , the sequence of features can be computed efficiently via convolution. The output sequence is then given by\n\ny ^ t subscript ^ \ud835\udc66 \ud835\udc61 \\displaystyle\\hat{y}_{t} = \u200b y ^ t \u2212 2 + \u2211 i = 1 3 M i u \u200b u t + 1 \u2212 i \u23df Auto \u2212 regressive \u200b Component + absent limit-from subscript \u23df subscript ^ \ud835\udc66 \ud835\udc61 2 superscript subscript \ud835\udc56 1 3 subscript superscript \ud835\udc40 \ud835\udc62 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc56 Auto regressive Component \\displaystyle=\\underbrace{\\vrule width=0.0pt,height=0.0pt,depth=13.77771pt\\hat{y}_{t-2}+\\sum_{i=1}^{3}M^{u}_{i}u_{t+1-i}}_{\\mathrm{Auto-regressive\\;Component}}+ \u2211 k = 1 K M k \u03d5 + \u200b \u03c3 k 1 / 4 \u200b U t \u2212 2 , k + + \u2211 k = 1 K M k \u03d5 \u2212 \u200b \u03c3 k 1 / 4 \u200b U t \u2212 2 , k \u2212 \u23df Spectral \u200b Component . subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 limit-from italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript superscript \ud835\udc48 \ud835\udc61 2 \ud835\udc58 superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 limit-from italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript superscript \ud835\udc48 \ud835\udc61 2 \ud835\udc58 Spectral Component \\displaystyle\\underbrace{\\sum_{k=1}^{K}M^{\\phi+}_{k}\\sigma_{k}^{1/4}U^{+}_{t-2,k}+\\sum_{k=1}^{K}M^{\\phi-}_{k}\\sigma_{k}^{1/4}U^{-}_{t-2,k}}_{\\mathrm{Spectral\\;Component}}. (4)\n\nThe above output contains a small auto-regressive component that essentially allows for stable learning of the spectral component as the memory grows. The primary differences we observe from the original spectral filtering class (2) are the introduction of a negative part in the spectral component and the slight change in the auto-regressive component. Both of these changes are necessitated by the requirement to capture negative eigenvalues of . Note that (4) corresponds to the specification of the algorithm presented in Hazan et al. (2018), when the eigenvalues are known to be real numbers. For completeness and ease of discourse we prove the following representation theorem in the Appendix which shows that the above class approximately contains any marginally-stable LDS with symmetric .222We discovered some small but easily fixable errors in the original proof of (Hazan et al., 2017) which we have corrected in our proof\n\nTheorem 3.1. Given any such that is a symmetric matrix with and given any numbers , there exists matrices , such that for all and all sequences satisfying for all the following holds. Let be the sequence generated by execution of the LDS given by (via (2)) and be the sequence generated by Spectral Filtering (via (4)) using the matrices . Then for all , we have that\n\n\u2016 y t LDS \u2212 y t SF \u2016 2 \u2264 c 1 \u22c5 L 3.5 \u22c5 c 2 \u2212 log \u2061 ( L ) / K superscript norm subscript superscript \ud835\udc66 LDS \ud835\udc61 subscript superscript \ud835\udc66 SF \ud835\udc61 2 \u22c5 subscript \ud835\udc50 1 superscript \ud835\udc3f 3.5 superscript subscript \ud835\udc50 2 \ud835\udc3f \ud835\udc3e \\|y^{\\mathrm{LDS}}_{t}-y^{\\mathrm{SF}}_{t}\\|^{2}\\leq c_{1}\\cdot L^{3.5}\\cdot c_{2}^{-\\log(L)/K}\n\nwhere depends upon and is a universal constant. The proof of the above theorem is provided in the appendix along with an alternative version of spectral filtering using slightly modified filters which also provide the same guarantee. Remark 3.2. Comparing Theorem 3.1 (our contribution) and Theorem 2.1 (Theorem 1 from (Hazan et al., 2017)), we note the following differences. Firstly the theorem holds for symmetric matrices and not just PSD matrices. In the paper (Hazan et al., 2017), the authors allude to a direct extension for the symmetric case but we believe that extension is not fully correct. We use a similar idea to prove this theorem. Secondly a minor difference is that in the sequential prediction setting the prediction is auto-regressive, i.e. uses its own to make the future predictions. Computational complexity\n\nAs noted before the features and can be computed efficiently in total time . Further computation of the spectral component takes and the autoregressive part can be implemented in total time . Therefore the overall runtime is . 3.1 Experiment: Learning a marginally-stable LDS\n\nWe provide a simple synthetic evaluation of the stability and training efficiency afforded by the STU. We consider a low-dimensional linear system generated as follows. are matrices with iid unit Gaussian entries. is a diagonal matrix with iid unit Gaussian entries and is a diagonal matrix with where is a random sign. By design this is a system with a very high stability constant (). As a training dataset we generated where is a random input sequence and is the output generated by applying the linear dynamical system on . We perform mini-batch (batch size 1) training with the l2 loss. As comparison we perform the same procedure with an LRU (Linear Recurrent Unit) layer as proposed by Orvieto et al. (2023) which directly parameterizes the linear system. The results of the training loss as seen by the two systems are presented in Figure 3. We use all the initialization/normalization techniques as recommended by Orvieto et al. (2023) for LRU including the stable exponential parameterization, -normalization and ring-initialization. Indeed we find that all these tricks were necessary to learn this system at all. We provide more details about the ablations and other hyperparameter setups in the appendix. We observe that the STU is significantly more efficient at learning the LDS as opposed to the LRU. We further find that there is a wide range of LRs where the STU has a stable optimization trajectory and the loss decreases continuously highlighting the advantages of a convex parameterization. On the other hand, LRU is able to eventually learn the system at the right learning rates, it requires almost 8x the number of samples to get to a system with non-trivial accuracy. More details can be found in the appendix. Curiously we observe that for the LRU training plateaus completely for the first 50% of training highlighting the difficulty of optimization via a non-convex landscape. The STU layer in the previous experiment employs . In Figure 4 we plot the performance of STU at various levels of . As predicted by the theory we observe an exponential decay in the error as increases with the error effectively plateauing after . 4 Stacked STU\n\nTo increase the representation capacity and to maintain the efficiency of prediction through linear dynamical systems, proposed models in the SSM literature take the form of stacking these sequence to sequence transforms into multiple layers. Non-linearities in the model can then be introduced by sandwiching them as layers lying in between these sequence to sequence transforms. In this paper we closely follow the stacking approach followed by Orvieto et al. (2023), replacing the LRU layers appropriately by STU layers. A schematic for the resultant multi-layer model is displayed in Figure 5. In a nutshell, the input sequence is first embedded via a time-invariant embedding function followed by multiple repetitions of alternating STU layers and non-linearities (in particular we use GLU). Finally the resulting output is time-pooled followed by a final readout layer according to the task at hand. This composite model can now be trained in a standard fashion via back-propagation and other commonly used deep-learning optimization techniques. 4.1 Experiments on Long Range Arena (Tay et al., 2021)\n\nWe evaluate the stacked STU model on the Long Range Arena (LRA) benchmark (Tay et al., 2021). This benchmark aims to assess the performance of sequence prediction models in long-context scenarios and consists of six tasks of various modalities, including text and images. The context length for the tasks ranges from 1K to 16K, and the tasks require capabilities such as hierarchical reasoning, matching and retrieval, and visual-spatial understanding. SSMs (Gu et al., 2021a) have shown significantly superior performance on most of the tasks compared to Transformer architectures. In particular for the hardest task in the suite, PathX (image classification with a context length of 16K), no transformer model has been able to achieve anything beyond the accuracy of random guessing. We provide the evaluation of the stacked STU model on the two hardest tasks namely PathFinder and PathX in the table below. We compare our performance against the ablation carried out by Orvieto et al. (2023) who find that ring initialization, stable exponential parameterization and -normalization are all crucial towards learning these tasks. In particular as reported by Orvieto et al. (2023) all three of the above interventions were necessary to learn on PathX to any non-trivial accuracy. This is a result of the much larger context length of 16K employed by the PathX task. On the other hand we find that the the stacked STU (with the STU component exactly as represented by (4)) is sufficient to learn on both these tasks to relatively high accuracies. Notably we do not require any other normalizations or initialization techniques (we randomly initialize our M matrices using standard Glorot initialization). This result in particular confirms and highlights the theoretical stability afforded by the STU even under learning tasks involving large sequence lengths. In the next section we highlight a simple technique towards significantly improving the achieved accuracy for the stacked STU model. 5 Hybrid Temporal and Spectral Units\n\nA simple extension to the STU model (Equation (4)) is to parameterize the dependence of on with a parameter , leading to the following prediction model\n\ny ^ t subscript ^ \ud835\udc66 \ud835\udc61 \\displaystyle\\hat{y}_{t} = \u200b \ud835\udc74 \ud835\udc9a \u200b y ^ t \u2212 2 + \u2211 i = 1 3 M i u \u200b u t + 1 \u2212 i \u23df Auto \u2212 regressive \u200b Component + absent limit-from subscript \u23df superscript \ud835\udc74 \ud835\udc9a subscript ^ \ud835\udc66 \ud835\udc61 2 superscript subscript \ud835\udc56 1 3 subscript superscript \ud835\udc40 \ud835\udc62 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc56 Auto regressive Component \\displaystyle=\\underbrace{\\vrule width=0.0pt,height=0.0pt,depth=13.77771pt\\boldsymbol{M^{y}}\\hat{y}_{t-2}+\\sum_{i=1}^{3}M^{u}_{i}u_{t+1-i}}_{\\mathrm{Auto-regressive\\;Component}}+ \u2211 k = 1 K M k \u03d5 + \u200b \u03c3 k 1 / 4 \u200b U t \u2212 2 , k + + \u2211 k = 1 K M k \u03d5 \u2212 \u200b \u03c3 k 1 / 4 \u200b U t \u2212 2 , k \u2212 \u23df Spectral \u200b Component . subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 limit-from italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript superscript \ud835\udc48 \ud835\udc61 2 \ud835\udc58 superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 limit-from italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript superscript \ud835\udc48 \ud835\udc61 2 \ud835\udc58 Spectral Component \\displaystyle\\underbrace{\\sum_{k=1}^{K}M^{\\phi+}_{k}\\sigma_{k}^{1/4}U^{+}_{t-2,k}+\\sum_{k=1}^{K}M^{\\phi-}_{k}\\sigma_{k}^{1/4}U^{-}_{t-2,k}}_{\\mathrm{Spectral\\;Component}}. (5)\n\nSetting we recover the guarantees afforded by Theorem 3.1 and thus the above model is strictly more powerful. We find that the above change leads to significant improvements over the accuracy achieved by the simple STU model. We can further extend the auto-regression to depend on multiple previous as opposed to just . Indeed as the following theorem shows adding sufficiently long auto-regression is powerful enough to capture any LDS. Theorem 5.1. Given an LDS parameterized by , there exist coefficients and matrices such that given any input sequence , the output sequence generated by the action of the LDS on the input satisfies for all\n\ny t = \u2211 i = 1 d \u03b1 i \u200b y t \u2212 i + \u2211 i = 0 d \u0393 i \u200b u t \u2212 i subscript \ud835\udc66 \ud835\udc61 superscript subscript \ud835\udc56 1 \ud835\udc51 subscript \ud835\udefc \ud835\udc56 subscript \ud835\udc66 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc56 0 \ud835\udc51 subscript \u0393 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 y_{t}=\\sum_{i=1}^{d}\\alpha_{i}y_{t-i}+\\sum_{i=0}^{d}\\Gamma_{i}u_{t-i}\n\nThis is a well-known observation and we provide a proof in the appendix (Section D). Motivated by the above theorem we modify the definition of AR-STU to add auto-regression over the previously produced outputs. In particular given a parameter we define AR-STU the model as\n\ny ^ t subscript ^ \ud835\udc66 \ud835\udc61 \\displaystyle\\hat{y}_{t} = \u200b \u2211 i = 1 k y \ud835\udc74 \ud835\udc8a \ud835\udc9a \u200b y ^ t \u2212 i + \u2211 i = 1 3 M i u \u200b u t + 1 \u2212 i \u23df Auto \u2212 regressive \u200b Component + absent limit-from subscript \u23df superscript subscript \ud835\udc56 1 subscript \ud835\udc58 \ud835\udc66 subscript superscript \ud835\udc74 \ud835\udc9a \ud835\udc8a subscript ^ \ud835\udc66 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc56 1 3 subscript superscript \ud835\udc40 \ud835\udc62 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc56 Auto regressive Component \\displaystyle=\\underbrace{\\vrule width=0.0pt,height=0.0pt,depth=13.77771pt\\sum_{i=1}^{k_{y}}\\boldsymbol{M^{y}_{i}}\\hat{y}_{t-i}+\\sum_{i=1}^{3}M^{u}_{i}u_{t+1-i}}_{\\mathrm{Auto-regressive\\;Component}}+ \u2211 k = 1 K M k \u03d5 + \u200b \u03c3 k 1 / 4 \u200b U t \u2212 2 , k + + \u2211 k = 1 K M k \u03d5 \u2212 \u200b \u03c3 k 1 / 4 \u200b U t \u2212 2 , k \u2212 \u23df Spectral \u200b Component . subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 limit-from italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript superscript \ud835\udc48 \ud835\udc61 2 \ud835\udc58 superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 limit-from italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript superscript \ud835\udc48 \ud835\udc61 2 \ud835\udc58 Spectral Component \\displaystyle\\underbrace{\\sum_{k=1}^{K}M^{\\phi+}_{k}\\sigma_{k}^{1/4}U^{+}_{t-2,k}+\\sum_{k=1}^{K}M^{\\phi-}_{k}\\sigma_{k}^{1/4}U^{-}_{t-2,k}}_{\\mathrm{Spectral\\;Component}}. (6)\n\nIn Table 2, we evaluate the performance of AR-STU. For most tasks including including ListOps, Text, Retrieval and PathX we find that setting is sufficient to get optimal results. For two tasks, CIFAR and Pathfinder, we found that led to significant performance gains. We initialize the parameters to be such that they recover the STU model at initialization, i.e. we set and for all at initialization (where is a hyper-parameter, but we find the choice of to be uniformly optimal). Overall we find that the STU model is competitive with state space models across the entire variety of tasks in the Long Range Arena without the need for specific initializations, discretizations and normalizations. We provide details of our experimental setup and more results in the appendix. 6 Conclusion\n\nInsprired by the success of State Space Models, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for learning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. Impact statement\n\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References\n\nArjovsky et al. (2016) Arjovsky, M., Shah, A., and Bengio, Y. Unitary evolution recurrent neural networks. In International conference on machine learning, pp. 1120\u20131128. PMLR, 2016. Bengio et al. (1994) Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994. Blelloch (1989) Blelloch, G. E. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526\u20131538, 1989. Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Cho et al. (2014) Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation.",
    "spectralssm-4": "arXiv preprint arXiv:1406.1078, 2014. Dao et al. (2022) Dao, T., Fu, D. Y., Saab, K.",
    "spectralssm-5": "K., Thomas, A. W., Rudra, A., and R\u00e9, C. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Elman (1990) Elman, J. L. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990. Fu et al. (2023) Fu, D. Y., Epstein, E. L., Nguyen, E., Thomas, A. W., Zhang, M., Dao, T., Rudra, A., and R\u00e9, C. Simple hardware-efficient long convolutions for sequence modeling. arXiv preprint arXiv:2302.06646, 2023. Ghai et al. (2020) Ghai, U., Lee, H., Singh, K., Zhang, C., and Zhang, Y. No-regret prediction in marginally stable systems. In Conference on Learning Theory, pp. 1714\u20131757. PMLR, 2020. Goel et al. (2022) Goel, K., Gu, A., Donahue, C., and R\u00e9, C. It\u2019s raw! audio generation with state-space models. In International Conference on Machine Learning, pp. 7616\u20137633. PMLR, 2022. Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Gu et al. (2020) Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. Hippo: Recurrent memory with optimal polynomial projections.",
    "spectralssm-6": "In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1474\u20131487. Curran Associates, Inc., 2020. Gu et al. (2021a) Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021a. Gu et al. (2021b) Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R\u00e9, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572\u2013585, 2021b. Gupta et al. (2022) Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces.",
    "spectralssm-7": "In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=RjS0j6tsSrf. Hazan & Singh (2022) Hazan, E. and Singh, K. Introduction to online nonstochastic control. arXiv preprint arXiv:2211.09619, 2022. Hazan et al. (2017) Hazan, E., Singh, K., and Zhang, C. Learning linear dynamical systems via spectral filtering.",
    "spectralssm-8": "In Advances in Neural Information Processing Systems, pp. 6702\u20136712, 2017. Hazan et al. (2018) Hazan, E., Lee, H., Singh, K., Zhang, C., and Zhang, Y. Spectral filtering for general linear dynamical systems.",
    "spectralssm-9": "In Advances in Neural Information Processing Systems, pp. 4634\u20134643, 2018. Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Hopfield (1982) Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554\u20132558, 1982. Jumper et al. (2021) Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., \u017d\u00eddek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold.",
    "spectralssm-10": "Nature, 596(7873):583\u2013589, 2021. Li et al. (2022) Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes convolutional models great on long sequence modeling?",
    "spectralssm-11": "arXiv preprint arXiv:2210.09298, 2022. Orvieto et al. (2023) Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. Pascanu et al. (2013) Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In International conference on machine learning, pp. 1310\u20131318. Pmlr, 2013. Poli et al. (2023) Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00e9, C. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. Rumelhart et al. (1985) Rumelhart, D. E., Hinton, G. E., Williams, R. J., et al. Learning internal representations by error propagation, 1985. Shi et al. (2023) Shi, J., Wang, K. A., and Fox, E. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pp. 31312\u201331327. PMLR, 2023. Simchowitz et al. (2018) Simchowitz, M., Mania, H., Tu, S., Jordan, M. I., and Recht, B. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pp. 439\u2013473. PMLR, 2018. Smith et al. (2023) Smith, J. T., Warrington, A., and Linderman, S. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. Tay et al. (2021) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers.",
    "spectralssm-12": "In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k. Tay et al. (2022) Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. ISSN 0360-0300. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Zhang et al. (2023) Zhang, M., Saab, K. K., Poli, M., Dao, T., Goel, K., and R\u00e9, C. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. Appendix A Proof of Theorem 3.1\n\nWe begin by observing that without loss of generality we can assume that is a real-diagonal matrix. This can be ensured by performing a spectral decomposition of and absorbing the by redefining the system. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any , , with . Further define the Hankel matrix as\n\nZ \u225c \u222b 0 1 \u03bc \u200b ( \u03b1 ) \u200b \u03bc \u200b ( \u03b1 ) \u22a4 \u200b \ud835\udc51 \u03b1 . \u225c \ud835\udc4d superscript subscript 0 1 \ud835\udf07 \ud835\udefc \ud835\udf07 superscript \ud835\udefc top differential-d \ud835\udefc Z\\triangleq\\int_{0}^{1}\\mu(\\alpha)\\mu(\\alpha)^{\\top}d\\alpha. As the following lemma shows the Hankel matrix above is the same Hankel matrix defined in the definition of STU (3). Lemma A.1. is a Hankel matrix with entries given as\n\nZ \u200b ( i , j ) = 2 ( i + j ) 3 \u2212 ( i + j ) \ud835\udc4d \ud835\udc56 \ud835\udc57 2 superscript \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 Z(i,j)=\\frac{2}{(i+j)^{3}-(i+j)}\n\nLemma A.2. We have that the following statements hold regarding for any ,\n\n\u2022\n\n\u2022\n\nFor any and any unit vector we have that\n\n( \u03bc \u200b ( \u03b1 ) \u22a4 \u200b v ) 2 \u2264 12 \u200b ( v \u22a4 \u200b H \u200b v ) superscript \ud835\udf07 superscript \ud835\udefc top \ud835\udc63 2 12 superscript \ud835\udc63 top \ud835\udc3b \ud835\udc63 (\\mu(\\alpha)^{\\top}v)^{2}\\leq 12(v^{\\top}Hv)\n\nLemma A.3. For any , let be the projection of on the subspace spanned by top eigenvectors of , then we have that\n\n\u2016 \u03bc \u200b ( \u03b1 ) \u2212 \u03bc ~ \u200b ( \u03b1 ) \u2016 2 \u2264 12 \u200b \u2211 i = k + 1 L \u03c3 i superscript norm \ud835\udf07 \ud835\udefc ~ \ud835\udf07 \ud835\udefc 2 12 superscript subscript \ud835\udc56 \ud835\udc58 1 \ud835\udc3f subscript \ud835\udf0e \ud835\udc56 \\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2}\\leq 12\\sum_{i=k+1}^{L}\\sigma_{i}\n\nFinally the following lemma from (Hazan et al., 2017) shows that the spectrum of the matrix decays exponentially.",
    "spectralssm-13": "Lemma A.4 (Lemma E.3 (Hazan et al., 2017)). Let be the top eigenvalue of . Then we have that\n\n\u03c3 j \u2264 min \u2061 ( 3 / 4 , \u0393 \u200b c \u2212 j / log \u2061 ( L ) ) subscript \ud835\udf0e \ud835\udc57 3 4 \u0393 superscript \ud835\udc50 \ud835\udc57 \ud835\udc3f \\sigma_{j}\\leq\\min\\left(3/4,\\Gamma c^{-j/\\log(L)}\\right)\n\nwhere and is an absolute constant. We now move towards proving Theorem 3.1. Consider the following calculation for the LDS sequence\n\ny t LDS = \u2211 i = 0 T C \u200b A i \u200b B \u200b u t \u2212 i + D \u200b u t , superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc56 0 \ud835\udc47 \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \\displaystyle y_{t}^{\\mathrm{LDS}}=\\sum_{i=0}^{T}CA^{i}Bu_{t-i}+Du_{t},\n\nand therefore we have that\n\ny t LDS \u2212 y t \u2212 2 LDS = ( C \u200b B + D ) \u200b u t + C \u200b A \u200b B \u200b u t \u2212 1 \u2212 D \u200b u t \u2212 2 + \u2211 i = 0 T C \u200b ( A i + 2 \u2212 A i ) \u200b B \u200b u t \u2212 2 \u2212 i \u23df Term \u200b of \u200b Interest superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 LDS \ud835\udc36 \ud835\udc35 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 2 subscript \u23df superscript subscript \ud835\udc56 0 \ud835\udc47 \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 2 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 Term of Interest \\displaystyle y_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}}=(CB+D)u_{t}+CABu_{t-1}-Du_{t-2}+\\underbrace{\\sum_{i=0}^{T}C(A^{i+2}-A^{i})Bu_{t-2-i}}_{\\mathrm{Term\\;of\\;Interest}}\n\nFor any we define the matrix whose column is the input vector .",
    "spectralssm-14": "We allow to be negative and by convention assume for any .",
    "spectralssm-15": "Denote the diagonal entries of by , i.e. . The term of interest above can then be written as\n\n\u2211 i = 0 L C \u200b ( A i + 2 \u2212 A i ) \u200b B \u200b u t \u2212 2 \u2212 i superscript subscript \ud835\udc56 0 \ud835\udc3f \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 2 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle\\sum_{i=0}^{L}C(A^{i+2}-A^{i})Bu_{t-2-i} = \u2211 l = 1 d h ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( \u03b1 l i + 2 \u2212 \u03b1 l i ) \u200b u t \u2212 2 \u2212 i ) absent superscript subscript \ud835\udc59 1 subscript \ud835\udc51 \u210e tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 2 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle=\\sum_{l=1}^{d_{h}}(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(\\alpha_{l}^{i+2}-\\alpha_{l}^{i})u_{t-2-i}\\right) = \u2211 l : \u03b1 l \u2265 0 ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( \u03b1 l 2 \u2212 1 ) \u200b \u03b1 l i \u200b u t \u2212 2 \u2212 i ) + \u2211 l : \u03b1 l < 0 ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( \u03b1 l 2 \u2212 1 ) \u200b \u03b1 l i \u200b u t \u2212 2 \u2212 i ) absent subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f superscript subscript \ud835\udefc \ud835\udc59 2 1 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f superscript subscript \ud835\udefc \ud835\udc59 2 1 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle=\\sum_{l:\\alpha_{l}\\geq 0}(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(\\alpha_{l}^{2}-1)\\alpha_{l}^{i}u_{t-2-i}\\right)+\\sum_{l:\\alpha_{l}<0}(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(\\alpha_{l}^{2}-1)\\alpha_{l}^{i}u_{t-2-i}\\right) = \u2211 l : \u03b1 l \u2265 0 ( \u03b1 l + 1 ) \u200b ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( \u03b1 l \u2212 1 ) \u200b \u03b1 l i \u200b u t \u2212 2 \u2212 i ) + \u2211 l : \u03b1 l < 0 ( 1 + | \u03b1 l | ) \u200b ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( | \u03b1 l | \u2212 1 ) \u200b | \u03b1 l | i \u200b ( \u2212 1 ) i \u200b u t \u2212 2 \u2212 i ) absent subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f subscript \ud835\udefc \ud835\udc59 1 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 1 subscript \ud835\udefc \ud835\udc59 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f subscript \ud835\udefc \ud835\udc59 1 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 superscript 1 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle=\\sum_{l:\\alpha_{l}\\geq 0}(\\alpha_{l}+1)(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(\\alpha_{l}-1)\\alpha_{l}^{i}u_{t-2-i}\\right)+\\sum_{l:\\alpha_{l}<0}(1+|\\alpha_{l}|)(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(|\\alpha_{l}|-1)|\\alpha_{l}|^{i}(-1)^{i}u_{t-2-i}\\right) = \u2211 l : \u03b1 l \u2265 0 ( \u03b1 l + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } \u200b \u03bc \u200b ( \u03b1 ) ) + \u2211 l : \u03b1 l < 0 ( | \u03b1 l | + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } \u2299 \ud835\udfcf \u00b1 ) \u200b \u03bc \u200b ( | \u03b1 l | ) absent subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f \ud835\udf07 \ud835\udefc subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 direct-product subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f superscript 1 plus-or-minus \ud835\udf07 subscript \ud835\udefc \ud835\udc59 \\displaystyle=\\sum_{l:\\alpha_{l}\\geq 0}(\\alpha_{l}+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\mu(\\alpha)\\right)+\\sum_{l:\\alpha_{l}<0}(|\\alpha_{l}|+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\odot\\mathbf{1}^{\\pm}\\right)\\mu(|\\alpha_{l}|)\n\nwhere is defined as the matrix whose every row is the alternating sign vector ) and is Hadamard product (i.e. entry-wise multiplication). y t LDS \u2212 y t \u2212 2 LDS superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 LDS \\displaystyle y_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}} = ( C \u200b B + D ) \u200b u t + C \u200b A \u200b B \u200b u t \u2212 1 \u2212 D \u200b u t \u2212 2 + \u2211 l : \u03b1 l \u2265 0 ( \u03b1 l + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } \u200b \u03bc \u200b ( \u03b1 ) ) \u23df P \u200b o \u200b s \u200b i \u200b t \u200b i \u200b v \u200b e \u200b P \u200b a \u200b r \u200b t absent \ud835\udc36 \ud835\udc35 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 2 subscript \u23df subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f \ud835\udf07 \ud835\udefc \ud835\udc43 \ud835\udc5c \ud835\udc60 \ud835\udc56 \ud835\udc61 \ud835\udc56 \ud835\udc63 \ud835\udc52 \ud835\udc43 \ud835\udc4e \ud835\udc5f \ud835\udc61 \\displaystyle=(CB+D)u_{t}+CABu_{t-1}-Du_{t-2}+\\underbrace{\\sum_{l:\\alpha_{l}\\geq 0}(\\alpha_{l}+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\mu(\\alpha)\\right)}_{PositivePart} + \u2211 l : \u03b1 l < 0 ( | \u03b1 l | + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } \u2299 \ud835\udfcf \u00b1 ) \u200b \u03bc \u200b ( | \u03b1 l | ) \u23df N \u200b e \u200b g \u200b a \u200b t \u200b i \u200b v \u200b e \u200b P \u200b a \u200b r \u200b t subscript \u23df subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 direct-product subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f superscript 1 plus-or-minus \ud835\udf07 subscript \ud835\udefc \ud835\udc59 \ud835\udc41 \ud835\udc52 \ud835\udc54 \ud835\udc4e \ud835\udc61 \ud835\udc56 \ud835\udc63 \ud835\udc52 \ud835\udc43 \ud835\udc4e \ud835\udc5f \ud835\udc61 \\displaystyle+\\underbrace{\\sum_{l:\\alpha_{l}<0}(|\\alpha_{l}|+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\odot\\mathbf{1}^{\\pm}\\right)\\mu(|\\alpha_{l}|)}_{NegativePart}\n\nRecall that we defined the sequence to be the eigenvalue and eigenvector pairs for the Hankel matrix . For any we define the projection of on the top eigenvectors as , i.e. . Further define STU parameters as follows\n\nM 1 u = C \u200b B + D , M 2 u = C \u200b A \u200b B , M 3 u = \u2212 D formulae-sequence superscript subscript \ud835\udc40 1 \ud835\udc62 \ud835\udc36 \ud835\udc35 \ud835\udc37 formulae-sequence superscript subscript \ud835\udc40 2 \ud835\udc62 \ud835\udc36 \ud835\udc34 \ud835\udc35 superscript subscript \ud835\udc40 3 \ud835\udc62 \ud835\udc37 M_{1}^{u}=CB+D,M_{2}^{u}=CAB,M_{3}^{u}=-D\n\nM k \u03d5 + = \u2211 l : \u03b1 l \u2265 0 ( \u03b1 l + 1 ) \u200b ( \u03bc \u200b ( \u03b1 l ) \u22a4 \u200b \u03d5 k ) \u200b \u03c3 k \u2212 1 / 4 \u200b ( c l \u2297 b l ) superscript subscript \ud835\udc40 \ud835\udc58 limit-from italic-\u03d5 subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 \ud835\udf07 superscript subscript \ud835\udefc \ud835\udc59 top subscript italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 M_{k}^{\\phi+}=\\sum_{l:\\alpha_{l}\\geq 0}(\\alpha_{l}+1)(\\mu(\\alpha_{l})^{\\top}\\phi_{k})\\sigma_{k}^{-1/4}(c_{l}\\otimes b_{l})\n\nM k \u03d5 \u2212 = \u2211 l : \u03b1 l < 0 ( | \u03b1 l | + 1 ) \u200b ( \u03bc \u200b ( | \u03b1 l | ) \u22a4 \u200b \u03d5 k ) \u200b \u03c3 k \u2212 1 / 4 \u200b ( c l \u2297 b l ) superscript subscript \ud835\udc40 \ud835\udc58 limit-from italic-\u03d5 subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 \ud835\udf07 superscript subscript \ud835\udefc \ud835\udc59 top subscript italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 M_{k}^{\\phi-}=\\sum_{l:\\alpha_{l}<0}(|\\alpha_{l}|+1)(\\mu(|\\alpha_{l}|)^{\\top}\\phi_{k})\\sigma_{k}^{-1/4}(c_{l}\\otimes b_{l})\n\nThe definition of STU prediction (4), using the above parameters, implies that the predicted sequence satisfies\n\ny t STU \u2212 y t \u2212 2 STU = ( C \u200b B + D ) \u200b u t + C \u200b A \u200b B \u200b u t \u2212 1 \u2212 D \u200b u t \u2212 2 superscript subscript \ud835\udc66 \ud835\udc61 STU superscript subscript \ud835\udc66 \ud835\udc61 2 STU \ud835\udc36 \ud835\udc35 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 2 \\displaystyle y_{t}^{\\mathrm{STU}}-y_{t-2}^{\\mathrm{STU}}=(CB+D)u_{t}+CABu_{t-1}-Du_{t-2} + \u2211 l : \u03b1 l \u2265 0 ( \u03b1 l + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } ) \u200b ( \u2211 k = 1 K ( \u03bc \u200b ( \u03b1 l ) \u22a4 \u200b \u03d5 k ) \u200b \u03d5 k \u23df = \u03bc ~ \u200b ( \u03b1 ) ) subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e \ud835\udf07 superscript subscript \ud835\udefc \ud835\udc59 top subscript italic-\u03d5 \ud835\udc58 subscript italic-\u03d5 \ud835\udc58 absent ~ \ud835\udf07 \ud835\udefc \\displaystyle+\\sum_{l:\\alpha_{l}\\geq 0}(\\alpha_{l}+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\right)\\left(\\underbrace{\\sum_{k=1}^{K}(\\mu(\\alpha_{l})^{\\top}\\phi_{k})\\phi_{k}}_{=\\tilde{\\mu}(\\alpha)}\\right) + \u2211 l : \u03b1 l < 0 ( | \u03b1 l | + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } \u2299 \ud835\udfcf \u00b1 ) \u200b ( \u2211 k = 1 K ( \u03bc \u200b ( | \u03b1 l | ) \u22a4 \u200b \u03d5 k ) \u200b \u03d5 k \u23df = \u03bc ~ \u200b ( | \u03b1 l | ) ) subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 direct-product subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f superscript 1 plus-or-minus subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e \ud835\udf07 superscript subscript \ud835\udefc \ud835\udc59 top subscript italic-\u03d5 \ud835\udc58 subscript italic-\u03d5 \ud835\udc58 absent ~ \ud835\udf07 subscript \ud835\udefc \ud835\udc59 \\displaystyle+\\sum_{l:\\alpha_{l}<0}(|\\alpha_{l}|+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\odot\\mathbf{1}^{\\pm}\\right)\\left(\\underbrace{\\sum_{k=1}^{K}(\\mu(|\\alpha_{l}|)^{\\top}\\phi_{k})\\phi_{k}}_{=\\tilde{\\mu}(|\\alpha_{l}|)}\\right)\n\nCombining the above displays we get that\n\ny t LDS \u2212 y t STU = y t \u2212 2 LDS \u2212 y t \u2212 2 STU superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 STU superscript subscript \ud835\udc66 \ud835\udc61 2 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 STU \\displaystyle y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}=y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}} + \u2211 l : \u03b1 l \u2265 0 ( \u03b1 l + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } ) \u200b ( \u03bc \u200b ( \u03b1 ) \u2212 \u03bc ~ \u200b ( \u03b1 ) ) subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f \ud835\udf07 \ud835\udefc ~ \ud835\udf07 \ud835\udefc \\displaystyle+\\sum_{l:\\alpha_{l}\\geq 0}(\\alpha_{l}+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\right)\\left(\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\right) + \u2211 l : \u03b1 l < 0 ( | \u03b1 l | + 1 ) \u200b ( c l \u2297 b l ) \u200b ( U \u00af { t \u2212 2 : t \u2212 1 \u2212 L } \u2299 \ud835\udfcf \u00b1 ) \u200b ( \u03bc \u200b ( | \u03b1 l | ) \u2212 \u03bc ~ \u200b ( | \u03b1 l | ) ) subscript : \ud835\udc59 subscript \ud835\udefc \ud835\udc59 0 subscript \ud835\udefc \ud835\udc59 1 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 direct-product subscript \u00af \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f superscript 1 plus-or-minus \ud835\udf07 subscript \ud835\udefc \ud835\udc59 ~ \ud835\udf07 subscript \ud835\udefc \ud835\udc59 \\displaystyle+\\sum_{l:\\alpha_{l}<0}(|\\alpha_{l}|+1)(c_{l}\\otimes b_{l})\\left(\\bar{U}_{\\{t-2:t-1-L\\}}\\odot\\mathbf{1}^{\\pm}\\right)\\left(\\mu(|\\alpha_{l}|)-\\tilde{\\mu}(|\\alpha_{l}|)\\right)\n\nUsing triangle inequality, the fact that the input and the system are bounded and Lemma A.3 we get that there exists a universal constant such that\n\n\u2016 y t LDS \u2212 y t STU \u2016 \u2264 \u2016 y t \u2212 2 LDS \u2212 y t \u2212 2 STU \u2016 + c \u22c5 L 2 \u200b \u2211 i = K + 1 L \u03c3 i norm superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 STU norm superscript subscript \ud835\udc66 \ud835\udc61 2 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 STU \u22c5 \ud835\udc50 superscript \ud835\udc3f 2 superscript subscript \ud835\udc56 \ud835\udc3e 1 \ud835\udc3f subscript \ud835\udf0e \ud835\udc56 \\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\|\\leq\\|y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}}\\|+c\\cdot L^{2}\\sqrt{\\sum_{i=K+1}^{L}\\sigma_{i}}\n\nApplying the above equation recursively and Lemma A.4 we get that there exists a universal constant such that\n\n\u2016 y t LDS \u2212 y t STU \u2016 \u2264 O \u200b ( L 3 \u200b \u2211 i = K + 1 L \u03c3 i ) \u2264 O \u200b ( L 3.5 \u22c5 c \u2212 K / log \u2061 ( L ) ) , norm superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 STU \ud835\udc42 superscript \ud835\udc3f 3 superscript subscript \ud835\udc56 \ud835\udc3e 1 \ud835\udc3f subscript \ud835\udf0e \ud835\udc56 \ud835\udc42 \u22c5 superscript \ud835\udc3f 3.5 superscript \ud835\udc50 \ud835\udc3e \ud835\udc3f \\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\|\\leq O\\left(L^{3}\\sqrt{\\sum_{i=K+1}^{L}\\sigma_{i}}\\right)\\leq O\\left(L^{3.5}\\cdot c^{-K/\\log(L)}\\right),\n\nwhich finishes the proof of the theorem.",
    "spectralssm-16": "A.1 Proofs of Lemmas\n\nProof of Lemma A.1. The lemma follows from the following simple calculations. Z \u200b ( i , j ) = \u222b 0 1 ( \u03b1 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 = \u222b 0 1 ( \u03b1 i + j + \u03b1 i + j \u2212 2 \u2212 2 \u200b \u03b1 i + j \u2212 1 ) \u200b \ud835\udc51 \u03b1 \ud835\udc4d \ud835\udc56 \ud835\udc57 superscript subscript 0 1 superscript \ud835\udefc 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc superscript subscript 0 1 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 1 differential-d \ud835\udefc \\displaystyle Z(i,j)=\\int_{0}^{1}(\\alpha-1)^{2}\\alpha^{i+j-2}d\\alpha=\\int_{0}^{1}\\left(\\alpha^{i+j}+\\alpha^{i+j-2}-2\\alpha^{i+j-1}\\right)d\\alpha = 1 ( i + j + 1 ) + 1 ( i + j \u2212 1 ) \u2212 2 ( i + j ) absent 1 \ud835\udc56 \ud835\udc57 1 1 \ud835\udc56 \ud835\udc57 1 2 \ud835\udc56 \ud835\udc57 \\displaystyle=\\frac{1}{(i+j+1)}+\\frac{1}{(i+j-1)}-\\frac{2}{(i+j)} = 2 ( i + j ) 3 \u2212 ( i + j ) absent 2 superscript \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 \\displaystyle=\\frac{2}{(i+j)^{3}-(i+j)}\n\nLemma A.3 is immediate from the second part of Lemma A.2. We show Lemma A.2 below. Proof of Lemma A.2. By definition for . Otherwise we have that for all ,\n\n| \u03bc \u200b ( \u03b1 ) | 2 = \u2211 i = 1 T ( \u03b1 \u2212 1 ) 2 \u200b \u03b1 2 \u200b i \u2212 2 \u2264 ( \u03b1 \u2212 1 ) 2 ( 1 \u2212 \u03b1 2 ) \u2264 1 \u2212 \u03b1 1 + \u03b1 \u2264 1 \u2212 \u03b1 superscript \ud835\udf07 \ud835\udefc 2 superscript subscript \ud835\udc56 1 \ud835\udc47 superscript \ud835\udefc 1 2 superscript \ud835\udefc 2 \ud835\udc56 2 superscript \ud835\udefc 1 2 1 superscript \ud835\udefc 2 1 \ud835\udefc 1 \ud835\udefc 1 \ud835\udefc |\\mu(\\alpha)|^{2}=\\sum_{i=1}^{T}(\\alpha-1)^{2}\\alpha^{2i-2}\\leq\\frac{(\\alpha-1)^{2}}{(1-\\alpha^{2})}\\leq\\frac{1-\\alpha}{1+\\alpha}\\leq 1-\\alpha\n\nTo prove the second part we consider drawing from the uniform distribution between . We get that\n\nE \u200b [ ( \u03bc \u200b ( \u03b1 ) \u22a4 \u200b v ) 2 ] = v \u22a4 \u200b Z \u200b v \ud835\udc38 delimited-[] superscript \ud835\udf07 superscript \ud835\udefc top \ud835\udc63 2 superscript \ud835\udc63 top \ud835\udc4d \ud835\udc63 E[(\\mu(\\alpha)^{\\top}v)^{2}]=v^{\\top}Zv\n\nWe now show that the worst case value is not significantly larger than the expectation. To this end we consider the function and we show that this is a 6-Lipschitz function. To this end consider the following,\n\n\u2016 \u2202 \u03bc \u200b ( \u03b1 ) \u2202 \u03b1 \u2016 2 2 superscript subscript norm \ud835\udf07 \ud835\udefc \ud835\udefc 2 2 \\displaystyle\\left\\|\\frac{\\partial\\mu(\\alpha)}{\\partial\\alpha}\\right\\|_{2}^{2} = \u2211 i = 0 T \u2212 1 { | \u2202 \u2202 \u03b1 \u200b ( 1 \u2212 \u03b1 ) \u200b \u03b1 i | 2 } absent superscript subscript \ud835\udc56 0 \ud835\udc47 1 superscript \ud835\udefc 1 \ud835\udefc superscript \ud835\udefc \ud835\udc56 2 \\displaystyle=\\sum_{i=0}^{T-1}\\left\\{\\left|\\frac{\\partial}{\\partial{\\alpha}}(1-\\alpha)\\alpha^{i}\\right|^{2}\\right\\} = \u2211 i = 0 T \u2212 1 ( ( 1 \u2212 \u03b1 ) \u200b i \u200b \u03b1 i \u2212 1 \u2212 \u03b1 i ) 2 absent superscript subscript \ud835\udc56 0 \ud835\udc47 1 superscript 1 \ud835\udefc \ud835\udc56 superscript \ud835\udefc \ud835\udc56 1 superscript \ud835\udefc \ud835\udc56 2 \\displaystyle=\\sum_{i=0}^{T-1}\\left((1-\\alpha)i\\alpha^{i-1}-\\alpha^{i}\\right)^{2} \u2264 2 \u200b ( 1 \u2212 \u03b1 ) 2 \u200b \u2211 i = 1 T \u2212 1 i 2 \u200b \u03b1 2 \u200b ( i \u2212 1 ) + 2 \u200b \u2211 i = 0 T \u2212 1 \u03b1 2 \u200b i absent 2 superscript 1 \ud835\udefc 2 superscript subscript \ud835\udc56 1 \ud835\udc47 1 superscript \ud835\udc56 2 superscript \ud835\udefc 2 \ud835\udc56 1 2 superscript subscript \ud835\udc56 0 \ud835\udc47 1 superscript \ud835\udefc 2 \ud835\udc56 \\displaystyle\\leq 2(1-\\alpha)^{2}\\sum_{i=1}^{T-1}i^{2}\\alpha^{2(i-1)}+2\\sum_{i=0}^{T-1}\\alpha^{2i} ( a + b ) 2 \u2264 2 \u200b ( a 2 + b 2 ) superscript \ud835\udc4e \ud835\udc4f 2 2 superscript \ud835\udc4e 2 superscript \ud835\udc4f 2 \\displaystyle(a+b)^{2}\\leq 2(a^{2}+b^{2}) \u2264 2 \u200b ( 1 \u2212 \u03b1 ) 2 \u200b ( 1 ( 1 \u2212 \u03b1 2 ) 2 + 2 \u200b \u03b1 2 ( 1 \u2212 \u03b1 2 ) 3 ) + 2 1 \u2212 \u03b1 2 absent 2 superscript 1 \ud835\udefc 2 1 superscript 1 superscript \ud835\udefc 2 2 2 superscript \ud835\udefc 2 superscript 1 superscript \ud835\udefc 2 3 2 1 superscript \ud835\udefc 2 \\displaystyle\\leq 2(1-\\alpha)^{2}\\left(\\frac{1}{(1-\\alpha^{2})^{2}}+\\frac{2\\alpha^{2}}{(1-\\alpha^{2})^{3}}\\right)+\\frac{2}{1-\\alpha^{2}} \u2211 i = 1 \u221e i 2 \u200b \u03b2 i \u2212 1 = 1 ( 1 \u2212 \u03b2 ) 2 + 2 \u200b \u03b2 ( 1 \u2212 \u03b2 ) 3 superscript subscript \ud835\udc56 1 superscript \ud835\udc56 2 superscript \ud835\udefd \ud835\udc56 1 1 superscript 1 \ud835\udefd 2 2 \ud835\udefd superscript 1 \ud835\udefd 3 \\displaystyle\\sum_{i=1}^{\\infty}i^{2}\\beta^{i-1}=\\frac{1}{(1-\\beta)^{2}}+\\frac{2\\beta}{(1-\\beta)^{3}} = 2 ( 1 + \u03b1 ) 2 + 4 \u200b \u03b1 2 ( 1 \u2212 \u03b1 2 ) \u200b ( 1 + \u03b1 ) 2 + 2 1 \u2212 \u03b1 2 . absent 2 superscript 1 \ud835\udefc 2 4 superscript \ud835\udefc 2 1 superscript \ud835\udefc 2 superscript 1 \ud835\udefc 2 2 1 superscript \ud835\udefc 2 \\displaystyle=\\frac{2}{(1+\\alpha)^{2}}+\\frac{4\\alpha^{2}}{(1-\\alpha^{2})(1+\\alpha)^{2}}+\\frac{2}{1-\\alpha^{2}}. Therefore we have that for all ,\n\n\u2202 f \u200b ( \u03b1 ) \u2202 \u03b1 \ud835\udc53 \ud835\udefc \ud835\udefc \\displaystyle\\frac{\\partial f(\\alpha)}{\\partial\\alpha} = 2 \u200b ( \u03bc \u200b ( \u03b1 ) \u22a4 \u200b v ) \u200b ( \u2202 \u03bc \u200b ( \u03b1 ) \u22a4 \u2202 \u03b1 \u200b v ) \u2264 2 \u200b \u2016 \u03bc \u200b ( \u03b1 ) \u2016 \u200b \u2016 v \u2016 2 \u200b \u2016 \u2202 \u03bc \u200b ( \u03b1 ) \u2202 \u03b1 \u2016 absent 2 \ud835\udf07 superscript \ud835\udefc top \ud835\udc63 \ud835\udf07 superscript \ud835\udefc top \ud835\udefc \ud835\udc63 2 norm \ud835\udf07 \ud835\udefc superscript norm \ud835\udc63 2 norm \ud835\udf07 \ud835\udefc \ud835\udefc \\displaystyle=2(\\mu(\\alpha)^{\\top}v)\\left(\\frac{\\partial\\mu(\\alpha)^{\\top}}{\\partial\\alpha}v\\right)\\leq 2\\|\\mu(\\alpha)\\|\\|v\\|^{2}\\|\\frac{\\partial\\mu(\\alpha)}{\\partial\\alpha}\\| \u2264 2 \u200b ( 1 \u2212 \u03b1 ) \u2217 ( 2 ( 1 + \u03b1 ) 2 + 4 \u200b \u03b1 2 ( 1 \u2212 \u03b1 2 ) \u200b ( 1 + \u03b1 ) 2 + 2 1 \u2212 \u03b1 2 ) absent 2 1 \ud835\udefc 2 superscript 1 \ud835\udefc 2 4 superscript \ud835\udefc 2 1 superscript \ud835\udefc 2 superscript 1 \ud835\udefc 2 2 1 superscript \ud835\udefc 2 \\displaystyle\\leq 2\\sqrt{(1-\\alpha)*\\left(\\frac{2}{(1+\\alpha)^{2}}+\\frac{4\\alpha^{2}}{(1-\\alpha^{2})(1+\\alpha)^{2}}+\\frac{2}{1-\\alpha^{2}}\\right)} \u2264 2 \u200b ( 2 \u200b ( 1 \u2212 \u03b1 ) ( 1 + \u03b1 ) 2 + 4 \u200b \u03b1 2 ( 1 + \u03b1 ) 3 + 2 1 + \u03b1 ) \u2264 6 . absent 2 2 1 \ud835\udefc superscript 1 \ud835\udefc 2 4 superscript \ud835\udefc 2 superscript 1 \ud835\udefc 3 2 1 \ud835\udefc 6 \\displaystyle\\leq 2\\sqrt{\\left(\\frac{2(1-\\alpha)}{(1+\\alpha)^{2}}+\\frac{4\\alpha^{2}}{(1+\\alpha)^{3}}+\\frac{2}{1+\\alpha}\\right)}\\leq 6. Now for the positive function which is -Lipschitz on let the maximum value be . It can be seen the lowest expected value of over the uniform distribution over , one can achieve is and therefore we have that\n\nR 2 / 12 \u2264 v \u22a4 \u200b Z \u200b v \u21d2 R \u2264 12 \u200b v \u22a4 \u200b H \u200b v , superscript \ud835\udc45 2 12 superscript \ud835\udc63 top \ud835\udc4d \ud835\udc63 \u21d2 \ud835\udc45 12 superscript \ud835\udc63 top \ud835\udc3b \ud835\udc63 R^{2}/12\\leq v^{\\top}Zv\\Rightarrow R\\leq\\sqrt{12v^{\\top}Hv},\n\nwhich finishes the proof. Appendix B Alternative Representation for capturing negative eigenvalues\n\nIn this section we setup an alternative version of STU wherein a different Hankel matrix is used but one can get a similar result. As before a single layer of STU (depicted in figure 2) is parameterized by a number , denoting the number of eigenfactors and matrices and . The matrices form the params of the layer. We use a different Hankel matrix whose entries are given by\n\nZ L \u200b [ i , j ] \u225c ( ( \u2212 1 ) i + j \u2212 2 + 1 ) \u22c5 8 ( i + j + 3 ) \u200b ( i + j \u2212 1 ) \u200b ( i + j + 1 ) . \u225c subscript \ud835\udc4d \ud835\udc3f \ud835\udc56 \ud835\udc57 \u22c5 superscript 1 \ud835\udc56 \ud835\udc57 2 1 8 \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 1 \ud835\udc56 \ud835\udc57 1 Z_{L}[i,j]\\triangleq((-1)^{i+j-2}+1)\\cdot\\frac{8}{(i+j+3)(i+j-1)(i+j+1)}. (7)\n\nand let be the eigenvalue-eigenvector pairs of ordered to satisfy . Given an input sequence , as before we first featurize the input sequence by projecting the input sequence till time on fixed filters . The main difference is that we do not need to create a negative featurization now. We define\n\nX t , k = \u2211 i = 0 t \u2212 1 u t \u2212 i \u22c5 \u03d5 k \u200b ( i ) . subscript \ud835\udc4b \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udc56 0 \ud835\udc61 1 \u22c5 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 subscript italic-\u03d5 \ud835\udc58 \ud835\udc56 X_{t,k}=\\sum_{i=0}^{t-1}u_{t-i}\\cdot\\phi_{k}(i). Note that for every , the sequence of features can be computed efficiently via convolution. The output sequence is then given by\n\ny ^ t = y ^ t \u2212 2 + \u2211 i = 1 3 M i u \u200b u t + 1 \u2212 i \u23df Auto \u2212 regressive \u200b Component + \u2211 k = 1 K M k \u03d5 \u200b \u03c3 k 1 / 4 \u200b X t \u2212 2 , k \u23df Spectral \u200b Component . subscript ^ \ud835\udc66 \ud835\udc61 subscript \u23df subscript ^ \ud835\udc66 \ud835\udc61 2 superscript subscript \ud835\udc56 1 3 subscript superscript \ud835\udc40 \ud835\udc62 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc56 Auto regressive Component subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e subscript superscript \ud835\udc40 italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 subscript \ud835\udc4b \ud835\udc61 2 \ud835\udc58 Spectral Component \\hat{y}_{t}=\\underbrace{\\vrule width=0.0pt,height=0.0pt,depth=13.77771pt\\hat{y}_{t-2}+\\sum_{i=1}^{3}M^{u}_{i}u_{t+1-i}}_{\\mathrm{Auto-regressive\\;Component}}+\\underbrace{\\sum_{k=1}^{K}M^{\\phi}_{k}\\sigma_{k}^{1/4}X_{t-2,k}}_{\\mathrm{Spectral\\;Component}}. (8)\n\nWe prove the following representation theorem which shows that the above class approximately contains any marginally-stable LDS with symmetric . Theorem B.1. Given a bounded input sequence and given any bounded matrices with being a symmetric matrix let be the sequence generated by execution of the LDS given by (via (2)). There exists matrices such that the outputs generated by Spectral Filtering (via (4)) satisfies for all ,\n\n\u2016 y t LDS \u2212 y t SF \u2016 2 \u2264 O \u200b ( L 3.5 \u22c5 c \u2212 K / log \u2061 ( L ) ) superscript norm subscript superscript \ud835\udc66 LDS \ud835\udc61 subscript superscript \ud835\udc66 SF \ud835\udc61 2 \ud835\udc42 \u22c5 superscript \ud835\udc3f 3.5 superscript \ud835\udc50 \ud835\udc3e \ud835\udc3f \\|y^{\\mathrm{LDS}}_{t}-y^{\\mathrm{SF}}_{t}\\|^{2}\\leq O(L^{3.5}\\cdot c^{-K/\\log(L)})\n\nwhere c is a universal constant.",
    "spectralssm-17": "In the following we prove the above theorem. B.1 Proof of Theorem B.1\n\nWithout loss of generality we assume that is a real-diagonal matrix. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any , , with . Further define the Hankel matrix as\n\nZ \u225c \u222b \u2212 1 1 \u03bc \u200b ( \u03b1 ) \u200b \u03bc \u200b ( \u03b1 ) \u22a4 \u200b \ud835\udc51 \u03b1 \u225c \ud835\udc4d superscript subscript 1 1 \ud835\udf07 \ud835\udefc \ud835\udf07 superscript \ud835\udefc top differential-d \ud835\udefc Z\\triangleq\\int_{-1}^{1}\\mu(\\alpha)\\mu(\\alpha)^{\\top}d\\alpha\n\nAs the following lemma shows the Hankel matrix above is the same Hankel matrix defined in the definition of STU (7). Lemma B.2. is a Hankel matrix with entries given as\n\nZ \u200b ( i , j ) = ( ( \u2212 1 ) i + j \u2212 2 + 1 ) \u22c5 8 ( i + j + 3 ) \u200b ( i + j \u2212 1 ) \u200b ( i + j + 1 ) \ud835\udc4d \ud835\udc56 \ud835\udc57 \u22c5 superscript 1 \ud835\udc56 \ud835\udc57 2 1 8 \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 1 \ud835\udc56 \ud835\udc57 1 Z(i,j)=((-1)^{i+j-2}+1)\\cdot\\frac{8}{(i+j+3)(i+j-1)(i+j+1)}\n\nProof. Consider the following simple computations\n\nH \u200b ( i , j ) \ud835\udc3b \ud835\udc56 \ud835\udc57 \\displaystyle H(i,j) = \u222b \u2212 1 1 ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 absent superscript subscript 1 1 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc \\displaystyle=\\int_{-1}^{1}(\\alpha^{2}-1)^{2}\\alpha^{i+j-2}d\\alpha = \u222b \u2212 1 0 ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 + \u222b 0 1 ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 absent superscript subscript 1 0 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc superscript subscript 0 1 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc \\displaystyle=\\int_{-1}^{0}(\\alpha^{2}-1)^{2}\\alpha^{i+j-2}d\\alpha+\\int_{0}^{1}(\\alpha^{2}-1)^{2}\\alpha^{i+j-2}d\\alpha = \u222b \u2212 1 0 ( | \u03b1 | 2 \u2212 1 ) 2 \u200b ( \u2212 1 ) i + j \u2212 2 \u200b | \u03b1 | i + j \u2212 2 \u200b \ud835\udc51 \u03b1 + \u222b 0 1 ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 absent superscript subscript 1 0 superscript superscript \ud835\udefc 2 1 2 superscript 1 \ud835\udc56 \ud835\udc57 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc superscript subscript 0 1 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc \\displaystyle=\\int_{-1}^{0}(|\\alpha|^{2}-1)^{2}(-1)^{i+j-2}|\\alpha|^{i+j-2}d\\alpha+\\int_{0}^{1}(\\alpha^{2}-1)^{2}\\alpha^{i+j-2}d\\alpha = \u222b 0 1 ( \u03b1 2 \u2212 1 ) 2 \u200b ( \u2212 1 ) i + j \u2212 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 + \u222b 0 1 ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 absent superscript subscript 0 1 superscript superscript \ud835\udefc 2 1 2 superscript 1 \ud835\udc56 \ud835\udc57 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc superscript subscript 0 1 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc \\displaystyle=\\int_{0}^{1}(\\alpha^{2}-1)^{2}(-1)^{i+j-2}\\alpha^{i+j-2}d\\alpha+\\int_{0}^{1}(\\alpha^{2}-1)^{2}\\alpha^{i+j-2}d\\alpha = ( ( \u2212 1 ) i + j \u2212 2 + 1 ) \u200b \u222b 0 1 ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 i + j \u2212 2 \u200b \ud835\udc51 \u03b1 absent superscript 1 \ud835\udc56 \ud835\udc57 2 1 superscript subscript 0 1 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc \ud835\udc56 \ud835\udc57 2 differential-d \ud835\udefc \\displaystyle=((-1)^{i+j-2}+1)\\int_{0}^{1}(\\alpha^{2}-1)^{2}\\alpha^{i+j-2}d\\alpha = ( ( \u2212 1 ) i + j \u2212 2 + 1 ) \u22c5 8 ( i + j + 3 ) \u200b ( i + j \u2212 1 ) \u200b ( i + j + 1 ) absent \u22c5 superscript 1 \ud835\udc56 \ud835\udc57 2 1 8 \ud835\udc56 \ud835\udc57 3 \ud835\udc56 \ud835\udc57 1 \ud835\udc56 \ud835\udc57 1 \\displaystyle=((-1)^{i+j-2}+1)\\cdot\\frac{8}{(i+j+3)(i+j-1)(i+j+1)}\n\nLemma B.3. We have that the following statements hold regarding for any ,\n\n\u2022\n\n\u2022\n\nFor any and any unit vector we have that\n\n( \u03bc \u200b ( \u03b1 ) \u22a4 \u200b v ) 2 \u2264 6 \u200b ( v \u22a4 \u200b Z \u200b v ) superscript \ud835\udf07 superscript \ud835\udefc top \ud835\udc63 2 6 superscript \ud835\udc63 top \ud835\udc4d \ud835\udc63 (\\mu(\\alpha)^{\\top}v)^{2}\\leq 6(v^{\\top}Zv)\n\nProof. By definition for . Otherwise we have that for all ,\n\n| \u03bc \u200b ( \u03b1 ) | 2 = \u2211 i = 1 T ( \u03b1 2 \u2212 1 ) 2 \u200b \u03b1 2 \u200b i \u2212 2 \u2264 ( \u03b1 2 \u2212 1 ) 2 ( 1 \u2212 \u03b1 2 ) = 1 \u2212 \u03b1 2 \u2264 1 . superscript \ud835\udf07 \ud835\udefc 2 superscript subscript \ud835\udc56 1 \ud835\udc47 superscript superscript \ud835\udefc 2 1 2 superscript \ud835\udefc 2 \ud835\udc56 2 superscript superscript \ud835\udefc 2 1 2 1 superscript \ud835\udefc 2 1 superscript \ud835\udefc 2 1 |\\mu(\\alpha)|^{2}=\\sum_{i=1}^{T}(\\alpha^{2}-1)^{2}\\alpha^{2i-2}\\leq\\frac{(\\alpha^{2}-1)^{2}}{(1-\\alpha^{2})}=1-\\alpha^{2}\\leq 1. To prove the second part we consider drawing from the uniform distribution between . We get that\n\nE \u200b [ ( \u03bc \u200b ( \u03b1 ) \u22a4 \u200b v ) 2 ] = v \u22a4 \u200b Z \u200b v 2 \ud835\udc38 delimited-[] superscript \ud835\udf07 superscript \ud835\udefc top \ud835\udc63 2 superscript \ud835\udc63 top \ud835\udc4d \ud835\udc63 2 E[(\\mu(\\alpha)^{\\top}v)^{2}]=\\frac{v^{\\top}Zv}{2}\n\nWe now show that the worst case value is not significantly larger than the expectation. To this end we consider the function and we show that this is a 6-Lipschitz function. To this end consider the following,\n\n\u2016 \u2202 \u03bc \u200b ( \u03b1 ) \u2202 \u03b1 \u2016 2 2 superscript subscript norm \ud835\udf07 \ud835\udefc \ud835\udefc 2 2 \\displaystyle\\left\\|\\frac{\\partial\\mu(\\alpha)}{\\partial\\alpha}\\right\\|_{2}^{2} = \u2211 i = 0 T \u2212 1 { | \u2202 \u2202 \u03b1 \u200b ( 1 \u2212 \u03b1 2 ) \u200b \u03b1 i | 2 } absent superscript subscript \ud835\udc56 0 \ud835\udc47 1 superscript \ud835\udefc 1 superscript \ud835\udefc 2 superscript \ud835\udefc \ud835\udc56 2 \\displaystyle=\\sum_{i=0}^{T-1}\\left\\{\\left|\\frac{\\partial}{\\partial{\\alpha}}(1-\\alpha^{2})\\alpha^{i}\\right|^{2}\\right\\} = \u2211 i = 0 T \u2212 1 ( ( 1 \u2212 \u03b1 2 ) \u200b i \u200b \u03b1 i \u2212 1 \u2212 2 \u200b \u03b1 i + 1 ) 2 absent superscript subscript \ud835\udc56 0 \ud835\udc47 1 superscript 1 superscript \ud835\udefc 2 \ud835\udc56 superscript \ud835\udefc \ud835\udc56 1 2 superscript \ud835\udefc \ud835\udc56 1 2 \\displaystyle=\\sum_{i=0}^{T-1}\\left((1-\\alpha^{2})i\\alpha^{i-1}-2\\alpha^{i+1}\\right)^{2} \u2264 2 \u200b ( 1 \u2212 \u03b1 2 ) 2 \u200b \u2211 i = 1 T \u2212 1 i 2 \u200b \u03b1 2 \u200b ( i \u2212 1 ) + 4 \u200b \u2211 i = 0 T \u2212 1 \u03b1 2 \u200b i + 2 absent 2 superscript 1 superscript \ud835\udefc 2 2 superscript subscript \ud835\udc56 1 \ud835\udc47 1 superscript \ud835\udc56 2 superscript \ud835\udefc 2 \ud835\udc56 1 4 superscript subscript \ud835\udc56 0 \ud835\udc47 1 superscript \ud835\udefc 2 \ud835\udc56 2 \\displaystyle\\leq 2(1-\\alpha^{2})^{2}\\sum_{i=1}^{T-1}i^{2}\\alpha^{2(i-1)}+4\\sum_{i=0}^{T-1}\\alpha^{2i+2} ( a + b ) 2 \u2264 2 \u200b ( a 2 + b 2 ) superscript \ud835\udc4e \ud835\udc4f 2 2 superscript \ud835\udc4e 2 superscript \ud835\udc4f 2 \\displaystyle(a+b)^{2}\\leq 2(a^{2}+b^{2}) \u2264 2 \u200b ( 1 \u2212 \u03b1 2 ) 2 \u200b ( 1 ( 1 \u2212 \u03b1 2 ) 2 + 2 \u200b \u03b1 2 ( 1 \u2212 \u03b1 2 ) 3 ) + 4 \u200b \u03b1 2 1 \u2212 \u03b1 2 absent 2 superscript 1 superscript \ud835\udefc 2 2 1 superscript 1 superscript \ud835\udefc 2 2 2 superscript \ud835\udefc 2 superscript 1 superscript \ud835\udefc 2 3 4 superscript \ud835\udefc 2 1 superscript \ud835\udefc 2 \\displaystyle\\leq 2(1-\\alpha^{2})^{2}\\left(\\frac{1}{(1-\\alpha^{2})^{2}}+\\frac{2\\alpha^{2}}{(1-\\alpha^{2})^{3}}\\right)+\\frac{4\\alpha^{2}}{1-\\alpha^{2}} \u2211 i = 1 \u221e i 2 \u200b \u03b2 i \u2212 1 = 1 ( 1 \u2212 \u03b2 ) 2 + 2 \u200b \u03b2 ( 1 \u2212 \u03b2 ) 3 superscript subscript \ud835\udc56 1 superscript \ud835\udc56 2 superscript \ud835\udefd \ud835\udc56 1 1 superscript 1 \ud835\udefd 2 2 \ud835\udefd superscript 1 \ud835\udefd 3 \\displaystyle\\sum_{i=1}^{\\infty}i^{2}\\beta^{i-1}=\\frac{1}{(1-\\beta)^{2}}+\\frac{2\\beta}{(1-\\beta)^{3}} = 2 + 8 \u200b \u03b1 2 ( 1 \u2212 \u03b1 2 ) . absent 2 8 superscript \ud835\udefc 2 1 superscript \ud835\udefc 2 \\displaystyle=2+\\frac{8\\alpha^{2}}{(1-\\alpha^{2})}. Therefore we have that for all ,\n\n\u2202 f \u200b ( \u03b1 ) \u2202 \u03b1 \ud835\udc53 \ud835\udefc \ud835\udefc \\displaystyle\\frac{\\partial f(\\alpha)}{\\partial\\alpha} = 2 \u200b ( \u03bc \u200b ( \u03b1 ) \u22a4 \u200b v ) \u200b ( \u2202 \u03bc \u200b ( \u03b1 ) \u22a4 \u2202 \u03b1 \u200b v ) \u2264 2 \u200b \u2016 \u03bc \u200b ( \u03b1 ) \u2016 \u200b \u2016 v \u2016 2 \u200b \u2016 \u2202 \u03bc \u200b ( \u03b1 ) \u2202 \u03b1 \u2016 absent 2 \ud835\udf07 superscript \ud835\udefc top \ud835\udc63 \ud835\udf07 superscript \ud835\udefc top \ud835\udefc \ud835\udc63 2 norm \ud835\udf07 \ud835\udefc superscript norm \ud835\udc63 2 norm \ud835\udf07 \ud835\udefc \ud835\udefc \\displaystyle=2(\\mu(\\alpha)^{\\top}v)\\left(\\frac{\\partial\\mu(\\alpha)^{\\top}}{\\partial\\alpha}v\\right)\\leq 2\\|\\mu(\\alpha)\\|\\|v\\|^{2}\\|\\frac{\\partial\\mu(\\alpha)}{\\partial\\alpha}\\| \u2264 2 \u200b ( 1 \u2212 \u03b1 2 ) \u2217 ( 2 + 8 \u200b \u03b1 2 ( 1 \u2212 \u03b1 2 ) ) absent 2 1 superscript \ud835\udefc 2 2 8 superscript \ud835\udefc 2 1 superscript \ud835\udefc 2 \\displaystyle\\leq 2\\sqrt{(1-\\alpha^{2})*\\left(2+\\frac{8\\alpha^{2}}{(1-\\alpha^{2})}\\right)} \u2264 2 \u200b 2 + 6 \u200b \u03b1 2 \u2264 6 . absent 2 2 6 superscript \ud835\udefc 2 6 \\displaystyle\\leq 2\\sqrt{2+6\\alpha^{2}}\\leq 6. Now for the positive function which is -Lipschitz on let the maximum value be . It can be seen the lowest expected value of over the uniform distribution over , one can achieve is and therefore we have that\n\nR 2 / 12 \u2264 v \u22a4 \u200b Z \u200b v 2 \u21d2 R \u2264 6 \u200b v \u22a4 \u200b Z \u200b v , superscript \ud835\udc45 2 12 superscript \ud835\udc63 top \ud835\udc4d \ud835\udc63 2 \u21d2 \ud835\udc45 6 superscript \ud835\udc63 top \ud835\udc4d \ud835\udc63 R^{2}/12\\leq\\frac{v^{\\top}Zv}{2}\\Rightarrow R\\leq\\sqrt{6v^{\\top}Zv},\n\nwhich finishes the proof.",
    "spectralssm-18": "\u220e\n\nA direct consequence of the above lemma is the following. Lemma B.4. For any , let be the projection of on the subspace spanned by top eigenvectors of , then we have that\n\n\u2016 \u03bc \u200b ( \u03b1 ) \u2212 \u03bc ~ \u200b ( \u03b1 ) \u2016 2 \u2264 6 \u200b \u2211 i = k + 1 L \u03c3 i superscript norm \ud835\udf07 \ud835\udefc ~ \ud835\udf07 \ud835\udefc 2 6 superscript subscript \ud835\udc56 \ud835\udc58 1 \ud835\udc3f subscript \ud835\udf0e \ud835\udc56 \\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2}\\leq 6\\sum_{i=k+1}^{L}\\sigma_{i}\n\nFinally the following lemma with a proof similar to A.3 shows that the spectrum of the matrix decays exponentially.",
    "spectralssm-19": "Lemma B.5. Let be the top eigenvalue of . Then we have that\n\n\u03c3 j \u2264 min \u2061 ( 2 , \u0393 \u200b c \u2212 j / log \u2061 ( L ) ) subscript \ud835\udf0e \ud835\udc57 2 \u0393 superscript \ud835\udc50 \ud835\udc57 \ud835\udc3f \\sigma_{j}\\leq\\min\\left(2,\\Gamma c^{-j/\\log(L)}\\right)\n\nwhere and is an absolute constant. We now move towards proving Theorem B.1. Consider the following calculation for the LDS sequence\n\ny t LDS = \u2211 i = 0 T C \u200b A i \u200b B \u200b u t \u2212 i + D \u200b u t , superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc56 0 \ud835\udc47 \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \\displaystyle y_{t}^{\\mathrm{LDS}}=\\sum_{i=0}^{T}CA^{i}Bu_{t-i}+Du_{t},\n\nand therefore we have that\n\ny t LDS \u2212 y t \u2212 2 LDS = ( C \u200b B + D ) \u200b u t + C \u200b A \u200b B \u200b u t \u2212 1 \u2212 D \u200b u t \u2212 2 + \u2211 i = 0 T C \u200b ( A i + 2 \u2212 A i ) \u200b B \u200b u t \u2212 2 \u2212 i \u23df Term \u200b of \u200b Interest superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 LDS \ud835\udc36 \ud835\udc35 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 2 subscript \u23df superscript subscript \ud835\udc56 0 \ud835\udc47 \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 2 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 Term of Interest \\displaystyle y_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}}=(CB+D)u_{t}+CABu_{t-1}-Du_{t-2}+\\underbrace{\\sum_{i=0}^{T}C(A^{i+2}-A^{i})Bu_{t-2-i}}_{\\mathrm{Term\\;of\\;Interest}}\n\nFor any we define the matrix whose column is the input vector .",
    "spectralssm-20": "We allow to be negative and by convention assume for any .",
    "spectralssm-21": "Denote the diagonal entries of by , i.e. . The term of interest above can then be written as\n\n\u2211 i = 0 L C \u200b ( A i + 2 \u2212 A i ) \u200b B \u200b u t \u2212 2 \u2212 i superscript subscript \ud835\udc56 0 \ud835\udc3f \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 2 superscript \ud835\udc34 \ud835\udc56 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle\\sum_{i=0}^{L}C(A^{i+2}-A^{i})Bu_{t-2-i} = \u2211 l = 1 d h ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( \u03b1 l i + 2 \u2212 \u03b1 l i ) \u200b u t \u2212 2 \u2212 i ) absent superscript subscript \ud835\udc59 1 subscript \ud835\udc51 \u210e tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 2 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle=\\sum_{l=1}^{d_{h}}(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(\\alpha_{l}^{i+2}-\\alpha_{l}^{i})u_{t-2-i}\\right) = \u2211 l = 1 d h ( c l \u2297 b l ) \u200b ( \u2211 i = 0 L ( \u03b1 l 2 \u2212 1 ) \u200b \u03b1 l i \u200b u t \u2212 2 \u2212 i ) absent superscript subscript \ud835\udc59 1 subscript \ud835\udc51 \u210e tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 superscript subscript \ud835\udc56 0 \ud835\udc3f superscript subscript \ud835\udefc \ud835\udc59 2 1 superscript subscript \ud835\udefc \ud835\udc59 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 2 \ud835\udc56 \\displaystyle=\\sum_{l=1}^{d_{h}}(c_{l}\\otimes b_{l})\\left(\\sum_{i=0}^{L}(\\alpha_{l}^{2}-1)\\alpha_{l}^{i}u_{t-2-i}\\right) = \u2211 l = 1 d h ( c l \u2297 b l ) \u200b ( U { t \u2212 2 : t \u2212 1 \u2212 L } \u200b \u03bc \u200b ( \u03b1 ) ) . absent superscript subscript \ud835\udc59 1 subscript \ud835\udc51 \u210e tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f \ud835\udf07 \ud835\udefc \\displaystyle=\\sum_{l=1}^{d_{h}}(c_{l}\\otimes b_{l})\\left(U_{\\{t-2:t-1-L\\}}\\mu(\\alpha)\\right). Therefore we get that\n\ny t LDS \u2212 y t \u2212 2 LDS superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 LDS \\displaystyle y_{t}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{LDS}} = ( C \u200b B + D ) \u200b u t + C \u200b A \u200b B \u200b u t \u2212 1 \u2212 D \u200b u t \u2212 2 + \u2211 l = 1 d h ( c l \u2297 b l ) \u200b ( U { t \u2212 2 : t \u2212 1 \u2212 L } \u200b \u03bc \u200b ( \u03b1 ) ) . absent \ud835\udc36 \ud835\udc35 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 2 superscript subscript \ud835\udc59 1 subscript \ud835\udc51 \u210e tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f \ud835\udf07 \ud835\udefc \\displaystyle=(CB+D)u_{t}+CABu_{t-1}-Du_{t-2}+\\sum_{l=1}^{d_{h}}(c_{l}\\otimes b_{l})\\left(U_{\\{t-2:t-1-L\\}}\\mu(\\alpha)\\right). Recall that we defined the sequence to be the eigenvalue and eigenvector pairs for the Hankel matrix . For any we define the projection of on the top eigenvectors as , i.e. . Further define STU parameters as follows\n\nM 1 u = C \u200b B + D , M 2 u = C \u200b A \u200b B , M 3 u = \u2212 D formulae-sequence superscript subscript \ud835\udc40 1 \ud835\udc62 \ud835\udc36 \ud835\udc35 \ud835\udc37 formulae-sequence superscript subscript \ud835\udc40 2 \ud835\udc62 \ud835\udc36 \ud835\udc34 \ud835\udc35 superscript subscript \ud835\udc40 3 \ud835\udc62 \ud835\udc37 M_{1}^{u}=CB+D,M_{2}^{u}=CAB,M_{3}^{u}=-D\n\nM k \u03d5 = \u2211 l ( \u03bc \u200b ( \u03b1 l ) \u22a4 \u200b \u03d5 k ) \u200b \u03c3 k \u2212 1 / 4 \u200b ( c l \u2297 b l ) superscript subscript \ud835\udc40 \ud835\udc58 italic-\u03d5 subscript \ud835\udc59 \ud835\udf07 superscript subscript \ud835\udefc \ud835\udc59 top subscript italic-\u03d5 \ud835\udc58 superscript subscript \ud835\udf0e \ud835\udc58 1 4 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 M_{k}^{\\phi}=\\sum_{l}(\\mu(\\alpha_{l})^{\\top}\\phi_{k})\\sigma_{k}^{-1/4}(c_{l}\\otimes b_{l})\n\nThe definition of STU prediction (using the above parameters) implies that the predicted sequence satisfies\n\ny t STU \u2212 y t \u2212 2 STU = ( C \u200b B + D ) \u200b u t + C \u200b A \u200b B \u200b u t \u2212 1 \u2212 D \u200b u t \u2212 2 superscript subscript \ud835\udc66 \ud835\udc61 STU superscript subscript \ud835\udc66 \ud835\udc61 2 STU \ud835\udc36 \ud835\udc35 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 2 \\displaystyle y_{t}^{\\mathrm{STU}}-y_{t-2}^{\\mathrm{STU}}=(CB+D)u_{t}+CABu_{t-1}-Du_{t-2} + \u2211 l ( c l \u2297 b l ) \u200b ( U { t \u2212 2 : t \u2212 1 \u2212 L } ) \u200b ( \u2211 k = 1 K ( \u03bc \u200b ( \u03b1 l ) \u22a4 \u200b \u03d5 k ) \u200b \u03d5 k \u23df = \u03bc ~ \u200b ( \u03b1 ) ) . subscript \ud835\udc59 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f subscript \u23df superscript subscript \ud835\udc58 1 \ud835\udc3e \ud835\udf07 superscript subscript \ud835\udefc \ud835\udc59 top subscript italic-\u03d5 \ud835\udc58 subscript italic-\u03d5 \ud835\udc58 absent ~ \ud835\udf07 \ud835\udefc \\displaystyle+\\sum_{l}(c_{l}\\otimes b_{l})\\left(U_{\\{t-2:t-1-L\\}}\\right)\\left(\\underbrace{\\sum_{k=1}^{K}(\\mu(\\alpha_{l})^{\\top}\\phi_{k})\\phi_{k}}_{=\\tilde{\\mu}(\\alpha)}\\right). Combining the above displays we get that\n\ny t LDS \u2212 y t STU = y t \u2212 2 LDS \u2212 y t \u2212 2 STU superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 STU superscript subscript \ud835\udc66 \ud835\udc61 2 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 STU \\displaystyle y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}=y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}} + \u2211 l ( c l \u2297 b l ) \u200b ( U { t \u2212 2 : t \u2212 1 \u2212 L } ) \u200b ( \u03bc \u200b ( \u03b1 ) \u2212 \u03bc ~ \u200b ( \u03b1 ) ) . subscript \ud835\udc59 tensor-product subscript \ud835\udc50 \ud835\udc59 subscript \ud835\udc4f \ud835\udc59 subscript \ud835\udc48 conditional-set \ud835\udc61 2 \ud835\udc61 1 \ud835\udc3f \ud835\udf07 \ud835\udefc ~ \ud835\udf07 \ud835\udefc \\displaystyle+\\sum_{l}(c_{l}\\otimes b_{l})\\left(U_{\\{t-2:t-1-L\\}}\\right)\\left(\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\right). Using triangle inequality, the fact that the input and the system are bounded and Lemma B.4 we get that there exists a universal constant such that\n\n\u2016 y t LDS \u2212 y t STU \u2016 \u2264 \u2016 y t \u2212 2 LDS \u2212 y t \u2212 2 STU \u2016 + c \u22c5 L 2 \u200b \u2211 i = K + 1 L \u03c3 i norm superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 STU norm superscript subscript \ud835\udc66 \ud835\udc61 2 LDS superscript subscript \ud835\udc66 \ud835\udc61 2 STU \u22c5 \ud835\udc50 superscript \ud835\udc3f 2 superscript subscript \ud835\udc56 \ud835\udc3e 1 \ud835\udc3f subscript \ud835\udf0e \ud835\udc56 \\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\|\\leq\\|y_{t-2}^{\\mathrm{LDS}}-y_{t-2}^{\\mathrm{STU}}\\|+c\\cdot L^{2}\\sqrt{\\sum_{i=K+1}^{L}\\sigma_{i}}\n\nApplying the above equation recursively and Lemma B.5 we get that there exists a universal constant such that\n\n\u2016 y t LDS \u2212 y t STU \u2016 \u2264 O \u200b ( L 3 \u200b \u2211 i = K + 1 L \u03c3 i ) \u2264 O \u200b ( L 3.5 \u22c5 c \u2212 K / log \u2061 ( L ) ) , norm superscript subscript \ud835\udc66 \ud835\udc61 LDS superscript subscript \ud835\udc66 \ud835\udc61 STU \ud835\udc42 superscript \ud835\udc3f 3 superscript subscript \ud835\udc56 \ud835\udc3e 1 \ud835\udc3f subscript \ud835\udf0e \ud835\udc56 \ud835\udc42 \u22c5 superscript \ud835\udc3f 3.5 superscript \ud835\udc50 \ud835\udc3e \ud835\udc3f \\|y_{t}^{\\mathrm{LDS}}-y_{t}^{\\mathrm{STU}}\\|\\leq O\\left(L^{3}\\sqrt{\\sum_{i=K+1}^{L}\\sigma_{i}}\\right)\\leq O\\left(L^{3.5}\\cdot c^{-K/\\log(L)}\\right),\n\nwhich finishes the proof of the theorem. Appendix C Experiment Details\n\nC.1 Synthetic Experiments with a marginally-stable LDS\n\nThe random system we generated for the experiments displayed in Figure 3 is as follows -\n\nA = [ \u2212 0.9999 0 .",
    "spectralssm-22": "0 . 0 . 0 . 0.9999 0 .",
    "spectralssm-23": "0 . 0 . 0 . \u2212 0.9999 0 .",
    "spectralssm-24": "0 .",
    "spectralssm-25": "0 . 0 . 0.9999 ] , B = [ 0.36858183 \u2212 0.34219486 0.1407376 0.18933886 \u2212 0.1243964 0.21866894 0.14593862 \u2212 0.5791096 \u2212 0.06816235 \u2212 0.3095346 \u2212 0.21441863 0.08696061 ] formulae-sequence \ud835\udc34 matrix 0.9999 0 0 0 0 0.9999 0 0 0 0 0.9999 0 0 0 0 0.9999 \ud835\udc35 matrix 0.36858183 0.34219486 0.1407376 0.18933886 0.1243964 0.21866894 0.14593862 0.5791096 0.06816235 0.3095346 0.21441863 0.08696061 A=\\begin{bmatrix}-0.9999&0.&0.&0.\\\\\n0.&0.9999&0.&0.\\\\\n0.&0.&-0.9999&0.\\\\\n0.&0.&0.&0.9999\\\\\n\\end{bmatrix},\\qquad B=\\begin{bmatrix}0.36858183&-0.34219486&0.1407376\\\\\n0.18933886&-0.1243964&0.21866894\\\\\n0.14593862&-0.5791096&-0.06816235\\\\\n-0.3095346&-0.21441863&0.08696061\\\\\n\\end{bmatrix}\n\nC = [ 0.5528727 \u2212 0.51329225 0.21110639 0.2840083 \u2212 0.18659459 0.3280034 0.21890792 \u2212 0.8686644 \u2212 0.10224352 \u2212 0.46430188 \u2212 0.32162794 0.1304409 ] , D = [ 1.5905786 0 .",
    "spectralssm-26": "0 . 0 . \u2212 0.45901108 0 .",
    "spectralssm-27": "0 . 0 . 0.3238576 ] formulae-sequence \ud835\udc36 matrix 0.5528727 0.51329225 0.21110639 0.2840083 0.18659459 0.3280034 0.21890792 0.8686644 0.10224352 0.46430188 0.32162794 0.1304409 \ud835\udc37 matrix 1.5905786 0 0 0 0.45901108 0 0 0 0.3238576 C=\\begin{bmatrix}0.5528727&-0.51329225&0.21110639&0.2840083\\\\\n-0.18659459&0.3280034&0.21890792&-0.8686644\\\\\n-0.10224352&-0.46430188&-0.32162794&0.1304409\\\\\n\\end{bmatrix},\\qquad D=\\begin{bmatrix}1.5905786&0.&0.\\\\\n0.&-0.45901108&0.\\\\\n0.&0.&0.3238576\\\\\n\\end{bmatrix}\n\nHyperparameters for STU:\n\nWe only tuned the learning rate in the set () for vanilla STU and used . Hyperparameters for LRU:\n\n\u2022\n\nModel Hyperparameters (Orvieto et al., 2023) provide a few recommendations for the model. We enabled the stable exp-parameterization, used -normalization and ring-initialization and attempted reducing the phase initialization. We found the stable exp-parameterization and -normalization to be essential for training in this problem. We did not observe any particular benefit of ring initialization or reducing the phase initialization (we set them to defaults eventually). \u2022\n\nOptimization Hyperparameters Given the comparatively slow training of the LRU model we employed standard deep-learning optimization tricks like tuning weight-decay as well as applying a cosine learning rate schedule with warmup. These optimization tricks did not lead to gains over standard training with Adam and a fixed learning rate in this problem. We tuned the learning rate in the set (). C.2 Experimental setup for LRA experiments\n\nOur training setup closely follows the experimental setup used by Orvieto et al. (2023). We use the same batch sizes and training horizons for all the tasks as employed by Orvieto et al. (2023). Hyperparameter tuning For all of our experiments on the LRA benchmark for both the vanilla STU model and the auto-regressive AR-STU model we tune the learning rate in the set and tune the weight decay in the set .",
    "spectralssm-28": "We fix the number of filters to be . We use Adam as the training algorithm with other optimization hyperparameters set to their default values. We use the same learning rate schedule as Orvieto et al. (2023), i.e. 10% warmup followed by cosine decay to 0. For the AR-STU model we used the default value of but also tried on certain workloads. In Table 3 we present a comparison of performance gains afforded by where applicable. Initialization For the STU model we initialized the matrices at 0. We found random initialization to also perform similarly. For the AR-STU model we initialize the matrices such that at initialization the model mimics the standard STU model, i.e.",
    "spectralssm-29": "and otherwise. We found setting to be better with respect to stability of the overall model. We tried a couple of values for the parameter such as but found it to have negligible impact on performace. We suggest using either of these values. Finally while training the AR-STU model as employed by the training setup of (Orvieto et al., 2023) we found using a smaller value of LR specifically for matrices to be useful. We decreased the value of LR by a factor or and tuned over this parameter. Appendix D Power of Auto-regression: Dimension-dependent representation for LDS\n\nIn this section we give a short proof that any partially-observed LDS can be perfectly predicted via a linear predictor acting over at most of its past inputs and outputs where is the hidden-state dimensionality (i.e. ). In particular\n\nTheorem D.1. Given an LDS parameterized by , there exist coefficients and matrices such that given any input sequence , the output sequence generated by the action of the LDS on the input satisfies for all\n\ny t = \u2211 i = 1 d \u03b1 i \u200b y t \u2212 i + \u2211 i = 0 d \u0393 i \u200b u t \u2212 i subscript \ud835\udc66 \ud835\udc61 superscript subscript \ud835\udc56 1 \ud835\udc51 subscript \ud835\udefc \ud835\udc56 subscript \ud835\udc66 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc56 0 \ud835\udc51 subscript \u0393 \ud835\udc56 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 y_{t}=\\sum_{i=1}^{d}\\alpha_{i}y_{t-i}+\\sum_{i=0}^{d}\\Gamma_{i}u_{t-i}\n\nProof. By unrolling the LDS we have that . By the Cayley Hamilton theorem, the matrix has a characteristic polynomial of degree , namely there exists numbers such that\n\np \u200b ( z ) = \u2211 i = 0 d c i \u200b z i \ud835\udc5d \ud835\udc67 superscript subscript \ud835\udc56 0 \ud835\udc51 subscript \ud835\udc50 \ud835\udc56 superscript \ud835\udc67 \ud835\udc56 p(z)=\\sum_{i=0}^{d}c_{i}z^{i}\n\nsatisfies . Without loss of generality we can assume the constant term in the polynomial is 1. We can now consider the series for as\n\ny t \u2212 D \u200b u t = C \u200b B \u200b u t C \u200b A \u200b B \u200b u t \u2212 1 \u2026 C \u200b A t \u200b B \u200b u 1 y t \u2212 1 \u2212 D \u200b u t \u2212 1 = 0 C \u200b B \u200b u t \u2212 1 \u2026 C \u200b A t \u2212 1 \u200b B \u200b u 1 \u22ee y t \u2212 d \u2212 D \u200b u t \u2212 d = 0 0 \u2026 C \u200b A t \u2212 d \u200b B \u200b u 1 matrix subscript \ud835\udc66 \ud835\udc61 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc36 \ud835\udc34 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \u2026 \ud835\udc36 superscript \ud835\udc34 \ud835\udc61 \ud835\udc35 subscript \ud835\udc62 1 subscript \ud835\udc66 \ud835\udc61 1 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 1 0 \ud835\udc36 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 1 \u2026 \ud835\udc36 superscript \ud835\udc34 \ud835\udc61 1 \ud835\udc35 subscript \ud835\udc62 1 \u22ee subscript \ud835\udc66 \ud835\udc61 \ud835\udc51 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc51 0 0 \u2026 \ud835\udc36 superscript \ud835\udc34 \ud835\udc61 \ud835\udc51 \ud835\udc35 subscript \ud835\udc62 1 \\displaystyle\\begin{matrix}y_{t}-Du_{t}&=&CBu_{t}&CABu_{t-1}&...&CA^{t}Bu_{1}\\\\\ny_{t-1}-Du_{t-1}&=&0&CBu_{t-1}&...&CA^{t-1}Bu_{1}\\\\\n\\vdots\\\\\ny_{t-d}-Du_{t-d}&=&0&0&...&CA^{t-d}Bu_{1}\\\\\n\\end{matrix}\n\nNow, if we take the combination of the above rows according to the coefficients of the characteristic polynomial, we get that\n\n\u2211 i = 0 d c i \u200b y t \u2212 i = \u2211 j = 0 t R j + \u2211 i = 0 d D \u200b u t \u2212 i superscript subscript \ud835\udc56 0 \ud835\udc51 subscript \ud835\udc50 \ud835\udc56 subscript \ud835\udc66 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc57 0 \ud835\udc61 subscript \ud835\udc45 \ud835\udc57 superscript subscript \ud835\udc56 0 \ud835\udc51 \ud835\udc37 subscript \ud835\udc62 \ud835\udc61 \ud835\udc56 \\sum_{i=0}^{d}c_{i}y_{t-i}=\\sum_{j=0}^{t}R_{j}+\\sum_{i=0}^{d}Du_{t-i} (9)\n\nwhere is the appropriate sum along the column of the matrix above. For all , this amounts to an expression of the form:\n\nj > d \u21d2 R j = \u2211 i = 0 d c i \u200b C \u200b A i \u22c5 A t \u2212 j \u200b B \u200b u t \u2212 j = C \u200b ( \u2211 i = 0 d c i \u200b A i ) \u22c5 A t \u2212 j \u200b B \u200b u t \u2212 j = C \u22c5 p \u200b ( A ) \u22c5 A t \u2212 j \u200b B \u200b u t \u2212 j = 0 . formulae-sequence \ud835\udc57 \ud835\udc51 \u21d2 subscript \ud835\udc45 \ud835\udc57 superscript subscript \ud835\udc56 0 \ud835\udc51 \u22c5 subscript \ud835\udc50 \ud835\udc56 \ud835\udc36 superscript \ud835\udc34 \ud835\udc56 superscript \ud835\udc34 \ud835\udc61 \ud835\udc57 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc57 \u22c5 \ud835\udc36 superscript subscript \ud835\udc56 0 \ud835\udc51 subscript \ud835\udc50 \ud835\udc56 superscript \ud835\udc34 \ud835\udc56 superscript \ud835\udc34 \ud835\udc61 \ud835\udc57 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc57 \u22c5 \u22c5 \ud835\udc36 \ud835\udc5d \ud835\udc34 superscript \ud835\udc34 \ud835\udc61 \ud835\udc57 \ud835\udc35 subscript \ud835\udc62 \ud835\udc61 \ud835\udc57 0 j>d\\ \\ \\Rightarrow\\ \\ R_{j}=\\sum_{i=0}^{d}c_{i}CA^{i}\\cdot A^{t-j}Bu_{t-j}=C(\\sum_{i=0}^{d}c_{i}A^{i})\\cdot A^{t-j}Bu_{t-j}=C\\cdot p(A)\\cdot A^{t-j}Bu_{t-j}=0. Since all but the first columns are zero, rearranging (9) and collecting terms, we get that there exists coefficients and matrices such that\n\ny t = \u2211 i = 1 d \u03b1 i \u200b y t \u2212 i + \u2211 j = 0 d \u0393 j \u200b u t \u2212 j . subscript \ud835\udc66 \ud835\udc61 superscript subscript \ud835\udc56 1 \ud835\udc51 subscript \ud835\udefc \ud835\udc56 subscript \ud835\udc66 \ud835\udc61 \ud835\udc56 superscript subscript \ud835\udc57 0 \ud835\udc51 subscript \u0393 \ud835\udc57 subscript \ud835\udc62 \ud835\udc61 \ud835\udc57 y_{t}=\\sum_{i=1}^{d}\\alpha_{i}y_{t-i}+\\sum_{j=0}^{d}\\Gamma_{j}u_{t-j}. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 14:58:50 2024 by LaTeXML"
}