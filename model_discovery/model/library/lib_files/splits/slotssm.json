{
    "slotssm-0": "Slot State Space Models\n\nJindong Jiang\u2217 Rutgers University &Fei Deng Rutgers University &Gautam Singh Rutgers University &Minseung Lee KAIST &Sungjin Ahn KAIST Correspondence to jindong.jiang@rutgers.edu and sungjin.ahn@kaist.ac.kr.",
    "slotssm-1": "Abstract\n\nRecent State Space Models (SSMs) such as S4, S5, and Mamba have shown remarkable computational benefits in long-range temporal dependency modeling. However, in many sequence modeling problems, the underlying process is inherently modular and it is of interest to have inductive biases that mimic this modular structure. In this paper, we introduce SlotSSMs, a novel framework for incorporating independent mechanisms into SSMs to preserve or encourage separation of information. Unlike conventional SSMs that maintain a monolithic state vector, SlotSSMs maintains the state as a collection of multiple vectors called slots. Crucially, the state transitions are performed independently per slot with sparse interactions across slots implemented via the bottleneck of self-attention. In experiments, we evaluate our model in object-centric video understanding, 3D visual reasoning, and video prediction tasks, which involve modeling multiple objects and their long-range temporal dependencies. We find that our proposed design offers substantial performance gains over existing sequence modeling methods. Project page is available at https://slotssms.github.io/\n\n1 Introduction\n\nState space models (SSMs) have recently emerged as a promising class of sequence models, achieving remarkable success in language modeling [17, 35, 14, 29, 5] due to their long-term memory capability and computational efficiency. Compared to Transformers [2] whose attention mechanisms also facilitate capturing long-range dependencies, SSMs are more efficient during both training and inference. Notably, SSMs offer parallel training with sub-quadratic complexity, and recurrent generation with constant cost per time step. These benefits have motivated the application of SSMs to sequences of other modalities such as audio [11] and video [6]. Typically, SSMs use a monolithic state vector to summarize all past information. This design can struggle to model sequences with modular underlying structures, which are common in physical processes and real-world dynamics. For example, physical objects largely follow independent dynamics based on their own properties, with strong interactions happening only sparsely (e.g., when objects come in close contact). A monolithic state vector would excessively entangle the dynamics of different entities, thereby hurting generalization. It could be beneficial to incorporate inductive biases for independent mechanisms [12] into the sequence modeling architecture. Recent progress in object-centric learning [25, 32, 22] has led to several methods for discovering modular object-centric structures and modeling their dynamics from videos with no or only weak supervision [23, 8, 34]. Similar to RIMs [12], they build modularity into the RNN architecture to separately keep track of the dynamics of each object. However, RNNs are prone to vanishing gradients [30] and are not amenable to parallel training, making it hard to scale these methods up to modeling long-range effects that span hundreds of time steps. In this paper, we propose Slot State Space Models (SlotSSMs), a novel and general SSM framework that have built-in inductive biases for discovering and maintaining independent mechanisms. Instead of using monolithic state vectors, SlotSSMs maintain a set of modular slot states whose transition dynamics are designed to be largely independent, with only sparse interaction across slots introduced through the bottleneck of self-attention. The number of slots can be flexible across the layers of SlotSSMs, allowing slots to have a different level of abstraction at each layer. Furthermore, SlotSSMs inherit the strengths of SSMs, namely parallelizable training, memory efficiency, and long-range reasoning capabilities, giving it an advantage over methods based on RNNs and Transformers. Our contributions are summarized as follows. First, we propose SlotSSMs, a novel and general architecture that incorporates independent mechanisms into SSMs for modeling inherently modular physical processes. Second, we show that SlotSSMs can be specialized to solve object-centric learning tasks. It achieves comparable or better performance than existing RNN-based methods and the Transformer baseline that we develop, while being more computationallly efficient. Third, we further investigate the abilities of SlotSSMs as a general sequence modeling framework, demonstrating its advantages in video understanding and prediction, long-range reasoning, and 3D visual reasoning. 2 Preliminaries\n\nA state space model (SSM) defines a sequence-to-sequence mapping between the input and the output by the following recurrence [18, 17, 35, 29]:\n\n\ud835\udc89 t subscript \ud835\udc89 \ud835\udc61 \\displaystyle\\bm{h}_{t} = \ud835\udc68 \u00af t \u200b \ud835\udc89 t \u2212 1 + \ud835\udc69 \u00af t \u200b \ud835\udc86 t , absent subscript bold-\u00af \ud835\udc68 \ud835\udc61 subscript \ud835\udc89 \ud835\udc61 1 subscript bold-\u00af \ud835\udc69 \ud835\udc61 subscript \ud835\udc86 \ud835\udc61 \\displaystyle=\\bm{\\overline{A}}_{t}\\bm{h}_{t-1}+\\bm{\\overline{B}}_{t}\\bm{e}_{t}\\ , (1) \ud835\udc9a t subscript \ud835\udc9a \ud835\udc61 \\displaystyle\\bm{y}_{t} = \ud835\udc6a t \u200b \ud835\udc89 t . absent subscript \ud835\udc6a \ud835\udc61 subscript \ud835\udc89 \ud835\udc61 \\displaystyle=\\bm{C}_{t}\\bm{h}_{t}\\ . Here, denotes the sequence length, are the input and output vectors at each time step , and is the hidden state that summarizes relevant information from the history . The matrices , , and are constructed from learnable parameters of the SSM in specific ways that encourage modeling long-range dependencies while maintaining computational efficiency. For example, commonly takes a diagonal or block-diagonal form, with its (complex) eigenvalues distributed close to the unit circle at initialization [15, 17, 19, 16, 35, 29]. When the SSM is time-invariant, meaning are constant with respect to , the computation of can be parallelized given , leading to efficient training. Recent works [14, 5] have further shown that it is possible to condition on the input without losing training efficiency. As illustrated in Figure 2 (Left), they employ learnable functions to generate input-dependent matrices :\n\n\ud835\udc68 \u00af t = \ud835\udc68 \u00af \u200b ( \ud835\udc86 t ) , \ud835\udc69 \u00af t = \ud835\udc69 \u00af \u200b ( \ud835\udc86 t ) , \ud835\udc6a t = \ud835\udc6a \u200b ( \ud835\udc86 t ) . formulae-sequence subscript bold-\u00af \ud835\udc68 \ud835\udc61 bold-\u00af \ud835\udc68 subscript \ud835\udc86 \ud835\udc61 formulae-sequence subscript bold-\u00af \ud835\udc69 \ud835\udc61 bold-\u00af \ud835\udc69 subscript \ud835\udc86 \ud835\udc61 subscript \ud835\udc6a \ud835\udc61 \ud835\udc6a subscript \ud835\udc86 \ud835\udc61 \\displaystyle\\bm{\\overline{A}}_{t}=\\bm{\\overline{A}}(\\bm{e}_{t})\\ ,\\quad\\bm{\\overline{B}}_{t}=\\bm{\\overline{B}}(\\bm{e}_{t})\\ ,\\quad\\bm{C}_{t}=\\bm{C}(\\bm{e}_{t})\\ . (2)\n\nThis brings the ability to selectively emphasize or ignore certain information based on the input, leading to more flexible sequence modeling. Because the (block-)diagonal structure of limits information propagation across dimensions, SSMs are typically interleaved with mixing layers such as linear projections or MLPs to mix information across dimensions. Mixing can also be promoted by using dense and matrices. 3 Slot State Space Models (SlotSSMs)\n\nStandard SSMs use monolithic vectors for inputs, outputs, and hidden states, and mix information across all dimensions. This lack of modularity could cause difficulties in modeling real-world dynamics such as object interactions, where the underlying process consists of multiple entities and is inherently modular [12]. In this section, we present slot state space models (SlotSSMs), a new class of SSMs with built-in inductive biases for encouraging and preserving modularity. Our key idea is to maintain a set of separate slot state representations (called slots in short), and process the slots independently and symmetrically. To do this, we format the input vector as a concatenation of slot representations , where . The output and hidden state are formatted similarly:\n\n\ud835\udc86 t = concat \u200b [ \ud835\udc94 t 1 , \u2026 , \ud835\udc94 t K ] , \ud835\udc9a t = concat \u200b [ \ud835\udc9a t 1 , \u2026 , \ud835\udc9a t K ] , \ud835\udc89 t = concat \u200b [ \ud835\udc89 t 1 , \u2026 , \ud835\udc89 t K ] , formulae-sequence subscript \ud835\udc86 \ud835\udc61 concat superscript subscript \ud835\udc94 \ud835\udc61 1 \u2026 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc3e formulae-sequence subscript \ud835\udc9a \ud835\udc61 concat superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e subscript \ud835\udc89 \ud835\udc61 concat superscript subscript \ud835\udc89 \ud835\udc61 1 \u2026 superscript subscript \ud835\udc89 \ud835\udc61 \ud835\udc3e \\displaystyle\\bm{e}_{t}=\\texttt{concat}\\left[\\bm{s}_{t}^{1},\\dots,\\bm{s}_{t}^{K}\\right],\\quad\\bm{y}_{t}=\\texttt{concat}\\left[\\bm{y}_{t}^{1},\\dots,\\bm{y}_{t}^{K}\\right],\\quad\\bm{h}_{t}=\\texttt{concat}\\left[\\bm{h}_{t}^{1},\\dots,\\bm{h}_{t}^{K}\\right], (3)\n\nwhere and are the output and the hidden state corresponding to slot , with . In this section, we focus on preserving modularity when the input already complies with the slot format. When coupled with a slot encoder, the SlotSSM can help encourage the emergence of modularity from unstructured inputs such as video frames, as we will discuss in Section 4. To preserve modularity, we make sure that SlotSSM do not mix information across different slots. More precisely, the hidden state and output only integrate information from the history of the corresponding input slot . As illustrated in Figure 2 (Right), this can be achieved by making block-diagonal, where the -th block is only conditioned on the -th slot:\n\n\ud835\udc68 \u00af t = diag \u200b ( { \ud835\udc68 \u00af \u200b ( \ud835\udc94 t k ) } k = 1 K ) , \ud835\udc69 \u00af t = diag \u200b ( { \ud835\udc69 \u00af \u200b ( \ud835\udc94 t k ) } k = 1 K ) , \ud835\udc6a t = diag \u200b ( { \ud835\udc6a \u200b ( \ud835\udc94 t k ) } k = 1 K ) . formulae-sequence subscript bold-\u00af \ud835\udc68 \ud835\udc61 diag superscript subscript bold-\u00af \ud835\udc68 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e formulae-sequence subscript bold-\u00af \ud835\udc69 \ud835\udc61 diag superscript subscript bold-\u00af \ud835\udc69 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e subscript \ud835\udc6a \ud835\udc61 diag superscript subscript \ud835\udc6a superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e \\bm{\\overline{A}}_{t}=\\mathrm{diag}\\left(\\{\\bm{\\overline{A}}(\\bm{s}_{t}^{k})\\}_{k=1}^{K}\\right),\\quad\\bm{\\overline{B}}_{t}=\\mathrm{diag}\\left(\\{\\bm{\\overline{B}}(\\bm{s}_{t}^{k})\\}_{k=1}^{K}\\right),\\quad\\bm{C}_{t}=\\mathrm{diag}\\left(\\{\\bm{C}(\\bm{s}_{t}^{k})\\}_{k=1}^{K}\\right). (4)\n\nImplementation details. The SlotSSM formulation in Equation 4 is general and can accommodate various choices of the functions. In our implementation, we adopt those from Mamba [14]. Specifically, are themselves block-diagonal matrices with blocks, one for each slot dimension. The -th blocks and are obtained by discretizing their continuous-time counterparts and using the time step and the zero-order hold (ZOH) rule:\n\n\ud835\udc68 \u00af ( i ) \u200b ( \ud835\udc94 t k ) , \ud835\udc69 \u00af ( i ) \u200b ( \ud835\udc94 t k ) = ZOH \u200b ( \u0394 ( i ) \u200b ( \ud835\udc94 t k ) , \ud835\udc68 ( i ) , \ud835\udc69 ( i ) \u200b ( \ud835\udc94 t k ) ) , i = 1 , \u2026 , D s . formulae-sequence superscript bold-\u00af \ud835\udc68 \ud835\udc56 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 superscript bold-\u00af \ud835\udc69 \ud835\udc56 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 ZOH superscript \u0394 \ud835\udc56 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 superscript \ud835\udc68 \ud835\udc56 superscript \ud835\udc69 \ud835\udc56 superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 \ud835\udc56 1 \u2026 subscript \ud835\udc37 \ud835\udc60 \\displaystyle\\bm{\\overline{A}}^{(i)}\\!(\\bm{s}_{t}^{k}),\\ \\bm{\\overline{B}}^{(i)}\\!(\\bm{s}_{t}^{k})=\\mathrm{ZOH}\\left({\\Delta\\!}^{(i)}\\!(\\bm{s}_{t}^{k}),\\ \\bm{A}^{(i)},\\ \\bm{B}^{(i)}\\!(\\bm{s}_{t}^{k})\\right),\\quad i=1,\\dots,D_{s}\\ . (5)\n\nHere, is the hidden state size per slot dimension, is an input-independent learnable model parameter, and are learnable functions implemented as neural networks. Similarly, the -th block is computed by the learnable function . For simplicity and efficiency, and are shared across all , and is parameterized as a diagonal matrix. 4 Modular Sequence Modeling with SlotSSM\n\nThe SlotSSM proposed in Section 3 are designed to preserve modularity when the input is already separated into slots. In this section, we complement SlotSSM with a slot encoder that extracts slot representations from unstructured inputs (Section 4.1), and a slot mixer that introduces sparse interactions across slots (Section 4.2). We then present a sequence modeling architecture (Section 4.3) that encourages discovery of underlying modular processes by stacking these components. 4.1 Slot Encoder\n\nWe assume the unstructured input at each time step is represented as a sequence of tokens:\n\n\ud835\udc99 t = ( \ud835\udc99 t 1 , \u2026 , \ud835\udc99 t M ) , \ud835\udc99 t m \u2208 \u211d D x . formulae-sequence subscript \ud835\udc99 \ud835\udc61 superscript subscript \ud835\udc99 \ud835\udc61 1 \u2026 superscript subscript \ud835\udc99 \ud835\udc61 \ud835\udc40 superscript subscript \ud835\udc99 \ud835\udc61 \ud835\udc5a superscript \u211d subscript \ud835\udc37 \ud835\udc65 \\displaystyle\\bm{x}_{t}=(\\bm{x}_{t}^{1},\\ \\dots,\\ \\bm{x}_{t}^{M})\\ ,\\quad\\bm{x}_{t}^{m}\\in\\mathbb{R}^{D_{x}}\\ . (6)\n\nFor example, image inputs can be represented as CNN feature maps ( is the number of cells in the feature map), or as embeddings of non-overlapping image patches ( is the number of patches), as proposed in ViT [7]. To extract slot representations from , we use learnable CLS tokens as queries and perform cross-attention with the input tokens through a Transformer [40]:\n\n{ CLS t k } k = 1 K \u2190 Transformer \u200b ( q = { CLS t k } k = 1 K , kv = { \ud835\udc99 t m } m = 1 M ) . \u2190 superscript subscript superscript subscript CLS \ud835\udc61 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e Transformer formulae-sequence q superscript subscript superscript subscript CLS \ud835\udc61 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e kv superscript subscript superscript subscript \ud835\udc99 \ud835\udc61 \ud835\udc5a \ud835\udc5a 1 \ud835\udc40 \\displaystyle\\{\\text{CLS}_{t}^{k}\\}_{k=1}^{K}\\leftarrow\\mathrm{Transformer}\\!\\left(\\texttt{q}=\\{\\text{CLS}_{t}^{k}\\}_{k=1}^{K},\\ \\texttt{kv}=\\{\\bm{x}_{t}^{m}\\}_{m=1}^{M}\\right)\\ . (7)\n\nThe Transformer also includes self-attention within the CLS tokens, allowing them to communicate with each other and capture information from different parts of the input, thereby facilitating the emergence of modularity. The slot representations are then obtained by applying a linear projection to the corresponding output embeddings of the CLS tokens:\n\n\ud835\udc94 t k = Linear \u200b ( CLS t k ) , k = 1 , \u2026 , K . formulae-sequence superscript subscript \ud835\udc94 \ud835\udc61 \ud835\udc58 Linear superscript subscript CLS \ud835\udc61 \ud835\udc58 \ud835\udc58 1 \u2026 \ud835\udc3e \\displaystyle\\bm{s}_{t}^{k}=\\mathrm{Linear}(\\text{CLS}_{t}^{k})\\ ,\\quad k=1,\\dots,K\\ . (8)\n\n4.2 Slot Mixer\n\nThe slot encoder obtains slot decomposition purely based on single time steps, which can be suboptimal. In addition, the SlotSSM processes each slot fully independently, making it hard to correct mistakenly decomposed slots or model interarctions across slots. To resolve both issues, we interleave SlotSSM with slot mixers. The slot mixer consists of two residual blocks, and is applied to the outputs of the SlotSSM. The first block introduces interaction across slots through self-attention [40], whereas the second block uses MLP to further process the gathered information within each slot:\n\n( \ud835\udc9a t 1 , \u2026 , \ud835\udc9a t K ) superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e \\displaystyle\\left(\\bm{y}_{t}^{1},\\ \\dots,\\ \\bm{y}_{t}^{K}\\right) \u2190 ( \ud835\udc9a t 1 , \u2026 , \ud835\udc9a t K ) + SelfAttn \u200b ( LN \u200b ( \ud835\udc9a t 1 ) , \u2026 , LN \u200b ( \ud835\udc9a t K ) ) , \u2190 absent superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e SelfAttn LN superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 LN superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e \\displaystyle\\leftarrow\\left(\\bm{y}_{t}^{1},\\ \\dots,\\ \\bm{y}_{t}^{K}\\right)+\\mathrm{SelfAttn}\\!\\left(\\mathrm{LN}(\\bm{y}_{t}^{1}),\\ \\dots,\\ \\mathrm{LN}(\\bm{y}_{t}^{K})\\right)\\ , (9) ( \ud835\udc9a t 1 , \u2026 , \ud835\udc9a t K ) superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e \\displaystyle\\left(\\bm{y}_{t}^{1},\\ \\dots,\\ \\bm{y}_{t}^{K}\\right) \u2190 ( \ud835\udc9a t 1 , \u2026 , \ud835\udc9a t K ) + ( MLP \u200b ( LN \u200b ( \ud835\udc9a t 1 ) ) , \u2026 , MLP \u200b ( LN \u200b ( \ud835\udc9a t K ) ) ) . \u2190 absent superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e MLP LN superscript subscript \ud835\udc9a \ud835\udc61 1 \u2026 MLP LN superscript subscript \ud835\udc9a \ud835\udc61 \ud835\udc3e \\displaystyle\\leftarrow\\left(\\bm{y}_{t}^{1},\\ \\dots,\\ \\bm{y}_{t}^{K}\\right)+\\left(\\mathrm{MLP}(\\mathrm{LN}(\\bm{y}_{t}^{1})),\\ \\dots,\\ \\mathrm{MLP}(\\mathrm{LN}(\\bm{y}_{t}^{K}))\\right)\\ . (10)\n\nHere, denotes layer normalization [1]. Because carries information from the entire history of each slot, it provides the opportunity to refine the slot representations based on temporal dynamics. 4.3 Sequence Modeling Architecture\n\nWe now present a generic architecture for modeling sequences with modular underlying processes. Given a sequence of unstructured inputs , our goal is to obtain a set of modular representations at each time step and at each layer that summarizes all underlying processes up to time . In general, the number of slots at each layer can be different, potentially allowing fewer but more abstract slots at higher layers. To accommodate this, we insert a slot encoder wherever the number of slots changes, and repurpose it to extract a different number of slots from existing slot representations. This is achieved by treating the slots output from the previous layer as keys and values in Equation 7. When the number of slots does not change, we can simply copy the slots from the previous layer. As shown in Figure 3, our proposed architecture stacks the (optional) slot encoder, SlotSSM, and slot mixer together at each layer. We use the subscript \u2018\u2019 to denote the relevant variables at layer . For notational convenience, we denote the output of slot mixer at layer by . This serves as the input to layer . We use to denote the input in Equation 6, where . The computation at each layer can then be summarized as follows:\n\n{ \ud835\udc94 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SlotEncoder \u200b ( { \ud835\udc99 t \u2223 l \u2212 1 k } k = 1 K l \u2212 1 ) , absent SlotEncoder superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 1 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 1 \\displaystyle=\\mathrm{SlotEncoder}\\!\\left(\\{\\bm{x}_{t\\mid l-1}^{k}\\}_{k=1}^{K_{l-1}}\\right)\\ , (11) { \ud835\udc9a t \u2223 l k } k = 1 K l , { \ud835\udc89 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc9a conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 superscript subscript superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{y}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}},\\ \\{\\bm{h}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SlotSSM \u200b ( { \ud835\udc94 t \u2223 l k } k = 1 K l , { \ud835\udc89 t \u2212 1 \u2223 l k } k = 1 K l ) , absent SlotSSM superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 superscript subscript superscript subscript \ud835\udc89 \ud835\udc61 conditional 1 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{SlotSSM}\\!\\left(\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}},\\ \\{\\bm{h}_{t-1\\mid l}^{k}\\}_{k=1}^{K_{l}}\\right)\\ , (12) { \ud835\udc99 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{x}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SlotMixer \u200b ( { \ud835\udc9a t \u2223 l k } k = 1 K l ) . absent SlotMixer superscript subscript superscript subscript \ud835\udc9a conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{SlotMixer}\\!\\left(\\{\\bm{y}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}}\\right)\\ . (13)\n\nThe final output can be used for various tasks, such as predicting the next observation and the properties of underlying processes (e.g., position, velocity). 5 Object-Centric Learning with SlotSSM\n\nIn this section, we present a concrete example of adapting the generic sequence modeling architecture proposed in Section 4 to solve a specific task. We consider the task of object-centric representation learning from unannotated videos of interacting objects, a typical example of sequences with modular underlying structures. The goal is to obtain a representation for each individual object that captures relevant attributes such as object position, size, shape, color, etc. without any object-level annotation. 5.1 Object-Centric SlotSSMs (OC-SlotSSMs)\n\nInspired by previous works [25, 43], we make slight modifications to our sequence modeling architecture to facilitate the discovery of modular structures. We call the resulting model OC-SlotSSMs. First, we use the same number of slots across all layers. It is thus unnecessary to have a slot encoder per layer. However, we find it helpful to still have it, but in another form that encourages iterative refinement of the slots. Specifically, we use the slots output from the previous layer as queries, and provide the input tokens as keys and values. Second, we introduce competition among slots in the attention layers of the slot encoder. We achieve this by using inverted attention [38, 43], which is essentially cross attention with the Softmax operation performed over the queries instead of the keys. This has the effect of softly assigning each input token to a slot, thereby promoting modularity. The computation at each layer can be summarized as follows:\n\n{ \ud835\udc94 t \u2223 l k } k = 1 K superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e \\displaystyle\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K} = InvAttn \u200b ( q = { \ud835\udc99 t \u2223 l \u2212 1 k } k = 1 K , kv = { \ud835\udc99 t \u2223 0 m } m = 1 M ) , absent InvAttn formulae-sequence q superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 1 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e kv superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 0 \ud835\udc5a \ud835\udc5a 1 \ud835\udc40 \\displaystyle=\\mathrm{InvAttn}\\!\\left(\\texttt{q}=\\{\\bm{x}_{t\\mid l-1}^{k}\\}_{k=1}^{K},\\ \\texttt{kv}=\\{\\bm{x}_{t\\mid 0}^{m}\\}_{m=1}^{M}\\right)\\ , (14) { \ud835\udc9a t \u2223 l k } k = 1 K , { \ud835\udc89 t \u2223 l k } k = 1 K superscript subscript superscript subscript \ud835\udc9a conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e superscript subscript superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e \\displaystyle\\{\\bm{y}_{t\\mid l}^{k}\\}_{k=1}^{K},\\ \\{\\bm{h}_{t\\mid l}^{k}\\}_{k=1}^{K} = SlotSSM \u200b ( { \ud835\udc94 t \u2223 l k } k = 1 K , { \ud835\udc89 t \u2212 1 \u2223 l k } k = 1 K ) , absent SlotSSM superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e superscript subscript superscript subscript \ud835\udc89 \ud835\udc61 conditional 1 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e \\displaystyle=\\mathrm{SlotSSM}\\!\\left(\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K},\\ \\{\\bm{h}_{t-1\\mid l}^{k}\\}_{k=1}^{K}\\right)\\ , (15) { \ud835\udc99 t \u2223 l k } k = 1 K superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e \\displaystyle\\{\\bm{x}_{t\\mid l}^{k}\\}_{k=1}^{K} = SlotMixer \u200b ( { \ud835\udc9a t \u2223 l k } k = 1 K ) . absent SlotMixer superscript subscript superscript subscript \ud835\udc9a conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 \ud835\udc3e \\displaystyle=\\mathrm{SlotMixer}\\!\\left(\\{\\bm{y}_{t\\mid l}^{k}\\}_{k=1}^{K}\\right)\\ . (16)\n\nWe note that the queries in the first inverted attention layer are the learnable CLS tokens . 5.2 Training Pipeline\n\nFollowing previous works in object-centric learning [25, 23, 34], we adopt an auto-encoding training pipeline. Given a sequence of video frames , we obtain the input to our sequence modeling architecture by applying a CNN encoder to each frame and adding a positional embedding for each feature map cell. The output slots are each decoded into an object image and an alpha mask by a spatial broadcast decoder [42]. The final reconstruction is given by the alpha-composition of the object images:\n\n\ud835\udc90 ^ t k , \ud835\udf36 t k = Decoder \u200b ( \ud835\udc99 t \u2223 L k ) , \ud835\udc90 ^ t = \u2211 k = 1 K exp \u2061 ( \ud835\udf36 t k ) \u2211 j = 1 K exp \u2061 ( \ud835\udf36 t j ) \u22c5 \ud835\udc90 ^ t k . formulae-sequence superscript subscript bold-^ \ud835\udc90 \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udf36 \ud835\udc61 \ud835\udc58 Decoder superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc3f \ud835\udc58 subscript bold-^ \ud835\udc90 \ud835\udc61 superscript subscript \ud835\udc58 1 \ud835\udc3e \u22c5 superscript subscript \ud835\udf36 \ud835\udc61 \ud835\udc58 superscript subscript \ud835\udc57 1 \ud835\udc3e superscript subscript \ud835\udf36 \ud835\udc61 \ud835\udc57 superscript subscript bold-^ \ud835\udc90 \ud835\udc61 \ud835\udc58 \\displaystyle\\bm{\\hat{o}}_{t}^{k},\\bm{\\alpha}_{t}^{k}=\\mathrm{Decoder}(\\bm{x}_{t\\mid L}^{k})\\ ,\\quad\\bm{\\hat{o}}_{t}=\\sum_{k=1}^{K}\\frac{\\exp(\\bm{\\alpha}_{t}^{k})}{\\sum_{j=1}^{K}\\exp(\\bm{\\alpha}_{t}^{j})}\\cdot\\bm{\\hat{o}}_{t}^{k}\\ . (17)\n\nThe training objective is to minimize the reconstruction error . 6 Related Work\n\nState Space Models (SSMs). Popularized by S4 [17], SSMs have attracted growing interest in language modeling and as a sequence modeling framework in general. The original S4 follows the HiPPO theory [15] to parameterize and initialize the state transition matrices, which is quite mathematically involved. Most recent works have proposed simplified versions that use diagonal transition matrices [19, 16, 35] and pure RNN formulation (i.e., without reliance on ODE discretization) [20, 29, 5]. Several works have proposed hybrid architectures of SSMs and Transformers to incorporate their complementary strengths [50, 27, 9, 21, 14]. In addition to language modeling, SSMs have been applied to various domains, including time-series generation [48], audio generation [11], visual classificiation and generation [28, 24, 21, 41, 49, 47], and reinforcement learning [4, 26, 6, 31]. Our study introduces the first SSM with inductive biases for modeling inherently modular processes. Object-Centric Learning. Object-centric learning seeks to discover modular structures and independent mechanisms [12] such as objects and their relations from multi-object images and videos with weak or no supervision. Recent works are predominantly based on the Slot Attention [25] model, which uses a GRU [3] and competitive attention mechanisms to iteratively refine slot representations [32, 34, 23, 8, 45, 33, 22, 46]. However, GRUs and RNNs in general are prone to vanishing gradient issues [30], and the training must be done in a sequential way. These weaknesses render them incapable of scaling up to long-range videos. Our SlotSSMs framework can be specialized to solve object-centric learning tasks. With SSMs at its core, SlotSSMs enjoy parallelizable training and remarkable long-term memory abilities. As a general framework, SlotSSMs can also tackle other tasks such as long-range visual reasoning. 7 Experiments\n\nWe present an extensive evaluation of our models across a variety of tasks. Section 7.1 illustrates the need for modular latent states through a multi-object video prediction task. Section 7.2 demonstrates the advantages of SlotSSMs over Transformers and RNNs using a newly proposed long-context reasoning benchmark. Section 7.3 investigates the object-centric learning capabilities of OC-SlotSSMs. Finally, Section 7.4 showcases the 3D visual reasoning capabilities using the CATER benchmark [10]. 7.1 Multi-Object Video Prediction\n\nWe begin with a multi-object video modeling task to demonstrate the benefit of incorperating modularity into state space. Dataset and Task. We utilize the bouncing balls video dataset introduced by [39]. This dataset comprises videos of white balls bouncing off each other within an empty background window. Each ball is initialized with random positions, velocities, and masses, which govern the physical behavior of their interactions. The task is to perform conditional video generation, specifically . This task is inherently modular as it requires models to maintain memory of each object\u2019s physical attributes and to understand the interaction rules of moving objects. Experimental Setup. We train the model using teacher-forcing on video sequences of 20 frames with binary cross-entropy loss as the training objective. During testing, we provide context frames and the model performs an autoregressive rollout of frames, using its own predictions. We evaluate model performance using Mean Squared Error (MSE) loss between the predicted and ground truth images. Models. We employ the SlotSSM architecture described in Section 4.3, and we adopt the Mamba Block [14] as the SSM implementation for both SlotSSM and baseline models. We use the same number of slots for all layers, and thus only apply the Slot Encoder at the first layer. We compare our model against the following baselines:\n\n1. Single State SSM: Shares the same architecture as our model but uses a monolithic state. 2. Single State SSM (Split): A Single State SSM paired with multi-slot encoder and decoder. Slots are concatenated and provided to the SSM, then split into multiple slots for the decoder. 3. RIM [12]: A slot-based RNN model that uses separate RNN weights for each slot and introduces sparse slot update and interaction based on input attention values. 4. Transformer: Vanilla Transformer model with single input embedding at each time step. 5. SlotTransformer: Transformer model with multiple input slots at each time step, analogous to SlotSSMs. All models share the same encoder and decoder architectures. For the encoder, we use the Transformer encoder described in Section 4.1, with a single CLS token for single-state models. The decoder consists of 3 Transformer layers with self-attention and cross-attention blocks, where patch-wise positional embeddings self-attend and then serve as queries to attend to the slots. For all slot-based models, we use 6 slots. For RIM, we set for top-k active modules as described in the original paper. We carefully design hyperparameters to match model sizes across baselines, with one exception: RIM inherently requires a larger model size due to separate RNN weights per slot.",
    "slotssm-2": "Additional implementation detials can be found in Appendix B. Results. Figure 4 presents the comparative performance of our models. SlotSSM outperforms all baselines, including a marginal improvement over SlotTransformer. The substantial performance gap between SlotSSM and Single State SSM highlights the importance of maintaining modular slot states for effective multi-object dynamics learning. The comparison between Transformer and SlotTransformer also corroborates this finding. Notably, SlotSSM also significantly outperforms Single State SSM (Split) which uses the same modular encoder and decoder. This result highlights that modularity in temporal modeling\u2014the core contribution of SlotSSM\u2014is the most critical factor for the improved performance. Additionally, the RNN-based multi-slot model, RIM, performs better than other single-state baselines but still lags significantly behind SlotSSM and SlotTransformer. 7.2 Long-Context Reasoning\n\nWe now evaluate the long-context reasoning capabilities of SlotSSM. To enable a rigorous assessment in a multi-object setting, we propose the novel Blinking Color Balls Benchmark. Blinking Color Balls Benchmark. The benchmark comprises two variants: the Earliest Color and the Most Frequent Color versions. Both variants consist of episodes of image sequence , partitioned into context images and a target image . For each timestep in , all bouncing balls are initially colored white. Then, one ball is randomly selected and assigned a non-white color, which is randomly sampled from a set of 5 predefined colors The coloring of balls in the target image depends on the variant:\n\n\u2022\n\nEarliest Color: Each ball\u2019s color is determined by the earliest non-white color assigned to it in the context sequence. If a ball is never assigned a non-white color, it remains white. \u2022\n\nMost Frequent Color: Each ball\u2019s color is determined by the most frequently assigned non-white color across all context frames. In case of ties, the earliest color is used. If a ball is never assigned a non-white color, it remains white. To transform the dataset into a long-range reasoning task, we patchify each context image into non-overlapping patches and provide the flattened patch sequence as input to the model. We present this process in Figure 5(a). Specifically, each context image is divided into patches, which are then flattened into a sequence of length . For a context length of , this results in a total input sequence length of . Consequently, the models must identify and track objects from partial views provided by the patches, while simultaneously remembering and counting the color assignments for each object, making the task highly challenging and non-trivial. The final task is to predict the target image given this long sequential input. Experimental Setup. We evaluate models on Earliest Color with and Most Frequent Color with , and we employ patch sizes of . The resulting input sequence lengths are . Note that Most Frequent Color with setting poses a greater challenge because the increased context length leads to more color assignments, requiring the models to exhibit stronger memorization and reasoning capabilities. Models. We employ the same encoder, decoder, and the SlotSSM architectures as described in Section 7.1. For slot encoding, each image patch is treated as a small image and processed by the transformer slot encoder. The resulting slots from the last time step are then provided to the transformer decoder to predict the full target image. We compare our SlotSSM against several baselines: Single State SSM, SlotTransformer, and RIM. Additionally, we introduce a novel multi-slot design called SlotRNNs, which shares model weights across the RNNs of each slot and uses self-attention layers between time steps as the slot mixer. SlotRNNs can be viewed as a special case of RIMs, with shared RNN weights across states and dense state updates instead of sparse state updates. Empirically, we find that SlotRNNs exhibit more stable training and improved performance compared to RIMs. For a fair comparison, all slot-based models use 6 slots. We also carefully match the model size as in Section 7.1\n\nResults. Figure 6 presents the target image prediction error. SlotSSM demonstrates clear advantages over Single State SSM, SlotRNN, and RIM across all sequence lengths. For shorter sequences (length 80 and 160), Single State SSM and SlotRNN maintain relatively low error rates. However, their performance degrades significantly for sequences longer than 320 frames. Surprisingly, despite its modular architecture, RIM fails to generalize on any sequence length. We hypothesize that the separate weights for each slot in RIMs hinder optimization, and this issue is partially addressed by our proposed baseline SlotRNNs, which share weights across slots while maintaining modularity. SlotTransformer performs competitively for sequences up to 640 frames, which benefits from their direct access to all historical inputs. Nonetheless, SlotSSM exhibits superior long-range reasoning capabilities, particularly for sequences of 1280 and 2560 frames, where other models can not run due to memory and computational constraints. Figure 5(b) highlights the computational efficiency of SlotSSM. While SlotTransformer achieves similar inference latency for shorter sequences, their performance quickly declines as the sequence length increases due to its quadratic computational complexity. In contrast, SlotSSM maintains a stable and efficient inference process across all sequence lengths. Note that due to the high memory consumption of SlotTransformer, a batch size of 6 is used to evaluate the inference latency of all models. We include qulitative comparisons in Appendix A.3 to offer further insights into these models\u2019 strengths and weaknesses. 7.3 Unsupervised Object-Centric Learning\n\nIn this section, we evaluate the performance of the Object-Centric SlotSSMs (OC-SlotSSM) variant in unsupervised object-centric representation learning. Datasets. We evaluate the OC-SlotSSM model using the MOVi video dataset [13], specifically focusing on the MOVi-A and MOVi-B subsets. Both datasets contain videos of up to 10 objects moving in a 3D environment. MOVi-B introduces additional complexity compared to MOVi-A by incorporating a wider variety of object types and multi-colored backgrounds. Tasks. Following the evaluation protocol in prior object-centric learning works [25, 22], we evaluate the models on two downstream tasks: unsupervised object segmentation and attribute prediction. For the segmentation task, we report the FG-ARI and mIoU metrics on video sequences. In the attribute prediction task, we measure the quality of representations by inferring object properties from them. For discrete attributes such as object shape, we report the prediction accuracy, while for continuous attributes like object position, we use the metric. Models. We compare OC-SlotSSM against SAVi [23], an RNN-based object-centric learning approach. Both models use a CNN encoder to extract image features as model input tokens , which are processed by their respective attention mechanisms (inverted attention in OC-SlotSSM and slot attention in SAVi) to produce slots. The slots are then used to reconstruct the image and generate segmentation masks for each object using a spatial broadcast decoder, with reconstruction as the training objective. For unsupervised object segmentation, we directly use the object masks obtained during unsupervised training. To evaluate attribute prediction, we match the slots to object IDs using Hungarian matching based on the segmentation masks, and then employ linear heads and 2-layer MLPs to predict discrete and continuous attributes, respectively, where the slots remain frozen.",
    "slotssm-3": "Results. Figure 7 presents the results. The results demonstrate that OC-SlotSSM consistently outperforms SAVi in unsupervised object segmentation on both MOVi-A and MOVi-B. The qualitative comparison (Figure 7, left) shows that OC-SlotSSM generates masks with tighter object boundaries and fewer object splitting, which also leads to improved attribute prediction accuracy (Figure 7, right). Furthermore, our experiments reveal that OC-SlotSSM exhibits superior stability during training compared to SAVi, which tends to collapse into a single slot representing the entire scene when trained for an extended period. This collapse is not reflected in the validation loss, so we apply early stopping based on manual inspection. In contrast, OC-SlotSSM does not suffer from this instability, demonstrating its robustness in learning object-centric representations. 7.4 3D Visual Reasoning\n\nFinally, we explore the application of SlotSSM and OC-SlotSSM to 3D visual reasoning tasks using the CATER benchmark [10]. CATER Benchmark. CATER consists of 300-frame video episodes of objects moving around in a 3D environment. The movement of the objects can lead to partial occlusions and even complete coverage of smaller objects by larger ones. The primary task is snitch localization, which involves predicting the location of a golden snitch at the final frame. Note that the snitch is always present but may be invisible due to occlusion or coverage by other objects. To solve this task, models must be able to reason about the snitch\u2019s location based on its last known position and the movement of other objects in a 3D space. Success in this task would demonstrate the models\u2019 capacity for complex visual reasoning and their potential for application in real-world dynamic 3D environments.",
    "slotssm-4": "Experimental Setup. We consider two experiment settings: direct training and pre-training fine-tuning. In the direct training setting, models are trained end-to-end on the snitch localization task without any auxiliary objectives. In the pre-training fine-tuning setting, models are first pre-trained on video-only inputs using a reconstruction objective, and then fine-tuned on the task-specific signal. During pre-training, we randomly sample 32 frames, which are not necessarily consecutive, from the original 300-frame videos as input to the model. For direct training and fine-tuning, we first split the input sequence into 50 non-overlapping segments, each containing 6 frames. Then, from each segment, we randomly select one frame, resulting in a subsampled sequence of 50 frames that spans the entire video duration. At test time, we evenly sample 50 frames from the video by skipping every 6 frames. The final snitch location is quantized into a 6x6 grid, and the problem is formulated as a classification task. Models. In our investigation, we evaluate the performance of SlotSSM, OC-SlotSSM, Single State SlotSSM, and SlotTransformer. We exclude RNN-based baselines, such as SAVi, as our preliminary experiments reveal that they are unstable when handling long video inputs and prone to collapse to a constant output. For the visual pre-training setting, we employ a spatial broadcast decoder, common to all models, to reconstruct the input images. During downstream training/fine-tuning, we feed the slots from the final time step into a transformer predictor with single CLS token, followed by a linear layer on the output CLS token to predict the snitch\u2019s position. Results. Table 1 presents the Top-1 and Top-5 prediction accuracy on the CATER Snitch Localization task. Firstly, consistent with our previous findings, SlotSSM outperforms Single State SSM, which demonstrates the importance of modular structure in latent states for reasoning tasks involving multiple objects. When comparing SlotSSM with SlotTransformer, we see a notable difference between the direct training and pre-training settings. In direct training, SlotTransformer surpasses SlotSSM, possibly due to their optimization advantage, as the model can directly access to all previous states which facilitates learning of the task. However, SlotSSM benefits more from the pre-training phase, potentially attributed to the explicit memory capacity enabled by SSM states. Consequently, pre-trained SlotSSMs outperform their SlotTransformer counterparts. Remarkably, OC-SlotSSM achieves the highest accuracy, outperforming all baselines by a large margin in both direct training and pre-training settings. This performance gain might be attributed to the explicit decomposition of the scene into object-centric representations, which facilitates the reasoning about object properties, relationships, and interactions. 8 Conclusion & Limitations\n\nIn this work, we presented SlotSSMs a novel approach to incorporating modular structure and inductive biases into State Space Models for improved sequence modeling. By maintaining a collection of independent slot vectors and performing state transitions independently per slot with sparse interactions via self-attention, SlotSSMs effectively captures the inherent modularity present in many real-world processes. The experimental results in object-centric video understanding and video prediction tasks demonstrate the substantial performance gains offered by SlotSSMs over existing sequence modeling methods. SlotSSMs\u2019 success illustrates the importance of designing architectures that align with the problem domain\u2019s underlying modular structure. It also paves the way for future research in modular and object-centric sequence modeling. However, it has some limitations that future studies could address. First, although the proposed architecture is not only applicable to video modeling but also to other modalities like text, we have not explored this aspect in this study. It remains a matter for future work. Second, due to our academic research lab\u2019s computing resource constraints, we were unable to significantly scale up the proposed model to industry-scale in terms of model size and data size. Lastly, future studies should investigate the effect of increased visual complexity in videos. Impact Statement\n\nThe introduction of SlotSSMs, a novel framework that incorporates independent mechanisms into State Space Models (SSMs), has the potential to significantly impact the field of sequence modeling. By leveraging the modular structure inherent in many real-world processes, SlotSSMs offers a more intuitive and effective approach to modeling long-range temporal dependencies in object-centric video understanding and prediction tasks. The substantial performance gains demonstrated by SlotSSMs over existing sequence modeling methods highlight the importance of designing architectures that align with the underlying structure of the problem domain. This breakthrough could lead to the development of more efficient and accurate models for a wide range of applications, such as robotics, autonomous vehicles, and video surveillance systems. Moreover, the success of SlotSSMs in capturing the modular nature of real-world processes could inspire further research into modular and object-centric sequence modeling. This could result in the development of even more advanced architectures that can better handle the complexity and diversity of real-world data. Because this is a general backbone architecture for sequence modeling, it doesn\u2019t raise direct ethical concerns. However, its ethical implications depend on the way downstream application developers use the model. Acknowledgements\n\nThis work is supported by Brain Pool Plus Program (No. 2021H1D3A2A03103645) and Young Researcher Program (No. 2022R1C1C1009443) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT. References\n\n[1] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. TransDreamer: Reinforcement learning with Transformer world models. In Deep RL Workshop NeurIPS 2021, 2021. [3] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [4] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers.",
    "slotssm-5": "In International Conference on Learning Representations, 2023. [5] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models.",
    "slotssm-6": "arXiv preprint arXiv:2402.19427, 2024. [6] Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model backbones: RNNs, Transformers, and S4. Advances in Neural Information Processing Systems, 36, 2024. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [8] Gamaleldin F. Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael Curtis Mozer, and Thomas Kipf. Savi++: Towards end-to-end object-centric learning from real-world videos.",
    "slotssm-7": "ArXiv, abs/2206.07764, 2022. [9] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards language modeling with state space models.",
    "slotssm-8": "In International Conference on Learning Representations, 2023. [10] Rohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning.",
    "slotssm-9": "In International Conference on Learning Representations, 2020. [11] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It\u2019s raw! Audio generation with state-space models. In International Conference on Machine Learning, 2022. [12] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Sch\u00f6lkopf. Recurrent independent mechanisms. ArXiv, abs/1909.10893, 2021. [13] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam H. Laradji, Hsueh-Ti Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: A scalable dataset generator.",
    "slotssm-10": "arXiv preprint arXiv:2203.03570, 2022. [14] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "slotssm-11": "arXiv preprint arXiv:2312.00752, 2023. [15] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent memory with optimal polynomial projections.",
    "slotssm-12": "In Advances in Neural Information Processing Systems, 2020. [16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [17] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [18] Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Advances in Neural Information Processing Systems, 2021. [19] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [20] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear RNNs. arXiv preprint arXiv:2212.00768, 2022. [21] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In ECCV, 2022. [22] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn. Object-centric slot diffusion. Advances in Neural Information Processing Systems, 36, 2024. [23] Thomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional Object-Centric Learning from Video. arXiv preprint arXiv:2111.12594, 2021. [24] David M Knigge, David W Romero, Albert Gu, Efstratios Gavves, Erik J Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn, and Jan-jakob Sonke. Modelling long range dependencies in D: From task-specific to a general purpose CNN. In International Conference on Learning Representations, 2023. [25] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention, 2020. [26] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [27] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In International Conference on Learning Representations, 2023. [28] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces.",
    "slotssm-13": "In Advances in Neural Information Processing Systems, 2022. [29] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, 2023. [30] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, 2013. [31] Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering memory tasks with world models.",
    "slotssm-14": "In The Twelfth International Conference on Learning Representations, 2024. [32] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose.",
    "slotssm-15": "In International Conference on Learning Representations, 2022. [33] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural Systematic Binder.",
    "slotssm-16": "In International Conference on Learning Representations, 2023. [34] Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for complex and naturalistic videos. arXiv preprint arXiv:2205.14065, 2022. [35] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In International Conference on Learning Representations, 2023. [36] Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, and Cordelia Schmid. Does visual pretraining help end-to-end reasoning? Advances in Neural Information Processing Systems, 36, 2024. [37] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers.",
    "slotssm-17": "In International Conference on Learning Representations, 2021. [38] Yao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov. Capsules with inverted dot-product attention routing.",
    "slotssm-18": "In International Conference on Learning Representations, 2020. [39] Sjoerd Van Steenkiste, Michael Chang, Klaus Greff, and J\u00fcrgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions.",
    "slotssm-19": "arXiv preprint arXiv:1802.10353, 2018. [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. [41] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In CVPR, 2023. [42] Nicholas Watters, Loic Matthey, Christopher P. Burgess, and Alexander Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. arXiv preprint arXiv:1901.07017, 2019. [43] Yi-Fu Wu, Klaus Greff, Gamaleldin Fathy Elsayed, Michael Curtis Mozer, Thomas Kipf, and Sjoerd van Steenkiste. Inverted-attention transformers can learn object representations: Insights from slot attention. In UniReps: the First Workshop on Unifying Representations in Neural Models, 2023. [44] Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Generative video transformer: Can objects be the words? In International Conference on Machine Learning, pages 11307\u201311318. PMLR, 2021. [45] Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, and Animesh Garg. Slotformer: Unsupervised visual dynamics simulation with object-centric models.",
    "slotssm-20": "arXiv preprint arXiv:2210.05861, 2022. [46] Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Animesh Garg. Slotdiffusion: Object-centric generative modeling with diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [47] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention, 2023. [48] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. In International Conference on Machine Learning, 2023. [49] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.",
    "slotssm-21": "arXiv preprint arXiv:2401.09417, 2024. [50] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. Efficient long sequence modeling via state space augmented Transformer. arXiv preprint arXiv:2212.08136, 2022. Appendix A Blinking Color Balls Benchmark\n\nA.1 Motivation\n\nReal-world videos are often inherently modular, involving multiple dynamic entities and their interactions across time. However, existing long-range reasoning tasks, such as those in the Long-Range Arena Benchmark [37], are typically designed to focus on single-object settings and recognizing a single dynamic pattern in the observations. To bridge this gap and facilitate more comprehensive evaluation, we propose the Blinking Color Balls Benchmark, a long-range visual reason benchmark desgined in a multi-object setting. A.2 Dataset Design\n\nWe provide an illustrative example of the dataset design in Figure 8. Each episode of the dataset contains a context-target pair . At each timestep in , all bouncing balls are first colored white, and then one ball is randomly picked and colored with one of 5 non-white colors. This process is repeated for all context frames, and it is represented in the rows in Figure 8(top). Note that the object picking and coloring are performed independently for each timestep, thus one ball could be selected none or multiple times and colorized with the same or different colors across different timesteps. The target images are then constructed with two rules: Earliest Color and Most Frequent Color. The Earliest Color rule picks the earliest non-white color assigned to the ball as the final color, while the Most Frequent Color rule counts the assignment of each non-white color and picks the color with the highest count (if there are ties, the earlier color among the highest is chosen). In Figure 8, we differentiate the two datasets using the same context sequence, which will result in different target images based on the rule. Note that regardless of the color assignment, the objects are moving and follow the physical bouncing rules throughout the full sequence. More image samples can be found in Figure 9. Finally, as illustrated in Figure 5(a), we transform the conditional image generation task into a long-range reasoning task by using patchified context images as input. Instead of providing the context images directly to the model, we flatten non-overlapping patches of the original images to create a long input sequence. Given patches per image, the context length becomes . Note that patchification is used intentionally to construct long sequences for the benchmark; SlotSSMs in general do not inherently require patchified inputs and instead use a Slot Encoder to extract slots as input at each time step. A.3 Challenges and Qualitative Comparison\n\nThe Blinking Color Balls tasks pose significant challenges for the models, as they are required to associate partial views of objects from temporally distant patches to learn their movement patterns and color assignment rules. We can define two levels of challenges: (1) identifying each object from the patches and predicting their positions based on their dynamics, and (2) determining the final color assignment of each object based on the given rules. The first challenge is relatively straightforward, as it primarily involves the dynamics of objects from the immediate past two frames prior to the target time step. However, the second challenge is particularly difficult, as it requires the model to reason over the entire input sequence, necessitating the identification of an object\u2019s history from partially observed patches in a long-range context. Figure 10 presents a qualitative comparison of the models\u2019 performance on the task. The results reveal a clear categorization of the models based on their capability to address the two levels of challenges. The baseline RIM model successfully predicts the object positions in the target image but struggles with learning the color assignment rules. Consequently, it predicts the color white that generally have the highest appearance probability for all objects. Note that the rendered images are based on the argmax of the logits over the color categories. Models such as SlotRNN and Single State SSM demonstrate the ability to learn color assignments, but they make mistakes in some cases. In contrast, SlotSSM and SlotTransformer successfully achieve both accurate position prediction and color assignment. Appendix B Additional Implementation Details\n\nB.1 SlotSSMs and OC-SlotSSMs\n\nSlot Encoder. The main difference between the SlotSSMs and OC-SlotSSMs variants is in the design of the Slot Encoders as illustrated in Figure 11. The Slot Encoder in SlotSSMs is implemented as a multi-layer transformer with self-attention and cross-attention modules. Given the input tokens , the structure of each layer in the Slot Encoder can be delineated into three modules:\n\n\ud835\udc9e t subscript \ud835\udc9e \ud835\udc61 \\displaystyle\\mathcal{C}_{t} = SelfAttn \u200b ( \ud835\udc9e t ) , absent SelfAttn subscript \ud835\udc9e \ud835\udc61 \\displaystyle=\\mathrm{SelfAttn}(\\mathcal{C}_{t})\\ , (18) \ud835\udc9e t subscript \ud835\udc9e \ud835\udc61 \\displaystyle\\mathcal{C}_{t} = CrossAttn \u200b ( q = \ud835\udc9e t , kv = \ud835\udcb3 t ) , absent CrossAttn formulae-sequence q subscript \ud835\udc9e \ud835\udc61 kv subscript \ud835\udcb3 \ud835\udc61 \\displaystyle=\\mathrm{CrossAttn}\\left(\\texttt{q}=\\mathcal{C}_{t},\\ \\texttt{kv}=\\mathcal{X}_{t}\\right)\\ , (19) \ud835\udc9e t subscript \ud835\udc9e \ud835\udc61 \\displaystyle\\mathcal{C}_{t} = MLP \u200b ( \ud835\udc9e t ) .",
    "slotssm-22": "absent MLP subscript \ud835\udc9e \ud835\udc61 \\displaystyle=\\mathrm{MLP}(\\mathcal{C}_{t})\\ . (20)\n\nWe use 3 layers in all our experiments. Note that we also apply skip connections and layer normalization in the input for all three modules, but have omitted them in the equations for brevity. The regular cross-attention used here employs softmax normalization over the attention weights applied to the input tokens:\n\nQ \ud835\udc44 \\displaystyle Q = W Q \u200b ( \ud835\udc9e t ) , K = W K \u200b ( \ud835\udcb3 t ) , V = W V \u200b ( \ud835\udcb3 t ) , formulae-sequence absent subscript \ud835\udc4a \ud835\udc44 subscript \ud835\udc9e \ud835\udc61 formulae-sequence \ud835\udc3e subscript \ud835\udc4a \ud835\udc3e subscript \ud835\udcb3 \ud835\udc61 \ud835\udc49 subscript \ud835\udc4a \ud835\udc49 subscript \ud835\udcb3 \ud835\udc61 \\displaystyle=W_{Q}(\\mathcal{C}_{t}),\\quad K=W_{K}(\\mathcal{X}_{t}),\\quad V=W_{V}(\\mathcal{X}_{t})\\ , (21) \ud835\udc9e t out subscript superscript \ud835\udc9e out \ud835\udc61 \\displaystyle\\mathcal{C}^{\\text{out}}_{t} = softmax \u200b ( Q \u200b K T D , axis=\u2018keys\u2019 ) \u200b V . absent softmax \ud835\udc44 superscript \ud835\udc3e \ud835\udc47 \ud835\udc37 axis=\u2018keys\u2019 \ud835\udc49 \\displaystyle=\\texttt{softmax}\\left(\\frac{QK^{T}}{\\sqrt{D}},\\quad\\texttt{axis=`keys'}\\right)V\\ . (22)\n\nIn the OC-SlotSSMs layers, the Slot Encoder is implemented as a single inverted attention layer. This layer differs from the regular cross attention by the way attention weights are normalized:\n\nQ \ud835\udc44 \\displaystyle Q = W Q \u200b ( \ud835\udc9e t ) , K = W K \u200b ( \ud835\udcb3 t ) , V = W V \u200b ( \ud835\udcb3 t ) , formulae-sequence absent subscript \ud835\udc4a \ud835\udc44 subscript \ud835\udc9e \ud835\udc61 formulae-sequence \ud835\udc3e subscript \ud835\udc4a \ud835\udc3e subscript \ud835\udcb3 \ud835\udc61 \ud835\udc49 subscript \ud835\udc4a \ud835\udc49 subscript \ud835\udcb3 \ud835\udc61 \\displaystyle=W_{Q}(\\mathcal{C}_{t}),\\quad K=W_{K}(\\mathcal{X}_{t}),\\quad V=W_{V}(\\mathcal{X}_{t})\\ , (23) A \ud835\udc34 \\displaystyle A = softmax \u200b ( Q \u200b K T D , axis=\u2018queries\u2019 ) , absent softmax \ud835\udc44 superscript \ud835\udc3e \ud835\udc47 \ud835\udc37 axis=\u2018queries\u2019 \\displaystyle=\\texttt{softmax}\\left(\\frac{QK^{T}}{\\sqrt{D}},\\quad\\texttt{axis=`queries'}\\right)\\ , (24) A i , j subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \\displaystyle A_{i,j} = A i , j \u2211 j = 1 N K A i , j , absent subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc57 1 subscript \ud835\udc41 \ud835\udc3e subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 \\displaystyle=\\dfrac{A_{i,j}}{\\sum_{j=1}^{N_{K}}A_{i,j}}, (25) \ud835\udc9e t out subscript superscript \ud835\udc9e out \ud835\udc61 \\displaystyle\\mathcal{C}^{\\text{out}}_{t} = A \u200b V . absent \ud835\udc34 \ud835\udc49 \\displaystyle=AV\\ . (26)\n\nThe inverted attention layer applies softmax normalization over the queries, introducing a competition among the query tokens over the attention to the input tokens and thereby promoting disentanglement for the input tokens. SSM Blocks. For the implementation of the SSM models, we leverage recent advances in linear state space models and design our SSM block in SlotSSM based on the Mamba architecture [14]. The block-diagonal transition of slots is implemented as parallel runs of SSM blocks that share the same model weights. { \ud835\udc9a t \u2223 l k } k = 1 K l , { \ud835\udc89 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc9a conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 superscript subscript superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{y}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}},\\ \\{\\bm{h}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SlotSSM \u200b ( { \ud835\udc94 t \u2223 l k } k = 1 K l , { \ud835\udc89 t \u2212 1 \u2223 l k } k = 1 K l ) absent SlotSSM superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 superscript subscript superscript subscript \ud835\udc89 \ud835\udc61 conditional 1 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{SlotSSM}\\!\\left(\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}},\\ \\{\\bm{h}_{t-1\\mid l}^{k}\\}_{k=1}^{K_{l}}\\right) (27) \u27f9 \ud835\udc9a t \u2223 l k , \ud835\udc89 t \u2223 l k \u27f9 superscript subscript \ud835\udc9a conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \\displaystyle\\Longrightarrow\\qquad\\bm{y}_{t\\mid l}^{k},\\bm{h}_{t\\mid l}^{k} = MambaBlock \u200b ( \ud835\udc94 t \u2223 l k , \ud835\udc89 t \u2212 1 \u2223 l k ) , \u2200 k \u2208 { 1 , \u2026 , K l } formulae-sequence absent MambaBlock superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 superscript subscript \ud835\udc89 \ud835\udc61 conditional 1 \ud835\udc59 \ud835\udc58 for-all \ud835\udc58 1 \u2026 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{MambaBlock}\\!\\left(\\bm{s}_{t\\mid l}^{k},\\bm{h}_{t-1\\mid l}^{k}\\right),\\quad\\forall k\\in\\{1,\\dots,K_{l}\\} (28)\n\nWe include pseudo-code of the Mamba block implementation in Algorithm 1. For a more detailed description of the Mamba architecture and its underlying principles, we refer the readers to the original paper [14]. B.2 Baseline Models\n\nWe use the official implementation of RIM from GitHub 111https://github.com/anirudh9119/RIMs, as well as the SAVi implementation from STEVE 222https://github.com/singhgautam/steve. We describe the implementation of the proposed baselines SlotRNN and SlotTransformer in the following. SlotRNN. SlotRNN adopts a similar design to SlotSSM, but replaces the SSMs with GRUs [3]. In this architecture, the slots are processed in parallel across different slots at each time step and sequentially across time steps. The implementation of each layer is summarized as follows. { \ud835\udc94 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SlotEncoder \u200b ( { \ud835\udc99 t \u2223 l \u2212 1 k } k = 1 K l \u2212 1 ) , absent SlotEncoder superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 1 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 1 \\displaystyle=\\mathrm{SlotEncoder}\\!\\left(\\{\\bm{x}_{t\\mid l-1}^{k}\\}_{k=1}^{K_{l-1}}\\right)\\ , (29) \ud835\udc89 t \u2223 l k superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \\displaystyle\\bm{h}_{t\\mid l}^{k} = GRU \u200b ( \ud835\udc94 t \u2223 l k , \ud835\udc89 t \u2212 1 \u2223 l k ) , \u2200 k \u2208 { 1 , \u2026 , K l } , formulae-sequence absent GRU superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 superscript subscript \ud835\udc89 \ud835\udc61 conditional 1 \ud835\udc59 \ud835\udc58 for-all \ud835\udc58 1 \u2026 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{GRU}\\!\\left(\\bm{s}_{t\\mid l}^{k},\\bm{h}_{t-1\\mid l}^{k}\\right),\\quad\\forall k\\in\\{1,\\dots,K_{l}\\}\\ , (30) { \ud835\udc89 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{h}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SelfAttention \u200b ( { \ud835\udc89 t \u2223 l k } k = 1 K l ) , absent SelfAttention superscript subscript superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{SelfAttention}\\!\\left(\\{\\bm{h}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}}\\right)\\ , (31) { \ud835\udc99 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{x}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = { \ud835\udc89 t \u2223 l k } k = 1 K l absent superscript subscript superscript subscript \ud835\udc89 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\{\\bm{h}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} (32)\n\nSlotTransformer. SlotTransformer uses the same SlotEncoder as SlotSSM to obtain slot representations.",
    "slotssm-23": "At each time step, the slots from the current step are concatenated with the slots from all previous time steps. This combined sequence is then processed using a Transformer with causal mask in time dimension which ensures that each slot can only obtain information from prior or current time steps. The implementation of each layer is summarized as follows:\n\n{ \ud835\udc94 t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}} = SlotEncoder \u200b ( { \ud835\udc99 t \u2223 l \u2212 1 k } k = 1 K l \u2212 1 ) , absent SlotEncoder superscript subscript superscript subscript \ud835\udc99 conditional \ud835\udc61 \ud835\udc59 1 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 1 \\displaystyle=\\mathrm{SlotEncoder}\\!\\left(\\{\\bm{x}_{t\\mid l-1}^{k}\\}_{k=1}^{K_{l-1}}\\right)\\ , (33) { \ud835\udc99 <= t \u2223 l k } k = 1 K l superscript subscript superscript subscript \ud835\udc99 absent conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle\\{\\bm{x}_{<=t\\mid l}^{k}\\}_{k=1}^{K_{l}} = Transformer \u200b ( { \ud835\udc94 t \u2223 l k } k = 1 K l \u222a { \ud835\udc94 < t \u2223 \u200b l k } k = 1 K l ) , absent Transformer superscript subscript superscript subscript \ud835\udc94 conditional \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 superscript subscript superscript subscript \ud835\udc94 bra \ud835\udc61 \ud835\udc59 \ud835\udc58 \ud835\udc58 1 subscript \ud835\udc3e \ud835\udc59 \\displaystyle=\\mathrm{Transformer}\\!\\left(\\{\\bm{s}_{t\\mid l}^{k}\\}_{k=1}^{K_{l}}\\cup\\{\\bm{s}_{<t\\mid l}^{k}\\}_{k=1}^{K_{l}}\\right)\\ , (34)\n\nB.3 Blinking Color Balls Experiemtns\n\nWe show the hyperparameters used in the experiments in Table 2.",
    "slotssm-24": "Input Tokenizer. Each patch in the input sequence is treated as an image and further split into non-overlapping patches of size . Each patch is then augmented with spatial and temporal positional embeddings, followed by an layer to compute the final tokens for the Slot Encoder. Decoder. During image decoding, we use a self-cross attention layer with positional embeddings as input and slots as context. Given the positional embeddings and slots from SlotSSM , each layer of the transformer decoder can be described as follows:\n\n\ud835\udcab t subscript \ud835\udcab \ud835\udc61 \\displaystyle\\mathcal{P}_{t} = SelfAttn \u200b ( \ud835\udcab t ) , absent SelfAttn subscript \ud835\udcab \ud835\udc61 \\displaystyle=\\mathrm{SelfAttn}(\\mathcal{P}_{t})\\ , (35) \ud835\udcab t subscript \ud835\udcab \ud835\udc61 \\displaystyle\\mathcal{P}_{t} = CrossAttn \u200b ( q = \ud835\udcab t , kv = \ud835\udcae t ) absent CrossAttn formulae-sequence q subscript \ud835\udcab \ud835\udc61 kv subscript \ud835\udcae \ud835\udc61 \\displaystyle=\\mathrm{CrossAttn}\\left(\\texttt{q}=\\mathcal{P}_{t},\\ \\texttt{kv}=\\mathcal{S}_{t}\\right)\\, (36) \ud835\udcab t subscript \ud835\udcab \ud835\udc61 \\displaystyle\\mathcal{P}_{t} = MLP \u200b ( \ud835\udcab t ) .",
    "slotssm-25": "absent MLP subscript \ud835\udcab \ud835\udc61 \\displaystyle=\\mathrm{MLP}(\\mathcal{P}_{t})\\ . (37)\n\nWe use a total of 3 layers, and the final pixel logits are computed using a linear head. Training Objective. During training, we transform the image prediction problem into a pixel-wise classification task. Specifically, for a target image , we compute a quantization by categorizing each pixel into one of 7 discrete color categories:\n\n\ud835\udc99 N Q \u200b ( i , j ) = Q \u200b ( \ud835\udc99 N \u200b ( i , j ) ) \u2200 i \u2208 { 1 , 2 , \u2026 , H } , j \u2208 { 1 , 2 , \u2026 , W } formulae-sequence subscript superscript \ud835\udc99 \ud835\udc44 \ud835\udc41 \ud835\udc56 \ud835\udc57 \ud835\udc44 subscript \ud835\udc99 \ud835\udc41 \ud835\udc56 \ud835\udc57 formulae-sequence for-all \ud835\udc56 1 2 \u2026 \ud835\udc3b \ud835\udc57 1 2 \u2026 \ud835\udc4a \\bm{x}^{Q}_{N}(i,j)=Q(\\bm{x}_{N}(i,j))\\quad\\forall\\;i\\in\\{1,2,\\ldots,H\\},\\;j\\in\\{1,2,\\ldots,W\\} (38)\n\nwhere is the quantization function that maps a 3-dimensional color vector to one of the 7 color categories in the set .",
    "slotssm-26": "Each represents a color vector corresponding to a discrete color category. This is a lossless quantization process since the raw images are generated with the same set of discrete colors. The final training objective is the cross-entropy loss between the model output and the target :\n\n\u2112 = \u2212 \u2211 i = 1 H \u2211 j = 1 W \u2211 k = 1 6 \ud835\udc99 N Q \u200b ( i , j , k ) \u200b log \u2061 ( \ud835\udc99 ^ N \u200b ( i , j , k ) ) \u2112 superscript subscript \ud835\udc56 1 \ud835\udc3b superscript subscript \ud835\udc57 1 \ud835\udc4a superscript subscript \ud835\udc58 1 6 subscript superscript \ud835\udc99 \ud835\udc44 \ud835\udc41 \ud835\udc56 \ud835\udc57 \ud835\udc58 subscript ^ \ud835\udc99 \ud835\udc41 \ud835\udc56 \ud835\udc57 \ud835\udc58 \\mathcal{L}=-\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\sum_{k=1}^{6}\\bm{x}^{Q}_{N}(i,j,k)\\log(\\hat{\\bm{x}}_{N}(i,j,k)) (39)\n\nB.4 Unsupervised Object-Centric Learning Experiments\n\nThe hyperparameters used in the experiments are presented in Table 2. Table 4 details the structure of the spatial broadcast decoder described in Section 5.2. To compute the input tokens, the input images are first processed by a CNN network to generate a 2D feature map. The architecture of the CNN network is described in Table 3. We use a downsampling factor of 2, resulting in an output 2D feature map of size for an input image size of . The 2D feature map is then flattened into a sequence of length 4096 and provided to the inverted attention mechanism. B.5 3D Visual Reasoning Experiments\n\nModel Configuration For both SlotSSM and OC-SlotSSM, we employ architectures similar to those described in Table 2, with a few modifications to accommodate the complexity of the 3D visual reasoning task.",
    "slotssm-27": "We use 12 slots and a hidden size of 64 for all modules to reduce memory consumption. Additionally, we utilize a spatial broadcast decoder for all models during the pre-training phase. Due to limited computational resources, we apply a model size of 8 for both pre-training and fine-tuning/direct training phases. It is worth noting that the model performance could potentially be further improved with a properly tuned batch size during training. Data Augmentation To prevent overfitting, we apply data augmentation on all training phases, following the practices of prior works [44, 36]. During the model pre-training phase, we randomly sample 32 frames from the original 300 frames. Note that the sampled frames may not be consecutive, which is an intentional aspect of the data augmentation. For the fine-tuning or direct training phase, we first split the 300 frames with non-overlapping segments of length 6, then we randomly select 1 frame from each segment to construct a input sequence of length 50. Predictor To predict the location of the snitch, we employ a transformer-based predictor. Specifically, we concatenate the learned slot representations with a learned CLS token and feed the resulting sequence into a 2-layer transformer. The output CLS token is then passed through a linear layer to predict the logits for snitch localization. Appendix C Emerging Modularity in SlotSSMs\n\nTo gain further insights into the learned representations of the slot-based models, we investigate how the slots are utilized in the image generation process. This can be done by visualizing the attention mechanisms in the decoders. Figure 12 presents the results of this analysis. For the transformer decoders used in the video prediction and blinking color balls tasks, we compute the argmax over the slots in the cross-attention map (Eq. 36), which represents the attention of the positional tokens over the slots employed to obtain information for reconstruction at each position. In the case of the spatial broadcast decoder, we take the argmax over the alpha-mixing logits (Eq. 17). The visualizations reveal that each slot tends to specialize in representing a specific object or a coherent part of the scene. This emerged object-centric representation allows the model to efficiently capture the dynamics and interactions of the objects, leading to improved performance in tasks such as video prediction and reasoning in the blinking color balls benchmark. Interestingly, even though the slot encoder used in the video prediction and blinking color balls benchmarks does not explicitly enforce spatial disentanglement constraints like the inverted attention mechanism in OC-SlotSSMs does, the models still learn to represent the sequences in an object-centric manner. This emergent modularity suggests that the SlotSSM design can naturally encourages the model to discover and exploit the underlying structure of the data which is a crucial capability for modeling complex visual inputs such as real-world videos.",
    "slotssm-28": "\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Jul 5 23:20:04 2024 by LaTeXML"
}