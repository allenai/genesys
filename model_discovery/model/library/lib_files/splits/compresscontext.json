{
    "compresscontext-0": "# Adapting Language Models to Compress Contexts \n\nAlexis Chevalier* Alexander Wettig* Anirudh Ajith Danqi Chen<br>Department of Computer Science \\& Princeton Language and Intelligence<br>Princeton University<br>\\{achevalier, anirudh.ajith\\}@princeton.edu<br>\\{awettig, danqic\\}@cs.princeton.edu\n\n\n#### Abstract\n\nTransformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.",
    "compresscontext-1": "We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts. ${ }^{1}$\n\n\n## 1 Introduction\n\nTransformer-based (Vaswani et al., 2017) language models (LMs) have recently seen a sharp rise in popularity and are now receiving millions of queries, processing billions of tokens, and generating text for a wide variety of applications (Brown et al., 2020; Touvron et al., 2023; Zhang et al.,\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-01.jpg?height=569&width=738&top_left_y=732&top_left_x=1090)\n\nFigure 1: AutoCompressors process long documents by recursively generating summary vectors which are passed as soft prompts to all subsequent segments. 2022). With this rise in popularity comes the challenge for researchers to make LMs more efficient, to speed up inference and to deploy LMs at scale, while increasing their versatility, thus allowing users to process more data in new ways. With these goals in mind, we propose to teach pre-trained LMs the ability to compress text into summary vectors. Summary vectors are short soft prompts (Lester et al., 2021), one or two orders of magnitude shorter than the pre-compressed plain text, that are obtained from the output states of a language model. Summary vectors serve two general purposes: they can help extend the language model's context window to very long documents with minimal computational overhead, and they help speed up inference on text for which summary vectors have been pre-computed and cached. Our models, which we call AutoCompressors, are trained with a simple unsupervised learning objective that encourages the model to store essential information in the summary vectors. Summary vectors are produced segment by segment from long documents and are used to improve language modeling in future segments (Figure 1). Our work\nbuilds on the recently proposed RMT architecture (Bulatov et al., 2022) with a crucial difference: we introduce summary accumulation, in which summary vectors from all segments are concatenated to produce the summary of the entire document. We also train AutoCompressors with randomly segmented inputs so they can better compress contexts of variable lengths in downstream tasks. We show that these innovations improve long-range information retention and enable new ways of reasoning over multiple passages. AutoCompressors can be initialized with pretrained LMs to produce powerful and versatile models. We fine-tune AutoCompressors from OPT2.7B (Zhang et al., 2022) and Llama-2-7B (Touvron et al., 2023) models on sequences from 6,144 up to 30,720 tokens with a single NVIDIA A100 GPU of 80 GB memory. We show that summary vectors are effective for improving perplexity over long documents and that these compression capabilities are robust to domain generalization. Our analysis suggests that AutoCompressors are able to reason over summary vectors, making them useful for a diverse set of downstream applications. We apply AutoCompressors to in-context learning (ICL) by compressing up to 90 in-context demonstrations. We consider 11 classification tasks, including 7 SuperGLUE tasks (Wang et al., 2019), and we find that summary vectors outperform few-shot ICL with a comparable number of in-context tokens on 8 out of 11 tasks. Finally, we explore two applications where AutoCompressors can reduce inference costs by pre-computing summary vectors for large corpora. First, we adopt a setting for retrieval-augmented language modeling (Shi et al., 2023). We find that for equal sequence lengths, using summary vectors achieves $1.5 \\times$ the perplexity gains compared to plain-text passages, and outperforms retrievalaugmented methods for similar computational budgets. Secondly, we consider a zero-shot passage re-ranking task (Sachan et al., 2022). We establish that re-ranking passages based on their summary vectors achieves the best trade-off between re-ranking performance and inference throughput. In summary, our main contributions are the following: (1) We introduce a method for extending LMs to long context windows under small-scale computational requirements by learning to generate summary vectors. We propose summary accumulation and training with randomized segmenting as key features of AutoCompressors. (2) We show that summary vectors encode useful information for downstream tasks and can be used to reduce the inference cost of in-context learning. (3) We demonstrate the benefits of pre-computing summary vectors for large corpora and using AutoCompressors in conjunction with retrievers. ## 2 Related Work\n\nSoft prompts Soft prompt tuning is an effective method to adapt pre-trained Transformers without updating existing parameters (Lester et al., 2021; Zhong et al., 2021; Liu et al., 2022). Newly initialized embeddings are prepended to the input sequence (the \"soft prompt\"), and optimization is performed with respect to these new parameters while the rest of the model is frozen. It is one of many parameter-efficient fine-tuning methods (Lialin et al., 2023) and is related to prefix tuning, where newly initialized parameters are prepended to the attention states instead (Li and Liang, 2021). Prompt compression Wingate et al. (2022) propose to learn a soft prompt $\\sigma$ to compress the information contained in a context $x$. Given a pretrained language model $p_{\\mathrm{LM}}$, they draw continuations $y \\sim p_{\\mathrm{LM}}(\\cdot \\mid x)$ based on $x$ and use a distillation objective to align the model's predictions conditioned on the soft prompt $p_{\\mathrm{LM}}(y \\mid \\sigma)$ to the predictions conditioned on the context $p_{\\mathrm{LM}}(y \\mid x)$. Wingate et al. (2022) find that soft prompts retain high-level information and facilitate controllable generation. However, the approach requires running the optimization for every new context $x$, with no knowledge transfer between similar contexts. In contrast, our AutoCompressors learn to predict their own soft prompts $\\sigma$ as a function of $x$. Context distillation A related line of work (Askell et al., 2021; Snell et al., 2022) aims to distill incontext information, e.g., instructions, into an unprompted student model. In concurrent work, Mu et al. (2023) teach models to compress instructions into short key-value attention prefixes. Our approach differs by learning to compress any context information, including long documents, and results in more compact soft prompts. Long-range Transformers A number of architectural modifications have been proposed to scale Transformers to longer context lengths while reducing the high memory costs of full attention. These include restricting and sparsifying the attention window (Dai et al., 2019; Child et al., 2019), ap-\nproximating the attention (Rae et al., 2020; Zheng et al., 2022; Choromanski et al., 2021), as well as introducing recurrent elements (Ma et al., 2022; Bulatov et al., 2022), conditional computation (Ainslie et al., 2023), and retrieving previous tokens from the context at the output layer (Zhong et al., 2022). See Tay et al. (2022) for a comprehensive survey of efficient long-range architectures. Most of these architectures typically require expensive training from scratch, or will deviate substantially from a pre-trained initialization. ${ }^{2}$ Moreover, many language models lack the inductive bias to extrapolate to longer sequences (Press et al., 2022). While AutoCompressors could in principle be trained from scratch, we show that they offer a straightforward solution for extending the context window of pre-trained models to longer sequences. ## 3 Method\n\nWe describe how we adapt a pre-trained language model to compress text into summary vectors. An overview of our architecture is shown in Figure 1. Summary vectors The AutoCompressor builds on the RMT architecture (Bulatov et al., 2022). We extend the input vocabulary of the base model by $\\kappa$ special summary tokens $\\langle\\text { Sum }\\rangle_{i}$ and initialize $\\kappa$ new input embeddings. ${ }^{3}$ When we append the sequence $\\langle\\text { Sum }\\rangle_{1} \\ldots\\langle\\text { Sum }\\rangle_{\\kappa}$ to an input, it signals to the model to output special summary vectors of the preceding context. These vectors can then be passed to the next text segment as a soft prompt of length $\\kappa$. Since the embedding spaces of pretrained language models can span thousands of dimensions, we expect that this mechanism has a high capacity for passing information to subsequent segments. Furthermore, a soft prompt can interpolate between many token embeddings, and therefore represent more abstract concepts than a single discrete token (Wingate et al., 2022). Summary accumulation We split long documents into segments $S_{1}, \\ldots, S_{n}$ and process them sequentially. Bulatov et al. (2022) incorporate information from previous segments by prepending the compressed summary $\\sigma_{i-1}$ produced from $S_{i-1}$ to the embedded inputs of $S_{i}$. We propose summary\n\n[^1]accumulation, which allows for a direct information pathway between each segment and all segments preceding it: we concatenate the summary vectors $\\sigma_{1} \\ldots, \\sigma_{i-1}$ to form $\\sigma_{<i}$ and prepend $\\sigma_{<i}$ to $S_{i}$. Note that the length of $\\sigma_{<i}$ is now $(i-1) \\kappa$, which grows linearly with the document length. Positional embeddings When using a base Transformer architecture with absolute positional embeddings, such as the OPT architecture (Zhang et al., 2022), we do not add positional embeddings to the summary tokens $\\langle\\text { Sum }\\rangle_{i}$, nor to the summary vectors. This allows us to use all pre-trained position embeddings as context tokens and makes it possible to scale the model to an arbitrary number of compression steps during training. The model still preserves the order of summary tokens due to their separate token embeddings. If the base Transformer uses relative positional embeddings, such as RoPE (Su et al., 2022), we apply the positional embedding to the summary tokens and vectors without any further modification. ### 3.1 Training Summary Vectors\n\nWe use a simple unsupervised training approach which encourages the model to learn to compress contexts over multiple steps. Training objective Write $\\left(x_{1}^{i}, \\ldots, x_{m_{i}}^{i}\\right)$ for the segment $S_{i}$ for every $i \\leq n$, where $m_{i}$ is the number of tokens in $S_{i}$. Conditioning on the concatenated summary vectors $\\sigma_{<i}$, we project the Transformer outputs with the language modeling head to obtain the next-token probabilities $p\\left(x_{t}^{i} \\mid x_{1}^{i}, \\ldots, x_{t-1}^{i}, \\sigma_{<i}\\right)$. We minimize the crossentropy loss over the entire document:\n\n$$\n\\mathcal{L}=-\\frac{1}{N} \\sum_{i=1}^{n} \\sum_{t=1}^{m_{i}} \\log p\\left(x_{t}^{i} \\mid x_{1}^{i}, \\ldots, x_{t-1}^{i}, \\sigma_{<i}\\right)\n$$\n\nwhere $N$ is the total number of tokens. This objective retains the pre-trained language model's abilities on the first segment $S_{1}$ and it incentivizes the model to store useful information in the summary vectors, which future segments can leverage to make better token predictions. Unlike Wingate et al. (2022), we do not train with a knowledge distillation objective, since the pre-trained LM has a limited context window as a teacher, whereas the AutoCompressor student learns to process much longer documents. Randomized segmenting We randomly vary the lengths $m_{i}$ of the segments $S_{i}$ during training, subject to the condition that each segment fits into\nthe model's context window. This allows AutoCompressors to compress documents of different lengths and improves performance under evaluation with fixed-length segments (see Figure 2). BPTT with stop-gradients We employ backpropagation through time (BPTT) and gradient checkpointing (Chen et al., 2016) for each segment to reduce the size of the computational graph. In addition, we compute and cache summary vectors and stop their gradients after 2 compression steps, similar to caching past attention states in TransformerXL training (Dai et al., 2019). This assumes that for learning to compress the useful information in $S_{i}$, it is sufficient to predict the tokens in the adjacent $S_{i+1}$. In Figure 2, we confirm that this incurs no penalty when predicting long segments, while further reducing GPU memory requirements. ## 4 Language Modeling Evaluation\n\nIn this section, we train AutoCompressors and evaluate their long-range language modeling capabilities by sampling long sequences which we split into segments of 2,048 tokens.",
    "compresscontext-2": "We fix the final segment and compress the previous $n$ segments. We track the perplexity of the final segment when conditioning on the summary vectors for each $n$. We conduct our main experiments and ablations with OPT models (Zhang et al., 2022) of 1.3B or 2.7B parameters, fine-tuned on 2B tokens from the Pile (Gao et al., 2020). In Section 4.1, we evaluate an AutoCompressor on sequences of 8,000 tokens and compare to an equivalent RMT model and an Extended Full Attention baseline. In Section 4.2, we fine-tune an AutoCompressor on sequences of 30,000 tokens to demonstrate the feasibility on very long sequences. Finally, in Section 4.3, we scale up AutoCompressors by fine-tuning a Llama-2-7B model on 15B tokens from RedPajama (TogetherAI, 2023). Full model hyperparameters and data information can be found in Appendix A. ### 4.1 Experiments on 8K-Token Sequences\n\nSetting We initialize all models with the 2.7Bparameter OPT model and fine-tune on 2B tokens from 4 domains form the Pile (Gao et al., 2020). Our AutoCompressor uses $\\kappa=50$ summary tokens and is fine-tuned with summary accumulation over four segments, each ranging from 1,024 to 2,048 tokens. Compressing 2,048 tokens into 50 summary vectors achieves a compression rate of 40 tokens per summary vector. We use the following baselines:\n\n1. We fine-tune an OPT-2.7B baseline on our data. This model is limited to sequences of 2,048 tokens due to pre-training. 2. Extended full attention: We fine-tune OPT-2.7B on sequences of up to 4,096 tokens by extending the model's positional embeddings. We initialize the embeddings for positions [2049..4096] with the embeddings for positions [1..2048].",
    "compresscontext-3": "We are not able to extend the context beyond 4,096 tokens due to GPU memory limitations. 3. RMT-2.7B: We fine-tune an RMT model on our data with $\\kappa=50$ summary vectors. We evaluate on documents of 8,192 tokens, drawn from the 4 training domains or 4 held-out domains. We generate summary vectors for up to 3 segments of 2,048 tokens, but also for single segments as short as 128 tokens. For the extended full-attention baseline we prepend the previous context tokens to the context window. Results We show the results in Table 1. We find that the AutoCompressor benefits from long contexts of 6,144 tokens and consistently outperforms the RMT model. We also find that the AutoCompressor benefits from much shorter sequences than seen during training, unlike RMT. See also Figure 2 and Table 6 for the usefulness of randomized segmenting. While extended full attention performs the best on 4,096-long sequences, we observe a trade-off for shorter contexts where AutoCompressors achieve the best performance. We also stress that the AutoCompressor attends to at most 150 additional soft prompts during evaluation, whereas the full attention model is given an additional 2,048 tokens. These trends hold for both in-domain and outof-domain evaluation. However, the gap between the AutoCompressor and the full-attention baseline increases in the out-of-domain setting, suggesting that the summary vectors generalize slightly less than pre-trained attention heads. ### 4.2 Experiments on 30K-Token Sequences\n\nSetting We fine-tune OPT-1.3B and OPT-2.7B as AutoCompressors on 2B tokens but train on sequences of 30,720 tokens with 20 compression steps. ${ }^{4}$ We use 50 summary tokens, randomized segmenting, and stop-gradients as before. We also\n\n[^2]| Segments <br> Context tokens | In-domain |  |  |  |  | Out-of-domain |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | - | - 1 | \u2014 | $-2-$ | $-3-$ | - | 1 - | \u2014 | $-2-$ | $-3-$ |\n|  | 128 | 512 | 2048 | 4096 | 6144 | 128 | 512 | 2048 | 4096 | 6144 |\n| Extended $\\mathrm{FA}^{\\dagger}$ | $6.33^{\\dagger} \\uparrow 1.0 \\%$ | $6.15^{\\dagger} \\downarrow 2.1 \\%$ | $5.94^{\\dagger}{ }_{5.4 \\%}$ | - | - | $8.57^{\\dagger}$ \u4e2a0.5\\% | $8.28^{\\dagger}+2.9 \\%$ | $7.93^{\\dagger}+7.0 \\%$ | - | - |\n| RMT | $6.42 \\uparrow 2.2 \\%$ | $6.19+1.4 \\%$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-05.jpg?height=41&width=114&top_left_y=433&top_left_x=768) | $6.02^{\\downarrow 4.1 \\%}$ | $6.01{ }_{\\downarrow 4.3 \\%}$ | $8.76 \\uparrow 2.7 \\%$ | $8.44_{\\downarrow 1.1 \\%}$ | $8.21 \\downarrow 3.8 \\%$ | $8.20 \\sqrt{ } 3.9 \\%$ | $8.20 \\downarrow 3.9 \\%$ |\n| AutoCompressor | $6.14{ }^{2} 2.2 \\%$ | $6.04 \\downarrow 3.8 \\%$ | $5.98+4.8 \\%$ | $5.94+5.4 \\%$ | 5.93 | $8.39+1.6 \\%$ | 8.26 | $8.17_{\\downarrow 4.2 \\%}$ | $8.12 \\sqrt{14.8 \\%}$ | 8.10 $\\ 5.0 \\%$ |\n\nTable 1: Held-out perplexity on 2,048 tokens, while varying the length of the preceding context (all the experiments are based on OPT-2.7B models). For RMT and AutoCompressor, we condition on summary vectors. We also report the perplexity gains compared to the fine-tuned OPT baseline without extra context, which achieves 6.28 in-domain and 8.53 out-of-domain (gains shown in colored numbers). $\\dagger$ : Although the extended full attention (Extended FA) achieves similar or slightly better perplexity, it uses up to 2,048 additional tokens and cannot extend further. However, the AutoCompressor uses only $50 \\times 3=150$ summary vectors to process 6,144 context tokens. | Segments <br> Context tokens | $-0-$ <br> 0 | $-7-$ <br> 14336 | $-14-$ <br> 28672 | CUDA <br> memory |\n| :---: | :---: | :---: | :---: | :---: |\n| RMT-1.3B | 13.18 | 12.50 | 12.50 | 54 GB |\n| AutoCompressor-1.3B | 13.21 | 12.49 | 12.47 | 38 GB |\n| RMT-2.7B | - | - | - | 00M |\n| AutoCompressor-2.7B | 11.86 | 11.21 | 11.18 | 75 GB |\n\nTable 2: Evaluation results for AutoCompressors trained on sequences of 30,720 tokens and evaluated on Books 3 (in-domain) and Gutenberg (out-of-domain). We train with a single NVIDIA A100 GPU and report the CUDA memory required for fine-tuning using a single sequence per batch. AutoCompressors require less memory because we stop gradients after two segments. | Segments | $-0-$ |  | 1 |  | $-2-$ | $-3-$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Context tokens | 0 | 128 | 512 | 2048 | 4096 | 6144 |\n| Llama-2 | 5.52 | 5.30 | 5.15 | 4.98 | - | - |\n| Extended FA | 5.40 | 5.19 | 5.06 | 4.88 | 4.80 | 4.76 |\n| AutoCompressor | 5.40 | 5.23 | 5.16 | 5.11 | 5.08 | $\\mathbf{5 . 0 7}$ |\n\nTable 3: Evaluation results for our AutoCompressor trained from Llama-2 7B on sequences of 6,144 tokens. For the AutoCompressor, we condition on summary vectors. For Llama-2 and the Extended Full Attention (Extended FA), we condition on plain text tokens. fine-tune an RMT model from OPT-1.3B, to use as a baseline. We are not able to fine-tune a $2.7 \\mathrm{~B}-$ parameter RMT baseline because the RMT method leads to an out-of-memory error. All models are evaluated on the final 2,048 heldout tokens of documents of size 30,720 tokens by compressing all previous 2,048-token segments. Results We collect our results in Table 2. The evaluation shows that both AutoCompressor models learn to utilize the entire 28 K tokens to reduce perplexity, while the RMT baseline does not benefit from doubling the number of context tokens from 14 K to 28 K . This shows that summary accumula- tion effectively captures long-range dependencies in documents. We also report the CUDA memory requirements for fine-tuning each model in Table 2. We train with one NVIDIA A100 GPU with 80GB of memory. Stopping gradients reduces CUDA memory and makes it possible to fine-tune an AutoCompressor from OPT-2.7B, while fine-tuning with RMT leads to out-of-memory at that scale. ### 4.3 Scaling Up AutoCompressors to Llama-2\n\nSetting We fine-tune a 7B-parameter Llama-2 model as an AutoCompressor on a single GPU by freezing the model and optimizing only the summary token embeddings and the attention weights via LoRA (Hu et al., 2022).",
    "compresscontext-4": "The model is trained on 15B tokens from RedPajama (TogetherAI, 2023), split into sequences of 6,144 tokens, and we use 50 summary tokens, randomized segmenting, and stop-gradients. We also fine-tune an Extended Full Attention baseline on the same dataset. The context window of the pre-trained model is extended by increasing the $\\theta$ value in RoPE following (Rozi\u00e8re et al., 2023). We compare both models to the pre-trained Llama-2-7B model, which has a context window of 4,096 tokens. All models are evaluated on the final 2,048 tokens of 8,192-token documents. Results We collect our results in Table 3. The AutoCompressor benefits from the entire context to reduce perplexity: compressing a 4,096-token context into 100 summary vectors achieves similar perplexity to the Extended Full Attention baseline with 512 plain text tokens, and compressing a 6,144 -token context into 150 summary vectors further improves perplexity slightly. Moreover, we find that summary vectors preserve perplexity when short contexts are compressed. However, Llama-2 and the Extended Full At-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-06.jpg?height=504&width=763&top_left_y=245&top_left_x=244)\n\nFigure 2: Perplexity on 2048 held-out tokens given different numbers of compressed tokens. Compression is performed on up to 3 segments of 2048 tokens. Ablations show that the different components of our finetuning strategy help boost performance and that stopgradients do not impact performance. tention baseline outperform the AutoCompressor when longer contexts are provided. Further research is needed to construct summary vectors that preserve all of the context information. ### 4.4 Analysis\n\nAblations We train OPT-2.7B models without randomized segmenting, summary accumulation, or stop gradients. The results are shown in Figure 2. We find that randomized segmenting leads to better compression of short segments, but still improves perplexity when compressing multiple 2048 token segments. As expected, summary accumulation helps improve perplexity beyond one compressed segment. We also confirm that stopping gradients does not impact performance despite reducing GPU memory requirements. In Table 2, we also show that stopping gradients helps reduce GPU memory. We also train AutoCompressors with $\\kappa=20$, 50,70 or 100 summary tokens and report the heldout perplexity results in Table 7 in the Appendix. Surprisingly, we find that performance does not increase with longer soft prompts, and $\\kappa=50$ performs the best overall. We hypothesize that learning a larger number of summary vectors may require a larger training budget. Token-level analysis We seek to better understand how summary vectors benefit individual token predictions. In Figure 5 in the Appendix, we show perplexity gains at each token position for the AutoCompressor with summary vectors and for the extended full-attention baseline. We find that conditioning on summary vectors improves perplexity over all 2048 token positions. We observe that the extended full attention baseline outperforms the AutoCompressor at the start of the sequence, whereas the AutoCompressor achieves the best performance towards the end of the sequence. This shows that summary vectors effectively capture long-range textual dependencies. In Appendix D, we show examples of sentences and tokens which benefit the most from summary vectors. We find that summary vectors contain salient information, such as names or dates, and that the model can reason over summary vectors. This confirms that summary vectors are useful summaries of the compressed text. ## 5 Compressing Demonstrations for In-Context Learning\n\nIn this section, we study the usefulness of summary vectors for performing downstream tasks. We show that in-context demonstrations can reliably be compressed down into summary vectors to improve performance while also increasing efficiency on a diverse set of NLP benchmarks. Evaluation We evaluate the in-context learning abilities of the AutoCompressor based on Llama2-7B from Section 4.3 on eleven classification and multiple-choice question-answering datasets. For each dataset, we evaluate the effect of compressing 1,2 or 3 segments of demonstrations into 50,100 or 150 summary vectors. For each segment, we include as many demonstrations as possible until we reach 750 tokens. For SST-2, this corresponds to 30 demonstrations per segment on average. We compare this compression approach with the results obtained by prompting the model using 150 and 750 tokens' worth of plain-text demonstrations. We use contextual calibration (Zhao et al., 2021) and class-balanced sampling when these techniques improve performance on a validation set. For each dataset, we report the mean accuracy and standard deviation over 7 random seeds. The detailed settings for each dataset can be found in Table 11. In Table 12 in the Appendix, we also compare the ICL performance of our OPT-2.7B based AutoCompressor models against the RMT baseline and a pre-trained OPT-2.7B, and include the performance of the pre-trained Llama-2-7B model.",
    "compresscontext-5": "Results We show evaluation results in Table 4. Results show that summary vectors consistently improve performance over the zero-shot baseline. Furthermore, summary vectors increase accuracy\n\n|  | AG News | SST-2 | BoolQ | WIC | WSC | RTE | CB | COPA | MultiRC | MR | Subj |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Zero-shot | $63.3_{(0.0)}$ | $67.7_{(0.0)}$ | $67.4_{(0.0)}$ | $50.8_{(0.0)}$ | $43.3_{(0.0)}$ | $58.8_{(0.0)}$ | $42.9_{(0.0)}$ | $52.5_{(0.0)}$ | $52.5_{(0.0)}$ | $57.4_{(0.0)}$ | $49.3_{(0.0)}$ |\n| 50 summary vecs.",
    "compresscontext-6": "| $79.6_{(4.9)}$ | $\\mathbf{9 4 . 2}_{(1.6)}$ | $\\mathbf{7 0 . 1}_{(3.3)}$ | $51.6_{(2.1)}$ | $47.7_{(8.7)}$ | $66.3_{(7.0)}$ | $46.4_{(18.7)}$ | $84.5_{(1.0)}$ | $52.6_{(2.8)}$ | $9_{\\left(21.5_{(1.0)}\\right.}$ | $53.5_{(3.6)}$ |\n| 100 summary vecs. | $\\mathbf{8 7 . 6}_{(1.2)}$ | $92.6_{(3.3)}$ | $66.3_{(2.8)}$ | $52.5_{(2.2)}$ | $42.9_{(2.5)}$ | $63.5_{(6.6)}$ | $\\mathbf{6 4 . 5}_{(5.9)}$ | $85.9_{(0.4)}$ | $\\mathbf{5 6 . 1}_{(1.2)}$ | $90.7_{(2.6)}$ | $57.0_{(5.6)}$ |\n| 150 summary vecs.",
    "compresscontext-7": "| $85.4_{(3.4)}$ | $92.3_{(2.9)}$ | $68.0_{(1.8)}$ | $\\mathbf{5 2 . 8}_{(1.5)}$ | $49.9_{(7.6)}$ | $65.3_{(6.6)}$ | $54.8_{(5.8)}$ | $\\mathbf{8 6 . 1}_{(0.6)}$ | $54.8_{(2.2)}$ | $91.1_{(2.2)}$ | $56.6_{(7.9)}$ |\n| ICL (150 tokens) | $74.5_{(2.2)}$ | $92.4_{(3.1)}$ | $67.4_{(0.0)}$ | $52.4_{(2.7)}$ | $\\mathbf{5 1 . 8}_{(6.9)}$ | $69.1_{(2.1)}$ | $46.4_{(23.0)}$ | $80.0_{(1.9)}$ | $52.5_{(0.0)}$ | $79.7_{(15.7)}$ | $57.9_{(10.7)}$ |\n| ICL (750 tokens) | $81.2_{(4.1)}$ | $93.8_{(1.2)}$ | $67.7_{(2.7)}$ | $52.4_{(2.0)}$ | $40.0_{(5.7)}$ | $\\mathbf{7 3 . 1}_{(3.5)}$ | $50.3_{(2.8)}$ | $82.6_{(1.6)}$ | $47.0_{(3.2)}$ | $\\mathbf{9 1 . 6}_{(0.8)}$ | $\\mathbf{6 0 . 7}_{(14.8)}$ |\n\nTable 4: Evaluation of the ICL performance of the Llama-2 7B model. Each summary is 50 tokens-long and corresponds to a segment of 750 tokens' worth of demonstrations. We also report accuracies when prompting the AutoCompressor with 150 and 750 tokens' worth of plaintext demonstrations as baselines. Note that for BoolQ and MultiRC, demonstrations are too long to fit into 150 tokens. compared to 150 tokens worth of plain demonstrations on $8 / 11$ tasks. On 8 tasks (AG News, SST-2, BoolQ, WiC, WSC, CB, COPA and MultiRC), summary vectors also out-perform ICL with 750 tokens' worth of plain text demonstrations. Summary vectors emerge as a strong alternative to plain text demonstrations, as they increase accuracy while reducing inference cost. In Table 12 (Appendix E), we find that the OPT2.7B AutoCompressor achieves higher accuracies than the RMT baseline on 8 out of 11 tasks and that the RMT model does not benefit from multiple compression steps. This shows that summary accumulation is an effective mechanism for compressing in-context demonstrations. We also observe that our fine-tuned Llama-2 AutoCompressor has substantially worse zero-shot accuracy on some tasks compared to the Llama-2 initialization, and slightly worse ICL performance. We suspect that this is due to domain mismatch in our fine-tuning data and the Llama-2 pre-training corpus. ## 6 Compressing Retrieval Corpora for Efficient Inference\n\nWe study the usefulness of pre-computing summary vectors for large collections of documents. These can be stored and later retrieved for efficient inference. Since inference is typically more expensive than storage, this approach has the potential to achieve good practical trade-offs. ### 6.1 Retrieval-augmented Language Modeling\n\nRetrieval-augmented language models improve token predictions by retrieving information from a data store. A number of approaches have been proposed to infuse external knowledge in the input layer (Guu et al., 2020; Shi et al., 2023), intermediate layers (Borgeaud et al., 2022) or at the output layer (Khandelwal et al., 2020; Zhong et al., 2022). ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-07.jpg?height=424&width=780&top_left_y=796&top_left_x=1049)\n\nFigure 3: Efficient retrieval-augmented language modeling with AutoCompressors. Large corpora can be pre-processed into compressed summary vectors which can be stored cheaply. Upon retrieval, compressed summaries are fused for efficient access to multiple documents in a single forward pass. REPLUG Our case study focuses on REPLUG (Shi et al., 2023), which is a simple method for combining a pre-trained language model with an offthe-shelf retriever to improve language modeling performance. Given access to an external corpus $\\mathcal{C}$, REPLUG retrieves $k$ passages $\\mathcal{D}=\\left\\{d_{1}, \\ldots, d_{k}\\right\\}$ based on a segment $x$ to score the next segment $y$. The overall probability for $y$ is computed by ensembling the predictions based on different passages:\n$p(y \\mid x, \\mathcal{D})=\\sum_{d \\in \\mathcal{D}} \\lambda(d, x) \\cdot p(y \\mid \\operatorname{CONCAT}(d, x))$,\nwhere $\\lambda(d, x)$ are the normalized similarity scores from the retriever and $\\operatorname{ConCAT}(d, x)$ denotes concatenation of $p$ and $x$. This method incurs a substantial overhead, since it requires $k$ forward passes over sequences $\\operatorname{CONCAT}(d, x, y)$. Fused Summaries We introduce a setting for retrieval-augmented language modeling close to fusion-in-decoder (Izacard and Grave, 2021). We concatenate the summary vectors of retrieved passages $\\mathcal{D}$ to form the fused summary vectors, $\\sigma_{\\mathcal{D}}=$ $\\operatorname{ConCAT}\\left(\\sigma_{d_{k}}, \\ldots, \\sigma_{d_{1}}\\right)$, where $d_{k}, \\ldots, d_{1}$ are ordered from least-to-most relevant. This resembles\n\n| Passages |  | Perplexity Gain (\\%) |  |  |  | Throughput (examples/s) |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | top-1 | top-2 | top-5 | top-10 | top-1 | top-2 | top-5 | top-10 |\n| 50 tokens | REPLUG | -0.64 | 0.58 | 1.68 | 2.35 | 51 | 38 | 16 | 9 |\n| 50 tokens | Fused Passages | 0.71 | 1.01 | 1.70 | 2.60 | 28 | 27 | 23 | 17 |\n| 512 tokens $\\rightarrow 50$ sum. vecs. | Fused Summaries | 1.04 | 1.67 | 2.63 | 3.74 | 28 | 27 | 23 | 17 |\n| 512 tokens | REPLUG | -1.47 | 2.24 | 5.25 | 8.30 | 18 | 10 | 6 | 3 |\n\nTable 5: PPL gains (\\%) from different retrieval-augmented language modeling settings, over the no-retrieval baseline. We evaluate the OPT-2.7B AutoCompressor and we report throughput on a single NVIDIA A100 GPU for each method without batching examples. Fused Summaries outperforms Fused Passages and REPLUG with 50-token passages. Moreover, Fused Summaries top-10 outperforms REPLUG top-2 with 512-token passages while also gaining a $1.7 \\times$ throughput increase. summary accumulation as described in Section 3. We also find it useful to smooth probability scores and re-order the retrieved passages based on their summary vectors (Appendix F). Figure 3 gives an overview of our approach. Fused Passages We establish a baseline for fusing summary vectors by concatenating the plaintext passages and computing smoothed probabilities, see Appendix F. Unlike summary vectors, this method is limited by the model's context window. Experiments We evaluate the OPT-2.7B AutoCompressor introduced in Section 4.1 without any additional fine-tuning.",
    "compresscontext-8": "Similar to Shi et al. (2023), we retrieve from the Pile. We use Books3, FreeLaw, GitHub, Wikipedia, Gutenberg, ArXiv, HackerNews, and YoutubeSubtitles. We index 10B tokens for each domain, which are split into passages of 512 or 50 tokens. We sample segments of 256 tokens from the Pile validation data, using the first 128 tokens as context $x$ for retrieval and the last 128 tokens $y$ for evaluation. We use the Contriever model (Izacard et al., 2022) for retrieval, and retrieve the top 10 passages. We also deduplicate our data by removing passages that overlap with $x$ by 64 tokens. Results Results are shown in Table 5. We find that Fused Summaries outperforms Fused Passages and REPLUG when 50-token passages are retrieved. We measure throughput empirically and show that for 10 retrieved documents, Fused Summary Vectors remains inexpensive. We note that compressing the 10B token datasets results in disk space of 5 TB per domain when stored in half-precision format. ${ }^{5}$ Therefore Fused Summaries achieves a good trade-off between storage costs and throughput. [^3]Moreover, Fused Summaries outperforms REPLUG top-2 with 512-token passages and sees a 1.7 x throughput increase, which shows that the model benefits from the diversity of compressed documents. However, REPLUG top-10 outperforms Fused Summaries. We leave it as future work to explore how to produce higher quality summary vectors to better utilize the compressed passages. We note that fusing summary vectors is effective despite a mismatch in training since we draw independent summary vectors from separate documents. Furthermore, our AutoCompressor model is only ever trained to accumulate 3 sets of summary vectors, and yet it benefits from fusing the summary vectors of up to 10 documents. ### 6.2 Unsupervised Passage Re-ranking\n\nFinally, we consider the case study of passage reranking, in which a fast off-the-shelf retriever like BM25 retrieves a large set of candidate passages, and a more capable re-ranker refines the ranking to increase the rank of the most relevant passages. Method Sachan et al. (2022) introduce an effective method for leveraging language models as re-rankers with no additional supervision or finetuning. Given a query $q$ and a set of candidate passages $\\left\\{p_{1}, \\ldots, p_{k}\\right\\}$, the language model scores the likelihood of the query $q$ conditioned on the prompt \"Passage: $\\left\\{p_{i}\\right\\}$. Please write a question based on this passage.\" for each passage $p_{i}$ and re-ranks the passages based on the scores. Experiments We consider the task of re-ranking BM25 passages on the NQ test set (Balachandran et al., 2021) and compare out-of-the-box AutoCompressors with 20 and 50 summary tokens to pretrained OPT models from 125 M to 2.7 B parameters. We pre-compute summary vectors for 21 M passages from a Wikipedia corpus (Karpukhin et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-09.jpg?height=663&width=743&top_left_y=248&top_left_x=245)\n\nFigure 4: We compare AutoCompressors (squares) in an unsupervised passage re-ranking setting to pre-trained language models (circles). The number on each data point shows how many passages retrieved by BM25 are re-ranked, and the vertical axis shows the Recall@20 performance of the re-ranking system on the NQ test set. We consider the throughput on a single NVIDIA A100 GPU and assume that multiple queries cannot be batched. By leveraging pre-computed summary vectors for passages, AutoCompressors lead to re-ranking solutions that lie on the Pareto front of recall vs. compute. 2020), which requires 2.1 TB and 5.4 TB disk space in half precision for 20 and 50 summary vectors respectively. We measure the quality of the re-ranked results using Recall@20. Results The results are shown in Figure 4. We measure throughput for individual un-batched queries on a single NVIDIA A100 80GB GPU and assume that the latency of loading summary vectors is negligible. Although the passages are only 100 words long, resulting in low compression rates, summary vectors substantially speed up the inference, while sacrificing on performance less than smaller models. This leads to a Pareto-optimal trade-off between compute and performance and demonstrates that summary vectors often retain sufficient information from a passage to assess its relevance for a particular query. ## 7 Conclusion\n\nWe have introduced a training strategy for adapting pre-trained LMs into AutoCompressors, which recursively compress contexts into summary vectors. Our experiments indicate that summary vectors retain important contextual information, that they can encode in-context demonstrations, and that they can be used in retrieval settings. Summary vectors can also be pre-computed, cached and re-used. This offers practical efficiency gains by reducing the size of the attention window. Significant future work remains in scaling AutoCompressors to bigger models and improving the quality of summary vectors to further close the gap with full attention over long-range contexts. ## Limitations\n\n1. We only apply AutoCompressors to OPT models of up to 2.7B parameters and a Llama model of 7B parameters. Future work needs to establish how AutoCompressors perform for even larger models. As the summary vector dimension grows, there is promise for retaining more information per vector. 2. Our results suggest that summary vectors ignore some useful information that is accessible via full attention. Additionally, models do not always benefit from increasing the number of summary vectors. We suspect that the training signal for learning summary vectors efficiently might be limited by pre-trained models being very good at making predictions from the plaintext tokens in the current segment. Future work is needed to improve this optimization. 3. Summary accumulation still leads to quadratic complexity with increasing number of segments, albeit at a much lower rate than full attention. Future work may explore ways to combine many summary vectors more efficiently. ## Acknowledgments\n\nWe thank Mengzhou Xia, Howard Chen, Vishvak Murahari, Aatmik Gupta, Zirui Wang, Jiatong Yu, and the members of the Princeton NLP group for helpful discussion and valuable feedback. This research is supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, and a Data Science Research Award from Adobe. AC also gratefully acknowledges support from the Minerva Research Foundation. ## References\n\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u00f1\u00f3n, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, YunHsuan Sung, and Sumit Sanghai. 2023. CoLT5: Faster long-range transformers with conditional computation.",
    "compresscontext-9": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.",
    "compresscontext-10": "Vidhisha Balachandran, Bhuwan Dhingra, Haitian Sun, Michael Collins, and William Cohen. 2021. Investigating the effect of background knowledge on natural questions. In Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 25-30, Online.",
    "compresscontext-11": "Association for Computational Linguistics. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems, volume 33, pages 1877-1901.",
    "compresscontext-12": "Curran Associates, Inc. Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. 2022. Recurrent memory transformer. In Advances in Neural Information Processing Systems. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. 2021. Rethinking attention with Performers. In International Conference on Learning Representations. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment.",
    "compresscontext-13": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy. Association for Computational Linguistics. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems. Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The CommitmentBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107-124.",
    "compresscontext-14": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The Pile: An 800 GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3929-3938. PMLR. R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, pages 785-794.",
    "compresscontext-15": "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, New Orleans, Louisiana. Association for Computational Linguistics. Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA.",
    "compresscontext-16": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Dominican Republic.",
    "compresscontext-17": "Association for Computational Linguistics. Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In 13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012, Proceedings of the International Conference on Knowledge Representation and Reasoning, pages 552-561. Institute of Electrical and Electronics Engineers Inc. 13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012.",
    "compresscontext-18": "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Linguistics. Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61-68, Dublin, Ireland. Association for Computational Linguistics. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2022. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655. Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity. In Proceedings of $A C L$, pages 271-278. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL, pages $115-124$. Mohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267-1273, Minneapolis, Minnesota. Association for Computational Linguistics. Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P.",
    "compresscontext-19": "Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations.",
    "compresscontext-20": "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal\n\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.",
    "compresscontext-21": "Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3781-3797, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: Retrievalaugmented black-box language models. arXiv preprint arXiv:2301.12652. Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning by distilling context. arXiv preprint arXiv:2209.15189. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. Roformer: Enhanced transformer with rotary position embedding. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient transformers: A survey. ACM Comput. Surv., 55(6). TogetherAI. 2023. RedPajama: An open source recipe to reproduce llama training dataset.",
    "compresscontext-22": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32.",
    "compresscontext-23": "Curran Associates, Inc. David Wingate, Mohammad Shoeybi, and Taylor Sorensen. 2022. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5621-5634, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR. Lin Zheng, Chong Wang, and Lingpeng Kong. 2022. Linear complexity randomized self-attention mechanism. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27011-27041. PMLR. Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [MASK]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5017-5033, Online. Association for Computational Linguistics. Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657-5673, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. ## A Models and Data\n\nAll models are fine-tuned from OPT models on the Pile. We conduct our experiments using a single NVIDIA A100 80GB GPU and we use Flash Attention (Dao et al., 2022) as an efficient implementation of exact attention over long sequences. We also use gradient checkpointing between compressed segments to reduce GPU memory. ## A. 1 OPT Experiments on 8K Tokens\n\nWe fine-tune our models on 2B tokens from the Pile. We sample 500 M tokens from the following Pile subdomains: Books3, FreeLaw, GitHub and Wikipedia. The following models use a learning rate of $2 \\mathrm{e}-5$, a batch size of 130 K tokens, 1,00 warm-up steps, and the Adam optimizer (Kingma and Ba, 2015):\n\n1. The fine-tuned OPT-2.7B baseline is fine-tuned on documents of up to 2,048 tokens. 2. The extended full-attention baseline is finetuned on documents of up to 4,096 tokens by extending the positional embeddings of OPT-2.7B to 4,096 positions. We initialize the embeddings for positions [2049..4096] with the embeddings for positions [1..2048].",
    "compresscontext-24": "3. The RMT baseline is fine-tuned on documents of up to 8,192 tokens. Each document is segmented into four segments of 2,048 tokens. We use $\\kappa=50$ summary vectors but we do not use summary accumulation, randomized segmenting, or stop-gradients. 4. Our AutoCompressor is fine-tuned on documents of up to 6,144 tokens. Each document is randomly segmented into four segments such that the first two segments add up to 3,072 tokens. The length of each segments ranges from 1,024 to 2,048 tokens. We use $\\kappa=50$ summary vectors and summary accumulation. We stop gradients every two compression steps. All models are evaluated on documents sampled from the Pile with a fixed length of 8,192 tokens. We sample 610 documents from each of the following domains: Books3, FreeLaw, GitHub, Wikipedia (in-domain), and ArXiv, Gutenberg, HackerNews, YoutubeSubtitles (out-of-domain). Examples of documents from each of those domains can be found in Tables 9 and 10. ## A. 2 OPT Experiments on 30K Tokens\n\nWe fine-tune our models on 2 billion tokens from the Books3 subdomain of the Pile. All models are fine-tuned on documents of up to 30,720 tokens. We use a learning rate of $2 \\mathrm{e}-5$, a batch size of 130k tokens, 1,000 warm-up steps and the Adam optimizer. 1. RMT-1.3B uses $\\kappa=50$ summary vectors and is fine-tuned without summary accumulation, randomized segmenting, or stop-gradients. Each document is split into 15 segments of 2,048 tokens Even with gradient checkpointing, attempting to fine-tune a 2.7B parameter RMT model on this dataset leads to an out-of-memory error. 2. The AutoCompressor models are fine-tuned from OPT-1.3B and 2.7B on documents of up to 30,720 tokens. Each document is split into 20 segments such that segment $2 i$ and segment $2 i+1$ add up to 3,072 tokens. The length of each segment is randomly sampled between 1,024 and 2,048 . We use $\\kappa=50$ summary vectors with summary accumulation and we stop gradients every two compression steps. All models are evaluated on documents of 30,720 tokens from the Pile. We use 1,000 documents from Books3 (in-domain) and 1,000 documents from Gutenberg (out-of-domain). ## A. 3 Llama-2 Experiments on 8K Tokens\n\nWe fine-tune our Llama-2 models on 15B tokens from RedPajama. We sample 1B tokens from long documents in ArXiv, Books, C4, GitHub, as well as 10B tokens from CommonCrawl, 800M from Wikipedia and 70M tokens from StackExchange. Both our AutoCompressor and our Extended Full Attention baseline are fine-tuned from Llama2-7B on sequences of 6,144 tokens with LoRA (Hu et al., 2022) parameter efficient fine-tuning applied to the attention heads. We use a LoRA dimension of 16 applied to the QKV- and Out-projections. We use a learning rate of $4 \\mathrm{e}-4$, a batch size of 200 K tokens, 5,000 warm-up steps and the Adam optimizer. For the AutoCompressor, we also optimize the newly initialized summary token embeddings. We train our AutoCompressor in the same way as the OPT-2.7B AutoCompressor, with $\\kappa=50$, randomly segmenting each sequence into four semgents, and stopping gradients every two compression steps. The Extended Full Attention baseline is fine-tuned with a $\\operatorname{RoPE} \\theta$ value of 80,000 . We evaluate our models on 500 sequences of 8,192 tokens from each of ArXiv, Books, C4, GitHub, StackExchange, and 5,000 sequences from CommonCrawl. ## B No-context Language Modeling\n\nIn Table 6, we verify that our fine-tuning strategy does not significantly affect the language modeling capabilities of the OPT AutoCompressors when no summary tokens are given. We find that the AutoCompressor performs slightly better than the RMT model and significantly better than the extended full attention model when no additional context is given. Moreover, the AutoCompressor almost matches the OPT02.7B fine-tuned baseline, with perplexity increasing by less than $1 \\%$. |  | In-domain | Out-of-domain |\n| :--- | :---: | :---: |\n| OPT-2.7B | $7.53 \\uparrow 19.9 \\%$ | $9.19 \\uparrow 7.7 \\%$ |\n| OPT-2.7B fine-tuned | 6.28 | 8.53 |\n| AutoCompressor-2.7B | $6.31 \\uparrow_{\\uparrow 0.5 \\%}$ | $8.60 \\uparrow_{\\uparrow 0.8 \\%}$ |\n| RMT-2.7B | $6.34 \\uparrow 1.0 \\%$ | $8.62 \\uparrow 1.1 \\%$ |\n| Extended full attention | $6.57 \\uparrow 6.4 \\%$ | $8.94{ }_{\\uparrow} 4.8 \\%$ |\n\nTable 6: Held-out perplexity of all models on 2048 tokens without summary vectors or additional context. ## C AutoCompressor Ablations\n\nWe train OPT AutoCompressor models as in Section 4.1 while varying $\\kappa=20,50,70,100$. In Table 7 , we report the perplexity evaluation on documents of 8192 tokens across all evaluation domains. |  | Compressed tokens |  |  |  |\n| ---: | :---: | :---: | :---: | :---: |\n| $\\kappa$ | 0 | 2048 | 4096 | 6144 |\n| 20 | $\\mathbf{7 .",
    "compresscontext-25": "3 6}$ | 7.05 | 7.01 | 7.00 |\n| 50 | 7.37 | $\\mathbf{6 . 9 9}$ | $\\mathbf{6 . 9 4}$ | $\\mathbf{6 . 9 3}$ |\n| 70 | 7.41 | 7.01 | 6.97 | 6.95 |\n| 100 | 7.48 | 7.07 | 7.01 | 7.00 |\n\nTable 7: Held-out perplexity across all evaluation domains for AutoCompressors based on OPT-2.7B trained with different numbers of summary tokens $\\kappa$. We observe that $\\kappa=50$ performs the best overall. ## D Token-level AutoCompressor Analysis\n\nIn Figure 5, we plot the perplexity gains achieved by the OPT AutoCompressor and the extended full attention baseline from Section 4.1 over the pre-trained OPT-2.7B model. We plot the gains achieved by the AutoCompressor both without any additional context and with the summary vectors obtained from 2048 compressed tokens. Results show that the summary vectors help reduce perplexity over the entire 2,048-token segment. This shows that summary vectors do not only contain information which helps continue the previous sequence. Figure 5 also shows that the extended fullattention baseline benefits more from the additional 2,048 context tokens than the AutoCompressor at the start of the sequence, but that the AutoCompressor achieves stronger gains at the end of the sequence. This shows that summary vectors effectively capture long-range textual dependencies and that fine-tuning AutoCompressors produces more robust models than fine-tuning extended fullattention models. ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-14.jpg?height=558&width=738&top_left_y=977&top_left_x=1067)\n\nFigure 5: We plot the perplexity gain over OPT-2.7B for our AutoCompressor model and the 4096-extended attention baseline. We track the perplexity at each token position in sequences of 2048 tokens. The AutoCompressor model almost matches the strong extendedattention baseline at the start of sequences and outperforms it at the end of sequences. In Tables 9 and 10, we give hand-picked examples of sequences from each evaluation domain, highlighting which tokens benefit the most from the compressed context. We compress the first 300 tokens in every document from the evaluation set and evaluate on the following 100 tokens. In the notation of Section 3.1, we measure the perplexity gain of each token as\n\n$$\n\\frac{p\\left(x_{t}^{2} \\mid x_{1}^{2}, \\ldots, x_{t-1}^{2}, \\sigma_{1}\\right)}{p\\left(x_{t}^{2} \\mid x_{1}^{2}, \\ldots, x_{t-1}^{2}\\right)}\n$$\n\nFor each example, we record the top 3-5 most improved token predictions. We find that the tokens which benefit the most from the summary vectors are often interpretable. Names of characters, dates, and locations are often\ncopied through the summary vectors (see the examples for Wikipedia, FreeLaw, or HackerNews). We also find that the model is able to reason over the summary vectors, as the tokens which benefit the most are sometimes not explicitly present in the compressed context, but are closely associated with the domain of speech (see the examples for Books3, Gutenberg and YoutubeSubtitles.). Finally, we find that summary vectors are often useful for continuing the previous sentence (see the GitHub example.)\n\n## E In-Context Learning Details\n\nWe evaluate on in-context examples of the following datasets: AG News (topic classification, Zhang et al.",
    "compresscontext-26": "(2015)), SST-2 (sentiment analysis, Socher et al. (2013)), BoolQ (Boolean Questions, Clark et al. (2019)), WiC (Word-in-Context, word sense dismabiguation, Pilehvar and Camacho-Collados (2019)), WSC (Winograd Schema Challenge, coreference resolution, Levesque et al. (2012)), RTE (Recognizing Textual Entailment, Dagan et al.",
    "compresscontext-27": "(2005); Haim et al. (2006); Bentivogli et al. (2009)), CB (CommitmentBank, de Marneffe et al. (2019)), COPA (Choice of Plausible Alternatives, Roemmele et al. (2011)), MultiRC (Multi-Sentence Reading Comprehension, Khashabi et al. (2018)), MR (Movie Reviews, Pang and Lee (2005)), Subj (Subjectivity, Pang and Lee (2004). We follow the GPT3 prompt templates (Brown et al., 2020) and detail our evaluation setting for OPT and Llama-2 in Table 11 . In Table 12, we compile evaluation results for OPT-2.7B, Llama-2-7B, as well as our AutoCompressor and RMT models. ## F Fused Retrieval-augmented Language Modeling\n\n|  | Perplexity Gain (\\%) |  |  |  |\n| :--- | :---: | :---: | :---: | :---: |\n| Passages | top-1 | top-2 | top-5 | top-10 |\n| Fused Summaries | 1.04 | $\\mathbf{1 . 6 7}$ | $\\mathbf{2 . 6 3}$ | $\\mathbf{3 . 7 4}$ |\n| Fused Summaries w/o re-ranking | 1.04 | 1.52 | 2.02 | 2.63 |\n\nTable 8: PPL gains (\\%) over the no-retrieval baseline for Fused Summary with and without re-ranking. In re-ranking, we order the passages based on the $\\ell_{2}$ norms of their summary vectors before concatenating the summary vectors, whereas w/o re-ranking we use the retrieval scores from the Contriever model. Re-ranking consistently produces higher perplexities. We provide details and ablations for our proposed REPLUG alternative. Inspired by fusion-indecoder (Izacard and Grave, 2021), we fuse summary vectors or passages in a single forward pass. Fused Summary Vectors The summary vectors of retrieved passages $\\mathcal{D}$ are concatenated in order of increasing retrieval scores to form fused summary vectors, $\\sigma_{\\mathcal{D}}=\\operatorname{Concat}\\left[\\sigma_{d_{k}}, \\ldots, \\sigma_{d_{1}}\\right]$. This resembles summary accumulation as described in Section 3, but differs in that the retrieved summary vectors were produced independently rather than recursively. Nevertheless, we find that AutoCompressors transfer well to this setting. Furthermore, we find it beneficial to smooth the conditioned probabilities with the unconditioned probabilities $p(y \\mid x)$, and compute\n\n$$\np(y \\mid x, \\mathcal{D})=\\frac{p\\left(y \\mid \\text { Concat }\\left[\\sigma_{\\mathcal{D}}, x\\right]\\right)+p(y \\mid x)}{2}\n$$\n\nWe also show that language-modeling performance improves when $\\mathcal{D}$ is re-ordered based on the smallest $\\ell_{2}$ distance between the summary vectors $\\left\\{\\sigma\\left(d_{1}\\right), \\ldots, \\sigma\\left(d_{k}\\right)\\right\\}$ and $\\sigma_{x}$. This incurs negligible overhead since $\\sigma_{x}$ can be constructed during the same forward pass which computes $p(y \\mid x)$. The ablation for this is shown in Table 8\n\nFused Passages We establish a baseline for Fusing Summary Vectors by concatenating the corresponding plain-text passages $D=$ Concat $\\left[d_{k}, \\ldots, d_{1}\\right]$ and computing\n\n$$\np(y \\mid x, \\mathcal{D})=\\frac{p(y \\mid \\operatorname{Concat}[D, x])+p(y \\mid x)}{2}\n$$\n\nNote that this approach is quickly limited by the size of the pre-trained language model's context window, especially when retrieving many long passages.",
    "compresscontext-28": "| Domain | Compressed context | Evaluation sequence | Most <br> improved <br> tokens |\n| :---: | :---: | :---: | :---: |\n| Books3 | Surrealism\u2014not for Breton's depreciation of \"Red Front,\" but for a seemingly insignificant <br> aside. In early March, before sending the text to press, Breton showed it to Aragon. The <br> latter consented to the publication, with one exception: a footnote in which Breton quoted the <br> PCF official's remark (which Aragon had earlier reported to him) about \"complicating the <br> simple, healthy relations between men and women\"-a clear illustration, Breton felt, of \"just <br> how much bad faith or mental indigence we were up against.\" Aragon considered internal <br> Party statements to be confidential, and asked that the footnote be removed; according to <br> him, Breton \"spontaneously crossed out the note on the galleys with a delete mark that I can <br> still recall... saying that he wanted to give the Party no excuse for expelling me.\" But when <br> _The Poverty of Poetry_ came off press the next day, the incriminating footnote was still <br> there. <br> Whether Breton retained the note as a test of Aragon's loyalty, or whether he deemed this <br> example of PCF stupidity too good to waste, or whether the printer simply neglected to make <br> the correction, no one has ever established. But the result was that this single act came to <br> represent for Aragon every philosophical difference, stricture, and humiliation that had ever <br> darkened his long friendship with Breton. On March 10, he responded to the tract via an <br> anonymous note in | _L'Humanit\u00e9_ : \"Our comrade Aragon in- <br> forms us that he has absolutely nothing to <br> do with the publication of a pamphlet entitled <br> _The Poverty of Poetry_... He wishes to make <br> it clear that he entirely disavows both the con- <br> tents of this pamphlet and the attention it has <br> drawn to his name, every Communist being <br> duty-bound to condemn the attacks contained <br> in this pamphlet as incompatible with the class <br> struggle.\" This short paragraph was the only <br> notice he ever saw fit to give of | Poverty <br> Po <br> Ar <br> agon <br> Human |\n| Wikipedia | </s>Shi Ce <br> Shi Ce (; born 15 December 1985) is a Chinese deaf female table tennis player.",
    "compresscontext-29": "She has <br> represented China at the Deaflympics four times from 2005-2017. Shi Ce has been regarded <br> as one of the finest athletes to have represented China at the Deaflympics, having won 14 <br> medals at the event since making her debut in the 2005 Summer Deaflympics. <br> Biography <br> Shi Ce was born in Yichun, Heilongjiang on 15 December 1985. She was born with an <br> ear condition that impaired her hearing which resulted in her deafness and has congenital <br> malformation in her right ear. Her parents decided to consult a doctor and took her to an <br> hospital in the Zhejiang Province in order to cure her ear impairment when she was just <br> five years old. The doctor suggested that surgery would cause facial paralysis after Shi Ce's <br> parents demanded for a surgery. Shi Ce took the sport of Table tennis and started playing it <br> at the age of nine. <br> Career <br> Shi Ce has won 14 medals in her Deaflympic career as a Table tennis player including 11 <br> gold medals. Shi Ce was eligible to compete at the National Games of China despite her <br> deafness, in 2015. In the competition, she secured gold medals in singles, doubles, mixed <br> doubles and in the team events. <br> 2005 Summer Deaflympics Shi Ce made her first appearance at an international sports | event during the 2005 Summer Deaflympics <br> and excelled on her debut Deaflympic event <br> after winning gold medals in the women's sin- <br> gles, doubles and in the mixed doubles.",
    "compresscontext-30": "She <br> was also the part of the Chinese Table ten- <br> nis team which secured the silver medal in <br> the 2005 Deaflympics. In the same year, she <br> received the Deaf Sportswoman of the Year <br> award from the ICSD for her remarkable per- <br> formances at the 2005 Summer Deaflympics. <br> Shi Ce | Ce <br> De <br> 2005 <br> Summer <br> Shi |\n| Github | $</s>import sys$ <br> $import datetime$ <br> $def basic(arguments):$ <br> $import api$ <br> $critic=api.critic.startSession\\left(for_testing=True\\right)$ <br> $repository=api.repository.fetch(critic, name=\"critic\")$ <br> $branch=api.branch.fetch(critic$, <br> $repository=repository, name=arguments.review)$ <br> $review=api.review.fetch(critic, branch=branch)$ <br> $alice=api.user.fetch(critic, name=\"alice\")$ <br> $bob=api.user.fetch(critic, name=\"bob\")$ <br> $dave=api.user.fetch(critic, name=\"dave\")$ <br> $erin=api.user.fetch(critic, name=\"erin\")$ <br> $all_comments=api.comment.fetchAll(critic)$ <br> $assert isinstance\\left(all_comments, list\\right)$ <br> f9cb1d6fb-7271-4b6e-bf38-e62de2824222}\\(\"type\": \"issue\", \"state\": \"open\" \\}, \\) <br> $1:\\{\\text { \"text\": \"This is a general note.\", }$ <br> \\(\"location\": None, \\) <br> \\(\"type\": \"issue\", \\){ffa580cff-5d81-46ab-a78b-f52b1c4808c6}1 <br> \\(location \\) <br> \\(\\} \\) <br> \\(text \\) <br> \\(issue \\){fd11f54c2-65b6-4429-98bd-2b759479ca9c}8 <br> \\(By the end of 1975, Farmers National was insolvent and under investigation by the Florida \\) <br> \\(Department of Insurance.",
    "compresscontext-31": "The Miami newspapers published a series of articles describing \\) <br> \\(the relationship between Hauser and the company. Lawrence Lee, an attorney for an Arizona \\) <br> \\(union group, investigated Farmers National in connection with an Old Security-Farmers \\) <br> \\(National proposal. He was told by the Florida insurance department that Farmers National \\) <br> \\(was connected with Hauser, that it had been injected with questionable assets which were \\) <br> \\(being investigated by the department, and that it had been fined  \\$ 5,000  for failing to disclose \\) <br> \\(both Hauser's ownership and a loan to one of its directors. Lee contacted Richard Halford, \\) <br> \\(vice-president at Old Security in charge of union group insurance, and related the information \\) <br> \\(he had received. Halford assured Lee that he was aware of Hauser's reputation, but that \\) <br> \\(Hauser was no longer involved with Farmers National. Halford then called Kavanagh, \\) <br> \\(who told him that Hauser had no official capacity with the company, and that the financial \\) <br> \\(problems had been cleared up. Halford did not attempt to check the accuracy of Kavanagh's \\) <br> \\(representations with the Florida Department of Insurance. \\) <br> 9 <br> \\(Hauser controlled a second company, Family Provider Life Insurance Company (\"Family \\) <br> \\(Provider\"). In 1975, the company had no business, no office, and assets of \\$50,000. Because \\) <br> \\(of Farmers National's insolvency, Hauser decided to activate \\){fb9a1adfb-4501-486e-9c0b-fb6f18533040}\\(Family Provider, and its assets were increased \\) <br> \\(to  \\$ 250,000 , the minimum required to con- \\) <br> \\(duct business in Arizona, where the company \\) <br> \\(was licensed. In January 1976, Boden and \\) <br> \\(Kavanagh met with Halford and Robert Bar- \\) <br> \\(ton, president of Old Security, to propose \\) <br> \\(a new agreement between Old Security and \\) <br> \\(Family Provider for the purpose of obtaining \\) <br> \\(the Fund business. Both Barton and Halford \\) <br> \\(considered Family Provider and Farmers Na- \\) <br> \\(tional to be \"synonymous\" and believed that \\) <br> \\(Kavanagh and Boden \\){f8b252269-9e73-4941-80fe-f1e052b253ce}\\(Security \\) <br> \\(Old \\) <br> \\(Family \\) <br> \\(avan \\) <br> \\(assets \\){f03018dab-efbe-48ea-aeae-cb9acc4addb6 |  |  |\n\nTable 9: Examples of sequences from in-domain test Pile domains. We highlight the tokens from the evaluation sequence which benefit the most from the summary vectors. In Books3, L'Humanit\u00e9 is prominent French newspaper associated with Breton and his circle. In GitHub, the summary vectors carry information about the logical and syntactical continuation of the context. | Domain | Compressed context | Evaluation sequence | Most <br> improved <br> tokens |\n| :---: | :---: | :---: | :---: |\n| HackerNews | Hackers steer Tesla into oncoming traffic by placing three stickers on the road <br> - velmu https://www.businessinsider.com/tesla-hackers-steer-into-oncoming-traffic-with- <br> stickers-on-the-road-2019-4 <br> $=====$ chrisbolt From yesterday: <br>  <br> (https://news.ycombinator.com/item?id=19536375) <br> gregmac <br> While I'm hugely skeptical of the current state of self-driving cars, you could probably get <br> human drivers to make the same mistake if you were to repaint the lines.",
    "compresscontext-32": "However, humans <br> will also notice the oncoming cars (if there are any) and avoid getting in a head-on collision. <br> The thing missing from this test is that critical practical piece: if there was an oncoming <br> car, will the Tesla do something to avoid the collision? I would assume that not getting in a <br> head-on crash is higher priority than staying in the lane markings. <br> Without oncoming traffic, all this is testing is what the Tesla considers valid line markings. <br> I'm sure there's room for improvement here (such as checking where the other lane is, raising <br> the requirement for how well-defined the lines have to be, etc), but | those are also going to involve trade-offs <br> where there are legitimate situations that will <br> stop working. <br> I think you could just as easily title this video <br> \"Tesla auto-pilot follows road markings even <br> if they're really bad\". <br> Edit: The best shot I could get from the video <br> makes me even more upset at this test: <br> these look like the temporary markings of- <br> ten used during construction, just before they <br> come and paint the normal lines using the big | Tesla <br> test <br> markings <br> auto |\n| ArXiv | $z_{k}=h_{k}\\left(x_{k}\\right)+v_{k}, \\quad v_{k} \\sim \\mathcal{N}\\left(0, R_{k}\\right)$ <br> In the above equations, we see that the transition matrix $F_{k, k-1}$ has been replaced by the <br> nonlinear vector-valued function $f_{k, k-1}(\\cdot)$, and similarly, the matrix $H_{k}$, which transforms <br> a vector from the state space into the measurement space, has been replaced by the nonlinear <br> vector-valued function $h_{k}(\\cdot)$. The method proposed by the Extended Kalman Filter is to <br> linearize the nonlinearities about the current state prediction (or estimate). That is, we <br> choose $F_{k, k-1}$ as the Jacobian of $f_{k, k-1}$ evaluated at $\\hat{x}_{k-1 \\mid k-1}$, and $H_{k}$ as the Jacobian of <br> $h_{k}$ evaluated at $\\hat{x}_{k \\mid k-1}$ and proceed as in the linear Kalman Filter of Section sec :: $k f .[\\hat{1} 8]$ <br> Numerical accuracy of these methods tends to depend heavily on the nonlinear functions. If <br> we have linear constraints but | a nonlinear $f_{k, k-1}(\\cdot)$ and $h_{k}(\\cdot)$, we can adapt <br> the Extended Kalman Filter to fit into the <br> framework of the methods described thus far. <br> Nonlinear Equality and Inequality Constraints <br> Since equality and inequality constraints we <br> model are often times nonlinear, it is important <br> to make the extension to nonlinear equality and <br> inequality constrained Kalman Fil | Extended <br> linear <br> h <br> k <br> Kal |\n| Gutenberg <br> YoutubeSubtitles | eight or nine cents. Telegrams in foreign languages are sent within the empire for five sen <br> per word, with a minimum charge of twenty-five sen for five words or a fraction thereof.",
    "compresscontext-33": "No <br> charge is made for delivery within a radius of 2-1/2 miles of the telegraph office. <br> There are no private telegraph corporations. The government builds, owns, and operates the <br> lines just as it does the mails. The postal and 101 telegraph systems are intimately connected, <br> and the same office does service for both. <br> The first telegraph line in Japan was opened in 1869. The venture proving a success, the <br> following year the line was extended and a general telegraphic system for the whole country <br> decided upon. The rapid construction of telegraph lines began in 1872, from which year it <br> has gone forward uninterruptedly. At present the lines extend to every corner of the empire. <br> The first lines were surveyed, built, and operated under foreign experts; but the natives have <br> learned so rapidly that they have been enabled to do away with all foreign employees. All of <br> the materials and instruments in use, with the exception of submarine cables and the most <br> delicate electrical measuring apparatus, are made in Japan. <br> MAILS.-The Japanese mail system was modeled after the American in 1871. <br> te que no voy a esa escuela.\" <br> Johnny Galecki <br> El Dr. Leonard Hofstadter obtuvo su doctorado a los 24 a\u00f1os, pero el actor que lo interpreta <br> s\u00f3lo lleg\u00f3 a medio camino de la secundaria. En una entrevista con Time Out Chicago en el <br> 2009, Johnny Galecki revel\u00f3 que abandon\u00f3 la escuela a mediados del octavo grado luego de <br> a\u00f1os de evitar ir a clases a toda costa. Le dijo a Time Out, \"Una vez que las divisiones largas <br> aparecieron en tercer grado, iba al ba\u00f1o por 45 minutos y nadie lo notaba, todos los d\u00edas a <br> la misma hora del d\u00eda, s\u00f3lo para escapar de ellas.\" Puede que Galecki no tenga un cerebro <br> matem\u00e1tico, pero siempre tuvo inteligencia callejera. \"El conocimiento es el mejor y m\u00e1s <br> seguro tesoro... Vaya, me aburro a m\u00ed mismo.\" A los 14 a\u00f1os, vivi\u00f3 solo en un apartamentito <br> en Burbank, California, mient | At first it was limited to postal service between <br> the three large cities of Tokyo, Kyoto, and Os- <br> aka; but in 1872 it was extended to the whole <br> country, with the exception of a certain part of <br> the Hokkaido, which was without roads and <br> almost without population. To-day there is no <br> village or hamlet in the whole land which does <br> not enjoy the convenience of a good postal sys- <br> tem. The mails are sent with promptness and <br> ras trabajaba en la comedia American <br> Dreamer, su primer gran trabajo. Su familia <br> pas\u00f3 nueve meses en Long Beach antes de <br> regresar a Chicago, y \u00e9l se qued\u00f3 para concen- <br> trarse en su carrera como actor. <br> Jim Parsons <br> El Dr. Sheldon Cooper fue un ni\u00f1o prodigio. <br> Comenz\u00f3 la universidad cuando ten\u00eda 11 a\u00f1os | limited <br> postal <br> Tokyo <br> Parsons <br> aba <br> Jim <br> Dr |\n\nTable 10: Examples of sequences from out-of-domain test Pile domains. We highlight the tokens from the evaluation sequence which benefit the most from the summary vectors. In Gutenberg, 'Tokyo' is not copied over from the compressed context but is inferred from the discussion of Japan. In YoutubeSubtitles, 'Jim Parsons' benefits the most from the summary vectors because the context discusses his co-star John Galecki in The Big Bang Theory. | Dataset | Prompt template | OPT-based models |  |  | Llama-2-based models |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Toks.",
    "compresscontext-34": "/ dem. | Cal. | Bal. | Toks. / dem.",
    "compresscontext-35": "| Cal. | Bal. |\n| AG News | Article: \\{text\\}\\nTopic: \\{label \\} | 65 | $\\checkmark$ |  | 75 | $\\checkmark$ |  |\n| SST-2 | Sentence: \\{sentence\\}\\nSentiment: \\{label \\} | 22 | $\\checkmark$ | $\\checkmark$ | 25 | $\\checkmark$ | $\\checkmark$ |\n| BoolQ | \\{passage\\}\\nquestion: \\{question\\}?\\nanswer: \\{label \\} | 165 | $\\checkmark$ |  | 170 | $\\checkmark$ |  |\n| WiC | \\{sentence1\\}\\n\\{sentence2\\}\\nquestion: Is the word '\\{word\\}' <br> used the same way in the two sentences above?\\nanswer: \\{label\\} | 45 | $\\checkmark$ |  | 45 | $\\checkmark$ <br> $\\checkmark$ |  |\n| WSC | Question: In the sentence \"\\{text\\}\", does the pronoun '\\{span2_text\\}' <br> refer to \\{span1_text\\}? \\nAnswer: \\{label\\} | 61 |  |  | 50 | $\\checkmark$ |  |\n| RTE | \\{premise\\}\\nquestion: \\{hypothesis\\} True or False?\\nanswer: \\{label\\} | 75 |  |  | 85 |  |  |\n| CB | \\{premise\\}\\nquestion: hypothesis. true, false or neither?\\nanswer: \\{label\\} | 98 |  | $\\checkmark$ | 95 |  | $\\checkmark$ |\n| COPA | Context: \\{premise\\}\\nAnswer: \\{answer\\} | 21 |  | $\\checkmark$ | 22 | $\\checkmark$ | $\\checkmark$ |\n| MultiRC | Context: \\{paragraph\\}\\n\\{question\\}\\n\\{answer\\}\\nanswer: \\{label \\} | 350 | $\\checkmark$ | $\\checkmark$ | 350 | $\\checkmark$ | $\\checkmark$ |\n| MR | Review: \\{text\\}\\nSentiment: \\{label\\} | 36 |  | $\\checkmark$ | 40 | $\\checkmark$ | $\\checkmark$ |\n| Subj | input: \\{text\\}\\ntype: \\{label \\} | 40 |  | $\\checkmark$ | 40 | $\\checkmark$ | $\\checkmark$ |\n\nTable 11: Details of the datasets and prompts used for the ICL evaluation of our OPT-2.7B and Llama-2-7B AutoCompressors and baselines.",
    "compresscontext-36": "\"Toks / dem.\" (Tokens per demonstration) denotes how long demonstrations are for the average example.",
    "compresscontext-37": "\"Cal.\" (Calibration) denotes whether we use calibration (Sachan et al., 2022), and \"Bal.\" (Balanced) means whether we enforce class-balanced sampling. We decide the ticks based on which method performs best on a held-out validation set. |  | AG News | SST-2 | BoolQ | WiC | WSC | RTE | CB | COPA | MultiRC | MR | Subj |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| OPT-2.7B AutoCompressor |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot | $68.2_{(0.0)}$ | $78.0_{(0.0)}$ | $60.2_{(0.0)}$ | $49.5_{(0.0)}$ | $60.6_{(0.0)}$ | $55.2_{(0.0)}$ | $43.6_{(0.0)}$ | $69.0_{(0.0)}$ | $43.8_{(0.0)}$ | $60.0_{(0.0)}$ | $56.7_{(0.0)}$ |\n| 50 summary vecs. | $720.7_{(1.4)}$ | $84.3_{(9.2)}$ | $55.8_{(4.2)}$ | $50.4_{(1.0)}$ | $61.3_{(5.8)}$ | $54.8_{(3.4)}$ | $55.9_{(5.4)}$ | $71.6_{(0.6)}$ | $44.1_{(1.1)}$ | $70.4_{(10.2)}$ | $63.2_{(7.7)}$ |\n| 100 summary vecs. | $71.2_{(3.8)}$ | $87.0_{(3.5)}$ | $57.5_{(4.6)}$ | $50.7_{(1.0)}$ | $60.2_{(6.7)}$ | $55.5_{(2.5)}$ | $54.4_{(4.0)}$ | $71.9_{(0.4)}$ | $45.6_{(2.8)}$ | $73.1_{(12.9)}$ | $62.2_{(5.8)}$ |\n| 150 summary vecs. | $68.2_{(3.3)}$ | $82.6_{(5.6)}$ | $59.8(1.8)$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=38&width=101&top_left_y=1332&top_left_x=867) | $63.5_{(0.0)}$ | $\\mathbf{5 5 . 8}_{(1.8)}$ | $\\mathbf{5 8 . 3}_{(5.1)}$ | $71.4_{(0.5)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=38&width=93&top_left_y=1334&top_left_x=1469) | $67.0_{(11.9)}$ | $58.5_{(6.7)}$ |\n| ICL (150 tokens) | $72.5_{(2.5)}$ | $70.8_{(12.6)}$ | $60.2_{(0.0)}$ | $50.4_{(1.1)}$ | $52.3_{(13.9)}$ | $57.6_{(4.3)}$ | $51.1_{(7.1)}$ | $71.3_{(1.5)}$ | $43.8_{(0.0)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=37&width=99&top_left_y=1366&top_left_x=1590) | $61.7_{(11.2)}$ |\n| ICL (750 tokens) | $67.3_{(3.4)}$ | $87.5_{(5.0)}$ | $69.1_{(1.0)}$ | $51.0_{(1.7)}$ | $62.9_{(0.8)}$ | $57.4_{(4.4)}$ | $49.0_{(1.1)}$ | $72.0_{(0.7)}$ | $52.0_{(5.4)}$ | $86.7_{(5.9)}$ | $73.6_{(13.9)}$ |\n| OPT-2.7B RMT |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot | $66.9_{(0.0)}$ | $72.8_{(0.0)}$ | $\\mathbf{5 8 .",
    "compresscontext-38": "4}_{(0.0)}$ | $50.3_{(0.0)}$ | $64.4_{(0.0)}$ | $55.2_{(0.0)}$ | $42.2_{(0.0)}$ | $68.8_{(0.0)}$ | $43.9_{(0.0)}$ | $62.5_{(0.0)}$ | $69.8_{(0.0)}$ |\n| 1-step summary vecs. | $66.3_{(5.5)}$ | $86.5_{(5.1)}$ | $49.6_{(8.1)}$ | $51.0_{(1.00}$ | $57.7_{(6.6)}$ | $51.3_{(1.2)}$ | $\\mathbf{5 3 . 3}_{(3.8)}$ | $67.4_{(1.1)}$ | $44.9_{(1.2)}$ | $52.6_{(2.8)}$ | $63.3_{(11.2)}$ |\n| 2-step summary vecs. | $65.2_{(7.2)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=37&width=96&top_left_y=1569&top_left_x=633) | $54.8_{(4.1)}$ | $50.3_{(0.8)}$ | $58.6_{(6.7)}$ | $50.2_{(1.4)}$ | $49.5_{(4.8)}$ | $68.2_{(1.2)}$ | $\\mathbf{4 5 . 5}_{(1.8)}$ | $54.1_{(1.9)}$ | $54.6_{(1.7)}$ |\n| 3 -step summary vecs. | $63.9_{(3.3)}$ | $84.5_{(6.6)}$ | $41.8_{(9.7)}$ | $50.6_{(0.6)}$ | $54.3_{(7.9)}$ | $50.2_{(1.4)}$ | $49.5_{(3.6)}$ | $68.0_{(0.9)}$ | $45.5_{(1.0)}$ | $52.8_{(1.6)}$ | $58.4_{(8.6)}$ |\n| ICL (150 tokens) | $\\mathbf{7 0 . 8}_{(1.9)}$ | $75.1_{(13.3)}$ | $58.4_{(0.0)}$ | $\\mathbf{5 1 . 7}_{(2.8)}$ | $52.5_{(13.1)}$ | $\\mathbf{5 7 . 2}_{(3.6)}$ | $46.5_{(3.6)}$ | $69.3_{(1.5)}$ | $43.9_{(0.0)}$ | $8^{89.0}(1.4)$ | $60.7_{(12.1)}$ |\n| ICL (750 tokens) | $65.8_{(4.2)}$ | $85.7_{(9.7)}$ | $57.2_{(7.6)}$ | $51.5_{(2.7)}$ | $59.2_{(8.5)}$ | $57.8_{(2.0)}$ | $48.2_{(0.7)}$ | $70.9_{(0.7)}$ | $54.6_{(3.6)}$ | $87.5_{(4.6)}$ | $71.6_{(12.6)}$ |\n| OPT-2.7B Pre-trained |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot | $65.1_{(0.0)}$ | $79.1_{(0.0)}$ | $55.8_{(0.0)}$ | $49.4_{(0.0)}$ | $53.9_{(0.0)}$ | $51.2_{(0.0)}$ | $21.2_{(0.0)}$ | $66.8_{(0.0)}$ | $43.7_{(0.0)}$ | $59.0_{(0.0)}$ | $66.2_{(0.0)}$ |\n| ICL (150 tokens) | $71.6_{(2.6)}$ | $68.56_{(14.9)}$ | $55.8_{(0.0)}$ | $50.6_{(1.0)}$ | $53.30_{(11.1)}$ | $56.1_{(2.4)}$ | $46.2_{(6.4)}$ | $71.7_{(1.2)}$ | $43.7_{(0.0)}$ | $86.7_{(4.3)}$ | $61.9_{(10.9)}$ |\n| ICL (750 tokens) | $63.3_{(5.1)}$ | $91.0_{(3.2)}$ | $63.0_{(1.3)}$ | $50.0_{(0.4)}$ | $63.5_{(0.6)}$ | $54.7_{(3.0)}$ | $52.1_{(4.8)}$ | $73.4_{(1.0)}$ | $53.5_{(6.2)}$ | $89.9_{(2.2)}$ | $64.4_{(10.7)}$ |\n| Llama-2-7B AutoCompressor |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot | $63.3_{(0.0)}$ | $67.7_{(0.0)}$ | $67.4_{(0.0)}$ | $50.8_{(0.0)}$ | $43.3_{(0.0)}$ | $58.8_{(0.0)}$ | $42.9_{(0.0)}$ | $52.5_{(0.0)}$ | $52.5_{(0.0)}$ | $57.4_{(0.0)}$ | $49.3_{(0.0)}$ |\n| 50 summary vecs.",
    "compresscontext-39": "| $79.6_{(4.9)}$ | 94.2 ${ }_{(1.6)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=36&width=101&top_left_y=1973&top_left_x=753) | $51.6_{(2.1)}$ | $47.7_{(8.7)}$ | $66.3_{(7.0)}$ | $46.4_{(18.7)}$ | $84.5_{(1.0)}$ | $52.6_{(2.8)}$ | $91.5_{(1.0)}$ | $53.5_{(3.6)}$ |\n| 100 summary vecs. | $8^{87.6}(1.2)$ | $92.6_{(3.3)}$ | $66.3_{(2.8)}$ | $52.5_{(2.2)}$ | $42.9_{(2.5)}$ | $63.5_{(6.6)}$ | 64.5 $_{(5.9)}$ | $85.9_{(0.4)}$ | $\\mathbf{5 6 . 1}_{(1.2)}$ | $90.7_{(2.6)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=36&width=100&top_left_y=2005&top_left_x=1711) |\n| 150 summary vecs. | $85.4_{(3.4)}$ | $92.3_{(2.9)}$ | $68.0_{(1.8)}$ | $\\mathbf{5 2 . 8}_{(1.5)}$ | $49.9_{(7.6)}$ | $65.3_{(6.6)}$ | $54.8_{(5.8)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=37&width=101&top_left_y=2037&top_left_x=1349) | $54.8_{(2.2)}$ | $91.1_{(2.2)}$ | $56.6_{(7.9)}$ |\n| ICL (150 tokens) | $74.5_{(2.2)}$ | $92.4_{(3.1)}$ | $67.4_{(0.0)}$ | $52.4_{(2.7)}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_95e252010bc71ba6600ag-18.jpg?height=37&width=92&top_left_y=2067&top_left_x=996) | $69.1_{(2.1)}$ | $46.4_{(23.0)}$ | $80.0_{(1.9)}$ | $52.5_{(0.0)}$ | $79.7_{(15.7)}$ | $57.9_{(10.7)}$ |\n| ICL (750 tokens) | $81.2_{(4.1)}$ | $93.8_{(1.2)}$ | $67.7_{(2.7)}$ | $52.4_{(2.0)}$ | $40.0_{(5.7)}$ | $73.1_{(3.5)}$ | $50.3_{(2.8)}$ | $82.6_{(1.6)}$ | $47.0_{(3.2)}$ | $91.6_{(0.8)}$ | $60.7_{(14.8)}$ |\n| Llama-2-7B Pre-trained |  |  |  |  |  |  |  |  |  |  |  |\n| Zero-shot | $68.8_{(0.0)}$ | $87.2_{(0.0)}$ | $70.0_{(0.0)}$ | $51.4_{(0.0)}$ | $65.4_{(0.0)}$ | $62.8_{(0.0)}$ | $32.1_{(0.0)}$ | $75.5_{(0.0)}$ | $54.5_{(0.0)}$ | $84.1_{(0.0)}$ | $48.9_{(0.0)}$ |\n| ICL (150 tokens) | $71.9_{(3.8)}$ | $91.6_{(2.9)}$ | $70.0_{(0.0)}$ | $51.0_{(1.9)}$ | $55.4_{(3.2)}$ | $70.9_{(1.7)}$ | $39.3_{(21.2)}$ | $84.2_{(1.3)}$ | $54.5_{(0.0)}$ | $90.6_{(3.3)}$ | $63.6_{(10.8)}$ |\n| ICL (750 tokens) | $78.2_{(3.8)}$ | $94.5_{(0.8)}$ | $70.3_{(6.1)}$ | $54.9_{(1.9)}$ | $42.2_{(5.0)}$ | $71.3_{(4.4)}$ | $51.3_{(3.5)}$ | $85.3_{(0.7)}$ | $47.0_{(1.5)}$ | $92.9_{(0.5)}$ | $65.4_{(14.5)}$ |\n\nTable 12: We evaluate the following models on 11 in-context learning tasks: The OPT-2.7B AutoCompressor and RMT model, the Llama-2-7B AutoCompressor, and the respective pre-trained models. For each fine-tuned model, numbers in bold are the highest evaluation results using at most 150 additional tokens. When using summary vectors, the OPT-2.7B AutoCompressor outperforms the RMT model on 8/11 tasks. Moreover, the OPT-2.7B AutoCompressor benefits from multiple compression steps on most tasks whereas the RMT model performs best without summary vectors on 7/11 tasks and benefits from 3 -step summary vectors on none of the above tasks. The Llama-2 AutoCompressor achieves the absolute highest accuracy using summary vectors on 7/11 tasks. It also achieves the highest accuracy with summary vectors on $9 / 11$ tasks using at most 150 additional tokens. [^0]:    *AC and AW contributed equally. This work was done when AC was at the Institute for Advanced Study and visited the Princeton NLP group. ${ }^{1}$ Our code and models are publicly available at https://github.com/princeton-nlp/AutoCompressors. [^1]:    ${ }^{2}$ In our pre-liminary experiments, even fine-tuning a pretrained OPT-2.7b model with Transformer-XL-style training (Dai et al., 2019) caused optimization difficulties and deterioriated the pre-trained model quality. ${ }^{3}$ When fine-tuning OPT models, we observe benefits with initializing the embeddings of the summary tokens with the pre-trained embedding for the end-of-sequence token $\\langle/ \\mathrm{s}\\rangle$. [^2]:    ${ }^{4}$ Due to the scarcity of very long sequences in the Pile, we only train on data from the Books3 domain, and use the Gutenberg domain as out-of-domain evaluation. [^3]:    ${ }^{5}$ For comparison, storing the transformer output at every single token (e.g., in an encoder-decoder setting) would take up 51 TB , and storing all attention states would be $3,276 \\mathrm{~TB}$. "
}