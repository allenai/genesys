{
    "staircaseattn-0": "Staircase Attention for Recurrent Processing of Sequences\n\nDa Ju Stephen Roller Sainbayar Sukhbaatar Jason Weston Facebook AI Research\n\nAbstract\n\nAttention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains. 1 Introduction\n\nEarly breakthrough work in neural language modeling considered a fixed context size of tokens that are embedded with a lookup table, followed by nonlinearities and a final softmax to produce a probability distribution for the next output token in a sequence [1]. Such models were replaced, pre-Transformer, with recurrent models such as RNNs and LSTMs [2, 3, 4] that were able to consider arbitrary context length via the ability to store state in their memory using recurrent steps through the data, in contrast to the fixed length constraint of earlier models. Moreover, the repeated application of the recurrent network across the sequence also made the models considerably deeper: a given representation is a function of a large number of nonlinearities due to previous state. This allows such models to track state, store long-term memories, and potentially solve highly nonlinear sequential tasks. Today, with the advent of attention-based models [5] and in particular Transformers [6], fixed length inputs that eschew recurrence are back as the norm, thanks mainly due to deep stacks of nonlinearities on those fixed inputs that are also well suited to modern hardware, leading the authors of [6] to claim that non-recurrent attention is \u201call you need.\u201d However, some of the advantages just mentioned of earlier models \u2013 tracking state, and solving highly nonlinear sequential tasks \u2013 have to some degree been lost. In this work we introduce a family of recurrent models that utilize a novel attention procedure called staircase attention. We show that our new family of models, which utilize both sequence aligned recurrence (in time) and recurrence in depth can bring advantages to modern models, in particular in terms of lower language modeling perplexities given the same number of parameters, and for solving nonlinear state-tracking tasks. Staircase attention, like self-attention, processes tokens in parallel for speed, but unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step (processing block) in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence). Thus, on each time step the block moves forward in time, retaining a memory comprised of multiple vectors stored in the backward tokens (the recurrent tokens). The blocks utilize the same model weights for each step, hence giving a recurrence in depth. An extreme \u201cLadder\u201d version of the blocks with a forward step of zero that simply repeats the Transformer on each step of the Ladder, sharing the weights, does not have time recurrence, but still retains depth recurrence. Compared to Transformers, utilizing staircase attention can retain a recurrent memory in time, and repeated application of the recurrent network over the sequence also makes the model considerably deeper for the same number of parameters (but not necessarily the same amount of compute). We show on two tasks requiring state-tracking that staircase attention models can perform successfully where Transformers fail. We then show on two language modeling and a dialogue modeling task for the same number of parameters, significantly lower perplexities can be obtained compared to standard Transformers for certain models in our family. We thus analyze our family of models and show that one can control the amount of recurrence and depth which can trade off compute for performance, which has practical implications depending on available memory and compute architecture constraints. Notably, Ladder models are surprisingly effective and simple to implement on standard modeling tasks which do not appear to require much recurrence in time, but fail on some state-tracking tasks. Our Staircase models perform well on both state-tracking tasks and language modeling tasks, providing good performance across the board. 2 Related Work\n\nSince attention was introduced [5], non-recurrent models have superseded in popularity recurrent models like RNNs and LSTMs [2, 3], which were for a time dominant in NLP applications, particularly when involving sequence generation. The first models to use stacked layers of attention over input word embeddings and position encodings, as a replacement to recurrence in time, were end-to-end memory networks [7]. Those models were shown on the task of language modeling to perform well compared to LSTMs, but in experiments still shared some weights across depth, which we refer to as recurrence in depth (also referred to as a \u201crecurrent attention mechanism\u201d in [6]). Transformers [6] removed the constraint of sharing any weights among layers at all, and showed this improves performance (at the cost of using more parameters). Transformers additionally contributed other notable improvements such as multi-head, self-attention and residual blocks. Such models do not have any recurrence at all, and are the current state-of-the-art architecture choice for many tasks. Since then, several variants of Transformers have arisen that attempt to incorporate recurrence again by sharing some weights. The Universal Transformer proposes an extreme variant of applying the same layer (with shared weights) repeatedly [8]. Similarly, ALBERT [9] also shares the same weights across all layers for a pretraining/finetuning setting where the BERT model [10] is effectively compressed; they also consider sharing only the self-attention or only the feed-forward weights. We note also that several works, while not sharing parameters of layers, have studied the ordering of the sublayers of Transformers, in particular Sandwich [11] and Macaron [12] Transformers. Some works have also attempted to incorporate sequence-aligned recurrence to Transformers. [13] incorporate LSTM layers into the Transformer, and [14] blend a non-recurrent and recurrent model (e.g., an RNN) together with a gating function. Transformer-XL [15] employs a segment-level recurrence mechanism to effectively cache and speed up computations in long-context sequence tasks. We note that a number of recent architectures have also focused on allowing long-context in Transformers, although typically without employing recurrence [16, 17, 18]. Finally, the Feedback Transformer [19], perhaps the most similar work to ours, incorporates step-wise recurrence in the Transformer, with a step size of one and a fixed cached memory in the past. It achieves good results but has relatively high computational cost due to its architecture not fully exploiting parallelism. In this work, we compare architectures with the number of model parameters fixed, and explore increasing recurrence and/or compute given that fixed budget. An orthogonal topic of study is to fix the compute budget instead, but do not fix the amount of parameters, e.g. research into large, sparse (typically non-recurrent) models that may require to be spread over a cluster [20, 21]. We focus on the former here, but learnings from each direction should be complementary. 3 Method\n\n3.1 Background\n\nIn this paper, we consider decoder-only Transformers [22, 15] that are applied to sequential tasks like language modeling. In this setting, a Transformer model takes as input a sequence of tokens and outputs a sequence of the same size\n\ny 1 , y 2 , \u2026 , y T = Transformer \u200b ( x 1 , x 2 , \u2026 , x T ) . subscript \ud835\udc66 1 subscript \ud835\udc66 2 \u2026 subscript \ud835\udc66 \ud835\udc47 Transformer subscript \ud835\udc65 1 subscript \ud835\udc65 2 \u2026 subscript \ud835\udc65 \ud835\udc47 y_{1},y_{2},\\ldots,y_{T}=\\textsc{Transformer}(x_{1},x_{2},\\ldots,x_{T}). (1)\n\nIf we separate out the input embedding and the final output module , we are left with the Transformer core\n\n\ud835\udc21 \u00af 1 , \ud835\udc21 \u00af 2 , \u2026 , \ud835\udc21 \u00af T = TransCore \u200b ( \ud835\udc21 1 , \ud835\udc21 2 , \u2026 , \ud835\udc21 T ) , subscript \u00af \ud835\udc21 1 subscript \u00af \ud835\udc21 2 \u2026 subscript \u00af \ud835\udc21 \ud835\udc47 TransCore subscript \ud835\udc21 1 subscript \ud835\udc21 2 \u2026 subscript \ud835\udc21 \ud835\udc47 \\mathbf{\\bar{h}}_{1},\\mathbf{\\bar{h}}_{2},\\ldots,\\mathbf{\\bar{h}}_{T}=\\textsc{TransCore}(\\mathbf{h}_{1},\\mathbf{h}_{2},\\ldots,\\mathbf{h}_{T}), (2)\n\nwhich consists of layers that compute final hidden states for each token. Each layer is composed of self-attention and feedforward sublayers. In the self-attention sublayer, causal masking is applied to prevent tokens from attending to any future token, and we use relative position embeddings [23]. See [6] for more details about the sublayer architecture of Transformers. 3.2 Staircase Attention Family of Models\n\nWe now describe our family of models that utilize staircase attention. Their graphical representation may be found in Figure 1. We start with Staircase models that are recurrent in both time and depth. 3.2.1 Staircase Model\n\nUnlike Transformers, a Staircase model ingests input tokens in smaller chunks, as shown Figure 1a. Inside a Staircase model lies a Transformer core that processes each input token in recurrent steps. With each recurrent step, a Staircase model moves tokens forward in time, which we call the forward step size. In addition to those forward tokens, the model simultaneously also processes tokens that come from the previous step, which we call backward tokens. We refer to the total number of tokens that are being processed in parallel as the step size. Let us denote a chunk of C input tokens as . Here is the embedding vector of the token . At each step, the Transformer core processes chunks in parallel\n\n\u210b i \u2212 N + 1 N , \u2026 , \u210b i \u2212 1 2 , \u210b i 1 = TransCore \u200b ( \u210b i \u2212 N + 1 N \u2212 1 , \u2026 , \u210b i \u2212 1 1 , \u210b i 0 ) . superscript subscript \u210b \ud835\udc56 \ud835\udc41 1 \ud835\udc41 \u2026 superscript subscript \u210b \ud835\udc56 1 2 superscript subscript \u210b \ud835\udc56 1 TransCore superscript subscript \u210b \ud835\udc56 \ud835\udc41 1 \ud835\udc41 1 \u2026 superscript subscript \u210b \ud835\udc56 1 1 superscript subscript \u210b \ud835\udc56 0 \\mathcal{H}_{i-N+1}^{N},\\ldots,\\mathcal{H}_{i-1}^{2},\\mathcal{H}_{i}^{1}=\\textsc{TransCore}(\\mathcal{H}_{i-N+1}^{N-1},\\ldots,\\mathcal{H}_{i-1}^{1},\\mathcal{H}_{i}^{0}). (3)\n\nAmong the input chunks, only contains new token embeddings while the remaining chunks come from the previous recurrent step. The superscript of denotes the number of computational passes through the Transformer core. After passes through the core module, the output for a particular token is computed with\n\ny t = f out \u200b ( \ud835\udc21 \u00af t ) for all \u200b \ud835\udc21 \u00af t \u2208 \u210b i N . formulae-sequence subscript \ud835\udc66 \ud835\udc61 subscript \ud835\udc53 out subscript \u00af \ud835\udc21 \ud835\udc61 for all subscript \u00af \ud835\udc21 \ud835\udc61 superscript subscript \u210b \ud835\udc56 \ud835\udc41 y_{t}=f_{\\text{out}}(\\mathbf{\\bar{h}}_{t})\\quad\\text{for all}\\;\\mathbf{\\bar{h}}_{t}\\in\\mathcal{H}_{i}^{N}. As you can see, an input token gets processed by the same core module times. This makes it possible to control the amount of computation by varying the number of recurrent steps without changing the number of parameters of the model. Feeding states computed by the previous step into the next step computation makes Staircase models recurrent in time like RNNs because each recurrent step moves forward tokens. There are two advantages to this type of recurrence. First, the number of non-linear computations between an input token and output token linearly increases with their distance . In contrast, Transformers are strictly a feedforward model that has a fixed number of computation steps. The second advantage is that information can pass to future steps without any limits, whereas standard Transformers are limited by their token truncation length. These two advantages make recurrent models capable of maintaining an internal state, but more importantly of updating this state continuously through time. However, unlike RNNs, Staircase models take advantage of the attention mechanism in the same way as Transformers, and store state across multiple vectors: the backward tokens. Like Transformers, they thus take advantage of parallelism. 3.2.2 Cached Staircase Model\n\nIn Staircase models, the self-attention sublayer processes tokens at a time. This means how far a token can directly attend to is limited by this context size . However, the hyperparameter also controls the number of recurrent computations, and one might want to decouple these two factors to control context size versus recurrence. Here we propose a simple solution for increasing the context size while keeping the recurrent computation constant, shown graphically in Figure 1b. We do this by introducing a new hyperparameter and put hidden states in a cache after recurrent steps. Once a hidden state is in the cache, it stays the same, requiring no additional computation\n\n\u210b i n = \u210b i M for \u200b n > M . formulae-sequence superscript subscript \u210b \ud835\udc56 \ud835\udc5b superscript subscript \u210b \ud835\udc56 \ud835\udc40 for \ud835\udc5b \ud835\udc40 \\mathcal{H}_{i}^{n}=\\mathcal{H}_{i}^{M}\\quad\\text{for}\\;n>M. This means the number of recurrent computations on a particular input is limited to . However, hidden states stay in the cache for the remaining steps so other tokens still can attend to them. This is achieved by including cached hidden states only when computing keys and values in the self-attention sublayer of the Transformer core. As a result, the self-attention sublayer will have keys and values, but only queries, reducing its computational complexity from to . As cached hidden states are excluded from the feedforward sublayer altogether, the computational complexity there changes from to . Thus, the context size can be increased by picking a larger , but the amount of computation can be reduced by choosing a smaller . For example, for , we can see that the reduction in computation is fold. 3.2.3 Global Cached Staircase Model\n\nFor sequence lengths that are not excessively long, it may be desirable at any stage of computation to always have access to all the tokens from the past, whereas the models discussed so far have the limit of tokens, see Figures 1a and 1b. We can extend the Cached Staircase model to look back across all tokens, called the Global Cached Staircase. This is achieved by increasing by one with each step, so all prior representations are in the cache and available during later computations, shown in Figure 1c. We still employ the cache hyperparameter as before to control the amount of recurrence and computation necessary during the steps of processing. 3.2.4 Ladder Model\n\nHere we introduce a special variant of the Staircase model, shown in Figure 1d. The forward step size is a variable that we can freely adjust. Instead of taking the same small forward steps each time, we consider an extreme case where the model ingests all tokens at once and then sets the forward step to 0 for the remaining steps. With this choice, the model is recurrent only in its depth and lacks recurrence in time, so we call it the Ladder model. This method can be viewed as applying the Transformer core multiple times on the given sequence of input tokens, on each application taking the output hidden state of the application as input for the next application (here, viewing each individual token as belonging to its own separate chunk). In this way, it may also be seen as a generalization of previous methods with shared weights [8, 9]. We use the same techniques as Transformer-XL [15] for processing very long or unbounded sequences. First, each token will attend to a fixed number of previous tokens rather than the whole sequence. This reduces the computational complexity of the self-attention from to assuming . Next, we split the input sequence into smaller manageable segments and let the model process them sequentially. To avoid the segment boundaries from obstructing attention across segments, the hidden states computed in the previous segments are kept in cache (different from the cache in Section 3.2.2). Then, this cache is made available in the self-attention sublayer for subsequent segments so a token can attend to a token in the previous segment. See [15] for more details about this caching mechanism. 3.3 Relation to Existing Models\n\nTransformer\n\nThe standard Transformer corresponds to a Staircase model with a large chunk size and no recurrence, or equivalently a Ladder model with only steps (i.e, no recurrence). While it is efficient at processing tokens in parallel, it has no ability of retaining and recomputing state across sequences, other than by fitting those tokens into the current processing block. Feedback Transformer\n\nThe Feedback Transformer [19] is equivalent to a Cached Staircase model with a chunk size of (i.e., a forward step of a single token), and caching after step, i.e. all tokens in the past are part of a fixed cached memory. In contrast, larger chunk sizes allow general Staircase models to exploit parallelism and be more efficient than the Feedback Transformer, while increasing can give more expressive power. We compare to this model in our experiments. Recurrent Neural Networks\n\nRNNs [2] that store recurrent state in a single vector and ingest tokens one at a time can be compared to a Staircase model with a single backward token and a single forward token, i.e. a chunk size of and . Staircase models exploit parallelism similar to Transformers while maintaining several chunks of recurrent (per token) features to more expressively track state than conventional RNNs. Memory Networks\n\nMemNets as implemented in [7] employ recurrence in the stacked layers of attention and computation for the current token, but only compute input embeddings for previous tokens, and can thus be seen as a kind of simplified Ladder model, or equivalently a Global Cached Staircase with a chunk size of and caching at all previous steps, . Transformer-XL\n\nTransformer-XL, like Cached Staircase, also has a caching mechanism which eases computation when dealing with earlier chunks of tokens. The difference is that Staircase models take the last state of earlier chunks and process that state further in a recurrent way; Transformer-XL on the other hand extends each layer of the Transformer\u2019s attention mechanism to using old cached states at each layer, i.e. does not build further computations on top of the old state. We use this as a baseline in our experiments. Universal Transformer\n\nUniversal Transformers [8] propose to tie all the layer weights in a Transformer, which can be seen as a Ladder model with a Transformer core of a single layer. We also compare to this model. 4 Experiments\n\nWe use two types of tasks to test our family of models and compare its variations, along with Transformer-XL [15] and Feedback Transformer [19] baselines. First, we have two artificial state tracking tasks specifically designed to test the model\u2019s ability to keep track of evolving changes. Next, we use real-world language modeling tasks. See the supplementary material for further details of our experimental setup for training, including all hyperparameter choices. 4.1 Tasks\n\nRandom Walk\n\nAt each time step, an agent in a small grid takes a random action that turns the agent, or moves it forward. A model has to predict the agent\u2019s location given these actions. This seemingly simple task requires recurrency and has been shown to make feedforward models like Transformers fail. We borrow this task from [19], but increase the length from 100 to 400 to make it more challenging. See [19] for more details about this task. Algorithm\n\nThis task consists of simple algorithmic operations that need to be executed sequentially. Some operations depend on the current variable values, which makes it necessary to keep track of variable values and update them if necessary. Thus, it also requires recurrency, and like the Random Walk task has been shown to make Transformers fail. However, it is more complex and also cannot be solved with LSTMs [19]. We use the 3 variable version of the task from [19]. Enwik8\n\nEnwik8 is a character-level language modeling task [24] that consists of 100M tokens taken from Wikipedia articles. The challenging part of this data is that it requires long-term reasoning [25] because tokens are characters instead of words. Pushshift.io Reddit\n\nWe use a variant of Reddit discussions, which has also been used in several existing studies, see e.g. [26, 27, 28, 29]. Following [30], we use a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io [31], training to generate a comment conditioned on the full thread leading up to the comment, spanning 1.5B training examples from Reddit obtained from Pushshift through July 2019.",
    "staircaseattn-1": "See [32] for more details. We concatenate the dataset to view it as a language modeling task. BASE Data\n\nWe use the language modeling dataset from [21], which consists of approximately 100B tokens, combining the corpora used in [33] that consists of Wikipedia, BookCorpus, CC-News, OpenWebTex and Stories, along with the English subset of the CC100 corpus [34]. 4.2 Results\n\nOur results on all of the tasks are summarized in Table 1. For each task, all the models have the same number of parameters and use the same Transformer core architecture implementation. For Random Walk and Algorithm tasks, we trained each model with multiple seeds and chose the best seed as measured by their validation performance. The specific configuration of each model can be found in Table 6 in the supplementary material. We see clear wins from our Staircase attention family of models on all the tasks, and the following subsections will analyze these results in detail. 4.2.1 Staircase models have strong performance on state tracking tasks\n\nThe Random Walk and Algorithm tasks are specifically designed to test a model\u2019s capability of tracking states: to store given information internally and update it with new information coming at each time step.",
    "staircaseattn-2": "In Table 1 we report results from running multiple training seeds, and selecting the one with best performance on the validation set. In Figure 2 we show detailed results when varying the recurrent computation steps, reporting the mean and standard deviations amongst the seeds. The powerful Transformer-XL baseline performs poorly here due to its lack of a recurrent mechanism, confirming the results from [19]. The self-attention can access a hidden state far away in the past, but updating that hidden state with a new piece of information brings it up one layer higher. Thus, in a Transformer with layers, a particular hidden state can be updated only times before it reaches the final output layer, and becomes unavailable for future computations. This limited computation depth is a problem in the Random Walk task, for example, because the model needs to internally store the agent\u2019s location and update it with actions taken at every time step for hundreds of steps. The Ladder model does perform better with more recurrent steps on the Random Walk task, eventually solving it with 8 steps. With recurrent steps, a token gets processed by layers. This, in turn, also increases the number of updates that can be applied to a particular hidden state. However, this was not sufficient on the harder Algorithm task, where the Ladder model failed. The Staircase model successfully solves both tasks, even with only two recurrent steps. Thanks to its recurrence through time, the computation depth is only restricted by the input sequence length itself. More concretely, each recurrent step can update the output from the previous step and feed it to the next step, making it possible to maintain and update an internal state without limit. The Cached Staircase model also performs reasonably well on those tasks. While we only ran this model with computation step, it is still recurrent in time which is more critical for these tasks than increased computation. The Global Cached Staircase models did not perform any better, see Table 7 and 8 in the supplementary material, so we do not consider them in further experiments. The Feedback Transformer solves both tasks, which is not surprising as it is a particular case of a Cached Staircase model with a forward step . However, such fine-grained steps make it slow to train in practice because of the reduced parallelism, as we will see in the analysis in the next section. 4.2.2 Staircase attention models outperform Transformers for the same number of parameters on language modeling tasks\n\nTable 1 shows results on the three language modeling tasks, Enwik8, Pushshift.io Reddit and BASE Data. We show performance versus recurrence plots for the first two tasks in particular in Figure 3. We also show more detailed performance numbers on the Pushshift.io Reddit task in Table 2. In all three tasks, one general trend is that more recurrent steps improve the performance significantly. On the Pushshift.io Reddit task, we see a 4 perplexity point improvement over the Transformer-XL baseline without adding any new parameters when using 8 recurrent steps, and a 5 point improvement on BASE Data. Making a twice as deep Transformer-XL (marked with \u201c2x\u201d in Table 2) improves the baseline at the expense of having twice as many parameters than the Staircase and Ladder models, but is still 1 perplexity point worse, showing the power of our recurrent models. Our family of models provides a new way of improving model performance without increasing the number of parameters that is generally applicable. On Enwik8, the Ladder models outperformed the Staircase models for matching recurrent steps, whereas on Pushshift.io Reddit and BASE Data they are about even. This could be due to the long context requirement of the character-level data of Enwik8. The Staircase model tries to compress past context into a fixed number of hidden states, equal to backward tokens to be precise. Table 2 also shows the time it takes for training on a single batch for each model. Models with more recurrent steps take longer to train as they have to perform more computations per token, but are still tractable and much faster than the Feedback Transformer. The Feedback Transformer does not perform more computations, but it is slow because it processes one token at a time instead of tokens, and also generally performs worse in our language model experiments. The Staircase model is slightly faster than the Ladder models because it generally has a smaller context size. 4.2.3 Staircase model\u2019s forward size and step size control its performance\n\nThe forward step chunk size and overall staircase step size are hyperparameters in Staircase models, where the effective number of recurrent steps is determined by those choices of in the Staircase model, or truncated to only steps due to caching in the Cached Staircase model. In Table 3, we compare different values of step size and forward size on the Pushshift.io Reddit task for those models with differing numbers of recurrent steps. We see that, in general, the models are robust to different choices of those values. Larger forward step sizes are preferable in terms of computational efficiency because they allow more parallelism, but if they are too large some performance in terms of perplexity is lost. We see evidence of this in Table 7 and Table 8 in the supplementary material where the Cached Staircase model performs poorly as its forward size increases. 4.2.4 Ladder model recurrence analysis\n\nIn this section we analyze the Ladder model by comparing it against some alternatives. Let us consider a Transformer core with layers A and B as an illustrative example. First, a Ladder model with 2 recurrent steps, i.e. the pattern ABAB, can be compared directly to a twice as big Transformer-XL baseline, i.e with a pattern ABCD. Both models in that case would be the same size and number of layers except the Ladder model has tied weights (and hence the baseline has twice as many parameters), and both would have the same test speed. As shown in Table 4, going from the baseline (AB pattern, with 8 layers) to the 2x baseline (ABCD pattern, with 16 layers) gives close to a 3 perplexity point improvement on Pushshift.io Reddit due to increasing the number of parameters, whereas a Ladder model with 2 recurrent steps (ABAB) is slightly behind but still improves over the 1x baseline by 1.5 perplexity points. However, with no extra parameters it matches the 2x baseline with 4 recurrent steps (ABABABAB), and outperforms the 2x baseline with 8 recurrent steps. We also try out another way of composing the layers of the Ladder model, an AABB pattern instead of ABAB \u2013 that is, to repeat each layer multiple times consecutively before going on to the next, instead of repeating the whole Transformer core. While there are slight differences, overall there was no clear winner. While they learn quite different functions (e.g if you take a trained model and place its learned weights in the other configuration the testing results will be poor) it seems learning can adapt well to either design. We note that the Universal Transformer (that uses the pattern AAAA\u2026) performs poorly, at least when using the same layer size as these other models, as shown in Table 2. 5 Conclusion\n\nWe present Staircase Attention, which re-introduces recurrence back into the family of Transformer-based models across both time and depth. We show that our Staircase models are able to solve tasks which require tracking of state that conventional Transformers cannot. Our models deliver more modeling power per parameter than conventional Transformers via their recurrence, thus also giving improved performance in language modeling tasks for the same number of parameters, which is especially important in regimes which are memory rather than compute bound. Future work should continue to investigate how recurrence can be built into sequence models, as without a memory component our systems will always be limited to only short-term reactive tasks with limited input. The approaches detailed here are one way forward and should be studied in further applications. References\n\nBengio et al. [2003] Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. The journal of machine learning research, 3:1137\u20131155, 2003. Elman [1990] J. Elman. Finding structure in time. Cogn. Sci., 14:179\u2013211, 1990. Hochreiter and Schmidhuber [1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Mikolov et al. [2010] Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan \u010cernock\u1ef3, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association, 2010. Bahdanau et al. [2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. Sukhbaatar et al. [2015] Sainbayar Sukhbaatar, Arthur D. Szlam, J. Weston, and R. Fergus. End-to-end memory networks. In NIPS, 2015. Dehghani et al. [2018] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers.",
    "staircaseattn-3": "arXiv preprint arXiv:1807.03819, 2018. Lan et al. [2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.",
    "staircaseattn-4": "arXiv preprint arXiv:1909.11942, 2019. Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Press et al. [2019] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers.",
    "staircaseattn-5": "arXiv preprint arXiv:1911.03864, 2019. Lu et al. [2019] Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu. Understanding and improving transformer from a multi-particle dynamic system point of view.",
    "staircaseattn-6": "arXiv preprint arXiv:1906.02762, 2019. Chen et al. [2018] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. The best of both worlds: Combining recent advances in neural machine translation.",
    "staircaseattn-7": "arXiv preprint arXiv:1804.09849, 2018. Hao et al. [2019] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer.",
    "staircaseattn-8": "arXiv preprint arXiv:1904.03092, 2019. Dai et al. [2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL (1), pages 2978\u20132988. Association for Computational Linguistics, 2019. Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.",
    "staircaseattn-9": "arXiv preprint arXiv:1904.10509, 2019. Kitaev et al. [2019] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019. Beltagy et al. [2020] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.",
    "staircaseattn-10": "arXiv preprint arXiv:2004.05150, 2020. Fan et al. [2020] Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. Fedus et al. [2021] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.",
    "staircaseattn-11": "arXiv preprint arXiv:2101.03961, 2021. Lewis et al. [2021] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021. Al-Rfou et al. [2019] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, 2019. Shaw et al. [2018] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.",
    "staircaseattn-12": "In NAACL-HLT (2), 2018. Mahoney [2011] Matt Mahoney. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html, 2011. Sukhbaatar et al. [2019] Sainbayar Sukhbaatar, \u00c9douard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers.",
    "staircaseattn-13": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331\u2013335, 2019. Yang et al. [2018] Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Learning semantic textual similarity from conversations. In Proceedings of The Third Workshop on Representation Learning for NLP, pages 164\u2013174, Melbourne, Australia, July 2018. Association for Computational Linguistics. Mazar\u00e9 et al. [2018] Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, and Antoine Bordes. Training millions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2775\u20132779, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. Keskar et al. [2019] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. CTRL: A conditional transformer language model for controllable generation.",
    "staircaseattn-14": "arXiv preprint arXiv:1909.05858, 2019. Shuster et al. [2019] Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason Weston. The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents, 2019.",
    "staircaseattn-15": "Humeau et al. [2019] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring.",
    "staircaseattn-16": "In Proceedings of the International Conference on Learning Representations, 2019. Baumgartner et al. [2020] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The pushshift reddit dataset.",
    "staircaseattn-17": "arXiv preprint arXiv:2001.08435, 2020. Roller et al. [2020] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637, 2020. Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.",
    "staircaseattn-18": "arXiv preprint arXiv:1907.11692, 2019. Conneau et al. [2019] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019. Appendix A Appendix\n\nA.1 Task Setups\n\nWe provide the hyperparameter setups shared across our models for each task in Table 5. In addition, the hyperparameters tuned for each model for the best performance are shown in Table 6, which were selected using validation performance. We also provide a textual description of some aspects of the base models below. Random Walk\n\nWe train 4-layer models with a hidden size of 256 and 4 attention heads. We use a learning rate of 1e-4 and 1000 warmup updates to train the models. They are trained for 50k updates with batch size 512. The global staircase models are trained for 400k updates since they need longer to converge. We ran each setting 10 times, except for the Cached Staircase model which was run 5 times. Algorithm\n\nWe train the 4-layer model with a hidden size of 256 and 4 attention heads. Models are trained to 100k updates with batch size of 256 and learning rate of 1e-4, 1000 warmup updates. We train the global staircase models for 400k steps. We ran each setting 10 times, except for the Cached Staircase model which was run 5 times. Pushshift.io Reddit\n\nWe train 8-layer models with hidden size of 1024, 8 attention heads. They are trained for 100k updates with a learning rate of 7e-4, 8000 warmup updates and a batch size of 512. BASE Data\n\nWe train 8-layer models with hidden size of 1024, 8 attention heads. They are trained for 80k updates with a learning rate of 7e-4, 8000 warmup updates and a batch size of 512. Enwik8\n\nWe train 8-layer models with 8 attention heads. They are trained for 100k updates with a learning rate of 7e-4, 8000 warmup updates and a batch size of 512. A.2 Further Detailed Results\n\nDetailed results for a number of our tasks beyond those results reported in the main paper are provided in Tables 7, 8, 9 and 10. Appendix B Computational Resources\n\nAll experiments were run in an internal cluster using 32GB V100 GPUs.The usage varies on recurrent steps; here, we list the maximum resources used in experiments (e.g. 8x ladder)\n\n\u2022\n\nRandom Walk experiment used maximum 8 GPUs for 7 hours. \u2022\n\nAlgorithm experiment used maximum 2 GPUs for 30 hours. \u2022\n\nLanguage modeling experiments used maximum 32 GPUs for 30 hours. Experiments were run only once. Appendix C Limitations, Scope and Societal Impact\n\nImprovements to language modeling could have implications on a large number of surfaces across humanity. Additionally, our experiments show how to obtain higher performance by using more compute (with the same number of parameters), therefore increasing carbon emissions. The datasets used contain PII and offensive content in the form of text, as they were originally procured from the internet (we do not release any data in this work). Our family of models improves perplexity in language modeling and tracking of state in certain tasks compared to methods with the same number of model parameters, but typically costs more in terms of compute.",
    "staircaseattn-19": "Therefore, whether this is beneficial depends on the end application and user requirements. E.g., these methods are good in memory-bound setups, but when compute-bound it will depend on the precise setup. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Mar 19 11:40:52 2024 by LaTeXML"
}