{
    "semanticcompress-0": "Extending Context Window of Large Language Models via Semantic Compression\n\nWeizhi Fei\u2020 \u2021 & Xueyan Niu \u2021 \u2020 Department of Mathematical Sciences, Tsinghua University, Beijing, China \u2021 Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.",
    "semanticcompress-1": "\\ANDPingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han Huawei Technologies Co., Ltd Correspondence to: niuxueyan3@huawei.com. Abstract\n\nTransformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead. 1 Introduction\n\nThe recent successful release of large language models (LLMs) such as ChatGPT (Radford et al., 2019) and LLaMA (Touvron et al., 2023) has sparked significant research efforts from both industry and academia. These LLMs have demonstrated the ability to engage in fluent and coherent conversations with human users, and have shown exceptional performance across various tasks, including document summarization, question-answering, dialogue bots, and code generation copilots. One critical issue faced by state-of-the-art (SoTA) LLMs is the restriction on the length of text that can be inputted into the model at once. When the input context exceeds the limit of the context window, the performance of these models rapidly declines. This limitation poses a challenge when it comes to handling long texts such as scientific papers, novels, and legal contracts with current LLMs. As a result, there has been a growing interest in finding ways to extend the input length without significantly compromising the model\u2019s performance. The limitation on the context window primarily stems from the quadratic computation of the self-attention mechanism in the transformer. Handling lengthy texts significantly increases the computational costs in terms of memory and time. Typically, models are trained on short contexts, and the maximum sequence length (i.e., the context window) is determined. If the models are compelled to generate contexts that exceed the context window, they tend to compromise the quality of the output due to the lack of position encoding information during the training process. Furthermore, generating long sequences imposes substantial memory requirements on the computational device. This accumulation of memory requirements and the lack of effective position encoding can result in length generalization failure (Anil et al., 2022), where the models struggle to generate meaningful and coherent text beyond a certain context window size. Some approaches have been developed to address the aforementioned challenges. One approach is to devise architectures with nearly linear complexity, which enables efficient scaling to handle very long sequences. However, training a large model from scratch incurs substantial cost. Another strategy involves employing interpolation and fine-tuning techniques to adapt the position encoding to unseen sequence lengths. While this method has the potential to compromise the overall performance of LLMs, it still demands significant time and GPU resources for fine-tuning and inference on long sequences. Therefore, it is more efficient and resource-friendly to design methods that do not necessitate altering the parameters of the pre-trained model. While most previous algorithms relied on modifying the pre-trained model, we instead exploit the statistical properties of input natural language. One empirical phenomenon, known as Zipf\u2019s law (Zipf, 2016), observes that a small set of the most frequent word tokens in a large corpus of natural language account for almost all occurrences. This pattern arises from the tendency of language users to minimize effort in their daily conversations. Consequently, by utilizing an expanded vocabulary, sentences can be significantly shortened while preserving the same semantic meaning. Moreover, it is common for language users to include redundant words during communication (Strunk Jr, 2007). These language habits are prevalent among users, and we propose to include a semantic compression module to mitigate the redundancy associated with these habits. Our proposed semantic compression method, reminiscent of lossy source coding in information theory, extends the context window by equivalently shortening the long text while preserving the semantic meaning. This procedure is conducted before inputting the tokens into the pre-trained LLMs. As illustrated in Fig. 1, the input undergoes compression before being transmitted to the LLM for various potential tasks. The semantic compression method can be customized and optimized for downstream tasks, taking into consideration practical constraints such as time and memory resources. The implementation of the semantic compression module is straightforward and can easily be incorporated into other interpolation-based context window extension methods and black box APIs. It demonstrates enhanced performance compared to SoTA interpolation-based methods on a range of tasks, including single-document question answering, multi-document question answering, summarization, few-shot learning, and information retrieval, using real-world datasets while incurring no extra parameter updates or memory consumption. Empirically, the proposed method is computational efficient and achieves 6-8 times context window extension. Our contributions:\n\n\u2022\n\nWe introduce a context window extension framework for LLMs that utilizes semantic compression. This framework serves as a plug-and-play tool to mitigate redundancy in input texts by efficiently performing topic modeling. \u2022\n\nWe construct a graph representation of the input to identify distinct sections of the text that pertain to different topics. The result is the segmentation of long texts into separate chunks, each focusing on a specific topic. We then conquer each chunk independently, resulting in a concise version of the original texts. This compression technique helps to condense the information while preserving the key ideas and context. \u2022\n\nWe demonstrate the applicability of our proposed semantic compression method through extensive experiments. The results highlight the advantages of our method in several key applications, including single-document question answering, multi-document question answering, summarization, few-shot learning, and information retrieval. 2 Related work\n\nWith the advancement of SoTA LLMs, significant progress has been made in extending the context window lengths. 2.1 Extrapolation and Interpolation\n\nThe mainstream line of research aims to adapt existing language models trained on short texts to accommodate longer ones during inference (Anil et al., 2022). The key idea is to modify the positional embedding, which has only been trained on short texts. Several studies are based on the Rotary Position Embeddings (RoPE) of LLaMA and methods of adjusting it to the longer sequences. Chen et al. (2023a) develops the Position Interpolation (PI) method to linearly scale the input positional indices. Peng et al. (2023) presents YaRN, an efficient extrapolate mechanism inspired by the neural tangent kernel, to extend the context window to k and k. 2.2 Efficient Attention Operations\n\nDue to the self-attention mechanism, the inference cost of LLMs grows quadratically with the sequence length.",
    "semanticcompress-2": "Many methods have been proposed to decrease the complexity. Dai et al. (2019) present Transformer-XL which utilize segment-level recurrence agency and a novel positional encoding scheme. Beltagy et al. (2020) introduce Longformer with a sparse attention mechanism that scales linearly with sequence length. Bo (2021) provides a faster transformer, RWKV, which combines the strength of RNN and has linear complexity during inference. Dao et al. (2022) propose FlashAttention, a chunking strategy for the input, and utilize recomputation to avoid the quadratic complexity of attention computation. While these methods have the potential to handle longer input sequences (Ding et al., 2023), training new models can be costly. Moreover, these methods are not effective when dealing with out-of-distribution content lengths. The introduction of new positional embeddings requires fine-tuning on long sequences to adapt to the increased length, which can be computationally expensive. To address this, LongLoRA is introduced by Chen et al. (2023b), offering an efficient fine-tuning method with limited computational costs. More details on several other chunking strategies are provided in the survey by Huang et al. (2023). 2.3 Prompting\n\nThere are ongoing efforts to extend the context window through smart prompting designs. Wingate et al. (2022) utilize soft prompts to encode more information using fewer tokens. Chevalier et al. (2023) present AutoCompressor, which utilizes soft prompts to compress the input sequence and then extends the original length of the base model. Both Zhou et al. (2023) and Wang et al. (2023) recurrently apply LLMs to summarize the input texts to maintain long short-term memory for specific purposes such as story writing and dialogue generation, respectively. 3 Methodology\n\nWe propose our semantic compression method for extending the context window. The core idea is to compress the input into shorter texts without losing the key information and important details. This enables us to effectively include more content within the fixed input length constraint of the LLM. Fig. 2 provides an overview of our method, which leverages pre-trained summarization models commonly used in Natural Language Processing (NLP). Existing summarization methods also have limitations regarding the length of the input. Here, we propose a divide-and-conquer based approach that takes into account the structure of the text. By identifying the topic structure of lengthy texts and dividing them into blocks that exhibit a certain level of mutual independence, the content within each block can be compressed efficiently due to their statistical correlation. Each block is then processed in parallel using pre-trained models, and the results are combined to create a condensed textual input that can be processed by the LLM. This approach aims to provide a more efficient and effective way of summarizing long texts by leveraging both the structure and content of the original text. 3.1 Model\n\nReal-world textual content, such as speech and book, frequently displays hierarchical structures, wherein each section is structured around a particular topic, and different sections differ in topic in a sequential manner. This hierarchical structure, based on topics, bears resemblance to cliques in graphs. To identify this structure within long texts, we utilize weighted graphs to represent them and employ clustering methods to detect cliques in these graphs. The cliques can then be utilized to represent the topic-based content of the text, allowing us to obtain chunks based on the semantic relevance of the topics. We begin by sequentially constructing sentence-level blocks within given lengths and representing them as nodes in our graph. In this step, we parse the text into different sentences or sub-sentences based on punctuation marks. Next, we sequentially fill the sentence-level blocks until they exceed the desired length before proceeding to the next blocks. Once we have obtained the sentence-level blocks, we connect the graph representation of long text based on a pre-trained sentence embedding model (e.g., MiniLM (Wang et al., 2020)), where the weight represents the semantic similarity between the -th and -th sentence-level blocks. Typically, this similarity is computed using cosine similarity, which measures the cosine of the angle between two embeddings. If the similarity between two blocks is higher, it indicates that they are closer in topics. 3.2 Topic-Based Chunking\n\nWe then apply clustering algorithms on the graph to identify the underlying topic structure. Within each cluster, we group the sentence-level blocks sequentially to obtain the topic-based chunks, which can then be handled simultaneously by the pre-trained model chosen according to the downstream task. The number of clusters can be adjusted to regulate the length of the text following semantic compression. If these semantic chunks still surpass the predetermined length, the identical procedure is repeated to acquire sub-level topic structures. The obtained topic structures are tree-like, which can be flattened in accordance with the order of the original content. As per the model, each chunk is semantically centered around a specific topic, and these topics are mutually exclusive. Consequently, these chunks can be compressed independently by utilizing a pre-trained summarization model. Choosing from different pre-trained summarization models allows a trade-off between efficiency and effectiveness. Consequently, we can opt to selectively substitute the original chunks with the output of these pre-trained models to ensure the preservation of the underlying topic structure. The semantic compressed text can be forwarded to the LLM directly or in combination with other extension schemes to further enhance the overall outcome. 4 Experiments\n\nWe demonstrate that the proposed method of semantic compression can effectively extend the context window by up to 7-8 times without modifying the parameters of the pre-trained models. Furthermore, the semantic compression module can be seamlessly integrated with existing methods, allowing for further extension of the context window. This versatility enables our approach to be adapted and combined with other techniques, enhancing the overall performance and flexibility. To evaluate the performance of our method, we conduct experiments on several language tasks that require understanding of long contexts. These tasks include passkey retrieval, single-document question answering, multi-document question answering, summarization, and few-shot learning. In each task, the model is provided with a sequence of context (typically lengthy texts) and a sequence of text (e.g., a prompt), and it is expected to generate the output answer . Additionally, we also investigate the perplexity metric (Peng et al., 2023), which measures the model\u2019s ability to predict the text and serves as an indicator of the fluency of the generated output. This analysis allows us to assess not only the effectiveness but also the quality of the generated output. 4.1 Tasks and Datasets\n\nWe begin by evaluating the proposed semantic compression method on various standard benchmark tasks, utilizing the pre-trained 7B LLaMA model (Touvron et al., 2023). The original context window size of this model is . The tasks and datasets employed in our evaluation are sourced from the SCROLLS benchmark (Shaham et al., 2022) and LongBench (Bai et al., 2023). These datasets provide comprehensive and diverse contexts for our analysis. Passkey Retrieval\n\nRetrieval has been an important application of LLMs. We evaluate the proposed method using a synthetic task for passkey retrieval introduced by Mohtashami & Jaggi (2023), where prompts are synthesized to conceal a generated passkey within a randomly chosen section of a long document. The passkey retrieval task assesses the model\u2019s capacity to extract important information from any position within lengthy contexts. An illustration of the task is shown in Fig. 3. The synthetic long text incorporates the passkey digits, and the task for the LLM is to retrieve these digits from the input text. Further specifics can be found in Appendix A. General NLP Tasks\n\nLongBench (Bai et al., 2023) is a multi-task benchmark designed for long text scenarios, consisting of six distinct tasks. In this study, we focus on the three English tasks from the set of four natural language tasks, namely single-document question answering, multi-document question answering, summarization, and few-shot learning.",
    "semanticcompress-3": "Each of the selected datasets contains 200 instances. Further information can be found in Appendix A. Fluency\n\nWe evaluate the fluency of our semantic compression method using the perplexity score, which is defined as the exponential of the average negative log-likelihood of the probabilistic model on the distribution i.e.,\n\nPPL \u200b ( D , P ) := exp \u2061 ( \u2212 \ud835\udd3c x \u2208 D \u200b log \u2061 P \u200b ( x ) ) . assign PPL \ud835\udc37 \ud835\udc43 subscript \ud835\udd3c \ud835\udc65 \ud835\udc37 \ud835\udc43 \ud835\udc65 \\mathrm{PPL}(D,P):=\\exp(-\\mathbb{E}_{x\\in D}\\log P(x)). A smaller perplexity score indicates more fluent sequences that are consistent with the model. 4.2 Baselines\n\nWe choose SoTA solutions from each mainstream approach as our baselines. Fixed-size chunking\n\nTo accommodate long context within a fixed-size context window, chunking is a straightforward yet efficient approach. In NLP related applications, large pieces of text are usually broken down into smaller segments for targeted applications. When the input length exceeds the context window, the fixed-size chunking method (Bai et al., 2023) truncates the input sequence from the middle. This is because the most significant information typically resides at the beginning and end of the sequence. Interpolation-based method\n\nYaRN (Peng et al., 2023) is a computationally efficient method for interpolating position encoding, which dynamically adjusts the Relative Positional Encoding (RoPE) over dimensions and scales the attention. YaRN offers multiple length-extended models for different versions of Llama2, with the models being trained on a total of 64 GPUs from 8 A100 machines. In order to ensure a fair comparison, we choose the model based on Llama2 7B, adjusted from 4k to 64k, as our baseline. Fine-tuning approach\n\nLongLoRA (Chen et al., 2023b) is an efficient approach for fine-tuning that combines LoRA and shifts sparse attention to reduce computational costs. LongLoRA applies this technique to Llama2 models of different sizes, ranging from Llama2 7B, Llama2 13B, to Llama2 70B, with token lengths extended from 4k to 32k on a single 8 device. In order to ensure a fair and unbiased comparison, we choose the Llama2 7B model with context extension achieved through improved LoRA fine-tuning as our baseline. 5 Results\n\nWe report the main results along with a comprehensive analysis. Fluency\n\nWe utilize the Llama2 model as our baseline to evaluate the fluency of generated texts by calculating the perplexity (PPL) score. Samples from the GovReport dataset are selected at varying lengths, and the reference texts are compared to the generated texts during the computation. In cases where the length of the input text exceeds the context window of Llama2, our semantic compression module shortens the input, thereby allowing the model to continue generating new content fluently. The resulting scores are depicted in Fig. 5. The plots indicate that the perplexity of Llama2 initially decreases, but once it surpasses the window length, it rapidly increases. However, when our semantic compression method is employed, the PPL remains consistently low. This suggests that our approach successfully extends the context window up to three times without compromising the generation quality of the language model. Passkey Retrieval\n\nWe present the results of the passkey retrieval task in Fig. 5. When employing Llama2 for passkey retrieval, we observe a rapid drop in accuracy to zero once the input length surpasses the window size of . However, by utilizing our method, the retrieval accuracy of the Llama2 model remains above 90% even for inputs with lengths of up to 30,000. This indicates that the semantic compression method extends the context window size of the language model by approximately 7-8 times. Furthermore, we combine our method with the SoTA interpolation-based method, YaRN, to further expand the context window size to up to 60,000, while consistently maintaining an accuracy above 90%. General NLP Tasks\n\nWe present our results on various general NLP tasks in Table 1, including single-document question answering, multi-document question answering, summarization, and few-shot learning. When the token length is less than 4k, there is no need to compress the context, and our method performs at the same level as the original Llama2 model. However, both the interpolation-based method YaRN and the fine-tuning approach LongLora negatively impact the performance of the Llama2 model across almost all tasks. In the 4k-8k range, our method outperforms others in 8 out of 11 tasks. It is worth noting that our model performs slightly worse in the few-shot learning task. This can be attributed to the fact that few-shot learning necessitates more detailed information, whereas our compression scheme maintains information within a fixed window. Moving on to the 8k-16k range, our method achieves the best results in 9 out of 12 tasks, exhibiting similar performance to the 4k-8k range. In the 16k-32k range, our method outperforms others in 6 out of 11 tasks. In the 32k+ range, other methods fail due to out-of-memory issues, while our method still maintains 70% of the performance achieved in the 4k range. 6 Conclusion\n\nIn this work, we propose a novel approach to addressing the limitation of input length in large language models using semantic compression. By leveraging the statistical properties of natural language and exploiting redundancy in communication, we are able to significantly shorten texts while preserving their semantic meaning. This allows for a 6-8 time extension of the context window without the need for modifying the parameters of the pre-trained model or incurring additional computational costs. Furthermore, the implementation of our semantic compression module is straightforward and can be easily integrated into other interpolation-based methods and black box APIs. This provides flexibility and adaptability to different downstream tasks, considering practical constraints such as time and memory resources. We believe our work can lead to simpler context window extension method to be used in practice, thereby reducing the cost of large language models. References\n\nAnil et al. (2022) Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546\u201338556, 2022. Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding.",
    "semanticcompress-4": "arXiv preprint arXiv:2308.14508, 2023. Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. Bo (2021) PENG Bo. Blinkdl/rwkv-lm: 0.01, August 2021. URL https://doi.org/10.5281/zenodo.5196577. Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation.",
    "semanticcompress-5": "arXiv preprint arXiv:2306.15595, 2023a. Chen et al. (2023b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models.",
    "semanticcompress-6": "arXiv, 2023b. Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. ArXiv, abs/2305.14788, 2023. URL https://api.semanticscholar.org/CorpusID:258865249. Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978\u20132988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285. Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness.",
    "semanticcompress-7": "In Advances in Neural Information Processing Systems, 2022. Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers.",
    "semanticcompress-8": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4599\u20134610, 2021. Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.",
    "semanticcompress-9": "In Proceedings of the 10th International Conference on Learning Representations, 2023. Fabbri et al. (2019) Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.",
    "semanticcompress-10": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074\u20131084, 2019. Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 6609\u20136625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling-main.580. Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419\u20131436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112. Huang et al. (2023) Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large language models: A comprehensive survey, 2023.",
    "semanticcompress-11": "Ko\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328, 2018. Li & Roth (2002) Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. Mohtashami & Jaggi (2023) Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019. Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences. arXiv preprint arXiv:2201.03533, 2022. Strunk Jr (2007) William Strunk Jr. The Elements of Style. Penguin, 2007. Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.",
    "semanticcompress-12": "arXiv preprint arXiv:2302.13971, 2023. Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition.",
    "semanticcompress-13": "Transactions of the Association for Computational Linguistics, 10:539\u2013554, 2022. doi: 10.1162/tacl\u02d9a\u02d900475. URL https://aclanthology.org/2022.tacl-1.31. Wang et al. (2023) Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. Recursively summarizing enables long-term dialogue memory in large language models.",
    "semanticcompress-14": "arXiv preprint arXiv:2308.15022, 2023. Wang et al. (2020) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.",
    "semanticcompress-15": "Advances in Neural Information Processing Systems, 33:5776\u20135788, 2020. Wingate et al. (2022) David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5621\u20135634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.412. URL https://aclanthology.org/2022.findings-emnlp.412. Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-domain meeting summarization.",
    "semanticcompress-16": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905\u20135921, 2021. Zhou et al. (2023) Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023. Zipf (2016) George Kingsley Zipf.",
    "semanticcompress-17": "Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology. Ravenio Books, 2016. Appendix A Datasets\n\nSingle-Doc QA\n\n\u2022\n\nNarrativeQA (Ko\u010disk\u1ef3 et al., 2018) is a standard question-answering dataset that includes books from Project Gutenberg3 and movie screenplays from a list of websites. Question-answer pairs were provided by annotators, so that each of the 1,567 books and scripts has about 30 questions and answers, and two reference answers are given for each question. \u2022\n\nQasper (Dasigi et al., 2021) is a question-answering dataset of NLP publications containing abstractive, extractive, and yes/no questions. \u2022\n\nMultiFieldQA-en (Bai et al., 2023) is a dataset created from multiple sources including legal documents, government reports, encyclopedias, and academic publications. Doctoral students were requested to annotate each article\u2019s queries and responses. Multi-Doc QA\n\n\u2022\n\nHotpotQA (Yang et al., 2018) includes many 2-hop questions written by native speakers based on two related paragraphs. \u2022\n\n2WikiMultihopQA (Ho et al., 2020) involves up-to 5-hop questions systematacially constructed by manual templates. Answering these questions requires reasoning paths and can not be solved by local content. \u2022\n\nMuSiQue(Trivedi et al., 2022) consists of up to 4-hop questions and removes shortcuts and naturalness questions. Each question contains 2-4 supplement paragraphs which present the reasoning path and related paragraphs. Summarization\n\n\u2022\n\nGovReport (Huang et al., 2021) collects detailed reports containing human-written summaries from the U.S. Government Accountability Office and Congressional Research Service. These reports span a wide variety of national policy issues. \u2022\n\nQMSum (Zhong et al., 2021) contains annotated meeting-summary pairs across many domains including including product, academic, and committee meetings. \u2022\n\nMultiNews(Fabbri et al., 2019) is a multi-document summarization dataset. (Bai et al., 2023) cluster 2-10 news articles discussing the same event or topic, each paired with a human-written summary and form a new long text summarization task. Few-Shot Learning\n\nTo construct few-shot learning with long text, (Bai et al., 2023) select a range of training examples in the following datasets to concatenate the context in LongBench. \u2022\n\nTREC (Li & Roth, 2002) is a classification dataset with fine-grained class label. \u2022\n\nTriviaQA (Zhong et al., 2021) is a classification dataset and involves messenger-like conversations with human-written summaries. \u2022\n\nSAMSum (Fabbri et al., 2019) reading comprehension dataset and consists of question-answer pairs annotated with evidence passages. Passkey\n\nThe randomly generated prompts of the passkey retrieval task is in the format of Fig.",
    "semanticcompress-18": "6. Appendix B Implementation Details\n\nIn this section, we provide details of our algorithm implementation. Our algorithm utilizes several mature open-source models. For graph representation, we make use of the sentence similarity models all-MiniLM-L6-v2 provided by the Sentence Transformer platform, which can be found at the following link: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2. For semantic compression, we employ the pre-trained model distilbart-cnn-12-6111Available at: https://huggingface.co/sshleifer/distilbart-cnn-12-6. In most of our experiments, we utilize Llama2-7B-chat-4k as the base large language model (Touvron et al., 2023). The experiments were conducted on a single A40 GPU with 48GB memory. Appendix C Complexity\n\nGiven a context with length , the origin complexity is . Considering the length limitations of the compression module, we assume it has a minimum input length and a maximum input length . We denote the compression ratio as . Our method utilizes a divide-and-conquer strategy, dividing the long text into chunks where the total length is represented as , and each chunk\u2019s length, , satisfies the condition . By , we can bound the complexity of the compression module\n\n\u2211 i = 1 k l i 2 \u2264 \u2211 i = 1 k \u03b3 2 2 = k \u200b \u03b3 2 2 \u2264 \u03b3 2 2 \u03b3 1 \u200b L . superscript subscript \ud835\udc56 1 \ud835\udc58 superscript subscript \ud835\udc59 \ud835\udc56 2 superscript subscript \ud835\udc56 1 \ud835\udc58 superscript subscript \ud835\udefe 2 2 \ud835\udc58 superscript subscript \ud835\udefe 2 2 superscript subscript \ud835\udefe 2 2 subscript \ud835\udefe 1 \ud835\udc3f \\sum_{i=1}^{k}l_{i}^{2}\\leq\\sum_{i=1}^{k}\\gamma_{2}^{2}=k\\gamma_{2}^{2}\\leq\\frac{\\gamma_{2}^{2}}{\\gamma_{1}}L. (1)\n\nThe complexity of inferring the compressed context is\n\n( \u2211 i = 1 k \u03b1 \u200b l i ) 2 = ( \u03b1 \u200b \u2211 i = 1 k l i ) 2 = \u03b1 2 \u200b L 2 . superscript superscript subscript \ud835\udc56 1 \ud835\udc58 \ud835\udefc subscript \ud835\udc59 \ud835\udc56 2 superscript \ud835\udefc superscript subscript \ud835\udc56 1 \ud835\udc58 subscript \ud835\udc59 \ud835\udc56 2 superscript \ud835\udefc 2 superscript \ud835\udc3f 2 (\\sum_{i=1}^{k}\\alpha l_{i})^{2}=(\\alpha\\sum_{i=1}^{k}l_{i})^{2}=\\alpha^{2}L^{2}. (2)\n\nThus the main complexity of our algorithms can be bounded by . The result suggests that our algorithm can reduce the computational complexity by a factor of the square of the compression ratio during the inference stage. The compression module exhibits linear growth and can be processed in parallel. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 14:14:55 2024 by LaTeXML"
}