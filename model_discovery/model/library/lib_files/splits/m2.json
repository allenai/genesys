{
    "m2-0": "# Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture \n\nDaniel Y. Fu ${ }^{1}$, Simran Arora*, ${ }^{*, 1}$ Jessica Grogan*,2, Isys Johnson*,2, Sabri Eyuboglu*,1,<br>Armin W. Thomas ${ }^{*, 3}$, Benjamin Spector ${ }^{1}$, Michael Poli ${ }^{1}$, Atri Rudra ${ }^{2}$, and Christopher R\u00e9 ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, Stanford University<br>${ }^{2}$ Department of Computer Science and Engineering, University at Buffalo, SUNY<br>${ }^{3}$ Department of Psychology, Stanford University<br>Contact Email: danfu@cs.stanford.edu\n\nOctober 18, 2023\n\n\n#### Abstract\n\nMachine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to $27 \\%$ fewer parameters, and achieves up to $9.1 \\times$ higher throughput at sequence length 4 K . On ImageNet, M2 outperforms ViT-b by $1 \\%$ in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE-showing for the first time that it may be possible to match Transformer quality without attention or MLPs. ## 1 Introduction\n\nMachine learning models in natural language processing and computer vision are being stretched to longer sequences and higher-dimensional representations to enable longer context and higher quality, respectively $[4,8,61,81]$. However, existing architectures exhibit time and space complexities that grow quadratically in sequence length and/or model dimension - which limits context length and makes scaling expensive. For example, attention and MLP in Transformers scale quadratically in sequence length and model dimension [13]. In this paper, we explore a natural question: can we find a performant architecture that is sub-quadratic in both sequence length and model dimension? [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-02.jpg?height=487&width=1624&top_left_y=229&top_left_x=244)\nFigure 1: Monarch matrices are a simple, expressive, and hardware-efficient class of sub-quadratic structured matrices. Monarch Mixer (M2) uses Monarch matrices to mix inputs first along the sequence dimension and then along the model dimension. See the Appendix for PyTorch implementation of an M2 layer. In our exploration, we seek a sub-quadratic primitive for both the sequence length and model dimension. Our framing takes inspiration from work such as MLP-mixer [72] and ConvMixer [56], which observed that many machine learning models operate by repeatedly mixing information along the sequence and model dimension axes, and used a single operator for both axes. Finding mixing operators that are expressive, sub-quadratic, and hardware-efficient is challenging. For example, the MLPs in MLP-mixer and convolutions in ConvMixer are expressive, but they both scale quadratically in their input dimension [56, 72]. Several recent studies have proposed sub-quadratic sequence mixing with long convolutions or state space models [25, 63, 75]-both computed using the FFT - but these models have poor FLOP utilization (3-5\\% [26]) and maintain quadratic scaling in model dimension. Meanwhile, there has been promising work in sparsifying dense MLP layers without losing quality, but some of the models can actually be slower than their dense counterparts, due to low hardware utilization $[5,6,12,24,33]$. We turn to an expressive class of sub-quadratic structured matrices called Monarch matrices [12] (Figure 1 left) to propose Monarch Mixer (M2). Monarch matrices are a family of structured matrices that generalize the fast Fourier transform (FFT) and have been shown to capture a wide class of linear transforms including Hadamard transforms, Toeplitz matrices [30], AFDF matrices [55], and convolutions. They are parameterized as the products of block-diagonal matrices, called monarch factors, interleaved with permutation. Their compute scales sub-quadratically: setting the number of factors to $p$ results in computational complexity of $O\\left(p N^{(p+1) / p}\\right)$ in input length $N$, allowing the complexity to interpolate between $O(N \\log N)$ at $p=\\log N$ and $O\\left(N^{3 / 2}\\right)$ at $p=2 .{ }^{1}$\n\nM2 uses Monarch matrices to mix information along the sequence and model dimension axes. It is both simple to implement and hardware-efficient: the block-diagonal Monarch factors can be computed efficiently on modern hardware using GEMMs (generalized matrix multiply algorithms). Our proof-of-concept implementation of an M2 layer, written in less than 40 lines of pure PyTorch (including imports), relies only on matrix multiplication, transpose, reshape, and elementwise products (see pseudocode in Figure 1 middle) and achieves $25.6 \\%$ FLOP utilization ${ }^{2}$ for inputs of size 64 K on an A100 GPU. On newer architectures such as the RTX 4090, a simple CUDA implementation achieves $41.4 \\%$ FLOP utilization at the same size. [^1]Non-Causal Settings As a first proof of concept of M2, we evaluate how it compares to Transformers in terms of speed and quality in non-causal settings such as BERT-style masked language modeling [19] and ImageNet classification. We introduce M2-BERT, which replaces the attention blocks in BERT with bidirectional gated convolutions implemented using Monarch matrices and replaces the dense matrices in the MLP with Monarch matrices. M2-BERT reduces parameter count but maintains quality - matching BERT-base and BERT-large in downstream GLUE quality with $27 \\%$ and $24 \\%$ fewer parameters, respectively. Sub-quadratic scaling in sequence length enables high throughput at longer sequences - up to $9.1 \\times$ higher throughput at sequence length 4 K than HuggingFace BERT, and $3.1 \\times$ higher throughput at sequence length 8 K than BERT optimized with FlashAttention [13]. For image classification, we adapt HyenaViT-b [63], an attention-free vision transformer based on gated convolutions. We replace the convolution operation with M2 primitives and replace the MLP layers with an M2 block as well. These changes reduce the parameter count compared to a ViT-b [20] model with the same model width and depth by a factor of 2. Surprisingly, despite this parameter reduction, we find that M2 slightly outperforms ViT-b and HyenaViT-b baselines, achieving $1 \\%$ higher accuracy on ImageNet [16]. Causal Settings Causal settings such as GPT-style [64] auto-regressive language modeling present a technical challenge: masking out the upper triangular elements in an attention matrix (or equivalent structure) introduces a quadratic bottleneck. To alleviate this quadratic bottleneck with Monarch matrices, we develop new theory to characterize which parameterizations of Monarch matrices maintain causality. To do so, we take a view of $p$-order Monarch matrix multiplication as $p$-variate polynomial evaluation and interpolation (e.g., $p=2$ factors corresponds to bivariate polynomials, Figure 2 left). Using this view, we show that the M2 convolution shown in Figure 1 (middle) can be viewed as manipulation of modular polynomial multiplication. This result allows us to develop conditions (Theorem 3) under which M2 is causal. We can use this causal parameterization to outperform GPT-style language models on causal language modeling by 0.2 PPL points on the PILE at model size 360M-without using either attention or MLP blocks. Summary Overall, our results present a potential path to building machine learning models with sub-quadratic primitives. We hope our work can serve as a starting point to explore models that are more efficient in both sequence length and model dimension. ## 2 Preliminaries\n\nIn this section, we provide some background on the key components behind the cost of operations on GPUs, and then discuss the scaling characteristics of some common primitives used to mix information across the sequence dimension and model dimension in modern machine learning models. GPU Accelerator Cost Model We provide a brief discussion of relevant factors affecting runtime performance of deep learning operations on GPUs. Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound [42]. In compute-bound operations, the time accessing GPU memory is relatively small compared to the time spent doing arithmetic operations. Typical examples are matrix multiply with large inner dimension, and short convolution kernels with a large number of channels. The speed of these operations is determined by the FLOP/s available on compute units, and the number of FLOPs necessary to complete the operation. In our paper, we exploit fast matrix multiply\nunits such as tensor cores. On the A100, tensor cores can achieve $312 \\mathrm{TFLOP} / \\mathrm{s}$ in half-precision matrix multiply operations, while non-matrix multiply operations are limited to $19 \\mathrm{TFLOP} / \\mathrm{s}$ [58]. This trend began with tensor cores in the V100 [57], and is continuing into the next-generation H100 chips [59]. In memory-bound operations, the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most elementwise operations (e.g., activation, dropout) and reductions (e.g., sum, softmax, batch norm, layer norm). The runtime of memory-bound operations is determined by the memory bandwidth of different layers of the memory hierarchy. GPU memory is large but relatively slow-up to 80 GB on A100, but with bandwidth of $2 \\mathrm{~TB} / \\mathrm{s}$ [58]. Higher levels of the memory hierarchy such as caches are much smaller ( 20 MB ) but an order of magnitude faster ( $19 \\mathrm{~TB} / \\mathrm{s}$ ). Common Mixer Primitives To help contextualize our work, we provide scaling and hardware utilization characteristics for a few common operations that are used to mix information in machine learning models, summarized in Table 1. Transformers [73] use attention to mix information across the sequence dimension, and MLP blocks to mix information across the model dimension. Both of these blocks scale quadratically in input length. MLP layers are compute-bound, so they have high FLOP utilization out\n\nTable 1: FLOP cost and utilization of various mixer layers, input dimension 64 K on an RTX 4090. | Layer | FLOP Cost | Util |\n| ---: | :---: | :---: |\n| MLP | $N^{2}$ | $95.5 \\%$ |\n| FlashAttn | $N^{2}$ | $24.0 \\%$ |\n| FFT | $N \\log N$ | $3.0 \\%$ |\n| M2 Conv | $N^{3 / 2}$ | $41.4 \\%$ |\n\nof the box. Attention blocks are memory-bound, so even the most optimized implementations such as FlashAttention [13] have relatively lower FLOP utilization. Recent work has made progress towards attention-free models by replacing attention layers with long convolution layers, interleaved with elementwise gating [25, 26, 34, 52, 63, 66-68]. These layers are computed using FFT operations using the FFT convolution theorem:\n\n$$\ny=\\mathbf{K} * \\mathbf{X}=F F T^{-1}(F F T(\\mathbf{X}) * F F T(\\mathbf{K}))\n$$\n\nWhile the FFT scales asymptotically well in $O(N \\log N)$, it is often memory-bound and thus has low FLOP utilization. In our work, we aim to construct a mixer that has both sub-quadratic scaling and high FLOP utilization. ## 3 Monarch Mixer\n\nIn this section, we recall Monarch matrices, introduce how M2 uses Monarch matrices to mix along the sequence and model dimensions, and benchmark a M2 convolution in terms of hardware utilization. ### 3.1 Monarch Matrices\n\nMonarch matrices [12] are a sub-quadratic class of structured matrices that are hardware-efficient and expressive. They can represent many linear transforms, including convolutions, Toeplitz-like transforms, low-displacement rank transforms, and orthogonal polynomials. Directly implementing these different structured transforms on GPUs as dense matrices can be inefficient. In contrast,\n\nTable 2: FLOP cost and utilization of M2 compared to dense MLP at different input sizes $N$, with block size $\\sqrt{N}$, on an A100 and RTX 4090. | $N$ | 4 K | 16 K | 64 K | 256 K |\n| ---: | :---: | :---: | :---: | :---: |\n| Dense Matmul TFLOP Cost | 0.025 | 0.412 | 6.60 | 106.0 |\n| M2 TFLOP Cost | 0.002 | 0.013 | 0.103 | 0.824 |\n| Dense FLOP Utilization (A100) | $63.0 \\%$ | $78.0 \\%$ | $80.0 \\%$ | OOM |\n| M2 FLOP Utilization (A100) | $4.78 \\%$ | $12.7 \\%$ | $25.6 \\%$ | $42.8 \\%$ |\n| Wall-Clock Speedup (A100) | $1.2 \\times$ | $5.1 \\times$ | $20.6 \\times$ | $>55.0 \\times$ |\n| Dense FLOP Utilization (4090) | $74.6 \\%$ | $96.7 \\%$ | $98.0 \\%$ | OOM |\n| M2 FLOP Utilization (4090) | $11.1 \\%$ | $32.1 \\%$ | $41.4 \\%$ | $53.7 \\%$ |\n| Wall-Clock Speedup (4090) | $2.2 \\times$ | $10.5 \\times$ | $27.0 \\times$ | $>69.1 \\times$ |\n\ntheir Monarch decompositions can be computed by interleaving matrix multiplications with tensor permutations. A Monarch matrix $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}$ of order $p$ is defined by the following:\n\n$$\n\\mathbf{M}=\\left(\\prod_{i=1}^{p} \\mathbf{P}_{i} \\mathbf{B}_{i}\\right) \\mathbf{P}_{0}\n$$\n\nwhere each $\\mathbf{P}_{i}$ is related to the 'base $\\sqrt[p]{N}$ ' variant of the bit-reversal permutation, and $\\mathbf{B}_{i}$ is a block-diagonal matrix with block size $b$. Setting $b=\\sqrt[p]{N}$ achieves sub-quadratic compute cost. For example, for $p=2, b=\\sqrt{N}$, Monarch matrices require $O\\left(N^{3 / 2}\\right)$ compute in sequence length $N$. In this paper, we use Monarch matrices to construct architectures that are sub-quadratic in both sequence length $N$ and model dimension $d$. We will often parameterize order- 2 Monarch matrices, written as $\\mathbf{M}=\\mathbf{P L P R P}$, where $\\mathbf{L}$ and $\\mathbf{R}$ are block-diagonal matrices (for \"left\" and \"right\"), and $\\mathbf{P}=\\mathbf{P}_{2}=\\mathbf{P}_{1}=\\mathbf{P}_{0}$ is a permutation that reshapes the input to 2 D , transposes it, and flattens it to 1D. A common case is to set $\\mathbf{L}=\\mathbf{R}=\\left(\\mathbf{I}_{\\sqrt{N}} \\otimes \\mathbf{F}_{\\sqrt{N}}\\right)$, where $\\mathbf{F}_{\\sqrt{N}}$ is a $\\sqrt{N}$ DFT matrix, and $\\otimes$ is the Kronecker product. ### 3.2 Monarch Mixer Architecture\n\nWe describe how Monarch Mixer uses Monarch matrices and elementwise operations to construct sub-quadratic architectures (Figure 1 middle). We take a mixer view of model architectures, where each layer is a sequence of mixing operations across the sequence and the model dimension axes. Each layer takes as input a sequence of embeddings $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$, and outputs a sequence $\\mathbf{Y} \\in \\mathbb{R}^{N \\times d}$, where $N$ is the sequence length, and $d$ is the model dimension. For simplicity, we show the order- 2 case here, though we can use higher-order blocks to scale to longer sequences and larger model dimensions. Let $\\mathbf{M}_{1}, \\mathbf{M}_{2} \\in \\mathbb{R}^{N \\times N}$ and $\\mathbf{M}_{3}, \\mathbf{M}_{4} \\in \\mathbb{R}^{d \\times d}$ be order-2 Monarch matrices, let $\\mathbf{K}_{1} \\in \\mathbb{R}^{N \\times d}$, let $\\sigma$ be an optional point-wise non-linearity (e.g. ReLU), and let $\\odot$ be elementwise multiplication. M2 uses Monarch matrices to construct expressive architectures. For example, a convolutional block with a sparse MLP can be expressed as follows:\n\n1. Mix along sequence axis:\n\n$$\n\\tilde{\\mathbf{X}}=\\mathbf{M}_{2}\\left(\\mathbf{K}_{1} \\odot \\mathbf{M}_{1} \\mathbf{X}\\right)\n$$\n\n2. Mix along embedding axis:\n\n$$\n\\mathbf{Y}^{\\top}=\\mathbf{M}_{4} \\sigma\\left(\\mathbf{M}_{3} \\tilde{\\mathbf{X}}^{\\top}\\right)\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-06.jpg?height=350&width=1411&top_left_y=248&top_left_x=368)\n\nFigure 2: Monarch multiplication can be interpreted as polynomial evaluation and interpolation. We derive sufficient conditions on the polynomial formulation of Monarch matrices for M2 to be causal. When $\\mathbf{M}_{1}$ is set to the DFT and $\\mathbf{M}_{2}$ is set to the inverse DFT, Equation 2 exactly corresponds to a convolution with kernel $\\mathbf{K}_{1}$ parameterized in frequency space. Equation 3 corresponds to an MLP with the dense matrices replaced by Monarch matrices. More expressive layers are also easily expressible; for example, replacing Equation 2 with $\\mathbf{V} \\odot \\mathbf{M}_{2}\\left(\\mathbf{K}_{1} \\odot \\mathbf{M}_{1}(\\mathbf{Q} \\odot \\mathbf{K})\\right.$ ), where $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ are linear projections of $\\mathbf{X}$, reproduces a gated convolution block, as in [25, 26, 63]. The basic M2 layer is simple to implement; pseudocode is shown in Figure 1 (middle), and the Appendix gives an efficient implementation of M2 in under 40 lines of pure PyTorch (including imports). The convolution case with Monarch matrices fixed to DFT and inverse DFT matrices also admits implementations based on FFT algorithms [9]. ### 3.3 Architecture Benchmarks\n\nWe benchmark the efficiency of the $\\mathbf{M}(\\mathbf{K} \\odot \\mathbf{M X}$ ) convolution operator (Equation 2) implemented in a simple CUDA kernel (calling standard cuBLAS sub-routines [60]), as the dimension $N$ increases. Equation 3 scales similarly, as dimension $d$ increases. We keep the block size $b$ fixed to $\\sqrt{N}$. Table 2 shows the FLOP cost and utilization of a M2 operator as a function of the input size on an A100 as well as on an RTX 4090. On the A100, the operator is more dominated by the data movement costs of the permutation operations (see the Appendix for a roofline analysis). For longer inputs, the sub-quadratic scaling allows Monarch Mixer to outperform dense matrix multiplication. On the RTX 4090, which has a larger and faster L2 cache than the A100, we can manually optimize an implementation to amortize data movement costs. ## 4 Theoretical Analysis: M2 as Polynomial Multiplication\n\nIn this section, we develop theory to make the M2 layer causal in the input $\\mathbf{X}$-e.g., ensure that an output $Y_{i}$ of the M2 should only depend on $X_{1}, \\ldots, X_{i}$. Our approach involves interpreting Monarch matrix multiplication as multivariate polynomial evaluation and interpolation. We then show that an M2 convolution is equivalent to modular polynomial manipulation in a univariate basis. The challenge is controlling the degrees of the resulting univariate polynomials, to prevent \"underflow\" under modular multiplication (see Figure 2 for an overview). Our key result is deriving sufficient conditions on the degrees of the bivariate polynomials defining the Monarch factors to prevent such underflow. We focus on the bivariate case (order $p=2$ ) in the body, and give the general multivariate case in the Appendix. We present proof sketches in the main body, and leave proofs and additional results for the Appendix. Monarch Multiplication as Polynomial Evaluation First, we show that order-2 Monarch matrix-vector multiplication $\\mathbf{M} \\cdot \\mathbf{u}$ is equivalent to bivariate polynomial evaluation. Fix a Monarch matrix $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}=\\mathbf{P L P R P}$, for two block-diagonal matrices $\\mathbf{L}$ and $\\mathbf{R}$ with blocks of size $b=\\sqrt{N}$. We can interpret Monarch matrices as bivariate polynomial evaluation by setting $A=\\left\\{\\omega_{0}, \\ldots, \\omega_{b-1}\\right\\}$ as a set of evaluation points (e.g., the $b$ th roots of unity), and letting $\\left\\{\\ell_{0}(X, Y), \\ldots, \\ell_{b-1}(X, Y)\\right\\},\\left\\{r_{0}(Y), \\ldots, r_{N-1}(Y)\\right\\}$ be sets of basis polynomials with individual degrees of $X, Y$ being $<\\sqrt{N}$. The values of $\\left\\{\\ell_{0}(X, Y), \\ldots, \\ell_{b-1}(X, Y)\\right\\}$ evaluated on $A^{2}$ determine the entries of $\\mathbf{L}$, and the values of $\\left\\{r_{0}(Y), \\ldots, r_{N-1}(Y)\\right\\}$ evaluated on $A$ determine the entries of $\\mathbf{R}$. We give the mapping from $\\ell, r$, and $A$ to $\\mathbf{L}$ and $\\mathbf{R}$ in the Appendix. Then, matrix-vector multiplication between $\\mathbf{M}$ and a vector $\\mathbf{u}$ is equivalent to polynomial evaluation of the basis functions $\\ell, r$ on the evaluation points $A^{2}$ :\n\nTheorem 1. Let $m(j)=j \\bmod \\sqrt{N}$. For any vector $\\mathbf{u} \\in \\mathbb{R}^{N}$, Mu is a bivariate polynomial $u(X, Y)$ evaluated at $A^{2}$, with $u(X, Y)=\\sum_{j=0}^{N-1} u_{j} f_{j}(X, Y)$, where $f_{j}(X, Y)=\\ell_{m(j)}(X, Y) r_{j}(Y)$. Monarch Inverse as Polynomial Interpolation Next, we exploit the fact that Monarch inverse multiplication $\\mathbf{M}^{-1} \\cdot \\mathbf{u}$ is equivalent to polynomial interpolation in the basis polynomials of $\\mathbf{M}$. Theorem 2. Let $\\mathbf{M}_{0}, \\mathbf{M}_{1}, \\mathbf{M}_{2}$ be Monarch matrices, and let $A$ be the set of $\\sqrt{N}$ roots of unity. Then, the operation\n\n$$\n\\mathbf{f}=\\mathbf{M}_{0}^{-1}\\left(\\left(\\mathbf{M}_{1} \\mathbf{k}\\right) \\odot\\left(\\mathbf{M}_{2} \\mathbf{u}\\right)\\right)\n$$\n\nis equivalent to representing the polynomial\n\n$$\nh(X, Y)=k(X, Y) u(X, Y) \\quad \\bmod \\left(X^{\\sqrt{N}}-1, Y^{\\sqrt{N}}-1\\right)\n$$\n\nin terms of the basis polynomials $\\ell, r$ corresponding to $\\mathbf{M}_{0}$, and where $k(X, Y)$ and $u(X, Y)$ are the polynomials corresponding to $\\mathbf{M}_{1} \\mathbf{k}$ and $\\mathbf{M}_{2} \\mathbf{u}$, respectively. The above follows from Theorem 1 and the fact that Monarch matrix-vector multiplication with an inverse Monarch matrix is equivalent to polynomial interpolation in a given basis. The mod part comes from the fact that $A$ is the set of roots of the polynomial $Z^{\\sqrt{N}}-1$. Causal Monarch Maps Now, we give a class of Monarch matrices from which we can build a causal map. First, we define a polynomial with minimum degree $j$ :\n\nDefinition 1. A polynomial of minimum degree $j$ (and maximum degree $N-1$ ) is defined as $\\bar{q}_{j}(Z)=\\sum_{a=j}^{N-1} \\bar{q}_{j}[a] Z^{a}$. To ensure causality, we first convert the bivariate polynomial basis into a univariate basis, and then we expand the degree of the univariate polynomial. The resulting univariate polynomial multiplication is naturally causal (exploiting similar properties as the causal FFT convolution). We use the Kronecker substitution $\\left(X \\leftarrow Z, Y \\leftarrow Z^{\\sqrt{N}}\\right.$ ) to convert the bivariate polynomial basis into a univariate basis:\n\n$$\nq_{j}(Z)=\\ell_{m(j)}(Z) r_{j}\\left(Z^{\\sqrt{N}}\\right)\n$$\n\nwhere $m(j)$ is defined as in Theorem 1 . Then, the following class of Monarch matrices (with the conversion to univariate polynomial basis as above) forms a causal map:\n\nTheorem 3. Let $\\mathbf{u}, \\mathbf{k} \\in \\mathbb{R}^{n}$, where $n<N / 2$. Let $m(j)$ be as in Theorem 1, and $k(j)=\\lfloor j / \\sqrt{N}\\rfloor$. Then define the basis polynomials $\\ell_{m(j)}$ to have minimum degree $m(j)$, basis polynomials $r_{j}$ to have minimum degree $k(j)$, and all polynomials $q_{j}(Z)$ to have maximum degree $<N / 2$ for all $j<N / 2$ and for $N / 2 \\leq j<N$ have maximum degree $N-1$. Let $\\mathbf{M}_{N}$ be defined by such basis polynomials via (5) where the evaluation points are now the Nth roots of unity. Then, we have that\n\n$$\n\\mathbf{u} \\mapsto\\left(\\mathbf{M}_{N}^{-1}\\left(\\mathbf{M}_{N}\\left(\\mathbf{k}, \\mathbf{0}_{N-n}\\right) \\odot \\mathbf{M}_{N}\\left(\\mathbf{u}, \\mathbf{0}_{N-n}\\right)\\right)\\right)[0: n-1]\n$$\n\ngives a causal map in $\\mathbf{u}$. Theorem 3 gives a causal map that can be computed entirely using Monarch matrices - enforcing causality with sub-quadratic scaling. The main technical ingredient in proving the above result is that the product $q_{j}(Z) q_{j^{\\prime}}(Z)$ can be written as a linear combination of $q_{a}(Z)$ for $j+j^{\\prime} \\leq a<N$ (this uses the above specified properties on the minimum and maximum degrees of $q_{j}(Z)$ ). This in turn implies that the term $k_{j^{\\prime}} u_{j} q_{j}(Z) q_{j^{\\prime}}(Z)$ only contributes to the coefficients of \"higher order\" basis polynomials $q_{a}(Z)$ for $a \\geq j+j^{\\prime}$ in the product $k(Z) u(Z)$, which is needed for causality. Figure 2 gives an example of restricted polynomials generating a causal map. ## 5 Experiments\n\nWe compare Monarch Mixer to Transformers on three tasks where Transformers have been dominant: BERTstyle non-causal masked language modeling, ViT-style image classification, and GPT-style causal language modeling. In each, we show that we can match Transformers in quality using neither attention nor MLPs. We additionally evaluate wall-clock speedups against strong Transformer baselines in the BERT setting. Additional experiments on speech and alternative architectures are given in Appendix B, and experimental details are given in Appendix C. ### 5.1 Non-Causal Language Modeling\n\nWe introduce M2-BERT, an M2-based architecture for non-causal language modeling. M2-BERT acts as a dropin replacement for BERT-style language models [19], which are a workhorse application of the Transformer architec-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-08.jpg?height=583&width=592&top_left_y=1145&top_left_x=1252)\n\nFigure 3: M2-BERT uses Monarch matrices to create a bidirectional gated long convolution in the sequence mixer, and uses Monarch matrices to replace the linear layers in the dimension mixer. ture $[1,37,38,43,46,47,50,54,82,86]$. We train M2BERT using masked language modeling over C 4 [65] with the bert-base-uncased tokenizer. M2-BERT starts with a Transformer backbone and replaces the attention and MLPs with M2 layers, shown in Figure 3. In the sequence mixer, we replace attention with bidirectional gated convolutions with a residual convolution (Figure 3 left). To recover convolutions, we set the Monarch matrices to DFT and inverse DFT matrices. Following [25, 63], we also add short depthwise convolutions after the projections. In the dimension mixer, we replace the two dense matrices in MLPs with learned block-diagonal matrices (Monarch matrix of order 1, $b=4$ ). We pretrain two M2-BERT-base models, at 80M and 110M, and two M2-BERT-large models, at 260 M and 341 M . These are equivalent to BERT-base and BERT-large, respectively. Table 3: Average GLUE Score for M2-BERT-base compared to BERT-base [18], along with change in parameters and GLUE score. | Model | GLUE Score | $\\Delta$ Params | $\\Delta$ GLUE Score |\n| ---: | :---: | :---: | :---: |\n| BERT-base (110M) | 79.6 | $-0 \\%$ | +0.0 |\n| M2-BERT-base (80M) | 79.9 | $-27 \\%$ | +0.3 |\n| M2-BERT-base (110M) | $\\mathbf{8 0 . 9}$ | $-0 \\%$ | +1.3 |\n\nTable 4: Average GLUE Score for M2-BERT-large compared to BERT-large [18], along with change in parameters and GLUE score. | Model | GLUE Score | $\\Delta$ Params | $\\Delta$ GLUE Score |\n| ---: | :---: | :---: | :---: |\n| BERT-large (340M) | 82.1 | $-0 \\%$ | +0.0 |\n| M2-BERT-large (260M) | 82.2 | $-24 \\%$ | +0.1 |\n| M2-BERT-large (341M) | $\\mathbf{8 2 . 8}$ | $+0.2 \\%$ | +0.7 |\n\nDownstream GLUE Scores First, we evaluate M2-BERT models on downstream fine-tuning compared to BERT-base and BERT-large from [18]. We take the pretrained models and fine-tune them on BERT, following the procedure in [36]. Table 3 shows performance for BERT-base equivalent models, and Table 4 shows performance for BERT-large equivalent models. M2-BERT-base can match BERT-base in GLUE quality with $27 \\%$ fewer parameters-or outperform BERT-base in quality by 1.3 points when parameter matched. M2-BERT-large matches BERT-large with $24 \\%$ fewer parameters, and outperforms by 0.7 points when parameter matched. GPU Throughput by Sequence Length Next, we evaluate throughput of M2-BERT models by sequence length, compared to HuggingFace implementations of BERT, as well as optimized implementations of BERT running FlashAttention [13]. Table 5 shows forward throughput for BERT-base equivalent models, and the appendix shows throughput for BERT-large (where the performance trends are similar). Inference times are reported in tokens $/ \\mathrm{ms}$ on an A100-40GB GPU. M2-BERT-base achieves higher throughput than even highly-optimized BERT models, and up to $9.1 \\times$ faster throughput than a standard HuggingFace implementation at sequence length 4 K . CPU Inference Latency Finally, we report CPU inference latency for M2-BERT-base (80M) compared to BERT-base, running direct PyTorch implementations for both. In short sequences, the impacts of data locality still dominate the FLOP reduction, and operations such as filter generation (which are not present in BERT) pay a higher cost. Starting at sequences 1 K and longer, M2-BERT-base starts to have speedup over BERT-base, up to $6.5 \\times$ at sequence length 8 K . We believe further optimization and applying IO-aware principles can further improve CPU performance. ### 5.2 Image Classification\n\nTo validate that our methods generalize to images as well as language for non-causal modeling, we next evaluate M2 on image classification. We compare M2 to ViT-style models and recent work, HyenaViT-b [63], which uses gated long convolutions to replace the attention layers in ViT-b. In our work, M2-ViT builds off HyenaViT-b and replaces the long convolutions with the M2 operator\n\nTable 5: Throughput in tokens/ms by context length for M2-BERT-base (80M) compared to BERT-base. | Model | $\\mathbf{5 1 2}$ | $\\mathbf{1 0 2 4}$ | $\\mathbf{2 0 4 8}$ | $\\mathbf{4 0 9 6}$ | $\\mathbf{8 1 9 2}$ |\n| ---: | :---: | :---: | :---: | :---: | :---: |\n| HF BERT-base (110M) | 206.1 | 130.8 | 71.3 | 39.0 | OOM |\n| FlashAttention BERT-base (110M) | 367.4 | 350.1 | 257.2 | 179.1 | 102.4 |\n| M2-BERT-base (80M) | $\\mathbf{3 8 6 .",
    "m2-1": "3}$ | $\\mathbf{3 8 0 . 7}$ | $\\mathbf{3 7 8 . 9}$ | $\\mathbf{3 5 3 . 9}$ | $\\mathbf{3 2 0 . 1}$ |\n| M2 Speedup over HF BERT-base (110M) | $1.9 \\times$ | $2.9 \\times$ | $5.2 \\times$ | $9.1 \\times$ | - |\n\nTable 6: CPU inference latency in milliseconds with a batch size of 1 at varied input sequence lengths. Measurements averaged over 10 examples on a $48 \\mathrm{vCPU}, 96 \\mathrm{~GB}$ RAM instance from the GCP n2-standard-48 series, which runs Intel Cascade Lake processors. This is based on the protocol in $[27]$. | Model | 512 | 1024 | 2048 | 4096 | 8192 |\n| ---: | :---: | :---: | :---: | :---: | :---: |\n| BERT-base (110M) | $\\mathbf{1 8 2}$ | 389 | 918 | 2660 | 11820 |\n| M2-BERT-base (80M) | 289 | $\\mathbf{3 6 1}$ | $\\mathbf{6 5 1}$ | $\\mathbf{9 4 8}$ | $\\mathbf{1 8 2 0}$ |\n| Speedup | $0.6 \\times$ | $1.1 \\times$ | $1.4 \\times$ | $2.8 \\times$ | $6.5 \\times$ |\n\nin Equation 2 (again setting the Monarch matrices to the DFT and inverse DFT). We replace the MLP blocks in HyenaViT-b with block-diagonal matrices, similarly to M2-BERT. Appendix B additionally compares M2 to the Swin-family of architectures [48, 49]. Table 7 shows the performance of Monarch Mixer against ViT-b, HyenaViT-b, and ViT-bMonarch (which replaces the MLP blocks of standard ViT-b with Monarch matrices) on ImageNet-1k. Monarch Mixer outperforms the other models with only half the parameters of the original ViT-s model. Surprisingly, Monarch Mixer also outperforms ResNet-152, with fewer parameters - even though the latter was explicitly designed for ImageNet performance. ### 5.3 Causal Language Modeling\n\nGPT-style causal language modeling is a critical application for Transformers [4, 29, 41]. We introduce M2-GPT, a M2-based architecture for causal language modeling. For the sequence mixer, M2-GPT combines the convolutional filter from Hyena [63], the state-of-the-art attention-free language model, with parameter sharing across multiple heads from H 3 [25]. We use the causal parameterization of Equation 2 to replace the FFT in these architectures, and we remove the MLP layers entirely. The resulting architecture is entirely attention- and MLP-free. We pretrain M2-GPT on the PILE, a standard dataset for causal language modeling. Following prior work $[26,63]$, we train models at two model sizes, with varying amounts of training data decaying the learning rate appropriately for each experiment. Table 8 shows the results. Even though our model is attention- and MLP-free, it outperforms both Transformers and Hyena in perplexity on pretraining. These results suggest that radically different architectures than Transformers may be performant on causal language modeling.",
    "m2-2": "Table 7: Accuracy on ImageNet-1k. ResNet-152 provided for reference. | Model | Top-1\\% | Top-5\\% | Description |\n| ---: | :---: | :---: | :---: |\n| ResNet-152 (60M) | 78.6 | 94.3 | ConvNet, MLP |\n| ViT-b (87M) | 78.5 | 93.6 | Attention, MLP |\n| ViT-b + Monarch (33M) | 78.9 | 94.2 | Attention, MLP-Free |\n| HyenaViT-b (88M) | 78.5 | 93.6 | Attention-Free, MLP |\n| M2-ViT-b (45M) | $\\mathbf{7 9 . 5}$ | $\\mathbf{9 4 . 5}$ | Attention-Free, MLP-Free |\n\nTable 8: Perplexity on the PILE when trained for different amounts of tokens. | Model | 5 B | 10 B | 15 B | Description |\n| ---: | :---: | :---: | :---: | :---: |\n| Transformer (125M) | 13.3 | 11.9 | 11.2 | Attention, MLP |\n| Hyena (155M) | 13.1 | 11.8 | 11.1 | Attention-Free, MLP |\n| M2-GPT (145M) | $\\mathbf{1 2 . 9}$ | $\\mathbf{1 1 . 6}$ | $\\mathbf{1 0 . 9}$ | Attention-Free, MLP-Free |\n| Transformer (355M) | 11.4 | 9.8 | 9.1 | Attention, MLP |\n| Hyena (360M) | 11.3 | 9.8 | 9.2 | Attention-Free, MLP |\n| M2-GPT (360M) | $\\mathbf{1 1 .",
    "m2-3": "0}$ | $\\mathbf{9 . 6}$ | $\\mathbf{9 . 0}$ | Attention-Free, MLP-Free |\n\n## 6 Related Work\n\nLong Convolutions Recent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [26,63, 66-68]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices. Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [79] or orthogonal polynomial bases [32]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases. Optimization of deep learning primitives There is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute. Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory $[44,76]$. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply-accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [62] have added kernel fusion capabilities, although the very best performance usually still arises from\nhandwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [53]. Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [13] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques. Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work $[5,9,12,14,15,17,24,88]$. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 21, 22, 40, 51]. Others use low-rank approximations of the attention matrix [11, 77, 88] or kernel methods instead [7, 39]. A subset use a combination of these techniques, such as $[6,71]$. Finally, a third category of methods $[25,63]$ aim to replace attention entirely, relying on state-space models [31]. ## 7 Discussion and Conclusion\n\nWe explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling. While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.",
    "m2-4": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed\nin this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government. JG and AR's work is supported by NSF grant\\# CCF-2247014. IJ's work is supported by an NSF Graduate Fellowship. ## References\n\n[1] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019. [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.",
    "m2-5": "arXiv preprint arXiv:2004.05150, 2020. [3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet-1k.",
    "m2-6": "arXiv preprint arXiv:2205.01580, 2022. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [5] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R\u00e9. Pixelated butterfly: Simple and efficient sparse training for neural network models.",
    "m2-7": "2021. [6] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "m2-8": "arXiv preprint arXiv:2009.14794, 2020. [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.",
    "m2-9": "arXiv preprint arXiv:2204.02311, 2022. [9] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702-703, 2020. [11] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing.",
    "m2-10": "Advances in neural information processing systems, 33:4271-4282, 2020. [12] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training.",
    "m2-11": "In International Conference on Machine Learning. PMLR, 2022 . [13] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [14] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations, 2020. [15] Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps, 2021. [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. [17] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019. [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018 . [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In arXiv:1810.04805, 2019. [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.",
    "m2-12": "arXiv preprint arXiv:2010.11929, 2020. [21] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages $5547-5569$. PMLR, 2022. [22] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022. [23] Wikimedia Foundation. Wikimedia downloads. [24] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [25] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. International Conference on Learning Representations, 2023. [26] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning, 2023. [27] Morgan Funtowicz. Scaling up bert-like model inference on modern cpu - part 1, 2021. [28] Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day. arXiv:2212.14034v1, 2022. [29] Google. Bard, https://bard.google.com/.",
    "m2-13": "2023. [30] Robert M Gray et al. Toeplitz and circulant matrices: A review.",
    "m2-14": "Foundations and Trends ${ }^{R}$ in Communications and Information Theory, 2(3):155-239, 2006. [31] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [32] Nicholas Hale and Alex Townsend. An algorithm for the convolution of legendre series.",
    "m2-15": "SIAM Journal on Scientific Computing, 36(3):A1207-A1220, 2014. [33] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.",
    "m2-16": "arXiv preprint arXiv:1510.00149, 2015 . [34] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [35] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty.",
    "m2-17": "arXiv preprint arXiv:1912.02781, 2019. [36] Peter Izsak, Moshe Berchansky, and Omer Levy. How to train bert with an academic budget. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10644-10652, 2021. [37] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018-8025, 2020. [38] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans.",
    "m2-18": "Transactions of the Association for Computational Linguistics, 8:64-77, 2020. [39] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "m2-19": "In International Conference on Machine Learning, pages 5156-5165. PMLR, 2020. [40] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "m2-20": "arXiv preprint arXiv:2001.04451, 2020. [41] Jan Koco\u0144, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd\u0142o, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. Chatgpt: Jack of all trades, master of none.",
    "m2-21": "arXiv preprint arXiv:2302.10724, 2023. [42] Elias Konstantinidis and Yiannis Cotronis. A practical performance model for compute and memory bound gpu kernels. In 2015 23rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, pages 651-658.",
    "m2-22": "IEEE, 2015. [43] MV Koroteev. Bert: a review of applications in natural language processing and understanding.",
    "m2-23": "arXiv preprint arXiv:2103.11943, 2021. [44] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [45] Lagrange polynomial. Lagrange polynomial - Wikipedia, the free encyclopedia, 2005. https: //en.wikipedia.org/wiki/Lagrange_polynomial. [46] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240, 2020. [47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [48] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022 . [49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows.",
    "m2-24": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [50] Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing Xiang. Universal text representation from bert: An empirical study.",
    "m2-25": "arXiv preprint arXiv:1910.07973, 2019. [51] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441-2453, 2021. [52] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention.",
    "m2-26": "arXiv preprint arXiv:2209.10655, 2022. [53] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax.",
    "m2-27": "arXiv preprint arXiv:1805.02867, 2018. [54] Derek Miller. Leveraging bert for extractive text summarization on lectures.",
    "m2-28": "arXiv preprint arXiv:1906.04165, 2019. [55] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. Acdc: A structured efficient linear layer.",
    "m2-29": "arXiv preprint arXiv:1511.05946, 2015. [56] Dianwen Ng, Yunqi Chen, Biao Tian, Qiang Fu, and Eng Siong Chng. Convmixer: Feature interactive convolution with curriculum learning for small footprint and noisy far-field keyword spotting. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3603-3607. IEEE, 2022. [57] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017. [58] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020. [59] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022. [60] NVIDIA. cuBLAS, 2023. [61] OpenAI. Gpt-4 technical report, 2023. [62] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [63] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. International Conference on Machine Learning, 2023. [64] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [66] David W Romero, R Bruintjes, Erik J Bekkers, Jakub M Tomczak, Mark Hoogendoorn, and JC van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In 10th International Conference on Learning Representations, 2022. [67] David W Romero, David M Knigge, Albert Gu, Erik J Bekkers, Efstratios Gavves, Jakub M Tomczak, and Mark Hoogendoorn. Towards a general purpose cnn for long range dependencies in $\\{N\\}$ d. arXiv preprint arXiv:2206.03398, 2022. [68] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data.",
    "m2-30": "In International Conference on Learning Representations, 2021. [69] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021. [70] G. Szeg\u00f6. Orthogonal Polynomials. Number v. 23 in American Mathematical Society colloquium publications. American Mathematical Society, 1967. [71] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [72] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlpmixer: An all-mlp architecture for vision. Advances in neural information processing systems, $34: 24261-24272,2021$. [73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017. [74] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.",
    "m2-31": "arXiv:1804.07461, 2018. [75] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention.",
    "m2-32": "arXiv preprint arXiv:2212.10544, 2022. [76] Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang Yuan, Xin Jin, Gang Huang, Yunxin Liu, and Xuanzhe Liu. Melon: Breaking the memory wall for resource-efficient ondevice machine learning. In Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services, pages 450-463, 2022. [77] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [78] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020.",
    "m2-33": "[79] Kuan Xu and Ana F. Loureiro. Spectral approximation of convolution operators. SIAM Journal on Scientific Computing, 40(4):A2336-A2355, 2018. [80] Yufan Xu, Saurabh Raje, Atanas Rountev, Gerald Sabin, Aravind Sukumaran-Rajam, and P Sadayappan. Training of deep learning pipelines on memory-constrained gpus via segmented fused-tiled execution. In Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction, pages 104-116, 2022. [81] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023. [82] Shanshan Yu, Jindian Su, and Da Luo. Improving bert-based text classification with auxiliary sentence and domain knowledge. IEEE Access, 7:176600-176612, 2019. [83] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
    "m2-34": "In Proceedings of the IEEE/CVF international conference on computer vision, pages $558-567,2021$. [84] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023-6032, 2019 . [85] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization.",
    "m2-35": "arXiv preprint arXiv:1710.09412, 2017. [86] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert.",
    "m2-36": "arXiv preprint arXiv:1904.09675, 2019. [87] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13001-13008, 2020. [88] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34:17723-17736, 2021. [89] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. ## Author Contributions\n\nD.Y.F. Conceptualized the research; coordinated collaborations; developed M2 architectures; led experimental and implementation efforts; assisted in development of theoretical results; coordinated writing. S.A. Assisted with the development of M2-BERT architecture; conducted BERT experiments; assisted in writing. J.G. Led development of theory and causal algorithms; wrote Appendix D. I.J. Led development of theory and causal algorithms; wrote Appendix D. S.E. Assisted with BERT experiments; conducted Swin experiments; wrote Listing A; assisted in writing. A.W.T. Conducted ViT experiments; assisted in writing. B.S. Assisted in optimized M2 implementation; conducted mixer benchmarks; assisted in writing. M.P. Assisted in development of M2-GPT architecture. A.R. Supervised theory development; developed proofs; reviewed manuscript. C.R. Supervised research; reviewed manuscript. Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, and Armin Thomas contributed equally to this work. ## Appendix\n\nAppendix A gives a PyTorch code listing of an M2 layer. Appendix B presents additional experiments. Appendix C gives details for the experiments, including model architectures and hyperparameters. Appendix D gives missing details and proofs for the theoretical analysis, as well as generalizations to broader results. ## A Implementation\n\n```\nfrom einops import rearrange\nimport torch\nfrom torch import nn\ndef blockdiag_matmul(x, w):\n    return torch.einsum(\n            \"bnm, ..bm-> ...bn\", w, x.view(*x.shape[:-1], w.shape[0], w.shape[-1])\n    ).reshape(*x.shape)\nclass MonarchMatrix(nn.Module):\n    def __init__(self, sqrt_n: int):\n        super().__init__()\n        self.sqrt_n = sqrt_n\n        self.L = nn.Parameter(torch.randn((sqrt_n, sqrt_n, sqrt_n)))\n        self.R = nn.Parameter(torch.randn((sqrt_n, sqrt_n, sqrt_n)))\n    def forward(self, x):\n        x = rearrange(x, \"...(m n) -> ...(n m)\", n=self.sqrt_n)\n        x = blockdiag_matmul(x, self.L)\n        x = rearrange(x, \" .. (m n) -> ...(n m)\", n=self.sqrt_n)\n        x = blockdiag_matmul(x, self.R)\n```\n\nTable 9: Fine-tuning performance on GLUE [74]. We report the standard metrics - F1 scores for QQP and MRPC, Matthew's correlation for CoLA, Spearman's correlation for STS-B, and accuracy for the remaining tasks, following the procedure from [36]. | Model | MNLI $(\\mathrm{m} / \\mathrm{mm})$ | RTE | QNLI | QQP | SST2 | STS-B | CoLA | MRPC | Average |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| M2-BERT-base (80M) | $78.4 / 78.6$ | 68.5 | 84.6 | 86.7 | 92.0 | 86.3 | 53.0 | 89.8 | 79.9 |\n| M2-BERT-base (110M) | $79.6 / 80.5$ | 69.3 | 86.0 | 87.0 | 92.3 | 86.9 | 56.0 | 89.2 | 80.9 |\n| M2-BERT-large (260M) | $81.7 / 81.9$ | 72.8 | 84.7 | 87.8 | 93.3 | 88.0 | 59.2 | 90.0 | 82.2 |\n| M2-BERT-large (341M) | $82.2 / 82.3$ | 75.0 | 87.0 | 87.7 | 92.4 | 88.3 | 59.6 | 90.1 | 82.8 |\n\nTable 10: Throughput in tokens/ms by context length for M2-BERT-base (80M) compared to 80 M BERT models.",
    "m2-37": "```\n    return rearrange(x, \"... (m n) -> ... (n m)\", n=self.sqrt_n)\nclass MonarchMixerLayer(nn.Module):\n    def __init__(self, sqrt_n: int, sqrt_d: int):\n        super().__init__()\n        self.m1 = MonarchMatrix(sqrt_n)\n        self.m2 = MonarchMatrix(sqrt_n)\n        self.m3 = MonarchMatrix(sqrt_d)\n        self.m4 = MonarchMatrix(sqrt_d)\n        self.n_kernel = nn.Parameter(torch.randn(sqrt_d ** 2, sqrt_n ** 2))\n        self.d_kernel = nn.Parameter(torch.randn(1, sqrt_d ** 2))\n        self.layer_norm = nn.LayerNorm(sqrt_d ** 2)\n    def forward(self, x: torch.Tensor): # x.shape = (b, n, d)\n        x_tilde = self.m2(torch.relu(self.n_kernel * self.m1(x.transpose(-1, -2)) )\n    ).transpose(-1, -2) # mix sequence\n        y = self.m4(torch.relu(self.d_kernel * self.m3(x_tilde))) # mix features\n        return self.layer_norm(y + x_tilde) # skip connection\n```\n\nListing 1: A basic implementation of the M2 layer. ## B Additional Experiments\n\n## B. 1 Per-Task GLUE Numbers\n\nWe report full GLUE numbers for M2-BERT-base and M2-BERT-large in Table 9. ## B. 2 Additional Throughput Results\n\nWe report the throughput of M2-BERT-base (80M) compared to BERT models of the same size (BERT-base with fewer parameters), as well as the throughput of M2-BERT-large (260M) compared\n\nTable 11: Throughput in tokens/ms by context length for M2-BERT-large (260M) compared to BERT-large. | Model | $\\mathbf{5 1 2}$ | $\\mathbf{1 0 2 4}$ | $\\mathbf{2 0 4 8}$ | $\\mathbf{4 0 9 6}$ | $\\mathbf{8 1 9 2}$ |\n| ---: | :---: | :---: | :---: | :---: | :---: |\n| HF BERT-large (340M) | 75.4 | 47.1 | 25.2 | OOM | OOM |\n| FlashAttention BERT-large (340M) | $\\mathbf{1 2 5 . 0}$ | 111.9 | 91.6 | 54.5 | OOM |\n| M2-BERT-large (260M) | 122.5 | $\\mathbf{1 1 8 . 6}$ | $\\mathbf{1 0 9 .",
    "m2-38": "4}$ | $\\mathbf{9 4 . 5}$ | $\\mathbf{7 5 . 0}$ |\n| M2 Speedup over HF BERT-large (340M) | $1.6 \\times$ | $2.5 \\times$ | $4.3 \\times$ | - | - |\n\nTable 12: ImageNet accuracy of Swin models. | Model | ImageNet (acc@1) | ImageNet (acc@5) |\n| ---: | :---: | :---: |\n| Swin-MLP-B | 81.3 | 95.3 |\n| Swin-V1-B | 83.5 | 96.5 |\n| Swin-V2-B | $\\mathbf{8 4 . 2}$ | $\\mathbf{9 6 . 9}$ |\n| M2-Swin-B | 83.5 | 96.7 |\n\nto BERT-large. Table 10 compares the performance of M2-BERT-base (80M) to BERT models parametermatched to 80 M parameters. M2 is slower than FlashAttention for sequence lengths 512 and 1 K , but outperforms FlashAttention starting at sequence length 2 K . We believe further optimization of the M2 kernel can close the gap to FlashAttention for short sequences. Table 11 compares M2-BERT-large (260M) to BERT-large. Trends are mostly similar to comparisons against BERT-base; M2 nearly matches FlashAttention at sequence length 512, and outperforms it for sequence length 1 K and longer. We also see up to $4.3 \\times$ speedup over HuggingFace BERT-large at sequence length 2 K . ## B. 3 ImageNet Comparison against Swin\n\nTable 12 reports the results of replacing attention and MLP in Swin-V2 using M2 as a drop-in replacement. Surprisingly, Swin-M2 outperforms Swin-MLP-B, is competitive with Swin-V1-B, and comes within 1 point of Swin-V2-B, even without any hyperparameter tuning or architecture adjustment from the ViT formula.",
    "m2-39": "We expect that performance may improve further with hyperparameter tuning specific to M2. ## B. 4 Speech Applications\n\nTable 13 presents the performance of M2 on Speech Commands-10, a speech classification task over raw 1-second clips sampled at $16 \\mathrm{kHz} . \\mathrm{M} 2$ is competitive with state-of-the-art architectures on this task. ## B. 5 CIFAR10\n\nTable 14 shows the performance of Monarch Mixer on CIFAR10. The trends are largely the same as on ImageNet. Table 13: Accuracy on Speech-Commands 10. An \"x\" means that the model did not fit in memory. | M2 | S4 | WaveGan-D | Trans | ormer | Performer | CKConv |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 97.9 | 97.5 | 96.3 |  |  | 30.8 | 71.7 |\n| Table 14: Accuracy on CIFAR-10. |  |  |  |  |  |  |\n| Model |  |  | Top-1\\% | Description |  |  |\n| ViT $(1.2 \\mathrm{M})$ |  |  | 78.6 | Attention + MLP |  |  |\n| ViT + Monarch $(607 \\mathrm{~K})$ |  |  | 79.0 | Attention, MLP-Free |  |  |\n| HyenaViT (1.3M) |  |  | 80.6 | Attention-Free + MLP |  |  |\n| HyenaViT-M2 $(741 \\mathrm{~K})$ |  |  | 80.8 | Attention-Free + MLP Free |  |  |\n\n## B. 6 Learnable Monarch Matrices in Sequence Mixer\n\nIn most of our models, we have used fixed Monarch matrices for the sequence mixer, and learnable Monarch matrices for the dimension mixer. Table 15 presents an experiment evaluating using learnable Monarch matrices for the sequence mixer on the sequential CIFAR task. We use a non-gated convolutional architecture based off long convolutions, as presented in [26]. Learning the Monarch matrices in the sequence mixer yields 1.5 points of lift. ## B. 7 Roofline Analysis\n\nFigure 4 shows a Roofline analysis of a simple PyTorch implementation of a single M2 operator $\\mathbf{M}^{-1}(\\mathbf{M u} \\odot \\mathbf{M k}$ on an A100 GPU, with 4 K input length. The operation is more dominated by the data movement operations, which helps explain why performance is higher on newer architectures like RTX 4090 (which have faster and larger L2 cache). ## B. 8 Associative Recall\n\nIn Table 16, we present a simple experiment demonstrating the causal parameterization of M2 on associative recall, a synthetic language designed to test in-context learning. The model demonstrates in-context learning abilities in sequences up to 128 K tokens, but Transformers do not scale past 8 K . ## B. 9 BERT Experiments with Alternative Architecture\n\nHere, we report results using an older version of the M2-BERT architecture, that uses non-gated convolutions and is trained on English Wikipedia [23] and English Bookcorpus [89]. For clarity, we refer to this model as M1-BERT. We found that M1-BERT could match Transformers on MLM quality, but underperformed on downstream fine-tuning. We attribute this gap in performance to sub-optimal training hyperparameters (optimized for throughput using NVIDIA MLPerf hyperparameters) as well as a sub-optimal architecture. We report results here for completeness, but refer to the gated convolution architecture in the main body as the proper M2-BERT model. These models followed the reference implementations and hyperparameters from Hugging Face Transformers examples [78] and Nvidia Deep Learning examples (https://github.com/NVIDIA/ DeepLearningExamples). In particular, we use the LAMB optimizer with a learning rate of $5 e-3$. Table 15: Accuracy on sequential CIFAR for fixed vs. learnable Monarch in the sequence mixer. | Model | sCIFAR Accuracy |\n| ---: | :---: |\n| M2, Fixed Monarch | 91.0 |\n| M2, Learnable Monarch | $\\mathbf{9 2 . 5}$ |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-24.jpg?height=616&width=733&top_left_y=543&top_left_x=661)\n\nFigure 4: Roofline plot of a PyTorch implementation of a single M2 operator $\\mathbf{M}^{-1}(\\mathbf{M u} \\odot \\mathbf{M k})$.",
    "m2-40": "For each sequence length, we use as large a minibatch size as possible that fits on the GPU (A10080GB in Table 17 and V100 in Table 18). We set the gradient accumulation to reach a global batch size of 65,536 sequences. To investigate the effect of sequence length, each model is trained for a fixed sequence length in a single phase of training (in contrast to some training protocols, which train the model in multiple phases, each at different sequence lengths). Time to a Fixed Pretraining Quality on 8 xA100 We compare time to a fixed pretraining quality, training M1-BERT-base on English Wikipedia [23] and English Bookcorpus [89]. We compare against BERT-base trained with FlashAttention [13], as well as the Monarch-BERTbase implementation from the original Monarch paper [12]. We measure wall-clock time for M1-BERT and the base Transformer to reach $50 \\%$ in masked language modeling accuracy on 8xA100 Nvidia GPUs with 80GB memory each. Table 17 summarizes results. In short sequence lengths, M1-BERT is comparable to FlashAttention, even without using a heavily-optimized fused kernel. In longer sequence lengths, the FLOP savings make M1-BERT more efficient-up to $2.4 \\times$ faster than BERT with FlashAttention at sequence length 4096. BERT in Half a Day Inspired by recent work focusing on training under limited resource constraints [28], we measure how far we can get when training on a single V100 GPU in 12 hours. In Table 18, we report the masked language modeling accuracy achieved by the same set of models and sequence lengths (except for the FlashAttention baseline, which is not supported on V100). We observe M1-BERT both achieves higher accuracy within the time limit and can be trained at longer sequence lengths than the baseline architectures. Downstream Fine-Tuning We evaluate the quality of M1-BERT-base models on the GLUE benchmark [74]. Table 19 shows fine-tuning performance on the GLUE tasks, using the same\n\nTable 16: In-context learning performance on associative recall at various sequence lengths, vocab size 20. $\\boldsymbol{X}$ indicates the Transformer did not finish in a week. | Model | 0.5 K | 2 K | 8 K | 32 K | 128 K |\n| ---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer | 100.0 | 100.0 | 100.0 | $\\boldsymbol{x}$ | $\\boldsymbol{x}$ |\n| Monarch Mixer | 98.7 | 99.4 | 99.4 | 99.4 | 99.4 |\n\nTable 17: Time in hours to reach $50 \\%$ masked language modeling validation accuracy on 8 xA100 with different sequence lengths. | Model | 512 | 1024 | 2048 | 4096 | Architecture Details |\n| ---: | :---: | :---: | :---: | :---: | :---: |\n| BERT-base-FlASHATTENTION (110M) | 2.7 | 3.8 | 5.7 | 13.2 | Attention, MLP |\n| BERT-base-HuggingFace (110M) | 3.3 | 5.6 | 13.1 | 26.7 | Attention, MLP |\n| BERT-Monarch-base (80M) | 3.1 | 4.7 | 10.3 | 22.1 | Attention, MLP-free |\n| M1-BERT-base (55M) | 2.5 | 3.5 | 4.0 | 5.5 | Attention-Free, MLP-free |\n| Speedup | $1.1 \\times$ | $1.1 \\times$ | $1.3 \\times$ | $2.4 \\times$ |  |\n\nhyperparameters and 5 epochs for all tasks and both models. M1-BERT-base is competitive with Transformers trained using MLPerf hyperparameters on Bookcorpus and Wikitext, but underperforms fully-trained transformers and M2-BERT-base. ## C Experiment Details\n\n## C. 1 Model Architectures\n\nIn this section, we describe the exact model architectures we used for each task, including the design of the block (residuals and gating). We additionally release our code for reproducibility,\n\nBERT Language Modeling The M2-BERT architectures use a standard BERT backbone, but replace the attention with bidirectional gated convolutions and replace the linear layers in the MLPs with block-diagonal matrices. All the M2-BERT architectures use an expansion factor of four. M2-BERT-base ( 80 M ) has a model width of 768 and 12 layers; M2-BERT-base (110M) has a model width of 960 and 12 layers; M2-BERT-large (260M) has a model width of 1536 and 12 layers; and M2-BERT-large (341M) has a model width of 1792 and 12 layers. We train all these models on C4 for 70,000 steps, with sequence length 128, and global batch size 4096 sequences. For all the models, we use decoupled AdamW with learning rate 8e-4 and decoupled weight decay 1e-5. We use linear learning rate decay with a warmup of $6 \\%$ of the steps, and we use MLM masking percentage of $30 \\%$. For GLUE fine-tuning, we do a small search of learning rate, weight decay, and number of epochs. Following [36], we fine-tune RTE, MRPC, and STS-B from the MNLI checkpoint. We fine-tune all tasks with sequence length 128. For some tasks, we also pool the embeddings of all the non-padding tokens instead of using the CLS token. The final hyperparameters for M2-BERT-base (80M) are decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 3 epochs for MNLI; AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay 0.01 for 6 epochs for RTE; AdamW with learning rate 3 e- 5 and weight decay 0.01 for 10 epochs on QQP; AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $1 \\mathrm{e}-5$ for 10 epochs with average pooling for QNLI; decoupled AdamW with learning rate 3e-5 and weight decay 3ed-6 for 3 epochs for SST-2; AdamW with learning rate 7e-5 and weight decay 0.01 for 10 epochs for STS-B; AdamW\n\nTable 18: Masked language modeling validation accuracy achieved on a single V100 in 12 hours with different sequence lengths. $\\boldsymbol{X}$ indicates the model does not fit on device with a batch size of 1. | Model | 512 | 1024 | 2048 | 4096 | 8192 | Architecture Details |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BERT-base (110M) | 11.5 | 7.8 | 6.8 | $\\boldsymbol{x}$ | $\\boldsymbol{x}$ | Attention, MLP |\n| BERT-Monarch-base | 6.9 | 8.5 | 6.8 | $\\boldsymbol{x}$ | $\\boldsymbol{x}$ | Attention, MLP-Free |\n| M1-BERT-base | 20.2 | 20.2 | 20.1 | 17.1 | 12.9 | Attention-Free, MLP-Free |\n\nTable 19: Fine-tuning performance on the GLUE benchmark [74], after pretraining on Wikipedia and Bookcorpus. We report the standard metrics - F1 scores for QQP and MRPC, Matthew's correlation for CoLA, Spearman's correlation for STS-B, and accuracy for the remaining tasks [19]. | Model | MNLI $(\\mathrm{m} / \\mathrm{mm})$ | RTE | QNLI | QQP | SST2 | STS-B | CoLA | MRPC | Architecture Details |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BERT no pretrain | $34.1 / 34.1$ | 47.3 | 50.0 | 68.6 | 79.9 | 17.8 | 0.0 | $\\mathbf{7 7 . 9}$ | Attention, MLP |\n| BERT-base | $\\mathbf{7 4 .",
    "m2-41": "5} / \\mathbf{7 4 . 7}$ | $\\mathbf{5 5 . 6}$ | 69.3 | $\\mathbf{8 1 . 8}$ | 83.9 | 19.8 | 12.1 | 74.2 | Attention, MLP |\n| M1-BERT-base | $69.9 / 70.5$ | 53.1 | $\\mathbf{7 3 .",
    "m2-42": "2}$ | 81.4 | $\\mathbf{8 5 .",
    "m2-43": "2}$ | $\\mathbf{6 8 . 1}$ | $\\mathbf{3 3 . 6}$ | 75.4 | Attention-free, MLP-free |\n\nwith learning rate $5 \\mathrm{e}-5$ and weight decay 0.01 for 10 epochs for MRPC; and decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 10 epochs for COLA. For M2-BERT-base (110M), the hyperparameters are decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 3 epochs for MNLI; decoupled AdamW with learning rate $1 \\mathrm{e}-5$ and weight decay $1 \\mathrm{e}-6$ for 3 epochs for RTE; decoupled AdamW with learning rate $3 \\mathrm{e}-5$ and weight decay $3 \\mathrm{e}-6$ for 5 epochs on QQP; decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $1 \\mathrm{e}-5$ for 10 epochs with average pooling for QNLI; decoupled AdamW with learning rate $3 \\mathrm{e}-5$ and weight decay 3ed-6 for 3 epochs for SST-2; decoupled AdamW with learning rate 8e-5 and weight decay 3e-6 for 10 epochs for STS-B; decoupled AdamW with learning rate 8e-5 and weight decay 8e-5 for 10 epochs for MRPC; and AdamW with learning rate $8 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 10 epochs for COLA. For M2-BERT-large (260M), the hyperparameters are decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 3 epochs for MNLI; decoupled AdamW with learning rate $1 \\mathrm{e}-5$ and weight decay 1e-6 for 3 epochs for RTE; decoupled AdamW with learning rate $3 \\mathrm{e}-5$ and weight decay $3 \\mathrm{e}-6$ for 5 epochs on QQP; decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $1 \\mathrm{e}-5$ for 10 epochs for QNLI; decoupled AdamW with learning rate 3 e-5 and weight decay 3ed-6 for 3 epochs for SST-2; decoupled AdamW with learning rate 7e-5 and weight decay 3e-6 for 10 epochs for STS-B; decoupled AdamW with learning rate 8e-5 and weight decay 8e-6 for 10 epochs for MRPC; and AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 10 epochs for COLA. For M2-BERT-large (341M), the hyperparameters are decoupled AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay $5 \\mathrm{e}-6$ for 3 epochs for MNLI; AdamW with learning rate $5 \\mathrm{e}-5$ and weight decay 1e-6 for 2 epochs for RTE; decoupled AdamW with learning rate $3 \\mathrm{e}-5$ and weight decay $3 \\mathrm{e}-6$ for 5 epochs on QQP; decoupled AdamW with learning rate 5e-5 and weight decay 1e-6 for 10 epochs for QNLI; decoupled AdamW with learning rate 3e-5 and weight decay 3ed-6 for 3 epochs for SST-2; decoupled AdamW with learning rate $8 \\mathrm{e}-5$ and weight decay $3 \\mathrm{e}-5$ for 8 epochs for STS-B; decoupled AdamW with learning rate $8 \\mathrm{e}-5$ and weight decay $8 \\mathrm{e}-6$ for 10 epochs for MRPC; and decoupled AdamW with learning rate 5e-5 and weight decay $1 \\mathrm{e}-6$ for 10 epochs for COLA. ViT We use a standard ViT model architecture as base [20]. In line with recent improvements to the ViT architecture [3, 69, 83], we use sinusoidal position embeddings and global average-pooling\n\nTable 20: ViT training settings. |  | ImageNet-1k | CIFAR-10 |\n| ---: | :---: | :---: |\n| Optimizer | AdamW |  |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.999$ |  |\n| Learning rate schedule | Cosine decay w/ linear warmup |  |\n| Dropout rate | 0 |  |\n| Label smoothing | 0.1 |  |\n| Image size | $224 \\times 224$ | $32 \\times 32$ |\n| Base learning rate | $1 \\mathrm{e}-3$ | $\\{1 \\mathrm{e}-4,3 \\mathrm{e}-4,1 \\mathrm{e}-3\\}$ |\n| Batch size | 1024 | 512 |\n| Training epochs | 300 | up to 500 |\n| Warmup epochs | 10 | 5 |\n| Stochastic depth rate | 0.1 | $\\{0,0.1\\}$ |\n| Weight decay | 0.05 | $\\{0,0.1\\}$ |\n|  |  |  |\n\n(GAP) instead of a class token. We adapt the ViT architecture by replacing its MLP and/or attention components with Monarch Matrices (similar to our adaptation of BERT):\n\nWe replace the MLP with randomly initialized Monarch Matrices of the same dimension as the dense matrices of the MLP and learn those matrices during training, setting the number of blocks in the block-diagonal matrices to 4 . We replace attention with the recently introduced Hyena operator [63]. The Hyena operator represents a recurrence of two efficient sub-quadratic primitives, an implicit long convolution and multiplicative element-wise gating of the projected input. Hyena operators apply the FFT algorithm to achieve fast long convolutions in sub-quadratic time. We further adapt the Hyena operator by replacing its long convolutions with the M2 operator and setting the Monarch Matrices to the DFT and inverse DFT. ViT for ImageNet-1k In line with other work [3,12, 63, 69], we use a ViT-base architecture with 12 layers, a hidden size of 768,12 attention heads per layer, an intermediate size of the MLP projection of 3,072 , and a patch size of $16 \\times 16$ pixels. For optimization, we follow the training procedure of T2T-ViT [83], including augmentations such as RandAugment [10] (magnitude $=$ 9 , magnitude-std $=0.5$, layers $=2$ ), Mixup [85] $(\\alpha=0.8)$, CutMix [84] $(\\alpha=1.0)$, Random erasing [87] (rate $=0.25)$, and AugMix [35]. See Table 20 for all other training settings. ViT for CIFAR-10 We use a ViT architecture with 6 layers, a hidden size of 128, 8 attention heads per layer, an intermediate size of the MLP projection of 512 , and a patch size of $4 \\times 4$ pixels. We further tune weight decay ( 0 or 0.1 ), stochastic depth rate ( 0 or 0.1 ), and base learning rate ( $1 e-4$ or $3 e-4$ or $1 e-3$ ) and report the test performance for the model variant that achieved the highest accuracy in a separate held-out validation dataset (randomly selected $10 \\%$ of training data).",
    "m2-44": "We also apply an early stopping rule such that training is stopped if the model's validation loss does not improve for 10 training epochs. See Table 20 for all other training settings. GPT Causal Language Modeling Similarly to our ViT approach, we also replace attention with the Hyena operator, using the same architecture as in [63] as a starting point. The Hyena\narchitecture has two convolutions, which can be computed using the FFT convolution theorem. In our architecture, we additionally replace these FFT operations with causal Monarch matrices. In addition, we re-use the heads extension from the H 3 architecture [25]. The heads extension groups the model dimension into heads, ties together the long convolution parameters in each head, and then computes the outer product between different input projections. An algorithmic listing adapted from the H 3 paper [25] is provided in Listing 1, with updates to replace the SSM layers with Hyena convolutions. We use a head dimension of 16 . Setting the head dimension to be 1 and replacing the Monarch matrices with FFT is equivalent to the Hyena layer. ```\nAlgorithm 1 M2 Hyena Layer with Heads\nInput: Input sequence \\(u \\in \\mathbb{R}^{N \\times d}\\) from the previous layer, weight matrices \\(\\mathbf{W}_{X 1}, \\mathbf{W}_{X 2}, \\mathbf{W}_{V}, \\mathbf{W}_{O} \\in \\mathbb{R}^{d \\times d}\\),\n    causal Monarch matrix M, short convolution kernels \\(\\mathbf{K}_{1}, \\mathbf{K}_{2}, \\mathbf{K}_{3}\\), a Hyena convolution kernel \\(\\mathbf{K}_{\\text {long }}\\), head\n    dimension \\(d_{h}\\). Output: Output sequence \\(y \\in \\mathbb{R}^{N \\times d}\\)\n    Compute \\(\\mathbf{X}_{1}=u \\mathbf{W}_{X 1}, \\mathbf{X}_{2}=u \\mathbf{W}_{X 2}, \\mathbf{V}=u \\mathbf{W}_{V} \\in \\mathbb{R}^{N \\times d}\\). Pass \\(\\mathbf{X}_{1}, \\mathbf{X}_{2}, \\mathbf{V}\\) each through the short convolution using the causal Monarch matrices: \\(\\overline{\\mathbf{X}_{1}}, \\overline{\\mathbf{X}_{2}}, \\overline{\\mathbf{V}}=\\)\n    \\(\\mathbf{M}^{-1}\\left(\\mathbf{M X}_{1} \\odot \\mathbf{M K}_{1}\\right), \\mathbf{M}^{-1}\\left(\\mathbf{M X}_{2} \\odot \\mathbf{M K}_{2}\\right), \\mathbf{M}^{-1}\\left(\\mathbf{M V} \\odot \\mathbf{M K}_{3}\\right)\\). Split \\(\\overline{\\mathbf{X}_{1}}, \\overline{\\mathbf{X}_{2}}, \\overline{\\mathbf{V}}\\) into \\(H\\) \"heads\" \\(\\left({\\overline{\\mathbf{X}_{1}}}^{(h)},{\\overline{\\mathbf{X}_{2}}}^{(h)}, \\overline{\\mathbf{V}}^{(h)}\\right.\\) for \\(\\left.h=1, \\ldots, H\\right)\\), each a sequence of \\(N\\) vectors of size\n    \\(d_{h}=d / H\\). for \\(1 \\leq h \\leq H\\) do\n```\n\nTake the batched outer product $\\overline{\\mathbf{X}}_{2}^{(h)}\\left(\\overline{\\mathbf{V}}^{(h)}\\right)^{\\top} \\in \\mathbb{R}^{N \\times d_{h} \\times d_{h}}$ (batched in the $N$-dimension) and pass it through the long convolution using the causal Monarch: $\\mathbf{X V}{ }^{(h)}=\\mathbf{M}^{-1}\\left(\\mathbf{M} \\overline{\\mathbf{X}}_{2}{ }^{(h)}\\left(\\overline{\\mathbf{V}}^{(h)}\\right)^{\\top} \\odot \\mathbf{M K}_{\\text {long }}\\right) \\in$ $\\mathbb{R}^{N \\times d_{h} \\times d_{h}}$. Batch-multiply by $\\overline{\\mathbf{X}_{1}}: \\mathbf{O}^{(h)}=\\left[\\overline{\\mathbf{X}}_{1}{ }_{1}^{(h)} \\mathbf{X V}_{1}^{(h)}, \\ldots, \\overline{\\mathbf{X}}_{1}{ }_{N}^{(h)} \\mathbf{X V}_{N}^{(h)}\\right] \\in \\mathbb{R}^{N \\times d_{h}}$ (batched in the $N$-dimension). Concatenate the output $\\mathbf{O}^{(h)}$ of each head, and multiply by the output projection matrix $\\mathbf{W}_{O} \\in \\mathbb{R}^{d \\times d}$. Finally, we remove the MLP layers entirely (equivalent to replacing the layer with an identity), and make the model wider to compensate (the depths match the equivalent Hyena models). The small model has a model width of 1160 with 18 layers and uses a learning rate of 0.0006 , and the medium model has model width of 1344 with 40 layers and uses a learning rate of 0.0008 . All other hyperparameters match the Hyena models [63]. ## D Missing details from Section 4\n\nThis section contains all the missing details (including proofs) from Section 4. In Appendix D.1, we review some definitions and results on multi-variate polynomials and set some notation needed for this section. In Appendix D.2, we explicitly connect Monarch matrices for $p=2$ and bivariate polynomial evaluation. Specifically, we prove Theorem 1 and Theorem 2. Then in Appendix D. 3 we show how to instantiate the bivariate basis polynomials so that we get a causal map. This includes converting the bivariate polynomials to univariate polynomials (with evaluations over the $N$ th roots of unity) and this proves Theorem 3. We then show how this causal map can be implemented only using GEMMs (and $O\\left(N^{3 / 2}\\right)$ FLOPs) in Appendix D.4. Next, we note that while our evaluations points are over complex numbers, our input and output to the Monarch convolution layers are over reals. Hence, it is natural to wonder if we can implement the entire layer just with operations over real numbers. One potential advantage of this is that we theoretically only have to keep $N$ real numbers for intermediate results (instead of $2 N$ reals numbers when we keep track of vectors in $\\mathbb{C}^{N}$ ). This can reduce the data movement costs. Further, multiplication of two complex numbers requires six operations over real numbers (four\nmultiplication and two addition). Thus, moving to an implementation that only uses real numbers could potentially lead to wall clock time speedup. We propose one such scheme in Appendix D. 5 that proves a version of Theorem 3 just over reals by moving to the Chebyshev basis (instead of the standard monomial basis). This creates new technical challenges, which we also address. Finally, we generalize our results to arbitrary $p \\geq 2$ in Appendix D.6. We would like to point out that to get a causal map (in Theorem 17) we need to 'embed' input vectors of size $n$ into vectors of size $N=2^{p} \\cdot n+O\\left(n^{1-1 / p}\\right)$.",
    "m2-45": "For $p=2$, we avoided the blowup of $2^{2}=4$ with a blowup of 2 instead (via Theorem 3). Whether this is possible to do (i.e. have a blowup of 2 instead of $2^{p}$ ) for $p>2$ is an interesting direction for future work. Further, the matrices that lead to causal map can be represented with $O\\left(p N^{2 / p}\\right)$ parameters while the matrices in Theorem 3 use more parameters. Extending the causal map for $p>2$ that uses $O\\left(N^{1+\\frac{1}{p}}\\right)$ parameters is an exciting direction for future work. ## D. 1 Background and Notation\n\nWe collect known facts and definitions about multi-variate polynomials in Appendix D.1.1 and recall some notation from [12] in Appendix D.1.2. These will be needed throughout this appendix section. ## D.1.1 Multi-variate Polynomials\n\nBasic Definitions Let $p \\geq 1$ be an integer. We recollect some definitions on $p$-variate polynomials (over $\\mathbb{R}$ ) in variables $X_{0}, \\ldots, X_{p-1}$. When $p \\in\\{1,2\\}$, we will use variables in $\\{X, Y, Z\\}$ for notational simplicity. We will use $\\mathbf{X}$ to denote the vector of variables $\\left(X_{0}, \\ldots, X_{p-1}\\right)$. Further for $\\mathbf{j} \\in \\mathbb{Z}_{\\geq 0}^{p}$, we use the notation\n\n$$\n\\mathbf{X}^{\\mathbf{j}}=\\prod_{a=0}^{p-1} X_{a}^{j_{a}}\n$$\n\n$\\mathbf{X}^{\\mathbf{j}}$ is a (standard basis) monomial, where $\\mathbf{j}=\\left(j_{0}, \\ldots, j_{p-1}\\right)$. A generic $p$-variate polynomial is defined as (with standard monomial representation)\n\n$$\nq(\\mathbf{X})=\\sum_{\\mathbf{j} \\in \\mathbb{Z}_{\\geq 0}^{p}} q_{\\mathbf{j}} \\cdot \\mathbf{X}^{\\mathbf{j}}\n$$\n\nwhere the coefficient $q_{j} \\in \\mathbb{R}$. We will need the following notion of degrees:\nDefinition 2 (Degree). Let $0 \\leq a<p$. The degree of $X_{a}$ in $\\mathbf{X}^{\\mathbf{j}}$ (with $\\mathbf{j}=\\left(j_{0}, \\ldots, j_{p-1}\\right)$ ) is $j_{a}$. The degree of $X_{a}$ of $q(\\mathbf{X})$, denoted by $\\operatorname{deg}_{X_{a}}(q)$ is the maximum degree of $X_{a}$ over all monomials $\\mathbf{X}^{\\mathbf{j}}$ with $q_{\\mathbf{j}} \\neq 0$. Note that for $p=1$ the above coincides with the usual notion of degree of a univariate polynomial $q(Z)$, in which case we just use $\\operatorname{deg}(q(Z))$ to denote $\\operatorname{deg}_{Z}(q(Z))$. We will need the notion of taking $\\bmod$ of a $p$-variate polynomial with $p$-tuple of polynomials. The notion of $\\bmod$ is well defined for a univariate polynomial (which we will assume as a given below) but in general for arbitrary $p$-variate polynomials $q(\\mathbf{X})$ and $q^{\\prime}(\\mathbf{X})$, the operation $q(\\mathbf{X})$ $\\bmod q^{\\prime}(\\mathbf{X})$ is not well defined. However, we will only need the following restricted operation:\n\nDefinition 3. Let $p \\geq 1$. Fix a p-tuple of polynomials $R_{0}\\left(X_{0}\\right), \\ldots, R_{p-1}\\left(X_{p-1}\\right)$. Then for any $\\mathbf{j} \\in \\mathbb{Z}_{\\geq 0}^{p}$, we define\n\n$$\n\\mathbf{X}^{\\mathbf{j}} \\bmod \\left(R_{0}\\left(X_{0}\\right), \\ldots, R_{p-1}\\left(X_{p-1}\\right)\\right)=\\prod_{a=0}^{p-1}\\left(X^{j_{a}} \\bmod \\left(R_{a}\\left(X_{a}\\right)\\right)\\right)\n$$\n\nFor a general polynomial $p(\\mathbf{X})$,\n\n$$\np(\\mathbf{X}) \\bmod \\left(R_{0}\\left(X_{0}\\right), \\ldots, R_{p-1}\\left(X_{p-1}\\right)\\right)\n$$\n\nis defined by extending the definition for $\\mathbf{X}^{\\mathbf{j}}$ by linearity. Polynomial Evaluation Given a $p$-variate polynomial $q(\\mathbf{X})$ and an point $\\mathbf{a} \\in \\mathbb{R}^{p}$, the evaluation of $q$ at a denoted by $q(\\mathbf{a})$ is evaluation of $q$ as a function at $\\mathbf{a}$. Given subsets $S_{a} \\subseteq \\mathbb{C}$, we define $q(\\mathbf{X})$ evaluated at $\\times_{a=0}^{p-1} S_{a}$ as the vector of values $q(\\mathbf{a})$ overall $\\mathbf{a} \\in \\times_{a=0}^{p-1} S_{a}$. In this paper, we will in many cases evaluate polynomials at the appropriate roots of unity. Specifically for an integer $N$, we will define\n\n$$\n\\omega_{N}=e^{2 \\pi \\iota / N}\n$$\n\nand note that the $N$ th roots of unity is the set $\\left\\{\\omega_{N}^{i} \\mid 0 \\leq i<N\\right\\}$. Polynomial Interpolation We now recall univariate and bivariate polynomial interpolation results (proved via the Lagrange basis), which we will use in later subsections.",
    "m2-46": "Theorem 4. Let $D \\geq 1$ be an integer. Given $y_{i}$ for $0 \\leq i<D$ and $\\alpha_{i}$ for $0 \\leq i<D$ there exists a unique univariate polynomial $P(X)$ with $\\operatorname{deg}(P)<D$, such that for all $0 \\leq i<D$,\n\n$$\nP\\left(\\alpha_{i}\\right)=y_{i} . $$\n\nProof. This proof is based on the Wikipedia entry for Lagrange polynomials [45]. Given a sequence of values $\\alpha_{i}$ for $0 \\leq i<D$ s.t. $\\alpha_{i} \\neq \\alpha_{j}, i \\neq j$, the Lagrange basis for polynomials of degree $<D$ for these values is the set of each polynomials $\\left\\{p_{0}(X), p_{1}(X), \\ldots p_{D-1}(X)\\right\\}$ each of degree $D-1$. Each basis polynomial are defined as:\n\n$$\np_{i}(X)=\\frac{X-\\alpha_{0}}{\\alpha_{i}-\\alpha_{0}} \\cdots \\frac{X-\\alpha_{i-1}}{\\alpha_{i}-\\alpha_{i-1}} \\cdot \\frac{X-\\alpha_{i+1}}{\\alpha_{i}-\\alpha_{i+1}} \\cdots \\frac{X-\\alpha_{D-1}}{\\alpha_{i}-\\alpha_{D-1}}=\\prod_{\\substack{0 \\leq j<D \\\\ j \\neq i}} \\frac{X-\\alpha_{j}}{\\alpha_{i}-\\alpha_{j}}\n$$\n\nBy definition,\n\n$$\np_{i}\\left(\\alpha_{j}\\right)= \\begin{cases}1 & \\text { for } j=i \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nThe Lagrange interpolating polynomial for those nodes through the corresponding values $y_{i}$ for $0 \\leq i<D$ is the linear combination:\n\n$$\nP(X)=\\sum_{i=0}^{D-1} y_{i} \\cdot p_{i}(X)\n$$\n\nBy (9), for all $0 \\leq i<D$ :\n\n$$\nP\\left(\\alpha_{i}\\right)=y_{i} \\text {. }\n$$\n\nFinally, the interpolating polynomial is unique. Assume there is another polynomial $M(X)$ of degree $<D$ such that $M\\left(\\alpha_{i}\\right)=y_{i}$ for all $0 \\leq i<D$. Then the difference $M(X)-P(X)$ is 0 at $D$ distinct points $\\alpha_{i}$ for $0 \\leq i<D$. And the only polynomials of degree $<D$ with more than $D-1$ roots is the 0 polynomial. So, $M(X)=P(X)$. Theorem 5. Let $D_{X}, D_{Y} \\geq 1$ be integers. Given values $y_{i j}$ for $0 \\leq i<D_{X}, 0 \\leq j<D_{Y}$ and $D_{X}$ distinct points $\\left(\\alpha_{0}, \\ldots, \\alpha_{D_{X}-1}\\right), D_{Y}$ distinct points $\\left(\\beta_{0}, \\ldots, \\beta_{D_{Y}-1}\\right)$ there exists a unique bivariate polynomial $P(X, Y)$ with $\\operatorname{deg}_{X}(P)<D_{X}$, $\\operatorname{deg}_{Y}(P)<D_{Y}$, such that for all $0 \\leq i<D_{X}$, $0 \\leq j<D_{Y}$ :\n\n$$\nP\\left(\\alpha_{i}, \\beta_{j}\\right)=y_{i j}\n$$\n\nProof. Define\n\n$$\nP(X, Y)=\\sum_{j=0}^{m} \\sum_{i=0}^{n} y_{i j} \\cdot p_{i}(X) \\cdot \\bar{p}_{j}(Y)\n$$\n\nwhere $p_{i}$ and $\\bar{p}_{j}$ are Lagrange basis polynomials defined in the proof of Theorem 4 such that for $0 \\leq i, k<D_{X}$,\n\n$$\np_{i}\\left(\\alpha_{k}\\right)= \\begin{cases}1 & \\text { for } i=k \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nand for $0 \\leq j, \\ell<D_{Y}$\n\n$$\n\\bar{p}\\left(\\beta_{\\ell}\\right)= \\begin{cases}1 & \\text { for } k=\\ell \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nFrom above, we have for all $i, j, k, \\ell$\n\n$$\np_{i}\\left(\\alpha_{k}\\right) \\cdot \\bar{p}_{j}\\left(\\beta_{\\ell}\\right)= \\begin{cases}1 & \\text { for } i=k \\text { and } j=\\ell \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nThen, for all $0 \\leq i<D_{X}, 0 \\leq j<D_{Y}$ :\n\n$$\nP\\left(\\alpha_{i}, \\beta_{j}\\right)=y_{i j}\n$$\n\nBy definition of Lagrange basis polynomials, $\\operatorname{deg}_{X}(P)<D_{X}$ and $\\operatorname{deg}_{Y}(P)<D_{Y}$. Finally, the interpolating polynomial is unique. Assume there is another polynomial $M(X, Y)$ with $\\operatorname{deg}_{X}(M)<D_{X}$ and $\\operatorname{deg}_{Y}(M)<D_{Y}$ such that $M\\left(\\alpha_{i}, \\beta_{j}\\right)=y_{i j}$ for all $0 \\leq i<D_{X}$ and $0 \\leq j<D_{Y}$. Then the difference $M(X, Y)-P(X, Y)$ is 0 at $D_{X} \\cdot D_{Y}$ distinct points, $\\left(\\alpha_{i}, \\beta_{j}\\right)$ for $0 \\leq i<D_{X}, 0 \\leq j<D_{Y}$. And the only polynomial with $\\operatorname{deg}_{X}<D_{X}$ and $\\operatorname{deg}_{Y}<D_{Y}$ that has $D_{X} \\cdot D_{Y}$ roots is the 0 polynomial. ## D.1.2 Notation\n\nHere we recall notation we will use from [12]. 1. The class of Monarch matrices is defined in appendix C of [12] as $\\mathcal{M}^{(b, N)}$ which are $N \\times N$ matrices with block size $b$ for any integer $0 \\leq b \\leq N$ that divides $N$. When $b=\\sqrt{N}$ we drop $b$ from the notation giving $\\left(i_{1}, i_{0}\\right)$ and $\\left(j_{1}, j_{0}\\right)$. For example, this is used in Proof of Corollary 2. 2. Row index $i$ can be represented as $\\left(i_{1}, i_{0}\\right)_{b}$. Which gives $i=i_{1} b+i_{0}$. 3. Similarly, column index $j$ can be represented as $\\left(j_{1}, j_{0}\\right)_{b}$. Which gives $j=j_{1} b+j_{0}$. Note that when $b=\\sqrt{N}, j_{1}=k(j)$ and $j_{0}=m(j)$. We choose to use the $\\left(j_{1}, j_{0}\\right)$ notation here since that notation is easier to generalize for $p>2$. 4. $\\overline{\\mathbf{L}} \\in \\mathcal{D B}^{(b, N)}$ is an $N \\times N$ matrix with $b \\times b$ blocks that are all diagonal matrices. 5. $\\mathbf{R} \\in \\mathcal{B D}^{(b, N)}$ meaning it's a block diagonal $N \\times N$ matrix with block size $b \\times b$. 6. We have a class of permutation matrices defined as $\\sigma_{(b, N)}(i)=i_{0} \\cdot \\frac{N}{b}+i_{1}$. This can be denoted by an $N \\times N$ matrix, $P_{(b, N)}$, where the $i^{\\text {th }}$ row is $\\mathbf{e}_{\\sigma(b, N)(i)}$. 7. We'll use $i$ or pair notation $\\left(i_{1}, i_{0}\\right)_{b}$ to denote the rows, and $j$ or pair notation $\\left(j_{1}, j_{0}\\right)_{b}$ to denote columns. It should be clear from context which one we're using. For any $0 \\leq j_{1}<\\sqrt{N}$, let $\\ell_{j_{1}}(X, Y)$ be an arbitrary bivariate polynomial with $\\operatorname{deg}_{X}\\left(\\ell_{j_{1}}\\right)$, $\\operatorname{deg}_{Y}\\left(\\ell_{j_{1}}\\right)<\\sqrt{N}$. For any $0 \\leq j_{1}, j_{0}<\\sqrt{N}$, let $r_{j_{1}, j_{0}}(Y)$ be an arbitrary univariate polynomial of degree $<\\sqrt{N}$. Let $A=\\left(\\alpha_{0}, \\ldots, \\alpha_{\\sqrt{N}-1}\\right), B=\\left(\\beta_{0}, \\ldots, \\beta_{\\sqrt{N}-1}\\right)$ each be a sequence of distinct eval points. Note that $A$ and $B$ need not be disjoint. From the proof of Theorem 3 in the Appendix C of the Monarch paper [12] we get,\n\n$$\n\\begin{aligned}\n\\mathbf{L} & =\\mathbf{P}_{(b, N)} \\cdot \\overline{\\mathbf{L}} \\cdot \\mathbf{P}_{(b, N)}^{\\top} \\\\\n& =\\mathbf{P}_{(b, N)} \\cdot \\overline{\\mathbf{L}} \\cdot \\mathbf{P}_{\\left(\\frac{N}{b}, N\\right)}\n\\end{aligned}\n$$\n\nTherefore,\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{L}} & =\\mathbf{P}_{(b, N)}^{\\top} \\cdot \\mathbf{L} \\cdot \\mathbf{P}_{(b, N)} \\\\\n& =\\mathbf{P}_{\\left(\\frac{N}{b}, N\\right)} \\cdot \\mathbf{L} \\cdot \\mathbf{P}_{(b, N)}\n\\end{aligned}\n$$\n\nDefine $\\mathcal{D B}$ and $\\mathcal{B D}$ as set of all such $\\overline{\\mathbf{L}}$ and $\\mathbf{R}$ matrices over $\\mathbb{R}^{N \\times N}$ where if $i_{0} \\neq k_{0}$\n\n$$\n\\overline{\\mathbf{L}}_{i_{1}, k_{1}}\\left[i_{0}, k_{0}\\right] \\stackrel{\\text { def }}{=} \\overline{\\mathbf{L}}\\left[\\left(i_{1}, i_{0}\\right)_{\\sqrt{N}},\\left(k_{1}, k_{0}\\right)_{\\sqrt{N}}\\right]=0\n$$\n\nand if $k_{1} \\neq j_{1}$\n\n$$\n\\mathbf{R}_{k_{1}, j_{1}}\\left[k_{0}, j_{0}\\right] \\stackrel{\\text { def }}{=} \\mathbf{R}\\left[\\left(k_{1}, k_{0}\\right)_{\\sqrt{N}},\\left(j_{1}, j_{0}\\right)_{\\sqrt{N}}\\right]=0\n$$\n\nPictorially, $\\overline{\\mathbf{L}}$ and $\\mathbf{R}$ look as follows:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-32.jpg?height=621&width=1174&top_left_y=1874&top_left_x=302)\n\nIn [12], Monarch matrices with block size $b=\\sqrt{N}, \\mathbf{M}^{\\prime}=\\overline{\\mathbf{L}} \\cdot \\mathbf{R}$, and thus for all $0 \\leq i_{1}, i_{0}, j_{1}, j_{0}<$ $\\sqrt{N}$ :\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right)_{\\sqrt{N}},\\left(j_{1}, j_{0}\\right)_{\\sqrt{N}}\\right]=\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right] \\cdot \\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right] . $$\n\nWe note that our definition of Monarch matrix $\\mathbf{M}$ in Section 3 is slightly different in that $\\mathbf{M}=\\mathbf{M}^{\\prime} \\mathbf{P}$ with $\\mathbf{M}^{\\prime}$ as defined in [12]. ## D. 2 Monarch Matrices and Bivariate Polynomial Evaluation\n\nGiven polynomials $\\ell_{j_{1}}(X, Y)$ for $0 \\leq j_{1}<\\sqrt{N}$, polynomials $r_{j_{1}, j_{0}}(Y)$ for $0 \\leq j_{1}, j_{0}<\\sqrt{N}$, evaluation points $A=\\left(\\alpha_{0}, \\ldots, \\alpha_{\\sqrt{N}-1}\\right) B=\\left(\\beta_{0}, \\cdots, \\beta_{\\sqrt{N}-1}\\right)$ (as in Appendix D.1.2), define the matrices $\\overline{\\mathbf{L}} \\in \\mathcal{D B}^{\\sqrt{N}, N}$ and $\\mathbf{R} \\in \\mathcal{B D}^{\\sqrt{N}, N}$ as:\n\n- For every $0 \\leq j_{1}, i_{1}, i_{0}<\\sqrt{N}$ :\n\n$$\n\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right] \\leftarrow \\ell_{j_{1}}\\left(\\alpha_{i_{1}}, \\beta_{i_{0}}\\right)\n$$\n\n- For every $0 \\leq j_{1}, j_{0}, i_{0}<\\sqrt{N}$ :\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right] \\leftarrow r_{j_{1}, j_{0}}\\left(\\beta_{i_{0}}\\right)\n$$\n\nNote that all entries of $\\overline{\\mathbf{L}}$ and $\\mathbf{R}$ not specified above are set to 0 . Let $f$ be the above function that maps coefficients of $\\ell_{j_{1}}(X, Y)$ (which are coefficients of monomials $X^{i_{1}} Y^{i_{0}}$ for all $0 \\leq i_{1}, i_{0}<\\sqrt{N}$ and hence represented by a matrix in $\\mathbb{R}^{\\sqrt{N} \\times \\sqrt{N}}$ ) and coefficients of $r_{j_{1}, j_{0}}(Y)$ (which are coefficients of monomials $Y^{i_{0}}$ for all $0 \\leq i_{0}<\\sqrt{N}$ and hence represented by a vector in $\\mathbb{R}^{\\sqrt{N}}$ ) for all $0 \\leq j_{1}, j_{0}<\\sqrt{N}$ to pairs of matrices in $\\mathcal{D} \\mathcal{B}^{\\sqrt{N}, N} \\times \\mathcal{B D}^{\\sqrt{N}, N}$.",
    "m2-47": "Theorem 6. Let $f$ be as defined above. Then $f$ is a bijection. Proof. To prove $f$ is bijection we must show $f$ is one-to-one and $f^{-1}$ is one-to-one (and exists). To show $f$ is one to one means each set of polynomials' coefficients given to $f$, will output a unique set of matrices $(\\overline{\\mathbf{L}}, \\mathbf{R}) \\in \\mathcal{D B}^{\\sqrt{N}, N} \\times \\mathcal{B D}^{\\sqrt{N}, N}$. This follows from (21), (22) and the known fact that polynomial evaluation is a function. Now, to show $f^{-1}$ exists and is one-to-one, we must show that there is a map from any pair $(\\overline{\\mathbf{L}}, \\mathbf{R}) \\in \\mathcal{D B}^{\\sqrt{N}, N} \\times \\mathcal{B D}^{\\sqrt{N}, N}$ to unique sets of polynomials, $\\bar{\\ell}, \\bar{r}$, with parameters as defined in Appendix D.1.2. Further, we need\n\n$$\n\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right]=\\bar{\\ell}_{j_{1}}\\left(\\alpha_{i_{1}}, \\beta_{i_{0}}\\right)\n$$\n\nand\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]=\\bar{r}_{j_{1}, j_{0}}\\left(\\beta_{i_{0}}\\right)\n$$\n\nWe will use Theorems 5 and 4 to show the existence of $\\bar{\\ell}_{j_{1}}$ and $\\bar{r}_{j_{1}, j_{0}}$ polynomials, giving us the mapping from the matrices to unique polynomials. We first show the existence of the unique polynomials in (24). Fix $0 \\leq j_{1}, j_{0}<\\sqrt{N}$. Then consider the values $0 \\leq i_{0}<\\sqrt{N}$ :\n\n$$\ny_{i_{0}} \\leftarrow \\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]\n$$\n\nThen by Theorem 4, there exists a unique polynomial of degree $<\\sqrt{N}$ (call it $\\bar{r}_{j_{1}, j_{0}}(Y)$ ) such that for all $0 \\leq i_{0}<\\sqrt{N}$,\n\n$$\n\\bar{r}_{j_{1}, j_{0}}\\left(\\beta_{i_{0}}\\right)=y_{i_{0}}\n$$\n\nwhich by (25) shows (24). Next we show the existence of the unique polynomials in (23). Fix $0 \\leq j_{1}<\\sqrt{N}$. Consider the values $0 \\leq i_{1}, i_{0}<\\sqrt{N}$ :\n\n$$\ny_{i_{1}, i_{0}} \\leftarrow \\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right]\n$$\n\nThen by Theorem 5 , there exists a unique bi-variate polynomial of $\\operatorname{deg}_{X}<\\sqrt{N}$ and $\\operatorname{deg}_{Y}<\\sqrt{N}$ (call it $\\bar{\\ell}_{j_{1}}(X, Y)$ ) such that for all $0 \\leq i_{1}, i_{0}<\\sqrt{N}$,\n\n$$\n\\bar{\\ell}_{j_{1}}\\left(\\alpha_{i_{1}}, \\beta_{i_{0}}\\right)=y_{i_{1}, i_{0}}\n$$\n\nwhich by (26) shows (23). Therefore $f$ is a bijection. We can now conclude:\nCorollary 1. For every matrix $\\mathbf{M}^{\\prime}$ as defined in (20), there exists unique polynomials $\\ell_{j_{1}}(X, Y)$ and $r_{j_{1}, j_{0}}(Y)$, such that for all $0 \\leq i_{1}, i_{0}, j_{1}, j_{0}<\\sqrt{N}$,\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right)_{\\sqrt{N}},\\left(j_{1}, j_{0}\\right)_{\\sqrt{N}}\\right]=\\ell_{j_{1}}\\left(\\alpha_{i_{1}}, \\beta_{i_{0}}\\right) \\cdot r_{j_{1}, j_{0}}\\left(\\beta_{i_{0}}\\right)\n$$\n\nProof. Follows from (20), (21), (22) and Theorem 6. ## D.2.1 Proof of Theorem 1\n\nWe begin with an immediate consequence of Corollary 1:\nCorollary 2. Let $A, B \\subset \\mathbb{C}$ such that $|A|=|B|=\\sqrt{N}$. Then the $j$ th column of $\\mathbf{M}^{\\prime}$ is the evaluation of the polynomial $\\ell_{j_{1}}(X, Y) \\cdot r_{j_{1}, j_{0}}(Y)$ over $A \\times B$. Proof. Observe that for fixed $j_{0}, j_{1}$ the right hand side of (28) is $\\ell_{j_{1}}(X, Y) \\cdot r_{j_{1}, j_{0}}(Y)$ evaluated at all $(\\alpha, \\beta) \\in A \\times B$. Thus, $\\left(j_{1}, j_{0}\\right)$ column is evaluation of $\\ell_{j_{1}}(X, Y) \\cdot r_{j_{1}, j_{0}}(Y)$ over points in $A \\times B$, as desired. Next, we state a generalization of Theorem 1 that follows from Corollary 2:\nCorollary 3. Let $A$ and $B$ be as in Corollary 2. For any vector $\\boldsymbol{u}, \\mathbf{M}^{\\prime} \\cdot \\boldsymbol{u}$ is $\\bar{u}(X, Y)$ evaluated at $A \\times B$. Further,\n\n$$\n\\bar{u}(X, Y)=\\sum_{0 \\leq j_{1}, j_{0}<\\sqrt{N}} u_{j_{1}, j_{0}} \\cdot \\ell_{j_{1}}(X, Y) \\cdot r_{j_{1}, j_{0}}(Y)\n$$\n\nwhere $\\ell$ and $r$ are defined by $\\mathbf{M}^{\\prime}$ as in Corollary 1. Proof. Follows from Corollary 2 and definition of matrix vector multiplication. In Theorem 1 and the following sections, we consider the polynomial evaluated over the basis polynomials defined by\n\n$$\n\\mathbf{M}=\\mathbf{M}^{\\prime} \\mathbf{P}\n$$\n\nCorollary 4. Let $A$ and $B$ be as in Corollary 2. For any vector $\\boldsymbol{u}$, and $\\mathbf{M} \\cdot \\mathbf{u}$ is $u(X, Y)$ evaluated at $A \\times B$. Further,\n\n$$\nu(X, Y)=\\sum_{0 \\leq j_{1}, j_{0}<\\sqrt{N}} u_{j_{0}, j_{1}} \\cdot \\ell_{j_{0}}(X, Y) \\cdot r_{j_{0}, j_{1}}(Y)\n$$\n\nwhere $\\ell$ and $r$ are defined by $\\mathbf{M}^{\\prime}$ as in Corollary 1. Proof. Follows from Corollary 3 and definition of M. Specifically, Theorem 1 is a special case of Corollary 4 where $\\ell_{j_{0}}(X, Y)=\\ell_{m(j)}(X, Y)$ and $r_{j}(Y)=r_{j_{0}, j_{1}}(Y)$. ## D.2.2 Proof of Theorem 2\n\nBy Corollary 4 , for all $0 \\leq j_{1}, i_{1}<\\sqrt{N}$,\n\n$$\n\\ell_{j_{0}}(X, Y), r_{j_{0}, j_{1}}(Y), \\bar{\\ell}_{j_{0}}(X, Y), \\bar{r}_{j_{0}, j_{1}}(Y)\n$$\n\nare the basis polynomials corresponding to $\\mathbf{M}_{1}$ and $\\mathbf{M}_{2}$. For the coefficient vector $\\mathbf{k}=\\left(k_{j_{1}, j_{0}}\\right)_{0 \\leq j_{1}, j_{0}<\\sqrt{N}}$ and similarly for $\\mathbf{u}=\\left(u_{j_{1}, j_{0}}\\right)_{0 \\leq j_{1}, j_{0}<\\sqrt{N}}$, we can construct two polynomials\n\n$$\n\\begin{aligned}\n& k(X, Y)=\\sum_{0 \\leq j_{1}, j_{0}<\\sqrt{N}} k_{j_{1}, j_{0}} \\cdot \\ell_{j_{0}}(X, Y) \\cdot r_{j_{0}, j_{1}}(Y) \\\\\n& u(X, Y)=\\sum_{0 \\leq j_{1}, j_{0}<\\sqrt{N}} u_{j_{1}, j_{0}} \\cdot \\bar{\\ell}_{j_{0}}(X, Y) \\cdot \\bar{r}_{j_{0}, j_{1}}(Y)\n\\end{aligned}\n$$\n\nwhose evaluation over $\\left(\\alpha_{i_{1}}, \\beta_{i_{0}}\\right)=\\left(\\omega^{i_{1}}, \\omega^{i_{0}}\\right)$ where recall as in Appendix D.1.1 $\\omega=e^{\\frac{2 \\pi \\iota}{\\sqrt{N}}}$, by Theorem 1 is equivalent to the products $\\mathbf{M}_{1} \\cdot \\mathbf{k}$ and $\\mathbf{M}_{2} \\cdot \\mathbf{u}$, respectively. Taking the component-wise product, $\\mathbf{y}=\\left(\\mathbf{M}_{1} \\cdot \\mathbf{k}\\right) \\odot\\left(\\mathbf{M}_{2} \\cdot \\mathbf{u}\\right)$, the entry at $i=\\left(i_{1}, i_{0}\\right)$ is given by\n\n$$\n\\mathbf{y}\\left[\\left(i_{1}, i_{0}\\right)\\right]=k\\left(\\omega^{i_{1}}, \\omega^{i_{0}}\\right) \\cdot u\\left(\\omega^{i_{1}}, \\omega^{i_{0}}\\right)\n$$\n\nNoting that the element of $A$, i.e. the $\\sqrt{N}$-th roots of unity, satisfy $Z^{\\sqrt{N}}=1$ means that the above are evaluations of\n\n$$\nh(X, Y)=k(X, Y) \\cdot u(X, Y) \\quad \\bmod \\left(X^{\\sqrt{N}}-1, Y^{\\sqrt{N}}-1\\right)\n$$\n\nat $A \\times A$. Finally, Theorem 1 and the fact that $\\mathbf{M}_{0}^{-1}$ exists implies $\\mathbf{M}_{0} \\cdot \\mathbf{y}$ is polynomial interpolation into basis polynomials corresponding to $\\mathbf{M}_{0}$. (Here we use the well known fact polynomial interpolation is the inverse of polynomial evaluation). ## D. 3 Proof of Theorem 3\n\nWe review some concepts in Appendix D.3.1. In Appendix D.3.2, we discuss square matrices and causality in terms of operations on univariate polynomials. This allows us to define a general class of operators for causal 1D convolution. In Appendix D.3.3, we give a class of matrices suitable for perform causal Monarch convolution. Specifically, we prove Theorem 3. ## D.3.1 Review\n\nConsider the linear operation on an input vector $\\mathbf{u}$ :\n\n$$\n\\mathbf{y}=\\mathbf{A} \\cdot \\mathbf{u}\n$$\n\nWe say that the map is causal to mean the entry $\\mathbf{y}[i]$ only depends on $\\mathbf{u}[0], \\mathbf{u}[1], \\ldots \\mathbf{u}[i]$. This will be the case when $\\mathbf{A}$ is a lower triangular matrix (we index the top left entry of $\\mathbf{A}$ as $(0,0)$ ). When $\\mathbf{A}$ is a lower triangular Toeplitz matrix with entries corresponding to some coefficient vector $\\mathbf{k}$, this operation is exactly the 1 D convolution\n\n$$\n\\mathbf{y}=\\mathbf{k} * \\mathbf{u}=\\left(\\mathbf{F}_{2 n}^{-1} \\cdot\\left(\\left(\\mathbf{F}_{2 n} \\cdot \\mathbf{k}^{\\prime}\\right) \\circ\\left(\\mathbf{F}_{2 n} \\cdot \\mathbf{u}^{\\prime}\\right)\\right)\\right)[0: n-1]\n$$\n\nwhere $\\mathbf{k}^{\\prime}=\\left(\\mathbf{k}, \\mathbf{0}_{n}\\right), \\mathbf{u}^{\\prime}=\\left(\\mathbf{u}, \\mathbf{0}_{n}\\right)$, and $\\mathbf{F}_{n}$ is the $n \\times n$ DFT matrix. Definition 4. For a matrix $\\overline{\\mathbf{M}} \\in \\mathbb{R}^{n \\times n}$, let us define the map\n\n$$\n\\mathbf{y}=\\mathbf{M}^{-1}(\\overline{\\mathbf{M}} \\cdot \\mathbf{k} \\odot \\overline{\\mathbf{M}} \\cdot \\mathbf{u})\n$$\n\nas matrix convolution. When $\\overline{\\mathbf{M}}$ is a Monarch matrix, (31) is called Monarch convolution. In this section, we are interested in determining large subclasses of matrices $\\overline{\\mathbf{M}}$ such that for any coefficient vector $\\mathbf{k}$, (31) is causal in $\\mathbf{u}$. We provide a class of matrices for which Monarch convolution is causal. We note that for general Monarch matrix M, (31) is not causal in u. By Theorem 2, we have\n\n$$\ny(X, Y)=k(X, Y) \\cdot u(X, Y) \\quad \\bmod \\left(X^{\\sqrt{N}-1}, Y^{\\sqrt{N}-1}\\right)\n$$\n\nThis is not causal because the $\\bmod \\left(X^{\\sqrt{N}-1}, Y^{\\sqrt{N}-1}\\right)$ term condenses higher order terms into lower order terms, hence the $\\mathbf{y}[i]$ wouldn't just depend on input information up to value $i$. ## D.3.2 Univariate Matrix Convolutions\n\nWe start with a couple of notation assumptions. Assumption 1. $N$ is a perfect square. Assumption 2. We will not use pair notation for this subsection since throughout we have $i=i_{1} \\sqrt{N}+i_{0}$ and $j=j+1 \\sqrt{N}+j_{0}$. In order to discuss square matrices in terms of univariate polynomials, we give univariate analogs of Theorem 1 and Theorem 2 for general univariate basis. With an eye toward towards performing causal convolution, we restrict our analysis to certain classes of univariate polynomials. We first define matrices whose $j^{\\text {th }}$ columns are the evaluation of a minimum degree $j$ (and maximum degree $N-1$ ) polynomial (recall Definition 1). We generalize Theorem 3 to such matrices. Lemma 1. For sequence of points $A=\\left\\{1, \\omega_{N}, \\cdots \\omega_{N}^{N-1}\\right\\}$ where $\\omega_{N}$ is the $N^{\\text {th }}$ root of unity, let $\\overline{\\mathbf{M}}$ be defined as\n\n$$\n\\overline{\\mathbf{M}}[i, j]=\\bar{q}_{j}\\left(\\omega_{N}^{i}\\right)\n$$\n\nwhere $\\bar{q}_{j}(Z)$ is defined as in Definition 1. Then for any vector $\\boldsymbol{v} \\in \\mathbb{R}^{N}, \\overline{\\mathbf{M}} \\cdot \\boldsymbol{v}$ is equivalent to evaluating the polynomial\n\n$$\nv(Z)=\\sum_{j=0}^{N-1} v_{j} \\cdot \\bar{q}_{j}(Z)\n$$\n\nat $\\left\\{1, \\omega_{N}, \\cdots \\omega_{N}^{N-1}\\right\\}$. Proof. By our definition of $\\overline{\\mathbf{M}}$, the column $\\overline{\\mathbf{M}}[:, j]$ is exactly the evaluation of the polynomial $\\bar{q}_{j}(Z)$ at each point in $A$. The claimed result comes from the definition of matrix vector multiplication and (33). Note that $\\overline{\\mathbf{M}}$ or any M's in this sub-section are not necessarily Monarch matrices. Next, we state the following intermediate result:\nProposition 1. Let $A$ be the set of the $N$-th roots of unity. Then for $\\mathbf{M}_{1}, \\mathbf{M}_{2}$ defined as in (32)\n\n$$\n\\mathbf{y}=\\left(\\mathbf{M}_{1} \\cdot \\mathbf{k}\\right) \\odot\\left(\\mathbf{M}_{2} \\cdot \\mathbf{u}\\right)\n$$\n\nis the same as evaluating the polynomial\n\n$$\np(Z):=k(Z) \\cdot u(Z) \\quad \\bmod \\left(Z^{N}-1\\right)\n$$\n\nover $A$ where $k(Z), u(Z)$ are of the form (33), corresponding to $\\mathbf{M}_{1}$ and $\\mathbf{M}_{2}$, respectively. In other words, for any $0 \\leq i<N$,\n\n$$\n\\mathbf{y}[i]=p\\left(\\omega_{N}^{i}\\right)\n$$\n\nProof. This result follows from Lemma 1 and the definition of the Hadamard product. Next, we state a re-interpretation of $\\mathbf{M}^{-1} \\mathbf{y}$ :\nProposition 2. Let $\\mathbf{M}$ be a full rank matrix whose columns are the evaluations of the basis polynomials $\\bar{q}_{j}(Z)$ from Definition 1 for $0 \\leq j<N$, and let $\\mathbf{y} \\in \\mathbb{R}^{N}$ be an arbitrary vector. If $\\mathbf{u}=\\mathbf{M}^{-1} \\mathbf{y}$, then for all $0 \\leq i<N$\n\n$$\n\\mathbf{y}[i]=u\\left(\\omega^{i}\\right)\n$$\n\nwhere $u(Z)$ is the same as in Lemma 1 for $\\mathbf{M}$. In other words, $\\mathbf{M}^{-1} \\mathbf{y}$ is the polynomial interpolaton problem for the polynomial basis $\\bar{q}_{j}(Z)$ for $0 \\leq j<N$.",
    "m2-48": "Proof. This follows from Lemma 1 and the fact that $\\mathbf{M}$ is invertible. From Propositions 1 and 2, we get the following generalization of Theorem 2:\nTheorem 7. For matrices $\\mathbf{M}_{0}, \\mathbf{M}_{1}, \\mathbf{M}_{2}$ as defined above, the operation\n\n$$\n\\mathbf{f}=\\mathbf{M}_{0}^{-1} \\cdot\\left(\\left(\\mathbf{M}_{1} \\cdot \\mathbf{k}\\right) \\circ\\left(\\mathbf{M}_{2} \\cdot \\mathbf{u}\\right)\\right)\n$$\n\nis equivalent to representing the polynomial\n\n$$\nf(Z)=k(Z) \\cdot u(Z) \\quad \\bmod \\left(Z^{N}-1\\right)\n$$\n\nin terms of the basis polynomials\n\n$$\n\\hat{q}_{j}(Z) \\text { for } j=0, \\ldots, N-1\n$$\n\nwhere $k(Z), u(Z)$ are defined as in Lemma 1 in terms of the basis polynomials corresponding to $\\mathbf{M}_{1}$ and $\\mathbf{M}_{2}$, respectively, and $\\left(\\hat{q}_{j}(Z)\\right)_{0 \\leq j<N}$ corresponds to $\\mathbf{M}_{0}$.",
    "m2-49": "Proof. Follows from Propositions 1 and 2. Now we give the class of matrices from which we can build a causal map.",
    "m2-50": "Specifically we prove a generalization of Theorem 3:\n\nTheorem 8. Let $n \\geq 1$, let $N=\\left\\lceil\\left.\\sqrt{2 n}\\right|^{2}\\right.$. Then define the basis polynomial $\\bar{q}_{j}(Z)$ to have minimum degree $j$ and maximum degree $n-1$ for $0 \\leq j<n$, and for $n \\leq j<N, \\bar{q}_{j}(Z)$ has minimum degree $j$ and maximum degree $N-1$. For all $\\mathbf{M}_{N}$ with basis columns defined by $\\left(\\bar{q}_{j}(Z)\\right)_{0 \\leq j<N}$ as above, the operation\n\n$$\n\\mathbf{u} \\mapsto\\left(\\mathbf{M}_{N}^{-1}\\left(\\mathbf{M}_{N} \\cdot\\left(\\mathbf{k}, \\mathbf{0}_{N-n}\\right) \\circ \\mathbf{M}_{N} \\cdot\\left(\\mathbf{u}, \\mathbf{0}_{N-n}\\right)\\right)\\right)[0: n-1]\n$$\n\ngives a causal map. Proof. To prove this is causal means each entry, $\\mathbf{f}[i]$ is dependent only on $\\mathbf{u}[0], \\mathbf{u}[1], \\ldots \\mathbf{u}[i]$, where $\\mathbf{f}=\\left(\\mathbf{M}_{N}^{-1}\\left(\\mathbf{M}_{N} \\cdot\\left(\\mathbf{k}, \\mathbf{0}_{N-n}\\right) \\circ \\mathbf{M}_{N} \\cdot\\left(\\mathbf{u}, \\mathbf{0}_{N-n}\\right)\\right)\\right)$. By Theorem 7 , we have\n\n$$\nf(Z)=k(Z) \\cdot u(Z) \\quad \\bmod \\left(Z^{N}-1\\right)\n$$\n\nwhere\n\n$$\nk(Z)=\\sum_{j=0}^{n-1} k_{j} \\bar{q}_{j}(Z) \\quad u(Z)=\\sum_{j^{\\prime}=0}^{n-1} u_{j^{\\prime}} \\bar{q}_{j^{\\prime}}(Z)\n$$\n\nSince $\\operatorname{deg}(k(Z) \\cdot u(Z)) \\leq 2 n-2 \\leq N-2$, this is equivalent to\n\n$$\n\\begin{aligned}\nf(Z) & =k(Z) \\cdot u(Z) \\\\\n& =\\sum_{j, j^{\\prime}=0}^{n-1} k_{j^{\\prime}} \\cdot u_{j} \\cdot \\bar{q}_{j^{\\prime}}(Z) \\cdot \\bar{q}_{j}(Z) . \\end{aligned}\n$$\n\nBy our choice of $\\bar{q}_{j}$, we ensure that $\\bar{q}_{j} \\cdot \\bar{q}_{j^{\\prime}}$ has minimum degree $j+j^{\\prime}$ and $\\operatorname{deg}\\left(\\bar{q}_{j} \\cdot \\bar{q}_{j^{\\prime}}\\right) \\leq 2 n-2<N$ for any $0 \\leq j, j^{\\prime}<n$. Then by Lemma 2 (see below), there exists coefficients $\\alpha_{j+j^{\\prime}, i^{\\prime}}$ such that,\n\n$$\n\\begin{aligned}\nf(Z) & =\\sum_{j, j^{\\prime}=0}^{n-1} k_{j^{\\prime}} \\cdot u_{j} \\cdot \\sum_{i^{\\prime}=j+j^{\\prime}}^{N-1} \\alpha_{j+j^{\\prime}, i^{\\prime}} \\cdot \\bar{q}_{i^{\\prime}}(Z) \\\\\n& \\left.=\\sum_{i=0}^{N-1}\\left(\\sum_{\\substack{j, j^{\\prime}=0 \\\\\nj+j^{\\prime} \\leq i}}^{n-1} \\alpha_{j+j^{\\prime}, i} \\cdot k_{j^{\\prime}} \\cdot u_{j}\\right)\\right) \\cdot \\bar{q}_{i}(Z) . \\end{aligned}\n$$\n\nIf we define\n\n$$\nf(Z)=\\sum_{i=0}^{N-1} f_{i} \\cdot \\bar{q}_{i}(Z)\n$$\n\nthen for $0 \\leq i<n$, we get:\n\n$$\nf_{i}=\\sum_{\\substack{j, j^{\\prime}=0 \\\\ j+j^{\\prime} \\leq i}}^{n-1} \\alpha_{j+j^{\\prime}, i} \\cdot k_{j^{\\prime}} \\cdot u_{j}\n$$\n\nNote that $f_{i}$ only depends on $\\mathbf{u}[0], \\mathbf{u}[1], \\ldots \\mathbf{u}[i]$, as desired.",
    "m2-51": "We used the following lemma in the proof of Theorem 8. Lemma 2. Let $\\bar{q}_{j}(Z)$ be defined as in Theorem 8. Then for any $0 \\leq j, j^{\\prime}<n$,\n\n$$\n\\bar{q}_{j}(Z) \\cdot \\bar{q}_{j^{\\prime}}(Z)=\\sum_{i=j+j^{\\prime}}^{N-1} \\alpha_{j+j^{\\prime}, i} \\cdot \\bar{q}_{i}(Z)\n$$\n\nfor some set of coefficients $\\alpha_{j+j^{\\prime}, i}$. Proof. We first note that by our choice of $\\bar{q}_{j}$, the minimum degree of $\\bar{q}_{j}(Z) \\cdot \\bar{q}_{j^{\\prime}}(Z)$ is $j+j^{\\prime}$, and $\\operatorname{deg}\\left(\\bar{q}_{j}(Z) \\cdot \\bar{q}_{j^{\\prime}}(Z)\\right) \\leq 2 n-2 \\leq N-1$. Our claim follows from that fact that any polynomial $p_{d}(Z)$ of minimum degree $d$ and $\\operatorname{deg}\\left(p_{d}\\right)<N$ can be expressed as a linear combination of $\\bar{q}_{d}(Z), \\bar{q}_{d+1}(Z), \\ldots, \\bar{q}_{N-1}(Z)$\n\n## D.3.3 Causal Monarch Convolutions\n\nIn this section we will prove Theorem 3. We will do so by showing that the basis polynomials $q_{j}(Z)$ as defined in Theorem 3 are a special case of the basis polynomials $\\bar{q}_{j}(Z)$ as defined in Theorem 8 .",
    "m2-52": "We start with a couple of notation assumptions. Assumption 3. In this sub-section we will using block size $b=\\sqrt{N}$ therefore, we are dropping the block size from index notation. For example, $\\left(i_{1}, i_{0}\\right)_{\\sqrt{N}}$ becomes $\\left(i_{1}, i_{0}\\right)$ for this section. Assumption 4. Permutation matrices in this subsection are all the same $\\mathbf{P}_{(\\sqrt{N}, N)}$ so we drop the subscript and just use $\\mathbf{P}$. Definition 5. Define\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z) \\stackrel{\\text { def }}{=} \\ell_{j_{1}}(Z) \\cdot r_{j_{1}, j_{0}}\\left(Z^{\\sqrt{N}}\\right)\n$$\n\n$\\ell_{j_{1}}(Z)$ has minimum degree $j_{1}$, and $r_{j_{1}, j_{0}}(Z)$ has minimum degree $j_{0}$. All polynomials $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ have maximum degree $\\leq N-1$. Next we argue that the above basis polynomials have a specific minimum degree. Lemma 3. Polynomial $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ as defined in equation (36) has minimum degree $j_{0} \\sqrt{N}+j_{1}$. Proof. We need to show that $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ is a minimum degree $j_{0} \\sqrt{N}+j_{1}$ polynomial. Note that\n\n$$\n\\begin{aligned}\nq_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z) & =\\ell_{j_{1}}(Z) \\cdot r_{j_{1}, j_{0}}\\left(Z^{\\sqrt{N}}\\right) \\\\\n& =Z^{\\sqrt{N}-1} \\cdot \\tilde{\\ell}_{\\sqrt{N}-1-j_{1}}\\left(\\frac{1}{Z}\\right) \\cdot Z^{(\\sqrt{N}-1) \\cdot \\sqrt{N}} \\cdot \\tilde{r}_{\\sqrt{N}-1-j_{0}}\\left(\\frac{1}{Z^{\\sqrt{N}}}\\right)\n\\end{aligned}\n$$\n\nwhere $\\tilde{\\ell}_{\\sqrt{N}-1-j_{1}}(\\cdot)$ has degree $\\sqrt{N}-1-j_{1}$ and $\\tilde{r}_{\\sqrt{N}-1-j_{0}}(\\cdot)$ has degree $\\sqrt{N}-1-j_{0}$. Simplifying we get\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)=Z^{N-1} \\cdot \\tilde{\\ell}_{\\sqrt{N}-1-j_{1}}\\left(\\frac{1}{Z}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-1-j_{0}}\\left(\\frac{1}{Z^{\\sqrt{N}}}\\right)\n$$\n\nThis claim follows since $\\tilde{\\ell}_{\\sqrt{N}-1-j_{1}}(Y) \\cdot \\tilde{r}_{j_{1}, j_{0}}\\left(Y^{\\sqrt{N}}\\right)$ has degree $=\\left(\\sqrt{N}-1-j_{1}\\right)+\\left(\\sqrt{N}-1-j_{0}\\right) \\cdot \\sqrt{N}=$ $(N-1)-\\left(j_{0} \\sqrt{N}+j_{1}\\right)$\n\n[^2]Note that the polynomial $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ has minimum degree $j_{0} \\sqrt{N}+j_{1}$. This is not $j_{1} \\sqrt{N}+j_{0}$ as in defined in equation (1), we will talk more about this soon. Next, we observe that the polynomials in (36) define a matrix that satisfies (20). Lemma 4. Let $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ for $0 \\leq j_{1} \\sqrt{N}+j_{0}<n$ be as in (36). Define\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right)\n$$\n\nThen $\\mathbf{M}^{\\prime}$ satisfies (20). Proof. If we evaluate the polynomials $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ at $\\omega_{N}^{i}$ for $0 \\leq i<N$, we get\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)}\\left(\\omega_{N}^{i}\\right)=\\ell_{j_{1}}\\left(\\omega_{N}^{i}\\right) \\cdot r_{j_{1}, j_{0}}\\left(\\omega_{N}^{i \\sqrt{N}}\\right)\n$$\n\nSince $\\omega_{N}^{i_{N}}=\\omega_{N}^{i_{1} N+i_{0} \\sqrt{N}}=\\omega_{N}^{i_{0} \\sqrt{N}}=\\omega_{\\sqrt{N}}^{i_{0}}$, we get\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\ell_{j_{1}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right) \\cdot r_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nThe above corresponds to how we define (20) since we have\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right] \\cdot \\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]\n$$\n\nwith\n\n$$\n\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right]=\\ell_{j_{1}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right)\n$$\n\nand\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]=r_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nRecall from Lemma 3 that $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}$ has minimum degree $j_{0} \\sqrt{N}+j_{1}$. For causality we need $q_{\\left(j_{1}, j_{0}\\right)}(Z)$ to have degree minimum $j_{1} \\sqrt{N}+j_{0}$ (then the polynomials will satisfy the minimum degree requirements from (1)). Therefore, we permute the columns of $\\mathbf{M}^{\\prime}$,\n\n$$\n\\mathbf{M}=\\mathbf{M}^{\\prime} \\cdot \\mathbf{P}\n$$\n\nNote that the above $\\mathbf{M}=\\mathbf{P L P R P}$ and is indeed a Monarch matrix as defined in Section 3. Note that the basis polynomials of $\\mathbf{M}$ are defined as,\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)}(Z)=\\ell_{j_{0}}(Z) \\cdot \\tilde{r}_{j_{0}, j_{1}}\\left(Z^{\\sqrt{N}}\\right)\n$$\n\nWe note that the above is same as $q_{j}(Z)$ defined in (5) where $j=j_{1} \\sqrt{N}+j_{0}$ with the correction in (5) that $q_{j}(Z)=\\ell_{m(j)} \\cdot r_{j}\\left(Z^{\\sqrt{N}}\\right)$ where $r_{j}(Y)=\\tilde{r}_{j_{0}, j_{1}}(Y)$.",
    "m2-53": "We are finally ready to prove Theorem 3 . Corollary 5 (Theorem 3 restated). Let $N=\\left\\lceil\\sqrt{2 n}^{2}\\right.$. Define $\\mathbf{M}_{N}$ by $q_{\\left(j_{1}, j_{0}\\right)}$ as in (38). Then,\n\n$$\n\\mathbf{u} \\mapsto\\left(\\mathbf{M}_{N}^{-1}\\left(\\mathbf{M}_{N} \\cdot\\left(\\mathbf{k}, \\mathbf{0}_{N-n}\\right) \\circ \\mathbf{M}_{N} \\cdot\\left(\\mathbf{u}, \\mathbf{0}_{N-n}\\right)\\right)\\right)[0: n-1]\n$$\n\ngives a causal map. Proof. Due to using $q$ polynomials as in (38), by Lemma 3 the degree of the $\\left(j_{1}, j_{0}\\right)^{\\text {th }}$ column is a polynomial with minimum degree $j_{1} \\sqrt{N}+j_{0} .{ }^{4}$ This implies that these basis polynomials are a subset of the more general causal maps in Theorem 8, which proves the claim. [^3]\n## D. 4 Block Algorithms for Complex Numbers and Block Size $\\sqrt{N}$\n\nIn the following subsections we restate the results in Section D.3.3 in terms of block operations. As mentioned earlier, this is so that we can do computations on Monarch matrices using only GEMM operations (and simple data movement operations like permutations). In Appendix D.4.1 we consider arbitrary Monarch matrices and in Appendix D.4.2 we consider the sub-class of Monarch matrices corresponding to Theorem 3. ## D.4.1 General Block Monarch Convolution\n\nIn this subsection we re-state general Monarch convolutions in terms of block operations. Recall from equation (22) we defined the block diagonal matrix $\\mathbf{R}$ as follows $\\left(0 \\leq i_{0}, j_{1}, j_{0}<\\sqrt{N}\\right)$ :\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right] \\leftarrow \\tilde{r}_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nwhere $\\operatorname{deg}\\left(\\tilde{r}_{j_{1}, j_{0}}\\right)<\\sqrt{N}$. To do so, we will first work with $\\mathbf{M}_{N}^{\\prime}$ such that $\\mathbf{M}_{N}=\\mathbf{M}_{N}^{\\prime} \\cdot \\mathbf{P}$. We want to express Monarch matrices, $\\mathbf{M}_{N}$, as univariate polynomial evaluation over $\\left\\{1, \\omega_{N}, \\ldots, \\omega_{N}^{N-1}\\right\\}$. Towards that end, define\n\n$$\nr_{j_{1}, j_{0}}(Z)=\\tilde{r}_{j_{1}, j_{0}}\\left(Z^{\\sqrt{N}}\\right)\n$$\n\nBy simple observation that $\\omega_{N}^{\\left(i_{1} \\sqrt{N}+i_{0}\\right) \\sqrt{N}}=\\omega_{N}^{i_{0} \\sqrt{N}}=\\omega_{\\sqrt{N}}^{i_{0}}$, we have\n\n$$\nr_{j_{1}, j_{0}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right)=\\tilde{r}_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nIn other words we have,\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]=r_{j_{1}, j_{0}}\\left(\\omega_{N}^{i}\\right)\n$$\n\nFix $0 \\leq j_{1}, j_{0}<\\sqrt{N}$ so we're looking at the $j_{0}^{\\text {th }}$ column of block $\\mathbf{R}_{j_{1}, j_{1}}$, going down the column we evaluate the polynomial $\\tilde{r}_{j_{1}, j_{0}}$ at points $\\left(1, \\omega_{\\sqrt{N}}, \\ldots, \\omega_{\\sqrt{N}}^{\\sqrt{N}}-1\\right)$. Which is equivalent to a matrix multiplication of Fourier matrix of size $\\sqrt{N}$ and a matrix of the coefficients of the $\\tilde{r}$ polynomials. ![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-41.jpg?height=375&width=945&top_left_y=1748&top_left_x=384)\n\nSo we can think of the blocks of $\\mathbf{R}$ as a Fourier matrix times a coefficient matrix. In other words, let us define a matrix $\\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ which will hold $\\sqrt{N}$ coefficient blocks $\\widetilde{\\mathbf{R}}_{0}, \\widetilde{\\mathbf{R}}_{1}, \\ldots \\widetilde{\\mathbf{R}}{ }_{\\sqrt{N}-1} \\in$ $\\mathbb{R}^{\\sqrt{N} \\times \\sqrt{N}}$ such that,\n\n$$\n\\widetilde{\\mathbf{R}}_{j_{1}}\\left[a, j_{0}\\right]=\\tilde{r}_{j_{1}, j_{0}}[a]\n$$\n\nwhere\n\n$$\n\\tilde{r}_{j_{1}, j_{0}}(Y)=\\sum_{a=0}^{\\sqrt{N}-1} \\tilde{r}_{j_{1}, j_{0}}[a] \\cdot Y^{a}\n$$\n\nThen we define\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}=\\mathbf{F}_{\\sqrt{N}} \\cdot \\widetilde{\\mathbf{R}}_{j_{1}}\n$$\n\nNext, we restate the above as product of two block diagonal matrices:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-42.jpg?height=321&width=1199&top_left_y=428&top_left_x=317)\n\nIn other words, we have\n\n$$\n\\mathbf{R}=\\operatorname{diag} \\underbrace{\\left(\\mathbf{F}_{\\sqrt{N}}, \\ldots, \\mathbf{F}_{\\sqrt{N}}\\right)}_{\\sqrt{N} \\text { times }} \\cdot \\operatorname{diag}\\left(\\widetilde{\\mathbf{R}}_{0}, \\ldots, \\widetilde{\\mathbf{R}}_{\\sqrt{N}-1}\\right)\n$$\n\nEquation (21) defined $\\mathbf{L}$ as evaluation of bivariate polynomials. We now wish to do the same with univariate polynomials. Recall that $\\overline{\\mathbf{L}}=\\mathbf{P L P}$. We define the $i_{0}^{\\text {th }}$ diagonal block $\\left(0 \\leq i_{1}, i_{0}, j_{1}<\\sqrt{N}\\right)$ for $\\mathbf{L}$ as:\n\n$$\n\\mathbf{L}_{i_{0}, i_{0}}\\left[i_{1}, j_{1}\\right]=\\ell_{j_{1}}\\left(\\omega_{N}^{i_{0} \\sqrt{N}+i_{1}}\\right)\n$$\n\nwhere $\\operatorname{deg}\\left(\\ell_{j_{1}}\\right)<N$. Let us define a matrix $\\widetilde{\\mathbf{L}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ that will hold the coefficients of polynomials $\\ell_{j_{1}}(Z)$ i.e. $$\n\\widetilde{\\mathbf{L}}\\left[a, j_{1}\\right]=\\tilde{\\ell}_{j_{1}}[a]\n$$\n\nwhere\n\n$$\n\\ell_{j_{1}}(Z)=\\sum_{a=0}^{N-1} \\tilde{\\ell}_{j_{1}}[a] \\cdot Z^{a}\n$$\n\nWe can multiply this matrix with the Fourier matrix of size $N \\times N$ to get the blocks of $\\mathbf{L}$ (which we will need to diagonalize). Specifically define\n\n$$\n\\mathbf{L}^{\\prime \\prime}=\\mathbf{F}_{N} \\cdot \\widetilde{\\mathbf{L}}\n$$\n\nThe rows of $\\mathbf{L}^{\\prime \\prime}$ and the rows of $\\mathbf{M}^{\\prime}$ are both indexed by $i$. Meaning they're ordered in lexicographic ordering $\\left(i_{1}, i_{0}\\right)$, which is a problem for the following reason. The block diagonal matrix $\\mathbf{L}$ made from $\\mathbf{L}^{\\prime \\prime}$ needs to be in lexicographic ordering $\\left(i_{0}, i_{1}\\right)$ (see (41)) since it gets permuted on the left $\\mathbf{M}=\\mathbf{P L P}$ and right allowing $\\mathbf{M}$ to be ordered by $\\left(i_{1}, i_{0}\\right)$. Therefore, when composing $\\mathbf{L}$ from $\\mathbf{L}^{\\prime \\prime}$ we must permute the rows by $\\mathbf{P}$. So we get,\n\n$$\n\\mathbf{L}^{\\prime}=\\mathbf{P} \\cdot \\mathbf{L}^{\\prime \\prime}\n$$\n\nLet\n\n$$\n\\mathbf{L}^{\\prime}=\\left[\\begin{array}{c}\n\\mathbf{L}_{0}^{\\prime} \\\\\n\\vdots \\\\\n\\mathbf{L}_{\\sqrt{N}-1}^{\\prime}\n\\end{array}\\right]\n$$\n\nThen\n\n$$\n\\mathbf{L}=\\operatorname{diag}\\left(\\mathbf{L}_{0}^{\\prime}, \\ldots \\mathbf{L}_{\\sqrt{N}-1}^{\\prime}\\right)\n$$\n\nPictorially:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-43.jpg?height=256&width=1296&top_left_y=409&top_left_x=317)\n\nLet $f$ be the function that maps coefficient matrices $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ to $(\\mathbf{L}, \\mathbf{R})$ where $\\mathbf{L}$ and $\\mathbf{R}$ are defined by (42) and (40).",
    "m2-54": "Theorem 9. Let $f$ be as defined above. Then $f$ is a bijection. Proof. To prove $f$ is a bijection we must show $f$ is one-to-one and $f^{-1}$ is one-to-one (and exists). To show $f$ is one-to-one means both $\\widetilde{\\mathbf{L}}$ and $\\widetilde{\\mathbf{R}}$ given to $f$ will output a unique pair $(\\mathbf{L}, \\mathbf{R})$. This follows from (42) and (40), and the fact that polynomial evaluation is a function. Now to show $f^{-1}$ exists and is one-to-one, we must show that there's a map for any $(\\mathbf{L}, \\mathbf{R})$ to unique $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$. Then for any pair $(\\mathbf{L}, \\mathbf{R})$ where both $\\mathbf{L}$ and $\\mathbf{R}$ are block diagonal matrices, there's a map to unique sets of polynomials, $\\ell, \\tilde{r}$ where each coefficient is an entry of $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}}$ (thus giving a unique mapping from ( $\\mathbf{L}, \\mathbf{R}$ )'s to $(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}}$ )'s). We need to show the existence of $\\ell_{j_{1}}(Z)$ and $\\tilde{r}_{j_{1}, j_{0}}(Y)$ such that:\n\n$$\n\\mathbf{L}_{i_{0}, i_{0}}\\left[i_{1}, j_{1}\\right]=\\ell_{j_{1}}\\left(\\omega_{N}^{i_{0} \\sqrt{N}+i_{1}}\\right)\n$$\n\nand\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]=\\tilde{r}_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nFix $0 \\leq j_{1}, j_{0}<\\sqrt{N}$. Then consider the values $0 \\leq i_{0}<\\sqrt{N}$ :\n\n$$\ny_{i_{0}} \\leftarrow \\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]\n$$\n\nThen by Theorem 4 there exists a unique polynomial of degree $<\\sqrt{N}$ (call it $\\tilde{r}_{j_{1}, j_{0}}$ ) such that for all $0 \\leq i_{0}<\\sqrt{N}$ :\n\n$$\n\\tilde{r}_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)=y_{i_{0}}\n$$\n\nThere will be $N$ polynomials with $\\sqrt{N}$ coefficients each, meaning there's unique sets of coefficients to make up the indices of $\\widetilde{\\mathbf{R}}$. Now to get the entries of $\\widetilde{\\mathbf{L}}$ from $\\mathbf{L}$, fix $0 \\leq j_{1}<\\sqrt{N}$. Then consider the values $0 \\leq i_{1}, i_{0}<\\sqrt{N}$ :\n\n$$\ny_{i_{1}, i_{0}} \\leftarrow \\mathbf{L}_{i_{0}, i_{0}}\\left[i_{1}, j_{1}\\right]\n$$\n\nThen by Theorem 4 there exists a unique polynomial of degree $<N$ (call it $\\ell_{j_{1}}$ ) such that for all $0 \\leq i_{1}, i_{0}<\\sqrt{N}:$\n\n$$\n\\ell_{j_{1}}\\left(\\omega_{N}^{i_{0} \\sqrt{N}+i_{1}}\\right)=y_{i_{1}, i_{0}}\n$$\n\nThere will be $\\sqrt{N}$ polynomials with $N$ coefficients each, meaning there's unique sets of coefficients to make up the indices of $\\widetilde{\\mathbf{L}}$. Algorithm 2 is pseudo code for the map from $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ to block diagonal matrices $\\mathbf{L}, \\mathbf{R}$. ```\nAlgorithm 2 Blocky \\(\\operatorname{Monarch}(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}})\\)\nInput: \\(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}\\)\nOutput: Block diagonal matrices \\(\\mathbf{L}, \\mathbf{R} \\in \\mathbb{C}^{N \\times N}\\)\n    Let \\(\\mathbf{F}_{N}\\) be the Fourier transform Monarch matrix \\(\\mathbf{P L}_{F} \\mathbf{P} \\mathbf{R}_{F} \\mathbf{P}\\)\n    \\(\\triangleright\\) First, compute \\(\\mathbf{L}\\) from \\(\\widetilde{\\mathbf{L}}\\)\n                            \\(\\triangleright\\) See Corollary 6\n\\(\\mathbf{L}^{\\prime} \\leftarrow \\mathbf{P} \\cdot \\mathbf{F}_{N} \\cdot \\widetilde{\\mathbf{L}}\\)\n    for \\(a \\leftarrow 0\\) to \\(\\sqrt{N}-1\\) do\n        \\(\\mathbf{L}_{a}^{\\prime} \\leftarrow \\mathbf{L}^{\\prime}[a \\sqrt{N}: a \\sqrt{N}+\\sqrt{N}-1,:]\\)\n    \\(\\mathbf{L} \\leftarrow \\operatorname{diag}\\left(\\mathbf{L}_{0}^{\\prime}, \\ldots \\mathbf{L}_{\\sqrt{N}-1}^{\\prime}\\right)\\)\n                                    \\(\\triangleright\\) Now compute \\(\\mathbf{R}\\) from \\(\\widetilde{\\mathbf{R}}\\)\n    for \\(a \\leftarrow 0\\) to \\(\\sqrt{N}-1\\) do\n    \\(\\mathbf{R}_{a} \\leftarrow \\mathbf{F}_{\\sqrt{N}} \\cdot \\widetilde{\\mathbf{R}}[a \\sqrt{N}: a \\sqrt{N}+\\sqrt{N}-1,:]\\)\n    \\(\\mathbf{R} \\leftarrow \\operatorname{diag}\\left(\\mathbf{R}_{0}, \\ldots, \\mathbf{R}_{\\sqrt{N}-1}\\right)\\)\n    return \\(\\overline{\\mathbf{L}}, \\mathbf{R}\\)\n```\n\nLemma 5. Algorithm 2 uses $O\\left(N^{3 / 2}\\right)$ FLOPs and $3 \\sqrt{N}$ GEMMs of two $\\sqrt{N} \\times \\sqrt{N}$ matrices. Proof. Lines 1 and 2 are multiplying a monarch matrix representing the Fourier transform times a $N \\times \\sqrt{N}$ matrix in block fashion again, giving $O\\left(N^{3 / 2}\\right)$ FLOPs. Similarly, lines 6 and 7 have two $\\sqrt{N}$ matrices multiplied $\\sqrt{N}$ times. Giving $O\\left(N^{3 / 2}\\right)$ FLOPs. Lines 3, 4, 5, 8, and 9 don't count towards FLOPs\nTherefore we have $O\\left(N^{3 / 2}\\right)$ FLOPS and $3 \\sqrt{N}$ GEMMs of two $\\sqrt{N} \\times \\sqrt{N}$ matrices. Now that we have the operations in terms of blocks, we will make them causal in the following sub-section. ## D.4.2 Causal Block Monarch Convolution\n\nRecall that a Monarch matrix is defined as\n\n$$\n\\mathbf{M}=\\mathbf{P L P R P}\n$$\n\nthen per equation (37) we have\n\n$$\n\\mathbf{M}^{\\prime}=\\mathbf{P L P R}\n$$\n\nThen if $q_{\\left(j_{1}, j_{0}\\right)}(Z)$ is the basis polynomial corresponding to the $\\left(j_{1}, j_{0}\\right)^{t h}$ column of $\\mathbf{M}$, by (38) the basis polynomial corresponding to M is\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)}(Z)=\\ell_{j_{0}}(Z) \\cdot \\tilde{r}_{j_{0}, j_{1}}\\left(Z^{\\sqrt{N}}\\right)\n$$\n\nwith the minimum degree of $j_{1} \\sqrt{N}+j_{0}$ (recall that we pick $\\ell_{j_{0}}$ and $\\tilde{r}_{j_{0}, j_{1}}$ to have minimum degree $j_{0}$ and $j_{1}$ respectively) as desired. Theorem 10. Let $\\mathbf{L}, \\mathbf{R} \\leftarrow$ Blocky $\\operatorname{Monarch}(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}})$. Let $\\mathbf{M}$ be as in (37). Then for every $0 \\leq i$, $j<N$, we have:\n\n$$\n\\mathbf{M}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=q_{\\left(j_{1}, j_{0}\\right)}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right)\n$$\n\nwhere $q_{\\left(j_{1}, j_{0}\\right)}$ is as in (44). Proof. To prove this we need to show the $\\left(j_{1}, j_{0}\\right)^{\\text {th }}$ column of $\\mathbf{M}$ is the basis polynomial $q_{\\left(j_{1}, j_{0}\\right)}$ evaluated at the $N^{\\text {th }}$ roots of unity, where $q_{\\left(j_{1}, j_{0}\\right)}$ is as defined in (44). Note we have\n\n$$\n\\begin{aligned}\nq_{\\left(j_{1}, j_{0}\\right)}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right) & =\\ell_{j_{0}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right) \\cdot \\tilde{r}_{j_{0}, j_{1}}\\left(\\omega_{N}^{\\left(i_{1} \\sqrt{N}+i_{0}\\right) \\sqrt{N}}\\right) \\\\\n& =\\ell_{j_{0}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right) \\cdot \\tilde{r}_{j_{0}, j_{1}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right) . \\end{aligned}\n$$\n\nBy definition we have,\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right] \\cdot \\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]\n$$\n\nBy (39) we have\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right]=\\tilde{r}_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nand by (41) and the fact that\n\n$$\n\\overline{\\mathbf{L}}=\\mathbf{P L P}\n$$\n\nwe have\n\n$$\n\\overline{\\mathbf{L}}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right]=\\ell_{j_{1}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right)\n$$\n\nThus, we have\n\n$$\n\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\ell_{j_{1}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right) \\cdot \\tilde{r}_{j_{1}, j_{0}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)\n$$\n\nSince\n\n$$\n\\mathbf{M} \\cdot \\mathbf{P}=\\mathbf{M}^{\\prime}\n$$\n\nwe have\n\n$$\n\\begin{aligned}\n\\mathbf{M}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right] & =\\mathbf{M}^{\\prime}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{0}, j_{1}\\right)\\right] \\\\\n& =\\ell_{j_{0}}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right) \\cdot r_{j_{0}, j_{1}}\\left(\\omega_{\\sqrt{N}}^{i_{0}}\\right)=q_{\\left(j_{1}, j_{0}\\right)}\\left(\\omega_{N}^{i_{1} \\sqrt{N}+i_{0}}\\right)\n\\end{aligned}\n$$\n\nwhere the last equality follows from (45). Corollary 6. The DFT is $\\mathbf{M}$ as in Theorem 10 when all blocks of $\\widetilde{\\mathbf{R}}$ and the top block of $\\widetilde{\\mathbf{L}}$ are the identity matrix of size $\\sqrt{N} \\times \\sqrt{N}$ (the rest of $\\widetilde{\\mathbf{L}}$ is all 0 's). Proof. Since only the diagonal of the top block of $\\widetilde{\\mathbf{L}}$ will contain any non-zero values we only index the first $0, \\ldots, \\sqrt{N}-1$ rows. And since all blocks of $\\widetilde{\\mathbf{R}}$ are the identity matrix we get\n\n$$\n\\widetilde{\\mathbf{L}}\\left[j_{1}, j_{1}\\right]=\\tilde{\\ell}_{j_{1}}\\left[j_{1}\\right]=1\n$$\n\nand\n\n$$\n\\widetilde{\\mathbf{R}}_{j_{1}}\\left[j_{0}, j_{0}\\right]=\\tilde{r}_{j_{1}, j_{0}}\\left[j_{0}\\right]=1\n$$\n\nAll other entries of $\\widetilde{\\mathbf{L}}$ and $\\widetilde{\\mathbf{R}}$ are 0 . Thus, we have\n\n$$\n\\ell_{j_{1}}(Z)=Z^{j_{1}}\n$$\n\nand\n\n$$\n\\tilde{r}_{j_{1}, j_{0}}(Z)=Z^{j_{0}}\n$$\n\nAs per Theorem 10,\n\n$$\n\\begin{aligned}\nq_{\\left(j_{1}, j_{0}\\right)}(Z) & =\\ell_{j_{0}}(Z) \\cdot \\tilde{r}_{j_{0}, j_{1}}\\left(Z^{\\sqrt{N}}\\right) \\\\\n& =Z^{j_{0}} \\cdot Z^{j_{1} \\sqrt{N}}=Z^{j_{0}+j_{1} \\sqrt{N}}=Z^{j}\n\\end{aligned}\n$$\n\nThen by Theorem 10 note\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)}\\left(\\omega_{N}^{i}\\right)=\\omega_{N}^{i j}=\\mathbf{M}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]\n$$\n\nwhich implies $\\mathbf{M}$ is $\\mathbf{F}_{N}$ as desired. Algorithm 3 is pseudo code for the Monarch convolution algorithm. It maps an input space of $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}}$ matrices, a kernel vector $\\mathbf{k}$, and input vector $\\mathbf{u}$ to a vector $\\mathbf{f}$. ```\nAlgorithm 3 BlockMonarchConv \\((\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}}, \\mathbf{k}, \\mathbf{u})\\)\nInput: \\(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}, \\mathbf{k}, \\mathbf{u} \\in \\mathbb{R}^{N}\\)\nOutput: \\(\\mathbf{f} \\in \\mathbb{R}^{N}\\)\n    1: \\(\\mathbf{L}, \\mathbf{R} \\leftarrow\\) Blocky \\(\\operatorname{MONARCh}(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}})\\)\n    2: \\(\\mathrm{M} \\leftarrow\\) PLPRP\n    \\(\\triangleright\\) Get \\(\\mathbf{M}^{\\prime}\\) from Blocky Monarch\n                                    \\(\\triangleright\\) Compute \\(\\mathbf{k}_{f}, \\mathbf{u}_{f}\\) from \\(\\mathbf{M}\\)\n    \\(\\mathbf{k}_{f} \\leftarrow \\mathbf{M} \\cdot \\mathbf{k}\\)\n    \\(\\mathbf{u}_{f} \\leftarrow \\mathbf{M} \\cdot \\mathbf{u}\\)\n    \\({ }_{5}: \\mathbf{f} \\leftarrow \\mathbf{M}^{-1} \\cdot\\left(\\mathbf{k}_{f} \\odot \\mathbf{u}_{f}\\right) \\quad \\triangleright\\) Compute f\n    return f\n```\n\nWe next outline how to make the map in Algorithm 3 causal.",
    "m2-55": "Towards that end, we observe:\nLemma 6. For any fixed $n \\geq 1$, let $N=\\left\\lceil\\sqrt{2 n}^{2}\\right.$. Define $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ such that for every $0 \\leq j_{1}<\\sqrt{N}$,\n\n$$\n\\widetilde{\\mathbf{L}}=\\left[\\begin{array}{c}\n\\widetilde{\\mathbf{L}}_{0} \\\\\n\\vdots \\\\\n\\widetilde{\\mathbf{L}}_{\\sqrt{N}-1}\n\\end{array}\\right] \\quad \\text { and } \\widetilde{\\mathbf{R}}=\\left[\\begin{array}{c}\n\\widetilde{\\mathbf{R}}_{0} \\\\\n\\vdots \\\\\n\\widetilde{\\mathbf{R}}_{\\sqrt{N}-1}\n\\end{array}\\right]\n$$\n\nDefine $\\widetilde{\\mathbf{L}}^{\\prime}, \\widetilde{\\mathbf{R}}^{\\prime} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ with corresponding coefficient blocks $\\left(\\widetilde{\\mathbf{L}}_{k}^{\\prime}\\right)_{0 \\leq k<\\sqrt{N}},\\left(\\widetilde{\\mathbf{R}}_{k}^{\\prime}\\right)_{0 \\leq k<\\sqrt{N}}$ as\n\n$$\n\\widetilde{\\mathbf{L}}^{\\prime}=\\left[\\begin{array}{c}\n\\widetilde{\\mathbf{L}}_{0}^{\\prime} \\\\\n\\mathbf{0}_{\\sqrt{N} \\times \\sqrt{N}} \\\\\n\\vdots \\\\\n\\mathbf{0}_{\\sqrt{N} \\times \\sqrt{N}}\n\\end{array}\\right] \\quad \\text { and } \\widetilde{\\mathbf{R}}^{\\prime}=\\left[\\begin{array}{c}\n\\widetilde{\\mathbf{R}}_{0}^{\\prime} \\\\\n\\widetilde{\\mathbf{R}}_{1}^{\\prime} \\\\\n\\vdots \\\\\n\\widetilde{\\mathbf{R}}_{\\sqrt{N}-1}^{\\prime}\n\\end{array}\\right]\n$$\n\nwhere\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-46.jpg?height=131&width=1616&top_left_y=2393&top_left_x=252)\n\n$$\n\\widetilde{\\mathbf{R}}_{j_{1}}^{\\prime}\\left[i_{0}, j_{0}\\right]=\\left\\{\\begin{array}{ll}\n0 & \\text { if } i_{0}<j_{0} \\text { or }\\left(\\left(i_{0} \\geq\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor\\right) \\quad \\text { and }\\left(j_{0}<\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor\\right)\\right) \\\\\n\\widetilde{\\mathbf{R}}_{j_{1}}\\left[i_{0}, j_{0}\\right] \\quad \\text { otherwise }\n\\end{array} \\quad \\text { for } j_{1}=0, \\ldots, \\sqrt{N}-1 .\\right. $$\n\nFurther, we require that $\\widetilde{\\mathbf{R}}_{j_{1}}^{\\prime}\\left[j_{0}, j_{0}\\right], \\widetilde{\\mathbf{L}}_{0}^{\\prime}\\left[j_{1}, j_{1}\\right]$ are all non-zero entries for all $0 \\leq j_{0}, j_{1}<\\sqrt{N}$. Then for $j_{0} \\sqrt{N}+j_{1}<\\sqrt{N}\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor$, the basis polynomial $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)=\\ell_{j_{1}}^{\\prime}(Z) r_{j_{1}, j_{0}}^{\\prime}\\left(Z^{\\sqrt{N}}\\right)$ of $\\mathbf{M}^{\\prime}=$ PLPR has minimum degree $j_{0} \\sqrt{N}+j_{1}$ and maximum degree $\\leq \\frac{N}{2}-1$, and for $\\sqrt{N}\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor \\leq$ $j_{0} \\sqrt{N}+j_{1}<N$, the basis polynomial $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)$ has minimum degree $j_{0} \\sqrt{N}+j_{1}$ and maximum degree $\\leq N-1$. Note that in $\\mathbf{M}=\\mathbf{M}^{\\prime} \\mathbf{P}$ the $\\left(j_{1}, j_{0}\\right)$ basis polynomial $q_{j_{1}, j_{0}}(Z)=q_{j_{0}, j_{1}}^{\\prime}(Z)$ has the required degree bound. Proof. Let $\\mathbf{L}^{\\prime}, \\mathbf{R}^{\\prime} \\leftarrow \\operatorname{Blocky} \\operatorname{Monarch}\\left(\\widetilde{\\mathbf{L}}^{\\prime}, \\widetilde{\\mathbf{R}}^{\\prime}\\right)$, and denote their corresponding polynomials as $\\ell_{j_{1}}^{\\prime}(Z), r_{j_{1}, j_{0}}^{\\prime}\\left(Z^{\\sqrt{N}}\\right)$, respectively for every $0 \\leq j_{1}, j_{0}<\\sqrt{N}$. By our definition, for all $0 \\leq j_{0}, j_{1}<$ $\\sqrt{N}, r_{j_{1}, j_{0}}^{\\prime}\\left(Z^{\\sqrt{N}}\\right)$ has minimum degree $j_{0} \\sqrt{N}$ and $\\ell_{j_{1}}^{\\prime}(Z)$ has minimum degree $j_{1}$. Then it follows that the basis polynomial $q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}(Z)=\\ell_{j_{1}}^{\\prime}(Z) r_{j_{1}, j_{0}}^{\\prime}\\left(Z^{\\sqrt{N}}\\right)$ has minimum degree $j_{1}+j_{0} \\sqrt{N}$. We now look at the degrees of each basis polynomial. From our definition of $\\mathbf{R}_{j_{1}}^{\\prime}$, all entries $\\mathbf{R}_{j_{1}}^{\\prime}\\left[i_{0}, j_{0}\\right]=0$ for $j_{0}<\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor$ and $i_{0} \\geq\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor$. This implies that for $0 \\leq j_{0}<\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor$, and $0 \\leq j_{1}<\\sqrt{N}$, we have $\\operatorname{deg}\\left(r_{j_{1}, j_{0}}^{\\prime}\\right)<\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor$. Since we are only looking at $\\mathbf{L}_{0}^{\\prime}(Z)$, note that degree $\\operatorname{deg}\\left(\\ell_{j_{1}}^{\\prime}\\right) \\leq \\sqrt{N}-1$. Then it follows that\n\n$$\n\\begin{aligned}\n\\operatorname{deg}\\left(q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}\\right) & \\leq \\sqrt{N}-1+\\left(\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor-1\\right) \\sqrt{N} \\\\\n& =\\sqrt{N}-1+\\left(\\sqrt{N}\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor-\\sqrt{N}\\right) \\\\\n& \\leq \\sqrt{N} \\frac{\\sqrt{N}}{2}-1 \\\\\n& =\\frac{N}{2}-1\n\\end{aligned}\n$$\n\n(Note that in the above $j_{0} \\sqrt{N}+j_{1} \\leq \\sqrt{N}\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor-1$ by same calculations above as needed.)\nFor $\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor \\leq j_{0}<\\sqrt{N}$ and $0 \\leq j_{1}<\\sqrt{N}, \\operatorname{deg}\\left(r_{j_{1}, j_{0}}^{\\prime}\\right)=\\sqrt{N}-1$, and $\\ell_{j_{1}}^{\\prime}(Z)$ degree $\\sqrt{N}-1$. Then it follows that\n\n$$\n\\begin{aligned}\n\\operatorname{deg}\\left(q_{\\left(j_{1}, j_{0}\\right)}^{\\prime}\\right) & \\leq \\sqrt{N}-1+(\\sqrt{N}-1) \\sqrt{N} \\\\\n& =N-1\n\\end{aligned}\n$$\n\nas desired. (Note that $j_{0} \\sqrt{N}+j_{1} \\geq\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor \\sqrt{N}$ as needed.)\nFinally, we use Lemma 6 and Theorem 8 to conclude the following:\n\nTheorem 11. For any fixed $n \\geq 1$, let $N=\\lceil\\sqrt{2 n}\\rceil^{2}$. Let $\\widetilde{\\mathbf{L}}^{\\prime}, \\widetilde{\\mathbf{R}}^{\\prime} \\in \\mathbb{R}^{N \\times N}$ with corresponding coefficient blocks $\\left(\\widetilde{\\mathbf{L}}_{k}^{\\prime}\\right)_{0 \\leq k<\\sqrt{N}},\\left(\\widetilde{\\mathbf{R}}_{k}^{\\prime}\\right)_{0 \\leq k<\\sqrt{N}}$ be defined as in Lemma 6. Then for any $\\mathbf{k}, \\mathbf{u} \\in \\mathbb{R}^{n}$, BlockMonarchConv $\\left(\\widetilde{\\mathbf{L}}^{\\prime}, \\widetilde{\\mathbf{R}}^{\\prime}, \\mathbf{k}^{\\prime}, \\mathbf{u}^{\\prime}\\right)[0: n-1]$, where $\\mathbf{k}^{\\prime}=\\left(\\mathbf{k}, \\mathbf{0}_{N-n}\\right), \\mathbf{u}^{\\prime}=\\left(\\mathbf{u}, \\mathbf{0}_{N-n}\\right)$, is causal in $\\mathbf{u}$.",
    "m2-56": "Proof.",
    "m2-57": "Note that this is the same setting as Theorem 8. If $N=\\lceil\\sqrt{2 n}\\rceil^{2}$, then $\\left\\lfloor\\frac{N}{2}\\right\\rfloor \\geq n$. Then by Lemma 6 , the basis polynomials $q_{j_{1}, j_{0}}(Z)$ of $\\mathbf{M}=$ PLPR have $\\operatorname{deg}\\left(q_{\\left(j_{1}, j_{0}\\right)}\\right)<\\frac{N}{2}$ for $0 \\leq j_{1} \\sqrt{N}+j_{0}<$ $\\sqrt{N}\\left\\lfloor\\frac{\\sqrt{N}}{2}\\right\\rfloor \\leq\\left\\lfloor\\frac{N}{2}\\right\\rfloor .^{5}$\n\nThen (34) computes BlockMonarchConv ( $\\left.\\widetilde{\\mathbf{L}}^{\\prime}, \\widetilde{\\mathbf{R}}^{\\prime}, \\mathbf{k}^{\\prime}, \\mathbf{u}^{\\prime}\\right)$.",
    "m2-58": "Since Algorithm 3 performs the same operation as (34), the result follows from Lemma 6 and Theorem 8. ## D. 5 Bivariate Polynomials with Kronecker Substitution Over Reals\n\nOur earlier results pertain to complex evaluation points. In this section we define causal convolution over real evaluation points. To do this, we redefine our basis polynomials in terms of the Chebyshev polynomials of the first kind. In Appendix D.5.1 we recover Theorem 8 for univariate polynomials defined over real evaluation points. This identifies a class of matrices that we use to define to define a structured subclass in Appendix D.5.2. We show that these matrices can be used to perform (31). Finally, in Appendix D.5.3 we give the analog of Theorem 11 over real evaluation points. However, the resulting matrices are not Monarch matrices but they are close. In Appendix D.5.4 and Appendix D.5.5 we discuss how we can exploit this closeness, and compute (31) more efficiently. ## D.5.1 Univariate Evaluation over Real Numbers\n\nFor any integer $a \\geq 0$, the Chebyshev polynomial of the first kind with degree $a$ is denoted as $T_{a}(Z)$ and is defined as\n\n$$\nT_{a}(\\cos \\theta) \\stackrel{\\text { def }}{=} \\cos (a \\theta)\n$$\n\nand has the property\n\n$$\nT_{a}(-\\cos \\theta)=(-1)^{a} \\cos (a \\theta)\n$$\n\nTo parallel Appendix D.3.2, we consider the class of basis polynomials with the form\n\n$$\n\\bar{q}_{j}^{N}(Z)=\\sum_{a=0}^{N-j-1} \\bar{q}_{j}[a] T_{a}(Z)\n$$\n\nevaluated over $\\left(\\omega_{N, i}\\right)_{0 \\leq i<N}$ where\n\n$$\n\\omega_{N, i} \\stackrel{\\text { def }}{=} \\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right)}{N}\\right)\n$$\n\nLet us consider the class of matrices defined over (48) and (49). [^4]Lemma 7. For sequence of points $A=\\left\\{\\omega_{N, 0}, \\ldots, \\omega_{N, N-1}\\right\\}$, let $\\mathbf{M}$ be defined as\n\n$$\n\\mathbf{M}[i, j]=\\bar{q}_{j}^{N}\\left(\\omega_{N, i}\\right)\n$$\n\nwhere $\\bar{q}_{j}^{N}(Z)$ and $\\omega_{N, i}$ are defined as in (48) and (49), respectively. Then for any vector $\\boldsymbol{u}, \\mathbf{M} \\cdot \\boldsymbol{u}$ is equivalent to evaluating the polynomial\n\n$$\nu(Z)=\\sum_{j=0}^{N-1} u_{j} \\cdot \\bar{q}_{j}^{N}(Z)\n$$\n\nat each point in $A$. Proof. By our definition of $\\mathbf{M}$, the column $\\mathbf{M}[:, j]$ is exactly the evaluation of the polynomial $\\bar{q}_{j}^{N}(Z)$ at each point in $A$. The claimed result comes from the definition of matrix vector multiplication and (51).",
    "m2-59": "This leads to the following analog of Theorem 7 . Theorem 12. For matrices $\\mathbf{M}_{0}, \\mathbf{M}_{1}, \\mathbf{M}_{2}$, each of form as in Lemma 7, the operation\n\n$$\n\\mathbf{f}=\\mathbf{M}_{0}^{-1} \\cdot\\left(\\left(\\mathbf{M}_{1} \\cdot \\mathbf{k}\\right) \\odot\\left(\\mathbf{M}_{2} \\cdot \\mathbf{u}\\right)\\right)\n$$\n\nis equivalent to representing the polynomial\n\n$$\nf(Z)=k(Z) \\cdot u(Z) \\quad \\bmod T_{N}(Z)\n$$\n\nin terms of the basis polynomials\n\n$$\n\\hat{q}_{j}^{N}(Z) \\text { for } j=0, \\ldots, N-1\n$$\n\nwhere $k(Z), u(Z)$ are defined in terms of the respective basis polynomials corresponding to $\\mathbf{M}_{1}$ and $\\mathbf{M}_{2}$ as in Lemma 7, and $\\left(\\hat{q}_{j}^{N}(Z)\\right)_{0 \\leq j<N}$ corresponds to $\\mathbf{M}_{0}$. Proof. For $A=\\left\\{\\omega_{N, i}\\right\\}_{0 \\leq i<N}$, define\n\n$$\nq_{A}(Z)=\\prod_{\\alpha \\in A}(Z-\\alpha)\n$$\n\nThen (52) follows since for\n\n$$\nf(Z)=k(Z) \\cdot u(Z) \\quad \\bmod q_{A}(Z)\n$$\n\nwe have the following for any $\\alpha \\in A$ :\n\n$$\nf(\\alpha)=k(\\alpha) \\cdot u(\\alpha)\n$$\n\nThe claim follows from Lemma 7, (52), the invertibility of $\\mathbf{M}_{0}$, and the known fact that $T_{N}(Z)=$ $q_{A}(Z)$. We also utilize the following result:\n\nLemma 8. Let $\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z)$ be defined as in (48). Then for any $0 \\leq j, j^{\\prime}<N$,\n\n$$\n\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z) \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z)=\\sum_{i=j+j^{\\prime}}^{N-1} \\alpha_{j+j^{\\prime}, i} \\cdot \\bar{q}_{i}^{N}(Z)\n$$\n\nfor some set of coefficients $\\alpha_{j+j^{\\prime}, i}$. Proof. From (48), we have\n\n$$\n\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z) \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z)=\\sum_{a=0}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor-j-1} q_{j}[a] T_{a}(Z) \\cdot \\sum_{a^{\\prime}=0}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor-j^{\\prime}-1} q_{j^{\\prime}}^{\\prime}\\left[a^{\\prime}\\right] T_{a^{\\prime}}(Z)\n$$\n\nRecall that within a $m$ dimensional vector space of polynomials, we can define a basis by choosing any set of polynomials with degrees $0, \\ldots, m-1$. Because $\\left.\\operatorname{deg}\\left(\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor} \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\frac{N}{2}\\right.}\\right\\rfloor\\right) \\leq 2\\left\\lfloor\\frac{N}{2}\\right\\rfloor-\\left(j+j^{\\prime}\\right)-2$, it can be written as a linear combination of any set of $2\\left\\lfloor\\frac{N}{2}\\right\\rfloor-\\left(j+j^{\\prime}\\right)-1$ polynomials where for $0 \\leq a \\leq 2\\left\\lfloor\\frac{N}{2}\\right\\rfloor-\\left(j+j^{\\prime}\\right)-2$, the $a$-th polynomial has degree $a$. In other words we can choose the $a$-th polynomial as $q_{N-a-1}^{N}(Z)$. Thus we have\n\n$$\n\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z) \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\frac{N}{j^{\\prime}}\\right\\rfloor}(Z)=\\sum_{a=0}^{2\\left\\lfloor\\frac{N}{2}\\right\\rfloor-\\left(j+j^{\\prime}\\right)-2} \\bar{\\alpha}_{j+j^{\\prime}, a} \\cdot \\bar{q}_{N-a-1}^{N}(Z)\n$$\n\nfor some set of coefficients $\\alpha_{j+j^{\\prime}, a}$. Then after reindexing $i \\leftarrow N-a-1$ we have\n\n$$\n\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z) \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\left\\lfloor\\frac{N}{2}\\right\\rfloor\\right.}(Z)=\\sum_{i=\\left(N-2\\left\\lfloor\\frac{N}{2}\\right\\rfloor\\right)+j+j^{\\prime}+1}^{N-1} \\bar{\\alpha}_{j+j^{\\prime}, N-i-1} \\cdot \\bar{q}_{i}^{N}(Z)\n$$\n\nThe claim follows by setting $\\alpha_{j+j^{\\prime}, j+j^{\\prime}+1}=0$, and if $N$ is odd, $\\alpha_{j+j^{\\prime}+1, j+j^{\\prime}+2}=0$, and $\\alpha_{j+j^{\\prime}, i}=$ $\\bar{\\alpha}_{j+j^{\\prime}, N-i-1}$ for other $i$. This allows us to prove the following causality result for convolutions over real evaluation points. Theorem 13. Fix a family of basis polynomials $\\bar{q}_{0}^{N}, \\bar{q}_{1}^{N} \\ldots, \\bar{q}_{N-1}^{N}$ as defined in (48). Let $N \\geq 1$ be a perfect square, $n \\leq\\left\\lfloor\\frac{N}{2}\\right\\rfloor, \\mathbf{k}, \\mathbf{u} \\in \\mathbb{R}^{n}$ and $\\mathbf{M}_{N}$ defined by basis $\\left(\\bar{q}_{j}^{N}(Z)\\right)_{0 \\leq j<N}$. Let $\\mathbf{k}^{\\prime \\prime}=\\left(\\mathbf{k}, \\mathbf{0}_{\\left\\lfloor\\frac{N}{2}\\right\\rfloor-n}\\right)$ and $\\mathbf{u}^{\\prime \\prime}=\\left(\\mathbf{u}, \\mathbf{0}_{\\left\\lfloor\\frac{N}{2}\\right\\rfloor-n}\\right)$. Then the operation\n\n$$\n\\mathbf{u} \\mapsto\\left(\\mathbf{M}_{N}^{-1}\\left(\\mathbf{M}_{N} \\cdot\\left(\\mathbf{0}_{\\left\\lceil\\frac{N}{2}\\right\\rceil}, \\mathbf{k}^{\\prime \\prime}\\right) \\circ \\mathbf{M}_{N} \\cdot\\left(\\mathbf{0}_{\\left\\lceil\\frac{N}{2}\\right\\rceil}, \\mathbf{u}^{\\prime \\prime}\\right)\\right)\\right)[0: n-1]\n$$\n\ndefines a causal map in $\\mathbf{u}$. Proof. Let\n\n$$\n\\mathbf{f}=\\left(\\mathbf{M}_{N}^{-1}\\left(\\mathbf{M}_{N} \\cdot\\left(\\mathbf{0}_{\\left\\lceil\\frac{N}{2}\\right\\rceil}, \\mathbf{k}^{\\prime \\prime}\\right) \\circ \\mathbf{M}_{N} \\cdot\\left(\\mathbf{0}_{\\left\\lceil\\frac{N}{2}\\right\\rceil}, \\mathbf{u}^{\\prime \\prime}\\right)\\right)\\right)[0: n-1]\n$$\n\nIn order to prove that (55) is actually causal in the input $\\mathbf{u} \\in \\mathbb{R}^{n}$, we must show that for all $0 \\leq i<N, \\mathbf{f}[i]$ is dependent only on $\\mathbf{u}[0], \\mathbf{u}[1], \\ldots \\mathbf{u}[i]$. Let $\\mathbf{k}^{\\prime}=\\left(\\mathbf{0}_{\\left\\lceil\\frac{N}{2}\\right\\rceil}, \\mathbf{k}^{\\prime \\prime}\\right)$ and $\\mathbf{u}^{\\prime}=\\left(\\mathbf{0}_{\\left\\lceil\\frac{N}{2}\\right\\rceil}, \\mathbf{u}^{\\prime \\prime}\\right)$. By Lemma $7, \\mathbf{M}_{N} \\cdot \\mathbf{k}^{\\prime}$ and $\\mathbf{M}_{N} \\cdot \\mathbf{u}^{\\prime}$ correspond to the evaluations of the polynomials\n\n$$\nk^{\\prime}(Z)=\\sum_{j=0}^{N-1} k_{j}^{\\prime} \\cdot \\bar{q}_{j}^{N}(Z), \\quad \\text { and } \\quad u^{\\prime}(Z)=\\sum_{j^{\\prime}=0}^{N-1} u_{j^{\\prime}}^{\\prime} \\cdot \\bar{q}_{j^{\\prime}}^{N}(Z)\n$$\n\nLet us define\n\n$$\nk(Z)=\\sum_{j=0}^{n-1} k_{j} \\cdot \\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z), \\quad \\text { and } \\quad u(Z)=\\sum_{j^{\\prime}=0}^{n-1} u_{j} \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z)\n$$\n\nNote that for $0 \\leq j<\\left\\lceil\\frac{N}{2}\\right\\rceil$, the coefficients $k_{j}^{\\prime}=u_{j}^{\\prime}=0$. Then (57) becomes\n\n$$\nk^{\\prime}(Z)=\\sum_{j=\\left\\lceil\\frac{N}{2}\\right\\rceil}^{N-1} k_{j}^{\\prime} \\cdot \\bar{q}_{j}^{N}(Z), \\quad \\text { and } \\quad u^{\\prime}(Z)=\\sum_{j^{\\prime}=\\left\\lceil\\frac{N}{2}\\right\\rceil}^{N-1} u_{j}^{\\prime} \\cdot \\bar{q}_{j^{\\prime}}^{N}(Z)\n$$\n\nwhich is equivalent to\n\n$$\nk^{\\prime}(Z)=\\sum_{j=0}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor-1} k_{j+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{\\prime} \\cdot \\bar{q}_{j+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{N}(Z), \\quad \\text { and } \\quad u^{\\prime}(Z)=\\sum_{j^{\\prime}=0}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor-1} u_{j^{\\prime}+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{\\prime} \\cdot \\bar{q}_{j^{\\prime}+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{N}(Z)\n$$\n\nFor $0 \\leq j<\\left\\lfloor\\frac{N}{2}\\right\\rfloor$, $\\operatorname{deg}\\left(\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}\\right)=\\left\\lfloor\\frac{N}{2}\\right\\rfloor-j-1$, and $\\operatorname{deg}\\left(\\bar{q}_{j+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{N}\\right)=N-\\left\\lceil\\frac{N}{2}\\right\\rceil-j-1=\\left\\lfloor\\frac{N}{2}\\right\\rfloor-j-1$. This implies that $\\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z)$ and $\\bar{q}_{j+\\left\\lceil\\frac{N}{N}\\right\\rfloor}^{N}(Z)$ are both linear combinations of $\\left\\lfloor\\frac{N}{2}\\right\\rfloor-j-1$ Chebychev polynomials. Then we can set $\\bar{q}_{j}^{\\left[\\frac{N}{2}\\right]}(Z)=\\bar{q}_{j+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{N}(Z)$. Similarly, note that for $0 \\leq j<n$, $k_{j+\\left\\lceil\\frac{N}{2}\\right\\rceil}^{\\prime}=k_{j}$. Then it follows that $k(Z)=k^{\\prime}(Z)$, and by a similar argument, $u(Z)=u^{\\prime}(Z)$. Then by Theorem 12 we have\n\n$$\n\\begin{aligned}\nf(Z) & =k(Z) \\cdot u(Z) \\quad \\bmod T_{N}(Z) \\\\\n& =k(Z) \\cdot u(Z)\n\\end{aligned}\n$$\n\nwhere the last statement follows since $\\operatorname{deg}(k(Z)), \\operatorname{deg}(u(Z)) \\leq n-1<\\left\\lfloor\\frac{N}{2}\\right\\rfloor$, implying that their product has $\\operatorname{deg}(\\bar{k}(Z) \\cdot \\bar{u}(Z))<\\operatorname{deg}\\left(T_{N}(Z)\\right)=N$. We want to write $f(Z)$ in the form\n\n$$\nf(Z)=\\sum_{i=0}^{N-1} f_{i} \\cdot \\bar{q}_{i}^{N}(Z)\n$$\n\nfor some set of coefficients set of coefficients $f_{i}$. From (58), (59) becomes\n\n$$\nf(Z)=\\sum_{j=0}^{n-1} \\sum_{j^{\\prime}=0}^{n-1} k_{j^{\\prime}} \\cdot u_{j} \\cdot \\bar{q}_{j^{\\prime}}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z) \\cdot \\bar{q}_{j}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor}(Z)\n$$\n\nThen by Lemma 8 we have\n\n$$\n\\sum_{i=0}^{N-1} f_{i} \\cdot \\bar{q}_{i}^{N}(Z)=\\sum_{j, j^{\\prime}=0}^{n-1} k_{j^{\\prime}} \\cdot u_{j} \\cdot \\sum_{i=j+j^{\\prime}}^{N-1} \\alpha_{j+j^{\\prime}, i} \\bar{q}_{i}^{N}(Z)\n$$\n\nWe show that for all $0 \\leq i<N, f_{i}$ is a function of $\\left(u_{i^{\\prime}}\\right)_{0 \\leq i^{\\prime} \\leq i}$. Then note from the above that each $k_{j}$ and $u_{j^{\\prime}}$ appears in terms of $\\bar{q}_{i}^{N}$ where $i \\geq j+j^{\\prime}$. Then we have\n\n$$\nf_{i}=\\sum_{\\substack{j, j=0 \\\\ j+j^{\\prime} \\leq i}}^{N-1} \\alpha_{j+j^{\\prime}, i} \\cdot k_{j^{\\prime}} \\cdot u_{j}\n$$\n\nas desired. ## D.5.2 Structured Causal Matrices\n\nIn this section we narrow our scope from the general class of matrices defined in Appendix D.5.1 to a particular structured subclass. Now let us define the class of structured causal matrices over the real numbers. Definition 6. Define\n\n$$\n\\ell_{j_{1}}(Z) \\stackrel{\\text { def }}{=} \\sum_{a=0}^{j_{1}} \\ell_{j_{1}}[a] T_{a}(Z), \\quad \\tilde{r}_{j_{1}, j_{0}}(Z) \\stackrel{\\text { def }}{=} \\sum_{a=0}^{j_{0}} \\tilde{r}_{j_{1}, j_{0}}[a] T_{a}(Z)\n$$\n\nwhere\n\n$$\n\\tilde{r}_{j_{1}, j_{0}}[a]=0 \\text { if }\\left(j_{0}-a\\right) \\text { is odd, }\n$$\n\nWe define structured causal (SC) matrix polynomials as\n\n$$\nq_{\\left(j_{1}, j_{0}\\right)_{\\sqrt{N}}^{N}}(Z)=\\ell_{\\sqrt{N}-j_{1}-1}(Z) \\cdot \\tilde{r}_{\\sqrt{N}-j_{1}-1, \\sqrt{N}-j_{0}-1}\\left(T_{\\sqrt{N}}(Z)\\right)\n$$\n\nA $N \\times N$ SC matrix is defined over the set of real evaluation points as\n\n$$\n\\mathbf{M}^{\\prime}\\left[i,\\left(j_{0}, j_{1}\\right)\\right]=q_{\\left(j_{1}, j_{0}\\right)}^{\\prime N}{ }_{\\sqrt{N}}\\left(\\omega_{N, i}\\right)=q_{\\left(j_{0}, j_{1}\\right)_{\\sqrt{N}}}^{N}\\left(\\omega_{N, i}\\right)\n$$\n\nWe note that in (63) we deviate from the usual ordering of indices $\\left(j_{1}, j_{0}\\right)$ to $\\left(j_{0}, j_{1}\\right)$. We show that the SC matrix falls under the category of matrices defined by (50). Lemma 9. Let $\\mathcal{M}^{C}$ denote the set of all matrices defined by of (50), and $\\mathcal{M}^{S C}$ denote the set of all matrices defined by Definition 6. Then $\\mathcal{M}^{S C} \\subset \\mathcal{M}^{C}$. Proof. To show that $\\mathcal{M}^{S C} \\subset \\mathcal{M}^{C}$, then it is sufficient to show that for $j=j_{0} \\sqrt{N}+j_{1}$, any $q^{\\prime N}{ }_{\\left(j_{1}, j_{0}\\right)_{\\sqrt{N}}}(Z)$ is equivalent to some $\\bar{q}_{j}^{N}(Z)$ as in (48). From (63) we have\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-52.jpg?height=100&width=1040&top_left_y=2113&top_left_x=537)\n\nNote that\n\n$$\n\\begin{aligned}\n\\operatorname{deg}\\left(q^{\\prime N}\\left(j_{1}, j_{0}\\right)_{\\sqrt{N}}\\right) & =\\sqrt{N}-j_{0}-1+\\left(\\sqrt{N}-j_{1}-1\\right) \\sqrt{N} \\\\\n& =N-\\sqrt{N} j_{1}-j_{0}-1 \\\\\n& =N-j-1\n\\end{aligned}\n$$\n\nFrom the fact that $\\operatorname{deg}\\left(T_{a}\\right)=a$, we can use $T_{a}$ as a polynomial basis. Then $q_{\\left(j_{1}, j_{0}\\right)}^{N}$ can be represented as a linear combination of $T_{a}(Z)$ like so:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-53.jpg?height=140&width=661&top_left_y=380&top_left_x=732)\nwhich is exactly the form of (48). Lemma 9 allows us to apply the causality result from Appendix D.5.1. Corollary 7. Fix a family of basis polynomials $q_{0,0}^{N}, q_{0,1}^{N}, \\ldots, q_{\\sqrt{N}-1, \\sqrt{N}-1}^{N}$ as defined in (62). For\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-53.jpg?height=93&width=1647&top_left_y=799&top_left_x=239) as in (63). Then the operation (55) with $\\mathbf{M}_{N} \\leftarrow \\mathbf{M}_{N}^{\\prime}$ defines a causal map in $\\mathbf{u}$. Proof. Follows from Theorem 13 and Lemma 9. ## D.5.3 Block Operations on Structured Causal Matrices\n\nIn this section we show how to build structured causal matrices through block operations. ## Constructing M\n\nRecall that in Appendix D.4.1, we defined $\\mathbf{L}, \\mathbf{R}$ in terms of coefficient matrices $\\widetilde{\\mathbf{R}}, \\widetilde{\\mathbf{L}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ with blocks $\\widetilde{\\mathbf{R}}_{k}, \\widetilde{\\mathbf{L}}_{k}$ for $0 \\leq k<\\sqrt{N}$, noting that for each block $\\mathbf{R}_{j_{1}, j_{1}}$,\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}=\\mathbf{F}_{\\sqrt{N}} \\cdot \\widetilde{\\mathbf{R}}_{j_{1}, j_{1}}\n$$\n\nand for $\\widetilde{\\mathbf{L}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$\n\n$$\n\\mathbf{L}^{\\prime}=\\mathbf{P F}_{N} \\cdot \\widetilde{\\mathbf{L}}\n$$\n\nThese matrices are then diagonalized into $\\mathbf{L}, \\mathbf{R}$. We use Definition 6 to similarly define $\\mathbf{L}, \\mathbf{R}, \\in$ $\\mathbb{R}^{N \\times N}$ with blocks $\\left\\{\\mathbf{L}_{j_{1}, j_{0}}\\right\\}_{0 \\leq j_{1}, j_{0}<\\sqrt{N}},\\left\\{\\mathbf{R}_{j_{1}, j_{0}}\\right\\}_{0 \\leq j_{1}, j_{0}<\\sqrt{N}}$ where:\n\n$$\n\\mathbf{L}_{i_{1}, j_{1}}\\left[i_{0}, i_{0}\\right]=\\ell_{\\sqrt{N}-j_{1}-1}\\left(\\omega_{N, i}\\right) \\quad \\mathbf{R}_{j_{1}, j_{1}}\\left[i_{0}, j_{0}\\right] \\leftarrow \\tilde{r}_{\\sqrt{N}-j_{1}-1, \\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right)\n$$\n\nand all other entries are zero. Let the coefficient matrices $\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}$ be defined with respect to blocks as follows:\n\n$$\n\\widetilde{\\mathbf{L}}\\left[a, j_{1}\\right]=\\ell_{\\sqrt{N}-j_{1}-1}[a] \\quad \\widetilde{\\mathbf{R}}_{j_{1}}\\left[a, j_{0}\\right]=\\tilde{r}_{\\sqrt{N}-j_{1}-1, \\sqrt{N}-j_{0}-1}[a]\n$$\n\nwhere the entries of $\\ell_{\\sqrt{N}-j_{1}-1}$ and $\\tilde{r}_{\\sqrt{N}-j_{1}-1, \\sqrt{N}-j_{0}-1}$ are defined as in (60) and (61). Now let $\\mathbf{C}_{N} \\in \\mathbb{R}^{N \\times N}$ where\n\n$$\n\\mathbf{C}_{N}[i, j]=T_{j}\\left(\\omega_{N, i}\\right)\n$$\n\nbe the Chebyshev transform. Then analogous to Appendix D.4.1, we define\n\n$$\n\\mathbf{R}_{j_{1}, j_{1}}=\\mathbf{C}_{\\sqrt{N}} \\cdot \\widetilde{\\mathbf{R}}_{j_{1}, j_{1}} \\quad \\mathbf{L}^{\\prime}=\\mathbf{P C}_{N} \\cdot \\widetilde{\\mathbf{L}}\n$$\n\nThis allows us to give an algorithm the following construction algorithm for $\\mathbf{L}$ and $\\mathbf{R}$. ```\nAlgorithm 4 BlockSC \\((\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}})\\)\nInput: \\(\\widetilde{\\mathbf{L}}, \\widetilde{\\mathbf{R}} \\in \\mathbb{R}^{N \\times \\sqrt{N}}\\)\nOutput: Block diagonal matrices \\(\\mathbf{L}, \\mathbf{R} \\in \\mathbb{R}^{N \\times N}\\)\n    \\(\\mathbf{L}^{\\prime} \\leftarrow \\mathbf{P} \\cdot \\mathbf{C}_{N} \\cdot \\widetilde{\\mathbf{L}}\\)\n    for \\(a \\leftarrow 0\\) to \\(\\sqrt{N}-1\\) do\n        \\(\\mathbf{L}_{a}^{\\prime} \\leftarrow \\overline{\\mathbf{L}}^{\\prime}[a \\sqrt{N}: a \\sqrt{N}+\\sqrt{N}-1,:]\\)\n    \\(\\mathbf{L} \\leftarrow \\operatorname{diag}\\left(\\mathbf{L}_{0}^{\\prime}, \\ldots \\mathbf{L}_{\\sqrt{N}-1}^{\\prime}\\right)\\)\n                                    \\(\\triangleright\\) Now compute \\(\\mathbf{R}\\) from \\(\\widetilde{\\mathbf{R}}\\)\n    for \\(a \\leftarrow 0\\) to \\(\\sqrt{N}-1\\) do\n        \\(\\mathbf{R}_{a} \\leftarrow \\mathbf{C}_{\\sqrt{N}} \\cdot \\widetilde{\\mathbf{R}}[a \\sqrt{N}: a \\sqrt{N}+\\sqrt{N}-1,:]\\)\n    \\(\\mathbf{R} \\leftarrow \\operatorname{diag}\\left(\\mathbf{R}_{0}, \\ldots, \\mathbf{R}_{\\sqrt{N}-1}\\right)\\)\n    return \\(\\mathbf{L}, \\mathbf{R}\\)\n```\n\nWe use Algorithm 4 to specify another type of matrix $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}$. Lemma 10. Let\n\n$$\n\\mathbf{M}=\\mathbf{P L P R P}\n$$\n\nwhere $\\mathbf{L}$ and $\\mathbf{R}$ are outputs from BlockSC. Then each entry in $\\mathbf{M}$ is defined as\n\n$$\n\\mathbf{M}[i, j]=\\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}\\left(\\omega_{\\sqrt{N}, i_{0}}\\right)\n$$\n\nProof. Let $\\mathbf{M}_{0}=\\mathbf{P L P R}$, then $\\mathbf{M}=\\mathbf{M}_{0} \\mathbf{P}$. Then we have\n\n$$\n\\mathbf{M}_{0}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\ell_{\\sqrt{N}-j_{1}-1}\\left(\\omega_{N, i}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-j_{1}-1, \\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i_{0}}\\right)\n$$\n\nThen we get\n\n$$\n\\mathbf{M}[i, j]=\\mathbf{M}_{0} \\mathbf{P}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\mathbf{M}_{0}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{0}, j_{1}\\right)\\right]\n$$\n\nThis gives us\n\n$$\n\\mathbf{M}\\left[\\left(i_{1}, i_{0}\\right),\\left(j_{1}, j_{0}\\right)\\right]=\\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}\\left(\\omega_{N, i_{0}}\\right)\n$$\n\nas desired. Next we will discuss the relation between matrices that we get from Lemma 10 and SC matrices. ## D.5.4 Constructing $\\mathrm{M}^{\\prime}$ from M\n\nSince $\\mathbf{C}_{N} \\notin \\mathcal{M}^{S C}{ }^{6}$, Algorithm 4 is not a proper analog of Algorithm 2. In this section, we show how to convert $\\mathbf{M}$ produced from Algorithm 4 into a matrix with a block decomposible form. Recall from (63) that\n\n$$\n\\mathbf{M}^{\\prime}[i, j]=q_{\\left(j_{1}, j_{0}\\right) \\sqrt{N}}^{\\prime N}\\left(\\omega_{N, i}\\right)=\\ell_{\\sqrt{N}-j_{0}-1}(Z) \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}\\left(T_{\\sqrt{N}}(Z)\\right)\n$$\n\n[^5]We note the distinction between the above and (67). Specifically, we note that $\\mathbf{M}$ is evaluated on two sets of evaluation points- the $\\ell_{\\sqrt{N}-j_{1}-1}(Z)$ and $\\tilde{r}_{\\sqrt{N}-j_{1}-1, \\sqrt{N}-j_{0}-1}$ polynomials are evaluated over the $N$ roots and $\\sqrt{N}$ roots of unity while $\\mathbf{M}^{\\prime}$ is only evaluated the $N$-th roots of unity. However, they are close due to the following property:\n\nLemma 11. Let $T_{a}$ be a Chebyshev polynomial of the first kind of degree a, and define $\\omega_{N, i}$ as in (49). Then\n\n$$\nT_{\\sqrt{N}}\\left(\\omega_{N, i}\\right)=(-1)^{i_{1}} \\cdot \\omega_{\\sqrt{N}, i_{0}}\n$$\n\nProof. $$\n\\begin{aligned}\nT_{\\sqrt{N}}\\left(\\omega_{N, i}\\right) & =\\cos \\left(\\frac{\\sqrt{N}\\left(i_{1} \\sqrt{N}+i_{0}+\\frac{1}{2}\\right) \\pi}{N}\\right) \\\\\n& =\\cos \\left(i_{1} \\pi+\\frac{\\pi\\left(i_{0}+\\frac{1}{2}\\right)}{\\sqrt{N}}\\right) \\\\\n& =(-1)^{i_{1}} \\cos \\left(\\frac{\\pi\\left(i_{0}+\\frac{1}{2}\\right)}{\\sqrt{N}}\\right) \\\\\n& =(-1)^{i_{1}} \\cdot \\omega_{\\sqrt{N}, i_{0}} . \\end{aligned}\n$$\n\nIn the above the first equality follows from (46). Analogous to how we utilized roots of unity, Lemma 11 allows us to express our basis polynomials in terms of two related sets of evaluation points, $\\omega_{\\sqrt{N}, i_{0}}$ and $\\omega_{N, i}$. The following lemma demonstrates how to translate between $\\mathbf{M}^{\\prime}$ and $\\mathbf{M}$. Lemma 12. For all $i, j$,\n\n$$\n\\mathbf{M}^{\\prime}[i, j]=(-1)^{i_{i}(\\sqrt{N}-1)}(-1)^{i_{i} j_{1}} \\cdot \\mathbf{M}[i, j]\n$$\n\nProof. By (68) we have\n\n$$\n\\begin{aligned}\n\\mathbf{M}^{\\prime}[i, j] & =\\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}\\left(T_{\\sqrt{N}}\\left(\\omega_{N, i}\\right)\\right) \\\\\n& =\\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}\\left((-1)^{i_{1}} \\omega_{\\sqrt{N}, i_{0}}\\right)\n\\end{aligned}\n$$\n\nwhere the second statement follows from Lemma 11. Then from (60) and (47) we get\n\n$$\n\\begin{aligned}\n\\mathbf{M}^{\\prime}[i, j] & =\\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\sum_{a=0}^{\\sqrt{N}-j_{1}-1} \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}[a] T_{a}\\left((-1)^{i_{1}} \\omega_{\\sqrt{N}, i_{0}}\\right) \\\\\n& =\\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\sum_{a=0}^{\\sqrt{N}-j_{1}-1} \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}[a](-1)^{i_{1} a} T_{a}\\left(\\omega_{\\sqrt{N}, i_{0}}\\right) . \\end{aligned}\n$$\n\nNote from (61) that $\\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}[a]=0$ if $\\left(\\sqrt{N}-j_{1}-1-a\\right)$ is odd. Then equivalently we get\n\n$$\n\\begin{aligned}\n\\mathbf{M}^{\\prime}[i, j]= & \\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot(-1)^{i_{1}\\left(\\sqrt{N}-j_{1}-1\\right)} \\\\\n& \\cdot \\sum_{a=0}^{\\sqrt{N}-j_{1}-1} \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}[a](-1)^{i_{1}\\left(\\sqrt{N}-j_{1}-1-a\\right)} T_{a}\\left(\\omega_{\\sqrt{N}, i_{0}}\\right) \\\\\n= & \\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot(-1)^{i_{1}\\left(\\sqrt{N}-j_{1}-1\\right)}\\left(\\sum_{a=0}^{\\sqrt{N}-j_{1}-1} \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}[a] T_{a}\\left(\\omega_{\\sqrt{N}, i_{0}}\\right)\\right) \\\\\n= & (-1)^{i_{1}\\left(\\sqrt{N}-j_{1}-1\\right)} \\ell_{\\sqrt{N}-j_{0}-1}\\left(\\omega_{N, i}\\right) \\cdot \\tilde{r}_{\\sqrt{N}-j_{0}-1, \\sqrt{N}-j_{1}-1}\\left(\\omega_{\\sqrt{N}, i_{0}}\\right) . \\end{aligned}\n$$\n\nThen from (67) we have\n\n$$\n\\begin{aligned}\n\\mathbf{M}^{\\prime}[i, j] & =(-1)^{i_{1}\\left(\\sqrt{N}-j_{1}-1\\right)} \\mathbf{M}[i, j] \\\\\n& =(-1)^{i_{1}(\\sqrt{N}-1)}(-1)^{i_{1} j_{1}} \\mathbf{M}[i, j]\n\\end{aligned}\n$$\n\nBlock Matrix Multiplication for Structured Causal Matrices Lemma 12 shows us how to compute $\\mathbf{M}^{\\prime}$ from $\\mathbf{M}$. However, this does not mean that the matrix vector multiplication problem for $\\mathbf{M}^{\\prime}$ can be implemented efficiently. In this section we show how to perform matrix vector multiplication of $\\mathbf{M}^{\\prime}$ with any $\\mathbf{u} \\in \\mathbb{R}^{N}$ from two matrix-vector multiplications of $\\mathbf{M}$ (and so these operations are indeed efficient). The key to our approach involves considering the parity of each block index $0 \\leq i_{1}<\\sqrt{N}$. Let us define a map $\\operatorname{Mix}: \\mathbb{R}^{\\left\\lfloor\\frac{N}{2}\\right\\rfloor} \\times \\mathbb{R}^{\\left\\lceil\\frac{N}{2}\\right\\rceil} \\mapsto \\mathbb{R}^{N}$ such that $\\operatorname{Mix}\\left(\\mathbf{u}_{0}, \\mathbf{u}_{1}\\right)=\\mathbf{u}$ where\n\n$$\n\\mathbf{u}[i]=\\mathbf{u}_{i_{1}} \\quad \\bmod { }_{2}[i / 2]\n$$\n\nWe use this map to show that $\\mathbf{M}^{\\prime} \\mathbf{u}$ and $\\mathbf{u}^{\\top} \\mathbf{M}^{\\prime}$ can be computed efficiently. Lemma 13. For any $\\mathbf{u} \\in \\mathbb{R}^{N}$ and $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}, \\mathbf{M}^{\\prime} \\mathbf{u}$ can be computed via two matrix-vector multiplications: $\\mathbf{M} \\cdot \\operatorname{Mix}\\left(\\mathbf{u}_{0}, \\mathbf{0}\\right)$ and $\\mathbf{M} \\cdot \\operatorname{Mix}\\left(\\mathbf{0}, \\mathbf{u}_{1}\\right)$. Proof. For $0 \\leq i=i_{1} \\sqrt{N}+i_{0}<N, 0 \\leq j=j_{1} \\sqrt{N}+j_{0}<N$, let $\\mathbf{D} \\in \\mathbb{R}^{N \\times N}$ be the diagonal matrix defined such that $\\mathbf{D}[i, i]=(-1)^{i_{1}(\\sqrt{N}-1)}$. Lemma 12 implies that\n\n$$\n\\mathbf{M}^{\\prime}[i, j]= \\begin{cases}\\mathbf{D} \\mathbf{M}[i, j] & \\text { if } i_{1} \\text { is even } \\\\ \\mathbf{D M}[i, j] & \\text { if } i_{1} \\text { is odd, } j_{1} \\text { is even } \\\\ -(\\mathbf{D M}[i, j]) & \\text { otherwise. }\\end{cases}\n$$\n\nWe want to shift the $(-1)$ to $\\mathbf{u} . C$ Compute $\\mathbf{z}_{0}=\\mathbf{M} \\cdot\\left(\\operatorname{Mix}\\left(\\mathbf{u}_{0}, \\mathbf{0}\\right)+\\operatorname{Mix}\\left(\\mathbf{0}, \\mathbf{u}_{1}\\right)\\right), \\mathbf{z}_{1}=\\mathbf{M}$. $\\left(\\operatorname{Mix}\\left(\\mathbf{u}_{0}, \\mathbf{0}\\right)-\\operatorname{Mix}\\left(\\mathbf{0}, \\mathbf{u}_{1}\\right)\\right)$. Define $\\mathbf{y}^{\\prime}$ such that\n\n$$\n\\mathbf{y}^{\\prime}[j]= \\begin{cases}\\mathbf{z}_{0}[j] & \\text { if } j_{1} \\text { is even } \\\\ \\mathbf{z}_{1}[j] & \\text { if } j_{1} \\text { is odd }\\end{cases}\n$$\n\nIt can be verified that $\\mathbf{D} \\cdot \\mathbf{y}^{\\prime}=\\mathbf{M}^{\\prime} \\mathbf{u}$, which completes the proof. We now give the analogous result for $\\mathbf{u}^{\\top} \\mathbf{M}^{\\prime}$. Lemma 14. For any $\\mathbf{u} \\in \\mathbb{R}^{N}$ and $\\mathbf{M} \\in \\mathbb{R}^{N \\times N}$, $\\mathbf{u}^{\\top} \\mathbf{M}^{\\prime}$ can be computed via two matrix-vector multiplications: $\\operatorname{Mix}\\left(\\mathbf{u}_{0}, \\mathbf{0}\\right)^{\\top} \\mathbf{M}$ and $\\operatorname{Mix}\\left(\\mathbf{0}, \\mathbf{u}_{1}\\right)^{\\top} \\mathbf{M}$. Proof. For $0 \\leq i=i_{1} \\sqrt{N}+i_{0}<N, 0 \\leq j=j_{1} \\sqrt{N}+j_{0}<N$, let $\\mathbf{D} \\in \\mathbb{R}^{N \\times N}$ be the diagonal matrix defined such that $\\mathbf{D}[i, i]=(-1)^{i_{1}(\\sqrt{N}-1)}$, and $\\mathbf{u}^{\\prime}=\\mathbf{u}^{\\top} \\mathbf{D}=\\operatorname{Mix}\\left(\\mathbf{u}_{0}^{\\prime}, \\mathbf{u}_{1}^{\\prime}\\right)$. Lemma 12 implies that\n\n$$\n\\mathbf{M}^{\\prime}[i, j]= \\begin{cases}\\mathbf{D M}[i, j] & \\text { if } i_{1} \\text { is even } \\\\ \\mathbf{D M}[i, j] & \\text { if } i_{1} \\text { is odd, } j_{1} \\text { is even } \\\\ -(\\mathbf{D M}[i, j]) & \\text { otherwise. }\\end{cases}\n$$\n\nWe want to shift the $(-1)$ to $\\mathbf{u}^{\\prime}$. Compute $\\mathbf{z}_{0}=\\left(\\operatorname{Mix}\\left(\\mathbf{u}_{0}^{\\prime}, \\mathbf{0}\\right)^{\\top}+\\operatorname{Mix}\\left(\\mathbf{0}, \\mathbf{u}_{1}^{\\prime}\\right)^{\\top}\\right) \\cdot \\mathbf{M}, \\mathbf{z}_{1}=$ $\\left(\\operatorname{Mix}\\left(\\mathbf{u}_{0}^{\\prime}, \\mathbf{0}\\right)^{\\top}-\\operatorname{Mix}\\left(\\mathbf{0}, \\mathbf{u}_{1}^{\\prime}\\right)^{\\top}\\right) \\cdot \\mathbf{M}$. If $\\mathbf{y}=\\mathbf{M}^{\\prime} \\mathbf{u}$, then one can check that\n\n$$\n\\mathbf{y}[j]= \\begin{cases}\\mathbf{z}_{0}[j] & \\text { if } j_{1} \\text { is even } \\\\ \\mathbf{z}_{1}[j] & \\text { if } j_{1} \\text { is odd }\\end{cases}\n$$\n\nwhich completes the proof. Lemma 13 implies that computing $\\mathbf{M}_{N} \\mathbf{k}$ and $\\mathbf{M}_{N} \\mathbf{u}$ in (55) can be done efficiently. However, we do not know how to compute $\\mathbf{M}_{N}^{-1} \\mathbf{y}$ for any arbitrary $\\mathbf{y}$. We address this partially in the next section. ## D.5.5 Inverse of Chebyschev Transform\n\nIn this subsection we show how computing $\\mathbf{C}_{N}^{-1}$ can be done efficiently. Lemma 15. Let $\\mathbf{C}_{N} \\in \\mathbb{R}^{N \\times N}$ be defined as (66). Then\n\n$$\n\\mathbf{C}_{N}^{-1}=\\mathbf{C}_{N}^{\\top} \\cdot \\operatorname{diag}(1 / N, 2 / N, \\ldots, 2 / N)\n$$\n\nProof. Follows since $\\mathbf{C}_{N}\\left(\\mathbf{C}_{N}\\right)^{\\top}=\\operatorname{diag}(N, N / 2, N / 2, \\ldots, N / 2)$ [70]. Given the above, the goal then is to compute $\\mathbf{C}_{N}^{\\top} \\cdot \\mathbf{u}$ or equivalently $\\mathbf{u}^{\\top} \\mathbf{C}_{N}$ for any $\\mathbf{u} \\in \\mathbb{R}^{N}$. Note that is sufficient to show that\n\n$$\n\\mathbf{C}_{N}=\\overline{\\mathbf{C}}_{N}+\\overline{\\mathbf{S}}_{N}\n$$\n\nfor $\\overline{\\mathbf{C}}_{N}, \\overline{\\mathbf{S}}_{N} \\in \\mathbb{R}^{N \\times N}$ such that $\\mathbf{u}^{\\top} \\overline{\\mathbf{C}}_{N}$ and $\\mathbf{u}^{\\top} \\overline{\\mathbf{S}}_{N}$ can be computed with two invocations of Lemma 14. Indeed we have\n\n$$\n\\begin{aligned}\n\\mathbf{C}_{N}[i, j] & =\\cos \\left(\\frac{\\left(i+\\frac{1}{2}\\right)\\left(j_{1} \\sqrt{N}+j_{0}\\right) \\pi}{N}\\right) \\\\\n& =\\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}+\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right) \\\\\n& =\\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right)-\\sin \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\sin \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right)\n\\end{aligned}\n$$\n\nWe use this to define $\\overline{\\mathbf{C}}_{N}$ and $\\overline{\\mathbf{S}}_{N}$, along with two additional matrices $\\widetilde{\\mathbf{C}}_{N}, \\widetilde{\\mathbf{S}}_{N} \\in \\mathbb{R}^{N \\times N}$ such that for $\\overline{\\mathbf{C}}_{N}$ and $\\widetilde{\\mathbf{C}}_{N}$ :\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{C}}_{N}[i, j] & =\\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right) \\\\\n& =\\cos \\left(\\pi i_{1} j_{1}+\\frac{\\pi\\left(i_{0}+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right) \\\\\n& =(-1)^{i_{1} j_{1}} \\cos \\left(\\frac{\\pi\\left(i_{0}+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\cos \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right) \\stackrel{\\text { def }}{=}(-1)^{i_{1} j_{1}} \\widetilde{\\mathbf{C}}_{N}[i, j]\n\\end{aligned}\n$$\n\nand similarly for $\\overline{\\mathbf{S}}_{N}$ and $\\widetilde{\\mathbf{S}}_{N}$ :\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{S}}_{N}[i, j] & =\\sin \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\sin \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right) \\\\\n& =(-1)^{i_{1} j_{1}} \\sin \\left(\\frac{\\pi\\left(i_{0}+\\frac{1}{2}\\right) j_{1}}{\\sqrt{N}}\\right) \\sin \\left(\\frac{\\pi\\left(i+\\frac{1}{2}\\right) j_{0}}{N}\\right) \\stackrel{\\text { def }}{=}(-1)^{i_{1} j_{1}} \\widetilde{\\mathbf{S}}_{N}[i, j]\n\\end{aligned}\n$$\n\nWe summarize these results into the following lemma. Lemma 16. Define $\\overline{\\mathbf{C}}_{N}, \\widetilde{\\mathbf{C}}_{N}, \\overline{\\mathbf{S}}_{N}$ and $\\widetilde{\\mathbf{S}}_{N}$ as in (71) and (72), respectively. Then\n\n$$\n\\mathbf{C}_{N}=\\overline{\\mathbf{C}}_{N}-\\overline{\\mathbf{S}}_{N}\n$$\n\nwhere $\\widetilde{\\mathbf{C}}_{N}$ and $\\widetilde{\\mathbf{S}}_{N}$ are of the form (67). Finally Lemma 14 and Lemma 16 imply the following:\nTheorem 14. For any $\\mathbf{u} \\in \\mathbb{R}^{N}$,\n\n$$\n\\mathbf{y}=\\mathbf{u}^{\\top} \\mathbf{C}_{N}\n$$\n\ncan be computed from four calls matrix vector multiplication with Monarch matrices. Proof Sketch for Theorem 14 Lemma 16 tells us that $\\mathbf{u}^{\\top} \\overline{\\mathbf{C}}_{N}$ and $\\mathbf{u}^{\\top} \\overline{\\mathbf{S}}_{N}$ are sufficient to compute $\\mathbf{u}^{\\top} \\mathbf{C}_{N}$, and (72) shows us that $\\widetilde{\\mathbf{C}}_{N}, \\widetilde{\\mathbf{S}}_{N}$ are Monarch matrices that allows $\\mathbf{u}^{\\top} \\overline{\\mathbf{C}}_{N}$ and $\\mathbf{u}^{\\top} \\overline{\\mathbf{S}}_{N}$ to be computed from two matrix- vector multiplications with $\\widetilde{\\mathbf{C}}_{N}$ and $\\widetilde{\\mathbf{S}}_{N}$, respectively.",
    "m2-60": "Then the claim follows from applying Lemma 14 twice. ## D. 6 Multivariate Monarch Mixer with Block Size $=\\sqrt[p]{N}$\n\nIn this section, we generalize Theorem 1, Theorem 2, and Theorem 3 to arbitrary $p \\geq 2$. In Appendix D.6.1, we set up notation. We then generalize Theorem 1 and Theorem 2 to general $p$ in Appendix D.6.2 and then generalize Theorem 3 in Appendix D.6.3. In Appendix D.6.4 we connect definition of $p$-variate Monarch in Appendix D.6.2 to the one defined in Section 3. Finally in Appendix D.6.5 we discuss some extensions and generalizations. ## D.6.1 Notation\n\nWe will use notation from Appendix D.5.1. Fix an integer $p \\geq 2$. We will be working with indices $\\mathbf{j}, \\mathbf{i}$ where $\\mathbf{j}=\\left(j_{0}, \\ldots, j_{p-1}\\right)$ and $\\mathbf{i}=\\left(i_{0}, \\ldots, i_{p-1}\\right)$ with $0 \\leq i_{a}, j_{a}<\\sqrt[p]{N}$ for every $0 \\leq a<p$. We will denote the set of all sub-indices as $[0, \\sqrt[p]{N})^{p}$, and the operator $\\preceq$ to denote the lexicographical ordering of vectors in $[0, \\sqrt[p]{N})^{p}$. For any $0 \\leq b^{\\prime} \\leq M \\leq N$ such that $b^{\\prime}$ divides $M$ and $M$ divides $N$, define the following permutation matrices:\n\n$$\n\\mathbf{P}_{b^{\\prime}, M, N}=\\operatorname{diag}(\\underbrace{\\mathbf{P}_{b^{\\prime}, M}, \\ldots, \\mathbf{P}_{b^{\\prime}, M}}_{\\frac{N}{M} \\text { times }}) . $$\n\nNote that $\\mathbf{P}_{b^{\\prime}, N, N}$ is exactly the same as $\\mathbf{P}_{b^{\\prime}, N}$ from earlier. If we use $\\sigma(b, M, N)$ to denote the corresponding permutation then it takes the input $\\left(i_{p-1}, \\ldots, i_{0}\\right)_{b}$ and maps it to $\\left(i_{p-1}, \\ldots, i_{p^{\\prime}}, i_{p^{\\prime}-2}, \\ldots, i_{0}, i_{p^{\\prime}}\\right)_{b}\\left(\\right.$ where $p=\\log _{b} N$ and $\\left.p^{\\prime}=\\log _{b} M\\right)$. I.e. $\\sigma(b, M, N)$ does not change the first $p-p^{\\prime}$ sub-indices and then does a left rotation on the remaining $p^{\\prime}$ sub-indices. For the rest of the section, assume $b=\\sqrt[p]{N}$ and then consider the following 'sub-index reversal' permutation matrix ${ }^{7}:$\n\n$$\n\\mathbf{P}_{b, b^{p}}^{R}=\\prod_{a=0}^{p-2} \\mathbf{P}_{b^{p-a-1}, b^{p-a}, b^{p}}\n$$\n\nIf $\\sigma^{R}(b, N)$ is the permutation corresponding to the permutation matrix above, then $\\sigma^{R}(b, N)$ maps $\\left(i_{p-1}, \\ldots, i_{0}\\right) \\mapsto\\left(i_{0}, \\ldots, i_{p-1}\\right)$. ## D.6.2 Generalizing Theorem 1 and Theorem 2\n\nFor a specific $0 \\leq a<p$, let\n\n$$\n\\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(X_{a}\\right)=\\sum_{m=0}^{\\sqrt[p]{N}-1} \\ell_{\\left(j_{a+1}, \\ldots, j_{p-1}\\right),\\left(i_{0}, \\ldots, i_{a-1}\\right)}^{(a)}[m] \\cdot T_{m}\\left(X_{a}\\right)\n$$\n\nbe an arbitrary polynomial of degree $<\\sqrt[p]{N}$ in the Chebyshev polynomial basis (see (47)). We will be interested in the evaluations of the above polynomials over the set\n\n$$\nA \\stackrel{\\text { def }}{=}\\left(\\omega_{\\sqrt[p]{N}, 0}, \\ldots, \\omega_{\\sqrt[p]{N}}, \\sqrt[p]{N}-1\\right)\n$$\n\nwhere $\\omega_{\\sqrt{N}, i}$ is defined as in (??). The $p$-variate version of Monarch matrices $\\mathbf{M}^{\\prime} \\in \\mathbb{R}^{N \\times N}$ as follows. (See Appendix D.6.4 to see how these are exactly related to the definition in (1)). For every row index $\\mathbf{i} \\in[0, \\sqrt[p]{N})^{p}$ and column index $\\mathbf{j} \\in[0, \\sqrt[p]{N})^{p}$, we have\n\n$$\n\\mathbf{M}^{\\prime}[\\mathbf{i}, \\mathbf{j}]=\\prod_{a=0}^{p-1} \\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(\\omega_{\\sqrt[p]{N}, i_{a}}\\right)\n$$\n\n[^6]To express the above in terms of a polynomial basis we will need the following definition. For any $0 \\leq a<p$ and $\\mathbf{i}$ define the Lagrange basis polynomial $\\Delta_{\\mathbf{i}}^{(a)}\\left(X_{0}, \\ldots, X_{a-1}\\right)$ such that for any $0 \\leq m_{0}, \\ldots, m_{a-1}<\\sqrt[p]{N}$, we have\n\n$$\n\\Delta_{\\mathbf{i}}^{(a)}\\left(\\omega_{\\sqrt[P]{N}, m_{0}}, \\ldots, \\omega_{\\sqrt[p]{N}, m_{a-1}}\\right)= \\begin{cases}1 & \\text { if } i_{0}=m_{0}, i_{1}=m_{1}, \\ldots, i_{a-1}=m_{a-1} \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nWe use the above to convert the polynomials in (73) to not depend on the sub-indices in $\\mathbf{i}$ (at least for the definition in (75)). For every $\\mathbf{j}$ and $0 \\leq a<p$, define:\n\n$$\n\\ell_{\\mathbf{j}}^{(a)}\\left(X_{0}, \\ldots, X_{a}\\right)=\\sum_{\\mathbf{i}=\\left(i_{0}, \\ldots, i_{a-1}, \\mathbf{0}_{p-a}\\right), i_{0}, \\ldots, i_{a-1} \\in[0, \\sqrt[p]{N})} \\Delta_{\\mathbf{i}}^{(a)}\\left(X_{0}, \\ldots, X_{a-1}\\right) \\cdot \\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(X_{a}\\right)\n$$\n\nNote that the summation fixes the last $p-a$ sub-indices in $\\mathbf{i}$ since the definition of $\\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(X_{a}\\right)$ only depends on $\\left(i_{0}, \\ldots, i_{a-1}\\right)$. This implies that for any $0 \\leq i_{0}, \\ldots, i_{a}<\\sqrt[p]{N}$, we have\n\n$$\n\\ell_{\\mathbf{j}}^{(a)}\\left(\\omega_{\\sqrt[p]{N}, i_{0}}, \\ldots, \\omega_{\\sqrt[p]{N}, i_{a}}\\right)=\\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(\\omega_{\\sqrt[p]{N}, i_{a}}\\right)\n$$\n\nWe are now ready to define our basis $p$-variate polynomials. For any index $\\mathbf{j} \\in[0, \\sqrt[p]{N})^{p}$, define\n\n$$\nq_{\\mathbf{j}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\prod_{a=0}^{p-1} \\ell_{\\mathbf{j}}^{(a)}\\left(X_{0}, \\ldots, X_{a}\\right)\n$$\n\nThen (75) can be re-written as\n\n$$\n\\mathbf{M}[\\mathbf{i}, \\mathbf{j}]=q_{\\mathbf{j}}^{N}\\left(\\omega_{\\sqrt[p]{N}, i_{0}}, \\ldots, \\omega_{\\sqrt[p]{N}, i_{p-1}}\\right)\n$$\n\nThe above leads to the following result, which generalizes Theorem 1 to general $p$ :\nTheorem 15. Let $\\mathbf{M}^{\\prime}, A$ and $q_{\\mathbf{j}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)$ be as defined in (75), (74) and (77). Then for any vector $\\mathbf{u}, \\mathbf{M} \\cdot \\mathbf{u}$ is equivalent to evaluating the polynomial\n\n$$\nu\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\sum_{\\mathbf{j}} u_{\\mathbf{j}} \\cdot q_{\\mathbf{j}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)\n$$\n\nat each point in $A^{p}$. Proof. By (78), the column $\\mathbf{M}^{\\prime}[:, j]$ is exactly the evaluation of the polynomial (77) at each point in $A^{p}$. Then the claim follows from the definition of matrix vector multiplication and (79). This then leads to the following result, (which generalizes Theorem 2):\nTheorem 16. For matrices $\\mathbf{M}_{0}, \\mathbf{M}_{1}, \\mathbf{M}_{2}$, each of form as in Theorem 15 , the operation\n\n$$\n\\mathbf{f}=\\mathbf{M}_{0}^{-1} \\cdot\\left(\\left(\\mathbf{M}_{1} \\cdot \\mathbf{k}\\right) \\odot\\left(\\mathbf{M}_{2} \\cdot \\mathbf{u}\\right)\\right)\n$$\n\nis equivalent to representing the polynomial\n\n$$\nf(\\mathbf{X})=k(\\mathbf{X}) \\cdot u(\\mathbf{X}) \\quad \\bmod \\left(T_{\\sqrt[p]{N}}\\left(X_{0}\\right), \\ldots, T_{\\sqrt[p]{N}}\\left(X_{p-1}\\right)\\right)\n$$\n\nin terms of the basis polynomials\n\n$$\n\\hat{q}_{\\mathbf{j}}^{N}(\\mathbf{X})\n$$\n\nwhere $k(\\mathbf{X}), u(\\mathbf{X})$ are defined in terms of the respective basis polynomials corresponding to $\\mathbf{M}_{1}$ and $\\mathbf{M}_{2}$ as in Theorem 15, and $\\hat{q}_{\\mathbf{j}}^{N}(\\mathbf{X})$ s corresponds to $\\mathbf{M}_{0}$. Proof. Define\n\n$$\nq_{A}(Z)=\\prod_{\\alpha \\in A}(Z-\\alpha)\n$$\n\nThen (80) follows since for\n\n$$\nf(\\mathbf{X})=k(\\mathbf{X}) \\cdot u(\\mathbf{X}) \\quad \\bmod \\left(q_{A}\\left(X_{0}\\right), \\ldots q_{A}\\left(X_{p-1}\\right)\\right. $$\n\nwe have the following for any $\\mathbf{a} \\in A^{p}$ we have:\n\n$$\nf(\\mathbf{a})=k(\\mathbf{a}) \\cdot u(\\mathbf{a})\n$$\n\nUsing the known fact that $T_{\\sqrt[p]{N}}(Z)=\\prod_{c=0}^{\\sqrt[p]{N}-1}\\left(Z-\\omega_{\\sqrt[p]{N}, c}\\right)=q_{A}(Z)$, the claim follows from Theorem 15, (80), and the invertibility of $\\mathbf{M}_{0}$. ## D.6.3 Generalizing Theorem 3 for $p \\geq 2$\n\nTo convert Theorem 16 into a causal map we basically have to blow up $n \\rightarrow 2^{p} \\cdot n$. Further, paralleling (48) we need to change the definition in (73) to have degree $\\sqrt[p]{N}-j_{a}-1$ instead of the earlier $\\sqrt[p]{N}-1$ :\n\n$$\n\\tilde{\\ell}_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(X_{a}\\right)=\\sum_{m=0}^{\\sqrt[p]{N}-j_{a}-1} \\widetilde{\\ell}_{\\left(j_{a}\\right)}^{(a)}[m] \\cdot T_{m}\\left(X_{a}\\right)\n$$\n\nNote that now the RHS only depends on $a$ and $j_{a}$ (let us call the RHS $\\tilde{\\ell}_{j_{a}}^{(a, \\sqrt[p]{N})}\\left(X_{a}\\right)$ ), so the next definition becomes easier:\n\n$$\n\\tilde{\\ell}_{\\mathbf{j}}^{(a)}\\left(X_{0}, \\ldots, X_{a}\\right)=\\widetilde{\\ell}_{j_{a}}^{(a, \\sqrt[p]{N})}\\left(X_{a}\\right)\n$$\n\nWe are now ready to define our causal basis polynomials. For any index $\\mathbf{j}$, define\n\n$$\n\\widetilde{q}_{\\mathbf{j}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\prod_{a=0}^{p-1} \\widetilde{\\ell}_{\\mathbf{j}}^{(a)}\\left(X_{0}, \\ldots, X_{a}\\right)\n$$\n\nThese polynomials form a structured subclass of the polynomials defined in (77). Lemma 17. The class of polynomials $\\widetilde{q}_{\\mathbf{j}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)$ defined in (83) are a special case of (77). Proof. This follows from the fact that (82) is a special case of (73) for every i, $\\mathbf{j}, 0 \\leq a<p$. We show that the product of two $\\tilde{q}_{\\mathbf{j}}^{\\left\\lfloor\\frac{p / N}{N}\\right\\rfloor^{p}}\\left(X_{0}, \\ldots, X_{p-1}\\right)$ type polynomials can be written has a linear combination of $\\tilde{q}_{\\mathbf{m}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)$ with the indices of $\\mathbf{m}$ being lexicographically larger than the original indices. ![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-61.jpg?height=113&width=1568&top_left_y=2166&top_left_x=238)\n\n$$\n\\left.\\tilde{q}_{\\mathbf{j}}^{\\left\\lfloor\\frac{p \\sqrt{N}}{2}\\right\\rfloor^{p}}\\left(X_{0}, \\ldots, X_{p-1}\\right) \\cdot \\tilde{q}_{\\mathbf{j}^{\\prime}}^{\\left\\lfloor\\frac{p \\sqrt{N}}{2}\\right.}\\right\\rfloor^{p}\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\sum_{\\mathbf{j}+\\mathbf{j}^{\\prime} \\preceq \\mathbf{m} \\in[0, \\sqrt[p]{N})^{p}} \\alpha_{\\mathbf{j}+\\mathbf{j}^{\\prime}, \\mathbf{m}} \\widetilde{q}_{\\mathbf{m}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)\n$$\n\nfor some set of coefficients $\\alpha_{\\mathbf{j}+\\mathbf{j}^{\\prime}, \\mathbf{m}}$. Proof. From (83) we have,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-62.jpg?height=138&width=1436&top_left_y=316&top_left_x=312)\n\nLet us fix $0 \\leq a<p$. Because (82) is of the same form as in (48), we can apply Lemma 8 to each product $\\tilde{\\ell}_{j_{a}}^{\\left(a,\\left\\lfloor\\frac{p_{N}^{N}}{2}\\right\\rfloor\\right)}\\left(X_{a}\\right)$. $\\tilde{\\ell}_{j_{a}^{\\prime}}^{\\left(a,\\left\\lfloor\\frac{p \\sqrt{N}}{2}\\right\\rfloor\\right)}\\left(X_{a}\\right)$, which gives us\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-62.jpg?height=157&width=1167&top_left_y=713&top_left_x=471)\n\nGoing back to (85), we get\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-62.jpg?height=146&width=1488&top_left_y=941&top_left_x=316)\n\nLet $\\alpha_{\\mathbf{j}+\\mathbf{j}^{\\prime}, \\mathbf{m}}=\\prod_{a=0}^{p-1} \\alpha_{j_{a}+j_{a}^{\\prime}, m_{a}}^{(a)}$. Then we get\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-62.jpg?height=153&width=1577&top_left_y=1173&top_left_x=274)\nas desired. We now define the following padding scheme, $\\operatorname{PAD}(\\mathbf{k})$. ```\nAlgorithm \\(5 \\operatorname{PAD}(\\mathbf{k})\\)\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-62.jpg?height=101&width=939&top_left_y=1543&top_left_x=238)\n\n```\nOutput: \\(\\mathbf{k}^{\\prime} \\in\\left(\\mathbb{R}^{\\sqrt[p]{N}}\\right)^{p}\\)\n    for \\(\\mathbf{j} \\in\\left[0, \\frac{p \\sqrt{N}}{2}\\right)^{p}\\) do\n        if \\(j_{a} \\geq\\left\\lfloor\\frac{p}{N}\\right\\rfloor\\) for \\(0 \\leq a<p\\) then\n            \\(\\mathbf{k}_{\\left(j_{0}, \\ldots, j_{p-1}\\right)}^{\\prime} \\leftarrow \\mathbf{k}_{\\left(j_{0}+\\left\\lceil\\frac{p / \\bar{N}}{2}\\right\\rceil, \\ldots, j_{p-1}+\\left\\lceil\\frac{p \\bar{N}}{2}\\right\\rceil\\right)}\\)\n        else\n            \\(\\mathrm{k}_{\\mathrm{j}}^{\\prime}=0\\)\n    return \\(\\mathrm{k}^{\\prime}\\)\n```\n\nThe above basis and padding scheme allows us to extend Theorem 3 to general $p$. Theorem 17. Fix a family of basis polynomials $\\widetilde{q}_{\\mathbf{j}}^{N}(\\mathbf{X})$ as defined in (83). Let $N \\geq 1$ be a perfect power of $p, n \\leq\\left\\lfloor\\frac{p}{2}\\right\\rfloor^{p}, \\mathbf{k}, \\mathbf{u} \\in \\mathbb{R}^{n}$ and $\\mathbf{M}_{N}^{\\prime}$ defined by basis $\\tilde{q}_{\\mathbf{j}}^{N}(\\mathbf{X})$. Then the operation\n\n$$\n\\mathbf{u} \\mapsto\\left(\\mathbf{M}_{N}^{\\prime-1}\\left(\\mathbf{M}_{N}^{\\prime} \\cdot \\operatorname{PAD}(\\mathbf{k}) \\circ \\mathbf{M}_{N}^{\\prime} \\cdot \\operatorname{PAD}(\\mathbf{u})\\right)\\right)[0: n-1]\n$$\n\ndefines a causal map in $\\mathbf{u}$. Proof. Let $\\mathbf{k}^{\\prime}=\\operatorname{PAD}(\\mathbf{k}), \\mathbf{u}^{\\prime}=\\operatorname{PAD}(\\mathbf{u})$, and\n\n$$\n\\mathbf{f}=\\left(\\mathbf{M}_{N}^{\\prime-1}\\left(\\mathbf{M}_{N}^{\\prime} \\cdot \\mathbf{k}^{\\prime} \\circ \\mathbf{M}_{N}^{\\prime} \\cdot \\mathbf{u}^{\\prime}\\right)\\right)[0: n-1]\n$$\n\nIn order to prove that (86) is causal in the input $\\mathbf{u} \\in \\mathbb{R}^{n}$, we must show that for all $\\mathbf{i} \\in[0, \\sqrt[p]{N})^{p}$, $\\mathbf{f}_{\\mathbf{i}}$ is dependent only on $\\mathbf{u}_{\\mathbf{i}^{\\prime}}$ for $\\mathbf{i}^{\\prime} \\preceq \\mathbf{i}$. By Theorem $15, \\mathbf{M}_{N}^{\\prime} \\cdot \\mathbf{k}^{\\prime}$ and $\\mathbf{M}_{N}^{\\prime} \\cdot \\mathbf{u}^{\\prime}$ correspond to the evaluations of the polynomials\n\n$$\n\\begin{aligned}\n& k^{\\prime}\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\sum_{\\mathbf{j}^{\\prime} \\in[0, \\sqrt[p]{N})^{p}} k_{\\mathbf{j}^{\\prime}}^{\\prime} \\cdot \\widetilde{q}_{\\mathbf{j}^{\\prime}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right), \\quad \\text { and } \\\\\n& u^{\\prime}\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\sum_{\\mathbf{j}^{\\prime} \\in[0, \\sqrt[p]{N})^{p}} u_{\\mathbf{j}^{\\prime}}^{\\prime} \\cdot \\widetilde{q}_{\\mathbf{j}^{\\prime}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right),\n\\end{aligned}\n$$\n\nrespectively. Let us define\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-63.jpg?height=158&width=1090&top_left_y=992&top_left_x=496)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-63.jpg?height=156&width=991&top_left_y=1147&top_left_x=491)\nLet us define $\\operatorname{deg}\\left(\\widetilde{q}_{\\mathbf{j}}^{N}\\right)=\\left(\\sqrt[p]{N}-j_{a}-1\\right)_{a=0}^{p-1}$ as the vector of length $p$ consisting of the degrees of the component univariate polynomials $\\widetilde{\\ell}_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(X_{a}\\right)$ defined as in (82). Then we have $\\left.\\operatorname{deg}\\left(\\tilde{q}_{\\mathbf{j}}^{\\left\\lfloor\\frac{p}{N}\\right.}\\right\\rfloor^{p}\\right)=$ $\\left(\\left\\lfloor\\frac{p}{2}\\right\\rfloor-j_{0}-1, \\ldots,\\left\\lfloor\\frac{\\sqrt[p]{N}}{2}\\right\\rfloor-j_{p-1}-1\\right)$ for $\\mathbf{j}=\\left(j_{0}, \\cdots, j_{p-1}\\right)$ such that $0 \\leq j_{a}<\\left\\lceil\\frac{p \\bar{N}}{2}\\right\\rceil$ for $0 \\leq$ $a<p$. Further, for $\\mathbf{j}^{\\prime}=\\left(j_{0}+\\left\\lceil\\frac{\\sqrt[p]{N}}{2}\\right\\rceil, \\cdots, j_{p-1}+\\left\\lceil\\frac{p \\sqrt{N}}{2}\\right\\rceil\\right)$ we have\n\n$$\n\\begin{aligned}\n\\operatorname{deg}\\left(\\widetilde{q}_{\\mathbf{j}^{\\prime}}^{N}\\right) & =\\left(\\sqrt[p]{N}-j_{0}^{\\prime}-1, \\ldots, \\sqrt[p]{N}-j_{p-1}^{\\prime}-1\\right) \\\\\n& =\\left(\\sqrt[p]{N}-\\left\\lceil\\frac{\\sqrt[p]{N}}{2}\\right\\rceil-j_{0}-1, \\ldots, \\sqrt[p]{N}-\\left\\lceil\\frac{\\sqrt[p]{N}}{2}\\right\\rceil-j_{p-1}-1\\right) \\\\\n& =\\left(\\left\\lfloor\\left.\\frac{\\sqrt[p]{N}}{2}\\right|^{p}-j_{0}-1, \\ldots,\\left\\lfloor\\left.\\frac{\\sqrt[p]{N}}{2}\\right|^{p}-j_{p-1}-1\\right)\\right.\\right. \\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-63.jpg?height=128&width=1647&top_left_y=2077&top_left_x=239) above, $k_{\\mathbf{j}^{\\prime}}^{\\prime}=k_{\\mathbf{j}}$ and $u_{\\mathbf{j}^{\\prime}}^{\\prime}=u_{\\mathbf{j}}$. Then it follows that $k\\left(X_{0}, \\ldots, X_{p-1}\\right)=k^{\\prime}\\left(X_{0}, \\ldots, X_{p-1}\\right)$, and by a similar argument, $u\\left(X_{0}, \\ldots, X_{p-1}\\right)=u^{\\prime}\\left(X_{0}, \\ldots, X_{p-1}\\right)$. Then by Theorem 16 we have\n\n$$\n\\begin{aligned}\nf\\left(X_{0}, \\ldots, X_{p-1}\\right) & =k\\left(X_{0}, \\ldots, X_{p-1}\\right) \\cdot u\\left(X_{0}, \\ldots, X_{p-1}\\right) \\quad \\bmod \\left(T_{\\sqrt[p]{N}}\\left(X_{0}\\right), \\ldots, T_{\\sqrt[p]{N}}\\left(X_{p-1}\\right)\\right) \\\\\n& =k\\left(X_{0}, \\ldots, X_{p-1}\\right) \\cdot u\\left(X_{0}, \\ldots, X_{p-1}\\right)\n\\end{aligned}\n$$\n\nwhere the second line follows by observing that each $0 \\leq a<p$, we have $\\operatorname{deg}_{X_{a}}(k(\\mathbf{X}) \\cdot u(\\mathbf{X}))<$ $2\\left\\lfloor\\frac{p \\sqrt{N}}{2}\\right\\rfloor$ and observing that $2\\left\\lfloor\\frac{\\sqrt[p]{N}}{2}\\right\\rfloor \\leq \\sqrt[p]{N}$. We want to write $f\\left(X_{0}, \\ldots, X_{p-1}\\right)$ in the form\n\n$$\nf\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\sum_{\\mathbf{m} \\in[0, \\sqrt[p]{N})^{p}} f_{\\mathbf{m}} \\cdot \\widetilde{q}_{\\mathbf{m}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)\n$$\n\nfor a set of coefficients $f_{\\mathbf{m}}$. From (89) and (90) we get\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-64.jpg?height=178&width=1485&top_left_y=621&top_left_x=320)\n\nThen by Lemma 18 we have\n\n$$\nf\\left(X_{0}, \\ldots, X_{p-1}\\right)=\\sum_{\\mathbf{j} \\cdot \\mathbf{j}^{\\prime} \\in\\left[0,\\left\\lfloor\\frac{p, \\bar{N}}{2}\\right\\rfloor\\right)^{p}} k_{\\mathbf{j}^{\\prime}} u_{\\mathbf{j}} \\cdot \\sum_{\\mathbf{j}+\\mathbf{j}^{\\prime} \\preceq \\mathbf{m} \\in[0, \\sqrt[p]{N})^{p}} \\alpha_{\\mathbf{j}+\\mathbf{j}^{\\prime}, \\mathbf{m}} \\widetilde{q}_{\\mathbf{m}}^{N}\\left(X_{0}, \\ldots, X_{p-1}\\right)\n$$\n\nThus, for any $\\mathbf{m} \\in[0, \\sqrt[p]{N})^{p}$, we have\n\n$$\nf_{\\mathbf{m}}=\\sum_{\\mathbf{j}+\\mathbf{j}^{\\prime} \\preceq \\mathbf{m}} \\alpha_{\\mathbf{j}+\\mathbf{j}^{\\prime}, \\mathbf{m}} \\cdot k_{\\mathbf{j}^{\\prime}} u_{\\mathbf{j}}\n$$\n\nimplying that $f_{\\mathbf{m}}$ depends only on $\\mathbf{u}_{\\mathbf{j}}$ for $\\mathbf{j} \\preceq \\mathbf{m}$, as desired. ## D.6.4 $p$-variate Monarch\n\nRecall that we have fixed $b=\\sqrt[p]{N}$ (and hence $N=b^{p}$ ). Define the $p$-variate Monarch matrix as follows:\n\n$$\n\\mathbf{M}^{\\prime}=\\mathbf{P}_{b, N}^{R}\\left(\\prod_{a=0}^{p-2} \\mathbf{B}_{p-1-a} \\cdot\\left(\\mathbf{P}_{b^{a+1}, b^{a+2}, N}\\right)^{\\top}\\right) \\mathbf{B}_{0}\n$$\n\nWhere each $\\mathbf{B}_{a}$ is block diagonal with $b \\times b$ blocks for every $0 \\leq a<p$. Recall that Equation (1) has a permutation $\\mathbf{P}_{0}$ at the end while the above definition does not have any permutation at the end. One trivial way to show the equivalence of above to Equation (1) is to define $\\mathbf{P}_{0}=\\mathbf{I}$. In Appendix D.6.5, we show that exists other non-trivial choices for $\\mathbf{P}_{0}$. Further for $1 \\leq i \\leq p$, the $\\mathbf{P}_{i}$ in Equation (1) connect to the above definition as follows:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8a07250b5c4bf4fa805cg-64.jpg?height=138&width=777&top_left_y=2053&top_left_x=666)\n\nFinally we connect the above definition of $p$-variate Monarch to the earlier definition based on polynomial evaluation:\n\nLemma 19.",
    "m2-61": "Equation (75) can be written as Equation (91). Proof. In (91), $\\mathbf{B}_{a}$ would correspond to the evaluations $\\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(\\omega_{\\sqrt[p]{N}, i_{a}}\\right)$ from earlier. Specifically the following holds for any $0 \\leq a<p$. We index the $q^{p-1}$ blocks of $\\mathbf{B}_{a}$ by $\\left(i_{0}, \\ldots, i_{a-1}\\right),\\left(j_{p-1}, \\ldots, j_{a+1}\\right)$ and the row and column 'offsets' within each such block are indexed by $i_{a}$ and $j_{a}$ respectively. Connecting back to $\\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}(\\cdot)$ we set\n\n$$\n\\mathbf{B}_{a}^{\\left(\\left(i_{0}, \\ldots, i_{a-1}\\right),\\left(j_{p-1}, \\ldots, j_{a+1}\\right)\\right)}\\left[i_{a}, j_{a}\\right]=\\ell_{\\mathbf{j}, \\mathbf{i}}^{(a)}\\left(\\omega_{\\sqrt[p]{N}, i_{a}}\\right)\n$$\n\nWe will prove the claim as follows. Let $\\mathbf{e}_{\\mathbf{j}} \\in \\mathbb{R}^{N}$ have a 1 in the $\\mathbf{j}^{\\text {th }}$ location and 0 's elsewhere. Our goal is to multiply this vector on the right of Equation (91) and show that we get the $\\mathbf{j}^{\\text {th }}$ column of $\\mathbf{M}^{\\prime}$ as defined in Equation (75). We will do so by induction. Define $\\mathbf{y}_{0}=\\mathbf{e}_{\\mathbf{j}}$ and $\\mathbf{y}_{1}=\\mathbf{B}_{0} \\cdot \\mathbf{y}_{0}$. Then for every $1 \\leq a<p$, define\n\n$$\n\\mathbf{y}_{a+1}=\\mathbf{B}_{a}\\left(\\mathbf{P}_{b^{p-a}, b^{p-a+1, N}}\\right)^{\\top} \\mathbf{y}_{a}\n$$\n\nNote that we have\n\n$$\n\\mathbf{M}^{\\prime} \\cdot \\mathbf{e}_{\\mathbf{j}}=\\mathbf{P}_{b, N}^{R} \\cdot \\mathbf{y}_{p}\n$$\n\nNext, we claim that for every $1 \\leq a \\leq p$, we have for every $\\left(i_{0}, \\ldots, i_{a-1}\\right) \\in[0, \\sqrt[p]{N})^{a-1}$,\n\n$$\n\\mathbf{y}_{a}\\left[\\left(\\left(i_{0}, \\ldots, i_{a-2}\\right),\\left(j_{p-1}, \\ldots, j_{a}\\right), i_{a-1}\\right)\\right]=\\prod_{b=0}^{a-1} \\ell_{\\mathbf{j}, \\mathbf{i}}^{(b)}\\left(\\omega_{\\sqrt[p]{N}, b)}\\right)\n$$\n\nwhere for $a=p$, we think of $\\left(\\left(i_{0}, \\ldots, i_{a-2}\\right),\\left(j_{p-1}, \\ldots, j_{a}\\right), i_{a-1}\\right)=\\left(i_{0}, \\ldots, i_{p-1}\\right)$. We first note that Equation (94) is enough to prove the claim. Indeed we have that\n\n$$\n\\mathbf{y}_{p}\\left[\\left(\\left(i_{0}, \\ldots, i_{p-1}\\right)\\right)\\right]=\\prod_{b=0}^{p-1} \\ell_{\\mathbf{j}, \\mathbf{i}}^{(b)}\\left(\\omega_{\\sqrt[p]{N}, b)}\\right)\n$$\n\nwhere the RHS in the above is the same as RHS in Equation (75). To see the claim note that by definition of $\\mathbf{P}_{b, N}^{R}$, we have $\\left(\\mathbf{P}_{b, N}^{R} \\cdot \\mathbf{y}_{p}\\right)\\left[\\left(i_{p-1}, \\ldots, i_{0}\\right)\\right]=\\mathbf{y}_{p}\\left[\\left(i_{0}, \\ldots, i_{p-1}\\right)\\right]$. Equation (93) then established the claim. To complete the proof we prove Equation (94) by induction on $a$. We next consider the base case of $a=1$. Note that $\\mathbf{y}_{1}$ is just the $\\mathbf{j}$ th column of $\\mathbf{B}_{0}\\left(\\right.$ as $\\left.\\mathbf{y}_{0}=\\mathbf{e}_{\\mathbf{j}}\\right)$ and in that case Equation (94) follows from Equation (92). For the inductive hypothesis, assume that Equation (94) is true for $a$ for some $a \\geq 1$. We now want to argue Equation (94) for $\\mathbf{y}_{a+1}$. Towards that end define\n\n$$\n\\mathbf{z}_{a}=\\left(\\mathbf{P}_{b^{p-a}, b^{p-a+1}, N}\\right)^{\\top} \\mathbf{y}_{a}=\\mathbf{P}_{b, b^{p-a+1}, N} \\mathbf{y}_{a}\n$$\n\nNote that\n\n$$\n\\mathbf{y}_{a+1}=\\mathbf{B}_{a} \\cdot \\mathbf{z}_{a}\n$$\n\nNow by definition of $\\mathbf{P}_{b, b^{p-a+1}, N}$, we have\n\n$$\n\\mathbf{z}_{a}\\left[\\left(\\left(i_{0}, \\ldots, i_{a-1}\\right),\\left(j_{p-1}, \\ldots, j_{a}\\right)\\right)\\right]=\\mathbf{y}_{a}\\left[\\left(\\left(i_{0}, \\ldots, i_{a-2}\\right),\\left(j_{p-1}, \\ldots, j_{a}\\right), i_{a-1}\\right)\\right]\n$$\n\nWe claim that Equation (94) is true for $a+1$ from the above along with Equation (95) and Equation (92). Indeed, fix any $\\left(i_{0}, \\ldots, i_{a-1}\\right)$.",
    "m2-62": "Then in Equation (95) the entry $\\mathbf{z}_{a}\\left[\\left(\\left(i_{0}, \\ldots, i_{a-1}\\right),\\left(j_{p-1}, \\ldots, j_{a}\\right)\\right)\\right]$ gets multiplied by the entries $\\mathbf{B}_{a}^{\\left(\\left(i_{0}, \\ldots, i_{a-1}\\right),\\left(j_{p-1}, \\ldots, j_{a+1}\\right)\\right)}\\left[i_{a}, j_{a}\\right]$ for all values of $i_{a} \\in[0, \\sqrt[p]{N})$. The inductive hypothesis and Equation (92) then proves the inductive step, as desired. Finally, we note that we can generalize Algorithm 4 for constructing $\\mathbf{B}_{a}$ for $0 \\leq a<p$ (since this is the multivariate case some of the steps are a bit simplified):\n\n```\nAlgorithm 6 Blocky MultiVar \\(\\operatorname{Monarch}\\left(\\widetilde{\\mathbf{B}}_{(0)}, \\ldots \\widetilde{\\mathbf{B}}_{(p-1)}, N, p\\right)\\)\nInput: \\(\\widetilde{\\mathbf{B}}_{(0)}, \\ldots, \\widetilde{\\mathbf{B}}_{(p-1)} \\in \\mathbb{R}^{N \\times b}\\) where \\(b=\\sqrt[p]{N}\\)\nOutput: Block diagonal matrices \\(\\mathbf{B}_{0}, \\ldots, \\mathbf{B}_{p-1} \\in \\mathbb{R}^{N \\times N}\\)\n                            \\(\\triangleright\\) Compose each output matrix from corresponding input matrix\nfor \\(y \\leftarrow 0\\) to \\(p-1\\) do\n        for \\(a \\leftarrow 0\\) to \\(\\frac{N}{b}-1\\) do\n            \\(\\mathbf{B}_{y}^{(a)} \\leftarrow \\mathbf{C}_{b} \\cdot \\widetilde{\\mathbf{B}}_{(y)}^{(a)}[a b: a b+b-1,:]\\)\n        \\(\\mathbf{B}_{y} \\leftarrow \\operatorname{diag}\\left(\\mathbf{B}_{y}^{(0)}, \\ldots \\mathbf{B}_{y}^{\\left(\\frac{N}{b}-1\\right)}\\right)\\)\nreturn \\(\\mathbf{B}_{0}, \\ldots, \\mathbf{B}_{p-1}\\)\n```\n\n\n## D.6.5 Extensions and Open Questions\n\nIn this sub-section we outline certain (fairly) straightforward extension of our theoretical results and conclude with some open questions.",
    "m2-63": "Comparing Equation (91) to Equation (1) We note that we can post multiply $\\mathbf{M}$ in Equation (91) with a large class of permutations for which Theorem 17 still holds. We outline the technical reason why this is true. At the heart of the argument for why Equation (86) gives a causal map is Lemma 18. Specifically note that the sum in RHS in Equation (84), is over all $\\mathbf{j}+\\mathbf{j}^{\\prime} \\preceq \\mathbf{m}$. The main observation is that this partial order still holds if we permute the $b$-variate representation of $\\mathbf{j}, \\mathbf{j}^{\\prime}$ and $\\mathbf{m}$ in the same way. In other words, for any permutation $\\sigma:[0, p) \\rightarrow[0, p)$ if we define $\\sigma(\\mathbf{j})=\\left(j_{\\sigma(0)}, \\ldots, j_{\\sigma(p-1)}\\right)$ and similarly $\\sigma\\left(\\mathbf{j}^{\\prime}\\right), \\sigma(\\mathbf{m})$. Then we still have $\\sigma(\\mathbf{j})+\\sigma\\left(\\mathbf{j}^{\\prime}\\right) \\preceq \\sigma(\\mathbf{m})$. This in turn implies the following. Let $\\mathbf{P}_{\\sigma}$ be a permutation that maps $\\mathbf{j} \\in[0, \\sqrt[p]{N})^{p}$ to $\\sigma(\\mathbf{j}) \\in[0, \\sqrt[p]{N})^{p}$. Then Theorem 17 holds if we replace $\\mathbf{M}^{\\prime}$ by $\\mathbf{M} \\cdot \\mathbf{P}_{\\sigma}$ with $\\mathbf{M}$ as in Equation (91). Evaluation points Our results as presented are for specific classes of evaluation points. A natural question to ask is if our results can be extended to more general set of evaluation points. It turns out that our results for $p$-variate Monarch matrices can be extended to a wider class of evaluation points. Specifically, for each $0 \\leq a<p$, let $S_{a} \\subset \\mathbb{C}$ with $\\left|S_{a}\\right|=\\sqrt[p]{N}$. Then our results in this sub-section hold if we replace the evaluation points from $A^{p}$ to $\\times_{a=0}^{p-1} S_{a}$. The only thing that changes in our proofs is that in Theorem 16, we replace $\\bmod \\left(T_{\\sqrt[p]{N}}\\left(X_{0}\\right), \\ldots, T_{\\sqrt[p]{N}}\\left(X_{p-1}\\right)\\right)$ by $\\bmod \\left(q_{S_{0}}\\left(X_{0}\\right), \\ldots, q_{S_{p-1}}\\left(X_{p-1}\\right)\\right)$, where $q_{A}(Z)$ is as defined in Equation (81).",
    "m2-64": "This result can then be propagated throughout the rest of our proofs. On the other hand, our results in Appendix D. 3 and Appendix D. 5 do exploit specific properties of the evaluation points (specifically $\\left(\\omega_{N}^{i}\\right)^{\\sqrt{N}}=\\omega_{\\sqrt{N}}^{i_{0}}$ for Appendix D. 3 and $T_{\\sqrt{N}}\\left(\\omega_{N, i}\\right)=(-1)^{i_{1}} \\omega_{\\sqrt{N}, i_{0}}$ for Appendix D.5). To generalize these results to other sets of evaluation points, we need the existence of degree $\\sqrt{N}$ polynomial that maps (in a $\\sqrt{N}$-to- 1 fashion) $A$ to a set of $\\sqrt{N}$ elements. Another interesting open question is to avoid the blowup $n \\rightarrow 2^{p} \\cdot n$ in Theorem 17 and ideally only pay a blowup $n \\rightarrow 2 n$ for every $p \\geq 2$ as we were able to do in Appendix D.",
    "m2-65": "3 and Appendix D. 5 (with $p=2$ ). [^0]:    ${ }^{*}$ Equal contribution. [^1]:    ${ }^{1}$ Monarch matrices were originally [12] parameterized with $p=2$, but the general $p$ case is a natural extension. ${ }^{2}$ For context, the most optimized attention implementations achieve $25 \\%$ FLOP utilization, while unoptimized implementations of attention can have as low as $10 \\%$ FLOP utilization [13].",
    "m2-66": "[^2]:    ${ }^{3}$ This claim can be shown using downward induction on $d=N-1, N-2, \\ldots$. [^3]:    ${ }^{4}$ It can also be verified that $q_{\\left(j_{1}, j_{0}\\right)}$ has maximum degree $\\leq N-1$. [^4]:    ${ }^{5}$ Recall that Lemma 6 is stated for basis polynomials $q_{j_{1}, j_{0}}^{\\prime}(Z)$ for $\\mathbf{M}^{\\prime}=\\mathbf{P L P R}$ where $q_{j_{1}, j_{0}}(Z)=q_{j_{0}, j_{1}}^{\\prime}(Z)$. [^5]:    ${ }^{6}$ We do not have a proof of this claim, but to us it seems unlikely that $\\mathbf{C}_{N} \\in \\mathcal{M}^{S C}$\n\n[^6]:    ${ }^{7} b=2$ gives the well known bit reversal permutation. "
}