{
    "lightnet-0": "# You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet \n\n${ }^{1}$ Zhen Qin, ${ }^{3}$ Yuxin Mao, ${ }^{2}$ Xuyang Shen, ${ }^{2}$ Dong Li, ${ }^{4}$ Jing Zhang, ${ }^{3}$ Yuchao Dai, ${ }^{2}$ Yiran Zhong ${ }^{\\square}$<br>${ }^{1}$ Taptap, ${ }^{2}$ OpenNLPLab, Shanghai AI Laboratory,<br>${ }^{3}$ Northwestern Polytechnical University, ${ }^{4}$ Australian National University<br>https://github.com/OpenNLPLab/LightNet\n\n\n#### Abstract\n\nLinear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed.",
    "lightnet-1": "However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a \"multiplicative\" linear recurrence and proposes an efficient alternative \"additive\" linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling. ## 1 Introduction\n\nLinear attention has emerged as an effective alternative to softmax attention due to its linear computational complexity and enhanced processing speed, especially in causal language models [1, 2]. The benefits of linear attention largely depend on its decay mechanism [1-3], which prevents attention dilution [4] and facilitates global interaction among tokens. However, the decay mechanism presents two primary issues: First, the decay mechanism is not easily applicable to high-dimensional inputs due to the need for multiple sequential scans to establish a global multi-dimensional receptive field, which reduces computational efficiency [5, 6]. Additionally, without the decay mechanism, linear attention lacks positional awareness during computations, leading to decreased performance [4]. In light of these challenges, we are investigating the feasibility of reducing sequential scans for multi-dimensional scenarios while preserving performance. We first analyze the types of linear recurrence and divide them into two categories: multiplicative and additive. In multiplicative recurrence, the decay rate is dependent only on the current moment, making it impossible to obtain information about subsequent moments with a single scan. By taking image processing as an example, using multiplicative recurrence will require at least two scans to retrieve the global information [5, 6]. Conversely, in additive recurrence, the decay rate depends on\n\n[^0]all moments through the summation of the importance score of each moment, enabling it to gather global information in a single scan. It is important to note that in non-causal situations, additive recurrence is permutation-invariant, which means it lacks local precedence and therefore diminishes the capture of positional information. To overcome this limitation, we put forth a new approach to positional encoding called Multi-Dimensional Toeplitz Positional Encoding (MD-TPE). This method utilizes the mathematical properties of the Toeplitz matrix to embed relative positional information with linear time complexity, thus ensuring efficiency in multi-dimensional scenarios. Additionally, we expand the Linearized Relative Positional Encoding (LRPE) [7] to high-dimensional scenarios, resulting in the creation of Multi-Dimensional Linearized Relative Positional Encoding (MD-LRPE). We then present LightNet, a new multi-dimensional linear attention model built on additive recurrence. LightNet features a pioneering decay mechanism, allowing for efficient single-scan processing of high-dimensional sequential data. Furthermore, it integrates highly effective multi-dimensional position encoding such as MD-TPE and MD-LRPE to precisely capture positional information. We conduct several evaluations of the performance of our proposed LightNet on a range of tasks, including image generation, image classification, image generation, bidirectional language modeling, and autoregressive language modeling. LightNet performs comparably or better than its competitors across all tasks. We summarize our main contributions as follows:\n\n- We analyze the types of linear recurrence, dividing them into two types: multiplicative and additive, where the additive type can obtain global information in a single scan. - We propose two multi-dimensional position encoding strategies, MD-TPE and MD-LRPE, to effectively capture positional information in multi-dimensional scenarios. - We propose LightNet, a new multi-dimensional linear attention model that can process high-dimensional sequences in a single scan. - We conduct thorough evaluations to assess the efficiency and efficacy of LightNet for multidimensional sequential modeling tasks. The LightNet demonstrates competitive performance in all scenarios. ## 2 Related Work\n\nLinear Attention. The linear attention mechanism has greatly advanced deep learning, particularly in natural language processing, by providing a scalable solution for long input sequences and reducing the computational demands of traditional attention models [8-11]. However, despite its faster training speeds, linear attention's performance still falls short of softmax attention due to the attention dilution issue [4]. The TNL/RetNet [2, 4] introduces a decay mechanism to address this problem. Additionally, GLA [12] incorporating gating mechanisms show the potential to enhance linear attention models. State Space Model. State Space Models (SSMs) are increasingly crucial in sequence modeling due to their structured approach to capturing temporal dynamics through latent variables. The S4 model [13] enhances state space modeling for long sequences by leveraging structured spaces to improve computational efficiency and tackle complex dynamics. With additional parameterizing and initializing diagonal state space strategy [14], the SSMs can achieve comparable performance to naive transformers. Furthermore, the Gated State Space (GSS) model [15] introduces a gating mechanism to SSMs, which is particularly effective for long-range language modeling by allowing nuanced control over information flow. The S5 model [16] reduces complexity using \"scan\" while maintaining the capability to handle intricate sequences. However, directly extending the SSM to multi-dimensional input usually requires multiple sequential scans, which will reduce the computational efficiency [6]. Linear RNN. Linear RNNs employ element-wise recursion for sequence modeling, and due to their linear recursive form, they can be accelerated using parallel scans [17]. At their core is the decay mechanism, where RWKV-4/LRU [1, 18] utilizes data-independent decay. HGRN [19, 20] leverage data-dependent decay to enhance performance. Linear RNNs have shown considerable potential in language modeling and long-sequence modeling tasks. Multi-dimensional Tasks with Linear Complexity Model. The development of linear attention in language models has led to its extension into multi-dimensional tasks. Building upon the cosFormer framework [10], VVT [21] explores a local prior of 2D linear attention and applies it to image classification tasks. Vim [6] and Vision-RWKV [5] utilize a sequential scan mechanism to expand\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ca5728fe5a821d021816g-03.jpg?height=526&width=1363&top_left_y=245&top_left_x=377)\n\nFigure 1: Processing time of 1 Scan and 2 Scan in relation to sequence length. 1 Scan is significantly faster than 2 Scan in both forward and backward passes. As the sequence length increases, the advantage of 1 Scan becomes more substantial. Mamba [22] and RWKV [23] for image classification. Additionally, leveraging the benefits structure of the diffusion transformer [24] in image generation, several works have extended linear complexity models into 2D space [25-28] to replace the traditional transformer architecture, achieving efficient image generation.",
    "lightnet-2": "However, some of these tasks encounter issues with inadequate performance. Moreover, frequent sequential scans can compromise the efficiency of the model. ## 3 Linear Recurrence in Multi-dimensional Space\n\nIn this section, we discuss the theoretical and practical computational complexity of linear recurrence (with decay) when dealing with high-dimensional data, and then analyze the types of linear recurrence. In subsequent discussions, we assume $n$ is the sequence length, $d$ is the embedding dimension, and $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$ is the transpose of the $t$-th row of matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$. ### 3.1 Computational Complexity of Linear Recurrence\n\nIt has been proved that all linear attention, state space model, and linear RNN can be expressed using a linear recurrence formula [29]. We use linear attention with decay [2,3] as an example. Below is the recursive form (i.e. a scan):\n\n$$\n\\mathbf{k} \\mathbf{v}_{0}=\\mathbf{0}, \\mathbf{k} \\mathbf{v}_{t}=\\lambda_{t} \\mathbf{k} \\mathbf{v}_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\mathbf{o}_{t}^{\\top}=\\mathbf{q}_{t}^{\\top} \\mathbf{k} \\mathbf{v}_{\\mathbf{t}}, t=1, \\ldots, n\n$$\n\nHere, $0<\\lambda_{t} \\leq 1$ is the decay rate. Note that the above formula is for the causal scenario. When dealing with non-causal scenarios, a common practice in the literature is to perform causal computation twice [5, 6]. We call this method \" 2 scan\":\n\n$$\n\\begin{aligned}\n& \\overrightarrow{\\mathbf{k v}}_{0}=\\mathbf{0}, \\overrightarrow{\\mathbf{k v}}_{t}=\\lambda_{t} \\overrightarrow{\\mathbf{k v}}_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\overrightarrow{\\mathbf{o}}_{t}^{\\top}=\\mathbf{q}_{t}^{\\top} \\overrightarrow{\\mathbf{k v}}_{t}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\mathbf{o}_{t}=\\overrightarrow{\\mathbf{o}}_{t}+\\overleftarrow{\\mathbf{o}}_{t}\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ca5728fe5a821d021816g-03.jpg?height=68&width=752&top_left_y=1858&top_left_x=681)\n\nWhen $\\lambda_{t}=1$, i.e. there is no decay, the right product trick [9] can be used in this case. We call this method \" 1 scan\". $$\n[\\mathbf{K V}]=\\mathbf{K}^{\\top} \\mathbf{V}, \\mathbf{O}=\\mathbf{Q}[\\mathbf{K V}]\n$$\n\nAlthough both of the above formulas have a time complexity of $O\\left(n d^{2}\\right)$, the \" 2 scan\" version is significantly slower than the \" 1 scan\" version. This is because causal computation requires block-level recursion [12,30], whereas the second formula can be fully parallelized due to matrix multiplication [9]. We provide a speed comparison in Fig. 1, where the \"2 scan\" is implemented with Lightning Attention [30], the fastest linear attention implementation so far. It can be seen that the \" 2 scan\" is several times slower than the \" 1 scan\" in both forward and backward passes. It is apparent that the need for multiple scans is mainly due to the presence of decay $\\lambda_{t}$. However, directly removing $\\lambda_{t}$ would lead to degraded performance [4]. A natural question arises: can we retain $\\lambda_{t}$ while only performing a single scan? In the next section, we will discuss the types of linear recurrence and answer the question. ### 3.2 Types of Linear Recurrence\n\nWe first explore the representation range of linear recurrences by 1D linear recurrence:\n\n$$\ny_{t}=a_{t} y_{t-1}+x_{t}, y_{0}=0\n$$\n\nUnroll the recursion equation of E.q 1, we obtain:\n\n$$\ny_{t}=\\sum_{s=1}^{t} \\frac{A_{s}}{A_{t}} x_{s} \\triangleq \\sum_{s=1}^{t} c_{t s} x_{s}, A_{t}=\\left(\\prod_{s=1}^{t} a_{s}\\right)^{-1}\n$$\n\nThe detailed proof of the unrolling process can be found in Appendix A.1. Note that $y_{t}$ is a linear combination of $x_{1}, \\ldots, x_{t}$. A natural question arises: Can every linear combination $\\sum_{s=1}^{t} c_{t s} x_{s}$ be represented as a linear recursion? We now prove that a linear recursion representation is possible only when the coefficients $c_{t s}$ satisfy certain conditions. Theorem 3.1. A linear recurrence $y_{t}=a_{t} y_{t-1}+x_{t}, y_{0}=0$ is equivalent to a linear combination $y_{t}=\\sum_{s=1}^{t} c_{t s} x_{s}$, iff $c_{t s}=\\frac{g(s)}{g(t)}$, where $g(\\cdot)$ is a function. Proof of Theorem 3.1. $\\Rightarrow$\nGiven a linear recurrence, we multiply it by $A_{t}=\\left(\\prod_{s=1}^{t} a_{s}\\right)^{-1}$ and the following recurrence equation:\n\n$$\nA_{t} y_{t}=A_{t} a_{t} y_{t-1}+A_{t} x_{t}=A_{t-1} y_{t-1}+A_{t} x_{t}\n$$\n\nUnroll it, we get:\n\n$$\nA_{t} y_{t}-A_{t-1} y_{t-1}=A_{t} x_{t}, \\ldots, A_{2} y_{2}-A_{1} y_{1}=A_{2} x_{2}\n$$\n\nTo derive an expression for $y_{t}$, we sum the recursive equations and obtain:\n\n$$\nA_{t} y_{t}-A_{1} y_{1}=\\sum_{s=2}^{t} A_{s} c x_{s}, y_{t} A_{t}=\\sum_{s=1}^{t} A_{s} x_{s}, y_{t}=\\sum_{s=1}^{t} \\frac{A_{s}}{A_{t}} x_{s}\n$$\n\nBy comparing the coefficients, we can obtain $c_{t s}=A_{s} / A_{t}$. $\\Leftarrow:$\nGiven the linear combination $y_{t}=\\sum_{s=1}^{t} c_{t s} x_{s}$ and $c_{t s}=\\frac{g(s)}{g(t)}$, we define $a_{t} \\triangleq \\frac{g(t-1)}{g(t)}$. Then $y_{t}$ can be expressed as:\n\n$$\n\\begin{aligned}\ny_{t} & =\\sum_{s=1}^{t} c_{t s} x_{s}=\\sum_{s=1}^{t-1} c_{t s} x_{s}+c_{t t} x_{t}=\\sum_{s=1}^{t-1} \\frac{g(s)}{g(t)} x_{s}+\\frac{g(t)}{g(t)} x_{t} \\\\\n& =\\frac{g(t-1)}{g(t)} \\sum_{s=1}^{t-1} \\frac{g(s)}{g(t-1)} x_{s}+x_{t}=a_{t} \\sum_{s=1}^{t-1} c_{t-1, s} x_{s}+x_{t}=a_{t} y_{t-1}+x_{t}\n\\end{aligned}\n$$\n\nBased on the Theorem 3.1, for linear recurrence, we can directly discuss $g(t)$, as $a_{t}$ can be obtained through $\\frac{g(t-1)}{g(t)}$. Intuitively, $g(t)$ can be interpreted as an importance score up to moment $t, c_{t s}=\\frac{g(s)}{g(t)}$ can be interpreted as the ratio of the score at moment $s$ relative to moment $t$, and $a_{t}$ can be interpreted as the ratio of the previous moment's score to moment $t$ 's score.",
    "lightnet-3": "Typically, to prevent numerical overflow, we assume $0 \\leq a_{t}=\\frac{g(t-1)}{g(t)} \\leq 1$. To meet this condition, we present the following two forms:\nProposition 3.2. For Linear Recurrence with $0 \\leq a_{t} \\leq 1$, there are two forms:\n\n1.",
    "lightnet-4": "Multiplicative: $g(t)=\\frac{1}{\\prod_{s=1}^{t} \\rho(s)}, a_{t}=\\rho(t), 0 \\leq \\rho(t) \\leq 1$;\n2. Additive: $g(t)=\\sum_{s=1}^{t} \\delta(s), a_{t}=\\frac{\\sum_{s=1}^{t-1} \\delta(s)}{\\sum_{s=1}^{t} \\delta(s)}, \\delta(s) \\geq 0$. ![](https://cdn.mathpix.com/cropped/2024_09_12_ca5728fe5a821d021816g-05.jpg?height=674&width=1190&top_left_y=221&top_left_x=467)\n\nFigure 2: The network structure of LightNet: each LightNet model is comprised of an Input Embedding, MD-TPE, and a stack of multiple LightNet Layers. Each LightNet Layer consists of an LNA and a GLU, with the computation of LNA illustrated in the figure on the right.",
    "lightnet-5": "Proof of Proposition 3.2. The condition $0 \\leq \\frac{g(t-1)}{g(t)} \\leq 1$, is equivalent to $\\rho(t)=\\frac{g(t-1)}{g(t)} \\leq 1$ or $\\delta(t)=g(t)-g(t-1) \\geq 0$. By expanding the first formula, we obtain the multiplicative type. By expanding the second formula, we get the additive type. It can be observed that the typical linear attention with decay corresponds to the Multiplicative form, where $\\rho(t)$ is utilized as $\\operatorname{Sigmoid}(\\cdot)$ [12], $\\exp (-\\exp (\\cdot))$ [22], or a fixed value [2, 3].",
    "lightnet-6": "Multiplicative requires multiple scans when dealing with high dimensions because $a_{t}$ itself cannot provide global information (as $a_{t}$ is only related to the moment $t$ ). However, for the Additive method, since the computation form is $a_{t}=\\frac{\\sum_{s=1}^{t-1} \\delta(s)}{\\sum_{s=1}^{t} \\delta(s)}$, by modifying the denominator to $\\Delta=\\sum_{s=1}^{n} \\delta(s)$ ( $n$ is the sequence length), global information can be obtained through $a_{t}=\\frac{\\sum_{s=1}^{t-1} \\delta(s)}{\\Delta}$. ## 4 LightNet\n\nBuilding upon the preceding analysis, we introduce a novel Linear Transformer architecture termed LightNet, designed to handle multi-dimensional data efficiently in a single scan. An overview of its structure is depicted in Fig. 2. LightNet comprises an Input Embedding, MD-TPE module, and several stacked LightNet Layers. ### 4.1 LightNet Layer\n\nThe LightNet Layer is composed of a LightNet Attention (LNA) and a Gated Linear Unit (GLU) [31]. Within the LNA, an additive decay is employed, with the parameter $\\delta$ implemented through the exponential function. Additionally, a parameter tiling strategy is utilized for both the key and decay, which has been empirically observed to enhance performance. This empirical evidence is detailed in Table 3. Furthermore, the integration of a low-rank output gate from TNL3 [2] and a normalization after linear attention [4] has been incorporated. In causal settings, the LightNet Layer can be represented as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{s}_{t}=\\mathbf{s}_{t-1}+\\exp \\left(\\mathbf{k}_{t}\\right), \\overline{\\mathbf{k}}_{\\mathbf{t}} & =\\exp \\left(\\mathbf{k}_{t}\\right) / \\mathbf{s}_{t}, \\mathbf{k} \\mathbf{v}_{t}=\\operatorname{diag}\\left\\{1-\\overline{\\mathbf{k}}_{\\mathbf{t}}\\right\\} \\mathbf{k} \\mathbf{v}_{t-1}+\\overline{\\mathbf{k}}_{\\mathbf{t}} \\mathbf{v}_{t}^{\\top} \\\\\n\\mathbf{o}_{t}^{\\top} & =\\operatorname{Norm}\\left[\\mathbf{k} \\mathbf{v}_{t}^{\\top} \\phi\\left(\\mathbf{q}_{t}\\right)\\right] \\odot \\psi\\left(u_{t}\\right)\n\\end{aligned}\n$$\n\nIn non-causal settings, the expression becomes:\n\n$$\n\\begin{gathered}\n\\mathbf{s}=\\sum_{t} \\exp \\left(\\mathbf{k}_{t}\\right), \\mathbf{o}_{t}=\\operatorname{Norm}\\left[\\phi\\left(q_{t}\\right) \\sum_{t}\\left(\\exp \\left(k_{t}\\right) / s\\right)^{\\top} \\mathbf{v}_{t}\\right] \\odot \\psi\\left(u_{t}\\right) \\\\\n\\mathbf{O}=\\operatorname{Norm}\\left[\\phi(\\mathbf{Q})\\left(\\operatorname{Softmax}(\\mathbf{K})^{\\top} \\mathbf{V}\\right)\\right] \\odot \\psi(\\mathbf{U})\n\\end{gathered}\n$$\n\nwhere X is the input of LNA:\n\n$$\n\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}, \\mathbf{U}=\\mathbf{X} \\mathbf{W}_{u 1} \\mathbf{W}_{u 2}, \\phi=\\text { Swish, } \\psi=\\text { Sigmoid }\n$$\n\n### 4.2 Multi Dimension Position Encoding\n\nIt is noted that additive recurrence does not have a locality prior like multiplicative recurrence and is permutation invariant in non-causal scenarios, as shown in E.q 6. Therefore, it is necessary to introduce new positional encoding. To tackle this challenge, we introduce two novel relative positional encoding methods, MD-TPE (Multi-Dimensional Toeplitz Positional Encoding) and expand the existing LRPE [7] to the high-dimensional context as MD-LRPE (Multi-Dimensional Linearized Relative Positional Encoding). This enhancement enables the management of relative positional relationships in any dimension. MD-TPE. Given multi-dimension input $\\mathbf{x}_{n_{1}, \\ldots, n_{k}}, 1 \\leq n_{s} \\leq N_{s}, s=1, \\ldots k$, we use the following equation to capture positional information:\n\n$$\n\\mathbf{y}_{n_{1}, \\ldots, n_{k}}=\\sum_{m_{k} \\leq n_{k}} \\ldots \\sum_{m_{1} \\leq n_{1}} \\mathbf{y}_{n_{1}-m_{1}, \\ldots, n_{k}-m_{k}} \\mathbf{x}_{m_{1}, \\ldots, m_{k}}\n$$\n\nHowever, the time complexity of implementing the aforementioned method is $O(N \\log N)$, where $N=\\prod_{s=1}^{k} n_{s}$, making it inefficient. To address this, we simplified the above formula by performing toeplitz matrix production for each dimension separately and using SSM for parameterization [32], we denotes $e$ as the hidden dimension of SSM below:\n\n$$\n\\mathbf{y}_{n_{1}, \\ldots, n_{k}}=\\sum_{s=1}^{k} \\sum_{m_{s}=1}^{n_{s}} \\mathbf{y}_{n_{s}-m_{s}} \\mathbf{x}_{n_{1}, \\ldots, m_{s}, \\ldots, n_{k}}=\\sum_{s=1}^{k} \\sum_{m_{s}=1}^{n_{s}} \\sum_{t=1}^{e} \\lambda_{t}^{n_{s}-m_{s}} \\mathbf{x}_{n_{1}, \\ldots, m_{s}, \\ldots, n_{k}}\n$$\n\nBy using a scan approach, the above calculation becomes linear in complexity, $O(N e)$. MD-LRPE. Given $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}, \\mathbf{x} \\in\\{\\mathbf{q}, \\mathbf{k}\\}$, LRPE transforms it through the matrix $\\mathbf{W}_{t}$ to $\\mathbf{W}_{t} \\mathbf{x}_{t}, \\mathbf{x} \\in$ $\\{\\mathbf{q}, \\mathbf{k}\\}$, and it holds that:\n\n$$\n\\left(\\mathbf{W}_{s} \\mathbf{q}_{s}\\right)^{\\mathrm{H}}\\left(\\mathbf{W}_{t} \\mathbf{k}_{t}\\right)=\\mathbf{q}_{s}^{\\mathrm{H}} \\mathbf{W}_{s}^{\\mathrm{H}} \\mathbf{W}_{t} \\mathbf{k}_{t}=\\mathbf{q}_{s}^{\\mathrm{H}} \\mathbf{W}_{t-s} \\mathbf{k}_{t}\n$$\n\nWe choose the complex version of LRPE, where:\n\n$$\n\\mathbf{W}_{t}=\\operatorname{diag}\\left\\{\\exp \\left(i t \\theta_{1}\\right), \\ldots, \\exp \\left(i t \\theta_{d}\\right)\\right\\}\n$$\n\nTo generalize to higher dimensions, i.e., given $\\mathbf{x}_{n_{1}, \\ldots, n_{k}} \\in \\mathbb{R}^{d}, \\mathbf{x} \\in\\{\\mathbf{q}, \\mathbf{k}\\}$, we divide the $d$ features into $k$ groups, each group has $d / k$ features, with the $s$-th group's features corresponding to dimension $n_{s}, s \\in[1, k]$. Specifically, we define:\n\n$$\n\\mathbf{W}_{n_{1}, \\ldots, n_{k}}=\\operatorname{diag}\\left\\{\\left[\\Theta_{1}, \\ldots, \\Theta_{k}\\right]\\right\\}, \\Theta_{s}=\\exp \\left(i n_{k} \\theta_{j}\\right), s d / k<j \\leq(s+1) d / k, \\theta_{j}=10000^{-2 j / d}\n$$\n\nThus:\n\n$$\n\\mathbf{W}_{n_{1}, \\ldots, n_{k}}^{\\mathbf{H}} \\mathbf{W}_{m_{1}, \\ldots, m_{k}}=\\mathbf{W}_{m_{1}-n_{1}, \\ldots, m_{k}-n_{k}}\n$$\n\nThen:\n\n$$\n\\begin{gathered}\n\\left(\\mathbf{W}_{n_{1}, \\ldots, n_{k}} \\mathbf{q}_{n_{1}, \\ldots, n_{k}}\\right)^{\\mathrm{H}}\\left(\\mathbf{W}_{m_{1}, \\ldots, m_{k}} \\mathbf{k}_{m_{1}, \\ldots, m_{k}}\\right)=\\mathbf{q}_{n_{1}, \\ldots, n_{k}}^{\\mathrm{H}} \\mathbf{W}_{s}^{\\mathrm{H}} \\mathbf{W}_{t} \\mathbf{k}_{m_{1}, \\ldots, m_{k}} \\\\\n=\\mathbf{q}_{n_{1}, \\ldots, n_{k}}^{\\mathrm{H}} \\mathbf{W}_{m_{1}-n_{1}, \\ldots, m_{k}-n_{k}} \\mathbf{k}_{m_{1}, \\ldots, m_{k}}\n\\end{gathered}\n$$\n\n## 5 Experiments\n\nWe comprehensively evaluate the substitutability of our LightNet in performance, scalability, flexibility, and efficiency. We validate the effectiveness of our model on various multi-dimensional sequential modeling tasks. We also test the proposed ability of LightNet to serve as a language model. ### 5.1 Setting\n\nTable 1: Performance comparison for image classification task on ImageNet1k. \"S.A.\" represents Softmax Attention, \"M.S.\" denotes multiple scans, and \"O.S.\" signifies one scan. | Model | Category | Tiny |  | Small |  | Base |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Acc $(\\%) \\uparrow$ | Params (M) | Acc (\\%) $\\uparrow$ | Params (M) | Acc (\\%) $\\uparrow$ | Params (M) |\n| DeiT [33] | S.A. | 72.20 | 5.7 | 79.90 | 22.00 | 81.80 | 86.00 |\n| HGRN [19] | M.S. | 74.40 | 6.1 | 80.09 | 23.70 | - | - |\n| Vim [6] | M.S. | 76.10 | 7.0 | 80.50 | 26.00 | - | - |\n| V-RWKV [5] | M.S. | 75.10 | 6.2 | 80.10 | 23.80 | 82.00 | 93.70 |\n| LightNet | O.S. | 74.46 | 6.0 | 80.12 | 22.64 | 81.90 | 87.74 |\n| LightNet w/o TPE | O.S. | 73.97 | 6.0 | 79.65 | 22.54 | 81.45 | 87.54 |\n| LightNet w/o LRPE | O.S. | 74.02 | 6.0 | 79.54 | 22.63 | 81.72 | 87.69 |\n\nImage Classification. We trained our LightNet model for image classification on the ImageNet-1K dataset [37]. Our approach modifies the network architecture and training protocols of DeiT [33], substituting its Transformer Layers with our proprietary LightNet Layers. Image Generation. We build our model upon the latent diffusion model $[24,36]$ and use our proposed LightNet as the denoising network. We adjust the model size across various configurations (S, B, L, XL) and patch sizes (8, 4, 2), consistent with DiT [24]. Experiments are conducted on the ImageNet dataset [37] at a resolution of $256 \\times 256$. Each model is trained over 0.4 M steps with a batch size of 256 to assess scaling capabilities. For the largest model variant, training is extended to 0.8 M steps with a batch size of 1024 , as opposed to the 7 M steps in DiT, to enhance generative performance. Bidirectional Language Modeling. We utilize Cramming-BERT [38] as our pipeline, employing a 24 -hour training regime to pre-train on the Pile dataset, subsequently finetuning on the GLUE benchmark [39]. During pre-training, we follow established guidelines by setting a learning rate of $1 \\mathrm{e}-3$, a sequence length of 128 , and a batch size of 8192. In the finetuning phase, we experiment with learning rates from the set $\\{5 \\mathrm{e}-$ $5,4 e-5,3 e-5,2 e-5\\}$ and determine the optimal outcome by finetuning over 5 epochs. Autoregressive Language Modeling. We evaluate two capabilities: perplexity (PPL) and zero-shot reasoning ability. The perplexity of the 44 M model is assessed on the Wikitext-103 dataset [40], and the 380M model's perplexity is tested on the Pile dataset, consuming 10 billion tokens . For large language model experiments, we train LightNet models at scales of 150M, $350 \\mathrm{M}, 1 \\mathrm{~B}$, and 3B using 100 billion tokens sampled from subsets of the Pile [41]. These models are then evaluated on commonsense reasoning tasks using the lm-eval-harness [42]. Detailed training hyperparameters are listed in Table 9. Table 2: Performance comparison for image generation task on ImageNet-1k. LightNetXL/2 achieves state-of-the-art FID with or without classifier-free guidance (-G) [34]. | Model | FID $\\downarrow$ sFID $\\downarrow$ IS $\\uparrow$ |  | Precision $\\uparrow$ Recall $\\uparrow$ |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| CDM [35] | 4.88 | - | 158.71 | - | - |\n| LDM-8 [36] | 15.51 | - | 79.03 | 0.65 | 0.63 |\n| LDM-8-G | 7.76 | - | 209.52 | 0.84 | 0.35 |\n| LDM-4 | 10.56 | - | 103.49 | 0.71 | 0.62 |\n| LDM-4-G | 3.60 | - | 247.67 | 0.87 | 0.48 |\n| DiT-XL/2 [24] | 9.62 | 6.85 | 121.50 | 0.67 | 0.67 |\n| DiT-XL/2-G | 2.27 | 4.60 | 278.24 | 0.83 | 0.57 |\n| LightNet-XL/2 | 5.35 | 5.93 | 171.18 | 0.73 | 0.65 |\n| LightNet-XL/2-G | 2.18 | 4.58 | 281.85 | 0.83 | 0.58 |\n\nTable 3: Performance comparison on Wikitext103. $\\downarrow$ means lower is better. We adopted the configuration of HGRN for Wikitext-103, and we can observe that LightNet significantly outperforms all other methods. | Model | PPL <br> $($ val) $\\downarrow$ | PPL <br> $($ test $) \\downarrow$ | Params <br> $(\\mathrm{M})$ |\n| :--- | :---: | :---: | :---: |\n| Attn-based |  |  |  |\n| Transformer | 24.40 | 24.78 | 44.65 |\n| FLASH | 25.92 | 26.70 | 42.17 |\n| 1+elu | 27.44 | 28.05 | 44.65 |\n| Performer | 62.50 | 63.16 | 44.65 |\n| cosFormer | 26.53 | 27.06 | 44.65 |\n| RNN-based |  |  |  |\n| S4 | 38.34 | 39.66 | 45.69 |\n| DSS | 39.39 | 41.07 | 45.73 |\n| GSS | 29.61 | 30.74 | 43.84 |\n| RWKV-4 | 24.31 | 25.07 | 46.23 |\n| LRU | 29.86 | 31.12 | 46.24 |\n| HGRN | 24.14 | 24.82 | 46.25 |\n| FFT-based |  |  |  |\n| TNN | 23.98 | 24.67 | 48.68 |\n| LightNet | 23.09 | 23.75 | 45.07 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ca5728fe5a821d021816g-08.jpg?height=933&width=1270&top_left_y=210&top_left_x=428)\n\nFigure 3: Scaling up the LightNet enhances the FID during every stages of training. We present the FID-50K across training iterations for twelve LightNet models. Enhancing the LightNet backbone results in improved generative models for all sizes of models and patches. ### 5.2 Results\n\nImage Classification. As shown in Table 1, the proposed LightNet shows competitive performance on the ImageNet-1k dataset. It can be observed that using only a single sequential scan, LightNet can achieve comparable performance to models with naive attention and multiple sequential scans.",
    "lightnet-7": "At the same time, the speed advantage of using a single sequential scan is shown in Fig.",
    "lightnet-8": "1. Image Generation. The image generation results are presented in Table 2. Our proposed LightNet demonstrates superior performance, achieving a lower Fr\u00e9chet Inception Distance (FID) and a higher Inception Score (IS) than DiT [24] with fewer training steps ( 0.8 M steps vs 7 M steps).",
    "lightnet-9": "Additionally, LightNet exhibits commendable scaling capabilities, as illustrated in Fig. 3. Bidirectional Language Modeling. As shown in Table 5, LightNet outperforms Crammed Bert [38] on the GLUE dataset, demonstrating its superior capability in handling natural language understanding tasks. Despite BERT-Base [43] achieving comparable performance, it is noteworthy that LightNet does so with a significantly lower computational cost, having been trained on a single A100 for 24 hours. Autoregressive Language Modeling. In the Wikitext-103 dataset, as depicted in Table 3, LightNet surpasses all competitors on both the validation and test datasets. Regarding large-scale datasets, as illustrated in Table 4, LightNet exhibits superior perplexity (PPL) compared to LLaMA [44] and TNL [2],\n\nTable 4: Performance comparison Pile for large-scale language modeling.. We trained under the 10 billion token subset of Pile, and it can be seen that LightNet's PPL is better than LLaMA's. and matches the performance of Mamba [22]. The ability of LightNet to achieve high performance with reduced parameter complexity underscores its potential for scalability and broader application across various large-scale data scenarios. ### 5.3 Ablation Studies\n\nEffectiveness of Parameters Sharing. As discussed in Sec. 4.2, we employ a parameter sharing strategy between decay and key, and the performance comparison is presented in Table 4. The\n\nTable 5: Performance Scores on GLUE Benchmark. We utilize the Cramming-BERT 24-hour training configuration and observe that LightNet outperforms Crammed BERT and achieves comparable results to BERT-Base, which is trained with more GPU hours. | Model | MNLI | SST-2 | STSB | RTE | QNLI | QQP | MRPC | CoLA | GLUE |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BERT-Base (Fully trained) | $83.2 / 83.4$ | 91.9 | 86.7 | 59.2 | 90.6 | 87.7 | 89.3 | 56.5 | 80.9 |\n| BERT-Base (No Pretrain) | $34.1 / 34.1$ | 79.9 | 17.8 | 47.3 | 50.0 | 68.6 | 77.9 | - | 45.5 |\n| Crammed BERT | $83.9 / 84.1$ | 92.2 | 84.6 | 53.8 | 89.5 | 87.3 | 87.5 | 44.5 | 78.6 |\n| LightNet | $83.3 / 83.5$ | 92.9 | 86.3 | 55.6 | 89.1 | 87.7 | 88.5 | 52.6 | 79.9 |\n| LightNet w/o TPE | $82.1 / 82.9$ | 92.4 | 79.4 | 57.8 | 89.2 | 87.7 | 83.8 | 44.1 | 77.7 |\n\nTable 6: Ablation studies on image generation for LightNet-B/2 Configurations. We compare the performance of FID under different training steps. | Model | 50 K | 100 K | 150 K | 200 K | 250 K | 300 K | 350 K | 400 K |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| LightNet-B/2 | 104.19 | 74.27 | 59.60 | 51.22 | 45.70 | 41.65 | 38.60 | 36.45 |\n| LightNet-B/2 w/o TPE | 105.86 | 77.64 | 64.81 | 57.24 | 51.98 | 48.12 | 44.90 | 42.74 |\n| LightNet-B/2 w/o LRPE | 132.17 | 82.99 | 67.79 | 59.02 | 52.88 | 48.41 | 44.94 | 42.37 |\n\nresults demonstrate that employing independent parameters for decay and key leads to performance deterioration, highlighting the significance of parameter sharing. Effectiveness of MD-TPE. The proposed MD-TPE provides relative positional information under linear complexity. We thus explore the effectiveness of the MD-TPE across all tasks, shown in Table $1,4,5,6$. We can observe that removing MD-TPE results in significant performance degradation, particularly for image generation, which highly depends on the relative position of the image content. Similarly, performance comparison in language modeling tasks also confirms the effectiveness of MD-TPE when reduced to a single dimension. Effectiveness of MD-LRPE. LRPE has already proven its effectiveness in the field of language modeling. Therefore, when faced with higher-dimensional inputs, the contributions of its extension, MD-LRPE should be systematically validated. To this end, we conduct numerous ablation experiments, and the results, as shown in Table 1,4,5,6, demonstrate the effectiveness of extending LRPE into a multi-dimensional space through MD-LRPE operation. Speed Test. The current linear complexity models employ multiplicative linear recurrence in sequence modeling and necessitate at least two scans for multi-dimensional data, resulting in processing time denoted by the \" 2 Scan\" in Fig. 1. In contrast, our LightNet requires only a single scan, leading to a processing time denoted by the \" 1 Scan\". As evident from the figure, the advantage of the \" 1 Scan\" becomes increasingly pronounced with the growth of sequence length. ## 6 Conclusion\n\nIn this paper, we have addressed the inefficiency of \"multiplicative\" linear recurrence in multidimensional sequence modeling by introducing a novel \"additive\" linear recurrence that handles multi-dimensional data within a single scan. We developed LightNet, a new multi-dimensional linear attention model enhanced by two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE. Empirical evaluations across tasks like image classification, image generation, bidirectional language modeling, and autoregressive language modeling demonstrate LightNet's superior performance and versatility. LightNet offers a significant advancement in efficiency and scalability, providing a promising pathway for future research and applications in multi-dimensional sequence modeling. Limitations. Our empirical evaluation of LightNet is conducted on a smaller scale compared to other large-scale models. Potential negative social consequences include the misuse of brain models for inappropriate purposes or applications, necessitating prohibition through appropriate regulations. ## 7 Acknowledgments\n\nThis research was supported in part by the National Natural Science Foundation of China (62271410), the National Key R\\&D Program of China (NO.2022ZD0160100), and the Fundamental Research Funds for the Central Universities. Yuxin Mao is sponsored by the Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University (CX2024014). ## References\n\n[1] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. [2] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong. Transnormerllm: A faster and better large language model with improved transnormer. arXiv preprint arXiv:2307.14995, 2023. [3] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yutopeg Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [4] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7025-7041, 2022. [5] Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, and Wenhai Wang. Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures. arXiv preprint arXiv:2403.02308, 2024. [6] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.",
    "lightnet-10": "arXiv preprint arXiv:2401.09417, 2024. [7] Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Linearized relative positional encoding.",
    "lightnet-11": "Transactions on Machine Learning Research (TMLR), 2023. [8] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In Proceedings of the International Conference on Learning Representations (ICLR), 2020. [9] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "lightnet-12": "In International Conference on Machine Learning (ICML), pages 5156-5165. PMLR, 2020. [10] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention.",
    "lightnet-13": "In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion.",
    "lightnet-14": "arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.",
    "lightnet-15": "arXiv preprint arXiv:2403.13802, 2024. [29] Zhen Qin, Xuyang Shen, Dong Li, Weigao Sun, Stan Birchfield, Richard Hartley, and Yiran Zhong. Unlocking the secrets of linear complexity sequence model from a unified perspective. Arxiv, 2024. [30] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention2: A free lunch for handling unlimited sequence lengths in large language models.",
    "lightnet-16": "arXiv preprint arXiv:2401.04658, 2024. [31] Noam Shazeer. Glu variants improve transformer.",
    "lightnet-17": "arXiv preprint arXiv:2002.05202, 2020. [32] Zhen Qin and Yiran Zhong. Accelerating toeplitz neural network with constant-time inference complexity. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers \\& distillation through attention. In International Conference on Machine Learning (ICML), pages 10347-10357. PMLR, 2021. [34] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [35] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research (JMLR), 23(47):1-33, 2022. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684-10695, 2022. [37] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.",
    "lightnet-18": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248-255. Ieee, 2009. [38] Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day. arXiv preprint arXiv:2212.14034, 2022. [39] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. [40] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In Proceedings of the International Conference on Learning Representations (ICLR), 2016. [41] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [42] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation. Zenodo, September 2021. [43] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library.",
    "lightnet-19": "arXiv preprint arXiv:1912.01703, 2019. ## A Appendix\n\n## A. 1 Proof of Eq 2\n\nNote that\n\n$$\n\\begin{gathered}\nA_{t} y_{t}=A_{t} a_{t} y_{t-1}+A_{t} x_{t}=A_{t-1} y_{t-1}+A_{t} x_{t} \\\\\nA_{t} y_{t}-A_{t-1} y_{t-1}=A_{t} x_{t} \\\\\n\\cdots \\\\\nA_{2} y_{2}-A_{1} y_{1}=A_{2} x_{2}\n\\end{gathered}\n$$\n\nBy summing up, we can obtain:\n\n$$\nA_{t} y_{t}-A_{1} y_{1}=\\sum_{s=2}^{t} A_{s} c x_{s}, y_{t} A_{t}=\\sum_{s=1}^{t} A_{s} x_{s}, y_{t}=\\sum_{s=1}^{t} \\frac{A_{s}}{A_{t}} x_{s}\n$$\n\n## A. 2 More experiments\n\nIn this section, we provide additional experimental results. In Table 7, we show the performance of LightNet under the Commonsense Reasoning Tasks. In Table 8, we present the effects of LightNet on image generation tasks across various sizes. Table 7: Performance Comparison on Commonsense Reasoning Tasks. PS, T, HS, WG stand for parameter size (billion), tokens (billion), HellaSwag, and WinoGrande, respectively. | Model | P | T | PIQA | HS | WG | ARC-e | ARC-c | OBQA | AVG |\n| :--- | :---: | :---: | :---: | :--- | :--- | :--- | :--- | :--- | :--- |\n| GPT-Neo | 0.13 | 300 | 63.06 | 30.40 | 50.43 | 43.73 | 23.12 | 26.20 | 39.49 |\n| OPT | 0.16 | 300 | 62.95 | 31.35 | 50.43 | 43.52 | 22.70 | 28.00 | 39.83 |\n| Pythia | 0.16 | 300 | 61.32 | 30.16 | 51.93 | 43.18 | 23.12 | 26.80 | 39.42 |\n| RWKV-4 | 0.17 | - | 65.07 | 32.26 | 50.83 | 47.47 | 24.15 | 29.60 | 4.56 |\n| HGRN | 0.15 | 100 | 65.02 | 33.33 | 50.20 | 46.68 | 23.81 | 28.60 | 4.27 |\n| LightNet | 0.15 | 100 | 63.49 | 30.55 | 50.83 | 46.04 | 24.40 | 27.40 | 40.45 |\n| OPT | 0.35 | 300 | 64.58 | 36.69 | 52.49 | 44.02 | 23.89 | 28.20 | 41.65 |\n| Pythia | 0.40 | 300 | 67.08 | 40.52 | 53.59 | 51.81 | 24.15 | 29.40 | 44.43 |\n| BLOOM | 0.56 | 350 | 64.09 | 36.97 | 52.80 | 47.35 | 23.98 | 28.20 | 42.23 |\n| RWKV-4 | 0.43 | - | 67.52 | 40.90 | 51.14 | 52.86 | 25.17 | 32.40 | 45.00 |\n| HGRN | 0.35 | 100 | 66.70 | 38.12 | 51.70 | 49.20 | 25.26 | 30.60 | 43.60 |\n| LightNet | 0.39 | 100 | 66.87 | 38.82 | 50.75 | 51.39 | 25.17 | 28.20 | 43.53 |\n| GPT-Neo | 1.3 | 300 | 71.11 | 48.93 | 54.93 | 56.19 | 25.85 | 33.60 | 48.44 |\n| OPT | 1.3 | 300 | 71.71 | 53.70 | 59.35 | 57.24 | 29.69 | 33.20 | 50.82 |\n| Pythia | 1.4 | 300 | 70.67 | 47.18 | 53.51 | 56.99 | 26.88 | 31.40 | 47.77 |\n| BLOOM | 1.1 | 350 | 67.14 | 42.98 | 54.93 | 51.47 | 25.68 | 29.40 | 45.27 |\n| RWKV-4 | 1.5 | - | 72.36 | 52.48 | 54.62 | 60.48 | 29.44 | 34.00 | 50.56 |\n| HGRN | 1.0 | 100 | 70.89 | 48.02 | 51.62 | 55.64 | 27.90 | 31.60 | 47.61 |\n| LightNet | 1.0 | 100 | 69.75 | 44.68 | 51.85 | 55.35 | 27.65 | 32.80 | 47.01 |\n| OPT | 2.7 | 300 | 73.83 | 60.60 | 61.01 | 60.77 | 31.31 | 35.2 .0 | 53.79 |\n| Pythia | 2.8 | 300 | 74.10 | 59.31 | 59.91 | 64.14 | 33.02 | 35.60 | 54.35 |\n| BLOOM | 3.0 | 350 | 70.57 | 54.53 | 58.48 | 59.43 | 30.38 | 32.20 | 50.93 |\n| RWKV-4 | 3.0 | - | 72.42 | 58.75 | 57.30 | 62.92 | 35.15 | 36.20 | 53.79 |\n| LightNet | 2.9 | 100 | 73.56 | 55.47 | 57.77 | 61.57 | 32.94 | 33.60 | 52.49 |\n\n## A. 3 Configurations\n\nIn this section, we provide training configurations for all experiments. The configuration for Bidirectional Language Modeling is the same as [38], while the configurations for the other experiments are as shown in Table 9, 10, 11, 12. We use Pytorch [45] and A100 for training. Table 8: Performance Metrics Across Different LightNet Configurations\n\n| Model | 50 K | 100 K | 150 K | 200K | 250K | 300 K | 350 K | 400 K |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| LightNet-S/8 | 192.79 | 172.23 | 161.23 | 154.34 | 150.25 | 147.40 | 145.27 | 143.31 |\n| LightNet-S/4 | 167.33 | 132.89 | 118.77 | 110.88 | 105.15 | 101.25 | 97.56 | 94.90 |\n| LightNet-S/2 | 145.66 | 119.20 | 104.90 | 94.45 | 87.18 | 82.41 | 78.63 | 75.61 |\n| DiT-S/2 | - | - | - | - | - | - | - | 67.16 |\n| LightNet-B/8 | 170.79 | 146.43 | 134.63 | 127.31 | 122.18 | 118.50 | 115.40 | 113.02 |\n| LightNet-B/4 | 126.37 | 93.86 | 81.44 | 74.11 | 68.80 | 65.09 | 62.34 | 59.81 |\n| LightNet-B/2 | 104.19 | 74.27 | 59.60 | 51.22 | 45.70 | 41.65 | 38.60 | 36.45 |\n| DiT-B/2 | - | - | - | - | - | - | - | 42.76 |\n| LightNet-L/8 | 157.76 | 130.29 | 116.06 | 107.50 | 101.10 | 96.47 | 92.79 | 89.51 |\n| LightNet-L/4 | 104.18 | 77.02 | 64.55 | 56.16 | 49.99 | 45.58 | 41.91 | 37.54 |\n| LightNet-L/2 | 84.38 | 48.98 | 35.32 | 28.05 | 23.75 | 21.06 | 18.94 | 17.42 |\n| DiT-L/2 | - | - | - | - | - | - | - | 24.37 |\n| LightNet-XL/8 | 158.75 | 129.23 | 114.72 | 105.75 | 99.35 | 94.53 | 90.66 | 87.22 |\n| LightNet-XL/4 | 101.39 | 70.84 | 56.75 | 48.04 | 42.04 | 37.43 | 34.16 | 31.51 |\n| LightNet-XL/2 | 79.22 | 45.46 | 31.61 | 25.55 | 21.37 | 18.74 | 16.84 | 15.52 |\n| DiT-XL/2 | - | - | - | - | - | - | - | 19.20 |\n\nTable 9: Comprehensive Configurations of the Model and Training Procedures for LightNet Experiments \"Total batch size\" means batch_per_gpu $\\times$ update_freq $\\times$ num_gpus; \"ALM\" stands for Autoregressive Language Model; \"IM\" stands for Image Modeling, \"IG\" stands for image generation. |  | ALM | IM | IG |\n| :--- | :--- | :--- | :--- |\n| Dataset | WikiText-103 | ImageNet-1k | ImageNet-1k |\n| Tokenizer method | BPE | - | - |\n| Src Vocab size | 50265 | - | - |\n| Sequence length | 512 | - | - |\n| Total batch size | 128 | 2048 | 256 |\n| Number of updates/epochs | 50 k updates | 300 epochs | 80 epochs |\n| Warmup steps/epochs | 4 k steps | 20 epochs | - |\n| Peak learning rate | $5 \\mathrm{e}-4$ | $5 \\mathrm{e}-4$ | $1 \\mathrm{e}-4$ |\n| Learning rate scheduler | Inverse sqrt | Cosine | - |\n| Optimizer | Adam | Adamw | Adamw |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-8$ | $1 \\mathrm{e}-8$ | $1 \\mathrm{e}-8$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.999)$ | $(0.9,0.98)$ | $(0.9,0.98)$ |\n| Weight decay | 0.1 | 0.1 for Base, else 0.05 | 0 |\n| Gradient clipping | - | 5.0 | - |\n| GPUS | 4 | 8 | 8 |\n\nTable 10: Configurations for LLM\n\n| Params(B) | Layers | Hidden Dim | L.R. | Batch Size Per GPU | SeqLen | GPUs |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 0.15 | 15 | 768 | $3.00 \\mathrm{E}-04$ | 26 | 2048 | 8 |\n| 0.385 | 26 | 1024 | $3.00 \\mathrm{E}-04$ | 15 | 2048 | 8 |\n| 1.0 | 18 | 2048 | $3.00 \\mathrm{E}-04$ | 10 | 2048 | 16 |\n| 2.9 | 36 | 2560 | $3.00 \\mathrm{E}-04$ | 36 | 2048 | 48 |\n\nTable 11: Model Configurations for Image Generation task. | Model | Layers | Hidden Dim | Heads | Params |\n| :---: | :---: | :---: | :---: | :---: |\n| LightNet-S | 18 | 384 | 6 | 33 M |\n| LightNet-B | 18 | 768 | 6 | 131 M |\n| LightNet-L | 36 | 1024 | 16 | 470 M |\n| LightNet-XL | 42 | 1152 | 16 | 680 M |\n\nTable 12: Model Configurations for Image Classification task. | Model | Layers | Hidden size | Heads | Params |\n| :---: | :---: | :---: | :---: | :---: |\n| LightNet-T | 12 | 192 | 6 | 6.0 M |\n| LightNet-S | 12 | 384 | 16 | 22.6 M |\n| LightNet-B | 12 | 768 | 16 | 87.7 M |\n\n\n[^0]:    ${ }^{\\square}$ Indicates corresponding author (Email address: zhongyiran@gmail.com). "
}