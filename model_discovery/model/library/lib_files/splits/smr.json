{
    "smr-0": "SMR: State Memory Replay for Long Sequence Modeling\n\nBiqing Qi1,2,\u2217, Junqi Gao3,, Kaiyan Zhang2, Dong Li3, Jianxing Liu1, Ligang Wu1,, Bowen Zhou2,\u2020 1 Department of Control Science and Engineering, Harbin Institute of Technology, 2 Department of Electronic Engineering, Tsinghua University, 3 School of Mathematics, Harbin Institute of Technology, {qibiqing7,gjunqi97,arvinlee826}@gmail.com, zhang-ky22@mails.tsinghua.edu.cn, {jx.liu,ligangwu}@hit.edu.cn, zhoubowen@tsinghua.edu.cn Equal contributions.Corresponding authors: Bowen Zhou and Ligang Wu.",
    "smr-1": "Abstract\n\nDespite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM\u2019s hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models. SMR: State Memory Replay for Long Sequence Modeling\n\nBiqing Qi1,2,\u2217, Junqi Gao3,\u2020\u2020thanks: Equal contributions., Kaiyan Zhang2, Dong Li3, Jianxing Liu1, Ligang Wu1,\u2020\u2020thanks: Corresponding authors: Bowen Zhou and Ligang Wu., Bowen Zhou2,\u2020 1 Department of Control Science and Engineering, Harbin Institute of Technology, 2 Department of Electronic Engineering, Tsinghua University, 3 School of Mathematics, Harbin Institute of Technology, {qibiqing7,gjunqi97,arvinlee826}@gmail.com, zhang-ky22@mails.tsinghua.edu.cn, {jx.liu,ligangwu}@hit.edu.cn, zhoubowen@tsinghua.edu.cn\n\n1 Introduction\n\nLong sequence modeling has attracted extensive interest due to its broad prospects in natural language processing (Beltagy et al., 2020; Brown et al., 2020; Ouyang et al., 2022).",
    "smr-2": "The mainstream architectures for sequence modeling mainly focus on attention-based Transformers (Vaswani et al., 2017). However, the quadratic complexity of softmax attention brings a computational bottleneck (Choromanski et al., 2020; Wang et al., 2020; Beltagy et al., 2020), which makes attention-based architectures inefficient for handling long sequences. Although the introduction of linear attention (Wang et al., 2020) reduces the computational complexity, it cannot well approximate the performance of the vanilla Transformer. More importantly, purely attention-based architectures cannot capture long-range dependencies well. On the other hand, state space model (SSM)-based architectures (Gu et al., 2021a; Gupta et al., 2022) show superior performance on the Long Range Arena (LRA) (Tay et al., 2021) benchmark for long sequence modeling due to their linear computational complexity and excellent long-range dependency capturing ability. Existing SSM-based model architectures, such as S5 (Smith et al., 2023) and S6 (Gu and Dao, 2023), primarily rely on recursive structures to tackle the varying sampling step issue. S5 introduced learnable step sizes for each step to improve the Sampling Step Adaptation (SSA) capability of SSM. S6 (Mamba) introduced data-dependent parameter settings, which makes the state propagation of the SSM model more flexible. However, this restricts its inference computation to parallel scanning instead of the original efficient convolution mode, significantly hampering training efficiency and imposing a heavier inference burden when handling long inputs at once. To address the mentioned issues, we aim to propose a method that goes beyond recursive constraints to improve SSA capability. This strategy seeks to enhance the SSM, making it more adaptable and flexible for various parallel convolution computation types, including advanced architectures like S4 (Gu et al., 2022), Mega (Ma et al., 2023), SPADE (Zuo et al., 2022), and more. Specifically, we leverage the Event-Triggered Control (ETC) Theory (Heemels et al., 2012; Tabuada, 2007) to provide the first demonstration of the Non-Stable State (NSS) problem in SSMs. We show that for a fixed-parameter SSM, varying sampling steps, deviating from the model\u2019s sampling point requirements, triggers error propagation and accumulation, ultimately leading to the divergence of the hidden state. Our analysis further reveals that adjustments based on early memories of the input sequence can achieve SSA, effectively solving the NSS problem. Inspired by this finding, we propose a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), it can significantly alleviate the NSS problem in SSMs by improving SSM the capability of SSA thus bring further sequence modeling capabilities. In particular, SMR can achieve better generalization ability at different sampling points, especially when dealing with the stochastic selected sampling points. We conduct experiments on autoregressive language modeling on Wikitext-103 (Merity et al., 2017) and long sequence modeling on LRA. The results show that the SMR mechanism can bring better performance to SSM-based model, on both autoregressive language modeling and long sequence modeling tasks. It can also further improve a series of competitive SSM-based models such as S5, SPADE, Mega, and S6, which verifies the generality and effectiveness of the proposed SMR mechanism. In summary, our main contributions are three folds:\n\n\u2022\n\nWe are the first to identify the NSS issue in SSMs. We theoretically analyze and experimentally verify the issue from a novel perspective of ETC theory, demonstrating that inputs that do not satisfy the stability condition can lead to the divergence of the hidden states of SSMs and affect model performance. \u2022\n\nBased on our theoretical analysis and experimental results, we reveal that adjustment of the input sequence with early memory can achieve adaptive sampling adjustment capability to solve the NSS problem. Motivated by this, we propose the SMR mechanism. \u2022\n\nSMR is able to enhance the existing SSM series models to improve sampling point generalization and sequence modeling capabilities in some real-world tasks with varying design sampling points, including autoregressive language modeling and long sequence modeling, without affecting computational efficiency. 2 Preliminaries: State Space Models\n\nThe state space model is formally defined by eq.(1) and eq.(2):\n\n\ud835\udc99 \u02d9 \u200b ( t ) = \ud835\udc68 \u200b \ud835\udc99 \u200b ( t ) + \ud835\udc69 \u200b u \u200b ( t ) , \u02d9 \ud835\udc99 \ud835\udc61 \ud835\udc68 \ud835\udc99 \ud835\udc61 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\dot{\\boldsymbol{x}}(t)={\\boldsymbol{A}}\\boldsymbol{x}(t)+{\\boldsymbol{B}}{u}(t), (1)\n\ny \u200b ( t ) = \ud835\udc6a \u200b \ud835\udc99 \u200b ( t ) + \ud835\udc6b \u200b u \u200b ( t ) , \ud835\udc66 \ud835\udc61 \ud835\udc6a \ud835\udc99 \ud835\udc61 \ud835\udc6b \ud835\udc62 \ud835\udc61 {y}(t)={\\boldsymbol{C}}\\boldsymbol{x}(t)+{\\boldsymbol{D}}{u}(t), (2)\n\nwhere , , , , denotes the input sequence with dimension , and is the latent state. S4\n\nPrevious works (Gu et al., 2021b, 2022) formed the S4 model, which constructed a set of Structured State-Space Sequence Model (S4) parameters for each dimension of the input to construct an Single-Input, Single-Output (SISO) system, i.e., for an input , the same set of SSM parameters is broadcasted to each dimension . Specifically, they employed the bilinear method to perform discretization:\n\n\ud835\udc99 k = \ud835\udc68 \u00af \u200b \ud835\udc99 k \u2212 1 + \ud835\udc69 \u00af \u200b u k ( p ) , subscript \ud835\udc99 \ud835\udc58 \u00af \ud835\udc68 subscript \ud835\udc99 \ud835\udc58 1 \u00af \ud835\udc69 superscript subscript \ud835\udc62 \ud835\udc58 \ud835\udc5d \\displaystyle\\boldsymbol{x}_{k}=\\overline{\\boldsymbol{A}}\\boldsymbol{x}_{k-1}+\\overline{\\boldsymbol{B}}{u}_{k}^{(p)}, (3) y k = \ud835\udc6a \u00af \u200b \ud835\udc99 k , subscript \ud835\udc66 \ud835\udc58 \u00af \ud835\udc6a subscript \ud835\udc99 \ud835\udc58 \\displaystyle{y}_{k}=\\overline{\\boldsymbol{C}}\\boldsymbol{x}_{k}, (4)\n\nwhere , , .",
    "smr-3": "The matrix is omitted here because it can be viewed as a residual connection. For each element , is a fixed discretization step, the same for each step. Then, the S4 became a parameterized model with trainable parameters , , , and . By assuming , we can obtain:\n\ny k = \ud835\udc6a \u200b \ud835\udc68 \u00af k \u2212 1 \u200b \ud835\udc69 \u00af \u200b u 1 + \u22ef + \ud835\udc6a \u200b \ud835\udc68 \u00af \u200b \ud835\udc69 \u00af \u200b u k \u2212 1 + \ud835\udc6a \u200b \ud835\udc69 \u00af \u200b u k , subscript \ud835\udc66 \ud835\udc58 superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc58 1 \u00af \ud835\udc69 subscript \ud835\udc62 1 \u22ef \u00af \ud835\udc6a \ud835\udc68 \u00af \ud835\udc69 subscript \ud835\udc62 \ud835\udc58 1 \u00af \ud835\udc6a \ud835\udc69 subscript \ud835\udc62 \ud835\udc58 {y}_{k}=\\overline{\\boldsymbol{CA}}^{k-1}\\overline{\\boldsymbol{B}}{u}_{1}+\\cdots+\\overline{\\boldsymbol{CA}}\\overline{\\boldsymbol{B}}u_{k-1}+\\overline{\\boldsymbol{CB}}{u}_{k}, (5)\n\nthus the output could be calculated efficiently by convolution , where\n\n\ud835\udc72 \u00af \u2208 \u211d L := \ud835\udca6 L \u200b ( \ud835\udc68 \u00af , \ud835\udc69 \u00af , \ud835\udc6a \u00af ) := ( \ud835\udc6a \u200b \ud835\udc68 \u00af i \u200b \ud835\udc69 \u00af ) i \u2208 [ L \u2212 1 ] \u00af \ud835\udc72 superscript \u211d \ud835\udc3f assign subscript \ud835\udca6 \ud835\udc3f \u00af \ud835\udc68 \u00af \ud835\udc69 \u00af \ud835\udc6a assign subscript superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc56 \u00af \ud835\udc69 \ud835\udc56 delimited-[] \ud835\udc3f 1 \\displaystyle\\overline{\\boldsymbol{K}}\\in\\mathbb{R}^{L}:=\\mathcal{K}_{L}(\\overline{\\boldsymbol{A}},\\overline{\\boldsymbol{B}},\\overline{\\boldsymbol{C}}):=\\left(\\overline{\\boldsymbol{CA}}^{i}\\overline{\\boldsymbol{B}}\\right)_{i\\in[L-1]} (6) = ( \ud835\udc6a \u200b \ud835\udc69 \u00af , \ud835\udc6a \u200b \ud835\udc68 \u200b \ud835\udc69 \u00af , \u2026 , \ud835\udc6a \u200b \ud835\udc68 \u00af L \u2212 1 \u200b \ud835\udc69 \u00af ) , absent \u00af \ud835\udc6a \ud835\udc69 \u00af \ud835\udc6a \ud835\udc68 \ud835\udc69 \u2026 superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc3f 1 \u00af \ud835\udc69 \\displaystyle=\\left(\\overline{\\boldsymbol{CB}},\\overline{\\boldsymbol{CAB}},\\ldots,\\overline{\\boldsymbol{CA}}^{L-1}\\overline{\\boldsymbol{B}}\\right),\n\nis the convolution kernel and is the sequence length. With their proposed Normal Plus Low-Rank (NPLR) parameterization, the S4 Convolution could be calculated in operations. S5\n\nGiven the uniform time step employed by S4 for each time interval, it encounters difficulties when confronted with irregularly sampled data. To overcome this limitation, S5 (Smith et al., 2023) introduced adaptive and learnable step sizes for each time step, enhanced its capability to effectively handle irregularly sampled data. Furthermore, S5 extended the S4-established Single-Input Single-Output (SISO) system to a more versatile Multiple-Input, Multiple-Output (MIMO) system. Specifically, by diagonalizing the SSM dynamics, they reparameterized matrix as a diagonal matrix. Simultaneously, and are configured as matrices rather than the vectorized and settings used in S4. However, introducing variable step sizes for different time steps constrains the efficient convolutional computation of the SSM, forcing it to resort to a slower recurrent-based computation. Even with the diagonalized state transition matrix setting, the computational complexity can only be reduced to , thereby restricted the training efficiency of the SSM. S6 (Mamba)\n\nThe SSM parameters in S4 and S5 are fixed after training, making them data-independent. This somewhat restricts the flexibility of both models. In contrast, S6, as known as Mamba (Gu and Dao, 2023), overcomes this limitation by introducing data-dependent S4 parameters. It achieves this by employing trainable linear layers to maps the input to each step\u2019s , , and time step in S4. Additionally, S6 extended its parameters to be time-variant, transforming from a time-invariant system (as in S4 and S5) to a time-variant one. This enhancement allows S6 to conduct more flexible sequence modeling. However, due to its time-dependent parameterization, S6 cannot efficiently perform SSM computations using convolution, maintaining a computational complexity of resulting in slower training compared to S4. 3 SSA via State Memory Replay\n\nIn this section, we aim to reveal the problem of NSS in SSM caused by changes in sampling points through ETC theory (Heemels et al., 2012; Tabuada, 2007) (Section 3.1). We demonstrate that unstable hidden states lead to errors in SSM (Section 3.2). Furthermore, through the analysis based on ETC theory, we propose a simple but effective step-size adaptation mechanism, SMR, to enhance the model\u2019s SSA capability thus alleviate the NSS problem in SSM with fixed step setting (Section 3.3). Experimental results indicate that the SMR mechanism can not only enhance the SSA capability of SSM with fixed parameters but can also be extended to other SSM-based models, improving their SSA capabilities (Section 3.4). 3.1 Non-Stable-States Phenomenon\n\nWith the help of the ETC theory, we provide a simple example to elucidate the phenomenon of NSS. In this context, ETC theory ensures the system\u2019s states remain stable by sampling the input control signal using triggered events. To maintain stability, the selection of sampling points, such as , must meet specific criteria. Typically, a Lyapunov function is employed to assess stability (Heemels et al., 2012), outside the stable point, it is monotonically decreasing, and the minimum value of is achieved at the stable point. Sampling points that result in a decreasing trend of are selected to ensure system stability. Specifically, consider the linear system described in eq.(1). Assuming the input control signal satisfies the linearity , where , then eq.(1) becomes:\n\n\ud835\udc99 \u02d9 \u200b ( t ) = \ud835\udc68 \u200b \ud835\udc99 \u200b ( t ) + \ud835\udc69 \u200b \ud835\udc7b \u200b \ud835\udc99 \u200b ( t ) . \u02d9 \ud835\udc99 \ud835\udc61 \ud835\udc68 \ud835\udc99 \ud835\udc61 \ud835\udc69 \ud835\udc7b \ud835\udc99 \ud835\udc61 \\dot{\\boldsymbol{x}}(t)={\\boldsymbol{A}}\\boldsymbol{x}(t)+{\\boldsymbol{B}}\\boldsymbol{T}\\boldsymbol{x}(t). (7)\n\nIt can be easily verified that is a Lyapunov function, where symmetric positive definite matrix satisfies:\n\n( \ud835\udc68 + \ud835\udc69 \u200b \ud835\udc7b ) \u22a4 \u200b \ud835\udc77 + \ud835\udc77 \u200b ( \ud835\udc68 + \ud835\udc69 \u200b \ud835\udc7b ) = \u2212 \ud835\udc74 , superscript \ud835\udc68 \ud835\udc69 \ud835\udc7b top \ud835\udc77 \ud835\udc77 \ud835\udc68 \ud835\udc69 \ud835\udc7b \ud835\udc74 (\\boldsymbol{A}+\\boldsymbol{BT})^{\\top}\\boldsymbol{P}+\\boldsymbol{P}(\\boldsymbol{A}+\\boldsymbol{BT})=-\\boldsymbol{M}, (8)\n\nto keep , where is also a symmetric positive definite matrix. Note that the actual sampled input is sampled at the sampling points , we denote the sampling error:\n\n\ud835\udc86 \u200b ( t ) = \ud835\udc99 \u200b ( t i ) \u2212 \ud835\udc99 \u200b ( t ) , \u2200 t \u2208 [ t i , t i + 1 ) , i \u2208 \u2115 , formulae-sequence \ud835\udc86 \ud835\udc61 \ud835\udc99 subscript \ud835\udc61 \ud835\udc56 \ud835\udc99 \ud835\udc61 formulae-sequence for-all \ud835\udc61 subscript \ud835\udc61 \ud835\udc56 subscript \ud835\udc61 \ud835\udc56 1 \ud835\udc56 \u2115 \\boldsymbol{e}(t)=\\boldsymbol{x}(t_{i})-\\boldsymbol{x}(t),\\quad\\forall t\\in[t_{i},t_{i+1}),i\\in\\mathbb{N}, (9)\n\nthen eq.(7) could be reformulated as:\n\n\ud835\udc99 \u02d9 \u200b ( t ) = \ud835\udc68 \u200b \ud835\udc99 \u200b ( t ) + \ud835\udc69 \u200b \ud835\udc7b \u200b ( \ud835\udc99 \u200b ( t ) + \ud835\udc86 \u200b ( t ) ) . \u02d9 \ud835\udc99 \ud835\udc61 \ud835\udc68 \ud835\udc99 \ud835\udc61 \ud835\udc69 \ud835\udc7b \ud835\udc99 \ud835\udc61 \ud835\udc86 \ud835\udc61 \\dot{\\boldsymbol{x}}(t)=\\boldsymbol{A}\\boldsymbol{x}(t)+{\\boldsymbol{B}}\\boldsymbol{T}(\\boldsymbol{x}(t)+\\boldsymbol{e}(t)). (10)\n\nTaking the derivative of , we have:\n\nd d \u200b t \u200b \u2112 V \u200b ( t ) = \u2212 \ud835\udc99 \u200b ( t ) T \u200b \ud835\udc74 \u200b \ud835\udc99 \u200b ( t ) + 2 \u200b \ud835\udc99 \u200b ( t ) \u22a4 \u200b \ud835\udc77 \u200b \ud835\udc69 \u200b \ud835\udc7b \u200b \ud835\udc86 \u200b ( t ) . \ud835\udc51 \ud835\udc51 \ud835\udc61 subscript \u2112 \ud835\udc49 \ud835\udc61 \ud835\udc99 superscript \ud835\udc61 \ud835\udc47 \ud835\udc74 \ud835\udc99 \ud835\udc61 2 \ud835\udc99 superscript \ud835\udc61 top \ud835\udc77 \ud835\udc69 \ud835\udc7b \ud835\udc86 \ud835\udc61 \\frac{d}{dt}\\mathcal{L}_{V}(t)=-\\boldsymbol{x}(t)^{T}\\boldsymbol{M}\\boldsymbol{x}(t)+2\\boldsymbol{x}(t)^{\\top}\\boldsymbol{PBT}\\boldsymbol{e}(t). (11)\n\nTherefore, set , we have the following triggering condition to ensure system stability:\n\nt i + 1 = inf { t \u2208 \u211d \u2223 t > t i \u2227 \u03ba \ud835\udc99 ( t ) \u22a4 \ud835\udc74 \ud835\udc99 ( t ) \\displaystyle t_{i+1}=\\inf\\left\\{t\\in\\mathbb{R}\\mid t>t_{i}\\wedge\\kappa\\boldsymbol{x}(t)^{\\top}\\boldsymbol{M}\\boldsymbol{x}(t)\\right. (12) \u2212 2 \ud835\udc99 ( t ) \u22a4 \ud835\udc77 \ud835\udc69 \ud835\udc7b \ud835\udc86 ( t \u2212 ) \u2264 0 } , \\displaystyle\\left.-2\\boldsymbol{x}(t)^{\\top}\\boldsymbol{PBT}\\boldsymbol{e}(t^{-})\\leq 0\\right\\},\n\nwhere is a optional constant, represents the left-hand limit of error at point . In other words, new control signals are inputted just before the system becomes unstable. In this way, the sampled input control sequence obtained can ensure exponential stability of the system:\n\n\u2112 V \u200b ( t ) \u2264 \u2112 V \u200b ( 0 ) \u200b e ( \u03ba \u2212 1 ) \u200b \u03b9 \u200b t , subscript \u2112 \ud835\udc49 \ud835\udc61 subscript \u2112 \ud835\udc49 0 superscript \ud835\udc52 \ud835\udf05 1 \ud835\udf04 \ud835\udc61 \\mathcal{L}_{V}(t)\\leq\\mathcal{L}_{V}(0)e^{(\\kappa-1)\\iota t}, (13)\n\nwhere is an positive constant. More specifically, we provide an example of a 1-D input where the selected parameters are as follows:\n\n\ud835\udc68 = [ 0 1 2 \u2212 3 ] , \ud835\udc74 = [ 0.5 0.25 0.25 1.5 ] , formulae-sequence \ud835\udc68 matrix 0 1 2 3 \ud835\udc74 matrix 0.5 0.25 0.25 1.5 \\displaystyle\\boldsymbol{A}=\\begin{bmatrix}0&1\\\\\n2&-3\\end{bmatrix},\\boldsymbol{M}=\\begin{bmatrix}0.5&0.25\\\\\n0.25&1.5\\end{bmatrix}, \ud835\udc77 = [ 1 0.25 0.25 1 ] , \ud835\udc69 = [ 0 1 ] , \ud835\udc7b = [ 1 \u2212 4 ] , formulae-sequence \ud835\udc77 matrix 1 0.25 0.25 1 formulae-sequence \ud835\udc69 matrix 0 1 \ud835\udc7b matrix 1 4 \\displaystyle\\boldsymbol{P}=\\begin{bmatrix}1&0.25\\\\\n0.25&1\\end{bmatrix},\\boldsymbol{B}=\\begin{bmatrix}0\\\\\n1\\end{bmatrix},\\boldsymbol{T}=\\begin{bmatrix}1\\\\\n-4\\end{bmatrix},\n\nThe selected time window is , with a time grid width of . Subsequently, we conduct simulation experiments on the system, and the results is shown in the leftmost of Fig.1, the triggering moment is marked with a gray dashed line. Under the sampled input obtained from ETC, the system\u2019s state eventually reaches the stabilization. NSS: Instability Arising from sampling Grid variation. To further substantiate this conclusion, we present an illustrative example. Specifically, we introduce minor perturbations to the sampled data points, strictly constrained within the temporal grid width. The second plot in Fig.1 illustrates the comparison between the perturbed input and the original input, where the disturbance is almost imperceptible. When utilizing the unaltered sampled data points obtained prior to perturbation as input, the third figure in Fig.1 visually represents the system\u2019s sustained stability. Nevertheless, upon the introduction of perturbated sampled data points into the system, as depicted in the rightmost in Fig.1, it becomes apparent that the system\u2019s stability cannot be guaranteed, leading to an exponential growth in magnitude reaching . This means that when the actual sampling points do not align with the desired sampling grid, it will result in highly unstable states. For SSM models formulated as in eq.(1) and eq.(2), encountering such an issue would lead to unavoidable numerical errors (Proposition 1). 3.2 Theoretical Understanding of NSS\n\nBased on the aforementioned considerations and insights, our understanding of the NSS problem in SSM models is as follows: For SSM models with fixed parameters, the NSS problem may arise when the input does not satisfy stability conditions. Once the sampling error propagates over an extended period along with the hidden states, numerical errors inevitably occur, as affirmed by Proposition 1. Proposition 1\n\nGiven bounded inputs satisfying , and , and defining the observation error caused by sampling points as , it can be concluded that when , where represents the largest eigenvalue of matrix , the prediction error will accumulate over time steps. To ascertain the presence of an NSS issue within the SSM model, we devise a simple sequence modeling task. We sample equidistant points from the function to serve as input . Then, we employ a single-layer S4 model for fitting, which underwent training for epochs, yielding the results displayed in the leftmost in Fig.2. Following a methodology akin to the previous example, we apply perturbations smaller than the sampling window width to the sampled points . Subsequently, we conduct sampling on the perturbed points . This process generates a set of perturbed inputs, denoted as , as illustrated in the second figure in Fig.2, where the sampling points underwent slight alterations. Subsequently, we employ the trained S4 model to predict , resulting in a numerical instability, as evident in the third figure in Fig.2. We graphically represent the latent states before and after perturbation in the rightmost figure in Fig.2. In both instances, unstable states were observed, and notably, the total magnitude of the state increased following the perturbation. We extend this verification to a -layer S4 model and observe analogous findings. The outcomes are detailed in Appendix A.2. Therefore, as our analysis reveals, SSMs indeed exhibit the issue of NSS, leading to larger errors when confronted with data exhibiting changes in sampling points. While S5, employing a strategy of assigning different step sizes at each step, can adapt to irregularly sampled data, the fixed SSM parameters during the inference phase still fail to ensure adaptive adjustments to various sampling data, thereby not completely avoiding NSS problems. On the other hand, S6 introduces data-dependent SSM parameterization, ensuring adaptive adjustments during the state transition process. However, this constraint limits S6 from efficiently computing in a convolutional form. In the subsequent analysis, we leverage ETC theory to provide insights and propose a strategy for adaptively adjusting inputs, aiming to address the NSS problem in SSM. 3.3 State Memory Replay Mechanism\n\nWe initiate our investigation by conducting a preliminary analysis rooted in ETC theory to derive insights for formulating adjust strategies. We examine an input perturbation denoted as at the sampling point, where . Assuming a tiny perturbation , we have . Hence, the observed state can be expressed as , and we also define the discrepancy between the observed state and the actual state as the error . Drawing inspiration from ETC theory, the Lyapunov function L is utilized as an indicator of observation error stability in the system.",
    "smr-4": "A smaller absolute value of indicates a reduced impact of noise and uncertainty on system performance, as demonstrated in (Vallarella and Haimovich, 2019). Then, we have Theorem 1. Theorem 1\n\nFor the input reply factor , the adjusted input , where is the state value of observer, considering the Lyapunov function , we have:\n\nd \u200b \u2112 \ud835\udc86 \u200b ( t ) d \u200b t \u2264 \ud835\udc86 \u22a4 \u200b ( t ) \u200b ( \ud835\udc77 \u200b \ud835\udc68 + \ud835\udc68 \u22a4 \u200b \ud835\udc77 ) \u200b \ud835\udc86 \u200b ( t ) \ud835\udc51 subscript \u2112 \ud835\udc86 \ud835\udc61 \ud835\udc51 \ud835\udc61 superscript \ud835\udc86 top \ud835\udc61 \ud835\udc77 \ud835\udc68 superscript \ud835\udc68 top \ud835\udc77 \ud835\udc86 \ud835\udc61 \\displaystyle\\frac{d\\mathcal{L}_{\\boldsymbol{e}}(t)}{dt}\\leq\\boldsymbol{e}^{\\top}(t)\\left(\\boldsymbol{PA}+\\boldsymbol{A^{\\top}P}\\right)\\boldsymbol{e}(t) (14) + 2 \u200b \u210f \u200b ( t ) \u200b ( \u222b 0 t \u2016 \ud835\udc8c \u200b ( t \u2212 l ) \u2016 \u200b | \u03b5 \u200b ( l ) | \u200b \ud835\udc51 l + \u2016 \ud835\udc69 \u2016 \u200b | \u03b5 \u200b ( t ) | ) , 2 Planck-constant-over-2-pi \ud835\udc61 superscript subscript 0 \ud835\udc61 norm \ud835\udc8c \ud835\udc61 \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 norm \ud835\udc69 \ud835\udf00 \ud835\udc61 \\displaystyle+2\\hbar(t)\\left(\\int_{0}^{t}\\left\\|\\boldsymbol{k}(t-l)\\right\\|\\left|\\varepsilon(l)\\right|dl+\\left\\|\\boldsymbol{B}\\right\\|\\left|\\varepsilon(t)\\right|\\right),\n\nwhere and is a positive definite symmetric matrix and is a fixed coefficient function determinded by the SSM parameters. Remark 1. Theorem 1 suggests that imposing additional constraints on the input controller can improve the convergence of the system. In particular, when (which corresponding to S4), we have . The control factor is required to incorporate information from the time interval . To accomplish this, a convolution with a kernel of length , denoted as , can be utilized. Moreover, an activation function, denoted as , can be employed to ensure that the condition is satisfied. This condition contributes to the enhancement of system stability. To meet this need, considering the analysis in Remark 1, we propose the design of a convolutional learnable variables that incorporates multi input states, enabling adaptive learning and refinement. Building upon Theorem 1, we understand the importance of having learnable variables that can incorporate multi input states to control how sampling information behaves, allowing for automatic adjustments. To fulfill this requirement, considering the analysis in Remark 1, we propose the SMR mechanism aimed at addressing the NSS problem caused by variations in sampling points. The SMR mechanism incorporates learnable memories to enhance the SSM model with multiple memory steps, through a convolutional learnable variables that incorporates multi input steps, enabling adaptive learning and refinement, as depicted in Fig.3. Formally,our proposed SMR mechanism can be formulated as:\n\nx k = \ud835\udc68 \u00af \u200b \ud835\udc99 k \u2212 1 + subscript \ud835\udc65 \ud835\udc58 limit-from \u00af \ud835\udc68 subscript \ud835\udc99 \ud835\udc58 1 \\displaystyle x_{k}=\\overline{\\boldsymbol{A}}\\boldsymbol{x}_{k-1}+ (15) \ud835\udc69 \u00af \u200b u k \u200b \u03c3 Sig \u200b ( \ud835\udca6 \u03c4 \u2217 ( u 1 , \u2026 , u 1 \u23df \u03c4 , \u2026 , u T ) ) k , \u00af \ud835\udc69 subscript \ud835\udc62 \ud835\udc58 subscript \ud835\udf0e Sig subscript \u2217 subscript \ud835\udca6 \ud835\udf0f subscript \u23df subscript \ud835\udc62 1 \u2026 subscript \ud835\udc62 1 \ud835\udf0f \u2026 subscript \ud835\udc62 \ud835\udc47 \ud835\udc58 \\displaystyle\\overline{\\boldsymbol{B}}u_{k}\\sigma_{\\text{Sig}}(\\mathcal{K}_{\\tau}\\ast(\\underbrace{u_{1},\\dots,u_{1}}_{\\tau},\\dots,u_{T}))_{k},\n\nwhere represents the convolutional kernel length, and refers to the Sigmoid function. In particular, integrating SMR into S4 ensures the efficient computation of SSM through convolutional operations. Simultaneously, it introduces enhanced flexibility to the SSM, enabling it to adapt to diverse sampling intervals and changing sample points. To validate the efficacy of SMR in mitigating NSS issues in SSMs, we conduct training and testing by incorporating SMR into the 1-layer S4 model, following the previously mentioned experimental configurations. The results are presented in Fig.4. The model\u2019s fitting results on is displayed in the left of Fig.4, demonstrating the successful mitigation of unstable numerical outputs and a substantial reduction in prediction errors. The results illustrate in the second figure of Fig.4 clearly indicate that the latent states of S4+SMR have achieved stability, characterized by a significantly reduced total volume of the absolute state values, shrinking from as shown in Fig.2 to . This implies that the integration of SMR significantly addresses the NSS issues in S4. Furthermore, experiments conducted on a 5-layer S4+SMR architecture also showed alleviation of NSS issues and improved predictive accuracy on perturbed data, the detailed results are presented in Appendix A.2. By incorporating the SMR (as its code shown in the code in List 1) into the SSMs at the positions indicated in Fig. 5, it is easily to integrate the SMR into a variety of SSMs. 3.4 Empirical Validation of SMR for SSA\n\nTo further investigate the impact of the SMR mechanism on enhancing the SSM model\u2019s SSA capability, we utilize a Pendulum dataset (Schirmer et al., 2022; Smith et al., 2023) characterized by irregularly sampled points and varying sampling intervals, to construct a regression task.",
    "smr-5": "The dataset comprises sequences of pendulum images with a length of as input. Each image, sized , is sampled at non-uniform time intervals ranging from to . Notably, the sampling points for each data instance exhibit variability. Some images in the sequence are intentionally corrupted by random noise, introducing \"occlusion\" and resulting in more irregular sampling trajectories. The prediction target is the sine and cosine values corresponding to the position of the pendulum in each image of the input sequence .",
    "smr-6": "Examples of this dataset can be found in Appendix A.3. To prevent model overfitting at each time point due to an excessive amount of constructed training data, ensuring that only models with strong generalization capabilities for changing sample points can effectively handle the task, we opt for a more challenging setup compared to the setting in (Schirmer et al., 2022) with training data and testing data points. Specifically, we allocate training data sequences and testing data sequences to make the task more challenging. In this task, we conduct comparative experiments with S4, both with and without the SMR mechanism. Additionally, to explore the generalization of our SMR mechanism to a broader range of SSM-based models, we include the more flexible SSM models, S5 and S6, in the comparison. Furthermore, we select two models that combine Attention with convolution-based SSM, Mega Ma et al. (2023) and SPADE Zuo et al. (2022), known for their competitive language modeling and long sequence modeling capabilities. We integrate SMR before Mega\u2019s EMA operation and before SPADE\u2019s S4 module to investigate the impact of SMR on various structures of SSMs. For the given model , we choose the Mean Squared Error (MSE) computed on the test set , i.e., , as the evaluation criterion. We report the best result obtained throughout training epochs in Tab.1. The integration of SMR brings about a significant improvement in S4\u2019s SSA capability. Notably, this enhancement is not exclusive to S4, even S5 shows a considerable performance boost upon incorporating SMR. To be more specific, the test MSE decreases by for S5, indicating that SMR significantly improves S5\u2019s capability to handle variations in sampling points. Additionally, S6, SPADE, and Mega all demonstrate a decrease in Test MSE after integrating SMR. This suggests that our proposed SMR not only assists convolution-based SSMs in enhancing its SSA capability but also generalizes to recurrence-based SSMs, offering widespread improvements. 4 Experiments\n\nAs stated previously, the integration of SMR further enhances SSM\u2019s SSA capability, thereby providing increased flexibility in sequence modeling capabilities. To further assess the improvement in sequential modeling capacity brought about by SMR for SSM-based models, we have chosen two more practical sequence modeling tasks: autoregressive language modeling and long-term dependency modeling. Our experimental setup follows that outlined in Section 3.4. For S4 (Gu et al., 2022), S5 (Smith et al., 2023), S6 (Gu and Dao, 2023), SPADE (Zuo et al., 2022) and Mega (Ma et al., 2023), we conducted ablation experiments with and without SMR inclusion to evaluate the generalizability benefits that the SMR mechanism confers upon SSM-based models in these sequence modeling tasks. To better illustrate the significance of the benefits brought by SMR, we introduced the comparative results on the respective tasks the Vanilla Transformer (Vaswani et al., 2017) and the state-of-the-art (on WikiText-103) Transformer-based model, Transformer-LS (LS) (Zhu et al., 2021). All experiments were conducted on four Tesla A800 GPUs. 4.1 Autoregressive language modeling\n\nTo evaluate the ability of autoregressive language modeling, we conducted experiments on the WikiText-103 dataset (Merity et al., 2017). This dataset comprises 103 million word-level tokens extracted from Wikipedia articles. In accordance with (Qin et al., 2023), all models were trained on the WikiText-103 dataset for steps, using a learning rate of . The sequence length is set to , and weight decay is set to for all models. Consistent with the configuration detailed in (Chen, 2021), all models were uniformly set up with six layers and a hidden dimension of . The performance of autoregressive language modeling is assessed by reporting perplexity (PPL) scores on both the validation and test sets. For more detailed information regarding the experiments, please refer to Appendix A.3. Tab.2 showcases consistent improvements in both validation and test perplexity (PPL) for all SSM-based models subjected to the experiments after incorporating the SMR mechanism. While S4, due to its fixed parameters and constant time-step settings, faces limitations in language tasks, integrating SMR yields a significant reduction of 2.84 and 1.86 in validation and test PPL, respectively. Notably, SMR incorporation in SPADE leads to a further 0.56 decrease in test PPL, even surpassing the performance of Transformer-LS. These findings solidify that the SMR mechanism enhances the flexibility of SSM models, ultimately contributing to advancements in the autoregressive language modeling capabilities of SSM-based architectures. 4.2 Long-range dependency modeling\n\nTo further assess the impact of SMR on long sequence modeling, we conducted experiments on five Long Range Arena (LRA) benchmark tasks: ListOps (Nangia and Bowman, 2018), Byte-level Text Classification (Maas et al., 2011), Byte-level Document Retrieval (Radev et al., 2013), Sequence CIFAR-10 (Krizhevsky and Hinton, 2009), and Pathfinder (Linsley et al., 2018).",
    "smr-7": "All models used consistent block and hidden dimension settings for each task. Detailed configurations in Appendix A.4. Results in Tab.3 demonstrate that SMR integration consistently improves the performance of various SSM-based models. Notably, SMR achieves an average performance gain of 2.38 and 1.51 on tasks S4 and S5, respectively. Furthermore, SMR contributes to performance improvements in models S6, Mega, and SPADE. These findings suggest that SMR universally enhances the long sequence modeling capabilities of SSM-based models. 4.3 The Impact of SMR on Training Speed\n\nTo propose a strategy that improves the flexibility of SSM without impacting their training efficiency, we investigated whether integrating the SMR mechanism could enhance sequence modeling capabilities while maintaining training speed. Therefore, we conducted experiments on the Wikitext-103 dataset, comparing the relative training speed ratios of various models with and without the SMR mechanism. Due to the fact that our implementation of S4 and S5 were solely based on torch without utilizing the acceleration provided by related CUDA extension, we included a version of S6 implemented purely with torch as a baseline (1.0 speed) for a more direct speed comparison between models. Experimental results, presented in Tab.4, demonstrate that SMR incorporation does not significantly decrease SSM training speed and preserves the relative speed relationships among different SSM-based models. This suggests SMR serves as an effective way to enhance the sequence modeling capabilities of SSM without compromising its training efficiency. 5 Conclusion\n\nIn this paper, we investigated the NSS issue in SSMs for long sequence modeling, we found that when input data deviates from the model\u2019s sampling requirements, it leads to error accumulation and hidden state divergence. Our analysis further revealed that early memory adjustments in the input sequence can achieve adaptive sampling, effectively solving the NSS problem. Inspired by this, we proposed a simple yet efficient plug-and-play mechanism, SMR. Theoretical analysis and experiments demonstrated that SMR effectively alleviates NSS, enhancing the generalization ability of SSMs to diverse sampling points and leading to superior sequence modeling performance. We evaluated SMR on various SSM-based models, including the convolution-based and recurrence-based SSMs, applying it to both autoregressive language modeling (on Wikitext-103) and the LRA benchmark. The results demonstrate that SMR significantly improves the performance of SSM-based models on these tasks, solidifying its effectiveness and broad applicability. 6 Limitations\n\nThis study investigates the NSS issue of SSMs for long sequence modeling from a novel theoretical perspective of ETC theory. We first conduct preliminary experimental analysis and theoretical verification to validate the existence of NSS. Inspired by the analysis, we design a simple yet effective SMR mechanism and verify its effectiveness on datasets with different sampling resolutions. Furthermore, experiments demonstrate significant improvements on convolution-based SSMs S4, Mega and SPADE, as well as recurrence-based SSMs S5 and S6 on benchmarks such as wikitext and LRA. However, the current study is preliminary. Future work will explore utilizing more advanced control theory to enhance the proposed mechanism. Additionally, we will investigate the performance of the SMR mechanism under different task types and data distributions. In conclusion, our research points out the NSS issue in SSMs and demonstrates that incorporating this factor into new long sequence model architectures is a promising direction that requires extensive exploration. We believe that these new findings can better promote the optimization and upgrading of SSM-based architectures. 7 Ethics Statement\n\nThe purpose of this paper is technical research, and the tasks, models, and datasets involved do not raise any ethical or moral concerns. References\n\nBeltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.",
    "smr-8": "arXiv preprint arXiv:2004.05150. Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
    "smr-9": "Chen (2021) Peng Chen. 2021. Permuteformer: Efficient relative position encoding for long sequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 10606\u201310618. Association for Computational Linguistics. Choromanski et al. (2020) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. In International Conference on Learning Representations. Gu and Dao (2023) Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752. Gu et al. (2021a) Albert Gu, Karan Goel, and Christopher Re. 2021a. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Gu et al. (2022) Albert Gu, Karan Goel, and Christopher R\u00e9. 2022. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Gu et al. (2021b) Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2021b. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572\u2013585. Gupta et al. (2022) Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982\u201322994. Heemels et al. (2012) W.",
    "smr-10": "P.",
    "smr-11": "M. H. Heemels, Karl Henrik Johansson, and Paulo Tabuada. 2012. An introduction to event-triggered and self-triggered control. In Proceedings of the 51th IEEE Conference on Decision and Control, CDC 2012, December 10-13, 2012, Maui, HI, USA, pages 3270\u20133285. IEEE. Krizhevsky and Hinton (2009) A. Krizhevsky and G.",
    "smr-12": "Hinton. 2009. Learning multiple layers of features from tiny images. Handbook of Systemic Autoimmune Diseases, 1(4).",
    "smr-13": "Linsley et al. (2018) Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. 2018. Learning long-range spatial dependencies with horizontal gated recurrent units. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 152\u2013164. Ma et al. (2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2023. Mega: Moving average equipped gated attention. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.",
    "smr-14": "OpenReview.net. Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 142\u2013150. The Association for Computer Linguistics. Merity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Nangia and Bowman (2018) Nikita Nangia and Samuel R. Bowman. 2018. Listops: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 2-4, 2018, Student Research Workshop, pages 92\u201399. Association for Computational Linguistics. Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F.",
    "smr-15": "Christiano, Jan Leike, and Ryan Lowe.",
    "smr-16": "2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Qin et al. (2023) Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. 2023. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Radev et al. (2013) Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The ACL anthology network corpus.",
    "smr-17": "Lang. Resour. Evaluation, 47(4):919\u2013944. Schirmer et al. (2022) Mona Schirmer, Mazin Eltayeb, Stefan Lessmann, and Maja Rudolph. 2022. Modeling irregular time series with continuous recurrent units. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 19388\u201319405.",
    "smr-18": "PMLR. Smith et al. (2023) Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. 2023. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Tabuada (2007) Paulo Tabuada. 2007. Event-triggered real-time scheduling of stabilizing control tasks. IEEE Trans. Autom. Control., 52(9):1680\u20131685. Tay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Vallarella and Haimovich (2019) Alexis J. Vallarella and Hernan Haimovich. 2019. State measurement error-to-state stability results based on approximate discrete-time models.",
    "smr-19": "IEEE Trans. Autom. Control., 64(8):3308\u20133315. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.",
    "smr-20": "arXiv preprint arXiv:2006.04768. Zhu et al. (2021) Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. 2021. Long-short transformer: Efficient transformers for language and vision. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 17723\u201317736. Zuo et al. (2022) Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. 2022. Efficient long sequence modeling via state space augmented transformer. CoRR, abs/2212.08136. Appendix A Appendix\n\nA.1 Proofs\n\nProof of Proposition 1\n\nDenote the sampled , where is the sampling error caused by variation in the sampling points. Consider the propagation of the error in the output values :\n\n[ y 1 \u2032 y 2 \u2032 \u22ee y L \u2032 ] = [ \ud835\udc6a \u200b \ud835\udc69 \u00af \ud835\udfce \u22ef \ud835\udfce \ud835\udc6a \u200b \ud835\udc68 \u200b \ud835\udc69 \u00af \ud835\udc6a \u200b \ud835\udc69 \u00af \u22ef \ud835\udfce \u22ee \u22ee \u22f1 \u22ee \ud835\udc6a \u200b \ud835\udc68 \u00af T \u2212 1 \u200b \ud835\udc69 \u00af \ud835\udc6a \u200b \ud835\udc68 \u00af T \u2212 2 \u200b \ud835\udc69 \u00af \u22ef \ud835\udc6a \u200b \ud835\udc69 \u00af ] \u200b [ u 1 + \u03b5 1 u 2 + \u03b5 2 \u22ee u T + \u03b5 t ] , matrix subscript superscript \ud835\udc66 \u2032 1 subscript superscript \ud835\udc66 \u2032 2 \u22ee subscript superscript \ud835\udc66 \u2032 \ud835\udc3f matrix \u00af \ud835\udc6a \ud835\udc69 0 \u22ef 0 \u00af \ud835\udc6a \ud835\udc68 \ud835\udc69 \u00af \ud835\udc6a \ud835\udc69 \u22ef 0 \u22ee \u22ee \u22f1 \u22ee superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc47 1 \u00af \ud835\udc69 superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc47 2 \u00af \ud835\udc69 \u22ef \u00af \ud835\udc6a \ud835\udc69 matrix subscript \ud835\udc62 1 subscript \ud835\udf00 1 subscript \ud835\udc62 2 subscript \ud835\udf00 2 \u22ee subscript \ud835\udc62 \ud835\udc47 subscript \ud835\udf00 \ud835\udc61 \\begin{bmatrix}y^{\\prime}_{1}\\\\\ny^{\\prime}_{2}\\\\\n\\vdots\\\\\ny^{\\prime}_{L}\\end{bmatrix}=\\begin{bmatrix}\\overline{\\boldsymbol{CB}}&\\mathbf{0}&\\cdots&\\mathbf{0}\\\\\n\\overline{\\boldsymbol{CAB}}&\\overline{\\boldsymbol{CB}}&\\cdots&\\mathbf{0}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\overline{\\boldsymbol{CA}}^{T-1}\\overline{\\boldsymbol{B}}&\\overline{\\boldsymbol{CA}}^{T-2}\\overline{\\boldsymbol{B}}&\\cdots&\\overline{\\boldsymbol{CB}}\\end{bmatrix}\\begin{bmatrix}u_{1}+\\varepsilon_{1}\\\\\nu_{2}+\\varepsilon_{2}\\\\\n\\vdots\\\\\nu_{T}+\\varepsilon_{t}\\end{bmatrix}, (16)\n\nthen\n\n\u2016 y t \u2032 \u2212 y t \u2016 norm subscript superscript \ud835\udc66 \u2032 \ud835\udc61 subscript \ud835\udc66 \ud835\udc61 \\displaystyle\\|y^{\\prime}_{t}-y_{t}\\| = \u2016 \ud835\udc6a \u200b \ud835\udc68 \u00af t \u2212 1 \u200b \ud835\udc69 \u00af \u200b \u03b5 1 + \ud835\udc6a \u200b \ud835\udc68 \u00af t \u2212 2 \u200b \ud835\udc69 \u00af \u200b \u03b5 2 + \u22ef + \ud835\udc6a \u200b \ud835\udc69 \u00af \u200b \u03b5 t \u2016 absent norm superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc61 1 \u00af \ud835\udc69 subscript \ud835\udf00 1 superscript \u00af \ud835\udc6a \ud835\udc68 \ud835\udc61 2 \u00af \ud835\udc69 subscript \ud835\udf00 2 \u22ef \u00af \ud835\udc6a \ud835\udc69 subscript \ud835\udf00 \ud835\udc61 \\displaystyle=\\left\\|\\overline{\\boldsymbol{CA}}^{t-1}\\overline{\\boldsymbol{B}}\\varepsilon_{1}+\\overline{\\boldsymbol{CA}}^{t-2}\\overline{\\boldsymbol{B}}\\varepsilon_{2}+\\dots+\\overline{\\boldsymbol{CB}}\\varepsilon_{t}\\right\\| (17) \u2264 \u2016 \ud835\udc6a \u00af \u2016 \u200b \u2016 \ud835\udc68 \u00af t \u2212 1 \u2016 \u200b \u2016 \ud835\udc69 \u00af \u2016 \u200b | \u03b5 1 | + \u2016 \ud835\udc6a \u00af \u2016 \u200b \u2016 \ud835\udc68 \u00af t \u2212 2 \u2016 \u200b \u2016 \ud835\udc69 \u00af \u2016 \u200b | \u03b5 2 | + \u22ef + \u2016 \ud835\udc6a \u00af \u2016 \u200b \u2016 \ud835\udc69 \u00af \u2016 \u200b | \u03b5 t | absent norm \u00af \ud835\udc6a norm superscript \u00af \ud835\udc68 \ud835\udc61 1 norm \u00af \ud835\udc69 subscript \ud835\udf00 1 norm \u00af \ud835\udc6a norm superscript \u00af \ud835\udc68 \ud835\udc61 2 norm \u00af \ud835\udc69 subscript \ud835\udf00 2 \u22ef norm \u00af \ud835\udc6a norm \u00af \ud835\udc69 subscript \ud835\udf00 \ud835\udc61 \\displaystyle\\leq\\left\\|\\overline{\\boldsymbol{C}}\\right\\|\\left\\|\\overline{\\boldsymbol{A}}^{t-1}\\right\\|\\left\\|\\overline{\\boldsymbol{B}}\\right\\|\\left|\\varepsilon_{1}\\right|+\\left\\|\\overline{\\boldsymbol{C}}\\right\\|\\left\\|\\overline{\\boldsymbol{A}}^{t-2}\\right\\|\\left\\|\\overline{\\boldsymbol{B}}\\right\\|\\left|\\varepsilon_{2}\\right|+\\dots+\\left\\|\\overline{\\boldsymbol{C}}\\right\\|\\left\\|\\overline{\\boldsymbol{B}}\\right\\|\\left|\\varepsilon_{t}\\right| \u2264 | \u03bb max | t \u2212 1 \u200b c \u200b b \u200b \u03b5 1 + | \u03bb max | t \u2212 2 \u200b c \u200b b \u200b \u03b5 2 + \u22ef + c \u200b b \u200b \u03b5 t . absent superscript subscript \ud835\udf06 \ud835\udc61 1 \ud835\udc50 \ud835\udc4f subscript \ud835\udf00 1 superscript subscript \ud835\udf06 \ud835\udc61 2 \ud835\udc50 \ud835\udc4f subscript \ud835\udf00 2 \u22ef \ud835\udc50 \ud835\udc4f subscript \ud835\udf00 \ud835\udc61 \\displaystyle\\leq\\left|\\lambda_{\\max}\\right|^{t-1}cb\\varepsilon_{1}+\\left|\\lambda_{\\max}\\right|^{t-2}cb\\varepsilon_{2}+\\dots+cb\\varepsilon_{t}. Note that if , becomes unbounded. If , then we have\n\n\u2016 \ud835\udc99 t \u2016 norm subscript \ud835\udc99 \ud835\udc61 \\displaystyle\\|\\boldsymbol{x}_{t}\\| = \u2016 \ud835\udc68 \u00af L \u2212 1 \u200b \ud835\udc69 \u00af \u200b u 1 + \ud835\udc68 \u00af L \u2212 2 \u200b \ud835\udc69 \u00af \u200b u 2 + \u22ef + \ud835\udc69 \u00af \u200b u t \u2016 absent norm superscript \u00af \ud835\udc68 \ud835\udc3f 1 \u00af \ud835\udc69 subscript \ud835\udc62 1 superscript \u00af \ud835\udc68 \ud835\udc3f 2 \u00af \ud835\udc69 subscript \ud835\udc62 2 \u22ef \u00af \ud835\udc69 subscript \ud835\udc62 \ud835\udc61 \\displaystyle=\\left\\|\\overline{\\boldsymbol{A}}^{L-1}\\overline{\\boldsymbol{B}}u_{1}+\\overline{\\boldsymbol{A}}^{L-2}\\overline{\\boldsymbol{B}}u_{2}+\\dots+\\overline{\\boldsymbol{B}}u_{t}\\right\\| (18) \u2264 \u2016 \ud835\udc68 \u00af L \u2212 1 \u2016 \u200b \u2016 \ud835\udc69 \u00af \u2016 \u200b | u 1 | + \u2016 \ud835\udc68 \u00af L \u2212 2 \u2016 \u200b \u2016 \ud835\udc69 \u00af \u2016 \u200b | u 2 | + \u22ef + \u2016 \ud835\udc69 \u00af \u2016 \u200b | u t | absent norm superscript \u00af \ud835\udc68 \ud835\udc3f 1 norm \u00af \ud835\udc69 subscript \ud835\udc62 1 norm superscript \u00af \ud835\udc68 \ud835\udc3f 2 norm \u00af \ud835\udc69 subscript \ud835\udc62 2 \u22ef norm \u00af \ud835\udc69 subscript \ud835\udc62 \ud835\udc61 \\displaystyle\\leq\\left\\|\\overline{\\boldsymbol{A}}^{L-1}\\right\\|\\left\\|\\overline{\\boldsymbol{B}}\\right\\|\\left|u_{1}\\right|+\\left\\|\\overline{\\boldsymbol{A}}^{L-2}\\right\\|\\left\\|\\overline{\\boldsymbol{B}}\\right\\|\\left|u_{2}\\right|+\\dots+\\left\\|\\overline{\\boldsymbol{B}}\\right\\|\\left|u_{t}\\right| \u2264 | \u03bb max | L \u2212 1 \u200b b \u200b \u03b6 + | \u03bb max | L \u2212 2 \u200b b \u200b \u03b6 + \u22ef + b \u200b \u03b6 , absent superscript subscript \ud835\udf06 \ud835\udc3f 1 \ud835\udc4f \ud835\udf01 superscript subscript \ud835\udf06 \ud835\udc3f 2 \ud835\udc4f \ud835\udf01 \u22ef \ud835\udc4f \ud835\udf01 \\displaystyle\\leq\\left|\\lambda_{\\max}\\right|^{L-1}b\\zeta+\\left|\\lambda_{\\max}\\right|^{L-2}b\\zeta+\\dots+b\\zeta,\n\nthus\n\nlim t \u2192 \u221e \u2016 \ud835\udc99 t \u2016 \u2264 lim t \u2192 \u221e ( | \u03bb max | L \u2212 1 \u200b b \u200b \u03b6 + | \u03bb max | L \u2212 2 \u200b b \u200b \u03b6 + \u22ef + b \u200b \u03b6 ) = b \u200b \u03b6 1 \u2212 | \u03bb max | < lim t \u2192 \u221e \u2016 \ud835\udc99 t \u2016 , subscript \u2192 \ud835\udc61 norm subscript \ud835\udc99 \ud835\udc61 subscript \u2192 \ud835\udc61 superscript subscript \ud835\udf06 \ud835\udc3f 1 \ud835\udc4f \ud835\udf01 superscript subscript \ud835\udf06 \ud835\udc3f 2 \ud835\udc4f \ud835\udf01 \u22ef \ud835\udc4f \ud835\udf01 \ud835\udc4f \ud835\udf01 1 subscript \ud835\udf06 subscript \u2192 \ud835\udc61 norm subscript \ud835\udc99 \ud835\udc61 \\lim_{t\\rightarrow\\infty}\\|\\boldsymbol{x}_{t}\\|\\leq\\lim_{t\\rightarrow\\infty}\\left(\\left|\\lambda_{\\max}\\right|^{L-1}b\\zeta+\\left|\\lambda_{\\max}\\right|^{L-2}b\\zeta+\\dots+b\\zeta\\right)=\\frac{b\\zeta}{1-\\left|\\lambda_{\\max}\\right|}<\\lim_{t\\rightarrow\\infty}\\|\\boldsymbol{x}_{t}\\|, (19)\n\nwhich contradicts the assumption, therefore there must be , which also implies that is unbounded. Remark Note that imposing the constraint on the state space model will cause the initial input to tend to zero as it propagates (). This causes all previous states to rapidly decay to during the propagation, thus severely limits the long-term memory capacity of the model. Proof of Theorem1\n\nTaking into account the error propagation in latent states of the SSM model, the grid deviation error emerges from signal misalignment and can be considered as an additional disturbance term. Assuming that the actual sampled value, denoted as , satisfies the relationship , where represents the error term, we can have\n\n[ \ud835\udc99 1 \ud835\udc99 2 \u22ee \ud835\udc99 T ] = [ \ud835\udc69 \u00af \ud835\udfce \u22ef \ud835\udfce \ud835\udc68 \u200b \ud835\udc69 \u00af \ud835\udc69 \u00af \u22ef \ud835\udfce \u22ee \u22ee \u22f1 \u22ee \ud835\udc68 \u00af T \u2212 1 \u200b \ud835\udc69 \u00af \ud835\udc68 \u00af T \u2212 2 \u200b \ud835\udc69 \u00af \u22ef \ud835\udc69 \u00af ] \u200b [ u 1 + \u03b5 1 u 2 + \u03b5 2 \u22ee u T + \u03b5 t ] , matrix subscript \ud835\udc99 1 subscript \ud835\udc99 2 \u22ee subscript \ud835\udc99 \ud835\udc47 matrix \u00af \ud835\udc69 0 \u22ef 0 \u00af \ud835\udc68 \ud835\udc69 \u00af \ud835\udc69 \u22ef 0 \u22ee \u22ee \u22f1 \u22ee superscript \u00af \ud835\udc68 \ud835\udc47 1 \u00af \ud835\udc69 superscript \u00af \ud835\udc68 \ud835\udc47 2 \u00af \ud835\udc69 \u22ef \u00af \ud835\udc69 matrix subscript \ud835\udc62 1 subscript \ud835\udf00 1 subscript \ud835\udc62 2 subscript \ud835\udf00 2 \u22ee subscript \ud835\udc62 \ud835\udc47 subscript \ud835\udf00 \ud835\udc61 \\begin{bmatrix}\\boldsymbol{x}_{1}\\\\\n\\boldsymbol{x}_{2}\\\\\n\\vdots\\\\\n\\boldsymbol{x}_{T}\\end{bmatrix}=\\begin{bmatrix}\\overline{\\boldsymbol{B}}&\\mathbf{0}&\\cdots&\\mathbf{0}\\\\\n\\overline{\\boldsymbol{AB}}&\\overline{\\boldsymbol{B}}&\\cdots&\\mathbf{0}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\overline{\\boldsymbol{A}}^{T-1}\\overline{\\boldsymbol{B}}&\\overline{\\boldsymbol{A}}^{T-2}\\overline{\\boldsymbol{B}}&\\cdots&\\overline{\\boldsymbol{B}}\\end{bmatrix}\\begin{bmatrix}u_{1}+\\varepsilon_{1}\\\\\nu_{2}+\\varepsilon_{2}\\\\\n\\vdots\\\\\nu_{T}+\\varepsilon_{t}\\end{bmatrix}, (20)\n\nobserve that\n\n\ud835\udc99 t subscript \ud835\udc99 \ud835\udc61 \\displaystyle\\boldsymbol{x}_{t} = \ud835\udc68 \u00af t \u2212 1 \u200b \ud835\udc69 \u00af \u200b ( u 1 + \u03b5 1 ) + \ud835\udc68 \u00af t \u2212 2 \u200b \ud835\udc69 \u00af \u200b ( u 2 + \u03b5 2 ) + \u22ef + \ud835\udc69 \u00af \u200b ( u t + \u03b5 t ) absent superscript \u00af \ud835\udc68 \ud835\udc61 1 \u00af \ud835\udc69 subscript \ud835\udc62 1 subscript \ud835\udf00 1 superscript \u00af \ud835\udc68 \ud835\udc61 2 \u00af \ud835\udc69 subscript \ud835\udc62 2 subscript \ud835\udf00 2 \u22ef \u00af \ud835\udc69 subscript \ud835\udc62 \ud835\udc61 subscript \ud835\udf00 \ud835\udc61 \\displaystyle=\\overline{\\boldsymbol{A}}^{t-1}\\overline{\\boldsymbol{B}}(u_{1}+\\varepsilon_{1})+\\overline{\\boldsymbol{A}}^{t-2}\\overline{\\boldsymbol{B}}(u_{2}+\\varepsilon_{2})+\\dots+\\overline{\\boldsymbol{B}}(u_{t}+\\varepsilon_{t}) (21) = \ud835\udc68 \u00af t \u2212 1 \u200b \ud835\udc69 \u00af \u200b u 1 + \ud835\udc68 \u00af t \u2212 2 \u200b \ud835\udc69 \u00af \u200b u 2 + \u22ef + \ud835\udc69 \u00af \u200b u t + L \u200b ( \u03b5 1 , \u03b5 2 , \u2026 , \u03b5 t ) , absent superscript \u00af \ud835\udc68 \ud835\udc61 1 \u00af \ud835\udc69 subscript \ud835\udc62 1 superscript \u00af \ud835\udc68 \ud835\udc61 2 \u00af \ud835\udc69 subscript \ud835\udc62 2 \u22ef \u00af \ud835\udc69 subscript \ud835\udc62 \ud835\udc61 \ud835\udc3f subscript \ud835\udf00 1 subscript \ud835\udf00 2 \u2026 subscript \ud835\udf00 \ud835\udc61 \\displaystyle=\\overline{\\boldsymbol{A}}^{t-1}\\overline{\\boldsymbol{B}}u_{1}+\\overline{\\boldsymbol{A}}^{t-2}\\overline{\\boldsymbol{B}}u_{2}+\\dots+\\overline{\\boldsymbol{B}}u_{t}+L(\\varepsilon_{1},\\varepsilon_{2},\\dots,\\varepsilon_{t}),\n\nwhere . Consider its continuous form and drawing upon the controller concept in ETC theory, we consider the following state propagation:\n\n\ud835\udc99 \u02d9 \u200b ( t ) = \ud835\udc68 \u200b ( \ud835\udc99 \u200b ( t ) + \u222b 0 t \ud835\udc8c \u200b ( t \u2212 l ) \u200b \u03b5 \u200b ( l ) \u200b \ud835\udc51 l ) + \ud835\udc69 \u200b u \u200b ( t ) , \u02d9 \ud835\udc99 \ud835\udc61 \ud835\udc68 \ud835\udc99 \ud835\udc61 superscript subscript 0 \ud835\udc61 \ud835\udc8c \ud835\udc61 \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\dot{\\boldsymbol{x}}(t)=\\boldsymbol{A}\\left(\\boldsymbol{x}(t)+\\int_{0}^{t}\\boldsymbol{k}(t-l)\\varepsilon(l)dl\\right)+\\boldsymbol{B}u(t), (22)\n\nwhere is a coefficient matrix that varies over time, and has the same shape as . Owing to the accumulation of errors in the time domain, we introduce a modifiable factor denoted as with backtracking capability to regulate the input. Specifically, the controlled input is defined as . then we have\n\n\ud835\udc99 \u02d9 \u200b ( t ) = \ud835\udc68 \u200b ( x \u200b ( t ) + \u222b 0 t \ud835\udc8c \u200b ( t \u2212 l ) \u200b h \u200b ( [ l \u2212 \u03c4 , l ] ) \u200b \u03b5 \u200b ( l ) \u200b \ud835\udc51 l ) + \ud835\udc69 \u200b h \u200b ( [ t \u2212 \u03c4 , t ] ) \u200b u \u200b ( t ) , \u02d9 \ud835\udc99 \ud835\udc61 \ud835\udc68 \ud835\udc65 \ud835\udc61 superscript subscript 0 \ud835\udc61 \ud835\udc8c \ud835\udc61 \ud835\udc59 \u210e \ud835\udc59 \ud835\udf0f \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 \ud835\udc69 \u210e \ud835\udc61 \ud835\udf0f \ud835\udc61 \ud835\udc62 \ud835\udc61 \\dot{\\boldsymbol{x}}(t)=\\boldsymbol{A}\\left(x(t)+\\int_{0}^{t}\\boldsymbol{k}(t-l)h([l-\\tau,l])\\varepsilon(l)dl\\right)+\\boldsymbol{B}h([t-\\tau,t])u(t), (23)\n\nthen has the ability to adjust the errors with coefficients carrying temporal phases. Taking into account the following observer used for sampling:\n\n\ud835\udc9b \u02d9 \u200b ( t ) = \ud835\udc68 \u200b \ud835\udc9b \u200b ( t ) + \ud835\udc69 \u200b h \u200b ( [ t \u2212 \u03c4 , t ] ) \u200b ( u \u200b ( t ) + \u03b5 \u200b ( t ) ) , \u02d9 \ud835\udc9b \ud835\udc61 \ud835\udc68 \ud835\udc9b \ud835\udc61 \ud835\udc69 \u210e \ud835\udc61 \ud835\udf0f \ud835\udc61 \ud835\udc62 \ud835\udc61 \ud835\udf00 \ud835\udc61 \\dot{\\boldsymbol{z}}(t)=\\boldsymbol{A}\\boldsymbol{z}(t)+\\boldsymbol{B}h([t-\\tau,t])(u(t)+\\varepsilon(t)), (24)\n\ndenote , we have\n\n\ud835\udc86 \u02d9 \u200b ( t ) = \ud835\udc68 \u200b \ud835\udc86 \u200b ( t ) + \ud835\udc68 \u200b \u222b 0 t \ud835\udc8c \u200b ( t \u2212 l ) \u200b h \u200b ( [ l \u2212 \u03c4 , l ] ) \u200b \u03b5 \u200b ( l ) \u200b \ud835\udc51 l \u2212 \ud835\udc69 \u200b h \u200b ( [ t \u2212 \u03c4 , t ] ) \u200b \u03b5 \u200b ( t ) . \u02d9 \ud835\udc86 \ud835\udc61 \ud835\udc68 \ud835\udc86 \ud835\udc61 \ud835\udc68 superscript subscript 0 \ud835\udc61 \ud835\udc8c \ud835\udc61 \ud835\udc59 \u210e \ud835\udc59 \ud835\udf0f \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 \ud835\udc69 \u210e \ud835\udc61 \ud835\udf0f \ud835\udc61 \ud835\udf00 \ud835\udc61 \\dot{\\boldsymbol{e}}(t)=\\boldsymbol{A}\\boldsymbol{e}(t)+\\boldsymbol{A}\\int_{0}^{t}\\boldsymbol{k}(t-l)h([l-\\tau,l])\\varepsilon(l)dl-\\boldsymbol{B}h([t-\\tau,t])\\varepsilon(t). (25)\n\nConsider the Lyapunov function , where is a positive definite symmetric matrix, we can obtain\n\nd \u200b \u2112 \ud835\udc86 \u200b ( t ) d \u200b t \ud835\udc51 subscript \u2112 \ud835\udc86 \ud835\udc61 \ud835\udc51 \ud835\udc61 \\displaystyle\\frac{d\\mathcal{L}_{\\boldsymbol{e}}(t)}{dt} = 2 \u200b \ud835\udc86 \u22a4 \u200b ( t ) \u200b \ud835\udc77 \u200b \ud835\udc86 \u02d9 \u200b ( t ) absent 2 superscript \ud835\udc86 top \ud835\udc61 \ud835\udc77 \u02d9 \ud835\udc86 \ud835\udc61 \\displaystyle=2\\boldsymbol{e}^{\\top}(t)\\boldsymbol{P}\\dot{\\boldsymbol{e}}(t) (26) = 2 \u200b \ud835\udc86 \u22a4 \u200b ( t ) \u200b \ud835\udc77 \u200b ( \ud835\udc68 \u200b \ud835\udc86 \u200b ( t ) + \ud835\udc68 \u200b \u222b 0 t \ud835\udc8c \u200b ( t \u2212 l ) \u200b h \u200b ( [ l \u2212 \u03c4 , l ] ) \u200b \u03b5 \u200b ( l ) \u200b \ud835\udc51 l \u2212 \ud835\udc69 \u200b h \u200b ( [ t \u2212 \u03c4 , t ] ) \u200b \u03b5 \u200b ( t ) ) absent 2 superscript \ud835\udc86 top \ud835\udc61 \ud835\udc77 \ud835\udc68 \ud835\udc86 \ud835\udc61 \ud835\udc68 superscript subscript 0 \ud835\udc61 \ud835\udc8c \ud835\udc61 \ud835\udc59 \u210e \ud835\udc59 \ud835\udf0f \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 \ud835\udc69 \u210e \ud835\udc61 \ud835\udf0f \ud835\udc61 \ud835\udf00 \ud835\udc61 \\displaystyle=2\\boldsymbol{e}^{\\top}(t)\\boldsymbol{P}\\left(\\boldsymbol{A}\\boldsymbol{e}(t)+\\boldsymbol{A}\\int_{0}^{t}\\boldsymbol{k}(t-l)h([l-\\tau,l])\\varepsilon(l)dl-\\boldsymbol{B}h([t-\\tau,t])\\varepsilon(t)\\right) = \ud835\udc86 \u22a4 \u200b ( t ) \u200b ( \ud835\udc77 \u200b \ud835\udc68 + \ud835\udc68 \u22a4 \u200b \ud835\udc77 ) \u200b \ud835\udc86 \u200b ( t ) + \u039b \u200b ( t ) , absent superscript \ud835\udc86 top \ud835\udc61 \ud835\udc77 \ud835\udc68 superscript \ud835\udc68 top \ud835\udc77 \ud835\udc86 \ud835\udc61 \u039b \ud835\udc61 \\displaystyle=\\boldsymbol{e}^{\\top}(t)\\left(\\boldsymbol{PA}+\\boldsymbol{A^{\\top}P}\\right)\\boldsymbol{e}(t)+\\Lambda(t),\n\nwhere\n\n\u039b \u200b ( t ) \u039b \ud835\udc61 \\displaystyle\\Lambda(t) = 2 \u200b \ud835\udc86 \u22a4 \u200b ( t ) \u200b \ud835\udc68 \u200b \u222b 0 t \ud835\udc8c \u200b ( t \u2212 l ) \u200b h \u200b ( [ l \u2212 \u03c4 , l ] ) \u200b \u03b5 \u200b ( l ) \u200b \ud835\udc51 l \u2212 \ud835\udc86 \u22a4 \u200b ( t ) \u200b \ud835\udc69 \u200b h \u200b ( [ t \u2212 \u03c4 , t ] ) \u200b \u03b5 \u200b ( t ) absent 2 superscript \ud835\udc86 top \ud835\udc61 \ud835\udc68 superscript subscript 0 \ud835\udc61 \ud835\udc8c \ud835\udc61 \ud835\udc59 \u210e \ud835\udc59 \ud835\udf0f \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 superscript \ud835\udc86 top \ud835\udc61 \ud835\udc69 \u210e \ud835\udc61 \ud835\udf0f \ud835\udc61 \ud835\udf00 \ud835\udc61 \\displaystyle=2\\boldsymbol{e}^{\\top}(t)\\boldsymbol{A}\\int_{0}^{t}\\boldsymbol{k}(t-l)h([l-\\tau,l])\\varepsilon(l)dl-\\boldsymbol{e}^{\\top}(t)\\boldsymbol{B}h([t-\\tau,t])\\varepsilon(t) (27) = 2 \u200b \u2016 \ud835\udc86 \u200b ( t ) \u2016 \u200b \u2016 \ud835\udc68 \u2016 \u200b \u222b 0 t \u2016 \ud835\udc8c \u200b ( t \u2212 l ) \u2016 \u200b | h \u200b ( [ l \u2212 \u03c4 , l ] ) | \u200b | \u03b5 \u200b ( l ) | \u200b \ud835\udc51 l + \u2016 \ud835\udc86 \u200b ( t ) \u2016 \u200b \u2016 \ud835\udc69 \u2016 \u200b | h \u200b ( [ t \u2212 \u03c4 , t ] ) | \u200b | \u03b5 \u200b ( t ) | absent 2 norm \ud835\udc86 \ud835\udc61 norm \ud835\udc68 superscript subscript 0 \ud835\udc61 norm \ud835\udc8c \ud835\udc61 \ud835\udc59 \u210e \ud835\udc59 \ud835\udf0f \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 norm \ud835\udc86 \ud835\udc61 norm \ud835\udc69 \u210e \ud835\udc61 \ud835\udf0f \ud835\udc61 \ud835\udf00 \ud835\udc61 \\displaystyle=2\\left\\|\\boldsymbol{e}(t)\\right\\|\\left\\|\\boldsymbol{A}\\right\\|\\int_{0}^{t}\\left\\|\\boldsymbol{k}(t-l)\\right\\|\\left|h([l-\\tau,l])\\right|\\left|\\varepsilon(l)\\right|dl+\\left\\|\\boldsymbol{e}(t)\\right\\|\\left\\|\\boldsymbol{B}\\right\\|\\left|h([t-\\tau,t])\\right|\\left|\\varepsilon(t)\\right| \u2264 2 \u200b \u2016 h \u03c4 \u2016 \u200b \u2016 \ud835\udc86 \u200b ( t ) \u2016 \u200b ( \u222b 0 t \u2016 \ud835\udc8c \u200b ( t \u2212 l ) \u2016 \u200b | \u03b5 \u200b ( l ) | \u200b \ud835\udc51 l + \u2016 \ud835\udc69 \u2016 \u200b | \u03b5 \u200b ( t ) | ) . absent 2 norm subscript \u210e \ud835\udf0f norm \ud835\udc86 \ud835\udc61 superscript subscript 0 \ud835\udc61 norm \ud835\udc8c \ud835\udc61 \ud835\udc59 \ud835\udf00 \ud835\udc59 differential-d \ud835\udc59 norm \ud835\udc69 \ud835\udf00 \ud835\udc61 \\displaystyle\\leq 2\\left\\|h_{\\tau}\\right\\|\\left\\|\\boldsymbol{e}(t)\\right\\|\\left(\\int_{0}^{t}\\left\\|\\boldsymbol{k}(t-l)\\right\\|\\left|\\varepsilon(l)\\right|dl+\\left\\|\\boldsymbol{B}\\right\\|\\left|\\varepsilon(t)\\right|\\right). Hence, selecting a value of strengthens the stability of the system, while corresponds to the case without a controller. Additionally, choosing a larger value can further enhance the control performance. A.2 NSS in 5-layers S4\n\nDue to space constraints, we present the analysis of the deep S4 model here. Specifically, we conducted an experiment on a 5-layer S4 model, extending from the experiment described in Section 2.3. We plotted the results of the hidden states in the first layer and observed the presence of the NSS issue in the 5-layer S4 model, as depicted in Fig.6. Notably, the S4 model without SMR exhibited a significant NSS phenomenon. In contrast, the S4 model incorporated with SMR demonstrated highly stable hidden states, as illustrated in Fig.6. The sum of absolute values of the states at each time step decreased from to , and the output error under perturbation was also reduced (Fig.6). A.3 Example of Pendulum Dataset\n\nWe present the input examples of the pendulum dataset used in Section 3.4 in Fig.7. The sampling intervals are not constant but variable, and the introduction of random noise in the image sequence makes the actual sampling intervals even more random. All models are uniformly adjusted to blocks with a hidden dimension of , and optimized using the AdamW optimizer with a learning rate of . A.4 Experiment Details\n\nHere, we provide specific configurations for the experiments mentioned in Section 4. The experimental settings for autoregressive language modeling are detailed in Tab.5, while the parameter configurations for various tasks on the LRA are presented in Tab.6. \u25c4 Feeling lucky?",
    "smr-21": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 13:25:50 2024 by LaTeXML"
}