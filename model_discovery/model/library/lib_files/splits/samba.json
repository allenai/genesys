{
    "samba-0": "# Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling \n\nLiliang Ren ${ }^{1,2 *}$ Yang Liu ${ }^{1 \\dagger} \\quad$ Yadong Lu ${ }^{1 \\dagger} \\quad$ Yelong Shen ${ }^{1}$<br>Chen Liang ${ }^{1} \\quad$ Weizhu Chen ${ }^{1}$<br>${ }^{1}$ Microsoft ${ }^{2}$ University of Illinois at Urbana-Champaign<br>\\{liliangren, yaliu10, yadonglu, yelong.",
    "samba-1": "shen\\}@microsoft.com<br>\\{chenliang1,wzchen\\}@microsoft.com\n\n\n#### Abstract\n\nEfficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present SAMBA, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). SAMBA selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2 T training tokens and show that SAMBA substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4 K length sequences, SAmBA can be efficiently extrapolated to 256 K context length with perfect memory recall and show improved token predictions up to 1 M context length. As a linear-time sequence model, SAmbA enjoys a $3.73 \\times$ higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128 K length, and $3.64 \\times$ speedup when generating 64 K tokens with unlimited streaming. A sample implementation of SAMBA is publicly available in https://github.com/microsoft/Samba. ## 1 Introduction\n\nAttention-based models [ $\\mathrm{VSP}^{+}$17, BCB14] have dominated the neural architectures of Large Language Models (LLMs) $\\left[\\mathrm{RWC}^{+} 19, \\mathrm{BMR}^{+} 20\\right.$, Ope23, $\\left.\\mathrm{BCE}^{+} 23\\right]$ due to their ability to capture complex long-term dependencies and the efficient parallelization for large-scale training [ $\\mathrm{DFE}^{+} 22$ ]. Recently, State Space Models (SSMs) [GGR21, SWL23, GGGR22, GD23] have emerged as a promising alternative, offering linear computation complexity and the potential for better extrapolation to longer sequences than seen during training. Specifically, Mamba[GD23], a variant of SSMs equipped with selective state spaces, has demonstrated notable promise through strong empirical performance and efficient hardware-aware implementation. Recent work also shows that transformers have poorer modeling capacities than input-dependent SSMs in state tracking problems [MPS24]. However, SSMs struggle with memory recall due to their Markovian nature [AET ${ }^{+}$23], and experimental results on information retrieval-related tasks [FDS ${ }^{+}$23, $\\mathrm{WDL}^{24}, \\mathrm{AEZ}^{+}$24], have further shown that SSMs are not as competitive as their attention-based counterparts. Previous works $\\left[\\mathrm{ZLJ}^{+} 22, \\mathrm{FDS}^{+} 23, \\mathrm{MZK}^{+} 23, \\mathrm{RLW}^{+} 23\\right]$ have explored different approaches to hybridize SSMs and the attention mechanism, but none of them achieve unlimited-length extrapolation\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-02.jpg?height=602&width=1332&top_left_y=271&top_left_x=395)\n\nFigure 1: Samba shows improved prediction up to 1 M tokens in the Proof-Pile test set while achieving a $3.64 \\times$ faster decoding throughput than the Llama-3 architecture [Met24] (a state-of-theart Transformer [ $\\mathrm{VSP}^{+}$17] with Grouped-Query Attention [ALTdJ $\\left.{ }^{+} 23\\right]$ ) on 64 K generation length. We also include an SE-Llama-3 1.6B baseline which applies the SelfExtend [JHY ${ }^{+}$24] approach for zero-shot length extrapolation. Throughput measured on a single A100 80GB GPU. All models are trained on the Phi-2 $\\left[\\mathrm{LBE}^{+} 23\\right]$ dataset with 4 K sequence length. with linear-time complexity. The existing length generalization techniques $\\left[\\mathrm{HWX}^{+} 23, \\mathrm{XTC}^{+} 23\\right.$, $\\mathrm{JHY}^{+}$24] developed for the attention mechanism suffer from quadratic computation complexity or limited context extrapolation ability. In this paper, we introduce SAMBA, a simple neural architecture that harmonizes the strengths of both the SSM and the attention-based models, while achieving an unlimited sequence length extrapolation with linear time complexity. SAMBA combines SSMs with attention through layer-wise interleaving Mamba [GD23], SwiGLU [Sha20], and Sliding Window Attention (SWA) [BPC20]. Mamba layers capture the time-dependent semantics and provide a backbone for efficient decoding, while SWA fills in the gap modeling complex, non-Markovian dependencies. We scale SAMBA with $421 \\mathrm{M}, 1.3 \\mathrm{~B}, 1.7 \\mathrm{~B}$ and up to 3.8 B parameters. In particular, the largest 3.8 B base model pre-trained with 3.2 T tokens achieves a 71.2 score for MMLU $\\left[\\mathrm{HBB}^{+} 21\\right], 54.9$ for HumanEval $\\left[\\mathrm{CTJ}^{+} 21\\right]$, and 69.6 for GSM8K $\\left[\\mathrm{CKB}^{+} 21\\right]$, substantially outperforming strong open source language models up to 8B parameters, as detailed in Table 1. Despite being pre-trained in the 4 K sequence length, SAMBA can be extrapolated to 1 M length in zero shot with improved perplexity on Proof-Pile [ZAP22] while still maintaining the linear decoding time complexity with unlimited token streaming, as shown in Figure 1. We show that when instruction-tuned in a 4 K context length with only 500 steps, SAMBA can be extrapolated to a 256 K context length with perfect memory recall in Passkey Retrieval [MJ23]. In contrast, the fine-tuned SWA-based model simply cannot recall memories beyond 4 K length. We further demonstrate that the instruction-tuned SaMBA 3.8B model can achieve significantly better performance than the SWA-based models on downstream long-context summarization tasks, while still keeping its impressive performance on the short-context benchmarks. Finally, we conduct rigorous and comprehensive analyzes and ablation studies, encompassing up to 1.7 billion parameters, to validate the architectural design of SAMBA. These meticulous investigations not only justify our architectural designs but also elucidate the potential mechanisms underpinning the remarkable effectiveness of this simple hybrid approach. ## 2 Methodology\n\nWe explore different hybridization strategies consisting of the layers of Mamba, Sliding Window Attention (SWA), and Multi-Layer Perceptron [Sha20, DFAG16]. We conceptualize the functionality of Mamba as the capture of recurrent sequence structures, SWA as the precise retrieval of memory, and MLP as the recall of factual knowledge. We also explore other linear recurrent layers including Multi-Scale Retention [ $\\mathrm{SDH}^{+}$23] and GLA [YWS ${ }^{+} 23$ ] as potential substitutions for Mamba in\n\nSection 3.2. Our goal of hybridization is to harmonize between these distinct functioning blocks and find an efficient architecture for language modeling with unlimited-length extrapolation ability. ### 2.1 Architecture\n\nAs illustrated in Figure 2, we explore three kinds of layerwise hybridization strategies on the 1.7B scale: Samba, Mamba-SWA-MLP, and Mamba-MLP. We also explore other hybridization approaches with full self-attention on smaller scales in Section 4. The number of layers $N$ is set to 48 for Samba, Mamba-MLP, and Mamba, while Mamba-SWA-MLP has 54 layers, so each model has approximately 1.7B parameters. We only modify the layer-level arrangement for each of the models and keep every other configuration the same to have apple-to-apple comparisons. More details on the configuration of each layer are explained in the following subsections. ![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-03.jpg?height=483&width=1413&top_left_y=744&top_left_x=357)\nFigure 2: From left to right: Samba, Mamba-SWA-MLP, Mamba-MLP, and Mamba. The illustrations depict the layer-wise integration of Mamba with various configurations of Multi-Layer Perceptrons (MLPs) and Sliding Window Attention (SWA). We assume the total number of intermediate layers to be $N$, and omit the embedding layers and output projections for simplicity. Pre-Norm $\\left[\\mathrm{XYH}^{+} 20\\right.$, ZS19] and skip connections [HZRS16] are applied for each of the intermediate layers. ### 2.1.1 Mamba Layer\n\nMamba [GD23] is a recently proposed SSM-based model with selective state spaces. It enables input-dependent gating to both the recurrent states and the input representation for a soft selection of the input sequence elements. Given an input sequence representation $\\mathbf{X} \\in \\mathbb{R}^{n \\times d_{m}}$, where $n$ is the length of the sequence and $d_{m}$ is the hidden size, Mamba first expands the inputs to a higher dimension $d_{e}$, i.e.,\n\n$$\n\\mathbf{H}=\\mathbf{X} \\mathbf{W}_{\\text {in }} \\in \\mathbb{R}^{n \\times d_{e}}\n$$\n\nwhere $\\mathbf{W}_{\\text {in }} \\in \\mathbb{R}^{d_{m} \\times d_{e}}$ is a learnable projection matrix. Then a Short Convolution (SC) [PMN ${ }^{+} 23$ ] operator is applied to smooth the input signal,\n\n$$\n\\mathbf{U}=\\operatorname{SC}(\\mathbf{H})=\\operatorname{SiLU}\\left(\\operatorname{DepthwiseConv}\\left(\\mathbf{H}, \\mathbf{W}_{\\text {conv }}\\right)\\right) \\in \\mathbb{R}^{n \\times d_{e}}\n$$\n\nwhere $\\mathbf{W}_{\\text {conv }} \\in \\mathbb{R}^{k \\times d_{e}}$ and the kernel size $k$ is set to 4 for hardware-aware efficiency. The Depthwise Convolution [HQW ${ }^{+}$19] is applied over the sequence dimension followed by a SiLU [EUD17] activation function. The selective gate is then calculated through a low-rank projection followed by Softplus $\\left[\\mathrm{ZYL}^{+} 15\\right]$,\n\n$$\n\\Delta=\\operatorname{Softplus}\\left(\\mathbf{U} \\mathbf{W}_{\\mathrm{r}} \\mathbf{W}_{\\mathrm{q}}+\\mathbf{b}\\right) \\in \\mathbb{R}^{n \\times d_{e}}\n$$\n\nwhere $\\mathbf{W}_{\\mathbf{r}} \\in \\mathbb{R}^{d_{e} \\times d_{r}}, \\mathbf{W}_{\\mathrm{q}} \\in \\mathbb{R}^{d_{r} \\times d_{e}}$ and $d_{r}$ is the low-rank dimension. $\\mathbf{b} \\in \\mathbb{R}^{d_{e}}$ is carefully initialized so that $\\Delta \\in\\left[\\Delta_{\\min }, \\Delta_{\\max }\\right]$ after the initialization stage. We set $\\left[\\Delta_{\\min }, \\Delta_{\\max }\\right]=[0.001,0.1]$, and find that these values are not sensitive to language modeling performance under the perplexity metric. The input dependence is also introduced for the parameters B and C of SSM,\n\n$$\n\\begin{aligned}\n& \\mathbf{B}=\\mathbf{U W}_{\\mathrm{b}} \\in \\mathbb{R}^{n \\times d_{s}} \\\\\n& \\mathbf{C}=\\mathbf{U W}_{\\mathrm{c}} \\in \\mathbb{R}^{n \\times d_{s}}\n\\end{aligned}\n$$\n\nwhere $d_{s}$ is the state dimension. For each time step $1 \\leq t \\leq n$, the recurrent inference of the Selective SSM (S6) is performed in an expanded state space $\\mathbf{Z}_{t} \\in \\overline{\\mathbb{R}}^{d_{e} \\times d_{s}}$, i.e.,\n\n$$\n\\begin{gathered}\n\\mathbf{Z}_{t}=\\exp \\left(-\\Delta_{t} \\odot \\exp (\\mathbf{A})\\right) \\odot \\mathbf{Z}_{t-1}+\\Delta_{t} \\odot\\left(\\mathbf{B}_{t} \\otimes \\mathbf{U}_{t}\\right) \\in \\mathbb{R}^{d_{e} \\times d_{s}} \\\\\n\\mathbf{Y}_{t}=\\mathbf{Z}_{t} \\mathbf{C}_{t}+\\mathbf{D} \\odot \\mathbf{U}_{t} \\in \\mathbb{R}^{d_{e}}\n\\end{gathered}\n$$\n\nwhere $\\mathbf{Z}_{0}=\\mathbf{0}, \\odot$ means the point-wise product, $\\otimes$ means the outer product and exp means the point-wise natural exponential function. $\\mathbf{D} \\in \\mathbb{R}^{d_{e}}$ is a learnable vector initialized as $D_{i}=1$ and $\\mathbf{A} \\in \\mathbb{R}^{d_{e} \\times d_{s}}$ is a learnable matrix initialized as $A_{i j}=\\log (j), 1 \\leq j \\leq d_{s}$, following the S4D-Real [GGGR22] initialization. In practice, Mamba implements a hardware-aware parallel scan algorithm for efficient parallelizable training. The final output is obtained through a gating mechanism similar to Gated Linear Unit [Sha20, DFAG16],\n\n$$\n\\mathbf{O}=\\mathbf{Y} \\odot \\operatorname{SiLU}\\left(\\mathbf{X} \\mathbf{W}_{\\mathrm{g}}\\right) \\mathbf{W}_{\\text {out }} \\in \\mathbb{R}^{n \\times d_{m}}\n$$\n\nwhere $\\mathbf{W}_{g} \\in \\mathbb{R}^{d_{m} \\times d_{e}}$ and $\\mathbf{W}_{\\text {out }} \\in \\mathbb{R}^{d_{e} \\times d_{m}}$ are learnable parameters. In this work, we set $d_{e}=2 d_{m}$, $d_{r}=d_{m} / 16$, and $d_{s}=16$. The Mamba layer in SAMBA is expected to capture the time-dependent semantics of the input sequence through its recurrent structure. The input selection mechanism in the Mamba layer enables the model to focus on relevant inputs, thereby allowing the model to memorize important information in the long term. ### 2.1.2 Sliding Window Attention (SWA) Layer\n\nThe Sliding Window Attention [BPC20] layer is designed to address the limitations of the Mamba layer in capturing non-Markovian dependencies in sequences. Our SWA layer operates on a window size $w=2048$ that slides over the input sequence, ensuring that the computational complexity remains linear with respect to the sequence length. The $\\operatorname{RoPE}\\left[\\mathrm{SLP}^{+} 21\\right]$ relative positions are applied within the sliding window. By directly accessing the contents in the context window through attention, the SWA layer can retrieve high-definition signals from the middle to short-term history that cannot be clearly captured by the recurrent states of Mamba. We use FlashAttention 2 [Dao23] for the efficient implementation of self-attention throughout this work. We also choose the 2048 sliding window size for efficiency consideration; FlashAttention 2 has the same training speed as Mamba's selective parallel scan at the sequence length of 2048 based on the measurements in [GD23]. ### 2.1.3 Multi-Layer Perceptron (MLP) Layer\n\nThe MLP layers in SAMBA serve as the architecture's primary mechanism for nonlinear transformation and recall of factual knowledge [DDH ${ }^{+}$22]. We use SwiGLU [Sha20] for all the models trained in this paper and denote its intermediate hidden size as $d_{p}$. As shown in Figure 2, Samba applies separate MLPs for different types of information captured by Mamba and the SWA layers. ## 3 Experiments and Results\n\nWe pre-train four SAMBA models with different parameter sizes, 421M, 1.3B, 1.7B and 3.8B, to investigate its performance across different scales. The details of the hyperparameters for the training and architecture designs are shown in Table 10 of Appendix A. We also train other hybrid architectures as mentioned in Section 2.1, including the baseline Mamba, Llama-3, and Mistral architecture on a scale of around 1.7B, with detailed hyperparameters in Table 9 of Appendix A. We do comprehensive downstream evaluations on a wide range of benchmarks, focusing on four main capabilities of the models: commonsense reasoning (ARC [CCE ${ }^{+}$18], PIQA [BZB ${ }^{+}$20], WinoGrande [SBBC21], SIQA [SRC ${ }^{+}$19]), language understanding (HellaSwag [ZHB ${ }^{+}$19], BoolQ [CLC ${ }^{+}$19], OpenbookQA [MCKS18], SQuAD [RZLL16], MMLU [HBB ${ }^{+}$21]), truthfulness (TruthfulQA [LHE22]) and math and coding (GSM8K $\\left[\\mathrm{CKB}^{+} 21\\right]$, MBPP $\\left[\\mathrm{AON}^{+} 21\\right]$, HumanEval $\\left[\\mathrm{CTJ}^{+} 21\\right]$ ). ### 3.1 Language Modeling on Textbook Quality Data\n\nWe first present results from our largest 3.8B SAMBA model, trained on the same data set used by Phi3 [AJA $\\left.{ }^{+} 24\\right]$ with 3.2 T tokens. We follow the same multi-phase pretraining strategy as Phi3-mini for a fair comparison. We also report the performance of the Transformer++ (TFM++ in Table 1) model, which uses the same architecture and training recipe as Phi3-mini, for a fair comparison. In Table 1, we conduct comprehensive evaluations on a diverse subset of the benchmarks to assess SAMBA's performance across all the domains mentioned above to ensure a thorough examination of the model's capabilities. The details of the generation configurations are included in Appendix A. Table 1: Downstream performance comparison of SAMBA 3.8B with other pretrained base language models without instruction tuning. ARC-C and HellaSwag are measured with character-normalized accuracy. MMLU and GSM8K are measured in 5-shot, while others are in zero-shot. We report the MC2 score for TruthfulQA, maj@1 for GSM8K, and pass @ 1 for HumanEval. * Measured by ours. | Model | Size | Tokens | MMLU | Hella- <br> Swag | ARC- <br> C | Wino- <br> Gran. | Truth. <br> QA | GSM <br> 8K | Hum. <br> Eval | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 6.7B | 2T | 45.3 | 77.2 | 45.9 | 69.2 | 38.8 | 14.6 | 12.8 | 43.4 |\n|  | 13B | 2T | 54.8 | 80.7 | 49.4 | 72.8 | 37.4 | 28.7 | 18.3 | 48.9 |\n| Mistral | 7.2B | - | 60.1 | $\\mathbf{8 1 . 3}$ | 55.5 | 75.3 | 42.2 | 35.4 | 30.5 | 53.6 |\n| Mamba | 2.8B | 600B | 26.2 | 71.0 | 41.7 | 65.9 | $34.4^{*}$ | $3.6^{*}$ | $7.3^{*}$ | 35.7 |\n| Gemma | 2.5B | 3 T | 42.3 | 71.4 | 42.1 | 65.4 | 33.1 | 17.7 | 22.0 | 42.0 |\n|  | 8.5 B | 6T | 64.3 | 81.2 | 53.2 | 72.3 | 44.8 | 46.4 | 32.3 | 56.4 |\n| R-Gemma | 2.7B | 2T | 38.4 | 71.0 | 42.3 | 67.8 | 35.1 | 13.4 | 21.3 | 41.3 |\n| Llama 3 | 8.0 B | $15 \\mathrm{~T}+$ | 66.6 | $79.2^{*}$ | $53.2^{*}$ | $72.6^{*}$ | 43.9 | 45.8 | $28.7^{*}$ | 55.8 |\n| TFM++ | 3.8 B | 3.2 T | 67.2 | 76.6 | 53.8 | 72.6 | $\\mathbf{4 7 . 3}$ | 51.5 | 51.8 | 60.1 |\n| SAMBA | 3.8 B | 3.2 T | $\\mathbf{7 1 .",
    "samba-2": "2}$ | 77.4 | $\\mathbf{5 5 . 7}$ | $\\mathbf{7 7 . 1}$ | 43.4 | $\\mathbf{6 9 .",
    "samba-3": "6}$ | $\\mathbf{5 4 . 9}$ | $\\mathbf{6 4 . 2}$ |\n\nWe compare with several strong baselines, including Llama 2 [TMS ${ }^{+}$23], Mistral [JSM ${ }^{+}$23], Mamba [GD23], Gemma [Tea24], Recurrent-Gemma (R-Gemma) [BDS+24], Llama 3 [Met24] and TFM++. As shown in Table 1, SAMBA achieves the highest average score on all benchmarks, demonstrating its superior performance in handling various language comprehension tasks. Notably, SAMBA excels in the GSM8K benchmark, achieving an absolute $18.1 \\%$ higher accuracy than TFM++ trained on the same dataset. This shows the surprising complementary effect of combining SSM with the attention mechanism. We conjecture that when combined with attention, Mamba, as an input-dependent SSM, can focus more on performing the arithmetic operation through its recurrent states than on doing the retrieval operation which can be easily learned by the sliding window attention. Table 2: Downstream evaluation of the architectures trained on 230B tokens of the Phi2 dataset. We report the unnormalized accuracy for multiple choice tasks. GSM8K is evaluated with 5 -shot examples while other tasks are in zero-shot. Best results are in bold, second best underlined. | Benchmark | Llama-3 <br> 1.6 B | Mistral <br> 1.6 B | Mamba <br> 1.8 B | Mamba-SWA- <br> MLP 1.6B | Mamba- <br> MLP 1.9B | SAMBA <br> 1.7 B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ARC-Easy | 76.85 | 77.02 | 77.99 | 76.68 | $\\underline{78.91}$ | $\\mathbf{7 9 . 2 5}$ |\n| ARC-Challenge | 43.26 | 44.20 | 45.22 | 46.16 | $\\underline{47.35}$ | $\\mathbf{4 8 . 2 1}$ |\n| PIQA | 76.66 | 75.79 | $\\underline{77.31}$ | 76.50 | $\\mathbf{7 8 . 8 4}$ | 77.10 |\n| WinoGrande | 70.01 | 70.72 | $\\underline{73.40}$ | $\\mathbf{7 3 . 7 2}$ | 72.38 | 72.93 |\n| SIQA | 51.23 | 52.00 | 53.12 | $\\mathbf{5 5 . 1 2}$ | $\\underline{54.30}$ | 53.68 |\n| HellaSwag | 46.98 | 47.19 | $\\underline{49.80}$ | 49.71 | $\\mathbf{5 0 . 1 4}$ | 49.74 |\n| BoolQ | 68.20 | 70.70 | $\\underline{74.83}$ | 74.74 | 73.70 | $\\mathbf{7 5 . 5 7}$ |\n| OpenbookQA | 34.00 | 32.80 | $\\underline{36.60}$ | 33.80 | 35.40 | $\\mathbf{3 7 . 2 0}$ |\n| SQuAD | 74.88 | 72.82 | 67.66 | $\\underline{76.73}$ | 63.86 | $\\mathbf{7 7 . 6 4}$ |\n| MMLU | 43.84 | 43.54 | 45.28 | $\\underline{47.39}$ | 43.68 | $\\mathbf{4 8 . 0 1}$ |\n| TruthfulQA (MC1) | 25.70 | 25.09 | 26.81 | 26.20 | $\\underline{26.44}$ | $\\mathbf{2 7 . 7 8}$ |\n| TruthfulQA (MC2) | 40.35 | 38.80 | 40.66 | $\\underline{40.80}$ | 40.04 | $\\mathbf{4 1 . 6 2}$ |\n| GSM8K | 32.68 | 32.45 | 32.07 | $\\mathbf{4 4 . 0 5}$ | 27.52 | $\\underline{38.97}$ |\n| MBPP | 46.30 | 47.08 | $\\underline{47.86}$ | 47.08 | 47.08 | $\\mathbf{4 8 . 2 5}$ |\n| HumanEval | 36.59 | 36.59 | 35.98 | $\\underline{37.80}$ | 31.10 | $\\mathbf{3 9 . 0 2}$ |\n| Average | 51.17 | 51.12 | 52.31 | $\\underline{53.77}$ | 51.38 | $\\mathbf{5 4 . 3 3}$ |\n\nTo examine the different hybridization strategies mentioned in Section 2.1, we train 6 models with around 1.7B parameters on the Phi $2\\left[\\mathrm{LBE}^{+} 23\\right]$ dataset with 230 B tokens and evaluate them in the full suite of 15 downstream benchmarks to have a holistic assessment of hybrid and purebred architectures. As shown in Table 2, SAMBA demonstrates superior performance on a diverse set of tasks, including commonsense reasoning (ARC-Challenge), language understanding (MMLU, SQuAD), TruthfulQA and code generation (HumanEval, MBPP). It outperforms both the pure attention-based and SSMbased models in most tasks and achieves the best average performance. We can observe that replacing Mamba blocks with MLPs does not harm commonsense reasoning ability, but its performance on language understanding and complex reasoning ability, such as coding and mathematical reasoning, degenerates significantly. We can also see that pure Mamba models fall short on retrieval intensive tasks such as SQuAD due to their lack of precise memory retrieval ability. The best results are achieved through the combination of the attention and Mamba modules, as shown with our Samba architecture. We can also notice that Mamba-SWA-MLP has significantly better performance on GSM8K, potentially resulting from a closer collaboration between the Mamba and the SWA layers. The distinct downstream performances of different hybridization strategies pose interesting future work for developing task-adaptive dynamic architectures. ### 3.2 Exploration on Attention and Linear Recurrence\n\nSince SSMs belong to a broader realm of linear recurrent models $\\left[\\mathrm{OSG}^{+} 23\\right.$, QYZ23, YWS ${ }^{+}$23, Kat23, QYS ${ }^{+24],}$ there exist multiple alternatives other than Mamba when combing attentionbased layers with recurrent neural networks. In addition to Mamba and Samba, we investigate the comparative analysis of the following architectures:\n\n- Llama-2 $\\left[\\mathrm{TMS}^{+} 23\\right]$ is an attention-based Transformer architecture that utilizes full selfattention across the entire sequence. - Llama-2-SWA is an attention-based architecture that replaces all full attention layers in Llama-2 with sliding window attention. - Sliding RetNet replaces Mamba layers in the Samba architecture with Multi-Scale Retention [ $\\mathrm{SDH}^{+}$23] layers. RetNet is a linear attention model with fixed and input-independent decay applying to the recurrent hidden states. - Sliding GLA replaces Mamba layers in the Samba architecture with Gated Linear Attention (GLA) [YWS ${ }^{+}$23]. GLA is a more expressive variant of linear attention with input-dependent gating. Table 3: Perplexity on the validation set of SlimPajama for different attention and linear recurrent model architectures trained at 4,096 context length. We use window size 2,048 for Sliding Window Attention (SWA). The perplexity results have a fluctuation around $\\pm 0.3 \\%$. | Architecture | Size | Layers | Training Speed <br> ( $\\times 10^{5}$ tokens/s) | Validation Context Length |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  | 4096 | 8192 | 16384 |\n| 20B training tokens on $8 \\times$ A100 GPUs |  |  |  |  |  |  |\n| Llama-2 | 438M | 24 | 4.85 | 11.14 | 47.23 | 249.03 |\n| Llama-2-SWA | 438M | 24 | 4.96 | 11.12 | 10.66 | 10.57 |\n| Mamba | 432M | 60 | 2.46 | 10.70 | 10.30 | 10.24 |\n| Sliding GLA | 438M | 24 | 4.94 | 10.43 | 10.00 | 9.92 |\n| Sliding RetNet | 438M | 24 | 4.32 | 10.38 | 9.96 | 9.87 |\n| Samba | 421M | 24 | 4.46 | 10.06 | 9.65 | 9.57 |\n| 100B training tokens on $64 \\times$ H100 GPUs |  |  |  |  |  |  |\n| Llama-2 | 1.3B | 40 | 25.9 | 7.60 | 44.32 | 249.64 |\n| Llama-2-SWA | 1.3B | 40 | 26.2 | 7.60 | 7.37 | 7.21 |\n| Mamba | 1.3B | 48 | 17.8 | 7.47 | 7.26 | 7.15 |\n| Sliding GLA | $1.2 B$ | 36 | 25.9 | 7.58 | 7.35 | 7.19 |\n| Sliding RetNet | $1.4 B$ | 36 | 23.0 | 7.56 | 7.35 | 7.56 |\n| Samba | 1.3B | 36 | 25.2 | 7.32 | 7.11 | 6.96 |\n\nWe pre-train all models on the same SlimPajama [SAKM ${ }^{+}$23] dataset under both around 438M and 1.3 B settings, and evaluate these models by calculating perplexity on the validation set with context length at 4096,8192 , and 16384 tokens to investigate their zero-shot length extrapolation ability. Peak training throughput is also measured as an efficiency metric. The details of the hyperparameter settings are included in Appendix A. As shown in Table 3, SAMBA consistently outperforms all other models in different context lengths and model sizes. The training speed of SAMBA is competitive compared to pure Transformer-based models on the 1.3B scale. Mamba has significantly worse training throughput because Mamba layers have slower training speed than MLP layers, and the purebred Mamba models need to have more layers than other models at the same number of parameters. We can notice that the full attention-based model cannot extrapolate beyond its context length without specific length extrapolation techniques, which motivates us to use SWA for Samba. In Section 4, we further show that even hybridizing with one full attention layer will still lead to exploding perplexity at 16 k sequence length. We can also find that while RetNet can extrapolate well under the 438 M scale, it has an increasing perplexity on 16 K length at the 1.4 B scale, which may indicate that its input-independent decay may need specific tuning at different scales to work well. ### 3.3 Efficient Length Extrapolation\n\nWe use the test split of the Proof-Pile [ZAP22] dataset to evaluate the length extrapolation ability of our models at a scale of around 1.7B parameters. We follow Position Interpolation [CWCT23] for data pre-processing. The sliding window approach [PSL21] is used for the perplexity evaluation with a window size of 4096 . Besides having the decoding throughput in Figure 1 for the generation efficiency metric, we also measure the prompt processing speed in Figure 3 for the models Samba 1.7B, Mistral 1.6B, Mamba 1.8B, Llama-3 1.6B and its Self-Extended [JHY ${ }^{+}$24] version SE-Llama-3 1.6B with the prompt length sweeping from 1 K to 128 K . We set the group size to 4 and the neighborhood window to 1024 for self-extension. We fix the total processing tokens per measurement to be 128 K and varying the batch size accordingly. The throughput\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-07.jpg?height=535&width=657&top_left_y=974&top_left_x=1100)\n\nFigure 3: Prompt processing throughput of different models with around 1.7B parameters. is measured on a single A100 GPU with the precision of bfloat 16 . We repeat the measurements 10 times and report the averaged results. We can see that Samba achieves $3.73 \\times$ higher throughput in prompt processing compared to Llama- 31.6 B at the 128 K prompt length, and the processing time remains linear with respect to the sequence length. We can also observe that the existing zero-shot length extrapolation technique introduces significant inference latency overhead on the full-attention counterpart, while it still cannot extrapolate infinitely with perplexity performance comparable to that of Samba. In Figure 1, we can also see that Mamba has a slowly and stably increasing perplexity up to 1 M sequence length, which indicates that linear recurrent models can still not extrapolate infinitely if the context length is extremely large. Beyond its efficiency in processing long context, Samba can also extrapolate its memory recall ability to 256 K context length through supervised fine-tuning, and still keeps its linear computation complexity. We fine-tune Samba 1.7B on Passkey Retrieval with a 4K training sequence length for only 500 steps. As presented in Figure 4, SAMBA 1.7B demonstrates a remarkable ability to recall information from significantly longer contexts compared to Mistral 1.6B, a model based solely on Sliding Window Attention (SWA). This capability is particularly evident in the heatmap, where SAMBA maintains the perfect retrieval performance across a wider range of pass-key positions in a long document of up to 256 K length. We also draw the training loss curve and the overall passkey retrieval accuracy across the fine-tuning procedure in Figure 6 and Figure 7 of Appendix B. We find that despite the fact that both architectures can reach near-zero training loss in less than 250 steps, Samba can achieve near-perfect retrieval early at 150 training steps, while the Mistral architecture struggles at around $30 \\%$ accuracy throughout the training process. This shows that Samba can have better long-range retrieval ability than SWA due to the input selection mechanism introduced by the Mamba layers. ### 3.4 Long-Context Understanding\n\nThe impressive results on the synthetic passkey retrieval task encourage us to perform full-cycle instruction tuning of the Samba-3.8B model. We follow the same post-training recipe used for the Phi-3-mini series and evaluate the downstream performance of the instruction-tuned Samba3.8B-IT (preview) on both the long-context summarization tasks (GovReport [HCP ${ }^{+}$21], SQuALITY [ $\\left.\\mathrm{WPC}^{+} 22\\right]$ ) and the main shortcontext benchmarks (MMLU, GSM8K, HumanEval), as shown in Table 4. We can see that Samba has substantially better performance\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-08.jpg?height=261&width=700&top_left_y=325&top_left_x=1057)\n\nFigure 4: Passkey Retrieval performance up to 256K context length for SAMBA 1.7B (Left) vs. Mistral 1.6B (right) instruction tuned on 4 K sequence length with 500 steps. than Phi-3-mini-4k-instruct on both the short-context (MMLU, GSM8K, HumanEval) and longcontext (GovReport) tasks, while still having the 2048 window size of its SWA layer and maintaining the linear complexity for efficient processing of long documents. Table 4: Downstream performance comparison between instruction-tuned Samba 3.8B and Phi-3mini- 4 K on both long-context and short-context tasks. We report 5 -shot accuracy (averaged by category) for MMLU, 8 -shot CoT [WWS ${ }^{+}$22] for GSM8K, 0 -shot pass@ 1 for HumanEval, ROUGEL for both GovReport and SQuALITY. $\\dagger$ Results from the Phi-3 technical report [AJA ${ }^{+}$24]. | Model | MMLU | GSM8K | HumanEval | GovReport | SQuality |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Phi-3-mini-4K-instruct $\\dagger$ | 68.8 | 82.5 | 58.5 | 14.4 | 21.6 |\n| Samba-3.8B-IT (preview) | $\\mathbf{7 1 .",
    "samba-4": "9}$ | $\\mathbf{8 7 .",
    "samba-5": "6}$ | $\\mathbf{6 2 . 8}$ | $\\mathbf{1 8 . 9}$ | 21.2 |\n\n## 4 Analysis\n\nIn this section, we analyze the experimental results of SAMBA by answering the following research questions. The perplexity results on SlimPajama have a fluctuation around $\\pm 0.3 \\%$. Training speed is measured on $8 \\times \\mathrm{H} 100$ GPUs by default. All the models in this section are trained on SlimPajama with 20B tokens and 4 K sequence length, unless otherwise specified. How to train models with Sliding Window Attention (SWA)? Since SWA has linear complexity with respect to the sequence length, it seems alluring to trade off the batch size to have a longer training sequence length without substantially decreasing the training throughput. However, as shown in Table 5, when the sequence length is increased, the validation perplexity also increases in all context lengths due to smaller batch sizes, and the optimal ratio of sequence length/window size observed is 2, resulting in a training length of 4096. Table 5: Perplexity on SlimPajama of Llama-2-SWA 438M models trained on different context sizes and batch sizes. We fix the sliding window size as 2048 and the training tokens per step as 2M. | Batch Size | Sequence Length | Training Speed | Validation Context Length |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | $\\left(\\times 10^{5}\\right.$ tokens/s) | 2048 | 4096 | 8192 | 16384 |\n| 1024 | 2048 (Full Attention) | 10.4 | $\\mathbf{1 1 . 5 9}$ | 38.12 | 156.18 | 357.32 |\n| 512 | 4096 | 9.88 | 11.87 | $\\mathbf{1 1 .",
    "samba-6": "1 6}$ | $\\mathbf{1 0 . 6 9}$ | $\\mathbf{1 0 . 6 1}$ |\n| 256 | 8192 | 9.66 | 11.98 | 11.26 | 10.79 | 10.69 |\n| 128 | 16384 | 9.48 | 12.37 | 11.63 | 11.12 | 11.02 |\n| 64 | 32768 | 9.29 | 12.94 | 12.46 | 11.96 | 11.86 |\n\nWhy not hybridize with full attention? Some previous works $\\left[\\mathrm{FDS}^{+}\\right.$23, $\\mathrm{LLB}^{+}$24] suggest a hybrid architecture of Mamba with full attention. However, as shown in Table 6, the extrapolation perplexity is exploding at a context length of 16 k even if a single full attention layer is placed at the beginning of the model. Samba also has much better training throughput compared to Mamba-MLP\nalternatives because self-attention with the FlashAttention 2 implementation is more training efficient than Mamba when the sequence length is 4096. Table 6: Perplexity on SlimPajama of Mamba-MLP architectures with full attention layers replacing Mamba layers at different block indices. We define a block as two consecutive layers with a Mamba/Attention layer followed by an MLP. All the models have 12 blocks in total. | Architecture | Size | Block Index <br> of Full Attention | Training Speed <br> $\\left(\\times 10^{5}\\right.$ tokens/s $)$ | Validation Context Length |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 4096 | 8192 | 16384 |  |  |\n|  | 449 M | 11 | 7.78 | 10.29 | 10.53 | 13.66 |\n| Mamba-MLP | 449 M | 5 | 7.78 | 10.10 | 10.05 | 12.83 |\n|  | 449 M | 0 | 7.78 | 10.89 | 10.55 | 10.63 |\n|  | 443 M | 1,5 | 7.93 | $\\mathbf{1 0 .",
    "samba-7": "0 6}$ | 10.34 | 13.57 |\n| SAMBA | 421 M | SWA at odd indices | 8.59 | $\\mathbf{1 0 .",
    "samba-8": "0 6}$ | $\\mathbf{9 . 6 5}$ | $\\mathbf{9 . 5 7}$ |\n\nHow many parameters should be allocated to Attention? Given that Mamba can already capture low-rank information in the sequences through recurrent compression, the attention layers in Samba theoretically will only need to focus on information retrieval where a small number of attention heads should suffice. In Table 7, we explore the techniques of query head grouping [ALTdJ ${ }^{+}$23, Sha19], for both the Llama and Samba models. Surprisingly, both the Llama-2-SWA architecture and the Samba architecture show improved validation perplexity when there is only one key-value head. We conjecture that this is because small language models can be more easily optimized with fewer KV heads to pay attention to the contexts. We can also see that Samba has a $2 \\times$ smaller optimal number of query heads than the SWA model, which confirms our hypothesis that Samba can support a smaller number of attention heads. Table 7: Perplexity on SlimPajama of Llama-2-SWA and Samba models at the 430M scales trained with different number of Query and Key-Value heads. \"KV Size\" means the size of Key-Value vectors per token. Since grouped query attention will reduce the parameters for attention from $4 d_{m}^{2}$ to roughly $2 d_{m}^{2}$, we increase the hidden size of MLP from $8 / 3 d_{m}$ to $3 d_{m}=4608$ to have roughly the same number of total parameters as the original models. | Query | Key-Value | Head | KV | Model | Training Speed <br> $\\left(\\times 10^{5}\\right.$ tokens/s) | Validation Context Length |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Head | Head | Dim. | Size | Size |  | 4096 | 8192 | 16384 |\n| Llama-2-SWA Architecture |  |  |  |  |  |  |  |  |\n| 12 | 2 | 128 | 512 | 419 M | 10.01 | 11.11 | 10.64 | 10.56 |\n| 6 | 1 | 256 | 512 | 419 M | 9.98 | 11.09 | 10.62 | 10.54 |\n| 12 | 1 | 128 | 256 | 414M | 10.25 | 10.89 | 10.44 | 10.35 |\n| 12 | 4 | 128 | 1024 | 428M | 9.85 | 11.11 | 10.64 | 10.56 |\n| Samba Architecture |  |  |  |  |  |  |  |  |\n| 12 | 2 | 128 | 512 | 426M | 8.55 | 10.09 | 9.68 | 9.60 |\n| 6 | 1 | 256 | 512 | 426M | 8.46 | 9.99 | 9.59 | 9.51 |\n| 12 | 1 | 128 | 256 | 424M | 8.62 | 10.07 | 9.66 | 9.58 |\n| 12 | 4 | 128 | 1024 | 431M | 8.57 | 10.02 | 9.62 | 9.55 |\n\nWhy hybrid is better? We examine the entropy of the attention distributions for both the Samba 1.7B and the Mistral 1.6B models. As shown in Figure 5a, the Samba model has a larger variance of the attention entropy distributed over the layer indices, with an interesting pattern that the upper and lower layers have entropy higher than the middle layers. This may indicate that the attention layers are more specialized in the Samba architecture, with the middle layers focusing on precise retrieval with low-entropy attention, and the top and bottom layers focusing on integrating the global information through high-entropy attention. We can also see in Figure 5b that, compared to the Mamba-MLP model, Samba has a higher entropy of input selection probabilities in the middle layers. This indicates that, given the memory recalling ability of the attention layers, the Mamba layers can focus more on modeling the recurrent structure rather than performing retrieval with precise input selections. This kind of specialization can be beneficial for the downstream model performance,\nwhich may explain the impressive results from the Samba architecture. Details on how entropy is calculated are included in Appendix C. ![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-10.jpg?height=446&width=1332&top_left_y=402&top_left_x=394)\n\nFigure 5: The average entropy of the attention mechanism and the Mamba's S6 input selection mechanism at each block of layers on 100 random samples from the GSM8K dataset. Table 8: Perplexity on the SlimPajama validation set of different linear recurrent and sliding window attention models with Short Convolution (SC) modules added separately to query, key and value representations. For hybrid models, SC is applied only to linear attention layers. The training speed is measured on $8 \\times \\mathrm{A} 100$ GPUs. | Architecture | Size | Training Speed | Validation Context Length |  |  |\n| ---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | $\\left(\\times 10^{5}\\right.$ tokens/s $)$ | 4096 | 8192 | 16384 |\n| Llama-2-SWA | 438 M | 4.96 | 11.12 | 10.66 | 10.57 |\n| + SC | 438 M | 4.69 | 10.83 | 10.39 | 10.31 |\n| Sliding GLA | 438 M | 4.94 | 10.43 | 10.00 | 9.92 |\n| + SC | 438 M | 4.44 | 10.39 | 9.96 | 9.87 |\n| Sliding RetNet | 438 M | 4.32 | 10.38 | 9.96 | 9.87 |\n| + SC | 438 M | 3.80 | 10.25 | 9.82 | 9.74 |\n\nFair comparison between Mamba and other linear recurrent models? We can notice that the Short Convolution (SC) operator in Equation (1) is independent to the design of other parts of Mamba and can be applied to other linear recurrent models. As shown in Table 8, we explore the effect of SC on model performance through enhancing Llama-2-SWA, Sliding GLA, and Sliding RetNet with SC. Surprisingly, besides boosting the performance of RetNet, adding SC can also significantly improve the SWA's performance, while the effect on GLA is less prominent. We think this is because GLA already has the fine-grained decays at the channel level, so the depthwise convolution doesn't add much of the useful inductive bias for better modeling power. Notably, even with the SC enhancer, Sliding GLA and Sliding RetNet still fall short than the original Samba 421M's performance shown in Table 3. This further justifies our choice of using Mamba for hybridization. We also find that adding SC to both the SWA and the linear attention layers in hybrid models produces negative results, and we leave it as a future work to understand the surprising effectiveness of SC in language modeling. ## 5 Conclusion\n\nIn this paper, we introduce SAMBA, a simple yet powerful hybrid neural architecture designed for efficient language modeling with unlimited context length. We show that SambA substantially outperforms state-of-the-art pure attention-based and SSM-based models across a wide range of benchmarks including common-sense reasoning, language understanding, mathematics and coding. Furthermore, SAMBA exhibits remarkable efficiency in processing long contexts, achieving substantial speedups in prompt processing and decoding throughput compared to the state-of-the-art Transformer architecture. The architecture's ability to extrapolate memory recall to very long contexts (up to 256 K ) through minimal fine-tuning underscores its practical applicability for real-world tasks requiring\nextensive context understanding. This efficient long-term memorization ability is further demonstrated to be useful by our evaluations in downstream long-context summarization tasks. Our analyses also provide insight into the optimal training configurations for hybrid models and underscore the benefits of combining attention mechanisms with SSMs. We find that allocating fewer parameters to the attention mechanism while leveraging Mamba's strengths for capturing recurrent structures leads to more efficient and effective language modeling. Our results suggest that SAMBA is a strong neural architecture for language modeling with unlimited context length. ## Acknowledgement\n\nWe want to thank Shuohang Wang and Liyuan Liu for helping with the training infrastructure, Mojan Javaheripi and the team for the pre-training data, Ziyi Yang, Jianwen Zhang, Junheng Hao and the team for helping with post-training. The first author also wants to thank Songlin Yang for her Triton implementation of Mamba. ## References\n\n$\\left[\\mathrm{AET}^{+}\\right.$23] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv: 2312.04927, 2023. [AEZ ${ }^{+}$24] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. [AJA ${ }^{+}$24] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S\u00e9bastien Bubeck, Martin Cai, Caio C\u00e9sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone.",
    "samba-9": "arXiv preprint arXiv: 2404.14219, 2024. $\\left[\\mathrm{ALTdJ}^{+}\\right.$23] J. Ainslie, J. Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr'on, and Sumit K. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. Conference on Empirical Methods in Natural Language Processing, 2023. $\\left[\\mathrm{AON}^{+}\\right.$21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv: 2108.07732, 2021. [BCB14] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. International Conference On Learning Representations, 2014. $\\left[\\mathrm{BCE}^{+} 23\\right]$ S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv: 2303.12712, 2023. [ $\\mathrm{BDS}^{+}$24] Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, L\u00e9onard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz GUStavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur\n\nZucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Cl\u00e9ment Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, and Nando de Frietas. Recurrentgemma: Moving past transformers for efficient open language models. arXiv preprint arXiv: $2404.07839,2024$. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BPC20] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv: Arxiv-2004.05150, 2020. $\\left[\\mathrm{BZB}^{+}\\right.$20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432-7439. AAAI Press, 2020. $\\left[\\mathrm{CCE}^{+}\\right.$18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv: 1803.05457, 2018. $\\left[\\mathrm{CKB}^{+}\\right.$21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.",
    "samba-10": "arXiv preprint arXiv: 2110.14168, 2021. $\\left[\\right.$ CLC $^{+}$19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.",
    "samba-11": "In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2924-2936. Association for Computational Linguistics, 2019. $\\left[\\mathrm{CTJ}^{+}\\right.$21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv: 2107.03374, 2021. [CWCT23] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation.",
    "samba-12": "arXiv preprint arXiv: 2306.15595, 2023. [Dao23] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning.",
    "samba-13": "arXiv preprint arXiv: $2307.08691,2023$. $\\left[\\mathrm{DDH}^{+}\\right.$22] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. $A C L, 2022$. [DFAG16] Y. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. International Conference On Machine Learning, 2016. [DFE $\\left.{ }^{+} 22\\right]$ Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [EUD17] Stefan Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 2017. [FDS ${ }^{+}$23] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [GD23] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "samba-14": "arXiv preprint arXiv:2312.00752, 2023. [GGGR22] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. ARXIV.ORG, 2022. [GGR21] Albert Gu, Karan Goel, and Christopher R'e. Efficiently modeling long sequences with structured state spaces. International Conference On Learning Representations, 2021. $\\left[\\mathrm{HBB}^{+}\\right.$21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. $\\left[\\mathrm{HBD}^{+}\\right.$19] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. International Conference on Learning Representations, 2019. $\\left[\\mathrm{HCP}^{+}\\right.$21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages $1419-1436,2021$. $\\left[\\mathrm{HQW}^{+}\\right.$19] Yihui He, Jianing Qian, Jianren Wang, Cindy X. Le, Congrui Hetang, Qi Lyu, Wenping Wang, and Tianwei Yue. Depth-wise decomposition for accelerating separable convolutions in efficient convolutional neural networks. arXiv preprint arXiv: 1910.09455, 2019. $\\left[\\mathrm{HWX}^{+}\\right.$23] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv: 2308.16137, 2023. [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.",
    "samba-15": "$C V P R, 2016$. [JHY ${ }^{+}$24] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv: $2401.01325,2024$. [JSM ${ }^{+}$23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv: 2310.06825, 2023 . [Kat23] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling.",
    "samba-16": "arXiv preprint arXiv: $2311.01927,2023$. $\\left[\\mathrm{LBE}^{+} 23\\right]$ Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report.",
    "samba-17": "arXiv preprint arXiv: 2309.05463, 2023. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.",
    "samba-18": "In International Conference on Learning Representations, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214-3252, Dublin, Ireland, may 2022. Association for Computational Linguistics. $\\left[L L B^{+}\\right.$24] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid transformer-mamba language model.",
    "samba-19": "arXiv preprint arXiv: 2403.19887, 2024. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. Conference on Empirical Methods in Natural Language Processing, 2018. [Met24] MetaAI. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL: https://ai.meta.com/blog/meta-llama-3/. [MJ23] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv: 2305.16300, 2023. [MPS24] William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models.",
    "samba-20": "arXiv preprint arXiv: $2404.08819,2024$. $\\left[\\mathrm{MZK}^{+}\\right.$23] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention.",
    "samba-21": "In The Eleventh International Conference on Learning Representations, 2023. [Ope23] OpenAI. Gpt-4 technical report. PREPRINT, 2023. [ $\\mathrm{OSG}^{+}$23] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. International Conference on Machine Learning, 2023. $\\left[\\mathrm{PMN}^{+}\\right.$23] Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. International Conference On Machine Learning, 2023. [PSL21] Ofir Press, Noah A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. International Conference On Learning Representations, 2021. [QYS ${ }^{+24]}$ Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv: 2404.07904, 2024. [QYZ23] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Neural Information Processing Systems, 2023. $\\left[\\right.$ RLW $^{+}$23] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. NEURIPS, 2023. $\\left[\\right.$ RWC $^{+}$19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. arXiv preprint, 2019. [RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text.",
    "samba-22": "EMNLP, 2016. $\\left[\\mathrm{SAKM}^{+}\\right.$23] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023. URL: https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama. [SBBC21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021. [SDH ${ }^{+}$23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [Sha19] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv: 1911.02150, 2019. [Sha20] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv: 2002.05202, 2020. [SLP ${ }^{+}$21] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "samba-23": "arXiv preprint arXiv: 2104.09864, 2021. $\\left[\\mathrm{SRC}^{+}\\right.$19] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv: 1904.09728, 2019. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [Tea24] Gemma Team. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv: $2403.08295,2024$. [TMS ${ }^{+}$23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.",
    "samba-24": "arXiv preprint arXiv: 2307.09288, 2023. [VSP ${ }^{+}$17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NIPS, 2017. [WDL24] Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. Rnns are not transformers (yet): The key bottleneck on in-context retrieval. arXiv preprint arXiv: 2402.18510, 2024. [WPC ${ }^{+}$22] Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. Squality: Building a long-document summarization dataset the hard way. Conference on Empirical Methods in Natural Language Processing, 2022. [WWS ${ }^{+}$22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E.",
    "samba-25": "Chi, F. Xia, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Neural Information Processing Systems, 2022. [XTC ${ }^{+}$23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks.",
    "samba-26": "arXiv preprint arXiv: 2309.17453, 2023. $\\left[\\mathrm{XYH}^{+}\\right.$20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10524-10533. PMLR, 2020. $\\left[\\mathrm{YWS}^{+}\\right.$23] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [YZ24] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. [ZAP22] Edward Ayers Zhangir Azerbayev and Bartosz Piotrowski. Proof-pile, 2022. URL: https: //github.com/zhangir-azerbayev/proof-pile. $\\left[\\mathrm{ZHB}^{+}\\right.$19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? Annual Meeting of the Association for Computational Linguistics, 2019. $\\left[\\right.$ ZLJ $^{+}$22] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. Efficient long sequence modeling via state space augmented transformer. arXiv preprint arXiv: $2212.08136,2022$. [ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Neural Information Processing Systems, 2019. $\\left[\\mathrm{ZYL}^{+}\\right.$15] Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural networks using softplus units. 2015 International Joint Conference on Neural Networks (IJCNN), pages $1-4,2015$. ## A Implementation Details\n\nTable 9: Detailed hyper-parameters of the baselines models trained on the Phi 2 dataset with 230B tokens. | Architecture | Llama-3 | Mistral | Mamba | Mamba-SWA-MLP | Mamba-MLP |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Parameters | 1.6 B | 1.6 B | 1.8 B | 1.6 B | 1.9 B |\n| Batch size | 2048 | 2048 | 2048 | 2048 | 2048 |\n| Learning rate | 0.0006 | 0.0006 | 0.0006 | 0.0006 | 0.0006 |\n| Weight decay | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 |\n| Gradient clipping | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 |\n| Sequence length | 4096 | 4096 | 4096 | 4096 | 4096 |\n| Sliding window size, $w$ | - | 2048 | - | 2048 | - |\n| Number of layers, $N$ | 48 | 48 | 64 | 54 | 48 |\n| Model width, $d_{m}$ | 2048 | 2048 | 2048 | 2048 | 2048 |\n| MLP intermediate size, $d_{p}$ | 8196 | 8196 | - | 8196 | 8196 |\n| Number of query heads | 32 | 32 | - | 32 | 32 |\n| Number of KV heads | 4 | 4 | - | 4 | 4 |\n| Number of Attention Layers | 24 | 24 | 0 | 18 | 0 |\n| Number of Mamba Layers | 0 | 0 | 64 | 18 | 24 |\n| Vocabulary size | 50304 | 50304 | 50304 | 50304 | 50304 |\n\nFor the GLA layer in the Sliding GLA architecture, we use the number of heads $d_{m} / 384$, a key expansion ratio of 0.5 , and a value expansion ratio of 1 . For the RetNet layer we use a number of head that is half of the number of attention query heads, key expansion ratio of 1 and value expansion ratio\nof 2. The GLA and RetNet implementations are from the Flash Linear Attention repository ${ }^{3}$ [YZ24]. We use the FlashAttention-based implementation for Self-Extend extrapolation ${ }^{4}$. The Mamba 432M model has a model width of 1024 and the Mamba 1.3B model has a model width of 2048. All models trained on SlimPajama have the same training configurations and the MLP intermediate size as Samba, unless otherwise specified. The training infrastructure on SlimPajama is based on a modified version of the TinyLlama codebase ${ }^{5}$. Table 10: Detailed hyper-parameters of the SAMBA models trained at different scales. We only show the optimization settings for the first training phase of the 3.8 B model. | Total Parameters | $\\mathbf{4 2 1 M}$ | $\\mathbf{1 . 3 B}$ | $\\mathbf{1 . 7 B}$ | $\\mathbf{3 . 8 B}$ |\n| :---: | :---: | :---: | :---: | :---: |\n| Dataset | SlimPajama | SlimPajama | Phi-2 | Phi-3 |\n| Batch size | 512 | 512 | 2048 | 2048 |\n| Learning rate | 0.0004 | 0.0004 | 0.0006 | 0.0006 |\n| Total training tokens | 20 B | 100 B | 230 B | 3.2 T |\n| Weight decay | 0.1 | 0.1 | 0.1 | 0.1 |\n| Gradient clipping | 1.0 | 1.0 | 1.0 | 1.0 |\n| Sequence length | 4096 | 4096 | 4096 | 4096 |\n| Sliding window size, $w$ | 2048 | 2048 | 2048 | 2048 |\n| Number of layers, $N$ | 24 | 36 | 48 | 64 |\n| Model width, $d_{m}$ | 1536 | 2304 | 2048 | 2816 |\n| MLP intermediate size, $d_{p}$ | 4096 | 6144 | 8196 | 9984 |\n| Number of query heads | 12 | 18 | 32 | 11 |\n| Number of key-value heads | 12 | 18 | 4 | 1 |\n| Vocabulary size | 32000 | 32000 | 50304 | 32064 |\n\nIn the generation configurations for the downstream tasks, we use greedy decoding for GSM8K, and Nucleus Sampling [HBD $\\left.{ }^{+} 19\\right]$ with a temperature of $\\tau=0.2$ and top $-p=0.95$ for HumanEval. For MBPP and SQuAD, we set $\\tau=0.01$ and top- $p=0.95$. ## B Additional Experiment Results\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-16.jpg?height=535&width=719&top_left_y=1581&top_left_x=692)\n\nFigure 6: Training loss curves of Samba 1.7B and Mistral 1.6B models during 500 steps of instruction tuning on Passkey Retrieval with 4 K sequence length. We plot the loss curves for both models using the simple moving average of window size 10 . We perform instruction tuning for both Mistral 1.6B and Samba 1.7B on Passkey Retrieval using document length 4096, where we generated the data on the fly through randomly sampling a 5-digit\n\n[^1]![](https://cdn.mathpix.com/cropped/2024_09_17_9d70629eeb35d107e133g-17.jpg?height=467&width=621&top_left_y=298&top_left_x=725)\n\nFigure 7: Overall passkey retrieval accuracy on the 256K document length of Samba 1.7B and Mistral 1.6B models during 500 steps of instruction tuning. integer passkey value and a location/depth between zero and the document length to insert the passkey. The model is then asked to generate the passkey given the full document. We train both models using batch size 2048, 250 warm-up steps with a peak learning rate of $1 e^{-4}$, and 0.1 weight decay with AdamW [LH18] optimizer. In both cases, the loss converges quickly in 100-200 steps. During the evaluation, we measure the overall average accuracies of the passkey retrieval at the document length of $[4 k, 8 k, 16 k, 32 k, 64 k, 128 k, 256 k]$, for each length we evaluate at 11 different depths of the document (from $0,0.1,0.2, \\ldots$ to 1.0 ). In addition, for each location of the passkey (depth) in the document, we evaluate the model with five different passkeys to measure accuracy. As seen in Figure 7, the average passkey retrieval accuracy for Samba 1.7B almost reaches 100\\% in around 150 steps, while the accuracy for Mistral 1.6B remains low, demonstrating the extrapolation ability of the Samba architecture. ## C Details of Entropy Measurement\n\nGiven a causal attention probability matrix $A \\in \\mathbb{R}^{h \\times n \\times n}, A_{i j k}=0 \\forall j<k$, with $h$ number of heads and a sequence length of $n$, and the generation length $0<l<n$, we calculate the average attention entropy per decoding step as follows,\n\n$$\n\\mathcal{H}_{a}=-\\frac{1}{l \\cdot h} \\sum_{i=1}^{h} \\sum_{j=n-l+1}^{n} \\sum_{k=1}^{n} A_{i j k} \\log \\left(A_{i j k}\\right)\n$$\n\nFor the selective gate $\\Delta \\in \\mathbb{R}^{n \\times d_{e}}$ used by S 6 in Equation (2) of the Mamba layers, we first normalize it to be in the simplex $[0,1]^{n \\times d_{e}}$, i.e.,\n\n$$\n\\Delta^{\\prime}=\\frac{\\Delta}{\\sum_{i=1}^{n} \\Delta_{i}} \\in[0,1]^{n \\times d_{e}}\n$$\n\nThe average selection entropy of S6 throughout the entire sequence is then calculated as\n\n$$\n\\mathcal{H}_{s}=-\\frac{1}{d_{e}} \\sum_{j=1}^{d_{e}} \\sum_{i=1}^{n} \\Delta_{i j}^{\\prime} \\log \\left(\\Delta_{i j}^{\\prime}\\right)\n$$\n\n## D Limitations\n\nAlthough Samba demonstrates promising memory retrieval performance through instruction tuning, its pre-trained base model has retrieval performance similar to that of the SWA-based model, as shown in Figure 7.",
    "samba-27": "This opens up future direction on further improving the Samba's retrieval ability without compromising its efficiency and extrapolation ability. In addition, the hybridization strategy of Samba is not consistently better than other alternatives in all tasks. As shown in Table 2, MambaSWA-MLP shows improved performance on tasks such as WinoGrande, SIQA, and GSM8K. This gives us the potential to invest in a more sophisticated approach to perform input-dependent dynamic combinations of SWA-based and SSM-based models. [^0]:    *Work partially done during internship at Microsoft. ${ }^{\\dagger}$ Equal second-author contribution. [^1]:    ${ }^{3}$ https://github.com/sustcsonglin/flash-linear-attention\n    ${ }^{4}$ https://github.com/datamllab/LongLM/blob/master/self_extend_patch/Llama.py\n    ${ }^{5}$ https://github.com/jzhang38/TinyLlama\n\n"
}