{
    "weightregnn-0": "Physics-Guided State-Space Model Augmentation Using Weighted Regularized Neural Networks\n\nYuhan Liu Roland T\u00f3th Maarten Schoukens Control Systems Group, Eindhoven University of Technology, Eindhoven, the Netherlands Systems and Control Laboratory, Institute for Computer Science and Control, Budapest, Hungary\n\nAbstract\n\nPhysics-guided neural networks (PGNN) is an effective tool that combines the benefits of data-driven modeling with the interpretability and generalization of underlying physical information.",
    "weightregnn-1": "However, for a classical PGNN, the penalization of the physics-guided part is at the output level, which leads to a conservative result as systems with highly similar state-transition functions, i.e. only slight differences in parameters, can have significantly different time-series outputs. Furthermore, the classical PGNN cost function regularizes the model estimate over the entire state space with a constant trade-off hyperparameter. In this paper, we introduce a novel model augmentation strategy for nonlinear state-space model identification based on PGNN, using a weighted function regularization (W-PGNN). The proposed approach can efficiently augment the prior physics-based state-space models based on measurement data. A new weighted regularization term is added to the cost function to penalize the difference between the state and output function of the baseline physics-based and final identified model. This ensures the estimated model follows the baseline physics model functions in regions where the data has low information content, while placing greater trust in the data when a high informativity is present. The effectiveness of the proposed strategy over the current PGNN method is demonstrated on a benchmark example. keywords:\n\nSystem Identification, Physics-Guided Neural Networks, State Space\n\n\u2020\u2020thanks: This work is funded by the European Union (ERC, COMPLETE, 101075836).",
    "weightregnn-2": "Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. 1 Introduction\n\nModel-based design plays a crucial role in achieving satisfactory performance for complex dynamic systems by providing an interpretable framework that facilitates a deep understanding of system behaviors, including nonlinearities such as damping and friction. However, the accurate and comprehensive system dynamics that can be modeled by first principle laws are often costly to obtain. Nonlinear system identification (Schoukens and Ljung, 2019) is a well-established topic and can be characterized by a wide range of model classes such as state-space models (Sch\u00f6n et al., 2011), block-oriented models (Schoukens and Tiels, 2017), NARMAX (Billings, 2013), etc. Wherein, extensive research (Verdult, 2002; Paduart et al., 2010; Sch\u00f6n et al., 2018) on identification with nonlinear state-space (NLSS) models has shown its flexibility for handling multi-variable systems with potentially fewer parameters. Estimation of state-space models is advantageous for the subsequent control design, given the dependency of many nonlinear control methods on such representation of the system behavior. Artificial neural networks (ANNs) have long been a focus of interest in the field of nonlinear system identification because of their high expressiveness, flexibility, and capability of approximating functions with arbitrary accuracy (Scarselli and Tsoi, 1998). In Suykens et al. (1995), recurrent neural networks have already been employed to represent a nonlinear state-space model. This structure is referred to as state-space neural network (SS-NN) and has been further discussed in (Amoura et al., 2011; Forgione and Piga, 2020; Schoukens, 2021; Beintema et al., 2023). Recently, Beintema et al. (2023) have introduced a computationally efficient nonlinear system state-space identification algorithm based on a subspace-encoder network (SUBNET). Nonetheless, ANNs are typically black-box models that lack physical interpretation, and exhibit poor generalization capabilities outside the training dataset, especially when the training data is limited. Hence, even though the ANNs may exhibit improved accuracy compared with first-principle modeling, deploying such models in practice or the controllers that are designed for them is simply dangerous. To address this issue, physics-guided neural network (PGNN) (Karpatne et al., 2017) has been introduced, also within the field of systems and control (Bolderman et al., 2024), that ensures the interpretability and generalization capabilities of the estimated models. Compared with the ANNs, a physics-based cost function is incorporated into the optimization objective of PGNN, ensuring that the learned model not only achieves high accuracy on the training dataset, but also shows consistency with known physics laws on the unseen region without the need for large amounts of ground truth data. However, there are some open technical issues with using PGNNs in nonlinear state space model identification. First, the classical PGNN does not perform the model augmentation, i.e., the prior model is only used to compute the physics-based regularization term in the cost function. Second, the classical PGNN penalizes the difference between the physics model and the identified model at the output level, which can lead to conservative estimation results. This is because systems with highly similar state-transition functions can have significantly different time-series outputs. Furthermore, the physics-based term of the classical PGNN regularizes the model difference over the entire state space, which makes this approach lose some flexibility, especially when the assumed prior model is inaccurate. Motivated by these facts, this paper proposes an innovative PGNN-based state-space modeling strategy for nonlinear system identification, namely, W-PGNN, to efficiently complete prior physics-based state-space models with a weighted regularized SS-NN. The main contributions are as follows:\n\n1) A new weighted-regularization cost function is designed to penalize the difference between both the state and output functions of the baseline physics-based and final identified models in regions where measured data provides little information. 2) Compared to the classical PGNNs, the proposed identification approach makes more extensive use of the pre-existing approximate model. The learned dynamics are capable of adhering to the data in regions with high information content, and preserving the behavior of the baseline physics model outside this region. This significantly enhances the flexibility of the SS-NN model. The remainder of this paper is organized as follows. Section 2 introduces the nonlinear model class and the identification method with a state-space neural network. The classical PGNN method is discussed in Section 3. The proposed W-PGNN method is detailed in Section 4. Numerical simulation results are provided in Section 5, followed by the conclusions in Section 6. Notation: and denote the sets of real numbers and integers, respectively. The 2-norm of a vector or a matrix is denoted as . denotes the column-wise composition of vectors. is the standard normal distribution, while represents a uniform distribution with a support from to . 2 Problem Statement\n\n2.1 Nonlinear Model Class\n\nConsider the following discrete-time state-space model as the data-generating system:\n\nx \u200b ( k + 1 ) \ud835\udc65 \ud835\udc58 1 \\displaystyle x(k+1) = f \u200b ( x \u200b ( k ) , u \u200b ( k ) ) , absent \ud835\udc53 \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 \\displaystyle=f(x(k),u(k)), (1) y 0 \u200b ( k ) subscript \ud835\udc66 0 \ud835\udc58 \\displaystyle y_{0}(k) = g \u200b ( x \u200b ( k ) , u \u200b ( k ) ) , absent \ud835\udc54 \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 \\displaystyle=g(x(k),u(k)),\n\nwhere denotes the input, is the state, is the output, and represents the discrete time. Additionally, and are bounded deterministic vector functions. The training dataset contains noisy outputs , collected from an experiment on (1), where the noise is assumed to be a zero-mean random signal with finite variance independent from the input . Assume that we only have access to an a priori known state-space model:\n\nx ~ \u200b ( k + 1 ) ~ \ud835\udc65 \ud835\udc58 1 \\displaystyle\\tilde{x}(k+1) = f ~ \u200b ( x ~ \u200b ( k ) , u \u200b ( k ) ) , absent ~ \ud835\udc53 ~ \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 \\displaystyle=\\tilde{f}\\left(\\tilde{x}(k),u(k)\\right), (2) y ~ 0 \u200b ( k ) subscript ~ \ud835\udc66 0 \ud835\udc58 \\displaystyle\\tilde{y}_{0}(k) = g ~ \u200b ( x ~ \u200b ( k ) , u \u200b ( k ) ) , absent ~ \ud835\udc54 ~ \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 \\displaystyle=\\tilde{g}\\left(\\tilde{x}(k),u(k)\\right),\n\nwith the state and output and that has the same model order as (1). Note that the functions and constitute the physically well-interpretable and a priori know dynamics of the system (1), i.e., the nominal model. However, the prior model (2) does not accurately capture the true dynamics (1). For instance, there may exist local nonlinearities in certain regions, which are not able to be obtained by a rough identification or modeling based on first principles. Hence, it is essential to augment this a priori known model using newly measured data through nonlinear system identification. 2.2 State-Space Neural Network Identification\n\nTo this end, we consider the following nonlinear discrete-time state-space model of (1), which has the following structure:\n\nx ^ \u200b ( k + 1 ) ^ \ud835\udc65 \ud835\udc58 1 \\displaystyle\\hat{x}(k+1) = f ~ \u200b ( x ^ \u200b ( k ) , u \u200b ( k ) ) + f \u03b8 \u200b ( x ^ \u200b ( k ) , u \u200b ( k ) ) , absent ~ \ud835\udc53 ^ \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 subscript \ud835\udc53 \ud835\udf03 ^ \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 \\displaystyle=\\tilde{f}\\left(\\hat{x}(k),u(k)\\right)+f_{\\theta}\\left(\\hat{x}(k),u(k)\\right), (3) y ^ \u200b ( k ) ^ \ud835\udc66 \ud835\udc58 \\displaystyle\\hat{y}(k) = g ~ \u200b ( x ^ \u200b ( k ) , u \u200b ( k ) ) + g \u03b8 \u200b ( x ^ \u200b ( k ) , u \u200b ( k ) ) , absent ~ \ud835\udc54 ^ \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 subscript \ud835\udc54 \ud835\udf03 ^ \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 \\displaystyle=\\tilde{g}\\left(\\hat{x}(k),u(k)\\right)+g_{\\theta}\\left(\\hat{x}(k),u(k)\\right),\n\nwhere and are the completion functions that model the dynamics that cannot be captured reliably by the idealistic model (2), and are represented by fully connected feedforward neural networks with one hidden layer containing neurons and a linear output layer:\n\nf \u03b8 \u200b ( x \u200b ( k ) , u \u200b ( k ) ) = W x \u200b \u03d5 \u200b ( [ W f \u200b x \u200b W f \u200b u ] \u200b [ x \u200b ( k ) u \u200b ( k ) ] + b f ) + b x subscript \ud835\udc53 \ud835\udf03 \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 subscript \ud835\udc4a \ud835\udc65 italic-\u03d5 delimited-[] subscript \ud835\udc4a \ud835\udc53 \ud835\udc65 subscript \ud835\udc4a \ud835\udc53 \ud835\udc62 delimited-[] \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 subscript \ud835\udc4f \ud835\udc53 subscript \ud835\udc4f \ud835\udc65 f_{\\theta}(x(k),u(k))=W_{x}\\phi\\left(\\left[W_{fx}W_{fu}\\right]\\left[\\begin{array}[]{l}x(k)\\\\\nu(k)\\end{array}\\right]+b_{f}\\right)+b_{x} (4)\n\nwhere denotes the activation function, , , , and represent the weight and bias parameters of the neural network with proper dimensions, respectively.",
    "weightregnn-3": "A similar representation is used for . As discussed in Suykens et al. (1995) and Schoukens (2021), the state-space model (3) can be written in a specific form of a recurrent neural network, i.e., an SS-NN. The parameters for the SS-NN can be trained by optimizing the data-based cost function over samples:\n\nV Data \u200b ( \u03b8 , N ) subscript \ud835\udc49 Data \ud835\udf03 \ud835\udc41 \\displaystyle V_{\\mathrm{Data}}(\\theta,N) = 1 N \u2211 k = 1 N \u2225 y ( k ) \u2212 y ^ ( k | \u03b8 ) \u2225 2 \\displaystyle=\\frac{1}{N}\\sum_{k=1}^{N}\\left\\|y(k)-\\hat{y}(k|\\theta)\\right\\|^{2} (5) \u03b8 ^ ^ \ud835\udf03 \\displaystyle\\hat{\\theta} = arg \u2061 min \u03b8 \u2061 V Data \u200b ( \u03b8 , N ) absent subscript \ud835\udf03 subscript \ud835\udc49 Data \ud835\udf03 \ud835\udc41 \\displaystyle=\\arg\\min_{\\theta}V_{\\mathrm{Data}}(\\theta,N)\n\nwhere is the simulated output of the model (3) given the parameter vector . More detailed discussions of SS-NN are provided in Section 4. From (5), it is obvious that the ANN simply learns the mapping between system input and output data without considering any prior knowledge about the underlying physics. This makes it difficult for ANN to have good generalization performance outside of the training region, especially when the dataset is limited. 3 Classical PGNN for System Identification\n\nIn this section, we briefly introduce the concept of classical PGNN. Compared with the baseline ANN approach, there is an additional regularization term in the cost function to force the learnt model to follow the prior model even outside the training region. The classical PGNN is trained by minimizing the following cost function:\n\nV \u200b ( \u03b8 , N , N \u00af ) \ud835\udc49 \ud835\udf03 \ud835\udc41 \u00af \ud835\udc41 \\displaystyle V(\\theta,N,\\bar{N}) = V Data \u200b ( \u03b8 , N ) + \u03b3 \u200b V Phy \u200b ( \u03b8 , N \u00af ) absent subscript \ud835\udc49 Data \ud835\udf03 \ud835\udc41 \ud835\udefe subscript \ud835\udc49 Phy \ud835\udf03 \u00af \ud835\udc41 \\displaystyle=V_{\\mathrm{Data}}(\\theta,N)+\\gamma V_{\\mathrm{Phy}}(\\theta,\\bar{N}) (6) \u03b8 ^ ^ \ud835\udf03 \\displaystyle\\hat{\\theta} = arg \u2061 min \u03b8 \u2061 V \u200b ( \u03b8 , N , N \u00af ) absent subscript \ud835\udf03 \ud835\udc49 \ud835\udf03 \ud835\udc41 \u00af \ud835\udc41 \\displaystyle=\\arg\\min_{\\theta}V(\\theta,N,\\bar{N})\n\nwhere is given by (5), and the physics-based penalized term is given by:\n\nV Phy ( \u03b8 , N \u00af ) = 1 N \u00af \u2211 k = 1 N \u00af \u2225 y \u00af ~ ( k ) \u2212 y \u00af ^ ( k | \u03b8 ) \u2225 2 V_{\\mathrm{Phy}}(\\theta,\\bar{N})=\\frac{1}{\\bar{N}}\\sum_{k=1}^{\\bar{N}}\\left\\|\\tilde{\\bar{y}}(k)-\\hat{\\bar{y}}(k|\\theta)\\right\\|^{2} (7)\n\nwhere and are the output response of the a priori known model (2) and the simulated model (3) given the regularization input signal , respectively, and is the constant trade-off hyperparameter that balances between the data-fitting term and the regularization term in the overall cost. In this way, the prior model (2) is embedded in the trained ANN. Note that the physics-based cost does not rely on the measurement from system (1). It is evaluated over a separate regularization dataset generated by the user using the baseline physics model (2). As can be seen in (6), the penalization of the physics-guided part is at the output level. However, this can lead to a conservative result as systems with highly similar state-transition functions, i.e. only slight differences in parameters, can have significantly different time-series outputs. A good example of this is given by two mass spring damper systems with slightly different resonance frequencies, as shown in Fig. 1. Furthermore, the classical PGNN cost function regularizes the model estimate for the whole state space, which means that the a priori known model is assumed to hold equally for any unseen region. Though this feature enables the classic PGNN to have better generalization performance than the baseline NN, but in an ideal setting, we would like to trust the information in the data when a high informativity is present, while we would like to follow the prior model in regions where the data provides little information, i.e., to preserve the behavior of the a priori known model. 4 Weighted PGNN Method\n\n4.1 Weighted function regularization\n\nUnlike other model augmentation strategies, for instance, (Hoekstra et al., 2024), our approach aims to regularize state-space neural network estimation using a reference model and penalize the difference between physics and identified model at both the state and output levels.",
    "weightregnn-4": "Moreover, the regularization should only be active in the regions where no data is present, i.e. the reference model prescribes the dynamics that the learned model should fall back to outside the training area. The proposed approach starts by generating a surrogate input sequence of length . It is worth noting that is not applied to the true system during optimization to acquire output measurements, but plays a role in the regularization of the proposed approach. Ideally, should cover the full range of operation of the system. Then, the model estimate is evaluated both applying and on system (3), where the second input sequence results in the state sequence . The cost function for the proposed W-PGNN is given by:\n\nV \u200b ( \u03b8 , N , N \u00af ) = V Data \u200b ( \u03b8 , N ) + V Reg \u200b ( \u03b8 , w , N \u00af ) \ud835\udc49 \ud835\udf03 \ud835\udc41 \u00af \ud835\udc41 subscript \ud835\udc49 Data \ud835\udf03 \ud835\udc41 subscript \ud835\udc49 Reg \ud835\udf03 \ud835\udc64 \u00af \ud835\udc41 V(\\theta,N,\\bar{N})=V_{\\mathrm{Data}}(\\theta,N)+V_{\\mathrm{Reg}}(\\theta,w,\\bar{N}) (8)\n\nwhere the novel weighed regularization term is given by:\n\nV Reg \u200b ( \u03b8 , w , N \u00af ) \u225c 1 N \u00af \u200b \u2211 j = 1 N \u00af w j \u200b ( \u03b3 x \u200b e j x + \u03b3 y \u200b e j y ) \u225c subscript \ud835\udc49 Reg \ud835\udf03 \ud835\udc64 \u00af \ud835\udc41 1 \u00af \ud835\udc41 superscript subscript \ud835\udc57 1 \u00af \ud835\udc41 subscript \ud835\udc64 \ud835\udc57 subscript \ud835\udefe \ud835\udc65 subscript superscript \ud835\udc52 \ud835\udc65 \ud835\udc57 subscript \ud835\udefe \ud835\udc66 subscript superscript \ud835\udc52 \ud835\udc66 \ud835\udc57 V_{\\mathrm{Reg}}(\\theta,w,\\bar{N})\\triangleq\\frac{1}{\\bar{N}}\\sum_{j=1}^{\\bar{N}}w_{j}\\left(\\gamma_{x}e^{x}_{j}+\\gamma_{y}e^{y}_{j}\\right) (9)\n\nwhere , .",
    "weightregnn-5": "The weight vector is defined as:\n\nw j subscript \ud835\udc64 \ud835\udc57 \\displaystyle w_{j} \u225c 1 \u2211 k = 1 N h k \u200b ( z \u00af j ) + \u03f5 , \u225c absent 1 superscript subscript \ud835\udc58 1 \ud835\udc41 subscript \u210e \ud835\udc58 subscript \u00af \ud835\udc67 \ud835\udc57 italic-\u03f5 \\displaystyle\\triangleq\\frac{1}{\\sum_{k=1}^{{N}}h_{k}(\\bar{z}_{j})+\\epsilon}, (10) h k \u200b ( z \u00af j ) subscript \u210e \ud835\udc58 subscript \u00af \ud835\udc67 \ud835\udc57 \\displaystyle h_{k}(\\bar{z}_{j}) = exp \u2061 ( \u2212 \u2016 z ^ k \u2212 z \u00af j \u2016 2 2 \u200b \u03c3 2 ) , absent superscript norm subscript ^ \ud835\udc67 \ud835\udc58 subscript \u00af \ud835\udc67 \ud835\udc57 2 2 superscript \ud835\udf0e 2 \\displaystyle=\\exp\\left(-\\frac{\\|\\hat{z}_{k}-\\bar{z}_{j}\\|^{2}}{2\\sigma^{2}}\\right),\n\nwith the state-input pairs and .",
    "weightregnn-6": "Furthermore, and denote the responses of the estimated model (3) to the training input and regularization input , respectively. represents the center width of , and is a small constant. Additionally, it is clear that if the weight is set to for all , then it is a classical PGNN with state-level regularization; if the weight is set to for all , then the cost function (8) will completely fall back to the baseline (5). One can observe from (10) that, the further away the current regularization state-input pair is from the training dataset, the larger the cost will be. This pushes and towards zero, consequently bringing the identified model closer to the prior model in that region of the joint input-state space. Additionally, the terms and share the same weight because both the state and the output function estimate depend on the same state-input pair. Note that the proposed cost function does not penalize the difference between the output of the a priori known model and the estimated model, but it rather penalizes the difference between both the state and output function of both models in regions where little information is provided by the measured data. As a consequence, the regularization state-input pair should cover the full intended range of operation of the completed model for an effective regularized model augmentation through the proposed approach. 4.2 Implementation\n\nThe whole structure of the physics-based SS-NN is illustrated in Fig. 2. Specifically, the SS-NN architecture mainly comprises two components, namely the physics state/output layer and the state/output completion layer, where the physics state/output layer is employed to represent the prior known model and given in (2), and the state/output completion layer is utilized for estimating the unknown dynamics and given in (3). It is worth mentioning that the prior model (2) should have the same state dimension as the estimated model (3), which is a limitation of the proposed approach. Training: The hyperparameters of the classical PGNN, , and the proposed W-PGNN, , , , , are determined by grid searching on a validation dataset. Specifically, the selection of depends on the density of data distribution, for instance, sparsely distributed data can necessitate choosing a larger . The weights and bias parameters of the SS-NN are trained by minimizing the cost function (8) via gradient-based approaches. Several optimization algorithms have been proposed to solve this problem, such as quasi-Newton (Fletcher and Powell, 1963) and conjugate gradients (Fletcher and Reeves, 1964) methods. In this paper, the Levenberg-Marquardt algorithm (Levenberg, 1944) is employed to find the minimum of (8). All the algorithms are implemented in the Matlab Deep Learning Toolbox and the Matlab Optimization Toolbox. Initialization of model completion layer: Due to the use of the Levenberg-Marquardt optimization algorithm, an initial guess of the parameter values is required. We adopt the method in Schoukens (2021) to intuitively initialize the weight and bias parameters of the model completion layer, i.e., an explicit linear approximation is introduced:\n\nf \u03b8 ( x ( k ) , u \\displaystyle f_{\\theta}(x(k),u ( k ) ) = A \u03b8 x ( k ) + B \u03b8 u ( k ) \\displaystyle(k))=A_{\\theta}x(k)+B_{\\theta}u(k) (11) + W ~ x \u200b \u03d5 \u200b ( [ W ~ f \u200b x \u200b W ~ f \u200b u ] \u200b [ x \u200b ( k ) u \u200b ( k ) ] + b ~ f ) + b ~ x subscript ~ \ud835\udc4a \ud835\udc65 italic-\u03d5 delimited-[] subscript ~ \ud835\udc4a \ud835\udc53 \ud835\udc65 subscript ~ \ud835\udc4a \ud835\udc53 \ud835\udc62 delimited-[] \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 subscript ~ \ud835\udc4f \ud835\udc53 subscript ~ \ud835\udc4f \ud835\udc65 \\displaystyle+\\tilde{W}_{x}\\phi\\left(\\left[\\tilde{W}_{fx}~{}\\tilde{W}_{fu}\\right]\\left[\\begin{array}[]{l}x(k)\\\\\nu(k)\\end{array}\\right]+\\tilde{b}_{f}\\right)+\\tilde{b}_{x}\n\ng \u03b8 ( x ( k ) , u \\displaystyle g_{\\theta}(x(k),u ( k ) ) = C \u03b8 x ( k ) + D \u03b8 u ( k ) \\displaystyle(k))=C_{\\theta}x(k)+D_{\\theta}u(k) (12) + W ~ y \u200b \u03d5 \u200b ( [ W ~ g \u200b x \u200b W ~ g \u200b u ] \u200b [ x \u200b ( k ) u \u200b ( k ) ] + b ~ g ) + b ~ y subscript ~ \ud835\udc4a \ud835\udc66 italic-\u03d5 delimited-[] subscript ~ \ud835\udc4a \ud835\udc54 \ud835\udc65 subscript ~ \ud835\udc4a \ud835\udc54 \ud835\udc62 delimited-[] \ud835\udc65 \ud835\udc58 \ud835\udc62 \ud835\udc58 subscript ~ \ud835\udc4f \ud835\udc54 subscript ~ \ud835\udc4f \ud835\udc66 \\displaystyle~{}+\\tilde{W}_{y}\\phi\\left(\\left[\\tilde{W}_{gx}~{}\\tilde{W}_{gu}\\right]\\left[\\begin{array}[]{l}x(k)\\\\\nu(k)\\end{array}\\right]+\\tilde{b}_{g}\\right)+\\tilde{b}_{y}\n\nwhich leaves quite some flexibility in initializing the weights and biases of the nonlinear layers. Then, the weights and biases of the linear layer are initialized as and . Additionally, the weights and biases of the nonlinear layer are initialized as , and are randomly initialized by . This chosen parameter initialization ensures that the initial model behaves like the a priori provided physics model. During the optimization, the weights will become nonzero, and this will activate the model completion part of the model. 5 Simulation Study\n\nIn this section, simulation results are presented to illustrate the effectiveness of our proposed W-PGNN approach. A 1-D example is conducted to validate the superior learning performance of the proposed W-PGNN approach over the baseline and classical PGNN approaches. Consider a SISO system:\n\nx \u200b ( k + 1 ) \ud835\udc65 \ud835\udc58 1 \\displaystyle x(k+1) = a \u200b x \u200b ( k ) + b \u200b u \u200b ( k ) + \u0394 \u200b ( x \u200b ( k ) ) , absent \ud835\udc4e \ud835\udc65 \ud835\udc58 \ud835\udc4f \ud835\udc62 \ud835\udc58 \u0394 \ud835\udc65 \ud835\udc58 \\displaystyle=ax(k)+bu(k)+\\Delta(x(k)), (13) y 0 \u200b ( k ) subscript \ud835\udc66 0 \ud835\udc58 \\displaystyle y_{0}(k) = x \u200b ( k ) absent \ud835\udc65 \ud835\udc58 \\displaystyle=x(k)\n\nwith and . The function is defined as:\n\n\u0394 \u200b ( x ) = 0.2 \u200b ( e \u2212 x 2 l 2 \u2212 e \u2212 ( x \u2212 c ) 2 l 2 ) \u0394 \ud835\udc65 0.2 superscript \ud835\udc52 superscript \ud835\udc65 2 superscript \ud835\udc59 2 superscript \ud835\udc52 superscript \ud835\udc65 \ud835\udc50 2 superscript \ud835\udc59 2 \\Delta(x)=0.2\\left(e^{-\\frac{x^{2}}{l^{2}}}-e^{-\\frac{(x-c)^{2}}{l^{2}}}\\right) (14)\n\nwith , , which represents the local nonlinearity that is not able to be expressed by the given baseline physics model. Then, the augmentation structure (3) is given in terms of the prior physics model , and the completion function aimed to identify while in this case. Thus, the goal is to augment the prior model with a well-estimated based on the proposed W-PGNN approach. With this SISO system (13), the training input is selected as with samples to generate the training dataset . Furthermore, the regularization input signal is designed as a concatenation of signals and , each with a length of 500, respectively, leading to a with size . In addition, the test input signal is selected as with samples, which will explore a much larger region of input-output space than the training dataset. It is worth noting that only noise at the output of the system is present with SNR40dB. The aforementioned signals are visualized in Fig. 3, which implies that the training dataset is significantly less informative than the test dataset. This is in line with the model augmentation philosophy of this work: as an adequate prior model is already in place, we only would like to augment this model using a simple dataset dedicated to a particular region. To construct the NN model, the activation function is chosen as the radial basis function because of its universal approximation capability. A total of 20 neurons () are used in the state/output completion layer. Moreover, to determine the most suitable hyperparameters for classical PGNN and the proposed W-PGNN, a grid search is conducted on the validation dataset , which is generated by validation input signal for the classical PGNN, and for the W-PGNN. Both of them are 500 samples long. The results of the hyperparameter search are: , , , and . Then all three approaches are trained on the obtained dataset and , of which the parameters are optimized by the Levenberg-Marquardt algorithm, as mentioned in Section 4.2. The estimation results in terms of , and the absolute value of estimation error are depicted in Fig. 4, where the shaded area indicates the training data region and the black dots represent the linear prior model. It is clear that all three approaches are capable of capturing the true model well inside the training region, however, the baseline NN approach has poor generalization performance with the unseen data. Moreover, both the classical PGNN and the proposed W-PGNN approaches show good learning results outside the training region. However, the performance of the proposed W-PGNN approach is approximately 20% better compared to the classical PGNN (see also Table 1), mainly resulting from the novel weighted-regularization physics-based term in the cost function, which enables the learned model to follow the ground truth within the range of the training data, and in turn, be forced toward the linear prior model within the low-informative data area.",
    "weightregnn-7": "This can also be seen in Fig. 5, where the zoom-in sub-figures show the estimation trajectories inside and outside the training region, respectively. It can be observed that despite the test dataset being much larger than the training dataset the proposed W-PGNN still has the capability of identifying the system in the whole state space with the highest estimation accuracy. Furthermore, a Monte Carlo simulation with 10 runs under random initial parameters is conducted to compare the estimation error of the three approaches. To assess the simulation performance of the identified models, the following root mean squared error (RMSE) on the test dataset is utilized:\n\ne RMSE = 1 N \u200b \u2211 k = 1 N ( y \u200b ( k ) \u2212 y ^ \u200b ( k | \u03b8 ) ) 2 subscript \ud835\udc52 RMSE 1 \ud835\udc41 superscript subscript \ud835\udc58 1 \ud835\udc41 superscript \ud835\udc66 \ud835\udc58 ^ \ud835\udc66 conditional \ud835\udc58 \ud835\udf03 2 e_{\\mathrm{RMSE}}=\\sqrt{\\frac{1}{N}\\sum_{k=1}^{N}(y(k)-\\hat{y}(k|\\theta))^{2}} (15)\n\nTable 1 quantifies the RMSE and its variability of the three considered approaches on the training and test dataset over 10 runs. One can see that the achieved RMSE of the proposed W-PGNN significantly improves and shows better generalization performance on the unseen dataset compared to the baseline NN and classical PGNN. 6 Conclusion\n\nA novel PGNN-based model completion strategy is proposed in this paper for nonlinear state-space model identification. Specifically, we enhance the interpretability and generalization performance of classical PGNN by introducing a weighted function regularization strategy, i.e., the W-PGNN. A new weighted regularization cost function is presented to penalize the difference between the physics and identified model at both the state and output levels in regions with low information content. The proposed strategy provides new perspectives into the fusion of physics-guided and black-box data-driven modeling approaches, especially in cases where the available data is limited. The effectiveness of W-PGNN has been analyzed and demonstrated by numerical simulations and compared with some classical ANN modeling methods. Future work will focus on extending the application scenarios of the proposed W-PGNN method to more complex and larger benchmarks.",
    "weightregnn-8": "References\n\nAmoura et al.",
    "weightregnn-9": "(2011) Amoura, K., Wira, P., and Djennoune, S. (2011). A state-space neural network for modeling dynamical nonlinear systems. In Proc. of the International Conference on Neural Computation Theory and Applications, 369\u2013376. Beintema et al. (2023) Beintema, G.I., Schoukens, M., and T\u00f3th, R. (2023). Deep subspace encoders for nonlinear system identification.",
    "weightregnn-10": "Automatica, 156, 111210. Billings (2013) Billings, S.A. (2013). Nonlinear system identification: NARMAX methods in the time, frequency, and spatio-temporal domains.",
    "weightregnn-11": "John Wiley & Sons. Bolderman et al. (2024) Bolderman, M., Butler, H., Koekebakker, S., van Horssen, E., Kamidi, R., Spaan-Burke, T., Strijbosch, N., and Lazar, M. (2024). Physics-guided neural networks for feedforward control with input-to-state-stability guarantees. Control Engineering Practice, 145, 105851. Fletcher and Reeves (1964) Fletcher, R. and Reeves, C.M. (1964). Function minimization by conjugate gradients. The computer journal, 7(2), 149\u2013154. Fletcher and Powell (1963) Fletcher, R. and Powell, M.J. (1963). A rapidly convergent descent method for minimization. The computer journal, 6(2), 163\u2013168. Forgione and Piga (2020) Forgione, M. and Piga, D. (2020). Model structures and fitting criteria for system identification with neural networks. In Proc. of the 14th International Conference on Application of Information and Communication Technologies, 1\u20136. Hoekstra et al. (2024) Hoekstra, J.H., Verhoek, C., T\u00f3th, R., and Schoukens, M. (2024). Learning-based model augmentation with LFRs. arXiv preprint arXiv:2404.01901. Karpatne et al. (2017) Karpatne, A., Watkins, W., Read, J., and Kumar, V. (2017). Physics-guided neural networks (PGNN): An application in lake temperature modeling. arXiv preprint arXiv:1710.11431, 2. Levenberg (1944) Levenberg, K. (1944). A method for the solution of certain non-linear problems in least squares. Quarterly of applied mathematics, 2(2), 164\u2013168. Paduart et al. (2010) Paduart, J., Lauwers, L., Swevers, J., Smolders, K., Schoukens, J., and Pintelon, R. (2010). Identification of nonlinear systems using polynomial nonlinear state space models. Automatica, 46(4), 647\u2013656. Scarselli and Tsoi (1998) Scarselli, F. and Tsoi, A.C. (1998). Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results. Neural networks, 11(1), 15\u201337. Sch\u00f6n et al. (2018) Sch\u00f6n, T.B., Svensson, A., Murray, L., and Lindsten, F. (2018). Probabilistic learning of nonlinear dynamical systems using sequential Monte Carlo. Mechanical systems and signal processing, 104, 866\u2013883. Sch\u00f6n et al. (2011) Sch\u00f6n, T.B., Wills, A., and Ninness, B. (2011). System identification of nonlinear state-space models. Automatica, 47(1), 39\u201349. Schoukens and Ljung (2019) Schoukens, J. and Ljung, L. (2019). Nonlinear system identification: A user-oriented road map. IEEE Control Systems Magazine, 39(6), 28\u201399.",
    "weightregnn-12": "Schoukens (2021) Schoukens, M. (2021). Improved initialization of state-space artificial neural networks. In Proc. of the European Control Conference, 1913\u20131918. Schoukens and Tiels (2017) Schoukens, M. and Tiels, K. (2017). Identification of block-oriented nonlinear systems starting from linear approximations: A survey. Automatica, 85, 272\u2013292. Suykens et al. (1995) Suykens, J.A., De Moor, B.L., and Vandewalle, J. (1995). Nonlinear system identification using neural state space models, applicable to robust control design. International Journal of Control, 62(1), 129\u2013152. Verdult (2002) Verdult, V. (2002). Nonlinear system identification: a state-space approach. Ph.D. thesis, University of Twente, The Netherlands. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 15:57:41 2024 by LaTeXML"
}