{
    "settransformer-0": "Supplementary Material for Set Transformer\n\nJuho Lee Yoonho Lee Jungtaek Kim Adam R. Kosiorek Seungjin Choi Yee Whye Teh\n\nSet transformer, Set-taking network, Attention-based network, Permutation-invariant network\n\n1 Proofs\n\nLemma 1. The mean operator is a special case of dot-product attention with softmax.",
    "settransformer-1": "Proof. Let and . Att \u200b ( s , X , X ; softmax ) = softmax \u200b ( s \u200b X \u22a4 d ) \u200b X = 1 n \u200b \u2211 i = 1 n x i Att \ud835\udc60 \ud835\udc4b \ud835\udc4b softmax softmax \ud835\udc60 superscript \ud835\udc4b top \ud835\udc51 \ud835\udc4b 1 \ud835\udc5b superscript subscript \ud835\udc56 1 \ud835\udc5b subscript \ud835\udc65 \ud835\udc56 \\displaystyle\\mathrm{Att}(s,X,X;\\mathrm{softmax})=\\mathrm{softmax}\\left(\\frac{sX^{\\top}}{\\sqrt{d}}\\right)X=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\n\nLemma 2. The decoder of a Set Transformer, given enough nodes, can express any element-wise function of the form . Proof. We first note that we can view the decoder as the composition of functions\n\nDecoder \u200b ( Z ) Decoder \ud835\udc4d \\displaystyle\\mathrm{Decoder}(Z) = rFF \u200b ( H ) absent rFF \ud835\udc3b \\displaystyle=\\mathrm{rFF}(H) (1) where H where \ud835\udc3b \\displaystyle\\textrm{where}\\ \\ H = rFF \u200b ( MAB \u200b ( Z , rFF \u200b ( Z ) ) ) absent rFF MAB \ud835\udc4d rFF \ud835\udc4d \\displaystyle=\\mathrm{rFF}(\\mathrm{MAB}(Z,\\mathrm{rFF}(Z))) (2)\n\nWe focus on in (2). Since feed-forward networks are universal function approximators at the limit of infinite nodes, let the feed-forward layers in front and back of the MAB encode the element-wise functions and , respectively. We let , so the number of heads is the same as the dimensionality of the inputs, and each head is one-dimensional. Let the projection matrices in multi-head attention () represent projections onto the jth dimension and the output matrix () the identity matrix. Since the mean operator is a special case of dot-product attention, by simple composition, we see that an MAB can express any dimension-wise function of the form\n\nM p \u200b ( z 1 , \u22ef , z n ) = ( 1 n \u200b \u2211 i = 1 n z i p ) 1 p . subscript \ud835\udc40 \ud835\udc5d subscript \ud835\udc67 1 \u22ef subscript \ud835\udc67 \ud835\udc5b superscript 1 \ud835\udc5b subscript superscript \ud835\udc5b \ud835\udc56 1 superscript subscript \ud835\udc67 \ud835\udc56 \ud835\udc5d 1 \ud835\udc5d \\displaystyle M_{p}(z_{1},\\cdots,z_{n})=\\left(\\frac{1}{n}\\sum^{n}_{i=1}z_{i}^{p}\\right)^{\\frac{1}{p}}. (3)\n\nLemma 3. A PMA, given enough nodes, can express sum pooling .",
    "settransformer-2": "Proof. We prove this by construction. Set the seed to a zero vector and let , where is any activation function such that . The identiy, sigmoid, or relu functions are suitable choices for . The output of the multihead attention is then simply a sum of the values, which is in this case. \u220e\n\nWe additionally have the following universality theorem for pooling architectures:\n\nTheorem 1. Models of the form are universal function approximators in the space of permutation invariant functions. Proof. See Appendix A of Zaheer2017. \u220e\n\nBy Lemma 3, we know that can express any function of the form . Using this fact along with Theorem 1, we can prove the universality of Set Transformers:\n\nProposition 1. The Set Transformer is a universal function approximator in the space of permutation invariant functions.",
    "settransformer-3": "Proof. By setting the matrix to a zero matrix in every SAB and ISAB, we can ignore all pairwise interaction terms in the encoder. Therefore, the can express any instance-wise feed-forward network (). Directly invoking Theorem 1 concludes this proof. \u220e\n\nWhile this proof required us to ignore the pairwise interaction terms inside the SABs and ISABs to prove that Set Transformers are universal function approximators, our experiments indicated that self-attention in the encoder was crucial for good performance. 2 Experiment Details\n\nIn all implementations, we omit the feed-forward layer in the beginning of the decoder () because the end of the previous block contains a feed-forward layer. All MABs (inside SAB, ISAB and PMA) use fully-connected layers with ReLU activations for rFF layers. In the architecture descriptions, denotes the fully-connected layer with units and activation function . denotes the SAB with units and heads. denotes the ISAB with units, heads and inducing points. denotes the PMA with units, heads and vectors. All MABs used in SAB and PMA uses FC layers with ReLU activations for FF layers. 2.1 Max Regression\n\nGiven a set of real numbers , the goal of this task is to return the maximum value in the set .",
    "settransformer-4": "We construct training data as follows. We first sample a dataset size uniformly from the set of integers . We then sample real numbers independently from the interval . Given the network\u2019s prediction , we use the actual maximum value to compute the mean absolute error . We don\u2019t explicitly consider splits of train and test data, since we sample a new set at each time step. We show the detailed architectures used for the experiments in Table 1. We trained all networks using the Adam optimizer (Kingma2015) with a constant learning rate of and a batch size of 128 for 20,000 batches, after which loss converged for all architectures. 2.2 Counting Unique Characters\n\nThe task generation procedure is as follows. We first sample a set size uniformly from the set of integers . We then sample the number of characters uniformly from . We sample characters from the training set of characters, and randomly sample instances of each character so that the total number of instances sums to and each set of characters has at least one instance in the resulting set. We show the detailed architectures used for the experiments in Table 3. For both architectures, the resulting -dimensional output is passed through a activation to produce the Poisson parameter . The role of is to ensure that is always positive. The loss function we optimize, as previously mentioned, is the log likelihood . We chose this loss function over mean squared error or mean absolute error because it seemed like the more logical choice when trying to make a real number match a target integer. Early experiments showed that directly optimizing for mean absolute error had roughly the same result as optimizing in this way and measuring . We train using the Adam optimizer with a constant learning rate of for 200,000 batches each with batch size 32. 2.3 Solving maximum likelihood problems for mixture of Gaussians\n\n2.3.1 Details for 2D synthetic mixtures of Gaussians experiment\n\nWe generated the datasets according to the following generative process.",
    "settransformer-5": "1. Generate the number of data points, . 2. Generate centers. \u03bc j , d \u223c Unif \u200b ( \u2212 4 , 4 ) , j = 1 , \u2026 , 4 , d = 1 , 2 . formulae-sequence similar-to subscript \ud835\udf07 \ud835\udc57 \ud835\udc51 Unif 4 4 formulae-sequence \ud835\udc57 1 \u2026 4 \ud835\udc51 1 2 \\displaystyle\\mu_{j,d}\\sim\\mathrm{Unif}(-4,4),\\quad j=1,\\dots,4,\\,\\,\\,d=1,2. (4)\n\n3. Generate cluster labels. \u03c0 \u223c Dir \u200b ( [ 1 , 1 ] \u22a4 ) , z i \u223c Categorical \u200b ( \u03c0 ) , i = 1 , \u2026 , n . formulae-sequence similar-to \ud835\udf0b Dir superscript 1 1 top formulae-sequence similar-to subscript \ud835\udc67 \ud835\udc56 Categorical \ud835\udf0b \ud835\udc56 1 \u2026 \ud835\udc5b \\displaystyle\\pi\\sim\\mathrm{Dir}([1,1]^{\\top}),\\quad z_{i}\\sim\\mathrm{Categorical}(\\pi),\\,\\,i=1,\\dots,n. (5)\n\n4. Generate data from spherical Gaussian. x i \u223c \ud835\udca9 \u200b ( \u03bc z i , ( 0.3 ) 2 \u200b I ) . similar-to subscript \ud835\udc65 \ud835\udc56 \ud835\udca9 subscript \ud835\udf07 subscript \ud835\udc67 \ud835\udc56 superscript 0.3 2 \ud835\udc3c \\displaystyle x_{i}\\sim{\\mathcal{N}}(\\mu_{z_{i}},(0.3)^{2}I). (6)\n\nTable 4 summarizes the architectures used for the experiments. For all architectures, at each training step, we generate 10 random datasets according to the above generative process, and updated the parameters via Adam optimizer with initial learning rate . We trained all the algorithms for steps, and decayed the learning rate to after steps. Table 5 summarizes the detailed results with various number of inducing points in the ISAB. Figure LABEL:fig:synthetic_clustering shows the actual clustering results based on the predicted parameters. 2.3.2 2D Synthetic Mixtures of Gaussians Experiment on Large-scale Data\n\nTo show the scalability of the set transformer, we conducted additional experiments on large-scale 2D synthetic clustering dataset. We generated the synthetic data as before, except that we sample the number of data points and set . We report the clustering accuracy of a subset of comparing methods in Table 6. The set transformer with only 32 inducing points works extremely well, demonstrating its scalability and efficiency. 2.3.3 Details for CIFAR-100 amortized clutering experiment\n\nWe pretrained VGG net (Simonyan2014) with CIFAR-100, and obtained the test accuracy 68.54%. Then, we extracted feature vectors of 50k training images of CIFAR-100 from the 512-dimensional hidden layers of the VGG net (the layer just before the last layer). Given these feature vectors, the generative process of datasets is as follows. 1. Generate the number of data points, . 2. Uniformly sample four classes among 100 classes. 3. Uniformly sample data points among four sampled classes. Table 7 summarizes the architectures used for the experiments. For all architectures, at each training step, we generate 10 random datasets according to the above generative process, and updated the parameters via Adam optimizer with initial learning rate . We trained all the algorithms for steps, and decayed the learning rate to after steps. Table 8 summarizes the detailed results with various number of inducing points in the ISAB. 2.4 Set Anomaly Detection\n\nTable 9 describes the architecture for meta set anomaly experiments. We trained all models via Adam optimizer with learning rate and exponential decay of learning rate for 1,000 iterations. 1,000 datasets subsampled from CelebA dataset (see Figure LABEL:fig:celeba_figures) are used to train and test all the methods. We split 800 training datasets and 200 test datasets for the subsampled datasets. 2.5 Point Cloud Classification\n\nWe used the ModelNet40 dataset for our point cloud classification experiments. This dataset consists of a three-dimensional representation of 9,843 training and 2,468 test data which each belong to one of 40 object classes. As input to our architectures, we produce point clouds with points each (each point is represented by coordinates). For generalization, we randomly rotate and scale each set during training. We show results our architectures in Table 10 and additional experiments which used points in Table LABEL:table:pointcloud. We trained using the Adam optimizer with an initial learning rate of which we decayed by a factor of every 20,000 steps. For the experiment with 5,000 points (Table LABEL:table:pointcloud), we increased the dimension of the attention blocks ( instead of ) and also decayed the weights by a factor of . We also only used one ISAB block in the encoder because using two lead to overfitting in this setting. 3 Additional Experiments\n\n3.1 Runtime of SAB and ISAB\n\nWe measured the runtime of SAB and ISAB on a simple benchmark (Figure 1). We used a single GPU (Tesla P40) for this experiment. The input data was a constant (zero) tensor of three-dimensional vectors. We report the number of seconds it took to process 10,000 sets of each size. The maximum set size we report for SAB is 2,000 because the computation graph of bigger sets could not fit on our GPU.",
    "settransformer-6": "The specific attention blocks used are and . \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Mar 1 18:15:01 2024 by LaTeXML"
}