{
    "ssmformal-0": "# The Expressive Capacity of State Space Models: A Formal Language Perspective \n\nYash Sarrof, Yana Veitsman, Michael Hahn<br>Saarland Informatics Campus<br>Saarland University, Germany<br>\\{ysarrof, yanav, mhahn\\}@lst.uni-saarland.de\n\n\n#### Abstract\n\nRecently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers.",
    "ssmformal-1": "However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.",
    "ssmformal-2": "The map $\\phi\\left(h_{t}, x_{t}\\right)$ includes a cascade of channel-mixing transformations and normalization, which we abstract as follows:\n\n$$\n\\phi\\left(h_{t}, x_{t}\\right)=\\operatorname{Mix}_{1}\\left(\\operatorname{Norm}\\left(\\operatorname{Mix}_{2}\\left(h_{t}, x_{t}\\right)\\right), x_{t}\\right)\n$$\n\nwhere $\\operatorname{Mix}_{j}(\\cdot)$ can contain linear or (Swi)GLU components [e.g. 56, 23]. We will take Norm to implement RMSNorm [75]; LayerNorm [4] can be covered by absorbing centering into Mix 2 . A Full SSM Real-world SSMs typically stack several layers of the form (1,2). Where needed, we use superscripts to indicate the layers in an $\\mathrm{SSM}: h_{t}^{(1)}, \\ldots, h_{t}^{(L)}$, where $L$ is the number of layers. We consider input words $\\mathbf{w}=w_{1 \\ldots|w|}$ over a discrete alphabet $\\Sigma$, and assume an encoding in terms of token embeddings $e(\\sigma) \\in \\mathbb{R}^{d}$, for $\\sigma \\in \\Sigma$. We will also write $e_{\\sigma}$ for $e(\\sigma)$. These feed into the lowest layer as $x_{t}^{(1)}:=e\\left(w_{t}\\right)$. The outputs of each layer feed into the next layer, as $x_{t}^{(l+1)}=z_{t}^{(l)}$. The transformations in (1) are specific to each layer: $A^{(1)}, \\ldots, A^{(L)}$ and similarly for $B, \\phi$. To keep notation simple, we will only show the superscripts where necessary for disambiguation. The activations $z_{t}^{(L)}$ at the highest layer are read out by some neural network $\\rho$ into vectors $q_{t} \\in \\mathbb{R}^{d_{p r e d}}$ describing classification or next-token predictions. We again take $\\rho$ to be an arbitrary function; importantly, all our constructions will allow $\\rho$ to operate correctly even at finite precision. Implementation Choices In Mamba, (1) directly maps onto Eqs. (2a) and (2b) in Gu and Dao [23]. The notation of Gu and Dao [23] use a matrix multiplication $\\bar{A} h_{t-1}$ instead of elementwise multiplication $A\\left(x_{t}\\right) \\circ h_{t-1}$ in 1), but importantly, Mamba's $\\bar{A}$ is diagonal, so we can take $A\\left(x_{t}\\right)_{i}=\\bar{A}_{i i}$. Some SSMs assume nondiagonal $A\\left(x_{t}\\right)$, but typically this matrix is diagonalizable [e.g. 24, 67], so that the SSM is still equivalent to one of the form (1). We discuss how other SSMs instantiate (1) in Appendix A. Some models assume complex-valued activations (Appendix A); our results largely do not depend on this distinction, but take it into account where needed (Theorem 13). Some SSMs [e.g. 23] use different numbers of channels in $x_{t}$ and $h_{t}$ using state expansion; as this does not affect expressive capacity, we will simply assume a constant dimensionality $d$. Local convolutions [e.g. 21] can be simulated with an SSM layer and do not increase expressive capacity (Remark 18). We will find that two design choices have nontrivial impact on expressive capacity: The first one is time invariance: we call an SSM time-InVARIANT if $A\\left(x_{t}\\right)$ does not depend on $x_{t}$. Some SSMs, such as S4 [24] and Retnet [67] are time-invariant; Mamba [23], Griffin [14], GLA [72], HGRN [57, 56], QRNN/SRU [8, 41] are not (Appendix A]. The second one is the sign of the entries of $A\\left(x_{t}\\right)$ : Across all non-time-invariant SSMs surveyed, we find that the gate is always nonnegative (Appendix A): $A\\left(x_{t}\\right) \\geq 0$ (NONNEGATIVE) due to exponential or sigmoid parameterizations of the gate - this choice turns out to limit expressive capacity (Theorem 2). Role of Parameterization While the abstract form (1,2) is common across the SSM literature, differences in parameterization may have substantial effect on efficiency and training stability. Studying expressiveness allows us to abstract away from these differences to a remarkable degree: We will allow $A, B, \\rho$ to be arbitrary functions with the given input-output properties. Our negative results are based on abstract properties of the setup (1-2), which fundamentally bottlenecks SSMs through elementwise linear state updates. For our positive results, will use empirical learnability experiments to verify that learnable solutions instantiating them (though not necessarily implementing the same constructions as used in the proofs) do exist in a recent SMM [Mamba, 23]. Traditional RNNs We contrast SSMs with traditional RNNs such as simple RNNs or LSTMs: for these, the recurrence in Eq. (1) is replaced by\n\n$$\nh_{t}=\\psi\\left(h_{t-1}, x_{t}\\right)\n$$\n\nwhere $\\psi$ could be linear, an MLP [17], or a more complex gated function [29]. Finite Precision Assumption While Eq. (1) assumes arbitrary real-valued activations, real-world implementations can only represent numbers with bounded precision. Formally, we adopt the finite precision notion used by Weiss et al. [70] in a study of the expressive power of traditional RNNs: We allow an unbounded number of integer bits, but only $p$ fractional bits, independent of the length of the input. See Appendix Efor discussion. ## 3 Theoretical Results\n\n### 3.1 Motivation\n\nWe study three foundational types of data structures needed for modeling formal languages [30]: finite state automata (Theorem 1, 2, 4), counters (Theorem 5), and stacks (Theorem 6). All of these data structures can be understood in two equivalent forms: One is to track a state sequence over an input, where each input symbol engenders a specific transformation on the state. The other onemore commonly considered in research on expressive capacity and closer to language modelingconsiders formal languages - sets of finite strings that are defined by the property that an automaton reaches one of a pre-specified set of \"accepting\" states after traversing the word. We will focus on the latter perspective, enabling easy comparison with existing results on transformers and RNNs. A finite-state-automaton (see Definition 77 represents a general state tracking problem over a finite state space, without imposing further structure on the state space: The automaton keeps track of a single state from a finite state space; when reading a string from left to right, each symbol engenders a specific transformation of the state. At each position, the current state determines which symbols can come next; membership in a formal language is determined by the state reached after reading the full string. Finite-state-automata are equivalent in expressivity to regular expressions, and define the regular languages [37]. Indeed, we will be able to provide a precise criterion identifying which such finite state tracking problems-or equivalently, regular languages-SSMs such as Mamba are capable of in the finite-precision setting (Theorem 4). Allowing an automaton to keep track of one or more counters [20]-integers that are incremented or decremented at each symbol read-turn the state space infinite, but in a highly structured manner. SSMs can model this datastructure (Theorem 57, as can RNNs and transformers [70, 6]. Stacks, a first-in-first-out datastructure, enable automata to keep track of hierarchical structure, foundational to natural language [11]. We show that SSMs can implement shortcut solutions to bounded hierarchical structure even without implementing a stack (Theorem 6) - these are likely to be most useful to natural language given the boundedness of human memory [50, 36]. ![](https://cdn.mathpix.com/cropped/2024_09_17_101ff39892b808f88e98g-04.jpg?height=188&width=1071&top_left_y=291&top_left_x=516)\n\nFigure 1: Three key formal languages: prefixes with the sets of possible next characters: Flip Flop (Theorem1), PARITY (Theorem 2), bounded-depth Dyck (Theorem6. In Flip Flop, after a r (read) instruction, the bit must match what came after the last w (write) instruction (here, 0). For PARITY, EOS can only follow when the number of ones in the prefix is even. For bounded-depth Dyck, a closing bracket can only appear if it matches the last unclosed opening bracket (here, \")\" matches \"(\")). Opening brackets can appear as long as the maximum depth (here, 5) hasn't been reached. ### 3.2 Background: Formal Language Prediction and Recognition\n\nWe fix a finite alphabet $\\Sigma$. Its elements are called characters or symbols. The set of all finite strings $\\mathbf{w}$ over $\\Sigma$ is denoted $\\Sigma^{*}$; such strings are often referred to as words. The length of $\\mathbf{w}$ is denoted $|\\mathbf{w}|$. A formal language $L$ is a subset of $\\Sigma^{*}$. Techically, we assume that the alphabet includes BOS and EOS symbols, which occurs at the beginning and end of each element of $L$ and nowhere else. We next need to define what it means for an SSM to model a formal language. A standard notion is the notion of recognition, where the task is to classify a full string as belonging to the language or not. Formally, for an SSM with $d_{\\text {pred }}=1$, we say that it recognizes a language $L$ if the output $\\rho\\left(z_{|\\mathbf{w}|}^{(L)}\\right)$ equals-when the SSM is run on $\\mathbf{w} \\in \\Sigma^{*}-1$ if $\\mathbf{w} \\in L$ and 0 else. However, such a classification task is arguably not always matched to dominant use cases in predictive sequence modeling, where the task is to predict the next token at each step. Thus, we also cast formal languages into a language modeling and sequence prediction framework. We adopt the task of Bhattamishra et al. [6], where the model is asked to output at each step in a sequence the set of possible next symbols. Let $\\operatorname{Prefix}(L):=\\left\\{w: w \\in \\Sigma^{*}, w \\Sigma^{*} \\cap L \\neq \\emptyset\\right\\}$ the set of valid prefixes of $L$. We then say that a model predictively models a language $L$ if (Figure 1), given a valid prefix $w \\in \\operatorname{Prefix}(L)$, it outputs the finite set\n\n$$\n\\left\\{\\sigma \\in \\Sigma: w \\sigma \\Sigma^{*} \\cap L \\neq \\emptyset\\right\\}\n$$\n\nWe think of each such set as an atomic label; the set of possible labels is the power set of the finite alphabet $\\Sigma$. Here, $d_{\\text {pred }}=2^{|\\Sigma|}$. Predictive modeling can be easily converted into recognition by checking whether any symbol in the sequence is not in the predictive set at the preceding position; this can be done by adding 1 SSM layer. Conversely, if we can show that SSMs cannot recognize a language, this proves they also cannot perform predictive modeling for it, as they then cannot correctly predict where EOS can appear. To get the strongest results, we thus prove positive results for predictive modeling, and negative results for recognition. ### 3.3 Length-Generalizing Representations for Flip-Flop State Tracking\n\nAs motivated in Section 3.1, we begin with finite-state-automata - equivalently, regular languages, a well-studied setting for understanding the expressive power of transformers [e.g. 25, 6, 3, 44, 45] and a fundamental model of state tracking when the number of possible states is finite [45, 49]. Traditional RNNs can emulate all finite-state-automata at finite precision [31, 32]. Starting from two specific languages, Flip Flop (Section 1) and PARITY (Section 2), we derive an exact characterization of the regular languages modeled by a broad class of SSMs at finite precision (Theorem 4). We begin on the positive side, by establishing a case where SSMs pattern with RNNs in avoiding a failure mode of self-attention. The Flip Flop languages [45] are a simple instance of state tracking defined in terms of write, read, and ignore instructions. Each write instruction comes with a piece of information; whenever a read instruction is encountered, the information written by the last write instruction is recalled. Formally, define $\\mathcal{L}_{F F}$ to be the set of finite strings $\\mathbf{x}$ over $\\Sigma=\\{\\mathrm{r}, \\mathrm{w}, \\mathrm{i}, 0,1\\}$, where $x_{1}, x_{3}, \\cdots \\in\\{\\mathrm{r}, \\mathrm{w}, \\mathrm{i}\\}, x_{2}, x_{4}, \\cdots \\in\\{0,1\\}$, and where the bit following any r matches the bit\nfollowing the last preceding occurrence of w. Liu et al. [44] show that the Flip Flop language, as an abstraction, is a fundamental ingredient of many long-range reasoning settings. It can be represented with a small finite-state-automaton, and LSTMs learn $\\mathcal{L}_{F F}$ well [45]. Transformers can in principle represent it [44, 45], though known constructions are not inherently length-generalizing. Indeed, transformers empirically struggle in generalization [45]; intuitively, this may happen because attention heads aggregate information in a commutative manner, and reliably attending to the last write instruction requires strong position dependence in the attention weights. SSMs pattern with traditional RNNs: they can easily represent Flip Flop at arbitrary input lengths:\nTheorem 1. A two-layer SSM with finite precision predictively models $\\mathcal{L}_{F F}$ at arbitrary lengths. In the construction, the first layer records the last instruction token, achieved in (1) by setting $A(e(\\mathrm{r}))=A(e(\\mathrm{w}))=A(e(\\mathrm{i}))=0$, and $A(e(0)=A(e(1))=1$, and setting $B(e(0))=B(e(1))=0$. Additional dimensions forward the current token to $h_{t}^{(1)}$. In the output of the first layer $z_{t}^{(1)}$, whenever the input is 0 or 1 , the model now has access both to the current token $w_{t}$ and the preceding token $w_{t-1}$, which must have been an instruction. Based on this information, the model can set the gate to overwrite the state $h_{t-1}^{(2)}$ with the current input token when the preceding token was w , and pass along the state $h_{t-1}^{(2)}$ unaltered otherwise. This, together with $z_{t}^{(1)}$, is sufficient for always identifying the legal next symbols in $\\mathcal{L}_{F F}$.",
    "ssmformal-3": "The formal proof is in Appendix B. 1 . ### 3.4 Difficulty of PARITY\n\nConversely, we next establish a design choice in SSMs which limits their power in emulating finitestate-automata, establishing - in the finite-precision setting - an even stronger separation between existing SSM variants and traditional RNNs than the circuit complexity arguments in Merrill et al. [49]. The language PARITY, the language of bitstrings with an even number of ones, is recognized by a finite-state automaton with 2 states, and is straightforwardly encoded into a traditional RNN, even a linear one, with finite precision. PARITY, while in principle expressible for transformers [9], is empirically very hard to learn for transformers [6, 15], as it can provably only be represented in sharp minima [26]. A sufficiently general SSM could easily recognize it at $d=1$ by setting $h_{0}=1$, $A\\left(e_{1}\\right)=-1, A\\left(e_{0}\\right)=0, B \\equiv 0$, so that the sign of the single entry of $h_{t}$ indicates the parity. Such an SSM would need to be non-time-invariant and require negative or complex gate values; i.e., satisfy neither TIME-INVARIANT nor NONNEGATIVE. Such properties are indeed provably needed:\nTheorem 2. No SSM satisfying NONNEGATIVE can recognize PARITY at arbitrary input lengths with finite precision. In particular, this applies to Mamba. The proof is in Appendix B.2 , it examines inputs of the form $1^{N}$ and shows that the activations $z_{N}$ converge as $N \\rightarrow \\infty$, and thus cannot reliably encode the parity of $N$. As we show in Theorem 13 . the same result holds even for SSMs evading NONNEGATIVE when they are TIME-INVARIANT, at least when the coefficients have rational angles in the complex planes. All extant SSMs we surveyed (Appendix, Section A) satisfy either NONnEGATIVE or TIME-INVARIANT. Hypothetical SSMs evading both NONNEGATIVE and TIME-INVARIANT would be strictly stronger and can represent not only PARITY, but all regular languages known to be in $\\mathrm{TC}^{0}$ (Theorem 21 . ### 3.5 Which Regular Languages can SSMs Model? We now combine Theorems 1 and 2 to derive an exact characterizations of the regular languages that modern non-time-invariant SSMs such as Mamba can recognize or predictively model - the two notions turn out to coincide here - in the finite-precision setting. The key insight is that $\\mathcal{L}_{F F}$ and PARITY are fundamental building blocks of two classes of regular languages: the star-free languages and their complement, the non-star-free languages [60, 46]:\nDefinition 3. A regular language is star-free if it can be defined using regular expressions involving only the empty set, the empty string, individual symbols, concatenation, and Boolean combinations - avoiding the Kleene star operation. $\\mathcal{L}_{F F}$ is star-free: there is a way to define it without Kleene star. PARITY is not star-free; any regular expression for it must involve the Kleene star. Even languages whose intuitive definition involves the Kleene star may turn out to be star-free. For example, $(01)^{*}$ has an expression avoiding the star ${ }^{1}$\n\n[^0]Equivalently, a language is star-free if it can be defined logically using only first-order quantifiers and the order relation [60]. Also, $\\mathcal{L}$ is non-star-free if and only if recognizing it involves counting modulo some $K$ [46]; PARITY being the simplest example. Modern non-time-invariant SSMs such as Mamba cannot perform modulo counting, but they can model all star-free languages:\n\nTheorem 4. The class of SSMs satisfying NONNEGATIVE can predictively model a regular language $\\mathcal{L}$ at finite precision if and only if $\\mathcal{L}$ is star-free. The proof in Appendix B. 3 uses the Krohn-Rhodes theorem [39] to reduce all star-free languages to flip flop-like state tracking. Importantly, there are well-known constructive criteria for deciding whether a given automaton defines a star-free language [60]; hence, we have a decidable criterion for the finite-state tracking problems that such SSMs satisfying NONNEGATIVE can solve. This is much simpler than the situation for transformers, where an exact characterization of their power within the regular languages is complicated: Angluin et al. [3] show that a certain formal abstraction of transformers (masked unique hard attention) also recognizes exactly the star-free languages, but constructions of realistic transformers via Krohn-Rhodes in Liu et al. [44] do not inherently length generalize, and empirical research indicates difficulty in generalization even for certain simple star-free languages [6, 45]. Known length-generalizing constructions are limited to very simple subclasses such as the piecewise testable languages [71]. In contrast, for SSMs we have a single model per language, at finite precision and for arbitrarily long inputs. Thus, we expect that the SSM architecture confers an advantage in star-free state tracking problems when compared to transformers - a prediction we will find supported experimentally (Figure 3). ### 3.6 Unbounded Counting\n\nHaving characterized the regular languages modeled by SSMs, we now consider languages requiring unbounded counting [20], specifically, languages recognized by keeping track of one or more counters, where each character causes a specific increment or decrement to each counter [38, 27, 70, 40]. A prime example is the Dyck-1 language of well-formed strings over \"(\" and \")\"; here a counter is incremented (decremented) whenever an opening (closing) bracket is encountered; a string is well-formed if and only if the counter is 0 at the end of the string. Some other relevant formal languages are Shuffle-Dyck- $k$ (the shuffles of multiple Dyck-1 languages), $a^{n} b^{n}$ - here, $a$ increments the counter and $b$ decrements it, and $a^{n} b^{n} c^{n}$ - here, there are two counters, one keeping track of $a^{n} b^{n}$ and one of $b^{n} c^{n}$ (See Appendix C.2). Such counter languages are fundamental as basic contextfree (Dyck-1, $a^{n} b^{n}$ ) or context-sensitive (e.g., $a^{n} b^{n} c^{n}$ ) languages [30], and have been the subject of studies of both transformers [6] and RNNs [70]. Many such languages are modeled by SSMs:\n\nTheorem 5. Consider the languages Dyck-1, Shuffle-Dyck-k, n-ary Boolean Expressions, $a^{n} b^{n}$, $a^{n} b^{n} c^{n}$, and $a^{n} b^{n} c^{n} d^{n}$, as defined in Appendix C.2 Then each of these is predictively modeled by an SSM. The proof is in Appendix B.4. Intuitively, an SSM can directly implement the required counters by setting $A \\equiv 1$ and by defining $B\\left(e_{\\sigma}\\right)$ to the be increment or decrement cased by $\\sigma$. In modeling such languages, SSMs pattern with both transformers [6] and LSTMs [70]. It may seem counterintuitive that NONNEGATIVE SSMs can perform unbounded counting but (by Theorem 2 not modular counting-the latter would seem to just require reading out the value of an unbounded counter. What is key is that, even though $h_{t}$ can encode unbounded counts, reading out the modular value of an unbounded integer is a formidable problem for typical neural network nonlinearities, in particular when the information has been pushed through normalization (2). We should note that there is a qualitative difference between this result and the preceding positive results about finite-state languages (Theorems 1 and 4), in that the construction in Theorem 5uses unboundedly large entries in the state $h_{t}$, whereas Theorems 1 and 4 use bounded values at finite precision. Indeed, we will find better length generalization in the finite-state case (Figure 3). A consequence of Theorem 5 is that SSMs can recognize some languages transcending the contextfree languages, as $a^{n} b^{n} c^{n}$ is not context-free. A second application of the theorem, of great linguistic interest, is to bounded hierarchical structure, as we discuss next. ### 3.7 Bounded Hierarchical Structure without Stacks\n\nIt is generally agreed that hierarchical structure is a key aspect of language, and that comprehending language at a human-like level requires the computational ability to process such structures [12, 43, 18]. The fundamental data structure for processing hierarchical structure is the stack, where information is stored and removed as one traverses to higher and lower levels of hierarchical embedding [30]. We now show that SSMs' counting ability can offer shortcuts even on such languages modeling hierarchical structure, eschewing the need for a stack. A useful abstraction of hierarchical structure as relevant to natural language is the family of Dyck languages. The bounded-depth Dyck language $D y c k_{K, h}$ with $K$ types of parentheses and depth $h$ is the language of well-bracketed strings over $(1,)_{1}, \\ldots,(K,)_{K}$, such that the number of yet unclosed brackets never exceeds $h$ in any prefix [28, 74]. The classical unbounded-depth Dyck languages - the limit where $h \\rightarrow \\infty$ - play a fundamental role as the backbone of context-free languages via the ChomskySch\u00fctzenberger theorem [12]. Bounding the depth reflects the fact that deep embedding is rare in natural language [36, 7]. Prior work has found that two-layer transformers [73] and traditional RNNs [28, 6] both model all $D y c k_{K, h}$ languages. The same turns out to hold for SSMs:\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_101ff39892b808f88e98g-07.jpg?height=424&width=442&top_left_y=371&top_left_x=1248)\n\nFigure 2: Test error on the validation set for $\\mathcal{L}_{F F}$, in the setup of Liu et al. [45]. On both the in-distribution (green) and out-of-distribution (orange) setting, Mamba achieves near-zero test error, in agreement with Theorem 1 , and avoiding the failures of transformers observed by Liu et al.",
    "ssmformal-4": "45]. Theorem 6. The bounded-depth Dyck language Dyck ${ }_{K, h}$ is predictively modeled by a two-layer SSM at finite precision, with $d=O(h \\log K)$. The proof is in Appendix B.5 Intuitively, the first layer records the depth of each parenthesis using the ideas from Theorem 5, and the second layer keeps track of the last open bracket at each depth using Theorem 1. We note that, since $D y c k_{K, h}$ is star-free, Theorem 4 already guarantees the existence of representing SSMs, but the depth and width guaranteed by Theorem 6is likely to be much better than what would be obtained by a black-box application of Theorem 4. As Hewitt et al. [28] show, $h \\log K$ units is optimal up to constants and is attained by traditional RNNs and LSTMs. The SSM construction is very different from that of Hewitt et al. [28] for traditional RNNs (both simple RNNs and LSTMs), which directly simulates a stack. Our construction is similar to the transformer construction in Theorem 4.2 in [73], which however has to rely on specific positional encodings, unlike the SSM construction. This highlights that stacks are not the only way of simulating bounded hierarchical structure in recurrent architectures, and non-stack-based strategies can even attain the same optimal scaling of hidden units. Probing whether such stack-free shortcuts are learned by SSM-based LLMs is an exciting problem for future research. ## 4 Experiments\n\nWe have derived a fine-grained theoretical characterization of expressiveness strengths and limitations of SSMs. We now show that our positive results can be instantiated and learned in a realistic SSM implementation, by evaluating a recent highly successful SSM, Mamba [23]. FlipFlop We empirically instantiate Theorem 1]using the dataset released by [45], reflecting the language $\\mathcal{L}_{F F}$ as defined in Section 3.3 Matching Figure 2 in Liu et al. [45], we evaluated both with in-distribution data, and with out-of-distribution data where the distance between read and write instructions tended to be larger. We evaluate for predicting the bits following $r$ instructions ${ }^{2}$ matching the \"deterministic/clean\" mode of Liu et al. [45], and considered predictions to be correct only if all predictions within a sequence were correct. We provide further details in Appendix D. 2\n\n[^1]![](https://cdn.mathpix.com/cropped/2024_09_17_101ff39892b808f88e98g-08.jpg?height=315&width=1389&top_left_y=249&top_left_x=368)\n\nFigure 3: Results on 27 formal languages, comparing our Mamba results (blue) with transformer results reported by [6] (orange), on in-distribution lengths (solid) and out-of-distribution lengths (dotted). As predicted by Theorem 4. Mamba performs strongly on star-free languages, and even shows perfect length generalization. Again as predicted by Theorem 4 it performs poorly on nonstar-free languages. Results for transformers from [6] are mixed. Mamba also succeeds on learning the counter languages from Theorem 5, showing perfect accuracy at in-distribution lengths at indistribution lengths, but length generalization lags behind transformers. A small one-layer ${ }^{3}$ Mamba model converged to 0 error in both validation sets after $\\sim 1400$ steps (Figure 2), compared to 500 steps for an LSTM reported by Liu et al.",
    "ssmformal-5": "[45]. In contrast, Liu et al. [45] found transformers to keep making occasional mistakes despite training for 10 K steps. Test Suite from Bhattamishra et al. [6] To test our theoretical results on regular and counter languages (Theorems 2, 4, 57, we test Mamba on 27 formal languages, including 18 regular languages and 9 counter languages, based on a prior study comparing transformers and RNNs [6]. The regular languages include a popular benchmark [68] and various regular expressions; 11 are star-free. The counter languages include the languages covered by Theorem 5 Definitions of all languages are provided in Appendix C We chose this test suite because it precisely covers Theorems 4 and 5 , and we have proven (in)expressibility results for each language in the set. Following [6], we trained the model for predictive modeling, i.e., at each step, the model outputs a label indicating the set of possible next characters (4), including EOS when required. Following [6], we count the model's response on an input string as correct if and only if predictive modelling was successful at all positions in the input. Such a evaluation setup makes random baselines fairly low, where a random predictor would have an accuracy exponentially small in $N$ in each of the $N$ positions. Training inputs have length in $[1,50]$; the model is then evaluated on held-out bins with length $[1,50]$ and $[51,100]$.",
    "ssmformal-6": "Further experimental details are in Appendix D. 1 . We show our Mamba results, together with Transformer results reported by Bhattamishra et al. [6], in Figure 3 , LSTMs perform perfectly on all languages, and are thus not shown. In a striking confirmation of Theorem 4 , Mamba learns all star-free languages with strong length generalization, and does poorly on all non-star-free languages. Transformers show somewhat more mixed performance, and do not always length-generalize even on the star-free languages. In agreement with Theorem 5 , Mamba, similar to Transformers, learns the counter languages, though it struggles with length generalization when compared to transformers. The differences in Mamba's performance between star-free and counter languages may stem from the fact that\n\n[^2]the construction for the former class (Theorem 4) is able to use finite precision and bounded state values at arbitrary input lengths, while the latter (Theorem5) uses unbounded state values. Bounded Hierarchical Structure To test Theorem 6, we recreate the experimental setup from Yao et al. [74]. Matching their Figure 4, we trained Mamba to predictively model $D y c k_{K, h}$ at $K=8$ and $h=10$. The training and the validation set contained samples of length $\\leq 700$, while the test set contained samples of length $700 \\leq n \\leq 1400$. Yao et al. [74] found both transformers and LSTMs achieved strong performance on this setup. We provide further details in Appendix D.3. Recall that Theorem 6 shows that two-layer SSMs can predictively model $D y c k_{K, h}$. We trained Mamba with 1 or 2 layers and varying dimensionality, finding that two layers can achieve essentially perfect performance across model sizes, even on the test set (Figure 4 and 57. ## 5 Discussion\n\nRelated Work Our work belongs to an incipient line of research into the expressiveness of SSMs [33, 49]. It is closely related to a long string of work studying the expressive capacity of neural sequence models, which has so far focused on recurrent networks [e.g. 62, 6, 28] and, more recently, self attention [e.g. 10, 48, 66]. A second link is to the classical and long-standing study of linear dynamical systems and control theory [34]. For instance, Theorem 2 relies the asymptotic convergence of an SSM on certain inputs, establishing a link to the asymptotics of linear systems [e.g. 55]. Take-Aways While theoretical in nature, our results have several actionable implications for SSM and LLM research, informing the rapidly growing research on SSM-based LLMs. First, encouragingly, SSMs can keep track of bounded hierarchical structure with optimal memory even without explicitly implementing a stack (Theorem 6), suggesting that simple diagonal linear state updates may be sufficiently powerful for modeling the hierarchical structure of language. Second, SSMs resolve a basic failure mode of self-attention in flip-flop state tracking while being parallellizable (Theorem 1). Overall, SSMs and attention have overlapping but distinct strengths. This lends support to the development of hybrid architectures interleaving SSM and attention layers, as instantiated very recently by Jamba [42]. Third, nonnegative gates as obtained by exponential or sigmoid parameterizations provably restrict expressive capacity, even in non-time-invariant SSMs (Theorem 2). While [23] found no evidence that complex-valued paramerizations improved over real-valued ones in the language modality, our results suggest revisiting this question, at least for tasks where periodic state-tracking abilities may be important. Fourth, while exactly characterizing the capacity of transformers has proven difficult even in the finite-state case, Theorem 4 provides a decidable characterization of the regular languages - equivalently, finite-state tracking problems - that SSMs such as Mamba can model. Such decidable characterizations may make it easier to theoretically predict abilities and anticipate failures of LLMs; exploring the implications of this characterization in more realistic setups is an exciting direction for future research. Limitations The main limitation of our theoretical results is that they focus on in-principle expressiveness, and do not directly make statements about learning and generalization. Future work could address this, for example, by examining whether our constructions result in reasonably flat minima, or by studying gradient flow dynamics. While we empirically verified that our positive results can indeed be instantiated, in a learnable manner, in one realistic SSM implementation, implementational differences might still result in practical differences between implementations. Studying the role of such implementational differences is an interesting problem for future work; we have made a first step by theoretically elucidating the implications of nonnegative gate values. ## 6 Conclusion\n\nWe have studied the expressive capacity of modern state space models (SSMs), through the lens of automata and formal languages. We have shown theoretically that SSMs can express star-free languages, a range of counter languages, and bounded hierarchical structure. By providing rigorous results about the expressiveness of the SSM architecture, our results can provide guidance to work on SSM-based language models. ## Acknowledgments\n\nWe thank Mark Rofin for useful discussion about Theorem 2\n\n## References\n\n[1] E.",
    "ssmformal-7": "Aky\u00fcrek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Arhitectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.",
    "ssmformal-8": "[2] J. Almeida. Finite semigroups and universal algebra, volume 3. World Scientific, 1995. [3] D. Angluin, D. Chiang, and A. Yang. Masked hard-attention transformers and boolean rasp recognize exactly the star-free languages.",
    "ssmformal-9": "arXiv preprint arXiv:2310.13897, 2023. [4] J. L. Ba, J. R. Kiros, and G.",
    "ssmformal-10": "E. Hinton. Layer normalization. stat, 1050:21, 2016. [5] D. A. M. Barrington, K. Compton, H. Straubing, and D. Th\u00e9rien. Regular languages in nc1. Journal of Computer and System Sciences, 44(3):478-499, 1992. [6] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096-7116, 2020. [7] D. Blasi, R. Cotterell, L. Wolf-Sonkin, S. Stoll, B. Bickel, and M. Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3938-3943, 2019. [8] J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016. [9] D. Chiang and P. Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654-7664, 2022. [10] D. Chiang, P. Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer encoders.",
    "ssmformal-11": "2023. [11] N. Chomsky. Syntactic structures, 1957. [12] N. Chomsky and M. P. Sch\u00fctzenberger. The algebraic theory of context-free languages. In Studies in Logic and the Foundations of Mathematics, volume 35, pages 118-161. Elsevier, 1963. [13] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. [14] S. De, S. L. Smith, A. Fernando, A. Botev, G. Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N. de Freitas, and \u00c7. G\u00fcl\u00e7ehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models. CoRR, abs/2402.19427, 2024. doi: 10.48550/ARXIV.2402.19427. URL https://doi.org/10.48550/arXiv.2402.19427. [15] G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, et al. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2022.",
    "ssmformal-12": "[16] S. Eilenberg. Automata, languages, and machines. Academic press, 1974.",
    "ssmformal-13": "[17] J. L. Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990. [18] M. B. Everaert, M. A. Huybregts, N. Chomsky, R. C. Berwick, and J. J. Bolhuis. Structures, not strings: linguistics as part of the cognitive sciences. Trends in cognitive sciences, 19(12): 729-743, 2015.",
    "ssmformal-14": "[19] P. C. Fischer, A. R. Meyer, and A. L. Rosenberg. Counter machines and counter languages. Mathematical systems theory, 2(3):265-283, Sep 1968. ISSN 1433-0490. doi: 10.1007/BF01694011. URLhttps://doi.org/10.1007/BF01694011. [20] P. C. Fischer, A. R. Meyer, and A. L. Rosenberg. Counter machines and counter languages.",
    "ssmformal-15": "Math. Syst. Theory, 2(3):265-283, 1968. doi: 10.1007/BF01694011. URL https://doi. org/10.1007/BF01694011. $[21]$ D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= COZDyOWYGg\n[22] A. Ginzburg. Algebraic theory of automata. Academic Press, 1968. [23] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL https://doi.org/10. 48550 /arXiv. 2312.00752\n[24] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. [25] M. Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. [26] M. Hahn and M. Rofin. Why are sensitive functions hard for transformers? CoRR, abs/2402.09963, 2024. doi: 10.48550/ARXIV.2402.09963. URL https://doi.org/10. 48550 /arXiv. 2402.09963\n[27] M. Hahn, A. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and the structure of nc ${ }^{1}$.",
    "ssmformal-16": "In G. F. Italiano, G. Pighizzini, and D. Sannella, editors, Mathematical Foundations of Computer Science 2015 - 40th International Symposium, MFCS 2015, Milan, Italy, August 24-28, 2015, Proceedings, Part II, volume 9235 of Lecture Notes in Computer Science, pages 384-394.",
    "ssmformal-17": "Springer, 2015. doi: 10.1007/978-3-662-48054-0\u3011_32. URL https://doi.org/ $10.1007 / 978-3-662-48054-0 \\_32$. [28] J. Hewitt, M. Hahn, S. Ganguli, P. Liang, and C.",
    "ssmformal-18": "D. Manning. Rnns can generate bounded hierarchical languages with optimal memory. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1978-2010, 2020. [29] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997.",
    "ssmformal-19": "[30] J. E. Hopcroft, R. Motwani, and J.",
    "ssmformal-20": "D. Ullman. Introduction to automata theory, languages, and computation. ACM New York, NY, USA, 2001. [31] B. Horne and D. Hush. Bounds on the complexity of recurrent neural network implementations of finite state machines.",
    "ssmformal-21": "Advances in neural information processing systems, 6, 1993. [32] P. Indyk. Optimal simulation of automata by neural nets. In Annual Symposium on Theoretical Aspects of Computer Science, pages 337-348. Springer, 1995. [33] S. Jelassi, D. Brandfonbrener, S. M. Kakade, and E. Malach. Repeat after me: Transformers are better than state space models at copying, 2024.",
    "ssmformal-22": "[34] R. E. Kalman. On the general theory of control systems. In Proceedings First International Conference on Automatic Control, Moscow, USSR, pages 481-492, 1960. [35] R. E. Kalman. Mathematical description of linear dynamical systems. Journal of the Society for Industrial and Applied Mathematics, Series A: Control, 1(2):152-192, 1963.",
    "ssmformal-23": "[36] F. Karlsson. Constraints on multiple center-embedding of clauses. Journal of Linguistics, 43 (2):365-392, 2007. [37] S. Kleene. Representation of events in nerve nets and finite automata. In Automata Studies. 1951. [38] A. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and constant depth circuits.",
    "ssmformal-24": "In E.",
    "ssmformal-25": "W. Mayr and N. Ollinger, editors, 32nd International Symposium on Theoretical Aspects of Computer Science, STACS 2015, March 4-7, 2015, Garching, Germany, volume 30 of LIPIcs, pages 594-607. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2015. doi: 10.4230/ LIPICS.STACS.2015.594. URLhttps://doi.org/10.4230/LIPIcS.STACS.2015.594. [39] K. Krohn and J. Rhodes. Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines. Transactions of the American Mathematical Society, 116: $450-464,1965$.",
    "ssmformal-26": "[40] M. Kutrib, A. Malcher, and M. Wendlandt. Input-driven multi-counter automata. Theoretical Computer Science, 870:121-136, 2021. [41] T. Lei, Y. Zhang, S. I. Wang, H. Dai, and Y. Artzi. Simple recurrent units for highly parallelizable recurrence. arXiv preprint arXiv:1709.02755, 2017. [42] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, O. Abend, R. Alon, T. Asida, A. Bergman, R. Glozman, M. Gokhman, A. Manevich, N. Ratner, N. Rozen, E. Shwartz, M. Zusman, and Y. Shoham. Jamba: A hybrid transformer-mamba language model, 2024.",
    "ssmformal-27": "[43] T. Linzen, E. Dupoux, and Y. Goldberg. Assessing the ability of LSTMs to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521535, 2016. [44] B. Liu, J. T. Ash, S. Goel, A.",
    "ssmformal-28": "Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. [45] B. Liu, J. T. Ash, S. Goel, A.",
    "ssmformal-29": "Krishnamurthy, and C. Zhang. Exposing attention glitches with flip-flop language modeling.",
    "ssmformal-30": "In A. Oh, T. Naumann, A. Globerson, K.",
    "ssmformal-31": "Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.",
    "ssmformal-32": "URL http://papers.nips.cc/paper_files/paper/2023/ hash/510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html. [46] R. McNaughton and S. A. Papert. Counter-Free Automata (MIT research monograph no.",
    "ssmformal-33": "65). The MIT Press, 1971. [47] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5MkYIYCbva,\n[48] W. Merrill and A. Sabharwal. A logic for expressing log-precision transformers. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.",
    "ssmformal-34": "[49] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In ICML, 2024.",
    "ssmformal-35": "[50] G. A. Miller and N. Chomsky. Finitary models of language users. 1963. [51] S. M. S. Mirac Suzgun, Yonatan Belinkov. On evaluating the generalization of 1stm models in formal languages.",
    "ssmformal-36": "volume 2, pages 277-286. University of Massachusetts Amherst Libraries, 1 2019. doi: 10.7275/s02b-4d91. URL https://openpublishing.library.umass.edu/ scil/article/id/1167/\n[52] A. Orvieto, S.",
    "ssmformal-37": "L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [53] B. Peng, S. Narayanan, and C. Papadimitriou. On limitations of the transformer architecture. arXiv preprint arXiv:2402.08164, 2024. [54] J. P\u00e9rez, J. Marinkovi\u0107, and P. Barcel\u00f3. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. [55] P. C. Phillips and V. Solo. Asymptotics for linear processes. The Annals of Statistics, pages 971-1001, 1992. [56] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W.",
    "ssmformal-38": "Sun, and Y. Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [57] Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [58] J. Sakarovitch. Elements of automata theory. Cambridge university press, 2009. [59] C. Sanford, D. J. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36, 2024.",
    "ssmformal-39": "[60] M. P. Sch\u00fctzenberger. On finite monoids having only trivial subgroups.",
    "ssmformal-40": "Inf. Control., 8(2): 190-194, 1965. [61] N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [62] H. Siegelman and E. D. Sontag. On the computational power of neural nets. Journal of Computer and System Sciences, 50:132-150, 1995. [63] H. T. Siegelmann. Neural networks and analog computation: beyond the Turing limit. Springer Science \\& Business Media, 1999. [64] H. Straubing. Finite automata, formal logic, and circuit complexity. Birkhaeuser, 1994.",
    "ssmformal-41": "[65] L. Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.",
    "ssmformal-42": "[66] L. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. Transformers as recognizers of formal languages: A survey on expressivity. CoRR, abs/2311.00208, 2023. doi: 10.48550/ ARXIV.2311.00208. URL https://doi.org/10.48550/arXiv.2311.00208. [67] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.",
    "ssmformal-43": "[68] M. Tomita. Dynamic construction of finite-state automata from examples using hill-climbing. In Proceedings of the Fourth Annual Conference of the Cognitive Science Society, pages 105108, 1982. [69] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. [70] G. Weiss, Y. Goldberg, and E. Yahav. On the practical computational power of finite precision rnns for language recognition. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740-745, 2018. [71] A. Yang and D. Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers. arXiv preprint arXiv:2404.04393, 2024. [72] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [73] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.292. URL/http://dx.doi.org/10.18653/v1/2021.",
    "ssmformal-44": "acl-long. 292\n[74] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages.",
    "ssmformal-45": "In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770-3785, Online, Aug. 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.acl-long.292. URL https://aclanthology.org/2021.acl-long. 292\n[75] B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. ## A Instantiations of General Framework in SSM Models\n\nHere, we survey how (1) is instantiated in a range of SSMs. As stated in Section 2, we refer to SSMs where the gate $A$ does not depend on $x_{t}$ as time-invariant. An equivalent terminology is the distinction between \"Weak Linear Time Invariant Convolutional Models\" (i.e., time-invariant) and \"Linear Time Variant Models\" (i.e., non-time-invariant) in Aky\u00fcrek et al. [1]. ## A. 1 Non-Time-Invariant Models\n\nApproximately simultaneously with or more recently than [23], a range of non-time-invariant SSMs have been introduced [14, 72, 57, 56]. This category also covers highly similar earlier RNN variants [8, 41]. Mamba In Mamba, (2) and (3) directly map onto Eqs. (2a) and (2b) in Gu and Dao [23]. The notation of Gu and Dao [23] use a matrix multiplication $\\bar{A} h_{t-1}$ instead of elementwise multiplication $A\\left(x_{t}\\right) \\circ h_{t-1}$ in (REF), but importantly, Mamba's $\\bar{A}$ is diagonal, so we can take $A\\left(x_{t}\\right)_{i}=\\bar{A}_{i i}$. Due to exponential parameterization, its entries are nonnegative. Griffin The RG-LRU layer of Griffin [14] uses the equation\n\n$$\nh_{t}=\\underbrace{a_{t}}_{A\\left(x_{t}\\right)} \\circ h_{t-1}+\\underbrace{\\sqrt{1-a_{t}^{2}} \\circ\\left(i_{t} \\circ x_{t}\\right)}_{B\\left(x_{t}\\right)}\n$$\n\nwhere $a_{t}, i_{t}$ are neurally parameterized in terms of $x_{t}$ but not $h_{<t}$; by design, $a_{t} \\in(0,1) . \\phi$ is instantiated in terms of linear transformations, GeLU, and RMSNorm (Figure 2 in De et al. [14]). The local attention used by Griffin can be subsumed into an SSM layer (Remark 18.)\n\nGated Linear Attention [GLA 72] This model (Section 4.4 in Yang et al. [72]) instantiates our framework using a recurrence of the form (1); while the state is two-dimensional in this model, the update is performed by elementwise products as in (1). The gate is obtained by applying sigmoid to a linear transformation of $x_{t}$; thus, its entries are in $(0,1) . \\phi$ is instantiated in terms of SwiGLU and LayerNorm. HGRN HGRN [57] and HGRN2 [56] are defined by a recurrence of the form (1]; the gate entries are $\\in(0,1)$ by design. $\\phi$ is instantiated in terms of GLU, linear transformations, and normalization. In HGRN, the state is complex, but crucially the gate remains real-valued. ## A. 2 Time-Invariant Models\n\nTime-invariant SSMs introduced before late 2023 are surveyed by Gu and Dao [23, Appendix B], such as [47, 67, 52]. Time-invariant SSMs have often used complex-valued states and gates; this does not have a major impact on our results: First, as complex-valued SSMs subsume real-valued ones, our positive results carry over. Second, our negative result about PARITY is affected by this distinction and requires a separate argument, see Theorem 13\nNote also that $A h_{t-1}$ is often described as a general matrix multiplication, but $A$ is diagonalizable (e.g. Lemma 3.2 in Gu et al. [24]; [67] for RetNet), which \u2014even though implementation may be based on non-diagonalized representations [24]-renders the model equivalent to one where $A$ is diagonal from the start. This equivalence is shown as Lemma 3.1 in Gu et al.",
    "ssmformal-46": "[24]. ## B Formal Definitions and Proofs\n\n## B. 1 Flip Flop\n\nWe begin by introducing key notions of automata theory. References for automata theory include Eilenberg [16], Hopcroft et al. [30], Sakarovitch [58]. We will provide those key notions that are\nnecessary to prove our results. We will focus on deterministic finite-state-automata (DFA), and simply refer to them as finite-state-automata ${ }^{4}$ First,\nDefinition 7. A (deterministic) finite-state-automaton $\\mathcal{A}$ consists of:\n\n- a finite alphabet $\\Sigma$\n- a finite state set $Q$\n- a starting state $q_{0} \\in Q$\n- a transition function $\u0438: Q \\times \\Sigma \\rightarrow Q$\n\nWe extend $u$ to a map $u: Q \\times \\Sigma^{*} \\rightarrow Q$ by setting:\n\n$$\n\\begin{aligned}\nu(q, \\varepsilon) & =q \\\\\nu\\left(q, w_{1 \\ldots i+1}\\right) & =u\\left(u\\left(q, w_{1 \\ldots i}\\right), w_{i+1}\\right)\n\\end{aligned}\n$$\n\nwhere $\\varepsilon$ is the empty word. Intuitively, $u\\left(q_{0}, \\mathbf{w}\\right)$ is the state that $\\mathcal{A}$ is in after reading $\\mathbf{w}$. The automaton recognizes a language $L \\subseteq \\Sigma^{*}$ if there is a recognizing set $R \\subseteq Q$ such that\n\n$$\nL:=\\left\\{w: u\\left(q_{0}, \\mathbf{w}\\right) \\in R\\right\\}\n$$\n\nKleene's Theorem [37] asserts that a language $L \\subseteq \\Sigma^{*}$ is regular (i.e., defined by a regular expression) if and only if it is recognized by some finite-state automaton. A very fundamental automaton underlying Flip Flop is:\nDefinition 8. A set-reset automaton is a finite-state-automaton where $\\left(Q \\backslash\\left\\{q_{0}\\right\\}\\right) \\subseteq \\Sigma$ and\n\n$$\nu(q, \\sigma)= \\begin{cases}q & \\text { if } \\sigma \\notin Q \\\\ \\sigma & \\text { else }\\end{cases}\n$$\n\nIntuitively, such an automaton keeps recording the last seen symbol from a designed set $Q \\subseteq \\Sigma$. Such an automaton is easily simulated with a single non-time-invariant SSM layer:\nLemma 9. Let $\\mathcal{A}=\\left\\langle\\Sigma, Q, q_{0}, u\\right\\rangle$ by a set-reset automaton. Then there is a single-layer $S S M$ with finite precision and width $d=1+\\log Q$ that maps each $w_{1 \\ldots T} \\in \\Sigma^{*}$ to the state sequence $u\\left(q_{0}, w_{1}\\right), u\\left(q_{0}, w_{12}\\right), \\ldots, u\\left(q_{0}, w_{1 \\ldots T}\\right) \\in Q^{T}$. Formally, there is an injective map $V: Q \\rightarrow \\mathbb{R}^{d}$ such that $\\rho\\left(z_{t}\\right)=V\\left(u\\left(q_{0}, w_{1 \\ldots t}\\right)\\right)$ for $t=1, \\ldots, T$. Proof. Let $B(\\sigma) \\in \\mathbb{R}^{\\log |Q|}$ be a binary encoding if $\\sigma \\in Q$, and $\\boldsymbol{0} \\in \\mathbb{R}^{\\log |Q|}$ else. Take $h_{0}=B\\left(q_{0}\\right)$. Let $A(\\sigma)=\\mathbf{0}$ if $\\sigma \\in Q$ and $A(\\boldsymbol{\\sigma})=\\mathbf{1}$ else. After processing a string, the state $h_{t}$ is $B(\\sigma)$ where $\\sigma$ is the last symbol in $Q$ that has occurred if any has, and $B\\left(q_{0}\\right)$ otherwise. Coming to $\\sqrt{2}$, in order to avoid division by zero when normalizing if no element of $Q$ has been read, we add a dummy dimension to $h_{t}$ whose value is always 1 . We take $\\operatorname{Mix}_{1}, \\mathrm{Mix}_{2}$ to be the identity. Note that, even though normalization will affect the numerical values, the binary encoding of $\\sigma \\in Q$ can still be read out with finite precision, as $1 \\leq\\left\\|h_{t}\\right\\|_{2} \\leq \\sqrt{1+\\log |Q|}$, and thus nonzero entries will remain bounded away from zero. Theorem 10. (Repeated from Theorem 1) A two-layer SSM with finite precision can predictively model $\\mathcal{L}_{F F}$ at arbitrary lengths. Proof. In the first layer, we use Lemma 9 to simulate a set-reset automaton over the input alphabet $\\Sigma_{1}=\\{w, r, i, 0,1\\}$ where $Q_{1}=\\Sigma_{1} \\cup\\left\\{q_{0}\\right\\}$. This layer outputs at each position whether the last instruction was write, read, or ignore. The layer additionally, at each position, forwards the input symbol using additional dimensions. Formally, at the first layer, $\\rho\\left(h_{t}\\right)$ allows us to read out the input symbols $x_{t-1}, x_{t} \\in \\Sigma$. [^3]In the second layer, we again use Lemma 9 to simulate a set-reset automaton over an extended alphabet $\\Sigma_{2}:=\\Sigma_{1} \\times \\Sigma_{1}$, where the first component indicates the input symbol $x_{t}$ and where the second component indicates $x_{t-1}$. In this set-reset automaton, $Q_{2}$ contains, besides a start state $q_{0}$, those elements of $\\Sigma_{2}$ whose second entry is $w$. The second layer thus keeps track of the input bit $b \\in\\{0,1\\}$ following the last write instruction. It additionally forwards the input symbol $x_{t}$ using additional dimensions. The second layer, via $\\rho$, then predicts the possible next symbols on the basis of this information: If $x_{t} \\in\\{0,1\\}$, any instruction in $\\{w, r, i\\}$ is possible. If $x_{t} \\in\\{w, i\\}$, any bit in $\\{0,1\\}$ is possible. If $x_{t}=r$, the bit stored after the last write instruction is possible; if no write instruction has appeared (hence, the second automaton is still in its start state), any bit in $\\{0,1\\}$ is possible. ## B. 2 Difficulty of Representing PARITY\n\nDefinition 11. PARITY is the regular language over $\\Sigma=\\{0,1\\}$ of strings where the number of ones is even. As a regular expression, PARITY is $\\left(0^{*} 10^{*} 10^{*}\\right)^{*}$. Theorem 12. (Repeated from Theorem 2) No SSM satisfying NONNEGATIVE can recognize PARITY at arbitrary input lengths with finite precision. Proof. We consider an SSM with multiple layers, and indicate the layer in superscript: $h_{t}^{(1)}, \\ldots, h_{t}^{(L)}$. We write $z_{t}^{(0)}$ for the input token embedding $e\\left(w_{t}\\right)$. Consider a SSM processing the word $1^{t}$, for $t \\rightarrow \\infty$. We show, by induction over the number of layers, the following claim:\n$(\\dagger)$ Each entry of $z_{t}^{(k)}$ converges to a value bounded, in absolute value, by a constant. By the assumption of finite precision, convergence automatically leads to the entries becoming ultimately constant. Once we have shown this, we know that $z_{t}^{(L)}$ is constant when $t$ is sufficiently large; thus, the parity of the string $1^{t}$ cannot be read out from $z_{t}^{(L)}$. As a consequence, the SSM cannot recognize PARITY. Indeed, we have shwon the stronger claim that the language (11)* - the language of even-length strings over one symbol - is not recognized by an SSM; we will use this stronger statement in Corollary 14\nWe proceed to proving $(\\dagger)$. The claim $(\\dagger)$ is trivially true at $k=0$, as the input token is always the same and we defined $z_{t}^{(0)}:=e\\left(w_{t}\\right)$. Now consider $k>0$. By hypothesis, the activations are given as\n\n$$\nh_{t}^{(k)}=A\\left(x_{t}\\right) \\circ h_{t-1}^{(k)}+B\\left(x_{t}\\right)\n$$\n\nwhere $A\\left(x_{t}\\right), B\\left(x_{t}\\right)$ are constant $\\alpha:=A\\left(x_{t}\\right), \\beta:=B\\left(x_{t}\\right)$ when $t>T_{0}$, for some $T_{0}>0$. The solution of the recurrence for $t>T_{0}$ is\n\n$$\nh_{t}=\\alpha^{t-T_{0}}\\left(h_{T_{0}}+\\frac{\\beta}{\\alpha-1}\\right)+\\frac{\\beta}{1-\\alpha}\n$$\n\nEach dimension $j=1, \\ldots, d$ of this vector can be constant (if $\\left(h_{T_{0}}\\right)_{j}+\\frac{\\beta_{j}}{\\alpha_{j}-1}=0$ ), diverge exponentially $\\left(\\alpha_{j}>1\\right)$, converge exponentially $\\left(\\alpha_{j}<1\\right)$ or diverge linearly $\\left(\\alpha_{j}=1\\right)$. We next need to show that $z_{t}=\\operatorname{Mix}_{2}\\left(\\operatorname{Norm}\\left(\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)\\right)\\right)$ converges. First, consider the effect of applying a linear transformation to the state $h_{t}$. Each entry of the result will be some linear combination\n\n$$\nu_{t}=\\lambda_{1}\\left(h_{t}\\right)_{1}+\\cdots+\\lambda_{d}\\left(h_{t}\\right)_{d}\n$$\n\nIf each $\\alpha_{j}<1$, then $u_{t}$ converges. If some $\\left|\\alpha_{j}\\right| \\geq 1$, there may be some cancellation if $\\alpha_{i}=\\alpha_{j}$ for some $i \\neq j$; cancellation can only lead to full erasure of the relevant terms or to a remaining term with the same exponent. In conclusion, each entry $u_{t}$ will again either converge to a finite value or diverge towards $\\pm \\infty$. We now need to understand the behavior of $\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)$. Recall that, based on our survey (Appendix A), we allowed it to contain linear, GLU [13], and SwiGLU [61] components. If $\\mathrm{Mix}_{1}\\left(h_{t}, x_{t}\\right)$ implements a linear transformation only, each entry likewise may converge, diverge linearly, or diverge exponentially. We note that-if $\\sigma$ is the sigmoid function- $\\sigma\\left(u_{t}\\right)$ always converges, as $\\sigma$\nsimply saturates to 0 or 1 if $u_{t}$ diverges. Hence, if $\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)$ implements GLU, each entry likewise may converge, diverge linearly, or diverge exponentially. Finally, if $\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)$ implements SwiGLU, each entry of the result will be a product of a linear combination of the form $u_{t}$, and $S w i s h_{\\beta}$ applied to another such linear combination. Depending on the behavior of these two $u_{t}$-like terms, the outcome will behave as a product of sequences that may converge exponentially, diverge exponentially, or diverge linearly - e.g., the outcome may also diverge quadratically, or converge as $n \\alpha^{-n}$, etc. If all dimensions of $\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)$ converge, then $\\operatorname{Norm}\\left(\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)\\right)$ will also converge to a scaled version of $\\frac{\\beta_{i}}{1-\\alpha_{i}}$, scaled by a bounded factor as $\\beta_{i} \\neq 0$. Now assume some dimensions of $\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)$ do not converge; in this case, for any two dimensions $i, j$, either their ratio will converge to a constant, or converge to 0 or $\\pm \\infty$. After applying $\\operatorname{Norm}(\\cdot)$, the entries asymptotically dominating the others will converge to a finite value bounded, in absolute value, by 1 ; the others will converge to zero. In conclusion, we have found that each entry of $\\operatorname{Norm}\\left(\\operatorname{Mix}_{1}\\left(h_{t}, x_{t}\\right)\\right)$ converges to some number bounded, in absolute value, by 1 . As $\\mathrm{Mix}_{2}$ is continuous, each entry of $z_{t}$ likewise converges, with a bound depending on the Lipschitz constant of $\\mathrm{Mix}_{2}$. We next show the result, referenced in the main paper text after Theorem 2, about time-invariant SSMs with complex-valued gates:\n\nTheorem 13. TIME-INVARIANT SSMs cannot recognize PARITY with finite precision at arbitrary input lengths, even with complex-valued gates, as long as each entry in each A has a rational angle in the complex plane. Here, by a rational angle, we refer to an angle that is a rational number when expressed in degrees; such angles are rational multiples of $2 \\pi$ when expressed in radians. As the rational angles are dense in the reals, one expects that even if some irrational angles permitted modeling PARITY, such solutions would be very hard to find - in particular given that irrational numbers are not exactly represented in finite precision. Proof. By assumption, any $A_{j} \\in \\mathbb{C}$ in any layer can be written as\n\n$$\nA_{j}=r_{j} \\exp \\left(2 \\pi i q_{j}\\right)\n$$\n\nwhere $q_{j} \\in[0,1]$ is rational and $r_{j} \\geq 0$ is real - here, $2 \\pi q_{j}$ is known as the argument of $A_{j}$; it describes the angle of $A_{j}$ in the complex plane in radians. Correspondingly, the angle in degrees is described by $q_{j} \\cdot 360^{\\circ}$; this is rational if and only if $q_{j}$ is. As a time-invariant SSM has a finite number of such values $A_{j}$, across all its layers, we can select a positive integer $W$ such that $W q_{j} \\in \\mathbb{N}$ for each $j$, in each layer. Importantly, $\\left(A_{j}\\right)^{W}=\\left(r_{j}\\right)^{W} \\in \\mathbb{R}$. Now consider the action of any layer of the SSM on an input sequence of the form $A_{T}=\\left(10^{W-1}\\right)^{T}$. The claim is that, for each $i=1, \\ldots, W$, the sequence\n\n$$\nz_{t W+i}^{(k)}\n$$\n\nconverges as $t \\rightarrow \\infty$. As in the proof of Theorem 2, in the finite-precision setting, converge entails that the sequence becomes ultimately stationary. Note that the parity of $A_{T}$ equals the parity of $T$; hence, it is impossible to read out the parity from $z_{T W}^{(k)}$ when $T$ is large. Now consider, suppressing the index for the dimension in $1, \\ldots, d$ :\n\n$$\n\\begin{aligned}\nh_{t W}^{(k)} & =\\sum_{i=1}^{t W} A^{t W-i} B\\left(z_{i}^{(k-1)}\\right) \\\\\n& =\\sum_{i=1}^{t W} A^{t W-i} B\\left(z_{i}^{(k-1)}\\right) \\\\\n& =\\sum_{s=1}^{t} \\sum_{j=s W}^{(s+1) W-1} A^{t W-j} B\\left(z_{s W+j}^{(k-1)}\\right) \\\\\n& =\\sum_{s=1}^{t} \\sum_{j=0}^{W-1} A^{(t-s) W-j} B\\left(z_{s W+j}^{(k-1)}\\right) \\\\\n& =\\sum_{s=1}^{t} \\sum_{j=0}^{W-1}(r \\exp (2 \\pi i q))^{(t-s) W-j} B\\left(z_{s W+j}^{(k-1)}\\right) \\\\\n& =\\sum_{s=1}^{t} \\sum_{j=0}^{W-1} r^{(t-s) W-j} \\exp (-2 \\pi i j q) B\\left(z_{s W+j}^{(k-1)}\\right) \\\\\n& =\\sum_{j=0}^{W-1} \\exp (-2 \\pi i j q) \\sum_{s=1}^{t} r^{(t-s) W-j} B\\left(z_{s W+j}^{(k-1)}\\right)\n\\end{aligned}\n$$\n\nSeparately considering summation beyond $T_{0}$ at which $z_{t W+j}^{(k-1)}$ has become stationary, we get\n\n$$\n=\\underbrace{\\left[\\sum_{j-0}^{T_{0}-1} \\cdots\\right]}_{U_{1}}+[\\underbrace{\\left(\\sum_{j=T_{0}}^{W-1} \\exp (-2 \\pi i j q) B\\left(z_{j}^{(k-1)}\\right) r^{-j}\\right)}_{U_{2}} \\underbrace{\\left(\\sum_{s=1}^{t} r^{(t-s) W}\\right)}_{U_{3}}]\n$$\n\n$U_{1}$ and $U_{2}$ do not depend on $t$. Intuitively, $U_{2} \\in \\mathbb{C}$ determines a direction in the complex plane, whereas $U_{3} \\in \\mathbb{R}$ determines a magnitude. It remains to understand $U_{3}$, which can be rewritten as:\n\n$$\nU_{3}=\\sum_{s=1}^{t} r^{(s-1) W}=r^{-W} \\sum_{s=1}^{t}\\left(r^{W}\\right)^{s}=r^{-W} \\sum_{s=0}^{t}\\left(r^{W}\\right)^{s}-r^{-W}=r^{-W} \\begin{cases}\\frac{1-\\left(r^{W}\\right)^{t}}{1-\\left(r^{W}\\right)}-1 & r \\neq 1 \\\\ s-1 & r=1\\end{cases}\n$$\n\nWe have now achieved a situation like in the proof of Theorem 2. $U_{3}$ can converge exponentially, diverge linearly, or diverge exponentially.",
    "ssmformal-47": "The remainder of the proof is analogous to that proof. The following Corollary of Theorem 2 will be used in the proof of Theorem 4\nCorollary 14. Assume NONNEGATIVE, SSMs with finite precision cannot recognize any non-starfree regular language. Proof. For any non-star-free regular language $\\mathcal{L}$, there are words $u, v, w$ such that the membership $u v^{n} w \\in \\mathcal{L}$ is determined by the parity of $n$ [46]. Fix any such $u, v, w \\in \\Sigma^{*}$. Now assume an SSM satisfying NONNEGATIVE can recognize $\\mathcal{L}$ with finite precision. We can subsume the action of $u$ into the state $h_{0}$ by taking $h_{0}$, in each layer, to be the state of the SSM after reading $u$. We now have an SSM that can determine the parity of $t$ when fed a word of the form $v^{t} w$. For this SSM, we want to show\n$(\\dagger)$ When fed words of the form $v, v^{2}, v^{3}, \\ldots$, for each $i=0, \\ldots,|v|-1$, and each layer $k=1, \\ldots, L$, the sequence $z_{t|v|+i}^{(k)}$ converges as $t \\rightarrow \\infty$. As in the preceding two proofs in this section, convergence entails becoming ultimately constant in the finite-precision setting. The claim $(\\dagger)$ is immediate at $k=0$. Now at $k>0$, we write:\n\n$$\n\\begin{aligned}\nh_{t|v|+i}^{(k)}= & A\\left(z_{t|v|+i}^{(k-1)}\\right) \\ldots A\\left(z_{(t-1)|v|+i+1}^{(k-1)}\\right) h_{(t-1)|v|+i}^{(k)} \\\\\n& +A\\left(z_{t|v|+i}^{(k-1)}\\right) \\ldots A\\left(z_{(t-1)|v|+i+2}^{(k-1)}\\right) B\\left(z_{(t-1)|v|+i+1}^{(k-1)}\\right) \\\\\n& +A\\left(z_{t|v|+i}^{(k-1)}\\right) \\ldots A\\left(z_{(t-1)|v|+i+3}^{(k-1)}\\right) B\\left(z_{(t-1)|v|+i+2}^{(k-1)}\\right) \\\\\n& +\\ldots \\\\\n& +B\\left(z_{(t-1)|v|+i}^{(k-1)}\\right)\n\\end{aligned}\n$$\n\nOn the RHS, as $t \\rightarrow 0$, all terms except for $h_{(t-1)|v|+i}^{(k)}$ become constant by the inductive hypothesis. Hence, there are some $\\alpha, \\beta$ such that, for sufficiently large $t$,\n\n$$\nh_{t|v|+i}^{(k)}=\\alpha \\circ h_{(t-1)|v|+i}^{(k)}+\\beta\n$$\n\nWe are now, for each $i$, in the same situation as in the proof of Theorem 2]: each dimension of this recurrence can converge exponentially, diverge exponentially, or diverge linearly; as in that proof, it follows that $z_{t|v|+i}^{(k)}$ converges as $t \\rightarrow \\infty$. We have shown $(\\dagger)$. We now follow up by showing that\n(*) When fed words of the form $v^{t} w$, for each $i=1, \\ldots,|w|$, and each layer $k=1, \\ldots, L$, the sequence $z_{t|v|+i}^{(k)}$ converges as $t \\rightarrow \\infty$. Again, at finite precision, convergence entails that the sequences are ultimately constant. Again, (*) is true at $k=0$ trivially. When feeding the SSM words of the form $v^{t} w$, in each layer, the final state is in each layer $k$, at each $i=1, \\ldots,|w|$ :\n\n$$\n\\begin{aligned}\nh_{t|v|+i}^{(k)}= & A\\left(z_{t|v|+|w|}^{(k-1)}\\right) \\ldots A\\left(z_{t|v|+1}^{(k-1)}\\right) h_{t|v|}^{(k)} \\\\\n& +A\\left(z_{t|v|+i}^{(k-1)}\\right) \\ldots A\\left(z_{t|v|+2}^{(k-1)}\\right) B\\left(z_{t|v|+1}^{(k-1)}\\right) \\\\\n& +\\ldots \\\\\n& +B\\left(z_{t|v|+i}^{(k-1)}\\right)\n\\end{aligned}\n$$\n\nBy inductive hypothesis, for large $t$, there are $\\psi_{i}, \\gamma_{i}$ such that\n\n$$\nh_{t|v|+i}^{(k)}=\\psi_{i} \\circ h_{t|v|}^{(k)}+\\gamma_{i}\n$$\n\nand, as shown before, each entry of $h_{t|v|}^{(k)}$ converges exponentially, diverges exponentially, or diverges linearly. Now, by assumption, one can read out, at finite precisiion, the parity of $t$ from\n\n$$\nz_{t|v|+|w|}^{(L)}=\\operatorname{Mix}_{1}\\left(\\operatorname{Norm}\\left(\\operatorname{Mix}_{2}\\left(\\Psi_{|w|} \\circ h_{t|v|+i}^{(k)}+\\gamma_{|w|}\\right)\\right)\\right)\n$$\n\nWe now simply absorb the operation $X \\mapsto \\psi_{|w|} \\circ X+\\gamma_{|w|}$ into $\\operatorname{Mix}_{2}$, and obtain by the same arguments as in the proof of Theorem 2 that $z_{t|v|+|w|}^{(L)}$ converges as $r \\rightarrow \\infty$. This is a contradiction to the claim that the parity of $t$ can be read out from $z_{t|v|+|w|}^{(L)}$ at finite precision. ## B. 3 Proof of Theorem 4\n\nOur proof of Theorem 4 will rely on the algebraic theory of finite automata, specifically the cascade product and the Krohn-Rhodes Theorem [39]. These techniques, originally developed in the 1960s,\nhave recently been introduced to the theoretical study of transformers by Liu et al. [44]; we provide self-contained definitions and somewhat different notation, tailored to our proofs about state-space models. In general, we will find that the properties of state-space models allow more natural and directly length-generalizing implementations of these algebraic notions than what is possible for transfomers. Recall the definition of a finite-state-automaton (Definition 7). Our construction will build on an important operation on automata, the cascade product [39, 16, 22]:\n\nDefinition 15. Given two automata $\\mathcal{A}_{1}, \\mathcal{A}_{2}$ with associated alphabets $\\Sigma_{1}, \\Sigma_{2}$ and state sets $Q_{1}, Q_{2}$ such that\n\n$$\n\\Sigma_{2}=Q_{1} \\times \\Sigma_{1}\n$$\n\nthe cascade product $A_{2} \\prec A_{1}$ is the automaton given by\n\u30fb $\\Sigma=\\Sigma_{1}$\n\u30fb $Q=Q_{2} \\times Q_{1}$\n\n- $q_{0}$ is the tuple of the starting states of $\\mathcal{A}_{2}, \\mathcal{A}_{1}$\n\n\u30fb $u(\\langle q, p\\rangle, \\sigma)=\\left\\langle u_{2}(q,\\langle p, \\sigma\\rangle), u_{1}(p, \\sigma)\\right\\rangle$\nWe note that the literature usually uses \"o\" for the cascade product [e.g. 16]. To avoid collision with the elementwise product \" $\\circ$ \" (e.g., (1)), we here instead use \"\"\", usually used for the wreath product - a product on monoids with an effect analogous to the cascade product [2]. While the formal definition is cumbersome, the intuition behind it is simple: The cascade product corresponds to first reading a word $\\mathbf{w}$ with $\\mathcal{A}_{1}$, recording the state sequence $q_{0}, q_{1}, \\ldots, q_{|\\mathbf{w}|} \\in Q_{1}$ and - at each $t=1, \\ldots,|\\mathbf{w}|$ - pasting the state $q_{t-1}$ together with the input symbol $w_{t} \\in \\Sigma_{1}$ - resulting in a word over a new alphabet $Q_{1} \\times \\Sigma_{1}$, and then running $A_{2}$ on the resulting word. The overall state of $A_{2}\\left\\langle A_{1}\\right.$ after reading a word is the tuple of the states reached by $A_{2}$ and $A_{1}$. Note that we write $A_{2}\\left\\langle A_{1}\\right.$,, rather than, $A_{1} \\backslash A_{2}$, because the second argument of the cascade product $\\left(A_{1}\\right)$ intuitively reads the input first, preprocessing it for the other automaton, $A_{2}$. The somewhat inscrutable update rule for $u(\\cdot, \\cdot)$ encodes the action of $\\mathcal{A}_{1}$ in the second component, and the action of $\\mathcal{A}_{2}$ on the extended alphabet in the first component. There is a close analogy to the stacking of sequence models, and we will leverage this analogy to translate cascade products into multilayer SSMs. The fundamental background here is the following classical fact:\n\nFact 16 (Consequence of Krohn-Rhodes Theorem [39] and Sch\u00fctzenberger's Theorem [60]). Each star-free regular language is recognized by an iterated cascade product of set-reset automata,\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_101ff39892b808f88e98g-20.jpg?height=47&width=914&top_left_y=1798&top_left_x=367)\n\nWe now formally show that cascade products can be translated to SSM stacking. We need an auxiliary lemma, which provides a single-layer SSM that encodes the input $w_{t-1}$ in state $h_{t}-$ we will use it to forward information about the state of $\\mathcal{A}_{1}$ at $t-1$ to $\\mathcal{A}_{2}$ at $t$ :\n\nLemma 17. Let $\\Sigma$ be an alphabet, and consider words $w \\in \\Sigma^{*}$. There is a one-layer SSM with $d=4|\\Sigma|$ such that, for $t=2, \\ldots,|w|$, the character $w_{t-1}$ can be read out from $z_{t}$ at finite precision. To prove Lemma 17 a first idea is to use an exponential moving average with $A=1 / 2$ to encode the recent input characters in $h_{t}$; this effectively encodes the full history into the binary expansion of $h_{t}$, and in particular allows reading out the second-last input in principle. However, such a construction does not work at finite precision, because rounding may make it impossible to extract even the second-most-significant bit ${ }^{5}$ We avoid this problem simply by taking $A=1 / 4$, effectively utilizing only every two digits in the binary expansion of $h_{t}$, ensuring that the second-last input can be read out at a constant margin. We now provide the formal proof:\n\n[^4]Proof. We begin by showing the claim in the special case $\\Sigma=\\{1,0\\}$. Here, we take $d=4$, and\n\n$$\n\\begin{aligned}\nh_{0} & =[0,0,0,0]^{T} \\\\\nA\\left(e_{0}\\right) & =[1 / 4,1 / 4,0,0]^{T} \\\\\nA\\left(e_{1}\\right) & =[1 / 4,1 / 4,0,0]^{T} \\\\\nB\\left(e_{0}\\right) & =[1,0,1,0]^{T} \\\\\nB\\left(e_{1}\\right) & =[0,1,0,1]^{T}\n\\end{aligned}\n$$\n\nNow we separately consider the state $h_{t}$ depending on the form of the prefix $w_{1 \\ldots t}:$ If $w_{1 \\ldots t}=\\ldots 00$, then\n\n$$\nh_{t}=\\left(\\begin{array}{c}\n\\in[1,2] \\\\\n\\in[0,1 / 8] \\\\\n1 \\\\\n0\n\\end{array}\\right)\n$$\n\nIf $w_{1 \\ldots .}=\\ldots 10$, then\n\n$$\nh_{t}=\\left(\\begin{array}{c}\n\\in[1,1.25] \\\\\n\\in[1 / 4,1 / 2] \\\\\n1 \\\\\n0\n\\end{array}\\right)\n$$\n\nIn particular, assuming $w_{t}=0$, one can read off $w_{t-1}$ from $\\left(h_{t}\\right)_{2}$ with a margin of size $1 / 8$. As $w_{t}$ is encoded in $h_{t}$ and due to symmetry, analogous statements hold when $w_{t}=1$. Now, for each $\\sigma \\in \\Sigma$, we run such a one-layer SSM where 0 represents $\\sigma$ and 1 represents all other characters ${ }^{6}$ By running these in parallel, we obtain an SSM with $d=4|\\Sigma|$ from whose states one can read out $w_{t-1}$ at finite precision. As the entries in $h_{t}$ are all bounded by 2 , we find $\\left\\|h_{t}\\right\\|_{2} \\leq 2 \\sqrt{d}$ independent of $t$, and the margin is still bounded away from zero after normalization, and thus in $z_{t}$, where we can assume $\\mathrm{Mix}_{1}, \\mathrm{Mix}_{2}$ to be the identity.",
    "ssmformal-48": "Remark 18. Some SSMs include local convolutions [e.g. 21. 23] or local attention [14], which aggregate information from a local window of some width $\\Delta>0$. These do not increase the expressive capacity beyond SSMs as we have defined in (1,2), as aggregation of local information can be simulated with a single SSM layer: Using the layer constructed in the proof of Lemma 17 given the state $h_{t}$, once one has read out $w_{t-1}$ as described in the proof, one can recover $h_{t-1}$ from $h_{t}$ and $x_{t}$; then inductively read out $w_{t-2}$ using $h_{t-1}$ and $x_{t-1}$, etc. Thus, up to any given width $\\Delta>0$, one can read out $w_{t-\\Delta}, \\ldots, w_{t-1}$ from the state $h_{t}$ of this layer at finite precision. We are now ready to translate cascade products into SSM stacking:\nLemma 19. Let $\\mathcal{A}_{1}, \\mathcal{A}_{2}$ be two finite-state-automata, and assume that there are two SSMs with top-level states $z^{\\left(L_{1}, 1\\right)}$ and $z^{\\left(L_{2}, 2\\right)}$ that map each $\\mathbf{w}$ to the state sequences under $\\mathcal{A}_{1}, \\mathcal{A}_{2}$, at finite precision. Formally, on a word $\\mathbf{w}, \\rho_{1}\\left(z_{t}^{\\left(L_{1}, 1\\right)}\\right)$ and $\\rho_{2}\\left(z_{t}^{\\left(L_{2}, 2\\right)}\\right)$ provide the state sequences of $\\mathcal{A}_{1}, \\mathcal{A}_{2}$. Then there is an SSM with $L_{1}+L_{2}+1$ layers that maps each $\\mathbf{w}$ to the state sequence under $\\mathcal{A}_{2} \\succ \\mathcal{A}_{2}$, again at finite precision. We note that a conceptually related result holds for transformers [Lemma 12 in 44]. However, SSMs allow a simpler and length-independent construction, as they do not require positional encodings to implement such a construction. Proof. The lower layers are based on the SSM modeling $\\mathcal{A}_{1}$. We duplicate each channel, so we now have $2 d$ dimensions. We further add $d$ further dimensions that directly pass on the input embeddings, i.e., $A \\equiv 0, B \\equiv 1, \\operatorname{Mix}_{j} \\equiv I d$ on these dimensions. In the resulting SSM, $z_{t}^{L_{1}}$ indicates both $w_{t}$ itself, and the state reached by $\\mathcal{A}_{1}$ after reading $w_{1 \\ldots t}$. The state is redundantly indicated by two separate sets of $d$ dimensions; the character $w_{t}$ is indicated by $d$ further state. [^5]Note, however, that the second automaton in the cascade product requires access to the state $q_{t-1}$ rather than $q_{t}$. For this, we add a layer provided by Lemma 17, of width $4|Q|$. Additional $2 d$ dimensions pass on (1) $w_{t}$, and (2) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $w_{1 \\ldots t}$. We now have $L_{1}+1$ layers where $z_{t}^{L_{t}+1}$ has $2 d+4|Q|$ dimensions and indicates (1) $w_{t}$, (2) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $w_{1 \\ldots t}$, (3) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $w_{1 \\ldots t-1}$. The first and third piece of information are now fed into the second SSM; the second piece is passed on in $d$ additional dimensions. As we allowed $A$ and $B$ to be arbitrary functions, we redefine these in the lowest layer of that second SSM to read out from the $4|Q|$-dimensional component indicating (3), providing the desired second-to-last state. We have constructed an SSM with $L_{1}+L_{2}+1$ layers, where $z_{t}^{L_{1}+L_{2}+1}$ indicates (1) $w_{t}$, (2) the state that $\\mathcal{A}_{1}$ reaches after reading the prefix $w_{1 \\ldots t}$, (3) the state that $\\mathcal{A}_{2}$ reaches after reading the prefix $w_{1 \\ldots t}$ pasted with the state sequence of $\\mathcal{A}_{1}$. This information is sufficient for reading out the state sequence of $\\mathscr{A}_{2}\\left\\langle\\mathcal{A}_{1}\\right.$. Note that the number of channels may not be consistent, as it is $3 d$ in the top and bottom parts, but $2 d+4|Q|$ in the middle; we simply pad to the larger dimensionality. We are now ready to show the existence of length-generalizing SSMs for any star-free state tracking problem, and conclude with the theorem:\nTheorem 20 (Repeated from Theorem 47. Assuming NonnEGATIVE, SSMs can predictively model a regular language $\\mathcal{L}$ at finite precision if and only if $\\mathcal{L}$ is star-free. Proof. We need to show:\n\n1. SSMs at finite precision can predictively model all star-free languages. For each language, a single SSMs is applicable at arbitrary lengths. 2. Assuming NONNEGATIVE, finite-precision SSMs cannot recognize any non-star-free regular language. The second statement is Corollary 14, it suffices to prove the first statement. Assume $\\mathcal{L}$ is star-free. By the Krohn-Rhodes theorem, there is an automaton $\\mathcal{A}$ that is a cascade product of some set-reset automata that recognizes $\\mathcal{L}$. By Lemmas 9 and 19 , there is an SSM that computes the state sequence of that automaton. Now we note that, since $\\mathcal{A}$ recognizes $\\mathcal{L}$, the state $q$ after reading $\\mathbf{w}$ is sufficient for determining the set of characters that can follow this prefix in any element of $\\mathcal{L}$. For, assume otherwise, then there are words $\\mathbf{w}, \\mathbf{w}^{\\prime}$ such that $u\\left(q_{0}, \\mathbf{w}\\right)=u\\left(q_{0}, \\mathbf{w}^{\\prime}\\right)$ and $\\sigma \\in \\Sigma$ such that $\\mathbf{w} \\sigma \\Sigma^{*} \\cap \\mathcal{L} \\neq \\emptyset$ but $\\mathbf{w}^{\\prime} \\sigma \\Sigma^{*} \\cap \\mathcal{L}=$ $\\emptyset$; then $u\\left(q_{0}, \\mathbf{w} \\sigma\\right)=u\\left(q_{0}, \\mathbf{w}^{\\prime} \\boldsymbol{\\sigma}\\right)$ but the set $R(5)$ is reachable from $u\\left(q_{0}, \\mathbf{w} \\sigma\\right)$ but not $u\\left(q_{0}, \\mathbf{w}^{\\prime} \\sigma\\right)$, contradiction. Hence, the SSM's outputs can be transformed, by composing $\\rho$ with a map from states to nextcharacter sets, to predictively model $\\mathcal{L}$. Theorem 21. SSMs with complex-valued coefficients evading both NONNEGATIVE and TIMEINVARIANT can represent all regular languages known to be in $\\mathrm{TC}^{0}$. We we do not use this theorem in the main paper, due to the nonexistence (as far as we know) of implemented SSMs with this property. Proof. SSMs evading both NONNEGATIVE and TIME-INVARIANT can count modulo any integer $k$, using $d=1$ and $A\\left(e_{1}\\right)=e^{2 \\pi i / k}, A\\left(e_{0}\\right)=1, B \\equiv 0, h_{0}=1$.",
    "ssmformal-49": "This is a generalization of the construction for PARITY described in Section B. 2 , since $e^{2 \\pi i / 2}=-1$. The set of regular languages known to be in $\\mathrm{TC}^{0}$ is the set of regular languages whose syntactic monoid contains no non-solvable groups [5]. These languages are recognized by cascade products\nof set-reset automata and automata perfoming modular counting [64]. By the remark above, together with Lemma 9 and Lemma 19, such cascade products can be simulated by SSMs. ## B. 4 Maintaining Counters\n\nAs the first step in showing Theorem 5, we show that SSMs can maintain unbounded counters, and that one can read out the values of such counters, up to finite bounds, even at finite precision:\nLemma 22.",
    "ssmformal-50": "Let $C>0$ be an integer. Let any function $u: \\Sigma \\rightarrow \\mathbb{Z}^{C}$ be given. Let $L \\in \\mathbb{N}$. Then a one-layer SSM with finite precision can compute, at each position $i=1, \\ldots, T$ :\n\n$$\n\\max \\left(\\min \\left(\\sum_{j=1}^{i} u\\left(w_{i}\\right), L\\right),-L\\right)\n$$\n\nin the sense that $\\rho$ can read this out from $z_{i}^{(1)}$ with finite precision.",
    "ssmformal-51": "Proof.",
    "ssmformal-52": "Define $d=2 L+1$. Define $h_{0}=\\mathbf{0} \\in \\mathbb{R}^{d}$. For each $x \\in \\Sigma$, define $A\\left(e_{x}\\right)=\\mathbf{1} \\in \\mathbb{R}^{d}$ and $B\\left(e_{x}\\right)_{i} \\in$ $\\mathbb{R}^{d}$ by $B\\left(e_{x}\\right)_{i}=u(x)$. In order to read out the state $h_{t}$ up to a limit $L$, we define\n\n$$\n\\phi\\left(h_{t}, x_{t}\\right)=\\operatorname{Norm}\\left(h_{t}+[0,1,-1,2,-2, \\ldots,-L, L]\\right)\n$$\n\nBy testing which entries of the result are negative or positive, one can read out the state up to $L$ even after rounding $\\phi\\left(h_{t}, x_{t}\\right)$ to finite precision.",
    "ssmformal-53": "The proof straightforwardly extends to multiple counters. We are ready to prove the theorem:\nTheorem 23. (Repeated from Theorem 5) Consider the languages Dyck-1, Shuffle-Dyck, n-ary Boolean Expressions, $a^{n} b^{n}, a^{n} b^{n} c^{n}$, and $a^{n} b^{n} c^{n} d^{n}$, as defined in Appendix C Each of these is predictively modeled by an SSM. Proof. For each of these languages, we first define an assignment $u: \\Sigma \\rightarrow \\mathbb{Z}^{C}$ :\nFor $a^{n} b^{n}:$ (here, $\\left.C=1\\right)$\n\n$$\n\\begin{aligned}\n& u(a)=1 \\\\\n& u(b)=-1\n\\end{aligned}\n$$\n\nFor Dyck-1: (here, $C=1$ )\n\n$$\n\\begin{aligned}\n& u(\"(\")=1 \\\\\n& u(\") \")=-1\n\\end{aligned}\n$$\n\nFor Shuffle-Dyck- $k$ (here, $C=k$ )\n$u(\"(i \")=(0, \\ldots, 0,1,0 \\ldots 0) \\quad$ where 1 is in the $i$-th slot\n$u(\") i$ \" $)=(0, \\ldots, 0,-1,0 \\ldots 0) \\quad$ where -1 is in the $i$-th slot\nFor $a^{n} b^{n} c^{n}:$ (here, $\\left.C=2\\right)$\n$u(a)=(1,0)$\n$u(b)=(-1,1)$\n$u(c)=(0,-1)$\nFor $a^{n} b^{n} c^{n} d^{n}:$ (here, $\\left.C=3\\right)$\n\n$$\n\\begin{aligned}\n& u(a)=(1,0,0) \\\\\n& u(b)=(-1,1,0) \\\\\n& u(c)=(0,-1,1) \\\\\n& u(d)=(0,-1,-1)\n\\end{aligned}\n$$\n\nFor Boolean Expressions: (here, $C=1$ )\n\n$$\n\\begin{aligned}\n& u(\\langle V A L U E\\rangle)=-1 \\\\\n& u(\\langle n-A R Y\\rangle)=+n\n\\end{aligned}\n$$\n\nFor each of these mappings, we use Lemma 22 at $L=1$ to construct a one-layer SSMs that can, for each of the $C$ counters, distinguish the values $\\leq-1,0, \\geq 1$. In parallel, we pass on the input symbol itself in $\\log |\\Sigma|$ further dimensions. Overall, the output $z_{t}$ of single SSM layer provides, at every position, both the original symbol in $\\Sigma$ and an element of $\\{\\leq-1,0 \\geq 1\\}^{C}$. We can thus view the output of this layer as a string over an enriched string of symbols $\\sigma_{1} \\times \\sigma_{2} \\in$ $\\Sigma \\times\\{\\leq-1,0 \\geq 1\\}^{C}$. Based on this, one can predictively model these languages as follows. For Dyck-1, the next token is EOS or \"(\" if $\\sigma_{2}=0$, and \"(\" or \")\" after any other prefix (note that predictive modeling assumes valid prefixes). Shuffle- $k$-Dyck is similar: EOS is allowed if and only if all counters are zero.",
    "ssmformal-54": "An opening bracket is always allowed. A closing bracket is only allowed if the respective counter is $>0$. For $a^{n} b^{n}$, the next token is $a$ or $b$ if $\\sigma_{1}=a$; $b$ if $\\sigma=(a, \\geq 1)$ or $(b, \\geq 1)$; EOS if $\\sigma=(b, 0)$. Constructions for $a^{n} b^{n} c^{n}, a^{n} b^{n} c^{n} d^{n}$ are similar. For Boolean expressions, the next token is $\\langle n-A R Y\\rangle$ or EOS if $\\sigma_{2}=0$, and any other token otherwise. All of these constructions can be encoded using an appropriate function $\\rho$ applying to $z_{t}$. ## B.5 Bounded-Depth Dyck\n\nDefinition 24. The language $D y c k_{K, h}$ [28 [74] is given by the $C F G$ with the nonterminals $\\left\\{S_{0}, S_{1}, \\ldots, S_{h-1}, S_{h}\\right\\}$ and the following production rules:\n\n$$\n\\begin{aligned}\nS_{h} & \\rightarrow\\left({ }_{1} S_{h-1}\\right)_{1}|\\ldots|\\left({ }_{K} S_{h-1}\\right)_{K} \\mid \\varepsilon \\\\\nS_{h-1} & \\rightarrow\\left({ }_{1} S_{h-2}\\right)_{1}|\\ldots|\\left({ }_{K} S_{h-2}\\right)_{K} \\mid \\varepsilon \\\\\n& \\ldots \\ldots \\\\\nS_{2} & \\rightarrow\\left({ }_{1} S_{1}\\right)_{1}|\\ldots|\\left({ }_{K} S_{1}\\right)_{K} \\mid \\varepsilon \\\\\nS_{1} & \\rightarrow\\left({ }_{1} S_{0}\\right)_{1}|\\ldots|\\left({ }_{K} S_{0}\\right)_{K} \\mid \\varepsilon \\\\\nS_{0} & \\rightarrow \\varepsilon\n\\end{aligned}\n$$\n\nand the start symbol $S_{h}$. Theorem 25. (Repeated from Theorem 6) The bounded-depth Dyck language Dyck ${ }_{K, h}$ is predictively modeled by a two-layer SSM at finite precision, with $d=O(h \\log K)$. Proof. In the first layer, we calculate each token depth up to $h$ using Lemma 22. After the first layer, at each position, the activations will indicate both the depth up to $h$, and the identity of the symbol. The space of activations is thus $\\{0, \\ldots, h\\} \\times\\left\\{(1,)_{1}, \\ldots,\\left({ }_{K},\\right)_{K}\\right\\}$. We then, for each depth $l=1, \\ldots, h$, define a set-reset automaton (Definition 8 given by the set $\\left.Q_{l}:=\\{l\\} \\times\\left\\{(1,)_{1}, \\ldots,{ }_{K},\\right)_{K}\\right\\}$. Running all of these set-reset automata will tell us, for each depth, the identity of the last bracket at that depth. We can deduce the maximum depth $h^{\\prime}$ at which the last bracket is an opening one, and thus infer the set of valid next symbols. The activity of these set-reset automata can, in parallel, be simulated by a second SSM layer using Lemma 9 We need $h$ such automata, and each SSM has width $\\log K$. ## C Definitions of Languages\n\nHere, we provide formal definitions of languages from the test suite based on Bhattamishra et al.",
    "ssmformal-55": "[6]. Descriptions follow Bhattamishra et al. [6], and are included here for self-conrainedness. In all cases, our data generation setup is directly taken from [6]. ## C. 1 Regular Languages\n\nTomita Grammars. Used primarily as a benchmark language family for assessing sequence to sequence models [68], some of the languages in this family are star-free (with dot-depth of 1) and\nsome non-star-free. All the regular languages of the family are defined on the alphabet $\\Sigma=\\{0,1\\}$. Individual language definitions are available in Table 1 . $\\boldsymbol{D}_{\\boldsymbol{n}}$. We follow the definition of [6] to define the $D_{n}$ family of star-free languages. In our experiments we only generate $D_{3}, D_{4}$, and $D_{1} 2$ languages, given the proven equivalence of Tomita- 2 and $D_{2}$. All the languages of the family are defined on the alphabet of $\\Sigma=\\{a, b\\} . D_{n}=\\left(a D_{n-1} b\\right)^{*}$ has level $n$ in the dot-depth hierarchy. PARITY. PARITY is the set of all strings on the alphabet $\\Sigma=\\{0,1\\}$ such that the number of 1 's is even. This language can be easily recognized by a DFA with just two states. Others. We further have the non-star-free languages $(a a)^{*},(a a a a)^{*}$ and $(a b a b)^{*}$, and the star-free languages $a a^{*} b b^{*} c c^{*} d d^{*} e e^{*},\\{a b\\}^{*} d\\{b, c\\}^{*}$, and $\\{0,1,2\\}^{*} 02^{*}$. ## C. 2 Counter Languages\n\nDyck and Shuffle-Dyck. Dyck-1 is defined on the alphabet $\\Sigma=\\{[]$,$\\} and derived using the follow-$ ing CFG production rule: $S \\rightarrow(S)|S S| \\varepsilon$. We further use the family of Shuffle-k languages [51]. Shuffle-Dyck-k is defined in terms of $\\Sigma=$ $\\left\\{(1,)_{1}, \\ldots,(k,)_{k}\\right\\}$. It is defined as the shuffle of $k$ Dyck-1 languages, each defined in terms of the alphabet $\\Sigma_{i}=\\left\\{(i,)_{i}\\right\\}$ where $i=1, \\ldots, k$. $\\boldsymbol{n}$-ary Boolean Expressions. This is the set of valid expressions over various operators. We focus on up-to-3-ary expressions, defined using the following grammar:\n\n$$\n\\begin{aligned}\n& S \\rightarrow\\langle\\text { VALUE }\\rangle \\\\\n& S \\rightarrow\\langle\\text { UNARY OPERATOR }\\rangle S \\\\\n& S \\rightarrow\\langle\\text { BINARY OPERATOR }\\rangle S S \\\\\n& S \\rightarrow\\langle\\text { TERNARY OPERATOR }\\rangle S S S\n\\end{aligned}\n$$\n\nThis language is recognized by a counter automaton [19]. Others We further include the languages of the forms $a^{n} b^{n}, a^{n} b^{n} c^{n}$, and $a^{n} b^{n} c^{n} d^{n}$. | Grammar | Star-Free | Definition |\n| :---: | :---: | :---: |\n| 1 | Yes | $1^{*}$ |\n| 2 | Yes | $(10)^{*}$ |\n| 3 | No | strings without $1^{2 n+1} 0^{2 m+1}$ <br> substrings |\n| 4 | Yes | strings without any 000 's sub- <br> strings |\n| 5 | No | strings with an even number of <br> 01 's and 10's |\n| 6 | No | strings where number of 0 's - <br> number of 1 's is divisible by 3 |\n| 7 | Yes | $0 * 1 * 0 * 1$ |\n\nTable 1: Tomita Grammars\n\n| Language | Model | Bin-1[1, 50] | Bin-2[51, 100] | Bin-3[101, 150] |\n| :---: | :---: | :---: | :---: | :---: |\n| Dyck-1 | Transformer | 100.0 | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 62.6 | 13.91 |\n|  | Mamba2 | 100.0 | 49.1 | 9.5 |\n|  | Mamba3 | 100.0 | 53.95 | 10.0 |\n| Shuffle-2 | Transformer | 100.0 | 100.0 | 93.0 |\n|  | Mambal | 100.0 | 49.5 | 2.3 |\n|  | Mamba2 | 100.0 | 61.5 | 8.2 |\n|  | Mamba3 | 100.0 | 65.5 | 9.7 |\n| Shuffle-4 | Transformer | 100.0 | 100.0 | 98.8 |\n|  | Mambal | 100.0 | 44.4 | 4.3 |\n|  | Mamba2 | 100.0 | 63.8 | 7.2 |\n|  | Mamba3 | 100.0 | 56.2 | 7.8 |\n| Shuffle-6 | Transformer | 100.0 | 99.9 | 94.0 |\n|  | Mambal | 100.0 | 39.4 | 3.4 |\n|  | Mamba2 | 100.0 | 61.2 | 6.75 |\n|  | Mamba3 | 100.0 | 59.6 | 9.85 |\n| Boolean-3 | Transformer | 100.0 | 100.0 | 99.8 |\n|  | Mambal | 99.75 | 65.7 | 7.05 |\n|  | Mamba2 | 99.95 | 47.25 | 2.3 |\n|  | Mamba3 | 100.0 | 73.45 | 8.6 |\n| Boolean-5 | Transformer | 100.0 | 99.8 | 99.0 |\n|  | Mambal | 99.9 | 30.05 | 7.6 |\n|  | Mamba2 | 100.0 | 80.2 | 14.9 |\n|  | Mamba3 | 99.25 | 60.7 | 6.25 |\n| $a^{n} b^{n}$ | Transformer | 100.0 | 100.0 | 100.0 |\n|  | Mambal | 100.0 | 4.1 | 0 |\n|  | Mamba2 | 100.0 | 9.4 | 0 |\n|  | Mamba3 | 100.0 | 21.3 | 0 |\n| $a^{n} b^{n} c^{n}$ | Transformer | 100.0 | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 0 | 0 |\n|  | Mamba2 | 100.0 | 7.6 | 0 |\n|  | Mamba3 | 100.0 | 5.1 | 0 |\n| $a^{n} b^{n} c^{n} d^{n}$ | Transformer | 100.0 | 100.0 | 99.4 |\n|  | Mambal | 100.0 | 4.76 | 0 |\n|  | Mamba2 | 100.0 | 0 | 0 |\n|  | Mamba3 | 100.0 | 0 | 0 |\n\nTable 2: Accuracies on the counter Languages from the Bhattamishra et al.",
    "ssmformal-56": "[6] test suite.",
    "ssmformal-57": "Transformer results reported based on Bhattamishra et al. [6]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count, from the first two bins, are shown in Figure 3. On these languages, there is also a third bin. ## D Experimental Details\n\nAll experiments used the Mamba reference implementation ${ }^{7}$. Unless stated otherwise, we followed the defaults given there ( $d_{\\text {state }}=16, d_{\\text {conv }}=4$, expand $=2$ ), as we found the default combination to work better than other options. We tuned $d_{\\text {model }}$ for each language. ## D. 1 Bhattamishra et al. [6] test suite\n\nData Preparation For all the languages, we use either the data prepared by Bhattamishra et al. [6] or-where not available-their data-generation scripts, allowing full comparability with results they reported for transformers. We used their official code and data release at https://github.com/ satwik77/Transformer-Formal-Languages (last commit 48eea2e; MIT license). Training sets\n\n[^6]| Language | Model | Bin-1[1, 50] | Bin-2[51, 100] |\n| :---: | :---: | :---: | :---: |\n| Tomita 1 | Transformer | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| Tomita 4 | Transformer(pos enc) | 100.0 | 92.4 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| Tomita 7 | Transformer | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| Tomita 2 | Transformer | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $a a^{*} b b^{*} c c^{*} d d^{*} e e^{*}$ | Transformer | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $\\multirow{4}{*}{a, b}^{*} d\\{b, c\\}^{*}$ | Transformer | 100.0 | 100.0 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $\\multirow{4}{*}{0,1,2}^{*} 02^{*}$ | Transformer | 100.0 | 68.7 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $D_{2}$ | Transformer | 74.6 | 3.1 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $D_{3}$ | Transformer | 80.9 | 8.5 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $D_{4}$ | Transformer | 90.2 | 3.3 |\n|  | Mamba1 | 100.0 | 100.0 |\n|  | Mamba2 | 100.0 | 100.0 |\n|  | Mamba3 | 100.0 | 100.0 |\n| $D_{12}$ | Transformer | 95.18 | 1.5 |\n|  | Mamba1 | 93.65 | 93.35 |\n|  | Mamba2 | 99.9 | 95.55 |\n|  | Mamba3 | 99.99 | 99.85 |\n\nTable 3: Accuracies on the regular Languages from the Bhattamishra et al. [6] test suite - 1st half.",
    "ssmformal-58": "Transformer results reported based on Bhattamishra et al. [6]. For Mamba, we report best settings (chosen based on inputs of length $[1,50]$ ) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count are also shown in Figure 3. typically consist of 10 K samples, with lengths varying between 1 to 50 . There are two heldout bins: one with in-distribution lengths ( $[1,50]$ ), and one testing length generalization (lengths [51,100]). The first one was used for hyperparameter optimization. Each bin typically contains around 2 K samples. However for languages such as $a^{n} b^{n}$, where the number of positive examples in each bin was limited, all possible examples for that bin are included. | Language | Model | Bin-1[1,50] | Bin-2[51, 100] |\n| :---: | :---: | :---: | :---: |\n| Parity | Transformer | 68.7 | 0 |\n|  | Mamba1 | 26.95 | 0 |\n|  | Mamba2 | 80.05 | 4.15 |\n|  | Mamba3 | 91.15 | 16.7 |\n| $(a a)^{*}$ | Transformer | 100.0 | 0 |\n|  | Mambal | 2.1 | 0 |\n|  | Mamba2 | 2.1 | 0 |\n|  | Mamba3 | 4.2 | 0 |\n| $(a a a a)^{*}$ | Transformer | 100.0 | 0 |\n|  | Mamba1 | 0 | 0 |\n|  | Mamba2 | 0 | 0 |\n|  | Mamba3 | 4.0 | 0 |\n| $(a b a b)^{*}$ | Transformer | 100.0 | 2.5 |\n|  | Mambal | 0 | 0 |\n|  | Mamba2 | 0 | 0 |\n|  | Mamba3 | 0 | 0 |\n| Tomita 3 | Transformer | 75.4 | 10.8 |\n|  | Mamba1 | 25.99 | 12.49 |\n|  | Mamba2 | 36.88 | 17.05 |\n|  | Mamba3 | 60.85 | 29.37 |\n| Tomita 5 | Transformer | 29.3 | 0.0 |\n|  | Mamba1 | 15.94 | 0 |\n|  | Mamba2 | 34.5 | 0 |\n|  | Mamba3 | 38.4 | 0 |\n| Tomita 6 | Transformer | 88.8 | 0 |\n|  | Mamba1 | 7.2 | 0 |\n|  | Mamba2 | 37.8 | 0 |\n|  | Mamba3 | 54.56 | 0.04 |\n\nTable 4: Accuracies on the regular Languages from the Bhattamishra et al. [6] test suite - continued.",
    "ssmformal-59": "Transformer results reported based on Bhattamishra et al. [6]. For Mamba, we report best settings (chosen based on inputs of length [1,50]) at 1 (Mamba1), 2 (Mamba2), 3 (Mamba3) layers. Results for the best-performing layer count are also shown in Figure 3 . Hyperparameters For each language, we conducted extensive hyperparameter search. We varied the $d_{\\text {model }}$ parameter in Mamba across the set $\\{16,32,64,128,256\\}$. Additionally, we experimented with the number of layers in our model, ranging from 1 to 3 , training each configuration for 100 epochs. For languages where Mamba performed well, this number of layers was sufficient. However, for languages where Mamba struggled, we increased the number of layers up to 12, with little to no success. We used the AdamW optimizer. To identify optimal learning rates, we started with a coarse hyperparameter search using values from the set $\\{0.001,0.0001,0.00001\\}$. If one of these learning rates showed high performance, we conducted a more fine-grained search to find the optimal learning rate. Finally, we varied the batch size from $\\{16,32,64\\}$ for datasets with 10 K training examples. For languages like $a^{n} b^{n}$ with limited training size, we searched for an optimal batch size within the set $\\{5,10\\}$. ## D. 2 FlipFlop\n\nWe obtained the dataset of Liu et al. [45] from their release, https: / /huggingface.co/datasets/ synthseq/flipflop(MIT license).",
    "ssmformal-60": "Our setup corresponds to the deterministic (\"clean\") mode in Liu et al.",
    "ssmformal-61": "[45]. Matching Figure 2 in Liu et al. [45], we evaluated both with in-distribution data (matching the distribution of the training dataset) with $p_{i}=0.8, p_{w}=0.1, p_{r}=0.1$, and using an out of distribution sparse tail with $p_{i}=0.98, p_{w}=0.01, p_{r}=0.01$, where $p_{i}, p_{w}, p_{r}$ refer to the probabilities of that instruction appearing in input sequences. ![](https://cdn.mathpix.com/cropped/2024_09_17_101ff39892b808f88e98g-29.jpg?height=462&width=889&top_left_y=249&top_left_x=605)\n\nFigure 5: Mamba Accuracy on $D y c k_{8,10}$, on the development set (length $\\leq 700$, same length range as training set) and test set (length $700 \\leq n \\leq 1400$ ). The latter is also plotted in Figure 4\n\nWe trained a one-layer Mamba with the default parameter ${ }^{8}$, setting $d_{\\text {model }}$ to 16 with the AdamW optimizer using a learning rate of $3 \\times 10^{-4}$ and a batch size of 16 . Following the evaluation criteria for LSTMs in Liu et al. [45], we compute the test every 100 training steps on our validation sets of choice, by randomly sampling around $10^{3}$ samples from each set in every evaluation cycle. ## D. 3 Bounded Hierarchical Structure\n\nWe built on the official code and data release of Yao et al. [74] at https://github.com/ princeton-nlp/dyck-transformer (last commit: 5d21fcf). We train a 2-layer Mamba and a 1-layer Mamba on Dyck ${ }_{K, h}$ with $K=8$ and $h=10$. The training set and the validation set contains samples of lengths $\\leq 700$, while the test set contains samples of lengths $700 \\leq n \\leq 1400$. We train Mamba with a varying number of layers $l \\in\\{1,2\\}$ and $d_{\\text {model }} \\in\\{20,30,40,50,60,70,80,90,100\\}$. We use the Adam optimizer with an initial learning rate of 0.01 or 0.001 , using cross-entropy loss. After training for 100 epochs (with early stopping allowed in case of convergence), we select the learning rate with the better training performance. ## E Finite Precision Assumption\n\nAs described in Section 2, we adopt the finite precision notion used by Weiss et al. [70]: We allow an unbounded number of integer bits, but only $p$ fractional bits, where $p$ is a sufficiently large constant (e.g., $p=8$ ), independent of the length of the input. There are a variety of related precision notions in the theoretical literature on neural sequence models - here, we discuss the effect of other notions on our results:\n\n1. Infinite precision Infinite precision allows any parameter and intermediate value to be an arbitrary number. Such a setting is unrealistic, as it would allow encoding arbitrary detail about the input into infinite precision [e.g. 63] and read these out with sufficiently powerful functions $(A, B, \\phi)$ in (5) - this would lead to the unrealistic conclusion that any function and language could be represented. For this reason, theoretical work has often adopted restricted precision notions. 2. Finite inventory of values, where integer and fractional bits are both restricted. Such a setup may be justified based on the fact that any real computer has bounded memory, though such a setup precludes any positive results on non-finite-state problems for any computational architecture ${ }^{9}$\n[^7]Such a restrictive setup would not affect our positive results on Flip-Flop, Star-Free, and bounded-depth Dyck languages (Theorems 1, 4, 6, as these all use bounded finite-precision activation values. As this is a more restricted setup than the one we are assuming, this also would not affect our negative results about PARITY and non-star-free languages (Theorems 24. These results are thus highly robust to variations of the finite precision assumption. Such a more restrictive definition would, however, mean that, for unbounded counting (Theorem 55, modeling is only possible up to a bound determined by the number of possible values-this is the one place where our results would be impacted. Indeed, we do observe that Mamba learns these counter languages on training lengths but struggles with length generalization. Transformers, on the other hand, can represent these languages with bounded activations (due to the constructions in [6]), and show strong length generalization. An intermediary between infinite and finite precision is notions of precision where the number of allowed bits slowly increases with the input length, e.g., logarithmically. Such a setup has particularly been adopted for transformers [48], because a finite-precision assumption leads to very low expressivity in transformers. For SSMs, on the other hand, we find that finite precision assumptions are sufficient for showing a broad range of positive results. [^0]:    ${ }^{1}$ It is the union of $\\varepsilon$ with the intersection of $0 \\Sigma^{*}, \\Sigma^{*} 1$, with the complements of $\\Sigma^{*} 00 \\Sigma^{*}$ and $\\Sigma^{*} 11 \\Sigma^{*}$. [^1]:    ${ }^{2}$ At all other positions, predictive modeling is trivial, as it only needs to consider the input symbol at that position. [^2]:    ${ }^{3}$ Theorem 1 constructs a two-layer SSM. We hypothesize that Mamba uses its local convolution (Remark 18, to replace the lower layer from the construction in Theorem 1\n\n[^3]:    ${ }^{4}$ A closely related notion is the semiautomaton, which is the notion considered in the closely related work Liu et al.",
    "ssmformal-62": "[44]. Semiautomata lack a fixed start state $q_{0}$. We include $q_{0}$, but this difference is not substantial for our formal results. [^4]:    ${ }^{5}$ Informally, in binary, $0.0111111 \\ldots 111$ and 0.1 are arbitrarily close. [^5]:    ${ }^{6}$ In fact, using a binary encoding of $\\Sigma$, one can achieve $d=4 \\log |\\Sigma|$. [^6]:    ${ }^{7}$ https://github.com/state-spaces/mamba/blob/main/README.md\n\n[^7]:    ${ }^{8}$ From https://github.com/state-spaces/mamba/blob/main/README.md\n    ${ }^{9}$ For instance, a Turing machine with bounded memory and thus a bounded tape is equivalent to a finite-state automaton. "
}