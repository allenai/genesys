{
    "ssmformal-0": "The Expressive Capacity of State Space Models: A Formal Language Perspective\n\nYash Sarrof, Yana Veitsman, Michael Hahn Saarland Informatics Campus Saarland University, Germany {ysarrof, yanav, mhahn}@lst.uni-saarland.de\n\nAbstract\n\nRecently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers.",
    "ssmformal-1": "However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today\u2019s large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that\u2014while formulated in terms of iterative state updates\u2014allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is well-understood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in , a circuit complexity class that is known to also cover transformers [48, 65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings \u2013 a problem well within . However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs\u2019 abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of . For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. 2 Background: State Space Models\n\nSSM Layers\n\nWe define a single layer of a state space model as a map, at input length ,\n\n\u211d T \u00d7 d \u2192 \u211d T \u00d7 d \u2192 superscript \u211d \ud835\udc47 \ud835\udc51 superscript \u211d \ud835\udc47 \ud835\udc51 \\displaystyle\\mathbb{R}^{T\\times d}\\rightarrow\\mathbb{R}^{T\\times d} ( x t ) t = 1 , \u2026 , T \u21a6 ( z t ) t = 1 , \u2026 , T maps-to subscript subscript \ud835\udc65 \ud835\udc61 \ud835\udc61 1 \u2026 \ud835\udc47 subscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc61 1 \u2026 \ud835\udc47 \\displaystyle(x_{t})_{t=1,\\dots,T}\\mapsto(z_{t})_{t=1,\\dots,T}\n\ngiven by the recurrence\n\nh t = subscript \u210e \ud835\udc61 absent \\displaystyle h_{t}= A \u200b ( x t ) \u2218 h t \u2212 1 + B \u200b ( x t ) \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 subscript \u210e \ud835\udc61 1 \ud835\udc35 subscript \ud835\udc65 \ud835\udc61 \\displaystyle A(x_{t})\\circ h_{t-1}+B(x_{t}) z t = subscript \ud835\udc67 \ud835\udc61 absent \\displaystyle z_{t}= \u03d5 \u200b ( h t , x t ) italic-\u03d5 subscript \u210e \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \\displaystyle\\phi(h_{t},x_{t}) (1)\n\nwhere denotes elementwise product, and, for each ,\n\nh 0 \u2208 subscript \u210e 0 absent \\displaystyle h_{0}\\in \u211d d superscript \u211d \ud835\udc51 \\displaystyle\\mathbb{R}^{d} B \u200b ( x t ) \u2208 \u211d d \u200b (increment) \ud835\udc35 subscript \ud835\udc65 \ud835\udc61 superscript \u211d \ud835\udc51 (increment) \\displaystyle B(x_{t})\\in\\mathbb{R}^{d}\\ \\text{(increment)} A \u200b ( x t ) \u2208 \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 absent \\displaystyle A(x_{t})\\in \u211d d \u200b (gate) superscript \u211d \ud835\udc51 (gate) \\displaystyle\\mathbb{R}^{d}\\ \\text{(gate)} \u03d5 : \u211d 2 \u200b d \u2192 \u211d d \u200b (transform) : italic-\u03d5 \u2192 superscript \u211d 2 \ud835\udc51 superscript \u211d \ud835\udc51 (transform) \\displaystyle\\phi:\\mathbb{R}^{2d}\\rightarrow\\mathbb{R}^{d}\\ \\text{(transform)}\n\nWe allow to be arbitrary smooth maps.",
    "ssmformal-2": "The map includes a cascade of channel-mixing transformations and normalization, which we abstract as follows:\n\n\u03d5 \u200b ( h t , x t ) = Mix 1 \u2061 ( Norm \u2061 ( Mix 2 \u2061 ( h t , x t ) ) , x t ) italic-\u03d5 subscript \u210e \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 subscript Mix 1 Norm subscript Mix 2 subscript \u210e \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \\phi(h_{t},x_{t})=\\operatorname{Mix}_{1}(\\operatorname{Norm}(\\operatorname{Mix}_{2}(h_{t},x_{t})),x_{t}) (2)\n\nwhere can contain linear or (Swi)GLU components [e.g. 56, 23]. We will take to implement RMSNorm [75]; LayerNorm [4] can be covered by absorbing centering into . A Full SSM\n\nReal-world SSMs typically stack several layers of the form (1\u20132). Where needed, we use superscripts to indicate the layers in an SSM: , where is the number of layers. We consider input words over a discrete alphabet , and assume an encoding in terms of token embeddings , for . We will also write for . These feed into the lowest layer as . The outputs of each layer feed into the next layer, as . The transformations in (1) are specific to each layer: and similarly for . To keep notation simple, we will only show the superscripts where necessary for disambiguation. The activations at the highest layer are read out by some neural network into vectors describing classification or next-token predictions. We again take to be an arbitrary function; importantly, all our constructions will allow to operate correctly even at finite precision. Implementation Choices\n\nIn Mamba, (1) directly maps onto Eqs. (2a) and (2b) in Gu and Dao [23]. The notation of Gu and Dao [23] use a matrix multiplication instead of elementwise multiplication in (1), but importantly, Mamba\u2019s is diagonal, so we can take . Some SSMs assume nondiagonal , but typically this matrix is diagonalizable [e.g. 24, 67], so that the SSM is still equivalent to one of the form (1). We discuss how other SSMs instantiate (1) in Appendix A. Some models assume complex-valued activations (Appendix A); our results largely do not depend on this distinction, but take it into account where needed (Theorem 13). Some SSMs [e.g. 23] use different numbers of channels in and using state expansion; as this does not affect expressive capacity, we will simply assume a constant dimensionality . Local convolutions [e.g. 21] can be simulated with an SSM layer and do not increase expressive capacity (Remark 18). We will find that two design choices have nontrivial impact on expressive capacity: The first one is time invariance: we call an SSM time-invariant if does not depend on . Some SSMs, such as S4 [24] and Retnet [67] are time-invariant; Mamba [23], Griffin [14], GLA [72], HGRN [57, 56], QRNN/SRU [8, 41] are not (Appendix A). The second one is the sign of the entries of : Across all non-time-invariant SSMs surveyed, we find that the gate is always nonnegative (Appendix A): (nonnegative) due to exponential or sigmoid parameterizations of the gate \u2013 this choice turns out to limit expressive capacity (Theorem 2). Role of Parameterization\n\nWhile the abstract form (1\u20132) is common across the SSM literature, differences in parameterization may have substantial effect on efficiency and training stability. Studying expressiveness allows us to abstract away from these differences to a remarkable degree: We will allow to be arbitrary functions with the given input-output properties. Our negative results are based on abstract properties of the setup (1\u20132), which fundamentally bottlenecks SSMs through elementwise linear state updates. For our positive results, will use empirical learnability experiments to verify that learnable solutions instantiating them (though not necessarily implementing the same constructions as used in the proofs) do exist in a recent SMM [Mamba, 23]. Traditional RNNs\n\nWe contrast SSMs with traditional RNNs such as simple RNNs or LSTMs: for these, the recurrence in Eq. (1) is replaced by\n\nh t = \u03c8 \u200b ( h t \u2212 1 , x t ) subscript \u210e \ud835\udc61 \ud835\udf13 subscript \u210e \ud835\udc61 1 subscript \ud835\udc65 \ud835\udc61 h_{t}=\\psi(h_{t-1},x_{t}) (3)\n\nwhere could be linear, an MLP [17], or a more complex gated function [29]. Finite Precision Assumption\n\nWhile Eq.(1) assumes arbitrary real-valued activations, real-world implementations can only represent numbers with bounded precision. Formally, we adopt the finite precision notion used by Weiss et al. [70] in a study of the expressive power of traditional RNNs: We allow an unbounded number of integer bits, but only fractional bits, independent of the length of the input. See Appendix E for discussion. 3 Theoretical Results\n\n3.1 Motivation\n\nWe study three foundational types of data structures needed for modeling formal languages [30]: finite state automata (Theorem 1, 2, 4), counters (Theorem 5), and stacks (Theorem 6). All of these data structures can be understood in two equivalent forms: One is to track a state sequence over an input, where each input symbol engenders a specific transformation on the state. The other one\u2014more commonly considered in research on expressive capacity and closer to language modeling\u2013considers formal languages\u2014sets of finite strings that are defined by the property that an automaton reaches one of a pre-specified set of \u201caccepting\u201d states after traversing the word. We will focus on the latter perspective, enabling easy comparison with existing results on transformers and RNNs. A finite-state-automaton (see Definition 7) represents a general state tracking problem over a finite state space, without imposing further structure on the state space: The automaton keeps track of a single state from a finite state space; when reading a string from left to right, each symbol engenders a specific transformation of the state. At each position, the current state determines which symbols can come next; membership in a formal language is determined by the state reached after reading the full string. Finite-state-automata are equivalent in expressivity to regular expressions, and define the regular languages [37]. Indeed, we will be able to provide a precise criterion identifying which such finite state tracking problems\u2013or equivalently, regular languages\u2013SSMs such as Mamba are capable of in the finite-precision setting (Theorem 4). Allowing an automaton to keep track of one or more counters [20]\u2014integers that are incremented or decremented at each symbol read\u2014turn the state space infinite, but in a highly structured manner. SSMs can model this datastructure (Theorem 5), as can RNNs and transformers [70, 6]. Stacks, a first-in-first-out datastructure, enable automata to keep track of hierarchical structure, foundational to natural language [11]. We show that SSMs can implement shortcut solutions to bounded hierarchical structure even without implementing a stack (Theorem 6) \u2013 these are likely to be most useful to natural language given the boundedness of human memory [50, 36]. 3.2 Background: Formal Language Prediction and Recognition\n\nWe fix a finite alphabet . Its elements are called characters or symbols. The set of all finite strings over is denoted ; such strings are often referred to as words. The length of is denoted . A formal language is a subset of . Techically, we assume that the alphabet includes BOS and EOS symbols, which occurs at the beginning and end of each element of and nowhere else. We next need to define what it means for an SSM to model a formal language. A standard notion is the notion of recognition, where the task is to classify a full string as belonging to the language or not. Formally, for an SSM with , we say that it recognizes a language if the output equals\u2014when the SSM is run on \u20141 if and 0 else. However, such a classification task is arguably not always matched to dominant use cases in predictive sequence modeling, where the task is to predict the next token at each step. Thus, we also cast formal languages into a language modeling and sequence prediction framework. We adopt the task of Bhattamishra et al. [6], where the model is asked to output at each step in a sequence the set of possible next symbols. Let the set of valid prefixes of . We then say that a model predictively models a language if (Figure 1), given a valid prefix , it outputs the finite set\n\n{ \u03c3 \u2208 \u03a3 : w \u200b \u03c3 \u200b \u03a3 \u2217 \u2229 L \u2260 \u2205 } conditional-set \ud835\udf0e \u03a3 \ud835\udc64 \ud835\udf0e superscript \u03a3 \ud835\udc3f \\{\\sigma\\in\\Sigma:w\\sigma\\Sigma^{*}\\cap L\\neq\\emptyset\\} (4)\n\nWe think of each such set as an atomic label; the set of possible labels is the power set of the finite alphabet . Here, . Predictive modeling can be easily converted into recognition by checking whether any symbol in the sequence is not in the predictive set at the preceding position; this can be done by adding 1 SSM layer. Conversely, if we can show that SSMs cannot recognize a language, this proves they also cannot perform predictive modeling for it, as they then cannot correctly predict where EOS can appear. To get the strongest results, we thus prove positive results for predictive modeling, and negative results for recognition. 3.3 Length-Generalizing Representations for Flip-Flop State Tracking\n\nAs motivated in Section 3.1, we begin with finite-state-automata \u2013 equivalently, regular languages, a well-studied setting for understanding the expressive power of transformers [e.g. 25, 6, 3, 44, 45] and a fundamental model of state tracking when the number of possible states is finite [45, 49]. Traditional RNNs can emulate all finite-state-automata at finite precision [31, 32]. Starting from two specific languages, Flip Flop (Section 1) and PARITY (Section 2), we derive an exact characterization of the regular languages modeled by a broad class of SSMs at finite precision (Theorem 4). We begin on the positive side, by establishing a case where SSMs pattern with RNNs in avoiding a failure mode of self-attention. The Flip Flop languages [45] are a simple instance of state tracking defined in terms of write, read, and ignore instructions. Each write instruction comes with a piece of information; whenever a read instruction is encountered, the information written by the last write instruction is recalled. Formally, define to be the set of finite strings over , where , , and where the bit following any matches the bit following the last preceding occurrence of . Liu et al. [44] show that the Flip Flop language, as an abstraction, is a fundamental ingredient of many long-range reasoning settings. It can be represented with a small finite-state-automaton, and LSTMs learn well [45]. Transformers can in principle represent it [44, 45], though known constructions are not inherently length-generalizing. Indeed, transformers empirically struggle in generalization [45]; intuitively, this may happen because attention heads aggregate information in a commutative manner, and reliably attending to the last write instruction requires strong position dependence in the attention weights. SSMs pattern with traditional RNNs: they can easily represent Flip Flop at arbitrary input lengths:\n\nTheorem 1. A two-layer SSM with finite precision predictively models at arbitrary lengths. In the construction, the first layer records the last instruction token, achieved in (1) by setting , and , and setting . Additional dimensions forward the current token to . In the output of the first layer , whenever the input is 0 or 1, the model now has access both to the current token and the preceding token , which must have been an instruction. Based on this information, the model can set the gate to overwrite the state with the current input token when the preceding token was , and pass along the state unaltered otherwise. This, together with , is sufficient for always identifying the legal next symbols in . The formal proof is in Appendix B.1. 3.4 Difficulty of PARITY\n\nConversely, we next establish a design choice in SSMs which limits their power in emulating finite-state-automata, establishing \u2013 in the finite-precision setting \u2013 an even stronger separation between existing SSM variants and traditional RNNs than the circuit complexity arguments in Merrill et al. [49]. The language PARITY, the language of bitstrings with an even number of ones, is recognized by a finite-state automaton with 2 states, and is straightforwardly encoded into a traditional RNN, even a linear one, with finite precision. PARITY, while in principle expressible for transformers [9], is empirically very hard to learn for transformers [6, 15], as it can provably only be represented in sharp minima [26]. A sufficiently general SSM could easily recognize it at by setting , , , , so that the sign of the single entry of indicates the parity. Such an SSM would need to be non-time-invariant and require negative or complex gate values; i.e., satisfy neither time-invariant nor nonnegative. Such properties are indeed provably needed:\n\nTheorem 2. No SSM satisfying nonnegative can recognize PARITY at arbitrary input lengths with finite precision. In particular, this applies to Mamba. The proof is in Appendix B.2; it examines inputs of the form and shows that the activations converge as , and thus cannot reliably encode the parity of . As we show in Theorem 13, the same result holds even for SSMs evading nonnegative when they are time-invariant, at least when the coefficients have rational angles in the complex planes. All extant SSMs we surveyed (Appendix, Section A) satisfy either Nonnegative or time-invariant. Hypothetical SSMs evading both nonnegative and time-invariant would be strictly stronger and can represent not only PARITY, but all regular languages known to be in (Theorem 21). 3.5 Which Regular Languages can SSMs Model? We now combine Theorems 1 and 2 to derive an exact characterizations of the regular languages that modern non-time-invariant SSMs such as Mamba can recognize or predictively model \u2013 the two notions turn out to coincide here \u2013 in the finite-precision setting. The key insight is that and PARITY are fundamental building blocks of two classes of regular languages: the star-free languages and their complement, the non-star-free languages [60, 46]:\n\nDefinition 3. A regular language is star-free if it can be defined using regular expressions involving only the empty set, the empty string, individual symbols, concatenation, and Boolean combinations \u2013 avoiding the Kleene star operation. is star-free: there is a way to define it without Kleene star. PARITY is not star-free; any regular expression for it must involve the Kleene star. Even languages whose intuitive definition involves the Kleene star may turn out to be star-free. For example, has an expression avoiding the star.111It is the union of with the intersection of , , with the complements of and . Equivalently, a language is star-free if it can be defined logically using only first-order quantifiers and the order relation [60]. Also, is non-star-free if and only if recognizing it involves counting modulo some [46]; PARITY being the simplest example. Modern non-time-invariant SSMs such as Mamba cannot perform modulo counting, but they can model all star-free languages:\n\nTheorem 4. The class of SSMs satisfying nonnegative can predictively model a regular language at finite precision if and only if is star-free. The proof in Appendix B.3 uses the Krohn-Rhodes theorem [39] to reduce all star-free languages to flip flop-like state tracking. Importantly, there are well-known constructive criteria for deciding whether a given automaton defines a star-free language [60]; hence, we have a decidable criterion for the finite-state tracking problems that such SSMs satisfying Nonnegative can solve. This is much simpler than the situation for transformers, where an exact characterization of their power within the regular languages is complicated: Angluin et al. [3] show that a certain formal abstraction of transformers (masked unique hard attention) also recognizes exactly the star-free languages, but constructions of realistic transformers via Krohn-Rhodes in Liu et al. [44] do not inherently length generalize, and empirical research indicates difficulty in generalization even for certain simple star-free languages [6, 45]. Known length-generalizing constructions are limited to very simple subclasses such as the piecewise testable languages [71]. In contrast, for SSMs we have a single model per language, at finite precision and for arbitrarily long inputs. Thus, we expect that the SSM architecture confers an advantage in star-free state tracking problems when compared to transformers \u2013 a prediction we will find supported experimentally (Figure 3). 3.6 Unbounded Counting\n\nHaving characterized the regular languages modeled by SSMs, we now consider languages requiring unbounded counting [20], specifically, languages recognized by keeping track of one or more counters, where each character causes a specific increment or decrement to each counter [38, 27, 70, 40]. A prime example is the Dyck-1 language of well-formed strings over \u201c(\u201d and \u201c)\u201d; here a counter is incremented (decremented) whenever an opening (closing) bracket is encountered; a string is well-formed if and only if the counter is 0 at the end of the string. Some other relevant formal languages are Shuffle-Dyck- (the shuffles of multiple Dyck-1 languages), \u2013 here, increments the counter and decrements it, and \u2013 here, there are two counters, one keeping track of and one of (See Appendix C.2). Such counter languages are fundamental as basic context-free (Dyck-1, ) or context-sensitive (e.g., ) languages [30], and have been the subject of studies of both transformers [6] and RNNs [70]. Many such languages are modeled by SSMs:\n\nTheorem 5. Consider the languages Dyck-1, Shuffle-Dyck-, n-ary Boolean Expressions, , , and , as defined in Appendix C.2. Then each of these is predictively modeled by an SSM. The proof is in Appendix B.4. Intuitively, an SSM can directly implement the required counters by setting and by defining to the be increment or decrement cased by . In modeling such languages, SSMs pattern with both transformers [6] and LSTMs [70]. It may seem counterintuitive that nonnegative SSMs can perform unbounded counting but (by Theorem 2) not modular counting\u2014the latter would seem to just require reading out the value of an unbounded counter. What is key is that, even though can encode unbounded counts, reading out the modular value of an unbounded integer is a formidable problem for typical neural network nonlinearities, in particular when the information has been pushed through normalization (2). We should note that there is a qualitative difference between this result and the preceding positive results about finite-state languages (Theorems 1 and 4), in that the construction in Theorem 5 uses unboundedly large entries in the state , whereas Theorems 1 and 4 use bounded values at finite precision. Indeed, we will find better length generalization in the finite-state case (Figure 3). A consequence of Theorem 5 is that SSMs can recognize some languages transcending the context-free languages, as is not context-free. A second application of the theorem, of great linguistic interest, is to bounded hierarchical structure, as we discuss next. 3.7 Bounded Hierarchical Structure without Stacks\n\nIt is generally agreed that hierarchical structure is a key aspect of language, and that comprehending language at a human-like level requires the computational ability to process such structures [12, 43, 18]. The fundamental data structure for processing hierarchical structure is the stack, where information is stored and removed as one traverses to higher and lower levels of hierarchical embedding [30]. We now show that SSMs\u2019 counting ability can offer shortcuts even on such languages modeling hierarchical structure, eschewing the need for a stack. A useful abstraction of hierarchical structure as relevant to natural language is the family of Dyck languages. The bounded-depth Dyck language with types of parentheses and depth is the language of well-bracketed strings over , , , , , such that the number of yet unclosed brackets never exceeds in any prefix [28, 74]. The classical unbounded-depth Dyck languages \u2013 the limit where \u2013 play a fundamental role as the backbone of context-free languages via the Chomsky-Sch\u00fctzenberger theorem [12]. Bounding the depth reflects the fact that deep embedding is rare in natural language [36, 7]. Prior work has found that two-layer transformers [73] and traditional RNNs [28, 6] both model all languages. The same turns out to hold for SSMs:\n\nTheorem 6. The bounded-depth Dyck language is predictively modeled by a two-layer SSM at finite precision, with . The proof is in Appendix B.5. Intuitively, the first layer records the depth of each parenthesis using the ideas from Theorem 5, and the second layer keeps track of the last open bracket at each depth using Theorem 1. We note that, since is star-free, Theorem 4 already guarantees the existence of representing SSMs, but the depth and width guaranteed by Theorem 6 is likely to be much better than what would be obtained by a black-box application of Theorem 4: As Hewitt et al. [28] show, units is optimal up to constants and is attained by traditional RNNs and LSTMs. The SSM construction is very different from that of Hewitt et al. [28] for traditional RNNs (both simple RNNs and LSTMs), which directly simulates a stack. Our construction is similar to the transformer construction in Theorem 4.2 in [73], which however has to rely on specific positional encodings, unlike the SSM construction. This highlights that stacks are not the only way of simulating bounded hierarchical structure in recurrent architectures, and non-stack-based strategies can even attain the same optimal scaling of hidden units. Probing whether such stack-free shortcuts are learned by SSM-based LLMs is an exciting problem for future research. 4 Experiments\n\nWe have derived a fine-grained theoretical characterization of expressiveness strengths and limitations of SSMs. We now show that our positive results can be instantiated and learned in a realistic SSM implementation, by evaluating a recent highly successful SSM, Mamba [23]. FlipFlop\n\nWe empirically instantiate Theorem 1 using the dataset released by [45], reflecting the language as defined in Section 3.3. Matching Figure 2 in Liu et al. [45], we evaluated both with in-distribution data, and with out-of-distribution data where the distance between read and write instructions tended to be larger. We evaluate for predicting the bits following instructions222At all other positions, predictive modeling is trivial, as it only needs to consider the input symbol at that position., matching the \u201cdeterministic/clean\u201d mode of Liu et al.",
    "ssmformal-3": "[45], and considered predictions to be correct only if all predictions within a sequence were correct. We provide further details in Appendix D.2. A small one-layer333Theorem 1 constructs a two-layer SSM. We hypothesize that Mamba uses its local convolution (Remark 18) to replace the lower layer from the construction in Theorem 1. Mamba model converged to 0 error in both validation sets after 1400 steps (Figure 2), compared to 500 steps for an LSTM reported by Liu et al.",
    "ssmformal-4": "[45]. In contrast, Liu et al. [45] found transformers to keep making occasional mistakes despite training for 10K steps. Test Suite from Bhattamishra et al. [6]\n\nTo test our theoretical results on regular and counter languages (Theorems 2, 4, 5), we test Mamba on 27 formal languages, including 18 regular languages and 9 counter languages, based on a prior study comparing transformers and RNNs [6]. The regular languages include a popular benchmark [68] and various regular expressions; 11 are star-free.",
    "ssmformal-5": "The counter languages include the languages covered by Theorem 5. Definitions of all languages are provided in Appendix C. We chose this test suite because it precisely covers Theorems 4 and 5, and we have proven (in)expressibility results for each language in the set. Following [6], we trained the model for predictive modeling, i.e., at each step, the model outputs a label indicating the set of possible next characters (4), including EOS when required. Following [6], we count the model\u2019s response on an input string as correct if and only if predictive modelling was successful at all positions in the input. Such a evaluation setup makes random baselines fairly low, where a random predictor would have an accuracy exponentially small in in each of the positions. Training inputs have length in [1,50]; the model is then evaluated on held-out bins with length [1,50] and [51,100]. Further experimental details are in Appendix D.1. We show our Mamba results, together with Transformer results reported by Bhattamishra et al. [6], in Figure 3. LSTMs perform perfectly on all languages, and are thus not shown. In a striking confirmation of Theorem 4, Mamba learns all star-free languages with strong length generalization, and does poorly on all non-star-free languages. Transformers show somewhat more mixed performance, and do not always length-generalize even on the star-free languages. In agreement with Theorem 5, Mamba, similar to Transformers, learns the counter languages, though it struggles with length generalization when compared to transformers. The differences in Mamba\u2019s performance between star-free and counter languages may stem from the fact that the construction for the former class (Theorem 4) is able to use finite precision and bounded state values at arbitrary input lengths, while the latter (Theorem 5) uses unbounded state values. Bounded Hierarchical Structure\n\nTo test Theorem 6, we recreate the experimental setup from Yao et al. [74]. Matching their Figure 4, we trained Mamba to predictively model at and . The training and the validation set contained samples of length , while the test set contained samples of length . Yao et al. [74] found both transformers and LSTMs achieved strong performance on this setup. We provide further details in Appendix D.3. Recall that Theorem 6 shows that two-layer SSMs can predictively model . We trained Mamba with 1 or 2 layers and varying dimensionality, finding that two layers can achieve essentially perfect performance across model sizes, even on the test set (Figure 4 and 5). 5 Discussion\n\nRelated Work\n\nOur work belongs to an incipient line of research into the expressiveness of SSMs [33, 49]. It is closely related to a long string of work studying the expressive capacity of neural sequence models, which has so far focused on recurrent networks [e.g. 62, 6, 28] and, more recently, self attention [e.g. 10, 48, 66]. A second link is to the classical and long-standing study of linear dynamical systems and control theory [34]. For instance, Theorem 2 relies the asymptotic convergence of an SSM on certain inputs, establishing a link to the asymptotics of linear systems [e.g. 55]. Take-Aways\n\nWhile theoretical in nature, our results have several actionable implications for SSM and LLM research, informing the rapidly growing research on SSM-based LLMs. First, encouragingly, SSMs can keep track of bounded hierarchical structure with optimal memory even without explicitly implementing a stack (Theorem 6), suggesting that simple diagonal linear state updates may be sufficiently powerful for modeling the hierarchical structure of language. Second, SSMs resolve a basic failure mode of self-attention in flip-flop state tracking while being parallellizable (Theorem 1). Overall, SSMs and attention have overlapping but distinct strengths. This lends support to the development of hybrid architectures interleaving SSM and attention layers, as instantiated very recently by Jamba [42]. Third, nonnegative gates as obtained by exponential or sigmoid parameterizations provably restrict expressive capacity, even in non-time-invariant SSMs (Theorem 2). While [23] found no evidence that complex-valued paramerizations improved over real-valued ones in the language modality, our results suggest revisiting this question, at least for tasks where periodic state-tracking abilities may be important. Fourth, while exactly characterizing the capacity of transformers has proven difficult even in the finite-state case, Theorem 4 provides a decidable characterization of the regular languages \u2013 equivalently, finite-state tracking problems \u2013 that SSMs such as Mamba can model. Such decidable characterizations may make it easier to theoretically predict abilities and anticipate failures of LLMs; exploring the implications of this characterization in more realistic setups is an exciting direction for future research. Limitations\n\nThe main limitation of our theoretical results is that they focus on in-principle expressiveness, and do not directly make statements about learning and generalization. Future work could address this, for example, by examining whether our constructions result in reasonably flat minima, or by studying gradient flow dynamics. While we empirically verified that our positive results can indeed be instantiated, in a learnable manner, in one realistic SSM implementation, implementational differences might still result in practical differences between implementations. Studying the role of such implementational differences is an interesting problem for future work; we have made a first step by theoretically elucidating the implications of nonnegative gate values. 6 Conclusion\n\nWe have studied the expressive capacity of modern state space models (SSMs), through the lens of automata and formal languages. We have shown theoretically that SSMs can express star-free languages, a range of counter languages, and bounded hierarchical structure. By providing rigorous results about the expressiveness of the SSM architecture, our results can provide guidance to work on SSM-based language models. Acknowledgments\n\nWe thank Mark Rofin for useful discussion about Theorem 2. References\n\nAky\u00fcrek et al. [2024] E. Aky\u00fcrek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Arhitectures and algorithms. arXiv preprint arXiv:2401.12973, 2024. Almeida [1995] J. Almeida. Finite semigroups and universal algebra, volume 3. World Scientific, 1995. Angluin et al. [2023] D. Angluin, D.",
    "ssmformal-6": "Chiang, and A. Yang. Masked hard-attention transformers and boolean rasp recognize exactly the star-free languages.",
    "ssmformal-7": "arXiv preprint arXiv:2310.13897, 2023. Ba et al. [2016] J. L. Ba, J. R. Kiros, and G.",
    "ssmformal-8": "E. Hinton. Layer normalization. stat, 1050:21, 2016. Barrington et al. [1992] D.",
    "ssmformal-9": "A. M. Barrington, K. Compton, H. Straubing, and D. Th\u00e9rien. Regular languages in nc1.",
    "ssmformal-10": "Journal of Computer and System Sciences, 44(3):478\u2013499, 1992. Bhattamishra et al. [2020] S. Bhattamishra, K.",
    "ssmformal-11": "Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096\u20137116, 2020. Blasi et al. [2019] D. Blasi, R. Cotterell, L. Wolf-Sonkin, S. Stoll, B. Bickel, and M. Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3938\u20133943, 2019. Bradbury et al. [2016] J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016. Chiang and Cholak [2022] D. Chiang and P. Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654\u20137664, 2022. Chiang et al. [2023] D. Chiang, P.",
    "ssmformal-12": "Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer encoders. 2023. Chomsky [1957] N. Chomsky. Syntactic structures, 1957. Chomsky and Sch\u00fctzenberger [1963] N. Chomsky and M. P. Sch\u00fctzenberger. The algebraic theory of context-free languages.",
    "ssmformal-13": "In Studies in Logic and the Foundations of Mathematics, volume 35, pages 118\u2013161. Elsevier, 1963. Dauphin et al. [2017] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933\u2013941. PMLR, 2017. De et al. [2024] S. De, S. L. Smith, A. Fernando, A. Botev, G. Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y.",
    "ssmformal-14": "W. Teh, R. Pascanu, N. de Freitas, and \u00c7. G\u00fcl\u00e7ehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models. CoRR, abs/2402.19427, 2024. doi: 10.48550/ARXIV.2402.19427. URL https://doi.org/10.48550/arXiv.2402.19427. Deletang et al. [2022] G. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, et al. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2022. Eilenberg [1974] S. Eilenberg. Automata, languages, and machines. Academic press, 1974. Elman [1990] J. L. Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990. Everaert et al. [2015] M. B. Everaert, M. A. Huybregts, N. Chomsky, R.",
    "ssmformal-15": "C. Berwick, and J. J. Bolhuis. Structures, not strings: linguistics as part of the cognitive sciences. Trends in cognitive sciences, 19(12):729\u2013743, 2015. Fischer et al. [1968a] P. C. Fischer, A. R. Meyer, and A.",
    "ssmformal-16": "L. Rosenberg. Counter machines and counter languages. Mathematical systems theory, 2(3):265\u2013283, Sep 1968a. ISSN 1433-0490. doi: 10.1007/BF01694011. URL https://doi.org/10.1007/BF01694011. Fischer et al. [1968b] P. C. Fischer, A. R. Meyer, and A.",
    "ssmformal-17": "L. Rosenberg. Counter machines and counter languages.",
    "ssmformal-18": "Math. Syst. Theory, 2(3):265\u2013283, 1968b.",
    "ssmformal-19": "doi: 10.1007/BF01694011. URL https://doi.org/10.1007/BF01694011. Fu et al. [2023] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=COZDy0WYGg. Ginzburg [1968] A. Ginzburg. Algebraic theory of automata. Academic Press, 1968. Gu and Dao [2023] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. URL https://doi.org/10.48550/arXiv.2312.00752. Gu et al. [2021] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. Hahn [2020] M. Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156\u2013171, 2020.",
    "ssmformal-20": "Hahn and Rofin [2024] M. Hahn and M. Rofin. Why are sensitive functions hard for transformers?",
    "ssmformal-21": "CoRR, abs/2402.09963, 2024. doi: 10.48550/ARXIV.2402.09963. URL https://doi.org/10.48550/arXiv.2402.09963. Hahn et al. [2015] M. Hahn, A. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and the structure of nc.",
    "ssmformal-22": "In G. F. Italiano, G. Pighizzini, and D. Sannella, editors, Mathematical Foundations of Computer Science 2015 - 40th International Symposium, MFCS 2015, Milan, Italy, August 24-28, 2015, Proceedings, Part II, volume 9235 of Lecture Notes in Computer Science, pages 384\u2013394.",
    "ssmformal-23": "Springer, 2015.",
    "ssmformal-24": "doi: 10.1007/978-3-662-48054-0\\_32. URL https://doi.org/10.1007/978-3-662-48054-0_32. Hewitt et al. [2020] J. Hewitt, M. Hahn, S. Ganguli, P. Liang, and C.",
    "ssmformal-25": "D. Manning. Rnns can generate bounded hierarchical languages with optimal memory. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1978\u20132010, 2020. Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. Hopcroft et al. [2001] J. E. Hopcroft, R. Motwani, and J.",
    "ssmformal-26": "D. Ullman. Introduction to automata theory, languages, and computation. ACM New York, NY, USA, 2001. Horne and Hush [1993] B. Horne and D. Hush. Bounds on the complexity of recurrent neural network implementations of finite state machines. Advances in neural information processing systems, 6, 1993. Indyk [1995] P. Indyk. Optimal simulation of automata by neural nets.",
    "ssmformal-27": "In Annual Symposium on Theoretical Aspects of Computer Science, pages 337\u2013348. Springer, 1995. Jelassi et al. [2024] S. Jelassi, D. Brandfonbrener, S. M. Kakade, and E. Malach. Repeat after me: Transformers are better than state space models at copying, 2024. Kalman [1960] R. E. Kalman. On the general theory of control systems. In Proceedings First International Conference on Automatic Control, Moscow, USSR, pages 481\u2013492, 1960. Kalman [1963] R. E. Kalman. Mathematical description of linear dynamical systems. Journal of the Society for Industrial and Applied Mathematics, Series A: Control, 1(2):152\u2013192, 1963.",
    "ssmformal-28": "Karlsson [2007] F. Karlsson. Constraints on multiple center-embedding of clauses. Journal of Linguistics, 43(2):365\u2013392, 2007. Kleene [1951] S. Kleene. Representation of events in nerve nets and finite automata. In Automata Studies. 1951. Krebs et al. [2015] A. Krebs, K. Lange, and M. Ludwig. Visibly counter languages and constant depth circuits.",
    "ssmformal-29": "In E.",
    "ssmformal-30": "W. Mayr and N. Ollinger, editors, 32nd International Symposium on Theoretical Aspects of Computer Science, STACS 2015, March 4-7, 2015, Garching, Germany, volume 30 of LIPIcs, pages 594\u2013607. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2015.",
    "ssmformal-31": "doi: 10.4230/LIPICS.STACS.2015.594. URL https://doi.org/10.4230/LIPIcs.STACS.2015.594. Krohn and Rhodes [1965] K. Krohn and J. Rhodes. Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines. Transactions of the American Mathematical Society, 116:450\u2013464, 1965. Kutrib et al. [2021] M. Kutrib, A.",
    "ssmformal-32": "Malcher, and M. Wendlandt. Input-driven multi-counter automata. Theoretical Computer Science, 870:121\u2013136, 2021. Lei et al. [2017] T. Lei, Y. Zhang, S. I. Wang, H. Dai, and Y. Artzi. Simple recurrent units for highly parallelizable recurrence. arXiv preprint arXiv:1709.02755, 2017. Lieber et al. [2024] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, O. Abend, R. Alon, T. Asida, A. Bergman, R. Glozman, M. Gokhman, A. Manevich, N. Ratner, N. Rozen, E. Shwartz, M. Zusman, and Y. Shoham. Jamba: A hybrid transformer-mamba language model, 2024.",
    "ssmformal-33": "Linzen et al. [2016] T. Linzen, E. Dupoux, and Y. Goldberg. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521\u2013535, 2016. Liu et al. [2022] B. Liu, J. T. Ash, S. Goel, A.",
    "ssmformal-34": "Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022. Liu et al. [2023] B. Liu, J. T. Ash, S. Goel, A.",
    "ssmformal-35": "Krishnamurthy, and C. Zhang. Exposing attention glitches with flip-flop language modeling.",
    "ssmformal-36": "In A. Oh, T. Naumann, A. Globerson, K.",
    "ssmformal-37": "Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html. McNaughton and Papert [1971] R. McNaughton and S. A. Papert. Counter-Free Automata (MIT research monograph no.",
    "ssmformal-38": "65). The MIT Press, 1971. Mehta et al. [2023] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5MkYIYCbva. Merrill and Sabharwal [2023] W. Merrill and A. Sabharwal. A logic for expressing log-precision transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Merrill et al. [2024] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In ICML, 2024. Miller and Chomsky [1963] G. A. Miller and N. Chomsky. Finitary models of language users. 1963. Mirac Suzgun [2019] S. M. S. Mirac Suzgun, Yonatan Belinkov. On evaluating the generalization of lstm models in formal languages.",
    "ssmformal-39": "volume 2, pages 277\u2013286. University of Massachusetts Amherst Libraries, 1 2019. doi: 10.7275/s02b-4d91. URL https://openpublishing.library.umass.edu/scil/article/id/1167/. Orvieto et al. [2023] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698. PMLR, 2023. Peng et al. [2024] B. Peng, S.",
    "ssmformal-40": "Narayanan, and C. Papadimitriou. On limitations of the transformer architecture. arXiv preprint arXiv:2402.08164, 2024. P\u00e9rez et al. [2019] J. P\u00e9rez, J. Marinkovi\u0107, and P. Barcel\u00f3. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. Phillips and Solo [1992] P. C. Phillips and V. Solo. Asymptotics for linear processes. The Annals of Statistics, pages 971\u20131001, 1992. Qin et al. [2024a] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W.",
    "ssmformal-41": "Sun, and Y. Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Qin et al. [2024b] Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Sakarovitch [2009] J. Sakarovitch. Elements of automata theory. Cambridge university press, 2009. Sanford et al. [2024] C. Sanford, D.",
    "ssmformal-42": "J. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36, 2024. Sch\u00fctzenberger [1965] M. P. Sch\u00fctzenberger. On finite monoids having only trivial subgroups.",
    "ssmformal-43": "Inf. Control., 8(2):190\u2013194, 1965. Shazeer [2020] N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Siegelman and Sontag [1995] H. Siegelman and E. D. Sontag. On the computational power of neural nets. Journal of Computer and System Sciences, 50:132\u2013150, 1995. Siegelmann [1999] H. T. Siegelmann. Neural networks and analog computation: beyond the Turing limit. Springer Science & Business Media, 1999. Straubing [1994] H. Straubing. Finite automata, formal logic, and circuit complexity. Birkhaeuser, 1994.",
    "ssmformal-44": "Strobl [2023] L. Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.",
    "ssmformal-45": "Strobl et al. [2023] L. Strobl, W. Merrill, G. Weiss, D. Chiang, and D. Angluin. Transformers as recognizers of formal languages: A survey on expressivity. CoRR, abs/2311.00208, 2023. doi: 10.48550/ARXIV.2311.00208. URL https://doi.org/10.48550/arXiv.2311.00208. Sun et al. [2023] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Tomita [1982] M. Tomita. Dynamic construction of finite-state automata from examples using hill-climbing.",
    "ssmformal-46": "In Proceedings of the Fourth Annual Conference of the Cognitive Science Society, pages 105\u2013108, 1982. Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017. Weiss et al. [2018] G. Weiss, Y. Goldberg, and E. Yahav. On the practical computational power of finite precision rnns for language recognition. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740\u2013745, 2018. Yang and Chiang [2024] A. Yang and D. Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers. arXiv preprint arXiv:2404.04393, 2024. Yang et al. [2023] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Yao et al. [2021a] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2021a. doi: 10.18653/v1/2021.acl-long.292. URL http://dx.doi.org/10.18653/v1/2021.acl-long.292. Yao et al. [2021b] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages.",
    "ssmformal-47": "In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770\u20133785, Online, Aug. 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.292. URL https://aclanthology.org/2021.acl-long.292. Zhang and Sennrich [2019] B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Appendix A Instantiations of General Framework in SSM Models\n\nHere, we survey how (1) is instantiated in a range of SSMs. As stated in Section 2, we refer to SSMs where the gate does not depend on as time-invariant. An equivalent terminology is the distinction between \u201cWeak Linear Time Invariant Convolutional Models\u201d (i.e., time-invariant) and \u201cLinear Time Variant Models\u201d (i.e., non-time-invariant) in Aky\u00fcrek et al. [1]. A.1 Non-Time-Invariant Models\n\nApproximately simultaneously with or more recently than [23], a range of non-time-invariant SSMs have been introduced [14, 72, 57, 56]. This category also covers highly similar earlier RNN variants [8, 41]. Mamba\n\nIn Mamba, (2) and (3) directly map onto Eqs. (2a) and (2b) in Gu and Dao [23]. The notation of Gu and Dao [23] use a matrix multiplication instead of elementwise multiplication in (REF), but importantly, Mamba\u2019s is diagonal, so we can take . Due to exponential parameterization, its entries are nonnegative. Griffin\n\nThe RG-LRU layer of Griffin [14] uses the equation\n\nh t subscript \u210e \ud835\udc61 \\displaystyle h_{t} = a t \u23df A \u200b ( x t ) \u2218 h t \u2212 1 + 1 \u2212 a t 2 \u2218 ( i t \u2218 x t ) \u23df B \u200b ( x t ) absent subscript \u23df subscript \ud835\udc4e \ud835\udc61 \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 subscript \u210e \ud835\udc61 1 subscript \u23df 1 superscript subscript \ud835\udc4e \ud835\udc61 2 subscript \ud835\udc56 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \ud835\udc35 subscript \ud835\udc65 \ud835\udc61 \\displaystyle=\\underbrace{a_{t}}_{A(x_{t})}\\circ h_{t-1}+\\underbrace{\\sqrt{1-a_{t}^{2}}\\circ(i_{t}\\circ x_{t})}_{B(x_{t})}\n\nwhere are neurally parameterized in terms of but not ; by design, . is instantiated in terms of linear transformations, GeLU, and RMSNorm (Figure 2 in De et al. [14]). The local attention used by Griffin can be subsumed into an SSM layer (Remark 18). Gated Linear Attention [GLA 72]\n\nThis model (Section 4.4 in Yang et al. [72]) instantiates our framework using a recurrence of the form (1); while the state is two-dimensional in this model, the update is performed by elementwise products as in (1). The gate is obtained by applying sigmoid to a linear transformation of ; thus, its entries are in . is instantiated in terms of SwiGLU and LayerNorm. HGRN\n\nHGRN [57] and HGRN2 [56] are defined by a recurrence of the form (1); the gate entries are by design. is instantiated in terms of GLU, linear transformations, and normalization. In HGRN, the state is complex, but crucially the gate remains real-valued. A.2 Time-Invariant Models\n\nTime-invariant SSMs introduced before late 2023 are surveyed by Gu and Dao [23, Appendix B], such as [47, 67, 52]. Time-invariant SSMs have often used complex-valued states and gates; this does not have a major impact on our results: First, as complex-valued SSMs subsume real-valued ones, our positive results carry over. Second, our negative result about PARITY is affected by this distinction and requires a separate argument, see Theorem 13. Note also that is often described as a general matrix multiplication, but is diagonalizable (e.g. Lemma 3.2 in Gu et al. [24]; [67] for RetNet), which \u2014even though implementation may be based on non-diagonalized representations [24]\u2014renders the model equivalent to one where is diagonal from the start.",
    "ssmformal-48": "This equivalence is shown as Lemma 3.1 in Gu et al. [24]. Appendix B Formal Definitions and Proofs\n\nB.1 Flip Flop\n\nWe begin by introducing key notions of automata theory. References for automata theory include Eilenberg [16], Hopcroft et al. [30], Sakarovitch [58]. We will provide those key notions that are necessary to prove our results. We will focus on deterministic finite-state-automata (DFA), and simply refer to them as finite-state-automata.444A closely related notion is the semiautomaton, which is the notion considered in the closely related work Liu et al.",
    "ssmformal-49": "[44]. Semiautomata lack a fixed start state . We include , but this difference is not substantial for our formal results. First,\n\nDefinition 7. A (deterministic) finite-state-automaton consists of:\n\n\u2022\n\na finite alphabet\n\n\u2022\n\na finite state set\n\n\u2022\n\na starting state\n\n\u2022\n\na transition function\n\nWe extend to a map by setting:\n\nu \u200b ( q , \u03f5 ) \ud835\udc62 \ud835\udc5e italic-\u03f5 \\displaystyle u(q,\\epsilon) = q absent \ud835\udc5e \\displaystyle=q u \u200b ( q , w 1 \u200b \u2026 \u200b i + 1 ) \ud835\udc62 \ud835\udc5e subscript \ud835\udc64 1 \u2026 \ud835\udc56 1 \\displaystyle u(q,w_{1\\dots i+1}) = u \u200b ( u \u200b ( q , w 1 \u200b \u2026 \u200b i ) , w i + 1 ) absent \ud835\udc62 \ud835\udc62 \ud835\udc5e subscript \ud835\udc64 1 \u2026 \ud835\udc56 subscript \ud835\udc64 \ud835\udc56 1 \\displaystyle=u(u(q,w_{1\\dots i}),w_{i+1})\n\nwhere is the empty word. Intuitively, is the state that is in after reading . The automaton recognizes a language if there is a recognizing set such that\n\nL := { w : u \u200b ( q 0 , \ud835\udc30 ) \u2208 R } assign \ud835\udc3f conditional-set \ud835\udc64 \ud835\udc62 subscript \ud835\udc5e 0 \ud835\udc30 \ud835\udc45 L:=\\{w:u(q_{0},{\\bf w})\\in R\\} (5)\n\nKleene\u2019s Theorem [37] asserts that a language is regular (i.e., defined by a regular expression) if and only if it is recognized by some finite-state automaton. A very fundamental automaton underlying Flip Flop is:\n\nDefinition 8. A set-reset automaton is a finite-state-automaton where and\n\nu \u200b ( q , \u03c3 ) = { q if \u200b \u03c3 \u2209 Q \u03c3 else \ud835\udc62 \ud835\udc5e \ud835\udf0e cases \ud835\udc5e if \ud835\udf0e \ud835\udc44 \ud835\udf0e else u(q,\\sigma)=\\begin{cases}q&\\text{ if }\\sigma\\not\\in Q\\\\\n\\sigma&\\text{ else}\\end{cases} (6)\n\nIntuitively, such an automaton keeps recording the last seen symbol from a designed set . Such an automaton is easily simulated with a single non-time-invariant SSM layer:\n\nLemma 9. Let by a set-reset automaton. Then there is a single-layer SSM with finite precision and width that maps each to the state sequence . Formally, there is an injective map such that for . Proof. Let be a binary encoding if , and else.",
    "ssmformal-50": "Take . Let if and else. After processing a string, the state is where is the last symbol in that has occurred if any has, and otherwise. Coming to (2, in order to avoid division by zero when normalizing if no element of has been read, we add a dummy dimension to whose value is always . We take to be the identity. Note that, even though normalization will affect the numerical values, the binary encoding of can still be read out with finite precision, as , and thus nonzero entries will remain bounded away from zero. \u220e\n\nTheorem 10. (Repeated from Theorem 1) A two-layer SSM with finite precision can predictively model at arbitrary lengths. Proof. In the first layer, we use Lemma 9 to simulate a set-reset automaton over the input alphabet where . This layer outputs at each position whether the last instruction was write, read, or ignore. The layer additionally, at each position, forwards the input symbol using additional dimensions. Formally, at the first layer, allows us to read out the input symbols . In the second layer, we again use Lemma 9 to simulate a set-reset automaton over an extended alphabet , where the first component indicates the input symbol and where the second component indicates . In this set-reset automaton, contains, besides a start state , those elements of whose second entry is . The second layer thus keeps track of the input bit following the last write instruction. It additionally forwards the input symbol using additional dimensions. The second layer, via , then predicts the possible next symbols on the basis of this information: If , any instruction in is possible. If , any bit in is possible. If , the bit stored after the last write instruction is possible; if no write instruction has appeared (hence, the second automaton is still in its start state), any bit in is possible. \u220e\n\nB.2 Difficulty of Representing PARITY\n\nDefinition 11. PARITY is the regular language over of strings where the number of ones is even. As a regular expression, PARITY is . Theorem 12. (Repeated from Theorem 2) No SSM satisfying nonnegative can recognize PARITY at arbitrary input lengths with finite precision. Proof. We consider an SSM with multiple layers, and indicate the layer in superscript: . We write for the input token embedding . Consider a SSM processing the word , for . We show, by induction over the number of layers, the following claim:\n\n() Each entry of converges to a value bounded, in absolute value, by a constant. By the assumption of finite precision, convergence automatically leads to the entries becoming ultimately constant. Once we have shown this, we know that is constant when is sufficiently large; thus, the parity of the string cannot be read out from . As a consequence, the SSM cannot recognize PARITY. Indeed, we have shwon the stronger claim that the language \u2013 the language of even-length strings over one symbol \u2013 is not recognized by an SSM; we will use this stronger statement in Corollary 14. We proceed to proving (). The claim () is trivially true at , as the input token is always the same and we defined . Now consider . By hypothesis, the activations are given as\n\nh t ( k ) = A \u200b ( x t ) \u2218 h t \u2212 1 ( k ) + B \u200b ( x t ) superscript subscript \u210e \ud835\udc61 \ud835\udc58 \ud835\udc34 subscript \ud835\udc65 \ud835\udc61 superscript subscript \u210e \ud835\udc61 1 \ud835\udc58 \ud835\udc35 subscript \ud835\udc65 \ud835\udc61 h_{t}^{(k)}=A(x_{t})\\circ h_{t-1}^{(k)}+B(x_{t})\\\\ (7)\n\nwhere are constant , when , for some . The solution of the recurrence for is\n\nh t = \u03b1 t \u2212 T 0 \u200b ( h T 0 + \u03b2 \u03b1 \u2212 1 ) + \u03b2 1 \u2212 \u03b1 subscript \u210e \ud835\udc61 superscript \ud835\udefc \ud835\udc61 subscript \ud835\udc47 0 subscript \u210e subscript \ud835\udc47 0 \ud835\udefd \ud835\udefc 1 \ud835\udefd 1 \ud835\udefc h_{t}=\\alpha^{t-T_{0}}\\left(h_{T_{0}}+\\frac{\\beta}{\\alpha-1}\\right)+\\frac{\\beta}{1-\\alpha} (8)\n\nEach dimension of this vector can be constant (if ), diverge exponentially (), converge exponentially () or diverge linearly ().",
    "ssmformal-51": "We next need to show that converges. First, consider the effect of applying a linear transformation to the state . Each entry of the result will be some linear combination\n\nu t = \u03bb 1 \u200b ( h t ) 1 + \u22ef + \u03bb d \u200b ( h t ) d subscript \ud835\udc62 \ud835\udc61 subscript \ud835\udf06 1 subscript subscript \u210e \ud835\udc61 1 \u22ef subscript \ud835\udf06 \ud835\udc51 subscript subscript \u210e \ud835\udc61 \ud835\udc51 u_{t}=\\lambda_{1}(h_{t})_{1}+\\dots+\\lambda_{d}(h_{t})_{d} (9)\n\nIf each , then converges. If some , there may be some cancellation if for some ; cancellation can only lead to full erasure of the relevant terms or to a remaining term with the same exponent. In conclusion, each entry will again either converge to a finite value or diverge towards . We now need to understand the behavior of . Recall that, based on our survey (Appendix A), we allowed it to contain linear, GLU [13], and SwiGLU [61] components. If implements a linear transformation only, each entry likewise may converge, diverge linearly, or diverge exponentially. We note that\u2014if is the sigmoid function\u2014 always converges, as simply saturates to 0 or 1 if diverges. Hence, if implements GLU, each entry likewise may converge, diverge linearly, or diverge exponentially. Finally, if implements SwiGLU, each entry of the result will be a product of a linear combination of the form , and applied to another such linear combination. Depending on the behavior of these two -like terms, the outcome will behave as a product of sequences that may converge exponentially, diverge exponentially, or diverge linearly \u2013 e.g., the outcome may also diverge quadratically, or converge as , etc. If all dimensions of converge, then will also converge to a scaled version of , scaled by a bounded factor as . Now assume some dimensions of do not converge; in this case, for any two dimensions , either their ratio will converge to a constant, or converge to 0 or . After applying , the entries asymptotically dominating the others will converge to a finite value bounded, in absolute value, by 1; the others will converge to zero. In conclusion, we have found that each entry of converges to some number bounded, in absolute value, by . As is continuous, each entry of likewise converges, with a bound depending on the Lipschitz constant of . \u220e\n\nWe next show the result, referenced in the main paper text after Theorem 2, about time-invariant SSMs with complex-valued gates:\n\nTheorem 13. Time-invariant SSMs cannot recognize PARITY with finite precision at arbitrary input lengths, even with complex-valued gates, as long as each entry in each has a rational angle in the complex plane.",
    "ssmformal-52": "Here, by a rational angle, we refer to an angle that is a rational number when expressed in degrees; such angles are rational multiples of when expressed in radians. As the rational angles are dense in the reals, one expects that even if some irrational angles permitted modeling PARITY, such solutions would be very hard to find \u2013 in particular given that irrational numbers are not exactly represented in finite precision. Proof. By assumption, any in any layer can be written as\n\nA j = r j \u200b exp \u2061 ( 2 \u200b \u03c0 \u200b i \u200b q j ) subscript \ud835\udc34 \ud835\udc57 subscript \ud835\udc5f \ud835\udc57 2 \ud835\udf0b \ud835\udc56 subscript \ud835\udc5e \ud835\udc57 A_{j}=r_{j}\\exp(2\\pi iq_{j}) (10)\n\nwhere is rational and is real \u2013 here, is known as the argument of ; it describes the angle of in the complex plane in radians. Correspondingly, the angle in degrees is described by ; this is rational if and only if is. As a time-invariant SSM has a finite number of such values , across all its layers, we can select a positive integer such that for each , in each layer. Importantly, . Now consider the action of any layer of the SSM on an input sequence of the form . The claim is that, for each , the sequence\n\nz t \u200b W + i ( k ) superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc4a \ud835\udc56 \ud835\udc58 z_{tW+i}^{(k)} (11)\n\nconverges as . As in the proof of Theorem 2, in the finite-precision setting, converge entails that the sequence becomes ultimately stationary. Note that the parity of equals the parity of ; hence, it is impossible to read out the parity from when is large. Now consider, suppressing the index for the dimension in :\n\nh t \u200b W ( k ) superscript subscript \u210e \ud835\udc61 \ud835\udc4a \ud835\udc58 \\displaystyle h_{tW}^{(k)} = \u2211 i = 1 t \u200b W A t \u200b W \u2212 i \u200b B \u200b ( z i ( k \u2212 1 ) ) absent superscript subscript \ud835\udc56 1 \ud835\udc61 \ud835\udc4a superscript \ud835\udc34 \ud835\udc61 \ud835\udc4a \ud835\udc56 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc56 \ud835\udc58 1 \\displaystyle=\\sum_{i=1}^{tW}A^{tW-i}B(z_{i}^{(k-1)}) = \u2211 i = 1 t \u200b W A t \u200b W \u2212 i \u200b B \u200b ( z i ( k \u2212 1 ) ) absent superscript subscript \ud835\udc56 1 \ud835\udc61 \ud835\udc4a superscript \ud835\udc34 \ud835\udc61 \ud835\udc4a \ud835\udc56 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc56 \ud835\udc58 1 \\displaystyle=\\sum_{i=1}^{tW}A^{tW-i}B(z_{i}^{(k-1)}) = \u2211 s = 1 t \u2211 j = s \u200b W ( s + 1 ) \u200b W \u2212 1 A t \u200b W \u2212 j \u200b B \u200b ( z s \u200b W + j ( k \u2212 1 ) ) absent superscript subscript \ud835\udc60 1 \ud835\udc61 superscript subscript \ud835\udc57 \ud835\udc60 \ud835\udc4a \ud835\udc60 1 \ud835\udc4a 1 superscript \ud835\udc34 \ud835\udc61 \ud835\udc4a \ud835\udc57 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc58 1 \\displaystyle=\\sum_{s=1}^{t}\\sum_{j=sW}^{(s+1)W-1}A^{tW-j}B(z_{sW+j}^{(k-1)}) = \u2211 s = 1 t \u2211 j = 0 W \u2212 1 A ( t \u2212 s ) \u200b W \u2212 j \u200b B \u200b ( z s \u200b W + j ( k \u2212 1 ) ) absent superscript subscript \ud835\udc60 1 \ud835\udc61 superscript subscript \ud835\udc57 0 \ud835\udc4a 1 superscript \ud835\udc34 \ud835\udc61 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc58 1 \\displaystyle=\\sum_{s=1}^{t}\\sum_{j=0}^{W-1}A^{(t-s)W-j}B(z_{sW+j}^{(k-1)}) = \u2211 s = 1 t \u2211 j = 0 W \u2212 1 ( r \u200b exp \u2061 ( 2 \u200b \u03c0 \u200b i \u200b q ) ) ( t \u2212 s ) \u200b W \u2212 j \u200b B \u200b ( z s \u200b W + j ( k \u2212 1 ) ) absent superscript subscript \ud835\udc60 1 \ud835\udc61 superscript subscript \ud835\udc57 0 \ud835\udc4a 1 superscript \ud835\udc5f 2 \ud835\udf0b \ud835\udc56 \ud835\udc5e \ud835\udc61 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc58 1 \\displaystyle=\\sum_{s=1}^{t}\\sum_{j=0}^{W-1}(r\\exp(2\\pi iq))^{(t-s)W-j}B(z_{sW+j}^{(k-1)}) = \u2211 s = 1 t \u2211 j = 0 W \u2212 1 r ( t \u2212 s ) \u200b W \u2212 j \u200b exp \u2061 ( \u2212 2 \u200b \u03c0 \u200b i \u200b j \u200b q ) \u200b B \u200b ( z s \u200b W + j ( k \u2212 1 ) ) absent superscript subscript \ud835\udc60 1 \ud835\udc61 superscript subscript \ud835\udc57 0 \ud835\udc4a 1 superscript \ud835\udc5f \ud835\udc61 \ud835\udc60 \ud835\udc4a \ud835\udc57 2 \ud835\udf0b \ud835\udc56 \ud835\udc57 \ud835\udc5e \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc58 1 \\displaystyle=\\sum_{s=1}^{t}\\sum_{j=0}^{W-1}r^{(t-s)W-j}\\exp(-2\\pi ijq)B(z_{sW+j}^{(k-1)}) = \u2211 j = 0 W \u2212 1 exp \u2061 ( \u2212 2 \u200b \u03c0 \u200b i \u200b j \u200b q ) \u200b \u2211 s = 1 t r ( t \u2212 s ) \u200b W \u2212 j \u200b B \u200b ( z s \u200b W + j ( k \u2212 1 ) ) absent superscript subscript \ud835\udc57 0 \ud835\udc4a 1 2 \ud835\udf0b \ud835\udc56 \ud835\udc57 \ud835\udc5e superscript subscript \ud835\udc60 1 \ud835\udc61 superscript \ud835\udc5f \ud835\udc61 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc60 \ud835\udc4a \ud835\udc57 \ud835\udc58 1 \\displaystyle=\\sum_{j=0}^{W-1}\\exp(-2\\pi ijq)\\sum_{s=1}^{t}r^{(t-s)W-j}B(z_{sW+j}^{(k-1)})\n\nSeparately considering summation beyond at which has become stationary, we get\n\n= [ \u2211 j \u2212 0 T 0 \u2212 1 \u2026 ] \u23df U 1 + [ ( \u2211 j = T 0 W \u2212 1 exp \u2061 ( \u2212 2 \u200b \u03c0 \u200b i \u200b j \u200b q ) \u200b B \u200b ( z j ( k \u2212 1 ) ) \u200b r \u2212 j ) \u23df U 2 \u200b ( \u2211 s = 1 t r ( t \u2212 s ) \u200b W ) \u23df U 3 ] absent subscript \u23df delimited-[] superscript subscript \ud835\udc57 0 subscript \ud835\udc47 0 1 \u2026 subscript \ud835\udc48 1 delimited-[] subscript \u23df superscript subscript \ud835\udc57 subscript \ud835\udc47 0 \ud835\udc4a 1 2 \ud835\udf0b \ud835\udc56 \ud835\udc57 \ud835\udc5e \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc57 \ud835\udc58 1 superscript \ud835\udc5f \ud835\udc57 subscript \ud835\udc48 2 subscript \u23df superscript subscript \ud835\udc60 1 \ud835\udc61 superscript \ud835\udc5f \ud835\udc61 \ud835\udc60 \ud835\udc4a subscript \ud835\udc48 3 \\displaystyle=\\underbrace{\\left[\\sum_{j-0}^{T_{0}-1}\\dots\\right]}_{U_{1}}+\\left[\\underbrace{\\left(\\sum_{j=T_{0}}^{W-1}\\exp(-2\\pi ijq)B(z_{j}^{(k-1)})r^{-j}\\right)}_{U_{2}}\\underbrace{\\left(\\sum_{s=1}^{t}r^{(t-s)W}\\right)}_{U_{3}}\\right]\n\nand do not depend on . Intuitively, determines a direction in the complex plane, whereas determines a magnitude. It remains to understand , which can be rewritten as:\n\nU 3 = \u2211 s = 1 t r ( s \u2212 1 ) \u200b W = r \u2212 W \u200b \u2211 s = 1 t ( r W ) s = r \u2212 W \u200b \u2211 s = 0 t ( r W ) s \u2212 r \u2212 W = r \u2212 W \u200b { 1 \u2212 ( r W ) t 1 \u2212 ( r W ) \u2212 1 r \u2260 1 s \u2212 1 r = 1 subscript \ud835\udc48 3 superscript subscript \ud835\udc60 1 \ud835\udc61 superscript \ud835\udc5f \ud835\udc60 1 \ud835\udc4a superscript \ud835\udc5f \ud835\udc4a superscript subscript \ud835\udc60 1 \ud835\udc61 superscript superscript \ud835\udc5f \ud835\udc4a \ud835\udc60 superscript \ud835\udc5f \ud835\udc4a superscript subscript \ud835\udc60 0 \ud835\udc61 superscript superscript \ud835\udc5f \ud835\udc4a \ud835\udc60 superscript \ud835\udc5f \ud835\udc4a superscript \ud835\udc5f \ud835\udc4a cases 1 superscript superscript \ud835\udc5f \ud835\udc4a \ud835\udc61 1 superscript \ud835\udc5f \ud835\udc4a 1 \ud835\udc5f 1 \ud835\udc60 1 \ud835\udc5f 1 U_{3}=\\sum_{s=1}^{t}r^{(s-1)W}=r^{-W}\\sum_{s=1}^{t}(r^{W})^{s}=r^{-W}\\sum_{s=0}^{t}(r^{W})^{s}-r^{-W}=r^{-W}\\begin{cases}\\frac{1-(r^{W})^{t}}{1-(r^{W})}-1&r\\neq 1\\\\\ns-1&r=1\\end{cases} (12)\n\nWe have now achieved a situation like in the proof of Theorem 2: can converge exponentially, diverge linearly, or diverge exponentially.",
    "ssmformal-53": "The remainder of the proof is analogous to that proof. \u220e\n\nThe following Corollary of Theorem 2 will be used in the proof of Theorem 4:\n\nCorollary 14. Assume nonnegative, SSMs with finite precision cannot recognize any non-star-free regular language. Proof. For any non-star-free regular language , there are words such that the membership is determined by the parity of [46]. Fix any such . Now assume an SSM satisfying nonnegative can recognize with finite precision. We can subsume the action of into the state by taking , in each layer, to be the state of the SSM after reading . We now have an SSM that can determine the parity of when fed a word of the form . For this SSM, we want to show\n\n() When fed words of the form , for each , and each layer , the sequence converges as . As in the preceding two proofs in this section, convergence entails becoming ultimately constant in the finite-precision setting. The claim () is immediate at . Now at , we write:\n\nh t \u200b | v | + i ( k ) = superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 absent \\displaystyle h_{t|v|+i}^{(k)}= A \u200b ( z t \u200b | v | + i ( k \u2212 1 ) ) \u200b \u2026 \u200b A \u200b ( z ( t \u2212 1 ) \u200b | v | + i + 1 ( k \u2212 1 ) ) \u200b h ( t \u2212 1 ) \u200b | v | + i ( k ) \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 1 \u2026 \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 1 \ud835\udc63 \ud835\udc56 1 \ud835\udc58 1 superscript subscript \u210e \ud835\udc61 1 \ud835\udc63 \ud835\udc56 \ud835\udc58 \\displaystyle A(z_{t|v|+i}^{(k-1)})\\dots A(z_{(t-1)|v|+i+1}^{(k-1)})h_{(t-1)|v|+i}^{(k)} + A \u200b ( z t \u200b | v | + i ( k \u2212 1 ) ) \u200b \u2026 \u200b A \u200b ( z ( t \u2212 1 ) \u200b | v | + i + 2 ( k \u2212 1 ) ) \u200b B \u200b ( z ( t \u2212 1 ) \u200b | v | + i + 1 ( k \u2212 1 ) ) \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 1 \u2026 \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 1 \ud835\udc63 \ud835\udc56 2 \ud835\udc58 1 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc61 1 \ud835\udc63 \ud835\udc56 1 \ud835\udc58 1 \\displaystyle+A(z_{t|v|+i}^{(k-1)})\\dots A(z_{(t-1)|v|+i+2}^{(k-1)})B(z_{(t-1)|v|+i+1}^{(k-1)}) + A \u200b ( z t \u200b | v | + i ( k \u2212 1 ) ) \u200b \u2026 \u200b A \u200b ( z ( t \u2212 1 ) \u200b | v | + i + 3 ( k \u2212 1 ) ) \u200b B \u200b ( z ( t \u2212 1 ) \u200b | v | + i + 2 ( k \u2212 1 ) ) \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 1 \u2026 \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 1 \ud835\udc63 \ud835\udc56 3 \ud835\udc58 1 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc61 1 \ud835\udc63 \ud835\udc56 2 \ud835\udc58 1 \\displaystyle+A(z_{t|v|+i}^{(k-1)})\\dots A(z_{(t-1)|v|+i+3}^{(k-1)})B(z_{(t-1)|v|+i+2}^{(k-1)}) + \u2026 \u2026 \\displaystyle+\\dots + B \u200b ( z ( t \u2212 1 ) \u200b | v | + i ( k \u2212 1 ) ) \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc61 1 \ud835\udc63 \ud835\udc56 \ud835\udc58 1 \\displaystyle+B(z_{(t-1)|v|+i}^{(k-1)})\n\nOn the RHS, as , all terms except for become constant by the inductive hypothesis. Hence, there are some such that, for sufficiently large ,\n\nh t \u200b | v | + i ( k ) = \u03b1 \u2218 h ( t \u2212 1 ) \u200b | v | + i ( k ) + \u03b2 superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 \ud835\udefc superscript subscript \u210e \ud835\udc61 1 \ud835\udc63 \ud835\udc56 \ud835\udc58 \ud835\udefd h_{t|v|+i}^{(k)}=\\alpha\\circ h_{(t-1)|v|+i}^{(k)}+\\beta (13)\n\nWe are now, for each , in the same situation as in the proof of Theorem 2]: each dimension of this recurrence can converge exponentially, diverge exponentially, or diverge linearly; as in that proof, it follows that converges as . We have shown (). We now follow up by showing that\n\n() When fed words of the form , for each , and each layer , the sequence converges as . Again, at finite precision, convergence entails that the sequences are ultimately constant. Again, () is true at trivially. When feeding the SSM words of the form , in each layer, the final state is in each layer , at each :\n\nh t \u200b | v | + i ( k ) = superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 absent \\displaystyle h_{t|v|+i}^{(k)}= A \u200b ( z t \u200b | v | + | w | ( k \u2212 1 ) ) \u200b \u2026 \u200b A \u200b ( z t \u200b | v | + 1 ( k \u2212 1 ) ) \u200b h t \u200b | v | ( k ) \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc64 \ud835\udc58 1 \u2026 \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 1 \ud835\udc58 1 superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc58 \\displaystyle A(z_{t|v|+|w|}^{(k-1)})\\dots A(z_{t|v|+1}^{(k-1)})h_{t|v|}^{(k)} + A \u200b ( z t \u200b | v | + i ( k \u2212 1 ) ) \u200b \u2026 \u200b A \u200b ( z t \u200b | v | + 2 ( k \u2212 1 ) ) \u200b B \u200b ( z t \u200b | v | + 1 ( k \u2212 1 ) ) \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 1 \u2026 \ud835\udc34 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 2 \ud835\udc58 1 \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 1 \ud835\udc58 1 \\displaystyle+A(z_{t|v|+i}^{(k-1)})\\dots A(z_{t|v|+2}^{(k-1)})B(z_{t|v|+1}^{(k-1)}) + \u2026 \u2026 \\displaystyle+\\dots + B \u200b ( z t \u200b | v | + i ( k \u2212 1 ) ) \ud835\udc35 superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 1 \\displaystyle+B(z_{t|v|+i}^{(k-1)})\n\nBy inductive hypothesis, for large , there are such that\n\nh t \u200b | v | + i ( k ) = superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 absent \\displaystyle h_{t|v|+i}^{(k)}= \u03c8 i \u2218 h t \u200b | v | ( k ) + \u03b3 i subscript \ud835\udf13 \ud835\udc56 superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc58 subscript \ud835\udefe \ud835\udc56 \\displaystyle\\psi_{i}\\circ h_{t|v|}^{(k)}+\\gamma_{i}\n\nand, as shown before, each entry of converges exponentially, diverges exponentially, or diverges linearly. Now, by assumption, one can read out, at finite precisiion, the parity of from\n\nz t \u200b | v | + | w | ( L ) = Mix 1 \u2061 ( Norm \u2061 ( Mix 2 \u2061 ( \u03c8 | w | \u2218 h t \u200b | v | + i ( k ) + \u03b3 | w | ) ) ) superscript subscript \ud835\udc67 \ud835\udc61 \ud835\udc63 \ud835\udc64 \ud835\udc3f subscript Mix 1 Norm subscript Mix 2 subscript \ud835\udf13 \ud835\udc64 superscript subscript \u210e \ud835\udc61 \ud835\udc63 \ud835\udc56 \ud835\udc58 subscript \ud835\udefe \ud835\udc64 \\displaystyle z_{t|v|+|w|}^{(L)}=\\operatorname{Mix}_{1}(\\operatorname{Norm}(\\operatorname{Mix}_{2}(\\psi_{|w|}\\circ h_{t|v|+i}^{(k)}+\\gamma_{|w|})))\n\nWe now simply absorb the operation into , and obtain by the same arguments as in the proof of Theorem 2 that converges as . This is a contradiction to the claim that the parity of can be read out from at finite precision. \u220e\n\nB.3 Proof of Theorem 4\n\nOur proof of Theorem 4 will rely on the algebraic theory of finite automata, specifically the cascade product and the Krohn-Rhodes Theorem [39]. These techniques, originally developed in the 1960s, have recently been introduced to the theoretical study of transformers by Liu et al. [44]; we provide self-contained definitions and somewhat different notation, tailored to our proofs about state-space models. In general, we will find that the properties of state-space models allow more natural and directly length-generalizing implementations of these algebraic notions than what is possible for transfomers. Recall the definition of a finite-state-automaton (Definition 7). Our construction will build on an important operation on automata, the cascade product [39, 16, 22]:\n\nDefinition 15. Given two automata with associated alphabets and state sets such that\n\n\u03a3 2 = Q 1 \u00d7 \u03a3 1 , subscript \u03a3 2 subscript \ud835\udc44 1 subscript \u03a3 1 \\Sigma_{2}=Q_{1}\\times\\Sigma_{1}, (14)\n\nthe cascade product is the automaton given by\n\n\u2022\n\n\u2022\n\n\u2022\n\nis the tuple of the starting states of\n\n\u2022\n\nWe note that the literature usually uses \u201c\u201d for the cascade product [e.g. 16]. To avoid collision with the elementwise product \u201c\u201d (e.g., (1)), we here instead use \u201c\u201d, usually used for the wreath product \u2013 a product on monoids with an effect analogous to the cascade product [2]. While the formal definition is cumbersome, the intuition behind it is simple: The cascade product corresponds to first reading a word with , recording the state sequence and \u2013 at each \u2013 pasting the state together with the input symbol \u2013 resulting in a word over a new alphabet , and then running on the resulting word. The overall state of after reading a word is the tuple of the states reached by and . Note that we write , rather than, , because the second argument of the cascade product () intuitively reads the input first, preprocessing it for the other automaton, . The somewhat inscrutable update rule for encodes the action of in the second component, and the action of on the extended alphabet in the first component. There is a close analogy to the stacking of sequence models, and we will leverage this analogy to translate cascade products into multilayer SSMs. The fundamental background here is the following classical fact:\n\nFact 16 (Consequence of Krohn-Rhodes Theorem [39] and Sch\u00fctzenberger\u2019s Theorem [60]). Each star-free regular language is recognized by an iterated cascade product of set-reset automata, , where each is a set-reset automaton. We now formally show that cascade products can be translated to SSM stacking. We need an auxiliary lemma, which provides a single-layer SSM that encodes the input in state \u2013 we will use it to forward information about the state of at to at :\n\nLemma 17. Let be an alphabet, and consider words . There is a one-layer SSM with such that, for , the character can be read out from at finite precision. To prove Lemma 17, a first idea is to use an exponential moving average with to encode the recent input characters in ; this effectively encodes the full history into the binary expansion of , and in particular allows reading out the second-last input in principle. However, such a construction does not work at finite precision, because rounding may make it impossible to extract even the second-most-significant bit.555Informally, in binary, 0.0111111\u2026111 and 0.1 are arbitrarily close. We avoid this problem simply by taking , effectively utilizing only every two digits in the binary expansion of , ensuring that the second-last input can be read out at a constant margin. We now provide the formal proof:\n\nProof. We begin by showing the claim in the special case . Here, we take , and\n\nh 0 subscript \u210e 0 \\displaystyle h_{0} = [ 0 , 0 , 0 , 0 ] T absent superscript 0 0 0 0 \ud835\udc47 \\displaystyle=[0,0,0,0]^{T} A \u200b ( e 0 ) \ud835\udc34 subscript \ud835\udc52 0 \\displaystyle A(e_{0}) = [ 1 / 4 , 1 / 4 , 0 , 0 ] T absent superscript 1 4 1 4 0 0 \ud835\udc47 \\displaystyle=[1/4,1/4,0,0]^{T} A \u200b ( e 1 ) \ud835\udc34 subscript \ud835\udc52 1 \\displaystyle A(e_{1}) = [ 1 / 4 , 1 / 4 , 0 , 0 ] T absent superscript 1 4 1 4 0 0 \ud835\udc47 \\displaystyle=[1/4,1/4,0,0]^{T} B \u200b ( e 0 ) \ud835\udc35 subscript \ud835\udc52 0 \\displaystyle B(e_{0}) = [ 1 , 0 , 1 , 0 ] T absent superscript 1 0 1 0 \ud835\udc47 \\displaystyle=[1,0,1,0]^{T} B \u200b ( e 1 ) \ud835\udc35 subscript \ud835\udc52 1 \\displaystyle B(e_{1}) = [ 0 , 1 , 0 , 1 ] T absent superscript 0 1 0 1 \ud835\udc47 \\displaystyle=[0,1,0,1]^{T}\n\nNow we separately consider the state depending on the form of the prefix : If , then\n\nh t = ( \u2208 [ 1 , 2 ] \u2208 [ 0 , 1 / 8 ] 1 0 ) subscript \u210e \ud835\udc61 matrix absent 1 2 absent 0 1 8 1 0 h_{t}=\\left(\\begin{matrix}\\in[1,2]\\\\\n\\in[0,1/8]\\\\\n1\\\\\n0\\end{matrix}\\right) (15)\n\nIf , then\n\nh t = ( \u2208 [ 1 , 1.25 ] \u2208 [ 1 / 4 , 1 / 2 ] 1 0 ) subscript \u210e \ud835\udc61 matrix absent 1 1.25 absent 1 4 1 2 1 0 h_{t}=\\left(\\begin{matrix}\\in[1,1.25]\\\\\n\\in[1/4,1/2]\\\\\n1\\\\\n0\\end{matrix}\\right) (16)\n\nIn particular, assuming , one can read off from with a margin of size 1/8. As is encoded in and due to symmetry, analogous statements hold when . Now, for each , we run such a one-layer SSM where represents and represents all other characters.666In fact, using a binary encoding of , one can achieve . By running these in parallel, we obtain an SSM with from whose states one can read out at finite precision. As the entries in are all bounded by , we find independent of , and the margin is still bounded away from zero after normalization, and thus in , where we can assume , to be the identity. \u220e\n\nRemark 18. Some SSMs include local convolutions [e.g. 21, 23] or local attention [14], which aggregate information from a local window of some width . These do not increase the expressive capacity beyond SSMs as we have defined in (1-2), as aggregation of local information can be simulated with a single SSM layer: Using the layer constructed in the proof of Lemma 17, given the state , once one has read out as described in the proof, one can recover from and ; then inductively read out using and , etc. Thus, up to any given width , one can read out from the state of this layer at finite precision. We are now ready to translate cascade products into SSM stacking:\n\nLemma 19. Let , be two finite-state-automata, and assume that there are two SSMs with top-level states and that map each to the state sequences under , , at finite precision. Formally, on a word , and provide the state sequences of , . Then there is an SSM with layers that maps each to the state sequence under , again at finite precision. We note that a conceptually related result holds for transformers [Lemma 12 in 44]. However, SSMs allow a simpler and length-independent construction, as they do not require positional encodings to implement such a construction. Proof. The lower layers are based on the SSM modeling . We duplicate each channel, so we now have dimensions. We further add further dimensions that directly pass on the input embeddings, i.e., , , on these dimensions. In the resulting SSM, indicates both itself, and the state reached by after reading . The state is redundantly indicated by two separate sets of dimensions; the character is indicated by further state. Note, however, that the second automaton in the cascade product requires access to the state rather than . For this, we add a layer provided by Lemma 17, of width . Additional dimensions pass on (1) , and (2) the state that reaches after reading the prefix . We now have layers where has dimensions and indicates (1) , (2) the state that reaches after reading the prefix , (3) the state that reaches after reading the prefix . The first and third piece of information are now fed into the second SSM; the second piece is passed on in additional dimensions. As we allowed and to be arbitrary functions, we redefine these in the lowest layer of that second SSM to read out from the -dimensional component indicating (3), providing the desired second-to-last state. We have constructed an SSM with layers, where indicates (1) , (2) the state that reaches after reading the prefix , (3) the state that reaches after reading the prefix pasted with the state sequence of . This information is sufficient for reading out the state sequence of . Note that the number of channels may not be consistent, as it is in the top and bottom parts, but in the middle; we simply pad to the larger dimensionality. \u220e\n\nWe are now ready to show the existence of length-generalizing SSMs for any star-free state tracking problem, and conclude with the theorem:\n\nTheorem 20 (Repeated from Theorem 4). Assuming Nonnegative, SSMs can predictively model a regular language at finite precision if and only if is star-free. Proof. We need to show:\n\n1. SSMs at finite precision can predictively model all star-free languages. For each language, a single SSMs is applicable at arbitrary lengths. 2. Assuming Nonnegative, finite-precision SSMs cannot recognize any non-star-free regular language. The second statement is Corollary 14; it suffices to prove the first statement. Assume is star-free. By the Krohn-Rhodes theorem, there is an automaton that is a cascade product of some set-reset automata that recognizes . By Lemmas 9 and 19, there is an SSM that computes the state sequence of that automaton. Now we note that, since recognizes , the state after reading is sufficient for determining the set of characters that can follow this prefix in any element of . For, assume otherwise, then there are words , such that and such that but ; then but the set (5) is reachable from but not , contradiction. Hence, the SSM\u2019s outputs can be transformed, by composing with a map from states to next-character sets, to predictively model . \u220e\n\nTheorem 21. SSMs with complex-valued coefficients evading both nonnegative and time-invariant can represent all regular languages known to be in . We we do not use this theorem in the main paper, due to the nonexistence (as far as we know) of implemented SSMs with this property. Proof. SSMs evading both nonnegative and time-invariant can count modulo any integer , using and , , , . This is a generalization of the construction for PARITY described in Section B.2, since . The set of regular languages known to be in is the set of regular languages whose syntactic monoid contains no non-solvable groups [5]. These languages are recognized by cascade products of set-reset automata and automata perfoming modular counting [64]. By the remark above, together with Lemma 9 and Lemma 19, such cascade products can be simulated by SSMs. \u220e\n\nB.4 Maintaining Counters\n\nAs the first step in showing Theorem 5, we show that SSMs can maintain unbounded counters, and that one can read out the values of such counters, up to finite bounds, even at finite precision:\n\nLemma 22.",
    "ssmformal-54": "Let be an integer.",
    "ssmformal-55": "Let any function be given. Let . Then a one-layer SSM with finite precision can compute, at each position :\n\nmax \u2061 ( min \u2061 ( \u2211 j = 1 i u \u200b ( w i ) , L ) , \u2212 L ) max min superscript subscript \ud835\udc57 1 \ud835\udc56 \ud835\udc62 subscript \ud835\udc64 \ud835\udc56 \ud835\udc3f \ud835\udc3f \\operatorname{max}\\left(\\operatorname{min}\\left(\\sum_{j=1}^{i}u(w_{i}),L\\right),-L\\right) (17)\n\nin the sense that can read this out from with finite precision.",
    "ssmformal-56": "Proof.",
    "ssmformal-57": "Define . Define . For each , define and by . In order to read out the state up to a limit , we define\n\n\u03d5 \u200b ( h t , x t ) = Norm \u2061 ( h t + [ 0 , 1 , \u2212 1 , 2 , \u2212 2 , \u2026 , \u2212 L , L ] ) italic-\u03d5 subscript \u210e \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 Norm subscript \u210e \ud835\udc61 0 1 1 2 2 \u2026 \ud835\udc3f \ud835\udc3f \\phi(h_{t},x_{t})=\\operatorname{Norm}(h_{t}+[0,1,-1,2,-2,\\dots,-L,L]) (18)\n\nBy testing which entries of the result are negative or positive, one can read out the state up to even after rounding to finite precision.",
    "ssmformal-58": "The proof straightforwardly extends to multiple counters. \u220e\n\nWe are ready to prove the theorem:\n\nTheorem 23. (Repeated from Theorem 5) Consider the languages Dyck-1, Shuffle-Dyck, n-ary Boolean Expressions, , , and , as defined in Appendix C. Each of these is predictively modeled by an SSM. Proof. For each of these languages, we first define an assignment :\n\nFor a n \u200b b n superscript \ud835\udc4e \ud835\udc5b superscript \ud835\udc4f \ud835\udc5b a^{n}b^{n} : (here, C \ud835\udc36 C =1) u \u200b ( a ) \ud835\udc62 \ud835\udc4e \\displaystyle u(a) = 1 absent 1 \\displaystyle=1 u \u200b ( b ) \ud835\udc62 \ud835\udc4f \\displaystyle u(b) = \u2212 1 absent 1 \\displaystyle=-1 For Dyck-1: (here, C \ud835\udc36 C =1) u ( ` ` ( \" ) \\displaystyle u(``(\") = 1 absent 1 \\displaystyle=1 u ( ` ` ) \" ) \\displaystyle u(``)\") = \u2212 1 absent 1 \\displaystyle=-1 For Shuffle-Dyck- k \ud835\udc58 k (here, C = k \ud835\udc36 \ud835\udc58 C=k ) u ( ` ` ( i \" ) \\displaystyle u(``(_{i}\") = ( 0 , \u2026 , 0 , 1 , 0 \u200b \u2026 \u200b 0 ) where 1 is in the i -th slot absent 0 \u2026 0 1 0 \u2026 0 where 1 is in the i -th slot \\displaystyle=(0,\\dots,0,1,0\\dots 0)\\ \\ \\ \\ \\text{ where $1$ is in the $i$-th slot} u ( ` ` ) i \" ) \\displaystyle u(``)_{i}\") = ( 0 , \u2026 , 0 , \u2212 1 , 0 \u200b \u2026 \u200b 0 ) where \u2212 1 is in the i -th slot absent 0 \u2026 0 1 0 \u2026 0 where \u2212 1 is in the i -th slot \\displaystyle=(0,\\dots,0,-1,0\\dots 0)\\ \\ \\ \\ \\text{ where $-1$ is in the $i$-th slot} For \u200b a n \u200b b n \u200b c n : : For superscript \ud835\udc4e \ud835\udc5b superscript \ud835\udc4f \ud835\udc5b superscript \ud835\udc50 \ud835\udc5b absent \\displaystyle\\text{For }a^{n}b^{n}c^{n}: (here, C \ud835\udc36 C =2) u \u200b ( a ) \ud835\udc62 \ud835\udc4e \\displaystyle u(a) = ( 1 , 0 ) absent 1 0 \\displaystyle=(1,0) u \u200b ( b ) \ud835\udc62 \ud835\udc4f \\displaystyle u(b) = ( \u2212 1 , 1 ) absent 1 1 \\displaystyle=(-1,1) u \u200b ( c ) \ud835\udc62 \ud835\udc50 \\displaystyle u(c) = ( 0 , \u2212 1 ) absent 0 1 \\displaystyle=(0,-1) For \u200b a n \u200b b n \u200b c n \u200b d n : : For superscript \ud835\udc4e \ud835\udc5b superscript \ud835\udc4f \ud835\udc5b superscript \ud835\udc50 \ud835\udc5b superscript \ud835\udc51 \ud835\udc5b absent \\displaystyle\\text{For }a^{n}b^{n}c^{n}d^{n}: (here, C \ud835\udc36 C =3) u \u200b ( a ) \ud835\udc62 \ud835\udc4e \\displaystyle u(a) = ( 1 , 0 , 0 ) absent 1 0 0 \\displaystyle=(1,0,0) u \u200b ( b ) \ud835\udc62 \ud835\udc4f \\displaystyle u(b) = ( \u2212 1 , 1 , 0 ) absent 1 1 0 \\displaystyle=(-1,1,0) u \u200b ( c ) \ud835\udc62 \ud835\udc50 \\displaystyle u(c) = ( 0 , \u2212 1 , 1 ) absent 0 1 1 \\displaystyle=(0,-1,1) u \u200b ( d ) \ud835\udc62 \ud835\udc51 \\displaystyle u(d) = ( 0 , \u2212 1 , \u2212 1 ) absent 0 1 1 \\displaystyle=(0,-1,-1) For Boolean Expressions: (here, C \ud835\udc36 C =1) u \u200b ( \u27e8 V \u200b A \u200b L \u200b U \u200b E \u27e9 ) \ud835\udc62 delimited-\u27e8\u27e9 \ud835\udc49 \ud835\udc34 \ud835\udc3f \ud835\udc48 \ud835\udc38 \\displaystyle u(\\langle VALUE\\rangle) = \u2212 1 absent 1 \\displaystyle=-1 u \u200b ( \u27e8 n \u2212 A \u200b R \u200b Y \u27e9 ) \ud835\udc62 delimited-\u27e8\u27e9 \ud835\udc5b \ud835\udc34 \ud835\udc45 \ud835\udc4c \\displaystyle u(\\langle n-ARY\\rangle) = + n absent \ud835\udc5b \\displaystyle=+n\n\nFor each of these mappings, we use Lemma 22 at to construct a one-layer SSMs that can, for each of the counters, distinguish the values . In parallel, we pass on the input symbol itself in further dimensions. Overall, the output of single SSM layer provides, at every position, both the original symbol in and an element of . We can thus view the output of this layer as a string over an enriched string of symbols . Based on this, one can predictively model these languages as follows. For Dyck-1, the next token is EOS or \u201c(\u201d if , and \u201c(\u201d or \u201c)\u201d after any other prefix (note that predictive modeling assumes valid prefixes). Shuffle--Dyck is similar: EOS is allowed if and only if all counters are zero. An opening bracket is always allowed. A closing bracket is only allowed if the respective counter is . For , the next token is or if ; if or ; EOS if . Constructions for , are similar. For Boolean expressions, the next token is or EOS if , and any other token otherwise. All of these constructions can be encoded using an appropriate function applying to . \u220e\n\nB.5 Bounded-Depth Dyck\n\nDefinition 24. The language [28, 74] is given by the CFG with the nonterminals and the following production rules:\n\nS h \u2192 \u2192 subscript \ud835\udc46 \u210e absent \\displaystyle S_{h}\\rightarrow ( 1 S h \u2212 1 ) 1 | \u2026 | ( K S h \u2212 1 ) K | \u03f5 \\displaystyle(_{1}S_{h-1})_{1}|\\dots|(_{K}S_{h-1})_{K}|\\epsilon S h \u2212 1 \u2192 \u2192 subscript \ud835\udc46 \u210e 1 absent \\displaystyle S_{h-1}\\rightarrow ( 1 S h \u2212 2 ) 1 | \u2026 | ( K S h \u2212 2 ) K | \u03f5 \\displaystyle(_{1}S_{h-2})_{1}|\\dots|(_{K}S_{h-2})_{K}|\\epsilon \u2026 italic-\u2026 \\displaystyle\\dots \u2026 italic-\u2026 \\displaystyle\\dots S 2 \u2192 \u2192 subscript \ud835\udc46 2 absent \\displaystyle S_{2}\\rightarrow ( 1 S 1 ) 1 | \u2026 | ( K S 1 ) K | \u03f5 \\displaystyle(_{1}S_{1})_{1}|\\dots|(_{K}S_{1})_{K}|\\epsilon S 1 \u2192 \u2192 subscript \ud835\udc46 1 absent \\displaystyle S_{1}\\rightarrow ( 1 S 0 ) 1 | \u2026 | ( K S 0 ) K | \u03f5 \\displaystyle(_{1}S_{0})_{1}|\\dots|(_{K}S_{0})_{K}|\\epsilon S 0 \u2192 \u2192 subscript \ud835\udc46 0 absent \\displaystyle S_{0}\\rightarrow \u03f5 italic-\u03f5 \\displaystyle\\epsilon\n\nand the start symbol . Theorem 25. (Repeated from Theorem 6) The bounded-depth Dyck language is predictively modeled by a two-layer SSM at finite precision, with . Proof. In the first layer, we calculate each token depth up to using Lemma 22. After the first layer, at each position, the activations will indicate both the depth up to , and the identity of the symbol. The space of activations is thus . We then, for each depth , define a set-reset automaton (Definition 8) given by the set . Running all of these set-reset automata will tell us, for each depth, the identity of the last bracket at that depth. We can deduce the maximum depth at which the last bracket is an opening one, and thus infer the set of valid next symbols. The activity of these set-reset automata can, in parallel, be simulated by a second SSM layer using Lemma 9. We need such automata, and each SSM has width . \u220e\n\nAppendix C Definitions of Languages\n\nHere, we provide formal definitions of languages from the test suite based on Bhattamishra et al.",
    "ssmformal-59": "[6]. Descriptions follow Bhattamishra et al. [6], and are included here for self-conrainedness. In all cases, our data generation setup is directly taken from [6]. C.1 Regular Languages\n\nTomita Grammars. Used primarily as a benchmark language family for assessing sequence to sequence models [68], some of the languages in this family are star-free (with dot-depth of 1) and some non-star-free. All the regular languages of the family are defined on the alphabet . Individual language definitions are available in Table 1. . We follow the definition of [6] to define the family of star-free languages. In our experiments we only generate , , and languages, given the proven equivalence of Tomita-2 and .",
    "ssmformal-60": "All the languages of the family are defined on the alphabet of . has level in the dot-depth hierarchy. PARITY. PARITY is the set of all strings on the alphabet such that the number of 1\u2019s is even. This language can be easily recognized by a DFA with just two states. Others. We further have the non-star-free languages , and , and the star-free languages , , and . C.2 Counter Languages\n\nDyck and Shuffle-Dyck. Dyck-1 is defined on the alphabet and derived using the following CFG production rule: . We further use the family of Shuffle-k languages [51]. Shuffle-Dyck-k is defined in terms of . It is defined as the shuffle of Dyck-1 languages, each defined in terms of the alphabet where . n-ary Boolean Expressions. This is the set of valid expressions over various operators. We focus on up-to-3-ary expressions, defined using the following grammar:\n\nThis language is recognized by a counter automaton [19]. Others\n\nWe further include the languages of the forms , , and . Appendix D Experimental Details\n\nAll experiments used the Mamba reference implementation777https://github.com/state-spaces/mamba/blob/main/README.md. Unless stated otherwise, we followed the defaults given there ( , , ), as we found the default combination to work better than other options. We tuned for each language. D.1 Bhattamishra et al. [6] test suite\n\nData Preparation\n\nFor all the languages, we use either the data prepared by Bhattamishra et al. [6] or\u2014where not available\u2014their data-generation scripts, allowing full comparability with results they reported for transformers. We used their official code and data release at https://github.com/satwik77/Transformer-Formal-Languages (last commit 48eea2e; MIT license). Training sets typically consist of 10K samples, with lengths varying between 1 to 50. There are two heldout bins: one with in-distribution lengths ([1,50]), and one testing length generalization (lengths [51,100]). The first one was used for hyperparameter optimization. Each bin typically contains around 2K samples. However for languages such as , where the number of positive examples in each bin was limited, all possible examples for that bin are included. Hyperparameters\n\nFor each language, we conducted extensive hyperparameter search. We varied the parameter in Mamba across the set {16, 32, 64, 128, 256}. Additionally, we experimented with the number of layers in our model, ranging from 1 to 3, training each configuration for 100 epochs. For languages where Mamba performed well, this number of layers was sufficient. However, for languages where Mamba struggled, we increased the number of layers up to 12, with little to no success. We used the AdamW optimizer. To identify optimal learning rates, we started with a coarse hyperparameter search using values from the set {0.001, 0.0001, 0.00001}. If one of these learning rates showed high performance, we conducted a more fine-grained search to find the optimal learning rate. Finally, we varied the batch size from {16, 32, 64} for datasets with 10K training examples. For languages like with limited training size, we searched for an optimal batch size within the set {5, 10}. D.2 FlipFlop\n\nWe obtained the dataset of Liu et al. [45] from their release, https://huggingface.co/datasets/synthseq/flipflop (MIT license). Our setup corresponds to the deterministic (\u201cclean\u201d) mode in Liu et al.",
    "ssmformal-61": "[45]. Matching Figure 2 in Liu et al. [45], we evaluated both with in-distribution data (matching the distribution of the training dataset) with , and using an out of distribution sparse tail with , where refer to the probabilities of that instruction appearing in input sequences. We trained a one-layer Mamba with the default parameters888From https://github.com/state-spaces/mamba/blob/main/README.md, setting to 16 with the AdamW optimizer using a learning rate of and a batch size of 16.",
    "ssmformal-62": "Following the evaluation criteria for LSTMs in Liu et al. [45], we compute the test every 100 training steps on our validation sets of choice, by randomly sampling around samples from each set in every evaluation cycle. D.3 Bounded Hierarchical Structure\n\nWe built on the official code and data release of Yao et al. [74] at https://github.com/princeton-nlp/dyck-transformer (last commit: 5d21fcf). We train a 2-layer Mamba and a 1-layer Mamba on with and . The training set and the validation set contains samples of lengths , while the test set contains samples of lengths . We train Mamba with a varying number of layers and . We use the Adam optimizer with an initial learning rate of 0.01 or 0.001, using cross-entropy loss. After training for 100 epochs (with early stopping allowed in case of convergence), we select the learning rate with the better training performance. Appendix E Finite Precision Assumption\n\nAs described in Section 2, we adopt the finite precision notion used by Weiss et al. [70]: We allow an unbounded number of integer bits, but only fractional bits, where is a sufficiently large constant (e.g., ), independent of the length of the input. There are a variety of related precision notions in the theoretical literature on neural sequence models \u2013 here, we discuss the effect of other notions on our results:\n\n1. Infinite precision Infinite precision allows any parameter and intermediate value to be an arbitrary number. Such a setting is unrealistic, as it would allow encoding arbitrary detail about the input into infinite precision [e.g. 63] and read these out with sufficiently powerful functions (, , ) in (5) \u2013 this would lead to the unrealistic conclusion that any function and language could be represented. For this reason, theoretical work has often adopted restricted precision notions. 2. Finite inventory of values, where integer and fractional bits are both restricted. Such a setup may be justified based on the fact that any real computer has bounded memory, though such a setup precludes any positive results on non-finite-state problems for any computational architecture.999For instance, a Turing machine with bounded memory and thus a bounded tape is equivalent to a finite-state automaton. Such a restrictive setup would not affect our positive results on Flip-Flop, Star-Free, and bounded-depth Dyck languages (Theorems 1, 4, 6), as these all use bounded finite-precision activation values. As this is a more restricted setup than the one we are assuming, this also would not affect our negative results about PARITY and non-star-free languages (Theorems 2, 4). These results are thus highly robust to variations of the finite precision assumption. Such a more restrictive definition would, however, mean that, for unbounded counting (Theorem 5), modeling is only possible up to a bound determined by the number of possible values\u2014this is the one place where our results would be impacted. Indeed, we do observe that Mamba learns these counter languages on training lengths but struggles with length generalization. Transformers, on the other hand, can represent these languages with bounded activations (due to the constructions in [6]), and show strong length generalization. An intermediary between infinite and finite precision is notions of precision where the number of allowed bits slowly increases with the input length, e.g., logarithmically. Such a setup has particularly been adopted for transformers [48], because a finite-precision assumption leads to very low expressivity in transformers. For SSMs, on the other hand, we find that finite precision assumptions are sufficient for showing a broad range of positive results. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 17:39:09 2024 by LaTeXML"
}