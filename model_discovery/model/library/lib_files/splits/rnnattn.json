{
    "rnnattn-0": "# Attention as an RNN \n\nLeo Feng<br>Mila \\& Borealis AI<br>leo.feng@mila.quebec\n\nFrederick Tung<br>Borealis AI<br>frederick.tung@borealisai.com\n\nHossein Hajimirsadeghi<br>Borealis AI<br>hossein.hajimirsadeghi@borealisai.com\n\nMohamed Osama Ahmed<br>Borealis AI<br>mohamed.o.ahmed@borealisai.com\n\nYoshua Bengio<br>Mila - Universit\u00e9 de Montr\u00e9al<br>yoshua.bengio@mila.quebec\n\nGreg Mori<br>Borealis AI<br>greg.mori@borealisai.com\n\n\n#### Abstract\n\nThe advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its many-to-one RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's many-tomany RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce Aaren, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on 38 datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient. ## 1 Introduction\n\nAdvancements in sequence modelling are highly impactful due to the wide range of applications, including reinforcement learning (e.g., robotics and autonomous driving), time series classification (e.g., financial fraud detection and medical diagnoses), and time series forecasting (e.g., weather and energy consumption predictions). Over the past several years, an extensively explored topic in sequence modelling is that of Transformer-based models (Vaswani et al. 2017). This is due to Transformers' strong performance and ability to leverage GPU parallelism. As a result, numerous Transformer-specific survey papers have been written for various sequential settings such as reinforcement learning (Agarwal et al., 2023, Li et al., 2023), time series (Lin et al., 2022, Jiang et al., 2024), and speech processing (Latif et al. 2023). With the rapid increase in low-resource domains such as battery-powered devices, deployed models in these domains must be computationally efficient. Transformers, on the other hand, are expensive due to their quadratic scaling in memory and computation. Although their efficiency can be enhanced at inference time using techniques such as KV-caching (Pope et al., 2023), Transformers remain expensive for low-resource domains due to requiring (1) linear memory in the number of tokens and (2) the caching of all preceding tokens to the model. The effect of these limitations is exacerbated in settings with long contexts (i.e., large number of tokens) such as those common in time series (e.g., climate and economics). To address this issue, we begin by examining attention, the component contributing to Transformers quadratic computational complexity. We show that (1) attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its many-to-one RNN output efficiently. Leveraging the RNN formulation of attention, we (2) show that popular attention-based models (e.g., Transformers and Perceivers) can be viewed as RNNs. However, unlike traditional RNNs such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014), these attention-based models are unable to perform efficient updates with new tokens, an important property in sequential problem settings where data is processed in a stream. To address this, we (3) introduce a new formulation of attention based on the parallel prefix scan algorithm (Blelloch, 1990) that efficiently computes attention's many-to-many RNN output. Building on this new attention formulation, we (4) introduce Aaren ([A]ttention [a]s a [re]current neural [n]etwork), a computationally efficient module that can not only (i) be trained in parallel (like Transformers) but also (ii) be efficiently updated with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on 38 datasets spread across four popular sequential data settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient. ## 2 Background\n\n### 2.1 Recurrent Neural Networks\n\nRecurrent Neural Networks (RNNs) are specialized models for sequence modelling. In brief, RNNs process sequential data by iteratively computing hidden states as follows:\n\n$$\nh_{t}=f_{\\theta}\\left(h_{t-1}, x_{t}\\right)\n$$\n\nwhere $t$ denotes the step index, $x$ represents a token, $h$ represents the hidden state, and $f_{\\theta}$ is a neural network parameterized by $\\theta$. The value of the initial hidden state $h_{0}$ is typically learned via backpropagation. Popular RNNs such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al. 2014) fix the size of the hidden states $h_{t}$ to a constant irrespective of the step index. As such, these models are efficient at test time, requiring only constant memory and time per token and can be easily updated with new tokens, an important property in sequence modelling. However, due to being iterative by design, these popular RNNs also suffer scalability issues due to their lack of parallelizability. As such, RNNs were replaced by a parallelizable attention-based module, Transformers (Vaswani et al. 2017), as the standard for many sequence modelling settings. ### 2.2 Attention\n\nAttention retrieves information from a set of $X_{C}$ context tokens for a given set of query tokens $X_{Q}$ as follows:\n\n$$\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(Q K^{T}\\right) V\n$$\n\nwhere $Q=X_{Q} W_{q}$ is the query matrix, $K=X_{C} W_{k}$ is the key matrix, and $V=X_{C} W_{v}$ is the value matrix. $W_{q}, W_{k}, W_{v} \\in \\mathbb{R}^{d \\times d}$ are weight matrices (learned parameters). softmax $\\left(Q K^{T}\\right)$ computes weights of the context tokens for a weighted average. Notably, unlike RNNs, attention is not designed to be iterative; instead, it is designed to easily leverage GPU parallelism. Transformers (Vaswani et al. 2017) use self-attention ${ }^{1}$ a special case of attention, where the query tokens are the same as the context tokens. However, self-attention requires quadratic computation with respect to the\n\n[^0]number of tokens and is not efficiently updateable with new tokens. As a result, Transformers are computationally expensive, limiting their applications in low-resource domains. ## 3 Methodology\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_006c6f857a52fec5694cg-03.jpg?height=311&width=1325&top_left_y=506&top_left_x=399)\n\nFigure 1: Attention as a many-to-one RNN. The query tokens are the initial hidden states of the RNNs. (a) The conventional method of computing attention only computes its final output. As such, it can be viewed as a method of computing attention's many-to-one RNN output. (b) Transformer's self-attention (Vaswani et al., 2017) uses the input tokens as the initial hidden states. (c) Perceiver's cross-attention (Jaegle et al., 2021) uses input-dependent latents as the initial hidden states. Addressing this, we propose an efficient attention-based module capable of leveraging GPU parallelism while being efficiently updateable. We begin by first showing in Section 3.1 that attention can be viewed as an RNN with the special ability to compute its many-to-one RNN (Figure 1a) output efficiently. Leveraging the RNN formulation of attention, we further show that popular attentionbased models such as Transformers (Figure 1b) and Perceivers (Figure 1c) can be viewed as RNNs. However, unlike traditional RNNs, these models are unable to efficiently update themselves with new tokens, limiting their potential in sequential problem settings where data arrives as a stream. Tackling this, we introduce in Section 3.2 an efficient method for computing attention as a many-to-many RNN based on the parallel prefix scan algorithm. Building on this, we introduce in Section 3.3 Aaren ([A]ttention [a]s a [re]current neural [n]etwork), a computationally efficient module that can not only (i) be trained in parallel (like Transformers) but also (ii) be efficiently updated with new tokens at inference time, requiring only constant memory for inferences (like traditional RNNs). ### 3.1 Attention as a (many-to-one) RNN\n\nAttention for a query vector $q$ can be viewed as a function that maps $N$ context tokens $x_{1: N}$ via their keys and values $\\left\\{\\left(k_{i}, v_{i}\\right)\\right\\}_{i=1}^{N}$ to a singular output $o_{N}=$ Attention $\\left(q, k_{1: N}, v_{1: N}\\right)$. Given $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$, the output $o_{N}$ can be formulated as:\n\n$$\no_{N}=\\sum_{i=1}^{N} \\operatorname{softmax}(s)_{i} v_{i}=\\frac{\\sum_{i=1}^{N} \\exp \\left(s_{i}\\right) v_{i}}{\\sum_{i=1}^{N} \\exp \\left(s_{i}\\right)}=\\frac{\\hat{a}_{N}}{\\hat{c}_{N}}\n$$\n\nwhere the numerator is $\\hat{a}_{N}=\\sum_{i=1}^{N} \\exp \\left(s_{i}\\right) v_{i}$ and the denominator is $\\hat{c}_{N}=\\sum_{i=1}^{N} \\exp \\left(s_{i}\\right)$. Viewing attention as an RNN, we can compute both iteratively as rolling sums $\\hat{a}_{k}=\\hat{a}_{k-1}+\\exp \\left(s_{k}\\right) v_{k}$ and $\\hat{c}_{k}=\\hat{c}_{k-1}+\\exp \\left(s_{k}\\right)$ for $k=1, \\ldots, N$. However, in practice, this is an unstable implementation, running into numerical issues due to limited precision representations and potentially very small or large exponents (i.e., $\\exp (s)$ ). Mitigating this, we re-write the recurrence with a cumulative maximum term ${ }^{2} m_{k}=\\max _{i \\in\\{1, \\ldots, k\\}} s_{i}$, computing instead\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_006c6f857a52fec5694cg-03.jpg?height=457&width=578&top_left_y=1731&top_left_x=1164)\n\nFigure 2: Attention's RNN Cell. [^1]$a_{k}=\\sum_{i=1}^{k} \\exp \\left(s_{i}-m_{k}\\right) v_{i}$ and $c_{k}=\\sum_{i=1}^{k} \\exp \\left(s_{i}-m_{k}\\right)$. Notably, the final result is the same $o_{N}=\\frac{\\hat{a}_{N}}{\\hat{c}_{N}}=\\frac{a_{N}}{c_{N}} \\cdot a_{k}, c_{k}$, and $m_{k}$ are thus computed recurrently as follows:\n\\[\n\n$$\n\\begin{aligned}\na_{k} & =a_{k-1} \\exp \\left(m_{k-1}-m_{k}\\right)+v_{k} \\exp \\left(s_{k}-m_{k}\\right) \\\\\nc_{k} & =c_{k-1} \\exp \\left(m_{k-1}-m_{k}\\right)+\\exp \\left(s_{k}-m_{k}\\right) \\\\\nm_{k} & =\\max \\left(m_{k-1}, s_{k}\\right)\n\\end{aligned}\n$$\n\\]\n\nBy encapsulating the recurrent computation of $a_{k}, c_{k}$, and $m_{k}$ from $a_{k-1}, c_{k-1}$, and $m_{k-1}$, we introduce an RNN cell that iteratively computes the output of attention (see Figure 2). Attention's RNN cell takes as input $\\left(a_{k-1}, c_{k-1}, m_{k-1}, q\\right)$ and computes $\\left(a_{k}, c_{k}, m_{k}, q\\right)$. Note that the query vector $q$ is carried over in the RNN cell. The initial hidden state of attention's RNN is $\\left(a_{0}, c_{0}, m_{0}, q\\right)=$ $(0,0,0, q)$. Methods for computing attention. By viewing attention as an RNN, we can see that there are different ways to compute attention: (1) recurrently token-by-token (i.e., sequentially) in $O(1)$ memory or (2) in the conventional manner (i.e., in parallel) requiring linear $O(N)$ memory. Since attention can be viewed as an RNN, the conventional method of computing attention can also be viewed as an efficient method of computing attention's many-to-one RNN output, i.e., the output of an RNN that takes as input multiple context tokens but only outputs a single token at the end of the RNN (see Figure 1a). Lastly, instead of fully sequential or fully in parallel, we can also compute attention as (3) an RNN that processes the tokens block-by-block requiring $O(b)$ memory where $b$ is the size of the block. This method, however, is outside the scope of this work. As such, the description of the block-by-block RNN is included in Appendix A\nViewing existing attention-based models as RNNs. By viewing attention as an RNN, existing attention-based models can also be viewed as variations of RNNs. For example, Transformers' self-attentions are RNNs (Figure 1b) with the context tokens as their initial hidden states. Perceiver's cross-attentions are RNNs (Figure 1c) with context-dependent latents as their initial hidden states. By leveraging the RNN formulation of their attention mechanisms, these existing models can compute their output memory efficiently. ## Challenges of viewing attention as an RNN for exist\n\ning models. However, when viewing existing attentionbased models such as Transformers as RNNs, the models lack important properties common in traditional RNNs such as LSTMs and GRUs. Notably, LSTMs and GRUs are capable of efficiently updating themselves with new tokens in only $O(1)$ constant memory and computation, an important feature for sequence modelling where data is received in a stream. In contrast, the RNN view of Transformer (see Figure 1b) would handle new tokens by adding a new RNN with the new token as its initial state. The new RNN processes all preceding tokens, requiring $O(N)$ linear computation in the number of tokens. In Perceiver, the latents ( $L_{i}$ in Figure 1C) are input-dependent due to their architecture, meaning that their values change when receiving a new token. Since the initial hidden states (i.e., latents) of their RNNs change, Perceiver would thus require recomputing their RNNs from scratch, requiring $O(N L)$ linear computation in the number of tokens $(N)$ and the![](https://cdn.mathpix.com/cropped/2024_09_12_006c6f857a52fec5694cg-04.jpg?height=637&width=616&top_left_y=1484&top_left_x=1145)\n\n### 3.2 Attention as a (many-to-many) RNN\n\nAddressing these limitations, we propose to develop an attention-based model capable of leveraging the RNN formulation's ability to perform efficient updates. To do so, we first introduce an efficient parallelized method of computing attention as a many-to-many RNN, i.e., a parallel method to compute $\\left\\{o_{i}=\\operatorname{Attention}\\left(q, x_{1: i}\\right)\\right\\}_{i=1}^{N}$. To do so, we leverage the parallel prefix scan algorithm (Blelloch 1990) (see Algorithm 1), a parallel computation method for computing $N$ prefix computations\nfrom $N$ sequential data points via an associative operator $\\oplus$. The algorithm efficiently computes $\\left\\{\\bigoplus_{i=1}^{k} x_{i}\\right\\}_{k=1}^{N}$ from $\\left\\{x_{k}\\right\\}_{k=1}^{N}$. Recall that $\\operatorname{Attention}\\left(\\mathrm{q}, \\mathrm{x}_{1: \\mathrm{k}}\\right)=o_{k}=\\frac{a_{k}}{c_{k}}$ where $a_{k}=\\sum_{i=1}^{k} \\exp \\left(s_{i}-m_{k}\\right) v_{i}, c_{k}=\\sum_{i=1}^{k} \\exp \\left(s_{i}-\\right.$ $\\left.m_{k}\\right)$, and $m_{k}=\\max _{i \\in\\{1, \\ldots, k\\}} s_{i}$ To compute $\\left\\{\\right.$ Attention( $\\left.\\left.\\mathrm{q}, \\mathrm{x}_{1: \\mathrm{k}}\\right)\\right\\}_{k=1}^{N}$ efficiently, we can compute $\\left\\{a_{k}\\right\\}_{k=1}^{N},\\left\\{c_{k}\\right\\}_{k=1}^{N}$, and $\\left\\{m_{k}\\right\\}_{k=1}^{N}$ via the parallel scan algorithm and afterwards combine $a_{k}$ and $c_{k}$ to compute Attention $\\left(\\mathrm{q}, \\mathrm{x}_{1: \\mathrm{k}}\\right)$. To do so, we propose the following associative operator $\\oplus$ that acts on 3-tuples $3^{3}$ of the form $\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right)$ where $A$ is a set of indices, $\\mathrm{m}_{A}=\\max _{i \\in A} s_{i}, \\mathrm{u}_{A}=$ $\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A}\\right)$, and $\\mathrm{w}_{A}=\\sum_{i \\in A} \\exp \\left(s_{i}-\\right.$ $\\left.\\mathrm{m}_{A}\\right) v_{i}$. The parallel scan algorithm takes as input $\\left\\{\\left(\\mathrm{m}_{\\{i\\}}, \\mathrm{u}_{\\{i\\}}, \\mathrm{w}_{\\{i\\}}\\right)\\right\\}_{i=1}^{N}=\\left\\{\\left(s_{i}, 1, v_{i}\\right)\\right\\}_{i=1}^{N}$. The algorithm recursively applies the operator $\\oplus$ which works as follows:\n$\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right) \\oplus\\left(\\mathrm{m}_{B}, \\mathrm{u}_{B}, \\mathrm{w}_{B}\\right)=\\left(\\mathrm{m}_{A \\cup B}, \\mathrm{u}_{A \\cup B}, \\mathrm{w}_{A \\cup B}\\right)$\nwhere $\\mathrm{m}_{A \\cup B}=\\max \\left(\\mathrm{m}_{A}, \\mathrm{~m}_{B}\\right), \\mathrm{u}_{A \\cup B}=\\mathrm{u}_{A} \\exp \\left(\\mathrm{m}_{A}-\\right.$ $\\left.\\mathrm{m}_{A \\cup B}\\right)+\\mathrm{u}_{B} \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right)$ and $\\mathrm{w}_{A \\cup B}=$ $\\mathrm{w}_{A} \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\mathrm{w}_{B} \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right)$.",
    "rnnattn-1": "Upon completion of applying the operator recursively, the algorithm outputs $\\left\\{\\left(\\mathrm{m}_{\\{1, \\ldots, k\\}}, \\mathrm{u}_{\\{1, \\ldots, k\\}}, \\mathrm{w}_{\\{1, \\ldots, k\\}}\\right)\\right\\}_{k=1}^{N}=$ $\\left\\{\\left(m_{k}, \\sum_{i=1}^{k} \\exp \\left(s_{i}-m_{k}\\right), \\sum_{i=1}^{k} \\exp \\left(s_{i}-\\right.\\right.\\right.$ $\\left.\\left.\\left.m_{k}\\right) v_{i}\\right)\\right\\}_{k=1}^{N}$. Also known as $\\left\\{\\left(m_{k}, c_{k}, a_{k}\\right)\\right\\}_{k=1}^{N}$. Combining the last two values of the output tuples, we retrieve Attention $\\left(\\mathrm{q}, \\mathrm{x}_{1: \\mathrm{k}}\\right)=o_{k}=\\frac{a_{k}}{c_{k}}$, resulting in an efficient parallelized method for computing attention as a many-to-many RNN (Figure 3). ![](https://cdn.mathpix.com/cropped/2024_09_12_006c6f857a52fec5694cg-05.jpg?height=343&width=592&top_left_y=322&top_left_x=1165)\n\nFigure 3: Attention as a many-to-many RNN\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_006c6f857a52fec5694cg-05.jpg?height=643&width=595&top_left_y=757&top_left_x=1161)\n\nFigure 4: Stacking Aarens for sequence modelling\n\n### 3.3 Aaren:\n\n## Attention as a Recurrent Neural Network\n\nLeveraging the parallelized many-to-many formulation of attention, we propose Aaren ([A]ttention [a]s a [re]current neural [n]etwork). The interface of Aaren is the same as a Transformer, mapping $N$ inputs to $N$ outputs whereas the $i$-th output is an aggregate of the 1 st to $i$-th input. As such, Aaren is also (1) naturally stackable and (2) capable of computing individual loss terms for each sequence token. However, unlike Transformers which use a causal self-attention, Aaren uses the aforementioned method of computing attention as a many-to-many RNN, making it more efficient. Aaren functions as follows:\n\n$$\n\\begin{aligned}\nh_{1}^{(0)}, \\ldots, h_{N}^{(0)} & \\leftarrow x_{1}, \\ldots, x_{N} \\\\\n{\\left[h_{1}^{(j+1)}, \\ldots, h_{N}^{(j+1)}\\right] } & \\leftarrow \\operatorname{Aaren}\\left(q^{(j)},\\left[h_{1}^{(j)}, \\ldots, h_{N}^{(j)}\\right]\\right)\n\\end{aligned}\n$$\n\nUnlike Transformers where the query is one of the input tokens to attention, Aaren's query token $q$ is learned during training via backpropagation. In Figure 4 , we include an example of a stacked Aaren model with input context tokens $x_{1: 3}$ and outputs $y_{1: 3}$. Notably, since Aaren leverages the RNN formulation of attention, the stacking of Aarens is also the stacking of RNNs. Therefore, Aarens are also able to perform updates with new tokens efficiently, i.e., the iterative computation of $y_{k}$ only requires constant computation as it relies solely on $h_{k-1}$ and $x_{k}$. Unlike Transformer-based models which (1) require linear memory (when using KV-caching) and (2) require storing all previous tokens, including those in intermediate Transformer layers, Aaren-based models (1) require only constant memory and (2) does not require storing all previous tokens, making Aarens significantly more computationally efficient than Transformers. [^2]\n## 4 Experiments\n\nOur objective in the experiments is to compare Aarens with Transformers in terms of (1) performance and (2) resources required (time and memory). To perform a comprehensive comparison, we evaluate across four problem settings: reinforcement learning, event forecasting, time series forecasting, and time series classification. Datasets. In total, we evaluate Aarens and Transformers on 38 datasets, the majority of which are real-world datasets. The datasets are split between problem settings as follows: 12 reinforcement learning datasets, 8 event forecasting datasets, 8 time series forecasting datasets, and 10 time series classification datasets. For each dataset, the models are evaluated with 5 seeds. Due to space limitations, we refer the reader to Appendix Cdescriptions of individual datasets. Models. To compare Aarens directly with Transformers, we replace the Transformers with Aarens in domain-specialized Transformer models. For reinforcement learning, we performed the comparison on Decision Transformer (Chen et al., 2021). For event forecasting, we performed the comparison Transformer Hawkes Process (Zuo et al., 2020, Bae et al., 2023). For time series forecasting, we performed the comparison on a Transformer with input normalization following Liu et al. (2022); For time series classification, we performed the comparison on a vanilla causal transformer following the library by Wu et al. (2023). Implementation Details. The experiments are run using the datasets and transformer implementations of popular repositories. More specifically, the reinforcement learning experiments with Decision Transformer were run on the code by Barhate (2022). The time series forecasting and time series classification experiments were run on the Time Series Library repository by Wu et al. (2023). The event forecasting experiments were run on the code by Bae et al. (2023). As Transformers and Aarens share the same interface and are both attention-based methods, they share the same set of hyperparameters. For fairness, the same hyperparameters are used for both Transformers and Aarens. Due to space limitations, specific hyperparameter details are included in Appendix $\\mathrm{F}^{+}$\n\n### 4.1 Reinforcement Learning\n\nIn these experiments, we compare Aarens with Transformers on reinforcement learning (RL). In RL, the model's objective during training is to learn a policy that learns from feedback/rewards obtained through trial and error while interacting in an environment. As such, RL is popular in interactive settings such as robotics, recommendation engines, and traffic control. For these experiments, we consider Decision Transformers (Chen et al., 2021), a popular method for training a policy in an offline manner on datasets of environment interactions. At test time, Decision Transformers are evaluated in an online manner. We evaluate on popular locomotion robotics environments from the D4RL benchmark (Fu et al., 2020): HalfCheetah, Ant, Hopper, and Walker. For each of the locomotion environments, we compare the models trained on three different kinds of datasets: Medium, Medium-Replay, and Medium-Expert. Each of these datasets consists of 1 million timesteps generated by a policy. In total, we compare model performances across $4 \\times 3=12$ RL datasets. We refer the reader to Appendix C. 1 for more details regarding the individual tasks. The results in Table 1 show that Aarens achieve competitive performance with Transformers across all twelve datasets and four environments. However, unlike Transformers, Aarens are able to efficiently process new environmental interactions in constant computation due to also being an RNN, making it more suitable for reinforcement learning. ### 4.2 Event Forecasting\n\nIn these experiments, we compare Aarens with Transformers on event forecasting (EF). In EF, the model is given a sequence of irregularly spaced discrete events in time and models the probability distribution of the next event time and its mark (i.e., event label/class). EF is popular in many real-world settings such as finance (e.g., transactions), healthcare (e.g., patient observations), and e-commerce (e.g., purchases) where user or system actions occur at irregularly spaced times. To perform the comparison, we replace the Transformers in Transformers Hawkes Process (Zuo et al., 2020) with Aarens. Following Bae et al. (2023), a mixture of log-normal distributions is used to\n\n[^3]| Reinforcement Learning (Score $\\uparrow$ ) |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Dataset | HalfCheetah |  |  | Ant |  |  |  |\n|  | Medium | Med-Replay | Med-Expert | Medium | Med-Replay | Med-Expert |  |\n| Transformer | $\\mathbf{4 1 .",
    "rnnattn-2": "8 8} \\pm \\mathbf{1 . 4 7}$ | $\\mathbf{3 6 . 5 7} \\pm \\mathbf{1 . 4 0}$ | $\\mathbf{7 5 . 9 8} \\pm \\mathbf{6 . 3 4}$ | $\\mathbf{9 4 . 2 5} \\pm \\mathbf{8 . 6 2}$ | $\\mathbf{8 9 . 3 9} \\pm \\mathbf{4 . 9 6}$ | $\\mathbf{1 2 5 . 4 7} \\pm \\mathbf{1 0 . 9 9}$ |  |\n| Aaren | $\\mathbf{4 2 . 1 6} \\pm \\mathbf{1 . 8 9}$ | $\\mathbf{3 7 . 9 1} \\pm \\mathbf{1 . 9 4}$ | $\\mathbf{7 5 . 7 4} \\pm \\mathbf{1 5 . 1 3}$ | $\\mathbf{9 3 . 2 9} \\pm \\mathbf{4 . 0 4}$ | $\\mathbf{8 5 . 5 3} \\pm \\mathbf{6 .",
    "rnnattn-3": "5 7}$ | $\\mathbf{1 1 9 . 7 2} \\pm \\mathbf{1 2 . 6 3}$ |  |\n| Dataset | Hopper |  |  |  | Walker |  |  |\n|  | Medium | Med-Replay | Med-Expert | Medium | Med-Replay | Med-Expert |  |\n| Transformer | $\\mathbf{8 0 .",
    "rnnattn-4": "1 8} \\pm \\mathbf{5 . 8 5}$ | $\\mathbf{7 9 . 7 3} \\pm \\mathbf{7 . 6 4}$ | $\\mathbf{9 8 . 8 2} \\pm \\mathbf{1 0 . 3 3}$ | $\\mathbf{7 7 . 8 4} \\pm \\mathbf{3 . 8 1}$ | $\\mathbf{7 2 . 3 6} \\pm \\mathbf{5 . 6 3}$ | $\\mathbf{1 0 9 . 6 6} \\pm \\mathbf{0 . 4 5}$ |  |\n| Aaren | $\\mathbf{8 0 . 8 6} \\pm \\mathbf{4 . 7 7}$ | $\\mathbf{7 7 . 8 7} \\pm \\mathbf{5 . 6 8}$ | $\\mathbf{1 0 3 . 8 9} \\pm \\mathbf{1 1 . 8 9}$ | $\\mathbf{7 4 . 4 4} \\pm \\mathbf{5 . 1 6}$ | $\\mathbf{7 1 . 4 4} \\pm \\mathbf{6 .",
    "rnnattn-5": "5 5}$ | $\\mathbf{1 1 0 . 5 1} \\pm \\mathbf{1 . 3 0}$ |  |\n\nTable 1: Reinforcement Learning Results. Measurement of the D4RL score (higher is better) (Fu et al.",
    "rnnattn-6": "2020). The bolded results indicate the best-performing method. model the probability distribution of the next event time. For this setting, we consider 8 popular benchmarking datasets for next event forecasting (Zhang et al., 2020, Zuo et al., 2020, Bae et al., 2023): MIMIC, Wiki, Reddit, Mooc, StackOverflow, Sin, Uber, and Taxi. 7 out of the 8 are real-world datasets whereas only Sin is a synthetic dataset. 3 out of the 8 datasets (Sin, Uber, and Taxi) do not include marks/labels. We refer the reader to Appendix C. 2 for details regarding individual datasets. The results in Table 2 show that Aarens performed comparably with Transformers across all datasets. Aaren's ability to efficiently process new inputs is a particularly useful feature in event forecasting settings where events arrive in an irregular stream. | Event Forecasting |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Metric | NLL ( $\\downarrow$ ) |  | RMSE ( $\\downarrow$ ) |  | $\\operatorname{Acc~}(\\uparrow)$ |  |\n| Dataset | MIMIC | Wiki | MIMIC | Wiki | MIMIC | Wiki |\n| Transformer | $1.22 \\pm 0.08$ | $9.66 \\pm 0.98$ | $1.60 \\pm 0.28$ | $0.28 \\pm 0.04$ | $84.07 \\pm 1.46$ | $23.60 \\pm 2.66$ |\n| Aaren | $1.21 \\pm 0.06$ | $8.98 \\pm 1.03$ | $1.56 \\pm 0.32$ | $0.22 \\pm 0.05$ | $84.53 \\pm 0.66$ | $21.26 \\pm 1.29$ |\n| Dataset | Reddit | Mooc | Reddit | Mooc | Reddit | Mooc |\n| Transformer | $0.40 \\pm 0.29$ | $-0.22 \\pm 0.57$ | $0.23 \\pm 0.03$ | $0.20 \\pm 0.04$ | $60.68 \\pm 1.62$ | $37.79 \\pm 0.42$ |\n| Aaren | $0.31 \\pm 0.30$ | $0.25 \\pm 1.61$ | $0.30 \\pm 0.04$ | $0.41 \\pm 0.28$ | $62.34 \\pm 0.40$ | $36.69 \\pm 1.48$ |\n| Dataset | StackOverflow | Sin | StackOverflow | Sin | StackOverflow |  |\n| Transformer | $2.92 \\pm 0.04$ | $0.68 \\pm 0.05$ | $1.44 \\pm 0.08$ | $1.75 \\pm 0.09$ | $46.44 \\pm 0.08$ |  |\n| Aaren | $2.91 \\pm 0.02$ | $0.78 \\pm 0.13$ | $1.27 \\pm 0.17$ | $2.03 \\pm 0.25$ | $46.34 \\pm 0.21$ |  |\n| Dataset | Uber | Taxi | Uber | Taxi |  |  |\n| Transformer | $3.33 \\pm 0.14$ | $2.01 \\pm 0.17$ | 73.63 $\\pm 5.73$ | $10.34 \\pm 0.32$ |  |  |\n| Aaren | $3.48 \\pm 0.10$ | $2.33 \\pm 0.12$ | $54.61 \\pm 5.40$ | $10.01 \\pm 0.52$ |  |  |\n\nTable 2: Event Forecasting Results. Sin, Uber, and Taxi datasets do not include marks/labels. ### 4.3 Time Series Forecasting\n\nIn these experiments, we compared Aarens with Transformers on time series forecasting (TSF). In TSF, the model is given a series of observations of temporally continuous signals. The objective of the model is to predict $T$ future values of the series. TSF models are commonly used in a wide range of domains, including those related to climate (e.g., weather), energy (e.g., supply and demand), and economics (e.g., stock prices). To perform the comparison, we consider a causally masked Transformer with input normalization following Liu et al. (2022). For this setting, we consider 8 real-world datasets used in prior works: Weather, Exchange, Traffic, ECL, ETTh1, ETTh2, ETTm1, and ETTm2. For details regarding the individual datasets, we refer the reader to Appendix C.3. Following Wu et al. (2023), the models are evaluated with $T \\in\\{96,192,336,720\\}$ given an input length of 96 . Due to space limitations, Table 3 only includes results for $T=192$. We refer the reader to Table 5 in Appendix D for the full results. The results in Table 3 show that Aarens perform comparably with Transformers across all datasets. However, unlike Transformers, Aarens efficiently processes the time series data, making it more suitable for time series-related domains. Time Series Forecasting\n\n| Metric | MSE ( $\\downarrow$ ) |  |  | MAE ( $\\downarrow$ ) |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Dataset | Weather | Exchange | Traffic | Weather | Exchange | Traffic |\n| Transformer | $\\mathbf{0 .",
    "rnnattn-7": "2 4} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 2 4} \\pm \\mathbf{0 . 0 2}$ | $\\mathbf{0 . 6 3} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 2 8} \\pm \\mathbf{0 . 0 0}$ | $\\mathbf{0 . 3 4} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 3 4} \\pm \\mathbf{0 . 0 0}$ |\n| Aaren | $\\mathbf{0 . 2 5} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 2 5} \\pm \\mathbf{0 . 0 3}$ | $\\mathbf{0 . 6 4} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 2 8} \\pm \\mathbf{0 . 0 0}$ | $\\mathbf{0 . 3 3} \\pm \\mathbf{0 . 0 2}$ | $0.35 \\pm 0.00$ |\n| Dataset | ETTh1 | ETTm1 | ECL | ETTh1 | ETTm1 | ECL |\n| Transformer | $\\mathbf{0 .",
    "rnnattn-8": "6 4} \\pm \\mathbf{0 . 0 5}$ | $\\mathbf{0 . 5 2} \\pm \\mathbf{0 . 0 5}$ | $\\mathbf{0 . 3 9} \\pm \\mathbf{0 . 0 3}$ | $\\mathbf{0 . 5 7} \\pm \\mathbf{0 . 0 2}$ | $\\mathbf{0 . 4 7} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 4 8} \\pm \\mathbf{0 . 0 2}$ |\n| Aaren | $\\mathbf{0 . 5 9} \\pm \\mathbf{0 . 0 3}$ | $\\mathbf{0 . 5 1} \\pm \\mathbf{0 . 0 3}$ | $\\mathbf{0 . 3 7} \\pm \\mathbf{0 . 0 2}$ | $\\mathbf{0 . 5 5} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 4 7} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 4 5} \\pm \\mathbf{0 . 0 1}$ |\n| Dataset | ETTh1 | ETTm1 |  | ETTh2 | ETTm2 |  |\n| Transformer | $\\mathbf{0 .",
    "rnnattn-9": "5 0} \\pm \\mathbf{0 . 0 3}$ | $\\mathbf{0 . 3 8} \\pm \\mathbf{0 . 0 2}$ |  | $\\mathbf{0 . 4 6} \\pm \\mathbf{0 . 0 1}$ | $\\mathbf{0 . 3 7} \\pm \\mathbf{0 . 0 1}$ |  |\n| Aaren | $\\mathbf{0 . 4 9} \\pm \\mathbf{0 . 0 3}$ | $\\mathbf{0 . 3 4} \\pm \\mathbf{0 . 0 4}$ |  | $\\mathbf{0 . 4 8} \\pm \\mathbf{0 . 0 2}$ | $\\mathbf{0 . 3 9} \\pm \\mathbf{0 . 0 2}$ |  |\n|  |  |  |  |  |  |  |\n\nTable 3: Time Series Forecasting Results. Following Wu et al. (2023), the models are evaluated with $T \\in\\{96,192,336,720\\}$ given an input length of 96 . Due to space limitations, this table only includes results for $T=192$. We refer the reader to Appendix D(Table 5) for the full results. | Time Series Classification (Acc $\\uparrow$ ) |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Dataset | EthanolConc. | FaceDetection | Handwriting | Heartbeat | Jap. Vowels |  |\n| Transformer | $\\mathbf{2 9 . 8 9} \\pm \\mathbf{1 . 6 3}$ | $\\mathbf{6 9 . 2 3} \\pm \\mathbf{0 . 5 2}$ | $\\mathbf{2 6 . 5 4} \\pm \\mathbf{2 . 2 5}$ | $\\mathbf{7 4 . 0 5} \\pm \\mathbf{1 . 2 1}$ | $\\mathbf{9 6 . 3 8} \\pm \\mathbf{0 . 9 1}$ |  |\n| Aaren | $\\mathbf{2 9 . 5 8} \\pm \\mathbf{2 . 3 0}$ | $\\mathbf{6 9 . 0 6} \\pm \\mathbf{0 . 6 1}$ | $\\mathbf{2 7 . 3 9} \\pm \\mathbf{1 . 4 6}$ | $\\mathbf{7 4 . 1 5} \\pm \\mathbf{0 . 7 7}$ | $\\mathbf{9 6 . 6 5} \\pm \\mathbf{0 . 7 5}$ |  |\n| Dataset | PEMS-SF | SelfReg. SCP1 | SelfReg. SCP2 | ArabicDigits | UWaveGesture |  |\n| Transformer | $\\mathbf{7 8 .",
    "rnnattn-10": "7 3} \\pm \\mathbf{2 . 0 6}$ | $\\mathbf{8 8 . 8 1} \\pm \\mathbf{0 . 9 2}$ | $\\mathbf{5 2 . 8 9} \\pm \\mathbf{2 . 4 7}$ | $\\mathbf{9 8 . 8 9} \\pm \\mathbf{0 . 5 7}$ | $\\mathbf{7 9 . 8 1} \\pm \\mathbf{1 . 5 1}$ |  |\n| Aaren | $\\mathbf{8 1 . 8 5} \\pm \\mathbf{2 . 6 0}$ | $\\mathbf{8 9 . 4 2} \\pm \\mathbf{1 . 8 5}$ | $\\mathbf{5 4 . 2 2} \\pm \\mathbf{1 . 5 0}$ | $\\mathbf{9 8 . 6 8} \\pm \\mathbf{0 .",
    "rnnattn-11": "2 0}$ | $\\mathbf{8 2 . 0 0} \\pm \\mathbf{1 . 9 3}$ |  |\n\nTable 4: Time Series Classification Results. ### 4.4 Time Series Classification\n\nIn these experiments, we compared Aarens with Transformers on time series classification (TSC).",
    "rnnattn-12": "In TSC, the model's objective is to predict the label of a time series.",
    "rnnattn-13": "This setting is common in many important applications such as pattern recognition (e.g., electrocardiograms), anomaly detection (e.g., bank fraud), or failure prediction (e.g., power grid fluctuations) (Dinger et al. 2022). For this setting, we consider 10 real-world popular datasets from the UEA time series classification Archive (Bagnall et al., 2018): EthanolConcentration, FaceDetection, Handwriting, Heartbeat, JapaneseVowels, PEMS-SF, SelfRegulationSCP1, SelfRegulationSCP2 ArabicDigits, and UWaveGesture. For details regarding the individual datasets, we refer the reader to Appendix C. 4 In Table 4, we see that Aarens perform comparably with Transformers across all datasets. ### 4.5 Analyses\n\nIn these experiments, we compare Aarens with Transformers in terms of the resources required. To do so, we use the code by Barhate (2022). For Transformers, we use KV-caching to improve their efficiency. Memory Complexity: In Figure 5(left), we compare the memory usage of Aarens and Transformers (using KV-caching) at inference time. We see that the memory usage of Transformers grow linearly with the KV-caching technique. In contrast, Aarens only use constant memory regardless of the number of tokens, making it significantly more efficient. Time Complexity: In Figure 5 (right), we compare the cumulative time needed by Aarens and Transformers (using KV-caching) for sequentially processing a sequence of tokens. In the case of Transformers, the cumulative amount of computation is quadratic in the number of tokens, i.e., $O(1+2+\\ldots+N)=O\\left(N^{2}\\right)$. In contrast, for Aaren, the cumulative amount of computation is linear. In the figure, we see a similar result in the cumulative time needed by the models. Specifically, the cumulative time needed by Transformers grows quadratically while that of Aaren grows linearly. Number of Parameters: Due to learning the initial hidden state $q$, Aaren modules require slightly more parameters than Transformer modules. However, the difference is marginal due to $q$ being\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_006c6f857a52fec5694cg-09.jpg?height=412&width=1361&top_left_y=233&top_left_x=384)\n\nFigure 5: Computational Resources Plots comparing Aarens and Transformers (using KV-caching) when processing a sequence of tokens.",
    "rnnattn-14": "(Left) Memory Usage Comparison. (Right) Cumulative Time Comparison. only a vector. Measuring this empirically in comparable models, we found that Transformers used $3,152,384$ parameters. In contrast, the equivalent Aarens used 3, 152, 896 parameters, representing only a marginal $\\sim 0.016 \\%$ parameter increase - a minor trade-off for the significant gains in memory and time complexities. ## 5 Related Work\n\nClosest to Aaren are approximations of attention such as those by RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), and Linear Transformer (Katharopoulos et al., 2020). These models proposed linearizations of the standard softmax-based attention that allow them to be formulated as an RNN. However, in doing so, these models also encode an exponential factor that biases tokens based on their timestamp, limiting their potential applications. In contrast, Aaren leverages an exact re-formulation of softmax attention as an RNN, allowing the model itself to compute the weight of each token. Feng et al. (2023) showed attention can be computed recurrently, using it to compress set-based inputs. Rabe and Staats (2022) introduced a recurrent formulation of attention, showing that self-attention can be computed efficiently. Katharopoulos et al. (2020) showed that Transformers with a causal mask can be viewed as an RNN. In contrast, we (1) show a more general result whereas any attention model can be viewed as an RNN. Furthermore, we (2) introduce Aaren, a new attention formulation based on parallel prefix sums, that achieves competitive results with that of Transformers while being more efficient. The problem of computing prefix scans/sums has been well studied with various efficient parallelized algorithms proposed for computing them. Since Aaren only requires the output of the prefix scan, any efficient algorithm for computing it can be used. In this work, we outlined the method by Hillis and Steele (1986). This method is time efficient for parallel computation, requiring $\\log _{2}(N)$ sequential steps and $\\mathcal{O}(N \\log (N))$ overall computation. In contrast, the method by Ladner and Fischer (1980) use mores sequential steps (specifically, $2 \\log _{2}(N)-2$ ) but only performs $\\mathcal{O}(N)$ overall computation. For a more in-depth introduction to parallel prefix sums algorithms, we refer the reader to the following work by Blelloch (1990). In this work, we applied Transformers to a subset of applications. For a broad overview of the applications of Transformers, we refer the reader to the following survey by Islam et al. (2023). For an overview of different transformer models applied to the specific settings considered in this paper, we refer the reader to the following surveys (1) on transformers in reinforcement learning by Li et al. (2023) and (2) on transformers in event forecasting, time series forecasting, time series classification, and more by Wen et al. (2022). ## 6 Conclusion\n\nIn this work, we showed that attention can be formulated as an RNN whereas the conventional way of computing attention is a parallelized method of computing its many-to-one RNN output. Building on the RNN formulation, we showed that existing attention-based models can be formulated as RNNs. However, unlike traditional RNNs such as LSTMs and GRUs, these methods cannot be updated efficiently with new tokens. Addressing this, we introduced a new parallelized method of computing attention's many-to-many RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we introduced Aaren, a new module that can not only (i) be trained in parallel (like Transformers) but also (ii) be efficiently updated at inference time, thereby requiring only constant memory (like RNNs). Empirically, we showed that Aarens achieve performance competitive with Transformers on 38 datasets spread across four sequential data settings: reinforcement learning, event forecasting, time series classification, and time series forecasting. Finally, we empirically show that Aarens are significantly more time and memory-efficient than Transformers. ## References\n\nAgarwal, P., Rahman, A. A., St-Charles, P.-L., Prince, S. J., and Kahou, S. E. (2023). Transformers in reinforcement learning: A survey.",
    "rnnattn-15": "arXiv preprint arXiv:2307.05979. Bae, W., Ahmed, M. O., Tung, F., and Oliveira, G. L. (2023). Meta temporal point processes. In International Conference on Learning Representations. Bagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., and Keogh, E. (2018). The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075.",
    "rnnattn-16": "Barhate, N. (2022). Minimal implementation of decision transformer. https://github.com/ nikhilbarhate99/min-decision-transformer\n\nBlelloch, G.",
    "rnnattn-17": "E. (1990). Prefix sums and their applications. Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon University. Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097. Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734.",
    "rnnattn-18": "Dinger, T., Chang, Y.-c., Pavuluri, R., and Subramanian, S. (2022). What is time series classification? https://developer.ibm.com/ learningpaths/get-started-time-series-classification-api/ what-is-time-series-classification/.",
    "rnnattn-19": "Feng, L., Tung, F., Hajimirsadeghi, H., Bengio, Y., and Ahmed, M.",
    "rnnattn-20": "O. (2023). Memory efficient neural processes via constant memory attention block.",
    "rnnattn-21": "arXiv preprint arXiv:2305.14567. Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020). D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219. Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. (2016). Deep learning, volume 1. MIT Press. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.",
    "rnnattn-22": "In International conference on machine learning, pages 1861-1870. PMLR. Hillis, W. D. and Steele, G. L. (1986). Data parallel algorithms.",
    "rnnattn-23": "Commun. ACM, 29:1170-1183. Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9:1735-80. Islam, S., Elmekki, H., Elsebai, A., Bentahar, J., Drawel, N., Rjoub, G., and Pedrycz, W. (2023). A comprehensive survey on applications of transformers for deep learning tasks. Expert Systems with Applications, page 122666 . Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. (2021). Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651-4664. PMLR. Jiang, Y., Pan, Z., Zhang, X., Garg, S., Schneider, A., Nevmyvaka, Y., and Song, D. (2024). Empowering time series analysis with large language models: A survey. arXiv preprint arXiv:2402.03182. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. (2020). Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "rnnattn-24": "In International conference on machine learning, pages $5156-5165$. PMLR. Ladner, R. E. and Fischer, M.",
    "rnnattn-25": "J. (1980). Parallel prefix computation.",
    "rnnattn-26": "J. ACM, 27(4):831-838. Latif, S., Zaidi, A., Cuayahuitl, H., Shamshad, F., Shoukat, M., and Qadir, J. (2023). Transformers in speech processing: A survey. arXiv preprint arXiv:2303.11607. Li, W., Luo, H., Lin, Z., Zhang, C., Lu, Z., and Ye, D. (2023). A survey on transformers in reinforcement learning. arXiv preprint arXiv:2301.03044.",
    "rnnattn-27": "Lin, T., Wang, Y., Liu, X., and Qiu, X. (2022). A survey of transformers. AI Open. Liu, Y., Wu, H., Wang, J., and Long, M. (2022). Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, $35: 9881-9893$. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Derczynski, L., et al. (2023). Rwkv: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14048-14077. Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J. (2023). Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5 . Rabe, M. N. and Staats, C. (2022). Self-attention does not need $o\\left(n^{2}\\right)$ memory.",
    "rnnattn-28": "arXiv preprint arXiv:2112.05682. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. (2023). Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30 . Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. (2022). Transformers in time series: A survey. arXiv preprint arXiv:2202.07125. Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. (2023). Timesnet: Temporal 2dvariation modeling for general time series analysis. In International Conference on Learning Representations. Zhang, Q., Lipani, A., Kirnap, O., and Yilmaz, E. (2020). Self-attentive hawkes process. In International conference on machine learning, pages 11183-11193. PMLR. Zheng, Q., Zhang, A., and Grover, A. (2022). Online decision transformer. In international conference on machine learning, pages 27042-27059. PMLR. Zuo, S., Jiang, H., Li, Z., Zhao, T., and Zha, H. (2020). Transformer hawkes process.",
    "rnnattn-29": "In International conference on machine learning, pages 11692-11702. PMLR. ## Appendix\n\n## A Block-by-block method of computing attention as a (many-to-one) RNN. In the main paper, we mentioned there are multiple ways to compute attention: (1) the conventional manner (i.e., in parallel) requiring $O(N)$ linear memory and (2) recurrently token-by-token as an RNN, requiring only $O(1)$ constant memory. In between these two methods, there is a third method which computes attention (3) in a block-by-block manner as follows:\n\n$$\n\\begin{aligned}\na_{i+b} & =a_{i} \\exp \\left(m_{i}-m_{i+b}\\right)+\\sum_{j=i}^{i+b} v_{j} \\exp \\left(s_{j}-m_{i+b}\\right) \\\\\nc_{i+b} & =c_{i} \\exp \\left(m_{i}-m_{i+b}\\right)+\\sum_{j=i}^{i+b} \\exp \\left(s_{j}-m_{i+b}\\right) \\\\\nm_{i+b} & =\\max \\left(m_{i}, s_{i+1}, \\ldots, s_{i+b}\\right)\n\\end{aligned}\n$$\n\nwhere $b$ is a block size that controls the rate at which data is processed. Computing attention in this manner requires $O(b)$ memory where $b$ is a hyperparameter. By setting $b=1$, we retrieve the RNN formulation that processes the tokens one-by-one, i.e., computing $a_{i}, c_{i}, m_{i}$ using the previous time step $a_{i-1}, c_{i-1}, m_{i-1}$. ## B Parallel Prefix Scan's Operator $\\oplus$\n\nIn this work, we proposed a formulation of Attention as a many-to-many RNN based on the parallel prefix scan algorithm. To do so, we defined an associative operator $\\oplus$ that is recursively applied during the algorithm. In this section, we show the correctness of the operator and prove that it satisfies the associative property. The operator $\\oplus$ acts on 3 -tuples of the form $\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right)$ where $A$ is a set of indices, $\\mathrm{m}_{A}=\\max _{i \\in A} s_{i}$, $\\mathrm{u}_{A}=\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A}\\right)$, and $\\mathrm{w}_{A}=\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A}\\right) v_{i}$. The operator $\\oplus$ functions as follows:\n\n$$\n\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right) \\oplus\\left(\\mathrm{m}_{B}, \\mathrm{u}_{B}, \\mathrm{w}_{B}\\right)=\\left(\\mathrm{m}_{A \\cup B}, \\mathrm{u}_{A \\cup B}, \\mathrm{w}_{A \\cup B}\\right)\n$$\n\nwhere:\n\n$$\n\\begin{aligned}\n& \\mathrm{m}_{A \\cup B}=\\max \\left(\\mathrm{m}_{A}, \\mathrm{~m}_{B}\\right) \\\\\n& \\mathrm{u}_{A \\cup B}=\\mathrm{u}_{A} \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\mathrm{u}_{B} \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right) \\\\\n& \\mathrm{w}_{A \\cup B}=\\mathrm{w}_{A} \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\mathrm{w}_{B} \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right)\n\\end{aligned}\n$$\n\n## B. 1 Correctness Proof\n\nTo confirm the correctness of the operator, we need to show: $\\mathrm{m}_{A \\cup B}=\\max _{i \\in A \\cup B} s_{i}, \\mathrm{u}_{A \\cup B}=$ $\\sum_{i \\in A \\cup B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right)$, and $w_{A \\cup B}=\\sum_{i \\in A \\cup B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right) v_{i}$. Expanding the computation of $m_{A \\cup B}$ :\n\n$$\n\\begin{aligned}\n\\mathrm{m}_{A \\cup B} & =\\max \\left(\\mathrm{m}_{A}, \\mathrm{~m}_{B}\\right) \\\\\n& =\\max \\left(\\max _{i \\in A} s_{i}, \\max _{i \\in B} s_{i}\\right) \\\\\n& =\\max _{i \\in A \\cup B} s_{i}\n\\end{aligned}\n$$\n\nExpanding the computation of $u_{A \\cup B}$ :\n\n$$\n\\begin{aligned}\n\\mathrm{u}_{A \\cup B} & =\\mathrm{u}_{A} \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\mathrm{u}_{B} \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right) \\\\\n& =\\left[\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A}\\right)\\right] \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\left[\\sum_{i \\in B} \\exp \\left(s_{i}-\\mathrm{m}_{B}\\right)\\right] \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right) \\\\\n& =\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right)+\\sum_{i \\in B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right) \\\\\n& =\\sum_{i \\in A \\cup B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right)\n\\end{aligned}\n$$\n\nExpanding the computation of $\\mathrm{w}_{A \\cup B}$ :\n\n$$\n\\begin{aligned}\n\\mathrm{w}_{A \\cup B} & =\\mathrm{w}_{A} \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\mathrm{w}_{B} \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right) \\\\\n& =\\left[\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A}\\right) v_{i}\\right] \\exp \\left(\\mathrm{m}_{A}-\\mathrm{m}_{A \\cup B}\\right)+\\left[\\sum_{i \\in B} \\exp \\left(s_{i}-\\mathrm{m}_{B}\\right) v_{i}\\right] \\exp \\left(\\mathrm{m}_{B}-\\mathrm{m}_{A \\cup B}\\right) v_{i} \\\\\n& =\\sum_{i \\in A} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right) v_{i}+\\sum_{i \\in B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right) v_{i} \\\\\n& =\\sum_{i \\in A \\cup B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right) v_{i}\n\\end{aligned}\n$$\n\nresulting in $\\mathrm{m}_{A \\cup B}=\\max _{i \\in A \\cup B} s_{i}, \\mathrm{u}_{A \\cup B}=\\sum_{i \\in A \\cup B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right)$, and $\\mathrm{w}_{A \\cup B}=$ $\\sum_{i \\in A \\cup B} \\exp \\left(s_{i}-\\mathrm{m}_{A \\cup B}\\right) v_{i}$ as needed. ## B. 2 Associative Proof\n\nFor the operator $\\oplus$ to be valid for the parallel scan algorithm, the operator $\\oplus$ must satisfy the associative property, i.e., $(\\mathrm{a} \\oplus \\mathrm{b}) \\oplus \\mathrm{c}=\\mathrm{a} \\oplus(\\mathrm{b} \\oplus \\mathrm{c})$. Since the detailed operator is applied to 3-tuples of the form $\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right)$ as follows:\n\n$$\n\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right) \\oplus\\left(\\mathrm{m}_{B}, \\mathrm{u}_{B}, \\mathrm{w}_{B}\\right)=\\left(\\mathrm{m}_{A \\cup B}, \\mathrm{u}_{A \\cup B}, \\mathrm{w}_{A \\cup B}\\right)\n$$\n\ntherefore showing that $\\oplus$ is associative means that:\n\n$$\n\\begin{aligned}\n{\\left[\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right) \\oplus\\left(\\mathrm{m}_{B}, \\mathrm{u}_{B}, \\mathrm{w}_{B}\\right)\\right] \\oplus\\left(\\mathrm{m}_{C}, \\mathrm{u}_{C}, \\mathrm{w}_{C}\\right) } & =\\left(\\mathrm{m}_{A}, \\mathrm{u}_{A}, \\mathrm{w}_{A}\\right) \\oplus\\left[\\left(\\mathrm{m}_{B}, \\mathrm{u}_{B}, \\mathrm{w}_{B}\\right) \\oplus\\left(\\mathrm{m}_{C}, \\mathrm{u}_{C}, \\mathrm{w}_{C}\\right)\\right] \\\\\n\\left(\\mathrm{m}_{(A \\cup B) \\cup C}, \\mathrm{u}_{(A \\cup B) \\cup C}, \\mathrm{w}_{(A \\cup B) \\cup C}\\right) & =\\left(\\mathrm{m}_{A \\cup(B \\cup C)}, \\mathrm{u}_{A \\cup(B \\cup C)}, \\mathrm{w}_{A \\cup(B \\cup C)}\\right)\n\\end{aligned}\n$$\n\nSince $\\cup$ is an associative operator that acts on sets, i.e., $(A \\cup B) \\cup C=A \\cup(B \\cup C)$, therefore $\\mathrm{m}_{(A \\cup B) \\cup C}=\\mathrm{m}_{A \\cup(B \\cup C)}, \\mathrm{u}_{(A \\cup B) \\cup C}=\\mathrm{u}_{A \\cup(B \\cup C)}$, and $\\mathrm{w}_{(A \\cup B) \\cup C}=\\mathrm{w}_{A \\cup(B \\cup C)}$, meaning that $\\oplus$ is associative as required. ## C Breakdown of Individual Datasets\n\n## C. 1 Reinforcement Learning\n\nOur code for the reinforcement learning experiments is based on that of Barhate (2022) (MIT License). For these experiments, we consider locomotion MuJoCo robotics environments (Apache 2.0 License) popular in deep RL:\n\n- HalfCheetah is a two-legged 2-d robot comprising of a head, horizontal torso, two thighs, two shins, and two feet. - Ant is a 3-d four-legged robot comprising of a torso and four legs (2 parts per leg). - Hopper is a single-legged 2-d robot comprising a torso, thigh, shin, and foot. - Walker(2D) is a two-legged 2-d robot comprising of a vertical torso, two thighs, two shins, and two feet. The datasets used in our experiments are from D4RL (Fu et al., 2020) (Apache 2.0 License):\n\n- Medium is a dataset generated by a pre-trained Soft Actor-Critic policy (Haarnoja et al. 2018) that was early stopped during training. - Medium-Expert is a dataset comprising of equal parts: expert and suboptimal demonstrations. - Medium-Replay is a dataset consisting of samples from the replay buffer during the training of the medium policy. ## C. 2 Event Forecasting\n\nOur code for the event forecasting experiments and datasets is based on that of Bae et al. (2023) (CC BY-NC-SA 4.0 License). For these experiments, we consider 8 datasets used in prior works (Bae et al., 2023, Zhang et al., 2020; Zuo et al., 2020). 5 out of the datasets include marks/labels:\n\n- MIMIC (MIMIC-II) is a dataset based on an electric medical record system, consisting of anonymous patients' clinical visits. - Wiki is a dataset based on Wikipedia, consisting of actions performed on the most edited pages. - Reddit is a dataset based on the social media platform, consisting of the actions of the most active users. - Mooc is a dataset based on an online course, consisting of the sequence of actions performed by various users. - StackOverflow is a dataset based on the question-answering website, consisting of user awards. 3 out of the 8 datasets do not include marks/labels:\n- Sin is a synthetic dataset generated from a sine function with a periodicity of $4 \\pi$ and a domain of $[0,32 \\pi]$. - Uber is a dataset based on Uber NYC pickup data from 2015. - Taxi is a dataset based on NYC Taxi pickup data from 2013. ## C. 3 Time Series Forecasting\n\nOur code for the time series forecasting experiments is based on the Time series Library repository (MIT License) by Wu et al. (2023). For these experiments, we consider 8 popular datasets available in the Time Series Library repository:\n\n- Weather is a dataset containing meteorological indicators recorded every 10 minutes for 2020. - Exchange is a dataset consisting of the daily exchange rates of 8 countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore) from 1990 to 2016. - Traffic is a dataset consisting of the hourly occupancy rate of 963 car lanes of San Francisco bay area freeways. - ECL is a dataset consisting of electricity consumption of 321 clients. - ETT (h1, h2, m1, and m2) are datasets consisting of data from two stations' electricity transformers whereas \" $h$ \" denotes the hourly measurement and \" m \" denotes the minute-byminute measurement. ## C. 4 Time Series Classification\n\nOur code for the time series classification experiments is based on the Time series Library repository (MIT License) by Wu et al. (2023). For these experiments, we consider 10 UEA datasets Bagnall et al. 2018) available in the Time Series Library repository:\n\n- EthanolConcentration is a dataset of raw spectra recordings of whisky bottles and their alcohol concentration. - FaceDetection is a dataset of MEG (Magnetoencephalography) recordings and their class labels (Face or Scramble). - Handwriting is a dataset of measurements taken from a smart watch while writing. - Heartbeat is a dataset of heart sound recordings derived from the 2016 PhysioNet/Computing in Cardiology Challenge Challenge. - JapaneseVowels is a dataset of recordings of Japanese-male vowel pronunciations. - PEMS-SF is a dataset describing the occupancy rate of car lanes in San Francisco Bay Area's freeways available on California's Department of Transportation PEMS (Performance Measurement System) website. - SelfRegulationSCP1 is a dataset of cortical potential recordings from a healthy subject. - SelfRegulationSCP2 is a dataset of cortical potential recordings from an artificially respirated ALS (Amyotrophic lateral sclerosis) patient. - SpokenArabicDigits is a dataset of recordings of ten spoken Arabic digits from 44 male and 44 female Arabic native speakers.",
    "rnnattn-30": "- UWaveGestureLibrary is a dataset of accelerometer measurements of eight gestures. ## D Additional Experiments\n\n## D. 1 Full Time Series Forecasting Results\n\nIn time series forecasting, the objective of the model is to predict $T$ future values of the series. Following the standard of previous works (Wu et al. 2023), we evaluated on $t \\in\\{96,192,336,720\\}$. However, due to space limitations, only $T=192$ was included in the main paper. Here, in Table 5. we include the full results. We see that for all values of $T$, Aarens performed comparably with Transformers across all datasets. However, unlike Transformers, Aarens can efficiently process new inputs, making it more advantageous in sequential settings such as this time series-related setting. ## E Hyperparameters + Implementation Details\n\nAs Transformers and Aarens share the same interface and are both attention-based methods, they share the same set of hyperparameters. For fairness, the same hyperparameters are used for both Transformers and Aarens. The specific hyperparameters for their respective settings are as follows:\nReinforcement Learning. For these experiments, we found that hyperparameters used by Zheng et al. (2022) performed better than the default Decision Transformers hyperparameters. As such, Zheng et al. (2022)'s hyperparameters were used for these experiments, i.e., an embedding dimension of 512, 4 attention heads, and 4 Transformers/Aarens blocks. Event Forecasting. For these experiments, the default hyperparameters (except for the learning rate) in Bae et al. (2023)'s repository were used. We found that the default learning rate value of 0.0001 caused early convergences to poor local optima. As such, the learning rate was set to 0.0005 instead. Time Series Forecasting. For these experiments, the default Transformers hyperparameters in the Time Series Library repository (Wu et al. 2023) were used. Time Series Classification. For these experiments, the default Transformers hyperparameters in the Time Series Library repository (Wu et al. 2023) were used. ## F Compute\n\nOur experiments were run using a mix of Nvidia GTX 1080 Ti (12 GB) and Nvidia Tesla P100 (16 GB) GPUs. The analyses were performed on Nvidia GTX 1080 Ti (12 GB). The reinforcement learning experiments required approximately the same amount of time: $\\sim 2-4$ hours each. The event forecasting experiments varied in time depending on the dataset:\n\n- MIMIC took $\\sim 0.5$ hours\n- Wiki took $\\sim 0.75$ hours\n- Reddit took $\\sim 3.5$ hours\n- Mooc took $\\sim 8$ hours\n\nTime Series Forecasting\n\n| {Metric <br> Models} |  | MSE ( $\\downarrow$ ) |  | $\\operatorname{MAE}(\\downarrow)$ |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Aaren | Transformer | Aaren | Transformer |\n| ETTh1 | 96 | $0.53 \\pm 0.04$ | $0.54 \\pm 0.01$ | $0.52 \\pm 0.02$ | $0.50 \\pm 0.01$ |\n|  | 192 | $0.59 \\pm 0.03$ | $0.64 \\pm 0.05$ | $0.55 \\pm 0.01$ | $0.57 \\pm 0.02$ |\n|  | 336 | $0.65 \\pm 0.03$ | $0.65 \\pm 0.02$ | $0.55 \\pm 0.01$ | $0.55 \\pm 0.01$ |\n|  | 720 | $0.67 \\pm 0.05$ | $0.70 \\pm 0.05$ | $0.62 \\pm 0.02$ | $0.58 \\pm 0.02$ |\n| ETTh2 | 96 | $0.38 \\pm 0.02$ | $0.41 \\pm 0.04$ | $0.44 \\pm 0.02$ | $0.40 \\pm 0.02$ |\n|  | 192 | $0.49 \\pm 0.03$ | $0.50 \\pm 0.03$ | $0.48 \\pm 0.02$ | $0.46 \\pm 0.01$ |\n|  | 336 | $0.57 \\pm 0.05$ | $0.59 \\pm 0.03$ | $0.47 \\pm 0.02$ | $0.50 \\pm 0.01$ |\n|  | 720 | $0.55 \\pm 0.03$ | $0.60 \\pm 0.03$ | $0.52 \\pm 0.01$ | $0.52 \\pm 0.01$ |\n| ETTm1 | 96 | \u5230 | $0.44 \\pm 0.01$ | $0.44 \\pm 0.01$ | $0.41 \\pm 0.01$ |\n|  | 192 | $0.51 \\pm 0.03$ | $0.52 \\pm 0.05$ | $0.47 \\pm 0.01$ | $0.47 \\pm 0.01$ |\n|  | 336 | $0.54 \\pm 0.02$ | $0.57 \\pm 0.03$ | $0.49 \\pm 0.01$ | $0.51 \\pm 0.01$ |\n|  | 720 | $0.60 \\pm 0.03$ | $0.66 \\pm 0.06$ | $0.52 \\pm 0.01$ | $0.56 \\pm 0.02$ |\n| ETTm2 | 96 | $0.24 \\pm 0.03$ | $0.25 \\pm 0.01$ | $0.30 \\pm 0.02$ | $0.30 \\pm 0.01$ |\n|  | 192 | $0.34 \\pm 0.04$ | $0.38 \\pm 0.02$ | $0.39 \\pm 0.02$ | $0.37 \\pm 0.01$ |\n|  | 336 | $0.41 \\pm 0.03$ | $0.49 \\pm 0.05$ | $0.42 \\pm 0.01$ | $0.43 \\pm 0.02$ |\n|  | 720 | $0.51 \\pm 0.03$ | $0.56 \\pm 0.02$ | $0.49 \\pm 0.02$ | $0.47 \\pm 0.01$ |\n| Weath | 96 | $0.18 \\pm 0.00$ | $0.18 \\pm 0.00$ | $0.23 \\pm 0.00$ | $0.23 \\pm 0.00$ |\n|  | 192 | $0.25 \\pm 0.01$ | $0.24 \\pm 0.01$ | $0.28 \\pm 0.00$ | $0.28 \\pm 0.00$ |\n|  | 336 | $0.31 \\pm 0.00$ | $0.31 \\pm 0.02$ | $0.32 \\pm 0.00$ | $0.34 \\pm 0.01$ |\n|  | 720 | $0.40 \\pm 0.00$ | $0.38 \\pm 0.02$ | $0.39 \\pm 0.00$ | $0.39 \\pm 0.01$ |\n| Exchange | 96 | $0.14 \\pm 0.01$ | $0.14 \\pm 0.01$ | $0.27 \\pm 0.01$ | $0.25 \\pm 0.01$ |\n|  | 192 | $0.25 \\pm 0.03$ | $0.24 \\pm 0.02$ | $0.33 \\pm 0.02$ | $0.34 \\pm 0.01$ |\n|  | 336 | $0.42 \\pm 0.04$ | $0.41 \\pm 0.02$ | $0.44 \\pm 0.02$ | $0.45 \\pm 0.01$ |\n|  | 720 | $1.20 \\pm 0.07$ | $1.44 \\pm 0.19$ | $0.79 \\pm 0.02$ | $0.81 \\pm 0.04$ |\n| Traffic | 96 | $0.63 \\pm 0.01$ | $0.61 \\pm 0.01$ | $0.35 \\pm 0.00$ | $0.34 \\pm 0.00$ |\n|  | 192 | $0.64 \\pm 0.01$ | $0.63 \\pm 0.01$ | $0.35 \\pm 0.00$ | $0.34 \\pm 0.00$ |\n|  | 336 | $0.65 \\pm 0.01$ | $0.64 \\pm 0.00$ | $0.35 \\pm 0.00$ | $0.34 \\pm 0.00$ |\n|  | 720 | $0.68 \\pm 0.01$ | $0.67 \\pm 0.00$ | $0.36 \\pm 0.01$ | $0.36 \\pm 0.00$ |\n| ECL | 96 | $0.36 \\pm 0.02$ | $0.35 \\pm 0.02$ | $0.46 \\pm 0.01$ | $0.43 \\pm 0.01$ |\n|  | 192 | $0.37 \\pm 0.02$ | $0.39 \\pm 0.03$ | $0.45 \\pm 0.01$ | $0.48 \\pm 0.02$ |\n|  | 336 | $0.47 \\pm 0.05$ | $0.48 \\pm 0.06$ | $0.52 \\pm 0.03$ | $0.55 \\pm 0.03$ |\n|  | 720 | $0.57 \\pm 0.05$ | $0.62 \\pm 0.06$ | $0.56 \\pm 0.02$ | $0.55 \\pm 0.03$ |\n\nTable 5: Full Time Series Forecasting Results. - StackOverflow took $\\sim 3.5$ hours\n- Sin took $\\sim 1.5$ hours\n- Uber took $\\sim 3$ hours\n- Taxi took $\\sim 1.5$ hours\n\nThe time series forecasting experiments were run as a single script for $T \\in\\{96,192,336,720\\}$. The experiments varied in time depending on the dataset:\n\n- Weather took $\\sim 6$ hours\n- Exchange took $\\sim 0.5$ hours\n- Traffic took $\\sim 1$ hours\n- ECL took $\\sim 4$ hours\n- ETTh1 took $\\sim 0.75$ hours\n- ETTm1 took $\\sim 11$ hours\n- ETTh2 took $\\sim 0.75$ hours\n- ETTm2 took $\\sim 11$ hours\n\nThe time series classification experiments were run as a single script.",
    "rnnattn-31": "Running the experiments on all datasets took in total $\\sim 1$ hour. ## G Limitations and Broader Impact\n\nLimitations. Unlike Transformers whose attention queries are input-dependent (i.e., $x_{i}$ ), Aarens' attention queries are input-independent (i.e., $q$ is a learned constant). In our experiments (reinforcement learning, event forecasting, time series forecasting, and time series classification), we found that Aarens performed comparably to Transformers regardless.",
    "rnnattn-32": "However, this difference could be a limitation in settings that require large highly expressive sequence models, e.g., large language models. This, however, is outside the scope of our work and our available resources. Broader Impact. In this work, we propose Aaren, a module that achieves comparable performance to Transformers while being more time and memory-efficient. Since Aarens can update themselves efficiently (as an RNN) unlike Transformers, therefore Aarens are more suitable for sequence modelling. Since (1) there exists a wide range of sequence modelling problem settings and (2) the number of low-resource domains (e.g., battery powered or IoT devices) is rapidly increasing, therefore Aaren has a broad potential impact. The general applicability of Aaren means that its societal impact is dependent on the downstream application. [^0]:    ${ }^{1}$ For sequential data, a causal mask is typically used to prevent attending to future timesteps. [^1]:    ${ }^{2} \\mathrm{~A}$ common trick for computing softmax numerically stably is to subtract the max term Goodfellow et al. 2016). The cumulative max is an extension of this trick for a recurrent computation of softmax. [^2]:    ${ }^{3}$ See Appendix B for proof of the correctness and associative nature of $\\oplus$.",
    "rnnattn-33": "[^3]:    ${ }^{4}$ The code will be released alongside the camera-ready. "
}