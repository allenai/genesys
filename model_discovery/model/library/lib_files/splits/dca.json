{
    "dca-0": "# Training-Free Long-Context Scaling of Large Language Models \n\nChenxin An ${ }^{* 12}$ Fei Huang ${ }^{2}$ Jun Zhang Shansan Gong ${ }^{1}$ Xipeng Qiu ${ }^{3}$ Chang Zhou ${ }^{2}$ Lingpeng Kong ${ }^{1}$\n\n\n#### Abstract\n\nThe ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables LLAMA2 70B to support context windows of more than 100 k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (InterChunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains $94 \\%$ of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at https: //github.com/HKUNLP/ChunkLlama. ## 1. Introduction\n\nThe ability to comprehend and process long-context information is essential for large language models (LLMs) (OpenAI, 2023; Touvron et al., 2023a;b; Bai et al., 2023; Anthropic, 2023) to cater to a wide range of applications effectively. These include analyzing and responding to inquiries within sizable PDFs, retaining extended dialogue history, and empowering interactive chatbots (Wei et al., 2023; Lee et al., 2023; Rula \\& D\u2019Souza, 2023; Saad-Falcon et al., 2023; Lv et al., 2024). [^0]Recent advances have shown that the long-context ability can be improved by further training a short-context model on long text sequences (Ruoss et al., 2023; Rozi\u00e8re et al., 2023). The impressive performance of Llama2 Long (Xiong et al., 2023), which is trained from a mix of long text data and the original Llama2 (Touvron et al., 2023b) pre-training corpus, stands as a testament to this approach. Nevertheless, due to the limited accessibility of these training corpora and the prohibitive cost of long-context finetuning, current open-source models often fall short in performance when compared to the proprietary counterparts, and are generally available in smaller sizes (e.g., 7B/13B). Given these constraints, approaches that do not require additional training for context scaling in LLMs become particularly attractive. Recent training-free methods, including LMinfinite (Han et al., 2023) and StreamingLLM (Xiao et al., 2023), have shown that LLMs trained on a limited context window can efficiently process text of infinite length (Zhang et al., 2023; 2024; Xiao et al., 2024; Qin et al., 2024). Assuming that LLMs are unable to generalize to texts longer than the training length, these models handle extended sequences by selectively retaining essential local information. Such paradigms effectively maintain a low Perplexity (PPL), yet they lose long-range dependencies. To retain the global information, another perspective is to effectively extrapolate to sequence lengths that surpass those encountered during their training (Sun et al., 2022; Kazemnejad et al., 2023; Liu et al., 2023b; Chi et al., 2023). Popular techniques for Llama-based models, including Position Interpolation (PI) (Chen et al., 2023b) and NTK-Aware RoPE (NTK) (LocalLLaMA, 2023b;a), are adaptations of Rotary Positional Encodings (RoPE) (Su et al., 2022). These scaled positional encodings necessitate fewer finetuning steps compared to the original RoPE, and their training costs can be further reduced via methods such as YaRN (Peng et al., 2023) and CLEX (Chen et al., 2023a). However, in a training-free setting, we find that these approaches usually lead to a notable increase in PPL especially in input lengths that are more than twice the training length ( $\\S 4$, Table 1). In this paper, we introduce Dual Chunk Attention (DCA), a new training-free framework to extrapolate the context window of LLMs. We avoid linearly downscaling the position indices or increasing the base frequency in RoPE ( Su et al., 2022). Instead, we opt to reuse the original position\nindices with their embeddings from the pretrained model, yet to redesign the construction of the relative position matrix in such a way that it can accurately reflect the relative position of two tokens as faithfully as possible. Inspired by efficient chunk-based attention patterns (Child et al., 2019; Song et al., 2023; Ratner et al., 2023; He et al., 2024), DCA segments self-attention computations for a long sequence into small chunks, each chunk being smaller than the size of the pretraining window. DCA consists of three components: (1) intra-chunk attention, tailored for processing tokens within the same chunk; (2) inter-chunk attention, for processing tokens between distinct chunks; and (3) successive chunk attention, for processing tokens in successive, distinct chunks. These respective treatments help the model effectively capture both long-range and short-range dependencies in a sequence. In addition to that, the chunk-based attention calculation can be seamlessly integrated with Flash Attention 2 (Dao et al., 2022; Dao, 2023), a key element for long-context scaling in the open-source community. ${ }^{1}$\n\nWe present a comprehensive evaluation of our models on a diverse range of tasks that include language modeling, passkey retrieval, and real-world long-context applications that span question answering (Pang et al., 2022; Ko\u010disk\u00fd et al., 2018; Dasigi et al., 2021; An et al., 2023) and summarization (Zhong et al., 2021). Unlike previous work that is usually limited to verification on $7 \\mathrm{~B} / 13 \\mathrm{~B}$ models, the significant training efficiency of our method makes it possible to validate on 70B models, ensuring robust conclusions. To verify the model's long-context ability independent of potential data exposure during pretraining, we used this paper itself as the input and crafted a series of questions for the models. ${ }^{2}$ Our empirical results reveal the following insights:\n\n1. Extrapolation. On language modeling, DCA marks a significant advance for training-free approaches. It first shows that LLMs with a 4 k context window can be expanded to more than 32 k without training, maintaining a negligible increase in PPL, whereas previous methods typically falter at context lengths beyond 8 k . Furthermore, we demonstrate that Llama2 70B, when integrated with DCA, showcases an exceptional extrapolation capability to handle context sizes exceeding 100k tokens. 2. Orthogonality. DCA is orthogonal to existing popular scaled positional encodings such as PI (Chen et al., 2023b) and NTK (LocalLLaMA, 2023b;a). We empirically show that existing long-context LLMs, which have already supported a 32 k context window, can further extrapolate to a 192 k context length while maintaining high passkey retrieval accuracy and low perplexity.",
    "dca-1": "[^1]3. Long-Context Understanding. We evaluate DCA on a suite of long-context understanding benchmarks in both zero-shot and few-shot settings. The results suggest that our training-free models achieve performance comparable to, or even surpassing, that of existing state-of-the-art models built through costly continual training.",
    "dca-2": "## 2. Background\n\n### 2.1. Positional Encoding\n\nThe original positional embedding from the Transformer model (Vaswani et al., 2017) maps absolute position indices to a $d$-dimensional feature space, and incorporates this into the input layer. The input $\\mathbf{x}$, associated with the position index $i$, is expressed as: $\\mathbf{x}_{i}=\\mathbf{x}+f(i)$, where $f: \\mathbb{N} \\rightarrow \\mathbb{R}^{d}$ is the (positional) embedding function. One of the most prevalent positional encoding methods for LLMs is the Rotary Positional Encoding (RoPE) (Su et al., 2022). RoPE eschews the conventional approach of infusing positional information into the input layer. Instead, it directly incorporates this information into the attention layer. For a sequence of $l$ tokens, we denote the position indices for keys and queries ${ }^{3}$ as follows:\n\n$$\nP_{\\mathbf{k}}=P_{\\mathbf{q}}=[0,1, \\ldots, l-1]\n$$\n\nWe abuse the notation $f$ for the embedding function of RoPE, which accepts a query vector $\\mathbf{q}$ or a key vector $\\mathbf{k}$, and the respective position index as arguments. For example, we have $\\mathbf{q}_{i}=f\\left(\\mathbf{q}, P_{\\mathbf{q}}[i]\\right)$ and $\\mathbf{k}_{j}=f\\left(\\mathbf{k}, P_{\\mathbf{k}}[j]\\right)$, where $[i]$ denotes the $i$-th element of the list. In the most straightforward case, we have $P[i]=i$. The function $f^{4}$ outputs a modified query or key vector that encapsulates the position index, ensuring that the inner product between the $i$-th query and the $j$-th key (for $i \\geq j$ ) captures the relative positional information $P_{\\mathbf{q}}[i]-P_{\\mathbf{k}}[j]$. Although RoPE takes absolute position indices as input, the result of the inner product of $\\mathbf{q}, \\mathbf{k}$ only contains relative position information (i.e., we have $\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{j}=\\mathbf{q}_{m}^{\\top} \\mathbf{k}_{n}$ whenever $m-n=i-j$ ). The relative position matrix $M$ introduced by RoPE during self-attention can be described as a Toeplitz matrix, as shown in Figure 1.",
    "dca-3": "Each element $M[i][j]=P_{\\mathbf{q}}[i]-P_{\\mathbf{k}}[j]$ signifies the relative position between $\\mathbf{q}_{i}$ (the $i$-th query) and $\\mathbf{k}_{j}$ (the $j$-th key). ### 2.2. Extrapolation of RoPE\n\nRecent work (Chen et al., 2023b; Chowdhury \\& Caragea, 2023; Chen et al., 2023a) has demonstrated that LLMs with the original RoPE lack robust length extrapolation capabilities, typically resulting in performance degradation when\n\n[^2]![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-03.jpg?height=478&width=483&top_left_y=216&top_left_x=344)\n\nFigure 1. Visualization of the Relative Position Matrix $M$ utilizing standard RoPE. The pretraining context window is 6 and the input sequence length is 12 . The x -axis $P_{\\mathrm{k}}$ indicates the position indices of keys, while the y-axis $P_{\\mathbf{q}}$ corresponds to the position indices of queries. Each matrix entry $M[i][j]$ represents the relative positional offset $P_{\\mathbf{q}}[i]-P_{\\mathbf{k}}[j]$. tested on input sequences longer than those seen during pretraining (Li et al., 2023b; Zhu et al., 2023). Recent studies (Chen et al., 2023b; Su, 2023; Jin et al., 2024) mainly attribute this limitation to the presence of unseen relative positions in pretraining phase and propose to redesign the relative position matrix. As illustrated in the example in Figure 1, the model is trained on sequences of 6 tokens, while inference is carried out on a sequence of 12 tokens. This discrepancy can lead to a high PPL because relative positions beyond 6 were never trained. Previous approaches, such as PI and NTK, aim to mitigate this issue by reducing the magnitude of $M[i][j]$ to ensure it falls within the scope of the observed context length during training. For instance, applying PI in this example would adjust the position indices by scaling: $P_{\\mathbf{q}}[i] \\Rightarrow P_{\\mathbf{q}}[i] / 2$ and $P_{\\mathbf{k}}[j] \\Rightarrow P_{\\mathbf{k}}[j] / 2$. Consequently, the relative position matrix is also scaled: $M[i][j]=M[i][j] / 2$. Here, a scaling factor $2=\\frac{12}{6}$ is employed to scale down the relative positions, leading to inferior resolution of the position information and weak extrapolation ability. ## 3. Method\n\nIn this section, we describe our new training-free framework Dual Chunk Attention in detail. A running example of dual chunk attention is shown in figure 2. Our method starts from the intra-chunk attention (Figure 2 (a)) which is a chunkbased efficient attention pattern (Child et al., 2019; Song et al., 2023). The position embedding of each chunk ranges from 0 to chunk size where the chunk size is set to be smaller than pretraining length. The intra-chunk attention pattern practically means directly truncating the input from left to the chunk size discarding information from previous chunks. Such truncation usually brings low perplexity (Xiao et al., 2023) but loses long-range information. To address this limitation, we implement inter-chunk attention (Figure 2 (b)) that enables attention calculations between different chunks, albeit with less precision for distant token positions. Finally, we introduce successive-chunk attention, a variant of inter-chunk attention depicted in Figure 2 (c), which is specifically applied when two chunks are adjacent in order to preserve locality. An ablation study to show how these attention mechanisms influence PPL and passkey retrieval accuracy can be found in Figure 4. ### 3.1. Intra-Chunk Attention\n\nIntra-Chunk Attention is employed to calculate the inner product of queries and keys within the same chunk. For a long sequence of length $l$, we partition the sequence into $n=\\frac{l}{s}$ chunks, ensuring that the position indices within each chunk will not exceed the chunk size $s$. Figure 2 (a) illustrates the process of segmenting a sequence of 12 tokens exceeding the pretraining length 10 into 2 chunks, with each chunk comprising $s=6<10$ tokens. Then the position indices for keys and queries are scaled within the chunk size 6. Concretely, we have position indices for keys $P_{\\mathbf{k}}=[\\underbrace{0,1,2,3,4,5}_{\\text {chunk } 0}, \\underbrace{0,1,2,3,4,5}_{\\text {chunk } 1}]$ and $P_{\\mathbf{q}}^{\\text {Intra }}=P_{\\mathbf{k}}$, where $P_{\\mathbf{q}}^{\\text {Intra }}$ means position indices for queries during intra-chunk attention. To formalize, in intra-chunk attention, we adjust the position indices for queries and keys as follows:\n\n$$\nP_{\\mathbf{q}}^{\\mathrm{Intra}}=P_{\\mathbf{k}}=[0,1, \\ldots, l-1] \\quad \\bmod s\n$$\n\nFor the absolute indices $i$ and $j$ within the same chunk i.e., $\\lfloor i / s\\rfloor=\\lfloor j / s\\rfloor$, satisfying $0 \\leq j \\leq i<l$, the element $M[i][j]$ is defined as the difference between the positional encodings of the query and the key:\n\n$$\nM[i][j]=P_{\\mathbf{q}}^{\\text {Intra }}[i]-P_{\\mathbf{k}}[j]\n$$\n\nWhen $\\lfloor i / s\\rfloor=\\lfloor j / s\\rfloor$, we calculate $M[i][j]$ follows Eq. 3 . The computed $M$ of the previous example where we have a sequence length of 12 and a chunk size of 6 , is illustrated in Figure 2 (a). The intra-chunk attention score for the interaction between the $i$-th query and the $j$-th key is then calculated as:\n\n$$\n\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{j}=f\\left(\\mathbf{q}, P_{\\mathbf{q}}^{\\text {Intra }}[i]\\right)^{\\top} f\\left(\\mathbf{k}, P_{\\mathbf{k}}[j]\\right)\n$$\n\n### 3.2. Inter-Chunk Attention\n\nTo aggregate information from other chunks, we introduce Inter-Chunk Attention. In Llama-based LLMs, the position indices for queries are greater than those of the keys to reflect the left-to-right information flow, i.e, we have $P_{\\mathbf{q}}[i] \\geq P_{\\mathbf{k}}[j]$ whenever $i \\geq j$. Using $P_{\\mathbf{q}}=P_{\\mathbf{q}}^{\\text {Intra }}$ and $P_{\\mathbf{k}}$ for attention calculation between different chunks clearly violates this property. For example, considering $\\mathbf{q}_{s}$ and $\\mathbf{k}_{1}$ where $s$ is the chunk size, their relative distance given by $P_{\\mathbf{q}}^{\\text {Intra }}[s]=0$ and $P_{\\mathbf{k}}[1]=1$ is -1 . We maintain the position\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-04.jpg?height=477&width=1369&top_left_y=224&top_left_x=343)\n\nFigure 2. Visualization of the Relative Position Matrix $M$ employing Dual Chunk Attention (DCA), with chunk size $s=6$, pretraining window size $c=10$, and local window size $w=4$ noted by the shadow in (c). The sequence is segmented into chunks to ensure that relative positions do not exceed 9 . The matrix element $M[i][j]$ represents the relative position between the $i$-th query vector $\\mathbf{q}$ and the $j$-th key vector $\\mathbf{k}$. Unlike the original position indices for $\\mathbf{q}, \\mathbf{k}$ in RoPE, DCA utilizes distinct position index sets $P_{\\mathbf{k}}, P_{\\mathbf{q}}^{\\text {Intra }}$ (defined in Eq.",
    "dca-4": "2), $P_{\\mathbf{q}}^{\\text {Inter }}$ (defined in Eq. 5), $P_{\\mathbf{q}}^{\\text {Succ }}$ (defined in Eq. 7) to compute the relative distances within different sections of $M$. indices for keys $P_{\\mathbf{k}}$ considering KV cache and seek for a new set of $P_{\\mathbf{q}}$ during inter-chunk attention, noted as $P_{\\mathbf{q}}^{\\text {inter }}$. Given Eq. 2, the position indices for keys are cyclically repeated with the maximum position index $\\max \\left(P_{\\mathbf{k}}\\right)=s-1$. To ensure that the queries have larger position indices than all keys from previous chunks, A simple strategy to distinguish distant queries is to assign them a considerably large position index, such as the maximum position index during pretraining $c-1>\\max \\left(P_{\\mathbf{k}}\\right)$, where $c$ is the pretraining context length:\n\n$$\nP_{\\mathbf{q}}^{\\text {Inter }}=[\\underbrace{c-1, c-1, \\ldots c-1}_{l \\text { elements }}]\n$$\n\nWhen $\\lfloor i / s\\rfloor \\neq\\lfloor j / s\\rfloor$, we can give the relative position matrix $M$ with $\\mathbf{q}_{i}$ and $\\mathbf{k}_{j}$ from distinct chunks as:\n\n$$\nM[i][j]=P_{\\mathbf{q}}^{\\mathrm{Intra}}[i]-P_{\\mathbf{k}}[j]=c-1-P_{\\mathbf{k}}[j] \\geq c-s\n$$\n\nAs reflected in Figure 2 (b), we assign $P_{\\mathbf{q}}^{\\text {Inter }}$ with a constant value of $c-1=9$ for all positions, which is larger than the maximum position index $s-1=5$ in $P_{\\mathbf{k}}$.",
    "dca-5": "We complete the rest part of the matrix $M$ left blank by intra-chunk attention with Eq.",
    "dca-6": "6 . ### 3.3. Successive-Chunk Attention\n\nSuccessive-Chunk Attention can be viewed as a special case for inter-chunk attention, proposed to maintain the locality of LLMs where locality means LLMs tend to heavily rely on the neighboring tokens to predict the next token (Xiao et al., 2023; Han et al., 2023). Simply using inter-chunk attention may no longer keep the precise relative position between neighboring tokens, leading to performance degradation. As shown in Figure 2(b), where the chunk size is $s=6$ and the pretraining length is $c=10$, the last key of the first chunk, $\\mathbf{k}_{5}$, with $P_{\\mathbf{k}}[5]=5$, is followed by the first query of the second chunk, $\\mathbf{q}_{6}$, with the position index $P_{\\mathbf{q}}^{\\text {Inter }}[6]=9$. Despite their absolute distance being 1 , the relative distance between $\\mathbf{q}_{6}$ and $\\mathbf{k}_{5}$ is $P_{\\mathbf{q}}^{\\text {Inter }}[6]-P_{\\mathbf{k}}[5]=4$. This configuration challenges the model's ability to maintain locality in its attention mechanism. Fortunately, this issue only occurs between successive chunks, so we introduce a new successive-chunk attention to deal with this case. Concretely, we propose to maintain the locality of $w$ neighboring tokens via adjusting the first $w$ position indices in for $P_{\\mathbf{q}}^{\\text {Inter }}$. For example, in Figure 2 (c), given pretraining context $c=10$, chunk size $s=6$, and $P_{\\mathbf{q}}^{\\text {Inter }}=[\\underbrace{9,9,9,9,9,9}_{\\text {chunk 0 }}, \\underbrace{9,9,9,9,9,9}_{\\text {chunk 1 }}]$, the position indices $P_{\\mathbf{q}}^{\\text {Succ }}$ can be set to $[\\underbrace{6,7,8,9,9,9}_{\\text {chunk } 0}, \\underbrace{6,7,8,9,9,9}_{\\text {chunk } 1}]$ for attention calculation between successive chunks, if we keep a local window of $w=4$. Formally, given chunk size $s$, pretraining size $c$ and local window $w$ we have:\n\n$$\nP_{\\mathbf{q}}^{\\text {Succ }}=[\\underbrace{\\overbrace{s, s+1, \\ldots, s+w-1}^{w \\text { elements }}, c-1, \\ldots, c-1}_{\\text {the same for all chunks }}]\n$$\n\nwhere $w$ means the local window size and can be directly set to the difference between pretraining length and chunk size $c-s$. For $i, j$ from successive chunks, the calculation results of $M[i][j]$ using $P_{\\mathbf{q}}^{\\text {Succ }}$ and $P_{\\mathbf{k}}$ are reflected in Figure 2 (c) where the shadow means the resulting local window. Eq 7 ensures that the neighboring $w$ keys have the closest distance to the current query. By combining intra-chunk, inter-chunk, and successive-\nchunk attention, we finally calculate $M[i][j]$ as:\n\n$$\nM[i][j]= \\begin{cases}P_{\\mathbf{q}}^{\\text {Intra }}[i]-P_{\\mathbf{k}}[j] & \\text { if }\\lfloor i / s\\rfloor-\\lfloor j / s\\rfloor=0 \\\\ P_{\\mathbf{q}}^{\\text {Succ }}[i]-P_{\\mathbf{k}}[j] & \\text { if }\\lfloor i / s\\rfloor-\\lfloor j / s\\rfloor=1 \\\\ P_{\\mathbf{q}}^{\\text {Inter }}[i]-P_{\\mathbf{k}}[j] & \\text { if }\\lfloor i / s\\rfloor-\\lfloor j / s\\rfloor>1\\end{cases}\n$$\n\nThe inner product of $\\mathbf{q}, \\mathbf{k}$ in DCA is consequently defined as:\n\n$$\n\\mathbf{q}_{i}^{T} \\mathbf{k}_{j}= \\begin{cases}f\\left(\\mathbf{q}, P_{\\mathbf{q}}^{\\text {Intra }}[i]\\right)^{T} f\\left(\\mathbf{k}, P_{\\mathbf{k}}[j]\\right), & \\text { if }\\lfloor i / s\\rfloor-\\lfloor j / s\\rfloor=0 \\\\ f\\left(\\mathbf{q}, P_{\\mathbf{q}}^{\\text {Succ }}[i]\\right)^{T} f\\left(\\mathbf{k}, P_{\\mathbf{k}}[j]\\right), & \\text { if }\\lfloor i / s\\rfloor-\\lfloor j / s\\rfloor=1 \\\\ f\\left(\\mathbf{q}, P_{\\mathbf{q}}^{\\text {Inter }}[i]\\right)^{T} f\\left(\\mathbf{k}, P_{\\mathbf{k}}[j]\\right), & \\text { if }\\lfloor i / s\\rfloor-\\lfloor j / s\\rfloor>1\\end{cases}\n$$\n\n### 3.4. Normalization\n\nSoftmax layer The inner product calculations within the DCA are formalized as shown in Equation 8. Subsequently, a softmax function is applied to normalize the computed inner products:\n\n$$\n\\mathbf{p}_{i}=\\operatorname{softmax}\\left(\\left[\\frac{\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{0}}{\\sqrt{d}}, \\frac{\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{1}}{\\sqrt{d}}, \\ldots, \\frac{\\mathbf{q} i^{\\top} \\mathbf{k}_{i}}{\\sqrt{d}}\\right]\\right)\n$$\n\nwhere $d$ denotes the dimension of hidden states. Flash Attention The PyTorch-style pseudocode for how integrating DCA with Flash Attention 2 (Dao, 2023), can be found in Algorithm 1. The explanation and complexity analysis of the code can be found in Appendix \u00a7A.3. With Flash Attention, DCA attains comparable GPU memory usage and inference speed to the original self-attention in Llama. Results can be found in Figure 3. ## 4. Experiments\n\nWe evaluate our framework, DCA, on various variants of Llama2 (Touvron et al., 2023b), specifically the 7B, 13B, and 70B models, along with their chat counterparts, which have a 4 k pretraining context. Our Llama2-based model is denoted as ChunkLlamar2. Additionally, we apply DCA to two popular open-source long context models: (1) Together-32k (Together, 2023) ${ }^{5}$ : This model uses Positional Interpolation (PI) as its positional encoding. The DCAenhanced version of this model is referred to as ChunkTogether. (2) CodeLlama (Rozi\u00e8re et al., 2023) ${ }^{6}$ : This model applies NTK-Aware RoPE. Following the application of DCA, the resulting model is termed ChunkCodeLlama. ### 4.1. Experimental Setup\n\nDCA can be implemented by a monkey patch to replace the inference code of the original LlamaAttention. Thanks\n\n[^3]to Flash Attention 2 (Dao, 2023), for the 7B/13B variants of ChunkLlama2, we only need one single NVIDIA A10080G GPU for the inference. When scaling up to 70B models, two A100 GPUs are enough to manage inference within a 16 k context length. The chunk size $s$ can be typically set to $\\frac{3}{4}$ training length and for Llama2, this value is 3072 . The number of chunks depends on the input sequence length. In addition to training-free evaluations, we also provide finetuned models from 7B/13B Llama2 checkpoints. This finetuning process leverages only long conversations with 16k input tokens, following Vicuna (LMSYS, 2023) and LongChat (Li et al., 2023a). The training dataset is sourced from ShareGPT ${ }^{7}$ and AlpacaGPT4 (Taori et al., 2023). For the data derived from ShareGPT, we specifically curate a subset by extracting responses generated by GPT-4, and dialogues that exceed 4 k tokens in length. This selection results in a compilation of 5,405 training instances. We adhere to the training hyperparameters as specified in the LongChat repository ${ }^{8}$. We further finetune Llama2 with over 16 k steps with a batch size of 1 . The finetuning process amounts to approximately 40 GPU hours for the 7B model and 60 GPU hours for the 13B variant. Datasets We evaluate the long sequence language modeling performance of our CHUNKLLAMA2 on the book corpus dataset PG19 (Rae et al., 2020), with context lengths ranging from 4 k to 192k tokens. For the 7 B and 13B models, we employ a sliding window of 256 , in line with previous work (Peng et al., 2023; Chen et al., 2023c). For 70B models, we adjust the sliding window size to 2048 and when dealing with contexts that exceed 96 k tokens, we adjust the sliding window to be half of the input length considering the running time. For few-shot experiments, we follow the settings in Llama2 Long (Xiong et al., 2023). Concretely, we evaluate 0 -shot performance of CHunKLLAMA 2 on NarrativeQA (Ko\u010disk\u00fd et al., 2018), 1-shot on QMSum (Zhong et al., 2021), 2-shot on QuALITY (Pang et al., 2022) , and 2-shot for Qasper (Dasigi et al., 2021). For zero-shot experiments, we test ChunkLlama2 on 4 closed-ended tasks from L-Eval (An et al., 2023): TOFEL, QuALITY (cleaned from Pang et al.",
    "dca-7": "(2022)), Coursera, SFiction. We also validate our model on passkey retrieval used in Mohtashami \\& Jaggi (2023). Evaluations on passkey retrieval (Mohtashami \\& Jaggi, 2023) can be found in Appendix A.1. Baselines We compare with popular open-source longcontext models available in Huggingface Transformers ${ }^{9}$. Base Models: Focused Transformer 3B (Tworkowski et al., 2023), CLEX 7B (Chen et al., 2023a), YaRN 7B/13B (Peng et al., 2023), MPT 30B (MosaicML, 2023b;a), Together 7B (Together, 2023), CodeLlama 7B (Rozi\u00e8re et al., 2023),\n\n[^4]Longlora 13B/70B (Chen et al., 2023c), and Llama2 Long 7B/13B/70B (Xiong et al., 2023). Chat Models: LongChatv1.5-32k 7B (Li et al., 2023a), Vicuna-v1.5-16k (LMSYS, 2023) 7B/13B, Longlora-Chat 70B (Chen et al., 2023c), and Llama2 Long-Chat 70B (Xiong et al., 2023). ### 4.2. Long-Sequence Language Modeling\n\nTable 1 presents the Perplexity (PPL) scores on the PG19 validation set for various training-free and finetuned models. All these baselines are Llama-based. We demonstrate that the previously best training-free method fails with a context length of 16 k . However, ChunkLlama2 can extrapolate to a context window of more than 32 k , with only an increase of 0.02 in PPL. We further demonstrate that ChunkLlama2 surpasses the results of finetuned models within a 16k context length. Notably, the 70B variant of CHunKLlamA2 exhibits consistency in performance across a range of context lengths, achieving a PPL score that only marginally rises from 5.18 to 5.59 . We also reveal that DCA can be integrated with models that have been further trained on longer contexts with PI (Chen et al., 2023b) or NTK-Aware RoPE (LocalLLaMA, 2023b;a) and support a context length of 192 k in Table 2. The encouraging outcomes observed with 64 k input tokens motivate us to test ChunkLlama2 on even longer contexts. We progressively tested the model with input token lengths extending from 32k to 192k (Table 2). For Llama2 70B, DCA has proven effective in extending the context window to 96 k tokens. This extension is achieved with only a minor increase of 0.56 PPL compared to its original performance at a 4 k context length. Alongside evaluating ChunkLlama2, we also applied DCA to existing long-context models that utilize different positional encodings. Integrating DCA with existing long-context models requires only an adjustment of the chunk size within the DCA framework. We show that CodeLlama and Together's Llama2 fork can be efficiently scaled to a 192 k context length using DCA with a chunk size of 24 k . We further validated the performance of our model on the passkey retrieval task (Mohtashami \\& Jaggi, 2023). The results also indicate that by integrating DCA with existing long-context models, the enhanced system maintains a $90 \\%$ retrieval accuracy with an extended context length of up to 192 k tokens (Figure 7). ### 4.3. Practical Tasks\n\nIn contrast to previous studies that typically validate their methods based on PPL, we also apply our framework to both base models and instruction-finetuned chat models on real-world benchmarks. Few-shot Results We validate DCA on models that have not undergone instruction tuning in a few-shot learning set-\nTable 1. Perplexity (PPL) evaluation on PG19 (Rae et al., 2020) validation set. The results highlighted in red indicate the Perplexity has increased by more than 1.0 compared to its original value at the pretraining context length of 4096 . $\\operatorname{ReRoPE}(\\mathrm{Su}, 2023)$ encounters OOM (Out of Memory) problems with 16k input tokens as it is currently not compatible with Flash Attention. The scaling factors in PI and NTK are dynamically changed. | Model | Evaluation Context Window |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n|  | 4096 | 8192 | 16384 | 32768 | 65536 |\n| Finetuned |  |  |  |  |  |\n| Longlora-32k 7B | $\\mathbf{8 . 1 4}$ | $\\mathbf{7 . 8 5}$ | $\\mathbf{7 . 7 0}$ | 7.80 | 91.79 |\n| Together-32k 7B | 8.21 | 7.95 | 7.76 | $\\mathbf{7 . 6 4}$ | $>10^{2}$ |\n| CodeLlama-16k 7B | 8.93 | 8.64 | 8.44 | 8.36 | $\\mathbf{8 . 6 5}$ |\n| CLEX-16k 7B | 8.84 | 7.66 | 7.43 | 7.57 | 8.73 |\n| Training-free |  |  |  |  |  |\n| Llama2 7B | $\\mathbf{7 . 8 7}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| Llama2-ReRoPE 7B | 7.94 | 7.75 | OOM | OOM | OOM |\n| Llama2-PI 7B | 7.87 | 9.19 | 15.11 | $>10^{2}$ | $>10^{2}$ |\n| Llama2-PI-Yarn 7B | 7.87 | 8.80 | 11.75 | 42.42 | $>10^{2}$ |\n| Llama2-NTK 7B | 7.87 | 11.98 | 26.12 | 58.91 | $>10^{2}$ |\n| Llama2-NTK-Yarn 7B | 7.87 | 8.06 | 9.82 | 11.74 | 41.57 |\n| CHUNKLLAMA2 7B | 7.87 | 7.67 | 7.64 | 7.89 | 15.87 |\n| CHUNKLLAMA2 13B | $\\mathbf{7 . 1 5}$ | 6.95 | 6.99 | 7.90 | 15.14 |\n| ChunKLLAMA2 70B | $\\mathbf{5 .",
    "dca-8": "2 4}$ | $\\mathbf{5 . 1 8}$ | $\\mathbf{5 . 2 1}$ | $\\mathbf{5 . 3 0}$ | $\\mathbf{5 . 5 9}$ |\n| Llama3 |  |  |  |  |  |\n| Llama3 8B | 9.04 | 8.71 | 78.88 | $>10^{2}$ | $>10^{2}$ |\n| Llama3 70B | 5.36 | 5.16 | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| CHUnKLLAMA3 8B | 9.04 | 8.71 | 8.61 | 8.62 | 8.95 |\n| ChunKLLAMA3 70B | $\\mathbf{5 .",
    "dca-9": "3 6}$ | $\\mathbf{5 . 1 6}$ | $\\mathbf{5 . 1 4}$ | $\\mathbf{5 . 1 4}$ | $\\mathbf{5 . 2 1}$ |\n\nting. The results are summarized in Table 3. Experimental settings are the same as those in Xiong et al. (2023). If the input prompts exceed an input length of 16 k tokens, they are truncated from the left side. Most test cases within NarrativeQA (Ko\u010disk\u00fd et al., 2018) and QMSum (Zhong et al., 2021) have input lengths exceeding 16k tokens, while the lengths of test cases in Qasper (Dasigi et al., 2021) and QuALITY (Pang et al., 2022) are generally under 8 k tokens. Without any training cost, both the 7B/13B variants of CHUNKLLAMA2 achieve results comparable to popular finetuned baselines such as YaRN (Peng et al., 2023), MPT (MosaicML, 2023b), Together (Together, 2023), which are based on previous scaled RoPE (Chen et al., 2023b; LocalLLaMA, 2023b) or Alibi (Press et al., 2022). Unlike previous studies that usually verify their techniques on smaller versions of Llama2, we also present results for DCA paired with Llama2 70B, where DCA improves performance by an average of more than 8.0 points over the original Llama2 model with a 4 k training length. Given the increasing cost of long-context finetuning for 70B models, we did not find many open-source 70B baselines. We compare our training-free method against the robust 70B baseline, Longlora (Chen et al., 2023c), which employs LoRA-based (Hu et al., 2021) efficient tuning based on the Redpajama dataset (Computer, 2023) for 1000 steps supporting a 32 k context window. The results demonstrate that our\n\nTable 2. Perplexity evaluation on PG19 (Rae et al., 2020) validation set with context lengths of up to 192k tokens. We test DCA on Llama2 70B together with 2 popular further pretrained models using PI and NTK. The results highlighted in red indicate the PPL has increased by more than 1.0 compared to its original value at the pretraining context length of 4096. | Model | Position | Training | Evaluation Context Window |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Emb | context | 4 k | 32 k | 64 k | 96 k | 128 k | 160 k | 192 k |\n| Llama2 7B | RoPE | 4 k | 7.87 | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| CHunKLLAMA2 7B | RoPE | 4 k | 7.87 | 7.89 | 15.87 | 43.57 | 96.21 | $>10^{2}$ | $>10^{2}$ |\n| Llama2 70B | RoPE | 4 k | $\\mathbf{5 . 2 4}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| CHUNKLLAMA2 70B | RoPE | 4 k | $\\mathbf{5 .",
    "dca-10": "2 4}$ | $\\mathbf{5 . 3 0}$ | $\\mathbf{5 . 5 9}$ | $\\mathbf{5 . 8 0}$ | $\\mathbf{6 . 1 2}$ | 6.52 | 7.05 |\n| Llama3 8B | RoPE | 8 k | 9.04 | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| CHUNKLLAMA3 8B | RoPE | 8 k | 9.04 | 8.61 | 8.62 | 8.95 | 9.43 | 10.04 | 10.66 |\n| Llama3 70B | RoPE | 8 k | $\\mathbf{5 . 3 6}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| ChUNKLLAMA3 70B | RoPE | 8 k | $\\mathbf{5 .",
    "dca-11": "3 6}$ | $\\mathbf{5 . 1 4}$ | $\\mathbf{5 . 1 4}$ | $\\mathbf{5 . 2 1}$ | $\\mathbf{5 .",
    "dca-12": "3 2}$ | $\\mathbf{5 . 4 0}$ | $\\mathbf{5 . 4 5}$ |\n| CodeLlama 7B | NTK | 16 k | 8.93 | 8.36 | 8.65 | 9.14 | 9.87 | 15.68 | 24.78 |\n| ChunkCodeLlama 7B | NTK | 16 k | 8.93 | 8.36 | 8.13 | 8.33 | 8.66 | 9.30 | 9.83 |\n| Together 7B | PI | 32 k | 8.21 | $\\mathbf{7 . 6 4}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ | $>10^{2}$ |\n| ChunkTogether 7B | PI | 32 k | 8.21 | $\\mathbf{7 .",
    "dca-13": "6 4}$ | $\\mathbf{7 . 5 9}$ | $\\mathbf{7 . 6 4}$ | $\\mathbf{7 . 6 7}$ | $\\mathbf{7 .",
    "dca-14": "7 4}$ | $\\mathbf{7 . 8 3}$ |\n\nTable 3. Comparison between popular open-source base models (first block) and proprietary models (last block) across four research benchmarks on their validation set. We underline the best results in each block. Results exceeding the previous best open-source finetuned model are in bold. Llama2 Long has been trained with a total of 400B tokens over 100,000 steps. The maximum allowed prompt length is set to 16,384 tokens. ${ }^{\\dagger}$ : results are taken from Xiong et al. (2023) We use the simplest prompt: long-document Question:... Answer:. In-context examples are randomly selected from the training set, and we also have a discussion on the selection of in-context examples in Appendix \u00a7 A.4. | Model | Further <br> training | Training <br> context | NarrativeQA <br> F1 (0-shot) | Qasper <br> F1 (2-shot) | QuALITY <br> EM (2-shot) | QMSum <br> R-g (1-shot) | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| FoT $3 B^{\\dagger}$ | $\\checkmark$ | 8 k | 16.3 | 15.4 | 20.5 | 10.6 | 15.7 |\n| Yarn 7B ${ }^{\\dagger}$ | $\\checkmark$ | 128k | 20.9 | 26.2 | 32.3 | 11.4 | 22.7 |\n| Together 7B ${ }^{\\dagger}$ | $\\checkmark$ | 32k | 23.3 | 27.3 | 41.2 | 12.6 | 26.1 |\n| Yarn 13B ${ }^{\\dagger}$ | $\\checkmark$ | 128 k | 23.4 | 27.1 | 46.4 | 11.9 | 27.2 |\n| Longlora 13B | $\\checkmark$ | 32k | 25.8 | 26.4 | 48.9 | 15.1 | 29.1 |\n| MPT 30B ${ }^{\\dagger}$ | $\\checkmark$ | 8 k | 22.9 | 29.0 | 41.5 | 10.3 | 25.9 |\n| Llama2-DynNTK 70B | $x$ | 4 k | 11.1 | 27.8 | 60.9 | 7.8 | 26.9 |\n| Llama2 70B ${ }^{\\dagger}$ | $x$ | 4 k | 25.7 | 27.5 | 53.0 | 11.9 | 29.5 |\n| Longlora 70B | $\\checkmark$ | 32 k | 34.2 | 29.0 | 69.9 | 15.6 | 37.2 |\n| ChunkLlama2 7B | $x$ | 4 k | 20.0 | 28.2 | 35.6 | 14.7 | 24.6 |\n| ChunkLlama2 13B | $x$ | 4 k | 26.3 | 29.3 | 47.9 | 15.2 | 29.7 |\n| ChunkLlama2 70 B | $x$ | 4 k | 32.5 | 29.6 | 73.2 | 16.0 | 37.8 |\n| ChUnKLlama3 8B | $x$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-07.jpg?height=43&width=38&top_left_y=1586&top_left_x=824) | 27.4 | 30.5 | 52.6 | 15.4 | 31.5 |\n| ChunkLlama3 70B | $x$ | 8 k | 33.7 | 33.1 | 75.4 | 16.0 | 39.5 |\n| proprietary models |  |  |  |  |  |  |  |\n| Llama2 Long $7 \\mathrm{~B}^{\\dagger}$ | $\\checkmark$ | 32 k | 21.9 | 27.8 | 43.2 | 14.9 | 27.0 |\n| Llama2 Long $13 \\mathrm{~B}^{\\dagger}$ | $\\checkmark$ | 32 k | 25.6 | 31.2 | 57.6 | 15.7 | 32.5 |\n| Llama2 Long 70B ${ }^{\\dagger}$ | $\\checkmark$ | 16 k | 30.9 | 35.7 | 79.7 | 16.5 | 40.7 |\n\n70B DCA model achieves comparable performance (37.8 vs. 37.2 ) requires no training steps. Compared to the strong proprietary baseline, Llama2 Long (Xiong et al., 2023), which has been trained with a total of 400 billion tokens (Llama2 pretraining corpus and new long text data) over 100,000 steps, the performance gaps for all sizes of models are generally within a 3-point range. The in-context examples used in this experiment are randomly selected from the training set. We have also tried other ways to select the examples, and the details are included in Appendix A.4. Zero-shot Results In addition to verifying DCA on base models, we also apply DCA on the chat version of Llama2 (with instruction tuning) in a zero-shot learning scenario. Specifically, we test our models on four closed-ended tasks from L-Eval (An et al., 2023) with diverse input lengths ranging from 3 k to 27 k . All these datasets adopt Exact Match (EM) as the evaluation metric. Overall, the conclusions are similar to the few-shot evaluation. Our trainingfree 7B/13B models show comparable performance with open-source models with further training. Notably, in zeroshot experiments, we demonstrate a significant improvement over the Chat version of Longlora 70B (Chen et al., 2023c). Furthermore, when compared with proprietary models such as GPT-3.5 with a 16k token context and the chat\n\nTable 4. Comparison with open-source chat models (first block) and proprietary models (last block) on 4 closed-ended tasks with various input lengths from L-Eval (An et al., 2023). We underline the best results in each block. Results exceeding previous the best open-source finetuned model are in bold. 'dialogues' means the mix of ShareGPT and AlpacaGPT4 used in our training. Llama2-PI-SFT and Llama2-NTK-SFT are models trained with the same data and training steps with ChUnKLLAMA2. ${ }^{\\ddagger}$ : results are taken from Xiong et al. (2023). | Model | Finetuning <br> corpus | Training <br> context | TOFEL <br> ( $3 \\mathrm{k} \\sim 5 \\mathrm{k}$ ) | QuALITY <br> ( $4 \\mathrm{k} \\sim 9 \\mathrm{k})$ | Coursera <br> ( $5 \\mathrm{k} \\sim 17 \\mathrm{k})$ | SFiction <br> $(6 \\mathrm{k} \\sim 27 \\mathrm{k})$ | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Llama2-Chat 7B | $x$ | 4 k | 51.67 | 37.62 | 29.21 | 60.15 | 48.74 |\n| Llama2-DynNTK 7B | $x$ | 4 k | 52.27 | 30.69 | 13.95 | 57.02 | 38.48 |\n| Longchat-v1.5-32k 7B | ShareGPT | 32 k | 39.77 | 37.62 | 32.99 | 57.02 | 41.85 |\n| Llama2-PI-SFT 7B | Dialogues | 16 | 56.13 | 38.61 | 36.19 | 53.90 | 46.20 |\n| Llama2-NTK-SFT 7B | Dialogues | 16 k | 53.90 | 38.11 | 34.01 | 64.06 | 47.51 |\n| Vicuna-v1.5-16k 7B | ShareGPT | 16 k | 55.39 | 39.60 | 38.66 | 60.15 | 48.45 |\n| Llama2-Chat 13B | $x$ | 4 k | 60.96 | 42.57 | 35.75 | 54.68 | 48.99 |\n| Llama2-DynNTK 13B | $x$ | 4 k | 62.45 | 33.16 | 37.06 | 60.93 | 48.40 |\n| Vicuna-v1.5-16k 13B | ShareGPT | 16 | 68.40 | 53.96 | 40.69 | 61.71 | 56.19 |\n| Longlora-Chat 70B | LongAlpaca | 32 k | 71.37 | 55.45 | $\\underline{44.76}$ | 67.96 | $\\underline{59.88}$ |\n| Training-free |  |  |  |  |  |  |  |\n| ChunkLlama2-Chat 7B | $x$ | 4 k | 57.62 | 35.14 | 32.12 | 61.72 | 46.64 |\n| ChunkLlama2-Chat 13B | $x$ | 4 k | 66.54 | 43.06 | 41.56 | 57.03 | 52.04 |\n| ChunkLlama2-Chat 70B | $x$ | 4 k | $\\underline{82.15}$ | 60.39 | 48.54 | 61.72 | 63.20 |\n| Llama3 |  |  |  |  |  |  |  |\n| ChunkLlama3-Instruct 8B | $x$ | 8 k | 83.27 | 63.86 | 56.24 | 70.31 | 68.42 |\n| ChunkLlama3-Instruct 70B | $x$ | 8 k | $\\underline{84.75}$ | $\\underline{82.17}$ | $\\underline{76.88}$ | $\\underline{75.78}$ | $\\underline{79.89}$ |\n| Finetuned |  |  |  |  |  |  |  |\n| ChunkLlama2-Chat 7B | Dialogues | 16 k | 62.08 | 41.58 | 39.68 | 64.06 | 51.85 |\n| ChunkLlama2-Chat 13B | Dialogues | 16 k | 65.42 | 53.96 | 44.76 | 65.62 | 57.94 |\n| proprietary models |  |  |  |  |  |  |  |\n| GPT3.5-16k-0613 | Unkown | - | 78.43 | 61.38 | 63.51 | 64.84 | 67.03 |\n| Claude 1.3-100k | Unkown | - | 83.64 | 60.03 | 73.76 | 72.65 | 72.52 |\n| Llama2 Long-Chat $70 \\mathrm{~B}^{\\ddagger}$ | Long doc+diag | 16 k | 81.8 | - | 52.9 | - | - |\n\nversion of Llama2 Long, the results suggest that the Llama2 70 B chat model can be directly scaled to a 16 k context window without additional training with DCA, achieving $94 \\%$ of the performance of gpt-3.5-turbo-16k. We also demonstrate that our model's performance can be enhanced through additional finetuning on long dialogue data following the approach used by Vicuna (LMSYS, 2023) and Longchat (Li et al., 2023a), both of which are popular finetuned baselines utilizing ShareGPT.",
    "dca-15": "With further training, ChunkLLAMA2-Chat outperforms the previously best 13B model, Vicuna-v1.5-13b-16k, by a significant margin of 1.75 points. ### 4.4. Analysis\n\nEfficiency In figure 3, the inference time and GPU memory of (a) the original self-attention mechanism as implemented in PyTorch, Flash Attention (Dao, 2023), and our proposed DCA (integrated with Flash Attention) are evaluated across various prompt lengths. These experiments are run on a single NVIDIA 80G A100 GPU using Llama2 7B. The input long prompt is from NarrativeQA (Ko\u010disk\u00fd et al., 2018). We conduct 20 trials and report the average performance. Without Flash Attention, we observe that the maximum input length manageable by a single GPU is roughly between 12 k and 16 k tokens. DCA sustains similar GPU memory consumption and inference speed, without adding considerable overhead, with the original Flash atten- tion. ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-08.jpg?height=348&width=811&top_left_y=1371&top_left_x=1058)\n\nFigure 3. Inference time and GPU memory of (a) the original selfattention implemented by Pytorch, (b) Flash Attention (Dao, 2023), and (c) DCA (this work). Ablation Study To validate the three attention mechanisms proposed in this work, we present an ablation study for DCA in Figure 4, focusing on language modeling and passkey retrieval tasks. We consider three experimental conditions: (1) Employing only intra-chunk attention. (2) Utilizing both intra-chunk and inter-chunk attention. (3) Combining all three types of attention: intra-chunk, interchunk, and successive chunk attention. From the results in language modeling, we observe that using intra-chunk attention which disregards information from previous chunks, is able to maintain a very low PPL but hinders the model's ability to retrieve passkeys from other chunks. Introducing inter-chunk attention, we notice an improvement in passkey retrieval performance at an input length of 12 k . However, the loss of locality causes a significant increase in the model's PPL. By integrating successive chunk attention, we achieve both a low PPL and high retrieval accuracy. ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-09.jpg?height=300&width=839&top_left_y=430&top_left_x=185)\n\nFigure 4. Ablation study of DCA on language modeling (left) and passkey retrieval (right). We test the three attention mechanisms with input sequences from 8 k to 32 k . ## 5. Conclusion\n\nIn this paper, we present Dual Chunk Attention (DCA) as a novel and efficient approach to overcoming the context length limitations inherent in LLMs. By ingeniously leveraging the model's existing position indices and introducing a multi-faceted attention mechanism, DCA allows for extrapolating more than 8 x the training length without the need for costly and time-consuming further training. ## Impact Statement\n\nNumerous studies have emerged targeting to expand the supported context length of LLMs; however, due to high training costs and incompatibilities with technologies such as Flash Attention, the industry mainly relies predominantly on expanding the base frequency of RoPE or PI. Our Dual Chunk Attention (DCA) method is compatible with Flash Attention and requires only modifications to the inference code, negating the need for extensive retraining. DCA preserves model performance within the training length, and only benefits it beyond this range, offering compatibility with models that have already undergone long-context finetuning. Consequently, our approach may have a substantial impact on the industry, providing a cost-effective solution for managing long-context scenarios in LLM applications. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ## Acknowledgements\n\nWe thank Yukang Chen and Hang Yan for their helpful comments and open-source code. This research was supported in part by the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21. ## References\n\nAn, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023. Anthropic. Introducing 100 K Context Windows, 2023. URL https://www.anthropic.com/index/ 100 k -context-windows. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical report, 2023. Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L. Clex: Continuous length extrapolation for large language models, 2023a. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation, 2023b. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307, 2023c. Chi, T.-C., Fan, T.-H., Rudnicky, A.",
    "dca-16": "I., and Ramadge, P. J. Dissecting transformer length extrapolation via the lens of receptive field analysis, 2023. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Chowdhury, J. R. and Caragea, C. Monotonic location attention for length generalization, 2023. Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https: / / github.com/ togethercomputer/RedPajama-Data. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., and Gardner, M. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pp. 4599-4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https: //aclanthology.org/2021.naacl-main. 365. Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models, 2023. He, Z., Feng, G., Luo, S., Yang, K., He, D., Xu, J., Zhang, Z., Yang, H., and Wang, L. Two stones hit one bird: Bilevel positional encoding for better length extrapolation, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y., Chen, H., and Hu, X. Llm maybe longlm: Self-extend llm context window without tuning, 2024. Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers, 2023.",
    "dca-17": "Ko\u010disk\u00fd, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328, 2018. doi: 10.1162/tacl_a_00023. URL https://aclanthology.org/Q18-1023. Lee, G., Hartmann, V., Park, J., Papailiopoulos, D., and Lee, K. Prompted llms as chatbot modules for long opendomain conversation. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023. doi: $10.18653 / \\mathrm{v} 1 /$ 2023.findings-acl.277. URL http://dx.doi.org/10.",
    "dca-18": "18653/v1/2023.findings-acl. 277. Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length.",
    "dca-19": "2023a. Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., Sanghai, S., Yang, Y., Kumar, S., and Bhojanapalli, S. Functional interpolation for relative positions improves long context transformers, 2023b.",
    "dca-20": "Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts, 2023a. Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D. Scaling laws of rope-based extrapolation, 2023b. LMSYS. Vicuna: An open-source chatbot impressing gpt-4 with 90 URL https://lmsys.org/blog/ 2023-03-30-vicuna/. LocalLLaMA. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023a. URL https://www.reddit.com/r/ LocalLLaMA/comments/14mrgpr/dynamically_ scaled_rope_further_increases/. LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended $(8 \\mathrm{k}+$ ) context size without any fine-tuning and minimal perplexity degradation., June 2023b. URL https://www.reddit.com/ r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/.",
    "dca-21": "Lv, K., Liu, X., Guo, Q., Yan, H., He, C., Qiu, X., and Lin, D. Longwanjuan: Towards systematic measurement for long text quality, 2024. Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. MosaicML. Introducing mpt-30b: Raising the bar for opensource foundation models, 2023a. URL www . mosaicml. com/blog/mpt-30b. Accessed: 2023-06-22. MosaicML. Introducing mpt-7b: A new standard for opensource, ly usable llms, 2023b.",
    "dca-22": "URL www.mosaicml. $\\mathrm{com} / \\mathrm{blog} / \\mathrm{mpt}-7 \\mathrm{~b}$. OpenAI. Gpt-4 technical report, 2023. Pang, R. Y., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V., Ma, J., Thompson, J., He, H., and Bowman, S. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5336-5358, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL https: / aclanthology.org/2022.naacl-main.",
    "dca-23": "391. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models, 2023. Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y. Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models.",
    "dca-24": "ArXiv, abs/2401.04658, 2024. URL https://api. semanticscholar.org/CorpusID:266900042. Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for longrange sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= SylKikSYDH. Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y. Parallel context windows for large language models, 2023. Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends ${ }^{\\circledR}$ in Information Retrieval, 3(4):333-389, 2009. Rozi\u00e8re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., D\u00e9fossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2023.",
    "dca-25": "Rula, A. and D'Souza, J. Procedural text mining with large language models, 2023. Ruoss, A., Del\u00e9tang, G., Genewein, T., Grau-Moya, J., Csord\u00e1s, R., Bennani, M., Legg, S., and Veness, J. Randomized positional encodings boost length generalization of transformers, 2023. Saad-Falcon, J., Barrow, J., Siu, A., Nenkova, A., Yoon, D.",
    "dca-26": "S., Rossi, R. A., and Dernoncourt, F. Pdftriage: Question answering over long, structured documents, 2023. Song, K., Wang, X., Cho, S., Pan, X., and Yu, D. Zebra: Extending context window with layerwise grouped localglobal attention, 2023. Su, J. Rectified rotary position embeddings. https:// github.com/bojone/rerope, 2023. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2022. Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. A lengthextrapolatable transformer, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https: / / github.com/tatsu-lab/stanford_alpaca, 2023. Together. Llama-2-7b-32k-instruct - and fine-tuning for llama-2 models with together api, 2023. URL https: / / together.ai/blog/llama-2-7b-32k-instruct. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017. Wang, L., Yang, N., and Wei, F. Learning to retrieve incontext examples for large language models, 2024. Wei, J., Kim, S., Jung, H., and Kim, Y.-H. Leveraging large language models to power chatbots for collecting user self-reported data, 2023. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory, 2024. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023. doi: 10.48550/ARXIV.2309.16039. URL https: / / doi . org/10.48550/arXiv.2309.16039. Ye, J., Wu, Z., Feng, J., Yu, T., and Kong, L. Compositional exemplars for in-context learning.",
    "dca-27": "arXiv preprint arXiv:2302.05698, 2023. Zhang, J., Jiang, S., Feng, J., Zheng, L., and Kong, L. Linear attention via orthogonal memory.",
    "dca-28": "ArXiv, abs/2312.11135, 2023. URL https://api.semanticscholar.org/ CorpusID:266359128. Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Soaring from 4 k to 400 k : Extending llm's context with activation beacon.",
    "dca-29": "ArXiv, abs/2401.03462, 2024. URL https://api.semanticscholar.org/ CorpusID:266844488. Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A. H., Celikyilmaz, A., Liu, Y., Qiu, X., and Radev, D. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905-5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https: //aclanthology.org/2021.naacl-main. 472. Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S. Pose: Efficient context window extension of llms via positional skip-wise training, 2023.",
    "dca-30": "## A. Appendix\n\n## A.1. Passkey retrieval\n\nIn addition to practical tasks, we evaluate the long-context capability of LLMs to perform the passkey retrieval task as defined in Mohtashami \\& Jaggi (2023). This task challenges a language model to locate a simple passkey (e.g., a fivedigit random number) embedded within a lengthy and otherwise nonsensical text sequence. The primary purpose of this task is to determine if a Large Language Model (LLM) can maintain awareness of information distributed throughout a lengthy input sequence. To assess retrieval accuracy, we randomly place the passkey at various document depths which are distributed uniformly. For each document depth, we run 20 times with different passkeys and we test the input sequence length from 4 k to 20k. We compare the performance of DCA with 2 popular extension methods: PI (Chen et al., 2023b), NTK-Aware (LocalLLaMA, 2023b;a), on the Llama2 13B model with a 4 k pretraining context window. The performance results are depicted in Figure 5. Notably, within a context length of 18 k tokens, our model CHUNKLLAMA2 consistently achieved a $100 \\%$ passkey retrieval accuracy across all depths tested. We expanded the scope of the passkey retrieval tasks by incrementally increasing the input token count from 2 k to 192 k . For each input context length, the model is evaluated 20 times, with the passkey's position randomly varied in each test. Additionally, we also verify the Together-32k 7B model (Together, 2023), which supports a 32 k token context window, and its ChunkTogether 7B counterpart. The outcomes for both the baseline and DCA-enhanced variants of these models are illustrated in Figure 7. With only a 4 k training context length, ChunkLlama2 maintains high retrieval accuracy up to a 32 k context length. By integrating these findings with existing long-context models, we can feasibly extend the supported context window to an impressive 192 k tokens using a learning-free approach. lost in the beginning: An intriguing observation is that the failure cases of PI appear to be largely unrelated to the document's depth, while the NTK-based approach typically excels when the passkey is positioned near the beginning of the document. However, its effectiveness significantly diminishes-with accuracy dropping to between $40 \\%$ and $80 \\%$-when the passkey is placed in the middle sections.",
    "dca-31": "This trend aligns with findings reported by Liu et al. (2023a). Conversely, as the input context is expanded, CHUNKLLAMA2 demonstrates improved performance in the middle sections but the first place where a drop in accuracy occurs is at the beginning of the text. ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-13.jpg?height=335&width=1609&top_left_y=1415&top_left_x=224)\n\nFigure 5. Testing Different Learning-Free Extension Methods with a 24K Context ('Needle in a Haystack\" Passkey Retrieval). All the models have a 4 k pretraining context and are not further trained. The X -axis represents the input context length, and the Y -axis indicates the depth of the passkey within the document. For each depth, we run 20 different test cases. ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-13.jpg?height=381&width=1372&top_left_y=1986&top_left_x=342)\n\nFigure 6. Pressure testing Mistral-7B-Instruct-v0.2 over a 192k context length for and its DCA enhanced version ((\"Needle In A HayStack'\")). ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-14.jpg?height=406&width=1354&top_left_y=217&top_left_x=352)\n\nFigure 7. Passkey retrieval over a 192k context length for Llama2 13B, Together-32k 7B and their DCA enhanced versions. ![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-14.jpg?height=429&width=437&top_left_y=696&top_left_x=345)\n(b) Inter-Chunk Attention\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-14.jpg?height=391&width=438&top_left_y=734&top_left_x=800)\n(c) Successive-Chunk Attention\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1a646067648f461a1a70g-14.jpg?height=396&width=457&top_left_y=729&top_left_x=1257)\n\nFigure 8. Visualization of the Relative Position Matrix $M$ employing Dual Chunk Attention (DCA) by splitting the whole sequence into 3 chunks and the chunk size $s=4$. In this case, we have the pretraining window size $c=8$, and local window size $w=3$. The sequence is segmented into 3 chunks to ensure that relative positions do not exceed 7 . The matrix element $M[i][j]$ represents the relative position between the $i$-th query vector $\\mathbf{q}$ and the $j$-th key vector $\\mathbf{k}$. ## A.2. More Examples\n\nIn this section, we give an example of handling a 12-token sequence but the pre-training length is only 8 . Based on Llama2, the key/query position indices will be initialized as:\n\n$$\n\\begin{aligned}\nP_{\\mathbf{q}} & =[0,1,2,3,4,5,6,7,8,9,10,11] \\\\\nP_{\\mathbf{k}} & =[0,1,2,3,4,5,6,7,8,9,10,11]\n\\end{aligned}\n$$\n\nWhen query $[11]$ and query $[0]$ perform an inner product, their relative position is 11 , which exceeds the pre-training size. In DCA, we set a chunk size, which is a hyperparameter smaller than the pre-training length. In this example, we can set the chunk size to 4 , which means we split the entire input into 3 chunks and give new position indices for keys:\n\n$$\n[0,1,2,3,4,5,6,7,8,9,10,11] \\Rightarrow[0,1,2,3,0,1,2,3,0,1,2,3]\n$$\n\nWe then give different position indices for queries. Intra-chunk Attention: Calculating the attention for tokens in the same chunk\n\n$$\n\\begin{aligned}\nP_{\\mathbf{q}}^{\\text {intra }} & =[0,1,2,3,0,1,2,3,0,1,2,3] \\\\\nP_{\\mathbf{k}} & =[0,1,2,3,0,1,2,3,0,1,2,3]\n\\end{aligned}\n$$\n\nThe resulting position matrix is shown in Appendix Figure 8 (a) and the maximum relative position is $3-0=3$. Inter-chunk Attention: Calculating the attention for tokens in the different chunks When the pre-training length is 8 , we have the maximum position index $=7$. $$\n\\begin{aligned}\nP_{\\mathbf{q}}^{\\text {inter }} & =[7,7,7,7,7,7,7,7,7,7,7,7] \\\\\nP_{\\mathbf{k}} & =[0,1,2,3,0,1,2,3,0,1,2,3]\n\\end{aligned}\n$$\n\nThe resulting position matrix after inter-chunk attention is shown in Appendix Figure 8 (b). Successive-chunk Attention: Calculating the attention for tokens in successive chunks. We change the first $w=3$ (a hyperparameter) elements in $P_{\\mathbf{q}}^{\\text {inter }}$ :\n\n$$\n\\begin{aligned}\nP_{\\mathbf{q}}^{\\text {succ }} & =[4,5,6,7,4,5,6,7,4,5,6,7] \\\\\nP_{\\mathbf{k}} & =[0,1,2,3,0,1,2,3,0,1,2,3]\n\\end{aligned}\n$$\n\nThe position matrix after successive-chunk attention is shown in Appendix Figure 8 (c). ## A.3. Flash Attention\n\nWe divide the standard self-attention into 3 separate flash attention calculations respectively obtaining the output from intra-chunk attention, inter-chunk-attention, and successive chunk-attention. Algorithm 1 showcases how the 3 attention introduced in DCA integrate with Flash Attention. We illustrate with the $i$-th query vector $\\mathbf{q}_{i}$ and it needs to calculate the inner product with all keys $\\mathbf{k}_{j}$ with $j \\leq i$. We have $n=\\lfloor i / s\\rfloor$ chunks before the current chunks. DCA calls 3 separate Flash Attention operations with complexity $O(i-n * s)$ (intra-chunk attention), $O(s)$ (succssive-chunk attention) and $O(s *(n-1))$. ```\nAlgorithm 1 Pseudocode of DCA with FlashAttention\n    \\# q: 1 x d query vector (tensor with shape \\([1, d]\\) )\n    \\# i: the absolute index of q (integer)\n\\# K, V: i x d matrices for keys and values (tensors with shape [i, d])\n\\# s: chunk size (integer)\n\\# P_k, P_q_intra, P_q_succ, P_q_inter: position ids (lists of integers)\nn = math.floor(i/s) \\# Number of chunks before the current chunk\n\\# Apply rotary position embeddings to the entire key matrix K\nK = apply_rotary_pos_emb (K, P_k) \\# K is [i, d] after embedding\n\\# ------------- Intra-chunk Attention, casual=True\nq_intra = apply_rotary_pos_emb(q, P_q_intra[i]) \\# q_intra is [1, d]\n\\# Select intra-chunk keys and values\nK_intra = K[s*n:i] \\# K_intra is [(i - s*n), d]\nV_intra \\(=\\) V[s*n:i] \\# V_intra is [(i - s*n), d]\n\\# Compute output and softmax attention map for intra-chunk attention\no_intra, map_intra = Flash(q_intra, K_intra, V_intra) \\# o_intra is [1, d], map_intra is [1, i - s*n]\n\\# ------------- Successive-chunk Attention, casual=False\nq_succ = apply_rotary_pos_emb(q, P_q_succ[i]) \\# q_succ is [1, d]\n\\# Select successive-chunk keys and values\nK_succ \\(=K[s *(n-1): s * n]\\) \\# K_succ is \\([s, d]\\)\nV_succ \\(=V[s *(n-1): s * n]\\) \\# V_succ is \\([s, d]\\)\n\\# Compute output and softmax attention map for successive-chunk attention\no_succ, map_succ = Flash(q_succ, K_succ, V_succ) \\# o_succ is [1, d], map_succ is [1, s]\n\\# ------------- Inter-chunk Attention, casual=False\nq_inter = apply_rotary_pos_emb(q, P_q_inter[i]) \\# q_inter is [1, d]\n\\# Select inter-chunk keys and values\nK_inter = K[:s* \\(\\mathrm{n}-1)]\\) \\# K_inter is \\([\\mathrm{s*}(\\mathrm{n}-1), \\mathrm{d}]\\)\nV_inter = V[:s*(n-1)] \\# V_inter is \\([\\mathrm{s*}(\\mathrm{n}-1), \\mathrm{d}]\\)\n\\# Compute output and softmax attention map for inter-chunk attention\no_inter, map_inter = Flash(q_inter, K_inter, V_inter) \\# o_inter is [1, d], map_inter is [1, s*(n-1)]\n\\# Normalization\n\\# Sum the attention maps for each attention type to get normalizers\nsum_intra = map_intra.sum (-1) \\# sum_intra is a scalar\nsum_inter = map_inter.sum(-1) \\# sum_inter is a scalar\nsum_succ = map_succ.sum(-1) \\# sum_succ is a scalar\nnormalizer = sum_intra + sum_inter + sum_succ \\# normalizer is a scalar\n\\# Concatenate attention outputs and divide by normalizer\noutput = (sum_intra*o_intra, sum_succ*o_succ, sum_inter*o_inter) / normalizer \\# output is [1, d]\n```\n\n\n## A.4. In-Context Examples Selection\n\nWe opt to select in-context examples from the training set which is a practical and common way to obtain the examples (Ye et al., 2023; Wang et al., 2024). We experimented with 2 different methods for this selection process: (1)Random Selection: randomly selecting examples from the training set. (2) Retrieval-Based Selection: Using the current query, we employ\n\nTable 5. Comparison of few-shot results using different in-context examples\n\n| Models | In-Context Examples | Qasper <br> F1 (2-shot) | QuALITY <br> EM (2-shot) | QMSum <br> R-g (1-shot) |\n| :--- | :---: | :---: | :---: | :---: |\n| CHUNKLLAMA2 7B | EXAMPLE BEST | 27.3 | 33.9 | 15.0 |\n| CHUNKLLAMA2 7B | EXAMPLE RANDOM | 28.2 | 35.6 | 14.7 |\n| ChUNKLLAMA2 7B | EXAMPLE WORST | 28.4 | 35.9 | 14.3 |\n| CHUNKLLAMA2 13B | EXAMPLE BEST | 28.5 | 46.2 | 15.6 |\n| CHUNKLLAMA2 13B | EXAMPLE RANDOM | 29.3 | 47.9 | 15.2 |\n| CHUNKLLAMA2 13B | EXAMPLE WORST | 29.0 | 47.5 | 15.5 |\n\nretrieval algorithms such as BM25 (Robertson et al., 2009) to find the most relevant examples from the training set. We refer to the in-context examples with the highest retrieval score as Example BeSt and those with the lowest as Example WORST. The performance of different selection approaches based on ChUnKLLAMA2 7B/13B is shown in Table 5. The performance on the summarization dataset QMSum (Zhong et al., 2021) generally is less likely to be influenced by prompt selection. However, on the 2 question-answering datasets, we find that using the closest examples, paradoxically, leads to the poorest outcomes and the performance of both random selection and choosing the worst example is relatively similar. A possible explanation for this phenomenon is that when the example is highly similar, LLMs tend to copy the response given in the example which usually leads to a wrong answer. ## A.5. Performance on Unseen Data\n\nCurrently, almost all benchmarks for LLMs fail to thoroughly address the potential of data contamination, which implies that the test data might have already been used in pretraining or finetuning phases. To demonstrate ChunkLlama's performance on previously unseen long-document data, we directly used the Latex code of this paper as a test case while omitting the title, abstract, and conclusion sections.",
    "dca-32": "After tokenization, the total input length is 19388 . We initiate the evaluation with a set of straightforward questions that do not necessitate prior knowledge for accurate responses (see Table 6). This was followed by a series of more challenging questions designed to assess the comprehension of the proposed DCA (refer to Table 7). The results indicate that, compared to the NTK, CHUNKLLAMA2 demonstrates a superior ability to correctly interpret instructions and provide accurate responses. However, the precision of the 13B model's answers remains suboptimal, even for relatively simple inquiries. For instance, when queried about the finetuning corpus used by DCA, it erroneously cites the Llama2 corpus rather than the correct corpora, which are ShareGPT and AlpacaGPT4. On these elementary questions, ChunkLlama 70B exhibits a remarkably high accuracy rate. Moreover, ChunkLlama 70B shows promising outcomes on more challenging queries. It is able to articulate the rationale behind our design of the interchunk and successive-chunk strategies accurately.",
    "dca-33": "However, when we pose questions that require a global understanding of this method, it still faces difficulties. Table 6. QA (Easy) results of ChUnKLlama2 13B/70B and Dynamic-NTK with Llama2 13B, using the content of this paper as input. Most of the questions do not require a deep understanding of this paper. The input prompt has 19388 tokens. Dynamic-NTK is unable to give the correct answer for all test cases. Low-quality generation results are marked with $X$. | Prompt | $<<$ SYS $>>$ Please read the long paper and answer the questions $<</$ SYS $>>1$. Introduction $\\backslash n \\backslash n$ <br> The ability to comprehend and process long-context information is essential for large language models (LLMs) <br> to cater to a wide range of applications effectively. These include analyzing and responding to inquiries within <br> sizable PDFs, retaining extended dialogue history, and empowering interactive chatbots... [/INST] |\n| :---: | :---: |\n| Question | Can you give a title for this paper |\n| ChunkLlama270B | Sure, here are some potential titles for the paper: <br> 1. \"Dual Chunk Attention: A Training-Free Framework for Long-Context Language Models\" <br> 2. \"Efficient Long-Context Language Modeling with Dual Chunk Attention\" <br> 3. \"Scaling Long-Context Language Models without Training: A Dual Chunk Attention Approach\" <br> 4. 'DCA: A Training-Free Method for Long-Context Language Modeling\" <br> 5. \"Long-Context Language Modeling with Dual Chunk Attention: A Training... |\n| ChunkLlama2 13B | Sure! Dual Chunk Attention: A Training-Free Framework for Long-Context Scaling of Large Language <br> Models. |\n|  | This title highlights the key contributions of the paper: the proposed Dual Chunk Attention (DCA) framework, <br> which enables long-context scaling without additional training, and its application to various tasks. The phrase <br> \"training-free\" is also included to emphasize the main innovation of the paper. |\n| Dynamic NTK 13B | The proposed Dual Chunk Attention (DCA) framework improves the long-context scaling of Lama2 by more <br> than 8 times, achieving a PPL of 5.59 at a context window of 2 k . $X$ (ignore the instruction) |\n| Question | Which model can be scaled up to 192 k context with DCA? |\n| ChunkLlama270B | The Llama2 70B model can be scaled up to a 192k context window using DCA, achieving a PPL score of 5.59 |\n| ChunkLlama2 13B | The Llama2 70B model can be scaled up to 192k context with DCA. |\n| Dynamic NTK 13B | \\{ \\end } \\{  enumerate \\} \\end\\{figure\\} [ht!]  }  <br>  \\end \\{document \\} \\end\\{document\\} \\begin\\{table\\}X(error)  } \\end{array}  <br>  Question   In addition to Llama2, have the authors validated their method on other models? <br>  ChunkLlama270B  $\\begin{array} { l } { \\text { Yes, the authors have validated their method on other models, including } } \\\\ { \\text { Together-32k, } } \\\\ { \\text { CodeLlama, } } \\\\ { \\text { which are all based on Llama. They have also compared their method to a variety of open-source and } } \\\\ { \\text { proprietary models. } } \\end{array}$ }  { \\hline  ChunkLlama2 13B  } & { $\\begin{array} { l } { \\text { The authors have validated their method on the following models: } } \\\\ { \\text { * Together-32k } } \\\\ { \\text { * CodeLlama } } \\\\ { \\text { * Llama2 Long } X \\text { (We did not verify DCA on Llama2 Long) } } \\end{array}$ }  { \\hline  Dynamic NTK 13B  } & { $\\begin{array} { l } { \\text { The paper introduces a new training-free framework called Dual Chunk Attention (DCA) that extends the } } \\\\ { \\text { context window by more than } 8 \\text { times for 7B/13B Llama2 variants and } 2 0 \\text { times for 70B models } X \\text { (ignore the } } \\\\ { \\text { instruction) } } \\end{array}$ }  { \\hline  Question  } & {  Describe the finetuning corpus used by DCA  }  { \\hline  ChunkLlama270B  } & {  The finetuning corpus used by DCA is a combination of long dialogue data from ShareGPT and AlpacaGPT4  }  { \\hline  ChunkLlama2 13B  } & { $\\begin{array} { l } { \\text { The finetuning corpus used by DCA is the same as the one used by the original Llama2 model, which is a } } \\\\ { \\text { combination of the training set and the validation set of the original Llama2 model } \\boldsymbol { X } \\text { (We did not use training } } \\\\ { \\text { set of Llama) } } \\end{array}$ }  { \\hline  Dynamic NTK 13B  } & { $\\begin{array} { l } { \\{ \\text { \\end\\{begin\\} \\begin\\{table\\} \\end\\{figure\\} } } \\\\ { \\text { [ht ! ] \\} \\end\\{document } \\} \\text { \\end } \\{ \\text { document } \\} \\backslash \\text { end } \\{ \\text { document } \\} \\boldsymbol { X } \\text { (error) } } \\end{array}$ }  { \\hline  Question  } & {  How many training steps does Llama2 Long need? }  { \\hline  ChunkLlama270B  } & {  Llama2 Long needs  1 0 0 , 0 0 0  training steps  }  { \\hline  ChunkLlama2 13B  } & {  Llama2 Long needs 100,000 training steps. }  { \\hline  Dynamic NTK 13B  } & {  Llama2 Long needs  4 0 , 0 0 0  steps for a  4 \\mathrm { k }  context window. X  (not  4 0 , 0 0 0  )  }  { \\hline  Question  } & {  Do the authors validate the model on L-Eval? and list the tasks  }  { \\hline  ChunKLLama270B  } & { $\\begin{array} { l } { \\text { Yes, the authors validate their model on L-Eval and list the tasks in Table 4. They evaluate their model on four } } \\\\ { \\text { closed-ended tasks from L-Eval with diverse input lengths ranging from } 3 \\mathrm { k } \\text { to } 2 7 \\mathrm { k } \\text { . The tasks include TOFEL, } } \\\\ { \\text { QuALITY, Coursera, and SFiction. } } \\end{array}$ }  { \\hline  ChunkLlama2 13B  } & { $\\begin{array} { l } { \\text { Yes, the authors do validate the model on L-Eval and list the tasks. In the section titled \"Experiments\", the } } \\\\ { \\text { authors present an extensive evaluation of their model on a diverse set of tasks, including NarrativeQA, Qasper, } } \\\\ { \\text { Quality, and QMSum. } X \\text { (These are not from L-Eval) } } \\end{array}$ }  { \\hline  Dynamic NTK 13B  } & { $\\begin{array} { l } { \\text { The paper presents a new training-free framework called Dual Chunk Attention (DCA) that extends the context } } \\\\ { \\text { window by more than } 8 \\text { times for } 7 \\mathrm { B } / 1 3 \\mathrm { B } \\text { Llama2 variants and... } \\boldsymbol { X } \\text { (ignore the instruction) } } \\end{array}$ }  \\hline \\end{array |\n\nTable 7. QA (hard) results of ChUnkLlama2 13B/70B and Dynamic-NTK with Llama2 13B, using the content of this paper as input. Questions in this table typically require readers to understand this method. The input prompt has 19388 tokens. Dynamic-NTK is unable to give the correct answer for all test cases. Low-quality generation results are marked with $X$. Prompt $[$ INST $]<<$ SYS $>>$ Please read the long paper and answer the questions $<</$ SYS $>>1$. Introduction $\\backslash \\mathrm{n} \\backslash \\mathrm{n}$ The ability to comprehend and process long-context information is essential for large language models (LLMs) to cater to a wide range of applications effectively. These include analyzing and responding to inquiries within sizable PDFs, retaining extended dialogue history, and empowering interactive chatbots... [/INST]\n\n\n#### Abstract\n\nQuestion Why do the authors introduce successive-chunk attention? (Hard) ChunkLlama270B The authors introduce successive-chunk attention to address the issue of losing locality in the attention mechanism when using inter-chunk attention. Inter-chunk attention can lead to a situation where the relative position between two tokens in different chunks is not accurately captured, resulting in a high perplexity. Successive-chunk attention helps to maintain locality by adjusting the position indices of the first $w$ tokens in each chunk to ensure that the relative position between neighboring tokens is accurately captured. This allows the model to better handle long-context tasks that require the ability to capture long-range dependencies. ChunkLlama2 13B The authors introduce successive-chunk attention to address the issue of long-range dependencies in longcontext language models. The original positional encoding methods, such as RoPE and NTK-aware RoPE, do not capture long-range dependencies effectively, leading to poor performance in language modeling and passkey retrieval tasks. By introducing successive-chunk attention, the model can capture both local and long-range dependencies.. $X$ (misunderstand successive-chunk attention) Dynamic NTK 13B The paper introduces a new training-free framework called Dual Chunk Attention (DCA) that extends the context window by more than 8 times for 7B/13B Llama2 variants... $X$ (ignore the instruction) Question Why do the authors introduce inter-chunk attention? (Hard) ChunkLlama270B The authors introduce inter-chunk attention to address the issue of poor long-range dependencies in the intra-chunk attention module, which uses a fixed context window for attention computation. The inter-chunk attention mechanism allows the model to attend to tokens from different chunks, enabling it to capture longrange dependencies and improve performance on tasks that require processing long sequences. ChunkLlama2 13B 13b: The author introduces inter-chunk attention to improve the passkey retrieval performance of the model. The author notes that using only intra-chunk attention, which disregards information from previous chunks, can maintain a low PPL but hinder the model's ability to retrieve passkeys from other chunks. $X$ (not accurate enough)\nDynamic NTK 13B The paper presents a new training-free framework called Dual Chunk Attention (DCA) that extends the context window by more than 8 times for 7B/13B Llama2 variants...X(ignore the instruction)\n\nQuestion Why the relative position in DCA will not exceed the pretraining context length? (Hard)\nChunKLLAMA270B The relative position in DCA is calculated using the position indices of the keys and queries, which are both less than or equal to the pretraining context length. Therefore, the relative position will not exceed the pretraining context length. (not clear enough)\nChunkLlama2 13B The relative position in DCA will not exceed the pretraining context length because the model only considers the relative positions within the same chunk. The chunks are segmented based on the pretraining context length, and the relative positions are calculated within each chunk. (not clear enough)\nDynamic NTK 13B The relative position in Dual Chunk Attention (DCA) does not exceed the pretraining context length because it is designed to maintain locality. (not clear enough)\n\n| Question |\n| :--- | :--- | | What are the key differences between this paper and earlier papers? (Hard) |\n| :--- |\n| ChunkLlamA270B | | The key differences between this paper and earlier papers on large language models (LLMs) are: |\n| :--- |\n| 1. The paper proposes a new method called Dual Chunk Attention (DCA) that enables LLMs to process |\n| long-context information without additional training costs. |\n| 2. DCA is designed to handle long-context information by segmenting the input sequence into smaller chunks |\n| and computing attention within each chunk, which reduces the computational cost and allows for efficient |\n| scaling of the context window... |\n\n\n[^0]:    ${ }^{*}$ Work done during internship at Alibaba Group ${ }^{1}$ The University of Hong Kong ${ }^{2}$ Alibaba Group ${ }^{3}$ Fudan University. Correspondence to: Chenxin An [cxan23@connect.hku.hk](mailto:cxan23@connect.hku.hk). Proceedings of the $41^{\\text {st }}$ International Conference on Machine Learning, Vienna, Austria.",
    "dca-34": "PMLR 235, 2024. Copyright 2024 by the author(s). [^1]:    ${ }^{1}$ Without Flash Attention, the maximum input tokens for Llama2 7B/13B is about 16 k , and for Llama2 70B, it is 5 k when tested on two A100 80G GPUs in our experiments\n    ${ }^{2} \\mathrm{We}$ invite interested readers to examine the results in Tables 6,7\n\n[^2]:    ${ }^{3}$ Queries and keys are usually derived by projecting the input x through a learnable linear layer. ${ }^{4} \\mathrm{~A}$ typical implementation of $f$ can be found in modeling_llama.py Line 211 apply_rotary_pos_emb ()\n\n[^3]:    ${ }^{5}$ https://huggingface.co/togethercomputer/ LLaMA-2-7B-32K\n    ${ }^{6}$ https: / /huggingface.co/codellama\n\n[^4]:    ${ }^{7}$ https://sharegpt.com/\n    ${ }^{8}$ https://github.com/DachengLi1/LongChat\n    ${ }^{9}$ prior to December 1, 2023\n\n"
}