{
    "mamba4gcn-0": "# MambaForGCN: Enhancing Long-Range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis \n\nAdamu Lawan ${ }^{1}$, Juhua $\\mathrm{Pu}^{1}$, Haruna Yunusa ${ }^{2}$, Aliyu Umar ${ }^{3}$, Muhammad Lawan ${ }^{4}$<br>${ }^{1}$ School of Computer Science and Engineering, Beihang University, Beijing, China<br>${ }^{2}$ School of Automation Science and Electrical Engineering, Beihang University, Beijing, China<br>${ }^{3}$ Jigawa State Institute of Information Technology, Kazaure<br>${ }^{4}$ Federal University, Gusau\n\nAbstract.",
    "mamba4gcn-1": "Aspect-based sentiment Analysis (ABSA) identifies and evaluates sentiments toward specific aspects of entities within text, providing detailed insights beyond overall sentiment. However, Attention mechanisms and neural network models struggle with syntactic constraints, and the quadratic complexity of attention mechanisms hinders their adoption for capturing long-range dependencies between aspect and opinion words in ABSA. This complexity can lead to the misinterpretation of irrelevant contextual words, restricting their effectiveness to short-range dependencies. Some studies have investigated merging semantic and syntactic approaches but face challenges in effectively integrating these methods. To address the above problems, we present MambaForGCN, a novel approach to enhance short and long-range dependencies between aspect and opinion words in ABSA. This innovative approach incorporates syntaxbased Graph Convolutional Network (SynGCN) and MambaFormer (Mamba-Transformer) modules to encode input with dependency relations and semantic information. The Multihead Attention (MHA) and Mamba blocks in the MambaFormer module serve as channels to enhance the model with short and long-range dependencies between aspect and opinion words. We also introduce the Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated feature representation system combining SynGCN and MambaFormer representations. Experimental results on three benchmark datasets demonstrate MambaForGCN's effectiveness, outperforming state-of-the-art (SOTA) baseline models. Keywords: Aspect-Based Sentiment Analysis, Multi-head Attention, State Space Model, Mamba, Gated Fusion, Kolmogorov-Arnold Networks. ## 1 Introduction\n\nIn Natural Language Processing (NLP), text classification is essential for categorizing and extracting meaningful insights from textual data. A critical subset of text classification is sentiment analysis, which identifies the emotional tone or sentiment expressed within a text. With the growth of online platforms and the surge of user-generated content, sentiment analysis has become increasingly significant for applications like customer feedback analysis and product recommendation systems. However, traditional sentiment analysis often fails to capture sentiments about specific aspects or features within the text. This shortcoming led to the advent of Aspect-Based Sentiment Analysis (ABSA), which determines the overall sentiment and identifies and analyzes sentiments tied to particular aspects or features mentioned in the text. ABSA offers a more detailed and nuanced understanding of sentiment, providing valuable insights for businesses to enhance decision-making and improve user experiences. Recent advancements in semantic-based models have significantly improved ABSA by incorporating various attention mechanisms. For instance, [1] introduced a deep memory network that emphasizes the importance of individual context words by integrating neural attention models over external memory. This approach effectively captures nuanced sentiment expressions. Similarly, [2] proposed an Attention-based Long Short-Term Memory Network (LSTM) tailored for ABSA, which uses attention mechanisms to highlight distinct sentence parts based on different aspects. Interactive Attention Networks (IAN), proposed by [3], facilitate interactive learning and generate distinct representations for targets and contexts, thus enhancing sentiment classification precision. [4] presented a framework utilizing a multiple-attention mechanism to capture sentiment features, integrating these attentions with a recurrent neural network for improved expressiveness. Similarly, [5] introduced a multi-grained attention network (MGAN) that employs fine-grained attention mechanisms to capture word-level interactions between aspects and context, further improving classification accuracy. Further semantic model refinement includes [6] 's work on better integrating syntactic information into the attention mechanism to capture the relationship between aspect terms\nand context. [7] proposed augmenting LSTM with a stacked attention mechanism for target and sentence levels, introducing Sentic-LSTM to integrate explicit and implicit knowledge. Additionally, a hybrid model combining LSTM and a recurrent additive network was proposed for a joint task of aspect detection and sentiment classification. [8] introduced a $\\mathbf{c o}=$ attention mechanism alternating between target-level and contextlevel attention, proposing Coattention-LSTM and Coattention-MemNet networks to enhance sentiment classification. [9] advanced the field by presenting a multi-head coattention network model with three modules: extended context, component focusing, and multi-headed co-attention. These modules enhance transformer-based sentiment analysis by improving context handling, emphasizing key adjectives and adverbs, and refining attention mechanisms for multi-word targets. On the other hand, syntax-based models leverage syntactical information and word dependencies to improve ABSA. For instance, [10] proposed merging convolution over a dependency tree (CDT) with bi-directional LSTM (Bi-LSTM) to analyze sentence structures effectively. This approach introduced Graph Convolutional Network $(\\mathrm{GCN})$ enhancements that directly engaged sentence dependency trees. Building on this, [11] proposed a GCN over a sentence's dependency tree to leverage syntactical information and word dependencies for enhancing aspect-specific sentiment classification. Further advancements in syntax-based models include Sentic GCN, introduced by [12], which leverages affective dependencies specific to aspects by integrating affective knowledge from SenticNet. [13] presented KDGN, a knowledge-aware Dependency Graph Network that incorporates domain knowledge, dependency labels, and syntax paths into the dependency graph framework. [14] introduced EK-GCN, a GCN enhanced with external knowledge, such as sentiment lexicons and part-of-speech information, incorporating sentiment score matrices to highlight sentiment word weights. Additionally, [15] introduced DGGCN, a dual-gated GCN enhanced with contextual affective knowledge, which integrates GCN with a gating mechanism to better interact with the given aspect and context. [16] presented a deformable convolutional network model that leverages phrases for improved sentiment analysis. This model uses deformable convolutions with adaptive receptive fields to capture phrase representations across various contextual distances. Moreover, [17] proposed IA-HiNET, a hierarchical network to strengthen the relationships between aspect words for improved sentence-\nlevel sentiment analysis. It employs part-of-speech and positional information, a GCN for capturing emotional cues, and an aspect-oriented self-attention mechanism. Lastly, [18] introduced APSCL, an Aspect-Pair Supervised Contrastive Learning model, which enhances aspect representation by capturing latent relationships in the sentiment subspace. Attention mechanisms and neural network models often struggle with syntactic constraints, and the quadratic complexity of attention mechanisms hinders their adoption for capturing long-range dependencies between aspect and opinion words in ABSA. This complexity can lead to the misinterpretation of irrelevant contextual words, restricting their effectiveness to short-range dependencies. Some studies have investigated merging semantic and syntactic approaches. Still, they struggle to effectively combine and fuse these approaches, hindering the integration of various informational elements. To effectively address the above problems, in this work, we present MambaForGCN, a novel approach aimed at enhancing short and long-range dependencies between aspect and opinion words in ABSA. This innovative approach incorporates a syntax-based GCN Graph Convolutional Network (SynGCN) module to encode the input with dependency relations. This innovative approach also incorporates a MambaFormer module to enrich the model with semantic information. The Multihead Attention (MHA) and Mamba blocks in the MambaFormer module serve as channels to enrich the model with short and long-range dependencies between aspect and opinion words. We also use the Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated feature representation system combining SynGCN and MambaFormer representations. As a filter at the end of the two modules, KAN gated fusion gathers and chooses essential data necessary for the ABSA task. This comprehensive approach significantly enhances the model's ability to understand and analyze sentiment within specific aspects. The main contributions of this paper are as follows:\n\n- We are the first to explore the potential of integrating the Selective Structured State Space model (Mamba), transformer and GCN in ABSA. - We integrate KANs into the gated fusion module to enhance the interaction between the SynGCN and MambaFormer modules. - MambaForGCN is an innovative architecture that improves the capture of short and long-range dependencies between aspect and opinion words in ABSA. It utilizes GCN, Mamba, Transformer, and KAN-gated fusion, effectively capturing complex dependencies and enhancing ABSA performance. - The experimental results on three benchmark datasets (Restaurant14, Laptop14, and Twitter) showcase the effectiveness of the MambaForGCN model, surpassing SOTA baseline models. ## 2 Related Work\n\nIn ABSA, methods related to our study fall into two categories: attention-based models focusing on semantics and graph-based syntactic models or a combination of the two methods. ### 2.1 Semantic-based Models\n\n[1] introduced a deep memory network for aspect-level sentiment classification, emphasizing the importance of individual context words by integrating neural attention models over external memory to capture nuanced sentiment nuances effectively. [2] developed an Attention-based Long Short-Term Memory Network (LSTM) tailored for aspect-level sentiment classification, using attention mechanisms to highlight distinct parts of a sentence based on different aspects. [3] proposed Interactive Attention Networks (IAN) to facilitate interactive learning and generate distinct representations for targets and contexts, enhancing sentiment classification precision. [4] presented a neural network-based framework utilizing a multiple-attention mechanism to capture longrange sentiment features. [5] introduced a multi-grained attention network (MGAN) that used fine-grained attention mechanisms to capture word-level interactions between aspects and context, enhancing classification accuracy. [6] refined target representation and integrated syntactic information into the attention mechanism to better capture the relationship between aspect terms and context. [7] proposed a knowledge-rich solution by augmenting LSTM with a stacked attention mechanism for target and sentence levels. Additionally, a hybrid model combining LSTM and a recurrent additive network\nwas proposed for a joint task of aspect detection and sentiment classification. [8] introduced an attention mechanism alternating between target-level and context-level attention to improving sentiment classification. [9] proposed a component focusing on a multi-head co-attention network model, enhancing bidirectional encoder representations and improving the weighting of adjectives and adverbs. ### 2.2 Syntax-based Models\n\n[10] proposed merging convolution over a dependency tree (CDT) with bi-directional long short-term memory (Bi-LSTM) to analyze sentence structures effectively, introducing GCN enhancements that directly engage with sentence dependency trees. [11] proposed a Graph Convolutional Network (GCN) over a sentence's dependency tree to leverage syntactical information and word dependencies for enhancing aspectspecific sentiment classification. [12] proposed Sentic GCN, a graph convolutional network based on SenticNet, to leverage affective dependencies specific to aspects. By integrating affective knowledge from SenticNet, the enhanced dependency graphs considered both the dependencies of contextual and aspect words and the affective information between opinion words and aspects. [13] introduced KDGN, a knowledgeaware Dependency Graph Network that integrates domain knowledge, dependency labels, and syntax paths into the dependency graph framework, enhancing sentiment polarity prediction in ABSA tasks. [14] EK-GCN is a graph convolutional network enhanced with external knowledge (sentiment lexicons and part-of-speech information), incorporating sentiment score matrices to highlight sentiment word weights and an interaction network to filter relevant sentence information. [15] introduced DGGCN, a dual-gated graph convolutional network enhanced with contextual affective knowledge. DGGCN integrates GCN with a gating mechanism to better interact with the given aspect and context, ensuring comprehensive node information aggregation and improved focus on the aspect. [16] introduced a deformable convolutional network model that leverages phrases for improved sentiment analysis, using deformable convolutions with adaptive receptive fields to capture phrase representations across various contextual distances. The model also incorporated a cross-correlation attention mechanism to capture interdependencies between phrases and words. This paper [17] presented IA-HiNET, a hierarchical network that strengthened the relationships between\naspect words for improved sentence-level sentiment analysis. It uses part-of-speech and positional information, a graph convolution network (GCN) for capturing emotional cues, and an aspect-oriented self-attention mechanism to map related aspect words into the same vector space. An information gate mechanism also filters out irrelevant emotional features, enhancing the analysis. This study [18] introduced APSCL, an AspectPair Supervised Contrastive Learning model, which enhances aspect representation by capturing latent relationships in the sentiment subspace. Through experiments, APSCL effectively narrows the representation discrepancy of aspect pairs in the same relation category while pushing apart those in different categories. This approach optimizes aspect feature representation based on existing label attributes, requiring no additional corpus, thus improving sentiment analysis through enhanced aspect relationships. These advancements illustrate the continuous evolution of methods in ABSA, integrating various forms of attention mechanisms, syntactic information, and external knowledge to improve the accuracy and effectiveness of sentiment classification. ### 2.3 Space State Models and Mamba\n\nState Space Models (SSMs) [19], [20], [21] have emerged as a promising architecture for sequence modeling. S4 is a structured SSM that utilizes the specialized Hippo [20] structure to capture long-range dependencies. Building on S4, Mamba[19] incorporates a selective mechanism to filter out irrelevant information and a hardware-aware algorithm for efficient implementation. Thanks to these innovations, Mamba has shown impressive performance across modalities such as language, audio, and genomics while maintaining linear complexity concerning sequence length. This positions Mamba as a potential alternative to the transformer model. Mamba's modeling capabilities and scalability have led to significant progress in various fields, including computer vision [22], [23], medicine [24], [25], graph theory [26], [27], and recommendation systems [28], [29], time series prediction [30]. A notable area of research involves combining the Transformer and Mamba for language modeling [30], [31], [32], [33]. Comparative studies have shown that Mambaformer is effective in in-context learning tasks. Jamba [32], the first production-grade hybrid model of attention mechanisms and SSMs, features 12 billion active and 52 billion available\nparameters, demonstrating strong performance for long-context data. We are interested in using Mamba to capture long-term dependency. ## 3 Preliminaries\n\n### 3.1 SSM\n\nSSM-based models, such as S4 and Mamba, are based on continuous systems that map a 1-D input sequence $x(t)$ to an output sequence $y(t)$ via a hidden state $h(t)$. This system is defined using parameters $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times 1}$, and $C \\in \\mathbb{R}^{1 \\times N}$ as follows:\n\n$$\n\\begin{gathered}\nh^{\\prime}(t)=A h(t)+B x(t) \\\\\ny(t)=\\operatorname{Ch}(t)\n\\end{gathered}\n$$\n\nS4 and Mamba are discrete adaptations of this continuous system, utilizing a timescale parameter $\\Delta$ to convert the constant parameters $A$ and $B$ into their discrete equivalents $\\bar{A}, \\bar{B}$ through a zero-order hold $(\\mathrm{ZOH})$ transformation:\n\n$$\n\\begin{gathered}\n\\bar{A}=\\exp (\\Delta A) \\\\\n\\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta A\n\\end{gathered}\n$$\n\nthe discrete form of the system, with step size $\\Delta$, is given by:\n\n$$\n\\begin{gathered}\nh_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\ny_{t}=C h_{t}\n\\end{gathered}\n$$\n\nfinally, these models compute the output using a global convolution:\n\n$$\n\\begin{gathered}\n\\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{M-1} \\bar{B}\\right) \\\\\ny=\\chi * K\n\\end{gathered}\n$$\n\nwhere $M$ represents the length of the input sequence $x$, and $\\bar{K} \\in \\mathbb{R}^{M}$ is a structured convolutional kernel. ### 3.2 Kolmogorov-Arnold Networks (KANs)\n\nKANs [34] feature a distinctive architecture that sets them apart from traditional MLPs. Instead of using fixed activation functions at nodes, KANs employ learnable\nactivation functions on the network edges. This fundamental change involves substituting conventional linear weight matrices with adaptive spline functions. These spline functions are parameterized and optimized during training, enabling a more flexible and responsive model architecture that dynamically adapts to complex data patterns. The Kolmogorov-Arnold representation theorem asserts that a multivariate function $f\\left(x_{1}, x_{2} \\ldots, x_{n}\\right)$ can be represented as:\n\n$$\nf\\left(x_{1}, x_{2} \\ldots, x_{n}\\right)=\\sum_{q=1}^{2 n+1} \\boldsymbol{\\Phi}_{q}\\left(\\sum_{p=1}^{n} \\emptyset_{q, p}\\left(x_{p}\\right)\\right)\n$$\n\nIn this context, $\\emptyset_{q, p}$ are univariate functions mapping each input variable $\\left(x_{p}\\right)$ as $\\varnothing_{q, p}:[\\mathbf{0}, \\mathbf{1}] \\rightarrow \\mathbb{R}$, and $\\boldsymbol{\\Phi}_{\\boldsymbol{q}}: \\mathbb{R} \\rightarrow \\mathbb{R}$ are also univariate functions. KANs organize each layer into a matrix of these learnable 1D functions:\n\n$$\n\\begin{gathered}\n\\Phi=\\left\\{\\emptyset_{q, p}\\right\\} \\\\\np=1,2, \\ldots, n_{\\text {in }} \\\\\nq=1,2, \\ldots, n_{\\text {out }}\n\\end{gathered}\n$$\n\nEach function $\\emptyset_{\\boldsymbol{q}, \\boldsymbol{p}}$ is defined as a B-spline, a spline function created from a linear combination of basis splines, which boosts the network's ability to learn complex data representations. In this context, $n_{\\text {in }}$ signifies the number of input features for a given layer, and $n_{\\text {out }}$ indicates the number of output features that layer generates, reflecting the dimensional changes throughout the network layers. The activation functions $\\emptyset_{l, i, j}$ within this matrix are implemented as trainable spline functions, formulated as:\n\n$$\n\\operatorname{spline}(x)=\\sum_{i} c_{i} B_{i}(x)\n$$\n\nThis formulation enables each $\\emptyset_{l, i, j}$ to adjust its shape according to the data, providing unparalleled flexibility in how the network captures input interactions. The overall architecture of a KAN resembles stacking layers in MLPs, but it goes further by employing complex functional mappings instead of fundamental linear transformations and nonlinear activations. $$\n\\operatorname{KAN}(x)=\\left(\\Phi_{L-1} \\circ \\Phi_{L-2} \\circ \\ldots \\circ \\Phi_{0}\\right)(x)\n$$\n\n## 4 Overview of our Proposed Model Framework\n\nWe introduce a novel MambaForGCN technique for capturing long-range dependencies between aspect and opinion words. This innovative approach incorporates a SynGCN module to encode the input with dependency relations. This innovative approach also incorporates a MambaFormer module for enriching the model with semantic information, where MHA weights are computed following the transformer's standard process [35]. The MHA and Mamba blocks serve as channels to enrich the model with short and long-range dependencies between aspect and opinion words. We also use the KAN gated fusion, an adaptively integrated feature representation system combining the syntax-based GCN module and MambaFormer representations. As a filter at the end of the two modules, gated fusion gathers and chooses essential data necessary for the ABSA task. This comprehensive approach significantly enhances the model's ability to understand and analyze sentiment within specific aspects. Our method enables the model to capture long-range dependencies between the element and the sentence accurately. ![](https://cdn.mathpix.com/cropped/2024_09_12_08783485676052d94005g-10.jpg?height=763&width=1108&top_left_y=1509&top_left_x=474)\n\nFig. 1. MambaForGCN complete architecture\n\n### 4.1 Embedding Module\n\nGiven a sentence-aspect pair $(s, a)$, where the sentence $s \\equiv\\left\\{w_{1}, w_{2}, \\ldots, w_{n}\\right\\}$ comprises a sequence of words, and the aspect $a \\equiv\\left\\{a_{1}, a_{2}, \\ldots, a_{m}\\right\\}$ is a subset of this sentence. We use a BiLSTM or BERT model as the sentence encoder to obtain hidden contextual representation. Initially, each word in the sentence is converted into a lowdimensional vector using an embedding matrix $E \\in \\mathbb{R}^{|V| \\times d_{e}}$, where $|V|$ represents the vocabulary size and $d_{e}$ is the dimensionality of the word embeddings. Thus, the sentence $s$ is represented by a series of word embeddings $x \\equiv\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}$. These word embeddings are then fed into a BiLSTM to generate hidden state vectors, where each hidden state $h_{i} \\in \\mathbb{R}^{2 d}$ captures contextual information. Within $H$, the subsequence $h_{a} \\equiv\\left\\{h_{a 1}, h_{a 2}, \\ldots, h_{a m}\\right\\}$ corresponds to the aspect term representation. This hidden state matrix $H$ is then used as the initial node representation in the MambaForGCN. For the BERT encoder, the input is formatted as \"[CLS] sentence [SEP] aspect [SEP].\" This allows BERT to process the sentence and aspect jointly, leveraging its robust contextual embeddings to capture complex relationships between opinion words and the aspect. ### 4.2 Syntax-based GCN Module\n\nThe SynGCN module utilizes syntactic information as its input. Instead of relying on the final discrete output from a traditional dependency parser, we encode syntactic information using a probability matrix that represents all possible dependency arcs like in [36]. This method captures a broader range of syntactic structures, providing a more detailed and flexible understanding of sentence syntax. By considering the likelihood of multiple dependency arcs, this approach reduces the impact of potential errors in dependency parsing. We employ the [37], a cutting-edge model in the field of dependency parsing, to generate this probability matrix. The LAL-Parser's output is a probability distribution over all possible dependency arcs, effectively encapsulating the latent syntactic relationships within a sentence. This comprehensive syntactic encoding allows SynGCN to better model and understand complex grammatical structures. The SynGCN module uses a syntactic adjacency matrix $A^{\\text {syn }} \\in \\mathbb{R}^{n \\times n}$ to process the hidden state vectors $H$ from the BiLSTM, which act as the initial node representations in the syntactic graph. Through the SynGCN module, the syntactic graph representation $H^{\\text {syn }}=\\left\\{h_{1}^{\\text {syn }}, h_{2}^{\\text {syn }}, \\ldots, h_{n}^{\\text {syn }}\\right\\}$ is derived using Equation (1). In this context, $h_{i}^{\\text {syn }} \\in R^{d}$ represents the hidden state of the $i^{t h}$ node. $$\nh_{i}^{l}=\\sigma\\left(\\sum_{j=1}^{n} A_{i j} W^{l} h_{j}^{l-1}+b^{l}\\right)\n$$\n\n### 4.3 Mambaformer Module\n\nThis module consists of two blocks, namely MHA and Mamba. They extract textual semantic information related to the given sentence and aspect. The MHA captures short-range dependencies, while the Mamba block captures long-range dependencies between aspect and opinion words. Masked Multihead Attention Block: To extract important textual semantic information related to the given sentence and aspect, specifically for short-range word dependencies, we employ the MHA mechanism as shown in Fig. 1. In the MHA block, computation adheres to the standard process of transformer architecture. The first step in computing attention weights score is to take the dot product of the keys $K$ and queries $Q$. Next, another dot product between and the values $V$ yields the output representation $H^{\\text {mha }}$ ' of the attention module. Below is an outline of this method:\n\n$$\n\\begin{gathered}\nK, Q, V=h_{j}^{l-1} W_{k}, h_{j}^{l-1} W_{q}, h_{j}^{l-1} W_{v} \\\\\n\\text { score }=\\frac{\\operatorname{softmax}(\\mathrm{QK}+\\text { Mask })}{\\sqrt{d_{k}}} \\\\\nH^{\\text {mha' }}=\\operatorname{score} \\cdot V \\\\\nH^{\\text {mha }}=\\operatorname{LayerNorm}\\left(H^{\\text {mhal }}+h\\right)\n\\end{gathered}\n$$\n\nThe trainable parameters $W_{v}, W_{k}$, and $W_{q}$ are used to calculate the values, keys, and queries, represented by $K, Q$, and $V$, respectively. A pre-trained masking language modeling matrix is defined by Mask, and the dimension of $K^{l}$ is indicated by $d_{k}$. score denotes the attention weight matrix sized $N \\times N$, where each entry determines the weight assigned to a token in the sentence and aspect for computing the representation of another token. Mamba Block: Although transformers have proven effective in capturing dependency, their quadratic complexity of attention mechanism prevents their further adoption in long-range word dependencies, thus limiting them to the short-range range. To tackle this problem, we utilize the Mamba block. This approach ensures that essential connections and long-range dependencies between aspect word features and semantic emotional features are maintained throughout the analysis. As seen in Fig. 1, the Mamba block is designed for sequence-to-sequence tasks with consistent input and output dimensions. It expands the input $H^{\\text {mha }}$ through two linear projections. One projection involves a convolutional layer and SiLU activation before passing through an SSM module, which filters irrelevant information and selects inputdependent knowledge. Simultaneously, another projection path with SiLU activation serves as a residual connection, combining its output with the SSM module's result via a multiplicative gate. Ultimately, the Mamba block outputs $H^{\\text {mam }}$ in $H^{\\text {mam }} \\in \\mathbb{R}^{B \\times L \\times D}$ through a final linear projection, providing enhanced sequence processing capabilities. Finally, $H^{\\text {sem }}$ represents the output of the MambaFormer module after applying layer normalization to the sum of the outputs from the Mamba and MHA layers. $$\n\\begin{gathered}\nH^{\\text {mam } 1}=\\operatorname{SiLU}\\left(\\operatorname{Conv} 1 D\\left(\\operatorname{Linear}\\left(H^{\\text {mha }}\\right)\\right)\\right) \\\\\nH^{\\text {mam } 2}=\\operatorname{SiLU}\\left(\\operatorname{Linear}\\left(H^{\\text {mha }}\\right)\\right) \\\\\nH^{\\text {mam } 3}=\\operatorname{Linear}\\left(\\operatorname{SSM}\\left(H^{\\text {mam } 1}\\right) \\otimes H^{\\text {mam } 2}\\right) \\\\\nH^{\\text {mam }}=\\operatorname{Linear}\\left(h^{\\text {mam } 3}\\right) \\\\\nH^{\\text {sem }}=\\operatorname{LayerNorm}\\left(H^{\\text {mam }}+H^{\\text {mha }}\\right)\n\\end{gathered}\n$$\n\n### 4.4 KAN Gated Fusion Module\n\nGated fusion has demonstrated effectiveness in language modeling tasks[38] [39]. To dynamically assimilate valuable insights from the syntax-based GCN and MambaFormer, we used a KAN-gated fusion module to reduce interference from unrelated data. Gating is a potent mechanism for assessing the utility of feature representations and orchestrating information aggregation accordingly. This module uses a simple addition-based fusion mechanism to achieve gating, which controls the flow of information through gate maps, as shown in Fig. 1. Specifically, the representations $H^{\\text {syn }}$ and $H^{\\text {sem }}$ are associated with gate maps Gate ${ }^{s y n}$ and Gate ${ }^{\\text {sem }}$, respectively. These\ngate maps originate from a KAN using a one-dimensional layer. These gate maps are used to provide technical specifications for the gated fusion process:\n\n$$\n\\begin{aligned}\nG_{a t e^{s y n}} & =\\sigma\\left(\\operatorname{KAN}\\left(H^{\\text {syn }}\\right)\\right) \\\\\n\\text { Gate }^{\\text {sem }} & =\\sigma\\left(\\operatorname{KAN}\\left(H^{\\text {sem }}\\right)\\right) \\\\\nH^{c}=\\text { Gate }^{\\text {syn }} H^{\\text {syn }} & +\\left(1-\\text { Gate }^{\\text {syn }}\\right) \\text { Gate }^{\\text {sem }} H^{\\text {sem }}\n\\end{aligned}\n$$\n\nWe employ mean pooling to condense contextualized embeddings $H^{c}$, which assists downstream classification tasks. Following this, we apply a linear classifier to generate logits. Finally, softmax transformation converts logits into probabilities, facilitating ABSA. Each component is pivotal in analyzing input text for ABSA tasks from the embedding layer to the sentiment classification layer. $$\n\\begin{gathered}\nH^{m p}=\\operatorname{MeanPooling}\\left(H^{c}\\right) \\\\\np(a)=\\operatorname{softmax}\\left(W_{p} H^{m p}+b_{p}\\right)\n\\end{gathered}\n$$\n\n$W_{p}$ and $b_{p}$ represents the trainable parameters, consisting of learnable weights and biases. ### 4.5 Training\n\nWe utilize the standard cross-entropy loss as our primary objective function:\n\n$$\nL(\\theta)=-\\sum_{(s, a) \\in D} \\sum_{c \\in C} \\log p(a)\n$$\n\ncomputed over all sentence-aspect pairs in the dataset $D$. For each pair $(s, a)$, representing a sentence $(s)$ with $\\operatorname{aspect}(a)$, we compute the negative log-likelihood of the predicted sentiment polarity $p(a)$. Here, $\\theta$ encompasses all trainable parameters, and $(C)$ denotes the collection of sentiment polarities. ## 5 Experiment\n\n### 5.1 Datasets\n\nThree publicly available sentiment analysis datasets are used in our experiments: the Twitter dataset used by [40] the Laptop and Restaurant 14 review datasets from the SemEval 2014 Task [41]. Table 1 provides comprehensive statistics for these datasets. Table 1. Statistics of three benchmark datasets\n\n| Dataset | Division | Positive | Negative | Neutral |\n| :--- | :--- | :--- | :--- | :--- |\n| Restaurants14 | Train | 2164 | 807 | 637 |\n|  | Test | 727 | 196 | 196 |\n| Laptop14 | Train | 976 | 851 | 455 |\n|  | Test | 337 | 128 | 167 |\n| Twitter | Train | 1507 | 1528 | 3016 |\n|  | Test | 172 | 169 | 336 |\n\n### 5.2 Implementation\n\nA ready-made parser is available for dependency parsing: the LAL-Parser [37]. The word embeddings are initialized for each trial with 300-dimensional Glove vectors that have been pretrained [42]. Position and part-of-speech (POS) embeddings have dimensionalities of 30 for each position (i.e., each word's relative position in a sentence about the aspect). Therefore, the word, POS, and position embeddings are concatenated and subsequently fed into a BiLSTM model with a hidden size of 50 . Our solution to reduce overfitting is to apply dropout to the BiLSTM's input word embeddings at a rate of 0.7. The number of layers for both the SynGCN and the MambaFormer is set to 2, and the dropout rate of the SynGCN modules is set to 0.1 . The MHA layer in the MambaFormer module incorporates a dropout of 0.05 and 4 heads. We configure the Mamba layer with 2 convolutional filters and a state vector with 16 dimensions. This configuration can effectively model long-range interdependence and complex temporal patterns while retaining computational efficiency. A uniform distribution is used to initialize every model weight. We employ the Adam optimizer [43] with a 0.002 learning rate. The model is trained with a batch size of 16 over 50 epochs. For MambaForGCN+BERT we use the pre-trained BERT model to extract word representations from the last hidden states [44]. Several simplified versions of MambaForGCN have also been explored: Mamba4ABSA denotes that we keep Mamba and remove the MHA block and the SynGCN module. MambaFormer denotes that we keep MambaFormer module and remove the SynGCN module. We provide practical and scalable training by implementing the model with the PyTorch framework. ### 5.3 Baseline Comparisons\n\nWe compare our model's results with SOTA baselines to thoroughly assess the performance of our model:\n\n## Attention-based models\n\n1 ATAE-LSTM[2]: The attention mechanism focuses the model's attention on the sentence's most crucial phrase. 2 IAN[3]: Two LSTMs and an interactive attention mechanism produce representations for aspects and phrases. 3 RAM[4]: Uses a memory network with recurrent attention processes to acquire phrase representations. 4 MGAN[5]: The model uses attention mechanisms with different levels of granularity to understand how aspects and contexts interact. 5 AEN [45]: The study presents an Attentional Encoder Network (AEN) to model the relationship between context and target without depending on recurrence. 6 Coattention-Memnet [8]: Introduced a coattention mechanism alternating between target-level and context-level attention to improve sentiment classification. 7 DCN-CA [16]: A deformable convolutional network model improves sentiment analysis using adaptive receptive fields and a cross-correlation attention mechanism to capture and integrate phrase representations and interdependencies. 8 CDT [10]: Learns aspect representations with syntactic information using a GCN over a dependency tree. 9 ASGCN-DT [11]: A Graph Convolutional Network (GCN) was proposed to operate over the dependency tree of a sentence to utilize syntactical information and word dependencies. 10 DGEDT [46]: Proposes enhancing the dual-transformer network with a dependency graph by considering both the graph-based representations from the dependency graph and the flat representations from the transformer. 11 Sentic-GCN [12]: Sentic-GCN leverages SenticNet's affective dependencies to enhance aspect-specific sentiment classification by integrating contextual and affective information into dependency graphs. 12 EK-GCN [14]: EK-GCN enhances sentiment classification by integrating sentiment lexicons, part-of-speech data, sentiment scores, and an interaction network. 13 DGGCN [15]: A dual-gated graph convolutional network enhanced with contextual affective knowledge. 14 IA-HiNET [17]: IA-HiNET enhances sentence-level sentiment analysis by using partof-speech and positional information, a GCN for emotional cues, aspect-oriented selfattention for mapping aspect words, and an information gate to filter irrelevant features. 15 APSCL [18]: APSCL improves ABSAs by using supervised contrastive learning to enhance aspect representation, narrowing discrepancies within the same relation category and optimizing features based on existing labels without additional data. 16 BERT [44]: The input format for the BERT model's text processing is \"[CLS] sentence [SEP] aspect [SEP]\". Table 2. Experimental results comparison on three publicly available datasets\n\n| Model | Restaurant14 |  | Laptop14 | Twitter |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Acc. | F1 | Acc. | F1 | Acc. | F1 |\n| ATAE-LSTM [2] | 77.20 | - | 68.70 | - | - | - |\n| IAN [3] | 78.60 | - | 72.10 | - | - | - |\n| RAM [4] | 80.23 | 70.80 | 74.49 | 71.35 | 69.36 | 67.30 |\n| MGAN [5] | 81.25 | 71.94 | 75.39 | 72.47 | 72.54 | 70.81 |\n| AEN [45] | 80.98 | 72.14 | 73.51 | 69.04 | 72.83 | 69.81 |\n| Coattention-Memnet [8] | 79.70 |  | 72.9 |  | 70.5 |  |\n| DCN-CA [16] | 83.96 | 76.84 | 77.85 | 73.65 | 75.48 | 74.98 |\n| CDT [10] | 82.30 | 74.02 | 77.19 | 72.99 | 74.66 | 73.66 |\n| ASGCN-DT [11] | 80.86 | 72.19 | 74.14 | 69.24 | 71.53 | 69.68 |\n| DGEDT [45] | 83.90 | 75.10 | 76.80 | 72.30 | 74.80 | 73.40 |\n| Sentic-GCN [12] | 84.03 | 75.38 | 77.90 | 74.71 |  |  |\n| EK-GCN [14] | 83.96 | 74.93 | 78.46 | 76.54 | 75.84 | 74.57 |\n| DGGCN [15] | 83.66 | 76.73 | 75.70 | 72.57 | 74.87 | 72.27 |\n| IA-HiNET [17] | 83.58 | 75.85 | 78.24 | 74.54 | 75.79 | 74.61 |\n| APSCL [18] | 83.37 | 77.31 | 77.14 | 73.86 |  |  |\n| Mamba4ABSA | 82.11 | 74.72 | 76.98 | 73.11 | 74.12 | 73.50 |\n| MambaFormer | 82.93 | 75.33 | 77.53 | 73.42 | 74.47 | 73.86 |\n| MambaForGCN (ours) | $\\mathbf{8 4 .",
    "mamba4gcn-2": "3 8}$ | $\\mathbf{7 7 . 4 7}$ | $\\mathbf{7 8 . 6 4}$ | $\\mathbf{7 6 .",
    "mamba4gcn-3": "6 1}$ | $\\mathbf{7 5 . 9 6}$ | $\\mathbf{7 4 . 7 7}$ |\n| BERT [44] | 85.79 | 80.09 | 79.91 | 76.00 | 75.92 | 75.18 |\n| KDGN+BERT [13] | 85.79 | 80.09 | 79.91 | 76.00 | 75.92 | 75.18 |\n| EK-GCN+BERT [14] | 87.01 | 81.94 | 81.32 | 77.59 | 77.64 | 75.55 |\n| DGGCN+BERT [15] | $\\mathbf{8 7 . 6 5}$ | 82.55 | 81.30 | 79.19 | 75.89 | 75.16 |\n| DCN-CA+BERT [16] | 86.89 | 80.32 | 81.50 | 78.51 | 76.94 | 75.07 |\n| A-HiNET+BERT [17] | 87.72 | 82.65 | 81.53 | 77.97 | 77.59 | 76.85 |\n| APSCL+BERT [18] | 86.79 | $\\mathbf{8 1 . 8 4}$ | 79.45 | 76.56 | 75.88 | 75.36 |\n| MambaForGCN+BERT | 86.68 | 80.86 | $\\mathbf{8 1 .",
    "mamba4gcn-4": "8 0}$ | $\\mathbf{7 8 . 5 9}$ | $\\mathbf{7 7 . 6 7}$ | $\\mathbf{7 6 . 8 8}$ |\n\n### 5.4 Experimental Results\n\nThe accuracy and macro-averaged F1-score serve as the primary evaluation criteria for the ABSA models. Table 2 displays the comparison's findings with each baseline model. First, MambaForGCN significantly improves sentiment classification accuracy compared to the syntax-based models DGEDT, Sentic-GCN, DGGCN, and the semantic-based model DCN-CA. This suggests that MambaForGCN's Mamba and MHA\nblocks help better capture long and short-range dependencies between aspect and opinion word relationships. Second, the gain in accuracy on three datasets indicates that the KAN gated fusion effectively filters noise and promotes information flow between SynGCN and MambaFormer modules in ABSA. Lastly, we can see that the fundamental BERT has outperformed specific ABSA models by a considerable margin. When our MambaForGCN is combined with BERT, the outcomes demonstrate that this potent model's efficacy is further enhanced. ### 5.5 Ablation Study\n\nTable 3. Results of an ablation study on three benchmark datasets (\\%). | Model | Restaurant14 | Laptop14 | Twitter |\n| :---: | :---: | :---: | :---: |\n|  | Acc.",
    "mamba4gcn-5": "| Acc. | Acc. |\n| MambaForGCN | 84.38 | 78.64 | 75.96 |\n| w/o MHA | 82.79 | 77.21 | 74.83 |\n| w/o Mamba | 82.67 | 76.96 | 74.55 |\n| w/o KAN gated fusion | 82.48 | 77.06 | 74.15 |\n\nWe performed ablation experiments on the datasets to examine the effects of various components in our MambaForGCN model on performance. The phrase \"w/o MHA\" describes how the MHA block in the MambaFormer module has been removed. This entails using the representation from the mamba layer and GCN module. Similarly, \"w/o mamba\" involves excluding the Mamba block from the MambaFormer module, thereby using MHA and GCN module. Additionally, w/o gated fusion indicates using a fully connected network to integrate representations from the two modules without employing the KAN fusion gate. The results are depicted in Table 3. Notably, without MHA, the performance of MambaForGCN experiences a decrease of $1.59 \\%, 1.43 \\%$, and $1.13 \\%$ for the Restaurant, Laptop, and Twitter datasets, respectively. Furthermore, the MHA layer's representation in the MambaFormer module must be integrated with the mamba layer's representation in the MambaFormer module, as the performance of MambaForGCN decreases by $1.71 \\%, 1.68 \\%$ and $1.41 \\%$, respectively, when solely relying on MHA in MambaFormer module. Finally, MambaForGCN performance drops by $1.90 \\%, 1.58 \\%$, and $1.81 \\%$ when a primary, fully connected network is substituted for the gated fusion module. Overall, MambaForGCN\nperforms better in capturing short and long-range dependencies between aspect and opinion words for ABSA when all components are effectively combined. It can adaptively integrate two features (syntax and semantic) from the GCN and MambaFormer. ### 5.6 Effect of MambaForGCN Layer Number\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_08783485676052d94005g-20.jpg?height=915&width=1198&top_left_y=796&top_left_x=429)\n\nFig. 2. Effect of different numbers of MambaForGCN layers. ### 5.7 Effect of A3SN Layer Number\n\nIn our investigation, as depicted in Fig. 3, we observed that both the Laptop and Restaurant datasets yielded the best results with two layers. When the number of layers is too low, dependency information won't be adequately communicated. When the number of layers in the model is too high, it becomes overfit and redundant information passes through, which lowers performance. Many trials must be carried out to determine an appropriate layer number. ### 5.8 Effect of Different MambaForGCN Head Numbers\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_08783485676052d94005g-21.jpg?height=909&width=1194&top_left_y=622&top_left_x=431)\n\nFig. 3. Effect of different numbers of MambaForGCN heads. Illustrating with examples from the Restaurant and Laptop domains, we examine the effect of varying head numbers in the MHA mechanism of our models. As depicted in Fig. 4, optimal performance is observed when the head number is set to 4 . This observation aligns with our intuition, suggesting that decreasing or increasing the head number results in a model that becomes prone to overfitting. This interpretation is grounded in the inherent characteristics of MHA, where an optimal balance is struck between complexity and generalization, leading to enhanced model performance. ## 6 Conclusion\n\nThis paper proposes the MambaForGCN framework, which integrates syntactic structure and semantic information for the ABSA tasks. We utilise SynGCN to enrich the model with syntactic knowledge. Then, we merge the selective space model (Mamba)\nand transformer to extract semantic information from the input and capture short and long-range dependencies. Furthermore, we fuse these modules with a KAN-gated feature fusion to maximize their interaction and filter out irrelevant information. The outcomes of our experiments show that our method works well on three publicly available datasets. Funding. The Chinese government scholarship supported this work. Data availability. Data will be made available at a reasonable request. Disclosure of Interests. The authors have no competing interests to declare relevant to this article's content. ## References\n\n[1] D. Tang, B. Qin, and T. Liu, \"Aspect Level Sentiment Classification with Deep Memory Network,\" 2016. [2] Y. Wang, M. Huang, L. Zhao, and X. Zhu, \"Attention-based LSTM for Aspectlevel Sentiment Classification,\" 2016. [3] D. Ma, S. Li, X. Zhang, and H. Wang, \"Interactive Attention Networks for Aspect-Level Sentiment Classification,\" Sep. 2017, [Online]. Available: http://arxiv.org/abs/1709.00893\n[4] C. Peng, S.",
    "mamba4gcn-6": "Zhongqian, B. Lidong, and W. Yang, \"Recurrent Attention Network on Memory for Aspect Sentiment Analysis,\" 2017.",
    "mamba4gcn-7": "[5] F.",
    "mamba4gcn-8": "Fan, Y. Feng, and D. Zhao, \"Multi-grained Attention Network for AspectLevel Sentiment Classification,\" 2018.",
    "mamba4gcn-9": "[6] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, \"Effective Attention Modeling for Aspect-Level Sentiment Classification,\" 2018.",
    "mamba4gcn-10": "[7] Y. Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, \"Sentic LSTM: a Hybrid Network for Targeted Aspect-Based Sentiment Analysis,\" Cognit Comput, vol.",
    "mamba4gcn-11": "10, no. 4, pp. 639-650, Aug. 2018, doi: 10.1007/s12559-018-9549-x. [8] C. Yang, H. Zhang, B. Jiang, and K. Li, \"Aspect-based sentiment analysis with alternating coattention networks,\" Inf Process Manag, vol.",
    "mamba4gcn-12": "56, no. 3, pp. 463478, May 2019, doi: $10.1016 /$ j.ipm.2018.12.004. [9] L. C. Cheng, Y. L. Chen, and Y. Y. Liao, \"Aspect-based sentiment analysis with component focusing multi-head co-attention networks,\" Neurocomputing, vol.",
    "mamba4gcn-13": "489, pp. 9-17, Jun. 2022, doi: 10.1016/j.neucom.2022.03.027. [10] K. Sun, R. Zhang, S. Mensah, Y. Mao, and X. Liu, \"Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree,\" 2019. [11] C.",
    "mamba4gcn-14": "Zhang, Q. Li, and D. Song, \"Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks.\" [Online]. Available: https://spacy.io/. [12] B. Liang, H. Su, L. Gui, E. Cambria, and R. Xu, \"Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks,\" Knowl Based Syst, vol.",
    "mamba4gcn-15": "235, Jan. 2022, doi: 10.1016/j.knosys.2021.107643. [13] H. Wu, C. Huang, and S. Deng, \"Improving aspect-based sentiment analysis with Knowledge-aware Dependency Graph Network,\" Information Fusion, vol.",
    "mamba4gcn-16": "92, pp. 289-299, Apr. 2023, doi: 10.1016/j.inffus.2022.12.004. [14] T. Gu, H. Zhao, Z. He, M. Li, and D. Ying, \"Integrating external knowledge into aspect-based sentiment analysis using graph neural network,\" Knowl Based Syst, vol.",
    "mamba4gcn-17": "259, Jan. 2023, doi: 10.1016/j.knosys.2022.110025. [15] H. Liu et al., \"Enhancing aspect-based sentiment analysis using a dual-gated graph convolutional network via contextual affective knowledge,\" Neurocomputing, vol. 553, Oct. 2023, doi: 10.1016/j.neucom.2023.126526. [16] C. Zhu, B. Yi, and L. Luo, \"Base on contextual phrases with cross-correlation attention for aspect-level sentiment analysis,\" Expert Syst Appl, vol. 241, May 2024, doi: 10.1016/j.eswa.2023.122683. [17] T. Gu, H. Zhao, and M. Li, \"Effective inter-aspect words modeling for aspectbased sentiment analysis,\" Applied Intelligence, vol.",
    "mamba4gcn-18": "53, no. 4, pp. 4366-4379, Feb. 2023, doi: 10.1007/s10489-022-03630-0. [18] P. Li, P. Li, and X. Xiao, \"Aspect-Pair Supervised Contrastive Learning for aspect-based sentiment analysis,\" Knowl Based Syst, vol.",
    "mamba4gcn-19": "274, Aug. 2023, doi: 10.1016/j.knosys. 2023.110648\n[19] A. Gu and T. Dao, \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces,\" Dec. 2023, [Online]. Available: http://arxiv.org/abs/2312.00752\n[20] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9, \"HiPPO: Recurrent Memory with Optimal Polynomial Projections,\" 2020. [Online]. Available: https://github.com/HazyResearch/hippo-code. [21] A. Gu, K. Goel, and C. R\u00e9, \"Efficiently Modeling Long Sequences with Structured State Spaces,\" Oct. 2021, [Online]. Available: http://arxiv.org/abs/2111.00396\n[22] Y. Tang, P. Dong, Z. Tang, X. Chu, and J. Liang, \"VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting,\" Mar. 2024, [Online]. Available: http://arxiv.org/abs/2403.16536\n[23] L. Zhu, B. Liao, Q. Zhang, X.",
    "mamba4gcn-20": "Wang, W. Liu, and X. Wang, \"Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model,\" Jan. 2024, [Online]. Available: http://arxiv.org/abs/2401.09417\n[24] J. Ma, F. Li, and B. Wang, \"U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation,\" Jan. 2024, [Online]. Available: http://arxiv.org/abs/2401.04722\n[25] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu, \"SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation,\" Jan. 2024, [Online]. Available: http://arxiv.org/abs/2401.13560\n[26] A. Behrouz and F. Hashemi, \"Graph Mamba: Towards Learning on Graphs with State Space Models,\" Feb. 2024, [Online]. Available: http://arxiv.org/abs/2402.08678\n[27] C. Wang, O. Tsepa, J. Ma, and B. Wang, \"Graph-Mamba: Towards LongRange Graph Sequence Modeling with Selective State Spaces,\" Feb. 2024, [Online]. Available: http://arxiv.org/abs/2402.00789\n[28] C. Liu, J. Lin, J. Wang, H. Liu, and J. Caverlee, \"Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models,\" Mar. 2024, [Online]. Available: http://arxiv.org/abs/2403.03900\n[29] J. Yang et al., \"Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation,\" Mar.",
    "mamba4gcn-21": "2024, [Online]. Available: http://arxiv.org/abs/2403.16371\n[30] X. Xu, Y. Liang, B. Huang, Z. Lan, and K. Shu, \"Integrating Mamba and Transformer for Long-Short Range Time Series Forecasting,\" Apr. 2024, [Online]. Available: http://arxiv.org/abs/2404.14757\n[31] M. Fathi, J. Pilault, O. Firat, C. Pal, P.-L. Bacon, and R. Goroshin, \"Block-State Transformers,\" Jun. 2023, [Online]. Available: http://arxiv.org/abs/2306.09539\n[32] O. Lieber et al., \"Jamba: A Hybrid Transformer-Mamba Language Model,\" Mar. 2024, [Online]. Available: http://arxiv.org/abs/2403.19887\n[33] J. Park et al., \"Can Mamba Learn How to Learn? A Comparative Study on InContext Learning Tasks,\" Feb. 2024, [Online]. Available: http://arxiv.org/abs/2402.04248\n[34] Z. Liu et al., \"KAN: Kolmogorov-Arnold Networks,\" Apr. 2024, [Online]. Available: http://arxiv.org/abs/2404.19756\n[35] A. Vaswani et al., \"Attention Is All You Need,\" Jun. 2017, [Online]. Available: http://arxiv.org/abs/1706.03762\n[36] R. Li, H. Chen, F. Feng, Z.",
    "mamba4gcn-22": "Ma, X. Wang, and E. Hovy, \"Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis.\" [Online]. Available: https://github.com/CCChenhao997/DualGCN-ABSA\n[37] K.",
    "mamba4gcn-23": "Mrini, F. Dernoncourt, Q. Tran, T. Bui, W. Chang, and N. Nakashole, \"Rethinking Self-Attention: Towards Interpretability in Neural Parsing,\" Nov.",
    "mamba4gcn-24": "2019, [Online]. Available: http://arxiv.org/abs/1911.03875\n[38] A. Lawan, J. Pu, H. Yunusa, J. Muhammad, and A. Umar, \"Amplifying AspectSentence Awareness: A Novel Approach for Aspect-Based Sentiment Analysis,\" 2024. doi: 10.48550/arXiv.2405.13013. [39] Y. Zhao, T. Xia, Y. Jiang, and Y. Tian, \"Enhancing inter-sentence attention for Semantic Textual Similarity,\" Inf Process Manag, vol.",
    "mamba4gcn-25": "61, no. 1, Jan. 2024, doi: 10.1016/j.ipm. 2023.103535. [40] L. Dong, F. Wei, C. Tan, D. Tang, M. Zhou, and K. Xu, \"Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,\" Association for Computational Linguistics, 2014. [41] M. Pontiki, H. Papageorgiou, D. Galanis, I. Androutsopoulos, J. Pavlopoulos, and S. Manandhar, \"SemEval-2014 Task 4: Aspect Based Sentiment Analysis,\" 2014.",
    "mamba4gcn-26": "[Online]. Available: http://alt.qcri. [42] J. Pennington, R. Socher, and C. D. Manning, \"GloVe: Global Vectors for Word Representation.\" [Online]. Available: http://nlp. [43] D. P. Kingma and J. Ba, \"Adam: A Method for Stochastic Optimization,\" Dec. 2014, [Online]. Available: http://arxiv.org/abs/1412.6980\n[44] J. Devlin, M.-W.",
    "mamba4gcn-27": "Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\" Oct. 2018, [Online]. Available: http://arxiv.org/abs/1810.04805\n[45] Y. Song, J. Wang, T. Jiang, Z. Liu, and Y. Rao, \"Attentional Encoder Network for Targeted Sentiment Classification,\" Feb.",
    "mamba4gcn-28": "2019, doi: 10.1007/978-3-03030490-4_9. [46] H. Tang, D. Ji, C. Li, and Q. Zhou, \"Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification.\"\n\n"
}