{
    "attnalignnfpe-0": "# Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation \n\nTa-Chung Chi<br>Carnegie Mellon University<br>tachungc@andrew.cmu.edu\n\nTing-Han Fan<br>Independent Researcher<br>tinghanf@alumni.princeton.edu\n\nAlexander I. Rudnicky<br>Carnegie Mellon University<br>air@cs.cmu.edu\n\n\n#### Abstract\n\nAn ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any fine-tuning. Such long-context utilization capability relies heavily on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings show improvement on the long-context utilization capability of T 5 on language modeling, retrieval, multi-document question answering, and code completion tasks without any fine-tuning. This suggests that a flexible positional embedding design and attention alignment can go a long way toward Transformer length extrapolation. ${ }^{1}$\n\n\n## 1 Introduction\n\nPre-training large Transformer language models on long sequences is inherently expensive due to self-attention's quadratic complexity w.r.t the input sequence length (Vaswani et al., 2017). Even with the help of memory-efficient attention (Rabe and Staats, 2021; Dao et al., 2022), the maximum supported input length of current open-source pretrained Transformer language models are capped at 4,096 tokens (Touvron et al., 2023), limiting their efficacy in handling long-context tasks. One notable research topic aiming to lift the input length restriction is Length Extrapolation (Press et al., 2022). Ideally, a lengthextrapolatable Transformer language model is trained on short sequences and can perform equally\n\n[^0]| Retrieval Tasks |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Criteria | Topic |  | Line |  | Passkey |  |\n|  | 512 | 15 k | 512 | 15k | 512 | 15k |\n| $\\mathrm{P}_{\\max }$ | 0.28 | 0.12 | 0.27 | 0.11 | 0.32 | 0.24 |\n| H | 3.47 | 6.63 | 3.47 | 7.04 | 3.09 | 5.97 |\n\nTable 1: The Dispersed Attention Issue of Flan-T5-\nXL Encoder. $P_{\\text {max }}$ is the average maximum probability and H is the average entropy. After increasing the sequence length from 512 to 15 k , we observe larger entropy and smaller maximum probability, implying a flatter self-attention distribution. well on longer ones without any further fine-tuning. This is made possible with carefully designed positional embeddings (Press et al., 2022; Chi et al., 2022, 2023). Unfortunately, existing approaches are tailored for natural language modeling, a task known to have strong recency bias, and they often do not perform well on other seemingly simple tasks such as passkey, topic, and line retrieval (Mohtashami and Jaggi, 2023; Li et al., 2023). To circumvent the recency bias, we sift through the positional embeddings of existing open-source large pre-trained Transformer language models, shown in Table 2, to find a flexible design, and the T5 family (Raffel et al., 2020) comes to our attention. As visualized in Figure 1, the flexibility of T5's positional embeddings allows it to encourage recency bias on one head and discourage that on another head. However, there is no free lunch: T5 suffers from the dispersed attention issue as shown in Table 1. That is, the attention distributions of long input sequences tend to be flatter than those of short input sequences. As a remedy, we propose two fine-tuning-free attention alignment strategies via Softmax temperature scaling (Yao et al., 2021; $\\mathrm{Su}, 2021$ ) to mitigate the dispersed attention issue: maximum probability $\\left(\\mathrm{P}_{\\max }\\right)$ and entropy $(\\mathrm{H})$ alignment. We validate the effectiveness of our alignment\n\n| Models | T5 (2020) | OPT (2022) | ChatGLM (2022) | LLaMA (2023) | Falcon (2023) | Pythia (2023) | XGen (2023) | BLOOM (2022) | MPT (2023) |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| PE. | Learned | Learned | Rotary | Rotary | Rotary | Rotary | Rotary | ALiBi | ALiBi |\n|  | Relative | Absolute | Relative | Relative | Relative | Relative | Relative | Relative | Relative |\n\nTable 2: Open-source Transformer language models and their positional embeddings. T5 is the only model equipped with learnable relative positional embeddings, which enable its long-context utilization capability. strategies on tasks including language modeling, retrieval, multi-document question answering, and code completion. We also provide a theoretical analysis of how the alignment strategies work under the hood by investigating the relation between the Softmax temperature and data distribution. ## 2 Related Work\n\n## Transformer Positional Embeddings\n\nTransformer-based models rely on positional embeddings to encode positional information. We summarize open-source large pre-trained Transformer language models and their positional embeddings in Table 2. The relative variants are widely adopted due to their better empirical performance ( Su et al., 2021) and possible length-extrapolation capability (Press et al., 2022). In this work, we place special focus on the T5 positional embeddings due to their flexibility as shown in Figure 1. Transformer Length Extrapolation Existing research on Transformer length extrapolation is mostly confined to the task of natural language modeling (Press et al., 2022; Chi et al., 2022, 2023). Unfortunately, the reported positive results do not carry over to long-context retrieval (Mohtashami and Jaggi, 2023; Li et al., 2023). This contrastive observation can be explained by models' short empirical receptive field (Chi et al., 2023). In short, the strong decaying prior of positional embeddings prevents models from accessing distant tokens that may be necessary for retrieval tasks. In this work, we improve the flexible positional embeddings of T5 to get around this limitation. Transformer Position Interpolation Instead of performing direct length extrapolation, a different line of research conducts model fine-tuning on long input sequences (Chen et al., 2023), where the main focus is to identify the most efficient fine-tuning scheme that can improve long-context utilization. Positive results have been reported on retrieval tasks (Li et al., 2023). However, we argue that fine-tuning incurs additional costs since it needs\n\n1) GPU resources to perform long sequence finetuning with large models and 2) a pre-defined target sequence length, which still imposes a sequence length upper limit. Our proposed methods can circumvent these two limitations. ## Retrieval Tasks with Transformers\n\n Transformer-based approaches often consist of a retriever and a reader to overcome the context length restriction (Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Borgeaud et al., 2022). The retriever retrieves relevant text snippets from a very large database and the reader digests the retrieved information to generate the correct output. Our proposed attention alignment strategy can be used to significantly increase the input sequence length of the reader, thereby allowing more retrieved information to participate in the decision process. For small-scale retrieval problems, our methods even obviate the need for context segmentation and the external key-value store used in prior work (Mohtashami and Jaggi, 2023), serving as a more elegant approach.Softmax Temperature Scaling To increase the length extrapolation capability of Transformers, previous work (Yao et al., 2021; Su, 2021) scales the temperature of Softmax logarithmically w.r.t the sequence length to ensure invariant entropy. Our entropy alignment strategy is also inspired by this line of research except that we adopt a different procedure outlined below in Algorithm 1. Interestingly, our results in $\\S 7$ show that the logarithmic temperature scaling scheme is more similar to our proposed maximum probability alignment strategy. ## 3 Long-context Retrieval Tasks with T5\n\n### 3.1 Why Retrieval? As suggested by recent work (Mohtashami and Jaggi, 2023; Li et al., 2023), the task of longcontext retrieval serves as a controllable benchmark to measure how well a Transformer language model utilizes long-context inputs. One prominent characteristic of retrieval tasks is that only a subset of the input is of interest, requiring a model to accu-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-03.jpg?height=580&width=1393&top_left_y=264&top_left_x=333)\n\nFigure 1: Visualization of T5 Positional Embeddings. To plot figures of $b_{m, n}$, we set $m=7500$ and vary the value of $n$ from 0 to 15 k . Each attention head of a Flan-T5-XL encoder learns a set of positional embeddings that capture different attention bias. For example, the positional embeddings in the left figure encourage the model to focus on nearby tokens. In contrast, the ones in the right figure let the model focus on only remote tokens. rately pick up the necessary information. The other characteristic is that the key information can sit anywhere in an input, requiring a model to attend flexibly. Finally, the controllable aspect allows us to gradually increase the input sequence length to test the models' length extrapolation capability. ### 3.2 Why T5? To solve retrieval tasks using Transformer language models, it is necessary to choose a positional embedding design that permits accurate and flexible length-extrapolatable attention. After checking through the existing positional embeddings in Table 2, we find that the T5 family (Raffel et al., 2020) fits our needs. As for other candidates, learnable absolute positional embeddings (Vaswani et al., 2017; Zhang et al., 2022) must be evaluated within the training length. ALiBi (Press et al., 2022) and Rotary (Su et al., 2021) have a recency bias; they cannot extrapolate easily without fine-tuning. For each attention head, T5 encoder maintains a bucket (B) of 32 learnable parameters and assigns the relative positional bias (rpe bias) $b_{m, n}$ as $^{2}$\n\n$$\n\\begin{aligned}\n& b_{m, n}= \\\\\n& \\left\\{\\begin{array}{l}\n\\mathrm{B}[m-n], \\text { if } 0 \\leq m-n<8 \\\\\n\\mathrm{~B}[n-m+16], \\text { if }-8<m-n<0 \\\\\n\\mathrm{~B}\\left[\\min \\left(15,8+\\left\\lfloor\\frac{\\log ((m-n) / 8)}{\\log (128 / 8)} \\cdot 8\\right\\rfloor\\right)\\right], \\text { if } 8 \\leq m-n \\\\\n\\mathrm{~B}\\left[\\min \\left(31,24+\\left\\lfloor\\frac{\\log ((n-m) / 8)}{\\log (128 / 8)} \\cdot 8\\right\\rfloor\\right)\\right], \\text { if } m-n \\leq-8\n\\end{array}\\right.",
    "attnalignnfpe-1": "\\end{aligned}\n$$\n\nwhere $0 \\leq m<L$ and $0 \\leq n<L$ are two position indices. $b_{m, n}$ will be added to the $(m, n)$ -\n\n[^1]th entry of the $L \\times L$ self-attention matrix. The summation becomes the input to the temperaturescaled Softmax. We plot the learned rpe bias of a T5 encoder in Figure 1. We can tell that its attention heads encode rich attention patterns. For example, head 1 learns to focus on the nearby tokens whereas head 27 learns to ignore the nearby tokens and allow access to faraway tokens. ### 3.3 The Dispersed Attention Issue of T5 Encoder\n\nUnfortunately, directly applying T5 models on retrieval tasks does not yield perfect results. Upon inspecting the intermediate model states, we find that a longer input sequence consists of more tokens competing for the same amount (i.e., Softmax sums to 1) of attention, resulting in the dispersed attention issue. In Table 1, we see that the longer the input sequence, the flatter the self-attention distribution. The situation is not hopeless if the desired information still attains a higher attention weight than the remaining tokens. Our proposed solution in $\\S 4$ will let the key information stand out. ## 4 Proposed Methods\n\nA natural solution to the dispersed attention issue described in $\\S 3$ is to sharpen the self-attention distribution. This can be achieved by reducing the temperature $\\tau$ during extrapolation. We set the extrapolation temperature $\\tau_{e x}$ such that the sharpness during training with $\\tau_{t r}=1$ and that during extrapolation with $\\tau_{e x}<1$ are roughly the same. As a measurement of sharpness, we explore the maxi-\n\n```\nAlgorithm 1 Attention Alignment Strategies\nRequire: A short sequence of length \\(L_{t r}\\) and a long sequence\n    of length \\(L_{e x}>L_{t r}\\).",
    "attnalignnfpe-2": "Encoder \\(E\\). Alignment mode \\(M\\). Ensure: The Softmax temperature \\(\\tau\\)\n    function \\(\\operatorname{FINDS}(\\tau, M)\\)\n        Set temperature of all Softmax to \\(\\tau\\)\n        \\(s \\leftarrow[]\\)\n        for operation in \\(E\\) do\n            Perform the operation\n            if operation is \\(\\operatorname{Softmax}_{\\tau}(l)\\) then\n                if \\(M\\) is Maximum Probability then\n                    Append \\(\\max \\left(\\operatorname{Softmax}_{\\tau}(l)\\right)\\) to \\(s\\)\n                    else if \\(M\\) is Entropy then\n                    Append \\(\\mathrm{H}\\left(\\operatorname{Softmax}_{\\tau}(l)\\right)\\) to \\(s\\)\n            end if\n            end if\n        end for\n        return \\(\\operatorname{avg}(s)\\)\n    end function\n    \\(\\overline{S^{t r}}(1) \\leftarrow \\operatorname{FINDS}(1.0, M)\\)\n    for \\(\\frac{\\tau_{e x}}{S^{e x}}=1.0,0.95,0.9, \\cdots, 0.5\\) do\n        \\(\\overline{S^{e x}}\\left(\\tau_{e x}\\right)=\\operatorname{FinDS}\\left(\\tau_{e x}, M\\right)\\)\n    end for\n    return \\(\\tau_{e x}\\) s.t. \\(\\overline{S^{e x}}\\left(\\tau_{e x}\\right) \\approx \\overline{S^{t r}}(1)\\)\n```\n\nmum probability or entropy of a distribution. In other words, our proposed solution is to align the maximum probability or entropy of training and extrapolation distributions by adjusting $\\tau_{e x}$. Concretely, let $l^{(i)} \\in \\mathbb{R}^{L}$ be the i-th pre-Softmax logit vector of a T5 encoder, where $L \\in\\left\\{L_{t r}, L_{e x}\\right\\}$ is the sequence length. The post-Softmax distribution of $l^{(i)}$ is $\\mathrm{P}^{(i)}(\\tau)=\\operatorname{Softmax}_{\\tau}\\left(l^{(i)}\\right)$. The maximum probability and entropy of $\\mathrm{P}^{(i)}(\\tau)$ are $\\mathrm{P}_{\\max }^{(i)}(\\tau)$ and $\\mathrm{H}^{(i)}(\\tau)$, respectively. Take the maximum probability alignment strategy as an example: We first run the forward pass and compute the average maximum probability under temperature $\\tau$ over all logit vectors: $\\overline{\\mathrm{P}_{\\max }}(\\tau)=(1 / N) \\sum_{i} \\mathrm{P}_{\\max }^{(i)}(\\tau)$ where $N=R \\times$ $H \\times L$ is the number of logit vectors in a T5 encoder with $R$ layers, $H$ heads, and length $L$ sequences. Since the temperature is 1 during training and $\\tau_{e x}$ during extrapolation, we denote the average maximum probability during training as $\\overline{\\mathrm{P}_{\\max }^{t r}}(1)$ and that during extrapolation as $\\overline{\\mathrm{P}_{\\max }^{e x}}\\left(\\tau_{e x}\\right)$. Finally, to align the maximum probabilities, we adjust $\\tau_{e x}$ s.t. $\\overline{\\mathrm{P}_{\\text {max }}^{e x}}\\left(\\tau_{e x}\\right) \\approx \\overline{\\mathrm{P}_{\\text {max }}^{t r}}(1)$. In practice, we do a grid search on $\\tau_{e x}$ from 1.0 to 0.5 . We outline the procedure of the alignment strategies in Algorithm 1. Note that our proposed methods do not require any model fine-tuning or gradient computations. The only overhead is estimating the temperature $\\tau_{e x}$ using Algorithm 1 and a few length $L_{e x}$ sequences. Once the temperature is decided, it will be held fixed, rendering our methods simple and efficient. In addition, our fine-tuning free methods do not lead to performance regression on short $L_{t r}$ sequences commonly observed on long-context fine-tuned models (Roziere et al., 2023). ## 5 Experiments\n\nWe compare the two alignment strategies against the length-only Softmax temperature scaling scheme $\\tau=\\log _{L_{e x}} L_{t r}$ (Yao et al., 2021; Su, 2021) and LongChat-13B-16K (Li et al., 2023). Note that LongChat-13B-16K (Li et al., 2023), the best baseline, was fine-tuned from LLaMA (Touvron et al., 2023) on long sequences of length 16 k while our proposed methods do not need any fine-tuning. | Language Modeling |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length ( $L_{e x}$ ) |  |  |  |  |  |\n|  | 1024 | 2048 | 4096 | 8192 | 15000 | Avg. |\n| T5-Large-LM | 35.9 | 40.1 | $>1 k$ | $>1 k$ | $>1 \\mathrm{k}$ | $>1 \\mathrm{k}$ |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 34.7 | 45.5 | 45.2 | 45.5 | 52.7 | 44.7 |\n| w/ H | 40.2 | 43.9 | 45.6 | 54.6 | 56.0 | 48.1 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 39.8 | 38.2 | 47.4 | 45.3 | 55.9 | 45.3 |\n| T5-XL-LM | 28.3 | $>1 \\mathrm{k}$ | $>1$ k | $>1$ k | $>1$ k | $>1 \\mathrm{k}$ |\n| ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-04.jpg?height=18&width=94&top_left_y=1327&top_left_x=1100) | 30.2 | 36.0 | 31.6 | 41.7 | 50.0 | 37.9 |\n| w/ H | 30.4 | 36.8 | 38.4 | 53.3 | 63.4 | 44.4 |\n| w/ $\\log _{L_{e x}} L_{t r}$ | 27.3 | 29.4 | 31.7 | 39.3 | 45.8 | 34.7 |\n| T5-XXL-LM | 109 | $>1 \\mathrm{k}$ | $>1$ k | $>1 \\mathrm{k}$ | $>1$ k | $>1 \\mathrm{k}$ |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 32.2 | 29.7 | 29.5 | 36.6 | 44.3 | 34.5 |\n| w/ H | 26.8 | 28.1 | 34.2 | 37.8 | 43.8 | 34.1 |\n| w/ $\\log _{L_{e x}} L_{t r}$ | 27.1 | 36.1 | 33.9 | 246 | 43.8 | 77.5 |\n\nTable 3: Language Modeling Performance. We report the average perplexity of 500 sequences. The lower the better. ### 5.1 Language Modeling\n\nWe use the LM-Adapted T5 models for this experiment ${ }^{3}$. We set $L_{t r}=512$. Following previous work on Transformer length extrapolation, we perform an intrinsic evaluation on language modeling (Press et al., 2022; Chi et al., 2022, 2023). Ideally, our proposed methods should alleviate the perplexity explosion problem during extrapolation. As we can see in Table 3, both alignment strategies dramatically improve (lower) the perplexity. We also observe that scaling the temperature solely based on sequence lengths is not the optimal strategy, as indicated by the sudden perplexity increase of the $\\log _{L_{e x}} L_{t r}$ strategy. We will provide an in-depth discussion on this topic in $\\S 7$. Note that perplexity\n\n[^2]| Retrieval Tasks |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Topic, \\# of topics |  |  |  |  | Line, \\# of lines |  |  |  |  |  | Passkey, \\# of sentences |  |  |  |  | Avg. |\n|  | 5 | 10 | 15 | 20 | 25 | 200 | 300 | 400 | 500 | 600 | 680 | 20k | 30k | 40k | 50k | 55 k |  |\n| Flan-T5-Large | 99 | 100 | 97 | 97 | 83 | 97 | 100 | 92 | 96 | 93 | 92 | 62 | 47 | 31 | 16 | 9 | 76 |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 96 | 90 | 86 | 94 | 98 | 99 | 98 | 98 | 98 | 98 | 100 | 84 | 90 | 85 | 79 | 85 | 92 |\n| w/ H | 59 | 32 | 16 | 0 | 3 | 97 | 90 | 94 | 83 | 93 | 88 | 29 | 25 | 21 | 15 | 22 | 48 |\n| $\\mathrm{w} / \\log _{L_{e r}} L_{t r}$ | 88 | 79 | 75 | 61 | 55 | 99 | 99 | 98 | 99 | 97 | 98 | 74 | 63 | 51 | 41 | 35 | 76 |\n| Flan-T5-XL | 100 | 100 | 100 | 100 | 100 | 96 | 90 | 77 | 57 | 45 | 26 | 100 | 100 | 100 | 100 | 100 | 87 |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 100 | 100 | 100 | 100 | 100 | 97 | 90 | 89 | 80 | 70 | 62 | 100 | 99 | 100 | 100 | 100 | 93 |\n| w/ H | 99 | 98 | 97 | 96 | 96 | 95 | 87 | 88 | 79 | 70 | 71 | 100 | 100 | 100 | 100 | 100 | 92 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 99 | 100 | 100 | 100 | 100 | 98 | 88 | 81 | 86 | 60 | 67 | 100 | 100 | 100 | 100 | 99 | 92 |\n| Flan-T5-XXL | 100 | 100 | 100 | 99 | 99 | 100 | 100 | 98 | 95 | 84 | 82 | 100 | 100 | 100 | 100 | 100 | 97 |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 100 | 100 | 100 | 99 | 99 | 97 | 99 | 96 | 97 | 94 | 95 | 100 | 98 | 100 | 100 | 100 | 98 |\n| w/ H | 100 | 100 | 97 | 98 | 94 | 99 | 92 | 92 | 76 | 58 | 58 | 100 | 100 | 100 | 100 | 100 | 92 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 100 | 100 | 99 | 98 | 92 | 100 | 98 | 94 | 93 | 84 | 90 | 100 | 100 | 100 | 100 | 100 | 97 |\n| LongChat | 100 | 100 | 100 | 99 | 89 | 100 | 91 | 93 | 83 | 78 | 59 | 100 | 100 | 99 | 100 | 99 | 93 |\n\nTable 4: Performance of Retrieval Tasks. The numbers are accuracy. Full score is 100. The LongChat model corresponds to the LongChat-13B-16K model (Li et al., 2023). It is LLaMA-13B (Touvron et al., 2023) model fine-tuned on sequences of length 16k using the positional interpolation technique (Chen et al., 2023). Flan-T5-XXL has 11B parameters. The maximum sequence lengths $\\left(L_{e x}\\right)$ of the three tasks are all around 14.5 k to 15.5 k tokens. is not our primary focus since it often cannot accurately reflect the long-context utilization capability of Transformers on practical tasks (Li et al., 2023). ### 5.2 Long-context Retrieval\n\nThe tasks are formulated in the Question Answering (QA) format; therefore, we use the Flan-T5 models to leverage their instruction-following capability. We set $L_{t r}=512$. Inspired by recently proposed retrieval tasks, we evaluate the proposed alignment strategies on three of these. Topic retrieval requires a model to retrieve the first topic in a long and multi-topic conversation ( Li et al., 2023). Line retrieval has a long series of key-value pairs, and a model needs to retrieve the value corresponding to the given key (Li et al., 2023). Passkey retrieval hides a passkey in a long junk text snippet, and a model needs to return that passkey (Mohtashami and Jaggi, 2023). As we can see in Table 4, the retrieval performance is greatly boosted after the Flan-T5 models are equipped with our proposed attention alignment strategies. In particular, the maximum probability alignment strategy provides better results across the board. Other baselines such as MPT (Team, 2023) and ChatGLM2 (Du et al., 2022) perform worse than LongChat.",
    "attnalignnfpe-3": "Please refer to Li et al. (2023) for more details. We also present the optimal temperature given by Algorithm 1 in Table 9 in Appendix A.5. In short, the temperature decreases when the input sequence length increases. We will provide additional temperature analysis below, in \u00a77. ### 5.3 Multi-document Question Answering\n\nWe again use the Flan-T5 models to leverage their instruction-following capability. We set $L_{t r}=512$. We choose the multi-document question-answering task as our downstream task (Liu et al., 2023). The model input consists of a question Q and multiple documents extracted from NaturalQuestions (Kwiatkowski et al., 2019) related to Q, where one of the documents (golden doc) contains the ground truth answer to Q. As shown in Table 5, when a model is equipped with the proposed maximum probability alignment strategy, it consistently outperforms the original model across model sizes and number of input documents. Apart from the better task performance, we believe that the attention dispersed attention issue discussed in $\\S 3$ can help demystify the lost-in-themiddle phenomenon (Liu et al., 2023) of this task: Transformer models tend to perform worse when the ground truth sits near the middle of the input context. Let us recall the relative positional embedding of head 27 learned in Figure 1, if the ground truth answer sits in the middle, it will have long contexts from both sides competing for the attention weight. If this hypothesis is correct, we can expect the performance boost to be more prominent when the answer appears near the middle. We reveal the performance breakdown when the num-\n\n| Multi-document Question Answering |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | 10 Docs | 20 Docs | 30 Docs, golden doc at different positions |  |  |  |  |  |  |  |\n|  | Avg. | Avg. | 0 | 4 | 9 | 14 | 19 | 24 | 29 | Avg. |\n| Flan-T5-Large | 52.4 | 43.3 | 52.6 | 42.0 | 36.5 | 34.0 | 33.9 | 33.9 | 37.9 | 38.7 |\n| $\\mathrm{w} / \\mathrm{P}_{\\text {max }}$ | 53.1 | 44.2 | 50.8 | 44.5 | 39.5 | 36.4 | 35.9 | 35.8 | 37.0 | 40.0 |\n| Improvement | +0.7 | +0.9 | -1.8 | +1.5 | +3.0 | +2.4 | +2.0 | +1.9 | -0.9 | +1.3 |\n| w/ H | 52.1 | 43.2 | 47.6 | 41.1 | 35.2 | 33.5 | 32.2 | 33.3 | 34.2 | 36.7 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 53.2 | 44.5 | 50.6 | 44.1 | 39.3 | 36.3 | 35.8 | 35.8 | 37.2 | 39.9 |\n| Flan-T5-XL | 59.4 | 51.2 | 58.4 | 44.6 | 40.0 | 39.9 | 41.7 | 46.4 | 54.8 | 46.5 |\n| $\\mathrm{w} / \\mathrm{P}_{\\text {max }}$ | 61.1 | 53.6 | 60.9 | 49.1 | 46.0 | 44.9 | 46.3 | 49.1 | 55.7 | 50.3 |\n| Improvement | +1.7 | +2.4 | +2.5 | +4.5 | +6.0 | +5.0 | +4.6 | +2.7 | +0.9 | +3.8 |\n| w/ H | 60.5 | 52.4 | 52.4 | 43.5 | 42.1 | 40.3 | 42.0 | 42.9 | 51.3 | 44.9 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 60.9 | 53.6 | 61.0 | 49.1 | 46.1 | 44.7 | 46.1 | 48.7 | 55.4 | 50.2 |\n| Flan-T5-XXL | 63.6 | 56.9 | 58.9 | 49.1 | 48.1 | 47.5 | 48.9 | 53.1 | 61.2 | 52.4 |\n| $\\mathrm{w} / \\mathrm{P}_{\\text {max }}$ | 63.7 | 57.7 | 60.4 | 52.5 | 51.0 | 50.2 | 51.3 | 53.5 | 59.1 | 54.0 |\n| Improvement | +0.1 | +0.8 | +1.5 | +3.4 | +2.9 | +2.7 | +2.4 | +0.4 | -2.1 | +1.6 |\n| w/ H | 63.6 | 57.1 | 61.0 | 53.4 | 50.8 | 50.3 | 50.7 | 51.9 | 55.7 | 53.4 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 63.9 | 57.6 | 61.5 | 53.3 | 51.3 | 50.3 | 51.1 | 53.0 | 57.2 | 54.0 |\n\nTable 5: Performance of Multi-document QA.",
    "attnalignnfpe-4": "Numbers are accuracy. Full score is 100. The maximum sequence length ( $L_{e x}$ ) of 30 documents is around 5 k . The improvement row represents the absolute accuracy improvement after a Flan-T5 model is equipped with our proposed maximum probability alignment strategy.",
    "attnalignnfpe-5": "For the full performance breakdown, please refer to Table 14 in Appendix A. 7. ber of input documents is 30 . As we can see in the improvement row, those cases indeed achieve greater improvements. Our strategies are not always perfect: The performance drops if the ground truth answer is at position 29 . We believe T5 might have already handled this case pretty well due to the recency bias learned on some attention heads, and our additional temperature scaling sharpens the distribution too aggressively. We acknowledge this trade-off in $\\S 9$. ### 5.4 Code Key Retrieval and Completion\n\nTo test the generalizability of the alignment strategies, we apply our methods to the CodeT5+ model (Wang et al., 2023) that was pre-trained on code data with 770 M parameters. ${ }^{4}$ We set $L_{t r}=768$. We do not experiment with larger CodeT5+ models since they do not follow the T5 architecture, but use other positional embeddings. We conduct two experiments on the LCC dataset (Guo et al., 2023), which is highly similar to the classic PY150 dataset (Raychev et al., 2016) except that the input context length is much longer. For the code key retrieval experiment, we sample several code files from LCC along with a special function that only returns an integer from 1 to 100. We concatenate them and ask a model to gener-\n\n[^3]ate the returned integer at the end (Roziere et al., 2023). Considering that this is essentially a passkey retrieval task in the code domain, we briefly report the average accuracy of 100 test cases when the input sequence length is around 16 k : 0 (Original CodeT5+), 87 (w/ $\\mathrm{P}_{\\max }$ ), $80(\\mathrm{w} / \\mathrm{H})$, and 85 (w/ $\\left.\\log _{L_{e x}} L_{t r}\\right)$. We can see that the maximum probability alignment strategy performs the best. For the code completion experiment, a model needs to generate the next line of code given some prior code as the context. The metrics are Exact Match (EM) and Edit similarity (ES) on a per line basis (Svyatkovskiy et al., 2020). We report the results in Table 6 using the context length bucketing format. While both alignment strategies improve the performance substantially, $\\mathrm{P}_{\\text {max }}$ is better; however, its EM performance lags behind $\\log _{L_{e x}} L_{t r}$ when the sequence length increases. We additionally include an extrapolation-free baseline, truncation, that truncates the long input context to the most recent $L_{t r}=768$ tokens. Both $\\mathrm{P}_{\\max }$ and $\\log _{L_{e x}} L_{t r}$ perform better than this baseline when $L_{e x}<6000$, indicating that they can indeed benefit from longer $(6000 / 768=7.8 \\mathrm{x})$ contexts without any fine-tuning. ### 5.5 Overall Observations\n\nFirst, the maximum probability alignment strategy is the most reliable and best-performing method\n\n| Code Completion Exact Match |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length $\\left(L_{e x}\\right)$ |  |  |  |  |  |\n|  | 1 k | 2 k | 3 k | 4 k | 5 k | 6 k |\n| CodeT5+ | 19.6 | 19.0 | 11.3 | 2.6 | 0.1 | 0.0 |\n| $\\mathrm{w} / \\mathrm{P}_{\\max }$ | 21.1 | 22.5 | 21.7 | 21.5 | 19.3 | 22.7 |\n| $\\mathrm{w} / \\mathrm{H}$ | 19.5 | 18.7 | 13.7 | 9.0 | 7.9 | 9.0 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | $\\mathbf{2 1 .",
    "attnalignnfpe-6": "6}$ | $\\mathbf{2 3 . 0}$ | $\\mathbf{2 2 . 1}$ | $\\mathbf{2 2 .",
    "attnalignnfpe-7": "0}$ | $\\mathbf{2 0 . 6}$ | $\\mathbf{2 4 . 3}$ |\n| w/ truncation | 20.0 | 19.2 | 19.3 | 19.2 | 17.1 | 21.4 |\n\n\n| Code Completion Edit Similarity |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length $\\left(L_{e x}\\right)$ |  |  |  |  |  |\n|  | 1 k | 2 k | 3 k | 4 k | 5 k | 6 k |\n| CodeT5+ | 62.4 | 59.6 | 53.1 | 38.9 | 18.3 | 10.4 |\n| $\\mathrm{w} / \\mathrm{P}_{\\max }$ | 65.9 | 65.7 | 65.3 | 65.6 | $\\mathbf{6 3 .",
    "attnalignnfpe-8": "1}$ | 64.9 |\n| $\\mathrm{w} / \\mathrm{H}$ | 64.8 | 62.5 | 54.1 | 43.0 | 43.0 | 44.8 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | $\\mathbf{6 6 .",
    "attnalignnfpe-9": "3}$ | $\\mathbf{6 6 . 1}$ | $\\mathbf{6 5 . 2}$ | $\\mathbf{6 6 . 4}$ | 63.0 | 66.1 |\n| $\\mathrm{w} /$ truncation | 65.3 | 64.2 | 64.2 | 65.6 | 62.2 | $\\mathbf{6 6 . 9}$ |\n\nTable 6: Code Completion Performance.",
    "attnalignnfpe-10": "Full score is 100 . We set $L_{t r}=768$. The bucket $n \\mathrm{k}$ contains the data with length in $[n \\mathrm{k},(n+1) \\mathrm{k}), n \\in[1,6]$. For example, the bucket 3 k contains data with length in $[3000,4000)$. See Table 12 and 13 in Appendix A. 6 for the full performance breakdown of $L_{e x}$ up to 16k tokens. across most tasks and settings, echoing our discussion in \u00a73.1: For most data, only a subset of the input is useful for a model process at a time. The maximum probability alignment strategy captures this characteristic naturally, thereby outperforming the entropy alignment strategy that cares more about the holistic distribution. Second, deciding the optimal temperature solely based on sequence lengths, e.g. $\\tau=\\log _{L_{e x}} L_{t r}$, is not robust enough. For example, the perplexity of $\\log _{L_{e x}} L_{t r}$ suddenly increases (worse) on T5XXL-LM, in Table 3, while the other strategies maintain stable results. As another example, it fails to improve the retrieval performance on the Flan-T5-Large model, shown in Table 4. ## 6 Theoretical Analysis\n\n### 6.1 Assumptions\n\nTo shed more light on the underlying mechanisms of the two alignment strategies, we establish the connection between the softmax temperature $\\tau$ and data distribution under empirically verified assumptions. We focus on the 0-th layer (closest to the input embeddings) and take the average over all logit vectors across attention heads. Note that this is just a crude approximation of Algorithm 1 for analysis purposes since 1) a Transformer language model typically encompasses multiple layers, and 2) in Algorithm 1, we take the maximum probability or entropy of individual logit vectors as opposed to the average one. Assumption 1. The length $L$ average logit vector is normally distributed, i.e., its entry $l_{i} \\sim N\\left(0, \\sigma^{2}\\right)$. To compute the average logit vector, we start with a input sequence of length $L$. Using a Transformer model with $H$ attention heads (specifically, a T5 Encoder in our context), we generate $H \\times L$ pre-softmax logit vectors, each with a length of $L$. Here, the number of layers is 1 because we focus on the 0 -th layer. These logit vectors are then individually sorted, and we subsequently calculate the average of all $H \\times L$ sorted logit vectors, resulting in the average logit vector of length $L$. To assess whether the average logit entries follow a Gaussian distribution, we make use of QQ plots, as illustrated in Figure 2. The linearity of the plot serves as an indicator - the closer the points are to the identity line, the more Gaussian the distribution. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-07.jpg?height=484&width=646&top_left_y=1420&top_left_x=1116)\n\nFigure 2: QQ plots of Flan-T5-XL. We experiment with short and long sequences. The red reference line is $\\mathrm{y}=\\mathrm{x}$. We use sequences of length around 512 for this plot. The plot for sequences of length around 15 k looks highly similar.",
    "attnalignnfpe-11": "Please refer to Appendix A.",
    "attnalignnfpe-12": "1 for details. Assumption 2. The largest logit entry of the average logit vector during training and extrapolation is the same: $l_{\\max }^{e x}=l_{\\max }^{t r}$. See Table 7. | Retrieval Tasks |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Criteria | Topic |  | Line |  | Passkey |  |\n|  | 512 | 15 k | 512 | 15k | 512 | 15k |\n| $l_{\\text {max }}$ | 8.61 | 8.80 | 8.71 | 8.97 | 8.75 | 8.85 |\n\nTable 7: Largest Logit Entry of Flan-T5-XL. $l_{\\text {max }}$ is the largest logit entry of the average logit vector. ### 6.2 Maximum Probability Alignment\n\nProposition 1. Under Assumption 1 and 2, we can adjust the temperature $\\tau$ to align the maximum probability $\\mathrm{P}_{\\max }^{t r}=\\mathrm{P}_{\\max }^{e x}$\n\n$$\n\\begin{aligned}\n\\tau & \\approx \\frac{\\log L_{t r}+\\log \\mathrm{P}_{\\max }^{t r}+\\sigma_{t r}^{2} / 2}{\\log L_{e x}+\\log \\mathrm{P}_{\\max }^{t r}+\\sigma_{e x}^{2} /\\left(2 \\tau^{2}\\right)} \\\\\n& =\\frac{B}{A+\\frac{C}{\\tau^{2}}}=\\frac{B \\tau^{2}}{A \\tau^{2}+C}\n\\end{aligned}\n$$\n\nAssuming $\\tau \\neq 0$, we solve the quadratic equation $A \\tau^{2}-B \\tau+C=0$ to get $\\tau$.",
    "attnalignnfpe-13": "We pick the larger root as our final solution. See proof in Appendix A.2. ### 6.3 Entropy Alignment\n\nProposition 2. Under Assumption 1, we can adjust the temperature $\\tau$ to align the entropy $H_{t r}=H_{e x}$\n\n$$\n\\tau \\approx \\frac{\\sigma_{e x}}{\\sqrt{\\sigma_{t r}^{2}+2 \\log \\frac{L_{e x}}{L_{t r}}}}\n$$\n\nSee proof in Appendix A.3. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-08.jpg?height=652&width=735&top_left_y=1344&top_left_x=249)\n\nFigure 3: Language Modeling Temperature Analysis. Curves are from Proposition $1 \\& 2$. Dots and crosses are from Algorithm 1. ## 7 Discussion\n\nThe goal of this section is to explain the observations made in $\\S 5$ via the lens of temperature analysis. We visualize Proposition 1 and 2 by plotting the temperature curves in Figure 3 and 4. We evaluate $\\mathrm{P}_{\\text {max }}^{t r}$ and $\\sigma_{t r}$ at the training length and $\\sigma_{e x}$ at every extrapolation length considering only the 0 -th layer. You may find the temperature curves for the other tasks in Appendix A.4. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-08.jpg?height=641&width=746&top_left_y=245&top_left_x=1063)\n\nFigure 4: Topic Retrieval Temperature Analysis. Curves are from Proposition $1 \\& 2$. Dots and crosses are from Algorithm 1. First, while both proposed strategies lower the temperature when the input sequence length increases, the entropy alignment strategy does so more aggressively, possibly leading to its inferior performance observed in Table 4 and $5(\\mathrm{w} / \\mathrm{H})$. This can be seen by comparing the curves from Propositions 1 and 2 or dots from Algorithm 1. Second, deciding the optimal temperature based on sequence lengths, e.g. $\\tau=\\log _{L_{e x}} L_{t r}$, is not the most robust method. It gives too high of a temperature in Figure 3 compared to Algorithm 1. In other words, it does not sharpen the distribution enough, possibly explaining its perplexity spike in Table 3. On the other hand, it overly lowers the temperature in Figure 4, thereby failing to improve the retrieval performance on Flan-T5-Large in Table 4. ## 8 Conclusion\n\nIn this paper, we show that the T 5 model family has great potential when it comes to Transformer length extrapolation. We propose the maximum probability and entropy alignment strategies to fix T5's dispersed attention issue without model fine-tuning. We conduct experiments on natural language modeling, retrieval, multi-document question answering, and code completion tasks to demonstrate the effectiveness of our proposed methods. Finally, we present a simplified theoretical analysis to elucidate how the temperature is scaled to achieve attention alignment. We hope that our work can inspire future length-extrapolatable Transformer designs. ## 9 Limitations\n\nWe base our theoretical analysis on a simplified Transformer language model, which might be further improved by taking all the layers and their interactions into account. In addition, we find that different layers have different degrees of distribution flatness, which could be leveraged in future work to perform per-layer fine-grained attention alignment. Finally, our temperature scaling scheme sometimes sharpens a distribution too aggressively in the multidocument question-answering and code completion experiments. This drawback could be possibly improved by designing a more fine-grained attention alignment strategy. ## Acknowledgment\n\nThe first author acknowledges the support from the Boeing Company (2019-STU-PA-259). ## References\n\nSalesforce AI. 2023. Long sequence modeling with xgen: A 7 b llm trained on 8 k input sequence length. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595. Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander I Rudnicky. 2022. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. In Advances in Neural Information Processing Systems (NeurIPS), New Orleans, USA. Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky, and Peter J Ramadge. 2023. Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. In Annual Meeting of the Association for Computational Linguistics (ACL), Toronto, Canada. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pretrained language model for code completion. In International Conference on Machine Learning. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How long can opensource llms truly promise on context length. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset\nfor falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116. Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. Markus N Rabe and Charles Staats. 2021. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint arXiv:2112.05682. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551. Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic model for code with decision trees. ACM SIGPLAN Notices, 51(10):731-747. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100. Jianlin Su. 2021. Scaling attention via the lens of entropy invariance.",
    "attnalignnfpe-14": "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.",
    "attnalignnfpe-15": "Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 1433-1443. The MosaicML NLP Team. 2023. Introducing mpt7b: A new standard for open-source, commercially usable llms. https://www.mosaicml.com/blog/mpt$30 b$. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation.",
    "attnalignnfpe-16": "arXiv preprint arXiv:2305.07922. Martin B Wilk and Ram Gnanadesikan. 1968. Probability plotting methods for the analysis for the analysis of data. Biometrika, 55(1):1-17. Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. 2021. Self-attention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770-3785, Online. Association for Computational Linguistics. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models.",
    "attnalignnfpe-17": "arXiv preprint arXiv:2205.01068. ## A Appendix\n\n## A. 1 QQ Plots for Assumption 1\n\nA QQ plot (Wilk and Gnanadesikan, 1968) is a graphical technique used for comparing two probability distributions by plotting their quantiles against each other. A point ( $x, y$ ) corresponds to a quantile from the second distribution (y-coordinate) plotted against the same quantile from the first distribution (x-coordinate). When the two distributions under comparison are similar, the points in the QQ plot will roughly align with the identity line, $y=x$. In our case, where we aim to determine the degree of Gaussian behavior in the average logit vector, the linearity of the plot serves as an indicator - the closer the points are to the identity line, the more Gaussian the distribution. We present the QQ plots for two lengths, 512 and 15k, on the three retrieval tasks in Figure 5.",
    "attnalignnfpe-18": "They are all close to the red reference line, indicating that their form is highly Gaussian. ## A. 2 Detailed Derivation of Proposition 1\n\nLet $l_{\\text {max }}$ be the largest value in the logit vector $l$. Let $\\tau$ be the temperature of the Softmax function. The probability of the largest entry is\n\n$$\n\\mathrm{P}_{\\max }=\\frac{e^{l_{\\max } / \\tau}}{\\sum_{i=1}^{L} e^{l_{i} / \\tau}}\n$$\n\nSince Softmax is shift-invariant, the logit vector can always be made zero-mean: $\\sum_{i} l_{i}=0$. Next, according to Assumption 1, the denominator of Softmax can be approximated as\n\n$$\n\\sum_{i=1}^{L} e^{l_{i} / \\tau} \\approx L \\cdot \\mathbb{E}\\left[e^{l_{i} / \\tau}\\right]=L \\cdot e^{\\sigma^{2} /\\left(2 \\tau^{2}\\right)}\n$$\n\nThis implies $\\mathrm{P}_{\\text {max }}$ is approximately\n\n$$\n\\mathrm{P}_{\\max } \\approx \\frac{e^{l_{\\max } / \\tau}}{L e^{\\sigma^{2} /\\left(2 \\tau^{2}\\right)}}\n$$\n\nDuring the training stage, the temperature $\\tau$ is 1\n\n$$\n\\mathrm{P}_{\\max }^{t r} \\approx \\frac{e^{l_{\\max }^{t r}}}{L_{t r} e^{\\sigma_{t r}^{2} / 2}}\n$$\n\nwhich gives an expression of the largest logit entry during the training stage\n\n$$\nl_{\\max }^{t r} \\approx \\log \\left(\\mathrm{P}_{\\max }^{t r} L_{t r} e^{\\sigma_{t r}^{2} / 2}\\right)\n$$\n\nAccording to Assumption 2, the largest probability during the extrapolation stage can be simplified as\n\n$$\n\\begin{aligned}\n& \\mathrm{P}_{\\max }^{e x} \\approx \\frac{e^{e_{\\max }^{e x} / \\tau}}{L_{e x} e^{\\sigma_{e x}^{2} /\\left(2 \\tau^{2}\\right)}} \\stackrel{\\mathrm{A} .2}{=} \\frac{e^{l_{\\max }^{t r} / \\tau}}{L_{e x} e^{\\sigma_{e x}^{2} /\\left(2 \\tau^{2}\\right)}} \\\\\n& \\stackrel{(2)}{\\approx} \\frac{\\left(\\mathrm{P}_{\\max }^{t r} L_{t r} e^{\\sigma_{t r}^{2} / 2}\\right)^{1 / \\tau}}{L_{e x} e^{\\sigma_{e x}^{2} /\\left(2 \\tau^{2}\\right)}}\n\\end{aligned}\n$$\n\nSince $\\tau$ is a free parameter during extrapolation, we adjust it to carry out the maximum probability alignment strategy. Rearranging the terms gives Proposition 1. ## A. 3 Detailed Derivation of Proposition 2\n\nThe entropy of a discrete probability computed by Softmax is\n$H=-\\sum_{i} \\frac{e^{l_{i} / \\tau}}{D} \\log \\frac{e^{l_{i} / \\tau}}{D}=\\log D-\\frac{\\sum_{i} \\frac{l_{i}}{\\tau} e^{l_{i} / \\tau}}{D}$,\nwhere $D=\\sum_{i} e^{l_{i} / \\tau}$ is the denominator of Softmax, which can be approximated using Eq. (1). On the other hand, we note that $\\sum_{i} l_{i} e^{l_{i}} \\approx L \\mathbb{E}\\left[l e^{l}\\right]$. When $l \\sim N\\left(0, \\sigma^{2}\\right), \\mathbb{E}\\left[l e^{l}\\right]$ is approximated as\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[l e^{l}\\right]=\\int_{-\\infty}^{\\infty} \\frac{l e^{l}}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{l^{2}}{2 \\sigma^{2}}} d l \\\\\n& =\\int_{-\\infty}^{\\infty} \\frac{l}{\\sigma \\sqrt{2 \\pi}} e^{\\frac{2 \\sigma^{2} l-l^{2}}{2 \\sigma^{2}}} d l \\\\\n& =\\int_{-\\infty}^{\\infty} \\frac{l}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(l-\\sigma^{2}\\right)^{2}-\\sigma^{4}}{2 \\sigma^{2}}} d l \\\\\n& =e^{\\sigma^{2} / 2} \\int_{-\\infty}^{\\infty} \\frac{l}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(l-\\sigma^{2}\\right)^{2}}{2 \\sigma^{2}}} d l \\\\\n& =e^{\\sigma^{2} / 2} \\sigma^{2}\n\\end{aligned}\n$$\n\nThus, combining Eq. (1) and (3), the entropy $H$ is approximated as\n\n$$\n\\begin{aligned}\nH & \\approx \\log L+\\frac{\\sigma^{2}}{2 \\tau^{2}}-\\frac{L e^{\\sigma^{2} /\\left(2 \\tau^{2}\\right)} \\frac{\\sigma^{2}}{\\tau^{2}}}{L e^{\\sigma^{2} /\\left(2 \\tau^{2}\\right)}} \\\\\n& =\\log L-\\frac{\\sigma^{2}}{2 \\tau^{2}}\n\\end{aligned}\n$$\n\nSince $\\tau$ is set to 1 during the training stage, we have $H_{t r} \\approx \\log L_{t r}-\\frac{\\sigma_{t r}^{2}}{2}$. During extrapolation, we align the entropy (i.e., $H_{e x}=H_{t r}$ ) by adjusting $\\tau$. $$\n\\log L_{e x}-\\frac{\\sigma_{e x}^{2}}{2 \\tau^{2}} \\approx H_{e x}=H_{t r} \\approx \\log L_{t r}-\\frac{\\sigma_{t r}^{2}}{2}\n$$\n\nSince $\\tau$ is a free parameter during extrapolation, we adjust it to apply the entropy alignment strategy. Rearranging the terms gives Proposition 2. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-12.jpg?height=556&width=1359&top_left_y=266&top_left_x=350)\n\nFigure 5: QQ plots of Flan-T5-XL. We experiment with short and long sequences. The red reference line is $\\mathrm{y}=\\mathrm{x}$. The more closely the scatter plots follow the red reference line, the more Gaussian they are. ## A. 4 More Real-world Temperature Plots\n\nWe verify Proposition 1 and 2 on the remaining tasks by plotting the temperature curves in Figure $6,7,8$, and 9 . We empirically evaluate $\\sigma_{t r}$ at the training length and $\\sigma_{e x}$ every extrapolation length considering only the 0 -th layer. The real temperatures given by Algorithm 1 are usually higher than those derived from the two propositions. After checking the per-layer attention distributions, we find that the 0 -th layer has flatter distributions compared to higher layers. Because the two propositions are derived based on the 0 -th layer and a flatter distribution needs a lower temperature to correct, the temperatures given by them tend to be lower than the ones given by Algorithm 1 that takes the average of temperatures across all layers. ## A. 5 Detailed Temperature Breakdown\n\nWe report the temperatures for all tasks across model sizes given by Algorithm 1 in Table 8, 9, 10, and 11 . ## A. 6 Performance Breakdown of Code Completion\n\nWe report the performance breakdown of Exact Match and Edit Similarity across lengths in Table 12 and 13. ## A. 7 Performance Breakdown of Multi-document Question Answering\n\nWe report the performance breakdown of different numbers of input documents in Table 14. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-12.jpg?height=641&width=764&top_left_y=999&top_left_x=1063)\n\nFigure 6: Line Retrieval Temperature Analysis.",
    "attnalignnfpe-19": "Curves are given by Proposition 1 and 2. Cross signs and dots are given by Algorithm 1. $\\log _{L} 512$ is given by Yao et al. (2021); Su (2021). | Language Modeling |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length $\\left(L_{e x}\\right)$ |  |  |  |  |\n| T5-Large-LM |  |  |  |  |  |\n| $\\mathrm{w} / \\mathrm{P}_{\\text {max }}$ | 0.9 | 0.85 | 0.8 | 0.75 | 0.7 |\n| w/ H | 0.8 | 0.7 | 0.6 | 0.5 | 0.5 |\n| T5-XL-LM |  |  |  |  |  |\n| $\\mathrm{w} / \\mathrm{P}_{\\text {max }}$ | 0.9 | 0.85 | 0.75 | 0.7 | 0.6 |\n| w/ H | 0.85 | 0.7 | 0.55 | 0.5 | 0.5 |\n| T5-XXL-LM |  |  |  |  |  |\n| $\\mathrm{w} / \\mathrm{P}_{\\text {max }}$ | 0.9 | 0.85 | 0.65 | 0.55 | 0.5 |\n| w/ H | 0.85 | 0.7 | 0.7 | 0.55 | 0.5 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-12.jpg?height=55&width=219&top_left_y=2459&top_left_x=1103) | 0.9 | 0.82 | 0.75 | 0.69 | 0.65 |\n\nTable 8: Temperatures of Language Modeling. We search the optimal temperature from $1.0,0.95,0.9, \\cdots$, 0.5 . We set $L_{t r}=512$. | Retrieval Tasks |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Topic, \\# of topics |  |  |  |  | Line, \\# of lines |  |  |  |  |  | Passkey, \\# of sentences |  |  |  |  |\n|  | 5 | 10 | 15 | 20 | 25 | 200 | 300 | 400 | 500 | 600 | 680 | 20k | 30k | 40k | 50k | 55 k |\n| Flan-T5-Large |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 0.85 | 0.8 | 0.75 | 0.75 | 0.75 | 0.85 | 0.8 | 0.8 | 0.75 | 0.75 | 0.75 | 0.85 | 0.80 | 0.80 | 0.75 | 0.75 |\n| w/ H | 0.7 | 0.6 | 0.55 | 0.5 | 0.5 | 0.65 | 0.55 | 0.55 | 0.5 | 0.5 | 0.5 | 0.6 | 0.55 | 0.5 | 0.5 | 0.5 |\n| Flan-T5-XL |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 0.8 | 0.75 | 0.7 | 0.65 | 0.65 | 0.8 | 0.75 | 0.75 | 0.7 | 0.70 | 0.7 | 0.85 | 0.8 | 0.75 | 0.75 | 0.75 |\n| w/ H | 0.7 | 0.55 | 0.55 | 0.5 | 0.5 | 0.6 | 0.55 | 0.55 | 0.5 | 0.5 | 0.5 | 0.7 | 0.65 | 0.6 | 0.6 | 0.6 |\n| Flan-T5-XXL |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 0.85 | 0.8 | 0.75 | 0.75 | 0.75 | 0.8 | 0.8 | 0.75 | 0.75 | 0.75 | 0.75 | 0.85 | 0.8 | 0.8 | 0.75 | 0.75 |\n| w/ H | 0.75 | 0.65 | 0.6 | 0.55 | 0.55 | 0.65 | 0.6 | 0.6 | 0.55 | 0.55 | 0.55 | 0.65 | 0.6 | 0.55 | 0.55 | 0.5 |\n| w/ $\\log _{L_{e x}} L_{t r}$ | 0.79 | 0.72 | 0.69 | 0.67 | 0.65 | 0.74 | 0.71 | 0.69 | 0.67 | 0.66 | 0.65 | 0.73 | 0.69 | 0.67 | 0.66 | 0.65 |\n\nTable 9: Temperatures of Retrieval Tasks. We search the optimal temperature from $1.0,0.95,0.9, \\cdots, 0.5$. The maximum lengths of the three tasks are all around 14.5 k to 15.5 k tokens $\\left(L_{e x}\\right)$. We set $L_{t r}=512$. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-13.jpg?height=644&width=761&top_left_y=1454&top_left_x=245)\n\nFigure 7: Passkey Retrieval Temperature Analysis. Curves are given by Proposition 1 and 2. Cross signs and dots are given by Algorithm 1. $\\log _{L} 512$ is given by Yao et al. (2021); Su (2021). ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-13.jpg?height=657&width=784&top_left_y=1462&top_left_x=1047)\n\nFigure 8: Multi-doc QA Temperature Analysis. Curves are from Proposition $1 \\& 2$. Dots and crosses are from Algorithm 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_0dede2f48b16f59a9502g-14.jpg?height=649&width=695&top_left_y=424&top_left_x=269)\n\nFigure 9: Code Completion Temperature Analysis.",
    "attnalignnfpe-20": "Curves are given by Proposition 1 and 2. Cross signs and dots are given by Algorithm 1. $\\log _{L} 768$ is given by Yao et al. (2021); Su (2021). Multi-document Question Answering\n\n| Models | 10 Docs | 20 Docs | 30 Docs |\n| :--- | :---: | :---: | :---: |\n|  | $L_{e x}=1700$ | $L_{e x}=3300$ | $L_{e x}=5000$ |\n| Flan-T5-Large |  |  |  |\n| w/ Max. | 0.9 | 0.85 | 0.8 |\n| w/ Ent. | 0.75 | 0.65 | 0.6 |\n| Flan-T5-XL |  |  |  |\n| w/ Max. | 0.85 | 0.75 | 0.75 |\n| w/ Ent. | 0.75 | 0.65 | 0.55 |\n| Flan-T5-XXL |  |  |  |\n| w/ Max. | 0.9 | 0.8 | 0.8 |\n| w/ Ent. | 0.75 | 0.7 | 0.65 |\n| w/ $\\log _{L_{e x}} L_{t r}$ | 0.84 | 0.77 | 0.73 |\n\nTable 10: Temperatures of Multi-document Question Answering. We search the optimal temperature from $1.0,0.95,0.9, \\cdots, 0.5$. Different golden document positions have the same temperature. We set $L_{t r}=512$. | Code Completion |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length $\\left(L_{e x}\\right)$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  | 1 k | 2k | 3 k | 4 k | 5 k | 6k | 7 k | 8k | 9 k | 10k | 11k | 12 k | 13 k | 14 k | 15 k | 16 k |\n| CodeT5+ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 0.95 | 0.8 | 0.75 | 0.75 | 0.7 | 0.7 | 0.6 | 0.6 | 0.6 | 0.6 | 0.55 | 0.55 | 0.55 | 0.55 | 0.5 | 0.5 |\n| w/ H | 0.85 | 0.55 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 0.96 | 0.87 | 0.83 | 0.8 | 0.78 | 0.76 | 0.75 | 0.74 | 0.73 | 0.72 | 0.71 | 0.71 | 0.7 | 0.7 | 0.69 | 0.69 |\n\nTable 11: Temperatures of Code Completion. We search the optimal temperature from 1.0, 0.95, $0.9, \\cdots, 0.5$.",
    "attnalignnfpe-21": "The maximum length is around 16 k tokens $\\left(L_{e x}\\right)$. We set $L_{t r}=768$. | Code Completion Exact Match |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length $\\left(L_{e x}\\right)$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  | 1k | 2 k | 3 k | 4 k | 5 k | 6k | 7 k | 8 k | 9 k | 10k | 11 k | 12 k | 13 k | 14 k | 15 k |\n| CodeT5+ | 19.6 | 19.0 | 11.3 | 2.6 | 0.1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 21.1 | 22.5 | 21.7 | 21.5 | 19.3 | 22.7 | 16.1 | 14.4 | 13.4 | 20.6 | 16.0 | 15.3 | 12.3 | 16.7 | 4.5 |\n| w/ H | 19.5 | 18.7 | 13.7 | 9.0 | 7.9 | 9.0 | 10.3 | 8.8 | 10.8 | 12.1 | 11.7 | 10.2 | 9.2 | 11.1 | 2.3 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 21.6 | 23.0 | 22.1 | 22.0 | 20.6 | 24.3 | 20.7 | 18.6 | 19.1 | 22.4 | 13.8 | 20.3 | 15.4 | 19.4 | 11.4 |\n| w/ truncation | 20.0 | 19.2 | 19.3 | 19.2 | 17.1 | 21.4 | 21.1 | 18.0 | 19.1 | 25.2 | 18.1 | 20.3 | 16.9 | 27.8 | 15.9 |\n\nTable 12: Full Exact Match Breakdown of Code Completion Edit Similarity.",
    "attnalignnfpe-22": "We set $L_{t r}=768$. Numbers in red are higher than their counterpart in the $\\mathrm{w} /$ truncation row. The bucket $n \\mathrm{k}$ contains the data with length in $[n \\mathrm{k}$, $(n+1) \\mathrm{k}), n \\in[1,15]$. | Code Completion |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Models | Sequence Length $\\left(L_{e x}\\right)$ |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  | 1k | 2k | 3 k | 4 k | 5 k | 6k | 7 k | 8k | 9 k | 10k | 11k | 12 k | 13 k | $14 k$ | 15 k |\n| CodeT5+ | 62.4 | 59.6 | 53.1 | 38.9 | 18.3 | 10.4 | 6.1 | 4.0 | 4.5 | 5.0 | 6.7 | 5.1 | 6.4 | 4.4 | 3.5 |\n| w/ $\\mathrm{P}_{\\text {max }}$ | 65.9 | 65.7 | 65.3 | 65.6 | 63.1 | 64.9 | 60.0 | 60.0 | 58.1 | 57.5 | 56.2 | 56.0 | 52.1 | 56.9 | 39.9 |\n| w/ H | 64.8 | 62.5 | 54.1 | 43.0 | 43.0 | 44.8 | 47.7 | 47.0 | 47.6 | 51.2 | 44.3 | 49.7 | 50.3 | 57.4 | 42.0 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 66.3 | 66.1 | 65.2 | 66.4 | 63.0 | 66.1 | 61.9 | 58.8 | 61.6 | 57.8 | 54.2 | 57.9 | 48.7 | 52.2 | 48.6 |\n| w/ truncation | 65.3 | 64.2 | 64.2 | 65.6 | 62.2 | 66.9 | 66.8 | 61.8 | 64.1 | 65.1 | 63.5 | 63.9 | 61.5 | 67.6 | 60.8 |\n\nTable 13: Full Edit Similarity Breakdown of Code Completion.",
    "attnalignnfpe-23": "We set $L_{t r}=768$.",
    "attnalignnfpe-24": "Numbers in red are higher than their counterpart in the w/truncation row. The bucket $n \\mathrm{k}$ contains the data with length in $[n \\mathrm{k},(n+1) \\mathrm{k})$, $n \\in[1,15]$. ## Multi-document Question Answering\n\n| Models | 10 Docs |  |  | 20 Docs |  |  |  |  | 30 Docs |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 0 | 4 | 9 | 0 | 4 | 9 | 14 | 19 | 0 | 4 | 9 | 14 | 9 | 24 | 29 |\n| Fan-TJ-Large | 60.6 | 48.5 | 48.0 | 54.5 | 44.0 | 39.6 | 38.0 | 40.2 | 5 | 42.0 | 36.5 | 3 | - | \\ | 37.9 |\n| w/ Max.",
    "attnalignnfpe-25": "| 60.9 | 49.8 | - 6 | . 5 | . | 40.8 | . 7 | 41.3 | 50.8 | 44.5 | 39.5 | 36.4 | 35.9 | 35.8 | 37.0 |\n| w/ Ent. | 58.9 | 50.1 | 47.3 | 52.4 | 45.2 | 40.4 | 38.0 | 40.0 | 47.6 | 41.1 | 35.2 | 33.5 | 32.2 | 33.3 | 34.2 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 60.2 | 51.1 | 48.4 | 53.8 | 46.0 | 41.4 | 39.4 | 41.7 | 50.6 | 44.1 | 39.3 | 36.3 | 35.8 | 35.8 | 37.2 |\n| Flan-T5-XL | 64.0 | 55.4 | 58.9 |  | 47.9 |  | 47.3 | 55.3 | 58.4 |  | $\\square$ |  | 41.7 | $\\square$ | 54.8 |\n| w/ Max. | 65.3 | 57.3 | 60.8 | 62.2 | 51.6 | 49.0 | 49.4 | 56.0 | 60.9 | 49.1 | 46.0 | 44.9 | 46.3 | 49.1 | 55.7 |\n| w/ Ent. | 6 | 56.7 | 60.0 | 59.3 | 50.1 | 4 | 49.8 | 55.1 | 52.4 | $\\square$ | 4 | 40 | 42 | 42.9 | 51.3 |\n| $\\mathrm{w} / \\log _{L_{e x}} L_{t r}$ | 65.1 | 57.0 | 60.6 | 62.2 | 51.7 | 48.8 | 49.5 | 56.0 | 61.0 | 49.1 | 46.1 | 44.7 | 46.1 | 48.7 | 55.4 |\n| Flan-T5-XXL | 65.1 | 6 | 6 | 61.1 | 53 | 52.4 | 54 | 62 | 58 | 49. | 48 | 47. | 48 | 53.1 | 61.2 |\n| w/ Max. | 66 | 61.8 | 63.2 | 62.8 | 55 | 54.4 | 55 | 59 | 60.4 | 52.5 | 51.0 | 50.2 | 51 | 53.5 | 59.1 |\n| w/ Ent. | 67.3 | 2.1 | 61.3 | 63.2 | 56.1 | 54.1 | 54.3 | 57.6 | 61.0 | 53.4 | 50.8 | 50.3 | 50.7 | 51.9 | 55.7 |\n| $\\underline{\\mathrm{w} / \\log _{L_{e x}} L_{t r}}$ | 66.7 | . 9 | 1 | 63.1 | 56.0 | 54.7 | 55.1 | 59.0 | 61.5 | 53.3 | 51.3 | 50.3 | 51.1 | 53.0 | 57.2 |\n\nTable 14: Full Performance Breakdown of Multi-document Question Answering. The numbers are accuracy. Full score is $100.0,4,9 \\ldots$ indicate the position of the golden document that contains the answer to a question. [^0]:    ${ }^{1}$ https://gi thub.com/chijames/\n    Attention-Alignment-Transformer-Length-Extrapolation\n\n[^1]:    ${ }^{2}$ https://github.com/huggingface/transformers/ blob/v4.33.2/src/transformers/models/t5/ modeling_t5.py\\#L390\n\n[^2]:    ${ }^{3}$ https://github.com/google-research/ text-to-text-transfer-transformer/blob/main/ released_checkpoints.md\\#lm-adapted-t511lm100k\n\n[^3]:    4https://huggingface.co/Salesforce/ codet 5 p-770m-py\n\n"
}