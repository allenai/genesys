{
    "genanatooptdesign-0": "# From Generalization Analysis to Optimization Designs for State Space Models \n\nFusheng Liu<br>National University of Singapore<br>fusheng@u.nus.edu\n\nQianxiao Li<br>National University of Singapore<br>qianxiao@nus.edu.sg\n\n\n#### Abstract\n\nA State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling.",
    "genanatooptdesign-1": "In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results. ## 1 Introduction\n\nSequence modeling has been a long-standing research topic in many machine learning areas, such as speech recognition (Hinton et al., 2012), time series prediction (Li et al., 2019), and natural language processing (Devlin et al., 2019). Various machine learning models have been successfully applied in sequence modeling to handle different types of sequence data, ranging from the (probabilistic) Hidden Markov model (Baum and Petrie, 1966) to deep learning models, e.g., Recurrent Neural Networks (RNNs), Long ShortTerm Memory units (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (Chung et al., 2014), and transformers (Vaswani et al., 2017). In this paper, we focus on the state space model (SSM), which has a simple mathematical expression: $h^{\\prime}(t)=A h(t)+B x(t), y(t)=C h(t)+D x(t)$ where $h(t)$ is the hidden state, $x(t)$ is the input sequence, $y(t)$ is the output sequence and $A, B, C, D$ are trainable parameters. To simplify the analysis, we omit the skip connection by letting $D=0$. In fact, our analysis can also applied to the case when $D$ is included (see the discussions in Section 4.2). Recent studies have demonstrated the power of SSMs in deep learning. For example, it was shown in Gu et al. (2022a) that by a new parameterization and a carefully chosen initialization, the structured state space sequence (S4) model achieved strong empirical results on image and language tasks. Following the S4 model, more variants of SSMs are proposed, e.g., diagonal SSMs (Gu et al., 2022b; Gupta et al., 2022) S5 (Smith et al., 2023), H3 (Fu et al., 2023), GSS (Mehta et al., 2023), Hyena Hierarchy (Poli et al., 2023), and Mamba (Gu and Dao, 2023). Theoretical analysis and understanding of the approximation and optimization of SSMs are well studied in the literature such as (Li et al., 2021; 2022; Gu et al., 2022a; 2023). Since the SSM can be regarded as a continuous linear RNN model (Li et al., 2022), most generalization analysis of SSMs is based on the generalization theory of RNNs (Zhang et al., 2018; Chen et al., 2019; Tu et al., 2019). However, these previous works did not study the effects of the temporal dependencies in the sequence data on the SSM generalization\n(see more details on the comparison in Section 4.1). As an attempt to understand the relationship between the temporal dependencies and the generalization performance, this paper aims to provide a generalization bound that connects the memory structure of the model with the temporal structure of the data. We can, in turn, use the proposed bound to guide us in designing new algorithms to improve optimization and generalization. Specifically, we discover two roles for the proposed generalization measure: (1) generalization bound as an initialization scheme; (2) generalization bound as a regularization method. The common initialization method for the S4 model and its variants follows from the HiPPO framework (Gu et al., 2022a; 2023), which is based on the prerequisite that the training sequence data is stable. To improve the robustness of the output value scales on SSMs to different temporal patterns in the sequence data, we consider to rescale the initialization of SSMs with respect to the generalization measure. This new initialization scheme makes the SSMs more resilient on their initial output value scales to variations in the temporal patterns of the training data. Except for the initialization setup, our generalization bound can also be served as a regularizer. Regularization methods like weight decay and dropout are widely applied to training SSMs, but the hidden state matrix $A$ is not regularized because its imaginary part controls the oscillating frequencies of the basis function $e^{A t} B$ (Gu et al., 2022b). By taking into account the interaction between the SSM structure and the temporal dependencies, we introduce a new regularization method based on our bound, and it can be applied to the hidden state space to improve the generalization performance. Combining the initialization scheme and the regularization method, our method is applicable to various tasks, ranging from image classification to language processing, while only introducing a minimal computational overhead. To summarize, our contributions are as follows:\n\n- We provide a data-dependent generalization bound for SSMs by taking into account the temporal structure. Specifically, the generalization bound correlates with the memory structure of the model and the (auto)covariance process of the data. It indicates that instead of the weight or the data norm, it is the interplay between the memory structure and the temporal structure of the sequence data that influences the generalization. - Based on the proposed generalization bound, we setup an initialization scaling rule by adjusting the magnitude of the model parameters with respect to the generalization measure at initialization. This scaling rule improves the robustness of the initial output value scales on SSMs across different temporal patterns of the sequence data. - Apart from the initialization scheme, we design a new regularizer for SSMs. Unlike weight decay, our regularizer does not penalize the parameter norm but encourages the model to find a minimizer with lower generalization bound to improve the generalization performance. ## 2 Related Works\n\nSince a SSM is also a continuous linear RNN, there are three lines of related work: generalization of RNNs, temporal structure analysis on RNNs, and optimization of SSMs. Generalization of RNNs. Existing works on the generalization of RNNs focus on the generalization error bound analysis. Specifically, in the early two works of Dasgupta and Sontag (1995) and Koiran and Sontag (1998), VC dimension-based generalization bounds were provided to show the learnability of RNNs.",
    "genanatooptdesign-2": "In recent studies, Zhang et al.",
    "genanatooptdesign-3": "(2018); Chen et al. (2019); Tu et al. (2019) proved norm-based generalization bounds, improving the VC dimension-based bounds by the Rademacher complexity technique (Bartlett and Mendelson, 2002) under the uniform-convergence framework. In the overparameterization settings, it was\nshown in Allen-Zhu and Li (2019) that RNNs can learn some concept class in polynomial time given that the model size is large enough. These generalization bounds, however, do not take into account the temporal dependencies and their effects on generalization. In this work, we provide a new generalization bound by combining the memory structure of the model and the temporal structure of the data. Temporal structure analysis on RNNs. Sequence data has long-range temporal dependencies across the time domain, which notably set it apart from non-sequence data. Recent studies have studied the effects of such temporal dependencies on the approximation and optimization of RNNs. For example, in the two works of Li et al. (2021; 2022), a \"curse of memory\" phenomenon was discovered when using linear RNNs to model the temporal input-output relationships. Particularly, when the target relationship between the input and output has a long-term memory, then both approximation and optimization become extremely challenging. In Wang et al. (2023), the \"curse of memory\" phenomenon on approximation and optimization was extended to non-linear RNNs based on the temporal relationships. In this paper, we conduct a finegrained analysis on the effects of the temporal structure analysis on the generalization of RNNs. Optimization of SSMs. RNN optimization is known for two issues: training stability and computational cost (Bengio et al., 1994; Pascanu et al., 2013). To address these issues and capture the long dependencies efficiently in sequence modeling, the S4 model was proposed by new paraemterization, initialization and discretization (Gu et al., 2022a). Recent variants for the S4 model simplified the hidden state matrix by a diagonal matrix to enhance computational efficiency (Gu et al., 2022b; Gupta et al., 2022; Smith et al., 2023; Orvieto et al., 2023). Regularization methods are also applied for SSMs to prevent overfitting, such as dropout, weight decay and the data continuity regularizer ( Qu et al., 2023). However, the principled way to regularize and initialize the parameters still remains to be explored. In this study, we design a new regularization and initialization scheme to improve both optimization and generalization. ## 3 Preliminaries\n\nIn this section, we briefly introduce the SSM in Section 3.1 and the motivation for optimization designs based on the generalization analysis in Section 3.2. ### 3.1 Introduction to SSMs\n\nIn this paper, we consider the following single-input single-output SSM,\n\n$$\nh^{\\prime}(t)=A h(t)+B x(t), \\quad y(t)=C h(t), \\quad t \\geq 0\n$$\n\nwhere $x$ is the input from an input space ${ }^{1} \\mathcal{X}:=C_{0}\\left(\\mathbb{R}_{\\geq 0}, \\mathbb{R}\\right) ; y(t) \\in \\mathbb{R}$ is the output at time $t ; h(t) \\in \\mathbb{R}^{m}$ is the hidden state with $h(0)=0 ; A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times 1}, C \\in \\mathbb{R}^{1 \\times m}$ are trainable parameters. Then (1) has an explicit solution $y(t)=\\int_{0}^{t} \\rho_{\\theta}(s) x(t-s) d s$, where $\\rho_{\\theta}(s):=C e^{A s} B$ with $\\theta=(C, A, B)$. The function $\\rho_{\\theta}(s)$ captures the memory structure of the model and the temporal input-output relationship (Li et al., 2022). For the S4 model and its variants (Gu et al., 2022a;b; Gupta et al., 2022; Gu et al., 2023), (1) is usually discretized by the Zero-Order Hold method, i.e., given a timescale $\\Delta \\in \\mathbb{R}, h_{k+1}=$ $\\bar{A} h_{k}+\\bar{B} x_{k}, \\quad y_{k}=\\bar{C} h_{k}, \\quad k=0,1, \\ldots$, where $\\bar{A}=e^{\\Delta \\cdot A}, \\bar{B}=\\left(\\bar{A}-\\mathbb{I}_{m}\\right) A^{-1} B, \\bar{C}=C$. Then, $y_{k}=\\bar{C} \\bar{A}^{k} \\bar{B} x_{0}+\\bar{C} \\bar{A}^{k-1} \\bar{B} x_{1}+\\ldots+\\bar{C} \\bar{B} x_{k}=[\\bar{K} * x]_{k}$ where $\\bar{K}=\\left(\\bar{C} \\bar{B}, \\bar{C} \\bar{A} \\bar{B}, \\ldots, \\bar{C} \\bar{A}{ }^{k} \\bar{B}\\right)$ and $*$ represents to convolution. [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-04.jpg?height=421&width=1435&top_left_y=242&top_left_x=402)\n\nFigure 1: The logic diagram goes from generalization analysis to optimization designs. ### 3.2 Motivation: a linear regression model\n\nIn this subsection, we use a linear regression model on non-sequential data as an example to illustrate the connection between the generalization analysis and the optimization designs. This example then motivates us to extend the connection to SSMs on sequential data. Linear regression. We consider a simple linear model $y=\\theta^{\\top} x$ with input $x \\in \\mathbb{R}^{d}$, output $y \\in \\mathbb{R}$ and parameter $\\theta \\in \\mathbb{R}^{d}$. Let the training data $\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{n}$ be i.i.d. sampled from a distribution $\\mathcal{D}$ such that $\\left\\|x_{i}\\right\\|_{2}=r,\\left|y_{i}\\right| \\leq 1(\\forall i \\in[1: n])$. Define the empirical risk $\\mathcal{L}_{n}(\\theta):=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\theta^{\\top} x_{i}-y_{i}\\right)^{2}$ and the population risk $\\mathcal{L}_{\\mathcal{D}}(\\theta):=\\mathbb{E}_{x, y}\\left[\\left(\\theta^{\\top} x-y\\right)^{2}\\right]$. Then given a norm-constrained space $\\Theta:=\\left\\{\\theta \\in \\mathbb{R}^{d}:\\|\\theta\\|_{2} \\leq\\right.$ $R\\}$, with probability at least $1-\\delta$ over $\\mathcal{D}$,\n\n$$\n\\sup _{\\theta \\in \\Theta}\\left|\\mathcal{L}_{n}(\\theta)-\\mathcal{L}_{\\mathcal{D}}(\\theta)\\right| \\leq(r R+1)^{2} \\cdot \\mathcal{O}(\\sqrt{\\log (1 / \\delta) / n})\n$$\n\nThis is a well-known norm-based generalization bound based on the Rademacher theory (Mohri et al., 2012), and we provide a proof in Appendix B for completeness. Notice that the key term $r^{2} R^{2}$ in the generalization bound (2) is also an upper bound for the magnitude of the linear model output, i.e., $\\sup _{\\theta \\in \\Theta}\\left(\\theta^{\\top} x_{i}\\right)^{2} \\leq r^{2} R^{2}$. Thus, we connect the model stability with the generalization bound stability, and this connection induces an initialization scheme for the initialization $\\theta^{(0)}$ by setting $\\left\\|\\theta^{(0)}\\right\\|_{2} \\sim \\mathcal{O}(1 / r)$. In particular, if we normalize each input $x_{i}$ such that $r$ is also $\\mathcal{O}(1)$, then $\\left\\|\\theta^{(0)}\\right\\|_{2} \\sim \\mathcal{O}(1)$. Since $\\theta^{(0)} \\in \\mathbb{R}^{d}$, one possible initialization scheme is that $\\theta^{(0)}$ follows a Uniform distribution $U[-1 / \\sqrt{d}, 1 / \\sqrt{d}]$, which corresponds to the Kaiming initialization (up to some constant) (He et al., 2015). When treating the term $r^{2} R^{2}$ as a regularizer to improve the generalization, we get the weight decay method, i.e., the $\\ell_{2}$ regularization w.r.t.",
    "genanatooptdesign-4": "$\\|\\theta\\|_{2}^{2}$. We summarize the above logic chain that connects the generalization analysis with optimization designs in Figure 1. Now for SSMs, we extend the generalization analysis from non-sequential data to sequential data by taking into account the temporal structure of the data. This linear regression example motivates us to apply the same logic diagram (Figure 1) to the SSMs, and this is exactly what we are going to present in the following part of this paper. ## 4 Main results\n\nIn this section, we first give a generalization bound for SSMs in Section 4.1, then we design a new initialization scheme in Section 4.2 based on this proposed bound. Apart from the initialization scheme, we introduce a new regularization method in Section 4.3. Finally, we conduct experiments to test the initialization scheme and the regularization method in Section 4.4. ### 4.1 A generalization bound of SSMs\n\nIn this section, we present a generalization bound for the SSM (1) and reveal the effects of the temporal dependencies on the generalization performance. We show that our bound gives a tighter estimate compared with previous norm-based bounds through a toy example. Following the same notation in Section 3.1, we define the empirical risk $R_{n}(\\theta)$ and the population risk $R_{x}(\\theta)$ as\n\n$$\nR_{n}(\\theta):=\\frac{1}{n} \\sum_{i=1}^{n}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-s) x_{i}(s) d s-y_{i}\\right|^{2}, \\quad R_{x}(\\theta):=\\mathbb{E}_{x}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-s) x(s) d s-y\\right|^{2}\n$$\n\nwhere $T>0$ is some finite terminal time, the training sequence data $\\left\\{x_{i}(t)\\right\\}_{i=1}^{n}$ are independently sampled from a stochastic process with mean $\\mathbb{E}[x(t)]:=\\mu(t)$ and covariance $\\mathbb{E}[(x(s)-\\mu(s))(x(t)-\\mu(t))]:=$ $K(s, t)$, and the label $y$ is generated by some underlying functional $H_{T}: \\mathcal{X} \\rightarrow \\mathbb{R}$, i.e., $y=H_{T}(x)$. We assume that $|y| \\leq 1$ for any $x \\in \\mathcal{X}$, otherwise, we truncate the value of the label to 1 . In the next, we make an assumption on the normalized process $\\tilde{x}(t):=(x(t)-\\mu(t)) / \\sqrt{K(t, t)}$ :\n\nAssumption 1. The normalized process $\\tilde{x}(t)$ is (1): almost surely H\u00f6lder continuous, i.e., $\\exists L, H>$ $0, s . t . \\forall s, t \\in[0, T],|\\tilde{x}(s)-\\tilde{x}(t)| \\leq L|s-t|^{H}$ a.s.; (2): is $\\sigma^{2}$-sub-Gaussian for every $t \\in[0, T]$, i.e., $\\exists \\sigma>0$, s.t. $\\forall u>0, P(|\\tilde{x}(t)| \\geq u) \\leq 2 \\exp \\left(-u^{2} / 2 \\sigma^{2}\\right)$ for any $t \\in[0, T]$. We leave the discussion of the assumption after the statement of the main theorem. Now we proceed to bound generalization gap $\\left|R_{x}(\\theta)-R_{n}(\\theta)\\right|$ by establishing uniform convergence of the empirical risk to its corresponding population risk, as stated in following theorem:\nTheorem 1. For a $\\operatorname{SSM} \\int_{0}^{T} \\rho_{\\theta}(T-s) x(s) d s$, following notations and settings in Section 3.1 \\& 4.1, we define $\\psi(\\Theta):=\\sup _{\\theta \\in \\Theta} \\int_{0}^{T}\\left|\\rho_{\\theta}(T-s)\\right| \\sqrt{K(s, s)} d s+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-s) \\mu(s) d s\\right|$. Then under Assumption 1, given a parameter space $\\Theta$ for $\\theta$, for any $\\delta \\in(0,1)$, with probability at least $1-\\delta$ over the training sequences,\n\n$$\n\\sup _{\\theta \\in \\Theta}\\left|R_{x}(\\theta)-R_{n}(\\theta)\\right| \\leq(\\psi(\\Theta)+1)^{2} \\cdot \\mathcal{O}\\left(\\log ^{3 / 2}(\\operatorname{Tn} / \\delta) / \\sqrt{n}\\right)\n$$\n\nWhere $\\mathcal{O}$ hides a constant that depends on $\\sigma, L, H$. The proof is given in Appendix E. We see that this bound decreases to zero as the sample size $n \\rightarrow \\infty$, provided that the terminal time $T$ is finite and $\\psi_{\\Theta}$ grows slower than $\\sqrt{n}$. For example, when the data statistics (e.g., $\\mu(s)$ and $K(s, s)$ ) are uniformly bounded along the time horizon, by the exponentially decay property of the SSM function $\\rho_{\\theta}(s)$, we have $\\psi_{\\Theta}$ is finite, then the generalization bound is $\\tilde{\\mathcal{O}}(1 / \\sqrt{n})$, yielding that the mean and variance at each length position together play important roles in generalization analysis. Proof sketch. The proof is based on Rademacher theory (Bartlett and Mendelson, 2002). The main difficulty is to bound the Rademacher complexity of the $\\operatorname{SSM}$ function $\\int_{0}^{T} \\rho_{\\theta}(T-s) x(s) d s$ for a stochastic process $x(s)$. We first use the H\u00f6lder inequality to get an upper bound for the Rademacher complexity w.r.t. the normalized process $\\tilde{x}(s)$, then combining H\u00f6lder continuity and the heavy-tail property in Assumption 1 , we show the finiteness of $\\sup _{s \\in[0, T]} \\tilde{x}(s)$. Finally we use an $\\varepsilon$-net argument to give an explicit bound for the Rademacher complexity, which then finishes the proof. Discussions of Assumption 1. This assumption contains two parts. H\u00f6lder continuity is used to bound $\\sup _{s \\in[0, T]} \\tilde{x}(s)$ and the Rademacher complexity of the SSM function class. By the Kolmogorov continuity theorem (Stroock and Varadhan, 1997), H\u00f6lder continuity covers a wide range of random process that satisfies certain inequalities for its moments. For the sub-Gaussian property, it ensures $\\tilde{x}(s)$ is bounded in a finite time set with high probability. Sub-Gaussian random variables include Gaussian and any bounded\nvariables. Specifically, for image classification tasks with flattened image pixels, if the range of the pixel values is a finite class (e.g., integer numbers from 0 to 255 ), then the H\u00f6lder continuity condition can be dropped.",
    "genanatooptdesign-5": "We leave more detailed discussions and provide some concrete examples that satisfy Assumption 1 in Appendix C. Comparison to previous bounds. Since a SSM is also a continuous linear RNN model, we compare (3) with previous bounds for linear RNNs. A generalization bound $\\widetilde{\\mathcal{O}}\\left(\\|x\\|_{2}\\|B\\|_{2}\\|C\\|_{2}\\|A\\|_{2} / \\sqrt{n}\\right)$ is provided In Chen et al. (2019), where $\\|x\\|_{2}$ is the 2 -norm of the discrete input sequence. In the continuous case, $\\|x\\|_{2}$ corresponds to the $L^{2}$ norm w.r.t. a Dirac measure. By changing the matrix 2-norm to matrix 1-norm, Tu et al. (2019) shows another similar generalization bound. These bounds separate the data complexity and the model complexity by the data norm and the model parameter norm individually, and do not account for the temporal dependencies across the time domain. In this work, instead, we incorporate the temporal dependencies via the sequence statistics (mean and variance) to get a generalization bound. Next, we use a toy example to illustrate that our bound gives a tighter estimation. Given a stochastic process $\\{x(t)\\}_{t \\in[0, T]}$ with mean $\\mu(t)$ and covariance $K(s, t)$, we consider the following two upscale transformations (by increasing $T$ to $2 T)$ :\n\n1. left zero padding: $x_{1}(t)=0, t \\in[0, T) ; \\quad x_{1}(t)=x(t-T), t \\in[T, 2 T]$\n2. right zero padding: $x_{2}(t)=x(t), t \\in[0, T] ; \\quad x_{2}(t)=0, t \\in(T, 2 T]$\n\nThen the two SSM outputs are given by $y_{i}(2 T)=\\int_{0}^{2 T} \\rho_{\\theta}(2 T-s) x_{i}(s) d s$ for $i=1,2$. Hence,\n\n$$\ny_{1}(2 T)=C \\int_{0}^{T} e^{A(T-s)} B x(s) d s, \\quad y_{2}(2 T)=C e^{A T} \\int_{0}^{T} e^{A(T-s)} B x(s) d s\n$$\n\nWe see that the magnitude of $y_{1}(2 T)$ and $y_{2}(2 T)$ differs with an exponential factor $e^{A T}$. Since all the eigenvalues of $A$ have negative real part, $y_{2}(2 T) \\rightarrow 0$ as $T$ increases. Hence, the right zero padding transformation degenerates the SSM function class to a zero function class for large $T$, inducing a minimal generalization gap that only contains the statistical sampling error (see (3) by letting $K(s, s)=\\mu(s)=0$ ). Therefore, a desired generalization bound should reflect such a difference caused by the different temporal dependencies. However, previous norm-based generalization bounds do not capture such a difference for these two transformations as they produce the same $L^{2}$ norm for the input sequence. Let us see what happens for our proposed generalization measure. For the left zero padding, the key term in (3) becomes\n\n$$\n\\int_{0}^{T}\\left|C e^{A(T-s)} B\\right| \\sqrt{K(s, s)} d s+\\left|\\int_{0}^{T} C e^{A(T-s)} B \\mu(s) d s\\right|+1\n$$\n\nFor the right zero padding, the key term in (3) becomes\n\n$$\n\\int_{0}^{T}\\left|C e^{A T} e^{A(T-s)} B\\right| \\sqrt{K(s, s)} d s+\\left|\\int_{0}^{T} C e^{A T} e^{A(T-s)} B \\mu(s) d s\\right|+1\n$$\n\nThe detailed derivations are given in Appendix D. By the same argument, our bound (3) indeed captures the difference on the magnitude of the generalization performance for these two sequence transformations. In particular, as $T \\rightarrow \\infty,(5)$ reduces to 1 , which yields a minimal generalization gap as expected for the zero function class. In that sense, we get a tighter bound for the SSMs. Zero shot transferability. A benefit of SSMs is the zero-shot transferability to other sampling frequencies (i.e., the timescale measure in continuous case). For example, for a SSM function $y_{T}=\\int_{0}^{T} \\rho_{\\theta}(T-$\ns) $x(s) d s$, if we downscale the input sequence $x(s)$ by half of the sampling frequency, then the SSM output becomes $y_{\\tilde{\\theta}}=\\int_{0}^{T / 2} \\rho_{\\theta}(T-2 s) x(2 s) d s$, which equals to $\\int_{0}^{T} \\frac{1}{2} \\rho_{\\theta}(T-s) x(s) d s$. Now for a new SSM parameter $\\tilde{\\theta}=(2 C, A, B)$, we have $\\rho_{\\tilde{\\theta}}(s)=2 \\rho_{\\theta}(s)$, indicating that by simply modifying the SSM parameters, one can transfer the model to half the sampling frequency while keeping the output invariant. One advantage for our generalization measure is that it is also zero shot transferable. To see this, we use the same example here. Under the downscale sampling, both $\\int_{0}^{T}\\left|\\rho_{\\theta}(T-s)\\right| \\sqrt{K(s, s)} d s$ and $\\left|\\int_{0}^{T} \\rho_{\\theta}(T-s) \\mu(s) d s\\right|$ remain invariant for the new parameter $\\tilde{\\theta}$ because $\\sqrt{K(s, s)}$ and $\\mu(s)$ have the same scaling as $x(s)$. Similarly, other sampling frequencies are also zero shot transferable for our generalization measure by simply adjusting the SSM parameters. ### 4.2 Generalization bound as an initialization scheme\n\nIn this section, we design a scaling rule for the SSM parameters at initialization based on the generalization bound (3). This new initialization scheme improves the robustness of the initial output value scales on SSMs across different temporal patterns of the sequence data. Our proposed initialization scheme is built on the HiPPO based initialization (Gu et al., 2023), which is a data independent initialization method. Specifically, the HiPPO framework initializes the hidden state matrices $A, B$ to produce orthogonal basis functions, and the matrix $C$ to be standard normal for training stability. However, the argument for the training stability relies on the prerequisite that the input sequence is constant along the time index (Gu et al. (2023, Corollary 3.4)), which has some limitations in applicability as the long-range dependencies may lead to very different temporal patterns on the input sequence. As the dashed lines in the left and the right part of Figure 2 show, the SSM output value scale and the loss value scale under the HiPPO based initialization vary much across different temporal dependencies, making the loss values inconsistent during training. To address this issue, we follow the logic diagram in Figure 1 by adjusting the generalization complexity to be $\\mathcal{O}(1)$. Specifically, we extract the dominant term in the generalization bound (3):\n\n$$\n\\tau(\\theta):=\\left(\\int_{0}^{T}\\left|\\rho_{\\theta}(T-s)\\right| \\sqrt{K(s, s)} d s+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-s) \\mu(s) d s\\right|\\right)^{2}\n$$\n\nNotice that $\\rho_{\\theta}(s)=C e^{A s} B$, if we rescale $C$ to $\\xi C$ for some $\\xi \\in \\mathbb{R}$, we have $\\tau(\\tilde{\\theta})=\\xi^{2} \\cdot \\tau(\\theta)$ for $\\tilde{\\theta}=(\\xi C, A, B)$. This induces a new initialization scheme, i.e., once the parameters $\\theta=(C, A, B)$ are initialized by the HiPPO method, we rescale $C$ to $\\tilde{C}$ such that\n\n$$\n\\tilde{C}=\\frac{1}{\\sqrt{\\tau(\\theta)}} \\cdot C\n$$\n\nThis rescaling method guarantees the SSM output value is bounded at initialization for any stochastic process that satisfies Assumption 1, ensuring the robustness of the initial loss value scales on SSMs across different temporal structures. We formalize the statement in Proposition 1. Proposition 1. Consider a $S S M \\int_{0}^{T} \\rho_{\\theta}(T-s) x(s) d s$ with $\\theta=(C, A, B)$, for any random process $x(s)$ satisfies Assumption 1, let $\\tilde{C}$ be given by the rescaling method (7), then for $\\tilde{\\theta}:=(\\tilde{C}, A, B)$, we have $\\mathbb{E}_{x}\\left[\\left|\\int_{0}^{T} \\rho_{\\tilde{\\theta}}(T-s) x(s) d s\\right|\\right] \\leq \\mathcal{O}(\\sqrt{\\log T})$. Here $\\mathcal{O}$ hides a constant that only depends on $\\sigma$ and $L$ as described in Assumption 1. ```\nAlgorithm 1 Training an \\(\\ell\\)-layer SSM with the scheme (7)\nInput: training sequences with length \\(L\\), model dimension \\(d\\), projection matrix \\(C_{i}\\) of \\(i\\)-th layer, number of\n    epochs \\(S\\)\n    for \\(s=0\\) to \\(S-1\\) do\n        if \\(s=0\\) then\n            Sample a minibatch sequence from the training sequences\n            for \\(i=1\\) to \\(\\ell\\) do\n            Compute mean \\(\\mu_{i} \\in \\mathbb{R}^{L \\times d}\\), variance \\(K_{i} \\in \\mathbb{R}^{L \\times d}\\) of the \\(i\\)-th layer's inputs along the batch\n    dimension \\(\\quad \\triangleright\\) Inputs of the \\(i\\)-th layer depend on model parameters of the first \\(i-1\\) layers\n            Calculate the SSM kernel \\(k_{i} \\in \\mathbb{R}^{L \\times d}\\) by the model parameters of the \\(i\\)-th layer\n            \\(\\tau_{i} \\leftarrow\\left[\\left|k_{i}\\right| * \\sqrt{K_{i}}+\\left|k_{i} * \\mu_{i}\\right|\\right]_{L} \\in \\mathbb{R}^{d}\\)\n            Averaging over the feature dimension: \\(\\tau_{i} \\leftarrow\\left\\|\\tau_{i}\\right\\|_{2}^{2} / d\\)\n            Update: \\(C_{i} \\leftarrow C_{i} / \\sqrt{\\tau_{i}}\\)\n                end for\n        end if\n        Regular training procedure for the updated initialization\n    end for\n```\n\nThe proof is provided in Appendix F. Proposition 1 shows that the expected SSM output value are bounded for any stochastic processes that satisfies Assumption 1, even when the input sequence is not almost surely bounded. This improves the robustness of the output value scales on SSMs in the sense that the scale of the output value does not depend on the variations of the temporal structures. It is worth noting that different from normalization methods such as min-max normalization and standardization, our method only changes the model parameters. This is important because normalization on data numerical values in language tasks can lead to loss of crucial information. For example, mathematical expressions like \" $\\max (1,9)=9$ \" have a contextual meaning where normalization may result in the loss of structured information that is essential to understand. Implementation for high-dimensional, multi-layer SSMs. In the practical training, the SSMs used for tasks such as image classification or language processing are usually deep and high dimensional ( $d>1$ ), while our initialization scheme (7) is designed based on the one-dimensional shallow SSM. To extend to high-dimensional SSMs, we empirically treat all features to be independent and calculate $\\tau(\\theta)$ by its average along the feature dimension. For an $\\ell$-layer SSM with the initial projection matrix $C_{1}, \\ldots, C_{\\ell}$ at each layer, we first calculate the complexity measure $\\tau_{1}$ for the first layer and rescale $C_{1}$ by $C_{1} / \\sqrt{\\tau_{1}}$. Then we calculate the complexity measure $\\tau_{2}$ for the second layer by the updated input sequence of layer 2 and rescale $C_{2}$ by $C_{2} / \\sqrt{\\tau_{2}}$. We repeat this process until the last layer. We describe the complete procedures in Algorithm 1 , where the $|\\cdot|$ and $\\sqrt{ }$ in Line 7 represent to element-wise absolute value and element-wise square root respectively. $[\\cdot]_{L}$ extracts the last position of an element obtained from the convolution. The extension of our theory to the multi-layer case is an interesting direction, which we leave for future work. Skip connections and nonlinearities. There are several gaps between the theory and the methodologies in this paper. The first one that the skip connection matrix $D$ is omitted in our defined model (1). This will not affect our generalization bound because we may express the explicit solution for (1) as $y(t)=$ $\\int_{0}^{t}\\left(\\rho_{\\theta}(s)+D \\delta(s)\\right) x(t-s) d s$ where $\\delta(\\cdot)$ is a delta function. In that case, the SSM is still a convolution model but with a new kernel function $\\rho_{\\theta}(s)+D \\delta(s)$. However, the initialization scheme (7) only adjusts $C$ and requires the kernel function to be linear in $C$. Hence, (7) may not work well when $D x(t)$ is much larger than $\\int_{0}^{t} \\rho_{\\theta}(s) x(t-s) d s$. However, we can still derive a proper rescaling scheme for this case. One straightforward\n\n```\nAlgorithm 2 Training an \\(\\ell\\)-layer SSM with the scheme (8)\nInput: training sequences with length \\(L\\), model dimension \\(d\\), initialization \\(\\theta_{0}\\), loss function \\(\\mathcal{L}\\), regulariza-\n    tion factor \\(\\lambda\\), optimizer OPT, number of epochs \\(S\\)\n    for \\(s=0\\) to \\(S-1\\) do\n        Sample a minibatch from the training sequences\n        Set total complexity \\(\\tau=0\\)\n        for \\(i=1\\) to \\(\\ell\\) do\n            Compute mean \\(\\mu_{i} \\in \\mathbb{R}^{L \\times d}\\) and variance \\(K_{i} \\in \\mathbb{R}^{L \\times d}\\) for inputs of the \\(i\\)-th layer along the batch\n    dimension\n            Calculate the SSM kernel \\(k_{i} \\in \\mathbb{R}^{L \\times d}\\) by the model parameters of the \\(i\\)-th layer\n            \\(\\tau_{i} \\leftarrow\\left[\\left|k_{i}\\right| * \\sqrt{K_{i}}+\\left|k_{i} * \\mu_{i}\\right|\\right]_{L} \\in \\mathbb{R}^{d}\\)\n            Averaging over the feature dimension: \\(\\tau_{i} \\leftarrow\\left\\|\\tau_{i}\\right\\|_{2}^{2} / d\\)\n            Add the complexity of the \\(i\\)-th layer: \\(\\tau \\leftarrow \\tau+\\tau_{i}\\)\n        end for\n        Compute the training loss: \\(\\mathcal{L} \\leftarrow \\mathcal{L}+\\lambda \\cdot \\tau\\)\n        Parameters update: \\(\\theta_{i+1} \\leftarrow \\operatorname{OPT}\\left(\\theta_{i}, \\mathcal{L}\\right)\\)\n    end for\nOutput: Updated model parameter \\(\\theta_{s}\\)\n```\n\nway is that we first calculate $\\tau(\\theta)$ for a given initialization, and then rescale $C, D$ as $C \\sqrt{\\tau(\\theta)}$ and $D / \\sqrt{\\tau(\\theta)}$ respectively. This reinitialization method guarantees that $\\tau(\\theta)=1$ after rescaling. The second gap is that our theory is for single-layer linear SSMs. When nonlinearities are added, our generalization bound still works for single-layer SSMs if the nonlinearity does not affect the H\u00f6lder condition and the sub-Gaussian property (Assumption 1). For Lipschitz (also H\u00f6lder continuous) nonlinearities, there are some known examples (see Appendix G ) where the sub-Gaussian condition still remains after the nonlinearity. ### 4.3 Generalization bound as a regularization method\n\nIn addition to its role as an initialization scheme, the generalization measure can also be regarded as a regularizer. In this section, we utilize the bound (3) to design a regularization method to improve the generalization performance, and simultaneously bring a little extra computational cost. For the generalization bound (3), we consider to use the dominant term (for large $T$ ) $\\tau(\\theta)$ defined in (6) as a regularizer. Then, the new empirical risk with regularization is given by\n\n$$\n\\tilde{R}_{n}(\\theta):=R_{n}(\\theta)+\\lambda \\cdot \\tau(\\theta)\n$$\n\nwhere $\\lambda \\geq 0$ is the regularization coefficient. When training multi-layer SSMs, we calculate the complexity $\\tau(\\theta)$ in (8) at each layer and add them together as a total regularization. We describe the training procedures for one-layer SSMs in Algorithm 2, where the notations follow Algorithm 1. Computational cost analysis. From the training procedures in Algorithm 2, we can see that the newly introduced training complexity mainly comes from the calculation for the convolution between the SSM kernel and the sequence statistics $(\\mu, K)$. Since the convolution can be conducted by the fast Fourier transform (Gu et al., 2022a) with complexity $\\mathcal{O}(b d L \\log L)$ where $b$ is the batch size. Then the new complexity for Algorithm 2 becomes $\\mathcal{O}((b+2) d L \\log L)$, which is acceptable in the practical training. We also include a concrete comparison of the running times in training real datasets to confirm this in Table 2. ![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-10.jpg?height=536&width=1627&top_left_y=247&top_left_x=245)\n\nFigure 2: Effects of the initialization scheme (7) on the model output scale, the gradient norm and the training loss under different temporal dependencies by varying the moment coefficient $b=0.01,0.1,1$. (Left) The output $\\mathbb{E}_{x}\\left[\\left|y_{L}\\right|\\right]$ at initialization w.r.t. the Gaussian white noise sequence $\\left(x_{1}, \\ldots, x_{L}\\right)$ for length $L$ from 1 to 1000; (Middle) The gradient norm $\\left\\|\\nabla R_{n}(\\theta)\\right\\|$ at initialization w.r.t. the mean squared error (MSE) for varied sequence length; (Right) The training MSE curve for the Gaussian white noise with length $L=1000$. |  | Training loss (MSE) |  |  | Test loss (MSE) |  |  | Generalization measure $\\psi_{\\Theta}^{2} / \\sqrt{n}$ |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $b=1$ | $b=0.1$ | $b=0.01$ | $b=1$ | $b=0.1$ | $b=0.01$ | $b=1$ | $b=0.1$ | $b=0.01$ |\n| w/o (7, 8) | $0.15_{ \\pm 0.002}$ | $0.67_{ \\pm 0.09}$ | $2.50_{ \\pm 0.52}$ | $0.25_{ \\pm 0.01}$ | $1.01_{ \\pm 0.14}$ | $4.70_{ \\pm 0.77}$ | $0.93_{ \\pm 0.20}$ | $5.16_{ \\pm 0.84}$ | $46.23_{ \\pm 7.49}$ |\n| $\\mathrm{w}(7)$ | $\\mathbf{0 . 1 1}_{ \\pm 0.006}$ | $\\mathbf{0 . 2 7}_{ \\pm 0.02}$ | $\\mathbf{0 . 2 0}_{ \\pm 0.01}$ | $0.20_{ \\pm 0.003}$ | $0.75_{ \\pm 0.05}$ | $1.06_{ \\pm 0.12}$ | $0.45_{ \\pm 0.03}$ | $2.36_{ \\pm 0.44}$ | $7.19_{ \\pm 1.60}$ |\n| $\\mathrm{w}(8)$ | $0.21_{ \\pm 0.008}$ | $0.97_{ \\pm 0.12}$ | $4.83_{ \\pm 0.52}$ | $0.22_{ \\pm 0.008}$ | $0.87_{ \\pm 0.07}$ | $3.59_{ \\pm 0.09}$ | $0.55_{ \\pm 0.05}$ | $2.766_{ \\pm 0.23}$ | $22.49_{ \\pm 0.78}$ |\n| $\\mathrm{w}(7,8)$ | $0.15_{ \\pm 0.005}$ | $0.37_{ \\pm 0.04}$ | $0.35_{ \\pm 0.02}$ | $\\mathbf{0 .",
    "genanatooptdesign-6": "1 8}_{ \\pm 0.004}$ | $\\mathbf{0 . 5 9}_{ \\pm 0.03}$ | $\\mathbf{0 . 6 0}_{ \\pm 0.01}$ | $\\mathbf{0 .",
    "genanatooptdesign-7": "2 3}_{ \\pm 0.01}$ | $\\mathbf{0 . 4 6}_{ \\pm 0.12}$ | $\\mathbf{0 . 4 6}_{ \\pm 0.07}$ |\n\nTable 1: Training loss, test loss and generalization measure $\\psi_{\\Theta}^{2} / \\sqrt{n}$ on Gaussian white noise sequences with different coefficients $b$ after convergence. By adding the initialization scheme (7), SSMs achieve better optimization performance and are more robust on the final training loss value across different temporal dependencies. By adding the regularization term (8), SSMs get better generalization performance (lower test loss). ### 4.4 Experiments\n\nThis section contains experiments to demonstrate the effectiveness of the proposed initialization scheme (7) and the regularization method (8). We use a synthetic dataset and the Long Range Arena (LRA) benchmark (Tay et al., 2021) for numerical validations. To simplify the notation, we use w/o (7, 8), w (7), w (8) and w $(7,8)$ to represent the baseline training without rescaling and regularization, training with rescaling, training with regularization and training with both rescaling and regularization respectively. A synthetic dataset. We consider a synthetic sequence dataset generated by a Gaussian white noise. To more closely resemble real datasets, we generate training inputs by sampling data from non-centered Gaussian white noise with mean $\\mu(s)=1$ and covariance $K(s, t)=\\frac{1}{|b| \\sqrt{\\pi}} e^{-((s-t) / b)^{2}}$, which is a stationary Gaussian process and satisfies Assumption 1 (see Section 4.1). Then we can get different temporal dependencies by varying the coefficient $b$, i.e., as the magnitude of $b$ decreasing, the temporal dependence of the\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-10.jpg?height=71&width=1649&top_left_y=2320&top_left_x=238)\n\n| ![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-11.jpg?height=43&width=471&top_left_y=227&top_left_x=253) |  | ListOps | Text | Retrieval | Image | Pathfinder | PathX | Average |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| S4-Legs | w/o $(7,8)$ | $61.16_{ \\pm 0.32}$ | $88.69_{ \\pm 0.07}$ | $91.21_{ \\pm 0.17}$ | $87.41_{ \\pm 0.14}$ | $95.89_{ \\pm 0.10}$ | $96.97_{ \\pm 0.31}$ | 86.89 |\n|  | w (7) | $60.79_{ \\pm 0.26}$ | $88.58_{ \\pm 0.21}$ | $91.29_{ \\pm 0.26}$ | $87.67_{ \\pm 0.29}$ | $95.79_{ \\pm 0.31}$ | $95.99_{ \\pm 0.18}$ | 86.69 |\n|  | w (8) | $\\mathbf{6 1 . 6 3}_{ \\pm 0.10}$ | $88.80_{ \\pm 0.27}$ | $91.17_{ \\pm 0.17}$ | $88.27_{ \\pm 0.14}$ | $\\mathbf{9 6 . 0 2}_{ \\pm 0.16}$ | $97.18_{ \\pm 0.20}$ | 87.18 |\n|  | w (7, 8) | $61.04_{ \\pm 0.25}$ | $88.53_{ \\pm 0.04}$ | $91.21_{ \\pm 0.31}$ | $\\mathbf{8 8 . 6 3}_{ \\pm 0.21}$ | $95.92_{ \\pm 0.45}$ | $96.51_{ \\pm 0.53}$ | 86.97 |\n|  | Time / epoch, w/o $(7,8)$ | $5 \\min 39 \\mathrm{~s}$ | $3 \\min 24 \\mathrm{~s}$ | 17 min 21 s | 1 min 55 s | $3 \\min 25 \\mathrm{~s}$ | 67 min 41 s | 16 min 34s |\n|  | Time / epoch, w (8) | 6 min 03s | 4 min 03s | 19 min 19 s | 2 min 08s | 3min 50s | 73min 10s | 18 min 6 s |\n| S4D-Legs | w/o $(7,8)$ | $60.80_{ \\pm 0.39}$ | $87.87_{ \\pm 0.03}$ | $90.68_{ \\pm 0.14}$ | $86.69_{ \\pm 0.29}$ | $94.87_{ \\pm 0.06}$ | $97.34_{ \\pm 0.07}$ | 86.38 |\n|  | w (7) | $60.97_{ \\pm 0.27}$ | $87.83_{ \\pm 0.16}$ | $91.08_{ \\pm 0.19}$ | $87.89_{ \\pm 0.11}$ | $94.72_{ \\pm 0.21}$ | $95.86_{ \\pm 0.66}$ | 86.40 |\n|  | w (8) | $61.32_{ \\pm 0.43}$ | $88.02_{ \\pm 0.06}$ | $91.10_{ \\pm 0.11}$ | $87.8_{ \\pm 0.09}$ | $\\mathbf{9 5 .",
    "genanatooptdesign-8": "0 4}_{ \\pm 0.07}$ | $\\mathbf{9 7 . 4 6}_{ \\pm 0.15}$ | 86.82 |\n|  | w (7, 8) | $\\mathbf{6 1 . 4 8}_{ \\pm 0.09}$ | $803.19_{ \\pm 0.42}$ | $\\mathbf{9 1 . 2 5}_{ \\pm 0.17}$ | $\\mathbf{8 8 . 1 2}_{ \\pm 0.25}$ | $94.93_{ \\pm 0.30}$ | $95.63_{ \\pm 0.48}$ | 86.60 |\n|  | Time / epoch, w/o $(7,8)$ | $5 \\min 10$ s | 3 min 07 s | 16 min 37s | 1 min 42 s | 3 min 02 s | 49min 39s | 13 min 13 s |\n|  | Time / epoch, w (8) | $5 \\min 33 \\mathrm{~s}$ | $3 \\min 13 \\mathrm{~s}$ | $18 \\min 43 \\mathrm{~s}$ | 1 min 565 | 3 min 28s | 55 min 33 s | 14 min 44 s |\n\nTable 2: Test accuracy and running time (per epoch on A100 GPU) on the LRA benchmark under different settings for different models.",
    "genanatooptdesign-9": "Mean and standard error are reported based on 3 independent runs. delta function $\\delta(x)$, entailing a zero temporal dependence for the sequence data. In the following experiment, we generate the sequence data by the Gaussian white noise with $b=$ $[1,0.1,0.01]$. For each input sequence $\\left(x_{1}, \\ldots, x_{L}\\right)$, its corresponding label is obtained by $\\sin \\left(x_{[L / 2]}\\right)$, i.e., the sine value of the time-lagged input. We use the S4-Legs model (Gu et al., 2022a) (that only contains the convolution layer) to train the sequence data. More details about the experiment setup are provided in Appendix A.1. In Figure 2, we plot the model output $\\mathbb{E}_{x}\\left[\\left|y_{L}\\right|\\right]$, the gradient norm $\\left\\|\\nabla R_{n}(\\theta)\\right\\|$ at initialization, and the training loss $(\\mathrm{w}(7))$ with different temporal patterns by varying the Gaussian white noise parameter $b$. We see that the initialization scheme (7) enhances the robustness of the output value scales (matches with Proposition 1), gradient norm at initialization and also the training loss value across different temporal structures. In Table 1, we report the training loss, test loss and the dominant generalization measure $\\psi_{\\Theta}^{2} / \\sqrt{n}$ after convergence. By comparing the final training loss with and without (7) in Table 1 (w/o $(7,8)$ vs w (7) and $\\mathrm{w}(8) \\mathrm{vs} \\mathrm{w}(7,8)$ ), we see that adding the rescaling scheme (7) also improves the training performance and makes the final training error more robust on different temporal dependencies (by varying $b$ ). For the regularization method (8), we compare the final test loss with and without (8) in Table 1 (w/o (7, 8) vs w (8) and $\\mathrm{w}(7) \\mathrm{vs} \\mathrm{w}(7,8)$ ). We can see that the our regularization method improves the generalization performance. Moreover, combining (7) and (8), the model get the best test performance across various temporal structures of the sequence data. The positive correlation between the generalization measure $\\psi_{\\Theta} / \\sqrt{n}$ and the test loss across different $b$ indicates that our generalization bound is able to capture different temporal dependencies. LRA benchmark. For real datasets, we investigate the effects of the initialization scheme (7) and the regularization method (8) on the LRA benchmark, which contains 6 tasks ranging from image classification to language processing. We consider to train two base models: 6-layer S4-Legs (Goel et al., 2022) and 6layer S4D-Legs (Gu et al., 2022b). For the S4-Legs model, the hidden state matrix $A$ is a full matrix while for the S4D-Legs model, $A$ is a diagonal matrix.",
    "genanatooptdesign-10": "We follow the training rules as described in Gu et al. (2023). When training with regularization (8), we vary the regularization coefficient $\\lambda$ with $10^{-3}, 10^{-4}, 10^{-5}$ for ListOps, Text, Retrieval, Image and Pathfinder tasks. For the most challenging task Path $X, \\lambda$ is taken from $10^{-4}, 10^{-5}, 10^{-6}$. We report the best test accuracy when training with regularization (8), and we include the exact running time for each epoch in Table 2. Note that the reproduction of the baseline numbers $(\\mathrm{w} / \\mathrm{o}(7,8))$ is inconsistent with the results in (Gu et al., 2022b). This is because we do not use the same PyTorch version and CUDA version as suggested in the official codebase, which may lead to the performance difference. However, these slight differences do not affect the scientific conclusions we draw from this paper. By comparing the best test accuracy for w/o $(7,8)$ vs w $(8)$ and w $(7)$ vs w $(7,8)$ in Table 2, we see that adding the regularization (8) enhances the generalization performance (test accuracy) in almost all the tasks for both S4-Legs and S4D-Legs models. When only adding the initialization scheme, by comparing w (7) vs w/o ( 7,8 ), the rescaling method becomes less effective compared to the synthetic case. This is because for the LRA benchmark, we follow the the original S4 paper (Gu et al., 2023) to add the batch norm/layer norm to the model, which may potentially help to decrease the temporal dependencies of the data, and thus the rescaling method is not so much effective as in the synthetic case. However, when combining the initialization scheme (7) and the regularization (8), one can still get the best test performance in half of tasks, indicating that our proposed optimization designs help to improve the generalization performance. We also compare the running time without or with the proposed optimization designs. Since (7) is conducted before training which will not introduce additional training complexity, we report the running time for w/o ( 7 , (8)) and w (8) in Table 2. The results show that the regularization brings a little extra computational cost, matching the computational cost analysis in Section 4.3. We provide an ablation study for the regularization coefficient $\\lambda$ in Appendix A.2. Results in Table 5 and Table 6 show that the test accuracy is much more sensitive to $\\lambda$ for the Pathfinder and PathX tasks compared to other tasks, which aligns with the findings of in Gu et al. (2023) that challenging tasks are more sensitive to the hyperparameters. More details on the dataset description and the experiment setup are given in Appendix A.2. We include additional experiment results in Appendix A. 3 for small S4-Legs and S4D-Legs with either smaller depth or smaller feature dimension. We can see in Table 9 that the improvements for small models are more significant (e.g., nearly $2 \\%$ on the most challenging PathX tasks for S4-Legs and $>1 \\%$ on the average accuracy for S4D-Legs). We also provide comparisons for different regularization schemes for both synthetic and real dataset. One regularization method is filter norm regularization, i.e., we regularize the $\\ell_{2}$ norm of the filter $\\rho_{\\theta}$, and another is weight decay on the hidden matrix $A$. Experiment results and details are shown in Appendix A.4. ## 5 Discussions\n\nIn this work, we study the optimization and the generalization for SSMs. Specifically, we give a datadependent generalization bound, revealing an effect of the temporal dependencies of the sequence data on the generalization. Based on the bound, we design two algorithms to improve the optimization and generalization for SSMs across different temporal patterns. The first is a new initialization scheme, by which we rescale the initialization such that the generalization measure is normalized. This initialization scheme improves the robustness of SSMs on the output scales across various temporal dependencies. The second is a new regularization method, which enhances the generalization performance in sequence modeling with only little extra computation cost. However, our theory does not apply to multi-layer SSMs and we do not address the feature dependencies when calculating the generalization measure (6) for high-dimensional SSMs, but simply treat all the features independent. It is interesting to understand the effects of depth and feature structures on optimization and generalization of SSMs, which we leave for future work. ## 6 Acknowledgement\n\nThis research is supported by the National Research Foundation, Singapore, under the NRF fellowship (project No. NRF-NRFF13-2021-0005). ## References\n\nZeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable generalization? Advances in Neural Information Processing Systems, 32, 2019. Ehsan Azmoodeh, Tommi Sottinen, Lauri Viitasaari, and Adil Yazigi. Necessary and sufficient conditions for h\u00f6lder continuity of gaussian processes. Statistics \\& Probability Letters, 94:230-235, 2014. Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002. Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554-1563, 1966. Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult.",
    "genanatooptdesign-11": "IEEE transactions on neural networks, 5(2):157-166, 1994. S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. OUP Oxford, 2013. Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural networks. arXiv preprint arXiv:1910.12947, 2019. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. Bhaskar Dasgupta and Eduardo Sontag. Sample complexity for learning recurrent perceptron mappings.",
    "genanatooptdesign-12": "Advances in Neural Information Processing Systems, 8, 1995. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics, June 2019. Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. In International Conference on Machine Learning, pages 7616-7633. PMLR, 2022. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 2022b. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages $1026-1034,2015$. Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine, 29(6):82-97, 2012. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. Pascal Koiran and Eduardo D Sontag. Vapnik-chervonenkis dimension of recurrent neural networks. Discrete Applied Mathematics, 86(1):63-79, 1998. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.",
    "genanatooptdesign-13": "2009. Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.",
    "genanatooptdesign-14": "Springer Science \\& Business Media, 2013. Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019. Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neural networks: Approximation and optimization analysis. In International Conference on Learning Representations, 2021. Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. The Journal of Machine Learning Research, 23(1):19972081, 2022. Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units.",
    "genanatooptdesign-15": "Advances in neural information processing systems, 31, 2018.",
    "genanatooptdesign-16": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150. Association for Computational Linguistics, June 2011. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. The MIT Press, 2012. Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning.",
    "genanatooptdesign-17": "arXiv preprint arXiv:1804.06028, 2018. Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. Eric Qu, Xufang Luo, and Dongsheng Li. Data continuity matters: Improving sequence modeling with lipschitz regularizer.",
    "genanatooptdesign-18": "In The Eleventh International Conference on Learning Representations, 2023. Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. The ACL Anthology network corpus. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries (NLPIR4DL), pages 54-61. Association for Computational Linguistics, August 2009. Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. Daniel W Stroock and SR Srinivasa Varadhan. Multidimensional diffusion processes, volume 233. Springer Science \\& Business Media, 1997. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. Zhuozhuo Tu, Fengxiang He, and Dacheng Tao. Understanding generalization in recurrent neural networks. In International Conference on Learning Representations, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Roman Vershynin. High-dimensional probability. University of California, Irvine, 2020. Shida Wang, Zhong Li, and Qianxiao Li. Inverse approximation theory for nonlinear recurrent neural networks. arXiv preprint arXiv:2305.19190, 2023. Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient svd parameterization.",
    "genanatooptdesign-19": "In International Conference on Machine Learning, pages 5806-5814. PMLR, 2018. ## A Experiments details\n\nIn this section, we provide more details for the experiments of the synthetic dataset and the LRA benchmark in Section 4.4. ## A. 1 The synthetic experiment\n\nFor the Gaussian white noise sequences, we generate 100 i.i.d. sequences for training and 1000 i.i.d. sequences for test. The timescale for the discrete sequences is set to be 1 , i.e., to generate a Gaussian white noise sequence with length $L$, we sample from a multivariate normal distribution with mean 1 and covariance matrix $K_{i, j}=h(i-j)$ for $i, j \\in[1: L]$, where $h(t)=\\frac{1}{|b| \\sqrt{\\pi}} e^{-(t / b)^{2}}$. The model that we use is the one-layer S4 model that only contains the FFTConv (fast Fourier transform convolution) layer and without activation and the skip connection ( $D=0$ ) (Gu et al., 2022a). The state space dimension for the FFTConv layer is 64 , other settings such as the discretization, the initialization and the parameterization follow the default settings in Gu et al. (2023), i.e., we use the ZOH discretization, the LegS initialization and the exponential parameterization for the hidden state matrix $A$. For the optimizer, we follow Gu et al. (2023) to set the optimizer by groups. For the (ZOH) timescale $\\Delta$, the hidden state matrices $A, B$, we use Adam optimizer with learning rate 0.001 , while for the matrix $C$, we use AdamW with learning rate 0.01 and decay rate 0.01 . For all the parameters, we use the cosine annealing schedule. The batch size is set to be 100 (full batch) and the training epochs is 100 . The regularization coefficient $\\lambda$ used for training with (8) is set to be 0.01 across all the temporal patterns.",
    "genanatooptdesign-20": "## A. 2 LRA benchmark\n\nDatasets. The datasets in the LRA benchmark contain (1) ListOps (Nangia and Bowman, 2018), a dataset that is made up of a list of mathematical operations with answers; (2) Text (Maas et al., 2011), a movie review dataset collected from IMDB, which is used for sentiment analysis; (3) Retrieval (Radev et al., 2009), a task of retrieving documents utilizing byte-level texts from the ACL Anthology Network. (4)Image (Krizhevsky et al., 2009), a sequential CIFAR10 dataset used for sequence classification; (5) Pathfinder (Linsley et al., 2018), a task that requires a model to tell whether two points in an image are connected by a dashed path. (6) PathX, a similar but more challenge task as Pathfinder with a higher image resolution increased from $32 \\times 32$ to $128 \\times 128$. Models. The models consist of S4-Legs and S4D-Legs. Both models use the default Legs initialization. Discretization and model parameterization are set to be consistent with Gu et al. (2023). For the optimizer, we also follow the standard setup in Gu et al. (2023) that the hidden state matrices are trained in a relatively small learning rate with no weight decay, while other parameters are trained with AdamW with a larger learning rate. Let $D, H, N$ denote the depth, feature dimension and hidden state space dimension respectively, we summarize the model hyperparameters for S4-Legs and S4D-Legs in Table 3 and Table 4 respectively. Ablation studies on $\\lambda$. When training with the regularization method (8), we vary the regularization coefficient $\\lambda$ for different magnitudes ranging from $10^{-6}$ to $10^{-3}$ when the model performs best on the validation set. In Table 5 and Table 6, we report the test accuracy on the LRA benchmark with different $\\lambda$ for the S4-Legs and S4D-Legs model respectively. From the results in Table 5 and Table 6, we find that for both models, adding the regularization helps the generalization performance (test accuracy) for all the tasks except for the Retrieval task trained by the S4-Legs model. In particular, the test accuracy is much more sensitive to the regularization coefficient $\\lambda$ for the Pathfinder and PathX tasks compared to other tasks. For\n\n|  | $D$ | $H$ | $N$ | Dropout | Learning rate | Batch size | Epochs | Weight decay |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 6 | 256 | 4 | 0 | 0.01 | 32 | 40 | 0.05 |\n| Text | 6 | 256 | 4 | 0 | 0.01 | 16 | 32 | 0.05 |\n| Retrieval | 6 | 256 | 4 | 0 | 0.01 | 64 | 20 | 0.05 |\n| Image | 6 | 512 | 64 | 0.1 | 0.01 | 50 | 200 | 0.05 |\n| Pathfinder | 6 | 256 | 64 | 0.0 | 0.004 | 64 | 200 | 0.05 |\n| PathX | 6 | 256 | 64 | 0.0 | 0.0005 | 16 | 50 | 0.05 |\n\nTable 3: List of the S4-Legs model hyperparameters for the LRA benchmark. |  | $D$ | $H$ | $N$ | Dropout | Learning rate | Batch size | Epochs | Weight decay |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 6 | 256 | 4 | 0 | 0.01 | 32 | 40 | 0.05 |\n| Text | 6 | 256 | 4 | 0 | 0.01 | 16 | 32 | 0.05 |\n| Retrieval | 6 | 256 | 4 | 0 | 0.01 | 64 | 20 | 0.05 |\n| Image | 6 | 512 | 64 | 0.1 | 0.01 | 50 | 200 | 0.05 |\n| Pathfinder | 6 | 256 | 64 | 0.0 | 0.004 | 64 | 200 | 0.05 |\n| PathX | 6 | 256 | 64 | 0.0 | 0.0005 | 16 | 50 | 0.05 |\n\nTable 4: List of the S4D-Legs model hyperparameters for the LRA benchmark. example, the variance of the test accuracy for the Pathfinder task is very high when $\\lambda=0.001$. For the PathX task, both the S4-Legs and the S4D-Legs model can not even learn the dataset when $\\lambda=0.0001$. The high sensitivity of the model in the hyperparameter aligns with the numerical findings in Gu et al.",
    "genanatooptdesign-21": "(2023). ## A. 3 Additional experiment results for small SSMs\n\nIn this section, we include more experiment results for smaller size of S4-Legs and S4D-Legs on the LRA benchmark. The best test accuracy results and the running time for the small models are reported in Table 9. The details for the model size and hyperparmeters are provided in Table 7 and Table 8, where the notations follow from Table 3. The ablation studies on the regularization coefficient $\\lambda$ (without the initialization scheme (7)) for the small S4-Legs and S4D-Legs are given in Table 10 and Table 11. From Table 9 , by comparing the test performance for w/o $(7,8)$ vs w $(8)$ and $w(7)$ vs w $(7,8)$, we can see that the regularization scheme (8) helps to improve the test performance for all the tasks except the Retrieval task for S4-Legs. This is also verified in the ablation studies of the regularization coefficient $\\lambda$, as shown in Table 10 and Table 11. Combining the initialization scheme (7) and the regularization method (8), more than half of the tasks can achieve the best test accuracy. For both S4-Legs and S4D-Legs, integrating the two methods (7) and (8) induces the best average test accuracy across all the 6 tasks in the LRA benchmark. Therefore, our methods also work for small size of SSMs with a little extra computation cost. ## A. 4 Comparisons with different regularization schemes\n\nIn this section, we add two additional regularization schemes for comparison. 1. Filter norm regularization. We regularize the $\\ell_{2}$ norm of the filter $\\rho_{\\theta}$, i.e., when calculating the reg-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-18.jpg?height=566&width=1577&top_left_y=273&top_left_x=271)\n\nTable 5: Test accuracy for S4-Legs on LRA benchmark by varying the regularization coefficient $\\lambda$.",
    "genanatooptdesign-22": "|  | ListOps |\n| :---: | :---: |\n| $\\lambda=0$ | $60.80_{ \\pm 0.39}$ |\n| $\\lambda=10^{-5}$ | $60.85_{ \\pm 0.62}$ |\n| $\\lambda=10^{-4}$ | $60.80_{ \\pm 0.44}$ |\n| $\\lambda=10^{-3}$ | $\\mathbf{6 1 . 3 2}_{ \\pm 0.43}$ |\n\n\n|  | Text |\n| :---: | :---: |\n| $\\lambda=0$ | $87.87_{ \\pm 0.03}$ |\n| $\\lambda=10^{-5}$ | $87.64_{ \\pm 0.17}$ |\n| $\\lambda=10^{-4}$ | $87.87_{ \\pm 0.36}$ |\n| $\\lambda=10^{-3}$ | $\\mathbf{8 8 . 0 2}_{ \\pm 0.06}$ |\n\n\n|  | Retrieval |\n| :---: | :---: |\n| $\\lambda=0$ | $90.68_{ \\pm 0.14}$ |\n| $\\lambda=10^{-5}$ | $91.04_{ \\pm 0.13}$ |\n| $\\lambda=10^{-4}$ | $90.95_{ \\pm 0.20}$ |\n| $\\lambda=\\times 10^{-3}$ | $\\mathbf{9 1 . 1 0}_{ \\pm 0.11}$ |\n\n\n|  | Image |\n| :---: | :---: |\n| $\\lambda=0$ | $86.69_{ \\pm 0.29}$ |\n| $\\lambda=10^{-5}$ | $86.91_{ \\pm 0.12}$ |\n| $\\lambda=10^{-4}$ | $86.6_{ \\pm 0.22}$ |\n| $\\lambda=10^{-3}$ | $\\mathbf{8 7 . 9 8}_{ \\pm 0.09}$ |\n\n\n|  | Pathfinder |\n| :---: | :---: |\n| $\\lambda=0$ | $94.87_{ \\pm 0.06}$ |\n| $\\lambda=10^{-5}$ | $\\mathbf{9 5 . 0 4}_{ \\pm 0.07}$ |\n| $\\lambda=10^{-4}$ | $94.38_{ \\pm 0.15}$ |\n| $\\lambda=10^{-3}$ | $64.56_{ \\pm 19.94}$ |\n\n\n|  | PathX |\n| :---: | :---: |\n| $\\lambda=0$ | $97.34_{ \\pm 0.07}$ |\n| $\\lambda=10^{-6}$ | $97.32_{ \\pm 0.14}$ |\n| $\\lambda=10^{-5}$ | $\\mathbf{9 7 . 4 6}_{ \\pm 0.15}$ |\n| $\\lambda=10^{-4}$ | $\\boldsymbol{X}$ |\n\nTable 6: Test accuracy for S4D-Legs on LRA benchmark by varying the regularization coefficient $\\lambda$. |  | $D$ | $H$ | $N$ | Dropout | Learning rate | Batch size | Epochs | Weight decay |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 4 | 128 | 64 | 0 | 0.01 | 50 | 40 | 0.05 |\n| Text | 4 | 128 | 64 | 0 | 0.01 | 50 | 50 | 0.0 |\n| Retrieval | 4 | 96 | 4 | 0 | 0.01 | 64 | 20 | 0.05 |\n| Image | 4 | 128 | 64 | 0.1 | 0.01 | 50 | 100 | 0.05 |\n| Pathfinder | 6 | 128 | 64 | 0.0 | 0.004 | 64 | 40 | 0.01 |\n| PathX | 4 | 96 | 64 | 0.0 | 0.0005 | 64 | 50 | 0.05 |\n\nTable 7: List of the small S4-Legs model hyperparameters for the LRA benchmark. ularization measure $\\tau(\\theta)$, we simply take $\\mu(s)=0$ and $K(s, s)=1$ to ignore the effects of the temporal structure of the data. 2. Weight decay on the hidden matrix $A$. In the original $S 4(\\mathrm{D})$ papers Gu et al. (2022a;b; 2023), the default training methods do not apply weight decay to the hidden matrix $A$, and there is no known\n\n|  | $D$ | $H$ | $N$ | Dropout | Learning rate | Batch size | Epochs | Weight decay |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 4 | 128 | 64 | 0 | 0.01 | 50 | 40 | 0.05 |\n| Text | 4 | 128 | 64 | 0 | 0.01 | 50 | 50 | 0.0 |\n| Retrieval | 4 | 96 | 4 | 0 | 0.01 | 64 | 20 | 0.05 |\n| Image | 4 | 128 | 64 | 0.1 | 0.01 | 50 | 100 | 0.05 |\n| Pathfinder | 6 | 128 | 64 | 0.0 | 0.004 | 64 | 40 | 0.01 |\n| PathX | 4 | 96 | 64 | 0.0 | 0.0005 | 64 | 50 | 0.05 |\n\nTable 8: List of the small S4D-Legs model hyperparameters for the LRA benchmark. | ![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-19.jpg?height=38&width=471&top_left_y=749&top_left_x=256) |  | ListOps | Text | Retrieval | Image | Pathfinder | PathX | Average |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| S4-Legs | w/o $(7,8)$ | $55.38_{ \\pm 0.76}$ | $84.72_{ \\pm 0.40}$ | $85.75_{ \\pm 0.46}$ | $82.07_{ \\pm 0.11}$ | $89.36_{ \\pm 0.38}$ | $88.75_{ \\pm 0.62}$ | 81.01 |\n|  | w (7) | $53.72_{ \\pm 1.59}$ | $85.21_{ \\pm 0.21}$ | $84.47_{ \\pm 1.50}$ | $83.71_{ \\pm 0.21}$ | $89.16_{ \\pm 1.38}$ | $88.96_{ \\pm 1.62}$ | 80.87 |\n|  | w (8) | $\\mathbf{5 5 . 4 3}_{ \\pm 1.55}$ | $85.12_{ \\pm 0.34}$ | $83.30_{ \\pm 1,75}$ | $83.86_{ \\pm 0.25}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-19.jpg?height=43&width=147&top_left_y=870&top_left_x=1402) | $\\mathbf{9 0 . 7 0}_{ \\pm 0.61}$ | 81.30 |\n|  | w ( 7,8$)$ | $54.97_{ \\pm 0.30}$ | $\\mathbf{8 5 . 2 7}_{ \\pm 0.21}$ | $\\mathbf{8 5 . 8 2}_{ \\pm 0.42}$ | $\\mathbf{8 4 . 7 4}_{ \\pm 0.18}$ | $88.64_{ \\pm 0.36}$ | $90.19_{ \\pm 0.90}$ | 81.61 |\n|  | Time / epoch, w/o $(7,8)$ | $2 \\min 06 \\mathrm{~s}$ | 50s | 5 min 57s | 33 s | $2 \\min 13 \\mathrm{~s}$ | 10 min 33 s | 3 min 42s |\n|  | Time / epoch, w (8) | 2 min 18s | 52 s | 6 min 28s | 37 s | $2 \\min 31 \\mathrm{~s}$ | 11min 46s | $4 \\min 6 \\mathrm{~s}$ |\n| S4D-Legs | w/o $(7,8)$ | $55.17_{ \\pm 0.20}$ | $83.60_{ \\pm 0.09}$ | $89.12_{ \\pm 0.14}$ | $81.07_{ \\pm 0.39}$ | $87.28_{ \\pm 0.47}$ | $89.91_{ \\pm 0.53}$ | 81.03 |\n|  | w (7) | $55.80_{ \\pm 0.11}$ | $85.30_{ \\pm 0.10}$ | $89.32_{ \\pm 0.17}$ | $82.35_{ \\pm 0.56}$ | $88.00_{ \\pm 0.82}$ | $90.15_{ \\pm 0.86}$ | 81.82 |\n|  | w (8) | ![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-19.jpg?height=43&width=148&top_left_y=1105&top_left_x=744) | $84.86_{ \\pm 0.38}$ | $89.21_{ \\pm 0.09}$ | $82.39_{ \\pm 0.18}$ | $87.86_{ \\pm 0.31}$ | $\\mathbf{9 0 .",
    "genanatooptdesign-23": "9 5}_{ \\pm 0.21}$ | 81.95 |\n|  | w (7, 8) | $55.82_{ \\pm 0.66}$ | $\\mathbf{8 5 .",
    "genanatooptdesign-24": "5 0}_{ \\pm 0.06}$ | $\\mathbf{8 9 . 3 4}_{ \\pm 0.04}$ | $83.79_{ \\pm 0.29}$ | $\\mathbf{8 8 . 5 3}_{ \\pm 0.69}$ | $90.51_{ \\pm 1.01}$ | 82.25 |\n|  | Time / epoch, w/o $(7,8)$ | 1 min 53s | 47 s | 5 min 40s | 29 s | 2 min | 9 min 52s | 3 min 27 s |\n|  | Time / epoch, w (8) | 2 min 11s | 48 s | 6 min 15 s | 34 s | $2 \\min 16 \\mathrm{~s}$ | 11 min 05s | $3 \\min 52 \\mathrm{~s}$ |\n\nTable 9: Test accuracy and running time (per epoch on A100 GPU) on the LRA benchmark under different settings for small S4-Legs and S4D-Legs.",
    "genanatooptdesign-25": "Mean and standard error are reported based on 3 independent runs. |  | ListOps |\n| :---: | :---: |\n| $\\lambda=0$ | $55.38_{ \\pm 0.76}$ |\n| $\\lambda=10^{-5}$ | $55.32_{ \\pm 1.03}$ |\n| $\\lambda=10^{-4}$ | $\\mathbf{5 5 . 4 3}_{ \\pm 1.55}$ |\n| $\\lambda=10^{-3}$ | $55.33_{ \\pm 0.44}$ |\n\n\n|  | Text |\n| :---: | :---: |\n| $\\lambda=0$ | $84.72_{ \\pm 0.40}$ |\n| $\\lambda=10^{-5}$ | $84.74_{ \\pm 0.21}$ |\n| $\\lambda=10^{-4}$ | $84.62_{ \\pm 0.18}$ |\n| $\\lambda=10^{-3}$ | $\\mathbf{8 5 . 1 2}_{ \\pm 0.34}$ |\n\n\n|  | Retrieval |\n| :---: | :---: |\n| $\\lambda=0$ | $\\mathbf{8 5 . 7 5}_{ \\pm 0.46}$ |\n| $\\lambda=10^{-5}$ | $83.30_{ \\pm 1.75}$ |\n| $\\lambda=10^{-4}$ | $82.71_{ \\pm 1.18}$ |\n| $\\lambda=10^{-3}$ | $82.09_{ \\pm 0.41}$ |\n\n\n|  | Image |\n| :---: | :---: |\n| $\\lambda=0$ | $82.07_{ \\pm 0.11}$ |\n| $\\lambda=10^{-5}$ | $82.80_{ \\pm 0.32}$ |\n| $\\lambda=10^{-4}$ | $82.98_{ \\pm 0.15}$ |\n| $\\lambda=10^{-3}$ | $\\mathbf{8 3 . 8 6}_{ \\pm 0.25}$ |\n\n\n|  | Pathfinder |\n| :---: | :---: |\n| $\\lambda=0$ | $89.36_{ \\pm 0.38}$ |\n| $\\lambda=10^{-5}$ | $\\mathbf{8 9 . 3 9}_{ \\pm 0.34}$ |\n| $\\lambda=10^{-4}$ | $89.20_{ \\pm 0.19}$ |\n| $\\lambda=10^{-3}$ | $50.54_{ \\pm 0.01}$ |\n\n\n|  | PathX |\n| :---: | :---: |\n| $\\lambda=0$ | $88.75_{ \\pm 0.62}$ |\n| $\\lambda=10^{-6}$ | $88.51_{ \\pm 0.70}$ |\n| $\\lambda=10^{-5}$ | $89.71_{ \\pm 0.40}$ |\n| $\\lambda=10^{-4}$ | $\\mathbf{9 0 . 7 0}_{ \\pm 0.61}$ |\n\nTable 10: Test accuracy for small S4-Legs on LRA benchmark by varying the regularization coefficient $\\lambda$. ablation study on the effect of weight decay on $A$. Here we add weight decay to compare with the proposed regularization schemes. For synthetic task, we follow the experiment settings in the main paper. The filter norm regularization\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-20.jpg?height=557&width=1580&top_left_y=273&top_left_x=274)\n\nTable 11: Test accuracy for small S4D-Legs on LRA benchmark by varying the regularization coefficient $\\lambda$. |  | Test loss (MSE) |  |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  | w/o (7, 8) | w (8) | Weight decay on $A$ | Filter norm regularization |\n| $b=1$ | $0.25_{ \\pm 0.01}$ | $\\mathbf{0 . 2 2}_{ \\pm 0.008}$ | $0.24_{ \\pm 0.004}$ | $\\mathbf{0 . 2 2}_{ \\pm 0.007}$ |\n| $b=0.1$ | $1.01_{ \\pm 0.14}$ | $\\mathbf{0 . 8 7}_{ \\pm 0.07}$ | $0.97_{ \\pm 0.07}$ | $0.96_{ \\pm 0.12}$ |\n| $b=0.01$ | $4.70_{ \\pm 0.77}$ | $\\mathbf{3 . 5 9}_{ \\pm 0.09}$ | $4.23_{ \\pm 0.23}$ | $4.61_{ \\pm 0.73}$ |\n\nTable 12: Test loss for different regularization methods on synthetic data after convergence. results are obtained by following the same training settings in the paper. The weight decay results are chosen from the best weight decay coefficient from $10^{-3}, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}$. We report the test loss in Table 12. For the LRA benchmark, we also follow the same training setup in the paper to compare the performance of different regularization schemes on the S4-Legs model. The test accuracy for each task is shown in Table 13. From the synthetic results, we see that our regularization scheme can achieve the best performance compared to the other regularization schemes across different temporal structures. For the LRA benchmark, the proposed regularization scheme also achieves the best performance on the average accuracy across different tasks. In particular, for the ListOps task, weight decay performs much worse than the other regularization methods. ## B Proof for the linear regression result in Section 3.2. In this section, we give the proof for the generalization bound (2). The proof is based on the following uniform-convergence generalization bound in Mohri et al.",
    "genanatooptdesign-26": "(2012). Lemma 1. Consider a family of functions $\\mathcal{F}$ mapping from $\\mathcal{Z}$ to $[a, b]$. Let $\\mathcal{D}$ denote the distribution according to which samples are drawn. Then for any $\\delta>0$, with probability at least $1-\\delta$ over the draw of an i.i.d. sample $S=\\left\\{z_{1}, \\ldots, z_{n}\\right\\}$, the following holds for all $f \\in \\mathcal{F}$ :\n\n$$\n\\mathbb{E}_{z \\sim \\mathcal{D}}[f(z)]-\\frac{1}{n} \\sum_{i=1}^{n} f\\left(z_{i}\\right) \\leq 2 \\mathcal{R}_{S}(\\mathcal{F})+3(b-a) \\sqrt{\\frac{\\log (2 / \\delta)}{2 n}}\n$$\n\n| S4-Legs | ListOps | Text | Retrieval | Image | Pathfinder | PathX | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| w/o $(7,8)$ | $61.16_{ \\pm 0.32}$ | $88.69_{ \\pm 0.07}$ | $91.21_{ \\pm 0.17}$ | $87.41_{ \\pm 0.14}$ | $9^{95.89}{ }_{ \\pm 0.10}$ | $96.97_{ \\pm 0.31}$ | 86.89 |\n| w (8) | $6^{61.63}{ }_{ \\pm 0.10}$ | $88.80_{ \\pm 0.27}$ | $91.17_{ \\pm 0.17}$ | $\\mathbf{8 8 .",
    "genanatooptdesign-27": "2 7}_{ \\pm 0.14}$ | $\\mathbf{9 6 . 0 2}_{ \\pm 0.16}$ | $97.18_{ \\pm 0.20}$ | 87.18 |\n| Weight decay for $A$ | $49.90_{ \\pm 0.67}$ | $86.58_{ \\pm 0.91}$ | $91.21_{ \\pm 0.17}$ | $87.65_{ \\pm 0.16}$ | $96.00_{ \\pm 0.09}$ | $\\mathbf{9 7 . 2 2}_{ \\pm 0.05}$ | 84.76 |\n| Filter norm regularization | $61.53_{ \\pm 0.39}$ | $\\mathbf{8 8 . 8 8}_{ \\pm 0.13}$ | $\\mathbf{9 1 . 4 4}_{ \\pm 0.08}$ | $87.70_{ \\pm 0.20}$ | $95.83_{ \\pm 0.14}$ | $97.16_{ \\pm 0.16}$ | 87.09 |\n\nTable 13: Test accuracy for different regularization methods on the LRA benchmark for S4-Legs. where $\\mathcal{R}_{S}(\\mathcal{F})$ is the empirical Rademacher complexity with respect to the sample $S$, defined as: $\\mathcal{R}_{S}(\\mathcal{F})=$ $\\mathbb{E}_{\\sigma}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f\\left(z_{i}\\right)\\right] .\\left\\{\\sigma_{i}\\right\\}_{i=1}^{n}$ are i.i.d. random variables drawn from $U\\{-1,1\\}$ with $P\\left(\\sigma_{i}=\\right.$ 1) $=P\\left(\\sigma_{i}=-1\\right)=0.5$. And the Talagrand's contraction lemma Ledoux and Talagrand (2013). Lemma 2. Let $H$ be a hypothesis set of functions mapping $\\mathcal{X}$ to $\\mathbb{R}$ and $\\Psi_{1}, \\ldots, \\Psi_{m}, \\mu$-Lipschitz functions for some $\\mu>0$. Then, for any sample $S$ of $m$ points $x_{1}, \\ldots, x_{m} \\in \\mathcal{X}$, the following inequality holds\n\n$$\n\\frac{1}{m} \\mathbb{E}_{\\sigma}\\left[\\sup _{h \\in H} \\sum_{i=1}^{m} \\sigma_{i}\\left(\\Psi_{i} \\circ h\\right)\\left(x_{i}\\right)\\right] \\leq \\frac{\\mu}{m} \\mathbb{E}_{\\sigma}\\left[\\sup _{h \\in H} \\sum_{i=1}^{m} \\sigma_{i} h\\left(x_{i}\\right)\\right]\n$$\n\nNow we begin our proof:\nProof. First, notice for any $i \\in[1: n]$ and $\\theta \\in \\Theta$, we have\n\n$$\n\\left(\\theta^{\\top} x_{i}-y_{i}\\right)^{2} \\leq 2\\left(\\theta^{\\top} x_{i}\\right)^{2}+2 y_{i}^{2} \\leq 2 r^{2} R^{2}+2\n$$\n\nSecond, note that $\\left(\\theta^{\\top} x_{i}-y_{i}\\right)^{2}$ is $2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|\\theta^{\\top} x_{i}-y_{i}\\right|$-Lipschitz (the maximum gradient norm) with respect to $\\theta^{\\top} x_{i}-y_{i}$, and we can bound the Lipschitz constant as\n\n$$\n2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|\\theta^{\\top} x_{i}-y_{i}\\right| \\leq 2 r R+2\n$$\n\nThen by Lemma 2, the Rademacher complexity for the linear model is bounded as\n\n$$\n\\begin{aligned}\n\\mathcal{R}_{S}(\\mathcal{F}) & =\\frac{1}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\|\\theta\\|_{2} \\leq R} \\sum_{i=1}^{n} \\sigma_{i}\\left(\\theta^{\\top} x_{i}-y_{i}\\right)^{2}\\right] \\\\\n& \\leq \\frac{2 r R+2}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\|\\theta\\|_{2} \\leq R} \\sum_{i=1}^{n} \\sigma_{i}\\left(\\theta^{\\top} x_{i}-y_{i}\\right)\\right] \\\\\n& =\\frac{2 r R+2}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\|\\theta\\|_{2} \\leq R} \\sum_{i=1}^{n} \\sigma_{i} \\theta^{\\top} x_{i}\\right] \\\\\n& \\leq \\frac{2 R(r R+1)}{n} \\mathbb{E}_{\\sigma}\\left\\|\\sum_{i=1}^{n} \\sigma_{i} x_{i}\\right\\| \\\\\n& \\leq \\frac{2 R(r R+1)}{n} \\sqrt{\\mathbb{E}_{\\sigma}\\left\\|\\sum_{i=1}^{n} \\sigma_{i} x_{i}\\right\\|^{2}} \\\\\n& =\\frac{2 R(r R+1)}{n} \\sqrt{\\sum_{i=1}^{n}\\left\\|x_{i}\\right\\|^{2}} \\\\\n& \\leq \\frac{2 r R(r R+1)}{\\sqrt{n}}\n\\end{aligned}\n$$\n\nCombining with the function value bound, we get the desired bound (2) by Lemma 1. ## C Detailed discussions of Assumption 1\n\nIn this section, we add more discussions on the Assumption 1 and provide some concrete examples for the stochastic processes that satisfy the assumption. We first write down the complete description for the Kolmogorov continuity theorem. Lemma 3 (Kolmogorov). Let $\\left\\{X_{t}\\right\\}_{t \\geq 0}$ be a real-valued stochastic process such that there exists positive constants $\\alpha, \\beta, C$ satisfying\n\n$$\n\\mathbb{E}\\left[\\left|X_{t}-X_{s}\\right|^{\\alpha}\\right] \\leq C|t-s|^{1+\\beta}\n$$\n\nfor all $s, t \\geq 0$. Then $X$ has a continuous modification which, with probability one, is locally $\\gamma$-H\u00f6lder continuous for every $0<\\gamma<\\beta / \\alpha$. In the case of Brownian motion on $\\mathbb{R}$, the choice of constants $\\alpha=4, \\beta=1, C=2$ will work in the Kolmogorov continuity theorem. When it comes to the Gaussian process, we have the following theorem (Azmoodeh et al., 2014, Theorem 1.) that gives a necessary and sufficient condition for H\u00f6lder continuity. Lemma 4. A centered (mean zero) Gaussian process $X$ is H\u00f6lder continuous of any order $a<H$, i.e.,\n\n$$\n\\left|X_{t}-X_{s}\\right| \\leq C_{\\varepsilon}|t-s|^{H-\\varepsilon}, \\quad \\forall \\varepsilon \\in(0, H)\n$$\n\nif and only if there exists constants $c_{\\varepsilon}$ such that\n\n$$\n\\mathbb{E}\\left[\\left(X_{t}-X_{s}\\right)^{2}\\right] \\leq c_{\\varepsilon}(t-s)^{2 H-2 \\varepsilon}, \\quad \\forall \\varepsilon \\in(0, H)\n$$\n\nFor a stationary Gaussian process with covariance $K(s-t)$, the H\u00f6lder continuity (in expectation) assumption is equivalent to $1-K(s-t) / K(0) \\leq c_{\\alpha}(t-s)^{2 \\alpha} / 2$ for any $\\alpha \\in(0, H)$. Now combining these results, we see that for any stationary Gaussian process with continuous mean $\\mu(t)$, covariance $K(s-t)$, and $1-K(s-t) / K(0) \\leq c_{\\alpha}(t-s)^{2 \\alpha} / 2, \\forall \\alpha \\in(0, H)$, it satisfies H\u00f6lder continuity in Assumption 1. As for the sub-Gaussian property, since the normalized Gaussian process $\\tilde{X}_{t}$ is standard normal at each time $t$, then any Gaussian process that satisfies H\u00f6lder continuity automatically satisfies the sub-Gaussian property in Assumption 1. Concrete examples include:\n\n- identical sequences: $x(t)=x$ for all $t \\in[0, T]$, where $x \\sim \\mathcal{N}(0,1)$\n- Gaussian white noise: $\\mu(t)=0, K(s, t)=\\frac{1}{|b| \\sqrt{\\pi}} e^{-((s-t) / b)^{2}}$ for some $b \\neq 0$\n- Ornstein-Uhlenbeck process: $\\mu(t)=0, K(s, t)=e^{-|s-t|}$\n\nRelaxations of Assumption 1. In fact, Assumption 1 is used to show upper bounds for two key terms (13) in the proof of Theorem 1. In particular, the sub-Gaussian property in Assumption 1 guarantees that the input random process is bounded in a finite time set with high probability. The H\u00f6lder condition then ensures the boundedness in a infinite time set $t \\in[0, T]$. Thus, if the input random process is from a finite subset of $\\mathbb{R}$, then the H\u00f6lder condition can be removed. For example, in computer vision tasks when the input image is flattened as a sequence, the range for each pixel value is a finite set (for a MNIST image, each pixel value is a positive integer between 0 to 255). In that case, the Holder continuity condition in Assumption 1 can be dropped. ## D Derivations for (4) and (5) in Section 4.1\n\nFor the left zero padding transformation, the key term in (3) becomes\n\n$$\n\\begin{aligned}\n& \\int_{0}^{2 T}\\left|\\rho_{\\theta}(2 T-t)\\right| \\sqrt{K_{1}(t, t)} d t+\\left|\\int_{0}^{2 T} \\rho_{\\theta}(2 T-t) \\mu_{1}(t) d t\\right|+1 \\\\\n= & \\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|+1\n\\end{aligned}\n$$\n\nFor the right zero padding transformation, the key term in (3) becomes\n\n$$\n\\begin{aligned}\n& \\int_{0}^{2 T}\\left|\\rho_{\\theta}(2 T-t)\\right| \\sqrt{K_{2}(t, t)} d t+\\left|\\int_{0}^{2 T} \\rho_{\\theta}(2 T-t) \\mu_{2}(t) d t\\right|+1 \\\\\n= & \\int_{0}^{T}\\left|\\rho_{\\theta}(2 T-t)\\right| \\sqrt{K(t, t)} d t+\\left|\\int_{0}^{T} \\rho_{\\theta}(2 T-t) \\mu(t) d t\\right|+1 \\\\\n= & \\int_{0}^{T}\\left|C e^{A T} e^{A(T-t)} B\\right| \\sqrt{K(t, t)} d t+\\left|\\int_{0}^{T} C e^{A T} e^{A(T-t)} B \\mu(t) d t\\right|+1\n\\end{aligned}\n$$\n\nThen we get (4) and (5).",
    "genanatooptdesign-28": "## E Proof for Theorem 1\n\nIn this section, we will prove Theorem 1. Before moving into the formal proof, we first introduce some useful lemmas that help to build the proof. The first lemma is the Massart Lemma for the Rademacher complexity with finite class. Lemma 5 (Massart). Let $\\mathcal{A}$ be some finite subset of $R^{m}$ and $\\sigma_{1}, \\ldots, \\sigma_{m}$ be independent Rademacher random variables. Let $r=\\sup _{a \\in \\mathcal{A}}\\|a\\|$. Then, we have,\n\n$$\n\\mathbb{E}_{\\sigma}\\left[\\sup _{a \\in \\mathcal{A}} \\sum_{i=1}^{m} \\sigma_{i} a_{i}\\right] \\leq r \\sqrt{2 \\log |\\mathcal{A}|}\n$$\n\nThe second lemma is to bound the supremum of a stochastic process that is H\u00f6lder continuous and sub-Gaussian. Lemma 6 (H\u00f6lder maximal inequality). Suppose $\\left\\{X_{t}\\right\\}_{t \\in[0, T]}$ is a centered H\u00f6lder process, i.e., $\\exists L, H>$ 0 , s.t. $\\left|X_{s}-X_{t}\\right| \\leq L|s-t|^{H}, \\forall s, t \\in[0, T]$. If further $X_{t}$ is $\\sigma^{2}$-sub-Gaussian for every $t \\in[0, T]$, i.e., $\\forall u>0, P\\left(\\left|X_{t}\\right| \\geq u\\right) \\leq 2 \\exp \\left(-u^{2} / 2 \\sigma^{2}\\right)$ for some $\\sigma>0$. Then with probability at least $1-\\delta$,\n\n$$\n\\sup _{t \\in[0, T]}\\left|X_{t}\\right| \\leq L+\\sigma \\sqrt{2 \\log (2 T / \\delta)}\n$$\n\nProof. The proof is based on the $\\varepsilon$-net and covering number argument. We first discretize the time interval $[0, T]$ into $N$ parts $[0, T / N] \\cup[T / N, 2 T / N] \\cdots \\cup[(N-1) T / N, T]$. Then for any time $t \\in[0, T]$, there exists a $\\pi(t) \\in\\{0, T / N, \\ldots,(N-1) T / N\\}$ such that $|t-\\pi(t)| \\leq T / N$. Therefore, by H\u00f6lder continuity, we have\n\n$$\n\\sup _{t \\in[0, T]}\\left|X_{t}\\right| \\leq \\sup _{t \\in[0, T]}\\left|X_{t}-X_{\\pi(t)}\\right|+\\sup _{t \\in[0, T]}\\left|X_{\\pi(t)}\\right| \\leq L\\left(\\frac{T}{N}\\right)^{H}+\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right|\n$$\n\nSince $X_{t}$ is sub-Guassian for every time $t \\in[0, T]$, then for each $i \\in[0: N-1]$, by letting $u=$ $\\sigma \\sqrt{2 \\log (2 N / \\delta)}$, we have with probability at least $1-\\delta / N$,\n\n$$\nX_{i T / N} \\leq \\sigma \\sqrt{2 \\log (2 N / \\delta)}\n$$\n\nTaking the union bound over all $i \\in[0: N-1]$, we have with probability at least $1-\\delta$,\n\n$$\n\\max _{i \\in[0: N-1]} X_{i T / N} \\leq \\sigma \\sqrt{2 \\log (N / \\delta)}\n$$\n\nHence,\n\n$$\n\\sup _{t \\in[0, T]} X_{t} \\leq L\\left(\\frac{T}{N}\\right)^{H}+\\sigma \\sqrt{2 \\log (2 N / \\delta)}\n$$\n\nholds for all $N$. Here we simply take $N=[T]+1$, then we get\n\n$$\n\\sup _{t \\in[0, T]} X_{t} \\leq L+\\sigma \\sqrt{2 \\log (2 T / \\delta)}\n$$\n\nNow we are ready to prove the main result Theorem 1. Proof. We let $g_{\\theta}(x):=\\int_{0}^{T} \\rho_{\\theta}(T-t) x(t) d t-y$, then the generalization gap is given by\n\n$$\nR_{x}(\\theta)-R_{n}(\\theta)=\\mathbb{E}_{x}\\left[g_{\\theta}^{2}(x)\\right]-\\frac{g_{\\theta}^{2}\\left(x_{1}\\right)+\\ldots+g_{\\theta}^{2}\\left(x_{n}\\right)}{n}\n$$\n\nNow let hypothesis space $\\mathcal{F}=\\left\\{x \\mapsto g_{\\theta}^{2}(x): \\theta \\in \\Theta\\right\\}$, then its empirical Rademacher complexity is given by\n\n$$\n\\begin{aligned}\n\\mathcal{R}_{S}(\\mathcal{F}) & =\\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} g_{\\theta}^{2}\\left(x_{i}\\right)\\right] \\\\\n& =\\frac{1}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\sum_{i=1}^{n} \\sigma_{i}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) x_{i}(t) d t-y_{i}\\right|^{2}\\right]\n\\end{aligned}\n$$\n\nBy the Talagrand's contraction Lemma 2, since $g_{\\theta}^{2}\\left(x_{i}\\right)$ is $2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right|$ Lipschitz, we have\n\n$$\n\\begin{aligned}\n\\mathcal{R}_{S}(\\mathcal{F}) & \\leq 2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right| \\cdot \\frac{1}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\sum_{i=1}^{n} \\sigma_{i}\\left(\\int_{0}^{T} \\rho_{\\theta}(T-t) x_{i}(t) d t-y_{i}\\right)\\right] \\\\\n& =\\frac{2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right|}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i} x_{i}(t) d t\\right]\n\\end{aligned}\n$$\n\nNow we separate the expectation into two parts: the unbiased part invovled with $x_{i}(t)-\\mu(t)$ and the biased part $\\mu(t)$, by noticing that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i} x_{i}(t) d t\\right] \\\\\n= & \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i}\\left(x_{i}(t)-\\mu(t)\\right) d t+\\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i} \\mu(t) d t\\right] \\\\\n\\leq & \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i}\\left(x_{i}(t)-\\mu(t)\\right) d t\\right]+\\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i} \\mu(t) d t\\right]\n\\end{aligned}\n$$\n\nFor the unbiased part, by the H\u00f6lder's inequality, for any $p, q \\in[1, \\infty]$ such that $\\frac{1}{p}+\\frac{1}{q}=1$,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i}\\left(x_{i}(t)-\\mu(t)\\right) d t\\right] \\\\\n\\leq & \\sup _{\\theta \\in \\Theta}\\left(\\int_{0}^{T}\\left|\\rho_{\\theta}^{p}(T-t)\\right| K^{p / 2}(t, t) d t\\right)^{1 / p} \\mathbb{E}_{\\sigma}\\left[\\left(\\int_{0}^{T}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|^{q} d t\\right)^{1 / q}\\right]\n\\end{aligned}\n$$\n\nFor the biased part,\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i} \\mu(t) d t\\right] & \\leq \\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right| \\mathbb{E}_{\\sigma}\\left[\\left|\\sum_{i=1}^{n} \\sigma_{i}\\right|\\right] \\\\\n& \\leq \\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right| \\sqrt{\\mathbb{E}_{\\sigma}\\left[\\left|\\sum_{i=1}^{n} \\sigma_{i}\\right|^{2}\\right]} \\\\\n& =\\sqrt{n} \\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|\n\\end{aligned}\n$$\n\nNow for the unbiased part (9), we take $p=1, q=\\infty$. Then we have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i}\\left(x_{i}(t)-\\mu(t)\\right) d t\\right] \\\\\n\\leq & \\sup _{\\theta \\in \\Theta}\\left(\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t\\right) \\mathbb{E}_{\\sigma}\\left[\\sup _{t \\in[0, T]}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|\\right]\n\\end{aligned}\n$$\n\nAlso by the same argument, note that\n\n$$\n\\begin{aligned}\n& \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right| \\\\\n= & \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) x_{i}(t) d t-y_{i}\\right| \\\\\n\\leq & \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t)\\left(x_{i}(t)-\\mu(t)\\right) d t\\right|+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|+1 \\\\\n\\leq & \\sup _{\\theta \\in \\Theta}\\left(\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t\\right) \\sup _{i \\in[1: n], t \\in[0, T]}\\left|\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\Re\\left(\\rho_{\\theta}(T-t)\\right) \\mu(t) d t\\right|+1\n\\end{aligned}\n$$\n\nThus, there are two terms that we need to bound:\n\n$$\n\\sup _{i \\in[1: n], t \\in[0, T]}\\left|\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|, \\quad \\mathbb{E}_{\\sigma}\\left[\\sup _{t \\in[0, T]}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|\\right]\n$$\n\nFor the first term, notice that the normalized Gaussian process $\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}$ is centered. By Assumption 1, it is H\u00f6lder continuous and $\\sigma^{2}$-sub-Gaussian on $t \\in[0, T]$. Therefore, we can directly apply Lemma 6 and get with probability at least $1-\\delta / 3 n$,\n\n$$\n\\sup _{t \\in[0, T]}\\left|\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right| \\leq L+\\sigma \\sqrt{2 \\log (6 T n / \\delta)}, \\quad \\forall i=1, \\ldots, n\n$$\n\nNow by taking a union bound over $i=1, \\ldots, n$, we get with probability at least $1-\\delta / 3$,\n\n$$\n\\sup _{i \\in[1: n], t \\in[0, T]}\\left|\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right| \\leq L+\\sigma \\sqrt{2 \\log (6 T n / \\delta)}\n$$\n\nFor the second term, we apply the $\\varepsilon$-net and covering number argument as in Lemma 6. We discretize the time interval $[0, T]$ into $N$ parts $[0, T / N] \\cup[T / N, 2 T / N] \\cdots \\cup[(N-1) T / N, T]$, then for any $t \\in[0, T]$, there exists a sub-interval such that $t \\in[(k-1) T / N, k T / N]$ for some $k \\in[1: N]$. Therefore, $\\forall t \\in[0, T]$ such that $t \\in[(k-1) T / N, k T / N]$ for some $k \\in[1: N]$, by H\u00f6lder continuity in Assumption 1 for the\nnormalized process, we have\n\n$$\n\\begin{aligned}\n\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right| & \\leq\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}\\left(\\frac{(k-1) T}{N}\\right)-\\mu\\left(\\frac{(k-1) T}{N}\\right)}{\\sqrt{K\\left(\\frac{(k-1) T}{N}, \\frac{(k-1) T}{N}\\right)}}\\right|+\\left|\\sum_{i=1}^{n} \\sigma_{i}\\left(\\frac{x_{i}\\left(\\frac{(k-1) T}{N}\\right)-\\mu\\left(\\frac{(k-1) T}{N}\\right)}{\\sqrt{K\\left(\\frac{(k-1) T}{N}, \\frac{(k-1) T}{N}\\right)}}-\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right)\\right| \\\\\n& \\leq \\max _{k=1, \\ldots, N}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}\\left(\\frac{(k-1) T}{N}\\right)-\\mu\\left(\\frac{(k-1) T}{N}\\right)}{\\sqrt{K\\left(\\frac{(k-1) T}{N}, \\frac{(k-1) T}{N}\\right)}}\\right|+\\|\\sigma\\| \\sqrt{n} L\\left(\\frac{T}{N}\\right)^{H} \\\\\n& =\\max _{k=1, \\ldots, N}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}\\left(\\frac{(k-1) T}{N}\\right)-\\mu\\left(\\frac{(k-1) T}{N}\\right)}{\\sqrt{K\\left(\\frac{(k-1) T}{N}, \\frac{(k-1) T}{N}\\right)}}\\right|+n L\\left(\\frac{T}{N}\\right)^{H}\n\\end{aligned}\n$$\n\nThen by the Massart Lemma 5 and the sup norm bound (14), with probability at least $1-\\delta / 3$,\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{\\sigma}\\left[\\sup _{t \\in[0, T]}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|\\right] & \\leq \\mathbb{E}_{\\sigma}\\left[\\max _{k=1, \\ldots, N}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}\\left(\\frac{(k-1) T}{N}\\right)-\\mu\\left(\\frac{(k-1) T}{N}\\right)}{\\sqrt{K\\left(\\frac{(k-1) T}{N}, \\frac{(k-1) T}{N}\\right)}}\\right|\\right]+n L\\left(\\frac{T}{N}\\right)^{H} \\\\\n& \\leq \\sqrt{2 n \\log N} \\sup _{i \\in[1: n], t \\in[0, T]}\\left|\\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|+n L\\left(\\frac{T}{N}\\right)^{H} \\\\\n& \\leq \\sqrt{2 n \\log N}(L+\\sigma \\sqrt{2 \\log (6 T n / \\delta)})+n L\\left(\\frac{T}{N}\\right)^{H}\n\\end{aligned}\n$$\n\nSince $N$ is an arbitrary integer number, we let $N=\\left[T n^{1 / H}\\right]+1$, then we get\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{\\sigma}\\left[\\sup _{t \\in[0, T]}\\left|\\sum_{i=1}^{n} \\sigma_{i} \\frac{x_{i}(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|\\right] & \\leq \\mathcal{O}(\\sqrt{n \\cdot \\log N \\cdot \\log (T n / \\delta)}) \\\\\n& \\leq \\mathcal{O}(\\sqrt{n} \\log (N T n / \\delta)) \\\\\n& =\\mathcal{O}(\\sqrt{n} \\log (T n / \\delta))\n\\end{aligned}\n$$\n\nCombining (14), (15), (10) and (11), we can further bound (12) as\n\n$$\n\\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right| \\leq \\sup _{\\theta \\in \\Theta}\\left(\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t\\right) \\mathcal{O}(\\sqrt{\\log (T n / \\delta)})+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\Re\\left(\\rho_{\\theta}(T-t)\\right) \\mu(t) d t\\right|+1\n$$\n\nAnd the Rademacher complexity is further bounded as\n\n$$\n\\begin{aligned}\n& \\mathcal{R}_{S}(\\mathcal{F}) \\\\\n& \\leq \\frac{2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right|}{n} \\mathbb{E}_{\\sigma}\\left[\\sup _{\\theta \\in \\Theta} \\int_{0}^{T} \\rho_{\\theta}(T-t) \\sum_{i=1}^{n} \\sigma_{i} x_{i}(t) d t\\right] \\\\\n& \\leq \\frac{2 \\sup _{\\theta \\in \\Theta, i \\in[1: n]}\\left|g_{\\theta}\\left(x_{i}\\right)\\right|}{n}\\left(\\sup _{\\theta \\in \\Theta} \\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|\\right) \\cdot \\mathcal{O}(\\sqrt{n} \\log (T n / \\delta)) \\\\\n& \\leq\\left(\\sup _{\\theta \\in \\Theta} \\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|+1\\right)^{2} \\cdot \\mathcal{O}\\left(\\frac{\\log ^{3 / 2}(T n / \\delta)}{\\sqrt{n}}\\right) . \\end{aligned}\n$$\n\nFinally, by the symmetrization of $R_{x}(\\theta)-R_{n}(\\theta)$, combining it with (16) and (1), we have with probability at least $1-\\delta$,\n\n$$\n\\sup _{\\theta \\in \\Theta}\\left|R_{x}(\\theta)-R_{n}(\\theta)\\right| \\leq\\left(\\sup _{\\theta \\in \\Theta} \\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\sup _{\\theta \\in \\Theta}\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|+1\\right)^{2} \\cdot \\mathcal{O}\\left(\\frac{\\log ^{3 / 2}(T n / \\delta)}{\\sqrt{n}}\\right)\n$$\n\n## F Proof for Proposition 1\n\nProof. First, notice that by the H\u00f6lder's inequality with $p=1, q=\\infty$, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{x}\\left[\\left|\\int_{0}^{T} \\rho_{\\tilde{\\theta}}(T-t) x(t) d t\\right|\\right] \\\\\n= & \\frac{\\mathbb{E}_{x}\\left[\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) x(t) d t\\right|\\right]}{\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|} \\\\\n\\leq & \\frac{\\mathbb{E}_{x}\\left[\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t)(x(t)-\\mu(t)) d t\\right|\\right]+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|}{\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|} \\\\\n\\leq & \\frac{\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t \\cdot \\mathbb{E}_{x}\\left[\\sup _{t \\in[0, T]}\\left|\\frac{x(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|\\right]+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|}{\\int_{0}^{T}\\left|\\rho_{\\theta}(T-t)\\right| \\sqrt{K(t, t)} d t+\\left|\\int_{0}^{T} \\rho_{\\theta}(T-t) \\mu(t) d t\\right|} \\\\\n\\leq & \\mathbb{E}_{x}\\left[\\sup _{t \\in[0, T]}\\left|\\frac{x(t)-\\mu(t)}{\\sqrt{K(t, t)}}\\right|\\right]+1\n\\end{aligned}\n$$\n\nWe let $X_{t}:=\\frac{x(t)-\\mu(t)}{\\sqrt{K(t, t)}}$, then by Assumption 1, $X_{t}$ is H\u00f6lder continuous and $\\sigma^{2}$ sub-Gaussian for any $t \\in[0, T]$. Again, we use an $\\varepsilon$-net argument to bound $\\mathbb{E}\\left[\\sup _{t \\in[0, T]}\\left|X_{t}\\right|\\right]$. By separating the time interval $[0, T]$ into $N$ parts $[0, T / N] \\cup[T / N, 2 T / N] \\cdots \\cup[(N-1) T / N, T]$. Then for any time $t \\in[0, T]$, there\nexists a $\\pi(t) \\in\\{0, T / N, \\ldots,(N-1) T / N\\}$ such that $|t-\\pi(t)| \\leq T / N$. Therefore, by H\u00f6lder continuity,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\sup _{t \\in[0, T]}\\left|X_{t}\\right|\\right] & \\leq \\mathbb{E}\\left[\\sup _{t \\in[0, T]}\\left|X_{t}-X_{\\pi(t)}\\right|\\right]+\\mathbb{E}\\left[\\sup _{t \\in[0, T]}\\left|X_{\\pi(t)}\\right|\\right] \\\\\n& \\leq L\\left(\\frac{T}{N}\\right)^{H}+\\mathbb{E}\\left[\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right|\\right]\n\\end{aligned}\n$$\n\nFor the maximum over a finite class, notice that for any $u_{0}>0$,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right|\\right] & =\\int_{0}^{\\infty} P\\left(\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right| \\geq u\\right) d u \\\\\n& =\\int_{0}^{u_{0}} P\\left(\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right| \\geq u\\right) d u+\\int_{u_{0}}^{\\infty} P\\left(\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right| \\geq u\\right) d u \\\\\n& \\leq u_{0}+\\int_{u_{0}}^{\\infty} \\sum_{i=0}^{N-1} P\\left(\\left|X_{i T / N}\\right| \\geq u\\right) d u\n\\end{aligned}\n$$\n\nSince $X_{i T / N}$ is $\\sigma^{2}$ sub-Gaussian for every $i \\in[0: N-1]$, then $\\forall u_{0}>0$,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right|\\right] & \\leq u_{0}+2 N \\int_{u_{0}}^{\\infty} \\exp \\left(-\\frac{u^{2}}{2 \\sigma^{2}}\\right) d u \\\\\n& \\leq u_{0}+2 N \\int_{u_{0}}^{\\infty} \\frac{u}{u_{0}} \\exp \\left(-\\frac{u^{2}}{2 \\sigma^{2}}\\right) d u \\\\\n& =u_{0}+\\frac{2 N \\sigma^{2}}{u_{0}} \\exp \\left(-\\frac{u_{0}^{2}}{2 \\sigma^{2}}\\right)\n\\end{aligned}\n$$\n\nMinimizing the above term over $u_{0}>0$, we can simply let $u_{0}=\\sigma \\sqrt{2 \\log 2 N}$, then\n\n$$\n\\mathbb{E}\\left[\\max _{i \\in[0: N-1]}\\left|X_{i T / N}\\right|\\right] \\leq \\sigma \\sqrt{2 \\log 2 N}+\\frac{\\sigma}{\\sqrt{2 \\log 2 N}} \\leq 2 \\sigma \\sqrt{2 \\log 2 N}\n$$\n\nNow back to the original upper bound, we get\n\n$$\n\\mathbb{E}\\left[\\sup _{t \\in[0, T]}\\left|X_{t}\\right|\\right] \\leq L\\left(\\frac{T}{N}\\right)^{H}+2 \\sigma \\sqrt{2 \\log 2 N}\n$$\n\nSince $N$ is an arbitrary positive integer, we simply take $N=[T]+1$, finally we get\n\n$$\n\\mathbb{E}\\left[\\sup _{t \\in[0, T]}\\left|X_{t}\\right|\\right] \\leq L+2 \\sigma \\sqrt{2 \\log (2 T+2)}=\\mathcal{O}(\\sqrt{\\log T})\n$$\n\n## G Lipschitz function of sub-Gaussian random variables\n\nIn this section, we provide some known examples for the sub-Gaussian random variables that remain the sub-Gaussian property under a Lipschitz function. 1. For a bounded random variable $X \\in[0,1]$, if $f:[0,1] \\rightarrow \\mathbb{R}$ is a quasi-convex function, i.e., $\\{x:$ $f(x) \\leq s\\}$ is a convex set for all $s \\in \\mathbb{R}$. If $f$ is further Lipschitz in [0,1], then $f(X)$ is sub-Gaussian.",
    "genanatooptdesign-29": "See Theorem 7.12 in Boucheron et al.",
    "genanatooptdesign-30": "(2013). 2. For a sub-Gaussian random variable $X$ that has density of the form $e^{-U(x)}$ with $U$ being twice continuously differentiable and $U^{\\prime \\prime}(x)>0, \\forall x \\in \\mathbb{R}$, then if $f$ is a Lipschitz function, $f(X)$ is also sub-Gaussian.",
    "genanatooptdesign-31": "See Theorem 5.2.15 in Vershynin (2020). [^0]:    ${ }^{1} \\mathrm{~A}$ linear space of continuous functions from $\\mathbb{R}_{\\geq 0}$ to $\\mathbb{R}$ that vanishes at infinity. "
}