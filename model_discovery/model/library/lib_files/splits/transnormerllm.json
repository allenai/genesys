{
    "transnormerllm-0": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer\n\nZhen Qin\u266f, Dong Li\u266f, Weigao Sun\u266f, Weixuan Sun\u266f, Xuyang Shen\u266f, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, Yiran Zhong OpenNLPLab, Shanghai AI Laboratory https://github.com/OpenNLPLab/TransnormerLLM Yiran Zhong is the corresponding author.",
    "transnormerllm-1": "Email: zhongyiran@gmail.com. equal contribution. Abstract\n\nWe present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer (Qin et al., 2022a) by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE (Qin et al., 2023b) together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over . Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. 1 Introduction\n\nThe field of Natural Language Processing (NLP) has been revolutionized by the advent of large-scale language models (LLMs) (Touvron et al., 2023a; Biderman et al., 2023; Brown et al., 2020). These models have demonstrated exceptional performance across a multitude of tasks, elevating abilities to comprehend, generate, and interact with human languages in computational frameworks. Previous language modeling development has predominantly centered around Transformer architectures, with seminal models such as vanilla Transformer (Vaswani et al., 2017), GPT series (Radford et al., 2018; 2019; Brown et al., 2020), BERT (Devlin et al., 2019), and BART (Lewis et al., 2019) standing as standard backbones in related fields. The success of Transformer architectures is premised on the softmax attention mechanism, which discerns dependencies among input tokens in a data-driven scheme and has global position awareness, offering the model an effective way to handle the long-range dynamism of natural language. Nevertheless, conventional Transformers are not without their constraints. Primarily, their quadratic time complexity with respect to the sequence length limits their scalability and hampers efficiency in terms of computational resources and time during the training and inference stages. Numerous efficient sequence modeling methods have been proposed in an attempt to reduce the quadratic time complexity to linear (Katharopoulos et al., 2020; Choromanski et al., 2021; Qin et al., 2022b; Zheng et al., 2023; 2022). However, there are two reasons that prohibit them to be applied to LLMs: 1) their performance in language modeling is often unsatisfactory; 2) they do not demonstrate speed advantages in real-world scenarios. In this paper, we introduce TransNormerLLM, the first linear attention-based LLM that surpasses conventional softmax attention in both accuracy and efficiency. The development of TransNormerLLM builds upon the foundations of the previous linear attention architecture, TransNormer (Qin et al., 2022a), while incorporating a series of advanced modifications to achieve superior performance. The key enhancements in TransNormerLLM include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, and inference acceleration. One notable improvement is the replacement of the TransNormer\u2019s DiagAttention with Linear Attention to enhance global interactions. To address the issue of dilution, we introduced LRPE (Qin et al., 2023b) with exponential decay (Press et al., 2022; Qin et al., 2023a; Peng et al., 2023a). Lightning Attention, a novel technique that significantly accelerates linear attention during training is introduced, resulting in a more than two-fold improvement, while also reducing memory usage by four times with IO awareness. Furthermore, we simplified GLU and Normalization, with the latter leading to a 20% speedup. A robust inference algorithm ensures the stability of numerical values and constant inference speed, regardless of the sequence length, thereby enhancing the efficiency of our model during both training and inference stages. We validate the efficacy of TransNormerLLM on our self-collected pre-train corpus, which is more than TB in size and contains over trillion tokens. We expand the original TransNormer model, ranging from 385M to 175B parameters, and benchmark models with sizes of 385M, 1B, and 7B. The benchmark results demonstrate that our models achieve competitive performance with existing state-of-the-art transformer-based LLMs with similar sizes while also having faster inference speeds. We will open-source our pre-trained models, enabling researchers and practitioners to build upon our work and explore efficient transformer structures in LLMs. 2 Related Work\n\n2.1 Transformer-based LLMs\n\nIn recent years, the field of Large Language Models (LLMs) has experienced significant advancements. Adhering to the scaling laws (Kaplan et al., 2020), various LLMs with over 100 billion parameters have been introduced, such as GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2022), PaLM (Chowdhery et al., 2022), GLM (Du et al., 2022) and etc.. More specialized models like Galactica (Taylor et al., 2022) have also emerged for specific domains like science. A notable development is Chinchilla (Hoffmann et al., 2022), an LLM model with 70 billion parameters that redefines these scaling laws, focusing on the number of tokens rather than model weights. Furthermore, LLaMA (Touvron et al., 2023a) has also sparked interest due to its promising performance and open-source availability. The discourse around LLMs also encompasses the dynamics between open-source and closed-source models. Open-source models such as BLOOM (Workshop et al., 2023), OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a), Pythia (Biderman et al., 2023) and Falcon (Penedo et al., 2023) are rising to compete against their closed-source counterparts, including GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022). To speed up training, Sparse Attention (Child et al., 2019; Beltagy et al., 2020) was introduced, but among large models, only GPT-3 adopted it (Brown et al., 2020; Scao et al., 2022). 2.2 Non-Transformer-based LLMs Candidates\n\nDespite the proliferation of Transformer-based large models in the research community, a portion of recent work has prioritized addressing its square time complexity. This focus has led to the exploration and development of a series of model architectures that diverge from the traditional Transformer structure. Among them, four significant contenders\u2014linear transformers, state space model, long convolution, and linear recurrence\u2014have shown promising results as substitutes for self-attention (SA) modules when modeling long sequences. These alternatives are favored for their superior asymptotic time complexity and competitive performances. Linear Transformer\n\nLinear Transformer decomposes Softmax Attention into the form of the inner product of hidden representations, which allows it to use the \"Right Product Trick,\" where the product of keys and values is computed to avoid the quadratic matrix.",
    "transnormerllm-2": "Different methods utilize various hidden representations. For example, Katharopoulos et al. (2020) use 1+elu as an activation function, Qin et al. (2022b) use the cosine function to approximate the properties of softmax, and Ke et al. (2021); Zheng et al. (2022; 2023) approximate softmax through theoretical approaches. Although its theoretical complexity is , the actual computational efficiency of Linear Attention becomes quite low when used in causal attention due to the need for cumsum operations (Hua et al., 2022). On the other hand, most Linear Transformers still exhibit a certain performance gap compared to traditional Transformers (Katharopoulos et al., 2020; Liu et al., 2022). State Space Model\n\nState Space Model is based on the State Space Equation for sequence modeling (Gu et al., 2022b), using special initialization (Gu et al., 2020; 2022a), diagonalization assumptions (Gupta et al., 2022), and some techniques (Dao et al., 2022b) to achieve performance comparable to Transformers. On the other hand, due to the characteristics of the State Space Equation, it enables inference to be conducted within constant complexity (Gu et al., 2022b). Long Convolution\n\nLong convolution models (Qin et al., 2023a; Fu et al., 2023) utilize a kernel size equal to the input sequence length, facilitating a wider context compared to traditional convolutions. Training these models involves the efficient Fast Fourier Transforms (FFT) algorithm. However, long convolutions pose certain challenges, such as the need for causal convolution inference, which necessitates caching all historical computations similar to SA\u2019s key-value (KV) cache. The memory requirements for handling long sequences, coupled with the higher inference complexity compared to RNNs, make them less ideal for processing long sequences. Linear RNN\n\nLinear RNNs (Orvieto et al., 2023; Peng et al., 2023b), in contrast, stand out as more suitable replacements for SA in long-sequence modeling. A notable example is the RWKV (Peng et al., 2023b) model, a linear RNN-based LLM that has shown competitive performance against similarly scaled GPT models. 3 TransNormerLLM\n\n3.1 Architecture Improvement\n\nIn this section, we thoroughly investigate each module of the network and propose several improvements to achieve an optimal balance between efficiency and performance. Below, we outline the key designs of each block along with the inspiration behind each change. For the details of configurations for TransNormerLLM variants from 385M to 175B parameters, see Appendix A. 3.1.1 Improvement 1: Position encoding\n\nIn TransNormer, DiagAttention is used at the lower layers to avoid dilution issues. However, this leads to a lack of global interaction between tokens. In TransNormerLLM, we leverage LRPE (Qin et al., 2023b) with exponential decay (Press et al., 2022; Qin et al., 2023a; Peng et al., 2023b) to address this issue, retaining full attention at the lower layers. The expression of our position encoding is as follows:\n\na s \u200b t = \ud835\udc2a s \u22a4 \u200b \ud835\udc24 t \u200b \u03bb s \u2212 t \u200b exp i \u200b \u03b8 \u200b ( s \u2212 t ) . subscript \ud835\udc4e \ud835\udc60 \ud835\udc61 superscript subscript \ud835\udc2a \ud835\udc60 top subscript \ud835\udc24 \ud835\udc61 superscript \ud835\udf06 \ud835\udc60 \ud835\udc61 superscript \ud835\udc56 \ud835\udf03 \ud835\udc60 \ud835\udc61 a_{st}=\\mathbf{q}_{s}^{\\top}\\mathbf{k}_{t}\\lambda^{s-t}\\exp^{i\\theta(s-t)}. (1)\n\nwhich we call LRPE-d - Linearized Relative Positional Encoding with exponential decay. Similar to the original LRPE, we set to be learnable. We empirically find that rather than applying LRPE-d to every layer, applying it to the first layer and keeping other layers with exponential decay can speed up training by approximately 15-20% but only with a subtle effect on the performance. Note that this position encoding is fully compatible with Linear Attention, as it can be decomposed with respect to and separately. The value of for the -th head in the -th layer (assuming there are a total of heads and layers) is given by:\n\n\u03bb = exp \u2061 ( \u2212 8 \u200b h H \u00d7 ( 1 \u2212 l L ) ) . \ud835\udf06 8 \u210e \ud835\udc3b 1 \ud835\udc59 \ud835\udc3f \\textstyle\\lambda=\\exp\\left(-\\frac{8h}{H}\\times\\left(1-\\frac{l}{L}\\right)\\right). (2)\n\nHere, corresponds to the decay rate of the -th head, while corresponds to the decay rate of the -th layer. The term ensures that the Theoretical Receptive Fields (TRF) (Qin et al., 2023c) at the lower layers is smaller compared to the higher layers, which aligns with TransNormer\u2019s motivation. It should be noted that the decay rate in the last layer is set to 1, allowing each token to attend to global information. We choose to be non-learnable since we empirically found that gradients become unstable when is learnable, leading to NaN values. 3.1.2 Improvement 2: Gating mechanism\n\nGate can enhance the performance of the model and smooth the training process. In TransNormerLLM, we adopted the approach from Flash (Hua et al., 2022) and used the structure of Gated Linear Attention (GLA) in token mixing:\n\nTokenMixer : \ud835\udc0e = Norm \u200b ( \ud835\udc10\ud835\udc0a \u22a4 \u200b \ud835\udc15 ) \u2299 \ud835\udc14 , : TokenMixer \ud835\udc0e direct-product Norm superscript \ud835\udc10\ud835\udc0a top \ud835\udc15 \ud835\udc14 \\mathrm{TokenMixer}:\\mathbf{O}=\\mathrm{Norm}(\\mathbf{Q}\\mathbf{K}^{\\top}\\mathbf{V})\\odot\\mathbf{U}, (3)\n\nwhere:\n\n\ud835\udc10 = \u03d5 \u200b ( \ud835\udc17\ud835\udc16 q ) , \ud835\udc0a = \u03d5 \u200b ( \ud835\udc17\ud835\udc16 k ) , \ud835\udc15 = \ud835\udc17\ud835\udc16 v , \ud835\udc14 = \ud835\udc17\ud835\udc16 u . formulae-sequence \ud835\udc10 italic-\u03d5 subscript \ud835\udc17\ud835\udc16 \ud835\udc5e formulae-sequence \ud835\udc0a italic-\u03d5 subscript \ud835\udc17\ud835\udc16 \ud835\udc58 formulae-sequence \ud835\udc15 subscript \ud835\udc17\ud835\udc16 \ud835\udc63 \ud835\udc14 subscript \ud835\udc17\ud835\udc16 \ud835\udc62 \\mathbf{Q}=\\phi(\\mathbf{X}\\mathbf{W}_{q}),\\mathbf{K}=\\phi(\\mathbf{X}\\mathbf{W}_{k}),\\mathbf{V}=\\mathbf{X}\\mathbf{W}_{v},\\mathbf{U}=\\mathbf{X}\\mathbf{W}_{u}. (4)\n\nWe choose to be swish (Ramachandran et al., 2017) activation function as we empirically find that it outperforms other activation functions, as shown in Table 6. To further accelerate the model, we propose Simple GLU (SGLU), which removes the activation function from the original GLU structure as the gate itself can introduce non-linearity. Therefore, our channel mixing becomes:\n\nChannelMixer : \ud835\udc0e = [ \ud835\udc15 \u2299 \ud835\udc14 ] \u200b \ud835\udc16 o , \ud835\udc15 = \ud835\udc17\ud835\udc16 v , \ud835\udc14 = \ud835\udc17\ud835\udc16 u , : ChannelMixer formulae-sequence \ud835\udc0e delimited-[] direct-product \ud835\udc15 \ud835\udc14 subscript \ud835\udc16 \ud835\udc5c formulae-sequence \ud835\udc15 subscript \ud835\udc17\ud835\udc16 \ud835\udc63 \ud835\udc14 subscript \ud835\udc17\ud835\udc16 \ud835\udc62 \\vspace{-1mm}\\mathrm{ChannelMixer}:\\mathbf{O}=[\\mathbf{V}\\odot\\mathbf{U}]\\mathbf{W}_{o},\\\\\n\\mathbf{V}=\\mathbf{X}\\mathbf{W}_{v},\\mathbf{U}=\\mathbf{X}\\mathbf{W}_{u}, (5)\n\nWe empirically find that not using an activation function in GLU will not lead to any performance loss, as demonstrated in Table 7. 3.1.3 Improvement 3: Tensor normalization\n\nWe employ the NormAttention introduced in TransNormer (Qin et al., 2022a) as follows:\n\n\ud835\udc0e = Norm \u200b ( ( \ud835\udc10\ud835\udc0a \u22a4 ) \u200b \ud835\udc15 ) \ud835\udc0e Norm superscript \ud835\udc10\ud835\udc0a top \ud835\udc15 \\mathbf{O}=\\mathrm{Norm}((\\mathbf{Q}\\mathbf{K}^{\\top})\\mathbf{V}) (6)\n\nThis attention mechanism eliminates the softmax and scaling operation. Moreover, it can be transformed into linear attention through right multiplication:\n\n\ud835\udc0e = Norm \u200b ( \ud835\udc10 \u200b ( \ud835\udc0a \u22a4 \u200b \ud835\udc15 ) ) \ud835\udc0e Norm \ud835\udc10 superscript \ud835\udc0a top \ud835\udc15 \\mathbf{O}=\\mathrm{Norm}(\\mathbf{Q}(\\mathbf{K}^{\\top}\\mathbf{V})) (7)\n\nThis linear form allows for recurrent prediction with a complexity of , making it efficient during inference. Specifically, we only update in a recurrent manner without computing the full attention matrix. In TransNormerLLM, we replace the RMSNorm with a new simple normalization function called SimpleRMSNorm, abbreviated as SRMSNorm:\n\nSRMSNorm \u200b ( \ud835\udc31 ) = \ud835\udc31 \u2016 \ud835\udc31 \u2016 2 / d . SRMSNorm \ud835\udc31 \ud835\udc31 subscript norm \ud835\udc31 2 \ud835\udc51 \\textstyle\\mathrm{SRMSNorm}(\\mathbf{x})=\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_{2}/\\sqrt{d}}. (8)\n\nWe empirically find that using SRMSNorm does not lead to any performance loss, as demonstrated in the ablation study in Table. 8. 3.1.4 The overall structure\n\nThe overall structure is illustrated in Figure 1. In this structure, the input is updated through two consecutive steps: First, it undergoes Gated Linear Attention (GLA) with the application of SimpleRMSNorm (SRMSNorm) normalization. Then, it goes through the Simple Gated Linear Unit (SGLU) with SRMSNorm normalization again. This overall architecture helps improve the model\u2019s performance based on the PreNorm approach. The pseudo-code of the overall process is as follows:\n\n\ud835\udc17 = \ud835\udc17 + GLA \u200b ( SRMSNorm \u200b ( \ud835\udc17 ) ) , \ud835\udc17 = \ud835\udc17 + SGLU \u200b ( SRMSNorm \u200b ( \ud835\udc17 ) ) . formulae-sequence \ud835\udc17 \ud835\udc17 GLA SRMSNorm \ud835\udc17 \ud835\udc17 \ud835\udc17 SGLU SRMSNorm \ud835\udc17 \\begin{gathered}\\mathbf{X}=\\mathbf{X}+\\mathrm{GLA}(\\mathrm{SRMSNorm}(\\mathbf{X})),\\\\\n\\mathbf{X}=\\mathbf{X}+\\mathrm{SGLU}(\\mathrm{SRMSNorm}(\\mathbf{X})).\\end{gathered} (9)\n\n3.2 Training Optimization\n\n3.2.1 Lightning Attention\n\nThe structure of linear attention allows for efficient attention calculation with a complexity of through right-multiplication. However, for causal prediction, right-multiplication is not efficient as it necessitates cumsum computation (Hua et al., 2022), which hinders parallelism training. As a result, during training, we continue to use the conventional left-multiplication version. To accelerate attention calculations, we introduce the Lightning Attention algorithm inspired by (Dao, 2023; Dao et al., 2022a), which makes our linear attention IO-friendly. It computes the following:\n\n\ud835\udc0e = ( \ud835\udc10\ud835\udc0a \u22a4 \u2299 \ud835\udc0c ) \u200b \ud835\udc15 . \ud835\udc0e direct-product superscript \ud835\udc10\ud835\udc0a top \ud835\udc0c \ud835\udc15 \\mathbf{O}=(\\mathbf{Q}\\mathbf{K}^{\\top}\\odot\\mathbf{M})\\mathbf{V}. (10)\n\nHere, is the attention mask which enables lower triangular causal masking and positional encoding. In the Lightning Attention, we split the inputs into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. Then we accumulate the final results. The computation speed is accelerated by avoiding the operations on slow HBM. The implementation details of Lightning Attention are shown in Appendix B, where Algorithm 3 for forward pass and Algorithm 4 for backward pass. 3.2.2 Model Parallelism on TransNormerLLM\n\nTo effectively execute large-scale pre-training for TransNormerLLM, we have put efforts on system optimization encompassing various dimensions. Specifically, we employ fully sharded data parallelism (FSDP) (Zhao et al., 2023), a technique that shards all model parameters, gradients, and optimizer state tensors across the entire cluster. This strategic partition significantly reduces the memory footprint on each individual GPU, thereby enhancing memory utilization. In our pursuit of greater efficiency, we leverage activation checkpointing (Shoeybi et al., 2019), which minimizes the cached activations in memory during the forward pass. Instead of retaining these activations, they are recomputed when calculating gradients in the backward pass. This approach saves huge GPU memory thus enable to apply bigger batch size. Furthermore, we harness automatic mixed precision (AMP) (Micikevicius et al., 2017) to simultaneously save GPU memory and expedite computational speed. It\u2019s noteworthy that in our experimental setup, we employ BFloat16 (Kalamkar et al., 2019) due to its observed advantage in enhancing the training stability of TransNormerLLM models. In addition to the previously mentioned optimization endeavors, we delve deeper into the realm of system engineering by implementing model parallelism specifically tailored to linear transformers, drawing inspiration from Megatron-LM model parallelism (Shoeybi et al., 2019). In a standard transformer model, each transformer layer comprises a self-attention block followed by a two-layer multi-layer perceptron (MLP) block. Megatron-LM model parallelism independently addresses these two constituent blocks. Similarly, within the architecture of TransNormerLLM, characterized by its two primary components, SGLU and GLA, we apply model parallelism to each of these components separately. The intricate details of our model parallelism strategies are elaborated below. Model Parallelism on SGLU\n\nRecall the SGLU structure in (5):\n\n\ud835\udc0e = [ ( \ud835\udc17\ud835\udc16 v ) \u2299 ( \ud835\udc17\ud835\udc16 u ) ] \u200b \ud835\udc16 o , \ud835\udc0e delimited-[] direct-product subscript \ud835\udc17\ud835\udc16 \ud835\udc63 subscript \ud835\udc17\ud835\udc16 \ud835\udc62 subscript \ud835\udc16 \ud835\udc5c \\mathbf{O}=[(\\mathbf{X}\\mathbf{W}_{v})\\odot(\\mathbf{X}\\mathbf{W}_{u})]\\mathbf{W}_{o}, (11)\n\nThe model parallelism adaptation of SGLU is as follows:\n\n[ \ud835\udc0e 1 \u2032 , \ud835\udc0e 2 \u2032 ] = \ud835\udc17 [ \ud835\udc16 v 1 , \ud835\udc16 v 2 ] \u2299 \ud835\udc17 [ \ud835\udc16 u 1 , \ud835\udc16 u 2 ] , = [ \ud835\udc17\ud835\udc16 v 1 , \ud835\udc17\ud835\udc16 v 2 ] \u2299 [ \ud835\udc17\ud835\udc16 u 1 , \ud835\udc17\ud835\udc16 u 2 ] , [\\mathbf{O}^{\\prime}_{1},\\mathbf{O}^{\\prime}_{2}]=\\mathbf{X}[\\mathbf{W}_{v}^{1},\\mathbf{W}_{v}^{2}]\\odot\\mathbf{X}[\\mathbf{W}_{u}^{1},\\mathbf{W}_{u}^{2}],=[\\mathbf{X}\\mathbf{W}_{v}^{1},\\mathbf{X}\\mathbf{W}_{v}^{2}]\\odot[\\mathbf{X}\\mathbf{W}_{u}^{1},\\mathbf{X}\\mathbf{W}_{u}^{2}], (12)\n\nwhich splits the weight matrices and along their columns and obtains an output matrix splitting along its columns too. Then the split output is multiplied by another matrix which is split along its rows as:\n\n\ud835\udc0e = [ \ud835\udc0e 1 \u2032 , \ud835\udc0e 2 \u2032 ] \u200b [ \ud835\udc16 o 1 , \ud835\udc16 o 2 ] \u22a4 = \ud835\udc0e 1 \u2032 \u200b \ud835\udc16 o 1 + \ud835\udc0e 2 \u2032 \u200b \ud835\udc16 o 2 \ud835\udc0e superscript subscript \ud835\udc0e 1 \u2032 superscript subscript \ud835\udc0e 2 \u2032 superscript superscript subscript \ud835\udc16 \ud835\udc5c 1 superscript subscript \ud835\udc16 \ud835\udc5c 2 top superscript subscript \ud835\udc0e 1 \u2032 superscript subscript \ud835\udc16 \ud835\udc5c 1 superscript subscript \ud835\udc0e 2 \u2032 superscript subscript \ud835\udc16 \ud835\udc5c 2 \\mathbf{O}=[\\mathbf{O}_{1}^{\\prime},\\mathbf{O}_{2}^{\\prime}][\\mathbf{W}_{o}^{1},\\mathbf{W}_{o}^{2}]^{\\top}=\\mathbf{O}_{1}^{\\prime}\\mathbf{W}_{o}^{1}+\\mathbf{O}_{2}^{\\prime}\\mathbf{W}_{o}^{2} (13)\n\nSimilar with model parallelism in Megatron-LM, this whole procedure splits three general matrix multiplies (GEMMs) inside the SGLU block across multiple GPUs and only introduces a single all-reduce collective communication operation in both the forward and backward passes, respectively. Model Parallelism on GLA\n\nRecall the GLA block in (3) and (4), its model parallelism version is:\n\n[ \ud835\udc0e \ud835\udfcf , \ud835\udc0e \ud835\udfd0 ] = SRMSNorm \u200b ( \ud835\udc10\ud835\udc0a \u22a4 \u200b \ud835\udc15 ) \u2299 \ud835\udc14 , subscript \ud835\udc0e 1 subscript \ud835\udc0e 2 direct-product SRMSNorm superscript \ud835\udc10\ud835\udc0a top \ud835\udc15 \ud835\udc14 [\\mathbf{O_{1}},\\mathbf{O_{2}}]=\\mathrm{SRMSNorm}(\\mathbf{Q}\\mathbf{K}^{\\top}\\mathbf{V})\\odot\\mathbf{U}, (14)\n\nwhere:\n\n\ud835\udc10 = [ \u03d5 \u200b ( \ud835\udc17\ud835\udc16 q 1 ) , \u03d5 \u200b ( \ud835\udc17\ud835\udc16 q 2 ) ] , \ud835\udc0a = [ \u03d5 \u200b ( \ud835\udc17\ud835\udc16 q 1 ) , \u03d5 \u200b ( \ud835\udc17\ud835\udc16 q 2 ) ] , \ud835\udc15 = \ud835\udc17 \u200b [ \ud835\udc16 v 1 , \ud835\udc16 v 2 ] , \ud835\udc14 = \ud835\udc17 \u200b [ \ud835\udc16 u 1 , \ud835\udc16 u 2 ] , formulae-sequence \ud835\udc10 italic-\u03d5 superscript subscript \ud835\udc17\ud835\udc16 \ud835\udc5e 1 italic-\u03d5 superscript subscript \ud835\udc17\ud835\udc16 \ud835\udc5e 2 formulae-sequence \ud835\udc0a italic-\u03d5 superscript subscript \ud835\udc17\ud835\udc16 \ud835\udc5e 1 italic-\u03d5 superscript subscript \ud835\udc17\ud835\udc16 \ud835\udc5e 2 formulae-sequence \ud835\udc15 \ud835\udc17 superscript subscript \ud835\udc16 \ud835\udc63 1 superscript subscript \ud835\udc16 \ud835\udc63 2 \ud835\udc14 \ud835\udc17 superscript subscript \ud835\udc16 \ud835\udc62 1 superscript subscript \ud835\udc16 \ud835\udc62 2 \\displaystyle\\mathbf{Q}=[\\phi(\\mathbf{X}\\mathbf{W}_{q}^{1}),\\phi(\\mathbf{X}\\mathbf{W}_{q}^{2})],\\mathbf{K}=[\\phi(\\mathbf{X}\\mathbf{W}_{q}^{1}),\\phi(\\mathbf{X}\\mathbf{W}_{q}^{2})],\\mathbf{V}=\\mathbf{X}[\\mathbf{W}_{v}^{1},\\mathbf{W}_{v}^{2}],\\mathbf{U}=\\mathbf{X}[\\mathbf{W}_{u}^{1},\\mathbf{W}_{u}^{2}], (15)\n\nNote that in our implementation, we use the combined QKVU projection to improve computation efficiency for linear attention. The obtained split output matrix again is multiplied by a weight matrix split along its columns which is similar to (13). 3.3 Robust Inference\n\nIn this section, we discuss the inference problem in TransNormerLLM. It is important to note that the formula 1 can be decomposed into the following form:\n\na s \u200b t = ( \ud835\udc2a s \u200b \u03bb s \u200b exp i \u200b \u03b8 \u200b s ) \u22a4 \u200b ( \ud835\udc24 t \u200b \u03bb \u2212 t \u200b exp i \u200b \u03b8 \u200b t ) . subscript \ud835\udc4e \ud835\udc60 \ud835\udc61 superscript subscript \ud835\udc2a \ud835\udc60 superscript \ud835\udf06 \ud835\udc60 superscript \ud835\udc56 \ud835\udf03 \ud835\udc60 top subscript \ud835\udc24 \ud835\udc61 superscript \ud835\udf06 \ud835\udc61 superscript \ud835\udc56 \ud835\udf03 \ud835\udc61 a_{st}=(\\mathbf{q}_{s}\\lambda^{s}\\exp^{i\\theta s})^{\\top}(\\mathbf{k}_{t}\\lambda^{-t}\\exp^{i\\theta t}). (16)\n\nThis allows TransNormerLLM to perform inference in the form of an RNN. Details of the procedure are shown in Algorithm 1. However, it is worth noting that , which results in:\n\n\u2016 \ud835\udc2a s \u200b \u03bb s \u200b exp i \u200b \u03b8 \u200b s \u2016 2 = \u2016 \ud835\udc2a s \u2016 2 \u200b \u03bb s \u2192 0 , \u2016 \ud835\udc24 t \u200b \u03bb \u2212 t \u200b exp i \u200b \u03b8 \u200b t \u2016 2 = \u2016 \ud835\udc24 t \u2016 2 \u200b \u03bb \u2212 t \u2192 \u221e , formulae-sequence subscript norm subscript \ud835\udc2a \ud835\udc60 superscript \ud835\udf06 \ud835\udc60 superscript \ud835\udc56 \ud835\udf03 \ud835\udc60 2 subscript norm subscript \ud835\udc2a \ud835\udc60 2 superscript \ud835\udf06 \ud835\udc60 \u2192 0 subscript norm subscript \ud835\udc24 \ud835\udc61 superscript \ud835\udf06 \ud835\udc61 superscript \ud835\udc56 \ud835\udf03 \ud835\udc61 2 subscript norm subscript \ud835\udc24 \ud835\udc61 2 superscript \ud835\udf06 \ud835\udc61 \u2192 \\|\\mathbf{q}_{s}\\lambda^{s}\\exp^{i\\theta s}\\|_{2}=\\|\\mathbf{q}_{s}\\|_{2}\\lambda^{s}\\to 0,\\\\\n\\|\\mathbf{k}_{t}\\lambda^{-t}\\exp^{i\\theta t}\\|_{2}=\\|\\mathbf{k}_{t}\\|_{2}\\lambda^{-t}\\to\\infty, (17)\n\nleading to numerical precision issues. To avoid these issues, we propose a Robust Inference Algorithm in 2. Since , , for simplicity, we will omit LRPE (Qin et al., 2023b) in the subsequent discussions, considering only We provide a mathematical proof of in Appendix C\n\n4 Experiments\n\nWe use PyTorch (Paszke et al., 2019) and Triton (Tillet et al., 2019) to implement TransNormerLLM in Metaseq framework (Zhang et al., 2022).",
    "transnormerllm-3": "Our model is trained using Adam optimizer (Kingma & Ba, 2017), and we employ FSDP to efficiently scale our model to NVIDIA A100 80G clusters. We additionally leverage the model parallel as appropriate to optimize performance. In ablation studies, all models are trained on a sampled corpus from our corpus with 300B tokens. In order to reduce the fluctuation of Losses and PPLs in the tables below, we compute the average Losses and PPLs of the last 1k iterations as the final metrics. For our benchmark models, we train our 385M, 1B, and 7B models on our corpus for 1 trillion, 1.2 trillion, and 1.4 trillion tokens respectively. We use an input sequence length of 8192 tokens in our pretraining process. For a comprehensive understanding of our corpus, encompassing intricate details such as data preprocessing methods and tokenization procedures, we direct interested readers to Appendix D. 4.1 Architecture Ablations\n\nTransformer vs TransNormerLLM\n\nWe carried out a meticulous series of comparative tests between our TransNormerLLM and Transformer, spanning over an array of disparate sizes. The comparative performance of these models is clearly illustrated in Table 1. Under identical configurations, it becomes evident that our TransNormerLLM exhibits a superior performance profile compared to Transformer. We observed that TransNormerLLM outperformed Transformer by a remarkable 5% at the size of 385M. More importantly, as the size reached 1B, this superiority became even more pronounced, with an advantage of 9% for TransNormerLLM over Transformer. TransNormer vs TransNormerLLM\n\nWe compare the original TransNormer and the improved TransNormerLLM and the results are shown in Table 2. TransNormerLLM exhibited an enhancement of 2% and 1% respectively. Positional Encoding\n\nIn the positional encoding experiment, we conducted a series of tests, comparing Mix (LRPE-d for the first layer, Exp-Decay for the rest), APE (Absolute Positional Encoding), LRPE, Exp-Decay (Exponential Decay), and LRPE-d. As evident from Table 3, Ours and LRPE-d achieve better performance than other options. We select the Mix positional encoding as it boosts the training speed up to 20% while only slightly worse than LRPE-d. We also perform ablations on the decay temperature in Eq. 2. The perplexity of the TransNormerLLM is reduced by adding the decay temperature, as shown in Table 4. Gating Mechanism\n\nWe conduct ablation studies to examine the effect of including the gating mechanism. As observed in Table 5, gate enabled the reduction of the loss value from 2.263 to 2.248. GLA Activation Functions\n\nWe conducted experiments on the GLA (Gated Linear Attention) structure with respect to the activation function. As shown in Table 6, using Swish and 1+elu leads to similar performance. However, in our experiments, using 1+elu in our 7B model may encounter a NaN problem, so we use Swish in our model. GLU Activation Functions\n\nWe conduct an experiment by removing the activation function within the Gated Linear Units (GLU) structure. As shown in Table 7, the results reveal that this alteration had a negligible impact on the final outcome. As a result, we decide to adopt the Simple Gated Linear Units (SGLU) structure in our final model configuration. Normalization functions\n\nIn our study, we conducted a series of ablation tests employing various normalization methods including SRMSNorm, RMSNorm and LayerNorm. The results indicate that there is almost no difference among these methods when applied to TransNormerLLM. Nevertheless, during the course of our testing, we revisited and re-engineered the SRMSNorm using Triton. As it is shown in Figure 2, empirical evidence supports that our modification offers a significant boost in computational speed when operating with larger dimensions, compared to the PyTorch implementation methods. Lightning Attention\n\nWe conducted a speed and memory comparison between our Lightning Attention and the baseline, which is the PyTorch implementation of the NormAttention (Qin et al., 2022a). Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass. Baseline runtime grows quadratically with sequence length, while Lightning Attention operates significantly faster, at least faster than the PyTorch implementation. Figure 3 (right) reports the memory footprint of Lightning Attention compared to the baseline. The memory footprint of Lightning Attention grows linearly with sequence length, which is up to more efficient than the baseline when the sequence length is 8192. Our proposed Lightning Attention achieves superior efficiency. 4.2 Benchmarks\n\nIn order to validate the effectiveness of TransNormerLLM, we tested our 385M, 1B, and 7B models on Commonsense Reasoning Task, MMLU(Hendrycks et al., 2021), CMMLU(Li et al., 2023), and C-Eval(Huang et al., 2023). For comparison, we selected several open-source models as competitors, including Transformer-based models such as OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023), BLOOM (Workshop et al., 2023), GPT-Neo (Black et al., 2022), GPT-J (Wang & Komatsuzaki, 2021), MPT (Team et al., 2023), Falcon (Almazrouei et al., 2023), LLaMA1/2 (Touvron et al., 2023a; b), OpenLLAMA v1/v2 (Geng & Liu, 2023), Baichuan 1/2 (Baichuan, 2023), ChatGLM 1/2 (Zeng et al., 2022; Du et al., 2022), and non-Transformer model RWKV (Peng et al., 2023a). It can be observed that, compared to these models, TransNormerLLM remains highly competitive. Commonsense Reasoning\n\nWe report BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2019), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and their average. We report 0-shot results for all benchmarks using LM-Eval-Harness (Gao et al., 2021). All of our models achieve competitive performance compared to existing state-of-the-art LLMs, showcasing a remarkable ability to comprehend and apply commonsense reasoning. Aggregated Benchmarks\n\nWe report the overall results for MMLU (Hendrycks et al., 2021), CMMLU (Li et al., 2023), C-Eval (Huang et al., 2023). Official scripts were used for evaluating MMLU, CMMLU, and C-Eval, with all evaluation results being conducted with a 5-shot setup. In comparison to top-tier open-source models available in the industry, our models have demonstrated matched performance in both English and Chinese benchmarks. 4.3 Scaling to 175B\n\nFurthermore, we have carried out a series of experiments to assess the efficacy of model parallelism as applied to the TransNormerLLM architecture. The comprehensive outcomes of these experiments have been thoughtfully presented in Appendix E.1. Moreover, our research extends to the meticulous evaluation of various cutting-edge system optimization techniques. This evaluation encompasses their impact on both training speed and context length across models ranging from 7B to 175B in scale. We have thoughtfully documented the detailed results of these experiments in Appendix E.2. 5 Conclusion\n\nWe introduced TransNormerLLM in this paper, an improved TransNormer that is tailored for LLMs. Our TransNormerLLM consistently outperformed Transformers in both accuracy and efficiency. Extensive ablations demonstrate the effectiveness of our modifications and innovations in position encoding, gating mechanism, activation functions, normalization functions, and lightning attentions. These modifications collectively contribute to TransNormerLLM\u2019s outstanding performance, positioning it as a promising choice for state-of-the-art language models. The benchmark results for models with sizes of 385 million, 1 billion, and 7 billion parameters unequivocally demonstrate that TransNormerLLM not only matches the performance of current leading Transformer-based Large Language Models (LLMs) but also enjoys faster inference speeds. We will release our pre-trained TransNormerLLM models to foster community advancements in efficient LLM.",
    "transnormerllm-4": "References\n\nAlmazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. Falcon-40b: an open large language model with state-of-the-art performance. Technical report, Technical report, Technology Innovation Institute, 2023. Baichuan (2023) Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305. Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023. Bisk et al. (2019) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.",
    "transnormerllm-5": "Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.",
    "transnormerllm-6": "Choromanski et al. (2021) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH. Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.",
    "transnormerllm-7": "Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.",
    "transnormerllm-8": "Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao et al. (2022a) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022a. Dao et al. (2022b) Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022b. doi: 10.48550/arXiv.2212.14052. URL https://doi.org/10.48550/arXiv.2212.14052. Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling, 2022.",
    "transnormerllm-9": "Fu et al. (2023) Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. CoRR, abs/2302.06646, 2023. doi: 10.48550/arXiv.2302.06646. URL https://doi.org/10.48550/arXiv.2302.06646. Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation.",
    "transnormerllm-10": "Version v0. 0.1. Sept, 2021. Geng & Liu (2023) Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama. URL: https://github. com/openlm-research/open_llama, 2023. Gu et al. (2020) Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020.",
    "transnormerllm-11": "Gu et al. (2022a) Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022a. URL http://papers.nips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html. Gu et al. (2022b) Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL https://openreview.net/forum?id=uYLFoz1vlAC. Gupta et al. (2022) Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html. Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. Hua et al. (2022) Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time.",
    "transnormerllm-12": "arXiv preprint arXiv:2202.10447, 2022. Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models, 2023.",
    "transnormerllm-13": "Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training.",
    "transnormerllm-14": "arXiv preprint arXiv:1905.12322, 2019. Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020. Ke et al. (2021) Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=09-528y2Fgf. Kingma & Ba (2017) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.",
    "transnormerllm-15": "Li et al. (2023) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.",
    "transnormerllm-16": "Liu et al. (2022) Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955, 2022. Micikevicius et al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training.",
    "transnormerllm-17": "arXiv preprint arXiv:1710.03740, 2017. Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018. Orvieto et al. (2023) Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences, 2023. Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024\u20138035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.",
    "transnormerllm-18": "Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.",
    "transnormerllm-19": "Peng et al. (2023a) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023a. Peng et al. (2023b) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023b. Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Qin et al. (2022a) Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 7025\u20137041, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.473. Qin et al. (2022b) Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Qin et al. (2023a) Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=IxmWsm4xrua. Qin et al. (2023b) Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Linearized relative positional encoding. Transactions on Machine Learning Research, 2023b. Qin et al. (2023c) Zhen Qin, Yiran Zhong, and Hui Deng. Exploring transformer extrapolation, 2023c. Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf, 2018. Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Rae et al. (2022) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022. Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017. Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.",
    "transnormerllm-20": "Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019.",
    "transnormerllm-21": "Scao et al. (2022) Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy. What language model to train if you have one million gpu hours?, 2022. Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.",
    "transnormerllm-22": "arXiv preprint arXiv:1909.08053, 2019. Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022. Team et al. (2023) MosaicML NLP Team et al. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www. mosaicml. com/blog/mpt-7b. Accessed, pp. 05\u201305, 2023. Tillet et al. (2019) Philippe Tillet, Hsiang-Tsung Kung, and David D. Cox. Triton: an intermediate language and compiler for tiled neural network computations. Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, 2019. Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.",
    "transnormerllm-23": "Advances in neural information processing systems, 30, 2017. Wang & Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021. Workshop et al. (2023) BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u015far, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.",
    "transnormerllm-24": "Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.",
    "transnormerllm-25": "Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model.",
    "transnormerllm-26": "arXiv preprint arXiv:2210.02414, 2022. Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.",
    "transnormerllm-27": "Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng et al. (2022) Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011\u201327041. PMLR, 2022. Zheng et al. (2023) Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong. Efficient attention via control variates.",
    "transnormerllm-28": "In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=G-uNfHKrj46. Appendix\n\nAppendix A Model\n\nWe present distinct model variants of the TransNormerLLM architecture, delineating their respective configurations with regard to parameters, layers, attention heads, and hidden dimensions. The detailed specifications are meticulously tabulated in Table 10. Appendix B Lightning Attention\n\nWe present the algorithm details of Lightning Attention includes forward pass and backward pass in Algorithm 3 and 4, respectively. Appendix C Proving robust inference algorithm\n\nWe will use induction to prove: . Base Case ():\n\n[ \ud835\udc24\ud835\udc2f ] 1 subscript delimited-[] \ud835\udc24\ud835\udc2f 1 \\displaystyle[\\mathbf{kv}]_{1} = ( [ \ud835\udc24\ud835\udc2f ] 0 + \ud835\udc24 \ud835\udfcf \u200b \u03bb \u2212 1 \u200b \ud835\udc2f 1 \u22a4 ) absent subscript delimited-[] \ud835\udc24\ud835\udc2f 0 subscript \ud835\udc24 1 superscript \ud835\udf06 1 superscript subscript \ud835\udc2f 1 top \\displaystyle=([\\mathbf{kv}]_{0}+\\mathbf{k_{1}}\\lambda^{-1}\\mathbf{v}_{1}^{\\top}) (18) = \u03bb \u2212 1 \u200b ( \ud835\udc24 \ud835\udfcf \u200b \ud835\udc2f 1 \u22a4 ) absent superscript \ud835\udf06 1 subscript \ud835\udc24 1 superscript subscript \ud835\udc2f 1 top \\displaystyle=\\lambda^{-1}(\\mathbf{k_{1}}\\mathbf{v}_{1}^{\\top}) = \u03bb \u2212 1 \u200b [ \ud835\udc24\ud835\udc2f \u00af ] 1 . absent superscript \ud835\udf06 1 subscript delimited-[] \u00af \ud835\udc24\ud835\udc2f 1 \\displaystyle=\\lambda^{-1}[{\\mathbf{\\overline{kv}}}]_{1}. Assume the statement holds for , i.e., . Then, when :\n\n[ \ud835\udc24\ud835\udc2f ] m subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc5a \\displaystyle[\\mathbf{kv}]_{m} = [ \ud835\udc24\ud835\udc2f ] m \u2212 1 + \ud835\udc24 \ud835\udc26 \u200b \u03bb \u2212 m \u200b \ud835\udc2f m \u22a4 absent subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc5a 1 subscript \ud835\udc24 \ud835\udc26 superscript \ud835\udf06 \ud835\udc5a superscript subscript \ud835\udc2f \ud835\udc5a top \\displaystyle=[\\mathbf{kv}]_{m-1}+\\mathbf{k_{m}}\\lambda^{-m}\\mathbf{v}_{m}^{\\top} (19) = \u03bb \u2212 ( m \u2212 1 ) \u200b [ \ud835\udc24\ud835\udc2f \u00af ] m \u2212 1 + \ud835\udc24 \ud835\udc26 \u200b \u03bb \u2212 m \u200b \ud835\udc2f m \u22a4 absent superscript \ud835\udf06 \ud835\udc5a 1 subscript delimited-[] \u00af \ud835\udc24\ud835\udc2f \ud835\udc5a 1 subscript \ud835\udc24 \ud835\udc26 superscript \ud835\udf06 \ud835\udc5a superscript subscript \ud835\udc2f \ud835\udc5a top \\displaystyle=\\lambda^{-(m-1)}[{\\mathbf{\\overline{kv}}}]_{m-1}+\\mathbf{k_{m}}\\lambda^{-m}\\mathbf{v}_{m}^{\\top} = \u03bb \u2212 m \u200b ( \u03bb \u200b [ \ud835\udc24\ud835\udc2f \u00af ] m \u2212 1 + \ud835\udc24 \ud835\udc26 \u200b \ud835\udc2f m \u22a4 ) absent superscript \ud835\udf06 \ud835\udc5a \ud835\udf06 subscript delimited-[] \u00af \ud835\udc24\ud835\udc2f \ud835\udc5a 1 subscript \ud835\udc24 \ud835\udc26 superscript subscript \ud835\udc2f \ud835\udc5a top \\displaystyle=\\lambda^{-m}(\\lambda[{\\mathbf{\\overline{kv}}}]_{m-1}+\\mathbf{k_{m}}\\mathbf{v}_{m}^{\\top}) = \u03bb \u2212 m \u200b [ \ud835\udc24\ud835\udc2f \u00af ] m , absent superscript \ud835\udf06 \ud835\udc5a subscript delimited-[] \u00af \ud835\udc24\ud835\udc2f \ud835\udc5a \\displaystyle=\\lambda^{-m}[{\\mathbf{\\overline{kv}}}]_{m},\n\nthe statement holds. Therefore, by induction, the statement holds for all . Thus, both the Origin Inference Algorithm and the Robust Inference Algorithm yield the same results. Appendix D Corpus\n\nWe gather an extensive corpus of publicly accessible text from the internet, totaling over TB in size. The collected data are processed by our data preprocessing procedure as shown in Figure 5, leaving a TB cleaned corpus with roughly 2 trillion tokens. We categorize our data sources to provide better transparency and understanding. The specifics of these categories are outlined in Table 11. D.1 Data Preprocessing\n\nOur data preprocessing procedure consists of three steps: 1). rule-based filtering, 2). deduplication, and 3). a self-cleaning scheme. Before being added to the training corpus, the cleaned corpus needs to be evaluated by humans. Rule-based filtering\n\nThe rules we used to filter our collected data are listed as follows:\n\n\u2022\n\nRemoval of HTML Tags and URLs: The initial step in our process is the elimination of HTML tags and web URLs from the text. This is achieved through regular expression techniques that identify these patterns and remove them, ensuring the language model focuses on meaningful textual content. \u2022\n\nElimination of Useless or Abnormal Strings: Subsequently, the cleaned dataset undergoes a second layer of refinement where strings that do not provide value, such as aberrant strings or garbled text, are identified and excised. This process relies on predefined rules that categorize certain string patterns as non-contributing elements. \u2022\n\nDeduplication of Punctuation Marks: We address the problem of redundant punctuation marks in the data. Multiple consecutive punctuation marks can distort the natural flow and structure of sentences when training the model. We employ a rule-based system that trims these duplications down to a single instance of each punctuation mark. \u2022\n\nHandling Special Characters: Unusual or special characters that are not commonly part of the language\u2019s text corpus are identified and either removed or replaced with a standardized representation. \u2022\n\nNumber Standardization: Numerical figures may be presented in various formats across different texts. These numbers are standardized into a common format to maintain consistency. \u2022\n\nPreservation of Markdown/LaTeX Formats: While removing non-textual elements, exceptions are made for texts in Markdown and LaTeX formats. Given their structured nature and ubiquitous use in academia and documentation, preserving these formats can enhance the model\u2019s ability to understand and generate similarly formatted text. Deduplication\n\nTo ensure the uniqueness of our data and avert the risk of overfitting, we employ an efficient de-duplication strategy at the document or line level using MinHash and Locality-Sensitive Hashing (LSH) algorithms. This combination of MinHash and LSH ensures a balance between computational efficiency and accuracy in the deduplication process, providing a robust mechanism for data deduplication and text watermark removal. Self-cleaning scheme\n\nOur data self-cleaning process involves an iterative loop of the following three steps to continuously refine and enhance the quality of our dataset. An issue of using model-based data filters is that the filtered data will have a similar distribution as the evaluation model, which may have a significant impact on the diversity of the training data. Assuming that the majority of the pre-processed data is of high quality, we can train an evaluation model on the entire set of pre-processed data, and the model will automatically smooth the data manifold distribution and outlet low-quality data while retaining the majority of the diversities. The self-cleaning scheme unfolds as follows:\n\n\u2022\n\nEvaluation Model: We train a 385M model on the pre-processed corpus to act as a data quality filter. \u2022\n\nModel-Based Data Filtering: We use the evaluation model to assess each piece of data with perplexity. Only data achieving a score above a certain threshold is preserved for the next step. Low-quality data are weeded out at this stage. \u2022\n\nHuman Evaluation: We sample a small portion of the filtered data and manually evaluate the quality. These steps are repeated in cycles, with each iteration improving the overall quality of the data and ensuring the resulting model is trained on relevant, high-quality text. This self-cleaning process provides a robust mechanism for maintaining data integrity, thereby enhancing the performance of the resulting language model. D.2 Tokenization\n\nWe tokenize the data with the Byte-Pair Encoding (BPE) algorithm. Notably, to enhance compatibility with Chinese language content, a significant number of common and uncommon Chinese characters have been incorporated into our vocabulary. In cases where vocabulary items are not present in the dictionary, the words are broken down into their constituent UTF-8 characters. This strategy ensures comprehensive coverage and flexibility for diverse linguistic input during model training. Appendix E Additional Experimental Results\n\nE.1 Model Parallelism on TransNormerLLM\n\nWe conduct a series of experiments with a 7B TransNormerLLM model to investigate the performance of model parallelism on TransNormerLLM in terms of speed and memory. These tests are carried out on a single Nvidia DGX node that houses eight A100 80G GPUs linked by NVLink. In this experiment, FSDP is enabled and Flash Attention (Dao et al., 2022a) is used on the Transformer. Table 12 shows the results for training speed and memory consumption. It can be seen that model parallelism has a significant effect on memory conservation, as increasing the number of partitions for the model results in lower memory consumption per GPU. Due to NVLink constraints, we kept the dimension of model parallelism within 8 in all of our experiments. The TransNormerLLM-7B model requires only 24.1GB of memory on a single GPU when the model parallel size is set to 8, representing a significant memory reduction of 62.3% when compared to the model parallel size of 1. In comparison, the Transformer-7B model consumes 28.7GB of memory under the same configuration. While model parallelism conserves memory, it is worth noting that training speed is only marginally reduced. TransNormerLLM consistently outperforms Transformer by a wide margin. E.2 Stress Tests on Model Size and Context Length\n\nA series of stress tests are performed to assess the efficacy of the designed system optimization strategy. The model is scaled up to 175B, which is the largest released version of the TransNormerLLM model. However, this augmentation poses significant training challenges. We use a wide range of distributed training techniques to effectively train such a large model, with the goal of reducing GPU memory consumption while increasing computational and communication efficiencies. To ensure the feasibility of training these massive TransNormerLLM models, Lightning Attention, FSDP, Model Parallelism, AMP, and Activation Checkpointing are used. For the Transformer models, we use Flash Attention (Dao et al., 2022a) in all experiments. Model Size\n\nWe perform training experiments on variously sized Transformer and TransNormerLLM models using a large-scale A100 80G GPU cluster, as shown in Table 13. To achieve the maximum speed for various model sizes, we keep the context length constant at 2048 and increased the batch size until we reached the GPU memory limit. TransNormerLLMs consistently outperform their Transformer counterparts in terms of computation speed. This observation validates the TransNormerLLM model\u2019s advantageous linear computational complexity, reinforcing its efficacy. Context Length\n\nOne of the strengths of TransNormerLLM lies in its utilization of linear attention computation, which exhibits computational and storage complexities linearly correlated with the sequence length. To validate this outstanding characteristic of TransNormerLLM, we conduct training experiments on Transformer and TransNormerLLM models with varying parameter sizes. While maintaining a batch size of 1, we aim to maximize the context length. All experiments run on a small cluster with 64 A100 GPUs. The results, as presented in Table 14, demonstrate the remarkable long context length training capability of TransNormerLLM. Under comparable computational resources, the TransNormerLLM model exhibits the ability to train with longer context lengths compared to conventional Transformer models and achieve higher computational speeds in the process. \u25c4 Feeling lucky?",
    "transnormerllm-29": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Feb 28 16:31:51 2024 by LaTeXML"
}