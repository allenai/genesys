{
    "hyenadistill-0": "# Laughing HYenA Distillery: <br> Extracting Compact Recurrences From Convolutions \n\nStefano Massaroli ${ }^{*, 1}$, Michael Poli ${ }^{*, 2}$, Daniel Y.",
    "hyenadistill-1": "Fu ${ }^{*, 2}$,<br>Hermann Kumbong ${ }^{2}$, Rom N. Parnichkun ${ }^{3}$, Aman Timalsina ${ }^{4}$,<br>David W. Romero ${ }^{5}$, Quinn McIntyre ${ }^{2}$, Beidi Chen ${ }^{6}$, Atri Rudra ${ }^{7}$, Ce Zhang ${ }^{8}$,<br>Christopher $\\operatorname{Re}^{2, \\dagger}$, Stefano Ermon ${ }^{2, \\dagger}$, Yoshua Bengio ${ }^{1, \\dagger}$\n\nNeurIPS 2023, Last Compiled: October 31, 2023. #### Abstract\n\nRecent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads - naively requiring a full pass (or caching of activations) over the input sequence for each generated token - similarly to attention-based models. In this paper, we seek to enable $\\mathcal{O}(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting lowdimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolutionbased layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves $10 \\times$ higher throughput than Transformers and $1.5 \\times$ higher than Hyena at 1.3 B parameters, without any loss in quality after distillation. ## 1 Introduction\n\nAttention-free approaches such as long convolution sequence models (LCSMs), e.g., H3 [1], Hyena [2], have shown promise in matching Transformer [3, 4] performance across a wide range of tasks, with sub-quadratic complexity with respect to sequence length. Despite the improved efficiency during training on long sequences, unless the convolution filters are either short or admit a low-dimensional state-state-space realization, LCSMs still need to process the entire growing sequence at every step of auto-regressive generation, similarly to Transformers. In this work, we seek to refine LCSMs in both efficiency and quality. First, we study the inference stage, and propose methods to enable a recurrent mode for auto-regressive generation. Recurrent modes prescribe the existence of a state encoding the past information of the process in a fixed-dimension memory, enabling constant per-step time and constant-memory in generation. Then, we draw upon an analysis of pre-trained models to develop architectural enhancements for the Hyena block, simultaneously improving model quality and efficiency of the distillation procedure. Distilling fast recurrences We introduce LaughingHyena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality. LaughingHyena seeks compact recurrences in the form of state-space models (SSMs) [5, 6] as the solution of a nonlinear interpolation problem involving the convolution filters of a pre-trained model. Since the total memory cost of SSMs grows linearly in the state dimension $d$, our distillation procedure enables high throughput by enabling processing of large batches during generation. [^0]We identify and address three core challenges related to distillation, including the identification of:\n\n- Target state dimension: we identify candidate state dimensions of our distilled SSMs by analyzing the spectrum of the Hankel operator associated with each convolution [7]. - Parametrization: we address issues with naive parametrizations by introducing a factorized modal form, inspired by barycentric [8] and Prony-like [9] methods . - Approximation metric: to ensure compatibility with any downstream task, we choose discrepancy metrics on the convolution filter, rather than model outputs. In auto-regressive workloads, LaughingHyena-distilled models with state dimension $d$ can generate $K$ tokens in $\\mathcal{O}(d K)$ time and with constant $\\mathcal{O}(d)$ memory - improving over the $\\mathcal{O}\\left(K^{2}\\right)$ time and $\\mathcal{O}(K)$ memory usage of $k v$-cached Transformers and naively executed long convolutions. At model sizes above one billion parameters, LaughingHyena achieves $10 \\times$ higher peak throughput over comparable Transformers (Figure 1.1), and can process larger batch sizes. Constant memory generation enables larger $K$ for a given a memory constraint e.g., generating 512 tokens with LaughingHyena requires $3 \\times$ less memory than with a Transformer. At smaller batch sizes, latency of LaughingHyena is also competitive with Transformers, reaching $\\geq 2 \\times$ speedups at longer prompt lengths. Improving pre-training quality We leverage our analysis of the distillation process to open up new avenues of improvement for LCSM architectures. Indeed, the high compression rates achievable through LaughingHyena hint at sub-utilization of the convolution. We revisit the multi-headed design of H3 [1]; tying weights across channels pushes long convolution filters towards larger effective dimension, and as an additional advantage reduces the runtime of post-training distillation and inference memory footprint. Further, multi-head Hyena models improve on pre-training perplexity over regular Hyena and GPT [10] architectures on the language dataset The Pile [11]. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-02.jpg?height=464&width=766&top_left_y=979&top_left_x=1045)\n\nFigure 1.1: Throughput (in generated tokens) of Transformers, H3 and Hyena models. LaughingHyena is a recurrent model distilled from a pre-trained Hyena. Workload involves generating 256 tokens given a prompt of length 512 . ## 2 Preliminaries and Related Work\n\nWe discuss convolutions, state spaces and auto-regressive generation workloads for sequence models. Convolutions Let $*$ denote the convolution operator. It is defined as the dual operation to point-wise multiplication under Fourier transform. In signal processing and deep learning alike, one often encounters the causal linear convolution of a filter $h$ (which may extend indefinitely) with an input $u$ of length $L$ :\n\n$$\n(h * u)_{t}=\\sum_{j=0}^{t} h_{t-j} u_{j}\n$$\n\nGenerally, $u_{t} \\in \\mathbb{R}^{D}$ where $D$ is the width of the signal - or in deep learning parlance - the number of channels. Without loss of generality, we specialize our analysis to single input single output layers, i.e. with $D=1$. For the input-output relations of type (2.1), we use the terms convolution layer and linear system interchangeably. Similarly, the function $t \\mapsto h_{t}$ is referred to as both the filter and the impulse response of a linear system. Existing convolution sequence models can be classified in terms of the parametrization used for their filters. The class of implicit convolutions represent the filter as a parametric function $\\gamma_{\\theta}: t \\mapsto h_{t}$. State-space realization One option is to select $\\gamma_{\\theta}$ as the impulse response function of a discrete linear time-invariant system,\n\n$$\n\\begin{aligned}\nx_{t+1} & =\\mathrm{A} x_{t}+\\mathrm{B} u_{t} \\\\\ny_{t} & =\\mathrm{C} x_{t}+h_{0} u_{t}\n\\end{aligned}, \\quad t \\mapsto h_{t}= \\begin{cases}h_{0} & t=0 \\\\\n\\mathrm{CA}^{t-1} \\mathrm{~B} & t>0\\end{cases}\n$$\n\nwith state $x_{t} \\in \\mathbb{R}^{d}$, input $u_{t} \\in \\mathbb{R}$, and output $y_{t} \\in \\mathbb{R}$. The matrices $\\mathrm{A} \\in \\mathbb{R}^{d \\times d}, \\mathrm{~B} \\in \\mathbb{R}^{d \\times 1}, \\mathrm{C} \\in \\mathbb{R}^{1 \\times d}$, and $h_{0} \\in \\mathbb{R}$ are the learnable parameters of the model while the initial state $x_{0}$ is usually set to zero such\nthat $u \\mapsto y$ is a pure convolution. While linear systems (2.2) are the staple of signal processes and control theory, their use as implicit parametrization of convolution filters in deep neural networks have only recently emerged [12, 6]. Other parametrizations $[13,14,2]$ select $\\gamma_{\\theta}(t)$ as different flavors of implicit representation neural networks $[15,16]$. The latter are generally more powerful in terms of the class of filters they can represent and flexibility during training, at the cost of losing a fixed state dimension. ### 2.1 Long Convolution Sequence Models\n\nThe H-family of convolution sequence models - H3 [1] and Hyena [2] - relies on a combination of long convolutions and data-controlled gating to replace attention with sub-quadratic scaling in sequence length ${ }^{1}$. We use the deep learning convention of naming different projections as query $q$, key $k$ and value $v$. Let $\\mathrm{M}_{q}$ and $\\mathrm{M}_{k}$ be the\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-03.jpg?height=210&width=512&top_left_y=449&top_left_x=1297)\n\nFigure 2.1: H-block. $\\mathbf{T}^{(q)}, \\mathbf{T}^{(k)}, \\mathbf{T}^{(v)}$ are short-convolution operators. $L$-by- $L$ diagonal matrices whose respective main diagonal entries are the respective entries of length- $L$ sequences $q$ and $k$. A H -block realizes a surrogate attention matrix with a data-controlled, parameterized decomposition in three terms:\n\n$$\n(q, k, v) \\mapsto \\mathrm{H}(q, k) v, \\quad \\mathrm{H}(q, k)=\\mathrm{M}_{q} \\mathrm{~T}_{h} \\mathrm{M}_{k}\n$$\n\nwhere $\\mathrm{T}_{h} \\in \\mathbb{R}^{L \\times L}$ is the Toeplitz matrix constructed from the learnable long convolution filter $h$, i.e., $\\mathrm{T}_{h}=\\left(h_{i-j}\\right)_{i, j=0}^{L-1}$. The $q k v$-projections are themselves the output of a convolution between the input sequence and three distinct short filters. The degrees of freedom in H-block design are the three short filters ${ }^{2}$ and the long filter $h$. The long filter can be parameterized using an implicit neural representation [2], state-space model [1], or explicit values [17]. The threefold decomposition of the attention operator, allows evaluation of (2.3) in just $\\tilde{\\mathcal{O}}(L):=\\mathcal{O}\\left(L \\log _{2} L\\right)$ time (two convolutions ${ }^{3}$ and two element-wise products), $y_{t}=q_{t}(h * k v)_{t}$. The overall operator acts on an input $u$ by constructing a third-order multi-variate polynomial of $u$ whose coefficients are controlled (nonlinearly) by parameters of the block. ### 2.2 Auto-Regressive Generation\n\nA typical workload for sequence models is auto-regressive generation. Given a length- $T$ prompt $u \\in \\mathbb{R}^{T}$, the model is tasked with producing the following $K$ additional outputs - one at a time - for a resulting output sequence $y$ of length $L=T+K$. Convolution sequence models After processing the initial prompt in $\\tilde{\\mathcal{O}}(T)$ time and obtaining a length$T$ output $u \\mapsto y_{0}, \\ldots, y_{T-1}$, a generic convolution layer can cache the output sequence and generate any additional outputs using (2.1) auto-regressively, i.e. $y_{t+1}=\\sum_{j=0}^{t} h_{t-j} y_{j}$ for $t=T-1, \\ldots, T+K-1$. It is important to note that auto-regressive generation with generic long convolutions is expensive. It comes with a quadratic cost in the number $K$ of tokens to be generated and require storing a cache of length up to $L$. Lemma 2.1. Generating $K$ tokens with a long convolution layer (2.1) from a length-T prompt has time complexity $\\mathcal{O}\\left(T \\log _{2} T+T K+K^{2}\\right)$ and requires $\\mathcal{O}(L)$ memory. State-space models When the linear system admits a state space realization (2.2), i.e. it is able to switch between convolution and recurrent mode, the cost of auto-regressive generation can be dramatically reduced. The memory footprint is $\\mathcal{O}(d)$ : all we need to cache is the state $x_{t}$, a $d$-dimensional vector. With some further machinery that we develop in next section, we can retain $\\tilde{\\mathcal{O}}(T)$ time and $\\mathcal{O}(T)$ memory to process the prompt ${ }^{4}$ and initialize the state $x_{T-1}$. Each additional generation step only requires $\\mathcal{O}(d)$ time. Lemma 2.2. Generating $K$ tokens with a state-space model (2.2) from a length-T prompt has time complexity $\\mathcal{O}\\left(T \\log _{2} T+d K\\right)$ and requires $\\mathcal{O}(T+d)$ memory. Note that long filters $h$ truncated to length $d$ (i.e. $h_{t}=0$ for $t>d-1$ ) can also be interpreted as $d$-dimensional SSMs (see Appendix A.7) where the state (a cache) coincides with the last $d$ inputs. [^1]Transformers Self-attention is certainly less efficient than long convolutions in processing the prompt, coming with a hefty $\\mathcal{O}\\left(T^{2}\\right)$ time complexity. However, Transformers can achieve a similar efficiency in auto-regressive generation by caching the sequences of past keys $\\left\\{k_{t}\\right\\}$ and values $\\left\\{v_{t}\\right\\}$. Specifically, from $t=T-1$ onward, the new projections $\\left(q_{t+1}, k_{t+1}, v_{t+1}\\right)$ are evaluated from the current output $y_{t}$, and the new output $y_{t+1}$ can be computed in linear time with two reductions\n\n$$\ny_{t+1}=\\frac{\\sum_{j=0}^{t+1} \\varphi\\left(q_{t+1} k_{j}\\right) v_{j}}{\\sum_{i=0}^{t+1} \\varphi\\left(q_{t+1} k_{j}\\right)} \\quad \\text { where } \\varphi: \\mathbb{R} \\rightarrow \\mathbb{R} \\text { is usually chosen as } \\varphi(x)=e^{x}\n$$\n\nLemma 2.3. Generating $K$ tokens with self-attention from a length-T prompt has time complexity $\\mathcal{O}\\left(T^{2}+T K+K^{2}\\right)$ and requires $\\mathcal{O}(L)$ memory. ## 3 The Laughing Hyena Distillery\n\nIn this section, we introduce our distillation method. We discuss choosing an approximation objective, a parametrization for the approximant and setting a target state dimension. Given any pre-trained LCSM, the objective of the distillation procedure is to convert each pre-trained convolution filter into a distinct state-space model (2.2). This should be achieved with the smallest state dimension $d$ which preserves, up to a certain tolerance, the input-output characteristics of the convolution layer. Formally, given a filter $h$ the distillation problem is defined as follows. Given the sequence $h_{1}, \\ldots, h_{L}$, find a state-space model (2.2) of dimension $d \\ll L$, whose input-output behavior approximates the one of the convolution with $h$ over the largest class of input sequences. The choice of approximation metrics and assumptions on the input sequences yield different distillation objectives. A distillation algorithm constitutes a systematic procedure for optimally choosing the systems matrices with respect to a particular objective. In instances where the original filter $h$ is itself the impulse response of a finite-dimensional state-space model, e.g., when attempting distillation of H 3 or S 4 [6] filters, the term distillation becomes analogous to model-order reduction. Hence, in such cases, the distillation algorithm should yield a state-space representation of a lower order state-dimension. There exist several algebraic solutions to the model reduction problem [18, 19, 20], typically seeking low-rank structures of the state space by inspecting some invariant of the system, e.g. the Gramians in balanced truncation [19, Ch. 7]. The lower-order system is then obtained as a projection of the system dynamics onto the found subspace where the system retains desired characteristics, e.g., input-output behavior, stability, etc. Truncated filters In theory, implicitly parameterized convolution filters can represent arbitrarily long signals. In practice, these filters are trained on a fixed maximum length $L$. At inference time the model can then be evaluated for sequences longer than $L$. During distillation it is nonetheless reasonable to treat the pre-trained filters as potentially very long (even beyond $L$ ) but finite impulse response functions [21, 22, 23, 24]. We show how this choice is supported by empirical evidence displaying how pre-trained filters typically decay to zero in finite time (see Appendix D). Transfer function representation An alternative description of the system (2.2) is its transfer function $H$, defined as the $z$-transform of the impulse response $H(z)=\\sum_{t=0}^{\\infty} h_{t} z^{-t}$ for all $z \\in \\mathbb{C}$ where the sum converges. The transfer function is a proper rational function of $z$\n\n$$\nH(z)=h_{0}+\\mathrm{C}(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B}=h_{0}+\\frac{b_{1} z^{-1}+\\cdots+b_{d} z^{-d}}{1+a_{1} z^{-1}+\\cdots+a_{d} z^{-d}}\n$$\n\nIn the $z$-domain, the transfer function defines the input-output map as $Y(z)=H(z) U(z)$. Here, $H(z)$ is defined outside the $\\mathbb{C}$-plane circle of radius $\\rho(\\mathrm{A}), \\mathbb{D}_{\\rho(\\mathrm{A})}:=\\{z \\in \\mathbb{C}:|z|>\\rho(\\mathrm{A})\\}$ where $\\rho(\\mathrm{A})$ is the spectral radius of $A$, i.e. the amplitude of its largest eigenvalue. We can recover all characteristics of a given system equivalently from either its transfer function or state-space representations (see Appendix A. 3 for further details and derivations). Notably, the transfer function is an invariant of the system: if we apply a change of variables to the state, the transfer function remains unchanged (Lemma A.3). This alone should discourage attempts at modeling filters by learning dense statespace matrices $A, B, C$ as such: there are infinitely many equivalent state-space realizations that map\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-05.jpg?height=361&width=1351&top_left_y=228&top_left_x=341)\n\nFigure 3.1: The LaughingHyena long convolution sequence model distillation blueprint.",
    "hyenadistill-2": "to the same system. Starting from coefficients $\\left(a_{i}\\right)$ and $\\left(b_{i}\\right)$ of the rational transfer function (3.1), we can compute the impulse response in $\\tilde{\\mathcal{O}}(L)$ time (Lemma A.6). Moreover, we can map back the transfer function to a special state-space realization - the companion canonical form - whose recurrence has time complexity $\\mathcal{O}(d)$ (Lemma A.7), compared to the $\\mathcal{O}\\left(d^{2}\\right)$ of dense state-space matrices. From Lemmas A. 3 and A. 7 we can also prove that any stable state-space model can be converted by canonicalization into its companion form, and thus can be equipped with an efficient recurrence (Thm.",
    "hyenadistill-3": "A.8). The distillation problem presents several challenges:\n\n1. Defining the distillation objective. A primary decision involves selecting a distillation objective. We are primarily interested in metrics of pure discrepancy between each filter of a pre-trained deep model and its approximator, rather than the expected input-output loss over a distribution of inputs.",
    "hyenadistill-4": "2. Choosing a state-space parametrization. It is crucial to determine a suitable parametrization of the distilled state-space realization. Once this is decided, the task is to identify the parameters that minimize the distillation desiderata, which can involve challenging optimization problems in itself. 3. Selecting the target state dimension. Lastly, a challenge is to estimate the degree to which the model's order can be reduced. In other words, we must select the target state dimension of the distillation process to identify the right trade-off between efficiency and accuracy. In the following, we address each of these challenges, and provide a comprehensive approach (summarized in Figure 3.1) to distill recurrences from convolution-based architectures. ### 3.1 Data-Free Distillation Objectives\n\nWe focus on distillation objectives that are independent of the training data and the overall architecture of the neural network under consideration. The distillation loss should be chosen as a pure measure of discrepancy between each convolution filter $h_{t}$ of the model and their finite-dimensional approximations $\\hat{h}_{t}=\\mathrm{CA}^{t-1} \\mathrm{~B}$. This approach ensures that we do not require a full sequential inference pass over the pretrained model at each step of distillation procedure and the distilled model can be more broadly applied to downstream tasks. This choice is supported by Young's convolution inequality [25, 26], which indicates that the output approximation error has a bound $\\|y-\\hat{y}\\|_{r} \\leq\\|h-\\hat{h}\\|_{q}\\|u\\|_{p}$ for properly chosen norms ${ }^{5}$. For maximum numerical stability and freedom of parametrization for the approximants, we favor modern unconstrained gradient-based approaches to then solve the resulting distillation program ${ }^{6}$. We design distillation algorithms which either match filters in time domain minimizing the $\\ell_{2}$ error $\\left(\\|h\\|_{2}:=\\left[\\sum_{t \\in \\mathbb{Z}}\\left|h_{t}\\right|^{2}\\right]^{1 / 2}\\right)$ or match their transfer functions optimally with respect to the $\\mathcal{H}_{2}$ norm $\\left(\\|H\\|_{2}:=\\left[(1 / 2 \\pi) \\int_{-\\pi}^{\\pi}\\left|H\\left(e^{i \\omega}\\right)\\right|^{2} \\mathrm{~d} \\omega^{1 / 2}\\right)^{7}\\right.$. As the distillation is carried out via gradient methods, $\\ell_{2}$ is a natural candidate. $\\mathcal{H}_{2}$ error minimization can instead be used to uniformy bound the worst-case discrepancy as $\\|h-\\hat{h}\\|_{\\infty} \\leq\\|H-\\hat{H}\\|_{2}$ (see Appendix A. 2 for further details). ### 3.2 Making Hyena Laugh with Modal Interpolation\n\nOur degrees of freedoms to solve the distillation problem are the matrices A, B, and C of the state-space realization, which determine the filter for all $t>0$. In distilled SSMs, the passthrough (residual) term\n\n[^2]cannot be freely assigned: it is simply $h_{0}$, the value of the original filter at zero. Alternatively, given its appealing invariance properties, we can parametrize a proper rational function $\\hat{H}(z)(3.1)$ and fit it to the (truncated) transfer function ${ }^{8}$ of the original filter $H_{L}(z):=\\sum_{t=0}^{L} h_{t} z^{-t}$ (see Appendix B.2). Modal canonical form Optimizing the full transfer function can be numerically challenging for several reasons e.g., ensuring stability ${ }^{9}$, and ill-posedness for high-order polynomials. A natural solution, inspired by barycentric approaches to rational function approximation [29, 8], is to assume $d$ distinct roots $\\lambda_{n}$ in the denominator's polynomial, $\\lambda_{n} \\in \\operatorname{roots}(\\operatorname{poly}(a))$. Proposition 3.1 ([5]). If poly $\\left(\\right.$ a) has distinct roots $\\left\\{\\lambda_{n} \\in \\mathbb{C}\\right\\}$, then the transfer function of the system can be factorized as $\\hat{H}(z)=\\sum_{n=1}^{d} R_{n} /\\left(z-\\lambda_{n}\\right), \\quad \\forall z \\in \\mathbb{D}_{\\rho(\\mathrm{A})}$ where $\\left\\{R_{n} \\in \\mathbb{C}\\right\\}$ is the residue associated with the pole $\\lambda_{n}$. Computing the inverse transform of the expanded transfer function via, e.g., the Cauchy residue theorem [30], shows that the resulting impulse response $\\hat{h}$ corresponds to a truncated basis of exponentially decaying complex sinusoids\n\n$$\n\\hat{h}_{t}=\\sum_{n=1}^{d} R_{n} \\lambda_{n}^{t-1}, \\quad R_{n}, \\lambda_{n} \\in \\mathbb{C}, t>0\n$$\n\nIn practice, this corresponds to the impulse response of state-space model with diagonal matrix $\\mathrm{A}=$ $\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{d}\\right)$ and such that $\\mathrm{B}_{i} \\mathrm{C}_{i}=R_{i}$ for all $i=1, \\ldots, d$. The distillation problem can be then defined in terms of the $L$-point nonlinear least squares interpolation error (squared $\\ell_{2}$ ) between $h_{1}, \\ldots, h_{L}$ and (3.2) evaluated for $t=1, \\ldots, L: \\min _{\\left\\{\\lambda_{n}, R_{n}\\right\\}}\\|\\hat{h}-h\\|_{2}^{2}$. Note that in case of the target filter $h$ being real-valued, the objective can be replaced by $\\|\\mathfrak{R}[\\hat{h}]-h\\|_{2}^{2}$. Modal Interpolation\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-06.jpg?height=366&width=740&top_left_y=1276&top_left_x=224)\n\nFigure 3.2: Example of modal interpolation. The approximant is a linear combination of exponentially-decaying complex exponential basis functions with learned decay rate. Although we find solutions of the distillation (interpolation) problem via modern gradient-based optimization techniques, it is worth mentioning that Prony showed how the nonlinear least square solution can be computed solving two linear problems [9]. However, similar to Pad\u00e9's method for rational approximation [31], these techniques can be numerically unstable. We opt for a parametrization similar to [32, 33] where each eigenvalue is parameterized in polar form $\\lambda_{n}:=A_{n} e^{i \\theta_{n}}$ and the residues in cartesian form ${ }^{10}$. Note that, with this parametrization we have $\\mathfrak{R}\\left[\\hat{h}_{t}\\right]=$ $\\sum_{n} A_{n}^{t-1}\\left[\\Re\\left(R_{n}\\right) \\cos \\left(\\theta_{n}(t-1)\\right)-\\Im\\left(R_{n}\\right) \\sin \\left(\\theta_{n}(t-1)\\right)\\right]$. We can also solve the distillation problem in the $\\mathcal{H}_{2}$ sense by evaluating $\\hat{h}_{t}$ and $h_{t}$ at $t=0, \\ldots, L-1$ and taking their respective (discrete) Fourier transform before computing the objective. Efficient evaluation of $(3.2)$ is crucial for distillation. In particular we show the following:\nLemma 3.1. Evaluation of $\\left(\\hat{h}_{t}\\right)_{t=0}^{L-1}$ (3.2) can be done in $\\mathcal{O}(d L)$ time from its modal form and in $\\tilde{\\mathcal{O}}(L)$ time from its proper rational form. ### 3.3 Minimal Distillation Orders\n\nDistilling into lower-dimensional systems is always desirable as they require fewer parameters to be optimized and they yield recurrences that are (linearly) more efficient in terms of time and memory complexity in postdistillation auto-regressive inference workloads. The dimension of the smallest possible state-space model with impulse response exactly $\\left\\{h_{t}\\right\\}_{t \\in \\mathbb{N}}$ is the so-called McMillan degree [34]:\n\n$$\nd^{\\star}=\\arg \\min _{d} d: \\exists \\mathrm{A} \\in \\mathbb{C}^{d \\times d}, \\mathrm{~B} \\in \\mathbb{C}^{d \\times 1}, \\mathrm{C} \\in \\mathbb{C}^{1 \\times d} \\text { with } h_{t}=\\mathrm{CA}^{t-1} \\mathrm{~B}, \\forall t>0\n$$\n\nTheorem 3.1 (Ho-Kalman [35, Theorem 2, Corollary]). Let S be the (infinite) Hankel matrix constructed with $h$, i.e. $S:=\\left(h_{i+j}\\right)_{i, j=1}^{\\infty}$. Then, $d^{\\star}=\\operatorname{rank}(\\mathrm{S})$. [^3]A lower bound for $d^{\\star}$ can be estimated from a truncated filter of length $L$ by constructing the $L \\times L$ principal sub-matrix $S_{L}$ and using the fact that $\\operatorname{rank}(S) \\geq \\operatorname{rank}\\left(S_{L}\\right)$. Inspecting how fast the Hankel singular values $\\left(\\sigma_{n}\\right)_{n=1}^{L}$ decay in pre-trained convolution models can be predictive of the approximation quality at a fixed dimension. As a rule of thumb, $d$ needs to be sufficiently large for $\\sigma_{d+1}$ to be sufficiently small ${ }^{11}$. Specifically, we can prove that the last singular value $\\sigma_{d}$ determines the upper bound of distillation quality with a SSM of dimension $d$, in terms of the Hankel norm [19]. This is a direct consequence of Adamjan-Arov-Krein theorem [7] and can be informally stated as follows. Theorem 3.2 (Informal). Let $h$ be a length-L filter, $\\hat{h}$ a distilled filter of order $d<L$ and let $\\mathrm{S}_{L}, \\hat{\\mathrm{S}}_{L}$ be the respective Hankel matrices. Then $\\inf _{\\hat{\\mathrm{S}}_{L}}\\left\\|\\mathrm{~S}_{L}-\\hat{\\mathrm{S}}_{L}\\right\\|_{2}=\\sigma_{d}$. ### 3.4 Deploying the Recurrence\n\nOnce all the filters of a pre-trained model have been distilled with the proposed modal interpolation technique described above, the model unlocks a recurrent mode which allocates a state $x_{t} \\in \\mathbb{C}^{d}$ for each filter and enables fast auto-regressive inference. Deployment of distilled model involves two critical steps: the prefilling and the recurrent update rule itself. Fast pre-filling During auto-regressive generation, when a length- $T$ prompt is fed to the model, we need to compute the state $x_{T}$ to start generating new tokens. Using the recurrence, the time complexity of initializing $x_{T}$ would be $\\mathcal{O}(d T)$ with a $\\mathcal{O}(d)$ memory footprint. One can alternatively distribute the computation on $d$ processors with a parallel scan operation [37,38] to reach a parallel time complexity $\\mathcal{O}\\left(d \\log _{2} T\\right)$ while incurring in an increased memory requirement of $\\mathcal{O}(d T)^{12}$. A third option is to use a single FFT convolution to obtain $x_{T}$ in $\\tilde{\\mathcal{O}}(T)$ time and $\\mathcal{O}(T)$ memory. Proposition 3.2. $x_{T}=\\left(\\nu_{T}, \\ldots, \\nu_{T-d}\\right)$ where $\\nu_{t}=(g * u)_{t}$ and $g$ is the filter whose transfer function is $1 / \\operatorname{den}(\\hat{H})(z)$ and can be evaluated in $\\tilde{\\mathcal{O}}(T)$. Note that, the fast pre-filling algorithm established by this result requires evaluating the denominator polynomial of $\\hat{H}$ from its roots before deployment. This is equivalent to converting the transfer function from its factorized representation to its rational form (3.1). Recurrent step The update rule is diagonal, thus efficiently evaluated in $\\mathcal{O}(d)$ time and memory:\nProposition 3.3. The filter (3.2) has a state space matrices $\\mathrm{A}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{d}\\right) \\in \\mathbb{C}^{d \\times d}, \\mathrm{~B}=$ $(1, \\ldots, 1)^{\\top} \\in \\mathbb{C}^{d \\times 1}, \\mathbb{C}=\\left(R_{1}, \\ldots, R_{d}\\right) \\in \\mathbb{C}^{1 \\times d}$ whose step can be evaluated in $\\mathcal{O}(d)$ time and memory. As we generally want the output $y_{t}$ to be real-valued, we can simply update the complex state $x_{t+1}=$ $\\mathrm{A} x_{t}+\\mathrm{B} u_{t}$ and then take the real part of the output, $y_{t}=\\mathfrak{R}\\left[\\mathrm{C} x_{t}\\right]+h_{0} u_{t}$. ## 4 Multi-head Long Convolutions\n\nWe can leverage the Hankel spectrum analysis discussed in Section 3.3 to study the dynamics of the effective dimensionality of each convolution filter during LCSMs pre-training. We find that, at initialization, filters correspond to high-dimensional SSMs, and gradually converge to lower-dimensional representations during training. See Appendix E. 2 for examples on Hyena and H3 models. This observation leads to the question: is it advantageous to perform independent long convolutions on each channel, or can we reduce the total number of filters without loss in quality? To answer this, we adapt the multi-head layer design proposed by H3 [1] to Hyena [2]:\n\n1. Given the projections $q, k, v \\in \\mathbb{R}^{L \\times D}$, we split them into $M$ chunks of size $N=D / M, q^{m}, k^{m}, v^{m} \\in \\mathbb{R}^{L \\times N}$. 2. Each chunk is processed by a modified Hyena operator: first, we perform the outer product of $k^{m}$ and $v^{m}$ along the spatial dimension, $z^{m}:=k^{m} \\otimes v^{m} \\in \\mathbb{R}^{L \\times N \\times N}$, apply a long convolution with filter $h^{m}$ to all $N \\times N$ elements independently, then compute $y_{t}^{m}=\\left(h^{m} * z^{m}\\right)_{t} q_{t}^{m}, y^{m} \\in \\mathbb{R}^{L \\times N}$ as shown in Figure 4. 3. Finally, we compose $y^{1}, \\ldots, y^{m}$ into a single output $y \\in \\mathbb{R}^{L \\times D}$ via concatenation. [^4]An instance of a MultiHyena is equipped with $M<D$ distinct long convolution filters, which leads to $(a)$ faster distillation, with less filters to approximate, $(b)$ lower memory footprint, via a total reduction of the states to cache during generation and $(c)$ faster filter generation, by tying the weights of filter parameters. We note that tying weights of key-value projections has also been shown to be an effective technique to reduce memory cost in Transformers [39, 40]. Crucially, the multi-head structure of MultiHyena enables us to prove favorable scaling in the associative recall synthetic task, which was shown in [2] to be predictive of performance at scale. In associative recall, the model is given a sequence of key-value pairs and a query, and is tasked with matching the query to a key in the sequence by returning its associated value. The difficulty of the task grows with the vocabulary size $s$ : larger vocabularies necessitate wider models. Theorem 4.1. The MultiHyena layer, with $\\mathcal{O}(\\log s)$ heads and model size $\\mathcal{O}(\\sqrt{s} \\log s)$ can solve the associative recall problem, where $s$ denotes the vocabulary size. In Appendix E.1, we empirically verify improved scaling in vocabulary size with multiple heads. ## 5 Experiments\n\n- Pretraining: We pretrain a suite of MultiHyena language models on The Pile [11], investigating scaling of perplexity with different amounts of total tokens ( $5,10,15$ billion), as well as larger training runs for 300 billion tokens. MultiHyena outperforms Transformers and Hyena. - Distillation analysis: We investigate the relation between optimal distillation orders, Hankel spectrum, and errors on the logits of distilled models. - Post-distillation downstreams: We evaluate the downstream impact of distilling long convolutional language models, reporting HELM [41] and LM-Eval-Harness [42] results. - Benchmarking: We benchmark latency, throughput and memory along the different axes of batch size, sequence length, number of generated tokens. We include base models, distilled models and equivalent Transformers. ### 5.1 Pre-training\n\nTo validate the multi-head formulation, we train 150 and 350 million parameter MultiHyena models on The Pile [11] using 8 heads and otherwise the same architecture as equivalent Hyena models, following the setup of [2]. Via the multi-head structure introduced in 4, MultiHyena outperforms both Hyena and Transformers, including on data scaling runs with increasing numbers of tokens and full 300B tokens runs (Table 5.1). ### 5.2 Distillation Analysis\n\nNext, we verify whether Hankel singular values are predictive of downstream errors, and whether large models can be distilled without loss in quality. We apply LaughingHyena distillation to pre-trained MultiHyena, Hyena and H 3 of different sizes. Concretely, for each layer and channel of a model, we parametrize the poles $\\left\\{\\lambda_{n}\\right\\}$ of the modal canonical forms (Section 3.2) at different orders $d$, and solve for each $\\ell_{2}$ approximation problem. Approximation errors and spectrum We investigate the magnitude of approximation errors introduced by LaughingHyena distillation. Given a pretrained MultiHyena model, we compute the errors between original and distilled filters at each layer, averaged across channels. We repeat this process for different distillation orders (state dimension of the model form of Section 3.2). Figure 5.2 visualizes minimum, maximum and average errors, per-layer errors and the distribution of the singular values of the Hankel operator\n\n| Model | PPL. |\n| :---: | :---: |\n| GPT | 9.3 |\n| Hyena | 9.3 |\n| MultiHyena | $\\mathbf{8 . 7}$ |\n\n\n| Model | $5 B$ | $10 B$ | $15 B$ |\n| :---: | :---: | :---: | :---: |\n| GPT $(125 \\mathrm{M})$ | 13.3 | 11.9 | 11.2 |\n| Hyena $(153 \\mathrm{M})$ | 13.3 | 11.8 | 11.1 |\n| MultiHyena $(153 \\mathrm{M})$ | $\\mathbf{1 2 . 1}$ | $\\mathbf{1 1 . 0}$ | $\\mathbf{1 0 . 6}$ |\n\n\n| Model | 5 B | 10 B | 15 B |\n| :---: | :---: | :---: | :---: |\n| GPT $(355 \\mathrm{M})$ | 11.4 | 9.8 | 9.1 |\n| Hyena $(355 \\mathrm{M})$ | 11.3 | 9.8 | 9.2 |\n| MultiHyena $(355 \\mathrm{M})$ | $\\mathbf{1 0 .",
    "hyenadistill-5": "6}$ | $\\mathbf{9 . 4}$ | $\\mathbf{8 . 9}$ |\n\nTable 5.1: [Left] Perplexity of small models on The Pile, after pre-training for 300 billion tokens. [Center and Right] Perplexity on The Pile for models trained until a total number of tokens e.g., 5 billion (different runs for each token total). ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-09.jpg?height=647&width=985&top_left_y=176&top_left_x=537)\n\nFigure 5.1: Errors between logits of pretrained and distilled MultiHyena. In blue, we plot (ordered) logits, in light blue the cumulative distribution function, and in black the relative errors. The green dotted line indicates the $99.99 \\%$ percentile. As the errors grow slowly as function of the percentiles, model outputs do not diverge from the base model. associated to each filter. We observe distillation orders $(>16)$ that yield smalls errors to be predicted by the distribution of singular values. Thus, analysis of the Hankel operator's spectrum is verified to be an effective approach to direct estimation of the optimal distillation order. We also note that the optimal order changes across layers, offering options for further optimization. Output errors Next, we compute relative $\\ell_{1}$ error between output logits of pre-trained and distilled models to ensure LaughingHyena can be used in generation workloads. The optimal minimal distillation order estimated via Hankel operators (16) is sufficient to keep the output distribution over the vocabulary ( $>50 \\mathrm{k}$ entries) close to the pre-trained model, as shown in Figure 5.2. Inspecting the error profile over logits sorted by magnitude reveals our approach to be robust to different sampling strategies for generation, including greedy decoding, top $-k$, top- $p$ [43]. Indeed, the relative errors are $<10^{-2}$ up to and including the $99.99 \\%$ percentile of the distribution, meaning e.g., a top- $p$ sampling strategy with large $p$ can be used on a distilled model without drift in outputs (mis-classified tokens). We note that the relative errors are maximum on small-norm logits, which are not required by most sampling strategies. In Appendix D.2, we provide a similar distillation error analysis for Hyena and H 3 models. We find that Hyena and can be distilled with less than 32 orders and H 3 with less than 8. ### 5.3 Downstream Evaluation\n\nWe check how distillation affects downstream performance on language benchmarks. We apply distillation of order 8, 16 and 32 to our The Pile-pretrained MultiHyena language model and benchmark (Table 5.3) its performance on a suite of canonical (zero shot) tasks from LM-Eval-Harness [42] and HELM [41]. The results are consistent with our error analysis: distillation orders equal or greater to 16 introduce little-to-no quality degradation. | Model | LAMBADA <br> acc | Winogrande <br> acc | PIQA <br> acc | HellaSwag <br> acc norm. | OpenbookQA <br> acc norm. |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Pythia (160M) | 32.8 | $\\mathbf{5 3 . 1}$ | 61.6 | 31.6 | $\\mathbf{2 9 . 2}$ |\n| MultiHyena (154M) | $\\mathbf{4 3 . 2}$ | 52.7 | $\\mathbf{6 4 . 6}$ | $\\mathbf{3 4 . 1}$ | 29.0 |\n| LaughingHyena-16 | 43.1 | 52.6 | 64.7 | 34.1 | 28.9 |\n| LaughingHyena-8 | 0.0 | 51.8 | 51.5 | 32.7 | 28.2 |\n| LaughingHyena-4 | 0.0 | 49.6 | 53.7 | 26.4 | 26.4 |\n\nTable 5.2: Evaluation of LaughingHyena-distilled models pre and post modal distillation. We test on LM-Eval-Harness tasks, reporting Pythia [44] performance as a Transformer baseline trained on the same data. LaughingHyena-d is a MultiHyena model with each filter distilled of order $d$. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-10.jpg?height=543&width=1477&top_left_y=220&top_left_x=288)\n\nFigure 5.2: Approximation error profiles (min, max, average) on the filters of MultiHyena model after distillation at different orders. We also visualize the distribution of Hankel singular values: if the spectrum decays after $n$ singular values, order $n$ distillation yields low errors. ### 5.4 Benchmarking\n\nWe measure throughput, latency and memory usage of LaughingHyena for auto-regressive generation workloads, with initial prompt length $T$ and number of generated tokens $K$. The throughput is computed as number of generated tokens over latency. For each setting (and additional benchmarks), we provide details in Appendix D.4. Peak throughput Distilled models do not need $k v$-caches. This reduces memory requirement during generation, enabling higher peak throughput in large-batch workloads. We achieve 10\u00d7 higher throughput than Transformers at size 1.3 billion parameters (Figure 1.1). Throughput is higher than Transformers even at fixed batch sizes, indicating lower latency. SSM state dimension and throughput For typical distillation orders $(<100)$, peak throughput is not greatly affected. We measure a $2 \\%$ reduction in throughput from 32 to 64 . Prompt length The throughput of LaughingHyena-distilled models is $4 \\times$ larger than Transformers at fixed batch size 64 and prompt length 1536 (Figure 5.3). As prompt length increases, the runtime gap between pre-filling via convolutions in LCSMs and pre-filling in Transformers widens (e.g., $\\tilde{\\mathcal{O}}(T)$ as detailed in Section 3.4, compared to $\\mathcal{O}\\left(T^{2}\\right)$ ). Memory footprint Recurrent models do not require $k v$-caches and use constant memory for generation of an arbitrary number of tokens (Figure 5.4). ## 6 Conclusion\n\nWe study the efficiency and quality of state-of-the-art long convolutional sequence models. First, we introduce LaughingHyena, a novel distillation method inspired by rational function approximation and modelorder reduction techniques. LaughingHyena can be applied after training to extract compact state-space models from each convolutional filter, without loss of quality. Distilled models achieve higher throughput than equivalently-sized Transformers, and can perform auto-regressive generation in constant memory by sidestepping the need to cache previous outputs. We theoretically and empirically investigate the trade-offs of different strategies for fast inference of recurrent models, and introduce architectural improvements to Hyena that improve pretraining quality. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-10.jpg?height=352&width=780&top_left_y=2314&top_left_x=227)\n\nFigure 5.3: Scaling in prompt length $T$. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-10.jpg?height=330&width=715&top_left_y=2325&top_left_x=1070)\n\nFigure 5.4: Peak GPU memory for generation. ## Acknowledgments\n\nWe would like to thank Together Computer for providing the compute used to train models in this paper. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and VMWare. This work is supported by NSF (1651565), AFOSR (FA95501910024), ARO (W911NF-21-10125), ONR, DOE (DE-SC0022222), CZ Biohub, and Sloan Fellowship.",
    "hyenadistill-6": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. AR's work is supported by NSF grant\\# CCF-2247014. ## Broader Impact\n\nIn this work, we focus on advances related to efficient models for long sequences. Efficiency Our distillation methods for constant-memory, high throughput inference in long convolution sequence models (LCSMs) can lead to energy savings during model deployement, enabling processing of longer-form content at a fraction of the cost and reducing environmental impact. Improved efficiency may also affect other aspects of AI safety, as it may make it easier produce malicious or harmful content. Accessibility By improving the efficiency of training and generation,LCSMs and LaughingHyena may contribute to increased accessibility of large language models, lowering the hardware barrier to entry for individuals and organizations with limited resources. Steerability New method based on LCSMs enable sequence models to process long-form prompts previously inaccessible by Transformers, which may lead to increased control over models via e.g., conditioning on additional instructions [45]. ## References\n\n[1] Daniel Y Fu et al. \"Hungry Hungry Hippos: Towards Language Modeling with State Space Models\". In: 2023 (cit. on pp. 1-3, 7, 23). [2] Michael Poli et al. \"Hyena Hierarchy: Towards Larger Convolutional Language Models\". In: (2023). arXiv: 2302.10866 (cit. on pp. $1,3,7,8,31,35,43$ ). [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate\". In: (2014). arXiv: 1409.0473 (cit. on p. 1). [4] Ashish Vaswani et al. \"Attention is all you need\". In: Advances in neural information processing systems 30 (2017) (cit.",
    "hyenadistill-7": "on p. 1). [5] Chi-Tsong Chen. Linear system theory and design. Saunders college publishing, 1984 (cit. on pp. 1, 6, $19,21,22)$. [6] Albert Gu, Karan Goel, and Christopher R\u00e9. \"Efficiently modeling long sequences with structured state spaces\". In: (2021). arXiv: 2111.00396 (cit. on pp. 1, 3, 4, 6, 20, 25). [7] Vadim Movsesovich Adamyan, Damir Zyamovich Arov, and Mark Grigor'evich Krein. \"Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem\". In: Matematicheskii Sbornik 128.1 (1971), pp.",
    "hyenadistill-8": "34-75 (cit. on pp. 2, 7, 29). [8] Yuji Nakatsukasa, Olivier S\u00e8te, and Lloyd N Trefethen. \"The AAA algorithm for rational approximation\". In: SIAM Journal on Scientific Computing 40.3 (2018), A1494-A1522 (cit.",
    "hyenadistill-9": "on pp. 2, 6). [9] GRB Prony. \"Essai experimental et analytique sur les lois de la dilatalrlite de fluids elastiques et sur cells de la vapeur de l'alcool, \u00e0 diff\u00e9rents tempoeratures'. In: Journal de l'Ecole Polytechnique (Par\u00eds) 1 (1795), pp. 24-76 (cit. on pp. 2, 6). [10] Alec Radford et al. \"Language models are unsupervised multitask learners\". In: OpenAI blog 1.8 (2019), p.",
    "hyenadistill-10": "9 (cit. on p. 2). [11] Leo Gao et al. \"The pile: An 800gb dataset of diverse text for language modeling\".",
    "hyenadistill-11": "In: (2020). arXiv: 2101.00027 (cit. on pp. $2,8,35$ ). [12] Albert Gu et al. \"Hippo: Recurrent memory with optimal polynomial projections\". In: Advances in Neural Information Processing Systems 33 (2020), pp.",
    "hyenadistill-12": "1474-1487 (cit. on pp. 3, 25). [13] David W Romero et al. \"Ckconv: Continuous kernel convolution for sequential data\". In: (2021). arXiv: 2102.02611 (cit. on p. 3). [14] David W Romero et al. \"Flexconv: Continuous kernel convolutions with differentiable kernel sizes\".",
    "hyenadistill-13": "In: (2021). arXiv: 2110.08059 (cit. on p. 3). [15] Vincent Sitzmann et al. \"Implicit neural representations with periodic activation functions\". In: Advances in neural information processing systems 33 (2020), pp.",
    "hyenadistill-14": "7462-7473 (cit. on p. 3). [16] Rizal Fathony et al. \"Multiplicative filter networks\". In: International Conference on Learning Representations. 2020 (cit. on p. 3). [17] Daniel Y. Fu et al. \"Simple Hardware-Efficient Long Convolutions for Sequence Modeling\". In: International Conference on Machine Learning (2023) (cit.",
    "hyenadistill-15": "on pp. 3, 25). [18] Kemin Zhou and John Comstock Doyle. Essentials of robust control.",
    "hyenadistill-16": "Vol. 104. Prentice hall Upper Saddle River, NJ, 1998 (cit.",
    "hyenadistill-17": "on p. 4). [19] Athanasios C Antoulas. Approximation of large-scale dynamical systems. SIAM, 2005 (cit. on pp. 4, $7,29)$. [20] Wilhelmus HA Schilders, Henk A Van der Vorst, and Joost Rommes. Model order reduction: theory, research aspects and applications.",
    "hyenadistill-18": "Vol. 13. Springer, 2008 (cit.",
    "hyenadistill-19": "on p. 4). [21] Sun-Yuan Kung. \"A new identification and model reduction algorithm via singular value decomposition\".",
    "hyenadistill-20": "In: Proc. 12th Asilomar Conf. on Circuits, Systems and Computer. 1978, pp. 705-714 (cit. on p. 4). [22] D Friedman. \"On approximating an FIR filter using discrete orthonormal exponentials\". In: IEEE Transactions on Acoustics, Speech, and Signal Processing 29.4 (1981), pp. 923-926 (cit. on p. 4). [23] J Bednar. \"On the approximation of FIR by IIR digital filters\". In: IEEE Transactions on Acoustics, Speech, and Signal Processing 31.1 (1983), pp.",
    "hyenadistill-21": "28-34 (cit. on p. 4). [24] Bartlomiej Beliczynski, Izzet Kale, and Gerald D Cain. \"Approximation of FIR by IIR digital filters: An algorithm based on balanced model reduction\". In: IEEE Transactions on Signal Processing 40.3 (1992), pp. 532-542 (cit. on pp. 4, 43, 44). [25] William Henry Young. \"On the multiplication of successions of Fourier constants\". In: Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character 87.596 (1912), pp. 331-339 (cit. on p. 5). [26] William Beckner. \"Inequalities in Fourier analysis on Rn\". In: Proceedings of the National Academy of Sciences 72.2 (1975), pp. 638-641 (cit. on p. 5). [27] John Fournier. \"Sharpness in Young's inequality for convolution\". In: Pacific Journal of Mathematics 72.2 (1977), pp.",
    "hyenadistill-22": "383-397 (cit. on p. 5). [28] Tong S Quek and Leonard YH Yap. \"Sharpness of Young's inequality for convolution\". In: Mathematica Scandinavica 53.2 (1983), pp. 221-237 (cit. on p. 5). [29] Jean-Paul Berrut and Lloyd N Trefethen. \"Barycentric lagrange interpolation\". In: SIAM review 46.3 (2004), pp. 501-517 (cit. on p. 6). [30] Marcos Vicente Moreira and Jo\u00e3o Carlos Basilio. \"Fair and Square Computation of Inverse Z-Transforms of Rational Functions\". In: IEEE Transactions on Education 55.2 (2011), pp.",
    "hyenadistill-23": "285-290 (cit. on p. 6). [31] Henri Pad\u00e9. \"Sur la repr\u00e9sentation approch\u00e9e d'une fonction par des fractions rationnelles\". In: Annales scientifiques de l'Ecole normale sup\u00e9rieure. Vol. 9. 1892, pp. 3-93 (cit.",
    "hyenadistill-24": "on pp. 6, 27). [32] Ankit Gupta, Albert Gu, and Jonathan Berant. \"Diagonal state spaces are as effective as structured state spaces\". In: Advances in Neural Information Processing Systems 35 (2022), pp.",
    "hyenadistill-25": "22982-22994 (cit. on pp. 6, 25). [33] Antonio Orvieto et al. \"Resurrecting Recurrent Neural Networks for Long Sequences\". In: (2023). arXiv: 2303.06349 (cit. on pp. 6, 25, 26). [34] Jeffrey M Hokanson. \"A data-driven McMillan degree lower bound\". In: SIAM Journal on Scientific Computing 42.5 (2020), A3447-A3461 (cit.",
    "hyenadistill-26": "on p. 6). [35] L Ho and Rudolf E Kalman. \"Effective construction of linear state-variable models from input/output functions\". In: at-Automatisierungstechnik 14.1-12 (1966), pp.",
    "hyenadistill-27": "545-548 (cit. on p. 6). [36] Carl Eckart and Gale Young. \"The approximation of one matrix by another of lower rank\". In: Psychometrika 1.3 (1936), pp.",
    "hyenadistill-28": "211-218 (cit. on p. 7). [37] Guy E Blelloch. \"Prefix sums and their applications\".",
    "hyenadistill-29": "In: (1990) (cit.",
    "hyenadistill-30": "on p. 7). [38] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. \"Simplified state space layers for sequence modeling\", In: (2022).",
    "hyenadistill-31": "arXiv: 2208.04933 (cit. on p. 7). [39] Noam Shazeer. \"Fast transformer decoding: One write-head is all you need\".",
    "hyenadistill-32": "In: (2019). arXiv: 1911. 02150 (cit. on p. 8). [40] Joshua Ainslie et al. \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\".",
    "hyenadistill-33": "In: (2023). arXiv: 2305.13245 (cit. on p. 8). [41] Percy Liang et al. \"Holistic evaluation of language models\". In: (2022). arXiv: 2211.09110 (cit. on pp. 8, 9, 36). [42] Leo Gao et al. A framework for few-shot language model evaluation.",
    "hyenadistill-34": "Version v0.0.1. Sept. 2021. DOI: 10.5281/zenodo.5371628. URL: https://doi.org/10.5281/zenodo.",
    "hyenadistill-35": "5371628 (cit.",
    "hyenadistill-36": "on pp. 8, 9, 36). [43] Ari Holtzman et al. \"The curious case of neural text degeneration\".",
    "hyenadistill-37": "In: (2019). arXiv: 1904. 09751 (cit. on p. 9). [44] Stella Biderman et al. \"Pythia: A suite for analyzing large language models across training and scaling\".",
    "hyenadistill-38": "In: (2023). arXiv: 2304.01373 (cit. on pp. 9, 36). [45] Yuntao Bai et al. \"Constitutional AI: Harmlessness from AI Feedback\".",
    "hyenadistill-39": "In: (2022). arXiv: 2212.08073 (cit. on p. 11). [46] Alan V Oppenheim. Discrete-time signal processing. Pearson Education India, 1999 (cit.",
    "hyenadistill-40": "on p. 21). [47] Lennart Ljung. System identification. Springer, 1998 (cit. on p. 21). [48] RP Guidorzi. \"Certain models from uncertain data: the algebraic case\". In: Systems 8 control letters 17.6 (1991), pp.",
    "hyenadistill-41": "415-424 (cit. on p. 21). [49] Irwin W Sandberg. \"On the theory of linear multi-loop feedback systems\". In: Bell System Technical Journal 42.2 (1963), pp.",
    "hyenadistill-42": "355-382 (cit. on p. 23). [50] Michael Zhang et al. \"Effectively Modeling Time Series with Simple Discrete State Spaces\". In: (2023). arXiv: 2303.09489 (cit. on p. 25). [51] Luca Perotti and Micha\u0142 Wojtylak. \"Matrix methods for Pad\u00e9 approximation: Numerical calculation of poles, zeros and residues\". In: Linear Algebra and its Applications 548 (2018), pp.",
    "hyenadistill-43": "95-122 (cit. on p. 27). [52] Amer Abu-Omar and Fuad Kittaneh. \"Estimates for the numerical radius and the spectral radius of the Frobenius companion matrix and bounds for the zeros of polynomials\". In: Annals of Functional Analysis 5.1 (2014), pp.",
    "hyenadistill-44": "56-62 (cit. on p. 27). [53] Julia Eaton et al. \"Polynomial root radius optimization with affine constraints\". In: Mathematical Programming 165 (2017), pp.",
    "hyenadistill-45": "509-528 (cit. on p. 27). [54] Gerlind Plonka and Vlada Pototskaia. \"Application of the AAK theory for sparse approximation of exponential sums\".",
    "hyenadistill-46": "In: (2016). arXiv: 1609.09603 (cit. on p. 29). [55] Jimmy Ba et al. \"Using fast weights to attend to the recent past\". In: Advances in neural information processing systems 29 (2016) (cit.",
    "hyenadistill-47": "on p. 31). [56] William B Johnson. \"Extensions of Lipschitz mappings into a Hilbert space\".",
    "hyenadistill-48": "In: Contemp. Math. 26 (1984), pp. 189-206 (cit. on p. 31). [57] Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of randomized algorithms.",
    "hyenadistill-49": "Cambridge University Press, 2009 (cit.",
    "hyenadistill-50": "on p. 34). [58] Ilya Loshchilov and Frank Hutter. \"Decoupled weight decay regularization\".",
    "hyenadistill-51": "In: (2017). arXiv: 1711. 05101 (cit. on p. 35). [59] Dale F. Enns. \"Model reduction with balanced realizations: An error bound and a frequency weighted generalization\". In: The 23rd IEEE Conference on Decision and Control (1984), pp.",
    "hyenadistill-52": "127-132 (cit. on p. 43). ## LAUGHing HYENA DistiLLERY Supplementary Material\n\n## Contents\n\n1 Introduction ..... 1\n2 Preliminaries and Related Work ..... 2\n2.1 Long Convolution Sequence Models ..... 3\n2.2 Auto-Regressive Generation ..... 3\n3 The Laughing Hyena Distillery ..... 4\n3.1 Data-Free Distillation Objectives ..... 5\n3.2 Making Hyena Laugh with Modal Interpolation ..... 5\n3.3 Minimal Distillation Orders ..... 6\n3.4 Deploying the Recurrence ..... 7\n4 Multi-head Long Convolutions ..... 7\n5 Experiments ..... 8\n5.1 Pre-training ..... 8\n5.2 Distillation Analysis ..... 8\n5.3 Downstream Evaluation ..... 9\n5.4 Benchmarking ..... 10\n6 Conclusion ..... 10\nA Linear Systems ..... 17\nA. 1 Extended Notation and System Theory Preliminaries ..... 17\nA. 2 Systems Norms ..... 17\nA. 3 Transfer Function of State-Space Models ..... 18\nA. 4 Truncated Transfer Functions ..... 19\nA. 5 From Transfer Function to State-Space ..... 21\nA.5.1 Isolating the $h_{0}$-term from Transfer Function by Long division ..... 21\nA.5.2 Construction of the State-Space from the Transfer Function ..... 22\nA. 6 From State-Space to Transfer Function ..... 23\nA. 7 State-Space Representation of Truncated Filters. ..... 23\nA. 8 Efficient Computation of State-Space Models ..... 24\nA.8.1 Fast Evaluation of the Transfer Function ..... 24\nA.8.2 Fast Companion Recurrence ..... 24\nA.8.3 Canonization of State-Space Models ..... 25\nB LaughingHyena: Further Details ..... 26\nB. 1 Parametrization of Modal Interpolators ..... 26\nB. 2 Distillation as Rational Interpolation ..... 27\nC Proofs ..... 28\nC. 1 Proof of Lemma 2.1 ..... 28\nC. 2 Proof of Lemma 2.2 ..... 28\nC. 3 Proof of Lemma 2.3 ..... 28\nC. 4 Proof of Proposition 3.1 ..... 29\nC. 5 Proof of Lemma 3.1 ..... 29\nC. 6 Proof of Theorem 3.2 ..... 29\nC. 7 Proof of Proposition 3.3 ..... 30\nC. 8 Proof of Proposition 3.2 ..... 30\nC. 9 Proof of Theorem 4.1 ..... 31\nD Experimental Details ..... 35\nD. 1 Pre-training ..... 35\nD. 2 Distillation Analysis ..... 35\nD.2.1 Pretrained Filters: Effective Dimension ..... 35\nD. 3 Downstream Evaluation ..... 36\nD. 4 Benchmarking ..... 36\nE Additional Experiments ..... 43\nE. 1 Associative Recall with MultiHyena ..... 43\nE. 2 Analysis of Hankel Singular Values of Pretrained Large Convolution Sequence Models ..... 43\nE. 3 Model Order Reduction of H3 ..... 43\nE.3.1 Modal Truncation ..... 43\nE.3.2 Balanced Truncation ..... 43\n\n## Authors Contribution\n\nS.M. Conceptualized the research; coordinated collaborations; lead theory development; conducted distillation experiments. M.P. Conceptualized the research; coordinated collaborations; lead the experimental (model pre-training, distillation, benchmarks, downstream evaluation) efforts; coordinated writing and conference submission; optimized inference stack. D.Y.F. Assisted in development of MultiHyena; assisted in pre-training and subsequent benchmarking of distilled models; assisted in writing. H.K. Developed benchmarking suite and interpreted results; assisted in writing. R.N.P. Assisted in theory and algorithmic development; performed model-order reduction experiments of H 3 models; assisted in writing. A.T. Conceived and proved Theorem 4.1; assisted in writing. D.W.R. Assisted in Hankel operator spectral analysis; Assisted in writing. Q.M. Assisted in theory development. B.C. Supervised development of benchmarking suite and model deployment. A.R. Supervised theory development (solving associative recall with MultiHyena, Th.",
    "hyenadistill-53": "4.1). C.Z. Supervised research; secured compute resources. C.R. Supervised research; reviewed manuscript; secured compute resources. S.E. Supervised research; reviewed manuscript. Y.B. Supervised research; reviewed manuscript. Stefano Massaroli, Michael Poli, and Dan Fu contributed equally to this work. Christopher R\u00e9, Stefano Ermon, and Yoshua Bengio share equal senior authorship. All authors read and approved the final manuscript. ## A Linear Systems\n\n## A. 1 Extended Notation and System Theory Preliminaries\n\nWe first introduce the notation and some mathematical concepts that will be used throughout the paper. By $\\mathbb{Z}$ we denote the set of integers, by $\\mathbb{R}$ the set of reals, and by $\\mathbb{C}$ the set of complex numbers. The variable $t$ stands for time. $\\ell_{p}(\\mathbb{Z})$ denotes the Banach space of complex-valued sequences $\\left(x_{t}\\right)_{t \\in \\mathbb{Z}}$ with finite energy, i.e. $\\|x\\|_{p}:=\\left[\\sum_{t \\in \\mathbb{Z}}\\left|x_{t}\\right|^{p}\\right]^{1 / p}<\\infty$ for some $1 \\leq p<\\infty . \\ell_{\\infty}(\\mathbb{Z})$ is instead is the space of sequences for which $\\|x\\|_{\\infty}:=\\sup _{t \\in \\mathbb{Z}}\\left|x_{t}\\right|<\\infty$. With $\\mathbb{S}$ denoting the unit circle in the complex plane, $\\mathbb{S}:=\\{z \\in \\mathbb{C}:|z|=1\\}$ we define $\\mathcal{H}_{p}(\\mathbb{S})$ as the space of functions $X$ from $\\mathbb{C}$ to itself such that $\\|X\\|_{p}:=\\left[(1 / 2 \\pi) \\int_{-\\pi}^{\\pi}\\left|X\\left(e^{i \\omega}\\right)\\right|^{p} \\mathrm{~d} \\omega\\right]^{1 / p}<\\infty$ and $\\mathcal{H}_{\\infty}(\\mathbb{S})$ the space for which $\\|X\\|_{\\infty}:=\\sup _{z \\in \\mathbb{S}}|X(z)|<\\infty$. Particularly, $\\mathcal{K}_{2}(\\mathbb{S})$ is a Hilbert space with inner product $\\langle X, Y\\rangle:=(1 / 2 \\pi) \\int_{-\\pi}^{\\pi} X\\left(e^{i \\omega}\\right) Y^{*}\\left(e^{i \\omega}\\right) \\mathrm{d} \\omega$ where \"*\" denotes complex conjugation. Although we acknowledge we are using the same notation for norms in both $\\ell_{p}(\\mathbb{Z})$ and $\\mathcal{H}_{p}(\\mathbb{S})$, the correct meaning will always be made clear by the context. The $\\mathcal{Z}$-transform of a sequence $x=\\left(x_{t}\\right)_{t \\in \\mathbb{Z}}$ is $X(z)=\\mathcal{Z}[x](z):=$ $\\sum_{t \\in \\mathbb{Z}} x_{t} z^{-t}$. We embrace the system theory convention of using capital letters to identify transformed sequences. The $\\mathcal{Z}$-transform is a projection of the sequence onto a basis of powers $e_{t}=r^{-t} e^{i \\omega t}$. This basis is not orthogonal unless $r=1$. That is the basis of the discrete-time Fourier transform $\\mathcal{F}$. Hence, $\\mathcal{F}$ is defined as $\\mathcal{F}[x]\\left(e^{i \\omega}\\right)=X\\left(e^{i \\omega}\\right):=\\sum_{t \\in \\mathbb{Z}} x_{t} e^{-i \\omega t}$. The discrete-time Fourier transform is an isometric isomorphism between $\\ell_{2}(\\mathbb{Z})$ and $L_{2}(\\mathbb{S})$. We say that sequences live in the time domain and their $\\mathcal{Z}$ (or $\\mathcal{F}$ ) transforms in the frequency domain. A linear system is a linear operator transforming an input sequence $u$ to an output sequence $y$. If the sequences have continuous support, i.e. $t$ ranges over a continuous set (e.g. $\\mathbb{R}$ ), we have a continuous-time system. Conversely, if the sequences have discrete support, i.e. $t$ ranges over a discrete set (e.g. $\\mathbb{Z}$ ), we have a discrete-time or digital system. In this manuscript we restrict ourselves to discrete-time systems. Systems can be single-input single-output (SISO) if $u$ and $y$ are scalar functions or multi-input multi-output if either $u$ or $y$ are vector-valued. We limit our discussion to SISO systems. The impulse response of a system is the output sequence $y$ when the input sequence $u$ is the Kronecker delta function $\\delta_{t}$ and is usually denoted by the letter $h$. The values $h_{t}$ of the impulse response sequence are also known as the Markov parameters of the system. The most common mathematical representation of a linear system is its convolution form: $y=h * u$, i.e. $y_{t}=\\sum_{j \\in \\mathbb{Z}} h_{t-j} u_{j}=\\sum_{j \\in \\mathbb{Z}} h_{j} u_{t-j}, t \\in \\mathbb{Z}$. In matrix form the input-output relation is given by the Toeplitz operator $\\mathrm{T}_{h}$ corresponding to the (possibly infinitely long) sequence $h$, i.e. $y=\\mathrm{T}_{h} u$. Taking the $\\mathcal{Z}$-transforms, we can write the input-output relation as $Y(z)=H(z) U(z)$ (this is just the Fourier convolution theorem extended outside the unit circle). $H(z)$ is called the transfer function of the system. When $z=e^{i \\omega}, H\\left(e^{i \\omega}\\right)$ is just the discrete-time Fourier transform of $h$ which is called the frequency response of the system. A linear system is causal if $h_{t}=0$ for $t<0$. A system is called stable if the $\\mathrm{T}_{h}$ is a bounded operator. If $u, y \\in \\ell_{2}(\\mathbb{Z})$, then stability implies $h \\in \\ell_{\\infty}$. In the following, we mainly focus on causal stable systems. ## A. 2 Systems Norms\n\nWhen quantitatively characterizing linear systems, several norms play a crucial role. These norms provide measures of various characteristics of the systems, which are essential in both analysis and filter design. The $\\ell_{2}$ and $\\mathcal{H}_{2}$ norms As defined above, the $\\ell_{2}$ norm represents the energy of a signal $h$,\n\n$$\n\\|h\\|_{2}:=\\left[\\sum_{t \\in \\mathbb{Z}}|h|_{t}^{2}\\right]^{1 / 2}\n$$\n\nwhile $\\mathcal{H}_{2}$ is the energy of the (continuous) spectrum of $h$,\n\n$$\n\\|H\\|_{2}:=\\left[\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|X\\left(e^{i \\omega}\\right)\\right|^{2} \\mathrm{~d} \\omega\\right]^{1 / 2}\n$$\n\nBy Parseval's theorem, the $\\ell_{2}$ and $\\mathcal{H}_{2}$ norms are equal, $\\|h\\|_{2}=\\|H\\|_{2}$. Further these norms are useful to study the approximation of convolutional filter. The following holds:\n\nLemma A. 1 ( $\\ell_{\\infty}$ output error). Consider the class of $\\ell_{2}$ measurable inputs such that $\\|u\\|_{2} \\leq \\zeta$, then for all $H, \\hat{H} \\in \\mathcal{H}_{2}$,\n\n$$\n\\|y-\\hat{y}\\|_{\\infty} \\leq \\zeta\\|H-\\hat{H}\\|_{2}\n$$\n\nProof. $$\n\\begin{array}{rlr}\n\\sup _{t>0}\\left|y_{t}-\\hat{y}_{t}\\right| & =\\sup _{t>0}\\left|\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left[Y\\left(e^{i \\omega}\\right)-\\hat{Y}\\left(e^{i \\omega}\\right)\\right] e^{i \\omega t} \\mathrm{~d} \\omega\\right| \\\\\n& \\leq \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|Y\\left(e^{i \\omega}\\right)-\\hat{Y}\\left(e^{i \\omega}\\right)\\right| \\mathrm{d} \\omega & \\\\\n& \\left.=\\frac{1}{2 \\pi} \\int_{\\pi}^{\\pi}\\left|H\\left(e^{i \\omega}\\right)-\\hat{H}\\left(e^{i \\omega}\\right)\\right| U\\left(e^{i \\omega}\\right) \\right\\rvert\\, \\mathrm{d} \\omega & \\\\\n& \\leq\\left[\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|H\\left(e^{i \\omega}\\right)-\\hat{H}\\left(e^{i \\omega}\\right)\\right|^{2} \\mathrm{~d} \\omega\\right]^{1 / 2}\\left[\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|U\\left(e^{i \\omega}\\right)\\right|^{2} \\mathrm{~d} \\omega\\right]^{1 / 2} & \\\\\n& \\text { H\u00f6lder Inequality } \\\\\n& \\leq\\left[\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|H\\left(e^{i \\omega}\\right)-\\hat{H}\\left(e^{i \\omega}\\right)\\right|^{2} \\mathrm{~d} \\omega\\right]^{1 / 2}\\|u\\|_{2} & \\\\\n& \\leq \\zeta\\|H-\\hat{H}\\|_{\\mathcal{H}_{2}} & \\text { Parseval Theorem }\n\\end{array}\n$$\n\nIf $u$ is the unit impulse function $u_{t}=\\delta_{t}$ then $\\zeta=1$. The results also holds for finite sequences of length $L$ using the discrete Fourier transform. Lemma A. 2 (Impulse response error on finite sequences). Consider filters $h, \\hat{h}$ with finite length $L$. Then, the following holds. $$\n\\|h-\\hat{h}\\|_{\\infty} \\leq\\|H-\\hat{H}\\|_{2}\n$$\n\nwhere $H$ and $\\hat{H}$ denote the discrete Fourier transforms of $h$ and $\\hat{h}$, respectively. Proof. $$\n\\begin{array}{rlrl}\n\\|y-\\hat{y}\\|_{\\infty}:=\\sup _{t>0}\\left|y_{t}-\\hat{y}_{t}\\right| & =\\sup _{t>0}\\left|\\frac{1}{2 \\pi} \\sum_{n=0}^{L-1}\\left[Y_{n}-\\hat{Y}_{n}\\right] e^{i 2 \\pi n t / L}\\right| \\\\\n& \\leq \\frac{1}{2 \\pi} \\sum_{n=0}^{L-1}\\left|Y_{n}-\\hat{Y}_{n}\\right| & & \\\\\n& =\\frac{1}{2 \\pi} \\sum_{n=0}^{L-1}\\left|H_{n}-\\hat{H}_{n}\\right|\\left|U_{n}\\right| & & \\\\\n& \\leq\\left[\\frac{1}{2 \\pi} \\sum_{n=0}^{L-1}\\left(H_{n}-\\hat{H}_{n}\\right)^{2}\\right]^{1 / 2}\\left[\\frac{1}{2 \\pi} \\sum_{n=0}^{L-1} U_{n}^{2}\\right]^{1 / 2} & & \\text { H\u00f6lder Inequality } \\\\\n& \\leq\\left[\\frac{1}{2 \\pi} \\sum_{n=0}^{L-1}\\left(H_{n}-\\hat{H}_{n}\\right)^{2}\\right]^{1 / 2}\\|u\\|_{2} & & \\text { Parseval Theorem } \\\\\n& =\\|H-\\hat{H}\\|_{2} & & \\text { using }\\|u\\|_{2}=1\n\\end{array}\n$$\n\n## A. 3 Transfer Function of State-Space Models\n\nThe transfer function (3.1) is derived by taking the $z$-transform of input and state, $U(z)=\\mathcal{Z}[u](z), X(z)=$ $\\mathcal{Z}[x](z)$. Plugging $U(z), X(z)$ in the state equation (2.2), it holds\n\n$$\nz X(z)=\\mathrm{A} X(z)+\\mathrm{B} U(z) \\Leftrightarrow X(z)=(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B} U(z)\n$$\n\nSubstituting in the output equation yields\n\n$$\nY(z)=\\mathrm{C}(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B} U(z)+h_{0} U(z)\n$$\n\nThe transfer function is then defined as\n\n$$\nH(z)=\\frac{Y(z)}{U(z)}=\\mathrm{C}(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B}+h_{0}\n$$\n\nAlternative derivation The transfer function can also be derived by direct $z$-transform of the impulse response $h_{t}$ of the system. This derivation is useful to highlight the region of convergence of the transfer function. $$\n\\begin{aligned}\nH(z) & =h_{0}+\\sum_{t=1}^{\\infty} z^{-t} \\mathrm{CA}^{t-1} \\mathrm{~B} & & h_{0} \\text { is pulled out via } h_{0} z^{0}=h_{0} \\\\\n& =h_{0}+\\mathrm{C}\\left[\\sum_{t=1}^{\\infty} z^{-t} \\mathrm{~A}^{t-1}\\right] \\mathrm{B} & & \\text { multiplication distributes over sum. } \\\\\n& =h_{0}+z^{-1} \\mathrm{C}\\left[\\sum_{t=1}^{\\infty} z^{-(t-1)} \\mathrm{A}^{t-1}\\right] \\mathrm{B} & & \\text { multiply by } z / z \\\\\n& =h_{0}+z^{-1} \\mathrm{C}\\left[\\sum_{t=0}^{\\infty}\\left(z^{-1} \\mathrm{~A}\\right)^{t}\\right] \\mathrm{B} & & \\text { change of index and collect like terms }\n\\end{aligned}\n$$\n\nWe look at the convergence of the series $\\sum_{t=0}^{\\infty}\\left\\|z^{-1} \\mathrm{~A}\\right\\|_{2}^{t}$. We have\n\n$$\n\\begin{aligned}\n\\left\\|z^{-1} \\mathrm{~A}\\right\\|_{2} & \\leq\\left\\|z^{-1}\\right\\|_{2}\\|\\mathrm{~A}\\|_{2} \\\\\n& =\\left\\|r^{-1} e^{-i \\omega}\\right\\|_{2}\\|\\mathrm{~A}\\|_{2} \\quad \\text { using } z:=r e^{i \\omega} \\in \\mathbb{C}, r, \\omega \\in \\mathbb{R} \\\\\n& \\leq r^{-1}\\|\\mathrm{~A}\\|_{2}=r^{-1} \\rho(\\mathrm{A})\n\\end{aligned}\n$$\n\nThe series converges to $1 /\\left(1-r^{-1} \\rho(\\mathrm{A})\\right)$ if and only if $r^{-1} \\rho(\\mathrm{A})<1$ i.e. for $r>\\rho(\\mathrm{A})$. Thus, in the exterior of the disk with radius $\\rho(\\mathrm{A}), \\mathbb{D}_{\\rho(\\mathrm{A})}:=\\{z \\in \\mathbb{C}:|z|>\\rho(\\mathrm{A})\\}, \\sum_{t=0}^{\\infty}\\left(z^{-1} \\mathrm{~A}\\right)^{t}$ converges to $\\left(\\mathrm{I}-z^{-1} \\mathrm{~A}\\right)^{-1}$ and\n\n$$\nz \\in \\mathbb{D}_{\\rho(\\mathrm{A})} \\Rightarrow H(z)=h_{0}+z^{-1} \\mathrm{C}\\left(\\mathrm{I}-z^{-1} \\mathrm{~A}\\right)^{-1} \\mathrm{~B}=h_{0}+\\mathrm{C}(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B}\n$$\n\nThe transfer function $H(z)=h_{0}+\\mathrm{C}(z \\mathrm{I}-A)^{-1} \\mathrm{~B}$ of a stable lumped discrete-time system is defined outside the disc in the complex plane that encloses all the eigenvalues of $A$. Invariance of the transfer function $H(z)$ as defined in (A.1) is a proper ${ }^{13}$ rational function of $z$. In case $h_{0}=0, H(z)$ is strictly proper and the denominator is monic:\n\n$$\nH(z)=\\frac{b_{1} z^{-1}+\\cdots+b_{d} z^{-d}}{1+a_{1} z^{-1}+\\cdots+a_{d} z^{-d}}\n$$\n\nSpecifically, the denominator could be derived from A with $\\operatorname{det}(z \\mathbf{I}-\\mathrm{A})$, and the numerator is $\\operatorname{det}(z \\mathbf{I}-\\mathrm{A}+$ $B C)+\\operatorname{det}(z I-A)$. We provide a detailed derivation below in Section A.6. While state-space representation involves the analysis and synthesis of model matrices $A, B, C$, the transfer function is entirely characterized by the coefficients $a=\\left(a_{n}\\right)_{n=1}^{d}, b=\\left(b_{n}\\right)_{n=1}^{d}$ of numerator and denominator polynomials. Notably, the transfer function is an invariant of the system: if we apply a change of variables to the state, the transfer function remains unchanged. Lemma A.3. Coefficients $a, b$ are invariant under any invertible change of variables. Proof. The proof can be found in [5, pp.95] and follows from the definition of equivalence transformation. Consider the state-space matrices of under change of variables $\\hat{x}=\\mathrm{K} x$,\n\n$$\n\\hat{\\mathrm{A}}=\\mathrm{KAK}^{-1}, \\quad \\hat{\\mathrm{B}}=\\mathrm{KB}, \\quad \\hat{\\mathrm{C}}=\\mathrm{CK}^{-1}, \\quad \\hat{h}_{0}=h_{0}\n$$\n\nThe resulting transfer function $H(z)$ can then be computed as\n\n$$\n\\hat{H}(z)=\\hat{\\mathrm{C}}(z \\mathrm{I}-\\hat{\\mathrm{A}})^{-1} \\hat{\\mathrm{B}}+\\hat{h}_{0}=\\mathrm{CK}^{-1}\\left[\\mathrm{~K}(z \\mathrm{I}-\\mathrm{A}) \\mathrm{K}^{-1}\\right]^{-1} \\mathrm{~KB}+h_{0}=H(z)\n$$\n\n## A. 4 Truncated Transfer Functions\n\nIn the case of generic truncated (finite) impulse response filters, such that $h_{t}=0$ for all $t$ greater than a certain value $L$ (which we refer to as the length of the filter), the transfer function is simply a polynomial in the complex variable $z$ of order $L$, i.e. $$\nH(z)=\\sum_{t=0}^{\\infty} h_{t} z^{-t}=\\sum_{t=0}^{L} h_{t} z^{-t}=h_{0}+h_{1} z^{-1}+\\cdots+h_{L} z^{-L}\n$$\n\n[^5]In case the filter is generated by a finite dimensional (lumped parameters) system, i.e. $h_{t}=\\mathrm{CA}^{t-1} \\mathrm{~B}$ $t=1, \\ldots, L$, then (A.4) can still be represented exactly by a rational function of order $d$. Lemma A. 4 (Truncated rational transfer functions). Consider the L-truncated impulse response $h_{t} \\in \\ell_{2}(\\mathbb{N})$ of a lumped-parameter filter $\\left(\\mathrm{A}, \\mathrm{B}, \\mathrm{C}, h_{0}\\right)$,\n\n$$\nh_{t}= \\begin{cases}h_{0} & t=0 \\\\ \\mathrm{CA}^{t-1} \\mathrm{~B} & 1 \\leq t \\leq L \\\\ 0 & t>L\\end{cases}\n$$\n\nThen its truncated transfer function is\n\n$$\nH_{L}(z)=\\mathcal{Z}\\{h\\}(z)=h_{0}+\\mathrm{C}\\left(\\mathrm{I}-z^{-L} \\mathrm{~A}^{L}\\right)(z \\mathbf{I}-\\mathrm{A})^{-1} \\mathrm{~B}\n$$\n\nProof. By definition of $z$-transform we have\n\n$$\n\\begin{aligned}\nH_{T}(z) & =\\sum_{t=0}^{\\infty} h_{t} z^{-t}=h_{0}+\\sum_{t=1}^{L} z^{-t} \\mathrm{CA}^{t-1} \\mathrm{~B} \\\\\n& =h_{0}+\\mathrm{C}\\left[\\sum_{t=1}^{L} z^{-t} \\mathrm{~A}^{t-1}\\right] \\mathrm{B}=h_{0}+z^{-1} \\mathrm{C}\\left[\\sum_{t=0}^{L-1}\\left(z^{-1} \\mathrm{~A}\\right)^{t}\\right] \\mathrm{B}\n\\end{aligned}\n$$\n\nThe sum $\\sum_{t=0}^{L-1}\\left(z^{-1} \\mathrm{~A}\\right)^{t}$ is a partial Neumann series and can be manipulated as follows. $$\n\\begin{aligned}\n\\sum_{t=0}^{L-1}\\left(z^{-1} \\mathrm{~A}\\right)^{t}\\left(\\mathrm{I}-z^{-1} \\mathrm{~A}\\right) & =\\sum_{t=0}^{L-1}\\left(z^{-1} \\mathrm{~A}\\right)^{t}-\\sum_{t=0}^{L-1}\\left(z^{-1} \\mathrm{~A}\\right)^{t+1} \\\\\n& =\\mathrm{I}-\\left(z^{-1} \\mathrm{~A}\\right)^{L}\n\\end{aligned}\n$$\n\nThus,\n\n$$\n\\sum_{t=0}^{L-1}\\left(z^{-1} \\mathrm{~A}\\right)^{t}=\\left(\\mathrm{I}-z^{-L} \\mathrm{~A}^{L}\\right)\\left(\\mathrm{I}-z^{-1} \\mathrm{~A}\\right)^{-1}\n$$\n\nwhich plugged in (A.5) gives $H_{L}(z)=h_{0}+\\mathrm{C}\\left(\\mathrm{I}-z^{-L} \\mathrm{~A}^{L}\\right)(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B}$, proving the result. Because of truncation, evaluating the transfer function $H_{L}(z)$ on the $L$ roots of unity $z=e^{i \\omega_{k}}, w_{k}=$ $2 \\pi k / T$ for $k=0, \\ldots L$ gives the length- $L$ discrete Fourier transform (DFT) of the filter:\n\n$$\n\\bar{H}_{k}:=H_{L}\\left(e^{i \\omega_{k}}\\right)=\\sum_{t=0}^{L-1} h_{t} e^{-i 2 \\pi k / L}, \\quad k=0, \\ldots, L-1\n$$\n\nIn practice, this means that $\\bar{H} \\in \\mathbb{C}^{L}$ is the FFT of $h, \\bar{H}=\\mathrm{FFT}_{L}[h]$. If we can find an efficient and stable algorithm to evaluate $\\bar{H}$ from the system matrices (A, B, C, $h_{0}$ ), then the FFT-based convolution of truncated filter with an input sequence $u \\in \\mathbb{R}^{L}$ can be evaluated in $\\tilde{O}(L)$ time. Reparametrization Assume training a LCSM equipped with SSM filters with input/target sequences to be all of length $L$ (smaller sequences can be padded with zeros to the maximum length). Thus, for training purposes, we are only interested in evaluating $\\bar{H}$ for the FFT-based convolution. The truncated transfer function $H_{L}$ is equal to the original one with a correction term $\\mathrm{I}-z^{-L} \\mathrm{~A}^{L}$ on the numerator polynomial. As already noted in S4 [6], $z^{-L}$ is conveniently equal to one on the roots of unity, $z^{i \\omega_{k} L}=e^{-i 2 \\pi k}=1$ for all $k=0, \\ldots, L-1$. Hence, the correction term due to truncation becomes constant: $H_{k}=\\mathrm{C}\\left(\\mathrm{I}-\\mathrm{A}^{L}\\right)(\\exp (-i 2 \\pi k / L) \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B}$; in DFT domain the truncated filter behaves as the infinitely long one with a perturbed $C$ matrix\n\n$$\n\\overline{\\mathrm{C}}=\\mathrm{C}-\\mathrm{CA}^{L}\n$$\n\nIf -as assumed- the SSM is stable $\\rho(\\mathrm{A})<1,(i)$ the transfer function is defined on the unit circle, term CA $^{L}$ will go to zero exponentially fast as $L \\rightarrow \\infty$ and $\\overline{\\mathrm{C}}=\\mathrm{C}$ (as expected). As advised in [6], it is desirable to parametrize directly $\\bar{C}$; the expensive computation of the correction term $C\\left(I-A^{L}\\right)$ is never carried out during training. Instead, the real $C$ matrix can be retrieved for recurrent inference by inverting the correction term $\\mathrm{C}=\\overline{\\mathrm{C}}\\left(\\mathrm{I}-\\mathrm{A}^{L}\\right)^{-1}$, always invertible for stable systems although possibly ill conditioned by eigenvalues too close to the stability margin (the unit circle). ## A. 5 From Transfer Function to State-Space\n\nSuppose the coefficients of the numerator and denominator polynomials of a proper transfer function $H$ is given:\n\n$$\nH(z)=\\frac{b_{0}+b_{1} z^{-1}+\\cdots+b_{d} z^{-d}}{1+a_{1} z^{-1}+\\cdots+a_{d} z^{-d}}\n$$\n\nA state-space representation of the form (2.2) can be rapidly realized in two steps:\n\n1. Get delay-free path From (A.6) we first notice that the bias term $h_{0}$ is $h_{0}=b_{0}$. We thus want to isolate $b_{0}$ from the rest of the numerator. This can be obtained via long division (see \u00a7A.5.1) and results in\n\n$$\nH(z)=\\frac{\\beta_{1} z^{-1}+\\cdots+\\beta_{N} z^{-d}}{1+a_{1} z^{-1}+\\cdots+a_{d} z^{-d}}+b_{0}, \\quad \\beta_{n}=b_{n}-b_{0} a_{n}\n$$\n\n2. Get state-space matrices Given the transfer function $H(z)$ with the isolated pass-through coeffient $b_{0}$ as in (A.7), we can construct the state-space matrices by companion canonical realization:\n\n$$\n\\left[\\begin{array}{c|c}\n\\mathrm{A} & \\mathrm{B} \\\\\n\\hline \\mathrm{C} & h_{0}\n\\end{array}\\right]=\\left[\\begin{array}{ccccc|c}\n-a_{1} & -a_{2} & \\cdots & -a_{d-1} & -a_{d} & 1 \\\\\n1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n0 & 1 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & 1 & 0 & 0 \\\\\n\\hline \\beta_{1} & \\beta_{2} & \\cdots & \\beta_{d-1} & \\beta_{d} & b_{0}\n\\end{array}\\right]\n$$\n\nDetails on the complete a la [5] derivation can be found in \u00a7A.5.2. A linear system with finite-dimensional state can be equivalently characterized: by its state-space matrices $\\left(A, B, C, h_{0}\\right)$, by its impulse response function $h$, or by the coefficients $a, b$ (or $\\beta$ ) of the transfer function. A fourth representation is its linearconstant-coefficients difference equation form\n\n$$\ny_{t}=\\sum_{j=0}^{d} b_{j} u_{t-j}-\\sum_{n=1}^{d} a_{j} y_{t-j}\n$$\n\ntypically used in signal processing literature in the theory of infinite impulse response filters (see [46]) and known, in the context of system identification of error-in-variables models, as auto-regressive moving-average filters $[47,48]$. ## A.5.1 Isolating the $h_{0}$-term from Transfer Function by Long division\n\nIf the rational transfer function $H(z)$ accounts for the $h_{0}$ term, then it is simply proper (order of numerator equals the order of denominator), $h_{0}$ is necessarily $h_{0}=b_{0}$ (the delay-free path). Given the transfer function in this form, we can isolate the $b_{0}$ term and the strictly rational term of (A.3) by long division. We start by expanding the fraction as\n\n$$\nH(z)=\\frac{q(z)}{p(z)}=\\frac{b_{0}}{p(z)}+\\frac{b_{1} z^{-1}+\\cdots+b_{d} z^{-d}}{p(z)}\n$$\n\nand\n\n$$\n\\frac{b_{0}}{p(z)}=\\frac{b_{0} z^{d}}{z^{d}+a_{1} z^{d-1}+\\cdots+a_{d}}\n$$\n\nWe then use the long division method to compute $b_{0} / p(z)$ :\n\n$$\n\\begin{aligned}\n& z ^ { d } + a _ { 1 } z ^ { d - 1 } + \\cdots + a _ { d } \\longdiv { b _ { 0 } z ^ { d } } \\\\\n& \\frac{b_{0} z^{d}+b_{0} a_{1} z^{d-1}+\\cdots+b_{0} a_{d}}{-b_{0} a_{1} z^{d-1}-\\cdots-b_{0} a_{d}} \\quad(\\text { reminder) }\n\\end{aligned}\n$$\n\nto finally get\n\n$$\n\\begin{aligned}\nH(z) & =b_{0}-\\frac{b_{0} a_{1} z^{d-1}+\\cdots+b_{0} a_{d}}{z^{d}+a_{1} z^{d-1}+\\cdots+a_{d}}+\\frac{b_{1} z^{-1}+\\cdots+b_{d} z^{-d}}{p(z)} \\\\\n& =b_{0}+\\frac{\\left(b_{1}-b_{0} a_{1}\\right) z^{-1}+\\cdots+\\left(b_{d}-b_{0} a_{d}\\right) z^{-d}}{1+a_{1} z^{-1}+\\cdots+a_{d} z^{-d}}\n\\end{aligned}\n$$\n\nNote that the coefficients $b_{n}$ in (A.3) correspond to $b_{n}-b_{0} a_{n}$ in (A.6), $b_{n} \\leftarrow b_{n}-b_{0} a_{n}$. It is indifferent to parameterize the coefficients of the transfer function in either forms. However, if we choose the simply proper representation (A.6), we need to apply the derived correction factor to the numerator coefficients when we separate the $h_{0}$ term and strictly proper part of $H(z)$. ## A.5.2 Construction of the State-Space from the Transfer Function\n\nChen's derivation The derivation is based on the steps reported for the continuous-time multi-input multi-output case in [5]. First, we define a pseudo-state $v$ such that\n\n$$\np(z) V(z)=U(z) \\quad \\Leftrightarrow \\quad V(z)=\\frac{1}{p(z)} U(z) . $$\n\nThen, we define the state $x_{t}:=\\left(x_{t}^{1}, \\ldots, x_{t}^{d}\\right) \\in \\mathbb{R}^{d}$ as\n\n$$\nx_{t}=\\left(v_{t-1}, v_{t-2}, \\cdots, v_{t-d}\\right) \\quad \\Leftrightarrow \\quad \\mathcal{Z}\\{x\\}(z)=X(z)=\\left[\\begin{array}{c}\nz^{-1} \\\\\n\\vdots \\\\\nz^{-d}\n\\end{array}\\right] V(z)\n$$\n\nFrom (A.9) we have\n\n$$\n\\begin{aligned}\nV(z)+a_{1} z^{-1} V(z)+\\cdots+a_{d} z^{-d} V(z)=U(z) & \\Leftrightarrow \\\\\nV(z)=-a_{1} z^{-1} V(z)-\\cdots-a_{d} z^{-d} V(z)+U(z) & \\Leftrightarrow \\\\\nv_{t}=-a_{1} v_{t-1}-\\cdots-a_{d} v_{t-d}+u_{t} & \\Leftrightarrow \\quad \\text { time-delay prop. of } \\mathcal{Z} \\text {-transform } \\\\\nx_{t+1}^{1}=-a_{1} x_{t}^{1}-\\cdots-a_{d} x_{t}^{d}+u_{t} & \\Leftrightarrow \\quad \\text { by def. of state (A.10) }\n\\end{aligned}\n$$\n\nThus, we have the overall recurrence\n\n$$\n\\begin{aligned}\n& x_{t+1}^{1}=-a_{1} x_{t}^{1}-\\cdots-a_{d} x_{t}^{d}+u_{t} \\\\\n& x_{t+1}^{2}=x_{t}^{1} \\\\\n& \\quad \\vdots \\\\\n& x_{t+1}^{d}=x_{t}^{d-1}\n\\end{aligned}\n$$\n\nwhich can be written in matrix form as\n\n$$\nx_{t+1}=\\left[\\begin{array}{cccc}\n-a_{1} & -a_{2} & \\cdots & -a_{N} \\\\\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{array}\\right] x_{t}+\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n0\n\\end{array}\\right] u_{t}\n$$\n\nThe output spectrum is then given by\n\n$$\n\\begin{array}{rlr}\nY(z) & =H(z) U(z)=\\frac{q(z)}{p(z)} U(z)+b_{0} U(z) & \\\\\n& =q(z) V(z)+b_{0} U(z) \\quad \\text { by def. of } V(z)\n\\end{array}\n$$\n\nTherefore,\n\n$$\n\\begin{aligned}\nY(z) & =q(z) V(z)+b_{0} U(z)=\\left[\\begin{array}{llll}\n\\beta_{1} & \\beta_{2} & \\cdots & \\beta_{N}\n\\end{array}\\right]\\left[\\begin{array}{c}\nz^{-1} \\\\\nz^{-2} \\\\\n\\vdots \\\\\nz^{-d}\n\\end{array}\\right] V(z)+b_{0} U(z) \\\\\n& =\\left[\\begin{array}{llll}\n\\beta_{1} & \\beta_{2} & \\cdots & \\beta_{d}\n\\end{array}\\right] X(z)+b_{0} U(z)\n\\end{aligned}\n$$\n\nand the output equation in time-domain is given by\n\n$$\ny_{t}=\\left[\\begin{array}{llll}\n\\beta_{1} & \\beta_{2} & \\cdots & \\beta_{d}\n\\end{array}\\right] x_{t}+b_{0} u_{t}\n$$\n\nyielding state-space matrices (A.8). ## A. 6 From State-Space to Transfer Function\n\nWe detail an implementation oriented method to compute the coefficients $\\left(a_{n}\\right)_{n=1}^{d},\\left(b_{n}\\right)_{n=0}^{d}$ of a SSM's transfer function. Recall that\n\n$$\nH(z)=\\mathrm{C}[z \\mathrm{I}-\\mathrm{A}]^{-1} \\mathrm{~B}+h_{0}=\\frac{\\mathrm{C} \\operatorname{Adj}(z \\mathrm{I}-\\mathrm{A}) \\mathrm{B}+\\operatorname{det}(z \\mathrm{I}-\\mathrm{A}) h_{0}}{\\operatorname{det}(z \\mathrm{I}-\\mathrm{A})}\n$$\n\nHence, the denominator coefficients $\\left(a_{n}\\right)_{n=1}^{d}$ are simply the coefficients of the characteristic polynomial of matrix A.",
    "hyenadistill-54": "They can be easily obtained by 1 . computing the eigenvalues of $A$ and 2 . calculating the coefficients of the polynomial whose roots are such eigenvalues. On the other hand, the numerator apparently involves more complex symbolic manipulation. This can be simplified recalling a classic matrix-determinant identity:\n\nLemma A. 5 ([49]). Let M, B, and C respectively denote matrices of orders $d \\times d, d \\times 1$, and $1 \\times d$. Then,\n\n$$\n\\operatorname{det}(M+B C)=\\operatorname{det}(M)+C \\operatorname{Adj}(M) B\n$$\n\nApplying Lemma A. 5 to (A.11) we obtain\n\n$$\nH(z)=\\frac{\\operatorname{det}(z \\mathrm{I}-\\mathrm{A}+\\mathrm{BC})+\\operatorname{det}(z \\mathbf{I}-\\mathrm{A})\\left(h_{0}-1\\right)}{\\operatorname{det}(z \\mathrm{I}-\\mathrm{A})}\n$$\n\nLet poly $(r)$ denote the coefficients of the polynomials with roots $r=\\left(r_{1}, \\ldots, r_{d}\\right)$. Then $a=\\operatorname{poly}(\\operatorname{eig}(\\mathrm{A}))$. Since A and A - BC are of equal dimension, their characteristic polynomials have equal order and therefore\n\n$$\nb=\\operatorname{poly}(\\operatorname{eig}(\\mathrm{A}-\\mathrm{BC}))+\\operatorname{poly}(\\operatorname{eig}(\\mathrm{A}))\\left(h_{0}-1\\right)\n$$\n\nListing 1: State-space $\\rightarrow$ transfer function conversion code\n\n```\ndef get_tf_from_ss(A,B,C,h0):\n    a = poly(eig(A))\n    b = poly(eig(A - outer(B,C))) + (h0-1)*a\n    return a, b\n```\n\n\n## A. 7 State-Space Representation of Truncated Filters. A truncated filter $h_{0}, \\ldots, h_{L}-$ as the ones found in any standard convolutional neural network - can be represented by a $L$-dimensional companion canonical SSM. The filter's transfer function $H(z)=h_{0}+h_{1} z^{-1}+$ $\\cdots+h_{L} z^{-L}$ is polynomial, i.e. a rational function with the denominator's coefficients set to zero. Following the canonical realization process detailed in Section A.5, the truncated filter has state-space form:\n\n$$\n\\begin{aligned}\nx_{t+1} & =\\left[\\begin{array}{ccccc}\n0 & 0 & \\cdots & 0 & 0 \\\\\n1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & 1 & 0\n\\end{array}\\right] x_{t}+\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n0\n\\end{array}\\right] u_{t} \\\\\ny_{t} & =\\left[\\begin{array}{llll}\nh_{1} & h_{2} & \\cdots & h_{L}\n\\end{array}\\right] x_{t}+h_{0} u_{t} . \\end{aligned}\n$$\n\nIf $x_{0}=\\mathbb{O}_{L}$ and $u_{t}=0$ for negative $t$, then at each $t>0$ the state is a shifted copy of the input sequence $x_{t}=\\left(u_{t-1}, \\ldots, u_{t-L}\\right) \\in \\mathbb{R}^{L}$. Nonetheless, the asymptotic complexity of computing one recurrent step is $\\mathcal{O}(L)$ as it requires only a shift operation and a length- $L$ dot product\n\n$$\n\\begin{aligned}\nx_{t+1}^{1} & =u_{t} \\\\\nx_{t+1}^{2: L} & =\\operatorname{shift}\\left(x_{t}\\right) \\\\\ny_{t} & =\\left\\langle h_{1: L}, x_{t}\\right\\rangle+h_{0} u_{t}\n\\end{aligned}\n$$\n\nThe memory footprint is also $\\mathcal{O}(L)$. In [1] it is proposed the use of shift-type SSMs to parametrize one of the filters of the H 3 block. ## A. 8 Efficient Computation of State-Space Models\n\n## A.8.1 Fast Evaluation of the Transfer Function\n\nComputing $H(z)$ at any point $z \\in \\mathbb{C}$ concerns the evaluation of the $d$-order polynomial of numerator and denominator,\n\n$$\nH(z)=\\frac{q(z)}{p(z)}=\\frac{\\sum_{n=1}^{d} b_{n} z^{-n}}{1+\\sum_{n=1}^{d} a_{n} z^{-n}}\n$$\n\nIn practice, we are mainly interested in a fast algorithm that allows computing $H$ on the $L$ roots of unity to obtain the DFT of the filter. The DFT of the filter can be then readily used to perform a FFT-based convolution with a length- $L$ input sequence $u$ or to recover the impulse response function via inverse DFT. We prove the following:\n\nLemma A.6. Given the coefficients $a, b$ of the transfer function, the frequency and impulse response of the filter can be evaluated in $\\tilde{\\mathcal{O}}(L)$ time. Proof. The result is proven showing that the transfer function can be evaluated in $\\tilde{\\mathcal{O}}(L)$ time on the $L$ roots of unity. The fastest method to evaluate polynomials on $L$ arbitrary points $z$ of the complex plane is generally the Horner's scheme. This method is based on a sequence of nested multiplications and computes the polynomial from its vector of coefficients, delivering a time complexity of $\\mathcal{O}(d L)$. More explicitly, Horner's scheme determines $p(z)$ as $\\left.p(z)=\\left(\\left(\\cdots\\left(\\left(a_{d} z^{-1}+a_{d-1}\\right) z^{-1}+a_{d-2}\\right) \\cdots\\right) z^{-1}+a_{2}\\right) z^{-1}+a_{1}\\right) z^{-1}+1$. Each step involves a multiplication and an addition, making a total of $2 d$ operations per evaluation point. Thus, for $L$ points, the total number of operations amounts to $\\mathcal{O}(d L)$. Effectively, Horner's approach implements the matrix-vector product of an $L$-by- $(d+1)$ Vandermonde matrix $\\mathrm{V} \\in \\mathbb{C}^{L \\times(d+1)}$ constructed by $L$ evaluation points $\\left(z_{0}, \\ldots, z_{L-1}\\right)$ with the vector of coefficients $a=\\left(1, a_{1}, \\ldots, a_{d}\\right)^{\\top}$\n\n$$\n\\left[\\begin{array}{c}\np\\left(z_{0}\\right) \\\\\np\\left(z_{1}\\right) \\\\\n\\vdots \\\\\np\\left(z_{L-1}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ccccc}\n1 & z_{0}^{-1} & z_{0}^{-2} & \\cdots & z_{0}^{-d} \\\\\n1 & z_{1}^{-1} & z_{1}^{-2} & \\cdots & z_{1}^{-d} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & z_{L-1}^{-1} & z_{L-1}^{-2} & \\cdots & z_{L-1}^{-d}\n\\end{array}\\right]\\left[\\begin{array}{c}\n1 \\\\\na_{1} \\\\\n\\vdots \\\\\na_{d}\n\\end{array}\\right]=\\mathrm{V} a\n$$\n\nSignificantly, if the polynomial is required to be evaluated at the roots of unity, the Vandermonde matrix simplifies corresponds to the $L \\times(d+1)$ DFT matrix. Further, zero-padding the coefficient vector to length $L$, enables the use a single length- $L$ FFT to compute the matrix-vector product in $\\tilde{\\mathcal{O}}(L)$ time. Thus, the numerator and denominator polynomials of the transfer function can be evaluated, on the roots of unity, in $\\tilde{\\mathcal{O}}(L)$ time by taking the FFT of the padded numerator / denominator coefficients $a, b$ and subsequently dividing element-wise the two sequences as $\\mathrm{FFT}_{L}[b] / \\mathrm{FFT}_{L}[a]$. The overall time complexity to obtain the impulse response is also $\\tilde{\\mathcal{O}}(L)$ since $h$ can be recovered taking an inverse FFT of the frequency response. ## A.8.2 Fast Companion Recurrence\n\nThe recurrent step of a generic SSM (2.2) with dense system matrices usually requires $\\mathcal{O}\\left(d^{2}\\right)$ operations due to the matrix-vector product $\\mathrm{A} x_{t}$. We show how the recurrence of SSMs in companion canonical form, i.e. with system's matrices (A.8), requires only $\\mathcal{O}(d)$ operations. Lemma A.7. The recurrent step of a state-space model in companion canonical form (A.8) can be evaluated in $\\mathcal{O}(d)$ time and memory. Proof. The companion state matrix A can be broken down into a lower shift matrix $\\mathrm{L}_{N}$ and a low-rank term. Particularly, with $e_{1}$ the first element of the canonical basis of $\\mathbb{R}^{N}$ and $\\alpha=\\left(a_{1}, \\ldots, a_{N}\\right)$, we have\n\n$$\n\\mathrm{A}=\\mathrm{L}_{N}-e_{1} \\otimes \\alpha\n$$\n\nIt follows that the recurrent update can be simplified to\n\n$$\n\\begin{aligned}\nx_{t+1} & =\\left(\\mathrm{L}_{N}-e_{1} \\otimes \\alpha\\right) x_{t}+\\mathrm{B} u_{t} \\\\\ny_{t} & =\\mathrm{C} x_{t}+b_{0} u_{t}\n\\end{aligned}\n$$\n\nThe peculiarity of this formulation is that we never need to construct the matrices to perform the recurrence. In particular we have:\n\n$$\n\\begin{aligned}\nx_{t+1}^{1} & =u_{t}-\\alpha^{\\top} x_{t} \\\\\nx_{t+1}^{2: N} & =\\operatorname{shift}\\left(x_{t}\\right) \\\\\ny_{t} & =\\beta^{\\top} x_{t}+b_{0} u_{t}\n\\end{aligned}\n$$\n\nThus, each step only requires two inner products ( $d$ multiplications and $d$ sums each) and one shift operation, totaling $\\mathcal{O}(d)$ operations. The proof of Lemma A. 7 yields the practical implementation of the recurrence:\nListing 2: Python implementation of the companion canonical recurrence\n\n```\ndef step(x, u, alpha, beta, b0):\n    y = dot(beta, x) + b0*u\n    lr}=u-\\operatorname{dot(alpha, x)\n    x = roll(x)\n    x[0] = lr\n    return x, y\n```\n\n\n## A.8.3 Canonization of State-Space Models\n\nThe companion canonical form discussed in Section A. 5 is the ideal representation to deploy SSM-based convolutional layers: $i$ ) it comes with a $\\mathcal{O}(d)$ fast recurrence and $i i)$ allows to swiftly switch between time and frequency domains with a direct mapping between state-space matrices and coefficient of the transfer function (which in turn allow $\\tilde{\\mathcal{O}}(L)$ fast convolutions). Aside from [50], which directly parametrizes S4 layers in companion canonical form, all the other parameterizations [12, 6, 32, 17, 33] can be converted (canonized), under mild assumptions. Lemma A. 8 (Canonization of SSMs). Any state-space model (2.2) with proper transfer function can be converted in companion canonical form.",
    "hyenadistill-55": "Proof.",
    "hyenadistill-56": "The result can be proved following the two-step conversion process. 1. Get the coefficients of the transfer function: Given the original state-space matrices $\\left(\\mathrm{A}, \\mathrm{B}, \\mathrm{C}, h_{0}\\right)$, the transfer function is given by $H(z)=\\mathrm{C}(z \\mathrm{I}-\\mathrm{A})^{-1} \\mathrm{~B}+h_{0}$. A proper rational function has the form $H(z)=q(z) / p(z)$ where the numerator $q(z)$ has coefficients $b=\\left(b_{n}\\right)_{n=0}^{d}$ and the denominator has coefficients $a=\\left(a_{n}\\right)_{n=0}^{d}\\left(a_{0}=1\\right.$ since $p$ is monic). As shown in Section A.6, the coefficients of the transfer function can be extracted in closed-form as $b=\\operatorname{poly}(\\operatorname{eig}(\\mathrm{A}-\\mathrm{BC}))+\\operatorname{poly}(\\operatorname{eig}(\\mathrm{A}))\\left(1-h_{0}\\right)$ and $a=\\operatorname{poly}(\\operatorname{eig}(\\mathrm{A}))^{14} ;$\n2. Construct companion matrices Given the coefficients $a$ and $b$ a new set of canonical state-space matrices which realize the transfer function can be obtained following the recipe of Section A.5. The resulting companion SSM is equivalent the the original one since they share the same transfer function. [^6]\n## B LaughingHyena: Further Details\n\n## B. 1 Parametrization of Modal Interpolators\n\nComplex-conjugate states Assuming even distilling dimension $d$, we pick poles $\\lambda_{n}$ and residues $\\mathbb{R}_{n}$ in complex-conjugate pairs:\n\n$$\n\\begin{aligned}\n& \\mathrm{A}=\\operatorname{diag}\\left(\\lambda_{1}, \\cdots, \\lambda_{d / 2}, \\lambda_{1}^{*}, \\cdots, \\lambda_{d / 2}^{*}\\right) \\\\\n& \\mathrm{C}=\\frac{1}{2}\\left[R_{1}, \\cdots, R_{d / 2}, R_{1}^{*}, \\cdots, R_{d / 2}^{*}\\right]\n\\end{aligned}\n$$\n\nwhich allow partitioning the state-space matrices as\n\n$$\n\\mathrm{A}=\\left[\\begin{array}{ll}\n\\lambda & \\\\\n& \\lambda^{*}\n\\end{array}\\right], \\quad \\mathrm{C}=\\frac{1}{2}\\left[\\begin{array}{ll}\nR & R^{*}\n\\end{array}\\right]\n$$\n\nwhere\n\n$$\n\\lambda=\\operatorname{diag}\\left(\\lambda_{1}, \\cdots, \\lambda_{d / 2}\\right) \\text { and } R=\\left[R_{1}, \\cdots, R_{d / 2}\\right]\n$$\n\nIf we also partition the state as $x=(\\bar{x}, \\tilde{x}), \\bar{x}, \\tilde{x} \\in \\mathbb{C}^{d / 2}$, the resulting recurrence has the form\n\n$$\n\\begin{aligned}\n& \\bar{x}_{t+1}=\\lambda \\bar{x}_{t}+\\mathbb{1}_{d / 2} u_{t} \\\\\n& \\tilde{x}_{t+1}=\\lambda^{*} \\tilde{x}_{t}+\\mathbb{1}_{d / 2} u_{t}\n\\end{aligned}\n$$\n\nWe have\n\n$$\n\\bar{x}_{t}=\\lambda^{t} \\bar{x}_{0}+\\sum_{j=0}^{t-1} \\lambda^{t-j-1} \\mathbb{1}_{d / 2} u_{t}, \\quad \\tilde{x}_{t}=\\left[\\lambda^{*}\\right]^{t} \\tilde{x}_{0}+\\sum_{j=0}^{t-1}\\left[\\lambda^{*}\\right]^{t-j-1} \\mathbb{1}_{d / 2} u_{t}\n$$\n\nThus, if $\\tilde{x}_{0}=\\bar{x}_{0}^{*}$, then $\\tilde{x}_{t}=\\bar{x}_{t}^{*}$ for all $t>0$. Hence, at inference time we only need to propagate forward half of the state - say $\\bar{x}-$ and then compute the output as\n\n$$\n\\begin{aligned}\ny_{t} & =\\mathrm{D} u_{t}+\\frac{1}{2}\\left(R \\bar{x}_{t}+R^{*} \\bar{x}_{t}^{*}\\right) \\\\\n& =\\mathrm{D} u_{t}+\\mathfrak{R}\\left\\{R \\bar{x}_{t}\\right\\} \\\\\n& =\\mathrm{D} u_{t}+\\mathfrak{R}\\{R\\} \\mathfrak{R}\\left\\{\\bar{x}_{t}\\right\\}-\\Im\\{R\\} \\Im\\left\\{\\bar{x}_{t}\\right\\}\n\\end{aligned}\n$$\n\nThis parametrization allows to update only half of the state, reducing the time and memory cost compared to an unconstrained linear system with complex coefficients. However, the implicitly achieved realness of the output (assuming $\\mathrm{D}=h_{0} \\in \\mathbb{R}$ and $u_{t} \\in \\mathbb{R}$ ) comes at a cost of expressivity: such a system is equivalent to an unconstrained complex linear system of dimension $d / 2$ of which we only keep the real part of the output. Poles and residues For the modal interpolation, the parametrization is analogous to the one of a diagonal state space model [33]. Poles $\\lambda_{n}$ and residues $R_{n}$ need both to be complex numbers. In [33] the authors suggest parametrizing real and imaginary components of $B$ and $C$ matrices while representing the eigenvalues $\\lambda_{n}$ in polar form, $\\lambda_{n}=r_{n} e^{i \\alpha_{n}}$ with $r_{n}$ and $\\alpha_{n}$ being themselves exponential functions of the actual trainable parameters, $r_{n}=e^{-e^{\\nu_{n}}}, \\alpha_{n}=e^{\\zeta_{n}}$ leading to\n\n$$\n\\lambda_{n}=\\exp \\left\\{-\\exp \\left\\{\\nu_{n}\\right\\}+i \\exp \\left\\{\\zeta_{n}\\right\\}\\right\\}, \\quad \\nu_{n}, \\zeta_{n} \\in \\mathbb{R}\n$$\n\nThis ensures stability of the poles $\\left|\\lambda_{n}\\right|<1$ and positive-only phases $\\alpha_{n}$. For the purpose of distillation we propose a simplified parametrization as follows:\n\n1. We only parametrize the $C$ vector. Parametrizing both $B$ and $C$ is redundant and increases the computational cost of performing each step of the recurrence. The residues $R_{n}$ correspond in fact to $R_{n}=\\mathrm{C}_{n} \\mathrm{~B}_{n}$ of a diagonal state space model. Setting $\\mathrm{B}=\\mathbb{1}_{d}$ saves parameters without harming expressivity. Further if B is different from $\\mathbb{1}_{k}$ it needs to be multiplied to $u_{t}$ at each recurrence step. $\\mathrm{C}_{n}=R_{n}=\\mathfrak{R}\\left[R_{n}\\right]+i \\Im\\left[R_{n}\\right]$ and $\\mathfrak{R}\\left[R_{n}\\right], \\Im\\left[R_{n}\\right]$ are the trainable parameters of the residue.",
    "hyenadistill-57": "2. For the purpose of distillation we have no benefit in forcing the eigenvalues of the model to be stable, i.e. constrained to lie strictly inside the unit circle. Instead, such constrain may actually harm the expressivity of the approximant. We choose the the simpler parametrization $\\lambda_{n}=r_{n} e^{i \\alpha_{n}}, r_{n}, \\alpha_{n} \\in \\mathbb{R}$. ## B. 2 Distillation as Rational Interpolation\n\nDistillation as rational interpolation Approximating a filter with an SSM can be thus achieved by fitting a proper rational function to the (truncated) transfer function of the original filter $H_{L}(z):=\\sum_{t=0}^{L} h_{t} z^{-t}$. That is,\n\n$$\n\\text { Find } a, b \\text { such that } h_{0}+h_{1} z^{-1}+\\cdots+h_{L} z^{-L} \\approx h_{0}+Q_{b}(z) / P_{a}(z)\n$$\n\nA modern ${ }^{15}$ way to solve this problem by $\\mathcal{H}_{2}$ error minimization via gradient descent ${ }^{16}$. We can use the Fast Fourier Transform (FFT) to evaluate both the target and distilled transfer functions and solve:\n\n$$\n\\min _{a, b \\in \\mathbb{R}^{d}} \\sum_{k=0}^{L}\\left|\\mathrm{FFT}_{L}[h]_{k}-h_{0}-\\mathrm{FFT}_{L}[b]_{k} / \\mathrm{FFT}_{L}[a]_{k}\\right|^{2}\n$$\n\nTo ensure stability of the distilled filters and well-conditioned gradient descent dynamics, the roots of the denominator polynomial must strictly lie inside the unit circle $(\\rho(\\mathrm{A})<1$ ). This, in turn, requires constraining the coefficients $a$ into the region $\\{a: \\operatorname{poly}(a)$ is stable $\\}$ which is by itself an open research problem [52,53]. Experimentally, we observe that standard coefficient normalization techniques overly restrict the parameters space and lead to poor distillation performances at reasonable order. [^7]\n## C Proofs\n\n## C. 1 Proof of Lemma 2.1\n\nGenerating $K$ tokens with a long convolution layer (2.1) from a length- $T$ prompt has time complexity $\\mathcal{O}\\left(T \\log _{2} T+T K+K^{2}\\right)$ and requires $\\mathcal{O}(L)$ memory. Proof. We compute the time complexity memory of a length- $T$ prompt processing (pre-filling) and subsequent auto-regressive decoding of $K$ tokens. The auto-regressive generation of long convolution computes the next token as by\n\n$$\nt=T, \\ldots, T+K-1 \\Rightarrow y_{t}=\\sum_{j=0}^{t-1} h_{t-j} y_{j}\n$$\n\nThe pre-filling step is needed to prime this recurrence by computing the first $T$ outputs till $y_{T-1}$ from the length- $T$ prompt $u$. This is just a convolution between two length- $T$ signal and requires $\\mathcal{O}\\left(T \\log _{2} T\\right)$ time and linear memory. The auto-regressive decoding of $K$ tokens requires $K$ steps (C.1) with the length of the sequences increasing by 1 at each step. Thus we have a total asymptotic complexity of\n\n$$\n\\sum_{k=0}^{K-1}(T+k)=T K+\\frac{1}{2} K(K+1)\n$$\n\nand requires at worst $(k=K-1)$ to store the length $T+K=L$ generated output sequence, i.e. $\\mathcal{O}(L)$ memory. In the limit we thus have a total time complexity of $\\mathcal{O}\\left(T \\log _{2} T+T K+K^{2}\\right)$ and $\\mathcal{O}(L)$ memory. ## C. 2 Proof of Lemma 2.2\n\nGenerating $K$ tokens with a SSM (2.2) from a length- $T$ prompt has time complexity $\\mathcal{O}\\left(T \\log _{2} T+d K\\right)$ and requires $\\mathcal{O}(d)$ memory. Proof. In autoregressive mode, the cost of generating one token is the cost of evaluating the state recurrence (2.2). Each step then requires $\\mathcal{O}(d)$ time and memory for the class of SSMs considered in this work (see Lemma A.8). Hence, generating $K$ tokens costs $\\mathcal{O}(d K)$ time and constant $\\mathcal{O}(d)$ memory (we only need to store the current state). The recurrence is initialized for autoregressive generation with the post-prompt state $x_{T-1}$ and output $y_{T-1}$. The latter can be recovered in linear time and memory $\\mathcal{O}(T)$ by definition $y_{T-1}=\\sum_{j=0}^{T-1} h_{t-j} u_{j}$ (assuming to have the impulse response $h$ available) and state $x_{T-1}$ in $\\mathcal{O}(d T)$ time and $d$ memory through the recurrence. The overall asymptotic cost is therefore $\\mathcal{O}(d L)$ time and $\\mathcal{O}(d)$ memory. Note that, for prompts and SSMs of practical sizes we usually have $d>\\log _{2} T$. In such a case the state $x_{T-1}$ can be computed in $T \\log _{2} T$ time rather than $d T$ by Proposition 3.2. ## C. 3 Proof of Lemma 2.3\n\nGenerating $K$ tokens with self-attention from a length- $T$ prompt has time complexity $\\mathcal{O}\\left(T^{2}+T K+K^{2}\\right)$ and requires $\\mathcal{O}(L)$ memory. Proof. The proof is identical to the one of Lemma 2.1, with the only difference of a quadratic asymptotic cost $\\mathcal{O}\\left(T^{2}\\right)$ to process the prompt obtain the $k v$ cache. Self-attention suffers with long contexts: it is significantly more expensive in prefilling than long convolutions and SSMs due to its quadratic cost. Nonetheless, in autoregressive mode, self-attention reaches the same overall asymptotic complexity $\\mathcal{O}\\left(T K+K^{2}\\right)$ as long convolutions (with the memory overhead of having to cache $k$ and $v$ ). ## C. 4 Proof of Proposition 3.1\n\nIf A has semi-simple eigenvalues $\\lambda_{n} \\in \\mathbb{C}$, then the transfer function of the system can be decomposed as $\\hat{H}(z)=\\sum_{n=1}^{d} R_{n} /\\left(z-\\lambda_{n}\\right)$ where $R_{n} \\in \\mathbb{C}$ is the residue associated with the pole $\\lambda_{n}$. Proof. If A is semi-simple, then it is diagonalized by a basis V of eigenvectors; it admits an eigenvalue decomposition $\\operatorname{diag}(\\lambda)=\\operatorname{VAV}^{-1}$ where $\\lambda=\\left(\\lambda_{1}, \\ldots, \\lambda_{d}\\right) \\in \\mathbb{C}^{d}$ contains the eigenvalues of A . Projecting the state onto the basis of eigenvectors, $s:=\\mathrm{V} x$, the state space model is is transformed into modal form:\n\n$$\n\\begin{aligned}\ns_{t+1} & =\\mathrm{VAV}^{-1} s_{t}+\\mathrm{VB} u_{t} \\\\\nu_{t} & =\\mathrm{CV}^{-1} s_{t}\n\\end{aligned} \\Leftrightarrow \\begin{aligned}\ns_{t+1} & =\\operatorname{diag}(\\lambda) s_{t}+\\tilde{\\mathrm{B}} u_{t} \\\\\nu_{t} & =\\tilde{\\mathrm{C}}_{s_{t}}\n\\end{aligned}\n$$\n\nwhere $\\tilde{\\mathrm{B}}:=\\mathrm{VB}=\\left(\\tilde{b}_{n}\\right)_{n=1}^{d}$ and $\\tilde{\\mathrm{C}}:=\\mathrm{CV}^{-1}=\\left(\\tilde{c}_{n}\\right)_{n=1}^{d}$. In modal form, the state equations are decoupled, i.e. $$\n\\begin{aligned}\ns_{t+1}^{n} & =\\lambda_{n} s_{t}^{n}+\\tilde{b}_{n} u_{t} \\\\\ny_{t} & =\\sum_{n=1}^{d} \\tilde{c}_{n} s_{t}^{n}\n\\end{aligned}\n$$\n\nTaking the $z$-transform of the output equation and each state equation yields\n\n$$\n\\begin{aligned}\nS_{n}(z) & =\\mathcal{Z}\\left[s^{n}\\right](z)=\\frac{\\tilde{b}_{n}}{z-\\lambda_{n}} U(z) \\quad n=1, \\ldots, d \\\\\nY(z) & =\\sum_{n=1}^{d} \\tilde{c}_{n} S_{n}(z)\n\\end{aligned}\n$$\n\nThus, the overall transfer function is\n\n$$\nH(z)=\\frac{Y(z)}{U(z)}=\\sum_{n=1}^{d} \\frac{\\tilde{b}_{n} \\tilde{c}_{n}}{z-\\lambda_{n}}\n$$\n\nLetting $R_{n}=\\tilde{b}_{n} \\tilde{c}_{n}$, proves the result. ## C. 5 Proof of Lemma 3.1\n\nThe distilled filter $\\hat{h}$ in modal form (3.2) can be computed in $\\mathcal{O}(d L)$ time from its modal form and in $\\tilde{\\mathcal{O}}(L)$ from its rational form. Recalling (3.2), $\\hat{h}_{t}=\\sum_{n=1}^{d} R_{n} \\lambda_{n}^{t-1}, R_{n}, \\lambda_{n} \\in \\mathbb{C}, t>0$, the $\\mathcal{O}(d L)$ complexity of the impulse response is apparent: for each of the $t=1, \\ldots, L, \\hat{h}_{t}$ can be computed in $\\mathcal{O}(d)$ time.",
    "hyenadistill-58": "The $\\tilde{\\mathcal{O}}(L)$ cost from the rational form follows by Lemma A. 6 . ## C. 6 Proof of Theorem 3.2\n\nLet $h$ be a length- $L$ filter, $\\hat{h}$ a distilled filter of order $d<L$ and let $S_{L}, \\hat{S}_{L}$ be the respective Hankel matrices. Then $\\inf _{\\hat{\\mathrm{S}}_{L}}\\left\\|\\mathrm{~S}_{L}-\\hat{\\mathrm{S}}_{L}\\right\\|_{2}=\\sigma_{d}$. Proof. The theorem characterizes the best-case scenario in terms of approximation error of the distilled SSMs or a certain order $d$ where it is clear that $\\operatorname{rank} \\hat{\\mathrm{S}} \\leq d$. This theorem is a direct application of the Adamyan-Arov-Krein (AAK) theory of infinite Hankel operators [7]. Let $\\hat{S}_{L}^{*}=\\arg \\inf _{\\hat{S}_{L}}\\left\\|\\mathrm{~S}_{L}-\\hat{\\mathrm{S}}_{L}\\right\\|_{2}$; the AAK theorem says that every causal system can be optimally approximated by another causal system of lower dimension. Optimal here means\n\n$$\n\\inf \\left\\|\\mathrm{S}_{L}-\\hat{S}_{L}^{*}\\right\\|=\\inf \\left\\|\\mathrm{S}_{L}-\\mathrm{K}\\right\\|\n$$\n\nwhere the first infimum is taken over all Hankel matrices $S_{L}^{*}$ and the second over all arbitrary matrices K (see [19, Chapter 8] and [54] for further details and references). ## C. 7 Proof of Proposition\n\nThe filter (3.2) has a state space matrices $\\mathrm{A}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{d}\\right) \\in \\mathbb{C}^{d \\times d}, \\mathrm{~B}=(1, \\ldots, 1)^{\\top} \\in \\mathbb{C}^{d \\times 1}, \\mathrm{C}=$ $\\left(R_{1}, \\ldots, R_{d}\\right) \\in \\mathbb{C}^{1 \\times d}, \\mathrm{D}=h_{0}$ whose step can be evaluated in $\\mathcal{O}(d)$ time and memory. D is set to $h_{0}$ by default. The result is proven showing that (3.2) can be written in the form $\\hat{h}_{t}=\\mathrm{CA}^{t-1} \\mathrm{~B}$ for $t>0$. If we choose $\\mathrm{A}=\\operatorname{diag}(\\lambda)$ then the impulse response becomes\n\n$$\n\\hat{h}_{t}=\\sum_{n=1}^{d} R_{n} \\lambda_{n}^{t-1}=\\mathrm{C}[\\operatorname{diag}(\\lambda)]^{t-1} \\mathrm{~B}=\\sum_{n=1}^{d} \\mathrm{C}_{n} \\mathrm{~B}_{n} \\lambda_{n}^{t-1}\n$$\n\nThe choice $\\mathrm{B}_{n}=1$ for all $n=1, \\ldots, d, \\mathrm{~B}=\\mathbb{1}_{d}$ and $\\mathrm{C}_{n}=R_{n}$ finalizes a modal canonical state-space realization of the distilled filter. The $\\mathcal{O}(d)$ time complexity of the corresponding recurrent step is guaranteed by the decoupling of each state equation from another,\n\n$$\n\\begin{aligned}\nx_{t+1}^{n} & =\\lambda_{n} x_{t}^{n}+u_{t} \\quad n=1, \\ldots, d \\\\\ny_{t} & =\\sum_{n=1}^{d} R_{n} x_{t}^{n}+h_{0} u_{t}\n\\end{aligned}\n$$\n\nEach of the $d$ state equations can be computed (in parallel) in $\\mathcal{O}(1)$ time. The output equation is a dot product requiring $d$ multiplications and $d$ additions, hence the $\\mathcal{O}(d)$ time compexity of the recurrence. ## C. 8 Proof of Proposition 3.2\n\n$x_{T}=\\left(v_{T}, \\ldots, v_{T-d}\\right)$ where $v=g * u$ and $g$ is the filter whose transfer function is $1 / \\operatorname{den}(\\hat{H})(z)$ and can be evaluated in $\\tilde{\\mathcal{O}}(T)$. Without loss of generality, let us assume to have converted the distilled filter in canonical form (i.e. we have unrestricted access to the coefficients of the rational transfer function) and let $D=0$. We use the notation of Section A.5. In $z$-domain, the state-to-input relation is given by\n\n$$\nY(z)=\\mathbf{C} X(z)=\\left[\\begin{array}{lll}\n\\beta_{1} & \\cdots & \\beta_{d}\n\\end{array}\\right] X(z)\n$$\n\nOn the other hand $Y(z)=\\hat{H}(z) U(z)=q(z) / p(z) U(z)$. Therefore,\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{lll}\n\\beta_{1} & \\cdots & \\beta_{d}\n\\end{array}\\right] X(z)=\\frac{q(z)}{p(z)} U(z) } \\\\\n\\Leftrightarrow & {\\left[\\begin{array}{lll}\n\\beta_{1} & \\cdots & \\beta_{d}\n\\end{array}\\right] X(z)=\\left[\\begin{array}{lll}\n\\beta_{1} & \\cdots & \\beta_{d}\n\\end{array}\\right]\\left[\\begin{array}{c}\nz^{-1} \\\\\n\\vdots \\\\\nz^{-d}\n\\end{array}\\right] \\frac{1}{p(z)} U(z) } \\\\\n\\Leftrightarrow & X(z)=\\left[\\begin{array}{c}\nz^{-1} \\\\\n\\vdots \\\\\nz^{-d}\n\\end{array}\\right] \\frac{1}{p(z)} U(z)\n\\end{aligned}\n$$\n\nLet $V(z)=U(z) / p(z)$. From the shift property of the $z$-transform it holds,\n\n$$\n\\mathcal{Z}\\{x\\}(z)=X(z)=\\left[\\begin{array}{c}\nz^{-1} \\\\\n\\vdots \\\\\nz^{-d}\n\\end{array}\\right] V(z) \\quad \\Leftrightarrow \\quad x_{t}=\\left(v_{t-1}, v_{t-2}, \\cdots, v_{t-d}\\right) \\quad \\forall t>0\n$$\n\n$v$ can be obtained in $\\tilde{\\mathcal{O}}(L)$ time via an FFT-convolution of the input $u$ and $g$, the filter resulting from inverse transforming $1 / p(z)$.",
    "hyenadistill-59": "The proof is convoluded setting $t=L$\n\n## C. 9 Proof of Theorem 4.1\n\nNotation. We will be denoting the all 1 row vector of size $k$, given by $\\left[\\begin{array}{lllll}1 & 1 & \\ldots & 1 & 1\\end{array}\\right]$, and the all 0 row vector of size $k$, given by $\\left[\\begin{array}{lllll}0 & 0 & \\ldots & 0 & 0\\end{array}\\right]$, as $\\mathbf{1}^{k}$ and $\\mathbf{0}^{k}$, respectively. We will also construe the standard basis vector $e_{i}$ as a column vector in these notes. Next, we will adhere to the following matrix indexing convention: $\\mathrm{A}_{i j}$ is the entry in the $i$ th row and the $j$ th column, $\\mathrm{A}[i,:] \\in \\mathbb{F}^{1 \\times n}$ denotes the $i$ th row, and $\\mathrm{A}[:, j] \\in \\mathbb{F}^{m \\times 1}$ denotes the $j$ th column of $\\mathrm{A} \\in \\mathbb{F}^{m \\times n}$. Here, we also use $\\mathbb{D}^{m \\times n} \\in \\mathbb{R}^{m \\times n}$ and $\\mathrm{I}_{n}$ to denote the matrix of all zeros and the identity matrix of dimension $n$, respectively. Moreover, we extend the outer product between two vectors to a tensor product using the symbol $\\otimes$, the computation of which is carried out batch-wise with some dimension of one or both of the input tensors. Finally, we express the binary encoding of $i \\in[n]$ in a row vector form, given by $\\mathrm{B}_{i} \\in \\mathbb{Z}_{2}^{P_{n}}$, where $P_{n}$ is the closest power of 2 to $n$. Language and Model Description. The language $\\Lambda$ has $s$ keys and $s$ values: $L_{K}:=\\left\\{k_{1}, \\ldots, k_{s}\\right\\}, L_{V}:=$ $\\left\\{v_{1}, \\ldots, v_{s}\\right\\}$. Formally, the language $\\Lambda$ consists of sequences $x \\in\\left(L_{K} \\times L_{V}\\right)^{s} \\times L_{K}$, where there is an associated mapping $f_{x}: L_{K} \\rightarrow L_{V}$. For each sequence, the odd indices in [ $L$ ] belong to $L_{K}$, for $x_{1}, x_{3}, \\ldots, x_{L}$, and we define\n\n$$\nx_{2 \\cdot i}=f_{x}\\left(x_{2 \\cdot i-1}\\right)\n$$\n\nThe last item $x_{L} \\in\\left\\{x_{1}, x_{3}, \\ldots, x_{L-1}\\right\\}$, called the query, must be one of the keys that has appeared in $x$ already. Our goal is to produce $f_{x}\\left(x_{L}\\right)$ at the end of the sequence, which we refer as the associated value. This problem is termed as the associative recall problem [55]. We will now outline the Hyena layer [2] with multiple heads as follows. ```\nAlgorithm 1 Hyena\nRequire: Input sequence \\(u \\in \\mathbb{R}^{L \\times D}\\) from the previous layer, long convolution filter \\(\\mathrm{T}_{h}\\), number of heads\n    \\(M\\). \\(q^{m}, k^{m}, v^{m} \\leftarrow \\operatorname{Projection}(u)\\) for \\(m \\in[M]\\). for \\(m=1, \\ldots, M\\) do\n        Perform the outer product \\(z^{m} \\leftarrow k^{m} \\otimes v^{m} \\in \\mathbb{R}^{L \\times N \\times N}\\), where \\(N:=D / M\\). Apply the convolution independently and compute \\(y_{t}^{m} \\leftarrow \\mathrm{T}_{h}\\left(z_{t}^{m}\\right) q_{t}^{m} \\in \\mathbb{R}^{L \\times N}\\)\n    Average the output \\(\\bar{y} \\leftarrow\\left(\\sum_{m}\\right) y^{m} / M\\)\n    Retrieve the value \\(f\\left(k_{L}\\right)\\) of the key \\(k_{L}\\) from \\(\\bar{y}[L,:]\\). ```\n\nIn order to prove Theorem 4.1, we need the following technical statement concerning sparse recovery of a heavy-hitter. Proposition C. 1 (Heavy-Hitter Recovery). Let $x \\in \\mathbb{R}^{s}$ be a vector with one entry bounded by $1 \\pm$ $\\frac{1}{3 \\sqrt[4]{s}}$-referred as the heavy-hitter - and the rest of the entries bounded by $\\pm \\frac{1}{3 \\sqrt[4]{s}}$. Then, there exists a matrix $S^{(m)} \\in \\mathbb{R}^{s \\times O(\\sqrt{s} \\log s)}$ such that the position of the heavy-hitter in $x$ can be inferred from the average of $M$ measurements with $\\mathrm{S}^{(m)}$ given by $\\left(\\sum_{m} x \\mathrm{~S}^{(m)}\\right) / M$ with probability of error $\\leq \\frac{1}{s}$. Before presenting the proof of Proposition C.1, we use it to prove Theorem 4.1 as follows. Proof of Theorem 4.1. We take $D=O\\left(\\sqrt{s} \\log ^{2} s\\right)$ and $M=243 \\cdot \\log s$ so that $N=O(\\sqrt{s} \\log s)$ and use the same projections and filters for each head. We will start by describing the projections of the input. To this end, let $E:[L] \\rightarrow 2 s$ define a map from the row indices of $u$ to the keys $k_{i}$ and values $f_{x}\\left(k_{i}\\right)$ given by\n\n$$\nE(t)= \\begin{cases}i, & t \\text { odd, } x_{t}=k_{i} \\\\ i+s, & t \\text { even, } x_{t-1}=k_{i}\\end{cases}\n$$\n\nHere, we note that we also have\n\n$$\nE(t)=E(t-1)+s, \\quad t \\text { even }\n$$\n\nas the even indices are defined as $x_{t}=f_{x}\\left(x_{t-1}\\right)$ for $t$ even (C.3), whence $x_{t-1} \\in L_{K}$ as $t-1$ is odd. Next, we can separate the keys $q$, queries $q$ and values $v$ from the input sequence $u$. For keys and queries, we will be using the Johnson-Lindenstrauss embedding [56]. We state its guarantee here. For a set of points $P \\subseteq \\mathbb{R}^{s}$, let $\\epsilon, \\delta>0$ with $k \\geq 2 \\ln \\left(\\frac{2 s}{\\delta}\\right) / \\epsilon^{2}$, and $f: \\mathbb{R}^{s} \\rightarrow \\mathbb{R}^{k}$ be the randomly constructed linear map from [56], then with probability of error $\\leq \\delta$, we have\n\n$$\n|\\langle f(x), f(y)\\rangle-\\langle x, y\\rangle| \\leq \\epsilon\n$$\n\nfor all $x, y \\in P$. More precisely, we take $\\mathrm{R} \\in \\mathbb{R}^{O(\\sqrt{s} \\log s) \\times s}$ to be the matrix representation of $f$ with $\\epsilon:=\\frac{1}{3 \\sqrt[4]{s}}$ and $\\delta:=\\frac{1}{s^{c}}$ for some $c>1$ so that $\\mathrm{R}[:, i]=f\\left(e_{i}\\right)$. Thus, we define\n\n$$\nq^{m}[t,:]= \\begin{cases}\\mathrm{R}[:, E(t-1)], & E(t-1) \\leq s \\\\ 0, & \\text { otherwise }\\end{cases}\n$$\n\nFor values, we use the heavy-hitter recovery matrix as described in Proposition C. 1 so that we have\n\n$$\nv^{m}[t,:]= \\begin{cases}\\mathrm{S}^{(m)}[E(t),:], & E(t)>s \\\\ 0, & \\text { otherwise }\\end{cases}\n$$\n\nFurther, using 1DConv (equivalently, in terms of polynomials, $h(X):=X$ ), we can shift the queries to get the projection for keys $k$ so that we have $k^{m}[t,:]=q^{m}[t-1,:]$\n\nThe Hyena filters, along with the specific convolution being performed by $\\mathrm{T}_{h}$, are specifically described in terms of polynomial multiplications, for all $m \\in 1, \\ldots, M$, as follows. $$\n\\mathrm{T}_{h}(X):=\\sum_{i=0}^{L} X^{i}\n$$\n\nHere, we note that $\\mathrm{T}_{h}(u)$ takes the cumulative sum over the input. That is, for all $i$, we have\n\n$$\n\\mathrm{T}_{h}(u)[i,:]=\\sum_{j=0}^{i} u[j,:]\n$$\n\nWe will now compute $z^{m}$ as follows\n\n$$\nz^{m}=k^{m} \\otimes v^{m}\n$$\n\nFurther, applying the convolution, we get\n\n$$\n\\mathrm{T}_{h}\\left(z_{t}^{m}\\right)=\\sum_{i=0}^{t} k^{m}[i,:] \\otimes v^{m}[i,:]\n$$\n\nFor inference, it suffices to show that the last row of the output $y$ recovers the output with high probability. Indeed, let $t^{\\prime} \\in[L]$ denote the row index of the value associated to the query such that the corresponding key has the following relation\n\n$$\nu_{t^{\\prime}-1}=u_{L}\n$$\n\nFinally, we multiply by the query $q$ across $L$. Specifically, we now look at the computation of the $L$ th row of $y$ :\n\n$$\n\\begin{aligned}\ny^{m}[L,:] & =\\left(\\sum_{t=0}^{L} k^{m}[t,:] \\otimes v^{m}[t,:]\\right) q^{m}[t,:] \\\\\n& =\\sum_{t=0}^{L}\\left(q^{m}[L,:]^{\\top} k^{m}[t,:]\\right) v^{m}[t,:] \\\\\n& =\\sum_{t=0}^{L}\\left(q^{m}\\left[t^{\\prime}-1,:\\right]^{\\top} q^{m}[t-1,:]\\right) v^{m}[t,:] \\\\\n& =\\sum_{\\substack{t \\in[L]}}^{t \\text { even }}\\left(\\left(\\mathrm{R}\\left[:, E\\left(t^{\\prime}-1\\right)\\right]\\right)^{\\top} \\mathrm{R}[:, E(t-1)]\\right) \\mathrm{S}^{(m)}[E(t),:] \\\\\n& =\\sum_{\\substack{t \\in[L] \\\\\nt \\text { even }}}^{L}\\left(\\operatorname{Re}_{E\\left(t^{\\prime}-1\\right)}^{\\top} \\operatorname{Re}_{E(t-1)}\\right) \\mathrm{S}^{(m)}[E(t),:]\n\\end{aligned}\n$$\n\nHere, we are using the fact that $k^{m}\\left[t^{\\prime}-1,:\\right]=q^{m}[L,:]$ due to (C.8) in (C.9). We then change the indexing from (C.9) and (C.11) by observing that all the odd entries corresponding to values are zeroed out in $\\mathbf{K}$ (cf. C.6). Finally, we simply substitute (C.6) and (C.7) in (C.10) and (C.11), respectively. Next, we define $x \\in \\mathbb{R}^{s+1}$ with\n\n$$\nx_{j}:=\\operatorname{Re} e_{E\\left(t^{\\prime}-1\\right)}^{\\top} \\operatorname{Re} e_{j}\n$$\n\nwhere $j=t-1$ with $t \\in[L]$ and $t$ even. Here, $x \\in \\mathbb{R}^{n+1}$ is a vector of size $s+1$ as there are $s+1$ such even numbers in $[L]$. Note that $x$ is the vector with a heavy-hitter from Proposition C.1. To see this, observe that we have $\\left|x_{E\\left(t^{\\prime}-1\\right)}-1\\right| \\leq \\frac{1}{3 \\sqrt[4]{s}}$ and $\\left|x_{j}\\right| \\leq \\frac{1}{3 \\sqrt[4]{s}}$ for all $j \\neq E\\left(t^{\\prime}-1\\right)$. Using (C.11), with probability $\\geq 1-\\frac{1}{s^{c}}$, we then have\n\n$$\ny^{m}[L,:]=x \\mathrm{~S}^{(m)}\n$$\n\nBy Proposition C.1, we can then infer the position of the key at $t^{\\prime}$ with probability of error $\\frac{1}{s}$. By the union bound, we can then retrieve the corresponding value with probability at least $1-\\left(\\frac{1}{s}+\\frac{1}{s^{c}}\\right)$. We will now prove C. 1 as follows. Proof of C.1. We will assume that $s$ is a power of 2 for the sake of simplicity. We first specify how we will construct such an $\\mathrm{S}^{(m)} \\in \\mathbb{R}^{s \\times O(\\sqrt{s} \\log s)}$. Let $h:[s] \\rightarrow[\\sqrt{s}]$ be a hash function. We define $\\tilde{S} \\in \\mathbb{R}^{s \\times \\sqrt{s}}$ to be\n\n$$\n\\tilde{\\mathrm{S}}[:, i]=\\sum_{j: h(j)=i} e_{j}\n$$\n\nThat is, each column $i$ of $\\tilde{S}$ is the sum of the standard basis vectors $\\mathbf{e}_{j}$ such that $j$ is mapped by $h$ to $i$. In other words, the locations of the non-zero entries in column $i$ correspond to the preimage of $i$ under $h$. We then multiply each non-zero entry of $\\tilde{S}$ independently at random by $\\pm 1$. Next, we replace the $k$ th row in $\\tilde{S}$ by multiplying all non-zero $\\pm 1$ entries at index $i$ with the binary representation of $i$ to get a matrix $\\mathrm{S}^{(m)} \\in \\mathbb{R}^{s \\times(\\sqrt{s} \\times \\log s)}$. That is, for a non-zero entry at index $i$ in row $k$, we replace the $i$ th entry with $\\pm 1 \\cdot \\mathbf{B}_{i}$. Note here that each column still has at most $\\sqrt{s}$ non-zero entries. Finally, we stack $243 \\cdot \\log s$-many copies of $\\mathrm{S}^{(m)}$ as heads so that each copy produces independent measurements $x \\mathbf{S}^{(m)}$. Here, we want to emphasize that each such copy uses fresh randomness for multiplying the non-zero entry of $\\tilde{S}$ independently at random by $\\pm 1$. Now, we will show that the average of the measurements with matrices $\\mathrm{S}^{(m)} \\in \\mathbb{R}^{s \\times \\sqrt{s} \\log s}$ can locate the heavy-hitter in $x$, where $x$ is the vector of inner products from (C.12). For this purpose, we first specify the algorithm for decoding the heavy-hitter. ```\nAlgorithm 2 Decoder\nRequire: The vector \\(y\\) such that \\(y=x \\mathrm{~S}\\). 1: Split \\(y\\) into \\(243 \\cdot \\log s\\) blocks \\(y^{(m)} \\in \\mathbb{R}^{\\sqrt{s} \\log s}\\), each of which is a result of multiplying \\(x\\) by \\(\\mathrm{S}^{(m)}, m \\in\\)\n        \\([243 \\cdot \\log n]\\). 2: Take the average \\(\\bar{y} \\leftarrow \\frac{1}{243 \\cdot \\log s} \\sum_{k} y^{(m)}\\). \\(\\boldsymbol{b} \\leftarrow I_{r}(|\\bar{y}|) \\in \\mathbb{R}^{\\sqrt{s} \\log s}\\), cf. (C.14). Retrieve \\(\\boldsymbol{b}\\) by isolating the binary representation of the position of the heavy-hitter in \\(x\\). ```\n\nHere, we define the function $I_{r}: \\mathbb{R}^{\\sqrt{s} \\log s} \\rightarrow \\mathbb{Z}_{2}^{\\sqrt{s} \\log s}$ to $[x \\mathrm{~S}]_{m}$ that rounds each entry of its input to the nearest integer:\n\n$$\nI_{r}\\left([x \\mathrm{~S}]_{m}\\right)=\\mathrm{S}^{(m)}[i,:]\n$$\n\nThat is, in both cases, we retrieve the row in $\\mathrm{S}^{(m)}$ that corresponds to the heavy-hitter in $x$. Since the rows in $\\mathrm{S}^{(m)}$ are distinct, we can also infer the position of the heavy-hitter in $x$ with probability 1. We now show that $y=x \\mathrm{~S}$, which consists of $243 \\cdot \\log n$ many independent copies of $y^{(m)}=x \\hat{\\mathrm{S}}^{(m)}$. Instead, notice that we can analyze $\\tilde{y}=x \\tilde{\\mathrm{S}}$ since each $\\hat{y}$ is the replacement of non-zero entries of $\\tilde{S}$ with the binary representation of their indices times $y$. We will drop the superscript for now to avoid cluttering the notation. We can then make the following claim:\n\n$$\n\\left|\\tilde{y}_{i}\\right|=1 \\pm O(\\epsilon \\cdot \\sqrt[4]{s}) \\text { and, for } j \\neq i,\\left|\\tilde{y}_{j}\\right|=O(\\epsilon \\cdot \\sqrt[4]{s})\n$$\n\nFor the above claim, we note that the first part follows from the latter as it suffices to show that all the non-zero heavy-hitters contribute $O(\\epsilon \\sqrt[4]{s})$ to the sum $\\tilde{y}_{i}=\\langle x, \\tilde{\\mathrm{S}}[:, i]\\rangle$. Since each column in $\\tilde{\\mathrm{S}}$ only interacts with $\\sqrt{s}$ sized sub-vector of $x$, each $\\tilde{y}_{j}$ for non-heavy hitters can be expressed as\n\n$$\n\\tilde{y}_{j}=\\left\\langle\\bar{x}, \\overline{\\mathrm{S}}_{j}\\right\\rangle \\text { with } \\bar{x}, \\overline{\\mathrm{S}}_{j} \\in \\mathbb{R}^{\\sqrt{s}}\n$$\n\nwhere $\\overline{\\mathrm{S}}_{j} \\in \\mathbb{R}^{\\sqrt{s}}$ contains the non-zero entries of $\\tilde{\\mathrm{S}}_{j}$ and $\\bar{x}$ is obtained by extracting the entries with corresponding indices from $x$. Here, we have $\\|\\bar{x}\\|_{2} \\leq \\epsilon \\sqrt[4]{s}$ since each entry associated with the non-heavy hitter is bounded as $x_{i} \\leq \\epsilon$, and thus, $\\|\\bar{x}\\|_{2}=\\sqrt{\\sum_{i} x_{i}^{2}} \\leq \\sqrt{\\sum_{i} \\epsilon^{2}}=\\sqrt{\\sqrt{s} \\cdot \\epsilon^{2}}=\\epsilon \\sqrt[4]{s}$. Consequently, as $\\tilde{\\mathrm{S}}_{j}$ is independently random $\\pm 1$, we then must have\n\n$$\n\\left|\\left\\langle\\bar{x}, \\bar{S}_{j}\\right\\rangle\\right| \\leq \\frac{1}{3}\n$$\n\nwith constant probability for $j \\neq i$. To see this, note that\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\langle\\bar{x}, \\bar{S}_{j}\\right\\rangle^{2}\\right] & =\\sum_{k, \\ell} \\mathbb{E}\\left[\\bar{S}_{j k} \\cdot \\overline{\\mathrm{S}}_{j \\ell}\\right] \\cdot \\bar{x}_{i} \\cdot \\bar{x}_{j} \\\\\n& =\\|\\bar{x}\\|_{2}^{2}\n\\end{aligned}\n$$\n\nwhere the last equality follows since $\\mathbb{E}\\left[\\mathrm{S}_{j k} \\cdot \\mathrm{S}_{j \\ell}\\right]=\\delta_{k, \\ell}$ by the distribution on entries of $\\mathrm{S}_{j}$. Now, we use Jensen's inequality [57] to get the following bound on the expectation of $\\left|\\left\\langle\\bar{x}, \\bar{S}_{j}\\right\\rangle\\right|$. $$\n\\mathbb{E}\\left[\\left|\\left\\langle\\bar{x}, \\overline{\\mathrm{S}}_{j}\\right\\rangle\\right|\\right] \\leq \\sqrt{\\mathbb{E}\\left[\\left\\langle\\bar{x}, \\overline{\\mathrm{S}}_{j}\\right\\rangle^{2}\\right]} \\leq \\epsilon \\sqrt[4]{s}\n$$\n\nWe then use the expectation above to bound the relevant probability as follows:\n\n$$\n\\begin{aligned}\n\\operatorname{Pr}\\left[\\left|\\left\\langle\\bar{x}, \\overline{\\mathrm{S}}_{j}\\right\\rangle\\right| \\leq \\frac{1}{3}\\right] & \\geq 1-\\operatorname{Pr}\\left[\\left|\\left\\langle\\bar{x}, \\overline{\\mathrm{S}}_{j}\\right\\rangle\\right| \\geq \\frac{1}{3}\\right] \\\\\n& \\geq 1-3 \\mathbb{E}\\left[\\left|\\left\\langle\\bar{x}, \\overline{\\mathrm{S}}_{j}\\right\\rangle\\right|\\right] \\\\\n& \\geq 1-3 \\epsilon \\cdot \\sqrt[4]{s}\n\\end{aligned}\n$$\n\nwhere we apply Markov's inequality [57] in (C.18). That is, we have shown that $\\tilde{y}_{j}$ is bounded by $1 / 3$ with constant probability for $j \\neq i$, and $\\tilde{y}_{i}$ is thus bounded by $1 \\pm \\frac{1}{3}$. Note here that each of the $m$-copies $\\tilde{y}_{i}^{(m)}$ will have identical guarantees. Now, define the average $\\overline{\\tilde{y}}_{j}:=\\frac{1}{243 \\cdot \\log s} \\sum_{m} \\tilde{y}^{(m)}$ so that $\\bar{y}_{j}$ (line 3 in Decoder) is the corresponding replacement of the non-zero entries with the binary representation of their indices. We now claim that this average $\\overline{\\tilde{y}}_{j} \\leq 4 / 9<1 / 2$ with high probability for $j \\neq i$. To this end, we employ the multiplicative Chernoff bound [57] on the independent random variables $\\left\\{\\tilde{y}_{j}^{(h)}\\right\\}_{h}[0,1]$ with $\\mathbb{E}\\left[\\sum_{m} \\tilde{y}_{j}^{(m)}\\right] \\leq 81 \\cdot \\log s$ to get\n\n$$\n\\begin{aligned}\n\\operatorname{Pr}\\left[\\overline{\\tilde{y}_{j}>\\frac{4}{9}}\\right] & =\\operatorname{Pr}\\left[\\sum_{m} \\tilde{y}_{j}^{(m)}>\\left(1+\\frac{1}{3}\\right) \\frac{1}{3} \\cdot 243 \\cdot \\log s\\right] \\\\\n& \\leq \\operatorname{Pr}\\left[\\sum_{m} \\tilde{y}_{j}^{(m)} \\geq\\left(1+\\frac{1}{3}\\right) 81 \\cdot \\log s\\right] \\\\\n& \\leq \\exp \\left(-\\left(\\frac{1}{3^{2}} \\cdot 81 \\cdot \\log s\\right) / 3\\right) \\\\\n& =\\frac{1}{s^{3}}\n\\end{aligned}\n$$\n\nTherefore, we have shown that the average $\\overline{\\tilde{y}}_{j}$ is less than $1 / 2$ with probability at least $1-\\frac{1}{s^{3}}$ for $j \\neq i$. Consequently, we will have $\\overline{\\tilde{y}}_{i}$ bounded by $1 \\pm \\frac{1}{2}$. Using the union bound over each $j \\neq i$ and the $\\log s$ bits in the binary representation of $j$, we can then show that $\\bar{y}_{j+m}<1 / 2$ for each $j \\neq i, m \\in[0, \\log s]$ with probability $1-\\frac{\\log s}{s^{2}} \\gg 1-\\frac{1}{s}$. ## D Experimental Details\n\n## D. 1 Pre-training\n\nTo verify the effect of introducing heads to Hyena as described in Section 4, we train a series of models on The Pile [11]. All MultiHyena models are set to 8 heads, and otherwise use the same hyperparameters of Hyena models of equivalent size. We set weight decay of Hyena filter parameters to 0 , and lower the frequency of sine activations in the implicit MLP to 4 . We follow the setup of [2], and first train models for 5,10 and 15 billion tokens, adjusting the learning rate scheduler accordingly. Then, we train for 300 billion tokens. The results are reported in Tables 5.1 and 5.1. ## D. 2 Distillation Analysis\n\nDistilling pre-trained long convolution sequence models (LCSM) with LaughingHyena can introduce errors on the convolution filter, which then propagate to the outputs. Setup We perform a series of extensive experiments on all variants of LCSM, including pre-trained H3 models of sizes 125 million, 355 million, 1.3 billion and 2.7 billion parameters; Hyena of size 153 million parameters, and MultiHyena of size 153 million parameters. For H3 models, we report approximation errors on both shift as well as diagonal SSMs (reported as IIR and FIR). Each point corresponds to distillation carried out at a particular order, using LaughingHyena modal interpolation. To optimize the parameters of the modal form, we use gradient-based optimization and minimize the $\\ell_{2}$ discrepancy between filters in time domain. In particular, we use the ADAMW [58] optimizer with learning rate $3 \\cdot 10^{-4}$, and a cosine annealing decay schedule down to $10^{-6}$ after 30 thousand iterations. Each individual filter of every layer is distilled in the same way. Discussion The errors are shown in Figures D.1, D.1, D.2, D.3, D. 4 and D.5. We observe H3 filters to be easier to distill into recurrences with small state without introducing significant errors, whereas Hyena variants learn filters with larger effective dimensions. This provides further evidence that training with implicit convolutions may yield in general more expressive filters. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-35.jpg?height=743&width=1377&top_left_y=1500&top_left_x=338)\n\nFigure D.1: Mean, lower and upper bounds across channels and layers of the distillation errors on 125M H3 model for both its IIR and FIR filters. ## D.2.1 Pretrained Filters: Effective Dimension\n\nVisualizations We qualitatively investigate LCSM filters at initialization and after pretraining. This visual inspection (Figures D.6, D.7 and D.8) complements the distillation error analysis of Section D.2. ## Distillation Error of H3 355M\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-36.jpg?height=680&width=1364&top_left_y=287&top_left_x=338)\n\nFigure D.2: Mean, lower and upper bounds across channels and layers of the distillation errors on 355M H3 model for both its IIR and FIR filters. Distillation Error of H 31.3 B\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-36.jpg?height=685&width=1366&top_left_y=1156&top_left_x=338)\n\nFigure D.3: Mean, lower and upper bounds across channels and layers of the distillation errors on 1.3 B 3 Hodel for both its IIR and FIR filters. Distribution of Hankel singular values We further compute the distribution of Hankel singular values of each long convolution filter in different models. The decay in the spectrum quantifies how easy it is to find a compact modal form with LaughingHyena, and serves as a proxy measure of effective dimension of the convolution. The results are shown in Figures D. 9 and D. 10. ## D. 3 Downstream Evaluation\n\nWe benchmark the downstream performance of MultiHyena and distilled MultiHyena on standard language modeling tasks from the LM-Eval-Harness [42] and HELM [41] suites. As a reference baseline, we evaluate Pythia [44] 160M. Our objective is to quantify the absolute performance of MultiHyena and the downstream impact of distillation. We use the same procedure outlined in Section D. 2 to distill MultiHyena. ## D. 4 Benchmarking\n\nTo demonstrate the superior performance of Laughing Hyena for autoregressive generation, we conduct a series of experiments to benchmark its latency, throughput, and memory usage for autoregressive generation\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-37.jpg?height=696&width=1366&top_left_y=278&top_left_x=338)\n\nFigure D.4: Mean, lower and upper bounds across channels and layers of the distillation errors on 2.7B H3 model for both its IIR and FIR filters. with initial prompt length $T$ and number of generated tokens $K$. For each experiment, we compare the performance of Laughing Hyena against a Transformer, a hybrid H 3 -attention model with 2 attention layers and a Hyena model. The latter two have been shown to match or achieve lower perplexity than Transformers on standard datasets (Wikitext103 and The Pile). All experiments are carried out on a NVIDIA A100 with 80GB in float16 precision. Missing measurements for any model indicate Out of Memory (OOM) errors while doing autoregressive inference for that particular model. Peak throughput We first evaluate the throughput (number of tokens generated per second) across different batch sizes, using a typical generation workload consisting of a prompt of length 512 and generating 256 tokens. Figure 1.1 measures peak throughput of different models. Since Laughing Hyena does not require caching intermediate kv-projections during generation, reduced memory requirements at a fixed model size allow it to process larger batch sizes. Prompt length Autoregressive generation in Laughing Hyena is achieved through a two-step process: an initial prefill step that uses the length $-T$ prompt to initialize the state $x_{T}$ and that generates all $K$ tokens. In Figure 5.3 we demonstrate how the prefill step scales for different prompt lengths, keeping batch sizes fixed at 64. Since prefilling in Laughing Hyena is carried out efficiently via convolutions (as described in Section 3.4), throughput scales more favorably than Transformers. Other models capable of prefilling via convolutions also achieve higher throughputs than Transformers but are ultimately slower than Laughing Hyena during the generation phase. State throughput We measure the impact of SSM state dimension on the throughput of Laughing Hyena. Keeping batch sizes fixed reveals minimal impact for all dimensions smaller than 100 , which are sufficient to distill all models discussed in this work. All other measurements provided in this Section are carried out with a standard order 16 . We note that it may be possible to further increase peak throughput by leveraging reduced memory footprints achieved by extremely small SSMs. Latency over sequence length We benchmark the time taken to generate a variable number of tokens, starting from a prompt of length 512 tokens at batch size 1 (Figure D.11). Laughing Hyena tracks highly optimized Transformers. We note that Laughing Hyena is asymptotically more efficient than Transformers; however, this regime is bottlenecked by hardware-specific implementation details and optimizations. We expect optimized, platform-specific implementations of Laughing Hyena to outperform Transformers even at batch size 1. When the prompt is long, the prefilling step becomes the bottleneck, and all convolutional models outperform Transformers. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-38.jpg?height=824&width=1457&top_left_y=230&top_left_x=305)\n\nFigure D.5: Mean, lower and upper bounds across channels and layers of the distillation errors on Hyena and MultiHyena models. Parameter scaling To better understand how the performance of Laughing Hyena scales, we benchmark its latency, throughput, and peak memory utilization for autoregressive generation and $125 \\mathrm{M}, 355 \\mathrm{M}, 1.3 \\mathrm{~B}$, 2.7B and 6.7B parameters. We compare the performance to that of Transformers, Hybrid-H3, and Hyena at the same number of parameters and report the results in Figure D.11. For the latency measurement, we use a batch size of 1 and benchmark the time taken to generate 128 tokens, starting from a prompt of length 512 tokens. For throughput and peak memory scaling against the number of parameters, we use a batch size of 64 and measure the throughput for generating 256 tokens starting with a prompt of length 512 . FIR Filters of H3 125M\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-39.jpg?height=527&width=1656&top_left_y=322&top_left_x=196)\n\nIIR Filters of H3 125M\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-39.jpg?height=528&width=1633&top_left_y=920&top_left_x=218)\n\nFigure D.6: Initialized and pre-trained convolution filters of H3. Filters of MultiHyena 155M (300B tokens)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-39.jpg?height=814&width=1623&top_left_y=1700&top_left_x=218)\n\nFigure D.7: Initialized and pre-trained long convolution filters of MultiHyena. Filters of Hyena 355M (200B tokens)\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-40.jpg?height=1550&width=1610&top_left_y=636&top_left_x=225)\n\nFigure D.8: Initialized and pre-trained long convolution filters of Hyena ( 355 M ). Hankel Singular Values of Hyena and MultiHyena Filters\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-41.jpg?height=812&width=1618&top_left_y=329&top_left_x=218)\n\nFigure D.9: Distribution of Hankel singular values for Hyena and MultiHyena long convolution filters. MultiHyena filters have larger effective dimension, as evidenced by slower decay. Hankel Singular Values of H3 (125M) FIR Filters\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-41.jpg?height=524&width=1624&top_left_y=1449&top_left_x=217)\n\nHankel Singular Values of H3 (125M) IIR Filters\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-41.jpg?height=529&width=1623&top_left_y=2041&top_left_x=218)\n\nFigure D.10: Distribution of Hankel singular values for H 3 long convolution filters. The values decay rapidly. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-42.jpg?height=824&width=1100&top_left_y=973&top_left_x=478)\n\nFigure D.11: Generation latency, throughput and peak memory of Transformers, H3, Hyena and Laughing Hyena. ## E Additional Experiments\n\n## E. 1 Associative Recall with MultiHyena\n\nWe follow the setup of [2] and train 2-layer Hyena and MultiHyena (with 8 heads) to solve associative recall via a standard next-token prediction objective. We focus on the sequence length 64 k , high vocabulary size setting, and push vocabulary sizes past the maximum values considered in [2]. At vocabulary size 60, a difference between MultiHyena and Hyena can be observed (Table E.1), as experimental support for Theorem 4.1. | Model | Accuracy |\n| :---: | :---: |\n| Hyena | 65 |\n| MultiHyena | 98 |\n\nTable E.1: Associative recall accuracy, sequence length 64 k , vocabulary size 60 .",
    "hyenadistill-60": "## E. 2 Analysis of Hankel Singular Values of Pretrained Large Convolution Sequence Models\n\n## E. 3 Model Order Reduction of H3\n\nThe H3 model is constructed with a combination of diagonal SSMs and shift SSMs. There exists various classical model order reduction techniques for these different types of SSMs. The following sections aim to present the formulation and effectiveness of two classical approaches on obtaining the compressed representation of a H3 model. More specifically, we study modal truncation and balanced truncation for compressing diagonal SSMs and shift SSMs respectively. ## E.3.1 Modal Truncation\n\nA discrete diagonal SSM $\\left(\\mathrm{A}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{d}\\right), \\mathrm{B} \\in \\mathbb{C}^{d \\times 1}\\right.$, and $\\left.\\mathrm{C} \\in \\mathbb{C}^{1 \\times d}\\right)$ can be directly converted into a residue-pole transfer function as follows:\n\n$$\n(\\mathrm{A}, \\mathrm{B}, \\mathrm{C}) \\rightarrow H(z)=\\sum_{i=1}^{d} \\frac{r_{i}}{z-\\lambda_{i}}\n$$\n\nwhere residue $r_{i}=\\mathrm{B}_{i} \\mathrm{C}_{i}$. Modal truncation aims to compress such a transfer function by essentially reducing the summation over $d$ to $n<d$, of the $n$ most influential modes. The influence from each node can be isolated by expressing it using the $h_{\\infty}$ norm of the system as follows:\n\n$$\n\\|H(z)\\|_{\\infty}=\\sum_{i=1}^{d}\\left\\|\\frac{r_{i}}{z-\\lambda_{i}}\\right\\|_{\\infty} \\leq \\sum_{i=1}^{d} \\frac{\\left|r_{i}\\right|}{|1-| \\lambda_{i}||}\n$$\n\nEach mode $i$ can be ranked using the bound formulated above. Subsequently, the $d-n$ lowest modes could be discarded to form a reduced order model. Figure E. 1 illustrates the monotonically decreasing $l_{\\infty}$ error with the increase in system order. However, this model reduction approach is only suitable for diagonalizable SSMs. ## E.3.2 Balanced Truncation\n\nA balanced SSM realization is one in which the observability $P$ and controllability $Q$ gramians are equal and diagonal. Such a realization can be formulated with the following Lyapunov equations [24]:\n\n$$\n\\begin{aligned}\n& \\mathrm{A} \\Sigma \\mathrm{A}^{\\top}+\\mathrm{BB}^{\\top}=\\Sigma \\\\\n& \\mathrm{A}^{\\top} \\Sigma \\mathrm{A}+\\mathrm{C}^{\\top} \\mathrm{C}=\\Sigma\n\\end{aligned}\n$$\n\nwhere $P=Q=\\Sigma=\\operatorname{diag}\\left(\\sigma_{1}, \\ldots, \\sigma_{d}\\right)$ and $\\sigma_{i} \\geq \\sigma_{i+1}$. Results from [59] shows that the $n$-order model reduction error is bounded by:\n\n$$\nE_{\\infty} \\triangleq\\left\\|H(s)-H_{n}(s)\\right\\|_{\\infty} \\leq 2 \\sum_{i=d-n}^{d} \\sigma_{i}\n$$\n\nModal Truncation Model Reduction Error of H3\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-44.jpg?height=518&width=1615&top_left_y=290&top_left_x=220)\n\nFigure E.1: Modal truncation model reduction error $\\left(\\left\\|h(t)-h_{n}(t)\\right\\|_{\\infty}\\right)$ across all diagonal SSM layers of the trained H 3 125 M model. Therefore, an $n$-order partition of the full balanced realization can be chosen, such that the discarded orders are the $d-n$ lowest contributor to the error. The steps taken by [24], computes the $n$-order partition of the balanced realization as follows:\n\n1. Form a Hankel matrix $\\mathrm{S}_{d}$ from the impulse response $h_{1: n}$ of the shift SSM. 2. Obtain the eigenvector matrix $V \\in \\mathbb{C}^{d \\times d}$, and the eigenvalues $\\lambda=\\sigma^{2}$ via the eigen-decomposition of $\\mathrm{S}_{d}$. 3. Choose the truncated model's order $n<d$, based on the bound in Equation E.4. 4. Compute the state-space matrices as follows:\n\n$$\n\\mathrm{A}=V_{2: d, 1: n}^{\\top} V_{1: d-1,1: n}, \\quad \\mathrm{~B}=V_{1,1: n}, \\quad \\mathrm{C}=h_{1: d}^{\\top} V_{1: d, 1: n}, \\quad \\mathrm{D}=h_{0}\n$$\n\nThis model reduction technique was applied to a trained 125 million parameter H3, MultiHyena, and Hyena models as shown in Figures E.2, E.3, and E.",
    "hyenadistill-61": "4 respectively. It could be noted that the all models encountered an undesirable non-monotonic error reduction with the increase in order. Moreover, order reduction configurations such as the one in Figure E. 3 layer 15 display signs of numerical instability. Balanced Truncation Model Reduction Error of H3\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-44.jpg?height=528&width=1613&top_left_y=1827&top_left_x=223)\n\nFigure E.2: Balanced truncation model reduction error $\\left(\\left\\|h(t)-h_{n}(t)\\right\\|_{\\infty}\\right)$ across all shift SSM layers of the trained H3 125M model. Note that Layer 6 is an Attention layer, therefore balanced truncation model order reduction is not possible. ## Balanced Truncation Model Reduction Error of MultiHyena\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-45.jpg?height=825&width=1623&top_left_y=367&top_left_x=218)\n\nFigure E.3: Balanced truncation model reduction error $\\left(\\left\\|h(t)-h_{n}(t)\\right\\|_{\\infty}\\right)$ across all convolutional layers of the trained MultiHyena 155M model. ![](https://cdn.mathpix.com/cropped/2024_09_12_f033d40ba7a09b1f15bdg-45.jpg?height=875&width=1638&top_left_y=1564&top_left_x=218)\n\nFigure E.4: Balanced truncation model reduction error $\\left(\\left\\|h(t)-h_{n}(t)\\right\\|_{\\infty}\\right)$ across all convolutional layers of the trained Hyena 155 M model.",
    "hyenadistill-62": "[^0]:    ${ }^{*}$ Equal contribution. \u2020 Equal senior authorship. ${ }^{1}$ Mila and Universit\u00e9 de Montr\u00e9al. ${ }^{2}$ Stanford University. ${ }^{3}$ The University of Tokyo. ${ }^{4}$ Purdue University. ${ }^{5}$ Vrije Universiteit Amsterdam. ${ }^{6}$ Carnegie Mellon University and Meta AI (FAIR). ${ }^{7}$ University of Buffalo, SUNY. ${ }^{8}$ University of Chicago and Together Computer. [^1]:    ${ }^{1}$ In this work, we consider second-order Hyena blocks [2] to automatically extend our findings to H 3 [1]. ${ }^{2}$ The short filters are explicitly parameterized, see [2]. ${ }^{3}$ The $q k v$ short convolutions can be evaluated in batch with a single pass. The second convolution is the one with the long filter $h$ and performed via Fast Fourier Transform (FFT), hence the $\\tilde{\\mathcal{O}}(L)$ complexity. ${ }^{4}$ In $\\S 3.4$ we show that multiple pre-filling strategies exist, with different trade-offs in time and memory. [^2]:    ${ }^{5} p, q, r>0$ should satisfy $1 / q+1 / p=1 / r+1$. In the case of infinite sequences defined on the all $\\mathbb{Z}$, the norms are taken in a $\\ell_{p}, \\ell_{q}, \\ell_{r}$ sense, respectively. The bound is potentially sharp [27, 28]\n    ${ }^{6}$ For completeness, we also test balanced and modal truncation techniques on a suite of pre-trained H 3 and Hyena models in Appendix E. 3 . ${ }^{7}$ Such norms are always well-defined for finite sequences of interest which are in $\\ell_{\\infty}$. [^3]:    ${ }^{8}$ As already partially discussed in [6], the truncation introduces a correction term in the approximant transfer function. See Appendix A.4. ${ }^{9}$ i.e normalizing denominator polynomial coefficients to constrain roots within the unit circle. ${ }^{10}$ We report additional details on the nuances of the parametrization in Appendix B.1. [^4]:    ${ }^{11}$ Formally, this is related to low-rank approximation characteristics of the Hankel operator; rigorous bounds can be constructed by application of the Eckart-Young-Mirsky theorem [36]. ${ }^{12}$ This strategy can also be use to evaluate the filter $\\hat{h}$ alternatively to the standard $\\mathcal{O}(d L)$ method\n\n[^5]:    ${ }^{13}$ i.e. such that the denominator's order is not less than the numerator's one. [^6]:    ${ }^{14}$ eig $(\\mathrm{A})$ contains the eigenvalues of $\\mathrm{A} . \\mathrm{poly}(r)$ yields the coefficients of the polynomial whose roots are the elements of $r \\in \\mathbb{C}^{d}$. [^7]:    ${ }^{15}$ In the late 19th century, Henri Pad\u00e9 had already proposed a closed-form solution of the above problem that achieves $o\\left(z^{-L}\\right)$ error for $z \\rightarrow \\infty$ using $L=2 d$ samples of the impulse response. His method [31] solves a $L$-dimensional linear problem that, however, is known to often become numerically ill-conditioned even with small $d$ [51]\n    ${ }^{16}$ In the case of finite sequences, the $\\mathcal{H}_{2}$ norm becomes the standard Euclidean metric evaluated on the $L+1$ roots of unity,i.e. $\\left(\\sum_{k=0}^{L}\\left|H\\left(e^{i 2 \\pi k /(L+1)}\\right)\\right|^{2}\\right)^{1 / 2}$.",
    "hyenadistill-63": ""
}