{
    "soft-0": "# SOFT: Softmax-free Transformer with Linear Complexity \n\nJiachen Lu $^{1}$ Jinghan Yao ${ }^{1}$ Junge Zhang ${ }^{1} \\quad$ Xiatian Zhu ${ }^{2} \\quad$ Hang Xu $^{3}$<br>Weiguo Gao ${ }^{1} \\quad$ Chunjing Xu ${ }^{3} \\quad$ Tao Xiang ${ }^{2} \\quad$ Li Zhang ${ }^{1 *}$<br>${ }^{1}$ Fudan University ${ }^{2}$ University of Surrey ${ }^{3}$ Huawei Noah's Ark Lab<br>https://fudan-zvg.github.io/SOFT\n\n\n#### Abstract\n\nVision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention.",
    "soft-1": "However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the selfattention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization.",
    "soft-2": "This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity. ## 1 Introduction\n\nRecently the step change brought by Transformers [34] in natural language processing (NLP) [10, 4] seems to have arrived in vision [11, 42, 48, 47]. Indeed, with less inductive bias in its architecture design than Convolution neural networks (CNNs), pure Vision Transformer (ViT) [11] and its variants have shown to be able to outperform CNNs on various vision tasks [8, 16]. However, there is a bottleneck in any Transformer based model, namely its quadratic complexity in both computation and memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens (e.g., words or image patches) as input, the self-attention module iteratively learns the feature representations by relating one token to all other tokens. This results in a quadratic complexity $O\\left(n^{2}\\right)$ with the token sequence length $n$ in both computation (time) and memory (space) since an $n \\times n$ sized attention matrix needs to be computed and saved during inference. This problem is particularly acute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP even with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from modeling images at high spatial resolutions, which are often crucial for visual recognition tasks. [^0]![](https://cdn.mathpix.com/cropped/2024_09_17_a9868363b814fed268dbg-02.jpg?height=579&width=1387&top_left_y=250&top_left_x=358)\n\nFigure 1: Top1-Accuracy on ImageNet [9] validation set with respect to parameters and the memory usage corresponding to the token sequence length in practice compared to other methods. (a) Comparison with CNN models: RegNet [27], ResNet [14] and Transformer models: PVT [36], DeiT [32], ViT [11], T2T-ViT [42], Twins-SVT [6] and SAN10 [46]; (b) Comparison with Transformer [34], Linformer [35], Nystr\u00f6former [40] and Performer [5]. The memory usage is measured with a batch size of 1 on a 16 GB Tesla V100. A natural solution is to reduce the complexity of self-attention computation via approximation. Indeed, there have been a number of attempts in NLP [35, 5, 19, 40]. For example, [35] takes a naive approach by shortening the length of Key and Value via learnable projections. Such a coarse approximation would inevitably cause performance degradation. In contrast, [5, 18] both leverage the kernel mechanism to approximate softmax normalization to linearize the computation in self-attention. [19] instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, [40] uses Nystr\u00f6m matrix decomposition to reconstruct the full attention matrix with polynomial iteration for approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization is simply duplicated across the matrix decomposition process, which is theoretically unsound. We empirically found that none of these methods are effective when applied to vision (see Sec. 4.2). In this work, we identify that the limitations of existing efficient Transformers are caused by the use of softmax self-attention, and for the first time propose a softmax-free Transformer. More specifically, in all existing Transformers (with or without linearization), a softmax normalization is needed on top of scaled dot-product between token feature vectors [34]. Keeping this softmax operation challenges any subsequent linearization efforts. To overcome this obstacle, we introduce a novel softmax-free self-attention mechanism, named as SOFT, with linear complexity $O(n)$ in both space and time. Specifically, SOFT uses Gaussian kernel to define the similarity (self-attention) function without the need for subsequent softmax normalization. With this softmax-free attention matrix, we further introduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of the approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably computing the Moore-Penrose inverse of the matrix. We make the following contributions. (I) We introduce a novel softmax-free Transformer with linear space and time complexity. (II) Our attention matrix approximation is achieved through a novel matrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual recognition tasks, we design a family of generic backbone architectures with varying capacities using SOFT as the core self-attention component. Extensive experiments show that with a linear complexity (Figure 1b), our SOFT models can take in as input much longer image token sequences. As a result, with the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on ImageNet [9] classification in the accuracy/complexity trade-off (Figure 1a). ## 2 Related work\n\nVision Transformers There is a surge of research interests recently in exploiting Transformers for visual recognition tasks [37,36, 42, 32, 45], inspired by their remarkable success in NLP [34, 10, 4]. Core to these NLP and vision transformers is the same self-attention mechanism [34] that computes a self-attention matrix by exhaustively comparing token pairs. This means a quadratic complexity with the sequence length in both space and time, which thus limits the scalability of Transformers in dealing with long sequences. This limitation is more serious in vision than NLP: To process an image with at least thousands of pixels, patch-wise tokenization is a must for Transformers to control the computational cost. Given higher resolution images, the patch size also needs to be enlarged proportionally sacrificing the spatial resolution. This limits the capability of Transformers, e.g., learning fine-grained feature representation as required in many visual recognition tasks. Linear Transformers Recently, there have been a number of linear/efficient variants [5, 35, 18, 19, $31,25,17]$ of Transformers in NLP. For example, [35] learns to shrink the length of Key and Value based on a low-rank assumption. [19] adopts a hashing strategy to selective the most similar pairs and only compute attention among them. [5, 18] utilize different kernel functions for approximating softmax-based self-attention matrix. [25] applies random feature mapping on the sequences to approach the original softmax function. [17] decreases the time and memory consumption of the attention matrix by replacing the softmax function with its linear-complexity recurrent alternative. When applied to visual recognition tasks, however, we show that these models have considerable performance degradation compared to the standard Transformers [34] (see Sec. 4.2). The most related work to SOFT is [40] which uses the Nystr\u00f6m matrix decomposition to avoid computing the full attention matrix. However, this method suffers from several theoretical defects: (1) As the standard self-attention needs to apply row-wise softmax normalization on the full attention matrix, a direct application of matrix decomposition is infeasible. As a workaround, softmax is simply applied to all the ingredient matrices in [40]. Such an approximation is not guaranteed theoretically. (2) With a polynomial iteration method, it is not guaranteed that the generalized attention matrix inverse can be computed when the matrix is a nearly singular one in practice. In contrast to all the above methods, in this paper we propose a softmax-free self-attention mechanism that facilitates matrix decomposition for complexity minimization with theoretical guarantees. ## 3 Method\n\n### 3.1 Softmax-free self-attention formulation\n\nA schematic illustration of our model is given in Figure 2. Let's first look at our attention module design. Given a sequence of $n$ tokens $X \\in \\mathbb{R}^{n \\times d}$ with each token represented by a $d$-dimensional feature vector, self-attention [34] aims to discover the correlations of all token pairs exhaustively. Formally, $X$ is first linearly projected into three $d_{e}$-dimensional spaces (query, key, and values) as:\n\n$$\nQ=X W_{q} \\in \\mathbb{R}^{n \\times d_{e}}, \\quad K=X W_{k} \\in \\mathbb{R}^{n \\times d_{e}}, \\quad V=X W_{v} \\in \\mathbb{R}^{n \\times d_{e}}\n$$\n\nwhere $W_{q}, W_{k}, W_{v} \\in \\mathbb{R}^{d \\times d_{e}}$ are learnable matrices. Self-attention can be expressed in a generic formulation as:\n\n$$\ny_{i,:}=\\sum_{j=1}^{n} \\alpha\\left(Q_{i,:}, K_{j,:}\\right) \\odot V_{j,:}\n$$\n\nwhere $\\odot$ is the Hadamard product, and $i, j \\in\\{1, \\cdots, n\\}$ index the tokens. The key self-attention function $\\alpha: \\mathbb{R}^{d_{e}} \\times \\mathbb{R}^{d_{e}} \\rightarrow \\mathbb{R}$ is composed of a nonlinear function $\\beta: \\mathbb{R} \\rightarrow \\mathbb{R}$ and a relation function $\\gamma: \\mathbb{R}^{d_{e}} \\times \\mathbb{R}^{d_{e}} \\rightarrow \\mathbb{R}$. A dominant instantiation of $\\alpha$ is the scaled dot-product based softmax self-attention [34], defined as\n\n$$\n\\beta(\\cdot)=\\operatorname{softmax}(\\cdot), \\quad \\gamma\\left(Q_{i,:}, K_{j,:}\\right)=\\frac{1}{\\sqrt{d_{e}}} \\cdot Q_{i,:}^{\\top} K_{j,:}\n$$\n\nWhilst this softmax self-attention has been the de facto choice and seldomly questioned, as discussed earlier it is not necessarily suited for linearization. To facilitate the design of linear self-attention, we introduce a softmax-free self-attention function with the dot-product replaced by a Gaussian kernel as:\n\n$$\n\\beta^{\\prime}(\\cdot)=\\exp (\\cdot), \\quad \\gamma^{\\prime}\\left(Q_{i,:}, K_{j,:}\\right)=-\\frac{1}{2 \\sqrt{d_{e}}} \\cdot\\left\\|Q_{i,:}-K_{j,:}\\right\\|_{2}^{2}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_a9868363b814fed268dbg-04.jpg?height=473&width=1402&top_left_y=300&top_left_x=359)\n\nFigure 2: Schematic illustration of the proposed softmax-free self-attention (SOFT) method.",
    "soft-3": "P . E . : Position embedding. Dash lines: linear projection. dh: the hidden dim of each attention head. o denotes the matrix dot product. To preserve the symmetric property of attention matrix as in Eq (3), we set the project matrices $W_{q}$ and $W_{k}$ in Eq (1) identical (i.e., $Q=K$ ). Our self-attention matrix is then written as:\n\n$$\nS_{i, j}=\\exp \\left(-\\frac{1}{2 \\sqrt{d_{e}}} \\cdot\\left\\|Q_{i,:}-K_{j,:}\\right\\|_{2}^{2}\\right)\n$$\n\nFor notation simplicity, we define the matrix formulation as: $S=\\exp (Q \\ominus K)$. Remarks Our self-attention matrix $S$ has three important properties: (1) It is symmetric; (2) All the elements lie in a unit range of $[0,1]$; (3) All diagonal elements hold the largest value 1 (selfreinforced), with the bottom ones (corresponding to most dissimilar token pairs) being close to 0 . As Gaussian kernel is a positive definite kernel [12], $S$ is deemed a Gram matrix. However, we find that when using our kernel-based self-attention matrix $S$ without linearization, the training of a transformer fails to converge. This might explain why softmax dot-product based self-attention [34] is so popular in vanilla transformers. ### 3.2 Low-rank regularization via matrix decomposition with linear complexity\n\nTo solve the convergence and quadratic complexity problems, we leverage matrix decomposition as a unified solution with low-rank regularization. In particular, we consider Nystr\u00f6m [39], which is originally a low-rank matrix approximation algorithm. This enables our model's complexity to be reduced significantly without computing the full self-attention matrix $S$. We make this choice because our $S$ is positive semi-definite (i.e., a Gram matrix) without follow-up normalization which are all necessary conditions for Nystr\u00f6m. In contrast, [40] totally ignores these requirements, leading to theoretical flaw in its approximation. To define the Nystr\u00f6m method formally, let us express $S=\\exp (Q \\ominus K)$ as a block matrix:\n\n$$\nS=\\left[\\begin{array}{cc}\nA & B \\\\\nB^{\\top} & C\n\\end{array}\\right] \\in \\mathbb{R}^{n \\times n}\n$$\n\nwhere $A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times(n-m)}, C \\in \\mathbb{R}^{(n-m) \\times(n-m)}$ with $m \\ll n$. Through Nystr\u00f6m decomposition (see derivative details in Appendix A.1), an approximation can be represented as:\n\n$$\n\\hat{S}=\\left[\\begin{array}{c}\nA \\\\\nB^{\\top}\n\\end{array}\\right] A^{\\dagger}\\left[\\begin{array}{ll}\nA & B\n\\end{array}\\right]=P^{\\top} A^{\\dagger} P, \\quad \\text { where } \\quad P=\\left[\\begin{array}{ll}\nA & B\n\\end{array}\\right]\n$$\n\nand $A^{\\dagger}$ is the Moore-Penrose (a generalized) inverse of $A$. Sampling In the standard Nystr\u00f6m formulation, $A$ and $B$ are sub-matrices of $S$ obtained by randomly sampled $m$ tokens, denoted as $\\widetilde{Q}$. We call the sampled $\\widetilde{Q}$ as bottleneck tokens. However,\n\n```\nAlgorithm 1: SOFT: Softmax-free attention\nInput: \\(Q \\in \\mathbb{R}^{n \\times d_{e}}\\), sampling function \\(f_{s}\\)\nSampling \\(\\widetilde{Q} \\leftarrow f_{s}(Q)\\);\n\\(A \\leftarrow \\exp (\\widetilde{Q} \\ominus \\widetilde{Q}), P \\leftarrow \\exp (\\widetilde{Q} \\ominus Q) ;\\)\n\\(\\hat{S} \\leftarrow P^{\\top} \\mathrm{NR}(A) P\\)\nOutput: \\(\\hat{S}\\)\nAlgorithm 2: NR: Newton-Raphson iteration\n```\n\n```\nInput: \\(A \\in \\mathbb{R}^{m \\times m}\\), and \\(\\mathcal{T} \\in \\mathbb{Z}^{+}\\)\n\\(\\alpha=2 /\\|A\\|_{1}^{2}\\). Initialize \\(A_{0} \\leftarrow \\alpha A\\);\nfor \\(k\\) from 1 to \\(\\mathcal{T}\\) do\n    | \\(A_{k} \\leftarrow 2 A_{k-1}-A_{k-1} A A_{k-1}\\)\nend\nOutput: \\(A_{\\mathcal{T}}\\)\n```\n\nwe find empirically that random sampling is considerably sensitive to the choice of $m$. We hence explore two additional options by leveraging the structural prior of visual data: (1) Using one convolutional layer with kernel size $k$ and stride $k$ to learn $\\widetilde{Q}$, and (2) Using average pooling with kernel size $k$ and stride $k$ to generate $\\widetilde{Q}$. For both, we need to reshape $Q$ to the form of $\\mathbb{R}^{H \\times W \\times d_{e}}$. Each slide of convolution or pooling produces a token. We set $k$ according to the length of $Q$ such that $m$ tokens can be obtained. Our experiments show that a convolution layer performs better in accuracy.",
    "soft-4": "We therefore use a convolution layer by default. As $K$ is identical to $Q$, we have $\\widetilde{K}=\\widetilde{Q}$. Given these $m$ tokens, we then compute $A$ and $P$ as:\n\n$$\nA=\\exp (\\widetilde{Q} \\ominus \\widetilde{K}), \\quad P=\\exp (\\widetilde{Q} \\ominus K)\n$$\n\nWe finally obtain the regularized self-attention matrix $\\hat{S}$ of SOFT as:\n\n$$\n\\hat{S}=\\exp (Q \\ominus \\widetilde{K})(\\exp (\\widetilde{Q} \\ominus \\widetilde{K}))^{\\dagger} \\exp (\\widetilde{Q} \\ominus K)\n$$\n\nleading to Algorithm 1. The low-rank regularization is conducted as follows. For computing the attention score between any two tokens, we first correlate each of them with sampled tokens using our self-attention function ( $\\mathrm{Eq}(5)$ ); With this correlation representation we then compute their similarity under the modulation of the generalized inverse of $\\widetilde{Q}$ 's correlation matrix. Similar as standard Nystr\u00f6m, our design associates the input tokens w.r.t. a small space spanned by sampled tokens, giving a proper estimation of the original attention relationships subject to a low-rank constraint. The correctness of this method is proved in Appendix A.1. Moore-Penrose inverse An accurate and commonly used way to calculate the Moore-Penrose inverse is to use Singular Value Decomposition (SVD). Given $A \\in \\mathbb{R}^{m \\times m}$ and its SVD form $A=U \\Sigma V^{\\top}$ where $U, V$ are $m \\times m$ unitary matrices and $\\Sigma$ is a $m \\times m$ diagonal matrix, the Moore-Penrose inverse of $A$ is $A^{\\dagger}=V \\Sigma^{\\dagger} U^{\\top}$. Nevertheless, SVD is not friendly to the training process on GPU hence harming the model training efficiency. To solve this issue, we adopt the Newton-Raphson method. It is an iterative algorithm with the $(k+1)$-th iteration formulated given the previous iteration as:\n\n$$\nA_{k+1}=2 A_{k}-A_{k} A A_{k}, \\quad \\text { and } \\quad A_{0}=\\alpha A\n$$\n\nWe now prove that $A_{k}$ finally converges to Moore-Penrose inverse of $A_{m \\times m}$, if $\\alpha$ is sufficiently small $[3]$. Theorem 1 When $\\alpha$ is sufficiently small, $A_{k+1}=2 A_{k}-A_{k} A A_{k}, A_{k}$ converges to $A^{\\dagger}$. Though $\\alpha=2 /\\|A\\|_{1}^{2}$ which ensures good convergence behavior in Algorithm 2 (see more details in Appendix A.2.1), in practice, we find that using an alternative form gives more stable training and faster convergence. Specifically, in $\\left\\|I-A \\frac{2 \\beta^{n}}{\\|A\\|_{1}^{2}}\\right\\|_{1} \\leq 1$ where $\\beta$ equals to 0.5 , we find the smallest $n_{i}$ that holds this inequality. Then, we initialize $\\alpha$ as $\\alpha=\\frac{2 \\beta^{n_{i}}}{\\|A\\|_{1}^{2}}$. The following proposition comes with the proof of Theorem 1:\nProposition $1\\left\\|A A_{k} A-A\\right\\|$ and $\\left\\|A_{k}-A^{\\dagger}\\right\\|$ decreases to 0 monotonously, if $\\alpha$ is sufficiently small.",
    "soft-5": "The detail of proposition 1 is shown in Appendix A.2.2. This ensures that our estimated inverse is sufficiently accurate for matrix decomposition, subject to that our SOFT attention is regularized. | Methods | Complexity | Memory | Params | FLOPs | Throughput (img/s) | Top-1 $\\%$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer [34] | $\\mathcal{O}\\left(n^{2}\\right)$ | $19.0 \\mathrm{~GB} \\dagger$ | 13 M | 3.9 G | $1073 / 3240$ | 79.1 |\n| Linformer [35] | $\\mathcal{O}(n)$ | 11.7 GB | 13 M | 1.9 G | $2767 / 3779$ | 78.2 |\n| Performer [5] | $\\mathcal{O}(n)$ | 15.0 GB | 13 M | 2.2 G | $2037 / 3657$ | 76.1 |\n| Nystr\u00f6mformer [40] | $\\mathcal{O}(n)$ | 17.2 GB | 13 M | 2.0 G | $1891 / 3518$ | 78.6 |\n| SOFT | $\\mathcal{O}(n)$ | 15.8 GB | 13 M | 1.9 G | $1730 / 3436$ | $\\mathbf{7 9 . 3}$ |\n\nTable 1: Comparison of different linear/efficient transformer variants on ImageNet [9], based on our multi-stage Tiny configuration (see Table 2). The memory usage is measured with the batch size of 1024 which is our standard training setting. Transformer is tested at a batch size of 256 , which is the maximal number possible with the GPU resource at our disposal. Throughput is in format as Train throughput / inference throughput. Complexity We summarize the complexity of SOFT in space and time. For time complexity, it involves: (1) Sampling: $\\mathcal{O}\\left(n d_{e}\\right)$. (2) Calculating three decomposed matrices: $\\mathcal{O}\\left(n m d_{e}+m n d_{e}+\\right.$ $\\left.m^{2} d_{e}\\right)=\\mathcal{O}\\left(2 m n d_{e}+m^{2} d_{e}\\right)$; (3) Moore-Penrose inverse: $\\mathcal{O}\\left(\\mathcal{T} \\times m^{3}\\right)=\\mathcal{O}\\left(\\mathcal{T} m^{3}\\right)$, where $\\mathcal{T}$ is the iteration steps. (4) All matrix multiplication: $\\mathcal{O}\\left(n m^{2}+m n d_{e}+m n d_{e}\\right)=\\mathcal{O}\\left(n m^{2}+2 m n d_{e}\\right)$. The total time complexity is $\\mathcal{O}\\left(\\left(d_{e}+4 m d_{e}+m^{2}\\right) n+\\mathcal{T} m^{3}+d_{e} m^{2}\\right)$. The space complexity is decided by four decomposed matrices with $\\mathcal{O}(n \\times m)+\\mathcal{O}(m \\times m)+\\mathcal{O}(m \\times n)+\\mathcal{O}\\left(n \\times d_{e}\\right)=$ $\\mathcal{O}\\left(\\left(2 m+d_{e}\\right) n+m^{2}\\right)$. As we keep $m(m \\ll n)$ a fixed constant in our model, both time and space complexity are $\\mathcal{O}(n)$, making SOFT a linear self-attention. ### 3.3 Instantiations\n\nFigure 2 shows how our proposed softmax-free self-attention block (SOFT block) can be implemented in a neural network. We replace the self-attention block with our SOFT block in the traditional Transformer, that is, we stack a SOFT block with a feed forward residual block [11] to form a softmax-free Transformer layer (SOFT layer). Focusing on the general image recognition tasks, we integrate our SOFT layer into the recent pyramidal Transformer architecture [36] to form our final model SOFT. Further, several improvements are introduced in patch embedding (i.e., tokenization). Specifically, unlike [36] that uses a combination of non-overlapping convolution and layer normalization [1], we adopt a stack of overlapping convolutions, batch normalization [15] and ReLU non-linearity. Concretely, the STEM is implemented by 3 units of $3 \\times 3$ Conv $\\rightarrow \\mathrm{BN} \\rightarrow \\operatorname{ReLU}$, with the stride of $2,1,2$ respectively. Then, one such unit is applied to each of three following down-sampling operations with stride of 2 in the multi-stage architecture. The architecture hyper-parameters of SOFT are: $d$ : the input channel dimension of SOFT layer. $d_{e}$ : the embedding dimension of tokens in SOFT block. In practice, we set $d_{e}=d$. $h$ : the head number of SOFT block. $d_{h}$ : the channel dimension of each head and $d_{h}=d_{e} / h . n$ : the input token sequence length of a SOFT block. $m$ : the bottleneck token sequence length of SOFT block. $s p$ : the sampling ratio of token sequence length sampling, which is the ratio between input token sequence length and the bottleneck token sequence length. $e$ : the expansion ratio of the 2-layer feed forward block. In SOFT, for all the stages we set $d_{h}=32, e=4$ and $m=49, s p$ varies in each stage according to the input token sequence length. Table 2 details the family of our SOFT configurations with varying capacities (depth and width). ## 4 Experiments\n\n### 4.1 Setup\n\nDataset: We evaluate the proposed SOFT on the ILSVRC-2012 ImageNet-1K dataset [9] with 1.28M training images and 50 K validation images from 1,000 classes. Following the common practice, we train a model on the training set and evaluate on the validation set. Metrics: For model performance, the top-1 accuracy on a single crop is reported. To assess the cost-effectiveness, we also report the model size and floating point operations (i.e., FLOPs). Implementation details: We use the code base [38] with the default setting to train and test all the models. Specifically, we use weight decay of 0.05 and 10 epochs of linear warm-up. We conduct 300 epochs training with an AdamW optimizer\n\n|  | Tiny |  | Small |  | Medium |  | Large |  | Huge |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Stage 1 | C33-BN-ReLU, 64-d |  |  |  |  |  |  |  |  |  |\n|  | sp.",
    "soft-6": "8 x 8 , <br> $64-d, 2-h$ | x 1 | sp. 8x8, <br> $64-\\mathrm{d}, 2-\\mathrm{h}$ | x 1 | sp. 8x8, <br> $64-\\mathrm{d}, 2-\\mathrm{h}$ | $x 1$ | sp. 8x8, <br> $64-\\mathrm{d}, 2-\\mathrm{h}$ | $x 1$ | sp. 8x8, <br> $64-\\mathrm{d}, 2-\\mathrm{h}$ | x 1 |\n| Stage 2 | C31-BN-ReLU, 128-d |  |  |  |  |  |  |  |  |  |\n|  | sp.",
    "soft-7": "4x4, <br> $128-\\mathrm{d}, 4-\\mathrm{h}$ | x 2 | sp. 4x4, <br> 128-d, 4-h | x 3 | sp. 4x4, <br> 128-d, 4-h | x 3 | sp. 4x4, <br> $128-\\mathrm{d}, 4-\\mathrm{h}$ | $x 3$ | sp. 4x4, <br> $128-\\mathrm{d}, 4-\\mathrm{h}$ | $x 5$ |\n| Stage 3 | C31-BN-ReLU, 320-d or 288-d |  |  |  |  |  |  |  |  |  |\n|  | $\\left.\\begin{array}{c}\\hline \\text { sp.",
    "soft-8": "2x2, } \\\\ 320-\\mathrm{d}, 10-\\mathrm{h}\\end{array}\\right]$ | $x 3$ | sp. 2x2, <br> $320-\\mathrm{d}, 10-\\mathrm{h}$ | $x 7$ | sp. 2x2, <br> $288-\\mathrm{d}, 9-\\mathrm{h}$ | x 29 | sp. 2x2, <br> $320-\\mathrm{d}, 10-\\mathrm{h}$ | x 40 | sp. 2x2, <br> $352-\\mathrm{d}, 11-\\mathrm{h}$ | x 49 |\n| Stage 4 <br> w. cls token | C31-BN-ReLU, 512-d |  |  |  |  |  |  |  |  |  |\n|  | sp.",
    "soft-9": "1x1, <br> $512-\\mathrm{d}, 16-\\mathrm{h}$ | $x 2$ | sp. 1x1, <br> $512-\\mathrm{d}, 16-\\mathrm{h}$ | $x 4$ | sp. 1x1, <br> $512-\\mathrm{d}, 16-\\mathrm{h}$ | $\\times 5$ | sp. 1x1, <br> $512-\\mathrm{d}, 16-\\mathrm{h}$ | x 5 | sp. 1x1, <br> $512-\\mathrm{d}, 16-\\mathrm{h}$ | $x 5$ |\n\nTable 2: Architecture specifications of SOFT variants.",
    "soft-10": "sp.: sampling ratio. $-d$ : the hidden dimension. $-h$ : the number of heads in the self-attention block. C33-BN-ReLU: three $3 \\times 3$ Conv-BN-ReLU, with the stride of 2, 1, 2 respectively. C31-BN-ReLU: one $3 \\times 3$ Conv-BN-ReLU, with a stride of 2 . and decreasing learning rate with the cosine annealing schedule. During training, random flipping, mixup [44] and cutmix [43] are adopted for data augmentation. Label smoothing [29] is used for loss calculation. All our variants are trained with a batch size of 1024 on 32G NVIDIA V100 GPUs. We also implement our method using the Mindspore [23]. ### 4.2 Comparison with existing linear Transformers\n\nWe compare our method with three existing linear Transformer models: Linformer [35], Performer [5], Nystr\u00f6mformer [40] in terms of model complexity and accuracy.",
    "soft-11": "Two experimental settings are adopted. Under the first setting, for all methods we use the same Tiny (Table 2) architecture for a fair comparison. That is, we replace the core self-attention block in SOFT with each baseline's own attention block with the rest of the architecture unchanged. Note that the spatial reduction module of [36] is a special case of Linformer [35]. We set the reduction ratio to be identical to ours. With the same uniform sampling idea, we replace the 1 D window averaging of Nystr\u00f6mformer [40] (for NLP tasks) with 2D average pooling (for images). The downsampling ratio remains identical to ours. It is also worth mentioning that there is no official code released for Reformer [19] and the local Sensitive Hash (LSH) module has strict requirements on the length of input tokens. We thus do not include this method in our comparison. From Table 1 we can make the following observations: (i) Linear Transformer methods substantially reduce the memory and FLOPs while maintain similar parameter size comparing to the Transformer on the Tiny architecture; (ii) Our approach SOFT achieves the best classification accuracy among all the linearization methods. (iii) Our inference speed is on-par with other compared linear Transformers and our training speed is slightly slower than Nystromformer and both are slower than Performer and Linformer. Note that the slow training speed of our model is mostly due to the Newton-Raphson iteration which can only be applied sequentially for ensuring the accuracy of Moore-Penrose inverse. In summary, due to the on-par inference speed we consider the training cost increase is a price worth paying for our superior accuracy. Under the second setting, we focus on the memory efficiency of SOFT against the baselines. Here we follow the ViT [11] network structure, stacking 12 attention layers with hidden dimension $d=384$, heads $h=12$, bottleneck token sequence length $m=49$. Different attention blocks from the three linearized Transformer variants, Linformer [35], Performer [5], and Nystr\u00f6mformer [40] are studied. For each Transformer variant, we adjust its token sequence length $n$ in a linear increment. Specifically, we use a token sequence length of $784 \\times p$ where $p=1,2,3,4,5,6,7,8$ and set batch size 1 to verify whether the memory consumption increases \"quadratically\" or \"linearly\". Figure 1b shows all compared transformer variants including our SOFT indeed have a linear memory usage complexity. This is in contrast with the standard Transformer which cannot cope with long token sequences with a quadratic complexity. ### 4.3 Comparison with state-of-the-art CNNs and ViTs\n\n| Model | Style | Resolution | M.S. Out.? | Params | FLOPs | Top-1 \\%. |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| ResNet-18 [14] | ConvNets | $224^{2}$ | $\\checkmark$ | 11 M | 1.9 G | 69.8 |\n| PVT-Tiny [36] | Transformers | $224^{2}$ | $\\checkmark$ | 13 M | $1.9 \\mathrm{G} \\dagger$ | 75.1 |\n| Coat-Lite Mini [41] | Transformers | $224^{2}$ | $\\checkmark$ | 11 M | 2.0 G | 78.9 |\n| LambdaNets-50 [2] | Transformers | $224^{2}$ | $\\checkmark$ | 16 M | - | 78.9 |\n| SOFT-Tiny | SOFT | $224^{2}$ | $\\checkmark$ | 13 M | 1.9 G | $\\mathbf{7 9 . 3}$ |\n| ResNet-50 [14] | Convolution | $224^{2}$ | $\\checkmark$ | 25 M | 4.1 G | 78.5 |\n| PVT-Small [36] | Transformer | $224^{2}$ | $\\checkmark$ | 24 M | $4.0 \\mathrm{G} \\dagger$ | 79.8 |\n| Deit-Small [32] | Transformer | $224^{2}$ | $\\boldsymbol{X}$ | 22 M | 4.6 G | 79.9 |\n| T2T-ViT $t$-14 [42] | Transformer | $224^{2}$ | $\\boldsymbol{X}$ | 21 M | 5.2 G | 80.7 |\n| CPVT-Small [7] | Transformer | $224^{2}$ | $\\checkmark$ | 22 M | - | 79.9 |\n| Twins-SVT-S [6] | Hybrid | $224^{2}$ | $\\checkmark$ | 24 M | 3.7 G | 81.7 |\n| SOFT-Small | SOFT | $224^{2}$ | $\\checkmark$ | 24 M | 3.3 G | $\\mathbf{8 2 . 2}$ |\n| ResNet-101 [14] | Convolution | $224^{2}$ | $\\checkmark$ | 44 M | 7.9 G | 79.8 |\n| PVT-Medium [36] | Transformer | $224^{2}$ | $\\checkmark$ | 44 M | $7.0 \\mathrm{G} \\dagger$ | 81.2 |\n| ViT-Small/16 [11] | Transformer | $224^{2}$ | $\\boldsymbol{X}$ | 48 M | 9.9 G | 80.8 |\n| SOFT-Medium | SOFT | $224^{2}$ | $\\checkmark$ | 45 M | 7.2 G | $\\mathbf{8 2 . 9}$ |\n| ResNet-152[14] | Convolution | $224^{2}$ | $\\checkmark$ | 60 M | 11.6 G | 80.8 |\n| PVT-Large [36] | Transformer | $224^{2}$ | $\\checkmark$ | 61 M | $10.1 \\mathrm{G} \\dagger$ | 81.7 |\n| T2T-ViT ${ }_{t}-24[42]$ | Transformer | $224^{2}$ | $\\boldsymbol{\\chi}$ | 64 M | 13.2 G | 82.2 |\n| BoTNet-S1-110[28] | Hybrid | $224^{2}$ | $\\checkmark$ | 55 M | - | 82.8 |\n| SOFT-Large | SOFT | $224^{2}$ | $\\checkmark$ | 64 M | 11.0 G | $\\mathbf{8 3 . 1}$ |\n| CaiT-S36[33] | Transformer | $224^{2}$ | $\\checkmark$ | 88 M | 13.9 G | 83.3 |\n| Swin-B[21] | Transformer | $224^{2}$ | $\\checkmark$ | 88 M | 15.4 G | 83.3 |\n| Twins-SVT-L [6] | Hybrid | $224^{2}$ | $\\checkmark$ | 99 M | 14.8 G | 83.3 |\n| SOFT-Huge | SOFT | $224^{2}$ | $\\checkmark$ | 87 M | 16.3 G | $\\mathbf{8 3 . 3}$ |\n\nTable 3: Evaluation results on ILSVRC-2012 ImageNet-1K [9] validation set. We report the results using the input size of $224 \\times 224$ pixels center cropped from resized images with $256 \\times 256$ pixels. M.S.Out. stands for whether the model is designed for multi-scale output. $\\dagger$ : Corrected FLOPs by taking into account the cost of attention matrix multiplication overlooked in the origin paper. We compare with state-of-the-art alternatives and report the top-1 accuracy on the ImageNet-1K validation set. FLOPs are calculated at batch size 1. From Figure 1a and Table 3, the following observations are made: (i) Overall, ViT and its variants yield better classification accuracy over CNNs. (ii) We achieve the best performance among the recent pure vision Transformer based methods including ViT [11] and DeiT [32], as well as the state-of-the-art CNN RegNet [27]. (iii) Our SOFT outperforms the most similar (in architecture configuration) Transformer counterparts PVT [36] at all variants. Since the attention module is the main difference, this validates directly the effectiveness of our model. (iv) We can also beat the latest ViT variants Twins [6] which is designed to address the efficiency limitation of ViT. We have done so with less parameters and fewer float point computation. To gain some insights into how attention is learned using our SOFT and the alternatives, Figure 3 shows the attention masks of various compared models. For each model, we show the output from the first two attention heads. It is evident that SOFT exhibits robustness and versatility in capturing local and long distance relations among pixels. It is interesting to note that, although SOFT is trained on an object categorization dataset in ImageNet [9], it seems to be able to learn both semantic concepts shared across instances in the same category and instance specific features.",
    "soft-12": "For instance, in the bottom-right example of a bird class, one attention head focuses on the black bird only, while the other attend to both birds in the image. More examples are shown in Appendix A.4. ### 4.4 Ablation studies\n\nPyramidal architecture: Unlike the earlier non-pyramidal vision Transformers (e.g., ViT [11]), most recent pyramidal (multi-scale) Transformers (e.g., PVT [36]) use convolution layers to reduce\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_a9868363b814fed268dbg-09.jpg?height=394&width=1397&top_left_y=234&top_left_x=362)\n\nFigure 3: Comparing the attention heatmaps of a query patch (marked by the cross \" + \") against all the patches of an image, produced by (a) Transformer [34], (b) Performer [5], (c) Nystromformer [40] and (d) Our SOFT.",
    "soft-13": "See Appendix A.",
    "soft-14": "4 for more examples. the spatial resolution (i.e., token sequence length) between stages. In this study, we ablate SOFT with a pyramidal architecture (our default SOFT-Small), SOFT w/o a pyramidal architecture and DeiT-S [32] (no pyramidal architecture either). We replace the Transformer layer with a SOFT layer to get SOFT w/o a pyramidal architecture. Note all three variants have similar parameters and FLOPs. Table 5a shows that the conv-based pyramidal architecture is clearly superior to a non-pyramidal design, and our non-pyramidal counterpart is even slightly better than DeiT-S [32] whilst enjoying linear complexity. Bottleneck token sequence length: In this study, we examine how the bottleneck token sequence length $m$, sampled from $n$ tokens, influences the model's performance. We change the bottleneck token sequence length in all stages to $36,49,64,81$. Table 4 a shows that longer bottleneck token would increase the memory cost and the computational overhead. $m=49$ seems to give the best trade-off between the performance and computational overhead. The memory usage is measured with the batch size of 128 . Token sampling: The sampling function in SOFT can assume different forms. Convolution: The sequence $Q \\in \\mathbb{R}^{n \\times d_{e}}$ is first reshaped to a feature map $\\mathbb{R}^{H \\times W \\times d_{e}} . r \\times r$ convolution kernel with stride of $r$ is applied for downsam-\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_a9868363b814fed268dbg-09.jpg?height=524&width=684&top_left_y=1191&top_left_x=1054)\n\nFigure 4: Convergence analysis for the approximation of the Moore-Penrose inverse on SOFT-Tiny. pling, where $r=\\sqrt{s p}$. The output channel size is also kept and no bias is used. At last, the feature map is reshaped back to the sequence. Average pooling: using a $r \\times r$ kernel and $r$ stride, where $r=\\sqrt{s p}$. Random sampling: $m$ tokens are randomly picked from $n$ tokens. Biased sampling: We pick $m$ tokens with a biased policy. Here, the first $m$ tokens are picked. Table 4 b shows that average pooling yields the best performance while with less computational overhead comparing to convolution. Biased sampling can miss the most salient samples, and there is no guarantee that random sampling can keep the uniformity of the chosen samples. This result thus justifies the choice of using average pooling in SOFT. ![](https://cdn.mathpix.com/cropped/2024_09_17_a9868363b814fed268dbg-09.jpg?height=254&width=1377&top_left_y=2187&top_left_x=385)\n\nTable 4: (a) Ablations on bottleneck token sequence length. (b) Ablations on sampling methods. Overlapped convolution: We ablate SOFT with overlapped convolution (our default choice, same as many recent works) and SOFT with non-overlapped convolution in our Tiny configuration. Table 5b shows that SOFT with overlapped convolution performs better than SOFT without overlapped convolution. Our non-overlapped convolution variant still outperforms the PVT [36] which also has the same non-overlapped convolution by a clear margin. | Methods | Pyramidal? | Top-1 $\\%$ |  | Methods | Overlapped? | Top-1 $\\%$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| DeiT-S [32] | $\\boldsymbol{X}$ | 79.8 |  | PVT [36] | $\\boldsymbol{X}$ | 75.1 |\n| SOFT | $\\boldsymbol{X}$ | 80.1 |  | SOFT | $\\boldsymbol{X}$ | 77.4 |\n| SOFT | $\\checkmark$ | 82.2 |  | SOFT | $\\checkmark$ | 79.3 |\n\nTable 5: (a) Ablations on pyramidal architecture.",
    "soft-15": "(b) Ablations on overlapped convolution. Newton-Raphson's convergence: We study how many iterations the Newton-Raphson method needs to converge when computing the Moore-Penrose inverse $A^{\\dagger}$. We use $\\left\\|A A_{k} A-A\\right\\|_{p} /\\|A\\|_{p}$ with $p=2$ (see Proposition 1) as the convergence metric to quantify the difference between $A_{k}$ and $A^{\\dagger}$. Figure 4 shows that our approximation converges within 20 iterations across all stages. ### 4.5 Additional experiments on NLP tasks\n\nIn this section, we evaluate our method against other linear counterparts on four tasks of the Long Range Arena (LRA) [30] benchmark e.g., Listops [24], byte-level IMDb reviews text classification [22], byte-level document retrieval [26], and image classification on sequences of pixels [20]. Implementations. We use the Pytorch version of LRA [30] benchmark, provided by [40]. For the evaluation protocol, we strictly follow [30, 40]. We omit the Pathfinder(1K) task as we cannot replicate the result of Nystr\u00f6mformer [40]. For our SOFT, we simply use the average pooling with window size 4 , stride 4 to sample the bottlenecks. We follow the configurations of [40], with 2 layers, 64 and 128 hidden dimension respectively, and 2 attention heads. The results in Table 6 shows that our SOFT outperforms both the standard and alternative efficient Transformers on three out of four tasks, as well as the average result. | Methods | Listops(2K) | Text(4K) | Retrieval(4K) | Image(1K) | Avg. \\% |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Transformer [34] | 37.10 | 65.02 | 79.35 | 38.20 | 54.92 |\n| Reformer [19] | 19.05 | 64.88 | 78.64 | 43.29 | 51.47 |\n| Linformer [35] | 37.25 | 55.91 | 79.37 | 37.84 | 52.59 |\n| Performer [5] | 18.80 | 63.81 | 78.62 | 37.07 | 49.58 |\n| Nystr\u00f6mformer [40] | 37.15 | $\\mathbf{6 5 . 5 2}$ | 79.56 | 41.58 | 55.95 |\n| SOFT | $\\mathbf{3 7 . 4 0}$ | 63.49 | $\\mathbf{8 1 . 7 7}$ | $\\mathbf{4 6 . 9 1}$ | $\\mathbf{5 7 . 3 9}$ |\n\nTable 6: Comparison of different linear/efficient Transformer variants on Long Range Arena [30], based on its official configuration. Our SOFT surpasses previous efficient methods on three tasks. ## 5 Conclusions\n\nWe have introduced a novel softmax-free self-attention (SOFT) mechanism for linearizing Transformer's complexity in space and time. Unlike existing linear Transformers that aim to approximate the conventional softmax based self-attention, SOFT employs a Gaussian kernel based attention which eliminates the need for softmax normalization. This design enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments show that SOFT yields superior trade-off in accuracy and complexity. Acknowledgment This work was funded in part by Shanghai Municipal Science and Technology Major Projects (No.2018SHZDZX01 and No.2021SHZDZX0103), Mindspore, National Science Foundation of China under Grant No.11690013, 71991471 and the scientific-technological innovation plan program of Universities guided by the Ministry of Education, China. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint, 2016. 6\n[2] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint, 2021.8\n[3] Adi Ben-Israel and Dan Cohen. On iterative computation of generalized inverses and associated projections. SIAM Journal on Numerical Analysis, 1966. 5, 15\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 1, 2\n[5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In ICLR, 2021. 2, 3, 6, 7, 9, 10, 15, 16\n[6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint, 2021. 2, 8\n[7] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint, 2021. 8\n[8] St\u00e9phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In ICML, 2021. 1\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In $C V P R, 2009.2,6,8$\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In $A C L, 2018.1,2$\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.",
    "soft-16": "In ICLR, 2021. $1,2,6,7,8$\n[12] Gregory E Fasshauer. Positive definite kernels: past, present and future. Dolomites Research Notes on Approximation, 2011. 4\n[13] Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin. Is attention better than matrix decomposition? In ICLR, 2021. 15\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In $C V P R, 2016.2,8$\n[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 6\n[16] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention. arXiv preprint, 2021. 1\n[17] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. 3\n[18] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. 2, 3\n[19] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR, 2020. 2, 3, 7, 10\n[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Citeseer, 2009. 10\n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows.",
    "soft-17": "arXiv preprint, 2021. 8\n[22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, 2011. 10\n[23] Mindspore. https://www.mindspore.cn/, 2020. 7\n[24] Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv preprint, 2018. 10\n[25] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. 3\n[26] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 2013. 10\n[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In CVPR, 2020. 2, 8\n[28] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition. arXiv preprint, 2021. 8\n[29] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 7\n[30] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint, 2020. 10\n[31] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv preprint, 2020. 3\n[32] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers \\& distillation through attention. arXiv preprint, 2020. 2, 8, 9, 10\n[33] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprint, 2021. 8\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 2, 3, 4, 6, $9,10,15,16$\n[35] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint, 2020. 2, 3, 6, 7, 10\n[36] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint, 2021. 2, 6, 7, 8, 10\n[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In $C V P R, 2018.2$\n[38] Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. 6\n[39] Christopher Williams and Matthias Seeger. Using the nystr\u00f6m method to speed up kernel machines. In NeurIPS, 2001. 4, 13\n[40] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In $A A A I, 2021.2,3,4,6,7,9,10,15,16$\n[41] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv preprint, 2021. 8\n[42] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
    "soft-18": "arXiv preprint, 2021. 1, 2, 8\n[43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 7\n[44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint, 2017. 7\n[45] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph message passing networks. In $C V P R, 2020.2$\n[46] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. 2\n[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In $C V P R, 2021.1$\n[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection.",
    "soft-19": "In ICLR, 2021. 1\n\n## A Appendix\n\n## A. 1 Nystr\u00f6m method\n\nNystr\u00f6m method [39] aims to calculate a low-rank approximation for a Gram matrix. For Transformers, the self-attention matrix can be viewed as a Gram matrix $S$ with a Gaussian kernel $k$ applied to the query $Q$, with each element $S_{i j}$ expressed as:\n\n$$\nS_{i j}=k\\left(Q_{i,:}, Q_{j,:}\\right)=\\exp \\left(-\\frac{\\left\\|Q_{i,:}-Q_{j,:}\\right\\|_{2}^{2}}{2 \\sqrt{d}}\\right)\n$$\n\n$k(x, y)$ means operating Gaussian kernel $k$ to $(x, y)$, which can be written in the feature space as:\n\n$$\nk(x, y)=\\sum_{i=1}^{n} \\lambda_{i} \\phi_{i}(x) \\phi_{i}(y)\n$$\n\n$n$ is the dimension of a feature space, $\\lambda_{i}$ denotes the eigenvalue and $\\phi_{i}$ denotes the eigenfunction of kernel $k$.",
    "soft-20": "According to the eigenfunction's definition, we can get:\n\n$$\n\\int k(y, x) \\phi_{i}(x) p(x) d x=\\lambda_{i} \\phi_{i}(y)\n$$\n\nwhere $p(x)$ is the probability distribution of $x$. And $\\left\\{\\phi_{i}(x)\\right\\}$ are $p$-orthogonal:\n\n$$\n\\int \\phi_{i}(x) \\phi_{j}(x) p(x) d x=\\delta_{i j}\n$$\n\n$\\delta_{i j}$ is 0 when $i \\neq j, 1$ when $i=j$. To get an approximation of the eigenfunctions, we sample $\\left\\{x_{1}, x_{2}, \\cdots, x_{q}\\right\\}$ from $p(x)$, then:\n\n$$\n\\frac{1}{q} \\sum_{t=1}^{q} k\\left(y, x_{t}\\right) \\phi_{i}\\left(x_{t}\\right) \\approx \\lambda_{i} \\phi_{i}(y)\n$$\n\n$$\n\\frac{1}{q} \\sum_{t=1}^{q} \\phi_{i}\\left(x_{t}\\right) \\phi_{j}\\left(x_{t}\\right) \\approx \\delta_{i j}\n$$\n\nThis inspires us to approximate the Gram matrix $S$. Let $S^{(m)}$ be a submatrix of $S$, consisting of $m \\times m$ elements from $S$. Gram matrix is a symmetric positive semi-definite matrix, so it has a spectral decomposition:\n\n$$\nS^{(m)} U^{(m)}=U^{(m)} \\Lambda^{(m)}\n$$\n\nwhere $U^{(m)}$ is column orthogonal and $\\Lambda^{(m)}$ is a diagonal matrix with the diagonal elements as the eigenvalues of $S^{(m)}$. Substituting the $y$ to $x_{j}$ and applying the approximation above to $S$, we can get:\n\n$$\n\\begin{aligned}\n\\phi_{i}\\left(x_{j}\\right) & \\approx \\sqrt{m} U_{j, i}^{(m)}, \\quad \\lambda_{i} \\approx \\frac{\\lambda_{i}^{(m)}}{m} \\\\\n\\phi_{i}(y) & \\approx \\frac{\\sqrt{m}}{\\lambda_{i}^{(m)}} \\sum_{t=1}^{m} k\\left(y, x_{t}\\right) \\phi_{i}\\left(x_{t}\\right)\n\\end{aligned}\n$$\n\n$\\lambda_{i}$ is eigenvalue of $S$ and $\\lambda_{i}^{(m)}$ is the eigenvalue of $S^{(m)}$. Denote $\\tilde{S}$ as the rank- $m$ approximation of $S$ and $\\tilde{U}, \\tilde{\\Lambda}$ as the approximation for spectral decomposition of $S$. Now we can get an approximation of $S$ with rank $m$ :\n\n$$\n\\tilde{S}=\\tilde{U} \\tilde{\\Lambda} \\tilde{U}^{T}=\\sum_{t=1}^{m} \\tilde{\\lambda}_{t}^{(n)} \\tilde{u}_{t}^{(n)}\\left(\\tilde{u}_{t}^{(n)}\\right)^{T}\n$$\n\nSimilarly, we have:\n\n$$\n\\phi_{i}\\left(x_{j}\\right) \\approx \\sqrt{n} U_{j, i}(n), \\quad \\lambda_{i} \\approx \\frac{\\tilde{\\lambda}_{i}^{(n)}}{n}\n$$\n\nThus\n\n$$\n\\begin{gathered}\n\\tilde{\\lambda}_{i}^{(n)} \\approx \\frac{n \\lambda_{i}^{(m)}}{m} \\\\\n\\tilde{u}_{t}^{(n)} \\approx \\sqrt{\\frac{m}{n}} \\frac{1}{\\lambda_{t}^{(m)}} S_{n, m} u_{t}^{(m)}\n\\end{gathered}\n$$\n\nThen we get an approximation of $S: \\tilde{S} \\approx S_{n, m} S_{m, m}^{\\dagger} S_{m, n} . S$ has a block representation below:\n\n$$\nS=\\left[\\begin{array}{cc}\nS_{m, m} & S_{m, n-m} \\\\\nS_{n-m, m} & S_{n-m, n-m}\n\\end{array}\\right]\n$$\n\n## A. 2 Newton method\n\n## A.2.1 Proof of theorem 1\n\nProof A. $1 A$ is a symmetric positive semi-definite matrix and $A_{i j} \\leq 1, \\forall 1 \\leq i, j \\leq n, A_{i i}=$ 1, $1 \\leq i \\leq n$ in our case. $A_{0}$ is chosen to be $\\alpha A$, so the $A_{k}$ can be written as $A_{k}=C_{k} A=A D_{k}$ for some matrix $C_{k}, D_{k}$, leading to the fact that\n\n$$\nA^{\\dagger} A A_{k}=A_{k}, \\quad A_{k} A A^{\\dagger}=A_{k}\n$$\n\nThis is because $A_{k+1}=A_{k}\\left(2 I_{n}-A A_{k}\\right)=\\left(2 I_{n}-A_{k} A\\right) A_{k}$ and $A_{0}=\\alpha$. We make a difference between $A^{\\dagger}$ and $A_{k+1}$ :\n\n$$\n\\begin{aligned}\nA^{\\dagger}-A_{k+1} & =A^{\\dagger}-2 A_{k}+A_{k} A A_{k} \\\\\n& =A^{\\dagger}-A_{k} A A^{\\dagger}-A^{\\dagger} A A_{k}+A_{k} A A_{k} \\\\\n& =\\left(A^{\\dagger}-A_{k}\\right) A\\left(A^{\\dagger}-A_{k}\\right)\n\\end{aligned}\n$$\n\nWe norm both sides of the equation above:\n\n$$\n\\begin{aligned}\n\\left\\|A^{\\dagger}-A_{k+1}\\right\\| & =\\left\\|\\left(A^{\\dagger}-A_{k}\\right) A\\left(A^{\\dagger}-A_{k}\\right)\\right\\| \\\\\n& \\leq\\left\\|A^{\\dagger}-A_{k}\\right\\|\\left\\|A\\left(A^{\\dagger}-A_{k}\\right)\\right\\|\n\\end{aligned}\n$$\n\nAnd we left multiply $A$ on the both sides of (26), then norm the equation:\n\n$$\n\\begin{aligned}\n\\left\\|A A^{\\dagger}-A A_{k+1}\\right\\| & =\\left\\|A\\left(A^{\\dagger}-A_{k}\\right) A\\left(A^{\\dagger}-A_{k}\\right)\\right\\| \\\\\n& \\leq\\left\\|A A^{\\dagger}-A A_{k}\\right\\|^{2}\n\\end{aligned}\n$$\n\nWe choose $\\alpha$ sufficiently small so that the initial value satisfy $\\left\\|A A^{\\dagger}-A A_{0}\\right\\|<1$. We set $\\alpha=\\frac{2}{\\|A\\|_{1}^{2}}$ to ensure it is small enough [3]. Then the $\\left\\|A A^{\\dagger}-A A_{k}\\right\\| \\rightarrow 0$, when $k \\rightarrow \\infty$. The inequality (27) implies that $A_{k} \\rightarrow A^{\\dagger}, k \\rightarrow \\infty$. ## A.2.2 Proof of proposition 1\n\nProof A. 2 Note that when we multiply $A$ on both sides of (26), the equation turns to be:\n\n$$\n\\begin{aligned}\nA-A A_{k+1} A & =A\\left(A^{\\dagger}-A_{k}\\right) A\\left(A^{\\dagger}-A_{k}\\right) A \\\\\n& =\\left(A A^{\\dagger}-A A_{k}\\right)\\left(A-A A_{k} A\\right)\n\\end{aligned}\n$$\n\nSimilarly norm both sides of (29), considering that $\\left\\|A A^{\\dagger}-A A_{k}\\right\\| \\rightarrow 0$ and $\\left\\|A A^{\\dagger}-A A_{k}\\right\\|<1$ always holds, $\\left\\|A-A A_{k} A\\right\\|$ monotonically decreases to 0 . The inequality (27) implies that $\\left\\|A_{k}-A^{\\dagger}\\right\\|$ decreases to 0 monotonously. Note that although $\\left\\|A-A A_{k} A\\right\\|$ monotonically decreases to $0,\\left\\|A_{k} A A_{k}-A_{k}\\right\\|$ cannot be proved to monotonically decrease to 0 . ## A. 3 Non-linearized gaussian kernel attention\n\nIn our formulation, instead of directly calculating the Gaussian kernel weights, they are approximated. More specifically, the relation between any two tokens is reconstructed via sampled bottleneck tokens. As the number $m$ (e.g., 49), of bottleneck tokens is much smaller than the token sequence length, our attention matrix is of low-rank. This has two favorable consequences: (I) The model now focuses the attentive learning on latent salient information captured by the $m$ bottleneck tokens. (II) The model becomes more robust against the underlying token noise due to the auto-encoder style reconstruction [13]. This explains why the model with an approximated gram matrix performs better than the one with a directly estimated matrix. Further, exact Gaussian kernel attention computation leads to training difficulties. We first hypothesized that this might be due to lacking normalization (as normalization often helps with training stability and convergence), and tested a variant with softmax on top of an exact Gaussian kernel attention matrix.",
    "soft-21": "However, it turns out to suffer from a similar failure.",
    "soft-22": "We cannot find a solid hypothesis so far and will keep investigate this problem. ## A. 4 Attention visualization\n\nFigure 5 shows more visualization of the attention masks by various Transformers [34, 5, 40] and our SOFT. For each model, we show the output from the first two attention heads (up and down row). It is noteworthy that SOFT exhibits better semantic diversity of the multi-head mechanism than other methods. Moreover, when we sample the patch at the boundary of multiple objects, SOFT is able to more precisely capture all these objects. ![](https://cdn.mathpix.com/cropped/2024_09_17_a9868363b814fed268dbg-16.jpg?height=1636&width=1326&top_left_y=488&top_left_x=398)\n\nFigure 5: Comparing the attention heatmaps of a query patch (marked by the cross \" + \") against all the patches of an image, produced by (a) Transformer [34], (b) Performer [5], (c) Nystr\u00f6mformer [40] and (d) Our SOFT.",
    "soft-23": "[^0]:    *Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan University.",
    "soft-24": ""
}