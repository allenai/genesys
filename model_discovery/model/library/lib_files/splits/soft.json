{
    "soft-0": "SOFT: Softmax-free Transformer with Linear Complexity\n\nJiachen Lu1 Jinghan Yao1 Junge Zhang1 Xiatian Zhu2 Hang Xu3 Weiguo Gao1 Chunjing Xu3 Tao Xiang2 Li Zhang1 1Fudan University 2University of Surrey 3Huawei Noah\u2019s Ark Lab https://fudan-zvg.github.io/SOFT Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan University.",
    "soft-1": "Abstract\n\nVision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity. 1 Introduction\n\nRecently the step change brought by Transformers [34] in natural language processing (NLP) [10, 4] seems to have arrived in vision [11, 42, 48, 47]. Indeed, with less inductive bias in its architecture design than Convolution neural networks (CNNs), pure Vision Transformer (ViT) [11] and its variants have shown to be able to outperform CNNs on various vision tasks [8, 16]. However, there is a bottleneck in any Transformer based model, namely its quadratic complexity in both computation and memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens (e.g., words or image patches) as input, the self-attention module iteratively learns the feature representations by relating one token to all other tokens. This results in a quadratic complexity with the token sequence length in both computation (time) and memory (space) since an sized attention matrix needs to be computed and saved during inference. This problem is particularly acute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP even with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from modeling images at high spatial resolutions, which are often crucial for visual recognition tasks. A natural solution is to reduce the complexity of self-attention computation via approximation. Indeed, there have been a number of attempts in NLP [35, 5, 19, 40]. For example, [35] takes a naive approach by shortening the length of Key and Value via learnable projections. Such a coarse approximation would inevitably cause performance degradation. In contrast, [5, 18] both leverage the kernel mechanism to approximate softmax normalization to linearize the computation in self-attention. [19] instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, [40] uses Nystr\u00f6m matrix decomposition to reconstruct the full attention matrix with polynomial iteration for approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization is simply duplicated across the matrix decomposition process, which is theoretically unsound. We empirically found that none of these methods are effective when applied to vision (see Sec. 4.2). In this work, we identify that the limitations of existing efficient Transformers are caused by the use of softmax self-attention, and for the first time propose a softmax-free Transformer. More specifically, in all existing Transformers (with or without linearization), a softmax normalization is needed on top of scaled dot-product between token feature vectors [34]. Keeping this softmax operation challenges any subsequent linearization efforts. To overcome this obstacle, we introduce a novel softmax-free self-attention mechanism, named as SOFT, with linear complexity in both space and time. Specifically, SOFT uses Gaussian kernel to define the similarity (self-attention) function without the need for subsequent softmax normalization. With this softmax-free attention matrix, we further introduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of the approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably computing the Moore-Penrose inverse of the matrix. We make the following contributions. (I) We introduce a novel softmax-free Transformer with linear space and time complexity. (II) Our attention matrix approximation is achieved through a novel matrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual recognition tasks, we design a family of generic backbone architectures with varying capacities using SOFT as the core self-attention component. Extensive experiments show that with a linear complexity (Figure 1(b)), our SOFT models can take in as input much longer image token sequences. As a result, with the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on ImageNet [9] classification in the accuracy/complexity trade-off (Figure 1(a)). 2 Related work\n\nVision Transformers\n\nThere is a surge of research interests recently in exploiting Transformers for visual recognition tasks [37, 36, 42, 32, 45], inspired by their remarkable success in NLP [34, 10, 4]. Core to these NLP and vision transformers is the same self-attention mechanism [34] that computes a self-attention matrix by exhaustively comparing token pairs. This means a quadratic complexity with the sequence length in both space and time, which thus limits the scalability of Transformers in dealing with long sequences. This limitation is more serious in vision than NLP: To process an image with at least thousands of pixels, patch-wise tokenization is a must for Transformers to control the computational cost. Given higher resolution images, the patch size also needs to be enlarged proportionally sacrificing the spatial resolution. This limits the capability of Transformers, e.g., learning fine-grained feature representation as required in many visual recognition tasks. Linear Transformers\n\nRecently, there have been a number of linear/efficient variants [5, 35, 18, 19, 31, 25, 17] of Transformers in NLP. For example, [35] learns to shrink the length of Key and Value based on a low-rank assumption. [19] adopts a hashing strategy to selective the most similar pairs and only compute attention among them. [5, 18] utilize different kernel functions for approximating softmax-based self-attention matrix. [25] applies random feature mapping on the sequences to approach the original softmax function. [17] decreases the time and memory consumption of the attention matrix by replacing the softmax function with its linear-complexity recurrent alternative. When applied to visual recognition tasks, however, we show that these models have considerable performance degradation compared to the standard Transformers [34] (see Sec. 4.2). The most related work to SOFT is [40] which uses the Nystr\u00f6m matrix decomposition to avoid computing the full attention matrix. However, this method suffers from several theoretical defects: (1) As the standard self-attention needs to apply row-wise softmax normalization on the full attention matrix, a direct application of matrix decomposition is infeasible. As a workaround, softmax is simply applied to all the ingredient matrices in [40]. Such an approximation is not guaranteed theoretically. (2) With a polynomial iteration method, it is not guaranteed that the generalized attention matrix inverse can be computed when the matrix is a nearly singular one in practice. In contrast to all the above methods, in this paper we propose a softmax-free self-attention mechanism that facilitates matrix decomposition for complexity minimization with theoretical guarantees. 3 Method\n\n3.1 Softmax-free self-attention formulation\n\nA schematic illustration of our model is given in Figure 2. Let\u2019s first look at our attention module design. Given a sequence of tokens with each token represented by a -dimensional feature vector, self-attention [34] aims to discover the correlations of all token pairs exhaustively. Formally, is first linearly projected into three -dimensional spaces (query, key, and values) as:\n\nQ = X \u200b W q \u2208 \u211d n \u00d7 d e , K = X \u200b W k \u2208 \u211d n \u00d7 d e , V = X \u200b W v \u2208 \u211d n \u00d7 d e , formulae-sequence \ud835\udc44 \ud835\udc4b subscript \ud835\udc4a \ud835\udc5e superscript \u211d \ud835\udc5b subscript \ud835\udc51 \ud835\udc52 \ud835\udc3e \ud835\udc4b subscript \ud835\udc4a \ud835\udc58 superscript \u211d \ud835\udc5b subscript \ud835\udc51 \ud835\udc52 \ud835\udc49 \ud835\udc4b subscript \ud835\udc4a \ud835\udc63 superscript \u211d \ud835\udc5b subscript \ud835\udc51 \ud835\udc52 Q=XW_{q}\\in\\mathbb{R}^{n\\times d_{e}},\\quad K=XW_{k}\\in\\mathbb{R}^{n\\times d_{e}},\\quad V=XW_{v}\\in\\mathbb{R}^{n\\times d_{e}}, (1)\n\nwhere are learnable matrices. Self-attention can be expressed in a generic formulation as:\n\ny i , : = \u2211 j = 1 n \u03b1 \u200b ( Q i , : , K j , : ) \u2299 V j , : , subscript \ud835\udc66 \ud835\udc56 : superscript subscript \ud835\udc57 1 \ud835\udc5b direct-product \ud835\udefc subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc3e \ud835\udc57 : subscript \ud835\udc49 \ud835\udc57 : y_{i,:}=\\sum_{j=1}^{n}\\alpha(Q_{i,:},K_{j,:})\\odot V_{j,:}, (2)\n\nwhere is the Hadamard product, and index the tokens. The key self-attention function is composed of a nonlinear function and a relation function . A dominant instantiation of is the scaled dot-product based softmax self-attention [34], defined as\n\n\u03b2 \u200b ( \u22c5 ) = softmax \u200b ( \u22c5 ) , \u03b3 \u200b ( Q i , : , K j , : ) = 1 d e \u22c5 Q i , : \u22a4 \u200b K j , : . formulae-sequence \ud835\udefd \u22c5 softmax \u22c5 \ud835\udefe subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc3e \ud835\udc57 : \u22c5 1 subscript \ud835\udc51 \ud835\udc52 superscript subscript \ud835\udc44 \ud835\udc56 : top subscript \ud835\udc3e \ud835\udc57 : \\beta(\\cdot)=\\text{softmax}(\\cdot),\\quad\\gamma(Q_{i,:},K_{j,:})=\\frac{1}{\\sqrt{d_{e}}}\\cdot Q_{i,:}^{\\top}K_{j,:}. (3)\n\nWhilst this softmax self-attention has been the de facto choice and seldomly questioned, as discussed earlier it is not necessarily suited for linearization. To facilitate the design of linear self-attention, we introduce a softmax-free self-attention function with the dot-product replaced by a Gaussian kernel as:\n\n\u03b2 \u2032 \u200b ( \u22c5 ) = exp \u200b ( \u22c5 ) , \u03b3 \u2032 \u200b ( Q i , : , K j , : ) = \u2212 1 2 \u200b d e \u22c5 \u2016 Q i , : \u2212 K j , : \u2016 2 2 . formulae-sequence superscript \ud835\udefd \u2032 \u22c5 exp \u22c5 superscript \ud835\udefe \u2032 subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc3e \ud835\udc57 : \u22c5 1 2 subscript \ud835\udc51 \ud835\udc52 superscript subscript norm subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc3e \ud835\udc57 : 2 2 \\beta^{\\prime}(\\cdot)=\\text{exp}(\\cdot),\\quad\\gamma^{\\prime}(Q_{i,:},K_{j,:})=-\\frac{1}{2\\sqrt{d_{e}}}\\cdot\\|Q_{i,:}-K_{j,:}\\|_{2}^{2}. (4)\n\nTo preserve the symmetric property of attention matrix as in Eq (3), we set the project matrices and in Eq (1) identical (i.e., ). Our self-attention matrix is then written as:\n\nS i , j = exp \u200b ( \u2212 1 2 \u200b d e \u22c5 \u2016 Q i , : \u2212 K j , : \u2016 2 2 ) . subscript \ud835\udc46 \ud835\udc56 \ud835\udc57 exp \u22c5 1 2 subscript \ud835\udc51 \ud835\udc52 superscript subscript norm subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc3e \ud835\udc57 : 2 2 S_{i,j}=\\text{exp}\\left(-\\frac{1}{2\\sqrt{d_{e}}}\\cdot\\|Q_{i,:}-K_{j,:}\\|_{2}^{2}\\right). (5)\n\nFor notation simplicity, we define the matrix formulation as: . Remarks\n\nOur self-attention matrix has three important properties: (1) It is symmetric; (2) All the elements lie in a unit range of ; (3) All diagonal elements hold the largest value (self-reinforced), with the bottom ones (corresponding to most dissimilar token pairs) being close to . As Gaussian kernel is a positive definite kernel [12], is deemed a Gram matrix. However, we find that when using our kernel-based self-attention matrix without linearization, the training of a transformer fails to converge. This might explain why softmax dot-product based self-attention [34] is so popular in vanilla transformers. 3.2 Low-rank regularization via matrix decomposition with linear complexity\n\nTo solve the convergence and quadratic complexity problems, we leverage matrix decomposition as a unified solution with low-rank regularization. In particular, we consider Nystr\u00f6m [39], which is originally a low-rank matrix approximation algorithm. This enables our model\u2019s complexity to be reduced significantly without computing the full self-attention matrix . We make this choice because our is positive semi-definite (i.e., a Gram matrix) without follow-up normalization which are all necessary conditions for Nystr\u00f6m. In contrast, [40] totally ignores these requirements, leading to theoretical flaw in its approximation. To define the Nystr\u00f6m method formally, let us express as a block matrix:\n\nS = [ A B B \u22a4 C ] \u2208 \u211d n \u00d7 n , \ud835\udc46 delimited-[] \ud835\udc34 \ud835\udc35 superscript \ud835\udc35 top \ud835\udc36 superscript \u211d \ud835\udc5b \ud835\udc5b S=\\left[\\begin{array}[]{cc}A&B\\\\\nB^{\\top}&C\\end{array}\\right]\\in\\mathbb{R}^{n\\times n}, (6)\n\nwhere , , with . Through Nystr\u00f6m decomposition (see derivative details in Appendix A.1), an approximation can be represented as:\n\nS ^ = [ A B \u22a4 ] \u200b A \u2020 \u200b [ A B ] = P \u22a4 \u200b A \u2020 \u200b P , where P = [ A B ] , formulae-sequence ^ \ud835\udc46 matrix \ud835\udc34 superscript \ud835\udc35 top superscript \ud835\udc34 \u2020 matrix \ud835\udc34 \ud835\udc35 superscript \ud835\udc43 top superscript \ud835\udc34 \u2020 \ud835\udc43 where \ud835\udc43 matrix \ud835\udc34 \ud835\udc35 \\hat{S}=\\begin{bmatrix}A\\\\\nB^{\\top}\\end{bmatrix}A^{\\dagger}\\begin{bmatrix}A&B\\end{bmatrix}=P^{\\top}A^{\\dagger}P,\\quad\\text{where}\\quad P=\\begin{bmatrix}A&B\\end{bmatrix}, (7)\n\nand is the Moore-Penrose (a generalized) inverse of . Sampling\n\nIn the standard Nystr\u00f6m formulation, and are sub-matrices of obtained by randomly sampled tokens, denoted as . We call the sampled as bottleneck tokens. However, we find empirically that random sampling is considerably sensitive to the choice of . We hence explore two additional options by leveraging the structural prior of visual data: (1) Using one convolutional layer with kernel size and stride to learn , and (2) Using average pooling with kernel size and stride to generate . For both, we need to reshape to the form of . Each slide of convolution or pooling produces a token. We set according to the length of such that tokens can be obtained. Our experiments show that a convolution layer performs better in accuracy. We therefore use a convolution layer by default. As is identical to , we have . Given these tokens, we then compute and as:\n\nA = exp \u200b ( Q ~ \u2296 K ~ ) , P = exp \u200b ( Q ~ \u2296 K ) . formulae-sequence \ud835\udc34 exp symmetric-difference ~ \ud835\udc44 ~ \ud835\udc3e \ud835\udc43 exp symmetric-difference ~ \ud835\udc44 \ud835\udc3e A=\\text{exp}(\\widetilde{Q}\\ominus\\widetilde{K}),\\quad P=\\text{exp}(\\widetilde{Q}\\ominus K). (8)\n\nWe finally obtain the regularized self-attention matrix of SOFT as:\n\nS ^ = exp \u200b ( Q \u2296 K ~ ) \u200b ( exp \u200b ( Q ~ \u2296 K ~ ) ) \u2020 \u200b exp \u200b ( Q ~ \u2296 K ) , ^ \ud835\udc46 exp symmetric-difference \ud835\udc44 ~ \ud835\udc3e superscript exp symmetric-difference ~ \ud835\udc44 ~ \ud835\udc3e \u2020 exp symmetric-difference ~ \ud835\udc44 \ud835\udc3e \\hat{S}=\\text{exp}\\left(Q\\ominus\\widetilde{K}\\right)\\left(\\text{exp}\\left(\\widetilde{Q}\\ominus\\widetilde{K}\\right)\\right)^{\\dagger}\\text{exp}\\left(\\widetilde{Q}\\ominus K\\right), (9)\n\nleading to Algorithm 1. The low-rank regularization is conducted as follows. For computing the attention score between any two tokens, we first correlate each of them with sampled tokens using our self-attention function (Eq (5)); With this correlation representation we then compute their similarity under the modulation of the generalized inverse of \u2019s correlation matrix. Similar as standard Nystr\u00f6m, our design associates the input tokens w.r.t.",
    "soft-2": "a small space spanned by sampled tokens, giving a proper estimation of the original attention relationships subject to a low-rank constraint. The correctness of this method is proved in Appendix A.1. Moore-Penrose inverse\n\nAn accurate and commonly used way to calculate the Moore-Penrose inverse is to use Singular Value Decomposition (SVD). Given and its SVD form where are unitary matrices and is a diagonal matrix, the Moore-Penrose inverse of is . Nevertheless, SVD is not friendly to the training process on GPU hence harming the model training efficiency. To solve this issue, we adopt the Newton\u2013Raphson method. It is an iterative algorithm with the -th iteration formulated given the previous iteration as:\n\nA k + 1 = 2 \u200b A k \u2212 A k \u200b A \u200b A k , and A 0 = \u03b1 \u200b A . formulae-sequence subscript \ud835\udc34 \ud835\udc58 1 2 subscript \ud835\udc34 \ud835\udc58 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 and subscript \ud835\udc34 0 \ud835\udefc \ud835\udc34 A_{k+1}=2A_{k}-A_{k}AA_{k},\\quad\\text{and}\\quad A_{0}=\\alpha A. (10)\n\nWe now prove that finally converges to Moore-Penrose inverse of , if is sufficiently small [3]. Theorem 1\n\nWhen is sufficiently small, , converges to . Though which ensures good convergence behavior in Algorithm 2 (see more details in Appendix A.2.1), in practice, we find that using an alternative form gives more stable training and faster convergence. Specifically, in where equals to , we find the smallest that holds this inequality. Then, we initialize as . The following proposition comes with the proof of Theorem 1:\n\nProposition 1\n\nand decreases to monotonously, if is sufficiently small. The detail of proposition 1 is shown in Appendix A.2.2. This ensures that our estimated inverse is sufficiently accurate for matrix decomposition, subject to that our SOFT attention is regularized. Complexity\n\nWe summarize the complexity of SOFT in space and time. For time complexity, it involves: (1) Sampling: . (2) Calculating three decomposed matrices: ; (3) Moore-Penrose inverse: , where is the iteration steps. (4) All matrix multiplication: . The total time complexity is . The space complexity is decided by four decomposed matrices with . As we keep () a fixed constant in our model, both time and space complexity are , making SOFT a linear self-attention. 3.3 Instantiations\n\nFigure 2 shows how our proposed softmax-free self-attention block (SOFT block) can be implemented in a neural network. We replace the self-attention block with our SOFT block in the traditional Transformer, that is, we stack a SOFT block with a feed forward residual block [11] to form a softmax-free Transformer layer (SOFT layer). Focusing on the general image recognition tasks, we integrate our SOFT layer into the recent pyramidal Transformer architecture [36] to form our final model SOFT. Further, several improvements are introduced in patch embedding (i.e., tokenization). Specifically, unlike [36] that uses a combination of non-overlapping convolution and layer normalization [1], we adopt a stack of overlapping convolutions, batch normalization [15] and ReLU non-linearity. Concretely, the is implemented by 3 units of , with the stride of 2, 1, 2 respectively. Then, one such unit is applied to each of three following down-sampling operations with stride of 2 in the multi-stage architecture. The architecture hyper-parameters of SOFT are: : the input channel dimension of SOFT layer. : the embedding dimension of tokens in SOFT block. In practice, we set . : the head number of SOFT block. : the channel dimension of each head and . : the input token sequence length of a SOFT block. : the bottleneck token sequence length of SOFT block. : the sampling ratio of token sequence length sampling, which is the ratio between input token sequence length and the bottleneck token sequence length. : the expansion ratio of the 2-layer feed forward block. In SOFT, for all the stages we set , and , varies in each stage according to the input token sequence length. Table 2 details the family of our SOFT configurations with varying capacities (depth and width). 4 Experiments\n\n4.1 Setup\n\nDataset: We evaluate the proposed SOFT on the ILSVRC-2012 ImageNet-1K dataset [9] with 1.28M training images and 50K validation images from 1,000 classes.",
    "soft-3": "Following the common practice, we train a model on the training set and evaluate on the validation set. Metrics: For model performance, the top-1 accuracy on a single crop is reported. To assess the cost-effectiveness, we also report the model size and floating point operations (i.e., FLOPs). Implementation details: We use the code base [38] with the default setting to train and test all the models. Specifically, we use weight decay of 0.05 and 10 epochs of linear warm-up. We conduct 300 epochs training with an optimizer and decreasing learning rate with the cosine annealing schedule. During training, random flipping, mixup [44] and cutmix [43] are adopted for data augmentation. Label smoothing [29] is used for loss calculation. All our variants are trained with a batch size of 1024 on 32G NVIDIA V100 GPUs. We also implement our method using the Mindspore [23]. 4.2 Comparison with existing linear Transformers\n\nWe compare our method with three existing linear Transformer models: Linformer [35], Performer [5], Nystr\u00f6mformer [40] in terms of model complexity and accuracy.",
    "soft-4": "Two experimental settings are adopted. Under the first setting, for all methods we use the same (Table 2) architecture for a fair comparison. That is, we replace the core self-attention block in SOFT with each baseline\u2019s own attention block with the rest of the architecture unchanged. Note that the spatial reduction module of [36] is a special case of Linformer [35]. We set the reduction ratio to be identical to ours. With the same uniform sampling idea, we replace the 1D window averaging of Nystr\u00f6mformer [40] (for NLP tasks) with 2D average pooling (for images). The downsampling ratio remains identical to ours. It is also worth mentioning that there is no official code released for Reformer [19] and the local Sensitive Hash (LSH) module has strict requirements on the length of input tokens. We thus do not include this method in our comparison. From Table 1 we can make the following observations: (i) Linear Transformer methods substantially reduce the memory and FLOPs while maintain similar parameter size comparing to the Transformer on the architecture; (ii) Our approach SOFT achieves the best classification accuracy among all the linearization methods. (iii) Our inference speed is on-par with other compared linear Transformers and our training speed is slightly slower than Nystromformer and both are slower than Performer and Linformer. Note that the slow training speed of our model is mostly due to the Newton-Raphson iteration which can only be applied sequentially for ensuring the accuracy of Moore-Penrose inverse. In summary, due to the on-par inference speed we consider the training cost increase is a price worth paying for our superior accuracy. Under the second setting, we focus on the memory efficiency of SOFT against the baselines. Here we follow the ViT [11] network structure, stacking 12 attention layers with hidden dimension , heads , bottleneck token sequence length . Different attention blocks from the three linearized Transformer variants, Linformer [35], Performer [5], and Nystr\u00f6mformer [40] are studied. For each Transformer variant, we adjust its token sequence length in a linear increment. Specifically, we use a token sequence length of where and set batch size 1 to verify whether the memory consumption increases \u201cquadratically\u201d or \u201clinearly\u201d. Figure 1(b) shows all compared transformer variants including our SOFT indeed have a linear memory usage complexity. This is in contrast with the standard Transformer which cannot cope with long token sequences with a quadratic complexity. 4.3 Comparison with state-of-the-art CNNs and ViTs\n\nWe compare with state-of-the-art alternatives and report the top-1 accuracy on the ImageNet-1K validation set. FLOPs are calculated at batch size 1. From Figure 1(a) and Table 3, the following observations are made: (i) Overall, ViT and its variants yield better classification accuracy over CNNs. (ii) We achieve the best performance among the recent pure vision Transformer based methods including ViT [11] and DeiT [32], as well as the state-of-the-art CNN RegNet [27]. (iii) Our SOFT outperforms the most similar (in architecture configuration) Transformer counterparts PVT [36] at all variants. Since the attention module is the main difference, this validates directly the effectiveness of our model. (iv) We can also beat the latest ViT variants Twins [6] which is designed to address the efficiency limitation of ViT. We have done so with less parameters and fewer float point computation. To gain some insights into how attention is learned using our SOFT and the alternatives, Figure 3 shows the attention masks of various compared models. For each model, we show the output from the first two attention heads. It is evident that SOFT exhibits robustness and versatility in capturing local and long distance relations among pixels. It is interesting to note that, although SOFT is trained on an object categorization dataset in ImageNet [9], it seems to be able to learn both semantic concepts shared across instances in the same category and instance specific features.",
    "soft-5": "For instance, in the bottom-right example of a bird class, one attention head focuses on the black bird only, while the other attend to both birds in the image. More examples are shown in Appendix A.4. 4.4 Ablation studies\n\nPyramidal architecture:\n\nUnlike the earlier non-pyramidal vision Transformers (e.g., ViT [11]), most recent pyramidal (multi-scale) Transformers (e.g., PVT [36]) use convolution layers to reduce the spatial resolution (i.e., token sequence length) between stages. In this study, we ablate SOFT with a pyramidal architecture (our default SOFT-), SOFT w/o a pyramidal architecture and DeiT-S [32] (no pyramidal architecture either). We replace the Transformer layer with a SOFT layer to get SOFT w/o a pyramidal architecture. Note all three variants have similar parameters and FLOPs. Table 5a shows that the conv-based pyramidal architecture is clearly superior to a non-pyramidal design, and our non-pyramidal counterpart is even slightly better than DeiT-S [32] whilst enjoying linear complexity. Bottleneck token sequence length:\n\nIn this study, we examine how the bottleneck token sequence length , sampled from tokens, influences the model\u2019s performance. We change the bottleneck token sequence length in all stages to . Table 4a shows that longer bottleneck token would increase the memory cost and the computational overhead.",
    "soft-6": "seems to give the best trade-off between the performance and computational overhead. The memory usage is measured with the batch size of 128. Token sampling:\n\nThe sampling function in SOFT can assume different forms. Convolution: The sequence is first reshaped to a feature map . convolution kernel with stride of is applied for downsampling, where . The output channel size is also kept and no bias is used. At last, the feature map is reshaped back to the sequence. Average pooling: using a kernel and stride, where . Random sampling: tokens are randomly picked from tokens. Biased sampling: We pick tokens with a biased policy. Here, the first tokens are picked. Table 4b shows that average pooling yields the best performance while with less computational overhead comparing to convolution. Biased sampling can miss the most salient samples, and there is no guarantee that random sampling can keep the uniformity of the chosen samples. This result thus justifies the choice of using average pooling in SOFT. Overlapped convolution:\n\nWe ablate SOFT with overlapped convolution (our default choice, same as many recent works) and SOFT with non-overlapped convolution in our configuration. Table 5b shows that SOFT with overlapped convolution performs better than SOFT without overlapped convolution. Our non-overlapped convolution variant still outperforms the PVT [36] which also has the same non-overlapped convolution by a clear margin. Newton-Raphson\u2019s convergence: We study how many iterations the Newton-Raphson method needs to converge when computing the Moore-Penrose inverse .",
    "soft-7": "We use with (see Proposition 1) as the convergence metric to quantify the difference between and . Figure 4 shows that our approximation converges within 20 iterations across all stages. 4.5 Additional experiments on NLP tasks\n\nIn this section, we evaluate our method against other linear counterparts on four tasks of the Long Range Arena (LRA) [30] benchmark e.g., Listops [24], byte-level IMDb reviews text classification [22], byte-level document retrieval [26], and image classification on sequences of pixels [20]. Implementations. We use the Pytorch version of LRA [30] benchmark, provided by [40]. For the evaluation protocol, we strictly follow [30, 40]. We omit the Pathfinder(1K) task as we cannot replicate the result of Nystr\u00f6mformer [40]. For our SOFT, we simply use the average pooling with window size 4, stride 4 to sample the bottlenecks. We follow the configurations of [40], with 2 layers, 64 and 128 hidden dimension respectively, and 2 attention heads. The results in Table 6 shows that our SOFT outperforms both the standard and alternative efficient Transformers on three out of four tasks, as well as the average result. 5 Conclusions\n\nWe have introduced a novel softmax-free self-attention (SOFT) mechanism for linearizing Transformer\u2019s complexity in space and time. Unlike existing linear Transformers that aim to approximate the conventional softmax based self-attention, SOFT employs a Gaussian kernel based attention which eliminates the need for softmax normalization. This design enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments show that SOFT yields superior trade-off in accuracy and complexity. Acknowledgment\n\nThis work was funded in part by Shanghai Municipal Science and Technology Major Projects (No.2018SHZDZX01 and No.2021SHZDZX0103), Mindspore, National Science Foundation of China under Grant No.11690013, 71991471 and the scientific-technological innovation plan program of Universities guided by the Ministry of Education, China. References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint, 2016. [2] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention.",
    "soft-8": "arXiv preprint, 2021. [3] Adi Ben-Israel and Dan Cohen. On iterative computation of generalized inverses and associated projections.",
    "soft-9": "SIAM Journal on Numerical Analysis, 1966. [4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. [5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In ICLR, 2021. [6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers.",
    "soft-10": "arXiv preprint, 2021. [7] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint, 2021. [8] St\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In ICML, 2021. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2018. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale.",
    "soft-11": "In ICLR, 2021. [12] Gregory E Fasshauer. Positive definite kernels: past, present and future. Dolomites Research Notes on Approximation, 2011. [13] Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin. Is attention better than matrix decomposition? In ICLR, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
    "soft-12": "In ICML, 2015. [16] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention.",
    "soft-13": "arXiv preprint, 2021. [17] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. [18] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. [19] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "soft-14": "In ICLR, 2020. [20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Citeseer, 2009. [21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows.",
    "soft-15": "arXiv preprint, 2021. [22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, 2011. [23] Mindspore. https://www.mindspore.cn/, 2020. [24] Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning.",
    "soft-16": "arXiv preprint, 2018. [25] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. [26] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 2013. [27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In CVPR, 2020. [28] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition. arXiv preprint, 2021. [29] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. [30] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint, 2020. [31] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv preprint, 2020. [32] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint, 2020. [33] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprint, 2021. [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [35] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "soft-17": "arXiv preprint, 2020. [36] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.",
    "soft-18": "arXiv preprint, 2021. [37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018. [38] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019. [39] Christopher Williams and Matthias Seeger. Using the nystr\u00f6m method to speed up kernel machines. In NeurIPS, 2001. [40] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In AAAI, 2021. [41] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv preprint, 2021. [42] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
    "soft-19": "arXiv preprint, 2021. [43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. [44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization.",
    "soft-20": "arXiv preprint, 2017. [45] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph message passing networks. In CVPR, 2020. [46] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. [47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In CVPR, 2021. [48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021. Appendix A Appendix\n\nA.1 Nystr\u00f6m method\n\nNystr\u00f6m method [39] aims to calculate a low-rank approximation for a Gram matrix. For Transformers, the self-attention matrix can be viewed as a Gram matrix with a Gaussian kernel applied to the query , with each element expressed as:\n\nS i \u200b j = k \u200b ( Q i , : , Q j , : ) = exp \u200b ( \u2212 \u2016 Q i , : \u2212 Q j , : \u2016 2 2 2 \u200b d ) , subscript \ud835\udc46 \ud835\udc56 \ud835\udc57 \ud835\udc58 subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc44 \ud835\udc57 : exp superscript subscript norm subscript \ud835\udc44 \ud835\udc56 : subscript \ud835\udc44 \ud835\udc57 : 2 2 2 \ud835\udc51 S_{ij}=k\\big{(}Q_{i,:},Q_{j,:}\\big{)}=\\text{exp}(-\\frac{\\|Q_{i,:}-Q_{j,:}\\|_{2}^{2}}{2\\sqrt{d}}), (11)\n\nmeans operating Gaussian kernel to , which can be written in the feature space as:\n\nk \u200b ( x , y ) = \u2211 i = 1 n \u03bb i \u200b \u03d5 i \u200b ( x ) \u200b \u03d5 i \u200b ( y ) , \ud835\udc58 \ud835\udc65 \ud835\udc66 superscript subscript \ud835\udc56 1 \ud835\udc5b subscript \ud835\udf06 \ud835\udc56 subscript italic-\u03d5 \ud835\udc56 \ud835\udc65 subscript italic-\u03d5 \ud835\udc56 \ud835\udc66 k(x,y)=\\sum\\limits_{i=1}^{n}\\lambda_{i}\\phi_{i}(x)\\phi_{i}(y), (12)\n\nis the dimension of a feature space, denotes the eigenvalue and denotes the eigenfunction of kernel .",
    "soft-21": "According to the eigenfunction\u2019s definition, we can get:\n\n\u222b k \u200b ( y , x ) \u200b \u03d5 i \u200b ( x ) \u200b p \u200b ( x ) \u200b \ud835\udc51 x = \u03bb i \u200b \u03d5 i \u200b ( y ) , \ud835\udc58 \ud835\udc66 \ud835\udc65 subscript italic-\u03d5 \ud835\udc56 \ud835\udc65 \ud835\udc5d \ud835\udc65 differential-d \ud835\udc65 subscript \ud835\udf06 \ud835\udc56 subscript italic-\u03d5 \ud835\udc56 \ud835\udc66 \\int k(y,x)\\phi_{i}(x)p(x)dx=\\lambda_{i}\\phi_{i}(y), (13)\n\nwhere is the probability distribution of . And {} are -orthogonal:\n\n\u222b \u03d5 i \u200b ( x ) \u200b \u03d5 j \u200b ( x ) \u200b p \u200b ( x ) \u200b \ud835\udc51 x = \u03b4 i \u200b j . subscript italic-\u03d5 \ud835\udc56 \ud835\udc65 subscript italic-\u03d5 \ud835\udc57 \ud835\udc65 \ud835\udc5d \ud835\udc65 differential-d \ud835\udc65 subscript \ud835\udeff \ud835\udc56 \ud835\udc57 \\int\\phi_{i}(x)\\phi_{j}(x)p(x)dx=\\delta_{ij}. (14)\n\nis when , when . To get an approximation of the eigenfunctions, we sample from , then:\n\n1 q \u200b \u2211 t = 1 q k \u200b ( y , x t ) \u200b \u03d5 i \u200b ( x t ) \u2248 \u03bb i \u200b \u03d5 i \u200b ( y ) , 1 \ud835\udc5e superscript subscript \ud835\udc61 1 \ud835\udc5e \ud835\udc58 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript italic-\u03d5 \ud835\udc56 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udf06 \ud835\udc56 subscript italic-\u03d5 \ud835\udc56 \ud835\udc66 \\frac{1}{q}\\sum\\limits_{t=1}^{q}k(y,x_{t})\\phi_{i}(x_{t})\\approx\\lambda_{i}\\phi_{i}(y), (15)\n\n1 q \u200b \u2211 t = 1 q \u03d5 i \u200b ( x t ) \u200b \u03d5 j \u200b ( x t ) \u2248 \u03b4 i \u200b j . 1 \ud835\udc5e superscript subscript \ud835\udc61 1 \ud835\udc5e subscript italic-\u03d5 \ud835\udc56 subscript \ud835\udc65 \ud835\udc61 subscript italic-\u03d5 \ud835\udc57 subscript \ud835\udc65 \ud835\udc61 subscript \ud835\udeff \ud835\udc56 \ud835\udc57 \\frac{1}{q}\\sum\\limits_{t=1}^{q}\\phi_{i}(x_{t})\\phi_{j}(x_{t})\\approx\\delta_{ij}.",
    "soft-22": "(16)\n\nThis inspires us to approximate the Gram matrix . Let be a submatrix of , consisting of elements from . Gram matrix is a symmetric positive semi-definite matrix, so it has a spectral decomposition:\n\nS ( m ) \u200b U ( m ) = U ( m ) \u200b \u039b ( m ) , superscript \ud835\udc46 \ud835\udc5a superscript \ud835\udc48 \ud835\udc5a superscript \ud835\udc48 \ud835\udc5a superscript \u039b \ud835\udc5a S^{(m)}U^{(m)}=U^{(m)}\\Lambda^{(m)}, (17)\n\nwhere is column orthogonal and is a diagonal matrix with the diagonal elements as the eigenvalues of . Substituting the to and applying the approximation above to , we can get:\n\n\u03d5 i \u200b ( x j ) \u2248 m \u200b U j , i ( m ) , \u03bb i \u2248 \u03bb i ( m ) m , formulae-sequence subscript italic-\u03d5 \ud835\udc56 subscript \ud835\udc65 \ud835\udc57 \ud835\udc5a superscript subscript \ud835\udc48 \ud835\udc57 \ud835\udc56 \ud835\udc5a subscript \ud835\udf06 \ud835\udc56 superscript subscript \ud835\udf06 \ud835\udc56 \ud835\udc5a \ud835\udc5a \\phi_{i}(x_{j})\\approx\\sqrt{m}U_{j,i}^{(m)},\\quad\\lambda_{i}\\approx\\frac{\\lambda_{i}^{(m)}}{m}, (18)\n\n\u03d5 i \u200b ( y ) \u2248 m \u03bb i ( m ) \u200b \u2211 t = 1 m k \u200b ( y , x t ) \u200b \u03d5 i \u200b ( x t ) , subscript italic-\u03d5 \ud835\udc56 \ud835\udc66 \ud835\udc5a superscript subscript \ud835\udf06 \ud835\udc56 \ud835\udc5a superscript subscript \ud835\udc61 1 \ud835\udc5a \ud835\udc58 \ud835\udc66 subscript \ud835\udc65 \ud835\udc61 subscript italic-\u03d5 \ud835\udc56 subscript \ud835\udc65 \ud835\udc61 \\phi_{i}(y)\\approx\\frac{\\sqrt{m}}{\\lambda_{i}^{(m)}}\\sum\\limits_{t=1}^{m}k(y,x_{t})\\phi_{i}(x_{t}), (19)\n\nis eigenvalue of and is the eigenvalue of . Denote as the rank- approximation of and as the approximation for spectral decomposition of . Now we can get an approximation of with rank :\n\nS ~ = U ~ \u200b \u039b ~ \u200b U ~ T = \u2211 t = 1 m \u03bb t ~ ( n ) \u200b u ~ t ( n ) \u200b ( u ~ t ( n ) ) T . ~ \ud835\udc46 ~ \ud835\udc48 ~ \u039b superscript ~ \ud835\udc48 \ud835\udc47 superscript subscript \ud835\udc61 1 \ud835\udc5a superscript ~ subscript \ud835\udf06 \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc62 \ud835\udc61 \ud835\udc5b superscript superscript subscript ~ \ud835\udc62 \ud835\udc61 \ud835\udc5b \ud835\udc47 \\tilde{S}=\\tilde{U}\\tilde{\\Lambda}\\tilde{U}^{T}=\\sum\\limits_{t=1}^{m}\\tilde{\\lambda_{t}}^{(n)}\\tilde{u}_{t}^{(n)}(\\tilde{u}_{t}^{(n)})^{T}. (20)\n\nSimilarly, we have:\n\n\u03d5 i \u200b ( x j ) \u2248 n \u200b U j , i \u200b ( n ) , \u03bb i \u2248 \u03bb i ~ ( n ) n . formulae-sequence subscript italic-\u03d5 \ud835\udc56 subscript \ud835\udc65 \ud835\udc57 \ud835\udc5b subscript \ud835\udc48 \ud835\udc57 \ud835\udc56 \ud835\udc5b subscript \ud835\udf06 \ud835\udc56 superscript ~ subscript \ud835\udf06 \ud835\udc56 \ud835\udc5b \ud835\udc5b \\phi_{i}(x_{j})\\approx\\sqrt{n}U_{j,i}(n),\\quad\\lambda_{i}\\approx\\frac{\\tilde{\\lambda_{i}}^{(n)}}{n}. (21)\n\nThus\n\n\u03bb i ~ ( n ) \u2248 n \u200b \u03bb i ( m ) m , superscript ~ subscript \ud835\udf06 \ud835\udc56 \ud835\udc5b \ud835\udc5b superscript subscript \ud835\udf06 \ud835\udc56 \ud835\udc5a \ud835\udc5a \\tilde{\\lambda_{i}}^{(n)}\\approx\\frac{n\\lambda_{i}^{(m)}}{m}, (22)\n\nu ~ t ( n ) \u2248 m n \u200b 1 \u03bb t ( m ) \u200b S n , m \u200b u t ( m ) . superscript subscript ~ \ud835\udc62 \ud835\udc61 \ud835\udc5b \ud835\udc5a \ud835\udc5b 1 superscript subscript \ud835\udf06 \ud835\udc61 \ud835\udc5a subscript \ud835\udc46 \ud835\udc5b \ud835\udc5a superscript subscript \ud835\udc62 \ud835\udc61 \ud835\udc5a \\tilde{u}_{t}^{(n)}\\approx\\sqrt{\\frac{m}{n}}\\frac{1}{\\lambda_{t}^{(m)}}S_{n,m}u_{t}^{(m)}. (23)\n\nThen we get an approximation of : . has a block representation below:\n\nS = [ S m , m S m , n \u2212 m S n \u2212 m , m S n \u2212 m , n \u2212 m ] . \ud835\udc46 matrix subscript \ud835\udc46 \ud835\udc5a \ud835\udc5a subscript \ud835\udc46 \ud835\udc5a \ud835\udc5b \ud835\udc5a subscript \ud835\udc46 \ud835\udc5b \ud835\udc5a \ud835\udc5a subscript \ud835\udc46 \ud835\udc5b \ud835\udc5a \ud835\udc5b \ud835\udc5a S=\\begin{bmatrix}S_{m,m}&S_{m,n-m}\\\\\nS_{n-m,m}&S_{n-m,n-m}\\end{bmatrix}. (24)\n\nA.2 Newton method\n\nA.2.1 Proof of theorem 1\n\nProof A.1\n\nis a symmetric positive semi-definite matrix and , , in our case. is chosen to be , so the can be written as for some matrix , leading to the fact that\n\nA \u2020 \u200b A \u200b A k = A k , A k \u200b A \u200b A \u2020 = A k . formulae-sequence superscript \ud835\udc34 \u2020 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 subscript \ud835\udc34 \ud835\udc58 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 A^{\\dagger}AA_{k}=A_{k},\\quad A_{k}AA^{\\dagger}=A_{k}. (25)\n\nThis is because and . We make a difference between and :\n\nA \u2020 \u2212 A k + 1 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 1 \\displaystyle A^{\\dagger}-A_{k+1} = A \u2020 \u2212 2 \u200b A k + A k \u200b A \u200b A k absent superscript \ud835\udc34 \u2020 2 subscript \ud835\udc34 \ud835\udc58 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 \\displaystyle=A^{\\dagger}-2A_{k}+A_{k}AA_{k} = A \u2020 \u2212 A k \u200b A \u200b A \u2020 \u2212 A \u2020 \u200b A \u200b A k + A k \u200b A \u200b A k absent superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 superscript \ud835\udc34 \u2020 superscript \ud835\udc34 \u2020 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 \\displaystyle=A^{\\dagger}-A_{k}AA^{\\dagger}-A^{\\dagger}AA_{k}+A_{k}AA_{k} = ( A \u2020 \u2212 A k ) \u200b A \u200b ( A \u2020 \u2212 A k ) . absent superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \\displaystyle=(A^{\\dagger}-A_{k})A(A^{\\dagger}-A_{k}). (26)\n\nWe norm both sides of the equation above:\n\n\u2016 A \u2020 \u2212 A k + 1 \u2016 norm superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 1 \\displaystyle\\|A^{\\dagger}-A_{k+1}\\| = \u2016 ( A \u2020 \u2212 A k ) \u200b A \u200b ( A \u2020 \u2212 A k ) \u2016 absent norm superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \\displaystyle=\\|(A^{\\dagger}-A_{k})A(A^{\\dagger}-A_{k})\\| \u2264 \u2016 A \u2020 \u2212 A k \u2016 \u200b \u2016 A \u200b ( A \u2020 \u2212 A k ) \u2016 . absent norm superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 norm \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \\displaystyle\\leq\\|A^{\\dagger}-A_{k}\\|\\|A(A^{\\dagger}-A_{k})\\|. (27)\n\nAnd we left multiply on the both sides of (26), then norm the equation:\n\n\u2016 A \u200b A \u2020 \u2212 A \u200b A k + 1 \u2016 norm \ud835\udc34 superscript \ud835\udc34 \u2020 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 1 \\displaystyle\\|AA^{\\dagger}-AA_{k+1}\\| = \u2016 A \u200b ( A \u2020 \u2212 A k ) \u200b A \u200b ( A \u2020 \u2212 A k ) \u2016 absent norm \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \\displaystyle=\\|A(A^{\\dagger}-A_{k})A(A^{\\dagger}-A_{k})\\| \u2264 \u2016 A \u200b A \u2020 \u2212 A \u200b A k \u2016 2 . absent superscript norm \ud835\udc34 superscript \ud835\udc34 \u2020 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 2 \\displaystyle\\leq\\|AA^{\\dagger}-AA_{k}\\|^{2}. (28)\n\nWe choose sufficiently small so that the initial value satisfy . We set to ensure it is small enough [3]. Then the , when . The inequality (27) implies that . A.2.2 Proof of proposition 1\n\nProof A.2\n\nNote that when we multiply on both sides of (26), the equation turns to be:\n\nA \u2212 A \u200b A k + 1 \u200b A \ud835\udc34 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 1 \ud835\udc34 \\displaystyle A-AA_{k+1}A = A \u200b ( A \u2020 \u2212 A k ) \u200b A \u200b ( A \u2020 \u2212 A k ) \u200b A absent \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 superscript \ud835\udc34 \u2020 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 \\displaystyle=A(A^{\\dagger}-A_{k})A(A^{\\dagger}-A_{k})A (29) = ( A \u200b A \u2020 \u2212 A \u200b A k ) \u200b ( A \u2212 A \u200b A k \u200b A ) . absent \ud835\udc34 superscript \ud835\udc34 \u2020 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 \ud835\udc34 subscript \ud835\udc34 \ud835\udc58 \ud835\udc34 \\displaystyle=(AA^{\\dagger}-AA_{k})(A-AA_{k}A). Similarly norm both sides of (29), considering that and always holds, monotonically decreases to . The inequality (27) implies that decreases to monotonously . Note that although monotonically decreases to , cannot be proved to monotonically decrease to 0. A.3 Non-linearized gaussian kernel attention\n\nIn our formulation, instead of directly calculating the Gaussian kernel weights, they are approximated. More specifically, the relation between any two tokens is reconstructed via sampled bottleneck tokens. As the number (e.g., 49), of bottleneck tokens is much smaller than the token sequence length, our attention matrix is of low-rank. This has two favorable consequences: (I) The model now focuses the attentive learning on latent salient information captured by the bottleneck tokens. (II) The model becomes more robust against the underlying token noise due to the auto-encoder style reconstruction [13]. This explains why the model with an approximated gram matrix performs better than the one with a directly estimated matrix. Further, exact Gaussian kernel attention computation leads to training difficulties. We first hypothesized that this might be due to lacking normalization (as normalization often helps with training stability and convergence), and tested a variant with softmax on top of an exact Gaussian kernel attention matrix. However, it turns out to suffer from a similar failure. We cannot find a solid hypothesis so far and will keep investigate this problem. A.4 Attention visualization\n\nFigure 5 shows more visualization of the attention masks by various Transformers [34, 5, 40] and our SOFT. For each model, we show the output from the first two attention heads (up and down row). It is noteworthy that SOFT exhibits better semantic diversity of the multi-head mechanism than other methods. Moreover, when we sample the patch at the boundary of multiple objects, SOFT is able to more precisely capture all these objects. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Mar 19 15:24:20 2024 by LaTeXML"
}