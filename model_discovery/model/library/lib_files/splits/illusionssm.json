{
    "illusionssm-0": "# The Illusion of State in State-Space Models \n\nWilliam Merrill ${ }^{1}$ Jackson Petty ${ }^{1}$ Ashish Sabharwal ${ }^{2}$\n\n\n#### Abstract\n\nState-space models (SSMs) have emerged as a potential alternative to transformers.",
    "illusionssm-1": "One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill \\& Sabharwal, 2023a), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks. But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of S4, Mamba, and related SSMs is limited very similarly to transformers (within $\\mathrm{TC}^{0}$ ), meaning these SSMs cannot solve simple state-tracking problems like permutation composition and consequently are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that S4 and Mamba indeed struggle with state tracking. Thus, despite their recurrent formulation, the \"state\" in common SSMs is an illusion: S4, Mamba, and related models have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world statetracking problems. Moreover, we show that only a minimal change allows SSMs to express and learn state tracking, motivating the development of new, more expressive SSM architectures. ## 1. Introduction\n\nRecent theoretical work has shown that transformer architecture based models are incapable of expressing inherently sequential computation (Merrill \\& Sabharwal, 2023a). These results reveal a surprising limitation of transformers: they\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_9e1af1411aaa068b025bg-01.jpg?height=560&width=551&top_left_y=655&top_left_x=1183)\n$\\mathrm{x}=[0,0,1,0,0]$ $\\mathrm{x}[1], \\mathrm{x}[3]=\\mathrm{x}[3], \\mathrm{x}[1]$ \\# Swap 1, 3\n\nAlice, Bob, Carl, Dan, and Emma each have a coin.",
    "illusionssm-2": "All are dimes except Carl's. Alice and Carl trade coins. Figure 1: We prove that SSMs, like transformers, cannot solve inherently sequential problems like permutation composition $\\left(S_{5}\\right)$, which lies at the heart of state-tracking problems like tracking chess moves in source-target notation (see Section 3.2), evaluating Python code, or entity tracking. Thus, SSMs cannot, in general, solve these problems either. Code: http://jpetty.org/ssm-illusion\ncannot express simple kinds of state tracking problems, such as composing sequences of permutations, which even simple recurrent neural networks (RNNs) can naturally express. In a different line of work, state space model (SSM) architectures (Gu et al., 2021; 2022a; Fu et al., 2023; Gu \\& Dao, 2023; Wang et al., 2024) have been introduced as an alternative to transformers, with the goal of achieving RNN-like expressive power for handling problems that are naturally stateful and sequential (Gu et al., 2021; 2022b). But does the seemingly stateful design of SSMs truly enable them to solve sequential and state-tracking problems that transformers cannot? If so, this would be a promising property of SSMs because state tracking is at the heart of large language model (LLM) capabilities such as tracking entities in a narrative\n(Heim, 1983; Kim \\& Schuster, 2023), playing chess under certain notation ${ }^{1}$, or evaluating code. This would motivate further research on SSM architectures and their deployment in the next generation of LLMs. In this work, we show that the apparent stateful design of SSMs is an illusion as far as their expressive power is concerned. In contrast to the suggestion by Gu et al. (2021; 2022b) (and, perhaps, a broader belief in the community) that SSMs have expressive power for state tracking similar to RNNs, we prove theoretically that linear and Mamba-style SSMs, like transformers, cannot express inherently sequential problems, including state-tracking problems like composing permutations that RNNs can easily express. Further, our experiments confirm this prediction: both transformers and these SSMs cannot learn to compose permutations with a fixed number of layers, whereas RNNs can compose permutations with just a single layer. Our results imply that arguments that current SSMs have an advantage over transformers due to being \"more recurrent\" or capable of tracking state are misguided. In fact, the SSM architectures we consider are just as theoretically unequipped for state tracking and recurrent computation as transformers are. We first establish the theoretical weakness of linear SSMs and near generalizations by proving they are in the complexity class L-uniform $\\mathrm{TC}^{0}$, which has been previously shown for transformers (Merrill \\& Sabharwal, 2023a). This implies these SSMs cannot solve inherently sequential problems (formally, problems that are $\\mathrm{NC}^{1}$-hard), including statetracking problems like permutation composition (Liu et al., 2023). Permutation composition is a fundamental problem at the heart of many real-world state-tracking problems such as playing chess, evaluating code, or tracking entities in a narrative (Figure 1), implying solutions to these problems, too, cannot be expressed by SSMs, at least in the worst case. At first glance, our results may appear to contradict Gu et al. (2021)'s claim that linear SSMs can simulate general recurrent models, which can express permutation composition. But the contradiction is resolved by a difference in assumptions: Gu et al. (2021) relied on infinite depth (number of layers) to show that SSMs could simulate RNNs. We, on the other hand, analyze the realistic setting with a bounded number of layers, under which we find that SSMs cannot simulate the recurrent state of an RNN and, in fact, suffer from similar limitations as transformers for state tracking. Empirically, we find that S4 (Gu et al., 2022a) and S6 (Gu \\& Dao, 2023) SSMs, as well as transformers, do not learn to solve the permutation composition state-tracking problem with a fixed number of layers, while simple RNNs can do so with just one layer. This provides empirical support for our\n\n[^1]![](https://cdn.mathpix.com/cropped/2024_09_12_9e1af1411aaa068b025bg-02.jpg?height=288&width=655&top_left_y=227&top_left_x=1147)\n\nFigure 2: Complexity hierarchy within $\\mathrm{NC}^{1}$. Transformers can only recognize languages within $\\mathrm{TC}^{0}$ (Merrill \\& Sabharwal, 2023a), and we show the same for SSMs (Theorems 4.2 and 4.4). Thus, both architectures cannot express the \"hard state tracking\" captured by $\\mathrm{NC}^{1}$-complete problems like $S_{5}$, which can be straightforwardly expressed by RNNs. The figure assumes the widely held conjecture $\\mathrm{TC}^{0} \\neq N \\mathrm{NC}^{1}$. theoretical separation in expressive power for state tracking between SSMs and true recurrent models. We also find that both transformers and SSMs struggle compared to RNNs on state-tracking problems less complex than permutation composition where it is not known whether they can express a solution. Thus, in practice, SSMs may struggle not just on the hardest state-tracking problems like permutation composition but also on easier variants. Finally, we consider a minimal extension of a linear SSM which makes the transition matrix input dependent, similar to Liquid S4 (Hasani et al., 2023). We show that this extension has sufficient expressive power for state tracking and permutation composition. Empirically, we show that our implementation of this extension learns to solve permutation composition with a single layer, just like an RNN, while being similarly parallelizable to other SSMs. It is an open question whether such SSM architectures with greater expressivity for state tracking are practically viable for large-scale language modeling. ## 2. Background\n\nWe first present the SSM architectures we will analyze (Section 2.1). Our analysis of the state tracking capabilities of SSMs borrows deeply from the circuit complexity and algebraic formal language theory literature. We thus review how circuit complexity can be used to analyze the power of neural networks (Section 2.3) and how state-tracking problems can be captured algebraically and analyzed within the circuit complexity framework (Section 3). ### 2.1. Architecture of State-Space Models\n\nSSMs are a neural network architecture for processing sequences similar in design to RNNs or linear dynamical systems. SSMs have been suggested to have two potential advantages compared to transformers owing to their recur-\nrent formulation: faster inference and, possibly, the ability to better express inherently sequential or stateful problems (Gu et al., 2021; 2022b). Several architectural variants of SSMs have been proposed, including S4 (Gu et al., 2022a) and Mamba (Gu \\& Dao, 2023). Recently, SSMs have been shown to achieve strong empirical performance compared to transformers in certain settings, particularly those involving a long context (Gu \\& Dao, 2023; Wang et al., 2024). SSMs consist of SSM layers, which can be thought of as simplified RNN layers. We define a generalized linear SSM layer that encapsulates both S4 (Gu et al., 2022a) and the S6 layer used by Mamba (Gu \\& Dao, 2023) as special cases. Definition 2.1 (Generalized linear SSM layer). Given a sequence ${ }^{2} \\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n} \\in \\mathbb{R}^{k}$, the recurrent form of a linear SSM layer defines a new sequence of states $\\mathbf{h}_{1}, \\ldots, \\mathbf{h}_{n} \\in$ $\\mathbb{R}^{d}$ using projections $\\overline{\\mathbf{A}}_{i} \\in \\mathbb{R}^{d \\times d}$ and $\\overline{\\mathbf{B}}_{i} \\in \\mathbb{R}^{d \\times k}$, which can themselves depend on $\\mathbf{x}_{i}$. For each $1 \\leq i \\leq n$,\n\n$$\n\\mathbf{h}_{i}=\\overline{\\mathbf{A}}_{i} \\mathbf{h}_{i-1}+\\overline{\\mathbf{B}}_{i} \\mathbf{x}_{i} . \\quad \\text { (Recurrent form) }\n$$\n\nThe convolutional form of the SSM layer defines the same ${ }^{3}$ $\\mathbf{h}_{1}, \\ldots, \\mathbf{h}_{n}$ computed differently as a summation:\n\n$$\n\\mathbf{h}_{i}=\\sum_{j=1}^{i}\\left(\\prod_{k=j+1}^{i} \\overline{\\mathbf{A}}_{k}\\right) \\overline{\\mathbf{B}}_{j} \\mathbf{x}_{j} . \\quad \\text { (Convolutional form) }\n$$\n\nThe layer outputs $\\mathbf{y}_{i}=\\mathbf{C}_{i} \\mathbf{h}_{i}+\\mathbf{D}_{i} \\mathbf{x}_{i} \\in \\mathbb{R}^{k}$, where $\\mathbf{C}_{i} \\in$ $\\mathbb{R}^{k \\times d}$ and $\\mathbf{D}_{i} \\in \\mathbb{R}^{k \\times k}$ depend on $\\mathbf{x}_{i}$. Two common cases of this layer are when $\\overline{\\mathbf{A}}_{i}$ does not depend on the input (\"non-gated\"; Section 4.2) and when $\\overline{\\mathbf{A}}_{i}$ is diagonal (Section 4.3). In both of these cases, we will show that the SSM can be simulated in $\\mathrm{TC}^{0}$. A generalized linear SSM is made up of multiple such layers, with a linear projection and a non-linearity applied after every layer (Rush \\& Karamcheti, 2022). Layer-norm can also be applied, either before or after the layer. Practical Details. In S4 and related SSMs, Definition 2.1 is applied elementwise $(k=1)$ across all $m$ elements of the previous layer output ( Gu et al., 2022a). In practice, the weight matrix initialization is crucial for training. Our expressivity results (Theorems 4.2 and 4.4) apply for any generalized linear SSM (including S4 and S6), independent of initialization. In contrast to S4 and S6, H3 (Fu et al., 2023) does not meet Definition 2.1 because the context is not represented by a single vector.",
    "illusionssm-3": "Rather, it resembles a transformer with SSM components. [^2]\n### 2.2. Numeric Datatype\n\nCircuit-complexity analysis of neural networks depends to some degree on low-level details about arithmetic and the underlying datatype $\\mathbb{D}$ used in the network's computation graph. We can think of $\\mathbb{D}$ as parameterized by the number of bits available to represent a number in $\\mathbb{D}$. For instance, non-negative integers in $\\left[0,2^{p}\\right]$ use $p$ bits, signed integers in $\\left[-2^{p}, 2^{p}\\right]$ use $p+1$ bits, FP16 uses 16 bits, etc. Our main results (Theorems 4.2 and 4.4) will go through for any datatype $\\mathbb{D}$ for which the following operations are efficiently parallel-computable (i.e., are in the complexity class L-uniform $\\mathrm{TC}^{0}$, to be defined shortly in Section 2.3):\n\n1. Iterated addition, i.e., summing $n$ numbers in $\\mathbb{D}$\n2. Iterated product, i.e., multiplying $n$ numbers in $\\mathbb{D}$\n3. Matrix powering, i.e., computing the $n$-th power of a fixed-size $d \\times d$ matrix over $\\mathbb{D}$\n\nWhen $\\mathbb{D}$ is any finite-precision datatype, i.e., has a fixed number of bits available (e.g., 16 or 64), then these operations are easily seen to be in L-uniform $\\mathrm{TC}^{0}$. As Merrill \\& Sabharwal (2023b) argue, however, finite-precision datatypes severely limit the expressivity of neural architectures from a formal perspective (e.g., finite-precision transformers cannot represent uniform attention), motivating the use of parameterized datatypes that can (approximately) represent any number with a sufficiently large parameter. Interestingly, when $\\mathbb{D}$ is the datatype of $n$-bit integers, all of the above operations are known to be in L-uniform TC ${ }^{0}$ (Hesse, 2001; Mereghetti \\& Palano, 2000). Realistically, however, neural model implementations use floating point numbers with much fewer than $n$ bits. Following Merrill \\& Sabharwal (2023b), we use the log-precision floating point model, i.e., $c \\log n$ bit floats where $c$ is some fixed constant (see Appendix A for a formal definition). Merrill \\& Sabharwal (2023a) showed that iterated addition over log-precision floats is in L-uniform $\\mathrm{TC}^{0}$. We extend the arguments of Hesse (2001) and Mereghetti \\& Palano (2000) to show that iterated product and matrix powering over log-precision floats are also in L-uniform $\\mathrm{TC}^{0}$ (see Appendix A). ### 2.3. Limits of Transformers via Circuit Complexity\n\nA line of recent work has used circuit complexity and logic formalisms to identify expressivity limitations of transformers on reasoning problems (Angluin et al., 2023; Merrill \\& Sabharwal, 2023a; Liu et al., 2023; Chiang et al., 2023; Merrill \\& Sabharwal, 2023b; Hao et al., 2022); see Strobl et al., 2024 for a survey. In particular, Merrill \\& Sabharwal (2023a) showed transformers can only solve problems in the complexity class $\\mathrm{TC}^{0}$, which is the set of problems that can be recognized by constant-depth, polynomial-size threshold circuit families. Such circuits, in addition to having standard\n\nAND, OR, and NOT gates (of arbitrary fan-in), can also use threshold gates that output 1 iff at least $k$ of the inputs are 1 , where $k$ is a parameter of the gate. Informally, $\\mathrm{TC}^{0}$ can be thought of as the class of problems that can be solved with extremely parallel (constant-depth) computation. ${ }^{4}$\nProblems outside TC $^{0}$, corresponding to problems that are inherently sequential and thus cannot be parallelized, cannot be solved by transformers. No problems in polynomial time are known unconditionally to be outside $\\mathrm{TC}^{0}$, but unless the widely held conjecture that $\\mathrm{TC}^{0} \\neq \\mathrm{NC}^{1}$ is false, many simple $\\mathrm{NC}^{1}$-hard problems are outside $\\mathrm{TC}^{0}$. In particular, this includes simulating finite automata ( $\\mathrm{NC}^{1}$-complete), evaluating boolean formulas ( $N C^{1}$-complete), determining graph connectivity (L-complete), and solving linear equations ( P complete). These problems have already been shown to be inexpressible by transformers (Merrill \\& Sabharwal, 2023a). By showing that SSMs can be simulated in $\\mathrm{TC}^{0}$, we will establish that they also cannot be solved by SSMs. ## 3. State Tracking\n\nInformally, a state-tracking problem is a problem where the text specifies some sequence of updates to the state of the world, and the goal of the problem is to determine what the world state is after the updates have been applied in sequence. The circuit complexity view on the power of neural networks can be combined with other insights from algebraic formal language theory to analyze the kinds of state tracking that SSMs can express. In particular, this theory reveals which kinds of state-tracking problems are (likely) not in $\\mathrm{TC}^{0}$. This will, in turn, allow us to find examples of hard state tracking that models like SSMs cannot express. ### 3.1. State Tracking as a Monoid Word Problem\n\nFrom the perspective of algebraic formal language theory, state tracking over a finite world can be captured as a word problem on a finite monoid (Liu et al., 2023). ${ }^{5}$ Different updates to the world become different elements in the monoid, and resolving the final world state after all the updates have been applied is equivalent to computing the product of a sequence of elements (also called a \"word\"). Definition 3.1 (Word problem). Let $M$ be a finite set, and $(M, \\cdot)$ a finite monoid (i.e., $M$ with identity and associative multiplication). The word problem for $M$ is to re-\n\n[^3]duce sequences in $M^{*}$ under multiplication; that is, send $m_{0} m_{1} \\cdots m_{k}$ to $m_{0} \\cdot m_{1} \\cdot \\ldots \\cdot m_{k} \\in M$. Solving the word problem requires reducing sequences of arbitrary length. Example 3.2. Consider the monoid $\\{0,1\\}$ where $\\cdot$ is addition modulo 2 . The word problem is to compute the parity of a string, e.g., $0011 \\mapsto 0$. From a state-tracking perspective, this monoid captures a world with a single light switch. Identity 0 corresponds to no action, and 1 flips the switch. Modeling state tracking with word problems lets us draw connections between circuit complexity and algebra to understand which word problems are hard to solve. Krohn \\& Rhodes (1965) established that not all word problems are created equal: some, like Example 3.2, are in $\\mathrm{TC}^{0}$, while others are $N C^{1}$-complete, requiring recurrent processing to solve (Immerman \\& Landau, 1989; Barrington, 1989). Because we will show SSMs can be simulated in $\\mathrm{TC}^{0}$, it follows that $\\mathrm{NC}^{1}$-complete state-tracking problems cannot be expressed by SSMs (cf. Figure 2). Whether or not a word problem is $\\mathrm{NC}^{1}$-complete depends on the algebraic structure of the underlying monoid. Barrington (1989) showed that the word problem of every finite nonsolvable ${ }^{6}$ group is $\\mathrm{NC}^{1}$-complete. That non-solvable groups have $\\mathrm{NC}^{1}$-complete word problems is notable because of the ubiquity with which non-solvable groups show up in tasks involving state tracking. The canonical example of an $\\mathrm{NC}^{1}$-complete word problem is that of $S_{5}$, the symmetric group on five elements that encodes the permutations over five objects. As an immediate instantiation of this, consider a document describing a sequence of transpositions: \"swap ball 1 and 3, swap ball 3 and 5, swap ball 4 and 2, ...\". Being able to answer the question \"where does ball 5 end up?\" for all possible swap sequences requires solving the $S_{5}$ word problem. ${ }^{7}$ Beyond permutations, Figure 1 shows how many natural state-tracking problems like tracking chess moves, evaluating code, or tracking entities also encode the structure of $S_{5}$, meaning these state-tracking problems also cannot be expressed by a model in $\\mathrm{TC}^{0}$. Rather, in order to solve these problems, the depth of the model would have to be expanded to accommodate longer inputs. Although the $S_{5}$ word problem is canonical, in this paper we will consider the word problem on a closely related group $A_{5}$ : the alternating group on five elements. We do this for simplicity: $A_{5}$ is a subgroup of $S_{5}$ containing only even permutations, and is the smallest non-solvable subgroup. We will compare the word problem on $A_{5}$ to two other baseline groups: $A_{4} \\times \\mathbb{Z}_{5}$, a non-abelian but solvable group;\n\n[^4]and $\\mathbb{Z}_{60}$, an abelian group encoding mod-60 addition. We choose these groups as points of comparison because they all have 60 distinct elements, meaning that the difficulty in learning their word problems will come only from the complexity of learning the group multiplication operation. ### 3.2. Encoding $S_{5}$ in Chess State Tracking\n\nFigure 1 already gives some intuition into how state-tracking problems encode $S_{5}$. Out of these examples, the most intricated case is chess. We now give a proper reduction from $S_{5}$ to tracking chess moves, showing formally that not just $S_{5}$, but chess state tracking as well, is $\\mathrm{NC}^{1}$-complete. We define the chess state-tracking problem as follows:\n\n- Input: A chessboard state and sequence of chess moves, where each move is written in UCI notation as a tuple (source square, target square). This differs from the standard SAN notation that represents other information like piece type (Toshniwal et al., 2021). - Output: The resulting board state after starting in the initial board state and applying the sequence of moves one after another, ignoring draws. If any move is illegal given the previous board state, a null state is returned. We show that $S_{5}$ can be reduced to chess state tracking, establishing its $\\mathrm{NC}^{1}$-completeness:\n\nProposition 3.3. $S_{5}$ can be reduced to chess state tracking in UCI notation via $\\mathrm{NC}^{0}$ reductions. Proof. Without loss of generality, we consider the variant of $S_{5}$ where the output is true if and only if the original first element returns to the first position after the given sequence of permutations has been applied. The idea, as illustrated in Figure 1, is to map each element of $S_{5}$ to a fixed sequence of chess moves that permutes five pieces accordingly on the chessboard. Given an instance of the $S_{5}$ word problem, we will construct an initial board state and a sequence of moves such that the final chessboard state encodes the output of that $S_{5}$ problem instance. Let $M$ denote the set of chess moves in the UCI, i.e., (source square, target square), notation. Initial Board State. We construct a chessboard similar to Figure 1 but with a black rook at a8 and black queens at b8 to e8. Chess Move Sequence. We then construct a finite function $f: S_{5} \\rightarrow M^{*}$ that encodes a permutation $\\pi$ as a sequence of chess moves. We first factor each permutation $\\pi$ to a sequence of transpositions $\\tau_{1}(\\pi) \\cdots \\tau_{m_{\\pi}}(\\pi)$. Each transposition $\\tau \\in T$ can in turn be expressed as a sequence of chess moves analogously to Figure 1. For example, transposing items 1 and 3 can be expressed as the move sequence: (a8, a7), (a1, b1), (c8, c6), (b1, a1), (a7, c7), (a1, b1), (c6, a6), (b1, a1), (c7, c8), (a1, b1), (a6, a8), (b1, a1), which has the crucial property that it transposes a8 with c8 . We denote the mapping from transpositions to chess move sequences as $f: T \\rightarrow M^{*}$. Putting it all together, we have\n\n$$\nf(\\pi)=\\bigcap_{j=1}^{m_{\\pi}} f\\left(\\tau_{j}(\\pi)\\right)\n$$\n\nTo reduce a sequence of permutations $w \\in S_{5}^{*}$, we let\n\n$$\nf(w)=\\bigodot_{i=1}^{n} f\\left(w_{i}\\right)\n$$\n\nPutting It All Together. We call our oracle for chess state tracking with the constructed initial board state and $f(w)$ as the sequence of chess moves. By construction, we can then return true if and only if the rook is at a8. The reduction can be implemented in $N \\mathrm{C}^{0}$ because it is a simple elementwise mapping of the input tokens, and decoding from the output chessboard is a finite table lookup. As a fun aside, we note that the chess board constructed in the above proof is reachable in a standard chess game. The chess sequences encoding permutation sequences are all valid in the game of chess, except that they ignore the fact that repeated board states in chess technically lead to a draw. Since $S_{5}$ is $\\mathrm{NC}^{1}$-complete under $\\mathrm{AC}^{0}$ reductions and $\\mathrm{NC}^{0} \\subseteq$ $A C^{0}$, we have:\nCorollary 3.4. The chess state-tracking problem is $\\mathrm{NC}^{1}$ complete under $\\mathrm{AC}^{0}$ reductions. Theorem 3.2 of Feng et al. (2023) uses a similar reduction to prove formula evaluation is $\\mathrm{NC}^{1}$-complete. Reductions can be constructed for evaluating Python or tracking entities in a dialog, as suggested by Figure 1. As for chess, the task formatting for entity tracking affects its hardness. For instance, the formatting used by Kim \\& Schuster (2023) in their Figure 1 is not $N C^{1}$-complete, whereas the variant shown in our Figure 1 is. This underscores the value of theory for constructing examples of hard state tracking. ## 4. SSMs Can be Simulated in $\\mathrm{TC}^{0}$\n\nIn this section, we show that the convolutional form of common variants of SSM can be simulated in $\\mathrm{TC}^{0}$. Assuming the convolutional form of the model computes the same function as the recurrent form, this implies such SSMs cannot solve inherently sequential problems, despite their appearance of recurrence and statefulness. We first show containment in TC $^{0}$ for non-gated SSMs (Theorem 4.2), and then show the same holds for diagonal SSMs (Theorem 4.4). ### 4.1. Conditions for Linear SSMs in $\\mathrm{TC}^{0}$\n\nBefore characterizing specific SSM architectures, we first show that the complexity of computing transition matrix products essentially determines the complexity of simulating an SSM with a circuit family. Lemma 4.1. Let $M$ be a log-precision generalized linear SSM. Then there exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes M's convolutional form if:\n\n1. For any integer interval $[j, k]$, the matrix product $\\prod_{i=j}^{k} \\overline{\\mathbf{A}}_{i}$ can be computed in L -uniform $\\mathrm{TC}^{0}$ as a function of $\\overline{\\mathbf{A}}_{j}, \\ldots, \\overline{\\mathbf{A}}_{k}($ to $c \\log n$ precision for any $c>0)$. 2. For $1 \\leq i \\leq n, \\overline{\\mathbf{A}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}$, and $\\mathbf{D}_{i}$ can be computed in L-uniform $\\mathrm{TC}^{0}$ as a function of $\\mathbf{x}_{i}$. Proof. Following the proof structure of Merrill \\& Sabharwal (2023a), we describe how to construct a log-space bounded Turing machine $T_{M}$ that, given $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ as input, prints a circuit that simulates $M$ on this input. We first note that for all processing done before or after an SSM layer (projection, non-linearity, layer norm, etc.), $T_{M}$ can follow known simulations of such operations for transformers (Merrill \\& Sabharwal, 2023a; 2024) to output a $\\mathrm{TC}^{0}$ circuit simulating this processing. We thus focus on simulating an individual SSM layer. Recall from Definition 2.1 that $M$ 's convolutional form requires computing $\\mathbf{h}_{i}=\\sum_{j=1}^{i}\\left(\\prod_{k=j+1}^{i} \\overline{\\mathbf{A}}_{k}\\right) \\overline{\\mathbf{B}}_{j} \\mathbf{x}_{j}$ and $\\mathbf{y}_{i}=\\mathbf{C}_{i} \\mathbf{h}_{i}+\\mathbf{D}_{i} \\mathbf{x}_{i}$. By the second precondition, $T_{M}$ can print a $\\mathrm{TC}^{0}$ circuit that computes all matrices involved here. Further, by the first precondition, $T_{M}$ can also print a $\\mathrm{TC}^{0}$ circuit that computes the innermost product in the computation of each hidden state $\\mathbf{h}_{i}$, namely $\\prod_{k=j+1}^{i} \\overline{\\mathbf{A}}_{k}$. It can now print a $\\mathrm{TC}^{0}$ circuit to multiply the resulting product $^{8}$ with $\\overline{\\mathbf{B}}_{j}$ and $\\mathbf{x}_{j}$, and then print a circuit to compute an iterated sum over the $i$ resulting vectors to compute $\\mathbf{h}_{i}$ (cf. iterated addition in Appendix A). It can similarly print a (simpler) circuit to compute $\\mathbf{y}_{i}$. Thus, the entire SSM layer can be simulated by an L-uniform $\\mathrm{TC}^{0}$ circuit. We will use Lemma 4.1 to show that any non-gated or diagonal generalized linear SSM can be simulated in $\\mathrm{TC}^{0}$. ### 4.2. Non-Gated SSMs are in $T C^{0}$\n\nTheorem 4.2 (Non-gated SSM). Let $M$ be a log-precision generalized linear SSM such that, for any $i$,\n\n$$\n\\overline{\\mathbf{A}}_{i}=\\overline{\\mathbf{A}}, \\quad \\overline{\\mathbf{B}}_{i}=\\overline{\\mathbf{B}}, \\quad \\mathbf{C}_{i}=\\mathbf{C}, \\quad \\mathbf{D}_{i}=\\mathbf{D}\n$$\n\n[^5]Then there exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes M's convolutional form.",
    "illusionssm-4": "Proof. We prove this by showing that both conditions from Lemma 4.1 are satisfied. Computing the matrix product reduces to powering $\\overline{\\mathbf{A}}^{k-j}$. Crucially, we can use the fact that matrix powering over floats is in L-uniform $\\mathrm{TC}^{0}$ (Lemma A.8, extending Mereghetti \\& Palano, 2000). Finally, $\\overline{\\mathbf{A}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}$, and $\\mathbf{D}_{i}$ can be computed in L-uniform $\\mathrm{TC}^{0}$ because they are constants. As S4 satisfies the premises of Theorem 4.2, we obtain:\nCorollary 4.3. There exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes $S 4$ 's convolutional form. ### 4.3. Diagonal SSMs are in $T C^{0}$\n\nTheorem 4.4 (Diagonal SSM). Let $M$ be a log-precision generalized linear $S S M$ where for $1 \\leq i \\leq n$ :\n\n1. the transition matrix $\\overline{\\mathbf{A}}_{i}$ is diagonal, denoted $\\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right)$ where $\\overline{\\mathbf{a}}_{i} \\in \\mathbb{R}^{d}$;\n2. each of $\\overline{\\mathbf{a}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}$ and $\\mathbf{D}_{i}$ can be computed in Luniform $\\mathrm{TC}^{0}$ as a function of $\\mathbf{x}_{i}$. Then there exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes $M$ 's convolutional form. Proof. By the first condition, $\\prod_{i} \\overline{\\mathbf{A}}_{i}=\\prod_{i} \\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right)$. Iterated multiplication of diagonal matrices is reducible to several iterated scalar multiplications, placing this product in L-uniform $\\mathrm{TC}^{0}$ (Lemma A.5). The second condition from Lemma 4.1 is satisfied by assumption. Thus, M's convolutional form is computable in L-uniform TC ${ }^{0}$. Since S6 satisfies the premises of Theorem 4.4, we have:\nCorollary 4.5. There exists an L-uniform $\\mathrm{TC}^{0}$ circuit family that computes S6's convolutional form (used by Mamba).",
    "illusionssm-5": "Proof. For the first condition, note that S6's transition matrix $\\overline{\\mathbf{A}}_{i}$ is defined as $\\exp \\left(\\delta_{i} \\mathbf{A}\\right)$ for a fixed diagonal $\\mathbf{A}$. The set of diagonal matrices is closed under scalar multiplication and matrix exponentiation, so $\\overline{\\mathbf{A}}_{i}$ is also diagonal. See Appendix B for a proof that the second condition is satisfied by the $S 6$ parameterization. Appendix C extends Theorem 4.4 to hold even when $\\left\\{\\overline{\\mathbf{A}}_{i}\\right\\}$ are simultaneously diagonalizable, rather than just diagonal. Specifically, we prove the following generalization:\nTheorem 4.6 (Simultaneously diagonalizable SSM). Let W be a fixed matrix. Let $M$ be a log-precision generalized linear SSM such that, for $1 \\leq i \\leq n$,\n\n1. the transition matrix $\\overline{\\mathbf{A}}_{i}$ is computable to log precision by the expression $\\mathbf{W} \\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right) \\mathbf{W}^{-1}$, where $\\overline{\\mathbf{a}}_{i} \\in \\mathbb{R}^{d}$;\n2. each of $\\overline{\\mathbf{a}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}$ and $\\mathbf{D}_{i}$ can be computed in $\\mathrm{L}-$ uniform $\\mathrm{TC}^{0}$ as a function of $\\mathbf{x}_{i}$. Then there exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes M's convolutional form. This, in turn, allows us to prove that a simultaneously diagonalizable transition matrix generalization of the S 6 layer is also in L-uniform TC $^{0}$ (Corollary C.7). ### 4.4. Discussion\n\nTheorems 4.2 and 4.4 establish that common SSM variants, like transformers, can only express solutions to problems in the class $\\mathrm{TC}^{0}$. This means these SSMs cannot solve $\\mathrm{NC}^{1}$ hard problems like evaluating boolean formulas or graph connectivity. In particular, it shows that they are limited as far as their state tracking capabilities as they are unable to compose permutations (solve the $S_{5}$ word problem):\n\nCorollary 4.7. Assuming $\\mathrm{TC}^{0} \\neq \\mathrm{NC}^{1}$, no log-precision SSM with the S4 or S6 architecture can solve the word problem for $S_{5}$ or any other $\\mathrm{NC}^{1}$-hard problem. In contrast, RNNs can easily express $S_{5}$ via standard constructions that encode finite-state transitions into an RNN (Minsky, 1954; Merrill, 2019). This shows that SSMs cannot express some kinds of state tracking and recurrence that RNNs can. This tempers the claim from Gu et al. (2021, Lemma 3.2) that SSMs have the expressive power to simulate RNNs, which relied on the assumption that SSMs can have infinite depth. In a more realistic setting with a bounded number of layers, our results show SSMs cannot express many state-tracking problems, including those which can be solved by fixed-depth RNNs. ## 5. Extending the Expressive Power of SSMs\n\nWe have shown that $S 4$ and S6, despite their seemingly \"stateful\" design, cannot express problems outside TC ${ }^{0}$, which includes state tracking like $S_{5}$. We show how SSMs can be extended to close the gap in expressive power with RNNs, allowing them to express $S_{5}$. Two simple extensions can bring about this increase in expressive power, assuming layer input dimension $k>1$. First, adding a nonlinearity makes the SSM into an RNN, adding expressive power but degrading parallelism. On the other hand, allowing $\\overline{\\mathbf{A}}_{i}$ to be input-dependent makes the SSM more like a weighted finite automaton (WFA; Mohri, 2009), adding expressive power while remaining parallelizable. ### 5.1. Via Nonlinearities\n\nOne extension to the SSM is to add a nonlinearity, effectively making it an RNN. We call this an RNN-SSM layer:\n\n$$\n\\mathbf{h}_{i}=\\operatorname{sgn}\\left(\\overline{\\mathbf{A}} \\mathbf{h}_{i-1}+\\overline{\\mathbf{B}} x_{i}\\right)\n$$\n\nA model with this architecture can solve the $S_{5}$ word problem when the input dimension $k>1$ :\nTheorem 5.1. For any regular language $L \\subseteq \\Sigma^{*}$ (including the word problem for $S_{5}$ ), there exists a one-layer logprecision $R N N-S S M$ with $k=|\\Sigma|$ that recognizes $L$. Proof. The standard constructions for simulating automata with RNNs (cf. Minsky, 1954; Merrill, 2019) apply. The condition $k=|\\Sigma|$ comes from needing to represent token types with linearly independent vectors. Adding a nonlinearity to the output of an SSM layer (as in Mamba) is not the same thing as an RNN-SSM. Rather, an RNN-SSM applies the nonlinearity at each recurrent update. A downside of this approach is that it becomes nonlinear to parallelize the RNN-SSM computation graph with the SCAN algorithm used by linear SSMs (Blelloch, 1990). ### 5.2. Via Input-Dependent Transition Matrices\n\nAnother way to get greater expressive power is to let the transition matrix $\\overline{\\mathbf{A}}_{i}$ be fully input-dependent, as explored by Liquid S4 (Hasani et al., 2023). To illustrate this, we define a minimally different SSM called Input-Dependent S4 (IDS4) that achieves greater expressive power for state tracking.",
    "illusionssm-6": "Let $\\pi_{\\mathbf{A}}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d \\times d}$ be some affine transformation where the output vector is interpreted as a $d \\times d$ matrix, and let $\\overline{\\mathbf{A}}_{i}=\\pi_{\\mathbf{A}}\\left(\\mathbf{x}_{i}\\right)$.",
    "illusionssm-7": "Let $\\overline{\\mathbf{B}}, \\mathbf{C}, \\mathbf{D}$ be fixed (w.r.t. $i$ ). By Definition 2.1, the IDS4 convolutional form computes an iterated product of non-diagonal, input-dependent matrices:\n\n$$\n\\mathbf{h}_{i}=\\sum_{j=1}^{i}\\left(\\prod_{k=j+1}^{i} \\pi_{\\mathbf{A}}\\left(\\mathbf{x}_{i}\\right)\\right) \\overline{\\mathbf{B}} \\mathbf{x}_{j}\n$$\n\nIn contrast to matrix powers or iterated products of diagonal matrices, iterated products of general matrices cannot be computed in TC $^{0}$ (Mereghetti \\& Palano, 2000).",
    "illusionssm-8": "This means that the arguments from Theorems 4.2 and 4.4 will not go through for IDS4. In fact, we can show IDS4 gains expressive power beyond TC $^{0}$ :\nTheorem 5.2. For any regular language $L \\subseteq \\Sigma^{*}$ (including the word problem for $S_{5}$ ), there exists a one-layer logprecision IDS4 SSM with $k=|\\Sigma|$ that recognizes $\\$ L$, where $\\$ \\notin \\Sigma$ is a special beginning-of-string symbol.",
    "illusionssm-9": "Proof. It suffices to show that IDS4 can simulate a deterministic finite automaton (DFA). We do this via a transition\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9e1af1411aaa068b025bg-08.jpg?height=552&width=1598&top_left_y=239&top_left_x=231)\n\nFigure 3: Minimum number of layers (lower is better) required to attain $>90 \\%$ validation accuracy on group multiplication problems by sequence length and group. RNN and IDS4 models of constant depth can solve arbitrarily long sequences, while transformer, S4, and Mamba models require depths monotonically increasing in sequence length. monoid construction. For any $w \\in \\Sigma^{*}$, let $\\delta_{w}: Q \\rightarrow Q$ be the function mapping a state to its eventual destination state after $w$ is read from that state. For any DFA, this set of functions forms a finite monoid (the transition monoid) under composition, following from the Myhill-Nerode theorem (Hopcroft et al., 2001). Further, each monoid element $\\delta_{w}$ can be represented as a boolean transition matrix, making matrix multiplication isomorphic to monoid composition. Computing the transition monoid of a DFA allows recognizing valid words: compute the monoid element for a word by multiplying the elements for its tokens and then check whether the initial state maps to an accepting state. Fix a DFA and its transition monoid $\\delta$. To complete the proof, we show there exists an SSM that, for all $w \\in \\Sigma^{*}$, computes $\\delta_{w}$ given input $x=\\$ w$. Let $\\overline{\\mathbf{A}}_{i}$ be the transition matrix representation of $\\delta_{x_{i}}$. Matrix multiplication is isomorphic to composition of transition monoid elements. We view indices in $\\mathbf{h}_{i}$ as states and define $\\overline{\\mathbf{B}} \\$$ as 1 at the initial state $q_{0}$ and 0 elsewhere. For other $\\sigma$, let $\\overline{\\mathbf{B}} \\sigma=\\overrightarrow{0}$. This yields the following convolutional form:\n\n$$\n\\mathbf{h}_{i}=\\left(\\prod_{k=2}^{i} \\overline{\\mathbf{A}}_{i}\\right) \\mathbf{B} \\$ \\equiv\\left(\\bigcap_{k=2}^{i} \\delta_{x_{k}}\\right)\\left(q_{0}\\right)\n$$\n\nSince $x=\\$ w$, we conclude that $\\mathbf{h}_{|x|} \\equiv \\delta_{w}\\left(q_{0}\\right)$. ### 5.3. Discussion\n\nTheorems 5.1 and 5.2 show that two minimal extensions of the SSM enable expressive power outside $\\mathrm{TC}^{0}$, allowing the model to solve hard state-tracking problems:\nCorollary 5.3. There exist a one-layer log-precision RNNSSM and WFA-SSM that express the word problem for $S_{5}$ (with a beginning-of-string symbol), and these these SSMs cannot be simulated in $\\mathrm{TC}^{0}$. But would these variants of SSMs be feasible to use in practice? Besides expressive power, there are two competing practical concerns that might make these extensions problematic: parallelism and the impact on learning dynamics. Parallelism. To be used effectively in an LLM, a model architecture must be parallelizable on practical hardware. Architectures in $\\mathrm{TC}^{0}$ are parallelizable by design (Merrill \\& Sabharwal, 2023a), but architectures in $\\mathrm{NC}^{1}$ may still be parallelizable to log depth even if they cannot be parallelized to constant depth. For IDS4, the bottleneck would be computing iterated matrix product with a log-depth computation graph. This could be achieved with the SCAN algorithm (Blelloch, 1990) similar to S4 and S6. In contrast, it is less clear how to parallelize a model with a nonlinearity. Learning Dynamics. Another potential concern for IDS4 is that learning dynamics could be degraded. In particular, an iterated product of matrices may lead to vanishing or exploding gradients. However, this is already potentially an issue for the S6 architecture, where the selective gating involves computing an iterated product of scalars. ## 6. Can SSMs Learn Permutations in Practice? Having established theoretical limitations of SSMs for state tracking, we empirically test how well SSMs can learn such tasks, focusing on the $A_{5}$ word problem. Since this problem is $\\mathrm{NC}^{1}$-complete and transformers, S 4 , and Mamba can only express functions in $\\mathrm{TC}^{0}$, these models should require a depth that grows with the input length to solve this problem. Task. We model word problems (see Section 3.1) as a token-tagging task. Models are given as input a sequence $g_{0} g_{1} g_{2} \\cdots g_{n}$ drawn from one of $A_{5}, A_{4} \\times \\mathbb{Z}_{5}$, or $\\mathbb{Z}_{60}$. At each step $i$, the label is the product of the first $i$ elements of\nthe sequence. Modeling the problem as a tagging task rather than as a sequence classification task provides the models with more supervision during training, making it as easy as possible to learn the correct function. We tokenize inputs such that each element gets a unique token. Models. We train a transformer as a TC ${ }^{0}$ baseline, an RNN that we expect can perform state tracking, and three SSMs: S4 (Gu et al., 2022a), Mamba (Gu \\& Dao, 2023), and IDS4 (Section 5.2). For IDS4, we initialize the affine projection $\\alpha$ as a random normal centered around the identity: $\\alpha\\left(\\mathbf{x}_{i}\\right) \\sim$ $\\mathbf{I}+\\mathcal{N}\\left(0, \\sigma^{2}\\right)$.",
    "illusionssm-10": "This ensures that, at initialization, inputdependent transitions tend to propagate the previous state, which we expect to aid learning efficiency. Experimental Setup. We train models on sequences of length $n$ for successively larger values of $n$ and report fullsequence accuracy on a test set. ${ }^{9}$ To validate the prediction that SSMs and transformers require growing depth to solve longer $A_{5}$ word problems, we plot the minimum depth with $90 \\%$ test accuracy as a function of input sequence length. Results. Figure 3 shows single-layer RNN and IDS4 models learn the word problem for arbitrarily long sequences for all three groups. In contrast, transformer, S4, and Mamba models require depth monotonically increasing in sequence length to attain good test accuracy for the non-commutative groups. We draw three conclusions from this:\n\n1. As expected, S4 and Mamba show the same limitations as transformers on the $A_{5}$ word problem. Longer $A_{5}$ sequences require deeper models, consistent with these models being in $\\mathrm{TC}^{0}$. In contrast, RNNs (Theorem 5.1) and IDS4 (Theorem 5.2) can efficiently solve the $A_{5}$ word problem. 2. Transformers, S4, and Mamba require greater depth even for $A_{4} \\times \\mathbb{Z}_{5}$, which can be theoretically expressed by $\\mathrm{TC}^{0}$ circuits. Although transformer and Mamba models of a given depth perform as good or better on $A_{4} \\times \\mathbb{Z}_{5}$ as they on $A_{5}$, they still require increasingly many layers to handle proportionally longer sequences. There are two possible interpretations of this. First, it could be that while these word problems are expressible in $\\mathrm{TC}^{0}$, they cannot be expressed by S4, Mamba, or transformers (which can each likely recognize only a proper subset of $\\mathrm{TC}^{0}$ ). On the other hand, it is possible that these word problems are expressible by transformers, S4, and Mamba but that effectively learning a constant-depth solution is difficult. 3. Despite this limitation, S4 and Mamba appear empirically better than transformer at approximate state tracking on the non-commutative tasks. For length- $n$ sequences from $A_{4} \\times \\mathbb{Z}_{5}$ or $A_{5}$, the transformer requires at least as many (and frequently more) layers as S 4 or Mamba. [^6]\n## 7. Conclusion\n\nWe formally analyzed a family of generalized linear SSMs and showed that, like transformers, common SSM variants including S4 and Mamba can only express computation within the complexity class L-uniform $\\mathrm{TC}^{0}$ of highly parallel computations. This means they cannot solve inherently sequential problems like graph connectivity, boolean formula evaluation, and-of particular interest for state tracking-the permutation composition problem $S_{5} . S_{5}$ can be naturally expressed by true recurrent models like RNNs and captures the essence of hard state tracking due to its $N C^{1}$-completeness. In practice, one-layer RNNs can easily learn a task capturing $S_{5}$ while linear SSMs require depth growing with the sequence length. These results reveal that S4, Mamba, and related SSMs cannot truly track state: rather, they can only solve simple state-tracking problems for which shallow shortcuts exist (Liu et al., 2023). On the other hand, we showed that an input-dependent SSM similar to Hasani et al.'s (2023) Liquid S4 can both express and learn the $S_{5}$ word problem, providing evidence that the expressiveness limitations of current SSMs can be overcome. Ultimately, this line of work could unlock new neural architectures that balance the parallelism of transformers and SSMs with full expressive power for state tracking, enabling LLMs that can benefit from scale while enjoying a greater capacity to reason about games, code, and language. ## Impact Statement\n\nThis paper aims to advance the foundational understanding of state-space architectures for deep learning. Such work can affect the development and deployment of deep learning models in a variety of ways, which in turn can have societal impacts. However, we find it difficult to meaningfully speculate about or anticipate these downstream impacts here. ## Acknowledgments\n\nThis work benefited from discussions with and valuable feedback from Chris Barker, Stefano Ermon, and Charles Foster. It was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise. It was funded by NSF award 1922658, and WM was supported by an NSF graduate research fellowship, AI2, and Two Sigma. ## References\n\nAngluin, D., Chiang, D., and Yang, A. Masked hardattention transformers and Boolean RASP recognize exactly the star-free languages, 2023. arXiv:2310.13897. Barrington, D. A. Bounded-width polynomial-\nsize branching programs recognize exactly those languages in nc1.",
    "illusionssm-11": "Journal of Computer and System Sciences, 38(1):150-164, 1989. URL https://www.sciencedirect.com/ science/article/pii/0022000089900378. Blelloch, G. E. Prefix sums and their applications. Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon University, November 1990. Chiang, D., Cholak, P., and Pillay, A. Tighter bounds on the expressivity of transformer encoders. In ICML, 2023. Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. Towards revealing the mystery behind chain of thought: A theoretical perspective. In NeurIPS, 2023. Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language modeling with state space models. In ICLR, 2023. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023.",
    "illusionssm-12": "arXiv:2312.00752. Gu, A., Johnson, I., Goel, K., Saab, K. K., Dao, T., Rudra, A., and Re, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In NeurIPS, 2021. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In ICLR, 2022a. Gu, A., Goel, K., Saab, K., and R\u00e9, C. Structured state spaces: Combining continuous-time, recurrent, and convolutional models, January 2022b. URL https://hazyresearch.stanford.edu/ blog/2022-01-14-s 4-3. Blog post accessed January $31,2024$. Hao, S., Angluin, D., and Frank, R. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. TACL, 10:800-810, 2022. Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space models.",
    "illusionssm-13": "In ICLR, 2023. Heim, I. File change semantics and the familiarity theory of definiteness. Semantics Critical Concepts in Linguistics, pp.",
    "illusionssm-14": "108-135, 1983. Hesse, W. Division is in uniform $T C^{0}$. In International Colloquium on Automata, Languages, and Programming, pp. 104-114, 2001. Hesse, W., Allender, E., and Barrington, D.",
    "illusionssm-15": "A. M. Uniform constant-depth threshold circuits for division and iterated multiplication.",
    "illusionssm-16": "J. Comput. Syst. Sci., 65:695-716, 2002. Hopcroft, J. E., Motwani, R., and Ullman, J. D. Introduction to automata theory, languages, and computation. ACM SIGACT News, 32(1):60-65, 2001.",
    "illusionssm-17": "Immerman, N. and Landau, S. The complexity of iterated multiplication. In [1989] Proceedings. Structure in Complexity Theory Fourth Annual Conference, pp. 104-111, 1989. doi: 10.1109/SCT.1989.41816. Kim, N. and Schuster, S. Entity tracking in language models. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), $A C L$, July 2023. Krohn, K. and Rhodes, J. Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines. Transactions of the American Mathematical Society, 116:450-464, 1965.",
    "illusionssm-18": "Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers learn shortcuts to automata. In ICLR, 2023.",
    "illusionssm-19": "Mereghetti, C. and Palano, B. Threshold circuits for iterated matrix product and powering.",
    "illusionssm-20": "RAIRO-Theor. Inf. Appl., 34(1):39-46, 2000. doi: 10.1051/ita:2000105. URL https://doi.org/10.1051/ita:2000105. Merrill, W. Sequential neural networks as automata. In Eisner, J., Gall\u00e9, M., Heinz, J., Quattoni, A., and Rabusseau, G. (eds.), Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, Florence, August 2019.",
    "illusionssm-21": "ACL. Merrill, W. and Sabharwal, A. The parallelism tradeoff: Limitations of log-precision transformers.",
    "illusionssm-22": "TACL, 11, 2023a.",
    "illusionssm-23": "Merrill, W. and Sabharwal, A. A logic for expressing logprecision transformers. In NeurIPS, 2023b. Merrill, W. and Sabharwal, A. The expressive power of transformers with chain of thought. In ICLR, 2024. Minsky, M. Neural nets and the brain-model problem. Unpublished doctoral dissertation, Princeton University, NJ, 1954. Mohri, M. Weighted Automata Algorithms, pp. 213254. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009. ISBN 978-3-642-01492-5. doi: 10.1007/ 978-3-642-01492-5_6. URL https: / /doi.org/10. $1007 / 978-3-642-01492-5 \\_6$.",
    "illusionssm-24": "Reif, J. H. and Tate, S. R. On threshold circuits and polynomial computation. SIAM Journal on Computing, 21(5):896-908, 1992. doi: 10.1137/0221053. URL https://doi.org/10.1137/0221053. Rush, S. and Karamcheti, S. The annotated S4. In Blog Track at ICLR 2022, 2022. URL https:// openreview.net/forum?id=xDaLPsMBZv-. Strobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin, D. What formal languages can transformers express?",
    "illusionssm-25": "A survey. TACL, 12, 2024. Toshniwal, S., Wiseman, S., Livescu, K., and Gimpel, K. Chess as a testbed for language model state tracking. In AAAI, 2021. Wang, J., Gangavarapu, T., Yan, J. N., and Rush, A. M. Mambabyte: Token-free selective state space model, 2024.",
    "illusionssm-26": "arXiv:2401.13660. ## A. Floating-Point Arithmetic\n\nOur results use the log-precision floating point model used by Merrill \\& Sabharwal (2023b) to analyze transformers. For some fixed constant $c \\in \\mathbb{Z}^{+}$, a $c \\log n$ precision float is a tuple $\\langle m, e\\rangle$ where $m, e$ are signed integers together taking $c \\log n$ bits.",
    "illusionssm-27": "Using $|x|$ to mean the number of bits used to represent integer $x$, this float represents the value $m \\cdot 2^{e-|m|+1}$. Unlike for integers, arithmetic operations over log-precision floats are not closed. That is, the product $\\phi_{1} \\times \\phi_{2}$ of two $p$-precision floats is a well-defined number but may not be exactly representable as a $p$-precision float. It is thus necessary to define approximate versions of these operations when formalizing log-precision floating-point arithmetic. To this end, Merrill \\& Sabharwal (2023a) define a natural notion of approximate iterated addition over log-precision floats and show that it is computable in L-uniform $\\mathrm{TC}^{0}$. We can naturally apply their definition of iterated addition for floats to matrices of floating points, defining iterated summation over matrices of datatype $\\mathbb{D}$ as the result of treating the numbers as reals, performing exact arithmetic, and casting the exact output $\\phi$ back to $\\mathbb{D}$, denoted cast ${ }_{\\mathbb{D}}(\\phi)$. Formally:\nDefinition A. 1 (Iterated $\\mathbb{D}$-matrix sum; Merrill \\& Sabharwal, 2023a). For matrices $\\mathbf{M}_{1}, \\ldots, \\mathbf{M}_{n}$ over $\\mathbb{D}$ with the same size, their iterated $\\mathbb{D}$-sum is\n\n$$\n\\bigoplus_{i=1}^{z} \\mathbf{M}_{i} \\triangleq \\operatorname{cast}_{\\mathbb{D}}\\left(\\sum_{i=1}^{z} \\operatorname{cast}_{\\mathbb{R}}\\left(\\mathbf{M}_{i}\\right)\\right)\n$$\n\nHere cast $\\mathbb{R}_{\\mathbb{R}}$ converts a number in $\\mathbb{D}$ to the corresponding real number. $\\mathbb{D}$ is implicit in the notations cast $\\mathbb{R}_{\\mathbb{R}}$ and $\\bigoplus$. Integer addition can be obtained as a special case for 1dimensional matrices. We can also analogously defined iterated summation, which will be necessary for formalizing SSMs:\nDefinition A. 2 (Iterated $\\mathbb{D}$-matrix product). For square matrices $\\mathbf{M}_{1}, \\ldots, \\mathbf{M}_{z}$ over $\\mathbb{D}$, their iterated $\\mathbb{D}$-product is\n\n$$\n\\bigotimes_{i=1}^{z} \\mathbf{M}_{i} \\triangleq \\operatorname{cast}_{\\mathbb{D}}\\left(\\prod_{i=1}^{z} \\operatorname{cast}_{\\mathbb{R}}\\left(\\mathbf{M}_{i}\\right)\\right)\n$$\n\nMerrill \\& Sabharwal (2023a) showed that iterated addition from for log-precision floats is in L -uniform $\\mathrm{TC}^{0}$. It naturally follows from their argument that iterated addition over log-precision float matrices is also in L-uniform TC ${ }^{0}$. In general, iterated matrix products are not necessarily computable in $\\mathrm{TC}^{0}$. However, we extend the arguments of Hesse (2001) and Mereghetti \\& Palano (2000) for integers to show that two special cases (iterated scalar multiplication and matrix powering) over log-precision floats are also computable in L-uniform TC ${ }^{0}$.",
    "illusionssm-28": "Finally, we define a canonical value for a compositional arithmetic expression over floats that enjoys the associative property. Definition A. 3 (Flattened expression evaluation). Let $\\phi$ be a compositional expression over floats, which may contain alternating sums and products as well as other operations like exp. We define the canonical value of $\\phi$ as the value returned by the computation graph obtained by flattening all adjacent sums into a single sum (and analogously for products).",
    "illusionssm-29": "Definition A. 3 has the nice effect of making Definition A. 2 associative. The only results that rely on this assumption are our analysis of diagonalizable SSMs in Appendix C. We also deal with the details of this assumption in Lemma 4.1, though the proof there also goes through directly without handling these details. ## A.1. Complexity of Iterated Scalar Multiplication\n\nThe first special case of iterated matrix products we analyze is when the matrices are simply scalars (or, w.l.o.g., diagonal matrices). In this case, the iterated product can be computed in L-uniform $\\mathrm{TC}^{0}$. Lemma A. 4 (Iterated $\\mathbb{D}$-product). Let $\\phi_{1}, \\ldots, \\phi_{z} \\in \\mathbb{D}$ be such that $z \\leq n$ and each $\\phi_{i}$ can be represented as an $n$-bit integer. If operators cast $\\mathbb{D}_{\\mathbb{D}}$ and cast $\\mathbb{R}_{\\mathbb{R}}$ are in L -uniform $\\mathrm{TC}^{0}$, then the iterated $\\mathbb{D}$-product $\\otimes_{i=1}^{z} \\phi_{i}$ can be computed in L -uniform $\\mathrm{TC}^{0}$. Proof. By preconditions of the lemma, we can compute $y_{i}=\\operatorname{cast}_{\\mathbb{R}}\\left(\\phi_{i}\\right)$ for each $i$ in L-uniform $\\mathrm{TC}^{0}$. Since each $\\phi_{i}$ is equivalent to an $n$-bit integer, $y_{i}$ can be viewed as an $n$-bit integer. The iterated integer product $y=\\prod_{i=1}^{z} y_{i}$ can be computed with an L-uniform $\\mathrm{TC}^{0}$ circuit (Hesse, 2001). Finally, by a precondition of the lemma, we can cast the result back to $\\mathbb{D}$, i.e., compute cast $\\mathbb{D}_{\\mathbb{D}}(y)$ which equals\nthe iterated $\\mathbb{D}$-product $\\bigotimes_{i=1}^{z} \\phi_{i}$, with an L-uniform $\\mathrm{TC}^{0}$ circuit. Lemma A. 5 (Iterated float product). Let $\\phi_{1}, \\ldots, \\phi_{z}$ be $c \\log n$ precision floats and $z \\leq n$. Then the iterated float product $\\bigotimes_{i=1}^{z} \\phi_{i}$ can be computed in L -uniform $\\mathrm{TC}^{0}$.",
    "illusionssm-30": "Proof. The idea is to convert (by scaling up) the sequence of $\\phi_{i}$ to another sequence of floats that are all representable as integers, apply Lemma A.4, reverse the scaling, and cast the result back to a $c \\log n$ precision float. Let $e$ be the smallest exponent across all $\\phi_{i}$ and $q=$ $\\max \\{0,-e\\}$. Construct re-scaled floats $\\psi_{i}=\\phi_{i} 2^{q}$ by adding $q$ to the exponent of $\\phi_{i}$, using up to $c \\log n$ additional bits in the exponent if necessary to keep the computation exact. Note that $e, q$, and all $\\psi_{i}$ can easily be computed exactly by an L-uniform $\\mathrm{TC}^{0}$ circuit as they involve fixed-arity arithmetic operations. Further, by construction, every $\\psi_{i}$ has a non-negative exponent and thus represents an integer. The maximum number representable by each $c \\log n$ precision float $\\phi_{i}$ is upper bounded by $2^{n^{c}}$. Thus, the maximum number representable by each entry $\\psi_{i}$ is $2^{n^{c}} \\times 2^{q}=2^{n^{c}+q}$. Let $m=n^{c}+q$. It follows that each $\\psi_{i}$ can be equivalently represented as an $m$-bit integer. Further, this integer can be computed by left-shifting the mantissa of $\\psi_{i}$ by a number of bits equal to the value of the exponent of $\\psi_{i}$ (which is non-negative). Finally, this left-shift, and thus the cast $\\mathbb{R}_{\\mathbb{R}}$ operation over $m$-precision floats, can be easily computed by an L-uniform threshold circuit of size poly $(m)$. In the other direction, casting from reals to $m$-precision floats can also be easily accomplished by an L-uniform threshold circuit of size poly $(m)$. Observing that $\\psi_{1}, \\ldots, \\psi_{z}$ is a sequence of floats each representable as an $m$-bit integer, we now apply Lemma A. 4 with $\\mathbb{D}$ being 'float' to conclude that iterated float product $\\tau=\\bigotimes_{i=1}^{z} \\psi_{i}$ can be computed by an L-uniform threshold circuit of size poly $(m)$. Since $m \\leq 2 n^{c}$, this circuit is also of size poly $(n)$. Finally, to compute the original iterated float product $\\otimes_{i=1}^{z} \\phi_{i}$, we divide $\\tau$ by $2^{q z}$. This can be accomplished by subtracting $q z$ from the exponent of $\\tau$; again, we do this computation exactly, using up to $(c+1) \\log n$ additional bits in the exponent if necessary. We then cast the resulting float back to a $c \\log n$ precision float. All this can be done in L-uniform $\\mathrm{TC}^{0}$, finishing the proof that $\\bigotimes_{i=1}^{z} \\phi_{i}$ can be computed in L-uniform $\\mathrm{TC}^{0}$. ## A.2. Complexity of Matrix Powering\n\nThe second special case we analyze is matrix powering: i.e., a matrix product where all the matrices being powered are the same. Mereghetti \\& Palano (2000) showed that when the datatype $\\mathbb{D}$ is $n$-bit integers, one can compute $\\mathbf{M}^{n}$ in $\\mathrm{TC}^{0}$. We note that their construction also works for computing $\\mathbf{M}^{z}$ for any $z \\leq n, z \\in \\mathbb{Z}^{+}$. Further, as they remark, their construction can, in fact, be done in uniform $\\mathrm{TC}^{0}$. Specifically, we observe most of their construction involves sums and products of constantly many $n$-bit integers, which can be done in L-uniform TC ${ }^{0}$. The only involved step is dividing a polynomial of degree (up to) $n$ by a polynomial of degree (up to) $d-1$ and returning the remainder. It turns out that this \"polynomial division with remainder\" operation can also be performed in L-uniform $\\mathrm{TC}^{0}$ (see Corollary 6.5 of Hesse et al., 2002 and an explanation in Appendix A.3). We thus have the following extension of Mereghetti \\& Palano's result:\nLemma A.6 (Integer matrix power, derived from Mereghetti \\& Palano, 2000). Let $d \\in \\mathbb{Z}^{+}$be a fixed constant. Let $\\mathbf{M}$ be a $d \\times d$ matrix over $n$-bit integers and $z \\leq n, z \\in \\mathbb{Z}^{+}$. Then integer matrix power $\\mathbf{M}^{z}$ can be computed in L-uniform $\\mathrm{TC}^{0}$. We extend this to matrix powers over $\\mathbb{D}$ rather than integers:\nLemma A. 7 ( $\\mathbb{D}$-matrix power). Let $d \\in \\mathbb{Z}^{+}$be a fixed constant. Let $\\mathbf{M}$ be a $d \\times d$ matrix over a datatype $\\mathbb{D}$ with entries equivalently representable as $n$-bit integers. Let $z \\leq n, z \\in \\mathbb{Z}^{+}$. If operators cast $\\mathbb{D}_{\\mathbb{D}}$ and cast $_{\\mathbb{R}}$ are in L uniform $\\mathbf{T C}^{0}$, then $\\mathbb{D}$-matrix power $\\mathbf{M}^{z}$ can be computed in L -uniform $\\mathrm{TC}^{0}$. Proof. By preconditions of the lemma, we can compute $\\operatorname{cast}_{\\mathbb{R}}(\\mathbf{M})$ in L-uniform $\\mathrm{TC}^{0}$. Since the entries of $\\mathbf{M}$ are equivalent to $n$-bit integers, cast ${ }_{R}(\\mathbf{M})$ can be viewed as a $d \\times d$ integer matrix of $n$-bit integers. By Lemma A. 6 , we can compute cast ${ }_{R}(\\mathbf{M})^{z}$ using an L-uniform $\\mathrm{TC}^{0}$ circuit. Finally, by a precondition of the lemma, we can cast the result back to $\\mathbb{D}$, i.e., compute cast $\\mathbb{D}_{\\mathbb{D}}\\left(\\operatorname{cast}_{\\mathbb{R}}(\\mathbf{M})^{z}\\right)$ which equals $\\mathbf{M}^{z}$, with an L-uniform $\\mathrm{TC}^{0}$ circuit. Lemma A. 8 (Float matrix power). Let $d, c \\in \\mathbb{Z}^{+}$be fixed constants. Let $\\mathbf{M}$ be a $d \\times d$ matrix over $c \\log n$ precision floats. Let $z \\leq n, z \\in \\mathbb{Z}^{+}$. Then float matrix power $\\mathbf{M}^{z}$ can be computed in L -uniform $\\mathrm{TC}^{0}$. Proof. The idea is to convert (by scaling up) $\\mathbf{M}$ to another float matrix all whose entries are representable as integers, apply Lemma A.7, reverse the scaling, and cast the result back to $c \\log n$ precision floats. Let $e$ be the smallest exponent across all float entries of M and $q=\\max \\{0,-e\\}$. Construct a re-scaled float matrix $\\tilde{\\mathbf{M}}=\\mathbf{M} 2^{q}$ by adding $q$ to the exponent of every entry of $\\mathbf{M}$, using up to $c \\log n$ additional bits in the exponent if necessary to keep the computation exact. Note that $e, q$, and $\\tilde{\\mathbf{M}}$ can easily be computed exactly by an L-uniform $\\mathrm{TC}^{0}$ circuit as they involve fixed-arity arithmetic operations. Further, by construction, $\\tilde{\\mathbf{M}}$ has non-negative exponents in\nall its float entries. Thus, every entry of $\\tilde{\\mathbf{M}}$ represents an integer. The maximum number representable by each $c \\log n$ precision float in $\\mathbf{M}$ is upper bounded by $2^{n^{c}}$. Thus, the maximum number representable by each entry of $\\tilde{\\mathbf{M}}$ is $2^{n^{c}} \\times 2^{q}=2^{n^{c}+q}$. Let $m=n^{c}+q$. It follows that each entry $\\phi$ of $\\tilde{\\mathbf{M}}$ can be equivalently represented as an $m$-bit integer. Further, this integer can be computed by left-shifting the mantissa of $\\phi$ by a number of bits equal to the value of the exponent of $\\phi$ (which is non-negative). Finally, this left-shift, and thus the cast $\\mathbb{R}_{\\mathbb{R}}$ operation over $m$-precision floats, can be easily computed by an L-uniform threshold circuit of size poly $(m)$. In the other direction, casting from reals to $m$-precision floats can also be easily accomplished by an L-uniform threshold circuit of size poly $(m)$. Note that $2^{q} \\in\\left[0, n^{c}\\right]$ and hence $m \\in\\left[n^{c}, 2 n^{c}\\right]$. In particular, $m \\geq n$. Thus $z \\leq n$ (a precision) implies $z \\leq m$. Observing that $\\tilde{\\mathbf{M}}$ is a matrix of floats each representable as an $m$-bit integer, we now apply Lemma A. 7 with $\\mathbb{D}$ being 'float' to conclude that float matrix power $\\tilde{\\mathbf{M}}^{z}$ can be computed by an L-uniform threshold circuit of size poly $(m)$. Since $m \\leq 2 n^{c}$, this circuit is also of size poly $(n)$. Finally, to compute $\\mathbf{M}^{z}$, we first divide each entry of $\\tilde{\\mathbf{M}}^{z}$ by $2^{q z}$. This can be accomplished by subtracting $q z$ from the exponent of each entry of $\\tilde{\\mathbf{M}}$; again, we do this computation exactly, using up to $(c+1) \\log n$ additional bits in the exponent if necessary. We then cast all entries of the resulting matrix back to $c \\log n$ precision floats. All this can be done in L-uniform $\\mathrm{TC}^{0}$, finishing the proof that $\\mathbf{M}^{z}$ can be computed in L-uniform $\\mathrm{TC}^{0}$. ## A.3. L-Uniformity of Polynomial Division in $\\mathrm{TC}^{0}$\n\nHesse et al. (2002) state that polynomial division is in Luniform $\\mathrm{TC}^{0}$ in Corollary 6.5. For historical reasons, this claim is preceded by weaker claims in older papers. We briefly clarify this situation to help understand why the stronger claim is valid. Reif \\& Tate (1992) establish that polynomial division can be performed in P -uniform $\\mathrm{TC}^{0}$, whereas we state our results for L-uniform $\\mathrm{TC}^{0}$, which is a smaller class. However, the only issue preventing the polynomial division result from originally going through in the L-uniform case is that, at the time of Reif \\& Tate's publication, it was not known whether integer division and iterated integer multiplication are computable in L-uniform TC ${ }^{0}$. However, Hesse (2001) later proved exactly this. Combining the two results, Theorem 3.2 of Reif \\& Tate (1992) goes through even with L-uniformity (not just P-uniformity). Its Corollary 3.3 then allows us to conclude that integer polynomial division can be solved by L-uniform $\\mathrm{TC}^{0}$ circuits because the output of integer polynomial division is an analytic function whose\nTaylor expansion has a finite number of terms (Reif \\& Tate, 1992). ## B. S6 Parameterization\n\nTo justify that the S 6 architecture used by Mamba is computable in $\\mathrm{TC}^{0}$, we justify that $\\overline{\\mathbf{A}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}, \\mathbf{D}_{i}$ can be computed as a function of $\\mathbf{x}_{i}$ in $\\mathrm{TC}^{0}$. We begin by summarizing how exactly is S 6 parameterized. S6 first defines continuous-time parameters:\n\n1. $\\mathbf{A}$ is a fixed, diagonal matrix that is invertible (each $a_{i i} \\neq 0$ );\n2. $\\mathbf{B}_{i}=\\pi_{\\mathbf{B}}\\left(\\mathbf{x}_{i}\\right)$ is computed via a projection;\n3. $\\mathbf{C}_{i}=\\pi_{\\mathbf{C}}\\left(\\mathbf{x}_{i}\\right)$ is computed via a projection;\n4. $\\mathbf{D}_{i}=\\mathbf{I}$. Next, we need to discretize the matrices A and B. S6 does this using an input-dependent discretization factor $\\delta_{i}$ :\n\n$$\n\\delta_{i}=\\operatorname{softplus}\\left(\\delta+\\pi_{\\delta}\\left(\\mathbf{x}_{i}\\right)\\right)\n$$\n\nThe discretized matrices are then defined as:\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{A}}_{i} & =\\exp \\left(\\delta_{i} \\mathbf{A}\\right) \\\\\n\\overline{\\mathbf{B}}_{i} & =\\left(\\delta_{i} \\mathbf{A}\\right)^{-1}\\left(\\overline{\\mathbf{A}}_{i}-\\mathbf{I}\\right) \\delta_{i} \\mathbf{B}_{i}\n\\end{aligned}\n$$\n\nIt is clear to see that the diagonalizability condition of Theorem 4.4 is satisfied because $\\overline{\\mathbf{A}}_{i}$ itself is diagonal. Additionally, all the relevant matrices can be computed in $\\mathrm{TC}^{0}$. Proposition B.1. $\\overline{\\mathbf{A}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}$, and $\\mathbf{D}_{i}$ can all be computed as functions of $\\mathbf{x}_{i}$ in L -uniform $\\mathrm{TC}^{0}$. To prove this, observe that $\\mathbf{A}, \\mathbf{B}_{i}, \\mathbf{C}_{i}, \\mathbf{D}_{i}$ can all be computed in L-uniform $\\mathrm{TC}^{0}$ because they are either constants or linear transformations of $\\mathbf{x}_{i}$. To justify that $\\overline{\\mathbf{A}}_{i}$ and $\\overline{\\mathbf{B}}_{i}$ can be computed in L-uniform $\\mathrm{TC}^{0}$, we just need to justify that we can invert diagonal matrices and compute softplus and $\\exp$ in L-uniform $\\mathrm{TC}^{0}$. Lemma B.2. Diagonal matrices over log-precision floats can be inverted in L -uniform $\\mathrm{TC}^{0}$. Proof. Inverting a diagonal matrix just involves forming the reciprocal along the diagonal. Scalar reciprocals can be approximated to error at most $2^{-n^{c}}$ (for any $c$ ) in $\\mathrm{TC}^{0}$ (Hesse et al., 2002). This means we can compute the reciprocal of a log-precision float (cf. Appendix A) exactly up to log precision. In Appendix D, we show that we can compute the nonlinearities exp and softplus over a bounded domain in $\\mathrm{TC}^{0}$. ## C. Diagonalizable SSMs\n\nWe extend Theorem 4.4 to cover the case when the SSMs transition matrices are simultaneously diagonalizable, rather than just diagonal. This requires us to note that when working with log-precision floating point representations of matrices, a diagonal matrix $\\mathbf{A}$ and its diagonalized decomposition $\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$ are numerically substitutable. Theorem 4.6 (Simultaneously diagonalizable SSM). Let W be a fixed matrix. Let $M$ be a log-precision generalized linear SSM such that, for $1 \\leq i \\leq n$,\n\n1. the transition matrix $\\overline{\\mathbf{A}}_{i}$ is computable to log precision by the expression $\\mathbf{W} \\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right) \\mathbf{W}^{-1}$, where $\\overline{\\mathbf{a}}_{i} \\in \\mathbb{R}^{d}$;\n2. each of $\\overline{\\mathbf{a}}_{i}, \\overline{\\mathbf{B}}_{i}, \\mathbf{C}_{i}$ and $\\mathbf{D}_{i}$ can be computed in Luniform $\\mathrm{TC}^{0}$ as a function of $\\mathbf{x}_{i}$. Then there exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes M's convolutional form. Proof. When the first condition is satisfied, the following equality holds over log-precision floats:\n\n$$\n\\prod_{i} \\overline{\\mathbf{A}}_{i}=\\prod_{i}\\left(\\mathbf{W} \\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right) \\mathbf{W}^{-1}\\right)\n$$\n\nBy the associativity of $\\mathbb{D}$-matrix products, we can remove the parentheses to get\n\n$$\n\\begin{aligned}\n\\prod_{i} \\overline{\\mathbf{A}}_{i} & =\\prod_{i} \\mathbf{W} \\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right) \\mathbf{W}^{-1} \\\\\n& =\\mathbf{W}\\left[\\prod_{i} \\operatorname{diag}\\left(\\overline{\\mathbf{a}}_{i}\\right)\\right] \\mathbf{W}^{-1}\n\\end{aligned}\n$$\n\nIterated multiplication of diagonal matrices is reducible to several iterated scalar multiplications, which is in L-uniform $\\mathrm{TC}^{0}$ (Lemma A.5). Then the product of all $\\overline{\\mathbf{A}}_{i}$ is the product of three L-uniform $\\mathrm{TC}^{0}$-computable matrices, so is itself L-uniform $\\mathrm{TC}^{0}$-computable. The second condition from Lemma 4.1 is satisfied by assumption. Thus, the convolutional form for $M$ can be computed in L-uniform $\\mathrm{TC}^{0}$. ## C.1. Diagonalizable S6\n\nWe can define an extension of S6 which satisfies these conditions to show that it is also in L-uniform TC ${ }^{0}$. Definition C.1. Diagonalizable S6 has continuous-time parameters:\n\n1. $\\mathbf{A}$ is a fixed matrix diagonalizable as $\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$ that is invertible (each $a_{i i} \\neq 0$ );\n2. $\\mathbf{B}_{i}=\\pi_{\\mathbf{B}}\\left(\\mathbf{x}_{i}\\right)$ is computed via a projection;\n3. $\\mathbf{C}_{i}=\\pi_{\\mathbf{C}}\\left(\\mathbf{x}_{i}\\right)$ is computed via a projection;\n4. $\\mathbf{D}=\\mathbf{I}$. As in the standard S6, the discretization of $\\mathbf{A}$ and $\\mathbf{B}$ is done by an input-dependent discretization factor $\\delta_{i}$ :\n\n$$\n\\delta_{i}=\\operatorname{softplus}\\left(\\delta+\\pi_{\\delta}\\left(\\mathbf{x}_{i}\\right)\\right)\n$$\n\nThe discretized matrices are then defined as\n\n$$\n\\begin{aligned}\n& \\overline{\\mathbf{A}}_{i}=\\exp \\left(\\delta_{i} \\mathbf{A}\\right) \\\\\n& \\overline{\\mathbf{B}}_{i}=\\left(\\delta_{i} \\mathbf{A}\\right)^{-1}\\left(\\overline{\\mathbf{A}}_{i}-\\mathbf{I}\\right) \\delta_{i} \\mathbf{B}_{i}\n\\end{aligned}\n$$\n\nTo prove that $\\overline{\\mathbf{A}}_{i}$ and $\\overline{\\mathbf{B}}_{i}$ have the necessary properties, we first introduce some lemmas dealing with matrix-valued functions of diagonalizable matrices. Lemma C.2. If a matrix $\\mathbf{A}$ is diagonalizable, then we can substitute its diagonalized decomposition $\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$ in a computation graph over log-precision floats involving A without incurring any meaningful error. Proof. Let A be diagonalizable. Then there exists invertible $\\mathbf{W}$ and diagonal $\\operatorname{diag}(\\mathbf{a})$ such that $\\mathbf{A}=\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$. Note that the product of a fixed number of matrices is in L-uniform $\\mathrm{TC}^{0}$, and so the first $c \\log n$ bits of $\\mathbf{A}$ and $\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$ are identical. Lemma C.3. Let $\\mathbf{A}$ be diagonalizable as $\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$, where $\\mathbf{a} \\in \\mathbb{R}^{d}$. Then $c \\cdot \\mathbf{A}$ is simultaneously diagonalizable with $\\mathbf{A} \\operatorname{via} c \\cdot \\mathbf{A}=\\mathbf{W} c \\cdot \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$.",
    "illusionssm-31": "Proof. Scalar multiplication commutes around matrix multiplication. Lemma C.4. Let $\\mathbf{A}$ be diagonalizable as $\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$, where $\\mathbf{a} \\in \\mathbb{R}^{d}$. Then $\\exp (\\mathbf{A})=\\mathbf{W} \\exp (\\operatorname{diag}(\\mathbf{a})) \\mathbf{W}^{-1}$. Proof. The matrix exponential is defined as a power series, so for diagonalizable $\\mathbf{A}$ it follows that\n\n$$\n\\begin{aligned}\n\\exp (\\mathbf{A}) & =\\exp \\left(\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}\\right) \\\\\n& =\\sum_{k=0}^{\\infty} \\frac{1}{k!}\\left(\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}\\right)^{k} \\\\\n& =\\sum_{k=0}^{\\infty} \\frac{1}{k!} \\mathbf{W} \\operatorname{diag}(\\mathbf{a})^{k} \\mathbf{W}^{-1} \\\\\n& =\\mathbf{W}\\left(\\sum_{k=0}^{\\infty} \\frac{1}{k!} \\operatorname{diag}(\\mathbf{a})^{k}\\right) \\mathbf{W}^{-1} \\\\\n& =\\mathbf{W} \\exp (\\operatorname{diag}(\\mathbf{a})) \\mathbf{W}^{-1}\n\\end{aligned}\n$$\n\nThe expressions in Lemma C. 4 are equivalent not just over real numbers but also over log-precision floats. This is because we know both expressions can be approximated in $\\mathrm{TC}^{0}$ with error at most $2^{-n^{c}}$, which means the $c \\log n$ bits of the approximation must be equivalent. Lemma C.5. Diagonalizable matrices over log-precision floats can be inverted in L -uniform $\\mathrm{TC}^{0}$. Proof. Let $\\mathbf{A}=\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-\\mathbf{1}}$. Then $\\mathbf{A}^{-1}=$ $\\mathbf{W}^{-1} \\operatorname{diag}(\\mathbf{a})^{-1} \\mathbf{W}$. We are guaranteed that each of these matrices exists, and furthermore by Lemma B. 2 we know that $\\operatorname{diag}(\\mathbf{a})^{-1}$ is computable in L-uniform $\\mathrm{TC}^{0}$. Their product, involving a finite number of additions and multiplies, is also computable in L-uniform TC ${ }^{0}$. Proposition C.6. $\\overline{\\mathbf{A}}_{i}$ and $\\overline{\\mathbf{B}}_{i}$ can be computed as functions of $\\mathbf{x}_{i}$ in L-uniform $\\mathrm{TC}^{0}$. Proof. We first show that $\\overline{\\mathbf{A}}_{i}$ is L-uniform $\\mathrm{TC}^{0}$ computable. By definition,\n\n$$\n\\overline{\\mathbf{A}}_{i}=\\exp \\left(\\delta_{i} \\mathbf{A}\\right)\n$$\n\nBy Corollary D. $2, \\delta_{i}$ is computable in L-uniform $\\mathrm{TC}^{0}$. The product $\\delta_{i} \\mathbf{A}$ is simultaneously diagonalizable with $\\mathbf{A}$ so\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{A}}_{i} & =\\exp \\left(\\mathbf{W} \\delta_{i} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}\\right) \\\\\n& =\\mathbf{W} \\exp (\\operatorname{diag}(\\mathbf{a})) \\mathbf{W}^{-1}\n\\end{aligned}\n$$\n\n(Lemma C.3)\n(Lemma C.4)\nSince the exponential of scalars is L-uniform $\\mathrm{TC}^{0}$ computable by Corollary D.2, then $\\overline{\\mathbf{A}}_{i}$ is as well. Turning to $\\overline{\\mathbf{B}}_{i}$, note that the term $\\left(\\delta_{i} \\mathbf{A}\\right)^{-1}$ is L-uniform $\\mathrm{TC}^{0}$ computable by Lemma C. 5 since $\\delta_{i} \\mathbf{A}$ is diagonalizable. Since $\\overline{\\mathbf{A}}_{i}$ is L-uniform $\\mathrm{TC}^{0}$ computable, the difference $\\overline{\\mathbf{A}}_{i}-$ $\\mathbf{I}$ is as well. Then every term in\n\n$$\n\\overline{\\mathbf{B}}_{i}=\\left(\\delta_{i} \\mathbf{A}\\right)^{-1}\\left(\\overline{\\mathbf{A}}_{i}-\\mathbf{I}\\right) \\delta_{i} \\mathbf{B}_{i}\n$$\n\nis L -uniform $\\mathrm{TC}^{0}$ computable, and so their product is as well. Remark. Since $\\mathbf{C}_{i}$ and $\\mathbf{D}_{i}$ are unchanged between the standard and diagonalizable versions of S6, the proofs of their computability as functions of $\\mathbf{x}_{i}$ in L-uniform $\\mathrm{TC}^{0}$ pass through from Appendix B. Corollary C.7. There exists an L -uniform $\\mathrm{TC}^{0}$ circuit family that computes Diagonalizable S6's convolutional form. Proof. Note that since $\\mathbf{A}=\\mathbf{W} \\operatorname{diag}(\\mathbf{a}) \\mathbf{W}^{-1}$ is fixed the set of transition matrices $\\left\\{\\overline{\\mathbf{A}}_{i}\\right\\}$ is simultaneously diagonalizable via $\\mathbf{W}$ for all $i$. Then Diagonalizable S6 meets the conditions for Theorem 4.6. ## D. Nonlinearities in L-Uniform $\\mathrm{TC}^{0}$\n\nThe parameterization of SSMs (and transformers) involves computing nonlinearities like exp and softplus. We leverage existing circuit complexity results (Reif \\& Tate, 1992) to show that, in general, any well-behaved nonlinearity should be computable in L-uniform $\\mathrm{TC}^{0}$ when used in conjunction with pre- or post-layer norm.",
    "illusionssm-32": "Lemma D. 1 (Adapts Corollary 3.3, Reif \\& Tate, 1992). Let $X=(-B, B)$ be a bounded interval. Let $f$ be a function over $X$ with a convergent Taylor series:\n\n$$\nf(x)=\\sum_{n=0}^{\\infty} \\frac{a_{n}}{b_{n}}\\left(x-x_{0}\\right)^{n}\n$$\n\nwhere $a_{n}, b_{n}$ are integers with magnitude at most $2^{n^{O(1)}}$ computable in $L$-uniform $\\mathrm{TC}^{0}$. Then $f$ can be approximated over $X$ by L -uniform $\\mathrm{TC}^{0}$ circuits to log precision (error at most $2^{-n^{c}}$ for any $c \\geq 1$ ).",
    "illusionssm-33": "Proof. Reif \\& Tate (1992) give a proof when $X=(-1,1)$. We generalize to $X=(-B, B)$, assuming w.l.o.g. $B=2^{k}$. The idea is to transform $f$ to have domain $(-1,1)$ via\n\n$$\ng(x)=f(B x)\n$$\n\nThen, we can apply Corollary 3.3 of Reif \\& Tate (1992) to approximate $g$ with error at most $2^{-n^{c}}$. Reif \\& Tate (1992) state their result for P -uniform $\\mathrm{TC}^{0}$, but through advances in circuit complexity since the time of publication (Appendix A.3), their construction naturally applies for L -uniform $\\mathrm{TC}^{0}$ as well.",
    "illusionssm-34": "To approximate $f$, compute $z=x / B$, which can be done exactly since $B=2^{k}$. We conclude by computing $g(z)=$ $f(x)$, which, as established, has error at most $2^{-n^{c}}$. Because of pre- and post-norm layers, the elements of $\\mathbf{x}_{i}$ in an SSM will remain in a bounded domain $(-B, B)$. Thus, the following lemma shows we can compute them:\nCorollary D.2. The pointwise nonlinearities exp, log, and softplus are computable over $(-B, B)$ in L -uniform $\\mathrm{TC}^{0}$. Proof. By Reif \\& Tate (1992, Corollary 3.3) know that the Taylor series for exp and $\\log$ is convergent with $a_{n}, b_{n}$ computable in L -uniform $\\mathrm{TC}^{0}$. Then exp and $\\log$ are themselves computable in L-uniform $\\mathrm{TC}^{0}$. Since $\\operatorname{softplus}(x)=\\log (1+\\exp (x))$ is a fixed composition of $L$-uniform TC $^{0}$-computable functions, it too is computable in $L$-uniform $\\mathrm{TC}^{0}$.",
    "illusionssm-35": "[^0]:    ${ }^{1}$ New York University ${ }^{2}$ Allen Institute for AI. Correspondence to: William Merrill [willm@nyu.edu](mailto:willm@nyu.edu), Jackson Petty $<$ petty@nyu.edu>, Ashish Sabharwal [ashishs@allenai.org](mailto:ashishs@allenai.org). Proceedings of the $41^{\\text {st }}$ International Conference on Machine Learning, Vienna, Austria.",
    "illusionssm-36": "PMLR 235, 2024. Copyright 2024 by the author(s). [^1]:    ${ }^{1}$ The hardness of chess state tracking holds with (source, target) notation, but standard notation may make state tracking easier. [^2]:    ${ }^{2}$ In practice, this sequence is often a vector $\\mathbf{x}_{1}, \\ldots \\mathbf{x}_{n} \\in \\mathbb{R}^{m}$ and the SSM is applied elementwise on each feature.",
    "illusionssm-37": "${ }^{3}$ The two forms express the same function over $\\mathbb{R}$ or any other distributive datatype. Over floating points (Section 2.2), they are not guaranteed to be the same, but we must assume the error is negligible for them to be well-defined and usable in practice. [^3]:    ${ }^{4} \\mathrm{We}$ use $\\mathrm{TC}^{0}$ to mean L -uniform $\\mathrm{TC}^{0}$, meaning the circuit family is constructible by a Turing machine that runs in space logarithmic in the size of the input (cf. Merrill \\& Sabharwal, 2023a; Strobl et al., 2024). We believe our results could be extended from L-uniform $\\mathrm{TC}^{0}$ to DLOGTIME-uniform TC ${ }^{0}$ using techniques similar to Merrill \\& Sabharwal (2023b) for composing TC $^{0}$ circuits in a way that preserves DLOGTIME uniformity. ${ }^{5}$ We consider finite monoids for simplicity, but the approach may be extendable to infinite (e.g., finitely generated) monoids. [^4]:    ${ }^{6} \\mathrm{We}$ focus on word problems on groups, which are monoids with inverses. Formally, a group $G$ is solvable exactly when there is a series of subgroups $1=G_{0}<G_{1}<\\cdots<G_{k}=G$ such that $G_{i-1}$ is normal in $G_{i}$ and $G_{i} / G_{i-1}$ is abelian. ${ }^{7}$ W.l.o.g., any permutation can be factored into a sequence of transpositions, or swaps. [^5]:    ${ }^{8}$ Let $c \\log n$ be the SSM's precision. We compute $\\prod_{k} \\overline{\\mathbf{A}}_{k}$ to $c^{\\prime} \\log n$ precision for a large enough $c^{\\prime}$ (similar to the proof of Lemma A.5) such that the full product $\\left(\\prod_{k} \\overline{\\mathbf{A}}_{k}\\right) \\overline{\\mathbf{B}}_{j} \\mathbf{x}_{j}$ is correct to at least $c \\log n$ bits, as technically required by Definition A.3. [^6]:    ${ }^{9}$ We always include all 3600 pairwise sequences of length 2 in the training data along with the training split of length- $n$ sequences.",
    "illusionssm-38": ""
}