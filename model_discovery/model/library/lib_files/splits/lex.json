{
    "lex-0": "# A Length-Extrapolatable Transformer \n\nYutao Sun, Li Dong, Barun Patra, Shuming Ma<br>Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei<br>Microsoft<br>https://github.com/microsoft/torchscale\n\n\n#### Abstract\n\nPosition modeling plays a critical role in Transformers.",
    "lex-1": "In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available at https://aka. $\\mathrm{ms} /$ LeX-Transformer. ## 1 Introduction\n\nTransformer (Vaswani et al., 2017) shows a strong performance in NLP and becomes a universal choice nowadays (Dosovitskiy et al., 2020; Radford et al., 2021; Wang et al., 2022). However, most of them have a crucial shortcoming: they can only deal with the in-distribution size of inputs. It is usually infeasible to train a model with all possible input lengths. Therefore, a length-extrapolatable Transformer is essential for wider usage. In sequence modeling, position information plays a crucial role in building the correct representation and understanding of the latent meaning. For Recurrent Neural Networks such as LSTM (Hochreiter and Schmidhuber, 1997), the calculation is done along the sequence order in $\\mathrm{O}(\\mathrm{n})$ time. However, the parallel attention module makes it hard to encode position effectively. First, Vaswani et al. (2017) propose absolute sinusoidal position embedding, and Devlin et al. (2019) adjust it to a learnable one. The absolute design is computation-efficient, but not comparable with subsequent relative ones (Shaw et al., 2018; Su et al.,\n2021; Press et al., 2021). Among many relative position embeddings, RoPE (Su et al., 2021) shows better performance and is used to many PLMs such as PaLM (Chowdhery et al., 2022).",
    "lex-2": "However, it can't deal with sequences with exceed length. Alibi (Press et al., 2021) mitigates the extrapolation problem but sacrifices the general performance. Since different strategies concentrate on some part of the position feature, it is essential to build a comprehensive view and guide the Transformer's design systematically. First, a Transformer should be sensitive to order. Otherwise, it will degenerate into a bag-of-word model which confuses the whole meaning. Then, position translation can't hurt the representation a lot especially combing with the proper attention-mask operations. After that, a good sequence model needs to deal with any input length. As illustrated before, the length problem is not universal but special for Transformer. Especially, when a Transformer is pre-trained under a maximal length, it is not affordable to re-train for applying to tasks with longer sequences. Finally, when a Transformer satisfies the principles above, we will evaluate the performance, which requires thorough experiments and empirical analysis. Considering all the properties above, we propose Extrapolatable Position Embedding (xPOS), which is a universal-good design for Transformers. Based on RoPE's design, we propose attention resolution as a metric to measure position monotonicity accurately. Then, we generalize its mathematical form, where an exponential decay is added to the rotation matrix. XPOS preserves the advantage of ROPE, and behaves stably at long-term dependency. Besides, we use blockwise causal attention to increase attention resolution, which improves the performance of length extrapolation for language modeling. We train different Transformers from scratch. On the pre-training corpus, LEX Transformer reaches minimal perplexity on the validation set. | Models | Translation Invariance | Length Extrapolation |\n| :---: | :---: | :---: |\n| Absolute Position Modeling |  |  |\n| Transformer (Sinusoidal) | $x$ | $x x$ |\n| GPT-2 (Learnable) | $x$ | $x X$ |\n| Relative Position Modeling |  |  |\n| PaLM / Roformer (RoPE) | $\\checkmark$ | $x$ |\n| T5 | $\\checkmark$ | $x$ |\n| BLOOM / Alibi | $\\checkmark$ | $\\checkmark$ |\n| LEX Transformer (Ours) | $\\checkmark$ | $\\checkmark$ |\n\nTable 1: Position modeling capabilities of Transformer variants for language modeling. We use the arXiv dataset (above 6k length) to evaluate the model's ability for extrapolation length. Our methods can continue decreasing the perplexity while other methods either can't extrapolate or increase the perplexity when the input length is very long. We summarize our contributions as follows:\n\n- We summarize the design principles of Transformers for position modeling. - We define attention resolution to indicate length extrapolation. - We propose an extrapolatable position embedding and use blockwise causal attention to improve length extrapolation. - We conduct experiments on language modeling and show that the proposed LEX Transformer achieves strong performance on both short and long texts. ## 2 Design Principles of Transformers for Position Modeling\n\n### 2.1 Order Variance\n\nTransformer aims to capture long-term dependency efficiently (Vaswani et al., 2017), so the distance between every two tokens is 1 . Transformer without position information is actually a bag-of-word model. With effective position information, Transformer models should be variant with permuting the order (Dufter et al., 2022):\n\n$$\nf\\left(P_{\\pi}(X)\\right) \\neq P_{\\pi}(f(X))\n$$\n\nAlthough for some tasks, bag-of-words models can achieve comparable performance (Wang et al., 2020a), position information is essential generally for sequence modeling. Almost every position modeling strategy satisfies this goal (Vaswani et al., 2017; Devlin et al., 2019; Shaw et al., 2018; Wang et al., 2020a; Raffel et al., 2020; Su et al., 2021). ### 2.2 Translation Invariance\n\nThe representation of a sequence should be robust with the position's translation. For instance, in fact, a sentence's meaning is variant with padding before or after the whole sentence. We give a general form for translation invariance similar with (Wang et al., 2020a): for a Transformer model $f$ (input, mask), any input sequence $X=\\left[x_{0}, x_{1}, \\ldots, x_{n}\\right]$ with mask $M=\\left[m_{0}, m_{1}, \\ldots, m_{n}\\right]$, the output should be same with the padding one:\n\n$$\n\\begin{aligned}\nX_{\\mathrm{pad}} & =[0]_{i} \\oplus X \\oplus[0]_{j} \\\\\nM_{\\mathrm{pad}} & =[0]_{i} \\oplus M \\oplus[0]_{j} \\\\\nf(X, M) & =f\\left(X_{\\mathrm{pad}}, M_{\\mathrm{pad}}\\right)[i: i+n]\n\\end{aligned}\n$$\n\nObviously, relative positions (Shaw et al., 2018; Raffel et al., 2020; Wang et al., 2020a; Su et al., 2021) have this property instead of absolute ones (Vaswani et al., 2017; Devlin et al., 2019). Even though absolute sinusoidal embedding has a similar property (Vaswani et al., 2017): $P E_{p o s+k}$ can be represented as a linear function of $P E_{p o s}$, the addition operation in the initial word embedding messes the attention weight, where the spread form of $Q K^{T}$ has 4 components whose geometric connection with position is unclear. ### 2.3 Length Extrapolation\n\nAs the cost of pre-training is getting bigger due to the larger model size and corpus, we do not hope to retrain a model just because of the longer length of downstream tasks. A Transformer model with a suitable design should be capable of dealing with any input length. First, learnable absolute position embedding (Devlin et al., 2019) is not able to extrapolate at all because it does not have any pre-defined position knowledge. With the evaluation of perplexity on different length (Press et al., 2021), almost every position embedding's performance drops significantly (Vaswani et al., 2017; Raffel et al., 2020; Su et al., 2021). Alibi (Press et al., 2021) solves this problem by adding an exponential decay on the attention matrix, which lower the influence of outof-distribution position like a soft sliding window. However, the absence of long-term dependency contributes to a performance drop compared with other relative strategies. Table 2 shows that Alibi's perplexity is larger than RoPE about $0.2 \\sim 0.3$. However, the extrapolation ability needs a systematic design where position embedding is a crucial but not only component. With the proper attention map, the relative position can deal with long text, where the perplexity does not explode but does not decrease at the same time. The ideal situation is to use the long context in the right way, in that case, the model should perform better instead of saturation. ## 3 A Length-Extrapolatable Transformer\n\nWe define attention resolution as the indicator of length extrapolation in Section 3.1. Then we propose two ways to maximize the resolution metric, i.e., improve the length extrapolation of Transformers. First, we introduce a relative position encoding method (Section 3.2) to explicitly maximize attention resolution. Second, we propose to use blockwise causal masking (Section 3.3) during inference for improved resolution. The proposed architecture is named Length-Extrapolatable (LEX) Transformer. ### 3.1 Attention Resolution\n\nThe monotonicity of attention scores is essential to represent distance in language models. We denote $s[n]$ as the score expectation when the distance of two tokens is $n$. We define attention resolution $R(s)$ as a metric to evaluate attention's ability to recognize position:\n\n$$\nR(s)=\\sum_{i=0}^{N} \\frac{e^{s[i]}\\left(e^{s[i]}-e^{s[i+1]}\\right)}{\\left(\\sum_{i=0}^{N} e^{s[i]}\\right)^{2}}\n$$\n\nFirst, $s[i]>s[i+1]$ is preferred to ensure monotonicity.",
    "lex-3": "Besides, we implement softmax opera- tion to simulate the attention probability. To mitigate the influence of long-tail distribution, the factor $e^{s[i]}$ is multiplied. We can estimate $s[n]$ and $R(s)$ quantitatively when we design Transformers. ### 3.2 Improve Resolution by Position Encoding\n\nSu et al. (2021) propose that by adding absolute position embedding on query and key, the attention matrix is actually encoded with relative position information.",
    "lex-4": "We use a similar but generalized strategy. First, a pseudo inner product is defined as $\\langle x, y\\rangle=\\sum \\operatorname{Re}\\left(x_{i} \\cdot y_{i}^{*}\\right)$, which is consistent with the exact inner product's definition when we map $\\mathbb{C}^{d / 2} \\rightarrow \\mathbb{R}^{d}$. Formally, the encoding must satisfy:\n\n$$\n\\left\\langle f_{q}(q, n+r), f_{k}(k, n)\\right\\rangle=\\left\\langle f_{q}(q, r), f_{k}(k, 0)\\right\\rangle\n$$\n\nA simple solution is as follows:\n\n$$\n\\begin{aligned}\nf_{q}(q, n) & =A_{q} q e^{\\lambda n} \\\\\nf_{k}(k, n) & =A_{k} k e^{-\\lambda n}\n\\end{aligned}\n$$\n\nThe scaling factor $A_{q}, A_{k}$ is unnecessary because $q, k$ is obtained by a linear transformation. $\\lambda=k+i \\theta \\in \\mathbb{C}^{d / 2}$ where $k, \\theta \\in \\mathbb{R}^{d / 2}$ :\n\n$$\n\\begin{aligned}\nf_{q}(q, n) & =q e^{\\xi n+i \\theta n} \\\\\nf_{k}(k, n) & =k e^{-\\xi n-i \\theta n}\n\\end{aligned}\n$$\n\nIf $\\xi=0$, the form is the same as RoPE ( Su et al., 2021). Geometrically, the transformation provides a rotation on vectors. If the relative angle between $q$ and $k$ is larger, the inner product is smaller. However, the cosine value is not monotony if the rotating angle is large than $\\pi$, which causes an unstable phenomenon that the expectation of the inner product oscillates dramatically with the growth of relative distance. Following the parameters (Vaswani et al., 2017; Su et al., 2021) $\\theta=\\left\\{\\theta_{i}=10000^{-2 i / d}, i \\in[0,1, \\ldots, d / 2]\\right\\}$, we will calculate the expectation as follows. For generate models, we assume $\\mathbb{E}(\\angle q) \\leq \\mathbb{E}(\\angle k)$ to ensure the monotony:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\langle q e^{m \\xi+i m \\theta}, k e^{n \\xi+i n \\theta}\\right\\rangle\\right] \\\\\n= & \\sum_{i=0}^{d / 2} \\mathbb{E}\\left[\\operatorname{Re}\\left(\\boldsymbol{q}_{i} \\boldsymbol{k}_{i} e^{(m-n) \\xi_{i}+i(m-n) \\theta_{i}}\\right)\\right] \\\\\n\\leq & \\sum_{i=0}^{d / 2} \\operatorname{Re}\\left(\\mathbb{E}\\left[\\left|\\boldsymbol{q}_{i} \\boldsymbol{k}_{i}\\right|\\right] e^{(m-n) \\xi_{i}+i(m-n) \\theta_{i}}\\right) \\\\\n\\propto & \\sum_{i=0}^{d / 2} \\cos (m-n) \\theta_{i} e^{(m-n) \\xi_{i}}\n\\end{aligned}\n$$\n\nThe inference here is different from ( Su et al., 2021) because of two reasons: 1) there is an additional assumption brought by language models; 2) the inequality scaling of (Su et al., 2021) is too strong to lose generality. We calculate expectation instead of the upper bound. Now we define a function to represent the property of relative position:\n\n$$\ng_{\\zeta}[n]=\\sum_{i=0}^{d / 2} \\cos n \\theta_{i} \\zeta_{i}^{n}\n$$\n\nStabilizing the curve of $g[n]$ is an intuitive way. Even though attention bias can achieve this goal, we do not hope additional position calculation. Instead, we can achieve this goal by selecting a good $\\zeta$ to maximize $R\\left(g_{\\zeta}\\right)$. Obviously, the oscillation mainly comes from large $\\theta_{i}$. Manually setting $\\zeta$ can achieve this goal:\n\n$$\n\\widetilde{\\zeta}_{i}=\\frac{i /(d / 2)+\\gamma}{1+\\gamma} \\in[0,1]\n$$\n\nwhere $\\widetilde{\\zeta}_{i}$ becomes smaller when $\\theta_{i}$ is larger. In this way, we punish the oscillation of unstable dimensions and keep the distribution of stable ones. Numerical optimization methods are tried to find optimal values for $\\zeta$. However, the results rely on the initial value and lack control when the hidden dimension changes. Besides, the numerical precision should be considered because of fp16's range. Finally, we find a sub-optimal solution by manually setting $\\gamma$ to both satisfy the resolution is recognizable ( $R\\left(g_{\\zeta}\\right)$ is partially optimized) and $\\zeta_{i}^{n}$ can be represented by fp 16 when $n$ is big ( 8192 in our setting). The optimized value $\\hat{\\zeta}$ will be used as the final value in LEX Transformer. The curves of $\\zeta=\\mathbf{1}, \\hat{\\zeta}$ are shown in Figure 1. The default rotary embedding contributes to a dramatic oscillation, especially in the large relative distance, which causes bad extrapolation performance and restricts the model's convergence speed. After adding a decay, the curve is almost stable, especially on long-term dependency. What's more, it does not hurt pure rotation's fitting ability because $\\zeta_{i}^{n} \\approx 1$ when $i$ is large or $n$ is small. In that way, short-term and long-term dependencies are divided continuously. Finally, we have Extrapolatable Position Embed-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_eb024fe2505b67400e7bg-4.jpg?height=578&width=781&top_left_y=248&top_left_x=1046)\n\nFigure 1: The long dependency curve of attention expectation. RoPE's dramatic oscillation confuses the attention resolution at long distances. In contrast, XPOS provides stable and accurate position modeling. ```\nAlgorithm 1: Attention with XPos\n    def \\(\\operatorname{rot}(x)\\) :\n    return \\(\\left[-x_{1}, x_{0},-x_{3}, x_{2}, \\ldots\\right]\\)\n    Initialization:\n    \\(\\theta_{i}=1 / 10000^{2 i / d}, \\theta \\in \\mathbb{R}^{d / 2}\\)\n    \\(\\hat{\\zeta}_{i}=(i /(d / 2)+\\gamma) /(1+\\gamma), \\hat{\\zeta} \\in \\mathbb{R}^{d / 2}\\)\n    Input: \\(Q, K, V \\in \\mathbb{R}^{h \\times l \\times d}, M \\in \\mathbb{R}^{d \\times d}\\)\n    \\(C_{m n}=\\cos m \\theta_{n}, C \\in \\mathbb{R}^{l \\times d / 2}\\)\n    \\(S_{m n}=\\sin m \\theta_{n}, S \\in \\mathbb{R}^{l \\times d / 2}\\)\n    \\(T_{m n}=\\hat{\\zeta}_{n}^{m}, T \\in \\mathbb{R}^{l \\times d / 2}\\)\n    \\(Q=(Q \\times C+\\operatorname{rot}(Q) \\times S) \\times T\\)\n    \\(K=(K \\times C+\\operatorname{rot}(K) \\times S) \\times T^{-1}\\)\n    output \\(=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}} \\cdot M\\right) V\\)\n    return output\n```\n\nding (xPos):\n\n$$\n\\begin{gathered}\nf_{q}(q, n)=\\left(\\begin{array}{c}\nq_{1} \\cos n \\theta_{1} \\hat{\\zeta}_{1}^{n}-q_{2} \\sin n \\theta_{1} \\hat{\\zeta}_{1}^{n} \\\\\nq_{2} \\cos n \\theta_{1} \\hat{\\zeta}_{1}^{n}+q_{1} \\sin n \\theta_{1} \\hat{\\zeta}_{1}^{n} \\\\\n\\vdots \\\\\nq_{n-1} \\cos n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{n}-q_{n} \\sin n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{n} \\\\\nq_{n} \\cos n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{n}+q_{n-1} \\sin n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{n}\n\\end{array}\\right) \\\\\nf_{k}(k, n)=\\left(\\begin{array}{c}\nk_{1} \\cos n \\theta_{1} \\hat{\\zeta}_{1}^{-n}-k_{2} \\sin n \\theta_{1} \\hat{\\zeta}_{1}^{-n} \\\\\nk_{2} \\cos n \\theta_{1} \\hat{\\zeta}_{1}^{-n}+k_{1} \\sin n \\theta_{1} \\hat{\\zeta}_{1}^{-n} \\\\\n\\vdots \\\\\nk_{n-1} \\cos n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{-n}-k_{n} \\sin n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{-n} \\\\\nk_{n} \\cos n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{-n}+k_{n-1} \\sin n \\theta_{d / 2} \\hat{\\zeta}_{d / 2}^{-n}\n\\end{array}\\right)\n\\end{gathered}\n$$\n\nIn the implementation, the transformation for key and value can be easily calculated by parallel addition and multiplication as shown in Algorithm 1 . ![](https://cdn.mathpix.com/cropped/2024_09_12_eb024fe2505b67400e7bg-5.jpg?height=518&width=761&top_left_y=232&top_left_x=245)\n\nFigure 2: Our language model is trained on shorter texts in the same way as vanilla Transformers, i.e., using causal masking. During inference, we use blockwise causal attention for longer sequences, which recurrently reuses the overlapped parts (i.e., key and value vectors). ### 3.3 Blockwise Causal Attention\n\nAnother way to improve attention resolution (Section 3.1) is using windowed attention. During inference, we use blockwise masking (Dai et al., 2019; Zaheer et al., 2020; Xiong et al., 2021) for selfattention. Notice that other window strategies, such as sliding window (Child et al., 2019), also work. We use blockwise causal attention because it is cache-friendly and easy to implement. As shown in Figure 2, if the pre-training length is $l$, we divide the query as blocks with $l / 2$ length, and each query interacts with its own block and the last block. In this way, the context information can be delivered by the reuse of key and value. The window constraint helps models to encode longer input with improved resolution. Different from training a long-sequence model with stop-gradient, we use vanilla attention in the training phase, because the pre-training corpus is not very long on average. However, during the inference phase, when dealing with long sequences, we directly implement BCA to help the model to be more position-recognizable. ## 4 Experiments\n\n### 4.1 Pre-training\n\nTo fairly evaluate different Transformer variants, we pre-train the Transformer from scratch. We use 1024 hidden dimension, 16 heads, and 24 layers, i.e., comparable to medium-size GPT-3 (Brown et al., 2020). The training corpus includes a subset of the Pile (Gao et al., 2020): Books3, OpenWebText2, Stack Exchange, PubMed Ab- stracts, Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. The training procedure is implemented on $16 \\times$ V100 GPUs. Maximal length is 1024 for saving memory and extrapolation evaluation. The learning rate is $3 \\times 10^{-4}$ and polynomial decay is used to adjust learning rate. The global batch size is 512 to follow GPT-3(Brown et al., 2020), i.e., 0.5 M token size. We use Adam (Kingma and Ba, 2015) optimizer with $\\beta_{1}=0.9, \\beta_{2}=0.98, \\epsilon=10^{-6}$. The code is based on TorchScale (Ma et al., 2022a). ### 4.2 Language Modeling\n\nWe first measure perplexity on arXiv, where the document length is usually larger than 6 k , which can show the model's ability for long-dependency modeling. We care about the performance on different input lengths to evaluate the model's interpolation and extrapolation capability. For every document, we select its first 4 k tokens and divide them into the target length to fairly compare the perplexity of different lengths. The results are shown in Table 2. For interpolation capability, we analyze the results where the length is no more than 1024. All Transformers converge to similar perplexity. XPOS have a stable advantage on others with $1 \\sim 3$ perplexity drop. For lengths 2048 and 4096, we use BCA in all position embeddings, and the following ablation study will discuss the performance without that. Press et al. (2021)'s experiment shows that most of the position strategies can't deal with input length longer than pre-training directly. In our experiment, with the improvement brought by BCA, RoPE gets a better performance while Absolute still can't extrapolate. XPos shows a stable decrease when the sequence length increases, which satisfies the assumption that a longer context makes the prediction better. While others' perplexity increases when the input length is 4096. Here, XPos's advantage towards RoPE is worth analyzing. With BCA, the position embedding does not extrapolate, so RoPE also has the potential to encode long documents. However, with the forward layer by layer, the distribution of hidden states is different from pre-training. Then, the resolution matters to building a recurrent-similar encoding. The experiment shows that XPos gets better performance on language modeling. With the stable advantage of any length, users can input any sen-\n\n| Length | $\\mathbf{2 5 6}$ | $\\mathbf{5 1 2}$ | $\\mathbf{1 0 2 4}$ | $\\mathbf{2 0 4 8}$ | $\\mathbf{4 0 9 6}$ |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n|  | Interpolation |  |  | Extrapolation |  |\n| Transformer | 46.34 | 36.39 | 29.94 | 132.63 | 1283.79 |\n| Alibi | 37.66 | 29.92 | 24.99 | 23.14 | 24.26 |\n| Roformer | 38.09 | 30.38 | 25.52 | 73.6 | 294.45 |\n| LeX Transformer (Ours) | $\\mathbf{3 4 .",
    "lex-5": "3}$ | $\\mathbf{2 7 . 5 5}$ | $\\mathbf{2 3 . 3 1}$ | $\\mathbf{2 1 . 6}$ | $\\mathbf{2 0 . 7 3}$ |\n\nTable 2: Results of perplexity with different lengths. The language models are trained with a length of 1024 and then evaluated on various lengths. LEX obtains better performance not only on shorter texts (i.e., interpolation) but also on longer texts (i.e., extrapolation). The red color indicates that the perplexity begins increasing compared with the shorter length. LEX is the only method that has lower perplexity along with increased evaluation length. | Length | $\\mathbf{1 0 2 4}$ <br> Interpolation | $\\mathbf{2 0 4 8}$ <br> Extrapolation |\n| :--- | :---: | :---: |\n| Transformer | 0.87 | 0.28 |\n| Alibi | 0.81 | 0.88 |\n| Roformer | 0.91 | 0.08 |\n| LEX (Ours) | $\\mathbf{0 .",
    "lex-6": "9 8}$ | $\\mathbf{1 . 0 8}$ |\n| - BCA | $\\mathbf{0 . 9 8}$ | 0.54 |\n\nTable 3: Results of resolution with different Transformer variants. Higher resolution indicates that the architecture tends to better distinguish context tokens. \"BCA\" is short for blockwise causal attention. tence freely without the concern of position. Besides, results also indicate that is not essential to build an explicit decay on the attention matrix, Instead, a proper design for an attention mask is actually better to deal with long-context tasks. ### 4.3 Measuring Resolution\n\nIn the previous section, we claim that resolution is a crucial index for building an effective Transformer. To verify the claim, we evaluate the resolution of different Transformer variants empirically. Equation 8 estimates the expectation of attention score for LEX. Denote attention score of query $i$ and key $j$ (before softmax) as $e_{i j}$, the expectation of $s[n]$ is as follows:\n\n$$\n\\hat{s}[n]=\\mathbb{E}[s[n]]=\\frac{1}{N-n} \\mathbb{E}\\left[\\sum_{i=n}^{N-1} e_{i(i-n)}\\right]\n$$\n\nThe resolution can be calculated by combining Equation 3 and 11. The final expectation is the average of different input text. Resolution is calculated in every layer, and the average resolution is shown in Table 3. The results show that XPOS makes the position more recognizable in training length (1024). For Alibi (Press et al., 2021), the\n\n| Methods | Perplexity |\n| :--- | :--- |\n| RoPE | 17.74 |\n| xPos (Ours) | $\\mathbf{1 7 . 5 4}$ |\n| - Rotation | 33.68 |\n\nTable 4: Ablation results on the validation set show that rotation of XPOS is necessary for strong performance.",
    "lex-7": "stable resolution comes from explicit decay, but it prevents the model from learning position dependency itself. Besides, we run an ablation on BCA. In length 2048, we measure the resolution with/without block. The result supports that BCA helps model distinguish positions better. ### 4.4 Ablation Studies\n\n### 4.4.1 Rotation Computation\n\nIn this part, we discuss the necessity of the combination of vector rotation and exponential decay. xPos without rotation means Equation 10 degenerates to $\\theta_{i}=0$ :\n$\\dot{f}_{q}(q, n)=\\left(\\begin{array}{c}q_{1} \\hat{\\zeta}_{1}^{n} \\\\ q_{2} \\hat{\\zeta}_{1}^{n} \\\\ \\vdots \\\\ q_{n-1} \\hat{\\zeta}_{d / 2}^{n} \\\\ q_{n} \\hat{\\zeta}_{d / 2}^{n}\\end{array}\\right) \\dot{f}_{k}(k, n)=\\left(\\begin{array}{c}k_{1} \\hat{\\zeta}_{1}^{-n} \\\\ k_{2} \\hat{\\zeta}_{1}^{-n} \\\\ \\vdots \\\\ k_{n-1} \\hat{\\zeta}_{d / 2}^{-n} \\\\ k_{n} \\hat{\\zeta}_{d / 2}^{-n}\\end{array}\\right)$\nAfter pre-training, we test the perplexity on the valid split of training corpus with 1 k length.",
    "lex-8": "The result in Table 4 shows that simple scaling operation can't perform as well as LEX. Therefore, the combination of rotation and decay means the combination of in-distribution and out-of-distribution ability. ### 4.4.2 Blockwise Causal Attention\n\nTo fairly compare different methods, we run the evaluation using different position embeddings (i.e.,\n\n| Methods | $\\mathbf{2 0 4 8}$ <br> Extrapolation |  |\n| :--- | :--- | :--- |\n| RoPE | 73.6 | 294.45 |\n| RoPE + BCA | 25.57 | 25.65 |\n| Alibi | 23.14 | 24.26 |\n| Alibi + BCA | 24.6 | 25.37 |\n| XPoS (Ours) | 22.56 | 28.43 |\n| xPos + BCA (Ours) | $\\mathbf{2 1 . 6}$ | $\\mathbf{2 0 . 7 3}$ |\n\nTable 5: Results of perplexity on arXiv dataset. \"BCA\" is short for blockwise causal attention. Alibi, RoPE, and XPOs) with or without blockwise causal attention. The results are shown in Table 5. First, Blockwise Causal Attention works for RoPE whose perplexity will explode without that. Alibi performs well without windowed attention because its \"soft window\" is broader than a hard block window. XPOS's perplexity without BCA increases by about 1 in 2048, and 8 in 4096 . However, with its high resolution, XPos can recognize position with BCA's constraint. ## 5 Related Work\n\n### 5.1 Long-Sequence Transformers\n\nLong-sequence Transformers aim to solve two key problems.",
    "lex-9": "First, the computation or memory consumption is not efficient enough for long sequences. Second, there is a trade-off between performance and efficiency. One popular solution (Wang et al., 2020b; Katharopoulos et al., 2020; Choromanski et al., 2020) is linear attention, i.e., using a kernel-based or low-rank approximation to replace vanilla attention. The methods typically target efficiency while underperforming vanilla Transformers for regular length. Another strand is sparse attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Xiong et al., 2021), which usually leverages structured sparsity to reduce computation. For causal sequence modeling, the recurrent-style designs (Dai et al., 2019; Hutchins et al., 2022; Ma et al., 2022b) are also competitive. In comparison, we focus on the extrapolation issue (Press et al., 2021) for language modeling, i.e., training on short texts while evaluating long texts. The training process is kept the same as vanilla Transformers, i.e., training on short sequences, and using dense attention computation. The capability of long-sequence modeling is given for free during inference. So the training efficiency (which is typically expensive for large-scale language models) is not affected compared with previous work. Moreover, the performance on regular length is perfectly retained, without trade-offs for long-sequence modeling. ### 5.2 Position Modeling\n\n### 5.2.1 Absolute Position Embedding\n\nAbsolute sinusoidal position embedding is proposed by Vaswani et al. (2017). For each dimension, different frequencies are encoded from $2 \\pi$ to $10000 \\times 2 \\pi$ :\n\n$$\n\\begin{aligned}\n\\mathrm{PE}_{(p o s, 2 i)} & =\\cos \\left(p o s / 10000^{2 i / d_{\\mathrm{model}}}\\right) \\\\\n\\mathrm{PE}_{(p o s, 2 i+1)} & =\\sin \\left(p o s / 10000^{2 i / d_{\\mathrm{model}}}\\right)\n\\end{aligned}\n$$\n\nwhere $\\mathrm{PE}_{p o s+k}$ is represented as a linear function of $\\mathrm{PE}_{p o s}$ to restore a relative-position property. ### 5.2.2 Relative Position Embedding\n\nShaw et al. (2018) propose relative position embedding as an alternative approach. Denote $e_{i j}$ as attention weight, $\\alpha_{i j}=\\operatorname{softmax}\\left(e_{i j}\\right), o_{i}$ as output, we have:\n\n$$\n\\begin{gathered}\ne_{i j}=\\frac{\\boldsymbol{q}_{i} \\cdot \\boldsymbol{k}_{j}}{\\sqrt{d}} \\Longrightarrow \\frac{\\boldsymbol{q}_{i} \\cdot\\left(\\boldsymbol{k}_{j}+\\boldsymbol{a}_{i j}^{K}\\right)}{\\sqrt{d}} \\\\\no_{i}=\\sum_{j} \\alpha_{i j} \\boldsymbol{v}_{j} \\Longrightarrow \\sum_{j} \\alpha_{i j}\\left(\\boldsymbol{v}_{j}+\\boldsymbol{a}_{i j}^{V}\\right)\n\\end{gathered}\n$$\n\nwhere $\\boldsymbol{a}_{i j}^{K}=\\omega_{\\operatorname{clip}(i-j, k)}^{K}, \\boldsymbol{a}_{i j}^{V}=\\omega_{\\operatorname{clip}(i-j, k)}^{V}$, and $\\omega^{K}$ and $\\omega^{V}$ are learnable parameters.",
    "lex-10": "The clipping strategy helps length generalization but cannot distinguish the positions that are larger than $k$. Yang et al. (2019) and He et al. (2020) further reparameterize the relative position vectors for better performance. T5 (Raffel et al., 2020) uses a simpler strategy to encode relative position:\n\n$$\ne_{i j}=\\frac{\\boldsymbol{q}_{i} \\cdot \\boldsymbol{k}_{j}}{\\sqrt{d}}+a_{\\text {bucket }(i-j)}\n$$\n\nwhere log-bucket scalars are added to attention scores. Recently, pre-defined position embedding is brought back by RoPE (Su et al., 2021). Alibi (Press et al., 2021) proposes to explicitly build an exponential decay on the attention matrix, which contributes to length extrapolation:\n\n$$\ne_{i j}=\\frac{\\boldsymbol{q}_{i} \\cdot \\boldsymbol{k}_{j}}{\\sqrt{d}}-\\mathrm{m}(i-j), \\quad \\mathrm{m}(\\cdot)>0\n$$\n\nwhere the values of $m(\\cdot)$ are manually defined. However, Alibi (Press et al., 2021)'s performance\ntends to be inferior to RoPE for the context whose length is shorter than the pre-training length. In this work, we propose a theoretically derived relative position embedding XPOS that optimizes the attention resolution between tokens. The XPos method not only has the nice property of length extrapolation but also achieves strong performance. ## Limitations\n\nIn this work, we focus on causal language modeling. It needs additional efforts to integrate the proposed methods into bidirectional attention, such as masked language modeling (Devlin et al., 2019). Moreover, xPos introduces about $6 \\%$ inference cost compared with absolute position embeddings, although it accelerates training convergence.",
    "lex-11": "## References\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. URL https://openai.com/blog/sparse-transformers.",
    "lex-12": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways. $A r X i v$, abs/2204.02311. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Philipp Dufter, Martin Schmitt, and Hinrich Sch\u00fctze. 2022. Position information in transformers: An overview. Computational Linguistics, 48(3):733763 . Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:17351780 . DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. 2022. Blockrecurrent Transformers. In Advances in Neural Information Processing Systems. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear attention.",
    "lex-13": "In International Conference on Machine Learning, pages 5156-5165.",
    "lex-14": "PMLR. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, San Diego, CA. Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022a. TorchScale: Transformers at scale. CoRR, abs/2211.13184. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2022b. Mega: Moving average equipped gated attention. arXiv preprint arXiv:2209.10655. Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.",
    "lex-15": "Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155.",
    "lex-16": "Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000-6010. Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen. 2020a. On position embeddings in bert. In International Conference on Learning Representations. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Selfattention with linear complexity. arXiv preprint arXiv:2006.04768. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. 2022. Image as a foreign language: BEiT pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442. Wenhan Xiong, Barlas O\u011fuz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Wen-tau Yih, and Yashar Mehdad. 2021. Simple local attentions remain competitive for long-context tasks. arXiv preprint arXiv:2112.07210. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding.",
    "lex-17": "In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283-17297.",
    "lex-18": ""
}