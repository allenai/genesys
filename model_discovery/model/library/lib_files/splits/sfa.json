{
    "sfa-0": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention\n\nMatteo Pagliardini* EPFL matteo.pagliardini@epfl.ch &Daniele Paliotta* University of Geneva daniele.paliotta@unige.ch &Martin Jaggi EPFL martin.jaggi@epfl.ch &Fran\u00e7ois Fleuret University of Geneva francois.fleuret@unige.ch\n\nAbstract\n\nTransformer-based language models have found many diverse applications requiring them to process sequences of increasing length.",
    "sfa-1": "For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by and for sequences of respectively and tokens. \u2020\u2020* Equal contribution. 1 Introduction\n\nMany methods have been developed to mitigate the quadratic cost of self-attention in Transformers (Vaswani et al., 2017). Some methods attempt to linearize the attention (Beltagy et al., 2020; Wang et al., 2020) by for instance linearizing the softmax operator to take advantage of the associativity of matrix products (Katharopoulos et al., 2020). Other methods rely on a predefined sparse masking of the attention matrix, e.g. to constrain the attention to a local temporal neighborhood (Zaheer et al., 2020; Child et al., 2019). While the structure is fixed, it is assumed that information from arbitrary locations in the sequence can still flow through this structure over several layers. All those methods impose static implicit or explicit constraints over the attention matrix. Another promising line of work consists in computing a dynamic modulation of a sub-part of the attention matrix. They are based, for instance, on dropping keys and queries (Kim et al., 2022) or using geometric hashing of the keys and queries to identify linear cost sub-blocks of the attention matrix that carry most of the weight (Kitaev et al., 2020). The promising theoretical computational complexity of these methods contrasts with the fact that today\u2019s most successfully deployed practical models instead rely on vanilla attention, in part thanks to the efficiency of FlashAttention (Dao et al., 2022). This implementation is mathematically identical to the vanilla attention proposed by Vaswani et al. (2017) in their seminal paper, but trades in additional compute for less memory I/O.While still avoiding a memory footprint quadratic with the sequence length, it delivers practical speedups of over compared to a naive implementation. Using an attention layer in an autoregressive model\u2014which has been key in the recent remarkable AI breakthroughs\u2014requires to make it causal. This is achieved by applying a mask to the attention matrix, so that information cannot flow from the future to the past during training. While FlashAttention can deal with vanilla causal masks, it does not provide enough flexibility to be used for situations where the causal attention mask is not perfectly regular, that is, lower triangular. This in particular prevents using it for models that dynamically drop keys and queries or rely on geometric hashing, which results in irregular causal structures as illustrated in Fig.",
    "sfa-2": "1 and Fig. 2. We propose an extension of FlashAttention\u2014Sparse Causal Flash Attention (SCFA)\u2014 that addresses this constraint. Our contribution is threefold:\n\n\u2022\n\nWe present the SCFA GPU kernel, which relaxes the constraint that the causal mask has to be triangular. This kernel can handle any sparsity pattern that can be expressed with a range of keys per query, and any causal masking in the resulting sub-blocks. See \u00a7 3. \u2022\n\nWe show that SCFA permits to revisit the promising paradigm of dynamic hash-based attention. We devise an algorithm that builds upon the fundamental idea of Reformer (Kitaev et al., 2020) to restrict the computation of the attention matrix over \u2018hash collision blocks\u2019, but avoids both the high computational cost, and the approximate coverage of the hash collisions. See \u00a7 3.2. \u2022\n\nWe propose a new approach implemented with SCFA that reduces computation by dynamically selecting, for each head, keys and queries to be removed from the attention operation, superseding existing methods that limited pruning to entire heads or entire queries/keys, due to the lack of an efficient fine-grained kernel implementation. See \u00a7 3.1. Experimental evaluations show that SCFA can efficiently be used for a variety of sequence modeling tasks, and that our open-source implementation in the Triton language and compiler (Tillet et al., 2019) significantly outperforms FlashAttention as we increase the sparsity and for longer sequences. Moreover, unlike the hash-based attention introduced in Reformer (Kitaev et al., 2020), our hash-based SCFA not only implements the exact computation, but also has a faster runtime (see \u00a7 4.2). Finally, we show that a prototype of query and key dropping can be implemented thanks to SCFA, and that the computational reduction is proportional to the fraction of query-key pairs dropped (see \u00a7 4.3). 2 Related work\n\nState-of-the-art sequence models have very high computational requirements. As a consequence, a lot of effort has been invested into developing methods to reduce the memory footprint in Transformers. Many efficient Transformer variants have been developed, with the main goal of taming the quadratic complexity of the attention mechanism (Tay et al., 2020). Several methods rely on kernelized attention (Katharopoulos et al., 2020; Choromanski et al., 2020), while others endow the Transformer with some auxiliary memory to increase the context (Wu et al., 2022; Borgeaud et al., 2021). In many cases, leveraging sparsity in the attention matrix has proven useful. The Sparse Transformer (Child et al., 2019) works with a factorized sparse representation of the attention. They employ several sparse attention patterns, where each output position only computes weightings from a subset of input positions. The Reformer (Kitaev et al., 2020) uses locality-sensitive-hashing (LSH) to sparsify the attention matrix and allow queries to restrict their context window to keys that collide with the same hash. However, to allow GPU-efficient processing, complex machinery has to be developed where the queries and keys are split into fixed-sized chunks, with the attention being applied only within the chunk and the immediate neighbor. FlashAttention introduced by Dao et al. (2022) has recently gained a lot of popularity as an efficient, IO-aware exact attention implementation. FlashAttention uses tiling to avoid materializing the full attention matrix on slow GPU HBM, splitting the computation over blocks of query, key, and value vectors. FlashAttention has already reached wide adoption, as it\u2019s now available directly in Pytorch as of version 2.0. Additionally, FlashAttention supports very efficient block-sparse structures. Bigbird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) are two more variants that work with sparsified version of the attention matrix. Both approaches rely on a fixed structure that is independent of the input values, using a combination of local, global, and random attention. Hash Attention. When computing the attention matrix for a query tensor and a key tensor , we consider the matrix of dot-products , which can become impractical to compute for very long sequences. However, we are only interested in the row-wise , meaning that the contribution of the keys to every query is dominated by the ones with the highest similarity. Thus, restricting the attention computation to queries and keys with high similarity is a natural choice to reduce the computation. Hash attention, introduced in the Reformer (Kitaev et al., 2020), allows to quickly select the closest key vectors for each query using locality-sensitive-hashing (LSH). In general, the LSH mechanism assigns a hash code to vectors with the requirement that vectors that are close in space are mapped to the same hash with high probability. For the hash attention, the Reformer assumes a shared query-key space (). After computing the hashes, the queries are sorted according to their hash bucket. In the sorted attention matrix, pairs that fall into the same bucket cluster near the diagonal. In order to implement the LSH-attention scheme efficiently on GPU, the Reformer splits the queries into fixed-sized chunks. Queries belonging to the same chunk can attend to each other and one chunk back. This results in a suboptimal mechanism where there is no guarantee that the attention will capture exactly all of the elements that belong to the same bucket (See Fig. 5). FlashAttention. The standard self-attention operation consists of multiplying a query tensor by a key tensor , to obtain a matching score matrix, which is then rescaled and row-normalized with softmax, to get a attention matrix . This matrix is then multiplied by a value tensor to obtain the final result. This is the core operation in a standard Multi-Head Attention layer, where additional operations take place to compute , , and from the layer\u2019s input, and multiple instances of this processing take place in parallel. The two key contributions of FlashAttention are (1) to compute the attention matrix block-wise, to minimize the transfer of keys and queries to the cache memory as much as possible, and (2) to compute the attention matrix on the fly during both the forward and the backward passes, which is faster than retrieving it from memory, and avoids a memory footprint quadratic with the sequence length . For the generalization that is of concern to this article, we focus on the block computation. In the implementation of FlashAttention, causal masking is done by using the row and column indexes of the blocks, and the row and column indexes of the keys and queries in individual blocks: attention blocks are computed fully for any block with a query index strictly larger than the key index. For the blocks for which the query index is equal to the key index, a regular lower triangular mask is applied.",
    "sfa-3": "This is illustrated on Fig. 2, bottom left. 3 Method\n\nWe develop an efficient CUDA kernel written in Triton (Tillet et al., 2019) that maintains the careful memory management of FlashAttention but can handle a causal structure defined through an arbitrary indexing of the keys and the queries. In the case where this indexing consists of a binary decision to drop or not the head of a query/key, this corresponds to our QK-sparse kernels as described in \u00a7 3.1. In the case where the indexing corresponds to bucket indices e.g. obtained from hashing, this corresponds to our Hash-sparse kernel described in \u00a7 3.2. Notations. Input tensors for attention as in Vaswani et al. (2017) are of shape , with being the batch size, the number of heads, the sequence length, and the dimension per head. In the following we take the view of a single head and instead consider a query tensor of shape , and a key and value tensors of shapes . The algorithms described below will be run in parallel for all elements of the Cartesian product . We split tensors into blocks: , . We define a tile , which corresponds to the dot products of a subpart of the attention matrix (see Fig. 2). 3.1 QK-Sparse Attention\n\nShrinking the attention matrix. Our QK-sparse attention kernel is best summarized in the first row of Fig. 1. Independently for each head, we decide to keep or drop keys and queries. We then remove dropped keys and queries to create smaller , , and tensors. Through this reduction we are left with a smaller attention matrix which still has a causal structure in that indices for the queries and keys are increasing monotonically. Leveraging non-triangular causal attention structure. Despite the advantageous structure of the smaller attention matrix, existing implementations fail to take advantage of it. Especially, as shown in Fig. 2 bottom-left, FlashAttention can leverage the causal structure when the causal mask is triangular, but does not support any other shape. In the forward pass, FlashAttention is, for each block of queries , processing blocks of keys one after the other, moving along a row of tiles: . Causality dictates that it is unnecessary to process a tile when . We cannot follow this rule anymore when working with compact representations. To leverage the causal structure of , we build a new kernel which gets as additional input vectors and representing the indices of the queries and keys in the original uncompressed tensors. Those are similarly split into blocks: . The condition for a tile to be unnecessary to compute is now to have . When processing a block of queries , we iterate over the key indices to find the index of the first block satisfying that condition. We then know we need to process the tiles for . Within each tile , we in addition apply a local causal mask by comparing indices in and . By computing in such a way we can leverage the causal structure and have runtimes matching those of FlashAttention.",
    "sfa-4": "The backward pass can be adapted in a similar fashion, see App. B for more details. Overhead. Computing , , and requires sorting and allocating new tensors. Moreover, as we drop keys and queries for every attention head, and for every sequence in the minibatch, we are forced to consider the largest sequence of non dropped keys/queries and use padding. However, while reordering and reshaping tensors can be costly, this overhead grows linearly with the sequence length and is largely compensated for larger sequences as we show in \u00a7 4.3. Edge cases. Dropping keys and queries can result in having stranded queries with no keys. This behaviour is undefined and results in NaNs when using the FlashAttention and naive Pytorch implementations. We solve this issue by modifying how softmax statistics are accumulated during the forward and backward passes and ensure stranded queries default to vectors.",
    "sfa-5": "see App. B for more details. 3.2 Hash-Sparse Attention\n\nRestructuring attention based on hashes. Independently for each head, we associate a bucket identifier to each key and query. We then need to reorder by sorting them along the sequence length dimension. As shown in the bottom row of Fig.1, this results in clusters of keys and queries with a similar hash index close to the diagonal. If the sorting is stable, i.e. it preserves ordering of queries and keys when the hash index is the same, then those blocks have a local causal structure in which the original indices (original position in the sequence) of keys and queries is a monotonic function within the block. This brings us in a case very similar to the previous one in section \u00a7 3.1, in that we now have the same structure but scattered by blocks within the full attention matrix. Taking advantage of the new structure. We would like to take advantage of the block structure and only compute attention for queries and keys falling into the same block while at the same time respecting causality. We adapt the FlashAttention kernel in a very similar way as for our QK-sparse kernel. We now provide additional bucket indices and to our kernel. Based on those hash indices, we now find not only the stopping index but also a starting index . is the first index for which some of the indices in are present in , is the first index for which all indices in are strictly larger than indices in . In a second step we refine now based on the indices and , the updated is the last index for which . As shown in the last column of Fig. 2, we then only compute tiles for . As for the QK-sparse method, we use and to apply a causal mask locally for each tile. In addition to the causal mask, we use and to mask interactions between keys and queries of different buckets.",
    "sfa-6": "See App. B for details and to see how to adapt the backward pass in a similar fashion. Overhead. As for the previous method, sorting and re-ordering , and is inducing some overhead increasing linearly with the sequence length. As shown in our experiments in \u00a7 4.2, this overhead is by large compensated for as the sequence length increases. 4 Experiments & Results\n\nIn this section we present our experimental setup and results. We show that (i) unlike naive implementations using existing libraries, our dynamic sparsity attention schemes can significantly improve over the FlashAttention runtime, (ii) this still holds in real-world sequence modeling tasks after factoring in all the non-attention operations, and (iii) it is possible to match\u2014and sometimes outperform\u2014the baselines in terms of perplexity while significantly gaining in speed. 4.1 Experimental Setup\n\nDatasets. We test our hash-based sparsity scheme on MNIST (LeCun et al., 1998) for autoregressive image generation, enwik8 (Hutter, 2012), and OpenWebText2 (Gao et al., 2020). We experiment with QK-dropping based sparsity on OpenWebText2. Models & Baselines. For our language modeling experiments on OpenWebText2, we use a base autoregressive transformer architecture with layers, a hidden size of , heads of dimensions each. For experiments on sequence length , we use a batch size of (batch size with accumulation steps and data parallelism over node). When we use a batch size of . The resulting models are of around M parameters. The goal not being to outperform the state-of-the-art perplexity, we train for iterations. The attention modules used are either using FlashAttention for the baselines or one of our sparse kernels for our methods. To ensure a fair comparison, and similarly to Kitaev et al. (2020), we set the keys equal to normalized queries for all of our models.",
    "sfa-7": "See App.",
    "sfa-8": "B for more details. Hardware. All of our timing experiments with random tensors are done on NVIDIA A100 GPUs, using bfloat16. For our language modeling tasks on OpenWebText2, we trained using data-parallelism on two or three A100s for experiments with sequence lengths of respectively and . When comparing runtimes in Fig 6 and Fig. 8, we normalize the times by multiplying by the number of GPUs used. Comparisons with the Reformer are performed on a single A100 or a single NVIDIA RTX 4090 GPU. 4.2 Hash-based Attention\n\nHashing mechanism For our experiments, we adopt the same hashing procedure as Kitaev et al. (2020). Namely, we use a shared query-key space, and we disallow queries to attend to themselves. We also adopt the LSH scheme from Andoni et al. (2015). This allows us to pick the number of unique hash codes. We refer to bucket as the set of vectors that map to a certain hash. Runtime performances in a vacuum. We test our implementation with different numbers of buckets and random keys, queries, and values. In these tests, we assume a hash bucket is provided for free for each head of each key and query (they are sampled uniformly at random (torch.randint(0,nb)). In practice, runtime experiments on sequence modeling tasks show that obtaining the buckets can be cheap and in no way prevents us from improving the attention runtime (see Fig. 6). We compare with causal FlashAttention over the entire sequence. Importantly, to ensure a fair comparison, we take into account pre-processing and post-processing steps required to reshape the tensors for both methods. For our method this includes stable sorting by bucket index and transposing tensors, for the baseline only the transposition is required, see App. B.2 for detailed code. Fig. 3.b summarises our findings. We observe large improvements in runtime as the number of buckets and the sequence length increases. Language modeling on OpenWebText2. For sequences of length and we train transformer language models using FlashAttention (F-LM), and identical models replacing only the FlashAttention by our Hash-based sparse attention (H-LM) using hash buckets. In Fig. 6 we see that it takes the same number of iterations for H-LM and F-LM to reach a given perplexity. However, H-LM iterations are and faster for respectively and . As a result, H-LM models reach a given perplexity much faster than their F-LM counterpart. Interestingly, we observe the H-LM models gain speed during training, see App. C for additional details. Comparison with Reformer. We compare the speed and performance of our hash-sparse implementation with the Reformer hashed attention. For all comparisons, we always equalize the average bucket size.",
    "sfa-9": "Results are summarized in Fig. 5. Benchmarks with random inputs show that both our hash-sparse implementation and the Reformer, as expected, are linear with respect to the sequence length (Fig.",
    "sfa-10": "5.a).",
    "sfa-11": "However, we still achieve a significant speedup thanks to our more efficient kernel. More importantly, Fig. 5.b shows that the fixed attention structure imposed by the Reformer does not allow to capture all of the hash collisions, with the coverage decreasing steeply as the sequence length increases. On the contrary, our method is exact and covers every bucket collision in the attention matrix. This is reflected in Table 5: our hash-sparse attention layer outperforms the Reformer attention even for shorter sequences. 4.3 Query/Key-Dropping Based Attention\n\nQ/K-dropping mechanism used. We show that naively dropping heads for each key and query at random can already yield competitive results while significantly improving the runtime. While better dropping schemes could be devised, they are outside of the scope of this work. Runtime performances in a vacuum. We test our implementation with different sparsity ratios, corresponding to the probability of dropping some head associated to a given key or query. We assume that the tensors indicating the dropping of each head of each query and key are given for free, along with some random key, query, and value tensors. To ensure a fair comparison, we take into account pre-processing and post-processing steps required to reshape the tensors for both methods, see App. B for more details. For our approach, we hope reducing the size of the key, query and value tensors and computing the attention on those would be faster than using FlashAttention over the entire sequence. For this, the time gained by computing the attention on smaller tensors should be larger than the overhead of re-ordering tensors to build those smaller tensors. In Fig. 7.a, we show a naive implementation using existing PyTorch functionalities only starts to provide a speedup when dropping more than of the keys and queries. Fig. 7.b shows that using our proposed implementation provides significant speedups even at relatively low sparsity levels. The linearly increasing cost of reshaping the tensors is rapidly compensated by large gains over the quadratic cost of self-attention. Language modeling on OpenWebText2. For sequences of length , we train transformer language models using FlashAttention (F-LM), as well as identical models replacing only the FlashAttention by our Q/K-dropping sparse attention (D-LM). We train with several sparsity ratios, dropping , , and of the heads of keys and queries at random. In Fig. 8 we observe that while high sparsity can negatively affect the perplexity, lower sparsity D-LM models are matching F-LM models in perplexity per iterations, while training nearly twice as fast. Importantly, the dropping pattern is not static. An interesting approach similar to curriculum learning in which we start the training with very large sparsity and reduce it linearly during training is studied in App. C. 5 Conclusion\n\nWe develop and validate an efficient kernel that can make sparse attention based on dynamic patterns very fast. We hope that our contribution will inspire the community to research dynamic attention patterns in a way that is less constrained by a tight computational budget. The computational cost of large attention models remains both a practical issue in scaling up to very large contexts, and a fundamental research question to close the gap between the energy usage of biological systems to that of GPU systems able to run very large models. Dynamically modulating the computation is an obvious direction to address this challenge. 6 Acknowledgments\n\nThe authors acknowledge support from the Swiss National Science Foundation under grant number CR\u2013 SII5\u2013193716 - \u201cRobust Deep Density Models for High-Energy Particle Physics and Solar Flare Analysis (RODEM)\u201d. We also thank Igor Krawczuk for interesting discussions and suggesting using Triton. References\n\nAndoni et al. (2015) Andoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I., and Schmidt, L. Practical and optimal lsh for angular distance, 2015.",
    "sfa-12": "Behnke & Heafield (2020) Behnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP (1), pp. 2664\u20132674. Association for Computational Linguistics, 2020. Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. Bender et al. (2021) Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, pp. 610\u2013623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Borgeaud et al. (2021) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/abs/2112.04426. Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "sfa-13": "CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794. Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. Gao et al. (2020) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. OpenWebText2 dataset, as part of \u2018the Pile: An 800gb dataset of diverse text for language modeling\u2018.",
    "sfa-14": "arXiv preprint arXiv:2101.00027, 2020. Goyal et al. (2020) Goyal, S., Choudhury, A.",
    "sfa-15": "R., Raje, S., Chakaravarthy, V. T., Sabharwal, Y., and Verma, A. Power-bert: Accelerating BERT inference via progressive word-vector elimination.",
    "sfa-16": "In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 3690\u20133699. PMLR, 2020. Hutter (2012) Hutter, M. The human knowledge compression contest. URL http://prize. hutter1. net, 6, 2012. Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast autoregressive transformers with linear attention.",
    "sfa-17": "In Proceedings of the International Conference on Machine Learning (ICML), pp. 5294\u20135303, 2020. URL https://fleuret.org/papers/katharopoulos-et-al-icml2020.pdf. Kim et al. (2022) Kim, S., Shen, S., Thorsley, D., Gholami, A., Kwon, W., Hassoun, J., and Keutzer, K. Learned token pruning for transformers.",
    "sfa-18": "In KDD, pp. 784\u2013794. ACM, 2022. Kitaev et al. (2020) Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In ICLR. OpenReview.net, 2020. LeCun et al. (1998) LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.",
    "sfa-19": "Proc. IEEE, 86(11):2278\u20132324, 1998. Li et al. (2021) Li, J., Cotterell, R., and Sachan, M. Differentiable subset pruning of transformer heads.",
    "sfa-20": "Trans. Assoc. Comput. Linguistics, 9:1442\u20131459, 2021. Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR (Poster). OpenReview.net, 2019. Michel et al. (2019) Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In NeurIPS, pp. 14014\u201314024, 2019. Peng et al. (2020) Peng, H., Schwartz, R., Li, D., and Smith, N. A. A mixture of h - 1 heads is better than h heads. In ACL, pp. 6566\u20136577. Association for Computational Linguistics, 2020. Peng et al. (2021) Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L. Random feature attention. In ICLR. OpenReview.net, 2021. Qin et al. (2022) Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y. cosformer: Rethinking softmax in attention. In ICLR. OpenReview.net, 2022. Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners, 2019.",
    "sfa-21": "Raganato et al. (2020) Raganato, A., Scherrer, Y., and Tiedemann, J. Fixed encoder self-attention patterns in transformer-based machine translation. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp. 556\u2013568. Association for Computational Linguistics, 2020. Tay et al. (2020) Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. CoRR, abs/2009.06732, 2020. URL https://arxiv.org/abs/2009.06732. Tay et al. (2021a) Tay, Y., Bahri, D., Metzler, D., Juan, D., Zhao, Z., and Zheng, C. Synthesizer: Rethinking self-attention for transformer models. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 10183\u201310192. PMLR, 2021a. Tay et al. (2021b) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers.",
    "sfa-22": "In ICLR. OpenReview.net, 2021b. Tillet et al. (2019) Tillet, P., Kung, H. T., and Cox, D. Triton: An intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, MAPL 2019, pp. 10\u201319, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367196. doi: 10.1145/3315508.3329973. URL https://doi.org/10.1145/3315508.3329973. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. Voita et al. (2019) Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL (1), pp. 5797\u20135808. Association for Computational Linguistics, 2019. Wang et al. (2021) Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture with cascade token and head pruning.",
    "sfa-23": "In HPCA, pp. 97\u2013110. IEEE, 2021. Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity.",
    "sfa-24": "CoRR, abs/2006.04768, 2020. Weidinger et al. (2021) Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., Isaac, W., Legassick, S., Irving, G., and Gabriel, I. Ethical and social risks of harm from language models. CoRR, abs/2112.04359, 2021. Wu et al. (2022) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers, 2022. Xia et al. (2022) Xia, M., Zhong, Z., and Chen, D. Structured pruning learns compact and accurate models. In ACL (1), pp. 1513\u20131528. Association for Computational Linguistics, 2022. Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Onta\u00f1\u00f3n, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big bird: Transformers for longer sequences. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html. Zheng et al. (2022) Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 27011\u201327041. PMLR, 2022. Zhou et al. (2020) Zhou, W., Ge, T., Wei, F., Zhou, M., and Xu, K. Scheduled drophead: A regularization method for transformer models. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp.",
    "sfa-25": "1971\u20131980. Association for Computational Linguistics, 2020. Appendix A Additional Background\n\nA.1 Structured attention\n\nWith a focus on performances on downstream applications, Raganato et al. (2020) show that it is possible to replace all but one attention head with static attentive patterns only relying on the position, e.g. attending to the previous token. Similarly, Tay et al. (2021a) investigate removing entirely the dot product attention and instead let the input token alone predict its attention pattern over the entire sequence, or use a random attention pattern. While they show their approach can be competitive on certain downstream task, results on language modeling seems to indicate those attention patterns are not expressive enough to model natural language. Peng et al. (2021) propose Random Feature Attention (RFA) which relies on kernel methods to approximate the softmax atttention and achieve a linear computational complexity w.r.t. the sequence length. A follow up work by Zheng et al. (2022) augments RFA with randomized attention to build an unbiased estimator of the softmax attention. Instead of looking for a linear approximation of the softmax, Qin et al. (2022) propose to replace the softmax by a linear projection kernel and a cos-based re-weighting mechanism which scales linearly with the sequence length. While kernel based method seem like a good compromise in terms of speed vs. performance, they have been shown to underperform on certain downstream tasks Tay et al. (2021b). Compared to those methods, we are not trying to replace or find an estimator of the softmax attention, we instead provide an efficient way to leverage different forms of sparsity of the softmax attention matrix. Our speed gains do not come from a conceptually different way to compute the attention but simply deciding not to compute certain regions of the softmax attention matrix. A.2 Pruning\n\nThe large body of works on pruning mostly focus on faster inference on downstream tasks. As a result, many methods are modifying the training process to facilitate pruning, often resulting in slower training. Pruning heads in transformers. Many works have investigated dropping entire attention heads in transformer architectures. Let the entire attention matrix be of shape with the batch size, the number of heads, and the sequence length. Dropping entire heads imposes an implicit structure over which is now of shape , with the number of non-dropped heads. Michel et al. (2019) and Voita et al. (2019) both observe\u2014in the context of Neural Machine Translation (NMT)\u2014that a large fraction of heads can be dropped without significantly impacting performance. While their goal is not primarily to speed up training, many of methods following on this insight try to incorporate a sparsifying mechanism during training which facilitate dropping heads at test time (Behnke & Heafield, 2020; Peng et al., 2020; Xia et al., 2022; Li et al., 2021). Still limited to NMT and downstream classification tasks, Zhou et al. (2020) found a regularizing effect of dropping heads during training. In comparison, less works have investigated dropping head in the context of language modeling, with text generations applications in mind. In comparison to those works, we propose a way to take advantage of dynamic sparsity structures, which is much more general but still includes head dropping. We show the potential of our work to enable speedup during training. Our work is orthogonal to head-dropping mechanisms and could be used in addition to those. Pruning tokens in transformers. While dropping heads can be seen as dropping the model parameters generating keys, queries and values for those heads, another more recent line of work looked into dropping tokens. Dropping entire tokens imposes an implicit structure over the attention matrix which is now of shape , with and the number remaining queries and keys. Goyal et al. (2020) obtain faster inference on downstream tasks by using an attention based scoring mechanism to eliminate redundant input vectors. Wang et al. (2021) develop a joint algorithm-architecture framework which speeds up inference for downstream classifications tasks and language generation tasks. Their method includes head and token pruning along with specific hardware optimizations. Our work can be used to implement those approaches and does not require custom hardware accelerators. We moreover allow dropping individual heads instead of entire tokens. Appendix B Details On The Implementation\n\nWe here give implementation details for experiments of \u00a7 4. First, in App. B.1, we describe in more details the implementations of our custom Triton kernels introduced in \u00a7 3. Secondly in App. B.2 we provide the python code used in our runtime benchmarks of \u00a7 4, including pre and post processing steps required to reshape and re-order tensors. Lastly in \u00a7 B.3 we give more details on the hyperparameters used in our sequence modeling experiments of \u00a7 4. The code for all our experiments can be found via the following link: https://github.com/epfml/dynamic-sparse-flash-attention. B.1 Triton Kernels\n\nQK-Sparse Triton Kernel. In Alg. 1 we detail the core of the QK-sparse algorithm from \u00a7 3.1. This algorithm is computing the softmax attention result only for one block of queries, corresponding to one head. In practice this algorithm would be run in parallel for all blocks of queries for all heads. We find the index of the last relevant tile to compute by iterating over block of key indices and comparing them with the largest query index for the current block of queries. This works as and have a monotonic structure thanks to the stable sort used when reshaping the tensors (see the pre-processing code in App. B.2 for more details). We apply causal masking locally by looking at and , q query can only attend to past keys: mask = q_idx[:, None] >= k_idx[None, :] . The backward pass relies exactly on the same trick, we first iterate over query indices to find the starting block of queries. Hash-Sparse Triton Kernel. In Alg. 2 we detail the core of the Hash-sparse algorithm from \u00a7 3.2. This algorithm is computing the softmax attention result only for one block of queries, corresponding to one head. In practice this algorithm would be run in parallel for all blocks of queries for all heads. We find the index of the first and last relevant tiles to compute by iterating over block of key hashes and comparing them with the largest and smallest query hashes for the current block of queries. This works as we reshaped our tensors by sorting them by their hash values (see the pre-processing code in App. B.2 for more details). In addition to causal masking, we also enforce attention to happen within the same bucket: mask = (q_idx[:, None] >= k_idx[None, :]) & (q_hash[:, None] == k_hash[None, :]). In our experiments we often replace >= by > to prevent a query to attend to itself as in the Reformer. As a side note, for most application it would also be fine to only enforce causal masking and allow attention across buckets within a tile. While this could add some serendipity in the attention computation, some applications might require masking based on hash. The backward pass relies exactly on the same trick, we first iterate over query indices to find the starting and end blocks of queries. Accumulating softmax statistics while avoiding NaNs. The following is a brief summary of how the FlashAttention algorithm (Dao et al., 2022) proposes to accumulate softmax statistics when iterating over blocks of keys. Given an input vector , our goal is to compute . Let , and . Hence:\n\nsoftmax \u200b ( \ud835\udc99 ) = f \u200b ( \ud835\udc99 , max \u2061 ( \ud835\udc99 ) ) \u2113 \u200b ( \ud835\udc99 , max \u2061 ( \ud835\udc99 ) ) softmax \ud835\udc99 \ud835\udc53 \ud835\udc99 \ud835\udc99 \u2113 \ud835\udc99 \ud835\udc99 \\text{softmax}({\\bm{x}})=\\frac{f({\\bm{x}},\\max({\\bm{x}}))}{\\ell({\\bm{x}},\\max({\\bm{x}}))}\n\nGiven a vector , let (global max), and , we notice:\n\nsoftmax \u200b ( x ) softmax \ud835\udc65 \\displaystyle\\text{softmax}(x) = f \u200b ( \ud835\udc99 , m g ) \u2113 \u200b ( \ud835\udc99 , m g ) absent \ud835\udc53 \ud835\udc99 subscript \ud835\udc5a \ud835\udc54 \u2113 \ud835\udc99 subscript \ud835\udc5a \ud835\udc54 \\displaystyle=\\frac{f({\\bm{x}},m_{g})}{\\ell({\\bm{x}},m_{g})} = f \u200b ( \ud835\udc99 1 , m 1 ) e m 1 \u2212 m g \u200b \u2113 \u200b ( \ud835\udc99 1 , m 1 ) + \u2113 \u200b ( \ud835\udc99 2 , m g ) + f \u200b ( \ud835\udc99 2 , m g ) e m 1 \u2212 m g \u200b \u2113 \u200b ( \ud835\udc99 1 , m 1 ) + \u2113 \u200b ( \ud835\udc99 2 , m g ) absent \ud835\udc53 subscript \ud835\udc99 1 subscript \ud835\udc5a 1 superscript \ud835\udc52 subscript \ud835\udc5a 1 subscript \ud835\udc5a \ud835\udc54 \u2113 subscript \ud835\udc99 1 subscript \ud835\udc5a 1 \u2113 subscript \ud835\udc99 2 subscript \ud835\udc5a \ud835\udc54 \ud835\udc53 subscript \ud835\udc99 2 subscript \ud835\udc5a \ud835\udc54 superscript \ud835\udc52 subscript \ud835\udc5a 1 subscript \ud835\udc5a \ud835\udc54 \u2113 subscript \ud835\udc99 1 subscript \ud835\udc5a 1 \u2113 subscript \ud835\udc99 2 subscript \ud835\udc5a \ud835\udc54 \\displaystyle=\\frac{f({\\bm{x}}_{1},m_{1})}{e^{m_{1}-m_{g}}\\ell({\\bm{x}}_{1},m_{1})+\\ell({\\bm{x}}_{2},m_{g})}+\\frac{f({\\bm{x}}_{2},m_{g})}{e^{m_{1}-m_{g}}\\ell({\\bm{x}}_{1},m_{1})+\\ell({\\bm{x}}_{2},m_{g})}\n\nTherefore, if we have , , and , we can update the softmax statistics for a new block of entries by following the following steps:\n\n1.",
    "sfa-26": "Compute new global max:\n\n2.",
    "sfa-27": "Compute\n\n3. Compute\n\n4. Compute new :\n\n5. Correct running softmax result:\n\n6. Add contribution from to :\n\nIn case all the keys are masked for a given query, we would have , given that the first is initialized to as well (see Alg.",
    "sfa-28": "1 and Alg. 2) the fourth and second steps above would be undefined and result in NaN values. We solve the problem by replacing values in by s when doing those two steps. Another potential issue is in step three: when a query has no matching key in then is now , which generate in step five. This is an issue as we process keys by blocks, and if there are no keys for a query in the current block, we might find matching keys in following blocks. Adding values to would prevent us to accumulate statistics later on. To get our desired behaviour and have s when queries have no matching key we replace values by s in during steps five and six.",
    "sfa-29": "We summarize those steps in Alg.",
    "sfa-30": "3. Hyperparameters. We extend the implementation of FlashAttention available in the Triton tutorial. In our benchmarks, we use a batch size , heads of dimensions each. B.2 Runtimes in a Vacuum\n\nBaseline implementation. We use Pytorch\u2019s FlashAttention implementation provided by the torch.nn.functional.scaled_dot_product_attention function. To ensure fairness, we assume that all benchmarked functions receive a tensor of shape (BATCH, CTX_Q, H, D_HEAD), and tensors of shapes (BATCH, CTX_KV, H, D_HEAD), where BATCH is the batch size, CTX_Q is the number of queries, CTX_KV is the number of keys and values, H is the number of heads, and D_HEAD is number of dimensions per head. For this reason the only pre and post processing steps required are transposing the input and output tensors. Our proposed interface. We propose the following interface to orchestrate between the Hash-sparse and the QK-sparse implementations:\n\nPre & post processing steps for Hash-sparse. In addition to having to transpose the tensors. The preprocessing steps consist in re-ordering the , and tensors based on bucket indices in q_hash and k_hash. We keep track of the original position of the queries, keys and values by storing the indices given by the sorting operations. Importantly, we use stable sorts to ensure the queries, keys and values are sorted within each bucket. The following code is showing how we implemented all those steps using Pytorch:\n\nPre & post processing steps for QK-sparse. In addition to having to transpose the tensors. The preprocessing steps consist in removing dropped keys, values and queries from , and . The sorting operations need to be stable to keep the original time ordering within the remaining keys and queries. Moreover, the index tensor has to be padded so our kernel can rely on those indices to compute which tile it should and shouldn\u2019t compute. B.3 Sequence Modeling Experiments\n\nLanguage modeling on OpenWebText2. Our implementation is based on NanoGPT (github.com/karpathy/nanoGPT). We use the AdamW optimizer (Loshchilov & Hutter, 2019). We used bfloat16 and NVIDIA A100-40GB GPUs for all our experiments. Here is a list of hyperparameters shared by all our language models (F-LM, H-LM, and D-LM):\n\n\u2022\n\nWeight-decay:\n\n\u2022\n\nDepth (number of transformer blocks):\n\n\u2022\n\nNumber of heads:\n\n\u2022\n\nDropout:\n\n\u2022\n\nLearning rate:\n\n\u2022\n\nPercentage of iterations for warmup: . We use a cosine learning rate scheduler. \u2022\n\nAdam beta1:\n\n\u2022\n\nAdam beta2:\n\n\u2022\n\nTokenizer: We use the GPT2 tokenizer provided by the tiktoken library (github.com/openai/tiktoken). \u2022\n\nHidden dimensions:\n\n\u2022\n\nDimensions per head:\n\nSequential MNIST and enwik8. For the comparisons with Reformer, we use a standard GPT2 (Radford et al., 2019) implementation. For the language modeling on enwik8, the Transformer has 12 blocks with 768 hidden dimensions, 8 attention heads, and 64 dimensions per head. Dropout is set to and the batch size is with gradient accumulation steps. The sequence length is . For autoregressive image generation on MNIST, we use a smaller model with 8 transformer blocks and a hidden dimension of .",
    "sfa-31": "Dropout is set to and the batch size is . We train for 25 epochs. Appendix C Additional Details and Analysis\n\nQuadratic computational cost of attention in transformers. In Fig. 9 we show the runtime (forward + backward) of a transformer language model as a function of the sequence length. We separate the time taken by the attention operations from the time taken by the rest of the model. We see how the attention computation ends up dominating the runtime as the sequence length increases. Additional runtimes performances in a vacuum. In Fig. 10 and Fig. 11 we show the runtime details for the forward and backward methods separately for respectively the Hash-sparse and QK-sparse methods. We also measure runtimes of the forward and backward passes when we assume the pre and post-processing steps aree free, see Fig. 12. Linear QK-dropping scheduler. In the main paper we show results dropping keys and queries at random with a fixed pre-defined probability. In an additional experiment we start by dropping of keys and queries at random and linearly decay this probability to . Our intuition is earlier iterations just aim to learn contextual cues which are very redundant (and therefore quite immune to random dropping) before requiring more fine-grained representations.",
    "sfa-32": "In Fig. 13 we show and analyse the results of that experiment. H-LM models speeding up during training. The speed of Hash-sparse attention is conditioned on the distribution of bucket indices over keys and queries\u2014e.g. if all the keys and queries were to fall in the same bucket then there would be no speedup over FlashAttention. Interestingly, when training on real data such as OpenWebText2, we observe our Hash-sparse based models are speeding up during training. In Fig. 14 we plot the number of iterations reached after hours of training (normalizing the time by the number of GPUs used for training). In Fig. 14.(a) we see a speedup for H-LM models early in the training. Different bucket sizes for H-LM. For sequences of tokens, we show the influence of increasing the number of buckets in Fig. 15. As increases the runtime decreases. Training H-LMs for more iterations. In Fig. 8 and Fig. 6 we show results of language models trained on OpenWebText2 for k iterations. To verify whether our finding are consistent when you train for more iterations we also try training for k iterations. For sequences of tokens, using the same hyperparameters described in App. B.3, we show in Fig. 16 that our findings for our Hash-sparse based models do hold when training for more iterations\u2014we match the perplexity per iterations of the baseline model using FlashAttention over the entire sequence while being significantly faster. Visualizing the low coverage of Reformer. In Fig. 17 we show the attention matrices for two different heads and show how the coverage\u2014the percentage of key-query interactions actually computed vs. what should be computed according to the hash indices computed for keys and queries\u2014of the Reformer LSH algorithm can be low. Appendix D Limitations and Societal Impacts\n\nLimitations. The aim of our work is to develop a method for efficiently computing attention with several sparsity structures. We don\u2019t focus on developing the best method for sparsification, although, for example, we improve the hashing-based mechanism. Moreover, as already explained, on very small sequences we incur in some constant overhead which limits our gains. Societal impacts. The attention mechanism is central to Large Language Models (LLMs). Moreover, efficient attention mechanisms can make these models more powerful, by making them faster to train and by increasing their context length. The social impact and risks associated with our work, therefore, are included in the risks associated with the deployment of such systems (Bender et al., 2021; Weidinger et al., 2021). \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Feb 29 02:11:49 2024 by LaTeXML"
}