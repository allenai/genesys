{
    "ssdlm-0": "Ssd-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control\n\nXiaochuang Han\u2660 Sachin Kumar\u2663 Yulia Tsvetkov\u2660 \u2660Paul G. Allen School of Computer Science & Engineering, University of Washington \u2663Language Technologies Institute, Carnegie Mellon University {xhan77, yuliats}@cs.washington.edu\u2660 sachink@cs.cmu.edu\u2663\n\nAbstract\n\nDespite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present Ssd-LM\u2014a diffusion-based language model with two key design choices. First, Ssd-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate Ssd-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, Ssd-LM also outperforms competitive baselines, with an extra advantage in modularity.111 Our code and models can be found at https://github.com/xhan77/ssd-lm. 1 Introduction\n\nDiffusion models (Sohl-Dickstein et al., 2015), trained to iteratively refine noised inputs, have recently emerged as powerful tools for generative modeling in several continuous-valued domains such as images (Ho et al., 2020), audio (Kong et al., 2021), video (Ho et al., 2022), among others. Attempts to adapt them for discrete domains such as text data, however, have only had limited success: prior work have shown to be promising on specialized cases and small datasets (Hoogeboom et al., 2021; Austin et al., 2021; Li et al., 2022; Chen et al., 2022), but diffusion models for text still underperform (and thus are not widely adopted) compared to autoregressive language models (AR-LMs) which remain the state-of-the-art general purpose text generators (Radford et al., 2019; Brown et al., 2020). Despite potential advantages of diffusion models for text, there are two key challenges. First, diffusion models generate text non-autoregressively, i.e., they generate (and update) the entire sequence simultaneously rather than token by token left-to-right. Although this property is useful in practice since each output token is informed by a broader bi-directional context (Lee et al., 2018; Ghazvininejad et al., 2019), it requires pre-defining an output sequence length. This limits the flexibility and applicability of trained models. On the other hand, non-autoregressive training with long sequences is expensive and difficult to optimize. In this work, we propose a semi-autoregressive solution which strikes a balance between length flexibility and the ability to alter previously generated tokens. A major advantage of diffusion models over the current standard of autoregressive LMs is their post-hoc controllability using guidance from auxiliary models such as style classifiers (Dhariwal and Nichol, 2021). However, controllability is hard to achieve without compromises in modularity in diffusion-based LMs for text. To enable diffusion generation into discrete text rather than continuous modalities, prior approaches have employed different approximations, e.g., training with embeddings, character, or byte-level methods (Li et al., 2022; Hoogeboom et al., 2021; Austin et al., 2021; Chen et al., 2022). In contrast, existing mainstream LMs and the guidance classifiers they derive often operate at a sub-word level with sub-word representations trained jointly with the language model (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020). Subsequently, changing the input representations to characters or embeddings requires developing guidance models from scratch, which can be expensive or infeasible in many cases. In this work, we propose a simplex-based solution which enables the diffusion over discrete texts while maintaining the advantages of diffusion models with plug-and-control guidance models. In sum, to enable diffusion-based LMs for text we present Ssd-LM (\u00a73), addressing the above two challenges. Ssd-LM is trained to generate text semi-autoregressively\u2014generating blocks of tokens left-to-right with bidirectional context within the block\u2014which offers the benefits of both AR-LMs and diffusion models. It supports training with and generating variable-length sequences. At the same time, it allows refinement within the token block, in contrast to token-level autoregressive decoding where previously generated tokens cannot be modified at all. Ssd-LM uses the same tokenization as popular AR-LMs, representing discrete text via a distribution (or simplex) defined over the vocabulary and is trained to reconstruct texts from noisy versions of the distributions. Due to its underlying representation, our method also offers an easy and modular way of guided (controlled) generation using off-the-shelf text classifiers under the minimal assumption of shared tokenizer. Our evaluation experiments show, for the first time, that a diffusion-based LM matches or outperforms strong AR-LMs on standard text generation benchmarks (\u00a74). We evaluate Ssd-LM on two tasks: (1) unconstrained prompt-based generation substantially outperforming existing diffusion LM approaches and performing on par with or outperforming strong autoregressive LM GPT-2 (Radford et al., 2019) on both quality and diversity (\u00a74.2); and (2) controlled text generation with guidance from off-the-shelf classifiers (no post-hoc training/adaptation) outperforming competitive controlled text generation baselines (\u00a74.3). 2 Background\n\n2.1 Diffusion model\n\nSince their inception as image generators, diffusion models (and their cousins score-based models (Song and Ermon, 2019)) have been widely adopted as high-quality generative models for multiple data modalities. Here, we briefly describe a simplified view of a canonical method, denoising diffusion probabilistic models (Ho et al., 2020, DDPM) which we adapt in this work for text generation. We assume a given dataset of continuous valued items (e.g., pixel values of an image) henceforth referred to as for simplicity. Training\n\nTraining a diffusion model first involves adding a series of Gaussian noise to the original data , through timesteps:\n\n\ud835\udc99 t subscript \ud835\udc99 \ud835\udc61 \\displaystyle\\boldsymbol{x}_{t} = \u03b1 \u00af t \u200b \ud835\udc99 0 + 1 \u2212 \u03b1 \u00af t \u200b \u03f5 t absent subscript \u00af \ud835\udefc \ud835\udc61 subscript \ud835\udc99 0 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript bold-italic-\u03f5 \ud835\udc61 \\displaystyle=\\sqrt{\\bar{\\alpha}_{t}}\\boldsymbol{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\boldsymbol{\\epsilon}_{t} (1)\n\nwhere and . , where follow a predefined schedule such that as . This process is called forward diffusion. A diffusion model (parameterized by ) is trained to reverse this forward process by predicting the added noise given with the following loss:\n\n\u2112 \u200b ( \u03b8 ) = \ud835\udd3c t \u223c \ud835\udcb0 \u200b ( 1 , T ) \u200b \u2225 \u03f5 \u03b8 \u200b ( \ud835\udc99 t , t ) \u2212 \u03f5 t \u2225 2 \u2112 \ud835\udf03 subscript \ud835\udd3c similar-to \ud835\udc61 \ud835\udcb0 1 \ud835\udc47 superscript delimited-\u2225\u2225 subscript italic-\u03f5 \ud835\udf03 subscript \ud835\udc99 \ud835\udc61 \ud835\udc61 subscript bold-italic-\u03f5 \ud835\udc61 2 \\displaystyle\\mathcal{L}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}(1,T)}\\lVert\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)-\\boldsymbol{\\epsilon}_{t}\\rVert^{2} (2)\n\nInference\n\nTo get an output from this model, we sample and iteratively reconstruct a sample by going back in time,\n\n\ud835\udc99 t \u2212 1 subscript \ud835\udc99 \ud835\udc61 1 \\displaystyle\\boldsymbol{x}_{t-1} = 1 \u03b1 t \u200b ( \ud835\udc99 t \u2212 1 \u2212 \u03b1 t 1 \u2212 \u03b1 \u00af t \u200b \u03f5 \u03b8 \u200b ( \ud835\udc99 t , t ) ) absent 1 subscript \ud835\udefc \ud835\udc61 subscript \ud835\udc99 \ud835\udc61 1 subscript \ud835\udefc \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript italic-\u03f5 \ud835\udf03 subscript \ud835\udc99 \ud835\udc61 \ud835\udc61 \\displaystyle=\\frac{1}{\\sqrt{\\alpha_{t}}}(\\boldsymbol{x}_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)) (3)\n\nfor .222We omit an additional noise term here for simplicity, which is present in DDPM but not in another variant DDIM Song et al.",
    "ssdlm-1": "(2021). The key obstacle in using vanilla diffusion models directly as text generators is that language consists of discrete tokens, i.e., a non-continuous to which a continuous valued Gaussian noise cannot be added. We propose a straightforward and effective solution by treating tokens as continuous valued simplexes over the vocabulary (Hoang et al., 2017). Other existing methods addressing this problem are discussed in \u00a75. 2.2 Autoregressive LM\n\nAn autoregressive LM model optimizes for the likelihood of a sequence of tokens . p \u03b8 \u200b ( \ud835\udc98 0 : L ) = \u220f c = 0 L \u2212 1 p \u03b8 \u200b ( w c \u2223 \ud835\udc98 < c ) subscript \ud835\udc5d \ud835\udf03 superscript \ud835\udc98 : 0 \ud835\udc3f superscript subscript product \ud835\udc50 0 \ud835\udc3f 1 subscript \ud835\udc5d \ud835\udf03 conditional superscript \ud835\udc64 \ud835\udc50 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle p_{\\theta}(\\boldsymbol{w}^{0:L})=\\prod_{c=0}^{L-1}p_{\\theta}(w^{c}\\mid\\boldsymbol{w}^{<c}) (4)\n\nTo decode from AR-LMs, one can provide a context and decode the next token iteratively by predicting and sampling from it to get the discrete token (Fan et al., 2018; Holtzman et al., 2020). Prior work has shown that these decoding approaches (and by extension the LMs themselves) are prone to degrade when generating long sequences and often devolve into repeating subsequences (Holtzman et al., 2020; Meister et al., 2022). In addition, such LMs do not provide a natural way to incorporate sequence-level control as tokens are generated one at a time without the ability to modify previously generated tokens (Dathathri et al., 2020; Kumar et al., 2022b). In this work, we present a method to train a semi-autoregressive LM that decodes blocks of tokens at a time, alleviating said issues with the support of diffusion models. Existing literature addressing the two issues individually are discussed in \u00a75. 3 Ssd-LM\n\nWe introduce Ssd-LM\u2014Semi-autoregressive Simplex-based Diffusion Language Model\u2014 adapting key components from both autoregressive LM and vanilla diffusion models. Conceptually, Ssd-LM uses diffusion model to decode , a block of tokens of length , given a Gaussian noise and a context of length . We show an intuitive diagram and pseudo-code for the training and decoding algorithm of Ssd-LM in Figure 1, Figure 2, and Figure 3. 3.1 Training\n\nContinuous data representation\n\nTo build a continuous representation for discrete tokens, we adopt an almost-one-hot simplex representation over the model\u2019s vocabulary . We define a simple operation to map a token to as follows. w ~ ( i ) = { + K \u200b when \u200b w = V ( i ) \u2212 K \u200b when \u200b w \u2260 V ( i ) subscript ~ \ud835\udc64 \ud835\udc56 cases \ud835\udc3e when \ud835\udc64 subscript \ud835\udc49 \ud835\udc56 otherwise \ud835\udc3e when \ud835\udc64 subscript \ud835\udc49 \ud835\udc56 otherwise \\displaystyle\\tilde{w}_{(i)}=\\begin{cases}+K\\text{ when }w=V_{(i)}\\\\\n-K\\text{ when }w\\neq V_{(i)}\\end{cases} (5)\n\nwhere is the index of the vocabulary. We call the logits for token , and gives a probability simplex over the vocabulary , with a probability mass concentrated on the token . There is no learnable parameter in this mapping. Forward diffusion\n\nFollowing Ho et al. (2020), we add a time-dependent Gaussian noise to the logits. \ud835\udc98 ~ 0 c : c + B superscript subscript ~ \ud835\udc98 0 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \\displaystyle\\tilde{\\boldsymbol{w}}_{0}^{c:c+B} = logits \u2212 generation \u2061 ( \ud835\udc98 c : c + B ) absent logits generation superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \\displaystyle=\\operatorname{logits-generation}(\\boldsymbol{w}^{c:c+B}) (6) \ud835\udc98 ~ t c : c + B superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \\displaystyle\\tilde{\\boldsymbol{w}}_{t}^{c:c+B} = \u03b1 \u00af t \u200b \ud835\udc98 ~ 0 c : c + B + 1 \u2212 \u03b1 \u00af t \u200b \u03f5 t absent subscript \u00af \ud835\udefc \ud835\udc61 superscript subscript ~ \ud835\udc98 0 : \ud835\udc50 \ud835\udc50 \ud835\udc35 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript bold-italic-\u03f5 \ud835\udc61 \\displaystyle=\\sqrt{\\bar{\\alpha}_{t}}\\tilde{\\boldsymbol{w}}_{0}^{c:c+B}+\\sqrt{1-\\bar{\\alpha}_{t}}\\boldsymbol{\\epsilon}_{t} (7)\n\nwhere , , and as . At the final step , are fully noisy simplexes over , with a logit-normal distribution (Atchison and Shen, 1980). Loss function\n\nIn Equation 2, a diffusion model is trained to predict the added noise from the noisy representations. Since the forward diffusion process can be computed in a single step (Equation 1), the notion here is equivalent to predicting the original data representation (Song et al., 2021; Li et al., 2022). Our objective follows the same intuition but estimates a likelihood instead of the L2 distance while conditioning on additional context:333L2 distance did not work in our pilot study potentially due to the intrinsically skewed simplex representation. \u2112 \u200b ( \u03b8 ) \u2112 \ud835\udf03 \\displaystyle\\mathcal{L}(\\theta) = \ud835\udd3c \u200b [ \u2212 log \u2061 p \u03b8 \u200b ( \ud835\udc98 c : c + B \u2223 \ud835\udc98 ~ t c : c + B , \ud835\udc98 < c ) ] absent \ud835\udd3c delimited-[] subscript \ud835\udc5d \ud835\udf03 conditional superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle=\\mathbb{E}[-\\log p_{\\theta}(\\boldsymbol{w}^{c:c+B}\\mid\\tilde{\\boldsymbol{w}}_{t}^{c:c+B},\\boldsymbol{w}^{<c})] (8) = \ud835\udd3c \u200b [ \u2211 j = c c + B \u2212 1 \u2212 log \u2061 p \u03b8 \u200b ( w j \u2223 \ud835\udc98 ~ t c : c + B , \ud835\udc98 < c ) ] absent \ud835\udd3c delimited-[] superscript subscript \ud835\udc57 \ud835\udc50 \ud835\udc50 \ud835\udc35 1 subscript \ud835\udc5d \ud835\udf03 conditional superscript \ud835\udc64 \ud835\udc57 superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle=\\mathbb{E}\\left[\\sum_{j=c}^{c+B-1}-\\log p_{\\theta}(w^{j}\\mid\\tilde{\\boldsymbol{w}}_{t}^{c:c+B},\\boldsymbol{w}^{<c})\\right] (9)\n\nis a shorthand for . The architecture for throughout this work is a bi-directional Transformer encoder (Vaswani et al., 2017). Specifically, the input to the model is a concatenation of the context and a sequence of noisy vocabulary simplexes of length . The target output is the original tokens at positions to . One minimal modification made to the Transformer model is that in addition to the conventional embedding lookup for , we modify the embedding layer to take as input a distribution over the vocabulary, , and compute the embedding vector as a weighted sum of the embedding table. A timestep embedding is also added before the first Transformer block to inform the model of the current timestep.444More specifically, we have word embeddings for the context, , and for the noisy diffusion representations, . The timestep embedding is added to the diffusion word embeddings, .",
    "ssdlm-2": "It is similar to positional embeddings, just not varying across sequence positions. We fold it in for notation simplicity. In \u00a7A, we present another interpretation of the training objective as an intuitive contrastive loss. 3.2 Decoding\n\nLogits projection\n\nSimilar to continuous-valued diffusion models, sampling from Ssd-LM involves reverse diffusion from starting with a Gaussian noise. At any timestep , our model takes as input noised logits and estimates the probability distribution of the original tokens in data by first predicting the logits:\n\n\ud835\udc98 logits , t c : c + B = logits \u03b8 \u2061 ( \ud835\udc98 c : c + B \u2223 \ud835\udc98 ~ t c : c + B , \ud835\udc98 < c ) superscript subscript \ud835\udc98 logits \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 subscript logits \ud835\udf03 conditional superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle\\boldsymbol{w}_{\\text{logits},t}^{c:c+B}=\\operatorname{logits}_{\\theta}(\\boldsymbol{w}^{c:c+B}\\mid\\tilde{\\boldsymbol{w}}_{t}^{c:c+B},\\boldsymbol{w}^{<c}) (10)\n\nwhich are then converted to a distribution via softmax. To feed this output to the next step of reverse diffusion, , we define a operation to build a predicted data representation close to the initial data representation (almost-one-hot mapping; Eq. 5). We consider three projection operations. \u2022\n\nGreedy: creates an almost-one-hot logit centered at the highest probability token.555This shares a similar intuition as a greedy clamping trick in the embedding-based diffusion in Li et al. (2022). w ^ ( i ) \u200b = \u200b { + K \u200b if i = \u200b argmax \u2061 ( \ud835\udc98 logits ) \u2212 K \u200b otherwise subscript ^ \ud835\udc64 \ud835\udc56 = cases \ud835\udc3e if i = argmax subscript \ud835\udc98 logits otherwise \ud835\udc3e otherwise otherwise \\displaystyle\\hat{w}_{(i)}\\text{=}\\begin{cases}+K\\text{ if $i$=}\\operatorname{argmax}(\\boldsymbol{w}_{\\text{logits}})\\\\\n-K\\text{ otherwise}\\end{cases} (11)\n\n\u2022\n\nSampling: creates an almost-one-hot logit centered around a token sampled from the output distribution using top- sampling (Holtzman et al., 2020). is a hyperparameter. w ^ ( i ) \u200b = \u200b { + K \u200b if i = top- \u200b p \u200b -sample \u200b ( \ud835\udc98 logits ) \u2212 K \u200b otherwise subscript ^ \ud835\udc64 \ud835\udc56 = cases \ud835\udc3e if i = top- \ud835\udc5d -sample subscript \ud835\udc98 logits otherwise \ud835\udc3e otherwise otherwise \\displaystyle\\hat{w}_{(i)}\\text{=}\\begin{cases}+K\\text{ if $i$=}\\text{top-}p\\text{-sample}(\\boldsymbol{w}_{\\text{logits}})\\\\\n-K\\text{ otherwise}\\end{cases} (12)\n\n\u2022\n\nMulti-hot: creates an almost-one-hot logit centered around all tokens in the top- nucleus. w ^ ( i ) \u200b = \u200b { + K \u200b if i \u2208 top- \u200b p \u200b -all \u200b ( \ud835\udc98 logits ) \u2212 K \u200b otherwise subscript ^ \ud835\udc64 \ud835\udc56 = cases \ud835\udc3e if i \u2208 top- \ud835\udc5d -all subscript \ud835\udc98 logits otherwise \ud835\udc3e otherwise otherwise \\displaystyle\\hat{w}_{(i)}\\text{=}\\begin{cases}+K\\text{ if $i\\in$ }\\text{top-}p\\text{-all}(\\boldsymbol{w}_{\\text{logits}})\\\\\n-K\\text{ otherwise}\\end{cases} (13)\n\nDecoding iteration\n\nStarting from pure noise , in each decoding timestep we compute:\n\n\ud835\udc98 ^ t c : c + B = logits \u2212 projection \u2061 ( \ud835\udc98 logits , t c : c + B ) subscript superscript ^ \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \ud835\udc61 logits projection superscript subscript \ud835\udc98 logits \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \\displaystyle\\hat{\\boldsymbol{w}}^{c:c+B}_{t}=\\operatorname{logits-projection}(\\boldsymbol{w}_{\\text{logits},t}^{c:c+B}) (14) \ud835\udc98 ~ t \u2212 1 c : c + B = \u03b1 \u00af t \u2212 1 \u200b \ud835\udc98 ^ t c : c + B + 1 \u2212 \u03b1 \u00af t \u2212 1 \u200b \ud835\udc9b superscript subscript ~ \ud835\udc98 \ud835\udc61 1 : \ud835\udc50 \ud835\udc50 \ud835\udc35 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript superscript ^ \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 1 \ud835\udc9b \\displaystyle\\tilde{\\boldsymbol{w}}_{t-1}^{c:c+B}=\\sqrt{\\bar{\\alpha}_{t-1}}\\hat{\\boldsymbol{w}}^{c:c+B}_{t}+\\sqrt{1-\\bar{\\alpha}_{t-1}}\\boldsymbol{z} (15)\n\nfor and . At , the final -token block is computed simply as . To generate the next block, we concatenate the generated block to the previous context to create a new context of length and follow the reverse-diffusion process again as described above. This process can be repeated until the maximum desired length is reached.666Alternatively, one can also terminate the process if certain special end-of-sequence tokens have been generated. It is worth noting that our proposed decoding algorithm is novel and different from the DDPM decoding (Equation 3). The DDPM decoding is designed for diffusion in a continuous space and failed to generate sensible outputs in our preliminary experiments based on simplexes. In \u00a7B, we draw a theoretical connection between our decoding algorithm and DDPM decoding, and also highlight the intuitive difference between the two. Highly-modular control\n\nA useful property of continuous diffusion models that naturally arises from their definition is the ability to guide the generated samples to have user-defined attributes at test time. This can be done using gradients from auxiliary models such as classifiers (Dhariwal and Nichol, 2021), e.g., guiding the output of an LM to be of a positive sentiment using a sentiment classifier. There is a vibrant community of developers on platforms such as HuggingFace where many such text classifiers are publicly available. The underlying data representation of Ssd-LM is based on vocabulary simplexes. Hence, as long as a classifier shares the same tokenizer as the LM, it can be used for control in an off-the-shelf manner without modifications. This is in contrast to prior work in diffusion language models that do not support such classifiers due to differences in their input representation space (Hoogeboom et al., 2021; Austin et al., 2021; Li et al., 2022; Chen et al., 2022) and require retraining the classifiers from scratch. This ability makes Ssd-LM highly modular for controlled text generation and offers key benefits: (1) Training accurate classifiers for many tasks requires huge amounts of data where retraining them can be quite expensive, and (2) this approach allows control from classifiers that are open to use but have been trained on closed source data. To guide Ssd-LM to generate texts with a target attribute via a standalone attribute model , we update (Eq. 10) at each timestep to the form below, drifting according to the gradients from the attribute classifier. \ud835\udc98 logits , t c : c + B + \u03bb \u200b \u2207 \ud835\udc98 logits , t c : c + B f \u03d5 \u200b ( y \u2223 \ud835\udc98 logits , t c : c + B , \ud835\udc98 < c ) superscript subscript \ud835\udc98 logits \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 \ud835\udf06 subscript \u2207 superscript subscript \ud835\udc98 logits \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 subscript \ud835\udc53 italic-\u03d5 conditional \ud835\udc66 superscript subscript \ud835\udc98 logits \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle\\boldsymbol{w}_{\\text{logits},t}^{c:c+B}+\\lambda\\nabla_{\\boldsymbol{w}_{\\text{logits},t}^{c:c+B}}f_{\\phi}(y\\mid\\boldsymbol{w}_{\\text{logits},t}^{c:c+B},\\boldsymbol{w}^{<c}) (16)\n\nwhere is a hyperparameter balancing the weight of control.",
    "ssdlm-3": "The parameters of the standalone attribute model are frozen. We make a trivial modification to the embedding computation as in \u00a73.1, to allow the classifier to take as input a simplex. 3.3 Additional details\n\nForward diffusion coefficient\n\nWe follow Nichol and Dhariwal (2021) for a cosine schedule of :\n\n\u03b1 \u00af t = r \u200b ( t ) r \u200b ( 0 ) , r ( t ) = cos ( t / T + s 1 + s \u22c5 \u03c0 2 ) 2 \\displaystyle\\bar{\\alpha}_{t}=\\frac{r(t)}{r(0)},~{}r(t)=\\cos(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2})^{2} (17)\n\nwhere is small offset set to 1e-4 in our work and . Fewer timesteps in decoding\n\nDecoding from diffusion models requires a series of timesteps () which can be computationally expensive if is large. Following Li et al. (2022), we consider using a smaller value of at test time to improve decoding speed. In this work, we primarily experiment with and . Flexible decoding block size\n\nOur Ssd-LM is trained with a fixed token block size . However, the decoding algorithm has a freedom to use a different .",
    "ssdlm-4": "In our experiments, we consider both scenarios of and . Nevertheless, we leave for future work a more detailed analysis of the impact of the difference between and on model performance. 4 Experiments\n\n4.1 Ssd-LM pretraining setup\n\nModel architecture\n\nWe use a bidirectional Transformer encoder RoBERTa-large (Liu et al., 2019) (0.4B, comparable size to GPT2-medium) as Ssd-LM\u2019s underlying architecture.777We initialize the model with RoBERTa\u2019s weights as well.",
    "ssdlm-5": "We observe in our initial exploration that it helps the training loss converge faster than a randomly initialized model. However, given enough computational resources, we conjecture that a randomly initialized model will offer similar performance. Note that RoBERTa uses a general BPE tokenization (Sennrich et al., 2016), same as a variety of LMs such as GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), etc. Any attribute classifier using the same tokenization strategy can be used to control Ssd-LM in a highly modular way. Pretraining data, constants, and resource\n\nWe train Ssd-LM on the same data as GPT2 to make fair comparisons possible: OpenWebText (Gokaslan and Cohen, 2019) which contains 9B tokens.",
    "ssdlm-6": "Following Zhang et al. (2022), we consider this data as one contiguous sequence of tokens and break it into sequences of length 200 (same as the maximum sequence length our model accepts). We randomly sample 99% of these sequences for pretraining while leaving the rest as held out for evaluation. We use the following model hyperparameters:888Future work can do a search given more resources. L = 200 , B train = 25 , T train = 5000 , K = 5 formulae-sequence \ud835\udc3f 200 formulae-sequence subscript \ud835\udc35 train 25 formulae-sequence subscript \ud835\udc47 train 5000 \ud835\udc3e 5 L=200,B_{\\text{train}}=25,T_{\\text{train}}=5000,K=5\n\nWe use an aggregated batch size of 6,144 and a learning rate of 1e-4 with an AdamW optimizer (Loshchilov and Hutter, 2019). We trained Ssd-LM for 100K steps, which took about 6 days on 32 Nvidia V100 GPUs. Pretraining loss\n\nCanonical training-time perplexity of LMs is not compatible with diffusion LMs due to the difference in the inputs to the models (Equation 4 and Eq. 9). Our pretraining loss is a per-token negative log-likelihood (NLL) that depends on the specific noise schedule being used. Ssd-LM gets an average NLL of 3.87 at the end of pretraining. We show a pretraining loss curve in the appendix (\u00a7D). 4.2 Unconstrained text generation\n\nSetup\n\nFirst, we benchmark Ssd-LM with autoregressive LMs trained on the same data (GPT2) on text generation quality. We randomly sample 1000 sequences from the held-out OpenWebText test data, extract their prefixes as prompts (context), and generate continuations from the LMs. We consider three setups: with prompt lengths 25, 50 and 100 with respective output lengths as 25, 50 and 100 tokens. In each setup, we sample 5 continuations for each input context, thus comparing the quality of 5,000 generations from baseline GPT-2 models and our Ssd-LM. We compare Ssd-LM with GPT2-medium, large and xl models (containing 0.4B, 0.8B and 1.6B parameters respectively) as baselines. For reference, our model size is comparable to GPT2-medium. We experiment with two popular decoding strategies for the baseline GPT-2 models with canonical parameters: nucleus sampling (Holtzman et al., 2020) with a top- of 0.9 and 0.95, and typical sampling (Meister et al., 2022) with a typical- of 0.2 and 0.95. For Ssd-LM, we consider three logits projection strategies, sampling and multi-hot with , and greedy (which is functionally equivalent to the sampling with top-=0).",
    "ssdlm-7": "We use a test block size () of 25. When generating samples of length 50 or 100, we semi-autoregressively sample in blocks of 25 and feed them as additional context to generate the next block as described in \u00a73.2. We evaluate the generated continuations on two axes: quality and diversity. As automatic quality metrics, we report perplexity measured by a separate, larger language model (GPT-Neo-1.3B, Black et al., 2021). Prior works, however, have shown that low perplexity of generated text is not necessarily an indication of high quality but of degenerate behavior (Nadeem et al., 2020; Zhang et al., 2021) and have proposed closeness to the perplexity of human-written text as a better evaluation. Hence, we also report the difference of log perplexity between the generated text and human-written continuations (). For diversity evaluation, we report Zipf\u2019s coefficient (Zipf) and average distinct -grams in the output samples (Li et al., 2016, Dist-). In addition, we also report the repetition rate (Welleck et al., 2020; Holtzman et al., 2020, Rep), measuring the proportion of output samples that end in repeating phrases. Finally, we report MAUVE (Pillutla et al., 2021) which evaluates both quality and diversity together by approximating information divergence between generated samples and human-written continuations (from the OpenWebText held-out set). Results\n\nfootnote 9 summarizes our main results on the 50-token prompt and output setup. We report the numbers for the best performing three settings for logits projection and decoding steps in Ssd-LM. We report the best setting for the baselines. The results for other generation lengths have a similar trend and can be found in the appendix (\u00a7D). We find that Ssd-LM, though being smaller in size, outperforms larger GPT-2 models on the unified metric MAUVE. On diversity, Ssd-LM outperforms GPT-2 in Dist- while achieving lower repetition rates. On perplexity, the results are slightly mixed. We observe a trade-off between MAUVE and perplexity for different settings we considered, indicating that further tuning of the hyperparameters may be required. However, one of our best performing settings (sampling top-=0.9, =2500) still achieves the closest perplexity to the gold continuation. In \u00a7D, we show the influence of different logits projection strategies and the associated parameters on the output text quality in Figure 4. We also show qualitative examples of the generations by Ssd-LM in Appendix D and a trajectory of intermediate states during the decoding process in Appendix D. Comparison with Li et al. (2022)\n\nA prior work to us, Li et al. (2022) propose Diffusion-LM, an embedding-based diffusion model trained on two small toy datasets, E2E (Novikova et al., 2017) and ROCStories (Mostafazadeh et al., 2016). In this subsection, we make a diversion to compare the embedding-based Diffusion-LM with our semi-autoregressive, simplex-based Ssd-LM. Following Li et al. (2022), we train a Diffusion-LM on ROCStories with a default embedding size of 128, 0.1B parameters under a BERT-base (Devlin et al., 2019) structure,111111We train two versions of Diffusion-LM, with and without BERT\u2019s encoder weights as an initialization.",
    "ssdlm-8": "The default no-initialization setup as in Li et al.",
    "ssdlm-9": "(2022) works reasonably, while the other degenerates. Details can be found in \u00a7C. and a sequence length of 100. For a fair comparison, only within this subsection we train a Ssd-LM with ROCStories sequences of 100 tokens, a decoding block size of 25, and a BERT-base initialization. Further details of the setup can be found in \u00a7C. On 2,700 held-out ROCStories sequences, we use the first 50 tokens of each sequence as a prompt and have the model generate the next 50. In footnote 10, we show the MAUVE score and perplexity of both models. We observe a substantially higher MAUVE score and lower perplexity with Ssd-LM. 4.3 Controlled text generation\n\nSetup\n\nTo evaluate Ssd-LM\u2019s ability for highly-modular control, we consider the task of sentiment controlled generation where given a prompt, the goal is to generate a continuation with a positive (or negative) polarity. We use a set of 15 short prompts as in Dathathri et al. (2020) and generate 20 samples per prompt per sentiment category, making the total number of generated samples to be 600. Following Mireshghallah et al. (2022), we generate samples with 3 different output lengths: 12, 20 and 50. For guidance, we simply import a popular sentiment classifier121212https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment from HuggingFace trained with Twitter sentiment data with over 58M training examples (Barbieri et al., 2020).",
    "ssdlm-10": "This model serves as as shown in Eq. 16. In addition to quality and diversity of the generated samples, we also evaluate them on control (that is measuring if the generated output is actually positive or negative in polarity). For this, we use an external sentiment classifier trained on a different dataset. Specifically, we use a classifier trained with Yelp reviews131313https://huggingface.co/textattack/bert-base-uncased-yelp-polarity (Zhang et al., 2015; Morris et al., 2020) following the evaluation setup in the baselines we consider. Again, we consider the sampling and multi-hot decoding strategies with , , and the multiplier for control . For the generation of 12/20/50 tokens, we use =12/20/25 and apply the decoding algorithm for =1/1/2 iterations respectively. Results\n\nWe show the quality of the controlled generations from three perspectives: target attribute via the external classifier accuracy, fluency via perplexity, and diversity via the distinctiveness measures. In footnote 14, we show the experimental results for output length 50. The results at length 12 and 20 have a similar trend and can be found in the appendix (\u00a7D). Among the baseline methods, DAPT (Gururangan et al., 2020), GeDi (Krause et al., 2021), and DExperts (Liu et al., 2021) require training customized language models aware of the desired attributes (denoted as CM in Table 7). PPLM (Dathathri et al., 2020), FUDGE (Yang and Klein, 2021), and MuCoLa (Kumar et al., 2022b) require training a customized attribute classifier (CC). While our proposed method Ssd-LM and M&M LM (Mireshghallah et al., 2022) can directly import mainstream existing attribute classifiers from platforms like HuggingFace and are thus highly modular (HMC).",
    "ssdlm-11": "We show the baseline results as reported in Mireshghallah et al.",
    "ssdlm-12": "(2022) and Kumar et al. (2022b). Ssd-LM shows strong controllability while possessing great modularity. Ssd-LM outperforms M&M LM, the other HMC method by a large margin. Even when comparing with the CC and CM methods, our method achieves a good balance in control, fluency, and diversity. In \u00a7D, we show the impact of the control weight and top- on the attribute accuracy and perplexity in Figure 5. We also show qualitative examples of the controlled generations by Ssd-LM in Appendix D. 5 Related work\n\nDiffusion models\n\nDiffusion models have demonstrated impressive performance in popular continuous-valued domains such as images (Ho et al., 2020), audio (Kong et al., 2021), video (Ho et al., 2022) and recently also been adopted for 3D-shapes, protein structures, and more (Zhou et al., 2021; Trippe et al., 2022; Wu et al., 2022). Since they are based on adding Gaussian noise, these approaches are not straightforward to apply to discrete valued domains like text. Hoogeboom et al. (2021); Austin et al. (2021) propose diffusing in the discrete space using categorical distributions which are modified using transition matrices. However, these methods do not straightforwardly support control and yield worse results than comparable autoregressive models. Li et al. (2022) propose to represent each token as a continuous embedding and apply diffusion in the embedding space. They train the LM to generate a fixed length sequence whereas Ssd-LM allows flexibility in the generated sequence length by generating block-wise. Further, their LM is trained with specialized datasets and not evaluated against general-purpose autoregressive LMs on unconstrained text generation. Their method supports post-hoc control but requires training a customized attribute classifier,151515The control for diffusion models can also be classifier-free (Ho and Salimans, 2021) but requires training with the target attribute in advance, which is not a focus of this work. since the diffusion operates on a learned embedding space. Gong et al. (2022), a concurrent work to ours, extend Li et al. (2022) to a sequence-to-sequence setup with a similar underlying embedding-based method. Our work is most closely related to Chen et al. (2022) which transform discrete data into a sequence of bits and represent each bit as +1 or -1 converting it into a continuous-valued domain. For textual data, however, it can lead to extremely long sequences which are difficult to optimize. In this work, we instead maintain a subword based vocabulary but represent each token as a sequence of manually defined logits. Language models\n\nThe majority of existing language models for text generation are trained autoregressively, i.e., they predict the next token given previously generated context. This paradigm scaled up both in terms of model size and training data size has resulted in impressive capabilities on many benchmarks (Brown et al., 2020; Chowdhery et al., 2022). However, they generate text one token at a time which does not provide flexible control over attributes of the generated text. Non-autoregressive models which generate the entire output sequence at the same time have also been explored in prior work other than diffusion models (Lee et al., 2018; Ghazvininejad et al., 2019). However, they are primarily focused on improving decoding efficiency and applied for specialized tasks like translation (Gu et al., 2018; Kaiser et al., 2018; Wang et al., 2019) and text editing (Gu et al., 2019). Many of these work have iterative processes in a discrete space, with some exploring continuous representations (Ma et al., 2019; Lee et al., 2020). To address the quality decline with the non-autoregressive methods compared to autoregressive models, prior work have also explored semi-autoregressive approaches (Wang et al., 2018; Qi et al., 2021). In the same vein, our work seeks to address the drawbacks of autoregressive language models and non-autoregressive diffusion models with a middle ground. Controllable text generation\n\nEarly solutions for controlling attributes of generated text focused on training or finetuning AR-LMs with specific control codes (Keskar et al., 2019; Gururangan et al., 2020; Chan et al., 2021). These methods are difficult to extend to new controls as it requires retraining the models. More recent work includes decoding approaches from pretrained AR-LMs without modifying the models, through altering the output probability distribution at each step using different control objectives (Dathathri et al., 2020; Krause et al., 2021; Yang and Klein, 2021; Liu et al., 2021; Lu et al., 2021; Pascual et al., 2021). However, these methods do not allow modifying a token once it is generated and are thus suboptimal for controls at the scope of the whole sequence. Closely related to Ssd-LM are Kumar et al. (2021); Qin et al. (2022); Kumar et al. (2022b), which propose gradient-based decoding algorithms from AR-LMs. They require computing a backward pass through the LMs for each iteration, an expensive operation. In contrast, Ssd-LM with its semi-autoregressive setup allows editing past tokens via diffusion. In addition, most of these approaches require training control functions from scratch whereas our model allows using off-the-shelf classifiers. Mireshghallah et al. (2022) propose a non-autoregressive LM based on Metropolis-Hastings sampling. It also supports off-the-shelf classifiers for control, and we therefore use it as a direct baseline for Ssd-LM. 6 Conclusion\n\nWe present Ssd-LM, a semi-autoregressive diffusion based language model trained to denoise corrupted simplexes over the output vocabulary. Compared to prior work in text-based diffusion, Ssd-LM offers more flexibility in output length by generating blocks of text and an ability to use off-the-shelf attribute classifiers for control without additional tuning. On unconstrained text generation, Ssd-LM performs on par with or outperforms strong and larger autoregressive baselines (GPT-2) in generation quality and diversity, while vastly outperforming diffusion baselines (Diffusion-LM). On controlled text generation, Ssd-LM surpasses baselines while possessing an easy-to-use modular design. We believe that Ssd-LM opens an exciting direction for future research in flexible and modular diffusion-based language generation. Limitations\n\nSample efficiency\n\nIn AR-LMs, an NLL loss is computed at training time for every token in the sequence of length (Equation 4). However, in Ssd-LM, each time a pretraining example is sampled, the loss is computed on only tokens (Eq. 9) leading to a lower sample efficiency than AR-LM. Towards improving this efficiency, future work could explore model architectures dedicated to semi-autoregressive diffusion rather than the vanilla Transformer encoder we use in this work. Decoding speed\n\nSince each block is generated by refining over several iterations, Ssd-LM has a considerably slower decoding speed than autoregressive models. For example, given a context of 50 tokens (single instance, unbatched), it takes Ssd-LM 25 seconds to generate the next block of 25 tokens (=1000). While our work focused on establishing the efficacy of diffusion-based LMs and modular controlled generation, future work could explore tuning to balance model performance and decoding speed, or more efficient training and decoding algorithms extending ideas from prior work on diffusion models for continuous domains (Song et al., 2021; Nichol and Dhariwal, 2021; Rombach et al., 2022; Meng et al., 2022). Decoding block size\n\nIn this work, although we allow setups where , the decoding block size remains the same across decoding iterations, leaving space for a more flexible decoding schedule. Future work can also explore learning (and ) rather than using constant pre-defined lengths. Larger scale experiments with different kinds of controls and their combinations can be done, as well as more sophisticated ways to incorporate them (Kumar et al., 2021). In addition, we plan to explore alternative methods to continuously represent and add noise to discrete text (Bakosi and Ristorcelli, 2013). This work experiments with pretraining data that is primarily in English. Future work can also explore challenges and benefits of diffusion-based LMs in a multilingual setup. Ethics statement\n\nLanguage models trained on data from the web can perpetuate social biases and toxic interactions, and can be prone to generating harmful language (Gehman et al., 2020; Wallace et al., 2019, 2020; Sheng et al., 2021; Weidinger et al., 2022). Further, language generation models could memorize and amplify patterns in data without deeper language understanding or control, so they can be factually inconsistent and generate disinformation (Maynez et al., 2020; Pagnoni et al., 2021; Zellers et al., 2019), or can compromise user privacy (Carlini et al., 2021). Prior works have outlined these risks (Sheng et al., 2021; Weidinger et al., 2021), discussed their points of origin, and advocated for future research on ethical development of LMs (Bender et al., 2021; Solaiman et al., 2019). While these studies have been conducted for autoregressive LMs, our diffusion-based LM is subject to these problems as well. However, since our method naturally incorporates controllability, future work may explore control functions that could potentially alleviate these issues (Liu et al., 2021; Kumar et al., 2022b). One risk is that controllability can also be misused maliciously, with models being intentionally exploited to generate biased, toxic, or non-factual content (Bagdasaryan and Shmatikov, 2022; Pagnoni et al., 2022). Therefore, apart from controlled generation, future work should aim to detect the generations under control as well to defend against the malicious use (Kumar et al., 2022a). Acknowledgements\n\nThe authors would like to thank Tianxiao Shen, Tianxing He, Jiacheng Liu, Ruiqi Zhong, Sidney Lisanza, Jacob Gershon, members of TsvetShop, and the anonymous ACL reviewers for their helpful discussions and feedback. X.H. gratefully acknowledges funding from the UW-Meta AI Mentorship program. S.K. gratefully acknowledges a Google Ph.D. Fellowship. Y.T. gratefully acknowledges an Alfred P. Sloan Foundation Fellowship. This research is supported in part by by the National Science Foundation (NSF) under Grants No. IIS2203097, IIS2125201, and NSF CAREER Grant No. IIS2142739. This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S.",
    "ssdlm-13": "Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. References\n\nAtchison and Shen (1980) J. Atchison and S.M. Shen. 1980. Logistic-normal distributions:Some properties and uses. Biometrika, 67(2):261\u2013272. Austin et al. (2021) Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. In Proc. NeurIPS. Bagdasaryan and Shmatikov (2022) Eugene Bagdasaryan and Vitaly Shmatikov. 2022. Spinning language models: Risks of propaganda-as-a-service and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP), pages 1532\u20131532. IEEE Computer Society. Bakosi and Ristorcelli (2013) J\u00f3zsef Bakosi and J.",
    "ssdlm-14": "Raymond Ristorcelli. 2013. A stochastic diffusion process for the dirichlet distribution.",
    "ssdlm-15": "arXiv: Mathematical Physics. Barbieri et al. (2020) Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020. Tweeteval: Unified benchmark and comparative evaluation for tweet classification. In Findings of EMNLP. Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proc. FAccT. Black et al. (2021) Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165. Carlini et al. (2021) Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models. In USENIX Security Symposium, pages 2633\u20132650. Chan et al. (2021) Alvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang, and Jie Fu. 2021. Cocon: A self-supervised approach for controlled text generation. In Proc. ICLR. Chen et al. (2022) Ting Chen, Ruixiang Zhang, and Geo rey E. Hinton. 2022. Analog bits: Generating discrete data using diffusion models with self-conditioning.",
    "ssdlm-16": "ArXiv, abs/2208.04202. Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311. Dathathri et al. (2020) Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation.",
    "ssdlm-17": "In Proc. ICLR. Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL-HLT. Dhariwal and Nichol (2021) Prafulla Dhariwal and Alex Nichol. 2021. Diffusion models beat gans on image synthesis. ArXiv, abs/2105.05233. Fan et al. (2018) Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proc. ACL. Gehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356\u20133369, Online. Association for Computational Linguistics. Ghazvininejad et al. (2019) Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models.",
    "ssdlm-18": "In Proc. EMNLP. Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. Gong et al. (2022) Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. 2022. Diffuseq: Sequence to sequence text generation with diffusion models. ArXiv, abs/2210.08933. Gu et al. (2018) Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. 2018. Non-autoregressive neural machine translation. In Proc. ICLR. Gu et al. (2019) Jiatao Gu, Changhan Wang, and Jake Zhao. 2019. Levenshtein transformer. In Proc. NeurIPS. Gururangan et al. (2020) Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A.",
    "ssdlm-19": "Smith. 2020. Don\u2019t stop pretraining: Adapt language models to domains and tasks.",
    "ssdlm-20": "In Proc. ACL. Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Proc. NeurIPS. Ho and Salimans (2021) Jonathan Ho and Tim Salimans. 2021. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Ho et al. (2022) Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.",
    "ssdlm-21": "Fleet. 2022. Video diffusion models. ArXiv, abs/2204.03458. Hoang et al. (2017) Cong Duy Vu Hoang, Gholamreza Haffari, and Trevor Cohn. 2017. Towards decoding as continuous optimisation in neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 146\u2013156, Copenhagen, Denmark. Association for Computational Linguistics. Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In Proc. ICLR. Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. 2021. Argmax flows and multinomial diffusion: Learning categorical distributions.",
    "ssdlm-22": "In Proc. NeurIPS. Kaiser et al. (2018) Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam Shazeer. 2018. Fast decoding in sequence models using discrete latent variables. In Proc. ICML, pages 2390\u20132399. PMLR. Keskar et al. (2019) Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858. Kong et al. (2021) Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2021. Diffwave: A versatile diffusion model for audio synthesis.",
    "ssdlm-23": "In Proc. ICLR. Krause et al. (2021) Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. Gedi: Generative discriminator guided sequence generation. In Proc. Findings of EMNLP. Kumar et al. (2022a) Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, and Yulia Tsvetkov. 2022a. Language generation models can cause harm: So what can we do about it? an actionable survey. arXiv preprint arXiv:2210.07700. Kumar et al. (2021) Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. 2021. Controlled text generation as continuous optimization with multiple constraints. In Proc. NeurIPS. Kumar et al. (2022b) Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. 2022b. Constrained sampling from language models via langevin dynamics in embedding spaces.",
    "ssdlm-24": "In Proc. EMNLP. Lee et al. (2018) Jason Lee, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic non-autoregressive neural sequence modeling by iterative refinement.",
    "ssdlm-25": "In Proc. EMNLP. Lee et al. (2020) Jason Lee, Raphael Shu, and Kyunghyun Cho. 2020. Iterative refinement in the continuous space for non-autoregressive neural machine translation. In Proc. EMNLP. Li et al. (2016) Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110\u2013119, San Diego, California. Association for Computational Linguistics. Li et al. (2022) Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusion-lm improves controllable text generation. ArXiv, abs/2205.14217. Liu et al. (2021) Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, and Yejin Choi. 2021. Dexperts: Decoding-time controlled text generation with experts and anti-experts.",
    "ssdlm-26": "In Proc. ACL. Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692. Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proc. ICLR. Lu et al. (2021) Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134299, Online. Association for Computational Linguistics. Ma et al. (2019) Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. 2019. Flowseq: Non-autoregressive conditional sequence generation with generative flow.",
    "ssdlm-27": "In Proc. EMNLP. Maynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, Online. Association for Computational Linguistics. Meister et al. (2022) Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2022. Locally typical sampling. ArXiv, abs/2202.00666. Meng et al. (2022) Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. 2022. On distillation of guided diffusion models. ArXiv, abs/2210.03142. Mireshghallah et al. (2022) Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor Berg-Kirkpatrick. 2022. Mix and match: Learning-free controllable text generationusing energy language models.",
    "ssdlm-28": "In Proc. ACL. Morris et al. (2020) John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119\u2013126. Mostafazadeh et al. (2016) Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849. Nadeem et al. (2020) Moin Nadeem, Tianxing He, Kyunghyun Cho, and James Glass. 2020. A systematic characterization of sampling algorithms for open-ended language generation. In Proc. AACL. Nichol and Dhariwal (2021) Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In Proc. ICML. Novikova et al. (2017) Jekaterina Novikova, Ond\u0159ej Du\u0161ek, and Verena Rieser. 2017. The e2e dataset: New challenges for end-to-end generation.",
    "ssdlm-29": "arXiv preprint arXiv:1706.09254. Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics.",
    "ssdlm-30": "In Proc. NAACL. Pagnoni et al. (2022) Artidoro Pagnoni, Martin Graciarena, and Yulia Tsvetkov. 2022. Threat scenarios and best practices to detect neural fake news. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1233\u20131249. Pascual et al. (2021) Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, and Roger Wattenhofer. 2021. A plug-and-play method for controlled text generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3973\u20133997, Punta Cana, Dominican Republic. Association for Computational Linguistics. Pillutla et al. (2021) Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Za\u00efd Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers.",
    "ssdlm-31": "In Proc. NeurIPS. Qi et al. (2021) Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, et al. 2021. Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining. In Proc. ICML, pages 8630\u20138639. PMLR. Qin et al. (2022) Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. Cold decoding: Energy-based constrained text generation with langevin dynamics. ArXiv, abs/2202.11705. Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Raffel et al. (2020) Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.",
    "ssdlm-32": "Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "ssdlm-33": "JMLR. Rombach et al. (2022) Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674\u201310685. Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units.",
    "ssdlm-34": "In Proc. ACL. Sheng et al. (2021) Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. Societal biases in language generation: Progress and challenges. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4275\u20134293, Online. Association for Computational Linguistics. Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics.",
    "ssdlm-35": "In Proc. ICML. Solaiman et al. (2019) Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203. Song et al. (2021) Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising diffusion implicit models.",
    "ssdlm-36": "In Proc. ICLR. Song and Ermon (2019) Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data distribution. In Proc. NeurIPS. Trippe et al. (2022) Brian Loeber Trippe, Jason Yim, Doug K Tischer, Tamara Broderick, David Baker, Regina Barzilay, and T.",
    "ssdlm-37": "Jaakkola. 2022. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem.",
    "ssdlm-38": "ArXiv, abs/2206.04119. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS. Wallace et al. (2019) Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153\u20132162, Hong Kong, China. Association for Computational Linguistics. Wallace et al. (2020) Eric Wallace, Mitchell Stern, and Dawn Song. 2020. Imitation attacks and defenses for black-box machine translation systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5531\u20135546, Online. Association for Computational Linguistics. Wang et al. (2018) Chunqi Wang, Ji Zhang, and Haiqing Chen. 2018. Semi-autoregressive neural machine translation. In Proc. EMNLP. Wang et al. (2019) Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. 2019. Non-autoregressive machine translation with auxiliary regularization. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 5377\u20135384. Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. Weidinger et al. (2022) Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2022. Taxonomy of risks posed by language models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201922, page 214\u2013229, New York, NY, USA.",
    "ssdlm-39": "Association for Computing Machinery. Welleck et al. (2020) Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In Proc. ICLR. Wu et al. (2022) Kevin E. Wu, Kevin Kaichuang Yang, Rianne van den Berg, James Zou, Alex X. Lu, and Ava P.",
    "ssdlm-40": "Amini. 2022. Protein structure generation via folding diffusion. ArXiv, abs/2209.15611. Yang and Klein (2021) Kevin Yang and Dan Klein. 2021. Fudge: Controlled text generation with future discriminators. In Proc. NAACL. Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. Advances in neural information processing systems, 32. Zhang et al. (2021) Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021. Trading off diversity and quality in natural language generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 25\u201333. Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068. Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Proc. NeurIPS. Zhou et al. (2021) Linqi Zhou, Yilun Du, and Jiajun Wu. 2021. 3d shape generation and completion through point-voxel diffusion.",
    "ssdlm-41": "In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5806\u20135815. IEEE. Appendix A A contrastive interpretation of the training loss\n\nThe training of Ssd-LM is simply maximizing the likelihood . This diverts from the exact objective of DDPM that is supported by a variational bound. However, below we give an intuitive interpretation to our objective. log \u2061 p \u03b8 \u200b ( \ud835\udc98 c : c + B \u2223 \ud835\udc98 ~ t c : c + B , \ud835\udc98 < c ) subscript \ud835\udc5d \ud835\udf03 conditional superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle\\log p_{\\theta}(\\boldsymbol{w}^{c:c+B}\\mid\\tilde{\\boldsymbol{w}}_{t}^{c:c+B},\\boldsymbol{w}^{<c}) (18) = \\displaystyle= log \u2061 p \u03b8 \u200b ( \ud835\udc98 c : c + B \u2223 \ud835\udc98 < c ) \u200b p \u03b8 \u200b ( \ud835\udc98 ~ t c : c + B \u2223 \ud835\udc98 c : c + B , \ud835\udc98 < c ) p \u03b8 \u200b ( \ud835\udc98 ~ t c : c + B \u2223 \ud835\udc98 < c ) subscript \ud835\udc5d \ud835\udf03 conditional superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 subscript \ud835\udc5d \ud835\udf03 conditional superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 subscript \ud835\udc5d \ud835\udf03 conditional superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 \\displaystyle\\log\\frac{p_{\\theta}(\\boldsymbol{w}^{c:c+B}\\mid\\boldsymbol{w}^{<c})~{}p_{\\theta}(\\tilde{\\boldsymbol{w}}_{t}^{c:c+B}\\mid\\boldsymbol{w}^{c:c+B},\\boldsymbol{w}^{<c})}{p_{\\theta}(\\tilde{\\boldsymbol{w}}_{t}^{c:c+B}\\mid\\boldsymbol{w}^{<c})} (19) = \\displaystyle= log \u2061 p \u03b8 \u200b ( \ud835\udc98 c : c + B \u2223 \ud835\udc98 < c ) \u23df likelihood of true data \u2212 log \u2061 p \u03b8 \u200b ( \ud835\udc98 ~ t c : c + B \u2223 \ud835\udc98 < c ) \u23df likelihood of noisy data at timestep \u200b t subscript \u23df subscript \ud835\udc5d \ud835\udf03 conditional superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 likelihood of true data subscript \u23df subscript \ud835\udc5d \ud835\udf03 conditional superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 absent \ud835\udc50 likelihood of noisy data at timestep \ud835\udc61 \\displaystyle~{}\\log\\underbrace{\\textstyle p_{\\theta}(\\boldsymbol{w}^{c:c+B}\\mid\\boldsymbol{w}^{<c})}_{\\mathclap{\\text{likelihood of true data}}}-\\log\\underbrace{\\textstyle p_{\\theta}(\\tilde{\\boldsymbol{w}}_{t}^{c:c+B}\\mid\\boldsymbol{w}^{<c})}_{\\mathclap{\\text{likelihood of noisy data at timestep }t}} + log \u2061 p \u200b ( \ud835\udc98 ~ t c : c + B \u2223 \ud835\udc98 c : c + B ) \u23df forward diffusion process independent of \u200b \u03b8 subscript \u23df \ud835\udc5d conditional superscript subscript ~ \ud835\udc98 \ud835\udc61 : \ud835\udc50 \ud835\udc50 \ud835\udc35 superscript \ud835\udc98 : \ud835\udc50 \ud835\udc50 \ud835\udc35 forward diffusion process independent of \ud835\udf03 \\displaystyle+\\log\\underbrace{\\textstyle p(\\tilde{\\boldsymbol{w}}_{t}^{c:c+B}\\mid\\boldsymbol{w}^{c:c+B})}_{\\mathclap{\\text{forward diffusion process independent of }\\theta}} (20)\n\nOptimizing is a contrastive objective: maximizing the estimated likelihood of true data, while penalizing the estimated likelihood of noisy data under a broad range of different noise scales. Appendix B Connection between our decoding algorithm and the DDPM decoding\n\nWe revisit the decoding step in DDPM introduced in Equation 3. Since we know that during the training phase is generated through a one-step forward diffusion process (Equation 1), a model predicting the added noise can therefore be considered as predicting an imaginary in one-step:\n\n\ud835\udc99 ^ 0 \u200b ( \ud835\udc99 t , t , \u03b8 ) subscript ^ \ud835\udc99 0 subscript \ud835\udc99 \ud835\udc61 \ud835\udc61 \ud835\udf03 \\displaystyle\\hat{\\boldsymbol{x}}_{0}(\\boldsymbol{x}_{t},t,\\theta) = 1 \u03b1 \u00af t \u200b ( \ud835\udc99 t \u2212 1 \u2212 \u03b1 \u00af t \u200b \u03f5 \u03b8 \u200b ( \ud835\udc99 t , t ) ) absent 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript \ud835\udc99 \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript italic-\u03f5 \ud835\udf03 subscript \ud835\udc99 \ud835\udc61 \ud835\udc61 \\displaystyle=\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}(\\boldsymbol{x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t)) (21)\n\nBelow we write as and as for simplicity. Rearranging the DDPM decoding transition (Equation 3), we have:\n\n\ud835\udc99 t \u2212 1 subscript \ud835\udc99 \ud835\udc61 1 \\displaystyle\\boldsymbol{x}_{t-1} = \u03b1 \u00af t \u2212 1 \u200b \ud835\udc99 ^ 0 + \u03b1 t \u2212 \u03b1 \u00af t 1 \u2212 \u03b1 \u00af t \u200b 1 \u2212 \u03b1 \u00af t \u2212 1 \u200b \u03f5 \u03b8 absent subscript \u00af \ud835\udefc \ud835\udc61 1 subscript ^ \ud835\udc99 0 subscript \ud835\udefc \ud835\udc61 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript italic-\u03f5 \ud835\udf03 \\displaystyle=\\sqrt{\\bar{\\alpha}_{t-1}}\\hat{\\boldsymbol{x}}_{0}+\\sqrt{\\frac{\\alpha_{t}-\\bar{\\alpha}_{t}}{1-\\bar{\\alpha}_{t}}}\\sqrt{1-\\bar{\\alpha}_{t-1}}\\epsilon_{\\theta} (22) \u2248 \u03b1 \u00af t \u2212 1 \u200b \ud835\udc99 ^ 0 + 1 \u2212 \u03b1 \u00af t \u2212 1 \u200b \u03f5 \u03b8 absent subscript \u00af \ud835\udefc \ud835\udc61 1 subscript ^ \ud835\udc99 0 1 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript italic-\u03f5 \ud835\udf03 \\displaystyle\\approx\\sqrt{\\bar{\\alpha}_{t-1}}\\hat{\\boldsymbol{x}}_{0}+\\sqrt{1-\\bar{\\alpha}_{t-1}}\\epsilon_{\\theta} (23)\n\nwith for most .161616Specifically, we adopt a cosine schedule for (Nichol and Dhariwal, 2021), and for 98% of all , with some outliers as and . Noting the format simlarity between Equation 1 and Eq. 23, we therefore interpret the DDPM decoding transition from to as (1) predicting an imaginary , and (2) applying a compensating forward diffusion step with a deterministic noise . Our decoding strategy in Eq. 15 is in a very similar form as Eq. 23. We also predict the initial data representation with and apply a forward diffusion step. The difference is that we sample a noise instead of using the deterministic , to encourage exploration. Appendix C Detailed setup of the comparison with Diffusion-LM (Li et al., 2022)\n\nWe apply block concatenation on ROCStories similarly as OpenWebText, resulting in 50K training sequences of 100 tokens. We train Diffusion-LM with a default batch size of 64, learning rate of 1e-4, and 400K steps. We train Ssd-LM with a batch size of 512, learning rate of 1e-4, and 20K steps. Both models use a tokenizer of BERT-base-uncased. For Ssd-LM, additional hyperparameters like decoding block size and one-hot constant remain the same as the main Ssd-LM benchmarked with GPT-2. For Diffusion-LM, the evaluation in the main paper is an infilling task. We use same decoding hyperparameters as Li et al. (2022). For Ssd-LM, the evaluation is a block-wise generation problem with =2 iterations. The result of Ssd-LM in footnote 10 is obtained with a decoding configuration of =2500 and top-=0.5. Our Ssd-LM in this subsection is initialized with BERT. For a fair comparison, apart from the default Diffusion-LM reported in footnote 10, we train another Diffusion-LM initialized with the encoder weights of BERT. However, this leads to degenerated results that are much worse than the default Diffusion-LM and our Ssd-LM: a MAUVE score of 0.4 out of 100 and a PPL of 73157. This problem is not due to overfitting, as all checkpoints of the model show the same degenerated result. Since Li et al. (2022) did not explore this setup in their original work as well, we conjecture that Diffusion-LM may be incompatible with pretrained weights from existing non-diffusion models by nature, a disadvantage to our Ssd-LM. Appendix D Additional results\n\nFigure 4 shows the influence of different logits projection strategies and the associated parameters on the unconstrained generations\u2019 output text quality. We observe that reducing top- 0 (greedy projection) can lead to a low perplexity but it is undesirable due to a high repetition rate. We also find the multi-hot projection strategy is overall worse performing than the sampling projection strategy in our setup, indicating it is better to commit the intermediate states to single rather than multiple tokens. This can be because our logits mapping involves putting probability mass on singular tokens. The multi-hot projection may still be a viable strategy if future work uses multi-hot logits mapping for the input tokens. Figure 5 shows the impact of the control weight and top- on the attribute accuracy and perplexity in controlled text generation. As expected, a larger control weight leads to a better external classifier accuracy. The perplexity at the same time increases with a larger , but under a reasonable range for a top- of 0.2 and 0.5. Figure 6 shows the pretraining loss trajectory. Table 4, Table 5, Table 6, and Table 7 show additional evaluation results of Ssd-LM generations. Appendix D and Appendix D show qualitative examples of Ssd-LM generations. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Tue Feb 27 02:12:45 2024 by LaTeXML"
}