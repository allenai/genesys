{
    "stm-0": "Efficient Long-Text Understanding with Short-Text Models\n\nMaor Ivgi Uri Shaham Jonathan Berant The Blavatnik School of Computer Science, Tel-Aviv University {maor.ivgi,uri.shaham,joberant}@cs.tau.ac.il\n\nAbstract\n\nTransformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles and long documents, due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step. 1 Introduction\n\nTransformer-based pretrained language models Vaswani et al.",
    "stm-1": "(2017); Devlin et al. (2019); Lewis et al. (2020); Raffel et al. (2020b); Brown et al. (2020) have been widely successful across all areas of natural language understanding (NLU). However, applying them over long texts (such as stories, scripts, or scientific articles) is prohibitive due to their quadratic complexity in the input length. To bridge this gap, recent work has developed more efficient transformer variants Kitaev et al. (2020a); Beltagy et al. (2020); Zaheer et al. (2020a); Guo et al. (2022) and applied them over long-range language understanding tasks Mehta et al. (2022); Shaham et al. (2022). However, most efficient transformers use specialized architectures with custom implementations that are not guaranteed to scale as well as vanilla transformers Tay et al. (2022a). Moreover, they require an expensive pretraining step and do not exploit off-the-shelf pretrained LMs that were trained for short texts. To date, their performance on long texts has not matched the success of their short-range counterparts. In this work, we present SLED: SLiding-Encoder and Decoder, a simple yet powerful method for applying off-the-shelf pretrained encoder-decoder models on long text problems, with a linear time and space dependency. Specifically (see Fig. 2), we partition long documents into overlapping chunks of tokens of constant length and encode each chunk independently with an already-pretrained encoder. Then, a pretrained decoder attends to all contextualized input representations to generate the output. Our main assumption is that input tokens can be contextualized through their local surrounding (using a short-text LM), and any global cross-chunk reasoning can be handled by the decoder, similar to fusion-in-decoder (FiD) Izacard and Grave (2021). Our approach can be readily applied to any pretrained encoder-decoder LM such as T5 Raffel et al. (2020b) and BART Lewis et al. (2020) (but is not applicable to decoder-only Brown et al. (2020) or encoder-only models Liu et al. (2019); Conneau et al. (2020)). We evaluate SLED on a wide range of language understanding tasks. To substantiate SLED\u2019s adequacy for text processing, we perform controlled experiments over modified versions of SQuAD 1.1 Rajpurkar et al. (2016) and HotpotQA Yang et al. (2018) to show that SLED can (a) find relevant information that is embedded within a long text sequence and (b) fuse information from chunks that were encoded separately. Our main evaluation is over SCROLLS, a recently-released benchmark that includes 7 long-range tasks across Question Answering (QA), Summarization, and Natural Language Inference (NLI).",
    "stm-2": "We show (Fig. 1) that taking a pre-trained encoder-decoder model, such as BART Lewis et al. (2020) or T5 Raffel et al. (2020b), and embedding it into SLED\u2019s framework results in dramatic improvement in performance (6 points on average across models). Moreover, BARTlarge-SLED\u2019s performance is comparable to LongT5base Guo et al. (2022), a model that was specifically pretrained to handle long-range dependencies, and surpasses UL2 Tay et al. (2022b), which contains 50x more parameters. Importantly, SLED-based models can use any future pretrained LM out-of-the-box without requiring additional pretraining to further improve performance. Due to its simplicity, SLED can also be used as a diagnostic tool for analyzing long-range benchmarks. We analyze the seven datasets in SCROLLS through the lens of SLED and show which datasets require the input to be contextualized with remote tokens. Specifically, we find that in QA and NLI tasks, relatively local contextualization is sufficient for high performance. While SLED is similar to FiD from a technical standpoint, past usage of FiD has centered around open-domain question answering Izacard and Grave (2021), where unrelated passages are naturally encoded independently. Here, we test fusion-in-decoder on long documents, where local encoding of chunks is a modeling assumption that needs testing. In recent work, Vig et al. (2022) proposed a similar architecture to tackle long inputs from QMSum Zhong et al. (2021), but did not systematically analyze it. We standardize this methodology for the first time, and extensively analyze the effectiveness of FiD for encoding long documents across multiple tasks. To summarize, our main contributions are:\n\n1. We present SLED, a simple and effective approach for processing long texts that leverages off-the-shelf encoder-decoder LMs based on fusion-in-decoder. 2. We demonstrate SLED\u2019s efficacy in both controlled experiments, as well as on the SCROLLS benchmark, which leads to competitive results compared to specialized models that include up to 50x more parameters. 3. We use SLED as a diagnostic tool for analyzing the long-range properties of datasets in the SCROLLS benchmark. 4. We provide an open-source implementation of SLED,111 https://github.com/Mivg/SLED seamlessly integrated into the Transformers library Wolf et al. (2020). 2 Background\n\nRecent advances in natural language processing have been by and large fueled by the transformer architecture Vaswani et al. (2017). A core component of the transformer is the self-attention layer where every input token \u201cattends\u201d to every other token to produce its contextualized representation. This results in quadratic time and space dependency w.r.t. the length of the input, limiting the ability of transformers to process long sequences. This long-text limitation has sparked ample interest in developing efficient transformer variants. One prominent family of methods is based on sparse attention, where each token attends to a constant number of other tokens, overcoming the quadratic dependency. Tokens typically attend either to their local surrounding Zaheer et al. (2020a); Beltagy et al. (2020); Ainslie et al. (2020); Gupta and Berant (2020) or to tokens that are semantically similar Kitaev et al.",
    "stm-3": "(2020b); Roy et al. (2021). Moreover, a constant number of global tokens that attend to and are attended by all input tokens are often added to each attention sub-layer. Recent analyses Xiong et al. (2022a) have shown that sparse transformers with local attention are competitive with other variants on multiple language understanding tasks. Our method, SLED, falls into the family of local attention variants. However, unlike prior work, SLED re-uses and extends existing short-range encoder-decoder models, and does not require specialized pretraining or dedicated CUDA implementations. In most local attention variants, e.g., LED Beltagy et al. (2020), attention is local per-layer, but the receptive field of tokens grows across layers. In SLED, which we describe next, tokens have access to the same number of tokens, independent of a layer\u2019s depth, which enables better parallelization.",
    "stm-4": "For a survey on the families of efficient transformers, see Tay et al. (2020). For an in-depth comparison of SLED and LED, we refer to Appendix B. 3 Method\n\nIn this work, we propose a simple approach for avoiding transformer\u2019s quadratic complexity, motivated by the Locality of information assumption:\n\nIn an encoder-decoder architecture, the encoder can effectively contextualize input tokens with local context only, leaving long-range dependencies to be handled by the decoder. SLED relies on said modeling assumption to encode shorter chunks independently and perform fusion of information in the decoder Izacard and Grave (2021). We now describe the SLED model in detail. Input\n\nSLED uses a pretrained encoder-decoder model as a backbone. SLED receives a tokenized document of length (blue squares in Fig. 2), and an optional short tokenized prefix of length , typically representing a question about the document, an instruction to perform some generation task, or a hypothesis (orange squares in Fig. 2). Unlike static task-specific prefixes (e.g., \u201csummarize\u201d), SLED supports also sample-specific prefixes that are part of the input (e.g., the question in QA datasets). Steps\n\nSLED follows the following steps:\n\n(a)\n\nDocument tokens are split into chunks of length (In Fig.",
    "stm-5": "2, ). The middle tokens in each chunk are contextualized from both the left and right by tokens, where ( in Fig. 2). We call these middle tokens the effective chunk, since they will constitute the output of the encoder, and term the tokens on each side by context padding. (b)\n\nEach chunk is prepended by (optional) prefix tokens (Fig. 2(b)). (c)\n\nEach chunk is encoded independently, using the backbone encoder (see Fig. 2(c)). (d)\n\nTo create a contextualized representation for each token, we keep from each chunk only the tokens from the effective chunk, and concatenate them (Fig. 2(d)). (e)\n\nTo give the decoder access to prefix tokens, we encode the prefix tokens with , and prepend the result to the contextualized representation (leftmost chunk in Fig. 2(a)-(d)). (f)\n\nFinally, we generate the output with the backbone decoder, , which uses standard cross-attention over the encoded tokens (Fig. 2(e)). SLED requires handling a few edge cases, namely, dealing with the first and last chunk that do not have bidirectional context. We refer to Appendix A for these details. SLED\u2019s Complexity\n\nSLED divides an input of length to chunks of size . Since , it follows that . While the complexity of encoding each chunk is quadratic in due to self-attention, is constant and thus the memory and compute dependency is linear in .222We assume the prefix length () is negligible and thus its effect on asymptotic complexity is negligible. In particular, the complexity to encode the input with a model of attention layers is:\n\n\ud835\udcaa \u200b ( l \u00d7 c 2 \u00d7 2 \u200b n c ) = \ud835\udcaa \u200b ( l \u00d7 c \u00d7 n ) . \ud835\udcaa \ud835\udc59 superscript \ud835\udc50 2 2 \ud835\udc5b \ud835\udc50 \ud835\udcaa \ud835\udc59 \ud835\udc50 \ud835\udc5b \\mathcal{O}\\left(l\\times c^{2}\\times\\frac{2n}{c}\\right)=\\mathcal{O}\\left(l\\times c\\times n\\right). Decoding is done as proposed by Vaswani et al. (2017), thus requiring memory. Assuming a constant output sequence length , this remains linear in . 4 Efficacy of Fusion in Decoder\n\nAs mentioned (\u00a73), SLED relies on the assumption that chunks can be encoded independently and fusion across them can be delegated to the decoder (Locality of information assumption). This is similar to the Fusion-in-Decoder approach, introduced by Izacard and Grave (2021) for open-domain question answering (ODQA). However, there, the encoder-decoder receives a set of independent passages and needs to generate an answer that can typically be extracted from a single passage. Here, we extend the scope of FiD by applying it over a single, long, and coherent input that potentially requires global contextualization. To demonstrate the viability of FiD for long text language tasks, we design two controlled experiments that quantify the extent to which FiD can perform two operations at the heart of long-text processing. First, can FiD find a \u201cneedle-in-a-haystack\u201d, i.e., locate a piece of short information embedded in long text, disregarding irrelevant information. Second, can FiD \u201cpiece the puzzle\u201d and fuse two pieces of information that are encoded independently when generating an output. 4.1 Needle in a haystack\n\nTo check if SLED can ignore irrelevant text and locate a single piece of information, we cast SQuAD 1.1 Rajpurkar et al. (2016) as a sequence-to-sequence task with long input. SQuAD is a question answering dataset, where given a question-paragraph pair the goal is to generate the answer (which lies within the paragraph). For each question-paragraph pair, we randomly sample 9 other paragraphs from the the dataset and concatenate them to form a long document.333We only consider paragraphs that are not within the gold document and do not contain the gold answer. We then finetune and evaluate our models in two settings: a) Ordered Distractors: the gold paragraph is the first one, and all other distractors are concatenated after it. b) Shuffled Distractors: we randomly shuffle the order of all paragraphs so the answer can be anywhere in the input document. Since this is a QA task, the prefix is the question. We use BARTbase Lewis et al. (2020) as our backbone model, , throughout \u00a74, and compare SLED to an oracle BARTbase that is given the gold paragraph only with no distractor paragraphs. this is an oracle setup since BARTbase can take 1,024 tokens as input and all gold paragraphs are shorter. If SLED can match the oracle performance, we can infer that indeed the decoder can find a needle in a haystack. In addition, we compare SLED to BARTbase which is given only the first 1K tokens and to LED Beltagy et al. (2020), which uses local sparse attention, similar to SLED (LED has the same backbone BARTbase). However, as explained in \u00a72, the receptive field of LED layers linearly grows with the number of layers, and thus information can be fused in the encoder, unlike SLED where cross-chunk fusion must be delegated to the decoder. Last, for QA tasks, LED defines the question tokens as global tokens, and as an additional sanity test we evaluate LEDL, i.e., a local LED model where no global tokens are used. For both LED and SLED we use a chunk size . Results\n\nFig 3(a) shows the results of our evaluation on the development set. SLED almost matches the performance of an oracle BARTbase that is not given any distractor paragraphs, reaching an F1 score of 87.6 compared to the oracle F1 of 88.1 (horizontal line in the figure). LED also achieves high performance (but lower than SLED in the shuffled setup), showing both models learn to ignore distracting information and find a needle in a haystack. As expected, both LEDL and BART suffers a significant drop in performance when the passages are shuffled, as the gold paragraph is not contextualized with the question. 4.2 Piecing a puzzle\n\nWe now verify that SLED can fuse pieces of information from different chunks. To this end, we modify HotpotQA Yang et al. (2018), a multi-hop question answering dataset, in which every question relies on two pieces of information (located in different paragraphs). While in the original setting, each input in HotpotQA has two gold paragraphs and 8 distractor paragraphs, we include only the two gold paragraphs in our experiments. To ensure SLED and LED encode the relevant two pieces of information in separate chunks, we set the chunk size to . Similar to \u00a74.1, we compare SLED to an oracle BARTbase with full attention over 1,024 tokens,444All examples have 1,024 tokens, including the prefix. to LED, and to LEDL. Finally, past work has shown that many examples in HotpotQA can be answered with access to the \u201csecond\u201d gold paragraph only, which contains the answer Jiang and Bansal (2019). Thus, we also evaluate a BART model that is given the second passage only. Results\n\nFig. 4(a) shows that indeed, SLED\u2019s decoder can effectively fuse information from two separately encoded chunks, reaching an F1 of 76.5, slightly lower than the oracle F1 of 78.6. Notably, SLED substantially outperforms a BART model with access to the entire second paragraph, showing that information is fused by the decoder. LED slightly outperforms SLED, but when denied access to global tokens (LEDL) its performance drops sharply. This shows that the large receptive field of deep LED layers does not suffice for information fusion and interaction between the question and text is crucial for the decoder. To summarize, our two controlled experiments show that SLED can perform the operations of retrieving and fusing information, which are fundamental for long text language tasks. 4.3 Ablations of design choices\n\nWe leverage our controlled experimental setup to further investigate the components of SLED. Efficacy of the encoder\n\nWhile \u00a74.2 shows that SLED can fuse separate pieces of information in the decoder, it is not clear to what extent local contextualization is necessary. To check whether it is possible for all fusion to occur in the decoder, we finetune SLED with a chunk size of , such that input tokens do not observe any context in the encoder.",
    "stm-6": "As can be seen in the leftmost bar(s) in Fig. 3(b) and Fig. 4(b), removing local contextualization results in poor performance, illustrating the importance of local contextualization. Contextualizing chunks with a prefix\n\nAs explained, SLED does not use global tokens, but instead contextualizes each chunk with a prepended prefix. To verify its necessity, we finetune a SLED model that treats the prefix as another chunk and does not prepend it to document chunks.555We add masked padding after the prefix to ensure chunking of the document remains identical.",
    "stm-7": "The second bar(s) in Fig. 3(b) and Fig. 4(b) shows a significant drop in performance for all settings, suggesting the prefix is needed during encoding. As expected, there is practically no difference between the Ordered and Shuffled settings in Fig. 3(b). In contrast, LEDL which is similar in concept (due to the lack of global tokens) shows a significant drop when paragraphs are shuffled. This shows the possible effectiveness of the increased receptive field in LED, but only when the gold paragraph is relatively close to the prefix. Encoding the prefix\n\nAfter showing the prefix is crucial for the encoder, we ask whether the decoder needs direct access to the prefix or whether relevant information from the prefix can be infused into the chunk representations. To test that, we finetune SLED as usual, but remove the prefix tokens from the final representation given to the decoder. The rightmost bar(s) in Fig. 3(b) and Fig. 4(b) shows that providing the decoder with prefix representations makes little difference if any at all, suggesting that indeed the encoder can infuse the important information from the prefix into the encoded document tokens. 5 Experiments\n\nWe evaluate SLED on SCROLLS Shaham et al. (2022), a recently-proposed benchmark for evaluating long text understanding. SCROLLS contains seven datasets that span three different language understanding tasks:\n\n1. Summariazation: GovReport Huang et al. (2021) is a summarization task over reports from the Congressional Research Service; SummScreenFD Chen et al. (2022) is a summarization dataset over TV scripts; QMSum Zhong et al. (2021) is a query-based summarization dataset over meeting transcripts from various domains. While GovReport and SummScreenFD do not contain a prefix, for QMSum we consider the query as the prefix.",
    "stm-8": "2. Question answering (QA): Qasper Dasigi et al. (2021) is a QA benchmark that contains questions over NLP papers; NarrativeQA Ko\u010disk\u00fd et al. (2018) contains questions over entire books and movie scripts; QuALITY Pang et al. (2022) is a multiple-choice QA dataset over books and articles. For all QA datasets, we set the question as the prefix. For QuALITY, we consider the four answer options part of the question. 3. Natural language inference: ContractNLI Koreeda and Manning (2021) contains short legal hypotheses (set as the prefix) and legal documents as the premise.",
    "stm-9": "Models are tasked to predict whether the premise entails, contradicts or is neutral w.r.t.",
    "stm-10": "to the hypothesis. For each task, we use the official evaluation metrics defined in SCROLLS, which are based on the metrics from the original datasets. 5.1 Settings\n\nWe evaluate SLED with both BART Lewis et al. (2020) and T5 Raffel et al. (2020b) as backbone models. For each backbone model, we compare performance with SLED, which can consume long sequences, vs. the backbone models alone that are fed with the first 1,024 tokens. For comparison, we also finetune LEDbase. In all SLED and LED experiments, we use a maximal sequence length of 16K tokens and chunk size of 256 to allow for a fair evaluation. For each model-dataset pair, we run hyperparameter tuning (detailed in Appendix C) based on the development set. Additionally, we submit generated predictions over the test set to SCROLLS leaderboard,666https://www.scrolls-benchmark.com/leaderboard and compare to the reported performance of other models at the time of submission. 5.2 Results\n\n\u25c4 Feeling lucky?",
    "stm-11": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Mar 13 19:22:41 2024 by LaTeXML"
}