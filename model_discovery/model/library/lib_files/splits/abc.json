{
    "abc-0": "# AbC: Attention with Bounded-Memory Control \n\nHao Peng^ Jungo Kasai ${ }^{\\wedge}$ Nikolaos Pappas** Dani Yogatama* ${ }^{\\star}$ Zhaofeng Wu ${ }^{\\star *}$<br>Lingpeng Kong\u00bb Roy Schwartz ${ }^{\u00a9}$ Noah A. Smith ${ }^{\\star} \\diamond$<br>${ }^{\\wedge}$ Paul G. Allen School of Computer Science \\& Engineering, University of Washington<br>$\\star$ Amazon Web Services ${ }^{\\star}$ DeepMind $\\diamond$ Allen Institute for Artificial Intelligence<br>${ }^{9}$ School of Computer Science \\& Engineering, Hebrew University of Jerusalem<br>${ }^{\\bullet}$ Department of Computer Science, The University of Hong Kong<br>\\{hapeng, jkasai,npappas, zfw7, nasmith\\}@cs.washington.edu<br>dyogatama@deepmind.com, lpk@cs.hku.hk\n\n\n#### Abstract\n\nrost Abstract Transformer architectures have achieved stateof-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem distinct. Second, this abstraction gives new insights-an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of $A B C$, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss. ## 1 Introduction\n\nTransformer architectures are now central in natural language processing (Vaswani et al., 2017). They rely on the attention mechanism (Bahdanau et al., 2015) to contextualize the input. The context can\n\n[^0]be seen as a random access memory whose size linearly grows with the sequence length; each query reads from it using a softmax-normalized linear combination, with overhead linear in the memory size. This amounts to a quadratic complexity overall, making transformers' computational overhead prohibitive, especially for long sequences. One way to improve attention's efficiency is to bound its memory size. Imposing a constantsized constraint over the memory ensures that reading from it has constant time and space overhead, yielding a linear overall complexity in sequence lengths. This is in fact a common strategy adopted by several recent works. In this work, we show that some of these works are closely connected in ways that, to date, have gone unremarked. We propose attention with bounded-memory control $(\\mathrm{ABC})$, a unified abstraction over them. In ABC , constant-sized memories are organized with various control strategies, e.g., induced from heuristic patterns (Beltagy et al., 2020; Zaheer et al., 2020; Ainslie et al., 2020; Rae et al., 2020, inter alia), locality assumptions (Parmar et al., 2018; Liu et al., 2018), or positions (Wang et al., 2020b). These strategies, by and large, are \"contextagnostic.\" In response to this, we propose $\\mathrm{ABC}_{\\text {MLP }}$, a particular instance of $A B C$ that learns a contextualized control strategy from data. Specifically, $\\mathrm{ABC}_{\\text {MLP }}$ uses a neural network to determine how to store each token into the memory (if at all). Compared to previous bounded-memory models, it strikes a better trade-off between accuracy and efficiency: controlling for the accuracy, $\\mathrm{ABC}_{\\text {MLP }}$ can get away with much smaller memory sizes. ABC models (including $\\mathrm{ABC}_{\\text {MLP }}$ ) come with a linear complexity in sequence lengths, and admit recurrent computation graphs in causal attention (self-attention over the prefix). Therefore they are appealing choices in a variety of applications, including text encoding, language modeling and text generation. This leads to a surprising finding. Lin-\nformer (Wang et al., 2020b), an established efficient attention method, was previously thought not to be applicable in causal attention or autoregressive decoding (Tay et al., 2020). Through the ABC view, we show that it actually is, and achieves competitive performance in our machine translation experiments. ABC connects existing models that would otherwise seem distinct, reveals new insights into established methods, and inspires new efficient attention architectures. We explore its applications in transformers, as a drop-in substitute for the canonical softmax attention. ABC offers a novel lens that can help future research in the analysis of transformers, where the theoretical insights are still catching up with empirical success. Experiments on language modeling, machine translation, and masked language model finetuning show that our $\\mathrm{ABC}_{\\text {MLP }}$ model outperforms previous $A B C$ approaches in accuracy with a much smaller memory size. Compared to the strong transformer baseline, $\\mathrm{ABC}_{\\text {MLP }}$ achieves a significant speedup and memory savings at inference time, with no or negligible accuracy loss. The efficiency improvements are more prominent for long sequences, suggesting that the asymptotic savings are even more appealing in applications involving long sequences. We release our code at https://github.com/Noahs-ARK/ABC. ## 2 An Outer-Product View of Attention\n\nThis section presents our outer-product memory perspective of attention, which allows for a smooth transition to later discussion. In attention, a sequence of queries $\\left\\{\\mathbf{q}_{i}\\right\\}_{i=1}^{N}$ attend to a memory with $N$ slots, each storing a key and value pair: $\\mathbf{K}=\\left[\\mathbf{k}_{1}, \\ldots, \\mathbf{k}_{N}\\right]^{\\top}, \\mathbf{V}=$ $\\left[\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{N}\\right]^{\\top} \\in \\mathbb{R}^{N \\times d}$. ${ }^{1}$ Query $\\mathbf{q}$ reads from the memory using a softmax-normalized linear combination, producing a $d$-dimensional vector:\n\n$$\n\\operatorname{attn}\\left(\\mathbf{q},\\left\\{\\mathbf{k}_{i}\\right\\},\\left\\{\\mathbf{v}_{i}\\right\\}\\right)=\\mathbf{V}^{\\top} \\text { softmax }(\\mathbf{K q})\n$$\n\nThis takes $\\mathcal{O}(N)$ time and space. When the attention with $N$ queries can be parallelized (e.g., in text encoding), it takes linear time and quadratic space; when it cannot be (e.g., in decoding), it takes quadratic time and linear space. The memory can be equivalently represented as sums of vector outer products: $\\mathbf{K}=\\mathbf{I K}=$\n\n[^1]$\\sum_{i=1}^{N} \\mathbf{e}_{i} \\otimes \\mathbf{k}_{i}, \\mathbf{V}=\\sum_{i=1}^{N} \\mathbf{e}_{i} \\otimes \\mathbf{v}_{i} . \\mathbf{I}$ is the identity matrix, and $\\otimes$ denotes the outer product: $[\\mathbf{x} \\otimes$ $\\mathbf{y}]_{i, j}=x_{i} y_{j} . N$-dimensional vectors $\\left\\{\\mathbf{e}_{i}\\right\\}$ form the standard basis: $\\mathbf{e}_{i}$ has the $i$ th element being one and others zeros. We can view $\\mathbf{e}_{i}$ as control vectors that determine where to store $\\mathbf{k}_{i}$ and $\\mathbf{v}_{i}$ :\n\\[\n\n$$\n\\begin{aligned}\n\\mathbf{e}_{i} \\otimes \\mathbf{k}_{i} & =[\\underbrace{0, \\ldots 0}_{i-1}, 1, \\underbrace{0, \\ldots, 0}_{N-i}]^{\\top} \\otimes \\mathbf{k}_{i} \\\\\n& =[\\underbrace{\\mathbf{0}}_{d \\times(i-1)} ; \\mathbf{k}_{i} ; \\underbrace{\\mathbf{0}}_{d \\times(N-i)}]^{\\top} . \\end{aligned}\n$$\n\\]\n\nThe $N$-by- $d$ matrix on the last line has its $i$ th row being $\\mathbf{k}_{i}^{\\top}$ and all others zeros; in this sense, $\\mathbf{k}_{i}$ is stored in the $i$ th slot by $\\mathbf{e}_{i}$, not affecting others. ## 3 Attention with Bounded Memory\n\nA straightforward way to improve attention's efficiency is to bound its memory size. Our outerproduct view of attention provides a straightforward way to devise this, by replacing $\\left\\{\\mathbf{e}_{i}\\right\\}$ with control vectors that select $n \\ll N$ vectors to attend to. We dub this approach attention with bounded-memory control (ABC). Concretely, let $\\widetilde{\\mathbf{K}}, \\widetilde{\\mathbf{V}} \\in \\mathbb{R}^{n \\times d}$ denote a constant-size memory with $n$ slots, with $n$ set a priori. $$\n\\widetilde{\\mathbf{K}}=\\sum_{i=1}^{N} \\boldsymbol{\\phi}_{i} \\otimes \\mathbf{k}_{i}, \\quad \\widetilde{\\mathbf{V}}=\\sum_{i=1}^{N} \\phi_{i} \\otimes \\mathbf{v}_{i}\n$$\n\n$\\left\\{\\phi_{i} \\in \\mathbb{R}^{n}\\right\\}_{i=1}^{N}$ denotes a sequence of control vectors. The output is calculated by attending to $\\widetilde{\\mathbf{K}}$ and $\\widetilde{\\mathbf{V}}: \\operatorname{ABC}\\left(\\mathbf{q},\\left\\{\\mathbf{k}_{i}\\right\\},\\left\\{\\mathbf{v}_{i}\\right\\},\\left\\{\\boldsymbol{\\phi}_{i}\\right\\}\\right)=$\n\n$$\n\\tilde{\\mathbf{V}}^{\\top} \\operatorname{softmax}(\\widetilde{\\mathbf{K}} \\mathbf{q})\n$$\n\nWe will discuss various ways to construct $\\left\\{\\phi_{i}\\right\\}$ in the subsequent sections. Reading from the memory takes a constant $\\mathcal{O}(n)$ time and space; therefore ABC's overall complexity is $\\mathcal{O}(N n)$, linear in the sequence length. ${ }^{2}$\n\nEq. 3 offers an equivalent recurrent computation, which is particularly useful in causal attention where only the prefix is looked at,\n\n$$\n\\widetilde{\\mathbf{K}}_{t+1}=\\widetilde{\\mathbf{K}}_{t}+\\phi_{t+1} \\otimes \\mathbf{k}_{t+1}\n$$\n\nlikewise for $\\tilde{\\mathbf{V}}_{t} . \\quad \\widetilde{\\mathbf{K}}_{t}$ and $\\widetilde{\\mathbf{V}}_{t}$ can be seen as the recurrent hidden state that encodes the prefix. In what follows, we study several existing efficient attention approaches and show that they are in fact instances of the $A B C$ abstraction. [^2]\n### 3.1 Linformer\n\nLinformer (Wang et al., 2020b) is an established efficient transformer variant that has proven successful in masked language modeling and text encoding. It assumes fixed-length inputs and learns a low-rank approximation of the attention weights. A learned $n$-by- $N$ matrix $\\mathbf{W}^{\\mathrm{LF}}$ down projects the $N$-by- $d$ dimensional keys and values along the timestep dimension, to an $n$-by- $d$ memory: $\\widetilde{\\mathbf{K}}^{\\mathrm{LF}}=\\mathbf{W}^{\\mathrm{LF}} \\mathbf{K}$, $\\widetilde{\\mathbf{V}}^{\\mathrm{LF}}=\\mathbf{W}^{\\mathrm{LF}} \\mathbf{V}$; they are then used for attention computation with Eq. 4. This yields a linear complexity in the input length. Linformer is an ABC instance with $\\phi_{i}^{\\mathrm{LF}}=\\mathbf{W}_{:, i}^{\\mathrm{LF}}$ ( $i$ th column), and in this sense, it learns a control vector for each position. Previous works have noted that Linformer cannot be efficiently applied in causal attention (Table 1 of Tay et al., 2020). Indeed, it is less straightforward to avoid mixing future with the past when projecting along the timestep dimension. ABC reveals that, in fact, Linformer is applicable in causal attention. Like all ABC models, it admits a linear-complexity recurrent computation (Eq. 5): $\\widetilde{\\mathbf{K}}_{t+1}^{\\mathrm{LF}}=\\widetilde{\\mathbf{K}}_{t}+\\phi_{t+1}^{\\mathrm{LF}} \\otimes \\mathbf{k}_{t+1}$. This confirms ABC's benefits: it reveals new insights about existing models and reassesses their applications and impact. Our experiments show that Linformer achieves competitive performance in machine translation. ### 3.2 Clustering-Based Attention\n\nImproving attention's efficiency with clustering has received an increasing amount of interest (Kitaev et al., 2020; Roy et al., 2020; Wang et al., 2020a, inter alia). ABC bears interesting connections to clustering-based methods. Here we discuss an approach that closely follows Vyas et al. (2020), except that it clusters keys and values instead of queries, and only attends to the centroids to reduce the effective context size. Formally, keys and values are grouped into $n<N$ clusters $\\left\\{\\widetilde{\\mathbf{k}}_{j}^{\\mathrm{CL}}\\right\\}_{j=1}^{n}$, $\\left\\{\\widetilde{\\mathbf{v}}_{j}^{\\mathrm{CL}}\\right\\}_{j=1}^{n}{ }^{3}$ Let an $N$-by- $n$ binary matrix M denote the cluster membership shared between keys and values. $M_{i, j}=1 \\mathrm{iff.} \\mathbf{k}_{i}$ is assigned to cluster $\\widetilde{\\mathbf{k}}_{j}^{\\mathrm{CL}}$ and $\\mathbf{v}_{i}$ to $\\widetilde{\\mathbf{v}}_{j}^{\\mathrm{CL}}$. The $j$ th centroid for the keys is\n\n$$\n\\widetilde{\\mathbf{k}}_{j}^{\\mathrm{CL}}=\\sum_{i=1}^{N} \\frac{M_{i, j}}{\\sum_{\\ell=1}^{N} M_{\\ell, j}} \\mathbf{k}_{i}\n$$\n\n[^3]likewise for the values. It then attends over the centroids using Eq. 4, with $\\widetilde{\\mathbf{K}}^{\\mathrm{CL}}=\\left[\\widetilde{\\mathbf{k}}_{1}^{\\mathrm{CL}}, \\ldots, \\widetilde{\\mathbf{k}}_{n}^{\\mathrm{CL}}\\right]^{\\top}=$\n\\[\n\n$$\n\\begin{aligned}\n\\sum_{j=1}^{n} \\mathbf{e}_{j} \\otimes \\widetilde{\\mathbf{k}}_{j}^{\\mathrm{CL}} & =\\sum_{j=1}^{n} \\mathbf{e}_{j} \\otimes \\sum_{i=1}^{N} \\frac{M_{i, j}}{\\sum_{\\ell=1}^{N} M_{\\ell, j}} \\mathbf{k}_{i} \\\\\n& =\\sum_{i=1}^{N}\\left(\\sum_{j=1}^{n} \\mathbf{e}_{j} \\frac{M_{i, j}}{\\sum_{\\ell=1}^{N} M_{\\ell, j}}\\right) \\otimes \\mathbf{k}_{i}\n\\end{aligned}\n$$\n\\]\n\nThe last line indicates that this model is an instance of ABC: $\\phi_{i}=\\sum_{j=1}^{n}\\left(M_{i, j} / \\sum_{\\ell=1}^{N} M_{\\ell, j}\\right) \\mathbf{e}_{j}$. The stack of centroids can be seen as the constant-size memory. Putting aside the clustering overhead (i.e., constructing M and computing centroids), it has a linear complexity in the sequence length. ### 3.3 Sliding-Window Attention\n\nIn some applications, being able to remove entries from the memory can be beneficial: clearing up older context frees slots for more recent ones, promoting a locality inductive bias. ABC offers the capability to do so, if augmented with an additional matrix multiplication. We use the sliding-window attention as an example. Attending to the most recent $n$ input tokens (Beltagy et al., 2020; Zaheer et al., 2020; Sukhbaatar et al., 2021, inter alia) can be seen as a firstin-first-out queue that \"pops\" out the oldest token while \"pushing\" in the most recent one: $\\widetilde{\\mathbf{K}}_{t}^{\\mathrm{WD}}=\\left[\\mathbf{k}_{t-n+1}, \\ldots, \\mathbf{k}_{t}\\right]^{\\top}$. The pop operation can be achieved by multiplying an $n$-by- $n$ upper shift matrix: $U_{i, j}=\\delta_{i+1, j}$, with $\\delta$ being the Kronecker delta (i.e., U has ones only on the superdiagonal and zeros elsewhere). Left-multiplying $\\mathbf{U}$ against $\\widetilde{\\mathbf{K}}_{t}^{\\mathrm{WD}}$ shifts its rows one position up, with zeros appearing in the last:\n\n$$\n\\begin{aligned}\n\\mathbf{U} \\widetilde{\\mathbf{K}}_{t}^{\\mathrm{WD}} & =\\mathbf{U}[\\underbrace{\\mathbf{k}_{t-n+1}, \\ldots, \\mathbf{k}_{t}}_{n}]^{\\top} \\\\\n& =[\\underbrace{\\mathbf{k}_{t-n+2}, \\ldots, \\mathbf{k}_{t-1}, \\mathbf{k}_{t}}_{n-1}, \\mathbf{0}]^{\\top} \\in \\mathbb{R}^{n \\times d} . \\end{aligned}\n$$\n\nThen the most recent token can be put into the slot freed up: $\\widetilde{\\mathbf{K}}_{t+1}^{\\mathrm{WD}}=\\mathbf{U} \\widetilde{\\mathbf{K}}_{t}^{\\mathrm{WD}}+\\mathbf{e}_{n} \\otimes \\mathbf{k}_{t+1} . \\mathbf{U}$ and $\\phi_{t}=\\mathbf{e}_{n}$ ensure a first-in-first-out queue. Dilated and stride convolution patterns (Beltagy et al., 2020) can be similarly recovered (\u00a7A.4). Recurrently multiplying $\\mathbf{U}$ simulates the discrete pop operation (Grefenstette et al., 2015; Joulin and Mikolov, 2015; Yogatama et al., 2018) in a differentiable way. This is reminiscent of recurrent neural networks, while in this case $\\mathbf{U}$ is never updated as\nparameters. It is exciting to explore learning $\\mathbf{U}$, but is beyond the scope of this work. Discussion. Besides the models discussed above, certain variants of Rae et al. (2020) and sparse attention patterns (local-to-global attention; Beltagy et al., 2020; Zaheer et al., 2020; Ainslie et al., 2020) can also be seen as instances of ABC (\u00a7A). ABC provides a unified perspective of them, and at the same time points out their limitations: their control strategies are context-agnostic. In response to this, in $\\S 4$ we propose to learn a contextualized strategy from data. Table 1 analyzes various ABC models, and Table 2 details their complexity. ## 4 Learned Memory Control\n\nThe ABC abstraction connects several existing approaches that would otherwise seem distinct. This inspires the design of new architectures. We hypothesize that learning a contextualized strategy can achieve better performance. This section introduces $A_{B C M L P}$. It parameterizes $\\phi$ with a singlelayer multi-layer perceptron (MLP) that takes as input the token's representation $\\mathbf{x}_{i}$, and determines which slots to write it into and how much. $$\n\\boldsymbol{\\alpha}_{i}=\\exp \\left(\\mathbf{W}_{\\boldsymbol{\\phi}} \\mathbf{x}_{i}\\right), \\quad \\boldsymbol{\\phi}_{i}=\\boldsymbol{\\alpha}_{i} / \\sum_{j=1}^{N} \\boldsymbol{\\alpha}_{j}\n$$\n\nMatrix $\\mathbf{W}_{\\phi}$ is learned. $\\exp$ is an elementwise activation function. The motivation is to allow for storing a \"fractional\" (but never negative) amount of input into the memory. ${ }^{4}$ Using a non-negative activation, however, has a drawback: the scales of $\\sum_{i} \\phi_{i} \\otimes \\mathbf{k}_{i}$ and $\\sum_{i} \\phi_{i} \\otimes \\mathbf{v}_{i}$ would grow with the sequence lengths, making training less stable. To overcome this, we divide $\\boldsymbol{\\alpha}_{i}$ vectors by their sum. This functions as normalization and aims to offset the impact of varying sequence lengths. ${ }^{5}$ It admits the recurrent computation graph as in Eq. 5, and has a linear complexity in the sequence length. A key design choice of $\\mathrm{ABC}_{\\text {MLP }}$ is that its $\\phi_{i}$ depends only on current input $\\mathbf{x}_{i}$. This helps (1) keep the recurrent computation efficient in practice (Lei et al., 2018), and (2) make it applicable\n\n[^4]in not only encoder self-attention and cross attention, but also causal attention.",
    "abc-1": "Concurrently to this work, Goyal et al. (2021) and Ma et al. (2021) also proposed methods to learn contextualized control. They compute $\\phi_{i}$ from previous layer's memory, revealing the full sequence to the control vectors. As a result, these two approaches are unsuitable for causal attention. ${ }^{6}$\n$\\mathrm{ABC}_{\\text {MLP }}$, as other ABC models, can be used as a drop-in replacement for the canonical softmax attention, and we apply its multihead variant in transformers. With proper parameter sharing, the number of additional parameters $\\mathrm{ABC}_{\\text {MLP }}$ incurs is small: inspired by Wang et al. (2020b), we tie $\\phi$-MLP's parameters across different layers, which adds less than $1 \\%$ parameters to the models. ABC ${ }_{\\text {MLP }}$ : context-agnostic then contextdependent attention. We now dissect $\\mathrm{ABC}_{\\text {MLP }}$ and show that it can be seen as a cascade of two attention mechanisms: one with a learned context-agnostic \"pseudo query\" followed by one with a context-dependent query. Our analysis starts with a one-dimensional example; the conclusion generalizes to higher-dimensional cases. Example 1. Consider $\\mathrm{ABC}_{\\text {MLP }}$ with a single memory $\\operatorname{slot}(n=1)$. It is parameterized with a learned vector $\\mathbf{w}_{\\phi}$, and $\\phi_{i}=\\exp \\left(\\mathbf{w}_{\\phi} \\cdot \\mathbf{x}_{i}\\right) / \\sum_{j=1}^{N} \\exp \\left(\\mathbf{w}_{\\phi}\\right.$.",
    "abc-2": "$\\left.\\mathbf{x}_{j}\\right)$. Since $\\phi_{i}$ is a scalar here, $\\phi_{i} \\otimes \\mathbf{k}_{i}=\\phi_{i} \\mathbf{k}_{i}^{\\top}$. $$\n\\begin{aligned}\n\\widetilde{\\mathbf{K}}^{\\top} & =\\sum_{i=1}^{N}\\left(\\phi_{i} \\otimes \\mathbf{k}_{i}\\right)^{\\top} \\\\\n& =\\sum_{i=1}^{N} \\frac{\\exp \\left(\\mathbf{w}_{\\phi} \\cdot \\mathbf{x}_{i}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{w}_{\\phi} \\cdot \\mathbf{x}_{j}\\right)} \\mathbf{k}_{i} \\\\\n& =\\operatorname{attn}\\left(\\mathbf{w}_{\\phi},\\left\\{\\mathbf{x}_{i}\\right\\}_{i=1}^{N},\\left\\{\\mathbf{k}_{i}\\right\\}_{i=1}^{N}\\right)\n\\end{aligned}\n$$\n\nIn other words, $\\widetilde{\\mathbf{K}}$ uses $\\mathbf{w}_{\\phi}$ as a \"pseudo-query\" to attend to $\\left\\{\\mathbf{x}_{i}\\right\\}$ and $\\left\\{\\mathbf{k}_{i}\\right\\}$. Likewise, $\\tilde{\\mathbf{V}}^{\\top}=$ $\\operatorname{attn}\\left(\\mathbf{w}_{\\phi},\\left\\{\\mathbf{x}_{i}\\right\\}_{i=1}^{N},\\left\\{\\mathbf{v}_{i}\\right\\}_{i=1}^{N}\\right)$. Despite its similarity to the standard softmax attention, Example 1 has a more efficient linear complexity in sequence lengths. $\\mathbf{w}_{\\phi}$ 's being context-independent is the key to the savings. Table 2 details its complexity. Example 1's conclusion generalizes to higherdimensional cases: the $j$ th dimension of $\\left\\{\\phi_{i}\\right\\}$ attends to $\\left\\{\\mathbf{x}_{i}\\right\\}$ and $\\left\\{\\mathbf{k}_{i}\\right\\}$ using the $j$ th row of $\\mathbf{W}_{\\phi}$ as the context-independent pseudo-query; $n$ such attention mechanisms run in parallel, stacking the\n\n[^5]| Model | Section | $\\phi_{t}$ | Mem. Control |\n| :--- | :--- | :--- | :--- |\n| Sliding-window | $\\S 3.3$ | $\\mathbf{e}_{n}$ | $\\widetilde{\\mathbf{K}}_{t+1}=\\mathbf{U} \\widetilde{\\mathbf{K}}_{t}+\\phi_{t+1} \\otimes \\mathbf{k}_{t+1}$ |\n| Linformer | $\\S 3.1$ | $\\mathbf{W}_{:, t}^{\\mathrm{LF}}$ |  |\n| L2G Pattern | $\\S \\mathrm{A} .1$ | $\\mathbf{e}_{i}$ if $\\mathbf{x}_{t}$ is the $i$ th global token |  |\n| ABC $_{\\text {RD }}$ | $\\S \\mathrm{A} .2$ | $\\mathbf{e}_{i t}$, where $i_{t} \\sim$ unif $\\{1, n\\}$ | $\\widetilde{\\mathbf{K}}_{t+1}=\\widetilde{\\mathbf{K}}_{t}+\\phi_{t+1} \\otimes \\mathbf{k}_{t+1}$ |\n| Comp. Trans. | $\\S \\mathrm{A} .3$ | $\\mathbf{e}_{\\lfloor n t / N\\rfloor}$ |  |\n| Clustering | $\\S 3.2$ | $\\sum_{j=1}^{n}\\left(M_{t, j} / \\sum_{\\ell=1}^{N} M_{\\ell, j}\\right) \\mathbf{e}_{j}$ |  |\n| ABC $_{\\text {MLP }}$ | $\\S 4$ | $\\exp \\left(\\mathbf{W}_{\\phi} \\mathbf{x}_{t}\\right) / \\sum_{i=1}^{t} \\exp \\left(\\mathbf{W}_{\\phi} \\mathbf{x}_{t}\\right)$ |  |\n\nTable 1: A comparison of different ABC models. $N$ denotes the sequence length, and $n$ the memory size. $\\phi_{t}$ denotes the memory control vector for $\\mathbf{k}_{t}$ and $\\mathbf{v}_{t}$, and unif is the discrete uniform distribution. |  | Time Complexity |  |  |  | Space Complexity |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Model | Mem. | Per Query | Overall |  | Mem. | Per Query | Overall |\n| Softmax Attention | - | $\\mathcal{O}(N)$ | $\\mathcal{O}\\left(N^{2}\\right)$ |  | - | $\\mathcal{O}(N)$ | $\\mathcal{O}\\left(N^{2}\\right)$ |\n| ABC | $\\mathcal{O}(N)$ | $\\mathcal{O}(n)$ | $\\mathcal{O}(n N)$ | $\\mathcal{O}(n)$ | $\\mathcal{O}(n)$ | $\\mathcal{O}(n N)$ |  |\n\nTable 2: ABC's time and space complexity in sequence length against the softmax attention's. \"Mem.\" indicates the time and space needed for calculating and storing memory $\\widetilde{\\mathbf{K}}, \\widetilde{\\mathbf{V}} . N$ denotes the sequence length, and $n$ the memory size. The time complexity analysis assumes that the softmax attention cannot be parallelized across the queries. In practice, this is common in autoregressive decoding or for long sequences where the accelerators (e.g., GPUs) do not have enough threads to fully parallelize softmax attention's computation across different queries. results into $n$-by- $d$ memory $\\widetilde{\\mathbf{K}}$ and $\\widetilde{\\mathbf{V}}$. Intuitively, it is the \"real queries\" $\\left\\{\\mathbf{q}_{i}\\right\\}$ that encode \"what information is useful for the prediction task.\" Without access to them, $\\mathrm{ABC}_{\\text {MLP }}$ summarizes the input for $n$ times using different pseudo-queries, aiming to preserve enough information in the memory for onward computation. The attention output is calculated with the context-dependent real queries using Eq.",
    "abc-3": "4. \u00a7B. 2 presents a detailed derivation. Connections to other prior works. Although starting from distinct motivations, $\\mathrm{ABC}_{\\text {MLP }}$ closely relates to hierarchical attention (HA; Yang et al., 2016). HA summarizes the context into higherlevel representations with a cascade of attention mechanisms, e.g., words to sentences, and then to documents. $\\mathrm{ABC}_{\\text {MLP }}$ applies two types of attention. The first learns context-agnostic pseudo-queries and attends to the same sequence for $n$ times in parallel, while the second retrieves from the memory with real queries. HA, in contrast, summarizes non-overlapping segments at each level. The learned pseudo-queries closely relate to the inducing point method in set attention (ISA; Lee et al., 2019). ISA applies a non-linear feedforward network between a cascade of two attention mod- ules. This precludes the outer-product memory computation and efficient recurrences in $A B C$. Another line of work \"linearizes\" attention through kernel tricks and also applies bounded memory: their feature map dimensions are analogous to memory sizes. They substitute the softmax with approximations (Peng et al., 2021; Choromanski et al., 2021), heuristically designed (Katharopoulos et al., 2020; Schlag et al., 2021), or learned (Kasai et al., 2021b) functions. $\\mathrm{ABC}_{\\text {MLP }}$ keeps the softmax, but over a smaller constant-sized context. This can be useful in practice: (1) ABC provides a unified perspective of several efficient attention methods, allowing for borrowing from existing wisdom to design new architectures; (2) it draws a close analogy to the canonical softmax attention, and is better-suited as its drop-in substitute in various application settings, as we will show in the experiments; (3) empirically, we find that $\\mathrm{ABC}_{\\text {MLP }}$ can get away with a much smaller memory size to retain the accuracy.",
    "abc-4": "Peng et al. (2021) and Schlag et al. (2021) use gating to promote recency bias. The same technique is equally applicable in $A B C$ models. The learned contextualized memory control is reminiscent of the content-based addressing in neu-\nral Turing machines (NTM; Graves et al., 2014). $\\mathrm{ABC}_{\\text {MLP }}$ computes the control vectors $\\left\\{\\phi_{i}\\right\\}$ as a function of the input, but not of the memory as in NTM. This ensures that the control vectors at different timesteps can be computed in parallel, improving the time efficiency in practice (Lei et al., 2018; Peng et al., 2018). Analogies between memory and neural architectures are also made by other previous works (Hochreiter and Schmidhuber, 1997; Weston et al., 2015; Le et al., 2020, inter alia). ## 5 Experiments\n\nWe evaluate ABC models on language modeling (\u00a75.1), sentence-level and document-level machine translation (\u00a75.2), and masked language model finetuning (\u00a75.3). Dataset statistics and implementation details are summarized in \u00a7C. ### 5.1 Language Modeling\n\nSetting. We experiment with WikiText-103, sampled text from English Wikipedia (Merity et al., 2017). The BASE model with standard softmax attention is the strong transformer-based language model by Baevski and Auli (2019). We compare the following ABC variants, which build on BASE, but replace the softmax attention with linearcomplexity bounded-memory attention alternatives while keeping other components the same. - $\\mathrm{ABC}_{\\text {MLP }}$, as described in $\\S 4$, learns a contextualized exp-MLP as the $\\phi$ function. - Linformer (\u00a73.1; Wang et al., 2020b). - $\\mathrm{ABC}_{\\mathrm{RD}}$ stores each token in a randomly-selected memory slot with $\\phi_{t}=\\mathbf{e}_{i_{t}}$. $i_{t}$ is uniformly drawn from $\\{1, \\ldots, n\\}$ at each time step. This helps us quantify the differences between random and learned bounded-memory controls. We consider two model size settings:\n- 16 layers (Baevski and Auli, 2019). All models have around $\\sim 242 \\mathrm{M}$ parameters. They train with 512-token segments, and evaluate with 0 or 480 context sizes: a 0 - or 480 - length prefix precedes each evaluation segment. - 32 layers (Kasai et al., 2021b). All models have $\\sim 484 \\mathrm{M}$ parameters. This setting applies layer dropout (Fan et al., 2020), and evaluates with a 256 context size. It aims to compare $\\mathrm{ABC}_{\\text {MLP }}$ to several kernel-based efficient attention variants: ELU (Katharopoulos et al., 2020), RFA (Peng et al., 2021), and T2R (Kasai et al., 2021b). Results. Table 3a compares ABC variants using Baevski and Auli (2019)'s 16-layer setting. Among\n\n|  |  | Dev. |  |  | Test |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Model | $n$ | $\\mathbf{0}$ | $\\mathbf{4 8 0}$ |  | $\\mathbf{0}$ | $\\mathbf{4 8 0}$ |\n| BASE | - | 19.8 | 18.4 |  | 20.5 | 19.0 |\n| Linformer | 64 | 26.5 | 27.1 |  | 27.2 | 30.7 |\n| ABC $_{\\text {RD }}$ | 64 | 23.2 | 22.3 |  | 24.0 | 23.1 |\n| ABC $_{\\text {MLP }}$ | 32 | 21.2 | 19.7 |  | 21.9 | 20.5 |\n| ABC $_{\\text {MLP }}$ | 64 | $\\mathbf{2 0 .",
    "abc-5": "4}$ | $\\mathbf{1 8 .",
    "abc-6": "9}$ |  | $\\mathbf{2 1 . 1}$ | $\\mathbf{1 9 . 5}$ |\n\n(a) 16-layer setting. 0/480 indicate evaluation context sizes. | Model | $n$ | Dev. | Test |\n| :--- | :---: | :---: | :---: |\n| $\\dagger$ BASE | - | 17.9 | 18.5 |\n| $\\dagger \\mathrm{ELU}$ | 128 | 22.0 | 22.8 |\n| $\\dagger \\mathrm{RFA}$ | 32 | 20.4 | 21.3 |\n| $\\dagger \\mathrm{T} 2 \\mathrm{R}$ | 32 | 20.1 | 20.8 |\n| $\\mathrm{ABC}_{\\text {MLP }}$ | 32 | $\\mathbf{1 9 . 2}$ | $\\mathbf{1 9 . 9}$ |\n\n(b) 32-layer setting. A 256-length context is used at evaluation time.",
    "abc-7": "$\\dagger$ numbers are due to Kasai et al. (2021b). Table 3: WikiText-103 language modeling perplexity (lower is better). $n$ denotes the memory size. Bold numbers perform the best among linear-complexity models. ABC models, $\\mathrm{ABC}_{\\text {MLP }}$ achieves the best performance for both context sizes. With a memory size $n=64$, ABC $_{\\text {MLP }}$ outperforms both Linformer and $\\mathrm{ABC}_{\\mathrm{RD}}$ by more than 2.9 test perplexity; and the gap is larger with the longer 480-length context: more than 3.6 test perplexity. $\\mathrm{ABC}_{\\mathrm{MLP}}-32$ outperforms its larger-memory $A B C$ counterparts by more than 2.1 test perplexity. These results confirm ABC MLP's advantages of using a contextualized strategy. Surprisingly, Linformer underperforms $\\mathrm{ABC}_{\\mathrm{RD}}$, and its performance drops with the larger 480-length context window. This suggests that, while successful in text encoding, Linformer's position-based strategy is a suboptimal design choice for causal attention, at least for long context. All ABC models underperform the BASE, with $\\mathrm{ABC}_{\\mathrm{MLP}}-64$ having the smallest gap of 0.5 perplexity. $\\mathrm{ABC}_{\\mathrm{MLP}}-32$ outperforms kernel-based methods by more than 0.9 test perplexity, using Kasai et al. (2021b)'s 32-layer setting (Table 3b). ### 5.2 Machine Translation\n\nDatasets. To assess their performance over various output lengths, we compare ABC models on sentence- and document- level machine translation. | Model | Cross $n$ | Causal $n$ | BLEU |\n| :--- | :---: | :---: | :---: |\n| BASE | - | - | 27.2 |\n| ABC $_{\\text {RD }}$ | 32 | 32 | 25.7 |\n| ABC $_{\\text {RD }}$ | 64 | 64 | 26.2 |\n| Linformer | 32 | 32 | 26.6 |\n| Linformer | 64 | 64 | 26.7 |\n| ABC $_{\\text {MLP }}$ | 32 | 8 | 27.1 |\n| ABC $_{\\text {MLP }}$ | 32 | 32 | $\\mathbf{2 7 . 3}$ |\n\n(a) Bolded number outperforms BASE. | Model | Cross $n$ | Causal $n$ | BLEU |\n| :--- | :---: | :---: | :---: |\n| BASE | - | - | 39.9 |\n| Linformer | 128 | 64 | - |\n| ABC $_{\\text {RD }}$ | 128 | 64 | 38.6 |\n| $\\mathrm{ABC}_{\\text {MLP }}$ | 128 | 64 | $\\mathbf{3 9 . 7}$ |\n\n(b) Linformer fails to converge even with multiple random seeds. Bold number performs the best among ABC models. Table 4: Machine translation test SacreBLEU. Left: sentence-level translation with WMT14 EN-DE; right: document-level translation with IWSLT14 ES-EN. - Sentence-level translation with WMT14 EN-DE (Bojar et al., 2014). The preprocessing and data splits follow Vaswani et al. (2017). - Document-level translation with IWSLT14 ESEN (Cettolo et al., 2014). We use Miculicich et al. (2018)'s data splits and preprocessing. Following standard practice (Voita et al., 2019), a 4 -sentence sliding window is used to create the dataset, i.e., each instance has 4 sentences. Setting. We compare ABC variants as in $\\S 5.1$. $\\S \\mathrm{C} .2$ further compares to the clustering-based ( $\\S 3.2$ ) and sliding-window ( $\\S 3.3$ ) ABC variants. The BASE model they build on is our implementation of transformer-base (Vaswani et al., 2017). $A B C$ variants replace decoder cross attention and causal attention with bounded-memory attention, while keeping softmax attention for the encoder, since its overhead is much less significant (Kasai et al., 2021a); other components are kept the same. \u00a7C. 2 studies a model that replaces all softmax attention with $\\mathrm{ABC}_{\\text {MlP }}$. It performs on par with BASE, confirming $\\mathrm{ABC}_{\\text {MLP }}$ 's broad applicability in various application scenarios.",
    "abc-8": "We evaluate with SacreBLEU (Post, 2018). Results. Table 4 a summarizes sentence-level machine translation results on the WMT14 EN-DE test set. Overall $\\mathrm{ABC}_{\\text {MLP }}$ performs on par with BASE, with either 32-32 cross-causal memory sizes or 328. Even with smaller memory sizes, it outperforms other $A B C$ variants by more than 1.1 BLEU. Differently from the trend in the language modeling experiment (\\$5.1), Linformer outperforms $\\mathrm{ABC}_{\\mathrm{RD}}$ by more than 0.5 BLEU. We attribute this to the smaller sequence lengths of this dataset. $\\mathrm{ABC}_{\\text {MLP }}$ outperforms other ABC models by more than 0.4 BLEU, even with smaller memory sizes. The trend is similar on document-level translation with IWSLT14 ES-EN (Table 4b), except that $\\mathrm{ABC}_{\\text {MLP }}$ slightly underperforms BASE by 0.2 BLEU. This suggests that even with longer sequences, $\\mathrm{ABC}_{\\text {MLP }}$ is effective despite its bounded memory size. Linformer fails to converge even with multiple random seeds, suggesting the limitations of its purely position-based strategy in tasks involving decoding varying-length text. ### 5.3 Masked Language Model Finetuning\n\nSetting. We compare the ABC variants as in $\\S 5.1$. It is interesting to pretrain $A B C$ from scratch, but we lack the resources to do so. Instead, we warm-start from a pretrained RoBERTa-base (Liu et al., 2019) trained with the softmax transformer, swap its attention with $A B C$ variants, and continue pretraining with the masked language modeling (MLM) objective on a concatenation of BookCorpus (Zhu et al., 2015), English Wikipedia, OpenWebText (Gokaslan and Cohen, 2019), and RealNews (Zellers et al., 2019). ${ }^{7}$ Then the models are finetuned and evaluated on downstream classification datasets from the the GLUE benchbark (Wang et al., 2019). This is an appealing setting, since it avoids reinvesting the huge amounts of resources already put into pretraining. ${ }^{8}$\n\nResults. Table 5 compares downstream text classification performance. BASE indicates a baseline that continues pretraining RoBERTa-base on our data. ${ }^{9}$ Following standard practice, we report development accuracy. Linformer achieves competitive\n\n[^6]| Model | $n$ | MNLI | QNLI | QQP | SST | Avg. |\n| :--- | ---: | ---: | ---: | ---: | ---: | ---: |\n| BASE | - | 87.2 | 92.4 | 91.7 | 94.3 | 91.4 |\n| Linformer | 64 | 85.3 | 91.8 | 90.8 | 92.4 | 90.1 |\n| Linformer | 128 | 86.1 | 91.9 | 91.4 | 93.7 | 90.8 |\n| ABC $_{\\text {MLP }}$ | 64 | 85.6 | 91.8 | $\\underline{91.7}$ | 93.8 | 90.7 |\n| ABC $_{\\text {MLP }}$ | 128 | $\\mathbf{8 7 .",
    "abc-9": "1}$ | $\\underline{\\mathbf{9 2 . 6}}$ | $\\underline{\\mathbf{9 1 . 8}}$ | $\\underline{\\mathbf{9 4 . 4}}$ | $\\underline{\\mathbf{9 1 . 5}}$ |\n\nTable 5: Text classification development set accuracy. All models continue pretraining RoBERTa-base on our data with the MLM objective. Bold numbers perform the best among ABC models, and underlined ones perform on par with or better than BASE. performance, aligned with Wang et al. (2020b)'s results. $\\mathrm{ABC}_{\\text {MLP }}$ outperforms Linformer, and performs on par with or better than BASE, affirming the benefits of using contextualized memory organization in MLM. $\\mathrm{ABC}_{\\mathrm{RD}}$ fails to converge in continued pretraining even with multiple seeds. Based on the above results, we think $\\mathrm{ABC}_{\\text {MLP }}$ can achieve competitive performance when pretrained from scratch, just as Linformer does (Wang et al., 2020b). Further empirical exploration is beyond our budget and left for future work. ## 6 Analysis\n\nDecoding efficiency over varying sequence lengths. ABC's efficiency gains can be more prominent for long sequences. We study ABC MLP's decoding overhead with varying sequence lengths. Following Kasai et al. (2021b), we consider a sequence-to-sequence generation experiment. Three linear-complexity models are compared: RFA (with $256 / 128$ cross/causal memory sizes; Peng et al., 2021), T2R (32/4; Kasai et al., 2021b), and $\\mathrm{ABC}_{\\text {MLP }}$ (32/8). The sizes are chosen to maximize efficiency without accuracy drop. T2R needs to be finetuned from a pretrained transformer to match its performance, while others don't. All linear-time models achieve consistent decoding speed for different lengths (Figure 1a), substantially outpacing the softmax attention baseline, especially for long sequences. In particular, $\\mathrm{ABC}_{\\text {MLP }}$ decodes $\\sim 1.25$ times faster than RFA, another competitive model that can match transformer's accuracy without a warm start from a pretrained model. This can be attributed to the fact that ABCMLP achieves similar accuracy with a much smaller memory. T2R's memory sizes are similar to ABCMLP's, but it decodes about $20 \\%$ faster. ![](https://cdn.mathpix.com/cropped/2024_09_12_bed2b9bb53913a589750g-08.jpg?height=900&width=593&top_left_y=225&top_left_x=1106)\n\nFigure 1: Sequence-to-sequence decoding speed (top) and memory consumption (bottom) varying sequence lengths. Greedy decoding is used, with batch size 16. This is because it does not compute the softmax when calculating attention output, while $\\mathrm{ABC}_{\\text {MLP }}$ does (Eq. 4). These results show that $\\mathrm{ABC}_{\\text {MLP }}$ is an appealing modeling choice for decoding tasks, especially when training from scratch is desired. $\\mathrm{ABC}_{\\text {MLP }}$ also achieves significant savings in terms of memory overhead (Figure 1b). $\\mathrm{ABC}_{\\text {MLP }}$, RFA, and T2R's curves are similar. Text encoding efficiency. We compare the efficiency of $A_{B C}$ MLP against softmax attention and Linformer when used as text encoders. The models' sizes mirror those in the MLM experiment (\u00a75.3). Table 6 summarizes inference time and memory overhead with 512-length inputs, batch size 16. Both $\\mathrm{ABC}_{\\text {MLP }}$ and Linformer achieve inference speed gains and memory savings over BASE. Linformer is faster, since its linear projection is cheaper to compute than $\\mathrm{ABC}_{\\text {MLP's MLP. Infer- }}$ ence speed is measured on the same V100 GPU. The trend in memory overhead is similar. Although $\\mathrm{ABC}_{\\text {MLP }}$ slightly underperforms Linformer in terms of inference speed, it can be a more appealing architectural choice in practice: in all of our 5 experiments, $\\mathrm{ABCMLP}_{\\text {outp }}$ outperforms other ABC models in accuracy. Linformer, in contrast, fails to converge or yields sub-optimal performance on some tasks. This confirms its flexibility and ap-\n\n|  | BASE | Linformer |  |  | ABC $_{\\text {MLP }}$ |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| $n$ | - | 64 | 128 |  | 64 | 128 |\n| Speed | $1.0 \\times$ | $1.7 \\times$ | $1.5 \\times$ |  | $1.5 \\times$ | $1.3 \\times$ |\n| Memory | $1.0 \\times$ | $0.5 \\times$ | $0.6 \\times$ |  | $0.5 \\times$ | $0.6 \\times$ |\n\nTable 6: Text encoding inference speed (higher is better) and memory (lower is better). Inputs are text segments with 512 tokens and batch size 16. | ![](https://cdn.mathpix.com/cropped/2024_09_12_bed2b9bb53913a589750g-09.jpg?height=100&width=175&top_left_y=711&top_left_x=317) |  | Cross $n$ |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 8 | 16 | 32 | 64 |\n| \u6c14 | 8 | 24.7 | 25.2 | 25.6 | 25.5 |\n|  | 16 | - | 25.4 | 25.7 | 25.6 |\n|  | 32 | - | - | 25.7 | 25.8 |\n|  | 64 | - | - | - | 25.8 |\n\nTable 7: ABC MLP's SacreBLEU on WMT14 EN-DE development data varying memory sizes. plicability in various settings. Memory size's impact on accuracy. Practically, one may want to minimize the memory size to improve efficiency. We use the WMT14 EN-DE experiment to investigate how memory size affects accuracy. Using the \u00a75.2's setup, we vary ABCMLP's cross and causal attention memory sizes and compare their translation quality on the development data. They are selected from $\\{8,16,32,64\\}$, with cross attention's equal to or larger than causal's: cross attention is more important than causal attention in machine translation (Michel et al., 2019). Our results (Table 7) align with this observation: when cross attention memory is large enough, reducing causal attention memory size from 64 to 8 has a minor 0.3 BLEU drop. Surprisingly, $\\mathrm{ABC}_{\\text {MLP }}$ with $8-8$ sized cross-causal memory is only 1.1 BLEU behind the best-performing configuration. ## 7 Conclusion\n\nWe presented attention with bounded-memory control ( ABC ). It provides a unified perspective of several recently-proposed models, and shows that they vary in the organization of the bounded memory. ABC reveals new insights into established methods and inspires new architectures. We proposed $\\mathrm{ABC}_{\\text {MLP }}$, a particular instance of ABC that learns a contextualized memory control. On language modeling, machine translation, and masked language model finetuning, $\\mathrm{ABC}_{\\text {MLP }}$ outperforms previous ABC models. Compared to the strong transformer baseline, $\\mathrm{ABC}_{\\text {MLP }}$ achieves substantial efficiency improvements with no or negligible accuracy loss. ## Acknowledgments\n\nWe would like to thank the ARK group at the University of Washington for their helpful feedback, and the anonymous reviewers for their thoughtful comments. This work was supported in part by NSF grant 2113530 and a Google Fellowship. Nikolaos Pappas was supported by the Swiss National Science Foundation grant P400P2_183911. ## References\n\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers.",
    "abc-10": "In Proc. of EMNLP. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In Proc. of ICLR. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. Ond\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proc. of WMT. Mauro Cettolo, Jan Niehues, Sebastian St\u00fcker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th IWSLT evaluation campaign. In Proc. of IWSLT. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, \u0141ukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2021. Rethinking attention with performers. In Proc. of ICLR. Korn\u00e9l Csernai. 2017, accessed September 1, 2020. First Quora Dataset Release: Question Pairs.",
    "abc-11": "Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing transformer depth on demand with structured dropout. In Proc. of ICLR. Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. http://Skylion007.github. io/OpenWebTextCorpus. Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, and Yoshua Bengio. 2021. Coordination among neural modules through a shared global workspace. Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. 2015. Learning to transduce with unbounded memory. In Proc. of NeurIPS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation. Armand Joulin and Tom\u00e1s Mikolov. 2015. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proc. of NeurIPS. Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A.",
    "abc-12": "Smith. 2021a. Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation.",
    "abc-13": "In Proc. of ICLR. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A.",
    "abc-14": "Smith. 2021b. Finetuning pretrained transformers into RNNs. In Proc. of EMNLP. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. 2020. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Proc. of ICML. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer.",
    "abc-15": "In Proc. of ICLR. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation.",
    "abc-16": "In Proc. of ACL. Hung Le, Truyen Tran, and Svetha Venkatesh. 2020. Self-attentive associative memory. In Proc. of ICML. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. 2019. Set transformer: A framework for attention-based permutation-invariant neural networks.",
    "abc-17": "In Proc. of ICML. Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. 2018. Simple recurrent units for highly parallelizable recurrence.",
    "abc-18": "In Proc. of EMNLP. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, \u0141ukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by summarizing long sequences. In Proc. of ICLR. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach.",
    "abc-19": "Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. Luna: Linear unified nested attention.",
    "abc-20": "In Proc. of NeurIPS. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In Proc. of ICLR. Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In Proc. of NeurIPS. Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Document-level neural machine translation with hierarchical attention networks. In Proc. of EMNLP. Sebastian Nagel. 2016. News dataset available. https://commoncrawl.org/2016/10/ news-dataset-available/. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, \u0141ukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer. In Proc. of ICML. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random feature attention. In Proc. of ICLR. Hao Peng, Roy Schwartz, Sam Thomson, and Noah A.",
    "abc-21": "Smith. 2018. Rational recurrences. In Proc. of EMNLP. Matt Post. 2018. A call for clarity in reporting BLEU scores.",
    "abc-22": "In Proc.",
    "abc-23": "of WMT. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling.",
    "abc-24": "In Proc. of ICLR. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text.",
    "abc-25": "In Proc. of EMNLP. Aurko Roy, Mohammad Taghi Saffar, David Grangier, and Ashish Vaswani. 2020. Efficient content-based sparse attention with routing transformers. TACL. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers.",
    "abc-26": "In Proc. of ICML. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proc. of ACL. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank.",
    "abc-27": "In Proc. of EMNLP. Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. 2021. Not all memories are created equal: Learning to forget by expiring. In Proc. of ICML. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient transformers: A survey. Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. of NeurIPS. Elena Voita, Rico Sennrich, and Ivan Titov. 2019. When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion. In Proc. of $A C L$. Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Proc. of NeurIPS. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. 2020a. Cluster-Former: Clustering-based sparse transformer for long-range dependency encoding. Findings of ACL. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Selfattention with linear complexity. Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory networks. In Proc. of ICLR. Adina Williams, Nikita Nangia, and Samuel R.",
    "abc-28": "Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference.",
    "abc-29": "In Proc. of NAACL. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification.",
    "abc-30": "In Proc. of NAACL. Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom. 2018. Memory architectures in recurrent neural network language models. In Proc. of ICLR. Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. In Proc. of NeurIPS. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Proc. of NeurIPS. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.",
    "abc-31": "In Proc. of ICCV. ## Appendices\n\n## A Other Abc Models\n\n## A. 1 Sparse Local-to-global Attention\n\nIt sparsifies attention pattern to reduce the number of tokens that are attended to (Beltagy et al., 2020; Zaheer et al., 2020, inter alia). All queries attend to a subset of $n<N$ \"global tokens,\" while ignoring others. Therefore the effective context size is reduced to $n$. The global tokens are usually preselected by positions according to some heuristics. Local-to-global attention is an instance of ABC: it can be recovered by letting $\\boldsymbol{\\phi}_{t}=\\mathbf{e}_{i}$ if $x_{t}$ is the $i$ th global token ( $i=1, \\ldots, n)$, and the zero vectors for others. ## A. 2 Random Memory Control\n\nAs a baseline, $\\mathrm{ABC}_{\\mathrm{RD}}$ stores each token in a randomly-selected memory slot. This is achieved by letting $\\phi_{t}=\\mathbf{e}_{i_{t}}$, where $i_{t}$ is uniformly drawn from $\\{1, \\ldots, n\\}$ for each $t$. It is designed as a baseline to $\\mathrm{ABC}_{\\text {MLP }}$ and Linformer to quantify the differences between random and learned boundedmemory control. Random sparse attention patterns are explored by Zaheer et al. (2020), where a subset of $n<N$ tokens are randomly selected to be attended to by all tokens. $\\mathrm{ABC}_{\\mathrm{RD}}$ is different, and it attends to all tokens, but randomly \"squash\" them into an $n$-slot memory. ## A. 3 Compressive Transformer with Mean Pooling\n\nThe compressive transformer (Rae et al., 2020) explores various ways to \"squash\" long context into smaller and more compact representations. It achieves state-of-the-art performance on several language modeling benchmarks. We show that at least the mean-pooling variant of the compressive transformer can be seen as an ABC instance. The mean-pooling variant of the compressive transformer compresses the context by\n\n$$\n\\begin{aligned}\n\\mathbf{K}= & {\\left[\\mathbf{k}_{1}, \\ldots, \\mathbf{k}_{N}\\right]^{\\top} \\in \\mathbb{R}^{N \\times d} } \\\\\n\\rightarrow \\widetilde{\\mathbf{K}}= & {[\\underbrace{\\left(\\mathbf{k}_{1}+\\cdots+\\mathbf{k}_{c}\\right)}_{c} / c} \\\\\n& \\underbrace{\\left(\\mathbf{k}_{c+1}+\\cdots+\\mathbf{k}_{2 c}\\right)}_{c} / c \\ldots \\\\\n& \\underbrace{\\left(\\mathbf{k}_{N-c+1}+\\cdots+\\mathbf{k}_{N}\\right)}_{c} / c]^{\\top} \\in \\mathbb{R}^{n \\times d} .",
    "abc-32": "\\end{aligned}\n$$\n\nwhere $c=N / n$ is the compression ratio. Here $N \\bmod n=0$ is assumed, since otherwise the sequence can be padded to. The above model is an ABC instance by letting\n\n$$\n\\phi_{i}=\\mathbf{e}_{\\lfloor(i-1) / c\\rfloor+1} / c\n$$\n\n## A. 4 Dilated Convolution Attention Patterns\n\nThe dilated attention pattern is similar to the sliding window attention and only considers the context within a predefined window. It differs in that it attends to every other token:\n\n$$\n\\widetilde{\\mathbf{K}}_{t}=\\left[\\mathbf{k}_{t-2 n+2}, \\mathbf{k}_{t-2 n+4}, \\ldots, \\mathbf{k}_{t-2}, \\mathbf{k}_{t}\\right]^{\\top}\n$$\n\nIt can be simulated with two separate queues $\\widetilde{\\mathbf{K}}^{\\text {odd }}$ and $\\widetilde{\\mathbf{K}}^{\\text {even }}$ :\n\n$$\n\\begin{aligned}\n\\widetilde{\\mathbf{K}}_{t}^{\\text {odd }} & = \\begin{cases}\\mathbf{U} \\widetilde{\\mathbf{K}}_{t-1}^{\\text {odd }}+\\mathbf{e}_{n} \\otimes \\mathbf{k}_{t}, & \\text { if } t \\text { is odd } \\\\\n\\widetilde{\\mathbf{K}}_{t-1}^{\\text {odd }}, & \\text { otherwise }\\end{cases} \\\\\n\\widetilde{\\mathbf{K}}_{t}^{\\text {even }} & = \\begin{cases}\\mathbf{U} \\widetilde{\\mathbf{K}}_{t-1}^{\\text {even }}+\\mathbf{e}_{n} \\otimes \\mathbf{k}_{t}, & \\text { if } t \\text { is even } \\\\\n\\widetilde{\\mathbf{K}}_{t-1}^{\\text {even }}, & \\text { otherwise }\\end{cases}\n\\end{aligned}\n$$\n\nLikewise for the values. Depending on $t$, the query attends to one of the two queues: output $=$\n\n$$\n\\begin{cases}\\left(\\widetilde{\\mathbf{V}}^{\\text {odd }}\\right)^{\\top} \\operatorname{softmax}\\left(\\widetilde{\\mathbf{K}}^{\\text {odd }} \\mathbf{q}_{t}\\right), & \\text { if } t \\text { is odd } \\\\ \\left(\\widetilde{\\mathbf{V}}^{\\text {even }}\\right)^{\\top} \\operatorname{softmax}\\left(\\widetilde{\\mathbf{K}}^{\\text {even }} \\mathbf{q}_{t}\\right), & \\text { otherwise. }\\end{cases}\n$$\n\nThe above implementation could incur considerable amount of overhead and may be actually more expensive than the the original dilated window formulation. Therefore it has more conceptual value than practical value. ## A.5 Shared Workspace and Linear Unified Nested Attention\n\nConcurrently to this work, shared workspace (SW; Goyal et al., 2021) and linear unified nested attention (LUNA; Ma et al., 2021) also propposed methods to learn contextualized memory control strategies. Both can be seen as instances of ABC . At layer $\\ell$, their $\\phi_{i}^{\\ell}$ is a function of previous layer's memory $\\widetilde{\\mathbf{X}}^{\\ell-1} \\in \\mathbb{R}^{n \\times d}$ and current layer's input $\\mathbf{X}^{\\ell} \\in \\mathbb{R}^{N \\times d}$ :\n\n$$\n\\phi_{i}=\\left[\\operatorname{softmax}\\left(\\widetilde{\\mathbf{X}}^{\\ell-1} \\mathbf{X}^{\\ell^{\\top}}\\right)\\right]_{:, i}\n$$\n\nwhere $[\\cdot]_{;, i}$ denotes the $i$ th column of a matrix. Query, key, and value projections are suppressed for notation clarity. SW and LUNA reveal the entire sequence to the control vectors, by constructing $\\phi$ as a function of previous layer's memory. Although both admit the recurrent computation as all ABC models do, they are ill-suited for causal attention and autoregressive decoding, since future information is \"leaked\" to $\\phi_{i}$ from the previous layer. LUNA resorts to a variant of Katharopoulos et al. (2020) in causal attention (Ma et al., 2021). In contrast, ABC MLP never conditions $\\phi_{i}$ on previous layer's memory, but only on the current layer's input. ## B More Details about Abc-MLP\n\n## B. 1 Normalization in Causal Attention\n\nAn equivalent implementation to Eq. 7 is to normalize $\\widetilde{\\mathbf{K}}$ and $\\widetilde{\\mathbf{V}}$ instead of $\\phi_{i}$ vectors:\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\alpha}_{i} & =\\exp \\left(\\mathbf{W}_{\\boldsymbol{\\phi}} \\mathbf{x}_{i}\\right), \\quad \\boldsymbol{\\phi}_{i}=\\boldsymbol{\\alpha}_{i} \\\\\n\\overline{\\mathbf{K}} & =\\widetilde{\\mathbf{K}} / \\sum_{j=1}^{N} \\boldsymbol{\\alpha}_{j} . \\quad \\overline{\\mathbf{V}}=\\tilde{\\mathbf{V}} / \\sum_{j=1}^{N} \\boldsymbol{\\alpha}_{j} \\\\\n\\text { output } & =\\overline{\\mathbf{V}}^{\\top} \\operatorname{softmax}(\\overline{\\mathbf{K}} \\mathbf{q})\n\\end{aligned}\n$$\n\n$\\mathbf{M} / \\mathbf{z}$ divides the $\\ell$ th row of matrix $\\mathbf{M}$ by vector z's $\\ell$ th dimension. This admits a linear complexity computation graph for the causal variant of ABCMLP.",
    "abc-33": "## B. 2 Higher-Dimensional Case of Example 1\n\nThis section generalizes Example 1 to higher dimensional cases. Assume that the constant-sized memory has $n$ slots.",
    "abc-34": "$\\phi_{i}$ is cauculated as in Eq. 7. Then $\\widetilde{\\mathbf{K}}=\\sum_{i=1}^{N} \\phi_{i} \\otimes \\mathbf{k}_{i} \\in \\mathbb{R}^{n \\times d}$. Each row of $\\widetilde{\\mathbf{K}}$ can be seen as a separate attention mechanism with a pseudo query. Let $[\\cdot]_{\\ell}$ denote the $\\ell$ th row/dimension of a matrix/vector. Then for any $\\ell=1, \\ldots, n$\n\n$$\n\\begin{aligned}\n{[\\widetilde{\\mathbf{K}}]_{\\ell} } & =\\sum_{i=1}^{N}\\left[\\phi_{i}\\right]_{\\ell} \\otimes \\mathbf{k}_{i} \\\\\n& =\\sum_{i=1}^{N} \\frac{\\exp \\left(\\left[\\mathbf{W}_{\\phi}\\right]_{\\ell} \\cdot \\mathbf{x}_{i}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\left[\\mathbf{W}_{\\phi}\\right]_{\\ell} \\cdot \\mathbf{x}_{j}\\right)} \\mathbf{k}_{i}^{\\top} \\\\\n& =\\operatorname{attn}\\left(\\left[\\mathbf{W}_{\\phi}\\right]_{\\ell},\\left\\{\\mathbf{x}_{i}\\right\\}_{i=1}^{N},\\left\\{\\mathbf{k}_{i}\\right\\}_{i=1}^{N}\\right)^{\\top} \\in \\mathbb{R}^{1 \\times d}\n\\end{aligned}\n$$\n\nIn other words, there are $n$ attention mechanisms in total, each with a separately-parameterized pseudoquery $\\left[\\mathbf{W}_{\\phi}\\right]_{\\ell}$. They summarize the context for $n$ times in parallel, each producing a $d$-dimensional vectors. These output vectors are then stacked into $n$-by- $d$ memory $\\widetilde{\\mathbf{K}} . \\widetilde{\\mathbf{V}}$ is similar. ## C Experimental Details\n\n## C. 1 Language Modeling\n\nWe closely build on Baevski and Auli (2019) and Kasai et al. (2021b). The hyperparameters are summarized in Table 10. All models are trained on 4 A100 GPUs. ## C. 2 Machine Translation\n\nWe experiment with a sentence-level (WMT14 ENDE, Bojar et al., 2014) and a document-level benchmark (IWSLT14 ES-EN, Cettolo et al., 2014) to assess model performance over various sequence lengths. The preprocessing and data splits of WMT14 EN-DE follow Vaswani et al. (2017). A 32,768 byte pair encoding (BPE; Sennrich et al., 2016) vocabulary is shared between source and target languages. For IWSLT14, we follow Miculicich et al. (2018) and use the dev 2010 subset for development and tst2010-2012 for testing. The tokenization is also the same as Miculicich et al. (2018): we tokenize and truecase Spanish and English with Moses (Koehn et al., 2007) and run byte-pair encoding with 30 k splits, shared between the two languages. The final dataset contains 1421, 8, and 42 documents for training, development, and testing. On average, each document contains 126.7 sentences, and each sentence contains 21.7(ES)/22.5(EN) BPE subwords. We use a sliding window with length-4 and stride-one to generate our dataset. During inference, we use predicted context on the target side. We average the checkpoints from the last five epochs to obtain the final model (Vaswani et al., 2017). In inference, we apply beam search with size 5 and length penalty 0.6 . Other hyperparameters are summarized in Table 11. All models are trained on 4 RTX 2080 Ti GPUs. Additional machine translation results. In addition to the results presented in $\\S 5.2$, Table 8 further compares, on the WMT14 EN-DE dataset, the clustering-based (\u00a73.2) and sliding-window (\u00a73.3) models of $A B C$, as well as ReLU and sigmoid variants of $\\mathrm{ABCMLP}_{\\text {M }}$ Clustering and sliding-window ABC variants underperform $\\mathrm{ABC}_{\\text {MLP }}$ with the same memory sizes by more than 0.5 BLEU. Both ReLU and sigmoid underperform their exp counterpart. MLP-exp-all replaces the encoder's softmax attention modules with ABC , in addition to the decoder's. It underperforms $\\mathrm{ABC}_{\\text {MLP }}$ by only 0.3 BLEU. | Model | $\\boldsymbol{\\phi}$ | Cross $n$ | Causal $n$ | Encoder $n$ | BLEU |\n| :--- | :--- | :---: | :---: | :---: | :---: |\n| BASE | - | - | - | - | 27.2 |\n| ABC | Window | 32 | 32 | - | 26.3 |\n|  | Cluster | 32 | 32 | - | 26.8 |\n|  | MLP-ReLU | 32 | 8 | - | - |\n|  | MLP-ReLU | 32 | 32 | - | 26.4 |\n|  | MLP-sigmoid | 32 | 8 | - | 26.8 |\n|  | MLP-sigmoid | 32 | 32 | - | 27.0 |\n|  | MLP-exp | 32 | 8 | - | 27.1 |\n|  | MLP-exp | 32 | 32 | - | $\\mathbf{2 7 . 3}$ |\n|  | MLP-exp-all | 32 | 32 | 32 | 27.0 |\n\nTable 8: ABC variants' performance (SacreBLEU) on the WMT14 EN-DE test set for sentence-level machine translation. MLP-ReLU with 32/8 memory sizes fails to converge. MLP-exp-all applies ABC in both the encoder and the decoder, while others only in the decoders. Figure 1b compares $\\mathrm{ABC}_{\\text {MLP }}$ 's (32-8 memory sizes) attention memory overhead with softmax attention's.",
    "abc-35": "Following Kasai et al. (2021b), we consider a synthetic sequence-to-sequence generation task with varying sequence lengths. A batch size of 16 and greedy decoding is used.",
    "abc-36": "The models are of the same size as those in $\\S 5.2$. ## C. 3 Masked Language Model Finetuning\n\nOur data for continued pretraining is a concatenation of BookCorpus (Zhu et al., 2015), English Wikipedia, OpenWebText (Gokaslan and Cohen, 2019), and RealNews (Zellers et al., 2019). Our data differs from RoBERTa's pretraining data, which we do not have access to. We replace their CC-News (Nagel, 2016) with RealNews, and drop Stories (Trinh and Le, 2018). At the time of this project, the public access to the Stories dataset is broken. ${ }^{10}$ Our machine does not have a large enough memory to load all the data. We therefore split the training data into 20 shards, after shuffling. Other preprocessing is the same as Liu et al. (2019). ${ }^{11}$ The hyperparameters for continued pretraining follow base-sized RoBERTa, part of which are summarized in Table 12. All models are trained on a single TPU v3 accelerator. For downstream task finetuning, we use the same\n\n[^7]hyperparameters as Liu et al. (2019). ${ }^{12}$ Table 13 briefly describes the tasks. The readers are referred to Wang et al. (2019) for futher details. [^8]| Data | Train | Dev. | Test | Vocab. | Sent./doc |\n| :--- | ---: | ---: | ---: | ---: | ---: |\n| WikiText-103 | 103 M | 218 K | 246 K | 268 K | - |\n| WMT14 EN-DE | 4.5 M | 3 K | 3 K | 32 K | - |\n| IWSLT14 ES-EN | 1713 | 8 | 56 | 30 K | 121.5 |\n\nTable 9: Statistics for the datasets. WikiText-103 split sizes are in number of tokens, WMT14 in number of sentences, and IWSLT14 in number of documents. | Hyperprams. | B\\&A | Kasai |\n| :--- | ---: | ---: |\n| \\# Layers | 16 | 32 |\n| \\# Heads | 8 | 8 |\n| Embedding Size | 1024 | 1024 |\n| Head Size | 128 | 128 |\n| FFN Size | 4096 | 4096 |\n| Batch Size | 64 | 64 |\n| Learning Rate | 1.0 | 1.0 |\n| Dropout | 0.3 | 0.3 |\n| Layer Dropout | - | 0.2 |\n| Memory size | $[32,64]$ | 64 |\n\nTable 10: Hyperparameters used in the language modeling experiments. B\\&A: Baevski and Auli (2019); Kasai: Kasai et al.",
    "abc-37": "(2021b). | Hyperprams. | WMT14 | IWSLT14 |\n| :--- | ---: | ---: |\n| \\# Layers | 6 | 6 |\n| \\# Heads | 8 | 8 |\n| Embedding Size | 512 | 512 |\n| Head Size | 64 | 64 |\n| FFN Size | 2048 | 1024 |\n| Warmup Steps | 6000 | 4000 |\n| Dropout | 0.1 | 0.3 |\n| Cross Attn. $n$ | 32 | 128 |\n| Causal Attn. $n$ | 8 | 64 |\n\nTable 11: Hyperparameters used in the machine translation experiments. | Hyperprams. | Values |\n| :--- | ---: |\n| \\# Layers | 12 |\n| \\# Heads | 12 |\n| Embedding Size | 768 |\n| Head Size | 64 |\n| FFN Size | 3072 |\n| Dropout | 0.1 |\n| Memory Size | $[64,128]$ |\n\nTable 12: Hyperparameters for continued pretraining in the masked language model finetuning experiments. | Data | Task | Train | Dev |\n| :--- | :--- | ---: | ---: |\n| MNLI | Entailment | 392 K | 9.8 K |\n| QNLI | Entailment | 105 K | 5.5 K |\n| QQP | Paraphrase | 363 K | 40 K |\n| SST-2 | Sentiment | 67 K | 873 |\n\nTable 13: GLUE datasets and statistics. MNLI: Williams et al. (2018); QNLI is compiled by GLUE's authors using Rajpurkar et al. (2016); QQP: Csernai (2017, accessed September 1, 2020); SST-2: Socher et al. (2013). [^0]:    *This work was done while Zhaofeng Wu and Nikolaos Pappas were at the University of Washington. [^1]:    ${ }^{1}$ The number of queries and key-value pairs may differ, e.g., in the cross attention of a sequence-to-sequence model. [^2]:    ${ }^{2}$ Using bounded memory distinguishes ABC from softmax attention. If growing-size memory were allowed $(n=N)$, an ABC with $\\phi_{i}=\\mathbf{e}_{i}$ would fall back to softmax attention. [^3]:    ${ }^{3}$ We use $\\widetilde{\\mathbf{k}}_{j}^{\\mathrm{CL}}$ to denote both the $j$ th cluster and its centroid. [^4]:    ${ }^{4}$ We experiment with other activations in $\\S \\mathrm{C} .2$. ${ }^{5}$ Here encoder self-attention or cross attention is assumed, and the normalization sums over the entire sequence. Causal attention is slightly different, normalizing by the sum over the prefix instead: $\\phi_{i}=\\boldsymbol{\\alpha}_{i} / \\sum_{j=1}^{i} \\boldsymbol{\\alpha}_{j}$. This does not require access to future tokens. $\\S$ B. 1 details a linear complexity computation graph of causal $\\phi_{i}$. [^5]:    ${ }^{6}$ Both are instances of ABC (\u00a7A.5).",
    "abc-38": "Ma et al. (2021) resorts to a variant of Katharopoulos et al. (2020) for causal attention. [^6]:    ${ }^{7}$ Our data differs from RoBERTa's, which we do not have access to. We replace CC-News (Nagel, 2016) with RealNews, and drop Stories (Trinh and Le, 2018), whose public access is broken at the time of this work. ${ }^{8}$ In preliminary experiments, we explored swapping in ABC , and then directly finetuning on downstream tasks without continued MLM pretraining; all models fail. ${ }^{9}$ BASE slightly underperforms RoBERTa-base. This could be due to overfitting, or the pretraining data discrepancy. [^7]:    ${ }^{10}$ https://console.cloud.google.com/ storage/browser/commonsense-reasoning/ reproduce/stories_corpus?pli=1\n    ${ }^{11}$ https://github.com/pytorch/fairseq/ blob/master/examples/roberta/README. pretraining.md\n\n[^8]:    12https://github.com/pytorch/fairseq/ blob/master/examples/roberta/README.glue. md\n\n"
}