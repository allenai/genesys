{
    "hope-0": "# There is HOPE to Avoid HiPPOs for Long-memory State Space Models \n\nAnnan Yu, ${ }^{1 *}$ Michael W. Mahoney, ${ }^{2,3,4}$ N. Benjamin Erichson ${ }^{2,3}$<br>${ }^{1}$ Center for Applied Mathematics, Cornell University<br>${ }^{2}$ Lawrence Berkeley National Laboratory<br>${ }^{3}$ International Computer Science Institute<br>${ }^{4}$ Department of Statistics, University of California at Berkeley\n\n\n#### Abstract\n\nState-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity. ## 1 Introduction\n\nState-space models (SSMs) [14] have gained popularity and success in sequence modeling. Known for its excellent efficiency and capability of handling long sequences, an SSM leverages the continuous-time linear, time-invariant (LTI) systems. These systems are often defined by four matrices $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ as\n\n$$\n\\mathbf{x}^{\\prime}(t)=\\mathbf{A} \\mathbf{x}(t)+\\mathbf{B u}(t), \\quad \\mathbf{y}(t)=\\mathbf{C x}(t)+\\mathbf{D u}(t)\n$$\n\nand they can be used to model the mappings from input time-series $\\mathbf{u}(\\cdot)$ to the output timesseries $\\mathbf{y}(\\cdot)$, where $\\mathbf{u}(t) \\in \\mathbb{R}^{m}$ and $\\mathbf{y}(t) \\in \\mathbb{R}^{p}$ for every $t$. The (hidden) states, which capture the latent dynamics, are denoted as $\\mathbf{x}=\\mathbf{x}(t) \\in \\mathbb{R}^{n}$. The system matrices are of dimensions $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}, \\mathbf{B} \\in \\mathbb{C}^{n \\times m}, \\mathbf{C} \\in \\mathbb{C}^{p \\times n}$, and $\\mathbf{D} \\in \\mathbb{C}^{p \\times m}$. Often, the size $n$ of the state vector $\\mathbf{x}$ is much larger than $m$ and $p$, which allows us to memorize information about the past inputs $\\left.\\mathbf{u}\\right|_{(-\\infty, t]}$ in the state vector $\\mathbf{x}(t)$ and retrieve it later to compute $\\mathbf{y}$ via $\\mathbf{C}$. [^0]The so-called S4 [14] and S4D [13] models both set $m=p=1$, and they differ in the structural requirement of $\\mathbf{A}$. This framework was later generalized to the case where $m, p>1$ by the S 5 model [27] via the parallel scans. Another line of research involves making the state transition rule $\\mathbf{A}$ depend on the input $\\mathbf{u}$, along which the two most notable models are Liquid-S4 [17] and Mamba [11], where the latter model achieves the state-of-the-art performance on large-scale real-world datasets. However, SSMs typically need to be initialized and trained (very) carefully. A randomly initialized SSM has suboptimal performance, but the so-called high-order polynomial projection operators (HiPPO) $[29,12,15]$ can be used to empirically improve it. Subsequent work has considered stably transforming HiPPO matrices into simplified structures [13, 32]. The empirical success of the HiPPO framework is traditionally ascribed to interpreting the state vector $\\mathbf{x}(t)$ as the projection of the inputs $\\left.\\mathbf{u}\\right|_{(-\\infty, t]}$ onto some orthogonal polynomial basis. However, such a claim is not completely satisfiable - indeed, the most popular HiPPO-LegS framework is not the Legendre-basis projection operator, as it misses the scaling factor $1 / t$ (see [15, Thm.",
    "hope-1": "5.]). On the other hand, even a properly initialized SSM needs to be trained with care. One often needs to set a smaller learning rate for the matrix $\\mathbf{A}$ [14], and the LTI systems require reparameterization to be trained stably [30]. To address these issues, we analyze SSMs through the lens of Henkel Operator theory. Specifically, we use the Hankel singular value decomposition (HSVD) to analyze an operator defined by $\\Gamma$. The decay of the Hankel singular values tells how \"complicated\" the LTI system is. If the Hankel singular values of $\\Gamma$ decay fast, then it informally means that our $\\Gamma$ cannot capture the complex patterns in the input space $\\{u(\\cdot)\\}$; in fact, the theory of reduced-order modeling (ROM) says that $\\Gamma$ can be well-approximated by a reduced system with a much smaller state-space dimension $n[9]$. We find that the decay of the Hankel singular values can be used for predicting the performance of an SSM, and that every previous effort in proposing a good initialization and training scheme can be viewed as an effort to avoid fast-decaying Hankel singular values. This is reminiscent of the works by [22] and [23] that connect the singular values of the weight matrices to a deep neural network's performance. In particular, we prove that a random initialization leads to a low-rank LTI system (i.e., its Hankel singular values decay rapidly), whereas the HiPPO initialization is high-rank (see Theorem 1). This serves as an alternative theory with practical consequences and explains why the HiPPO initialization is preferred over a random one. Moreover, the stability of the LTI systems depends heavily on the parameters themselves (see Theorem 2). Even a high-degree LTI system may become a low-degree one by a small amount of perturbation. This explains why one needs to train $\\mathbf{A}$ on a logarithmic scale with a reduced learning rate in practice. Based on these insights, we propose a completely different parametrization of the LTI systems. Instead of parametrizing the continuous-time systems by matrices $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$, we parameterize our LTI systems by the Markov parameters of the so-called discrete Hankel operator (HOPE). A discrete Hankel operator is defined by a doubly infinite Hankel matrix, and is naturally associated with a discrete LTI system, and with a continuous-time system via the bilinear transform with $\\Delta t=1$ [9]. While this continuous-time LTI system acts on our sequential data, the optimization algorithms are applied to the Markov parameters of the Hankel matrix. (See Figure 1.) In contrast to Theorem 1 and Theorem 2 for a canonical SSM, we prove that a random Hankel matrix has slowly decaying singular values (see Theorem 3) and a global stability to perturbation (see Theorem 4). Hence, unlike a canonical SSM, our HOPE-SSM does not need to be carefully initialized by predesigned matrices, and the Markov parameters of the LTI systems need no reparameterization and enjoy the same learning rate as other model parameters; hence, less hyperparameter tuning is needed. We show that our HOPE-SSM can be implemented by nonuniformly sampling the transfer function, which shares the same\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-03.jpg?height=790&width=1185&top_left_y=243&top_left_x=444)\n\nFigure 1: There are many equivalent ways to represent an LTI system. While most of the canonical SSMs use continuous LTI systems as their parameters, we propose to parameterize an SSM by the Markov parameters in its Hankel operator. The feedthrough matrix D is not shown in the diagram, but it is also a parameter of the LTI layers in both the canonical SSMs and our HOPE-SSM. computational complexity as the S4D model. Moreover, it requires only $1 / 3$ the number of parameters of an LTI system in an S4D model to parameterize an LTI system in a HOPE-SSM. The practical benefits of our novel parametrization are improved robustness (with respect to initialization and training stability) and performance (with respect to model quality and parameter count). Moreover, we show that the memory of our HOPE-SSM does not decay (see eq. (8)), making it possible to solve tasks that involve even longer-range dependency. This addresses the well-known issue that the LTI system of a canonical SSM suffers from exponentially decaying memory [2]. We remark that [2] also proposes a different model called spectral SSM that achieves non-decaying memory using the so-called spectral filtering method. While they show that a spectral SSM is exponentially close to an SSM, our HOPE-SSM is an actual SSM containing LTI systems, and it only differs from a canonical SSM in the parameterization.",
    "hope-2": "Contributions.",
    "hope-3": "Our main contributions are summarized as follows. 1. We show a strong relationship between the performance of an SSM and the Hankel singular values of its LTI systems. We justify this relationship using ideas in ROM. We theoretically prove that high-degree LTI systems need careful designs and are unstable over training. This explains the difficulties in initializing and training an SSM. 2. We propose a new parametrization of LTI systems using the Hankel operator, which can be implemented efficiently by nonuniformly sampling the transfer function and requires $1 / 3$ the number of parameters in an S4D model for an LTI system. We prove that our HOPE-SSM can be (i) randomly initialized, (ii) stably trained, and (iii) has non-decaying long memory. 3. We empirically demonstrate that our HOPE-SSM is robust using the sCIFAR-10 task and that it has long-term memory using a noise-padded sCIFAR-10 dataset. We test the performance of a full-scale HOPE-SSM on the Long-Range Arena (LRA) and observe that its performance exceeds that of its S4 and S4D counterparts for most tasks. ## 2 Preliminaries\n\nLet $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ be a continuous-time LTI system defined in eq. (1). One can take a bilinear transform to obtain a discrete system $\\bar{\\Gamma}=(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}, \\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})$ so that the underlying dynamics is given by\n\n$$\n\\overline{\\mathbf{x}}_{k+1}=\\overline{\\mathbf{A}} \\overline{\\mathbf{x}}_{k}+\\overline{\\mathbf{B}} \\overline{\\mathbf{u}}_{k}, \\quad \\overline{\\mathbf{y}}_{k}=\\overline{\\mathbf{C}} \\overline{\\mathbf{x}}_{k}+\\overline{\\mathbf{D}} \\overline{\\mathbf{u}}_{k}\n$$\n\nThe transfer functions of $\\Gamma$ and $\\bar{\\Gamma}$ are\n\n$$\nG(s)=\\mathbf{C}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\mathbf{D} \\quad \\text { and } \\quad \\bar{G}(z)=\\overline{\\mathbf{C}}(z \\mathbf{I}-\\overline{\\mathbf{A}})^{-1} \\overline{\\mathbf{B}}+\\overline{\\mathbf{D}}\n$$\n\nrespectively. The importance of the transfer functions is that they bring the inputs to the outputs in the Laplace domain by multiplication, which reduces to the Fourier domain if we assume that $\\mathbf{u}$ is bounded and compactly supported:\n\n$$\n\\hat{\\mathbf{y}}(s)=G(i s) \\hat{\\mathbf{u}}(s), \\quad \\hat{\\overline{\\mathbf{y}}}=\\bar{G}(\\boldsymbol{\\omega}) \\hat{\\overline{\\mathbf{u}}}\n$$\n\nwhere the hat of a function and a vector means the Fourier transform and the Fourier coefficients, respectively, and $\\boldsymbol{\\omega}$ is the vector of roots of unity. Throughout this paper, we always assume that our LTI systems are asymptotically stable, i.e., the eigenvalues of $\\mathbf{A}$ have negative real parts, or equivalently, the eigenvalues of $\\overline{\\mathbf{A}}$ are contained in the open unit disk in the complex plane. Given a discrete LTI system $\\bar{\\Gamma}$, its Hankel operator is defined by a doubly infinite Hankel matrix\n\n$$\n\\overline{\\mathbf{H}}: \\ell^{2} \\rightarrow \\ell^{2}, \\quad \\overline{\\mathbf{H}}_{i, j}=\\overline{\\mathbf{C A}}^{i+j} \\overline{\\mathbf{B}}, \\quad i, j \\geq 0\n$$\n\nThis discrete Hankel operator on $\\ell^{2}$ is a bounded linear operator of rank $\\leq n$, the number of latent states. We denote its singular values by $\\sigma_{1}(\\overline{\\mathbf{H}}) \\geq \\sigma_{2}(\\overline{\\mathbf{H}}) \\geq \\cdots \\geq \\sigma_{n}(\\overline{\\mathbf{H}}) \\geq 0$. It maps the past inputs to the future outputs: if $\\overline{\\mathbf{u}}_{k}=\\mathbf{0}$ for all $k \\geq 0$, then we have $\\left[\\begin{array}{lll}\\overline{\\mathbf{y}}_{0}^{\\top} & \\overline{\\mathbf{y}}_{1}^{\\top} & \\cdots\\end{array}\\right]^{\\top}=\\overline{\\mathbf{H}}\\left[\\begin{array}{lll}\\overline{\\mathbf{u}}_{-1}^{\\top} & \\overline{\\mathbf{u}}_{-2}^{\\top} & \\cdots\\end{array}\\right]^{\\top}$. Analogously, one can define a continuous Hankel operator $\\mathbf{H}: L^{2}([0, \\infty)) \\rightarrow L^{2}([0, \\infty))$ given a continuous-time LTI system $\\Gamma$. If $\\Gamma$ and $\\bar{\\Gamma}$ are related by the bilinear transform, then the singular values of $\\mathbf{H}$ are equivalent to those of $\\overline{\\mathbf{H}}$, i.e., $\\sigma_{j}(\\mathbf{H})=\\sigma_{j}(\\overline{\\mathbf{H}})$. (See Appendix B for more details.) In this paper, we consider the decay of the Hankel singular values. To quantitatively measure how \"small\" a singular value is, we introduce the numerical rank of an LTI system. Given a small number $\\epsilon \\geq 0$, we define the $\\epsilon$-rank of $\\Gamma$ to be\n\n$$\n\\operatorname{rank}_{\\epsilon}(\\Gamma)=\\max \\left\\{j \\mid \\sigma_{j}(\\mathbf{H}) / \\sigma_{1}(\\mathbf{H})>\\epsilon\\right\\}\n$$\n\nWhen $\\epsilon=0$, we reproduce the exact rank of $\\mathbf{H}$, which is rarely $<n$ in the floating-point arithmetic. The $\\epsilon$-rank allows us to eliminate the small but perhaps positive singular values. ## 3 Unravel a Mystery: Hankel Singular Values in Initialization and Training\n\nWe begin our exploration by presenting a mystery. We train an S4D model to learn the sCIFAR10 image classification task [19, 28]. We consider three initialization schemes of the LTI systems in the SSM: $\\Gamma_{1}, \\Gamma_{2}$, and $\\Gamma_{3}$, where $\\Gamma_{1}$ and $\\Gamma_{2}$ are two \"random\" systems described in section 3.2 and section 3.1, respectively, and $\\Gamma_{3}$ is based on the HiPPO-LegS framework. For an SSM initialized by a certain LTI system $\\Gamma_{j}(1 \\leq j \\leq 3)$, we train it in two different ways: either by freezing $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ and only training the other model parameters or by assigning $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ a small learning rate. The three initializations and two learning rate assignments comprise a total of six combinations, summarized in Figure 2. Figure 2 naturally raises two questions:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-05.jpg?height=528&width=1511&top_left_y=195&top_left_x=256)\n\nFigure 2: Test accuracy of the SSMs on the sCIFAR task. The LTI systems are initialized in three different ways and are either trained or untrained. We notice that when the LTI systems are initialized using $\\Gamma_{1}$, training the LTI system is impairing the model accuracy. This is in contrast to SSMs initialized by the HiPPO-LegS systems $\\Gamma_{3}$, where training the LTI system is helping the performance. (a) Why does one random initialization $\\Gamma_{1}$ outperform the other one $\\Gamma_{2}$ ? (b) While freezing $\\Gamma_{1}$ and $\\Gamma_{3}$ leads us to similar performance, why does a small learning rate foster the $\\Gamma_{3}$-initialized SSM but deteriorate the $\\Gamma_{1}$-initialized one? To answer these questions, we study the Hankel singular values of the systems, but why are they relevant? The reason is that the Hankel singular values measure the \"complexity\" of an LTI system. The easiest way to see this is via ROM of the system $\\Gamma[1,9]$ :\n\nFor any $k<n$, there exists a reduced-order approximation $\\tilde{\\Gamma}_{H}=\\left(\\tilde{\\mathbf{A}}_{H}, \\tilde{\\mathbf{B}}_{H}, \\tilde{\\mathbf{C}}_{H}, \\tilde{\\mathbf{D}}_{H}\\right)$ with $\\tilde{\\mathbf{A}}_{H} \\in \\mathbb{C}^{k \\times k}$, such that $\\sigma_{1}(\\mathbf{H}-\\tilde{\\mathbf{H}}) \\leq \\sigma_{k+1}(\\mathbf{H})$, where $\\tilde{\\mathbf{H}}$ is the Hankel operator of $\\tilde{\\Gamma}_{H}$. In addition, there exists another reduced-order approximation $\\tilde{\\Gamma}_{\\infty}=\\left(\\tilde{\\mathbf{A}}_{\\infty}, \\tilde{\\mathbf{B}}_{\\infty}, \\tilde{\\mathbf{C}}_{\\infty}, \\tilde{\\mathbf{D}}_{\\infty}\\right)$ with $\\tilde{\\mathbf{A}}_{\\infty} \\in \\mathbb{C}^{k \\times k}$, such that $\\|G-\\tilde{G}\\|_{\\infty} \\leq \\sum_{j=k+1}^{n} \\sigma_{j}(\\mathbf{H}) \\leq(n-k) \\sigma_{k+1}(\\mathbf{H})$, where $\\tilde{G}$ is the transfer function of $\\tilde{\\Gamma}_{\\infty}$ and $\\|\\cdot\\|_{\\infty}$ is the infinity norm over the imaginal axis. In particular, if $\\sigma_{k+1}(\\mathbf{H})$ is small, then that means given the same past input, the future outputs of $\\Gamma$ and $\\tilde{\\Gamma}_{H}$ must be similar. Moreover, if the sum of the trailing singular values $\\sum_{j=k+1}^{n} \\sigma_{j}(\\mathbf{H})$ is small, then by eq. (3), $\\Gamma$ and $\\tilde{\\Gamma}_{\\infty}$ produce similar outputs on any input, which is a stronger notion of approximation than using the Hankel norm $\\sigma_{1}$. Hence, ROM says that if the Hankel singular values decay fast, then we can replace the LTI system with a much smaller one without too much loss; in other words, most states in $\\mathbf{x}(t)$ are not properly used to memorize the input. In our experiment, each SSM has 4 layers and 128 channels. These comprise $4 \\times 128=512$ different copies of single-input/single-output $\\Gamma_{j}(j=1,2$, or 3$)$. Note that every HiPPO-LegS system $\\Gamma_{3}$ is also distinct because $\\mathbf{C}$ is initialized randomly. Every LTI has $n=64$ latent states. These make up $512 \\times 64=32768$ relative Hankel singular values $\\sigma_{j}(\\mathbf{H}) / \\sigma_{1}(\\mathbf{H})$ to consider. In Figure 3, we show the histograms of all these relative Hankel singular values at two different stages of training. Note that the three histograms on the second row only apply to the case when the LTI systems are trained with a small learning rate; when they are frozen, the distributions always stay the same as those at initialization. We can explain the behaviors of the three SSMs using their Hankel singular values:\n\n1. The system $\\Gamma_{1}$ initially has a high numerical rank. Hence, when the system is untrained, $\\Gamma_{1}$ defines a random mapping that captures the rich content in the input data and yields the rest of the work to other model parameters in the SSM. However, when the system is trained, its Hankel singular values start to decay rapidly with only $27.87 \\%$ of the singular values satisfying that $\\sigma_{j}(\\mathbf{H}) / \\sigma_{1}(\\mathbf{H})>0.01$, and the system can no longer handle a variety\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-06.jpg?height=636&width=1621&top_left_y=200&top_left_x=218)\n\nFigure 3: The distribution of all relative Hankel singular values $\\sigma_{j}(\\mathbf{H}) / \\sigma_{1}(\\mathbf{H})$ of the LTI systems in an SSM. For each initialization, the distribution is shown both at initialization and after the SSM is trained for 10 epochs.",
    "hope-4": "The second row only applies to the case where the LTI systems are not frozen. of distinct inputs. This makes it harder to parse complicated images in the sCIFAR-10 dataset. 2. No matter whether trained or not, the Hankel singular values of $\\Gamma_{2}$ decay very fast, which means $\\Gamma_{2}$ is almost equivalent to a much smaller system and cannot effectively capture the complicated patterns in the input space. Hence, the SSMs with both trained and untrained LTI systems could not learn the task quickly.",
    "hope-5": "3. The system $\\Gamma_{3}$, trained or not, has a high numerical rank. In particular, over $87.82 \\%$ of the singular values satisfy that $\\sigma_{j}(\\mathbf{H}) / \\sigma_{1}(\\mathbf{H})>0.01$ after 10 epochs. In this case, training the LTI systems allows us to further accelerate the optimization. While the distribution of the Hankel singular values allows us to explain the mysterious behaviors of SSMs initialized by different LTI systems, a perhaps more interesting question is why we observe such certain patterns of the histograms. In the next two subsections, we theoretically show why the random system $\\Gamma_{2}$ has fast-decaying Hankel singular values and why $\\Gamma_{1}$ is sensitive to training. ### 3.1 Random LTI Systems Have Low Ranks\n\nIn our experiment, we sample $\\bar{\\Gamma}_{2}$ by assuming the diagonal entries of $\\overline{\\mathbf{A}}=\\operatorname{diag}\\left(a_{1}, \\ldots, a_{n}\\right)$ are uniformly sampled on the unit disk and $\\overline{\\mathbf{B}} \\circ \\overline{\\mathbf{C}}^{\\top}$ is a random vector with each entry sampled i.i.d. from a normal distribution $\\mathcal{N}(0,1)$. We then compute $\\Gamma_{2}$ from the discrete system $\\bar{\\Gamma}_{2}$ using the bilinear transform. In this section, we make a relaxed assumption that $a_{j}$ are sampled i.i.d. with\n\n$$\n\\mathbb{P}\\left(\\left|a_{j}\\right|>(1-\\rho)\\right)=\\mathcal{O}\\left(\\rho^{\\alpha}\\right) \\quad \\text { as } \\quad \\rho \\rightarrow 0^{+}\n$$\n\nfor some $\\alpha>0$. For example, if $a_{j}$ is uniformly distributed on the open unit disk, then we have $\\alpha=1$. With these assumptions, we formally show that the system has a low $\\epsilon$-rank with high probability. Theorem 1. Given any $\\epsilon>0,0<\\alpha \\leq 1$, and $0<\\delta \\leq 1$, with probability at least $1-\\delta$, the $\\epsilon$-rank of the system $\\bar{\\Gamma}_{2}=(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}, \\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})$ is $\\mathcal{O}\\left(\\ln \\left(\\delta^{-3 / 2} \\epsilon^{-1} n\\right) n^{\\beta}\\right)$, where\n\n$$\n\\beta \\leq \\frac{1}{1+\\alpha}+\\frac{\\ln (2+\\sqrt{\\ln (1 / \\delta) / 2})}{\\ln (n)}\n$$\n\nand the constant in $\\mathcal{O}$ is universal. We defer the proof to Appendix C. As explained in section 2, the $\\epsilon$-ranks of the continuous $\\Gamma_{2}$ and the discrete $\\bar{\\Gamma}_{2}$ are the same as the two systems share the same Hankel singular values. What Theorem 1 says is that if we ignore the logarithmic factors in the $\\mathcal{O}$-notation, then the $\\epsilon$ rank of $\\bar{\\Gamma}$ scales like $n^{\\beta}$ as $n \\rightarrow \\infty$, where $\\beta<1$. For example, when $a_{j}$ are uniformly distributed on the unit disk, we have $\\beta=1 / 2$ plus a small number bounded by $\\ln (2+\\sqrt{\\ln (1 / \\delta) / 2}) / \\ln (n)$. In practice, we find that this term can almost be ignored (see Appendix F). Hence, unlike a randomly initialized Hankel matrix (see section 4), the $\\epsilon$-rank of a random LTI system is unfortunately sublinear in $n$. ### 3.2 LTI Systems are Unstable under Perturbations\n\nIn the last subsection, we explained how we sampled $\\Gamma_{2}$ and showed that a randomly initialized system $\\Gamma_{2}$ has a low numerical rank. To sample a random $\\Gamma_{1}$, we create random samples of the transfer function $\\left\\{\\left(i s_{j}, G_{j}\\right)\\right\\}_{j=1}^{N}$, where $s_{j} \\in \\mathbb{R}$ and $G_{j} \\in \\mathbb{C}$. We then identify a system $\\Gamma_{1}$ whose transfer function $G$ satisfies that $G\\left(i s_{j}\\right) \\approx G_{j}$. In this section, we aim to understand why the system $\\Gamma_{1}$ tends to lose many numerical ranks through training while the HiPPO-LegS system $\\Gamma_{3}$ does not. We prove a theorem that says the stability of a system under perturbation highly depends on its parameters. We assume that $\\mathbf{A}$ is a diagonal matrix and we perturb its diagonal entries. This aligns with the structure of the S4D and S5 models [13, 27]. Theorem 2. Let $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ be a stable continuous-time LTI system, where $\\mathbf{A}=$ $\\operatorname{diag}\\left(a_{1}, \\ldots, a_{n}\\right)$ is diagonal. Let $\\tilde{\\Gamma}=(\\tilde{\\mathbf{A}}, \\tilde{\\mathbf{B}}, \\tilde{\\mathbf{C}}, \\mathbf{D})$ be a perturbed stable system with $\\tilde{\\mathbf{A}}=$ $\\operatorname{diag}\\left(\\tilde{a}_{1}, \\ldots, \\tilde{a}_{n}\\right)$. Assume there exist $\\Delta_{\\mathbf{A}}, \\Delta_{\\mathbf{B}}>0$ such that $\\left|a_{j}-\\tilde{a}_{j}\\right| \\leq \\Delta_{\\mathbf{A}} \\leq \\min _{j}\\left|\\operatorname{Re}\\left(a_{j}\\right)\\right| / 2$ and $\\left|b_{j} c_{j}-\\tilde{b}_{j} \\tilde{c}_{j}\\right| \\leq \\min \\left(\\left|b_{j} c_{j}\\right|, \\Delta_{\\mathbf{B}}\\right)$ for all $j=1, \\ldots, n$. Let $G$ and $G$ be the transfer functions of $\\Gamma$ and $\\tilde{\\Gamma}$, respectively. Then, the following statements hold. (a) We have\n\n$$\n\\|G-\\tilde{G}\\|_{\\infty} \\leq 4 n \\Delta_{\\mathbf{A}} \\max _{j} \\frac{\\left|b_{j} c_{j}\\right|}{\\left|\\operatorname{Re}\\left(a_{j}\\right)\\right|^{2}}+n \\Delta_{\\mathbf{B}} \\max _{j} \\frac{1}{\\left|\\operatorname{Re}\\left(a_{j}\\right)\\right|}\n$$\n\n(b) The upper bound is tight up to a factor of $n$. That is, given any $\\Gamma, \\Delta_{\\mathbf{A}} \\leq \\min _{j}\\left|\\operatorname{Re}\\left(a_{j}\\right)\\right| / 2$, and $\\Delta_{\\mathbf{B}} \\leq \\min _{j}\\left|b_{j} c_{j}\\right|$, there exist two systems $\\tilde{\\Gamma}_{\\mathbf{A}}$ and $\\tilde{\\Gamma}_{\\mathbf{B}}$, with transfer functions $\\tilde{G}_{\\mathbf{A}}$ and $\\tilde{G}_{\\mathbf{B}}$, respectively, that satisfy the above perturbation conditions and have\n\n$$\n\\left\\|G-\\tilde{G}_{\\mathbf{A}}\\right\\|_{\\infty} \\geq \\Delta_{\\mathbf{A}} \\max _{j} \\frac{\\left|b_{j} c_{j}\\right|}{\\left|\\operatorname{Re}\\left(a_{j}\\right)\\right|^{2}}, \\quad\\left\\|G-\\tilde{G}_{\\mathbf{B}}\\right\\|_{\\infty} \\geq \\Delta_{\\mathbf{B}} \\max _{j} \\frac{1}{\\left|\\operatorname{Re}\\left(a_{j}\\right)\\right|}\n$$\n\nThe proof can be found in Appendix D. Theorem 2 says that when a diagonal LTI system starts to deviate, its stability depends on two things: the proximity of its poles $a_{j}$ to the imaginary axis and the magnitudes of $\\mathbf{B}$ and $\\mathbf{C}$. We first explain why each of these might cause a problem and then use Theorem 2 to show why $\\Gamma_{1}$ is unstable during training. As stated in [5], \"[the Hankel singular values] decay more rapidly the farther the $\\Lambda(\\mathbf{A})$ falls in the left half of the complex plane,\" where $\\Lambda(\\mathbf{A})$ is the spectrum of $\\mathbf{A}$, which is equivalent to $\\left\\{a_{1}, \\ldots, a_{n}\\right\\}$ in the diagonal case. Hence, many diagonal LTI systems with a high $\\epsilon$-rank would have eigenvalues $a_{j}$ close to the imaginary axis, making the system unstable. Moreover, one may believe that $\\|G\\|_{\\infty}$ is highly related to $\\left|b_{j} c_{j}\\right|$, and hence, the fact that the system's stability depends on $\\left|b_{j} c_{j}\\right|$ is not an issue. However, this is not true as a system with a small transfer function may have an arbitrarily large system matrix $\\mathbf{B} \\circ \\mathbf{C}^{\\top}$. Such a system can be numerically hazardous to work with even without perturbation due to the so-called cancellation errors [34]. We can now understand why $\\Gamma_{1}$ is much more sensitive to perturbation than $\\Gamma_{3}$. In Figure 4, we show the locations of the poles $a_{j}$ and their associated magnitudes $\\left|b_{j} c_{j}\\right|$. Compared to $\\Gamma_{3}$,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-08.jpg?height=607&width=1611&top_left_y=167&top_left_x=220)\n\nFigure 4: A random perturbation to the imaginary part of $\\mathbf{A}$ is added to the random system $\\Gamma_{1}$ and the HiPPO-LegS system $\\Gamma_{3}$. The magnitude of the perturbation is set to $0.1 \\%$ and $1 \\%$ of the original matrix A. For each system $\\Gamma_{j}$, on the left, we show the relative Hankel singular values $\\sigma_{j} / \\sigma_{1}$ of the original and perturbed systems; on the right, we plot the location of each $a_{j}$ in the complex plane and use the color to indicate the magnitude of its associated $\\left|b_{j} c_{j}\\right|$. the poles of $\\Gamma_{1}$ are much closer to the imaginary axis whereas the values of $\\left|b_{j} c_{j}\\right|$ are also much larger.",
    "hope-6": "Hence, by Theorem 2, the system $\\Gamma_{1}$ should be much less stable than $\\Gamma_{3}$. This is indeed seen in Figure 4. ## 4 HOPE-SSM: A Rankful, Stable, and Long-memory Parameterization\n\nGiven the potential issues of an SSM discussed in section 3, we propose an entirely different parametrization of the LTI systems called HOPE. We first describe the details of our HOPESSM and then explain how it resolves the low-rank and unstable issues of an LTI system.",
    "hope-7": "In addition, it also benefits from long-term memory. Our strategy is to use a Hankel matrix defined in eq. (4) to parameterize an LTI system. Therefore, instead of having A, B, C, and D as the model parameters, we now have a vector $\\mathbf{h}$ of length $n$ and the skip connection $\\mathbf{D}$. The finite Hankel matrix $\\overline{\\mathbf{H}} \\in \\mathbb{C}^{n \\times n}$ is then defined by the Markov parameters in $\\mathbf{h}$ :\n\n$$\n\\overline{\\mathbf{H}}_{i, j}=\\mathbb{1}_{\\{i+j<n\\}} \\mathbf{h}_{i+j}\n$$\n\nThis Hankel matrix corresponds to a discrete LTI system $\\bar{\\Gamma}$, which is further associated with a continuous-time system $\\Gamma$ via the bilinear transform. Notably, in our HOPE-SSM, it is the continuous system $\\Gamma$ that takes the role of $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C})$ in a canonical SSM, e.g., S4 and S4D. If we set $\\Delta t=1$ and discretize $\\Gamma$, the convolutional kernel is exactly $\\mathbf{h}$ padded with $L-n$ zeros, where $L$ is the length of the sequential input. However, we usually want to discretize with a different $\\Delta t$. We can do so via resampling the transfer functions of $\\Gamma$ and $\\bar{\\Gamma}$, which equal [25]\n\n$$\nG(s)=\\sum_{j=0}^{n-1} \\mathbf{h}_{j}((1+s) /(1-s))^{-j-1} \\quad \\Leftrightarrow \\quad \\bar{G}(z)=\\sum_{j=0}^{n-1} \\mathbf{h}_{j} z^{-j-1}\n$$\n\nLet $\\boldsymbol{\\omega}^{(L)}=\\left[\\begin{array}{lll}\\omega_{0}^{(L)} & \\cdots & \\omega_{L-1}^{(L)}\\end{array}\\right]^{\\top}$ be the vector of the $L$ th roots of unity. Given an input $\\mathbf{u} \\in \\mathbb{C}^{L \\times 1}$ of length $L$, when $\\Delta t=1$, the outputs can be evaluated as\n\n$$\n\\mathbf{y}=\\operatorname{iFFT}\\left(\\operatorname{FFT}(\\mathbf{u}) \\circ \\bar{G}\\left(\\boldsymbol{\\omega}^{(L)}\\right)\\right), \\quad \\bar{G}\\left(\\boldsymbol{\\omega}^{(L)}\\right)=\\left[\\begin{array}{lll}\n\\bar{G}\\left(\\omega_{0}^{(L)}\\right) & \\cdots & \\bar{G}\\left(\\omega_{L-1}^{(L)}\\right)\n\\end{array}\\right]^{\\top}\n$$\n\nFor a different $\\Delta t$, one way to think of it is that we have compressed or dilated the time domain of $\\mathbf{u}$ by a factor of $\\Delta t$. Hence, the frequency domain of its Fourier transform $\\hat{\\mathbf{u}}$ is dilated or compressed by a factor of $1 / \\Delta t$. That is, we should relocate our samplers in the frequency domain as\n\n$$\n\\boldsymbol{\\omega}^{(L, \\Delta t)}=\\left[\\begin{array}{lll}\n\\omega_{0}^{(L, \\Delta t)} & \\cdots & \\omega_{L-1}^{(L, \\Delta t)}\n\\end{array}\\right]^{\\top}, \\quad \\omega_{j}^{(L, \\Delta t)}=\\frac{1+s_{j}^{(L)} / \\Delta_{t}}{1-s_{j}^{(L)} / \\Delta_{t}}, \\quad s_{j}^{(L)}=\\frac{\\omega_{j}^{(L)}-1}{\\omega_{j}^{(L)}+1}\n$$\n\nwhere $s_{j}^{(L)} / \\Delta t$ and $\\omega_{j}^{(L, \\Delta t)}$ are the scaled samplers in the time domain and the angular domain, respectively. Then, the output vector $\\mathbf{y}$ can be computed as\n\n$$\n\\mathbf{y}=\\operatorname{iFFT}\\left(\\operatorname{FFT}(\\mathbf{u}) \\circ \\bar{G}\\left(\\boldsymbol{\\omega}^{(L, \\Delta t)}\\right)\\right), \\quad \\bar{G}\\left(\\boldsymbol{\\omega}^{(L, \\Delta t)}\\right)=\\left[\\begin{array}{lll}\n\\bar{G}\\left(\\omega_{0}^{(L, \\Delta t)}\\right) & \\ldots & \\bar{G}\\left(\\omega_{L-1}^{(L, \\Delta t)}\\right)\n\\end{array}\\right]^{\\top}\n$$\n\nWe provide a detailed derivation of eq. (7) in Appendix G. With $L$ processors, we can sample the transfer function $\\bar{G}$ independently at $L$ locations, each of which takes $\\mathcal{O}(n)$ time. Computing the FFT and iFFT takes $\\mathcal{O}(L \\log L)$. Overall, the evaluation of the Hankel-parameterized LTI system takes $\\tilde{\\mathcal{O}}(L+n)$ time, which agrees with the complexity of the S 4 and S 4 D models. To make the model recurrent during the inference time, one can either identify a system $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C})$ whose Hankel operator is $\\mathbf{H}$ and compute as if it is an S4D model [18, 3], or compute directly the convolutional kernel based on eq. (7). ### 4.1 Advantage I: A HOPE-SSM has a High Numerical Rank\n\nThe properties of a random Hankel matrix have been studied in [8] and [24].",
    "hope-8": "We can build upon their work to prove the following result. Theorem 3. Let $h_{1}, h_{2}, \\ldots$ be a sequence of i.i.d. random variables with mean 0 and variance 1 that have finite third moments. We almost surely have that for any $\\epsilon>0$, the $(\\epsilon / \\sqrt{\\ln (n)})$-rank of $\\overline{\\mathbf{H}}_{n}$ is $\\Omega(n)$, where $\\overline{\\mathbf{H}}_{n}$ is the $n \\times n$ Hankel matrix defined in eq. (5). Hence, if we ignore the logarithmic factor of $\\sqrt{\\ln (n)}$, then the numerical rank of a random Hankel matrix should be proportional to $n$ as $n \\rightarrow \\infty$. This means if we imagine the landscape of parameters, the high-rank LTI systems parameterized by the system matrices $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ are sparse islands in a big ocean, whereas those parameterized by the Markov parameters $\\mathbf{h}$ are large continents. ### 4.2 Advantage II: A HOPE-SSM is Stable under Perturbation\n\nUnlike the LTI system, a Hankel matrix $\\overline{\\mathbf{H}}$ is very stable under perturbation, as shown in the following theorem. Theorem 4. Let $\\mathbf{h} \\in \\mathbb{C}^{n \\times 1}$ be a vector and $G$ be the transfer function defined in eq. (6). Suppose we perturb $\\mathbf{h}$ to $\\tilde{\\mathbf{h}}$ and let $\\tilde{G}$ be its transfer function. Then, we have $\\|G-\\tilde{G}\\|_{\\infty} \\leq$ $\\sqrt{n}\\|\\mathbf{h}-\\tilde{\\mathbf{h}}\\|_{2}$. Therefore, the stability of the HOPE-SSM does not depend on the Markov parameters themselves. That is, the HOPE-SSM can be trained stably without reparameterization (see [30]). ### 4.3 Advantage III: A HOPE-SSM's Memory Does Not Fade\n\nWhile many LTI systems are tailored for long-term memory [29, 12], an asymptotically stable system must inevitably suffer from an exponential decay in its memory [2]. This can be manifested by the Hankel matrix: since $\\left[\\begin{array}{lll}\\mathbf{y}_{0}^{\\top} & \\mathbf{y}_{1}^{\\top} & \\cdots\\end{array}\\right]^{\\top}=\\overline{\\mathbf{H}}\\left[\\begin{array}{lll}\\mathbf{u}_{-1}^{\\top} & \\mathbf{u}_{-2}^{\\top} & \\cdots\\end{array}\\right]^{\\top}$, we can write\n\n$$\n\\mathbf{y}_{j}=\\sum_{k=1}^{\\infty} \\overline{\\mathbf{H}}_{j, k-1} \\mathbf{u}_{-k}=\\sum_{k=1}^{\\infty} \\overline{\\mathbf{H}}_{0, k+j-1} \\mathbf{u}_{-k}, \\quad j \\geq 0\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-10.jpg?height=670&width=1597&top_left_y=179&top_left_x=219)\n\nFigure 5: The test accuracy of the HOPE-SSM on the sCIFAR-10 task and the evolution of the Hankel singular values of the model.",
    "hope-9": "The plots are to be compared with Figure 2 and Figure 3. That means the effect of the input $\\mathbf{u}_{-k}$ on the output $\\mathbf{y}_{j}$ depends on the magnitude of $\\overline{\\mathbf{H}}_{0, k+j-1}$. For a discrete LTI system, $\\overline{\\mathbf{H}}_{0, k+j-1}=\\overline{\\mathbf{C A}}^{k+j-1} \\overline{\\mathbf{B}}$. Since the spectrum of $\\overline{\\mathbf{A}}$ is contained in the open unit disk, this value decays exponentially as long as $k+j-1>0$. However, if we parameterize the LTI system by the Markov parameters in the Hankel matrix, then $\\overline{\\mathbf{H}}_{0, k+j-1}=\\mathbf{h}_{k+j-1}$, which does not have to decay as long as $k+j-1<n$. When $k+j-1 \\geq n$, we have $\\overline{\\mathbf{H}}_{0, k+j-1}=0$, which means the LTI system has essentially no memory after time $n$. However, this is not an issue because given a discrete sequential input, we can control $\\Delta t$ to fit all data inside the $t \\in[0, n]$ sliding window. We will elaborate on this discussion using an experiment in section 5 . ## 5 Experiments and Discussions\n\n### 5.1 Experiment I: Singular Values of a HOPE-SSM\n\nIn this section, we implement a randomly initialized HOPE-SSM to learn the sCIFAR-10 task. We use the same model hyperparameters as the S4D models in section 3. In particular, the Hankel matrices in this model are 64 -by-64. We randomly initialize the Hankel matrix and do not set a smaller learning rate for the Hankel matrix entries h, i.e., all model parameters except for $\\Delta t$ have the same learning rate. In Figure 5, we show the test accuracy and the Hankel singular values of the HOPE-SSM. Compared to Figure 2, a random HOPE-SSM can be trained to a high accuracy. In addition, training does not reduce the numerical rank of a Hankel matrix. This corroborates our findings in Theorem 3 and Theorem 4. ### 5.2 Experiment II: HOPE-SSMs Have Long Memory\n\nIn section 4, we claim that a HOPE-SSM benefits from non-decaying memory. We show this experimentally in this section. To do so, for each flattened picture in the sCIFAR-10 dataset, which contains 1024 vectors of length 3, we append a random sequence of 1024 vectors of length 3 to the end of it. The goal is still to classify an image by its first 1024 pixels. We call this task noise-padded sCIFAR-10. This task obviously requires a long memory so that the earlier data can still be retrieved after the noises are taken. We train both an S4D model and our HOPESSM to learn this task, using a common discretization size of $\\Delta t=0.1$ (see Appendix H. 2 for different values of $\\Delta t$ ). We see in Figure 6 that the HOPE-SSM significantly outperforms the S4D model on this task. To understand this gap, we give the LTI systems in both the S4D model and our HOPE-SSM after training a unit impulse at $t=0$. We watch how the impulse response\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-11.jpg?height=553&width=1268&top_left_y=203&top_left_x=404)\n\nFigure 6: Left: the test accuracies of the S4D model and our HOPE-SSM on the noisy-sCIFAR task. Right: a unit impulse at $t=0$ is acted on the LTI system. The plot shows the decay of $|\\mathbf{y}(t)|$ as $t$ increases. Data are collected for all 512 LTI systems in a trained model. The dark curve shows the median of $|\\mathbf{y}(t)|$ over the 512 numbers. The darkly shaded region is from the first quartile to the third quartile. The lightly shaded region is from the minimum to the maximum. | Model | ListOps | Text | Retrieval | Image | Pathfinder | Path-X | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| DSS | 57.60 | 76.60 | 87.60 | 85.80 | 84.10 | 85.00 | 79.45 |\n| S4++ | 57.30 | 86.28 | 84.82 | 82.91 | 80.24 | - | - |\n| Regularized S4D | $\\mathbf{6 1 . 4 8}$ | 88.19 | 91.25 | 88.12 | 94.93 | 95.63 | 86.60 |\n| Spectral SSM | 60.33 | $\\mathbf{8 9 . 6 0}$ | 90.00 | - | $\\mathbf{9 5 . 6 0}$ | 90.10 | - |\n| S4 | 59.60 | 86.82 | 90.90 | 88.65 | 94.20 | 96.35 | 86.09 |\n| S4D | $\\underline{60.47}$ | 86.18 | 89.46 | 88.19 | 93.06 | 91.95 | 84.89 |\n| HOPE-SSM | 60.20 | $\\underline{\\mathbf{8 9 . 6 0}}$ | $\\underline{\\mathbf{9 1 . 5 2}}$ | $\\underline{\\mathbf{8 8 . 7 6}}$ | $\\underline{94.89}$ | $\\underline{\\mathbf{9 6 . 8 4}}$ | $\\underline{\\mathbf{8 6 . 9 7}}$ |\n\nTable 1: Test accuracies in the Long-Range Arena of our HOPE-SSM and other models.",
    "hope-10": "The bold numbers indicate the best performance on a task and the underlined numbers indicate the best performance within a comparable group. An entry is left blank if no result is found. The S4 and S4D results are from the original papers [14, 13]. We use the same model hyperparameters as those in the S4D model. While our HOPE-SSM is a direct substitute for the S4 and S4D models, we also include some other S4-based models, where DSS is by [16], S4++ is by [26], Regularized S4D is by [21], and Spectral SSM is by [2]. $\\mathbf{y}(t)$ decays as time goes by. In Figure 6, we see that the memory of the trained S4D model decays exponentially while that of the trained HOPE-SSM does not decay at all for $t \\in[0,64]$, where $n=64$ is the size of the Hankel matrix.",
    "hope-11": "This is exactly what we expected in eq. (8). ### 5.3 Experiment III: Performance in the Long-Range Arena\n\nFinally, we present the performance of HOPE-SSM on large datasets. We use the same model architecture as in the S4D paper [13], except that we replace the LTI blocks with our HOPE blocks. The specific model and training hyperparameters are reported in Appendix H. We show the performance of our model in Table 1, where we see that our HOPE-SSM outperforms the comparable S4 and S4D models on most tasks. ## 6 Conclusion\n\nIn this paper, we presented a new theory based on the Hankel singular values to understand the difficulties in initializing and training an SSM. We proposed a new parameterization scheme,\ncalled HOPE, that is based on the Markov parameters in a discrete Hankel operator. We proved that a HOPE-SSM can be randomly initialized, stably trained, and has a long memory. ## Acknowledgments\n\nAY was supported by the SciAI Center, funded by the Office of Naval Research under Grant Number N00014-23-1-2729. NBE would like to acknowledge NSF, under Grant No. 2319621, and the U.S. Department of Energy, under Contract Number DE-AC02-05CH11231 and DEAC02-05CH11231, for providing partial support of this work. We would also like to thank Alex Townsend, Anil Damle, and Sarah Dean for some inspiring discussions. ## References\n\n[1] Vadim Movsesovich Adamyan, Damir Zyamovich Arov, and Mark Grigor'evich Krein. Analytic properties of schmidt pairs for a hankel operator and the generalized schur-takagi problem. Matematicheskii Sbornik, 128(1):34-75, 1971. [2] Naman Agarwal, Daniel Suo, Xinyi Chen, and Elad Hazan. Spectral state space models. arXiv preprint arXiv:2312.06837, 2023. [3] Quirin Aumann and Ion Victor Gosea. Practical challenges in data-driven interpolation: dealing with noise, enforcing stability, and computing realizations. arXiv preprint $\\operatorname{arXiv:2301.04906,2023.}$\n[4] Anthony P Austin. On trigonometric interpolation in an even number of points. Electronic Transactions on Numerical Analysis, 58:271-288, 2023. [5] Jonathan Baker, Mark Embree, and John Sabino. Fast singular value decay for lyapunov solutions with nonnormal coefficients. SIAM Journal on Matrix Analysis and Applications, $36(2): 656-668,2015$. [6] Alex H Barnett. How exponentially ill-conditioned are contiguous submatrices of the fourier matrix? SIAM Review, 64(1):105-131, 2022. [7] Arup Bose. Patterned random matrices. Chapman and Hall/CRC, 2018. [8] W\u0142odzimierz Bryc, Amir Dembo, and Tiefeng Jiang. Spectral measure of large random Hankel, Markov and Toeplitz matrices.",
    "hope-12": "2006. [9] Keith Glover. All optimal Hankel-norm approximations of linear multivariable systems and their L, $\\infty$-error bounds.",
    "hope-13": "International journal of control, 39(6):1115-1193, 1984. [10] Leslie Greengard and June-Yub Lee. Accelerating the nonuniform fast fourier transform.",
    "hope-14": "SIAM review, 46(3):443-454, 2004. [11] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections.",
    "hope-15": "Advances in neural information processing systems, 33:1474-1487, 2020. [13] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [14] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [15] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections.",
    "hope-16": "International Conference on Learning Representations, 2023. [16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:2298222994, 2022. [17] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. International Conference on Learning Representations, 2023. [18] Boris Kramer and Alex A Gorodetsky. System identification via cur-factored hankel approximation. SIAM Journal on Scientific Computing, 40(2):A848-A866, 2018. [19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.",
    "hope-17": "2009. [20] Alan J. Laub, Michaelt Heath, C Paige, and R Ward. Computation of system balancing transformations and other applications of simultaneous diagonalization algorithms.",
    "hope-18": "IEEE Trans. Automat. Contr., 32(2):115-122, 1987. [21] Fusheng Liu and Qianxiao Li. From generalization analysis to optimization designs for state space models. arXiv preprint arXiv:2405.02670, 2024. [22] Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):1-73, 2021. [23] Charles H Martin, Tongsu Peng, and Michael W Mahoney. Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data.",
    "hope-19": "Nature Communications, 12(1):4122, 2021. [24] VV Nekrutkin. Remark on the norm of random hankel matrices. Vestnik St. Petersburg University: Mathematics, 46:189-192, 2013. [25] Vladimir Peller. An introduction to hankel operators. In Hankel Operators and Their Applications, pages 1-59.",
    "hope-20": "Springer, 2003. [26] Biqing Qi, Junqi Gao, Dong Li, Kaiyan Zhang, Jianxing Liu, Ligang Wu, and Bowen Zhou. S4++: Elevating long sequence modeling with state memory reply.",
    "hope-21": "2024.",
    "hope-22": "[27] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [29] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [30] Shida Wang and Qianxiao Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization.",
    "hope-23": "arXiv preprint arXiv:2311.14495, 2023. [31] Heather Wilber, Ethan N Epperly, and Alex H Barnett. A superfast direct inversion method for the nonuniform discrete fourier transform. arXiv preprint arXiv:2404.13223, 2024. [32] Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, and N. Benjamin Erichson. Robustifying state-space models for long sequences via approximate diagonalization. In The Twelfth International Conference on Learning Representations, 2024. [33] Annan Yu and Alex Townsend. On the stability of unevenly spaced samples for interpolation and quadrature. BIT Numerical Mathematics, 63(2):23, 2023. [34] Annan Yu and Alex Townsend. Leveraging the hankel norm approximation and data-driven algorithms in reduced order modeling. Numerical Linear Algebra with Applications, page e2555, 2024. [35] Kemin Zhou and John Comstock Doyle. Essentials of robust control, volume 104. Prentice Hall, Upper Saddle River, NJ, 1998. ## Supplementary Material\n\nThis is the supplementary material for the paper titled \"There is HOPE to Avoid HiPPOs for Long-memory State Space Models.\" It is organized as follows. In Appendix A, we survey the background of the bilinear transform of the LTI systems, the Mobius transform of the transfer function domains, and the Hankel operators more thoroughly. In Appendix B, we survey the basic properties of Hankel singular values and different strategies to compute them. The proofs of Theorem 1, Theorem 2, and Theorem 3-4 are given in Appendix C, Appendix D, and Appendix E, respectively. In Appendix F, we conduct some numerical experiments to corroborate the four theorems presented in the main text. In Appendix G, we carefully derive how we can use the nonuniform samples of the transfer function to enable a different discretization step size $\\Delta t$. Finally, the details of the experiments shown in the main text are given in Appendix H. ## A More background on the LTI system and Hankel operators\n\nThe purpose of this section is to provide a more thorough exposition of section 2 , which leaves out some concepts that are unnecessary in order to understand the main text. Let $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ be a continuous-time LTI system defined in eq. (1). One can take a bilinear transformation to obtain a discrete LTI system:\n\n$$\n\\overline{\\mathbf{A}}=(\\mathbf{I}+\\mathbf{A})(\\mathbf{I}-\\mathbf{A})^{-1}, \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}+\\overline{\\mathbf{A}}) \\mathbf{B} / \\sqrt{2}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}(\\mathbf{I}+\\overline{\\mathbf{A}}) / \\sqrt{2}, \\quad \\overline{\\mathbf{D}}=\\mathbf{D}+\\overline{\\mathbf{C}}(\\mathbf{I}-\\overline{\\mathbf{A}})^{-1} \\overline{\\mathbf{B}}\n$$\n\nThe bilinear transformation is invertible and defines a one-to-one correspondence between $\\Gamma$ in eq. (1) and $\\bar{\\Gamma}=(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}, \\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})$ given by\n\n$$\n\\begin{aligned}\n\\overline{\\mathbf{x}}_{k+1} & =\\overline{\\mathbf{A}}_{\\mathbf{x}_{k}}+\\overline{\\mathbf{B}} \\overline{\\mathbf{u}}_{k} \\\\\n\\overline{\\mathbf{y}}_{k} & =\\overline{\\mathbf{C}} \\overline{\\mathbf{x}}_{k}+\\overline{\\mathbf{D}} \\overline{\\mathbf{u}}_{k}\n\\end{aligned}\n$$\n\nThe transfer functions of $\\Gamma$ and $\\bar{\\Gamma}$ are\n\n$$\nG(s)=\\mathbf{C}(s \\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{B}+\\mathbf{D} \\quad \\text { and } \\quad \\bar{G}(z)=\\overline{\\mathbf{C}}(z \\mathbf{I}-\\overline{\\mathbf{A}})^{-1} \\overline{\\mathbf{B}}+\\overline{\\mathbf{D}}\n$$\n\nrespectively. The two transfer functions are equivalent via a Mobius transformation:\n\n$$\nG(s)=\\bar{G}((1+s) /(1-s)) \\quad \\Leftrightarrow \\quad \\bar{G}(z)=G((z-1) /(z+1))\n$$\n\nThe importance of the transfer functions is that they bring the inputs to the outputs in the Laplace domain by multiplication, which reduces to the Fourier domain if we assume that $\\mathbf{u}$ is bounded and compactly supported:\n\n$$\n\\hat{\\mathbf{y}}(s)=G(i s) \\hat{\\mathbf{u}}(s), \\quad \\hat{\\hat{\\mathbf{y}}}=\\bar{G}(\\boldsymbol{\\omega}) \\hat{\\overline{\\mathbf{u}}}\n$$\n\nwhere the hat of a function and a vector means the Fourier transform and the Fourier coefficients, respectively, and $\\boldsymbol{\\omega}$ is the vector of roots of unity. Given a continuous-time LTI system $\\Gamma$, one can define its Hankel operator by\n\n$$\n\\mathbf{H}: L^{2}(0, \\infty) \\rightarrow L^{2}(0, \\infty), \\quad(\\mathbf{H v})(t)=\\int_{0}^{\\infty} \\mathbf{C} \\exp ((t+\\tau) \\mathbf{A}) \\mathbf{B v}(\\tau) d \\tau\n$$\n\nThe Hankel operator maps the past inputs to the future outputs, i.e., if $\\mathbf{u}(t)=0$ for $t \\geq 0$ and we define $\\mathbf{v}(t)=\\mathbf{u}(-t)$, then we have $\\mathbf{y}(t)=(\\mathbf{H v})(t)$. Analogously, the Hankel matrix of a discrete LTI system $\\bar{\\Gamma}$ is a doubly infinite matrix defined by\n\n$$\n\\overline{\\mathbf{H}}: \\ell^{2} \\rightarrow \\ell^{2}, \\quad \\overline{\\mathbf{H}}_{i, j}=\\overline{\\mathbf{C A}}^{i+j} \\overline{\\mathbf{B}}, \\quad i, j \\geq 0\n$$\n\nThe Hankel matrix has the similar physical interpretation: if $\\overline{\\mathbf{u}}_{k}=0$ for all $k \\geq 0$, then we have $\\left[\\begin{array}{lll}\\overline{\\mathbf{y}}_{0}^{\\top} & \\overline{\\mathbf{y}}_{1}^{\\top} & \\cdots\\end{array}\\right]^{\\top}=\\overline{\\mathbf{H}}\\left[\\begin{array}{lll}\\overline{\\mathbf{u}}_{-1}^{\\top} & \\overline{\\mathbf{u}}_{-2}^{\\top} & \\cdots\\end{array}\\right]^{\\top}$. Both $\\mathbf{H}$ and $\\overline{\\mathbf{H}}$ are bounded linear operators of rank $\\leq n$, the number of latent states. In fact, the singular values $\\sigma_{1}(\\mathbf{H}) \\geq \\sigma_{2}(\\mathbf{H}) \\geq \\cdots \\geq \\sigma_{n}(\\mathbf{H}) \\geq 0$ of $\\mathbf{H}$ are equivalent to those of $\\overline{\\mathbf{H}}$, and they are called the Hankel singular values of $\\Gamma$ and $\\bar{\\Gamma}$. ## B More background on Hankel singular values\n\nIn section 2, we define the Hankel singular values $\\sigma_{1}, \\ldots, \\sigma_{n}$ to be the singular values of the finite-rank bounded linear Hankel operator on a separable Hilbert space. Throughout the paper, we treated them as black boxes and used their distributions to understand the performance of SSMs. The goal of this section is to open the black boxes and introduce more useful properties of the Hankel singular values. Note that the proof of Theorem 1 to be presented in Appendix C heavily relies on the background discussed in this section. The concepts in this section can be presented on a continuous-time LTI system $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ or analogously on a discrete LTI system $(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}, \\overline{\\mathbf{C}}, \\overline{\\mathbf{D}})$. We focus mainly on a continuous-time system for cleanliness. Since $\\mathbf{D}$ has no effect on the Hankel singular values, we further assume that $\\mathbf{D}=\\mathbf{0}$ and write only $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C})$. ## B. 1 Hankel singular values from balanced realization\n\nOne of the most popular ways to compute the Hankel singular values is via the so-called balanced realization.",
    "hope-24": "Assume $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C})$ is asymptotically stable, i.e. the spectrum of $\\mathbf{A}$ is contained in the open left half-plane. Then, there exist two Hermitian and positive semi-definite matrices $\\mathbf{P} \\in \\mathbb{C}^{n \\times n}$ and $\\mathbf{Q} \\in \\mathbb{C}^{n \\times n}$ such that\n\n$$\n\\begin{aligned}\n\\mathbf{A P}+\\mathbf{P A}^{*}+\\mathbf{B B}^{*} & =\\mathbf{0} \\\\\n\\mathbf{A}^{*} \\mathbf{Q}+\\mathbf{Q A}+\\mathbf{C}^{*} \\mathbf{C} & =\\mathbf{0}\n\\end{aligned}\n$$\n\nThe equations in eq. (10) are called the Lyapunov equations and $\\mathbf{P}$ and $\\mathbf{Q}$ are called the controllability Gramian and the observability Gramian, respectively. They can be explicitly expressed by the following matrix integrals:\n\n$$\n\\mathbf{P}=\\int_{0}^{\\infty} \\exp (\\mathbf{A} t) \\mathbf{B B}^{*} \\exp \\left(\\mathbf{A}^{*} t\\right) d t, \\quad \\mathbf{Q}=\\int_{0}^{\\infty} \\exp \\left(\\mathbf{A}^{*} t\\right) \\mathbf{C}^{*} \\mathbf{C} \\exp (\\mathbf{A} t) d t\n$$\n\nThe controllability Gramian $\\mathbf{P}$ is positive definite if and only if the system is controllable, i.e., for any $\\mathbf{x}_{0}, \\mathbf{x}_{1} \\in \\mathbb{C}^{n}$ and any $T>0$, there exists an input $\\mathbf{u}$ on $[0, T]$ that makes $\\mathbf{x}(T)=\\mathbf{x}_{1}$ when $\\mathbf{x}(0)=\\mathbf{x}_{0}$. Likewise, $\\mathbf{Q}$ is positive definite if and only if the system is observable, i.e., for any $T>0$, the initial state $\\mathbf{x}(0)$ can be determined by the input $\\mathbf{u}$ and the output $\\mathbf{y}$ on $[0, T][35]$. The singular values of $\\mathbf{P Q}$ turn out to be exactly the squares of the Hankel singular values, i.e., $\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}$, of the system $\\Gamma$. In general, $\\mathbf{P}$ and $\\mathbf{Q}$ are dense matrices. However, one can use the so-called balanced realization algorithm [20] to compute an equivalent system $\\Gamma_{b}=$ $\\left(\\mathbf{A}_{b}, \\mathbf{B}_{b}, \\mathbf{C}_{b}\\right)=\\left(\\mathbf{V}^{-1} \\mathbf{A V}, \\mathbf{V}^{-1} \\mathbf{B}, \\mathbf{C V}\\right)$, where $\\mathbf{V} \\in \\mathbb{C}^{n \\times n}$ is an invertible matrix, so that its Gramians $\\mathbf{P}_{b}$ and $\\mathbf{Q}_{b}$ are equivalent and diagonal, i.e.,\n\n$$\n\\mathbf{P}_{b}=\\mathbf{Q}_{b}=\\operatorname{diag}\\left(\\sigma_{1}, \\ldots, \\sigma_{n}\\right)\n$$\n\nBalanced realization is an algebraic method that is based on singular value decompositions (SVDs). In practice, it is a very popular method to compute the Hankel singular values of a system. ## B. 2 Hankel singular values as a rational approximation problem\n\nThe balanced realization gives us a good way to compute the Hankel singular values in practice. However, it does not offer too much insight in theoretically analyzing them. The theory of Hankel singular values is usually derived via function approximation. Here, we introduce how a Hankel singular value can be reframed as the solution to a rational approximation problem. To this end, we let $H_{+}^{\\infty}$ (resp. $H_{-}^{\\infty}$ ) be the Hardy space with functions $h: \\mathbb{C} \\rightarrow \\mathbb{C}^{m \\times p}$ that are\nbounded and analytic in the open right (resp. left) half-plane. The non-tangential limit of $h$ exists almost everywhere on the imaginary axis, and by the Maximum Modulus Principle, we must have\n\n$$\n\\|h\\|_{\\infty}:=\\operatorname{ess} \\sup _{\\operatorname{Re}(s)=0}\\|h(s)\\|_{2}=\\sup _{\\operatorname{Re}(s)>0}\\|h(s)\\|_{2}, \\quad h \\in H_{+}^{\\infty}\n$$\n\nand\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-17.jpg?height=100&width=1049&top_left_y=524&top_left_x=509)\n\nThe Adamyan-Arov-Krein (AAK) Theory [1] says the following:\n\n$$\n\\sigma_{k+1}=\\inf _{R_{k}, F}\\left\\|G-R_{k}-F\\right\\|_{\\infty}, \\quad 0 \\leq k \\leq n-1\n$$\n\nwhere $R_{k}$ ranges over all rational functions with at most $k$ poles, all contained in the open left half-plane, and $F$ ranges over $H_{-}^{\\infty}$.",
    "hope-25": "In fact, as discussed in section $2, R_{k}$ is the transfer function of a reduced system $\\tilde{\\Gamma}=(\\tilde{\\mathbf{A}}, \\tilde{\\mathbf{B}}, \\tilde{\\mathbf{C}}, \\tilde{\\mathbf{D}})$ with $\\tilde{\\mathbf{A}} \\in \\mathbb{C}^{k \\times k}$. Hence, eq. (12) tells us that if the transfer function of $\\Gamma$ can be well-approximated by rational functions, then it has fast-decaying Hankel singular values. The other direction is not true, due to the existence of $F$. That is, if the Hankel singular values decay fast, then it does not necessarily mean that $G$ can be wellapproximated by rational functions. However, section 2 shows that this is true if the sum of the tails of the Hankel singular values decay rapidly. ## C Proof of Theorem 1\n\nLet $\\bar{G}$ be the transfer function of a random system $\\bar{\\Gamma}_{2}$, i.e.,\n\n$$\n\\bar{G}(z)=\\overline{\\mathbf{C}}(z \\mathbf{I}-\\overline{\\mathbf{A}})^{-1} \\overline{\\mathbf{B}}+\\overline{\\mathbf{D}}\n$$\n\nBy the AAK theory, the Hankel singular values of $\\bar{\\Gamma}$ can be studied via the rational approximation of $\\bar{G}$. Since the matrix $\\overline{\\mathbf{D}}$ does not affect the Hankel singular values of a system, we assume, without loss of generality, that $\\overline{\\mathbf{D}}=\\mathbf{0}$. We let $\\sigma_{1}, \\ldots, \\sigma_{n}$ be the random variables that are equal to the singular values of the random LTI system $\\bar{\\Gamma}_{2}$. We approach Theorem 1 in three steps:\n\n1. We separate out the poles of the transfer function $G$ of $(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}, \\overline{\\mathbf{C}}, \\mathbf{0})$ that are close to the boundary of the unit disk. This breaks $\\bar{G}$ into two low-rank systems $\\bar{G}_{1}$ and $\\bar{G}_{2}$, where $\\bar{G}_{1}$ is low-rank because it has few poles and $\\bar{G}_{2}$ is low-rank because its poles are far away from the boundary. 2. We then estimate the decay of the singular values of $\\bar{G}_{2}$ using the information of the maximum moduli of its poles. 3. Finally, we control $\\sigma_{1}$ in probability. This gives a control on the relative singular values.",
    "hope-26": "We will make these three steps into three lemmas and use them to derive the result at the end. Lemma 1. Given $\\gamma>0$, with probability at least $1-\\delta$, there are at most $n^{\\beta}$ poles of $\\bar{G}(z)$ outside the disk $D\\left(0,1-n^{-\\gamma}\\right)$, where\n\n$$\n\\beta=1+\\log _{n}\\left(n^{-\\gamma \\alpha}+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 n}}\\right)\n$$\n\nProof. Let $Z$ be the number of poles inside $D\\left(0,1-n^{-\\gamma}\\right)$. Then, $Z$ has a binomial distribution\n\n$$\nZ \\sim B\\left(n, 1-n^{-\\gamma \\alpha}\\right)\n$$\n\nFrom Hoeffding's inequality, we have\n\n$$\n\\mathbb{P}\\left(Z \\leq n-n^{\\beta}\\right) \\leq \\exp \\left(-2 n\\left(1-n^{-\\gamma \\alpha}-\\frac{n-n^{\\beta}}{n}\\right)^{2}\\right)=\\exp \\left(-2 n\\left(-n^{-\\gamma \\alpha}+n^{\\beta-1}\\right)^{2}\\right)\n$$\n\nSet\n\n$$\n\\beta=1+\\log _{n}\\left(n^{-\\gamma \\alpha}+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 n}}\\right)=1-\\gamma \\alpha+\\log _{n}\\left(1+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 n^{1-2 \\gamma \\alpha}}}\\right)\n$$\n\nThen, we have\n\n$$\nn^{\\beta-1}=n^{-\\gamma \\alpha}+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 n}}\n$$\n\nso that\n\n$$\n\\mathbb{P}\\left(Z \\leq n-n^{\\beta}\\right) \\leq \\exp \\left(-2 n \\frac{\\ln (1 / \\delta)}{2 n}\\right)=\\delta\n$$\n\nThis finishes the proof. Lemma 2. For any $\\gamma>0$, let the random variable $k$ be the number of poles of $\\bar{G}(z)$ inside $D\\left(0,1-n^{-\\gamma}\\right)$. Let $\\kappa>\\gamma$ be given. Then, with conditional probability (given $k$ ) at least $1-\\delta$, we have\n\n$$\n\\sigma_{(n-k)+n^{\\kappa}+2} \\leq \\mathcal{O}\\left(\\sqrt{n^{\\gamma+1}(\\ln (1 / \\delta)+n)}\\right) e^{-\\left(n^{(\\kappa-\\gamma)}\\right)}\n$$\n\nwhere the constant in $\\mathcal{O}$ is universal. Proof. Let $z_{1}, \\ldots, z_{k}$ be the poles of $\\bar{G}(z)$ inside $D\\left(0,1-n^{-\\gamma}\\right)$. Assume $G(z)$ can be written as\n\n$$\n\\bar{G}(z)=\\bar{G}_{1}(z)+\\sum_{i=1}^{k} \\frac{c_{i}}{z-z_{i}}\n$$\n\nwhere $\\bar{G}_{1}(z)$ is a degree- $(n-k)$ rational function with poles inside the annulus of inner radius $1-n^{-\\gamma}$ and outer radius 1 and $c_{i}$ 's are i.i.d. random variables with distribution $\\mathcal{N}(0,1)$. We can further write\n\n$$\n\\sum_{i=1}^{k} \\frac{c_{i}}{z-z_{i}}=\\sum_{i=1}^{k} c_{i} \\sum_{j=0}^{\\infty} z_{i}^{j} z^{-j-1}=\\sum_{j=1}^{\\infty} z^{-1-j}\\left(\\sum_{i=1}^{k} c_{i} z_{i}^{j}\\right)\n$$\n\nBy the AAK theory, for any $K>0$, we have\n\n$$\n\\sigma_{(n-k)+K} \\leq \\sup _{|z|=1}\\left|\\sum_{j=K-2}^{\\infty} z^{-1-j}\\left(\\sum_{i=1}^{k} c_{i} z_{i}^{j}\\right)\\right| \\leq \\sum_{j=K-2}^{\\infty}\\left\\|\\mathbf{c} \\mathbf{z}^{j}\\right\\|_{1} \\leq\\|\\mathbf{c}\\|_{2} \\sum_{j=K-2}^{\\infty}\\left\\|\\mathbf{z}^{j}\\right\\|_{2}\n$$\n\nwhere $\\mathbf{c}=\\left[c_{1}, \\ldots, c_{k}\\right]^{\\top}$ and $\\mathbf{z}^{j}=\\left[z_{1}^{j}, \\ldots, z_{k}^{j}\\right]^{\\top}$, and the last step follows from H\u00f6lder's inequality. Since $\\|\\mathbf{c}\\|_{2}^{2}$ follows the $\\chi_{k}^{2}$ distribution, for $M>n$, we have that\n\n$$\nP\\left(\\|\\mathbf{c}\\|_{2}^{2}>M\\right) \\leq \\exp \\left(-\\frac{n}{2}\\left(\\frac{M}{n}-1-\\ln \\left(\\frac{M}{n}\\right)\\right)\\right)\n$$\n\nHence, there exists a universal constant $C>0$ such that when $M \\geq C(\\ln (1 / \\delta)+n)^{1}$, we have\n\n$$\nP\\left(\\|\\mathbf{c}\\|_{2}^{2}>M\\right) \\leq \\exp \\left(-\\frac{n}{2} \\frac{M}{n}\\right) \\leq \\delta\n$$\n\n[^1]Moreover, since $\\left|z_{i}^{j}\\right| \\leq\\left(1-n^{-\\gamma}\\right)^{j}$ for all $1 \\leq i \\leq k$, we have\n\n$$\n\\left\\|\\mathbf{z}^{j}\\right\\|_{2} \\leq \\sqrt{n}\\left(1-n^{-\\gamma}\\right)^{j}\n$$\n\nThat is, with probability at least $1-\\delta$, we have\n\n$$\n\\sigma_{(n-k)+K} \\leq \\mathcal{O}(\\sqrt{\\ln (1 / \\delta)+n}) \\sqrt{n} \\sum_{j=K-2}^{\\infty}\\left(1-n^{-\\gamma}\\right)^{j}=\\mathcal{O}\\left(\\sqrt{n^{\\gamma+1}(\\ln (1 / \\delta)+n)}\\right)\\left(1-n^{-\\gamma}\\right)^{K-2}\n$$\n\nSuppose $K \\geq n^{\\kappa}+2$. Then, we have\n\n$$\n\\left(1-n^{-\\gamma}\\right)^{K-2} \\leq\\left(1-n^{-\\gamma}\\right)^{\\left(n^{\\kappa}\\right)}=\\left(\\left(1-n^{-\\gamma}\\right)^{\\left(n^{\\gamma}\\right)}\\right)^{\\left(n^{\\kappa-\\gamma}\\right)}\n$$\n\nSince $\\left(1-n^{-\\gamma}\\right)^{n^{\\gamma}} \\rightarrow e^{-1}$ as $n \\rightarrow \\infty$, if $\\kappa>\\gamma$, then $\\left(1-n^{-\\gamma}\\right)^{K-2}$ decays faster than any negative power of $n$. Hence, we have\n\n$$\n\\sigma_{(n-k)+K} \\leq \\mathcal{O}\\left(\\sqrt{n^{\\gamma+1}(\\ln (1 / \\delta)+n)}\\right) e^{-\\left(n^{(\\kappa-\\gamma)}\\right)}\n$$\n\nas desired. Lemma 3. With probability at least $1-\\delta$, the leading Hankel singular value $\\sigma_{1}$ satisfies\n\n$$\n\\sigma_{1} \\geq \\mathcal{O}(\\sqrt{n} \\delta)\n$$\n\nwhere the constant in $\\mathcal{O}$ is universal. Proof. The leading Hankel singular value $\\sigma_{1}$ is equivalent to the spectral norm of the Hankel matrix\n\n$$\n\\left[\\begin{array}{cccc}\n\\overline{\\mathbf{C B}} & \\overline{\\mathbf{C A B}} & \\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}} & \\cdots \\\\\n\\overline{\\mathbf{C A B}} & \\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}} & \\cdots & \\cdots \\\\\n\\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}} & \\vdots & \\ddots & \\vdots \\\\\n\\vdots & \\vdots & \\cdots & \\ddots\n\\end{array}\\right] . $$\n\nHence, we have\n\n$$\n\\sigma_{1} \\geq|\\overline{\\mathbf{C B}}|\n$$\n\nwhere $\\overline{\\mathbf{C B}} / \\sqrt{n} \\sim \\mathcal{N}(0,1)$. Hence, if we set $M=C \\sqrt{n} \\delta$ for some universal constant $C>0$, we have\n\n$$\nP\\left(\\sigma_{1} \\leq M\\right) \\leq P(|\\overline{\\mathbf{C B}}| \\leq M)=P(|\\overline{\\mathbf{C B}}| / \\sqrt{n} \\leq M / \\sqrt{n}) \\leq \\delta\n$$\n\nThis completes the proof. Now, let's assemble our ultimate statement. Proof of Theorem 1. By Lemma 2 and Lemma 3, with probability at least $1-\\delta / 2$, we have that\n\n$$\n\\frac{\\sigma_{\\left(n-k+n^{\\kappa}+2\\right)}}{\\sigma_{1}} \\leq \\mathcal{O}\\left(\\sqrt{n^{\\gamma}(\\ln (1 / \\delta)+n)} \\delta^{-1}\\right) e^{-\\left(n^{(\\kappa-\\gamma)}\\right)}\n$$\n\nHence, if we set $\\kappa$ so that\n\n$$\nn^{\\kappa} \\geq n^{\\gamma} \\ln \\left(C \\delta^{-1} \\epsilon^{-1} \\sqrt{n^{\\gamma}(\\log (1 / \\delta)+n)}\\right)=n^{\\gamma} \\ln \\left(\\mathcal{O}\\left(\\delta^{-3 / 2} \\epsilon^{-1} n^{(\\gamma+1) / 2}\\right)\\right)\n$$\n\nfor a sufficiently large universal constant $C>0$, then we guarantee that\n\n$$\n\\frac{\\sigma_{\\left(n-k+n^{\\kappa}+2\\right)}}{\\sigma_{1}} \\leq \\epsilon\n$$\n\nBy Lemma 1, we have that with probability at least $1-\\delta / 2$,\n\n$$\nn-k \\leq n^{\\beta}, \\quad \\beta=1+\\log _{n}\\left(n^{-\\gamma \\alpha}+\\sqrt{\\frac{\\ln (2 / \\delta)}{2 n}}\\right)\n$$\n\nSet $\\gamma=1 /(1+\\alpha)$. Then, we have\n\n$$\n\\begin{aligned}\n\\beta & =1+\\log _{n}\\left(n^{-\\gamma \\alpha}\\left(1+\\sqrt{\\frac{\\ln (2 / \\delta)}{2 n^{1-2 \\gamma \\alpha}}}\\right)\\right)=1-\\gamma \\alpha+\\log _{n}\\left(1+\\sqrt{\\frac{\\ln (2 / \\delta)}{2 n^{1-2 \\gamma \\alpha}}}\\right) \\\\\n& \\leq \\frac{1}{1+\\alpha}+\\log _{n}(1+\\sqrt{\\ln (2 / \\delta) / 2})<\\frac{1}{1+\\alpha}+\\frac{\\ln (2+\\sqrt{\\ln (1 / \\delta) / 2})}{\\ln (n)}\n\\end{aligned}\n$$\n\nSince\n\n$$\nn^{\\kappa}+2=\\mathcal{O}\\left(n^{\\gamma} \\ln \\left(\\delta^{-3 / 2} \\epsilon^{-1} n\\right)\\right) \\leq \\mathcal{O}\\left(n^{\\beta} \\ln \\left(\\delta^{-3 / 2} \\epsilon^{-1} n\\right)\\right)\n$$\n\nThe claim is proved. ## D Proof of Theorem 2\n\nIn this section, we prove Theorem 2. Our proof focuses on the worst-case perturbation by construction. As a consequence, it simultaneously proves the sharpness of the result. Intuitively, consider a rational function\n\n$$\ns \\mapsto \\frac{b c}{s-a}\n$$\n\nsince we only care about its values on the imaginary axis, the closer the pole $a$ is to the imaginary axis, the less stable it is. On the other hand, it is obvious that the size of $b c$ also controls the (absolute) conditioning of the rational function.",
    "hope-27": "We state the rigorous proof below. Proof of Theorem 2. Without loss of generality, we assume that $\\mathbf{B}=\\tilde{\\mathbf{B}}=\\left[\\begin{array}{llll}1 & 1 & \\cdots & 1\\end{array}\\right]^{\\top} .^{2}$ The transfer functions of $\\Gamma$ and $\\tilde{\\Gamma}$ are\n\n$$\nG(s)=\\sum_{j=1}^{n} \\frac{c_{j}}{s-s_{j}} \\quad \\text { and } \\quad \\tilde{G}(s)=\\sum_{j=1}^{n} \\frac{\\tilde{c}_{j}}{s-\\tilde{s}_{j}}\n$$\n\nrespectively. Then, for any $s$ on the imaginary axis, we have\n\n$$\n\\begin{aligned}\n\\left|\\frac{c_{j}}{s-s_{j}}-\\frac{\\tilde{c}_{j}}{s-\\tilde{s}_{j}}\\right| & =\\left|\\frac{c_{j}\\left(s-\\tilde{s}_{j}\\right)-\\tilde{c}_{j}\\left(s-s_{j}\\right)}{\\left(s-s_{j}\\right)\\left(s-\\tilde{s}_{j}\\right)}\\right|=\\left|\\frac{c_{j} s-c_{j} \\tilde{s}_{j}-\\tilde{c}_{j} s+\\tilde{c}_{j} s_{j}+\\tilde{c}_{j} \\tilde{s}_{j}-\\tilde{c}_{j} \\tilde{s}_{j}}{\\left(s-s_{j}\\right)\\left(s-\\tilde{s}_{j}\\right)}\\right| \\\\\n& \\leq \\frac{\\left|c_{j}-\\tilde{c}_{j}\\right|\\left|s-\\tilde{s}_{j}\\right|+\\left|\\tilde{c}_{j}\\right|\\left|s_{j}-\\tilde{s}_{j}\\right|}{\\left|s-s_{j}\\right|\\left|s-\\tilde{s}_{j}\\right|}=\\frac{\\left|c_{j}-\\tilde{c}_{j}\\right|}{\\left|s-s_{j}\\right|}+\\frac{\\left|\\tilde{c}_{j}\\right|\\left|s_{j}-\\tilde{s}_{j}\\right|}{\\left|s-s_{j}\\right|\\left|s-\\tilde{s}_{j}\\right|} \\\\\n& \\leq \\frac{\\Delta_{\\mathbf{B}}}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|}+\\frac{2\\left|c_{j}\\right| \\Delta_{\\mathbf{A}}}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|^{2} / 2}\n\\end{aligned}\n$$\n\n[^2]Hence, we have\n\n$$\n\\begin{aligned}\n\\|G-\\tilde{G}\\|_{\\infty} & \\leq \\sum_{j=1}^{n}\\left(\\frac{\\Delta_{\\mathbf{B}}}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|}+4 \\frac{\\left|c_{j}\\right| \\Delta_{\\mathbf{A}}}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|^{2}}\\right) \\\\\n& \\leq n \\Delta_{\\mathbf{B}} \\max _{j} \\frac{1}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|}+4 n \\Delta_{\\mathbf{A}} \\max _{j} \\frac{\\left|c_{j}\\right|}{\\mid \\operatorname{Re}\\left(s_{j}\\right)^{2}}\n\\end{aligned}\n$$\n\nThis proves the upper bound. To prove the lower bound, let $j_{1}$ be an index that maximizes $1 /\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|$ and $j_{2}$ be an index that maximizes $\\left|c_{j}\\right| /\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|^{2}$. Define $\\tilde{\\Gamma}_{\\mathbf{B}}$ by perturbing $c_{j_{1}}$ to $c_{j_{1}}+\\Delta_{\\mathbf{B}}$. Then, we have\n\n$$\n\\left\\|G-\\tilde{G}_{\\mathbf{B}}\\right\\|_{\\infty}=\\left\\|\\frac{\\Delta_{\\mathbf{B}}}{s-s_{j_{1}}}\\right\\|_{\\infty}=\\Delta_{\\mathbf{B}} \\max _{j} \\frac{1}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|}\n$$\n\nDefine $\\tilde{\\Gamma}_{\\mathbf{A}}$ by perturbing $s_{j_{2}}$ to $s_{j_{2}}+\\Delta_{\\mathbf{A}}$. Then, we have\n\n$$\n\\begin{aligned}\n\\left\\|G-\\tilde{G}_{\\mathbf{A}}\\right\\|_{\\infty} & =\\left|c_{j_{2}}\\right|\\left\\|\\frac{1}{s-s_{j_{2}}}-\\frac{1}{s-s_{j_{2}}+\\Delta_{\\mathbf{A}}}\\right\\|_{\\infty}=\\left|c_{j_{2}}\\right| \\Delta_{\\mathbf{A}}\\left\\|\\frac{1}{\\left(s-s_{j_{2}}\\right)\\left(s-s_{j_{2}}+\\Delta_{\\mathbf{A}}\\right)}\\right\\|_{\\infty} \\\\\n& \\geq\\left|c_{j_{2}}\\right| \\Delta_{\\mathbf{A}} \\frac{1}{\\left|\\operatorname{Re}\\left(s_{j_{2}}\\right)\\right|^{2}}=\\Delta_{\\mathbf{A}} \\max _{j} \\frac{\\left|c_{j}\\right|}{\\left|\\operatorname{Re}\\left(s_{j}\\right)\\right|^{2}}\n\\end{aligned}\n$$\n\nThis proves the sharpness of the theorem. ## E Proof of Theorem 3 and Theorem 4\n\nThe proof of Theorem 3 is a straightforward assembly of two results in random matrix theory. The first result, due to [24], controls the Hankel norm $\\sigma_{1}\\left(\\overline{\\mathbf{H}}_{n}\\right)$ of a random Hankel matrix, whereas the second result by [8] studies the distribution of all absolute singular values $\\sigma_{j}\\left(\\overline{\\mathbf{H}}_{n}\\right)$ of a random Hankel matrix. Our study of the relative Hankel singular values is achieved by taking the quotient of the subjects of the two prior works. Proof of Theorem 3. By [24], with probability 1, we have that\n\n$$\n\\sigma_{1}\\left(\\overline{\\mathbf{H}}_{n}\\right)=\\left\\|\\overline{\\mathbf{H}}_{n}\\right\\|=\\mathcal{O}(\\sqrt{n \\ln n})\n$$\n\nDefine $\\overline{\\mathbf{K}}_{n}=\\overline{\\mathbf{H}}_{n}(1:\\lceil n / 2\\rceil, 1:\\lceil n / 2\\rceil)$. Then, by [8], with probability 1, we have that $\\mu\\left(\\overline{\\mathbf{K}}_{n} / \\sqrt{n}\\right)$ converges in distribution to a fixed probability measure, where\n\n$$\n\\mu\\left(\\overline{\\mathbf{K}}_{n} / \\sqrt{n}\\right)=\\frac{1}{\\lceil n / 2\\rceil} \\sum_{j=1}^{\\lceil n / 2\\rceil} \\delta_{\\lambda_{j}\\left(K_{n} / \\sqrt{n}\\right)}\n$$\n\nis the spectral measure of $\\overline{\\mathbf{K}}_{n} / \\sqrt{n}$. Since $\\overline{\\mathbf{K}}_{n}$ is symmetric, the singular values of $\\overline{\\mathbf{K}}_{n}$ are the moduli of the eigenvalues of $\\overline{\\mathbf{K}}_{n}$. Hence, fix some $\\epsilon>0$, we have that\n\n$$\n\\left|\\left\\{j \\mid \\sigma_{j}\\left(\\overline{\\mathbf{K}}_{n}\\right) / \\sigma_{1}\\left(\\overline{\\mathbf{H}}_{n}\\right)>\\epsilon / \\sqrt{\\ln (n)}\\right\\}\\right|=\\Omega(n) . $$\n\nSince $\\overline{\\mathbf{K}}_{n}$ is a submatrix of $\\overline{\\mathbf{H}}_{n}$, we have $\\sigma_{j}\\left(\\overline{\\mathbf{H}}_{n}\\right) \\geq \\sigma_{j}\\left(\\overline{\\mathbf{K}}_{n}\\right)$ for all $1 \\leq j \\leq\\lceil n / 2\\rceil$. Hence, its $(\\epsilon / \\sqrt{\\ln (n)})$-rank can be controlled as\n\n$$\n\\left|\\left\\{j \\mid \\sigma_{j}\\left(\\overline{\\mathbf{H}}_{n}\\right) / \\sigma_{1}\\left(\\overline{\\mathbf{H}}_{n}\\right)>\\epsilon / \\sqrt{\\ln (n)}\\right\\}\\right| \\geq\\left|\\left\\{j \\mid \\sigma_{j}\\left(\\overline{\\mathbf{K}}_{n}\\right) / \\sigma_{1}\\left(\\overline{\\mathbf{H}}_{n}\\right)>\\epsilon / \\sqrt{\\ln (n)}\\right\\}\\right|=\\Omega(n)\n$$\n\nThis finishes the proof. We remark that in the statement of Theorem 3, we study the $\\epsilon / \\sqrt{\\ln (n)}$-rank of the Hankel matrix instead of the $\\epsilon$-rank. The reason is that, unlike the singular values of a random matrix, the spectral measure of a normalized random Hankel matrix $\\mathbf{H}_{n} / \\sqrt{n}$ has unbounded support. In order to pull the $1 / \\sqrt{\\ln (n)}$ factor out and make it into the $\\Omega(n)$ bound, we need to study the distribution of the spectral measure of $\\mathbf{H}_{n} / \\sqrt{n}$. As pointed out by [7], however, this seems to be a hard problem. Nevertheless, we can empirically test the statement by numerical experiments. Next, we provide the proof of Theorem 4, which is almost immediate from H\u00f6lder's inequality. Proof of Theorem 4. By H\u00f6lder's inequality, we have\n\n$$\n\\|G-\\tilde{G}\\|_{\\infty}=\\|\\bar{G}-\\bar{G}\\|_{\\infty} \\leq \\max _{|z|=1} \\sum_{j=0}^{n-1}\\left|\\mathbf{h}_{j}-\\tilde{\\mathbf{h}}_{j}\\right||z|^{-j-1} \\leq\\|\\mathbf{h}-\\tilde{\\mathbf{h}}\\|_{2} \\sqrt{n}\n$$\n\n## F Some numerical experiments on Hankel matrices\n\n## F. 1 Numerical ranks of random LTI systems and random Hankel matrices in practice\n\nWhile the theoretical part of our paper focuses on the $\\epsilon$-rank of an LTI system, in the main text, we showed the distribution of all Hankel singular values of different LTI systems. The main reason for showing the histograms instead of a single number (i.e., the $\\epsilon$-rank) is that the histogram gives us more information while the $\\epsilon$-rank is merely a cutoff. In this section, we empirically compute the $\\epsilon$-rank to verify the two theorems (i.e., Theorem 1 and Theorem 3). In this experiment, we always set $\\epsilon=0.01$. For every $n$ in our experiment, we first randomly initialize a random $n \\times n$ Hankel matrix\n\n$$\n\\overline{\\mathbf{H}}_{n}=\\left[\\begin{array}{ccccc}\nh_{0} & h_{1} & h_{2} & \\cdots & h_{n-1} \\\\\nh_{1} & h_{2} & \\cdots & h_{n-1} & 0 \\\\\nh_{2} & \\cdots & h_{n-1} & 0 & 0 \\\\\n\\vdots & . \\cdot & . \\cdot & \\vdots & \\vdots \\\\\nh_{n-1} & 0 & \\cdots & 0 & 0\n\\end{array}\\right]\n$$\n\nwhere $h_{j}$ are i.i.d. random Gaussian variables with variance of 1 . We compute its $\\epsilon$-rank and we repeat the experiment for 1000 trials. Similarly, for every $n$, we randomly initialize an LTI system with\n\n$$\n\\mathbf{A}=\\operatorname{diag}\\left(a_{1}, \\ldots, a_{n}\\right), \\quad a_{j} \\sim \\operatorname{Uniform}(\\mathbb{D})\n$$\n\nwhere $\\mathbb{D}$ is the open unit disk in the complex plane and the elements of $\\mathbf{B} \\circ \\mathbf{C}^{\\top}$ are sampled i.i.d. from $\\mathcal{N}(0,1)$. We compute its $\\epsilon$-rank and also repeat the experiment for 1000 trials. From Figure 7, we see that a random LTI system has a low rank, whereas a random Hankel matrix has a high rank in the sense that it is about proportional to $n$. This observation aligns with our theory in Theorem 1 and Theorem 3,\n\n## F. 2 Hankel matrices are stable to perturbation in practice\n\nTheorem 4 predicts that the Hankel matrices are very stable when being perturbed. In this section, we run experiments in parallel with those in Figure 4, where we perturb a Hankel matrix $\\overline{\\mathbf{H}}$. As in Figure 4, we also set the size of the random perturbation to be $1 \\%$ and $0.1 \\%$, respectively, of the original system. Theorem 4 is corroborated by Figure 8, where we see that a small perturbation has a minimal effect on the Hankel singular values. ![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-23.jpg?height=607&width=1106&top_left_y=259&top_left_x=452)\n\nFigure 7: The $\\epsilon$-rank of a random LTI system of size $n$, where $\\epsilon=0.01$. The random systems are parameterized by a Hankel matrix or by the matrices $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$. The lines are the average rank and the shaded regions indicate the $10 \\%-90 \\%$ range over 1000 trials. ![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-23.jpg?height=615&width=1106&top_left_y=1075&top_left_x=452)\n\nFigure 8: A random perturbation is added to $\\overline{\\mathbf{H}}$. The magnitude of the perturbation is set to $0.1 \\%$ and $1 \\%$ of the original matrix. We show the relative Hankel singular values $\\sigma_{j} / \\sigma_{1}$ of the original and perturbed systems. In this case, the three curves are almost overlapping. ## G Adjusting the discretization step via NUFFT\n\nAssume we have a discrete LTI system whose transfer function is $\\bar{G}$. As explained in section 4, if we assume $\\Delta t=1$, then the output of the system given an input sequence can be computed as\n\n$$\n\\mathbf{y}=\\operatorname{iFFT}\\left(\\operatorname{FFT}(\\mathbf{u}) \\circ \\bar{G}\\left(\\boldsymbol{\\omega}^{(L)}\\right)\\right), \\quad \\quad \\bar{G}\\left(\\boldsymbol{\\omega}^{(L)}\\right)=\\left[\\begin{array}{lll}\n\\bar{G}\\left(\\omega_{0}^{(L)}\\right) & \\cdots & \\bar{G}\\left(\\omega_{L-1}^{(L)}\\right)\n\\end{array}\\right]^{\\top}\n$$\n\nTo understand why this relationship holds, assume there exists a continuous function $u$ on the unit circle $\\partial \\mathbb{D}$ where the discrete inputs $\\mathbf{u}$ are sampled from. Then, FFT allows us to write $u$ into the Fourier expansion:\n\n$$\nu(z)=\\sum_{j=0}^{L-1}[\\operatorname{FFT}(\\mathbf{u})]_{j} \\exp \\left(-2 \\pi i \\frac{j}{L} z\\right)\n$$\n\nBy the property of the transfer function eq. (3), we know that the output function $y$ is equal to\n\n$$\ny(z)=\\sum_{j=0}^{L-1} \\underbrace{\\left([\\operatorname{FFT}(\\mathbf{u})]_{j} \\bar{G}\\left(\\omega_{j}^{(L)}\\right)\\right)}_{\\hat{\\mathbf{y}}_{j}} \\exp \\left(-2 \\pi i \\frac{j}{L} z\\right)\n$$\n\nTo compute the discrete output $\\mathbf{y}$, one samples $y$ at $z=\\omega_{0}^{(L)}, \\ldots, \\omega_{L-1}^{(L)}$, which is equivalent to an inverse $\\operatorname{FFT}$ on $\\operatorname{FFT}(\\mathbf{u}) \\circ \\bar{G}\\left(\\boldsymbol{\\omega}^{(L)}\\right)$. Now, if we want to change $\\Delta t$, one way to think of it is as if our LTI system is unchanged, but the time domain of $u(z)$ is scaled by a factor of $\\Delta t$. That is, we now have ${ }^{3}$\n\n$$\nu^{(\\Delta t)}(z)=\\sum_{j=0}^{L-1}[\\operatorname{FFT}(\\mathbf{u})]_{j} \\exp \\left(-2 \\pi i \\frac{j}{L} z^{(\\Delta t)}\\right), \\quad z^{(\\Delta t)}=\\frac{1+s / \\Delta_{t}}{1-s / \\Delta_{t}}, \\quad s=\\frac{z-1}{z+1}\n$$\n\nThe output function $y^{(\\Delta t)}$ is now equal to\n\n$$\ny^{(\\Delta t)}(z)=\\sum_{j=0}^{L-1}\\left([\\operatorname{FFT}(\\mathbf{u})]_{j} \\bar{G}\\left(\\omega_{j}^{(L, \\Delta t)}\\right)\\right) \\exp \\left(-2 \\pi i \\frac{j}{L} z^{(\\Delta t)}\\right)\n$$\n\nThe only real difficulty is that when we sample $y^{(\\Delta t)}$ at $z=\\omega_{0}^{(L)}, \\ldots, \\omega_{L-1}^{(L)}$ to obtain $\\mathbf{y}^{(\\Delta t)}$, we note that $z^{(\\Delta t)}=\\omega_{0}^{(L, \\Delta t)}, \\ldots, \\omega_{L-1}^{(L, \\Delta t)}$ are not uniform on the unit circle.",
    "hope-28": "Hence, it cannot be achieved via inverse FFT. However, this sampling can be done via the so-called nonuniform FFT (NUFFT), which also takes $\\mathcal{O}(L \\log L)$. The stability of the NUFFT procedure is studied in $[6,33,4]$, and fast algorithms can be found in $[10,6,31]$. This is how we obtained\n\n$$\n\\mathbf{y}^{(\\Delta t)}=\\operatorname{NUFFT}\\left(\\operatorname{FFT}(\\mathbf{u}) \\circ \\bar{G}\\left(\\boldsymbol{\\omega}^{(L, \\Delta t)}\\right)\\right)\n$$\n\nNow, consider the following function\n\n$$\n\\tilde{y}^{(\\Delta t)}(z)=y^{(\\Delta t)}\\left(z^{(1 / \\Delta t)}\\right)=\\sum_{j=0}^{L-1}\\left([\\operatorname{FFT}(\\mathbf{u})]_{j} \\bar{G}\\left(\\omega_{j}^{(L, \\Delta t)}\\right)\\right) \\exp \\left(-2 \\pi i \\frac{j}{L} z\\right)\n$$\n\nThis output function is a scaled version of $y^{(\\Delta t)}$, where we scale the time domain by a factor of $1 / \\Delta t$. One can sample $\\tilde{y}^{(\\Delta t)}$ at $z=\\omega_{0}^{(L)}, \\ldots, \\omega_{L-1}^{(L)}$ using iFFT:\n\n$$\n\\tilde{\\mathbf{y}}^{(\\Delta t)}=\\operatorname{iFFT}\\left(\\operatorname{FFT}(\\mathbf{u}) \\circ \\bar{G}\\left(\\boldsymbol{\\omega}^{(L, \\Delta t)}\\right)\\right)\n$$\n\nThis leads us to eq.",
    "hope-29": "(7). ## H Experimental details\n\nIn this section, we provide the details of the three experiments presented in the main text. [^3]![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-25.jpg?height=747&width=1192&top_left_y=249&top_left_x=432)\n\nFigure 9: A visualization of appendix G. ## H. 1 Analyzing Hankel singular values using the sCIFAR-10 task\n\nAs mentioned in section 3, in the experiments presented in Figure 2 and Figure 5, we always train an SSM with 4 layers and 128 channels. Each channel in a layer is modeled by an LTI system with $n=64$ states. When we parameterize the LTI systems using $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$, and $\\mathbf{D}$, we assign a learning rate of 0.001 to $\\mathbf{A}$ and of 0.01 to the rest. When we freeze the system matrices, then we set the learning rate of $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$ to 0 while keeping that of $\\mathbf{D}$ to be 0.01 . Note that the matrix $\\mathbf{D}$ does not affect the Hankel singular values. All other model parameters are trained with a learning rate of 0.01 . For an LTI system parameterized by the Hankel matrix $\\overline{\\mathbf{H}}$, we adopt the same setting, except that $\\overline{\\mathbf{H}}$ is trained with a non-reduced learning rate of 0.01. To compute the Hankel singular values of an LTI system (A, B, C, D), we use its balanced realization (see Appendix B). To compute the Hankel singular values of a system parameterized by $\\overline{\\mathbf{H}}$, we apply an SVD to the matrix $\\overline{\\mathbf{H}}$. ## H. 2 Testing HOPE-SSMs long memory using noisy-sCIFAR\n\nIn this experiment (see Figure 6), we modify the sequential CIFAR-10 dataset by padding random sequences to the right. For each sequence of length 1024 from the original dataset, we pad another sequence of length 1024 to the end of it. The entries are sampled independently from the Gaussian distribution on the same magnitude as the entries in the original sequences. We adopt the same model architectures and learning rates as described in Appendix H. 1 but make two exceptions. First, we fix the discretization size to be $\\Delta t=0.1$, and therefore, it does not need a learning rate. In addition, a canonical SSM decodes the output sequence by first doing a pooling. Here, instead of pooling over all 2048 output vectors, to make the problem more challenging and require longer memory, we only pool over the last 1024 output vectors. These correspond to the output vectors when the noises are fed. We also test the models using different discretization sizes $\\Delta t$. We see that when $\\Delta t=1$, the S4D model fails to converge while our HOPE-SSM performs relatively well; on the other hand, when $\\Delta t=0.01$, both models tend to have a relatively good performance. These observations align with our theory because a larger $\\Delta t$ means that we put the discrete data on a continuous time domain with a longer span; hence, longer memory capacity is needed. ![](https://cdn.mathpix.com/cropped/2024_09_12_89795fa8c8dd89273165g-26.jpg?height=447&width=1564&top_left_y=219&top_left_x=218)\n\nFigure 10: Performance of the HOPE-SSM and the S4D model on the noise-padded sCIFAR10 task using different values of $\\Delta t$. ## H. 3 Hyperparameters of HOPE-SSMs in the Long-Range Arena\n\nIn this section, we present the table of hyperparameters used to train our HOPE-SSM on the LRA tasks [28] (Apache License, Version 2.0). (See Table 2.) Our codes are adapted from the code associated with the original S4 and S4D papers [14, 13] (Apache License, Version 2.0). Note that compared to the hyperparameters used to train S4 and S4D, we use the same model hyperparameters and only slightly tune the training hyperparameters. All experiments are done on a NVIDIA A30 Tensor Core GPU with 24 GB of memory. The time efficiency of our model is roughly the same as that of the S 4 D model. | Task | Depth | \\#Features | Norm | Prenorm | DO | LR | BS | Epochs | WD | $\\Delta$ Range |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 8 | 256 | BN | False | 0. | 0.002 | 20 | 500 | 0.03 | $(0.0001,1)$ |\n| Text | 6 | 256 | BN | True | 0. | 0.01 | 16 | 150 | 0.005 | $(0.001,0.1)$ |\n| Retrieval | 6 | 128 | BN | True | 0. | 0.008 | 32 | 50 | 0.03 | $(0.001,0.1)$ |\n| Image | 6 | 128 | LN | False | 0.1 | 0.004 | 32 | 1500 | 0.01 | $(0.001,10)$ |\n| Pathfinder | 6 | 128 | BN | True | 0. | 0.001 | 8 | 250 | 0.01 | $(0.0001,0.1)$ |\n| Path-X | 6 | 128 | BN | True | 0. | 0.001 | 16 | 80 | 0.03 | $(0.001,1)$ |\n\nTable 2: Configurations of the HOPE-SSM model, where DO, LR, BS, and WD stand for dropout rate, learning rate, batch size, and weight decay, respectively.",
    "hope-30": "[^0]:    ${ }^{*}$ Corresponding author: ay262@cornell.edu. [^1]:    ${ }^{1}$ Here, $n$ is to guarantee that $M>n$\n\n[^2]:    ${ }^{2}$ Otherwise, we can redefine $\\mathbf{B}=\\tilde{\\mathbf{B}}=\\left[\\begin{array}{llll}1 & 1 & \\cdots & 1\\end{array}\\right]^{\\top}, \\mathbf{C}=\\mathbf{B}^{\\top} \\circ \\mathbf{C}$, and $\\tilde{\\mathbf{C}}=\\tilde{\\mathbf{B}}^{\\top} \\circ \\tilde{\\mathbf{C}}$, and the redefined transfer functions are unchanged. [^3]:    ${ }^{3}$ Note that we could alternatively scale the angular domain instead of the time domain, i.e., $z^{(\\Delta t)}=z / \\Delta t$. The difference is on the level of discretization. However, we find that discretizing the time domain gives us a better performance in general. "
}