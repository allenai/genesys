{
    "glrsm-0": "# A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models \n\nItamar Zimerman*<br>Ameen Ali $^{*}$<br>Lior Wolf<br>The Blavatnik School of Computer Science, Tel Aviv University<br>\\{zimerman1,ameenali\\}@mail.tau.ac.il, wolf@cs.tau.ac.il\n\n\n#### Abstract\n\nRecent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models.",
    "glrsm-1": "In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available. () https://github.com/Itamarzimm/UnifiedImplicitAttnRepr\n\n## 1 Introduction\n\nThe very recent State Space Model (SSM) named Mamba by Gu and Dao [20] has attracted considerable attention since its recent debut [31, 32, 64, 30, 43, 60, 40, 63], further establishing it as an efficient and accurate general-purpose model. Like other SSM models [21, 24, 22], Mamba is autoregressive during inference and trains efficiently through convolutions. Recently, Ali et al. [2] have highlighted a third aspect of the Mamba model; namely, that it is also an attention model, since it implicitly computes attention. Attention models can be defined as models that linearly combine the values associated with different elements to create the next set of such associated values. When discussing sequences of tokens, an attention operator considers the values obtained for each token separately, as a hidden representation, and mixes these to obtain a new set of values for each token. The mixing coefficients are also a function of the hidden representations. Let $X$ be the matrix whose columns are the hidden values associated with each token, and let $\\alpha$ be the matrix of mixing coefficients. The set of values of the next layer is initially obtained as $Y=\\alpha X$ and it can then undergo other forms of processing, such as nonlinear activations and per-token processing. [^0]Given a neural architecture, one always linearizes mixing operators and writes them in the form $Y=\\alpha X$. However, to be considered an attention model it is required that $\\alpha$ be a function of $X$, which means that the linear operator is data-dependent. This property is shown by Ali et al. [2] to hold only for the recent selective SSM (S6) [20], but not for earlier SSMs. Specifically, for standard state-space layers, it has been demonstrated that they can be linearized into a constant operator, represented by a constant matrix alpha, which is solely controlled by the layer's parameters. However, in the S6 layers, alpha is influenced by both the input and the layer's parameters. The implicit attention matrix of Ali et al. [2] considers the S6 mechanism and ignores the influence of other critical mixer components, such as Conv1D, gate branch, linear layers, and SiLU activations. The formulation we propose in this work incorporates these additional elements and, as we show empirically, leads to improved interpretability results in both computer vision and NLP. Furthermore, using a similar holistic formulation, we show that S 6 is not the only sequence model that implicitly computes attention and that an implicit attention representation can also describe other recent layers, such as RWKV [41], Griffin [14],HGRN [47] and more, as illustrated in Fig. 1 ]\n\nOur main contributions are as follows: (i) We introduce the implicit self-attention representation, unifying transformers with non-transformer layers, such as Griffin [14], RWKV [41], ReNet [52], and others. (ii) We refine the approach of [2] to produce more accurate attention matrices. The previous work focused exclusively on the S6 layer, without considering the gating and Conv1D sub-layers in Mamba, while our representation incorporates all these factors. (iii) While \"Attention is not Explanation\" [27], transformer explainability relies heavily on attention matrices. We demonstrate that our implicit attention representation of non-transformer models can be used to develop new explainability and interpretability techniques for non-transformer models, enhancing the community's ability to understand, explore, and manage aspects of robustness, bias, fairness, and safety. As a sample downstream application, we demonstrate excellent out-of-the-box results for weakly supervised semantic segmentation. (iv) Finally, our framework facilitates comparisons between transformers and other recent architectures, by providing a unified attention view and setting the stage for further improvements and insights. ## 2 Related Work\n\nThis section describes the scientific context and provides the necessary terminology and symbols for discussing self-attention and selective SSM layers. Self-Attention Self-attention, a cornerstone of Transformer architectures [55], has profoundly influenced recent developments in NLP and computer vision. This mechanism leverages pairwise token interactions to dynamically allocate focus across different parts of the input sequence, assessing the relevance of each token in relation to others. The computational formula is given by:\n\n$$\n\\operatorname{Self}-\\operatorname{Attention}(Q, K, V)=\\alpha V, \\quad \\alpha=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)\n$$\n\nHere, $Q, K$, and $V$ denote the queries, keys, and values respectively, with $d_{l}$ representing the key dimension. Transformers enhance this mechanism by incorporating $H$ parallel attention heads, thus capturing a wider range of dependencies. Applications of Attention Matrices Attention matrices play a crucial role in transformers, as multiplying these matrices with value vectors is the core operation that captures interactions between tokens. Beyond this essential role in computing self-attention, they are also used for various purposes: (i) Explainability and Interpretability: Although attention itself is not inherently explainable [27], many methods in these domains rely on attention matrices to understand and analyze model behavior [1, 11, 10, 2] . (ii) Multi-modal Learning: Numerous multi-modal learning schemes are based on variations of cross-attention, enabling dependencies to be learned between any pair of tokens of different modalities [33, 53]. (iii) Weakly Supervised Tasks: Attention matrices can provide a valuable source of supervision, highlighting relevant regions or relationships within the data to guide model learning. These techniques are popular in semantic segmentation [48, 59, 49], and robustness enhancement [12]. Finally, (iv) Inductive Bias and Regularization Methods: Since attention matri-\nces represent interactions between tokens, they inherently carry semantic meaning. Therefore, they can be manipulated to incorporate domain knowledge or regulate the model effectively [29, 4, 9, 65]. S6 Layers and Mamba The recently presented selective SSM [20] (S6) outperforms the previous SSMs and various other architectures in NLP [3, 58], vision [32, 64], graph classification [56, 6], and more. S6 incorporates a dynamic input-dependent form of the discrete matrices $\\bar{A}, \\bar{B}$, and $C$, such that for every time-step the SSM employs a different recurrent rule. This technique differs from the previous state-space layers, which use the same set of matrices and recurrent rules for each time step. Denoting the input sequence by $\\hat{x}:=\\left(\\hat{x}_{1}, \\cdots, \\hat{x}_{L}\\right) \\in \\mathbb{R}^{L \\times D}$ where $\\hat{x}_{i} \\in \\mathbb{R}^{D}$, the discrete matrices for time step $i$, namely $\\bar{A}_{i}, \\bar{B}_{i}$, and $C_{i}$ are defined as:\n\n$$\nB_{i}=S_{B}\\left(\\hat{x}_{i}\\right), \\quad C_{i}=S_{C}\\left(\\hat{x}_{i}\\right), \\quad \\Delta_{i}=\\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{i}\\right)\\right), \\quad \\bar{A}_{i}=\\exp \\left(\\Delta_{i} A\\right), \\quad \\bar{B}_{i}=\\Delta_{i} B_{i}\n$$\n\nwhere $S_{B}, S_{C}, S_{\\Delta}$ are linear projection layers, and SoftPlus is the smooth elementwise ReLU approximation. While other SSMs employ complex values and non-diagonal matrices, S6 employs a real-diagonal parametrization. The usage of input-dependent time-variant layers adds to the expressivity of the layer, allowing it to adapt to the input, and potentially captures more complex dependencies. While other inputdependent time-variant mechanisms have been proposed in previous works through gated RNNs, the S5 layer [50], or adaptive filtering via input-dependent IIR filters [34], S6 also presents an efficient IO-aware implementation, which is parallelized on GPUs via work-efficient parallel scanners [8, 36]. The Mamba block combines the S6 layer, Conv1D and other elementwise operators. It borrows elements from Gated MLP, and given an input $x:=\\left(x_{1}, \\cdots x_{L}\\right)$, it is computed by:\n$\\left.\\hat{x}=\\operatorname{SiLU}(\\operatorname{Conv1D}(\\operatorname{Linear}(x))), \\quad \\hat{z}=\\operatorname{SiLU}(\\operatorname{Linear}(x)), \\quad \\hat{y}^{\\prime}=\\operatorname{Linear}(\\operatorname{Selective} \\operatorname{SSM}(\\hat{x}) \\otimes \\hat{z})\\right)$,\nwhere $\\otimes$ denotes elementwise multiplication. The entire mamba model contains $\\Lambda$ stacked mamba blocks with $D$ channels per block. Below, the tensors of the j -th channel in the i-th block are denoted by superscript indices of the form $i, j$. The vision mamba architectures [32, 64] (ViM) follow the vision transformer (ViT) [15] but replace the Transformer's self-attention mechanism by two Mamba layers, where each layer is applied in a bidirectional manner. These vision models outperform the standard ViT model in terms of accuracy and efficiency, for models of similar parameter counts. Gated-Linear RNNs RNNs, along with their advanced versions, such as GRU [13] and LSTM [26], play a fundamental role in deep sequence modeling. Their auto-regressive design decouples sequence length from computational complexity per step, making them highly efficient at decoding. However, they don't scale as effectively as transformers and often face challenges, such as slow training and vanishing gradients. Recently, linear RNNs have shown improved abilities in capturing long-range dependencies [21, 39] and enhanced scalability [42, 14]. Furthermore, gated linear RNNs deliver surprisingly strong language modeling performance [37, 57, 41, 47]. The most advanced gated linear RNNs include the following variants: (i) RWKV-6 [41], which draws inspiration from attentionfree transformers (AFT) [62], (ii) Mamba [20], which employs selective SSM, (iii) HGRN2 [46], which utilizes state expansion, and (iv) Hawk [14], which is built upon an enhanced variant of the LRU [39]. Other notable examples include GLA [61], GateLoop [28], and RenNet [52]. These layers achieve results comparable to transformers on larger scales, matching well-known models, such as Pythia [7] and LLaMA 2 [54]. Moreover, several studies show that hybrid models combining attention mechanisms with gated linear RNNs can be complementary [14, 31, 45, 35, 5, 19], enhancing both approaches. Despite these successes, interoperability and explainability techniques for these models remain relatively unexplored. ## 3 Method\n\nIn this section, we present a general and holistic data-control linear operator representation that can be applied to (at least) many of the recent non-transformer architectures and which incorporates all components of the architecture. Sec. 3.1 formulates the entire Mamba architecture as a datacontrol linear operator, incorporating subcomponents such as Conv1D, gate branches, and activations. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=551&width=1400&top_left_y=234&top_left_x=360)\n\nFigure 1: Unified and Interpretable Formulation of Attention-Free Architectures via Attention Matrices: (Left) Schematic overview of the architectures of Mamba, Griffin, and RWKV. (Right) A new view of those layers that rely on implicit attention. Our perspective enables the generation of attention maps, offering valuable applications in areas such as Explainable AI. Subsequently, Sections. 3.2 and 3.3 extend our approach to other architectures, such as Griffin [14] and RWKV [41]. Additionally, in Appendix A, we present how to extract holistic data-controlled attention matrices for RetNet [52] and HGRN [47]. ### 3.1 Formulation of Mamba via Attention matrices\n\nMamba can be formulated in a way that separates the components that mix channels from those that mix tokens, see Fig. 3 in [20]. $$\n\\operatorname{Mamba}(x)=\\operatorname{Linaer}_{3}\\left(\\operatorname{SILU}\\left(\\operatorname{Linear}_{2}\\left(\\operatorname{Linear}_{1}(x)\\right)\\right) \\otimes \\operatorname{S6}\\left(\\operatorname{SILU}\\left(\\operatorname{Conv1D}\\left(\\operatorname{Linear}_{1}(x)\\right)\\right)\\right)\\right)\n$$\n\nSince Linear $_{1}$ and Linear $_{3}$ do not mix tokens, they are irrelevant (similarly to the MLP layers) and we consider the simplified expression:\n\n$$\n\\operatorname{Mamba}(x)=\\left(\\operatorname{SILU}\\left(\\operatorname{Linear}_{2}(x)\\right)\\right) \\otimes(\\operatorname{S6}(\\operatorname{SILU}(\\operatorname{Conv1D}(x))))\n$$\n\nReplacing the element-wise gating multiplication with matrix multiplication leads to:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=93&width=992&top_left_y=1623&top_left_x=558)\n\nThe S6 layer can be formalized as a data-control linear operator (see Eq. 12 in [2]):\n\n$$\n\\mathbf{S 6}(x)=\\hat{\\alpha} x, \\quad \\hat{\\alpha}_{i, j}=C_{i}\\left(\\prod_{k=j+1}^{i} \\bar{A}_{k}\\right) \\bar{B}_{j}\n$$\n\nBy plugging Eq. 7 into Eq. 6 and since $\\operatorname{SILU}(x)=\\mathbf{S i g}(x) \\cdot x$ :\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=136&width=1193&top_left_y=1905&top_left_x=463)\n\nRecall that causal Conv1D layer with filter $f=\\left(f_{1}, \\cdots, f_{\\hat{L}}\\right)$ can be converted into a matrix form by arranging shifted copies of the filter into rows, forming a convolution matrix $M$.",
    "glrsm-2": "This matrix is then multiplied by the input sequence to produce an output, where each element represents the dot product of the filter and a corresponding segment of the input. By plugging the convolution matrix $M$ and the gate matrix $W_{x}^{\\prime}$ into Eq. 8 , we get:\n\n$$\n\\operatorname{Mamba}(x)=W_{x}^{\\prime} \\hat{\\alpha} Z_{x^{\\prime}} M x=H x, \\quad H=W_{x}^{\\prime} \\hat{\\alpha} Z_{x^{\\prime}} M\n$$\n\nTherefore, the entire Mamba layer can be viewed as a data-control linear operator, which implicitly parameterizes the per-channel implicit attention matrices through the parameters of the S6 layer, the Conv1D filter, the linear layer in the gate branch, and is controlled by the input $x$\n\n### 3.2 Formulation of Griffin via Attention Matrices\n\nThe component that captures interactions between tokens in Hawk and Griffin (regardless of selfattention) is the temporal mixing block, which is built on top of a Real-Gated Linear Recurrent Unit (RG-LRU), Conv1D, and gating. It can be formalized as follows:\n\n$$\ny=\\operatorname{Linear}_{3}\\left(\\left(\\operatorname{GeLU}\\left(\\operatorname{Linear}_{1}\\left(x^{\\prime}\\right)\\right)\\right) \\otimes\\left(\\mathbf{R G}-\\mathbf{L R U}\\left(\\operatorname{Conv1D}\\left(\\text { Linear }_{2}\\left(x^{\\prime}\\right)\\right)\\right)\\right)\\right. $$\n\nWe first rearrange the linear layers and replace elementwise gating with matrix multiplication:\n\n$$\nx=\\operatorname{Linear}_{2}\\left(x^{\\prime}\\right), \\quad y=\\operatorname{Linear}_{3}\\left(\\operatorname { d i a g } \\left(\\operatorname{GeLU}\\left(\\operatorname{Linear}_{1}^{\\prime}(x)\\right)(\\mathbf{R G}-\\mathbf{L R U}(\\operatorname{Conv1D}(x)))\\right.\\right. $$\n\nNote that Linear $_{1}^{\\prime}:=$ Linear $_{1}$ Linear $_{2}$ and Linear $_{3}$ do not mix tokens and can therefore be omitted. By substituting Conv1D with matrix multiplication using a causal convolution matrix $M$, we derive:\n\n$$\ny=\\operatorname{diag}\\left(\\mathbf{G e L U}\\left(\\text { Linear }_{1}^{\\prime}(x)\\right)(\\mathbf{R G}-\\mathbf{L R U}(M x))\\right. $$\n\nRG-LRU is defined by the following recurrent rule:\n$r_{t}=\\sigma\\left(W_{a} x_{t}+b_{a}\\right), \\quad i_{t}=\\sigma\\left(W_{x} x_{t}+b_{x}\\right), \\quad a_{t}=a^{c r_{t}}, \\quad h_{t}=a_{t} \\otimes h_{t-1}+\\sqrt{1-a_{t}^{2}} \\otimes\\left(i_{t} \\otimes x_{t}\\right)$\nThis linear recurrent rule can be converted to a matrix form as follows:\n$h=\\tilde{\\alpha} x,\\left[\\begin{array}{c}h_{1} \\\\ h_{2} \\\\ \\vdots \\\\ h_{L}\\end{array}\\right]=\\left[\\begin{array}{cccc}\\sqrt{1-a_{1}^{2}} \\otimes i_{1} & 0 & \\cdots & 0 \\\\ a_{2} \\sqrt{1-a_{1}^{2}} \\otimes i_{1} & \\sqrt{1-a_{2}^{2}} \\otimes i_{2} & \\cdots & 0 \\\\ \\vdots \\\\ \\vdots & \\ddots & 0 \\\\ \\prod_{k=2}^{L} a_{k} \\sqrt{1-a_{1}^{2}} \\otimes i_{1} & \\prod_{k=3}^{L} a_{k} \\sqrt{1-a_{2}^{2}} \\otimes i_{2} & \\cdots & \\sqrt{1-a_{L}^{2}} \\otimes i_{L}\\end{array}\\right]\\left[\\begin{array}{c}x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{L}\\end{array}\\right]$\nBy plugging Eq 14 into Eq 12 , we see that the entire temporal mixing block can be formalized as a data-control linear operator:\n\n$$\ny=\\operatorname{diag}\\left(\\mathbf{G e L U}\\left(\\text { Linear }_{1}^{\\prime}(x)\\right) \\tilde{\\alpha} M x=H x, \\quad H=\\operatorname{diag}\\left(\\mathbf{G e L U}^{\\left(\\text {Linear }_{1}^{\\prime}(x)\\right)} \\tilde{\\alpha} M\\right.\\right. $$\n\n### 3.3 Formulation of RWKV via Attention Matrices\n\nThe time-mixing block of RWKV includes three components: the WKV operator, a gate branch, and a token shift. For simplicity, we will ignore the token shift operation over the values. The simplified RWKV, which maps the input $x_{t}$ to the output $o_{t}$, can be formulated as follows:\n\n$$\n\\begin{gathered}\nr_{t}=W_{r} \\cdot\\left(u_{r} \\otimes x_{t}+\\left(1-u_{r}\\right) \\otimes x_{t-1}\\right), \\quad k_{t}=W_{k} \\cdot\\left(u_{k} \\otimes x_{t}+\\left(1-u_{k}\\right) \\otimes x_{t-1}, \\quad v_{t}=x_{t}\\right. \\\\\nw k v_{t}=\\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}} \\otimes v_{i}+e^{u+k_{t}} \\otimes v_{t}}{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}}+e^{u+k_{t}}}, \\quad o_{t}=W_{o} \\sigma\\left(r_{t}\\right) \\otimes w k v_{t}\n\\end{gathered}\n$$\n\nwhere $W_{r}, W_{k}, W_{o}$ are linear projections, and $u, w, u_{r}, u_{k}$ are learnable parameters. Now, we will reformulate the $w k v_{t}$ operator into a form of causal self-attention:\n\n$$\n\\hat{\\alpha}_{i, j}=\\left\\{\\begin{array}{ll}\n\\frac{e^{u+k_{i}}}{\\sum_{m=1}^{i-1} e^{-(t-1-m) w+k_{i}}+e^{u+k_{t}}} & \\text { if } j=i \\text { holds, } \\\\\n\\frac{e^{-(i-1-j) w+k_{t}}}{\\sum_{m=1}^{i-1} e^{-(t-1-m) w+k_{i}}+e^{u+k_{t}}} & \\text { if } j<i \\text { holds, } \\\\\n0 & \\text { otherwise. }\n\\end{array} \\quad \\hat{\\alpha} x=w k v\\right. $$\n\nNote that $W_{o}$ does not mix tokens and can therefore be omitted. By plugging Eq. 18 into Eq. 16, and replacing element-wise gating with matrix multiplication, we obtain:\n\n$$\no=\\boldsymbol{\\operatorname { d i a g }}(\\sigma(r)) \\hat{\\alpha} x\n$$\n\n### 3.4 Shared properties\n\nThe proposed formulation for Griffin, Mamba, and RWKV is based on the similarities in the structure of the architecture. Our formulation focuses on three main components: (i) the core of the linear attention mechanism (S6 for Mamba, RG-LRU for Griffin, or the WKV operator for RWKV), (ii) a short filter operation implemented via Conv1D in Griffin and Mamba and token shift in RWKV, and (iii) the gate branch, as illustrated in Fig. 1. Additionally, our formulation builds on the following key components: (i) rearranging linear layers and omitting operators that don't influence the mixer components, (ii) representing the gate branch as a data control linear operator using diagonal matrices instead of vectors, (iii) unrolling the linear recurrent layer to obtain a token-to-token map, and (iv) fusing several cascaded linear operators. Efficient computation Naive computation of hidden attention maps is computationally expensive. For each channel, it involves computing an attention matrix, which requires quadratic memory and results in $O\\left(H L^{3}\\right)$ time complexity per sample. However, in many applications, we are interested in an attention vector that describes the influence of each token on the CLS token. In such cases, the attention vector map per channel can be efficiently computed using the following three strategies: (i) Compute only a single vector, corresponding to a single row in the attention matrices. (ii) Instead of incorporating dependencies through matrix multiplication by M , we convolve the causal filter vector (with suitable padding) with the CLS-attention vector. Finally, (iii) we avoid materializing the entire gate branch, using only the elements associated with the CLS token. Consequently, we can compute the CLS attention vector with linear memory complexity. ## 4 Experiments\n\nWe conduct experiments using Mamba, RWKV, and Griffin. Our focus centers on examining the semantic diversity of these models and their utility within explainable AI frameworks. Visualization of Attention Matrices In Figure 2, we present a comparative visualization of the attention matrices from Mamba, RWKV, Griffin, and Transformer models. To enhance clarity, we applied the Softmax function to each row of the attention matrices from the transformers and conducted min-max normalization on the absolute values of the matrices from the non-transformer models. In every instance, we used a uniform prompt of size 32. For each architecture, we examined the attention matrices derived from the standard pre-trained models available in the Hugging Face library, including the Recurrent Gemma-2B, RWKV-430M trained on the Pile, and a Mamba-based LLM with 2.8B parameters also trained on the Pile. As illustrated, the implicit attention matrices of Mamba, Griffin, and RWKV exhibit similarities to those derived from traditional transformers. Echoing findings from [2], we note that dependencies between distant tokens become more apparent in the deeper layers of the model, as shown in the lower rows. Additionally, the attention matrices from RWKV are characterized by distinct horizontal tiles, whereas those from Mamba display a more continuous structure. Visualization of Attention Maps Additionally, we present attention maps of the [CLS] token for the ViM model across samples from the ImageNet validation set for various methods, including the previously proposed attention formulation by [2]. for Mamba. We explore three explanation methods: raw attention, attention rollout [1], and attribution following [2, 10], along with a comparison to the ViT counterparts, see Fig. 3 . Evidently, the explanation methods that are based on our attention formulation depict much more accurate and sharp maps compared to those of [2] and the ViT counterparts. In Fig. 4 we show similar visualizations in the NLP domain. This indicates that the segmentation maps obtained from our method can be effectively utilized in tasks such as weaklysupervised semantic segmentation, as demonstrated in Appendix. B.",
    "glrsm-3": "More qualitative results for the NLP domain can be found in Appendix. D. Segmentation Tests We evaluated our proposed Mamba's implicit attention mechanism by comparing its generated foreground segmentation maps against ground truth from the ImageNetSegmentation dataset [23]. We employed established metrics (pixel accuracy, mean Intersectionover-Union (mIoU), and mean Average Precision (mAP)) aligning with prior works [10, 11, 38, 25]. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-07.jpg?height=852&width=1149&top_left_y=234&top_left_x=484)\n\nFigure 2: Hidden Attention Matrices: Attention matrices of LLMs. Each row represents a different layer within the models, showcasing the evolution of the attention matrices at $25 \\%$ (top), $50 \\%$, and $75 \\%$ (bottom) of the layer depth. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-07.jpg?height=543&width=1367&top_left_y=1290&top_left_x=378)\n\nFigure 3: Qualitative results for the different explanation methods for the ViT-small and the Mambasmall models. (a) the original image, (b) the Raw-Attention of [2] over Mamba-Small, (c) AttentionRollout [2] over Mamba-Small, (d) the Mamba-Attribution [2] method over Mamba-Small, (e) the Raw-Attention with our proposed attention over Mamba-Small, (f) Attention-Rollout with our proposed attention over Mamba-Small, (g) the Mamba-Attribution with our attention over MambaSmall, (h) Raw-Attention of ViT-Small, (i) Attention Rollout [1] for ViT-Small, (j) TransformerAttribution [11] for ViT-Small\n\nNotably, we compared our method with both the ViT and the previously proposed Mamba's implicit attention from [2]. Results presented in 1 demonstrate that our proposed Mamba's implicit attention outperforms both the Vision Transformer and the previous proposed Mamba's attention of [2] on all metrics over the three different XAI methods. This superior performance suggests the potential of these maps for downstreaming tasks such as weakly supervised semantic segmentation, and mitigating background bias in classifiers [12]. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-08.jpg?height=219&width=430&top_left_y=267&top_left_x=395)\n(a)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-08.jpg?height=243&width=441&top_left_y=258&top_left_x=842)\n(b)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-08.jpg?height=221&width=433&top_left_y=276&top_left_x=1299)\n(c)\n\nFigure 4: Qualitative results for NLP, samples are taken from IMDB movie sentiment classification. In (a), we show the results for the previously proposed Mamba's attention [2], (b) our proposed Mamba's attention, and in (c) we show our proposed method over RWKV. In the upper row, we show a negative sentiment, and in the lower row, we show a positive sentiment. Table 1: Segmentation performance on the ImageNet-Segmentation dataset (percent). Higher is better. The upper part depicts results for ViM-small, and the lower part for ViT-small. | Model | Method | pixel accuracy | mAP | mIoU |\n| :--- | :--- | :---: | :---: | :---: |\n| DeiT S | Raw-Attention | 59.69 | 77.25 | 36.94 |\n| VisionMamba-S | Raw-Attention [2] | 67.64 | 74.88 | 45.09 |\n| Vision Mamba-S | Raw-Attention Ours | $\\mathbf{6 7 . 6 6}$ | $\\mathbf{8 0 . 0 0}$ | $\\mathbf{4 7 . 2 8}$ |\n| DeiT S | Attn-Rollout [1] | 66.84 | 80.34 | 47.85 |\n| Vision Mamba-S | Attn-Rollout [2] | 71.01 | 80.78 | 51.51 |\n| Vision Mamba-S | Attn-Rollout Ours | $\\mathbf{7 6 . 4 0}$ | $\\mathbf{8 3 . 9 0}$ | $\\mathbf{5 8 . 4 8}$ |\n| DeiT S | Transformer-Attr [11] | 79.26 | 84.85 | 60.63 |\n| Vision Mamba-S | Mamba-Attr [2] | 74.72 | 81.70 | 54.24 |\n| Vision Mamba-S | Mamba-Attr Ours | $\\mathbf{7 9 .",
    "glrsm-4": "6 0}$ | $\\mathbf{8 6 . 4 0}$ | $\\mathbf{6 2 . 5 1}$ |\n\nPerturbation Tests To assess the faithfulness of explanations, we adopted an input perturbation scheme similar to [11, 10]. This method involves systematically masking image pixels based on their predicted relevance from the explanation method. We conducted two experiments: positive and negative perturbation. (1) Positive Perturbation: in this setup, a good explanation prioritizes relevant pixels. We expect the model's accuracy (specifically, top-1 accuracy) to gradually decrease as we mask pixels in descending order of relevance (most relevant first). (2) Negative Perturbation: Conversely, a robust explanation should maintain model accuracy even when irrelevant pixels are masked. Here, we mask pixels in ascending order of relevance (least relevant first). In both scenarios, we evaluate the explanation quality using the Area-Under-Curve (AUC) metric. AUC considers the model's accuracy as a function of the percentage of masked pixels (ranging from $10 \\%$ to $90 \\%$ ). The perturbations results are summarized in Table 2 for various explanation methods under both positive and negative perturbation scenarios on the ImageNet validation set. In the positive perturbation scenario, where lower AUC values indicate better performance, our proposed Mamba's attention method consistently outperforms the other methods. Specifically, our method achieves the lowest AUC values across all explanation methods, with an AUC of 13.264 for Raw-Attention, 12.830 for Attn-Rollout, and a notably low 11.350 for Attribution. In the negative perturbation scenario, where higher AUC values are better, our method shows the best performance, with AUC values of 47.705 for Raw-Attention, 50.035 for Attn-Rollout, and 51.310 for Attribution, outperforming both the method of [2] and the counterpart XAI methods for Vision Transformer. Perturbation experiments for the NLP domain can be found in Appendix C. In these settings, we fine-tune various LLMs with an additional linear layer classifier and append the [CLS] token to all samples to generate explanation maps, similar to methods used in vision models. These results also demonstrate that our attention formulation surpasses the previous formulation proposed by [2] over the mamba model and is effective also for the RWKV model. Ablation study The architectures we explored implicitly parametrize attention matrices through a composition of several different sub-layers, see Eq 9 , and 15 . Examples of these sub-layers include linear recurrent layers, gate mechanisms, and other components, such as token-shift or depth-wise convolutions. To better understand the contribution of each of these components, we\n\nTable 2: Positive and Negative perturbation AUC results (percentages) for the predicted class on the ImageNet validation set. For positive perturbation lower is better, and for negative perturbation higher is better. Previous results by [2] denoted by $\\ddagger$. | ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-09.jpg?height=98&width=197&top_left_y=381&top_left_x=371) | Positive Perturbation |  |  |  | Negative Perturbation |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Mamba $\\ddagger$ | Mamba Ours | Transformer |  | Mamba $\\ddagger$ | Mamba Ours | Transformer |\n| Raw-Attention | 17.268 | 13.264 | 20.687 |  | 34.025 | 47.705 | 40.766 |\n| Attn-Rollout | 18.806 | 12.830 | 20.594 |  | 41.864 | 50.035 | 43.525 |\n| Attribution | 16.619 | 11.350 | 15.351 |  | 39.632 | 51.310 | 48.089 |\n|  | Mamba | Mamba w.o Conv |  | Mamba w.o Gate |  | S6 |  |\n|  | ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-09.jpg?height=90&width=103&top_left_y=800&top_left_x=618) | ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-09.jpg?height=174&width=92&top_left_y=719&top_left_x=824) |  |  | $\\square$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-09.jpg?height=191&width=184&top_left_y=717&top_left_x=1351) |  |\n\nFigure 5: Comparative visualization of ablated hidden matrices\nconduct a sequence of ablation studies. Initially, in Fig 5. we visualize the implicit attention of Mamba, ablating the Conv1D or the gate branch, or focusing solely on the S6 layer. As expected, it seems that the Conv1D layer causes a smoothing effect, and the final Mamba implicit attention is significantly sharper than that of the S6 matrices. In Tab3, we compare several ablation variants of our method. As can be seen, our method, which utilizes all the components of Mamba, achieves a much better score than the ablated versions, illustrating the importance of all components. This experiment reveals that including the Conv1D layer is crucial for high performance. However, the activation has a relatively low impact on the implicit representation. A similar ablation study was conducted for RWKV and presented in Appendix C. It is evident that including the gate branch, as presented in our method, consistently improves performance. Table 3: Ablation studies for our method on the ImageNet-Segmentation dataset. Higher is better. | Model | Method | pixel accuracy | mAP | mIoU |\n| :--- | :--- | :---: | :---: | :---: |\n| vMamba S | Mamba-Attr V2 | $\\mathbf{7 9 . 6 0}$ | $\\mathbf{8 6 . 4 0}$ | $\\mathbf{6 2 . 5 1}$ |\n| vMamba S | Mamba-Attr V2 w/o act | 79.32 | 86.22 | 62.41 |\n| vMamba S | Mamba-Attr V2 w/o conv | 70.01 | 78.87 | 50.64 |\n| vMamba S | Mamba-Attr V2 w/o gate | 75.11 | 80.12 | 55.78 |\n| vMamba S | Mamba-Attr V2 S6-Attn | 72.39 | 80.09 | 53.19 |\n\n## 5 Conclusions\n\nIn this study, we have extended the use of self-attention from its traditional role as the core mechanism of transformers to a representation of neural sequence layers. Our unified framework facilitates the exploration of similarities and differences among non-attention layers, such as Mamba, RWKV, and Griffin, and their interconnections with transformer architectures. Additionally, it enables the development of innovative explainability techniques for the latest attention-free architectures. Our contributions provide the research community with new tools for analyzing the performance, fairness, and robustness of gated-linear RNN variants, while also identifying their potential vulnerabilities. These advancements set the stage for future improvements and support the implementation of weakly supervised downstream tasks. Looking ahead, we aim to incorporate additional layers, such as Hyena [44], and HGRN2 [46] into our framework, including their vision-specific variants [16, 18, 66, 51]. Furthermore, we plan to examine how differences in these architectures are reflected in their self-attention matrices and explore whether such insights can reveal more about the inductive biases inherent in each architecture. ## 6 Acknowledgments\n\nThis work was supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD). This research was also supported by the Ministry of Innovation, Science \\& Technology ,Israel (1001576154) and the Michael J. Fox Foundation (MJFF-022407). The contribution of the first author is part of a PhD thesis research conducted at Tel Aviv University. ## References\n\n[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190-4197, 2020 . [2] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. [3] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models.",
    "glrsm-5": "arXiv preprint arXiv:2402.01771, 2024. [4] Giuseppe Attanasio, Debora Nozza, Dirk Hovy, and Elena Baralis. Entropy-based attention regularization frees unintended bias mitigation from lists.",
    "glrsm-6": "arXiv preprint arXiv:2203.09192, 2022 . [5] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: A general spatial layer for visual transformers. arXiv preprint arXiv:2306.06635, 2023. [6] Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space models.",
    "glrsm-7": "arXiv preprint arXiv:2402.08678, 2024. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling.",
    "glrsm-8": "In International Conference on Machine Learning, pages 2397-2430. PMLR, 2023. [8] Guy E Blelloch. Prefix sums and their applications.",
    "glrsm-9": "Technical Report, 1990. [9] Helena Bonaldi, Giuseppe Attanasio, Debora Nozza, and Marco Guerini. Weigh your own words: Improving hate speech counter narrative generation via attention regularization. arXiv preprint arXiv:2309.02311, 2023. [10] Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 397-406, 2021. [11] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782-791, 2021. [12] Hila Chefer, Idan Schwartz, and Lior Wolf. Optimizing relevance maps of vision transformers improves robustness.",
    "glrsm-10": "Advances in Neural Information Processing Systems, 35:33618-33632, 2022. [13] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [14] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models.",
    "glrsm-11": "arXiv preprint arXiv:2402.19427, 2024. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [16] Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, and Wenhai Wang. Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures. arXiv preprint arXiv:2403.02308, 2024. [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88: $303-338,2010$. [18] Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, and Ran He. Rmt: Retentive networks meet vision transformers. arXiv preprint arXiv:2309.11523, 2023. [19] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. [20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [22] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021. [23] Matthieu Guillaumin, Daniel K\u00fcttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. International Journal of Computer Vision, 110:328 - 348, 2014. URL https://api.semanticscholar.org/CorpusID:1005559. [24] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022. [25] Shir Gur, Ameen Ali, and Lior Wolf. Visualization of supervised and self-supervised neural networks via attribution guided factorization. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11545-11554, 2021. [26] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. [27] Sarthak Jain and Byron C Wallace. Attention is not explanation. In Proceedings of NAACL-HLT, pages 3543-3556, 2019. [28] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling.",
    "glrsm-12": "arXiv preprint arXiv:2311.01927, 2023. [29] Jian Li, Zhaopeng Tu, Baosong Yang, Michael R Lyu, and Tong Zhang. Multi-head attention with disagreement regularization.",
    "glrsm-13": "arXiv preprint arXiv:1810.10183, 2018. [30] Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-nd: Selective state space modeling for multi-dimensional data. arXiv preprint arXiv:2402.05892, 2024. [31] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [32] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model.",
    "glrsm-14": "arXiv preprint arXiv:2401.10166, 2024. [33] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.",
    "glrsm-15": "Advances in neural information processing systems, 32, 2019. [34] Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive iir filters). arXiv preprint arXiv:2305.14952, 2023. [35] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention.",
    "glrsm-16": "arXiv preprint arXiv:2209.10655, 2022. [36] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. [37] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [38] Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, and Seong-Whan Lee. Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $2501-2508,2020$. [39] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [40] Badri Narayana Patro and Vijay Srinivas Agneeswaran. Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges. arXiv preprint arXiv:2404.16112, 2024. [41] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [42] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.",
    "glrsm-17": "arXiv preprint arXiv:2404.05892, 2024. [43] Maciej Pi\u00f3ro, Kamil Ciebiera, Krystian Kr\u00f3l, Jan Ludziejewski, and Sebastian Jaszczur. Moe-mamba: Efficient selective state space models with mixture of experts.",
    "glrsm-18": "arXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.",
    "glrsm-19": "arXiv preprint arXiv:2402.19305, 2024. [52] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [53] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019. [54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017. [56] Chloe Wang, Oleksii Tsepa, Jun Ma, and Bo Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024. [57] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [58] Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush. Mambabyte: Token-free selective state space model.",
    "glrsm-20": "arXiv preprint arXiv:2401.13660, 2024. [59] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen. Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12275-12284, 2020. [60] Rui Xu, Shu Yang, Yihui Wang, Bo Du, and Hao Chen. A survey on vision mamba: Models, applications and challenges.",
    "glrsm-21": "arXiv preprint arXiv:2404.18861, 2024. [61] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [62] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer.",
    "glrsm-22": "arXiv preprint arXiv:2105.14103, 2021. [63] Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, and Zi Ye. A survey on visual mamba. arXiv preprint arXiv:2404.15956, 2024. [64] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.",
    "glrsm-23": "arXiv preprint arXiv:2401.09417, 2024. [65] Itamar Zimerman and Lior Wolf. On the long range abilities of transformers. arXiv preprint arXiv:2311.16620, 2023. [66] Itamar Zimerman and Lior Wolf. Multi-dimensional hyena for spatial inductive bias. In International Conference on Artificial Intelligence and Statistics, pages 973-981. PMLR, 2024. ## A Representing additional architectures via implicit attention\n\nIn sec. 3 we present the formulation of Griffin, RWKV, and Mamba via attention matrices. In this section, we extend our method to other layers, such as RetNet [52] and HGRN [47]. RetNet The Retention Network is composed of two primary blocks: (i) the Multi-Scale Retention (MSR) layer and the (ii) FFN layer, which operates independently across tokens. The MSR layer, responsible for token mixing, is built on top of the retention sub-layer and is defined as follows:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-14.jpg?height=59&width=1397&top_left_y=556&top_left_x=364)\n\nFurthermore, the outputs are scaled using a data-control gate branch, parameterized by a matrix $W_{G} \\in \\mathbb{R}^{D \\times D}:$\n\n$$\n\\operatorname{MSR}(\\mathbf{X})=\\left(\\operatorname{swish}\\left(X W_{G}\\right) \\otimes Y\\right)\n$$\n\nTo refine this formulation, we can represent the element-wise multiplication as a matrix multiplication using a diagonal matrix $G=\\operatorname{diag}\\left(\\mathbf{s w i s h}\\left(X W_{G}\\right)\\right)$. Additionally, per-head statistics can be integrated into $G$. Given that the parallel representation of Retention can be depicted via an attention matrix $R$ (see Eq. 5 in [52]), the entire MSR block simplifies to:\n\n$$\n\\operatorname{Retention}(x)=G R x\n$$\n\nHGRN The Hierarchically Gated RNN (HGRN) is first defined with the following recurrent rule:\n\n$$\n\\begin{array}{cc}\n\\mathbf{f}_{t}=\\operatorname{Sigmoid}\\left(\\mathbf{x}_{t} \\mathbf{W}_{f}+\\mathbf{b}_{f}\\right) \\in \\mathbb{R}^{1 \\times d}, & \\mathbf{i}_{t}=\\operatorname{Sigmoid}\\left(\\mathbf{x}_{t} \\mathbf{W}_{i}+\\mathbf{b}_{i}\\right) \\in \\mathbb{R}^{1 \\times d} \\\\\n\\mathbf{c}_{t}=\\operatorname{SiLU}\\left(\\mathbf{x}_{t} \\mathbf{W}_{t}+\\mathbf{b}_{z}\\right) \\in \\mathbb{R}^{1 \\times d}, & \\mathbf{h}_{t}=\\mathbf{f}_{t} \\otimes \\mathbf{h}_{t-1}+\\mathbf{i}_{t} \\otimes \\mathbf{c}_{t} \\in \\mathbb{R}^{1 \\times d}\n\\end{array}\n$$\n\nwhere the output of the recurrent $h_{t}$ is multiplied by $g_{t}=\\operatorname{SiLU}(\\operatorname{Linear}(x))$ to produce the output:\n\n$$\no_{t}=g_{t} \\otimes h_{t}\n$$\n\nNote that the recurrent rule of the HGRN layer can be computed via an implicit attention represented by a matrix $\\alpha_{r}$ (see Eq. 5 in [47]), as follows:\n\n$$\nH:=\\left(h_{1}, \\cdots, h_{L}\\right), \\quad C=\\left(c_{1}, \\cdots, \\quad c_{L}\\right), \\quad H=\\alpha_{r} c\n$$\n\nHence, by define $G=\\operatorname{diag}(\\operatorname{SiLU}(\\operatorname{Linar}(x))), G_{a c t}=\\operatorname{diag}(\\operatorname{sigmoid}(x))$. Furthermore, we can rearrange the linear layer such that $W_{t}$ and $b_{z}$ will be omitted, and obtain:\n\n$$\nG_{\\mathbf{A C T}}=\\operatorname{diag}(\\operatorname{Sigmoid}(\\mathbf{x})), \\quad o=G \\alpha_{r} G_{\\mathbf{A C T}} x\n$$\n\nwhich is a linear operator characterized by an input-dependent matrix, defined as $G=$ $\\operatorname{diag}(\\boldsymbol{\\operatorname { S i g m o i d }}(x))$. The output $o$ is given by $o=G \\alpha_{r}$. $$\n\\begin{array}{cc}\n\\mathbf{f}_{t}=\\operatorname{Sigmoid}\\left(\\mathbf{x}_{t} \\mathbf{W}_{f}+\\mathbf{b}_{f}\\right) \\in \\mathbb{R}^{1 \\times d}, & \\mathbf{i}_{t}=\\operatorname{Sigmoid}\\left(\\mathbf{x}_{t} \\mathbf{W}_{i}+\\mathbf{b}_{i}\\right) \\in \\mathbb{R}^{1 \\times d} \\\\\n\\mathbf{c}_{t}=\\operatorname{SiLU}\\left(\\mathbf{x}_{t} \\mathbf{W}_{t}+\\mathbf{b}_{z}\\right) \\in \\mathbb{R}^{1 \\times d}, & \\mathbf{h}_{t}=\\mathbf{f}_{t} \\otimes \\mathbf{h}_{t-1}+\\mathbf{i}_{t} \\otimes \\mathbf{c}_{t} \\in \\mathbb{R}^{1 \\times d}\n\\end{array}\n$$\n\nwhere the output of the recurrent $h_{t}$ is multiplied by $g_{t}=\\operatorname{SiLU}(\\operatorname{Linar}(x))$ to produce the output:\n\n$$\no_{t}=g_{t} \\otimes h_{t}\n$$\n\nNote that the recurrent rule of the HRGU layer can be computed via an implicit attention represented by a matrix $\\alpha_{r}$ (see Eq. 5 in [47]), as follows:\n\n$$\nH:=\\left(h_{1}, \\cdots, h_{L}\\right), \\quad C=\\left(c_{1}, \\cdots, \\quad c_{L}\\right), \\quad H=\\alpha_{r} c\n$$\n\nHence, by define $G=\\operatorname{diag}(\\operatorname{SiLU}(\\operatorname{Linar}(x))), G_{a c t}=\\operatorname{diag}(\\operatorname{sigmoid}(x))$. Furthermore, we can rearrange the linear layer such the $W_{t}, b_{z}$ will be omitted, and obtain:\n\n$$\nG_{\\mathbf{A C T}}=\\operatorname{diag}(\\operatorname{Sigmoid}(\\mathbf{x})), \\quad o=G \\alpha_{r} G_{\\mathbf{A C T}} x\n$$\n\nas requested. ## B Weakly Supervised Semantic Segmentation\n\nIn weakly supervised semantic segmentation (WSSS), a common approach involves first training a classifier on image-level labels and then extracting Class Activation Maps (CAMs) for individual images, which highlight regions that the classifier deems relevant to specific classes. The state-of-theart methods then employ these CAMs as pseudo-masks to train a segmentation decoder. In this context, we adopt our proposed Mamba-Attr XAI method for vision-mamba models. We assess its competitiveness against the well-established CAMs in generating pseudo-labels for transformers. To ensure a fair comparison, we fine-tune both DeiT-Small and Vision Mamba-Small models under identical conditions over the Pascal-voc 2012 [17] dataset, excluding multi-scale training, inference, or any other modifications. This controlled setting isolates the influence of our Mamba-Attr method on the quality of the generated pseudo-labels. The results are presented in Table 4 Evidently, Mamba-Attr XAI method achieves competitive results surpassing the baseline approach of Class Activation Maps (CAMs) without any additional modifications. This is evident in the mean Intersection-over-Union (mIoU) score, where Mamba-Attr $(52.11 \\%)$ outperforms the CAM of DeiT-Small $(35.99 \\%)$ by a sizable gap. While Mamba-Attr does not reach the state-of-the-art performance of Toco [49] (61.10\\%), it achieves, out of the box, a substantial improvement over CAM and comes surprisingly close to this much more elaborate multi-phase learning method which utilizes multiple loss terms specifically designed to enhance the quality of the initial CAM map. These results suggest that Mamba-Attr XAI offers a powerful and efficient solution for WSSS tasks with vision-mamba models. Table 4: Evaluation and comparison of the pseudo-labels for the different classes in Pascal-voc 2012 [17]. Results are in mIoU\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-15.jpg?height=104&width=1387&top_left_y=1477&top_left_x=369)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-15.jpg?height=39&width=1375&top_left_y=1566&top_left_x=375)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-15.jpg?height=28&width=1370&top_left_y=1601&top_left_x=383)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-15.jpg?height=39&width=1359&top_left_y=1626&top_left_x=380)\n\n## C Perturbation Experiments for NLP\n\nIn Figure 6, we present perturbation results for both positive and negative settings. We utilize Mamba ${ }^{2}$ RWKV ${ }^{3}$ and BERT $4^{4}$ models as the models of interest and fine-tune them on the IMDB sentiment classification dataset, employing the [CLS] token for all samples. Subsequently, we evaluate the explanation quality using a similar procedure as proposed in [2] for the perturbation experiment. The results reveal that Mamba-attr, based on our new attention formulation, achieves superior AUC for both negative and positive perturbations compared to the previous attention formulation by [2]. Additionally, our unified attention formulation is effective for RWKV models, yielding comparable results to those of Mamba and BERT. Moreover, as an ablation, the first column of Fig. 6demonstrates that including the gate branch, as presented in our full method, consistently improves performance. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-16.jpg?height=1347&width=1354&top_left_y=263&top_left_x=383)\n\nFigure 6: Evaluation of explanations using input perturbations. Results for IMDB activation task (top row) in which the most relevant words are added first, and for IMDB pruning task (lower row) in which the words of least relevance are removed first. Results are shown for 3 different models: RWKV, Mamba, and BERT, respectively. ## D Additional qualitative results for NLP\n\nAdditional NLP results obtained on IMDB dataset are presented in Figure 5. In panel (a), we show the results for the previously proposed Mamba's attention [2]. Panel (b) shows our proposed Mamba's attention. Lastly, panel(c) presents our proposed method over RWKV. In red, we show a negative sentiment, and in blue, we show a positive sentiment. As can be seen from these qualitative results, the explanation maps generated by our new attention formulation exhibit sparser and more accurate heatmaps of relevant words than those of Ali et al. [2], aligning with the desired properties of XAI methods. Similarly, the results for RWKV models show comparable success to those of Mamba. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-17.jpg?height=2357&width=977&top_left_y=155&top_left_x=563)\n\n\n[^0]:    *These authors contributed equally to this work. [^1]:    ${ }^{2}$ https://huggingface.co/trinhxuankhai/mamba_text_classification\n    3 https://huggingface.co/BlinkDL/rwkv-2-pile-430m\n    ${ }^{4}$ https://github.com/hila-chefer/Transformer-Explainability\n\n"
}