{"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "abstract": "Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 98, "influentialCitationCount": 10, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.10749", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is found that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics."}, "embedding": {"model": "specter_v2", "vector": [0.49951881170272827, 0.7278734445571899, -0.30691656470298767, 0.22358141839504242, -0.47616443037986755, -0.22070127725601196, 0.3885389566421509, -0.3356323838233948, -0.2396719604730606, -0.5018722414970398, 0.00024637117167003453, -0.40978217124938965, 0.11918999999761581, -0.3329361081123352, -0.4113098680973053, 0.14685918390750885, -0.9457626938819885, 0.04787443205714226, 0.17188583314418793, -0.2802158296108246, 0.22680509090423584, 0.05299445614218712, -1.7183804512023926, 0.10706394165754318, 0.2972506284713745, 0.6325185894966125, -0.6754574179649353, 1.230453372001648, -0.1934959441423416, 1.0448802709579468, 0.679683268070221, -0.4193570911884308, 0.13301752507686615, 0.009907872416079044, -0.8321832418441772, -0.274208128452301, 0.09003442525863647, -0.290735125541687, -0.7847776412963867, 0.33990341424942017, -0.6658511161804199, -0.08975274115800858, 0.19940689206123352, -0.998116672039032, 0.5651527047157288, 0.6938698291778564, 0.5701794028282166, 0.7083869576454163, -0.2781279385089874, -0.5127062201499939, 1.5096533298492432, -0.8760661482810974, 0.38780269026756287, 0.7668623924255371, 0.7947894334793091, 0.7803649306297302, -0.6408618092536926, -0.5909925699234009, 0.15485087037086487, 0.036415714770555496, -0.9190782308578491, -0.6124147772789001, -0.2542251646518707, 0.10221635550260544, 1.6297715902328491, -0.31948554515838623, -0.0008939357358030975, -0.15358875691890717, 0.28921717405319214, 1.2730282545089722, 0.41805294156074524, -0.7010626792907715, -0.1920573115348816, 0.10571340471506119, 0.9150080680847168, 1.0038760900497437, 0.1980174034833908, 0.42789149284362793, -0.8838502764701843, -0.37782061100006104, 0.5977444052696228, 0.5737996697425842, 0.004356544930487871, -0.7715286016464233, 0.10837164521217346, 0.45519718527793884, 0.27805963158607483, 0.7361485362052917, -0.01967470534145832, 1.106003999710083, 0.4878532886505127, 0.6189106106758118, -0.6327646970748901, 0.1942460685968399, -0.06395334005355835, 0.269234836101532, -0.3987816274166107, 0.18643036484718323, 0.3216747045516968, 0.7556542754173279, 0.3776979148387909, -0.03454161062836647, -0.3356669247150421, -0.36863771080970764, 0.946107804775238, -0.29602402448654175, 0.10612586885690689, -0.9256268739700317, -0.16530314087867737, -0.09534205496311188, 0.6043921113014221, -0.3002876341342926, -0.08180980384349823, -0.12474988400936127, -0.8051113486289978, -1.1863638162612915, -0.09686627984046936, 0.1937507688999176, -0.6591988801956177, 0.6510012149810791, -0.3178013861179352, 0.6478573679924011, 0.15294994413852692, 0.5261939764022827, 0.2474353015422821, 0.6457472443580627, -0.07550020515918732, -0.2886163890361786, 0.8227908611297607, -0.3039688467979431, -0.49272289872169495, -0.8993034362792969, 1.2998583316802979, 0.3294735848903656, 0.18569575250148773, -0.34228751063346863, -1.4157376289367676, -0.8105835318565369, -1.0942600965499878, 0.28995415568351746, -0.5092071890830994, -0.17658506333827972, 1.3472295999526978, 0.8150975108146667, -1.054654836654663, 0.6929247975349426, -0.6296725869178772, 0.23450839519500732, 0.20949290692806244, 0.546570360660553, 0.23404327034950256, -0.5549605488777161, -0.8984346985816956, 0.3971875309944153, 0.13811266422271729, -0.6003937721252441, -0.6377382278442383, -0.5133426189422607, -1.0737791061401367, 0.0836692601442337, 0.6884978413581848, -0.7086890339851379, 1.6220253705978394, 0.4024851322174072, -1.0484137535095215, 0.9819803833961487, -0.3324359655380249, -0.3027825653553009, 0.09399605542421341, 0.3849896490573883, -0.3715539574623108, -0.10711517930030823, -0.4786304831504822, -0.319480836391449, 0.317466676235199, -0.5631366968154907, -0.04841087758541107, 0.049356549978256226, -0.37103497982025146, -0.9409407377243042, 0.23766937851905823, 0.6519822478294373, 0.2784308195114136, 0.36085620522499084, 0.29610294103622437, 0.29951241612434387, -0.36537230014801025, -0.3426344096660614, -0.002784351585432887, -0.9572932124137878, 0.3793187737464905, 0.7075009346008301, 1.282638669013977, -0.6897982954978943, -0.7041634917259216, 0.19652025401592255, 0.10788626223802567, -0.19679135084152222, -0.5002967119216919, 0.5139087438583374, -0.46965500712394714, 0.8761889934539795, -0.3374674320220947, -0.6762084364891052, -0.06797485798597336, -0.04981057345867157, -0.9491594433784485, -0.21395868062973022, 0.2152671366930008, 0.9890668392181396, -0.599679172039032, 0.17269818484783173, -0.13185690343379974, -0.3419288098812103, -0.9974285960197449, 1.4708060026168823, -0.15718889236450195, -0.22428135573863983, 0.024507906287908554, -0.6478592753410339, -0.07037777453660965, -0.5319991111755371, 0.4843624234199524, 0.021987587213516235, -0.23129802942276, 0.8399779200553894, -0.5263317227363586, 1.0809561014175415, -0.641334056854248, 0.04952928423881531, 0.27650630474090576, -0.7977195978164673, -0.3022204339504242, 0.22895018756389618, -0.40197572112083435, -0.10381074994802475, 0.40265119075775146, 0.36945202946662903, -0.3161422908306122, 0.34129810333251953, 0.7744302749633789, 0.7213507890701294, -0.7482434511184692, 0.9469276070594788, 0.05728526413440704, -0.6602241396903992, -0.10982950776815414, 0.5582018494606018, 1.2033482789993286, -0.0033296742476522923, 0.16332665085792542, 0.30408668518066406, 0.26492103934288025, -0.802028238773346, 0.053088657557964325, 0.8627368211746216, 0.6559049487113953, 0.26775023341178894, 0.21071453392505646, -0.9331431984901428, -0.3173254132270813, 0.158616304397583, 0.4958297908306122, 1.2045940160751343, 0.11799640953540802, -0.7702004909515381, -0.9282374978065491, 0.011672372929751873, -0.5996114015579224, 0.15243177115917206, -0.3800172507762909, -0.701168954372406, -0.5876206159591675, -0.923321545124054, 1.1674152612686157, 0.4639156460762024, 1.32282555103302, -0.513786256313324, -1.0411454439163208, -0.39046573638916016, 0.44703245162963867, -0.7495054602622986, -0.2647441625595093, 0.5816061496734619, -0.8142656683921814, -0.16396120190620422, 0.508969783782959, -0.2816348671913147, 0.2140851765871048, -0.2724955081939697, 0.6205769777297974, 0.3094569146633148, -0.687124490737915, 0.5373229384422302, 1.3199812173843384, -0.2427864521741867, -0.7324032187461853, 0.27111417055130005, -0.10826414823532104, -0.2927266061306, 0.10255664587020874, 0.3908676207065582, 0.13991397619247437, 0.2699759006500244, -0.5081011056900024, -0.005936112254858017, 0.042090900242328644, 0.23775924742221832, 0.43502941727638245, 0.10719475895166397, -0.23338709771633148, -1.2499061822891235, 0.27554747462272644, 0.32417216897010803, -0.5419468283653259, 0.5397447347640991, -1.0967384576797485, 0.20673684775829315, 0.258881539106369, -0.6538993120193481, 0.09616763144731522, -1.3106476068496704, 0.7675479054450989, -0.22965183854103088, -0.059313852339982986, 0.007827073335647583, 0.415187805891037, -0.09598130732774734, -0.08037776499986649, 0.7414857149124146, 0.2722642421722412, 0.4909782111644745, 0.2766023576259613, -1.0850160121917725, 0.2448708564043045, 0.0065942383371293545, 0.3471307158470154, -0.555226743221283, 0.4237850606441498, -0.523726224899292, -0.35935088992118835, 0.21138016879558563, 0.33926671743392944, -0.16474489867687225, -0.3145100772380829, -0.13196605443954468, -1.399810552597046, 0.405063271522522, -0.7435504198074341, -0.8035658001899719, 0.06893769651651382, -0.8860671520233154, -0.5658323168754578, -1.095372200012207, -1.645087480545044, -1.220327615737915, -0.017973147332668304, -0.6722016930580139, 0.2615262269973755, -0.05571344494819641, -0.30739930272102356, -0.07256818562746048, -0.2299622744321823, -0.2514899671077728, 0.6799901127815247, -0.6735079288482666, 1.1712857484817505, -0.028593115508556366, -1.067429780960083, 0.0679660215973854, 0.31636273860931396, -0.20544004440307617, -0.6752316355705261, 0.44018083810806274, -0.8634514808654785, 0.05935518071055412, -0.30285951495170593, -0.7569385170936584, 0.12517617642879486, -0.002998964162543416, 1.1647697687149048, -0.5943981409072876, -0.6990309953689575, 0.10693603754043579, 1.3121404647827148, -0.48895686864852905, 0.8390719294548035, 0.1601133495569229, 0.6402583718299866, 0.2278037816286087, -0.015781322494149208, 0.3025490939617157, 0.42764732241630554, -0.006678925361484289, -0.011273732408881187, 0.4558001160621643, 0.45365089178085327, -0.5767145156860352, 0.4575699269771576, 1.294669270515442, 0.3500756621360779, 0.32524552941322327, -0.9672300815582275, 0.4146191477775574, -1.088434100151062, -0.8338673710823059, 0.7429762482643127, 1.5353703498840332, 0.3452147841453552, 0.05201159790158272, -0.0490567721426487, 0.6661630272865295, 0.532463788986206, -0.10642963647842407, -0.15837395191192627, -1.0251047611236572, 0.15476058423519135, 1.0508450269699097, 0.4304146468639374, 0.4579732120037079, 0.44091033935546875, 0.5820610523223877, 14.729321479797363, 0.6993986964225769, 0.14412614703178406, 0.3706279695034027, 0.6902063488960266, 0.45715779066085815, -0.6938929557800293, 0.45152607560157776, -0.868397057056427, -0.47899124026298523, 1.9370806217193604, 0.020181016996502876, 0.7768582105636597, -0.20307497680187225, -0.6822821497917175, 0.1944369673728943, -0.4929642677307129, 0.5889771580696106, -0.13285814225673676, -1.4637550115585327, 0.507506787776947, 0.3627977967262268, 0.18640978634357452, -0.07956146448850632, 0.8583173751831055, 0.7294958829879761, 0.6686391830444336, -0.8020467758178711, 0.33281221985816956, 0.48090532422065735, 1.145891547203064, -0.7465106844902039, 0.4113819897174835, 0.6375665664672852, -1.0434210300445557, -0.21008726954460144, 0.13094395399093628, -1.3801707029342651, -0.5295592546463013, -0.28807833790779114, -0.427884966135025, -0.5823022127151489, -0.3992547392845154, 0.5436320900917053, 0.757983922958374, 0.16026338934898376, -0.2632859945297241, 0.546339213848114, -0.303427517414093, -0.16465090215206146, -0.35351747274398804, 0.42699575424194336, -0.1050146296620369, -0.5111939311027527, -0.04206865653395653, 0.37481454014778137, 0.003809396643191576, 0.34142038226127625, -0.522807240486145, -0.4893113374710083, -0.8082103729248047, -0.4091264009475708, 0.21387001872062683, 0.5915195941925049, 0.6723082065582275, 0.18665766716003418, 0.12207254767417908, 0.0296998992562294, 0.4350757300853729, -0.0157261174172163, 0.02619875967502594, -0.4192327857017517, 0.3475610613822937, -0.065398208796978, -0.32873591780662537, 0.4645308554172516, -0.6236287951469421, -0.4562637507915497, -0.9136390089988708, -0.5762161612510681, 0.6036198139190674, -1.0040584802627563, -0.21845535933971405, 0.511210024356842, -0.5165884494781494, -0.050687141716480255, 0.3226715326309204, -0.40973493456840515, -0.2888311743736267, 0.0013658058596774936, -1.0136423110961914, -0.5990397930145264, 0.29331064224243164, -0.0016344661125913262, -0.7493621706962585, 0.42110252380371094, 1.5277115106582642, -0.4048084318637848, -0.5226597785949707, 0.39540132880210876, -0.46251344680786133, 0.10341677814722061, -0.7490315437316895, -0.9712913632392883, 0.6570786833763123, 0.42883965373039246, 0.03273957595229149, 0.9126952290534973, 0.18168461322784424, 0.515320360660553, -1.242570161819458, -0.040802594274282455, 0.7416694164276123, -1.4616875648498535, 0.05154545605182648, -0.5379844903945923, -0.9122701287269592, 0.7870780825614929, 0.025108542293310165, -0.1825861781835556, 0.18555383384227753, -0.08653942495584488, -0.8131510615348816, -0.5647435784339905, -0.9136102795600891, 0.4357203543186188, 1.1814689636230469, -1.4456286430358887, -0.559453547000885, -0.9479522705078125, 0.08774475753307343, -0.7507666349411011, -0.5196378827095032, 0.03197775036096573, 0.5976485013961792, -0.3696412146091461, 0.5241518020629883, -0.5159757137298584, 0.761799156665802, 0.7544232606887817, 0.1761350780725479, -0.16608311235904694, -0.0027474681846797466, -1.5767565965652466, -0.16062769293785095, 0.2397213876247406, 0.750931978225708, -1.0739352703094482, 0.09434055536985397, 1.0849525928497314, 0.37299564480781555, -0.32047536969184875, -0.8567145466804504, 0.4057232141494751, -0.06695175915956497, -0.5577101707458496, 0.5445711016654968, 0.012391971424221992, -0.012262186966836452, 0.03545922785997391, 0.4671298563480377, 0.9219465255737305, -0.10317085683345795, -0.21331018209457397, 0.7151131629943848, -0.4698471426963806, 0.20082855224609375, -0.7226276993751526, -0.6061321496963501, -1.1862024068832397, 0.026248160749673843, -0.8962646126747131, 0.20890666544437408, -0.7316256165504456, -0.4847696125507355, -0.11197085678577423, -0.39947831630706787, -0.03222696855664253, 0.7567436695098877, -0.510854959487915, -1.273299217224121, -0.496722549200058, -0.445983350276947, 0.48331505060195923, 0.3098042607307434, -0.49543049931526184, 0.28023019433021545, 0.14702680706977844, 0.07137233018875122, -0.03884304687380791, 0.49968409538269043, -0.8194762468338013, -0.7289707064628601, -0.41840362548828125, 0.3512761890888214, 0.3902946412563324, 0.105024054646492, -0.6869301795959473, 0.8178113698959351, 0.23898884654045105, -0.45005178451538086, 0.2768663763999939, 0.1215895339846611, -0.922149121761322, 0.0805189311504364, 0.8073362708091736, -1.0534697771072388, 0.17125913500785828, 0.25488901138305664, -0.5683332681655884, 0.3592013418674469, 0.7221219539642334, -0.03186660259962082, -0.8138206601142883, 0.021933449432253838, 0.31437990069389343, -1.2387160062789917, -0.01940925605595112, -0.132563516497612, -0.357196569442749, -1.178091287612915, 0.014549694955348969, 0.10245991498231888, 0.5134214162826538, 0.21495217084884644, 0.39417633414268494, 0.3383108973503113, -0.538173258304596, 0.4841143786907196, 0.4087192416191101, 0.16112655401229858, 0.13518913090229034, 0.3662143051624298, 0.3454984426498413, -0.1889238804578781, 0.09543228894472122, 0.32983455061912537, 0.4324866235256195, -0.9791744947433472, 0.09250228106975555, 0.9897861480712891, -0.47983694076538086, -0.10608196258544922, 0.6315835118293762, -0.36710068583488464, -0.43681100010871887, -0.20350691676139832, -1.6227059364318848, -0.168854758143425, -0.7618255019187927, 0.4234759211540222, 0.15643641352653503, -0.5134202241897583, 0.27362069487571716, -0.6249671578407288, 0.703227698802948, -0.22863249480724335, -0.49510493874549866, 0.19497165083885193, -0.025364205241203308, -0.6145204901695251, 1.0404794216156006, 0.14026960730552673, -0.5560344457626343, -0.3133116662502289, -0.7548151016235352, -0.04482627287507057, -0.03750075772404671, 0.3855002224445343, 0.0024557174183428288, -0.26755043864250183, 0.994930624961853, 0.4543425142765045, 0.8111456036567688, 0.09791747480630875, -0.41267770528793335, -0.1366720199584961, 0.7271462678909302, 0.8621067404747009, -0.03050180710852146, -0.43862518668174744, 0.9600292444229126, 0.7178561687469482, -0.08269418776035309, 0.1327088177204132, -0.5995507836341858, -0.2183207869529724, 0.611526608467102, 0.45324817299842834, -0.3152270019054413, 0.5677872896194458, 0.08332457393407822, -0.1341361403465271, 0.16732557117938995, -1.6355822086334229, -0.2214651256799698, -0.013466441072523594, 0.7919806838035583, 0.003035115310922265, 0.2058599889278412, 0.3110561966896057, 1.328966498374939, 0.09613431990146637, 0.5309576988220215, 1.132590889930725, 1.070784568786621, -0.2723327577114105, 0.051726218312978745, 0.33049046993255615, 0.35295864939689636, -1.04110848903656, -0.1502312272787094, -0.286418080329895, 0.7065714597702026, -0.12903441488742828, 0.5101107954978943, 0.5085244178771973, -0.26238512992858887, 0.5132343769073486, 0.24140401184558868, 0.8486067056655884, -0.3906067907810211, -0.3720468580722809, -0.8264631628990173, -0.4041385054588318, 0.026038717478513718, -0.10812009125947952, -0.2527134418487549, -0.7765060663223267, -0.6112970113754272, -0.13069437444210052, 0.6973252892494202, -0.14520218968391418, 1.1041792631149292, 0.6800073385238647, 0.08206337690353394, 0.4545501470565796, -0.15306006371974945, -0.3741162121295929, -0.37723132967948914, 0.3155794143676758, -1.0019828081130981, 0.22047457098960876, -0.18310168385505676, -0.6607434153556824, -0.6462168097496033]}, "authors": [{"authorId": "51033208", "name": "Bingbin Liu"}, {"authorId": "40401847", "name": "J. Ash"}, {"authorId": "9935792", "name": "Surbhi Goel"}, {"authorId": "37019006", "name": "A. Krishnamurthy"}, {"authorId": "15943185", "name": "Cyril Zhang"}], "references": [{"paperId": "d4c60620570801a231a7756f931dda1740288fb9", "title": "Looped Transformers as Programmable Computers"}, {"paperId": "6edd112383ad494f5f2eba72b6f4ffae122ce61f", "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"}, {"paperId": "235303a8bc1e4892efd525a38ead657422d8a519", "title": "Transformers are Sample Efficient World Models"}, {"paperId": "f5f5616f39493566a9d502f611adcc8f1ceb394e", "title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "d253beffd28d88cc3150c9e80511a6187ea6613b", "title": "Unveiling Transformers with LEGO: a synthetic reasoning task"}, {"paperId": "0bc131cf98f9fbb0a736a6b7776afd721fcdbd57", "title": "Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials"}, {"paperId": "67dc33f02c8b5f24cd213b6b5fb5c74cead581aa", "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "c9143b978f91ee35429f1644a2266e5b036dad3a", "title": "End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "257aee73d83a87921fd2d56b524de394dcf6a264", "title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "3b3d7adb9047d01af6dfa2975ad8addd69715e96", "title": "Mastering Atari Games with Limited Data"}, {"paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737", "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "b61de520bc1ae57abde895601b62b4f92d82c0b4", "title": "Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "ebe510dc7b8025e496e11530192f2cccc184d002", "title": "Can contrastive learning avoid shortcut solutions?"}, {"paperId": "0735fb79bf34698c1df4461a05ed51c232c412e4", "title": "Thinking Like Transformers"}, {"paperId": "f7664102a451332ed7e1286561b2f621eaff128d", "title": "Programming Puzzles"}, {"paperId": "941612bd6750efa76e1a75bdc64b6e3d7ed66457", "title": "Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks"}, {"paperId": "f864d4d2267abba15eb43db54f58286aef78292b", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "f5d738dd6ffd3755036b02d5e29581f5bb93b879", "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks"}, {"paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea", "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"}, {"paperId": "14014c024674991149f3ecf9314c93f7e029ef1a", "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"}, {"paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a", "title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "9e656adf7bf18bb35bfe7f1243e4536a04af28c4", "title": "On Krohn-Rhodes theory for semiautomata"}, {"paperId": "227fe850a72fab24998c7e08d75db214715dc74e", "title": "The EOS Decision and Length Extrapolation"}, {"paperId": "92ed2e34501903d922d74f28a012d6e337418fa4", "title": "How Does Mixup Help With Robustness and Generalization?"}, {"paperId": "52d87b59a1fa6561b90d5ea56f21180a9b1a505c", "title": "How Can Self-Attention Networks Recognize Dyck-n Languages?"}, {"paperId": "6b989b8327db3a7212141c59c1569f0219775058", "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks"}, {"paperId": "10c86505de83647c7b4157595ab10f64e97c94ef", "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"}, {"paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b", "title": "Generative Language Modeling for Automated Theorem Proving"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "1b04936c2599e59b120f743fbb30df2eed3fd782", "title": "Shortcut learning in deep neural networks"}, {"paperId": "2f33459606121e25e3e6142416b0203bc6b40358", "title": "Learning Parities with Neural Networks"}, {"paperId": "0cc956565c7d249d4197eeb1dbab6523c648b2c9", "title": "Dream to Control: Learning Behaviors by Latent Imagination"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62", "title": "Deep Learning for Symbolic Mathematics"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f", "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "87e15e8593258ced60bbd7fbfddfee907186be75", "title": "Depth Separations in Neural Networks: What is Actually Being Separated?"}, {"paperId": "beb051c652f02c2d5829d783fbc4f3acce99bc3c", "title": "Visualizing Attention in Transformer-Based Language Representation Models"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4065d2f8c0641df9ac80202e444936a86aa2b48a", "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU gates"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e60b88313fad52c1ef8dd02b482785651d09ad66", "title": "Depth Separation for Neural Networks"}, {"paperId": "2aee9fcdac405d696b47c8573013ecd6cafbaf57", "title": "On the Ability of Neural Nets to Express Distributions"}, {"paperId": "e7c2874f7b5db50a27d79298ee4d051462c1b8a3", "title": "Reliably Learning the ReLU in Polynomial Time"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "df0402517a7338ae28bc54acaac400de6b456a46", "title": "WaveNet: A Generative Model for Raw Audio"}, {"paperId": "d0156126edbfc524c8d108bdc0cf811cfe3129aa", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "4206c84525a7904df3613b843491c0ae6a5507eb", "title": "Benefits of Depth in Neural Networks"}, {"paperId": "a9da71715d54e33959751c88bb69a5875a23e324", "title": "The Power of Depth for Feedforward Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "ae3aba9c444f33242f9ade7ca607fa6002a7a12a", "title": "Computational Holonomy Decomposition of Transformation Semigroups"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "5b7bd2a9f5c3424f633bd991d1cf2c98ae013e3c", "title": "Computational Complexity: A Modern Approach"}, {"paperId": "17594df98c222217a11510dd454ba52a5a737378", "title": "On the computational power of neural nets"}, {"paperId": "738d39d9fd380539d922021278a42d5fb1d953b3", "title": "Finite-Automaton Aperiodicity is PSPACE-Complete"}, {"paperId": "8da1dda34ecc96263102181448c94ec7d645d085", "title": "Approximation by superpositions of a sigmoidal function"}, {"paperId": "f22f6972e66bdd2e769fa64b0df0a13063c0c101", "title": "Multilayer feedforward networks are universal approximators"}, {"paperId": "d0f8504ca88110e812889e9bd392e095ee18f53a", "title": "Finite permutation groups with large abelian quotients."}, {"paperId": "d51eb16dfed68bf6e16b8b4516d607370b91189a", "title": "The Complexity of Markov Decision Processes"}, {"paperId": "43333a606de052e929fc443a1c8d4d127a6ccf4c", "title": "On Threshold Circuits and Polynomial Computation"}, {"paperId": "f4b7a72d4e34d767b0c20953c3a124e4b28f2544", "title": "Data parallel algorithms"}, {"paperId": "5a84ccdd776fc3a47a50bac1c511206d66c9a04f", "title": "Bounded-width polynomial-size branching programs recognize exactly those languages in NC1"}, {"paperId": "54861b04fdcd68dda1f488230928b4d109b69a63", "title": "Unbounded fan-in circuits and associative functions"}, {"paperId": "75f57a8e71ebc9f05b6c9983032a75fc1edb1597", "title": "Parity, circuits, and the polynomial-time hierarchy"}, {"paperId": "fed7aca2268f3c7ebfc0c52740144d14197db589", "title": "Automata, languages, and machines. A"}, {"paperId": "0e83b9ad091596cf5c8edce23a669f1dbc34bac2", "title": "Cascade synthesis of finite-state machines"}, {"paperId": "87e35220fe89b4ea5c36ff43e7ee897d5d6bd7a9", "title": "On Finite Monoids Having Only Trivial Subgroups"}, {"paperId": "3b0cc63c910d36b8e29cedeb97f29e4b0dcfc105", "title": "Algebraic theory of machines. I. Prime decomposition theorem for finite semigroups and machines"}, {"paperId": null, "title": "A mechanistic interpretability analysis of grokking"}, {"paperId": null, "title": "2022) study essentially the same problem however they model the task as a natural language task and use pretrained Transformers"}, {"paperId": "1781517a91be8248dd0febad65211a1b6614e199", "title": "On the Power of Saturated Transformers: A View from Circuit Complexity"}, {"paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922", "title": "Attention is Turing-Complete"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}, {"paperId": null, "title": "1959) states that all context-free languages can be (homomorphically) represented by the intersection of a Dyck language and a regular language"}, {"paperId": "777eb55a1e80e8e45a52e46154ea06a365a944b8", "title": "Applications Of Automata Theory And Algebra Via The Mathematical Theory Of Complexity To Biology"}, {"paperId": null, "title": "2022) perform a theoretical and empirical study of the ability of Transformers (and other architectures) to learn sparse parities where the support size k T"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2021) for a richer class of problems. Our use of scratchpad is inspired by Anil et al. (2022)"}, {"paperId": null, "title": "2020) study size generalization in graph neural networks where they train on small graphs and evaluate on larger sized graphs with similar structural properties"}, {"paperId": "759d4baa4fe6b27f04505aa9eb54a5351f3fcfdd", "title": "On the Krohn-Rhodes Cascaded Decomposition Theorem"}, {"paperId": "b0a31111bcb7635544afe676b89d308f52330ba8", "title": "Threshold circuits for iterated matrix product and powering"}, {"paperId": "7b0c195a10526a10161fb4a7844be642da32bccd", "title": "On the Cascaded Decomposition of Automata, its Complexity and its Application to Logic"}, {"paperId": null, "title": "23There is nothing in general preventing quotient groups from being extremely large groups which are not realizable as smaller permutation groups"}, {"paperId": "1289b74b11fff38eb26266bbea455d75af6d9dfb", "title": "The number of semigroups of order n"}, {"paperId": "ed1fbc8829224a446bcd79f324cc71579e3794fe", "title": "The Algebraic Theory of Context-Free Languages*"}, {"paperId": null, "title": "Produit complet des groupes de permutations et probleme d\u2019extension de groupes II"}, {"paperId": null, "title": "\u2022 It is intuitively clear that we can (and should) compute the sequence corresponding to \"direction at time t"}, {"paperId": null, "title": "heads = max{sim N .heads, sim H .heads}"}]}