{"paperId": "59fed7ca092c7e83583906456756abba8ce9295a", "abstract": "Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (illustrated via experiments) and offer taxonomic mitigation strategies for them. Finally, we highlight some unsolved research challenges and present promising future directions.", "venue": "Journal of machine learning research", "year": 2022, "citationCount": 30, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.06640", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "First, the *algorithmic speedup* problem is formalized, then the fundamental building blocks of algorithmically efficient training are used to develop a taxonomy, which highlights commonalities of seemingly disparate methods and reveals current research gaps."}, "embedding": {"model": "specter_v2", "vector": [0.1258457899093628, 0.1879771500825882, -0.22702434659004211, -0.06373846530914307, -0.22420309484004974, 0.01891971193253994, 0.31561681628227234, -0.08866937458515167, -1.2167803049087524, -0.4367735683917999, 0.0018169768154621124, -0.04644833505153656, 0.6275017857551575, -0.15668052434921265, -0.3032204210758209, -0.0109760332852602, -0.7700831294059753, -0.07598607987165451, 0.03314143046736717, 0.16702669858932495, -0.02324441447854042, 0.36954066157341003, -1.7818348407745361, 0.0747857466340065, 0.13957887887954712, 1.0310790538787842, -0.265931099653244, 1.3524943590164185, -0.4144655168056488, 0.4289432466030121, 0.348771870136261, -0.14161980152130127, 0.5063167810440063, 0.1519210934638977, -0.5728012323379517, -0.32510095834732056, 0.7343132495880127, -0.850553035736084, -0.6730085015296936, 0.6651232242584229, -0.06975056976079941, 0.3919870853424072, 0.04011564329266548, -0.817185640335083, 0.5321163535118103, -0.27511900663375854, 0.13423840701580048, 1.1463483572006226, -0.8994244337081909, -0.11143051087856293, 1.0066217184066772, -1.1335047483444214, 0.1240113377571106, 1.0881470441818237, 0.8738517761230469, 0.2554134130477905, -0.6078858971595764, -0.4392622113227844, 0.16234327852725983, -0.28227996826171875, -0.5542957186698914, -0.5519653558731079, 0.0930091142654419, -0.286452978849411, 1.9575315713882446, -0.30102118849754333, -0.11204203963279724, 0.038900770246982574, 0.12227330356836319, 1.5668944120407104, 0.38896653056144714, -0.6219359636306763, 0.12766942381858826, -0.2219444364309311, 0.9406285285949707, 0.7571280002593994, 0.25151973962783813, 0.4738374650478363, -1.2133883237838745, 0.0006270394078455865, 0.1345660537481308, 0.5004493594169617, 0.39833641052246094, -0.27634602785110474, 0.23402243852615356, 0.9367230534553528, 0.7363839745521545, 0.12955501675605774, -0.638934850692749, 1.4449124336242676, 0.7535170912742615, 0.31308412551879883, 0.21908225119113922, 0.5021891593933105, -0.013796340674161911, 0.17152610421180725, -1.0475809574127197, 0.026586400344967842, 0.1448342502117157, 0.7642550468444824, 0.11689143627882004, 0.2028142213821411, -0.09683068841695786, 0.08975612372159958, 0.4769042134284973, -0.16889265179634094, -0.006559199653565884, -0.6705556511878967, 0.5605489611625671, -0.42179355025291443, -0.2604323923587799, -0.3186012804508209, -0.32225334644317627, -0.3992188572883606, -1.300826907157898, -0.4333658218383789, -0.717231273651123, -0.25149837136268616, -0.823722243309021, 0.07143773883581161, -0.3685394525527954, 0.4400448203086853, 0.29827776551246643, 0.7317590117454529, 0.6252191066741943, 0.6998223662376404, 0.07787118852138519, 0.3762596547603607, 1.1720833778381348, -1.1224738359451294, 0.09311746805906296, -1.1030025482177734, 0.531395435333252, -0.14226862788200378, 0.1992846578359604, -0.257312536239624, -1.36304771900177, -1.0886046886444092, -0.9749684929847717, -0.18697264790534973, -0.5065699815750122, 0.036484166979789734, 1.7829009294509888, 0.4218598008155823, -1.3038116693496704, 0.9916897416114807, -0.8057502508163452, 0.0945519432425499, 0.6785913109779358, 0.5149185657501221, 0.4873557984828949, -0.38226866722106934, -0.2698531150817871, -0.1927119642496109, 0.21051980555057526, -0.8758723735809326, -0.36427566409111023, -0.7135054469108582, -0.4197409451007843, 0.4055493175983429, -0.16632217168807983, -1.0376240015029907, 1.1974492073059082, -0.35913753509521484, -0.6278986930847168, 0.8444625735282898, -0.17906399071216583, 0.07227744162082672, 0.5175641775131226, -0.0372212678194046, -0.14425797760486603, 0.05629609525203705, -0.49680283665657043, 0.7308093905448914, 0.6346213221549988, -0.0022618004586547613, -0.3707282841205597, 0.03646036610007286, -0.049591150134801865, -0.17022885382175446, -0.6682886481285095, 0.9509312510490417, -0.6099591255187988, -0.30843162536621094, 0.6482049822807312, 0.4065019488334656, -0.5852630138397217, 0.07811782509088516, 0.12952764332294464, -0.6199608445167542, 0.731816291809082, 0.22405853867530823, 0.560455322265625, -1.1641196012496948, -1.1558120250701904, 0.017236217856407166, 0.428731769323349, -0.38367021083831787, -0.557794451713562, -0.04797475039958954, -0.24815496802330017, 0.148008793592453, -0.05512194707989693, -0.9440860748291016, -0.15759390592575073, -0.3578594923019409, -0.5283833146095276, -0.13837620615959167, 0.26789408922195435, 0.740750253200531, -0.5210548043251038, 0.3780486285686493, -0.4765389561653137, 0.4475530982017517, -1.302312970161438, 1.1803609132766724, 0.07113998383283615, -0.14933118224143982, 0.1410444676876068, -0.10090447217226028, 0.5851688981056213, -0.6813597083091736, 0.6710749268531799, -0.6585306525230408, -0.03967138007283211, 0.6628105640411377, -0.524398922920227, 1.1862279176712036, -0.6506616473197937, 0.6048281788825989, 0.4106019139289856, -0.7225462198257446, -0.17523810267448425, -0.11416725814342499, -0.16764844954013824, -0.44262462854385376, 0.5955145955085754, 0.4338487684726715, -0.6583203673362732, 0.515959620475769, 1.2753539085388184, 1.1626133918762207, -0.1730128675699234, 0.5297824144363403, 0.5666824579238892, -0.4228264093399048, 0.4524759352207184, 0.24265959858894348, 0.8564005494117737, 0.09461479634046555, -0.19733434915542603, -0.24280448257923126, -0.07440369576215744, -0.6594895124435425, -0.2747747600078583, 0.6541821956634521, 0.4033384919166565, 0.19467748701572418, 0.47844430804252625, -1.011893391609192, -0.5621531009674072, -0.04394496977329254, 0.6943879127502441, 1.8676694631576538, -0.010193068534135818, 0.36580729484558105, -0.7754126787185669, -0.7429547309875488, -0.12918607890605927, -0.2817452847957611, 0.17789863049983978, -0.07851912081241608, -0.755276620388031, -1.5007593631744385, 0.7879990935325623, 0.6751212477684021, 1.3599201440811157, -0.24598410725593567, -1.1929670572280884, -0.6621728539466858, 0.8489694595336914, -0.8168061375617981, -0.3808533847332001, 1.0210119485855103, -1.4902844429016113, -0.2684582471847534, 0.33250290155410767, -0.3707509934902191, 0.6144970655441284, 0.10191510617733002, 1.2540912628173828, 0.12704579532146454, -0.48904579877853394, -0.18390631675720215, 0.9128528833389282, -0.6691159605979919, -0.16480307281017303, 0.3606279790401459, 0.12148509174585342, -0.5410177111625671, 0.3073959946632385, 0.11993888765573502, -0.2605784833431244, -0.004937103949487209, -0.460787832736969, -0.001411289325915277, 0.3429081439971924, 0.1574934870004654, 1.024117350578308, -0.05716756358742714, 0.026268230751156807, -1.2350503206253052, 1.0753411054611206, 0.23668739199638367, -0.526245653629303, -0.06376457214355469, -1.0315918922424316, 0.5542159676551819, 0.49832162261009216, -0.5599454045295715, -0.1624705046415329, -0.6318141222000122, 0.21339313685894012, -0.7330541014671326, -0.28358203172683716, -0.3674173355102539, 0.9444505572319031, -0.6482322812080383, 0.43321871757507324, 0.31890130043029785, 0.6258285045623779, -0.030916685238480568, 0.2503076195716858, -0.9521734714508057, 0.5586926937103271, -0.06826964020729065, 0.0012382755521684885, -0.008374798111617565, 0.16096918284893036, -0.315819650888443, -0.468229204416275, -0.2333003282546997, -0.15693289041519165, -0.40464645624160767, -0.008971543051302433, -0.4193415939807892, -0.8562396168708801, -0.21143195033073425, -1.0289660692214966, -0.19679196178913116, 0.20062774419784546, 0.005699863191694021, 0.08635897934436798, -1.3104463815689087, -1.5485975742340088, -0.2974063754081726, -1.2862789630889893, -1.4774267673492432, 0.38945627212524414, 0.35936403274536133, -0.18529093265533447, -0.1740202158689499, -0.573161244392395, -0.552582323551178, 1.0493109226226807, -0.2916993498802185, 0.6897739171981812, 0.0757693201303482, -0.4272150695323944, -0.10121489316225052, -0.5808463096618652, 0.400289922952652, -1.1963834762573242, 0.10222294926643372, -1.314095377922058, -0.22788317501544952, -0.4166727364063263, -0.8022401332855225, 0.6603402495384216, 0.29612818360328674, 0.9182440638542175, -0.0589478574693203, -0.24273689091205597, 1.0845681428909302, 1.5858694314956665, -0.8957838416099548, -0.18029217422008514, -0.019870910793542862, 0.9307852983474731, -0.021487971767783165, -0.45647096633911133, 0.39791038632392883, -0.3670975863933563, 0.10022587329149246, 0.09629391878843307, -0.44725480675697327, -0.6237171292304993, -0.04183225706219673, 0.027500135824084282, 1.3726792335510254, 0.3848431706428528, 0.48019441962242126, -0.777337908744812, 0.24729935824871063, -0.831449568271637, -0.15775325894355774, 0.5735213160514832, 0.6562573313713074, 0.04251125827431679, 0.167534738779068, -0.18805339932441711, 0.054338544607162476, 0.32601308822631836, 0.750134289264679, -0.8038091659545898, -0.9365242719650269, 0.15829473733901978, 0.9703515768051147, 0.8839338421821594, 0.19753175973892212, -0.30560463666915894, 0.4583616852760315, 14.445039749145508, 0.9668655395507812, -0.22581066191196442, 0.7399402260780334, 0.9595046639442444, -0.24707898497581482, 0.012383854016661644, -0.285867303609848, -1.0220352411270142, -0.10393742471933365, 1.2830106019973755, 0.37302958965301514, 0.7222980856895447, 0.8384563326835632, -0.3377169966697693, -0.18076711893081665, -0.7963341474533081, 1.2092384099960327, 0.25977784395217896, -1.7687232494354248, 0.0024184356443583965, 0.2696138620376587, 0.5221648216247559, 0.654911458492279, 0.4974910616874695, 0.9687885046005249, 0.47429901361465454, -0.3454645872116089, 0.3353732228279114, -0.11604364216327667, 1.4298561811447144, -0.6585634350776672, 0.8440335988998413, 0.415661096572876, -1.1767048835754395, 0.15389907360076904, -0.2786024510860443, -1.2007323503494263, -0.2464388906955719, 0.26151272654533386, -0.38882800936698914, -0.48005279898643494, 0.1442323625087738, 0.4927051067352295, -0.2307879626750946, 0.4264257252216339, -0.12264112383127213, 0.3741018772125244, -0.4348132610321045, -0.27214133739471436, 0.20557807385921478, 0.3454267680644989, -0.4797861576080322, 0.1507219821214676, -0.39632418751716614, 0.11436313390731812, 0.07523687928915024, 0.12000593543052673, -0.9968419671058655, -0.5304845571517944, -0.08619553595781326, -0.07724405080080032, 0.11343975365161896, 0.6685681939125061, -0.012285363860428333, 0.3360862731933594, -0.26244431734085083, 0.4396410584449768, 0.7643774151802063, -0.11739615350961685, -0.3596422076225281, -0.36280569434165955, 0.5390462875366211, -0.7397640347480774, -0.40124446153640747, 0.4209651052951813, -1.034993052482605, -0.5297061204910278, -0.6129603385925293, -0.47296571731567383, 0.6189402341842651, -0.5383996367454529, -0.4375147223472595, 0.8467727303504944, -0.5974936485290527, 0.11404051631689072, 0.6909787654876709, -1.0810856819152832, -0.2067164033651352, 0.5801987051963806, -1.3388853073120117, 0.04553235322237015, -0.12292542308568954, -0.3073650002479553, -0.523240864276886, -0.0849468782544136, 1.0749365091323853, 0.34243661165237427, -0.5484117865562439, 0.31687045097351074, 0.1245478093624115, -0.04271011799573898, -0.545123279094696, -0.4770655333995819, 1.0162397623062134, 0.7193649411201477, -0.4804490804672241, -0.07808826863765717, -0.12889832258224487, 0.4935418665409088, -1.0276074409484863, -0.266762375831604, 0.35699576139450073, -0.28938814997673035, -0.29597634077072144, -0.5215063095092773, -0.3987962305545807, 0.18091987073421478, -0.12063059955835342, 0.02494116500020027, 0.3345350921154022, 0.28901201486587524, -0.7887111902236938, -0.44738104939460754, -0.9175336956977844, 0.13852559030056, 0.8141499757766724, -1.051418423652649, -0.20896394550800323, 0.04287644475698471, 0.37219181656837463, -0.9457306861877441, -0.5169824957847595, -0.3125919997692108, -0.12989819049835205, -0.3781072497367859, 1.1319313049316406, -0.331745445728302, 0.8865159153938293, 1.0306307077407837, -0.03121350146830082, -0.1899462193250656, 0.3465728759765625, -0.4884878993034363, -0.1530357152223587, -0.3581571578979492, 0.05874190106987953, -0.5486435294151306, 1.189149260520935, 0.7406673431396484, 0.1333715170621872, -0.3414451479911804, -0.5892398953437805, 0.03216275945305824, -0.49027758836746216, -0.5589954853057861, 0.38132789731025696, 0.10033103823661804, -0.5911129117012024, -0.009618660435080528, 0.8725231885910034, 0.4630058705806732, 0.26749181747436523, -0.09381186217069626, 0.5548682808876038, -0.4754372239112854, -0.23155143857002258, -1.08378267288208, -0.6668413281440735, -1.67173171043396, 0.10362478345632553, -1.0859744548797607, -0.3425412178039551, -0.30583009123802185, -0.5545499324798584, -0.24428610503673553, -0.18802933394908905, 0.11599849909543991, 0.30087941884994507, 0.02149665728211403, -0.6035491228103638, -0.18929168581962585, -0.45259419083595276, 0.43402430415153503, 0.4431461691856384, -0.3006942570209503, -0.00959303230047226, -0.3097713589668274, 0.1448727697134018, 0.5771265029907227, 0.3902779221534729, -0.5743908882141113, -0.6387750506401062, -1.629270315170288, 0.19639119505882263, 0.053837742656469345, 0.028411850333213806, -0.9891716241836548, 0.8753263354301453, 0.28733086585998535, -0.1766621321439743, -0.06485171616077423, 0.021296458318829536, -1.0172808170318604, -0.5519941449165344, 0.9028365612030029, -0.43515533208847046, 0.5469464063644409, 0.6208195686340332, -0.4290708303451538, -0.21204371750354767, 0.587655246257782, 0.25926515460014343, -0.15535132586956024, -1.2196156978607178, 0.3136095404624939, -0.17837713658809662, -0.016150599345564842, 0.08569715172052383, -0.3188684284687042, -1.4640246629714966, 0.39659276604652405, -0.0807046964764595, 0.2458248734474182, -0.3117526173591614, 0.15888401865959167, 0.2959533929824829, -0.903897762298584, 0.36672452092170715, 0.39947786927223206, -0.4649881422519684, 0.37931719422340393, 0.5120916366577148, 0.7047438621520996, -1.4147043228149414, 0.15836195647716522, -0.09351260215044022, 0.2992100417613983, -1.0194458961486816, 0.003742528846487403, 0.8868435025215149, -0.5376104712486267, -0.16383284330368042, 1.3998396396636963, -0.4162020981311798, -0.7773209810256958, 0.3481120467185974, -0.980691134929657, 0.06645966321229935, -0.12717266380786896, 0.32234951853752136, 0.6082838177680969, 0.47368401288986206, 0.6803910732269287, -0.7531885504722595, -0.06661944836378098, 0.111915722489357, -0.3502248525619507, 0.5769863128662109, 0.36737722158432007, -0.3616260588169098, -0.06876610219478607, 1.053412675857544, -0.8442225456237793, -1.2409265041351318, -0.7950189113616943, 0.07628606259822845, -0.19456708431243896, 0.8892691731452942, -0.022166818380355835, -1.3829920291900635, 0.8954026103019714, 0.50862056016922, 0.08613012731075287, 0.793563723564148, -0.5358449816703796, 0.4272560477256775, 0.7133421897888184, 0.46919092535972595, -0.6364133954048157, -0.6132514476776123, 1.0937700271606445, 0.5855529308319092, -0.9457417130470276, 0.6833133101463318, -0.31636083126068115, -0.18722766637802124, 0.8489651083946228, 0.45687785744667053, -0.42255428433418274, 1.0168519020080566, 0.0408361554145813, -0.4233003854751587, 0.16555340588092804, -0.720092236995697, 0.04919474199414253, 0.6064015030860901, 0.450045108795166, 0.4859773516654968, 0.23089779913425446, 0.03829436004161835, 1.0487970113754272, -0.13581404089927673, -0.012951080687344074, 0.24219529330730438, 0.9834602475166321, 0.13291533291339874, 0.5289924740791321, -0.06267080456018448, 0.7299036383628845, -0.8551957011222839, -0.6096130609512329, 0.28633710741996765, 1.0252355337142944, 0.2744981348514557, 0.480907142162323, 1.0994023084640503, -0.250722199678421, 0.5646376609802246, -0.2325475811958313, 0.43725305795669556, -0.2927675247192383, -0.6937641501426697, -0.2028430551290512, -0.344043105840683, -0.38639819622039795, -0.00015134474961087108, -0.21017903089523315, -0.43995511531829834, -1.3034390211105347, 0.7694956064224243, -0.06192780286073685, 0.7305458784103394, 0.4793967604637146, 1.0163884162902832, 1.1742080450057983, -0.09094934910535812, -1.0703104734420776, -0.4090845286846161, -0.30680233240127563, 0.0705898255109787, -0.68983393907547, -0.3587677478790283, 0.24524074792861938, -0.5581792593002319, -0.17067542672157288]}, "authors": [{"authorId": "41053241", "name": "Brian Bartoldson"}, {"authorId": "1749353", "name": "B. Kailkhura"}, {"authorId": "2969944", "name": "Davis W. Blalock"}], "references": [{"paperId": "8973ad5fc1264594a1fda3bd9e04258074cea9cc", "title": "Neural Architecture Search: Insights from 1000 Papers"}, {"paperId": "4b308ba40e67b0b4b25c6fde17195d5a456a2f41", "title": "Cramming: Training a Language Model on a Single GPU in One Day"}, {"paperId": "bd8412c233bf3815d8e905911b58e12bd6f279da", "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model"}, {"paperId": "c155d642c380bee8bd2668c56ff3d486042ecb32", "title": "Speeding up NAS with Adaptive Subset Selection"}, {"paperId": "e320bded2fdcec07f9926cd3104a4a2cad7432ad", "title": "Partitioned Gradient Matching-based Data Subset Selection for Compute-Efficient Robust ASR Training"}, {"paperId": "86fb2ee92569d35a8b471d814fa4c7653728536f", "title": "Wide Attention Is The Way Forward For Transformers"}, {"paperId": "34e1c62586f0c86af60a6ff2c3e1121c1ebd779a", "title": "Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging"}, {"paperId": "33fd110d1e4ca5f91d1b7ca7ff24ce1e9335359e", "title": "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics"}, {"paperId": "81521a80f7aa939281863c15b42e446ff5a0e65a", "title": "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models"}, {"paperId": "3deca3e77cfddb11feb4783f8acb2ceb860800e5", "title": "Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training"}, {"paperId": "0d10c6c656c7c28858004284db7f86f46d615a8a", "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout"}, {"paperId": "8b3a67c7e5289eed160d2acfd04d71cfb552c67d", "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models"}, {"paperId": "1d63856b1163b5b96558abed446af04f6bc58659", "title": "Scalable K-FAC Training for Deep Neural Networks With Distributed Preconditioning"}, {"paperId": "45122c8f76a4e2fd0163d1f0522db37e97ea4721", "title": "Beyond neural scaling laws: beating power law scaling via data pruning"}, {"paperId": "86264bc56781615ecc06c234fde50771aefbbd8e", "title": "Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-\u0141ojasiewicz Functions when the Non-Convexity is Averaged-Out"}, {"paperId": "6a8db14262ca2017cb253e12b8daeb57989a38df", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"}, {"paperId": "00df5cf0d83c48657d453ab8083d8805a67f744f", "title": "Measuring the Carbon Intensity of AI in Cloud Instances"}, {"paperId": "b294dc2703d25caa8988ecfd96b1d02bfff82f7e", "title": "Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning Rates"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "159be298e25b7210ae577d7962cceb5e73aee687", "title": "Automated Progressive Learning for Efficient Training of Vision Transformers"}, {"paperId": "71aa819311ebd94807650b991507057b89c1dc21", "title": "Benchmarking Test-Time Unsupervised Deep Neural Network Adaptation on Edge Devices"}, {"paperId": "a64dae353fce4ec2ee0686bf4796e4bd058ffaff", "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning"}, {"paperId": "2f4aef9cbb475196b515b6f4cf561b3eb69f561a", "title": "Unsupervised Test-Time Adaptation of Deep Neural Networks at the Edge: A Case Study"}, {"paperId": "0b0d7d87c58d41b92d907347b778032be5966f60", "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": "07f46ed2ebba37fd639dc060503e012ea752fe01", "title": "Towards Efficient and Scalable Sharpness-Aware Minimization"}, {"paperId": "af64997bf7cebd1c7bf848fee99dc9c3a59c8d57", "title": "Gradients without Backpropagation"}, {"paperId": "76cb108e37d9d2a06f5a49df04e993f5fb123c26", "title": "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink"}, {"paperId": "98850975e574e08695a9f32b4c8747dc7f8bcc17", "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam"}, {"paperId": "927a5203363fc9c8ba48599dc749cf0cc647444b", "title": "Compute Trends Across Three Eras of Machine Learning"}, {"paperId": "e468f74ebffa8bbdd99bf8d0233822a1d2a9b430", "title": "Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "5b25320fb65713e4e8be182cf37939b7d67b2d23", "title": "Accelerating Deep Learning with Dynamic Data Pruning"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "d9cdf21e73519edc593bdf1a00fcd778764b13f6", "title": "Training Neural Networks with Fixed Sparse Masks"}, {"paperId": "25d12dc62ae62812085384196dfe465c9cfbd6e0", "title": "Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters"}, {"paperId": "7a91b4f00b76f7908363406c0ba424a71c2f86ab", "title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "d18d01be0a0a40327e13c1c89aa547a5c73fe3e8", "title": "MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge"}, {"paperId": "66d735987a31d666a6459566ae026c40ab9a1c3a", "title": "The Efficiency Misnomer"}, {"paperId": "4d41c2fa74dd018e39ddb3cbbfead1b42615612c", "title": "Parameter Prediction for Unseen Deep Architectures"}, {"paperId": "5bcc379da187b69d705a81e93bf5ddbb90cda1b1", "title": "No One Representation to Rule Them All: Overlapping Features of Training Methods"}, {"paperId": "a25370452533bf47549243e97852b9cdf7a0ee0e", "title": "Learning the Transformer Kernel"}, {"paperId": "4c188c99cb75e065e025a74e87a68dd0f3ff0f84", "title": "Trade-offs of Local SGD at Scale: An Empirical Study"}, {"paperId": "a82ae40ecc5ea5e33b52c87c9464510cab7bf9d9", "title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "309037d7fa4bf15768b23ba77c959ceb8cd18c87", "title": "Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping"}, {"paperId": "2e8ddd75d725c5b5d862e37796ea9b527f99896f", "title": "Deep Learning's Diminishing Returns: The Cost of Improvement is Becoming Unsustainable"}, {"paperId": "f454f6b5f2ca9749ddf442eb5134612ef7f758c1", "title": "ResNet strikes back: An improved training procedure in timm"}, {"paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2", "title": "Primer: Searching for Efficient Transformers for Language Modeling"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "9aed8c6e3af9cd89b6b5cd4e42062d60125bea5a", "title": "Deep Learning Based on Fourier Convolutional Neural Network Incorporating Random Kernels"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "39bf40fec9bb8a68198471d86bbd8b5a763d47be", "title": "Dataset Distillation with Infinitely Wide Convolutional Networks"}, {"paperId": "256172390ae241d04a3016cf2a9822e9ba826f20", "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition"}, {"paperId": "a6e25ca9ee9d3e45c6d1957c0dc3324a9816c34e", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "1d4d10136c1e19e904c40c2a2a97f93130a26bff", "title": "One-Cycle Pruning: Pruning Convnets With Tight Training Budget"}, {"paperId": "71ca4c16fe6ba98bfa6a6a2b0b94151f35b809b4", "title": "KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks"}, {"paperId": "7ac3e74f927b01f2b54661b09d9014a4e3a77bf8", "title": "Physics-Guided Deep Learning for Dynamical Systems: A survey"}, {"paperId": "439d158e3ab3910d836535dd1aec693f5c0420cf", "title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity"}, {"paperId": "fb4dfeb0fed5d7abaab9231dfa7e7ffeb7b04c70", "title": "Training Data Subset Selection for Regression with Controlled Generalization Error"}, {"paperId": "219864e62494b7b4eb74beb88bf93cef3e4e2f3c", "title": "Lossy Compression for Lossless Prediction"}, {"paperId": "217d1b66d45adfe73c9324a6751890d651e99dd5", "title": "Multiplying Matrices Without Multiplying"}, {"paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe", "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"}, {"paperId": "e55249b7c00ca7e46c6586b9be7a6d68ad2d2951", "title": "Super-Acceleration with Cyclical Step-sizes"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "a93632237958800217341d7bad847200afdd60e3", "title": "Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better"}, {"paperId": "7bb118ed195e5d39ffbeab40dae2e5800eb33beb", "title": "A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "9257963d2b442c72eb8d17f7951da582b22d9990", "title": "RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "6bc4681828143f5ecc49b7ecd388a86c70c7237a", "title": "Efficient Lottery Ticket Finding: Less Data is More"}, {"paperId": "7b3495bd964876072efb23a0b04f688787423515", "title": "Image Coding For Machines: an End-To-End Learned Approach"}, {"paperId": "0cce7503b023656b3e3adec7894e2d39010a9b4a", "title": "LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes"}, {"paperId": "efcafa65a6e69fa52ceba41d7b6356c17c241edc", "title": "Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "63d8426ba1f51a8525dd19fd8ec92934ec71aea5", "title": "A Survey of Data Augmentation Approaches for NLP"}, {"paperId": "04a3f5f95c802cb0a55e3e5eca2875fafbc669dd", "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "8f8f73f0f208302546c825ed474432389ed63be4", "title": "EfficientNetV2: Smaller Models and Faster Training"}, {"paperId": "55635aac4cd439a00356f83dad52bd8d7b0ea87e", "title": "A Survey on Curriculum Learning"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "9efe8dbde586d6248ecfc69f08b918012e2ac478", "title": "Revisiting ResNets: Improved Training and Scaling Strategies"}, {"paperId": "73d5dbfebca74e5ef8a7333c842a2a9ee5c07fd6", "title": "Fast and Accurate Model Scaling"}, {"paperId": "d99ae9e262dbba2d0232cac12bb1a3bfadeabb53", "title": "On the Utility of Gradient Compression in Distributed Training Systems"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "49af5d407aa113239376d06a73c7bbdad25dd944", "title": "Provable Super-Convergence With a Large Cyclical Learning Rate"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "0ee7dbb58be11c3583e0be47f6c5e672ffb6a819", "title": "Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset"}, {"paperId": "c16835c8e535ebd9c10a550ca9455fe384a14449", "title": "High-Performance Large-Scale Image Recognition Without Normalization"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "b78d89fc34ef459f8d5e50af9a13686e14687719", "title": "Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization"}, {"paperId": "9d6acac70b2d1fdb861a08b00766ef263109cd7f", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "c4547ea56633ce91599ae5880163c6c276d6529c", "title": "Clairvoyant Prefetching for Distributed Machine Learning I/O"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "2b8088253e2378fce001a090fe923b81e8dedf25", "title": "RepVGG: Making VGG-style ConvNets Great Again"}, {"paperId": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96", "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets"}, {"paperId": "199ac333b1ff37917d1ab4a2e2002d9605b3db1e", "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning"}, {"paperId": "9d2c96574019305a8c86cc5b84cb9f616ccf0eb3", "title": "When Do Curricula Work?"}, {"paperId": "1536206f88a2c0d2bd2c1b1b87e805232e3358aa", "title": "Deconstructing the Structure of Sparse Neural Networks"}, {"paperId": "908ef4da472415ded4537b61d9f409d0f341156d", "title": "FreezeNet: Full Performance by Reduced Storage Costs"}, {"paperId": "52456607169bd4633a23a1f41b4c745544bd3bfe", "title": "ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "1ac1d3eb086c42d72a0509a42f744f626e8b5711", "title": "Teaching with Commentaries"}, {"paperId": "4a36eaf26bd5f3c4e2cf819c6c0537c52bd010f5", "title": "SC20: International Conference for High Performance Computing, Networking, Storage and Analysis"}, {"paperId": "7f2304c14ce896624fbdf4580bdad27e2f914333", "title": "Meta-Learning with Adaptive Hyperparameters"}, {"paperId": "8212605d274d5e68bcedf990728f4f5c26f88168", "title": "Dataset Meta-Learning from Kernel Ridge-Regression"}, {"paperId": "292839cb8e5c601be8cd467184939de7873fdd44", "title": "Chasing Carbon: The Elusive Environmental Footprint of Computing"}, {"paperId": "aac045fd44a6a29bb5a683c740582dc5665d13e6", "title": "\u03bcNAS: Constrained Neural Architecture Search for Microcontrollers"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "e585f6e752fb2668b33f7d4b18c8af9bd5abc1a4", "title": "The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research"}, {"paperId": "a2cd073b57be744533152202989228cb4122270a", "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "f8c87389d78fc059f18bfc9d04b37614c19c3f91", "title": "Normalization Techniques in Training DNNs: Methodology, Analysis and Application"}, {"paperId": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb", "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?"}, {"paperId": "e3ceaf1fbb353493be6b203d76bc00c9fd92fb37", "title": "'Less Than One'-Shot Learning: Learning N Classes From M"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "91cfd15b587c5ed604e7e49326db6d045276c2a5", "title": "The Computational Limits of Deep Learning"}, {"paperId": "0b72faba82f0b652a5a082b57872c1c8ecaf7e9a", "title": "Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "1d59fc5ce16a22a7c51fef5a02b961a5fa481a6c", "title": "Practical Quasi-Newton Methods for Training Deep Neural Networks"}, {"paperId": "9848db2098c0d3db6ee6f10a177402bc4ec67f83", "title": "A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications"}, {"paperId": "5a94bcc168330318d3020aa4d41bd73cf68ab285", "title": "Dataset Condensation with Gradient Matching"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "fd0e72b4f74ae0d880e6eebe697fdaaa3268a93e", "title": "Optimizing Neural Networks via Koopman Operator Theory"}, {"paperId": "0d5cb85a6825ce7cd48c9b2ce49f3c1dc0daf8e1", "title": "A Comprehensive Survey of Neural Architecture Search"}, {"paperId": "20438e2a38a0c4723fbd9de50b44b7335f6f43cb", "title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning"}, {"paperId": "4de08637d620ae781783a91b46f82ee8d9be405f", "title": "Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "7913730bd9a9f8d47f8359c8a0bfd23ad2b2da65", "title": "Measuring the Algorithmic Efficiency of Neural Networks"}, {"paperId": "5290d7921f0266c8b50b79fc8a0b7d22868f4f60", "title": "The Cost of Training NLP Models: A Concise Overview"}, {"paperId": "f7e43eaf7d7b3cf10d868ed46d5d9390adbb239c", "title": "Gradient Centralization: A New Optimization Technique for Deep Neural Networks"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "de66ada65cd9d36e46f1f8dd2c8be480180038ec", "title": "What is the State of Neural Network Pruning?"}, {"paperId": "13ddbb187a4a52c9b8109786b6add0a732a7a792", "title": "Training Machine Learning on JPEG Compressed Images"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "8771679aac0e90371340bd8c657317f5be113e81", "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "57475eef962f45f6b1c249b590b4f894202f8681", "title": "The Early Phase of Neural Network Training"}, {"paperId": "90d7ee9cfb44cf1e15bd92c5664156ab9b3f7334", "title": "Second Order Optimization Made Practical"}, {"paperId": "74b4f16c5ac91e3e7c88ae81cc8c91416b71d151", "title": "Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"}, {"paperId": "c13e85e9c2692b3ddb0cbd42416062dab8bcab18", "title": "Gradient descent with momentum - to accelerate or to super-accelerate?"}, {"paperId": "1c36fb88e2e663d18790357425f646ad5ac7dac3", "title": "Approximating Activation Functions"}, {"paperId": "6cce1d4f64f0fdc4a69e4a108e596c555879083b", "title": "Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks"}, {"paperId": "468c1aee8894abfb237d8cd504e29727952b83b7", "title": "A Framework for Democratizing AI"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "fd431005d26100f5453590080683cbae9dc1189f", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "9f73c3f86026c21d0e5e55c70462952c6ada1175", "title": "Accelerating Deep Learning by Focusing on the Biggest Losers"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "336868be817536e7c7fc88c391a2860cd869ea2b", "title": "Drawing early-bird tickets: Towards more efficient training of deep networks"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "e5fe3ea87e21ff71b77e60c0600f419c255d24d3", "title": "Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency"}, {"paperId": "3e9a40a567c4a95b591530ff5771296b478a0f0c", "title": "Machine Learning at the Network Edge: A Survey"}, {"paperId": "a72134493b29473d638c000a05be1960f85e7e96", "title": "Faster Neural Network Training with Data Echoing"}, {"paperId": "3813b88a4ec3c63919df47e9694b577f4691f7e5", "title": "A survey on Image Data Augmentation for Deep Learning"}, {"paperId": "cc7c4864f6a9bb7f5489aa17546c1b0b3de59993", "title": "MACHINE LEARNING APPLICATIONS IN OCEANOGRAPHY"}, {"paperId": "b06d260b504a20e90d07180aa3c4eaecb3b5cde5", "title": "Selection Via Proxy: Efficient Data Selection For Deep Learning"}, {"paperId": "c0aaee2337e5af680e5dca1bfc349a737dfec573", "title": "Fixing the train-test resolution discrepancy"}, {"paperId": "339e2610de8487ccb54af05cb59b63854d25f02d", "title": "Meta Learning via Learned Loss"}, {"paperId": "790985a4bee821046992ff3d5322ff11dd1b4262", "title": "Coresets for Data-efficient Training of Machine Learning Models"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "131a2b1c2fb51d8c7eae9a359425664361ce71bd", "title": "Training Data Subset Search With Ensemble Active Learning"}, {"paperId": "1469ef5dbf41bc3e666a36def00c693d6370c730", "title": "Improved Training Speed, Accuracy, and Data Utilization Through Loss Function Optimization"}, {"paperId": "678daf80a33b8ca4c1789e62bf07e5454d1b27bc", "title": "Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness"}, {"paperId": "788ae69e1afacf9bbc0cafcbcac40e67e289129f", "title": "Band-limited Training and Inference for Convolutional Neural Networks"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "f79bfe86ced096597e6a8b4a88ca12f4b53be115", "title": "SWALP : Stochastic Weight Averaging in Low-Precision Training"}, {"paperId": "0d9eb72ea89bb7bf5720a7b2c2f3f77c26fc67a6", "title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks With Octave Convolution"}, {"paperId": "ba013acca784bfea74241c879e18b72984950493", "title": "On The Power of Curriculum Learning in Training Deep Networks"}, {"paperId": "2b49156cf855dbb39768ae0ba7d7cb9263d17e5c", "title": "Informed Machine Learning \u2013 A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems"}, {"paperId": "0af0fedaa1fee7293fcdc1abe180ba6fb37c5a4b", "title": "Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need"}, {"paperId": "39d0b5e6340717273cb1f169d4f16b8cf7fbff94", "title": "Augment your batch: better training with larger batches"}, {"paperId": "8b2c6ea7bbc5b616548dd07b46c492a32b6472d1", "title": "PruneTrain: fast neural network training by dynamic sparse model reconfiguration"}, {"paperId": "cd545daa24d4a05db40c12efcd4114dfa76f205b", "title": "Learning From Less Data: A Unified Data Subset Selection and Active Learning Framework for Computer Vision"}, {"paperId": "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "title": "An Empirical Model of Large-Batch Training"}, {"paperId": "f9120540af08f83f75dd3710c19d54606690ddbd", "title": "Pre-Defined Sparse Neural Networks With Hardware Acceleration"}, {"paperId": "ab561713a71da567d315c09da693060e32ee3470", "title": "Learning to Teach with Dynamic Loss Functions"}, {"paperId": "cb4147fbd0704398c692667078efff935a36bb6d", "title": "Understanding and correcting pathologies in the training of learned optimizers"}, {"paperId": "a2b5d224895d96bfe2e384e2dcf1ebd136ac3782", "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning"}, {"paperId": "4a1004ecd34118116344633c7cdcc34493c423ee", "title": "Rethinking the Value of Network Pruning"}, {"paperId": "ba6985ee462ed8b09385a924aded5a45f74e7a59", "title": "Dynamic Sparse Graph for Efficient Deep Learning"}, {"paperId": "7d3e40e12cc34199dab502ccd2f13b1dfc2f6404", "title": "Clustering Convolutional Kernels to Compress Deep Neural Networks"}, {"paperId": "93ef5b740fa1b54929ead6eb177e0698d7f19719", "title": "Don't Use Large Mini-Batches, Use Local SGD"}, {"paperId": "55be861931d147d8bf91abcfe73af6a151283955", "title": "Constructing Fast Network through Deconstruction of Convolution"}, {"paperId": "7cfa76a82be96c74b2eff514265b7fd271a179cd", "title": "Local SGD Converges Fast and Communicates Little"}, {"paperId": "f723eb3e7159f07b97464c8d947d15e78612abe4", "title": "AutoAugment: Learning Augmentation Policies from Data"}, {"paperId": "f603b1cc0d3f5297c54003709296a973eb72b20f", "title": "Faster Neural Network Training with Approximate Tensor Operations"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "b8989afff14fb630ca58b6afa917fb42574228ee", "title": "Averaging Weights Leads to Wider Optima and Better Generalization"}, {"paperId": "d23a1cd6c73e3e43295c3585b3db147eb1c3ee91", "title": "How to Start Training: The Effect of Initialization and Architecture"}, {"paperId": "6c06ee9811fc2af7877477a4e164cda574cd2b3b", "title": "Slow and Stale Gradients Can Win the Race"}, {"paperId": "453f7610c6e66bfafcb989d7a9a08e889559f041", "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling"}, {"paperId": "d8c09661b1bebfb690f0566167c87d64c5628d73", "title": "Demystifying Parallel and Distributed Deep Learning"}, {"paperId": "0679950558d72791f16031dd08c39367d8dd47b8", "title": "Shampoo: Preconditioned Stochastic Tensor Optimization"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "c2862da569a775900852d8e1900f34ae0b7a5ae4", "title": "BLOCK-NORMALIZED GRADIENT METHOD: AN EMPIRICAL STUDY FOR TRAINING DEEP NEURAL NETWORK"}, {"paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54", "title": "signSGD: compressed optimisation for non-convex problems"}, {"paperId": "d40faad69f08c3d17d67306656172a9c3c888a7f", "title": "Faster Neural Networks Straight from JPEG"}, {"paperId": "c0877abb93cbe0b28e98929591ec57e7ed213fef", "title": "Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "325093f2c5b33d7507c10aa422e96aa5b10a33f1", "title": "In-place Activated BatchNorm for Memory-Optimized Training of DNNs"}, {"paperId": "3124a3a6fed01e2ce200defb23c8ca1f5f78d6ce", "title": "Critical Learning Periods in Deep Neural Networks"}, {"paperId": "70c810ba62c5ee40d611e134b2ac2ca61c4de16b", "title": "Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "3299aee7a354877e43339d06abb967af2be8b872", "title": "Don't Decay the Learning Rate, Increase the Batch Size"}, {"paperId": "f152cfd441a52c9ceb2ae724d601fb4fb9ec77ea", "title": "Regularization for Deep Learning: A Taxonomy"}, {"paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a", "title": "Automatic differentiation in PyTorch"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "9e21d177a7dcfa4acfb674b93103b3d12bbb5b32", "title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates"}, {"paperId": "90a16f34d109b63d95ab4da2d491cbe3a1c8b656", "title": "Learning Efficient Convolutional Networks through Network Slimming"}, {"paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "title": "Improved Regularization of Convolutional Neural Networks with Cutout"}, {"paperId": "1e3d18beaf3921f561e1b999780f29f2b23f3b7d", "title": "Large Batch Training of Convolutional Networks"}, {"paperId": "6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"}, {"paperId": "a3f7a30abe44424e5ef8348a02cc103237ac5210", "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability"}, {"paperId": "799dd570e04bc96d0d6ed51497d9dec6bfaa886e", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory"}, {"paperId": "a7484ac305e3746c9e612d558345d9664bd436de", "title": "FreezeOut: Accelerate Training by Progressively Freezing Layers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "5288a4b55f6b8e059aed55c0b9b2ab5ae5ec3a98", "title": "Gabor filter assisted energy efficient fast learning Convolutional Neural Networks"}, {"paperId": "9bc3eaa4f816d186b9deac9ba980f278f85a1cd8", "title": "Introspection: Accelerating Neural Network Training By Learning Weight Evolution"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "9d1fb89b99aafc01ee56fc92df1ad150fba67c22", "title": "On orthogonality and learning recurrent networks with long term dependencies"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c", "title": "Densely Connected Convolutional Networks"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "759956bb98689dbcc891528636d8994e54318f85", "title": "Strategies for Training Large Vocabulary Neural Language Models"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "a546966db50cc8fdef98bb744990f35248932930", "title": "Adding Gradient Noise Improves Learning for Very Deep Networks"}, {"paperId": "751c8884c1e857e675d85d8594c5f9b608005ed5", "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification"}, {"paperId": "2f48296c526de31553887875cc433768ff5b19b9", "title": "Online Batch Selection for Faster Training of Neural Networks"}, {"paperId": "54c3e878bf0ff2fdde16e439b5579ee99ee0d0d8", "title": "ACDC: A Structured Efficient Linear Layer"}, {"paperId": "16cb6876666f3a7b56a636c1d85ad00bd0d98bf3", "title": "Net2Net: Accelerating Learning via Knowledge Transfer"}, {"paperId": "510a6ec82a2af8cdee185418c3296cb8e2b9716c", "title": "8-Bit Approximations for Parallelism in Deep Learning"}, {"paperId": "afdb6b7a083e3b7304c354925a3f3528716cb44e", "title": "Submodularity in Data Subset Selection and Active Learning"}, {"paperId": "941e30afcae061a115301c65a1afe49d8856f14e", "title": "Natural Neural Networks"}, {"paperId": "37b5dfe87d82ba8f310155165d5bf841dc92dea2", "title": "Cyclical Learning Rates for Training Neural Networks"}, {"paperId": "b89d7f7439cab841934a1ede06bf6b1f593c754f", "title": "Accelerating Very Deep Convolutional Networks for Classification and Detection"}, {"paperId": "cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "27a99c21a1324f087b2f144adc119f04137dfd87", "title": "Deep Fried Convnets"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "7995ff63046f24a65bd6e1373cba7364e3e37c0c", "title": "Submodularity for Data Selection in Machine Translation"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "04f16203f1e66e8d2151dc359fd0405a0f482da7", "title": "Mean-normalized stochastic gradient for large-scale deep learning"}, {"paperId": "6312e1457ffe687f7a0d841d6a6cdc2ca572707e", "title": "Unsupervised submodular subset selection for speech data"}, {"paperId": "3f125860f6dbc55cb72b05b11e26da35c26b1434", "title": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "d1f8d9e5934e8e9c4654afe36bbddeb52a07278a", "title": "Using Document Summarization Techniques for Speech Data Subset Selection"}, {"paperId": "5cea23330c76994cb626df20bed31cc2588033df", "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e", "title": "On the difficulty of training recurrent neural networks"}, {"paperId": "2845609e2026bd7374b09b04395fd9b7289297b8", "title": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning"}, {"paperId": "7fa7057c8e560083af61218637c6f67df9c3fba7", "title": "A case for redundant arrays of inexpensive disks (RAID)"}, {"paperId": "2fa1c532b5db1cbbab7ee42015993bc643f9e8e1", "title": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2022, New Orleans, LA, USA, June 19-20, 2022"}, {"paperId": "aa2c44ff32c5055657d1873acad36212e68416ab", "title": "Questions for Flat-Minima Optimization of Modern Neural Networks"}, {"paperId": "8623348f1d39c50a5c1d05a3b8b723f9d7233b15", "title": "ORIENT: Submodular Mutual Information Measures for Data Subset Selection under Distribution Shift"}, {"paperId": "afd60ef58e012023903d2815273f21ce4e49a506", "title": "Learning by Directional Gradient Descent"}, {"paperId": null, "title": "2022a,b]. These limitations are holding us back from exploiting our advanced sensing capabilities to their full potential and, in turn, achieving game-changing societal advances"}, {"paperId": null, "title": "Blazingly Fast Computer Vision Training with the Mosaic ResNet and Composer"}, {"paperId": null, "title": "Seyed Kamyar Seyed Ghasemipour"}, {"paperId": null, "title": "Lacoste et al. 2019] and [Henderson et al. 2020] proposed tools to estimate the energy and carbon"}, {"paperId": null, "title": "Sharpnessaware training for free"}, {"paperId": null, "title": "AI and Compute, How Much Longer Can Computing Power Drive Artificial Intelligence Progress?"}, {"paperId": "e4f7d8d9e09893327189e562a9d4f05b922a16b7", "title": "How Does Loss Function Affect Generalization Performance of Deep Learning? Application to Human Age Estimation"}, {"paperId": null, "title": "2021d. Actnn: Reducing training memory"}, {"paperId": "502667269ea02bd3c35c47ad7c8674c9d8b8b5b0", "title": "AC-GC: Lossy Activation Compression with Guaranteed Convergence"}, {"paperId": "2a358c088e7006f3568240211d5879d6c8039d72", "title": "GRAD-MATCH: Gradient Matching based Data Subset Selection for Ef\ufb01cient Deep Model Training"}, {"paperId": "bbbf586f4ed5896801e13af858b895054abc5c90", "title": "PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836"}, {"paperId": null, "title": "2021]. Such resource-limited applications are limitless and can be categorized under the umbrella term Edge-AI [Murshed et al"}, {"paperId": null, "title": "The Mosaic ML Team"}, {"paperId": null, "title": "Minsik Cho, Vinod Muthusamy, Brad Nemanich, and Ruchir Puri"}, {"paperId": null, "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities"}, {"paperId": null, "title": "Efficient pytorch: Tensor memory format matters"}, {"paperId": null, "title": "Review of Parameter Counts in Machine Learning. Published: Alignment Forum (blog) (2021)"}, {"paperId": "8f499e296ccf520b8e1b9d91478dffe24b610abb", "title": "A Statistical Mechanics Framework for Task-Agnostic Sample Design in Machine Learning"}, {"paperId": null, "title": "DeepSpeed: Extreme-scale model training for everyone"}, {"paperId": null, "title": "arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006"}, {"paperId": "3303aa49fdbca958792ddbecebdfa75dc9297f2c", "title": "Reservoir Transformer"}, {"paperId": "a9278b95d8bd1ef3833424611a85856a3ff62bc7", "title": "Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation"}, {"paperId": null, "title": "2020]. Notably, current large-scale models use"}, {"paperId": null, "title": "trade-offs in distributed SGD"}, {"paperId": null, "title": "2019b. Mix & match: training convnets with mixed image"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "72bf9a21d336881697a2c8dbe5537e97737e0f13", "title": "GradZip: Gradient Compression using Alternating Matrix Factorization for Large-scale Deep Learning"}, {"paperId": null, "title": "Efficient data selection for deep learning. arXiv preprint arXiv:1906.11829"}, {"paperId": null, "title": "IPCC says limiting global warming to 1.5 [degrees] C will require drastic action"}, {"paperId": null, "title": "FastAI -progressive resizing"}, {"paperId": null, "title": "A Efros. Dataset distillation"}, {"paperId": "b245959da6bdaa0b711341844aeaa473b7706453", "title": "DAWNBench : An End-to-End Deep Learning Benchmark and Competition"}, {"paperId": null, "title": "Using compression to speed up image classification in artificial neural networks"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks"}, {"paperId": "f8b1b43f284f1246ca015cc002ac949bb67c5645", "title": "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures"}, {"paperId": "aaab8f1869da451a1c8ecb016e16b03b48bc2e2e", "title": "Conference on Computer Vision and Pattern Recognition"}, {"paperId": "c19fbefdeead6a4154a22a9c8551a18b1530033a", "title": "Hierarchical Probabilistic Neural Network Language Model"}, {"paperId": "61bca7664ef7ef336f75ccff1efd211c5863db73", "title": "IEEE International Conference on Acoustics Speech and Signal Processing"}, {"paperId": "9634af346c2356a8d40cbb939c49a7a7a9ba62fc", "title": "Cramming More Components Onto Integrated Circuits"}, {"paperId": "d92bf274d0a28378c3638d18415be4756ffa1c52", "title": "A solvable connectionist model of immediate recall of ordered lists"}, {"paperId": "f4ea5a6ff3ffcd11ec2e6ed7828a7d41279fb3ad", "title": "Comparing Biases for Minimal Network Construction with Back-Propagation"}, {"paperId": null, "title": "A method of solving a convex programming problem with convergence rate o(k\u02c62)"}, {"paperId": null, "title": "1965] suggests one might expect a corresponding doubling in application performance"}, {"paperId": "4b53e3f719ff983eef867c6d8deac5dbe38aecb4", "title": "Some methods of speeding up the convergence of iteration methods"}, {"paperId": null, "title": "Training giant neural networks using weight streaming on cerebras wafer-scale systems"}]}