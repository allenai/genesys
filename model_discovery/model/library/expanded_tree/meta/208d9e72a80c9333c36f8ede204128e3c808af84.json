{"paperId": "208d9e72a80c9333c36f8ede204128e3c808af84", "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This approach involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions, and significantly outperforms all state-of-the-art baselines."}, "embedding": {"model": "specter_v2", "vector": [-0.004146929364651442, 0.08328988403081894, -0.28459712862968445, -0.31334224343299866, -0.4629063904285431, -0.4916086792945862, 0.602627694606781, -0.1402496099472046, -0.7001794576644897, -0.05531654506921768, 0.6271395683288574, -0.4739276170730591, 0.6505241990089417, 0.39847612380981445, -0.16384364664554596, 0.4210908114910126, -0.5638301968574524, 0.4945283532142639, -0.24651026725769043, -0.6100754141807556, -0.7183180451393127, -0.8816398978233337, -0.5328408479690552, 0.3474191129207611, 0.664513885974884, 0.5211847424507141, 0.20261426270008087, 1.2408931255340576, -0.426691472530365, -0.2711752951145172, 0.5161677598953247, -0.8019165396690369, 0.34851163625717163, -0.17657674849033356, -0.2034834921360016, -0.23305991291999817, 0.19414015114307404, -0.6656405329704285, 0.16296140849590302, 1.060973882675171, -0.163519486784935, -0.14845071732997894, 0.5783188939094543, -0.7148892879486084, -0.46946361660957336, 1.3202171325683594, 0.4993200898170471, 0.8262204527854919, -0.2425905019044876, -0.6043299436569214, 1.3512080907821655, -1.4561938047409058, 0.27260729670524597, 1.851919174194336, 0.7757823467254639, 0.744011640548706, -0.39691290259361267, -0.9956344366073608, 0.7593702673912048, 0.039223410189151764, -0.7505991458892822, -0.3404120206832886, -0.23886461555957794, 0.013286740519106388, 1.9304234981536865, -0.8017104864120483, -0.14228418469429016, 0.7239410877227783, -0.24285465478897095, 1.4281290769577026, -0.010506050661206245, -0.8154273629188538, -0.47537630796432495, 0.10342871397733688, 0.14883998036384583, 0.7107037901878357, -0.2649807035923004, 0.39884069561958313, -0.8968227505683899, -0.1439191997051239, 0.32772183418273926, -0.483865350484848, -0.30240362882614136, 0.40207234025001526, -0.2806045114994049, 0.8472748398780823, 0.4170944392681122, 0.7513290643692017, -0.07408839464187622, 0.10395533591508865, 0.6597435474395752, 0.5057205557823181, 0.20655599236488342, 0.1450294852256775, -0.6148690581321716, 0.7276874780654907, -0.9614989757537842, 0.09982514381408691, 0.5587697625160217, 0.7383450865745544, -0.13376539945602417, -0.18509246408939362, -0.3153803050518036, 0.3872954547405243, 1.4196377992630005, -0.08151984214782715, 0.3245963752269745, -0.5338046550750732, 0.5482869744300842, -0.4763461947441101, 0.011646074242889881, -0.4483891725540161, -0.030681187286973, -0.08374901860952377, -0.8156613707542419, -1.2384517192840576, -0.4123178720474243, -0.20489123463630676, -0.7558076977729797, 1.1442629098892212, -0.29206526279449463, 0.18634532392024994, 0.8185394406318665, 0.27715209126472473, 0.4288944602012634, 0.6968759298324585, 0.607587456703186, -0.13466739654541016, 0.9769338965415955, -0.836405336856842, -0.518966019153595, -1.0980138778686523, 0.7072766423225403, -0.49644550681114197, 0.13412614166736603, -0.27969592809677124, -1.0278078317642212, -1.1142909526824951, -1.030626893043518, -0.014693573117256165, -0.6376662254333496, 0.5307125449180603, 0.75409334897995, 0.36419999599456787, -0.8966202139854431, 0.5443562269210815, 0.0718783289194107, -0.2546240985393524, 0.0841425210237503, -0.11043485999107361, -0.12768621742725372, -0.7207580208778381, -1.6387991905212402, 0.6088741421699524, 0.8840681910514832, -0.046016886830329895, -0.1638806015253067, -0.8336424231529236, -1.0378141403198242, -0.44447845220565796, 0.1522504836320877, -0.713421642780304, 1.394919991493225, -0.198072150349617, -1.0737786293029785, 0.7469184994697571, -0.5215477347373962, 0.15594197809696198, 0.5758385062217712, -0.39836734533309937, -0.8890065550804138, -0.851640522480011, 0.12635651230812073, 0.7784979939460754, 0.027558708563447, -0.11659952253103256, -0.5052820444107056, 0.16086597740650177, -0.20942306518554688, 0.08074353635311127, -0.1105884537100792, 0.747218668460846, -0.49791792035102844, -0.10017639398574829, 0.46057194471359253, 0.7651285529136658, 0.0540618859231472, -0.27730581164360046, -0.5043490529060364, -1.2972447872161865, 0.9558196663856506, -0.3490012586116791, 0.5951048135757446, -0.5991625785827637, -0.4430831968784332, -0.1925657093524933, -0.42348435521125793, -0.019182177260518074, -0.7657748460769653, 0.7051858305931091, -0.1658959835767746, 0.631138801574707, -0.34494471549987793, -1.5056089162826538, -0.20517276227474213, -0.05019918084144592, -0.7114993333816528, -0.5765255689620972, 0.5364459753036499, 1.09343683719635, -0.8658583760261536, 0.08154010027647018, -0.18039339780807495, 0.4632369577884674, -1.0246214866638184, 1.0729198455810547, -0.6091213226318359, 0.4128947854042053, 0.11521467566490173, -0.2163054347038269, 0.21369630098342896, -0.11190494894981384, 0.5450040698051453, -0.919708251953125, -0.24629643559455872, 0.26601654291152954, -0.32348790764808655, 1.5717179775238037, -0.6144848465919495, 0.26365193724632263, 0.023755565285682678, -0.4619351923465729, -0.08641514182090759, 0.5550855994224548, -0.7360676527023315, -0.10734742879867554, 0.7095460891723633, 0.8073775768280029, -0.4021680951118469, 0.5710178017616272, 0.4418456554412842, 0.5614587068557739, -0.5068891644477844, 0.2307368367910385, 0.7705026865005493, -0.5072222948074341, 0.638113260269165, 0.31939855217933655, 0.3981250524520874, 0.3970317244529724, 0.5164979696273804, -0.28683608770370483, 0.9821026921272278, -0.6090352535247803, -0.16065381467342377, 0.22458529472351074, 0.5586628913879395, 0.2932411730289459, 0.3079270124435425, -0.4558170437812805, -0.3216148316860199, -0.009168842807412148, 0.6577105522155762, 1.5570098161697388, -0.2791500985622406, 0.15801653265953064, -0.9143918752670288, -0.5546890497207642, -0.15590062737464905, 0.21948537230491638, -0.3549400568008423, 0.10753389447927475, -0.2639693021774292, -0.9162240624427795, 0.8988476991653442, 0.4403896927833557, 1.1806433200836182, -0.3098171353340149, 0.43387171626091003, 0.04891565814614296, -0.31081172823905945, -1.2489122152328491, -0.5944373607635498, 0.1609588861465454, -0.39777958393096924, -0.31700828671455383, 0.35548853874206543, 0.005127739626914263, 0.1738377809524536, -0.8323467373847961, 1.148880958557129, -0.5369028449058533, -0.15445256233215332, -0.02263895608484745, 0.54548579454422, -0.6905099153518677, -1.4165821075439453, 0.08615612238645554, 0.5456302762031555, -0.28534382581710815, 0.6960902810096741, 0.9505253434181213, 0.30569297075271606, 0.3184456527233124, -0.38477423787117004, 0.35513925552368164, 0.04035390168428421, 0.11815131455659866, 0.9710071682929993, -0.21734748780727386, 0.32719871401786804, -1.3209813833236694, 1.146324872970581, 0.15955382585525513, -0.6921293139457703, 0.7899734377861023, -0.43888503313064575, -0.3762304484844208, 0.5828593969345093, -0.4631718099117279, -0.6967023611068726, -0.9740756154060364, 0.15042968094348907, -0.17248372733592987, -0.26998481154441833, 0.5350204110145569, 0.01144685409963131, 0.762533962726593, 0.3413587212562561, 0.16584910452365875, 0.2508934438228607, -0.31531500816345215, 0.795943021774292, -0.790216326713562, 0.8106831908226013, 0.35674068331718445, -0.07168981432914734, -0.2641676962375641, -0.7974751591682434, -0.5806193947792053, -0.8063584566116333, -0.4698559641838074, -0.6555007100105286, -0.09422462433576584, 0.4959662854671478, -0.7331379055976868, -0.3880771994590759, 0.13335484266281128, -1.5394936800003052, 0.09072557091712952, 0.839780867099762, 0.1143740713596344, 0.2004132717847824, -1.3967671394348145, -1.0657018423080444, -0.2446603775024414, -0.7859519124031067, -0.8425145149230957, 0.436381071805954, 0.1922667920589447, -0.3836097717285156, -0.8894684910774231, 0.12499280273914337, -0.0516967810690403, 0.93780118227005, -0.8017882108688354, 1.009192943572998, 0.07363862544298172, 0.15910308063030243, -0.361775279045105, -0.16524790227413177, 1.0515515804290771, -0.21448732912540436, 0.43213939666748047, -1.144095540046692, 0.20518985390663147, -0.3517417311668396, -0.5650952458381653, 0.18932466208934784, 0.24819321930408478, 0.1905672550201416, 0.4177863299846649, -0.6272324919700623, 0.3190333843231201, 1.5297043323516846, -1.1100544929504395, -0.5194465517997742, -0.296485960483551, 0.7369701862335205, 0.3656989336013794, -0.4873347878456116, -0.13743677735328674, 0.4997343122959137, -0.06303412467241287, -0.08997092396020889, -0.221489816904068, -0.0010487500112503767, -0.7562445998191833, 0.5341466665267944, 1.8089468479156494, 0.13694339990615845, -0.12617017328739166, -1.1588624715805054, 0.5787902474403381, -1.3044970035552979, -0.14451543986797333, 0.6741665601730347, 0.48264703154563904, 0.6820825934410095, -0.31924715638160706, -0.6796231269836426, -0.37001949548721313, 0.6982133984565735, -0.0393279492855072, -0.8331146836280823, -0.6980427503585815, -0.1903316229581833, -0.016361186280846596, -0.04818923771381378, 0.7068364024162292, -0.5309168100357056, 1.0695960521697998, 14.176722526550293, 0.7178621292114258, 0.2053767591714859, 0.9204954504966736, 0.7664802670478821, 0.6237276792526245, -0.6161504983901978, -0.16572967171669006, -1.920714020729065, -0.7075517773628235, 1.0240243673324585, 0.22816826403141022, 0.4057268798351288, 0.11479704082012177, 0.1922616809606552, 0.20246459543704987, -0.6510790586471558, 0.6669847369194031, 0.568210780620575, -1.1655266284942627, 0.7144330739974976, -0.10904493927955627, 0.7353474497795105, 1.0036553144454956, 0.24748805165290833, 0.9728389978408813, 0.8434433341026306, -0.37162142992019653, 0.2768137454986572, -0.16688193380832672, 0.45432838797569275, 0.09765969961881638, 0.8765377998352051, 0.5900720357894897, -0.818430483341217, -0.10994383692741394, -0.6863555908203125, -0.7983874082565308, 0.599601149559021, 0.19433905184268951, -0.6652460694313049, -0.39302289485931396, -0.03385916352272034, 0.8551384210586548, 0.28516891598701477, -0.22751694917678833, -0.7622790336608887, 0.6528974175453186, -0.19214299321174622, 0.3790881037712097, 0.2988246977329254, 0.4375953674316406, 0.13244058191776276, 0.17436617612838745, -0.09055480360984802, -0.10412733256816864, 0.1748117059469223, 0.6320351958274841, -0.8761136531829834, 0.012252388522028923, -0.21228843927383423, -0.17784474790096283, -0.23476625978946686, 0.6250535249710083, 0.39283156394958496, 0.07280294597148895, -0.593476414680481, 0.5122408866882324, 0.7159392833709717, 0.13848857581615448, -0.22949330508708954, 0.6106706261634827, 0.38873833417892456, -0.8377458453178406, -0.022572996094822884, 0.4613945484161377, 0.08351512253284454, -0.6438955068588257, -0.6240363717079163, -0.3287684917449951, 0.4637592136859894, -0.7568163871765137, -1.407476782798767, 1.0765899419784546, -0.18232981860637665, 0.07982279360294342, -0.2553454637527466, -0.924372136592865, -0.31259632110595703, 0.8248221278190613, -1.7870404720306396, -1.1620895862579346, 0.5992981791496277, 0.01456950418651104, -0.16920796036720276, -0.24729089438915253, 1.522369623184204, 0.07961665838956833, -0.48279663920402527, -0.02875705249607563, -0.0001661270798649639, 0.29875442385673523, -0.13477519154548645, -0.484244704246521, 1.021884799003601, 0.458817720413208, -0.23442286252975464, 0.20396733283996582, -0.22924228012561798, -0.24316705763339996, -0.5002378225326538, -0.13889944553375244, 0.9330300688743591, -0.8178045749664307, -0.2974761724472046, -0.5839816331863403, -0.9661032557487488, 0.18158544600009918, 0.6116877198219299, -0.36138761043548584, 0.663914144039154, 0.38381466269493103, -1.1162033081054688, 0.09894053637981415, -0.884956419467926, 0.1344621628522873, 0.49986234307289124, -0.8735148906707764, -0.21585769951343536, 0.02115408144891262, 0.4010520875453949, -0.8783596158027649, -0.6952599883079529, -0.00829354953020811, 0.20975156128406525, 3.530170215526596e-05, 0.6738569736480713, -0.5636200904846191, 0.5001946091651917, 0.7054263353347778, -0.3292250335216522, -0.9880242347717285, 0.4041770398616791, -0.5559413433074951, 0.3887788951396942, -0.0007150028832256794, 1.3310636281967163, -0.2749142348766327, -0.39704015851020813, 0.7276220321655273, 0.3512641489505768, -0.19264289736747742, -0.6304444074630737, -0.17841200530529022, 0.27831417322158813, -0.698706328868866, 0.24432967603206635, 0.12069600820541382, -0.33154499530792236, 0.22890524566173553, 0.47647348046302795, 1.1933393478393555, -0.18035593628883362, -0.7873706817626953, 0.3407761752605438, -0.2398790419101715, -0.20454736053943634, -0.5186694264411926, -0.2661007046699524, -1.3596795797348022, 0.38841521739959717, -1.0117807388305664, -0.01297886110842228, -1.1209180355072021, -0.557208776473999, 0.138191357254982, -0.17155319452285767, 0.2543565630912781, 0.11054133623838425, -0.21225713193416595, -0.735731840133667, -0.2794562876224518, -0.02041497454047203, 0.4705568850040436, 0.8285032510757446, -0.6117125749588013, 0.009024608880281448, 0.03758072853088379, -0.04256811365485191, 0.5986844301223755, 0.5215091109275818, 0.05123580992221832, -0.8426744341850281, -1.8821485042572021, 0.37650907039642334, -0.18597541749477386, -0.04031644016504288, -0.40253764390945435, 0.7831400036811829, 0.7745442986488342, -0.1525738537311554, -0.14347228407859802, 0.3112037181854248, -0.9222151637077332, -0.6205724477767944, 0.4555399417877197, -0.4611876308917999, 0.2887989282608032, 0.5519316792488098, -0.384331077337265, -0.6270217895507812, 0.8297659158706665, -0.08145096153020859, -1.0581916570663452, -1.0682951211929321, 0.39198675751686096, -0.23886708915233612, 0.0692230612039566, -0.3807273805141449, 0.09848754853010178, -0.9491110444068909, -0.31599071621894836, -0.040954411029815674, 0.6869399547576904, -0.7153893113136292, 1.167537808418274, 0.2563340961933136, -0.8585094213485718, -0.5884853601455688, 0.7304325699806213, 0.14680320024490356, -0.007712777238339186, 0.5089390277862549, 0.24168090522289276, -0.19718126952648163, 0.5313071012496948, 0.5459489822387695, -0.19817782938480377, -0.8694707751274109, -0.5085956454277039, 1.102725863456726, -0.4149015545845032, 0.06539899855852127, 1.480800747871399, -0.3637872636318207, -1.748110055923462, -0.19157400727272034, -1.134696125984192, -0.5385981798171997, -0.42053428292274475, 0.7706122994422913, 0.013179787434637547, 0.2059723436832428, -0.21643386781215668, -0.2774912714958191, 0.3463466465473175, -0.30698922276496887, -0.7366613745689392, 0.4664710760116577, -0.24284397065639496, -0.5504422783851624, 0.4767359495162964, 0.6789049506187439, -0.866243839263916, -0.6428734660148621, -0.8843538761138916, -0.41929706931114197, 0.19637010991573334, 0.599289059638977, -0.906711220741272, -0.5584421753883362, 0.6325394511222839, 0.3524932265281677, 0.1548878252506256, 0.15158097445964813, -0.10729985684156418, 0.07275025546550751, 0.7248028516769409, 0.011942161247134209, -0.6419323086738586, -1.0114628076553345, 1.3937149047851562, 1.388939619064331, -1.692856788635254, -0.16397571563720703, -0.4272998869419098, -0.860651969909668, 0.8886815309524536, 0.4031353294849396, 0.31595858931541443, 1.3592625856399536, -0.33734211325645447, 0.5707867741584778, 0.42070597410202026, -0.9340578317642212, -0.12787696719169617, 0.743003249168396, 0.8676294088363647, 0.8237971663475037, 0.48272567987442017, 0.10262802988290787, 0.7860106825828552, -0.051342692226171494, 0.12645985186100006, 0.2289896309375763, 0.04560830816626549, -0.03696146607398987, -0.3096573054790497, 0.23706164956092834, 0.33126556873321533, -0.45527413487434387, -0.8638325333595276, -0.0653093233704567, 0.6983866095542908, 0.5373585820198059, 0.9875861406326294, 0.5364152789115906, 0.3014003038406372, 0.7199694514274597, 0.33538249135017395, 0.26623672246932983, -0.5120256543159485, -0.05918838456273079, -0.059607598930597305, -0.6312164068222046, -0.0029402144718915224, -0.667681097984314, -0.31351375579833984, -0.3280475437641144, -0.27538955211639404, 0.05801810324192047, 0.0762002244591713, 0.8884419202804565, 1.5058982372283936, 0.42069682478904724, 0.2045024186372757, -0.6523579955101013, -0.12319085747003555, -0.4304262101650238, -1.2899110317230225, 0.31925976276397705, -0.7978441715240479, -0.3134426772594452, 0.15279293060302734, -0.21983341872692108, -0.18848831951618195]}, "authors": [{"authorId": "2276964586", "name": "Taixi Lu"}, {"authorId": "51225422", "name": "Haoyu Wang"}, {"authorId": "2286886107", "name": "Huajie Shao"}, {"authorId": "2284861474", "name": "Jing Gao"}, {"authorId": "2273953218", "name": "Huaxiu Yao"}], "references": [{"paperId": "4cf818ab6b92ea5616fdd0cab144ccf802be8cc5", "title": "Selective Learning: Towards Robust Calibration with Dynamic Regularization"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "2a11cfe18a6f39de9e00a24f0c7040d2b7ec7d64", "title": "Conservative Prediction via Data-Driven Confidence Minimization"}, {"paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa", "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "0c1d2ac6436e5efd3def80d80b100b0efd613e2d", "title": "Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models"}, {"paperId": "8fd462f6248d5e3f1b6602697c09489086b5655f", "title": "Distilling Reasoning Capabilities into Smaller Language Models"}, {"paperId": "33be243ac9dd8723e6267dea45fd6a6172d4f6a5", "title": "Less is More: Task-aware Layer-wise Distillation for Language Model Compression"}, {"paperId": "3e565c544a8639cc9c7568833e484d7610f5e5d4", "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"}, {"paperId": "c2d29e2b10572d229e83e8c0005f9b48223332eb", "title": "Mitigating Neural Network Overconfidence with Logit Normalization"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "4c13e8071dafeed6a257e4e127b24ffc637bcf81", "title": "AugMax: Adversarial Composition of Random Augmentations for Robust Training"}, {"paperId": "84310f76cf8909f87c6a7f2ed30ae28214cc9eab", "title": "LeeBERT: Learned Early Exit for BERT with cross-level optimization"}, {"paperId": "03662672662f49e6b06148e94b407b60b0bb72f3", "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models"}, {"paperId": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3", "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "14c26c4f14c9cad4d805583eb36b7ba52120423e", "title": "CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade"}, {"paperId": "f61855fa6acf7bd28b20187ad1cf219e96429534", "title": "Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"}, {"paperId": "297ad41c0e7264e67ae078921e2a57436293ce72", "title": "XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "04a7021fe6be6bddcfae476493fcc7571e7c613c", "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "809cc93921e4698bde891475254ad6dfba33d03b", "title": "How Multilingual is Multilingual BERT?"}, {"paperId": "a3143eaa68040d366848a9c324b29d3f56f97a5d", "title": "Shallow-Deep Networks: Understanding and Mitigating Network Overthinking"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "d65ce2b8300541414bfe51d03906fca72e93523c", "title": "On Calibration of Modern Neural Networks"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": "cbde5598c1a78285adfcfd77fb3636f5498987a0", "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning"}, {"paperId": "7b37c0a4976c4d2a5a440d494fbb0f3daede2a00", "title": "BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression"}, {"paperId": null, "title": "knowledge distillation to train a small generative model. Pruning focuses on finding redundant parameters and sets them as zero to achieve highly sparse neural network"}, {"paperId": null, "title": "(Sun et al., 2020)"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "A Related Work Model"}, {"paperId": null, "title": "lottery ticket hypothesis (Frankle and Carbin, 2018) for BERT, and Sheared Llama (Xia et al., 2023) designs a structured pruning method for generative pre-trained language models."}, {"paperId": null, "title": "are hardware-friendly and efficient for matrix computation. Specifically, Q8BERT (Zafrir et al., 2019) proposes 8-bit quantization for BERT; MKQ-BERT (Tang et al., 2022) designs 4-bit quantization"}, {"paperId": null, "title": "2023. Macedon: Minimizing representation coding rate reduction for cross-lingual natural language understanding"}, {"paperId": null, "title": "2022. Holistic evaluation of language models"}, {"paperId": null, "title": "We include the prompt we use for Llama-2 in our experiment on PAWS-X in this section. Detailed prompt is shown in"}, {"paperId": null, "title": "meth-ods often need to train compressed models from scratch, which is expensive"}, {"paperId": null, "title": "quantization. Knowledge distillation is to leverage a powerful teacher model to guide the learning of a lightweight student model. The lightweight student can support efficient inference"}, {"paperId": null, "title": "2023. Shifting attention to relevance: To-wards the uncertainty estimation of large language models"}]}