{"paperId": "54983a66dc8bc15af89890bc9e1a63e056594179", "abstract": "the family of efficient attention approaches in which the lengths of the attention operands are reduced to M ( < N ) by applying an abstraction function, resulting in reduced complexity of the attention while retaining the form of basic attention computation in Eq. 3. Abstrac-tive attentions can be further categorized to either resolution preserving or non-preserving , according to which operands are chosen to be abstracted. Resolution non-preserving attention is the", "venue": "International Conference on Learning Representations", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": null}, "embedding": {"model": "specter_v2", "vector": [0.2127816379070282, 0.4922875165939331, -0.3106034994125366, 0.04590319097042084, -0.49108049273490906, 0.2131563276052475, 0.17948469519615173, 0.22828857600688934, -0.04638008400797844, -0.2228412628173828, 0.5856426954269409, 0.7785568833351135, 0.19112145900726318, -0.40188321471214294, 0.19600704312324524, 0.22764095664024353, -1.1079232692718506, -0.018127860501408577, 0.8419344425201416, -0.19123411178588867, 0.215698704123497, -0.27137190103530884, -1.6716164350509644, 0.8329654932022095, 0.38483715057373047, 1.249332308769226, -0.12042456120252609, 0.8980768918991089, -0.40379875898361206, 0.591453492641449, 0.27963948249816895, -0.191520556807518, 0.5006287097930908, -0.07741841673851013, -0.5749937295913696, -0.30976876616477966, 0.553676962852478, -0.41410526633262634, -0.1257992833852768, 0.857092022895813, -0.29773128032684326, 0.20261183381080627, -0.11170212924480438, -1.0712018013000488, -0.0009049487416632473, 1.1746134757995605, 0.3528737425804138, 0.9438447952270508, -0.2731977701187134, -1.2657893896102905, 1.3866767883300781, -1.5468792915344238, -0.15943044424057007, 1.4048320055007935, 0.204405277967453, 0.5074828267097473, -0.2420012354850769, -0.19277192652225494, 0.504740834236145, 0.38893330097198486, -0.7371845841407776, -0.6286469101905823, 0.16590549051761627, 0.32688331604003906, 1.7352027893066406, -0.07095001637935638, 0.08919564634561539, -0.4464349150657654, 0.3119593858718872, 1.3316426277160645, -0.14540231227874756, -0.9131823182106018, -0.2264949232339859, -0.043142955750226974, 0.4937885105609894, 0.8094110488891602, -0.2946885824203491, 0.010470948182046413, -1.4395644664764404, -0.1903245896100998, 0.9158469438552856, 0.4492354989051819, 0.13680341839790344, -0.7624382972717285, -0.04479248821735382, 0.25544455647468567, 0.17850449681282043, 1.0236166715621948, -0.358349472284317, 0.49100443720817566, 0.7280864715576172, 0.5931987762451172, -0.21675726771354675, 0.31597238779067993, -0.002813502447679639, 0.709898829460144, -0.6791253089904785, -0.038237158209085464, -0.20974020659923553, 1.087520718574524, -0.06940726935863495, 0.26387539505958557, -0.3270176947116852, 0.19274169206619263, 0.8554008603096008, -0.12281052023172379, 0.2937853932380676, -0.657404899597168, -0.14122694730758667, -0.06051388010382652, 0.10660921782255173, -0.37450534105300903, -0.00667260866612196, 0.034629397094249725, -0.660295844078064, -0.4817180037498474, -0.5959096550941467, 0.620562732219696, -0.49817630648612976, 0.44867661595344543, -0.564675509929657, -0.2843116223812103, 0.02925361692905426, 0.3639605939388275, 0.22255776822566986, 0.22165316343307495, 0.3422727882862091, -0.20233114063739777, 1.3237146139144897, -1.1928846836090088, -0.5316890478134155, -0.6899151802062988, 0.00788157805800438, 0.0028680074028670788, 0.09660019725561142, -0.2698119580745697, -1.6412875652313232, -1.1584159135818481, -0.9887161254882812, 0.12718477845191956, -0.6258036494255066, -0.5750483274459839, 0.7806675434112549, -0.14836566150188446, -1.5562553405761719, 0.514680027961731, -0.2952825129032135, -0.4182676374912262, 0.13993152976036072, 0.3132169246673584, 0.1926913857460022, 0.022227175533771515, -1.2836507558822632, 0.2164774388074875, 0.06766849011182785, -0.9184048175811768, -0.1933404952287674, -0.8873147368431091, -1.0443274974822998, 0.3926476538181305, 0.6974163055419922, -0.7474246621131897, 1.0619926452636719, -0.03260093554854393, -0.7110897898674011, 0.3045930862426758, -0.32376378774642944, 0.112916499376297, -0.34346678853034973, -0.12017395347356796, -0.7898203730583191, 0.16910214722156525, -0.10097326338291168, 0.3411036431789398, 0.45439550280570984, -0.2820148169994354, -0.690894603729248, -0.20759108662605286, -0.17271578311920166, 0.10478751361370087, -0.3381425440311432, 1.3200719356536865, -0.1097969263792038, -0.08783892542123795, 0.48065605759620667, 1.1441192626953125, 0.05544215813279152, -0.007528278976678848, -0.023821944370865822, -0.4914136826992035, 0.8004993796348572, 0.6384209394454956, 1.4167499542236328, -0.8135974407196045, -0.5999962687492371, -0.45515522360801697, -0.0779021605849266, -0.16651010513305664, -0.6577165126800537, 0.5137786865234375, -0.7517129182815552, 0.4569014608860016, -0.12126367539167404, -0.24012549221515656, -0.42829638719558716, -0.7251605987548828, -0.7334187626838684, -0.6969677209854126, -0.03591740503907204, 0.7378650903701782, -0.7626738548278809, -0.2879685163497925, -0.1760837584733963, -0.06595145165920258, -0.6732184290885925, 1.4166595935821533, -0.05444718152284622, -0.14470267295837402, -0.39690330624580383, 0.13918006420135498, 0.04980279505252838, -0.01166589092463255, -0.012370927259325981, -0.548258364200592, -0.3497248888015747, -0.049964338541030884, -0.5761846899986267, 1.2614247798919678, -0.15746758878231049, -0.035154834389686584, 0.173202782869339, -0.3734893500804901, 0.2352718859910965, 0.590272843837738, -0.13905327022075653, -0.5824479460716248, -0.0703832134604454, 0.30253854393959045, -0.653296947479248, 0.03189289942383766, 0.9612811803817749, 1.0238069295883179, -0.8947295546531677, 0.2005639225244522, 0.43875357508659363, -0.20347031950950623, 0.17946219444274902, -0.01546450424939394, 0.8807792067527771, 0.027516363188624382, 0.9878259301185608, -0.6904553174972534, 0.6995258331298828, -0.7393319606781006, 0.18175122141838074, 0.4850884675979614, 0.1302298903465271, 0.7151727676391602, 0.49599623680114746, -1.1087543964385986, -0.37933865189552307, -0.053213782608509064, 0.6246274709701538, 1.8644660711288452, 0.26521793007850647, -0.23965173959732056, -0.6405965685844421, -0.14200608432292938, -0.5351529121398926, -0.16728977859020233, -0.3519096076488495, -0.36190372705459595, -0.6716853380203247, -0.511405885219574, 0.9494515657424927, 0.7646732926368713, 1.4158324003219604, -1.0273070335388184, -1.014570713043213, -0.21260575950145721, 0.5377708673477173, -0.49899056553840637, -0.6457178592681885, 0.5762560963630676, -0.5184632539749146, -0.18476741015911102, 0.1950497329235077, -0.4343327581882477, 0.28236255049705505, -0.6587520837783813, 0.7951248288154602, -0.3251625597476959, -0.3855840265750885, 0.23478594422340393, 0.6130251288414001, -0.5683948993682861, -0.24830959737300873, -0.013240399770438671, -0.16605785489082336, -0.25571486353874207, 0.6382131576538086, 0.23413649201393127, 0.15111520886421204, 0.22646741569042206, -0.8445587754249573, 0.12595689296722412, 0.1783052682876587, 0.03092203475534916, 0.8945097327232361, -0.22098462283611298, 0.14247572422027588, -1.0421061515808105, 0.8569228053092957, 0.5832200050354004, -0.5028377175331116, 0.39675310254096985, -0.4373776614665985, -0.14131709933280945, 0.6144618988037109, -0.4078555405139923, 0.025499597191810608, -0.9145558476448059, 0.907569408416748, -0.5615653395652771, -0.3886168301105499, -0.021991096436977386, -0.22630997002124786, -0.2434546947479248, -0.02328219823539257, 0.6052864193916321, -0.028705600649118423, 0.054162416607141495, 0.23810246586799622, -0.9271127581596375, 0.3968038558959961, 0.06557781249284744, -0.28134971857070923, -0.20109106600284576, 0.12807172536849976, -0.5520491003990173, -0.2578475773334503, -0.2724391520023346, -0.44439950585365295, 0.16190847754478455, -0.0816752165555954, -0.2589162290096283, -0.5715596675872803, 0.14076943695545197, -0.9280547499656677, -0.2461298704147339, -0.006193930748850107, -0.6504876017570496, -0.4828176498413086, -1.366495132446289, -1.3127690553665161, -0.2493119090795517, -0.7071282863616943, -1.0520356893539429, 0.43277257680892944, 0.04197470843791962, -0.3036821186542511, -0.13948838412761688, -0.34151914715766907, -0.9134423732757568, 1.3389928340911865, -0.3414049744606018, 1.1649425029754639, -0.5815742015838623, -0.8302885890007019, -0.11134009063243866, 0.17409861087799072, 0.04673309996724129, 0.07821264117956161, -0.3346133530139923, -0.4600749909877777, 0.7447665333747864, 0.3876399099826813, -0.19895023107528687, 0.13967189192771912, 0.2758072316646576, 1.1571838855743408, 0.1060139462351799, -0.5174322724342346, -0.2708521783351898, 1.8330931663513184, -0.00541660375893116, 0.3695783019065857, 0.08859719336032867, 0.5876091122627258, 0.3240388035774231, 0.19338549673557281, 0.4054521322250366, 0.13094808161258698, 0.2886883020401001, 0.5288888812065125, 0.05888883396983147, -0.36792731285095215, 0.24412788450717926, 0.16383109986782074, 1.3818211555480957, 0.05510454624891281, 0.2867603600025177, -1.2034220695495605, 1.0654391050338745, -1.3337212800979614, -1.009492039680481, 1.2977030277252197, 1.0129401683807373, 0.33278197050094604, -0.20691291987895966, -0.24447603523731232, 0.10179923474788666, 1.136217713356018, 0.3798317611217499, -0.1107398122549057, -1.2021169662475586, 0.42273104190826416, 0.587173342704773, -0.024414556100964546, 0.9898180365562439, -0.17666766047477722, 0.5880955457687378, 14.641854286193848, 0.23555682599544525, 0.08521581441164017, 0.288947194814682, 0.5510076284408569, 0.18052707612514496, -0.5144924521446228, -0.18768945336341858, -1.3797484636306763, -0.3225499391555786, 1.204332947731018, -0.26060032844543457, 0.4794793128967285, 0.24588897824287415, -0.5032030344009399, 0.23675620555877686, -0.8576365113258362, 0.08930211514234543, 0.6828778982162476, -0.7885774374008179, 0.4653346836566925, 0.13226337730884552, -0.005470586940646172, -0.05311991646885872, 0.3390897512435913, 0.5737037062644958, 0.4221753776073456, -0.29978108406066895, 0.42324963212013245, 0.2818472385406494, 0.925899088382721, -0.6657273173332214, 0.33152249455451965, 0.058553967624902725, -1.3797907829284668, -0.3589525818824768, -0.5703226923942566, -1.136747121810913, -0.13385604321956635, 0.01425931602716446, -0.008105007000267506, -0.6168466210365295, 0.07166045159101486, -0.09534390270709991, 0.2587639391422272, 0.5895771980285645, -0.4321213662624359, -0.28546372056007385, 0.16167092323303223, 0.06198704615235329, -0.5260122418403625, 1.034371256828308, 0.4230184555053711, -0.4411213994026184, 0.08959272503852844, 0.4203217029571533, 0.032831352204084396, 0.28396838903427124, -0.24366037547588348, -0.2642430067062378, -0.49846184253692627, 0.13355202972888947, 0.3078285753726959, 0.8233807682991028, 0.4250592291355133, 0.17145198583602905, -0.5733347535133362, -0.04359988868236542, 0.5118893980979919, -0.1265338808298111, -0.22162438929080963, -0.8838334083557129, 0.31956738233566284, -0.2941000461578369, 0.245914027094841, 0.8935103416442871, -0.7558419108390808, -0.17699480056762695, -0.8622865676879883, -0.26668575406074524, 0.8710862994194031, -0.8670312166213989, -0.17025426030158997, 1.4884519577026367, 0.11480307579040527, -0.5619924664497375, 0.4253309369087219, -0.19773393869400024, -0.3724079728126526, 0.132310688495636, -1.1933798789978027, -0.8343283534049988, 0.1872836947441101, -0.05494629964232445, -0.02977020852267742, 0.15171785652637482, 1.4962661266326904, -0.4914664924144745, -0.24685852229595184, -0.08930637687444687, -1.082561731338501, -0.09431128948926926, 0.10200586169958115, -0.9275959134101868, 0.6087968945503235, 0.9450199604034424, -0.5530015230178833, 0.34238550066947937, -0.19645081460475922, 0.28965941071510315, -0.43679651618003845, -0.11520103365182877, 0.8796473145484924, -0.6258630752563477, -0.26168200373649597, -0.6962418556213379, -1.2624624967575073, 0.4912610352039337, 0.9024055004119873, 0.17581209540367126, 0.18895424902439117, 0.023657290264964104, -0.47018343210220337, -0.7484802007675171, -1.0769213438034058, 0.3961913585662842, 0.6011213064193726, -0.776918351650238, -0.8399956822395325, -0.9390933513641357, 0.6882797479629517, -0.9511212706565857, -0.9279140830039978, -0.1057465448975563, 0.07447724789381027, -0.398333340883255, 0.9046186804771423, -0.25127941370010376, 0.9210584163665771, 0.5600722432136536, -0.3864755928516388, -0.518644392490387, -0.40104976296424866, -0.6496886610984802, -0.02535167708992958, 0.478567898273468, 0.5698015689849854, -0.46631669998168945, -0.012273651547729969, 0.877566397190094, -0.039811115711927414, -0.22721822559833527, -0.4595668613910675, 0.3653064966201782, -0.43321478366851807, -0.31544703245162964, 0.7975780367851257, -0.16107448935508728, 0.5040673017501831, -0.26065781712532043, 0.691318690776825, 1.1277416944503784, -0.16975553333759308, 0.09089533239603043, -0.06590984761714935, -0.09004409611225128, 0.3873522877693176, -0.7076063752174377, -0.8554010391235352, -0.6599839329719543, -0.2951713502407074, -0.7806488275527954, 0.4745102524757385, -0.977420449256897, -0.34290578961372375, 0.5755124688148499, -0.8474231958389282, 0.7004558444023132, 0.40349024534225464, -0.1880153864622116, -0.6396716833114624, -0.2668742835521698, -0.7391515374183655, 0.1907275766134262, 0.8382576704025269, -0.28170499205589294, -0.020362412557005882, -0.30498242378234863, -0.411033034324646, 0.2675386071205139, 0.5717865824699402, -0.6066579818725586, -0.5767418742179871, -1.1061856746673584, 0.13143213093280792, 0.3858810365200043, -0.17832806706428528, -0.867613673210144, 1.2061586380004883, 0.3396974503993988, -0.10759709030389786, 0.04096880555152893, 0.2025853395462036, -0.960500955581665, -0.706495463848114, 0.42251867055892944, -1.0004634857177734, 0.3007238209247589, 0.5498755574226379, -0.8810662627220154, -0.7400780320167542, 0.8183497190475464, 0.17752541601657867, -1.283632516860962, -1.4573923349380493, 0.48980963230133057, -0.7118169665336609, 0.17407841980457306, 0.04885537549853325, -0.2261212170124054, -1.1041947603225708, 0.25702330470085144, -0.06722672283649445, -0.13498464226722717, 0.03237009048461914, 0.8556731343269348, 0.6336410641670227, -1.2858747243881226, 0.24259085953235626, 0.9082963466644287, 0.001817414304241538, 0.2619963586330414, 0.3042733669281006, 0.37643441557884216, -0.13959355652332306, 0.4605615437030792, -0.45592883229255676, 0.2600523829460144, -0.874578595161438, 0.24961073696613312, 0.3660074472427368, -0.34690749645233154, -0.08386758714914322, 0.898847222328186, -0.23626083135604858, -0.28486567735671997, -0.1227223202586174, -1.2555921077728271, -0.8185746073722839, -0.4202445447444916, 1.5771276950836182, 0.46244657039642334, -0.11099603772163391, 0.037812020629644394, -0.8283685445785522, 0.23715724050998688, -0.34714072942733765, -0.3453409671783447, 0.32914042472839355, -0.2942925691604614, -0.9984642863273621, 0.6058894395828247, -0.04412595555186272, -0.20312151312828064, -0.2748440206050873, -0.4798237085342407, -0.14131581783294678, -0.16386070847511292, 0.08559444546699524, -0.22040413320064545, -0.6088910102844238, 0.6228326559066772, 0.34104475378990173, 0.9764849543571472, 0.6286791563034058, -0.33327072858810425, -0.1983184814453125, 0.11475609242916107, 0.6119576096534729, -0.24069683253765106, -0.40120968222618103, 0.8303582668304443, 1.6294713020324707, -0.16583313047885895, 0.055602047592401505, -0.6478211283683777, -0.6017318367958069, 1.2294002771377563, 0.8174435496330261, -0.44931352138519287, 0.5799981951713562, 0.5587388277053833, 0.19356265664100647, 0.3463556170463562, -1.2560211420059204, -0.36784565448760986, 0.5410864353179932, 1.2530142068862915, 0.30150389671325684, -0.020786922425031662, 0.33416491746902466, 1.1987909078598022, 0.6133414506912231, 0.2353987842798233, 1.01840078830719, 0.5180474519729614, -0.713413655757904, 0.14357809722423553, -0.04857411980628967, 0.2627946734428406, -0.8864455223083496, -0.6207756400108337, 0.10548087954521179, 1.0292969942092896, -0.09378492087125778, 0.4883124828338623, 0.8781993985176086, 0.005286023020744324, 0.7327159643173218, 0.3536900281906128, 0.46562835574150085, -0.9195061326026917, -0.32757213711738586, 0.006551216822117567, -0.7350316047668457, -0.6138503551483154, -0.3282473683357239, -0.2368553876876831, -0.44625720381736755, 0.3274194896221161, 0.35168778896331787, 0.06647919118404388, 0.04558005928993225, 0.7176713943481445, 0.9097334146499634, 0.8922067880630493, -0.3147459626197815, -0.2393680214881897, -0.374040424823761, -0.7303175330162048, -0.08066823333501816, -0.9538698792457581, 0.10808372497558594, 0.1459352821111679, -0.09202694892883301, -0.46526142954826355]}, "authors": [{"authorId": "31118467", "name": "Mingu Lee"}, {"authorId": "1470532669", "name": "Saurabh Pitre"}, {"authorId": "144830318", "name": "Tianyu Jiang"}, {"authorId": "46255163", "name": "Pierre-David L\u00e9tourneau"}, {"authorId": "3487254", "name": "Matthew J. Morse"}, {"authorId": "2220895637", "name": "Kanghwan Jang"}, {"authorId": "2099649252", "name": "Joseph B. Soriaga"}, {"authorId": "1897950", "name": "Parham Noorzad"}, {"authorId": "2223191879", "name": "Hsin-Pai Cheng"}, {"authorId": "2064072404", "name": "Chris Lott"}], "references": [{"paperId": "240300b1da360f22bf0b82c6817eacebba6deed4", "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "f864d4d2267abba15eb43db54f58286aef78292b", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "501d54cd4731dba3c7aad5735058be3da2753621", "title": "Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "41a78e2885b5dc8c719495a33985b5f4880f5b48", "title": "Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "cac62816dec634452ef6106988e0702a3cf42798", "title": "Multivariate polynomial approximation in the hypercube"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": "015ca32bca81dbda1e2e432445eef798582236e1", "title": "Conference Paper"}]}