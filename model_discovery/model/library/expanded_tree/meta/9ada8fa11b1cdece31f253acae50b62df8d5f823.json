{"paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 246, "influentialCitationCount": 35, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.07922", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks, and proposes a mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy."}, "embedding": {"model": "specter_v2", "vector": [0.15838535130023956, 0.5846665501594543, -0.44135648012161255, 0.26642680168151855, -0.5358374118804932, -0.4581155478954315, 0.478857159614563, 0.23179423809051514, 0.06520888209342957, -0.45876386761665344, 0.5239928364753723, -0.4814203977584839, 0.6293562054634094, 0.1316930651664734, -0.41992130875587463, 0.27994397282600403, -0.6727644801139832, 0.055870916694402695, -0.3055557608604431, -0.37011101841926575, 0.234130859375, -0.9898802042007446, -1.0234010219573975, 0.40663209557533264, 1.13308846950531, 0.18292327225208282, 0.20377080142498016, 1.0127216577529907, -0.5897920727729797, 0.6008026003837585, 0.6945694088935852, -0.5848859548568726, 0.12936808168888092, -0.18078620731830597, -0.7320114970207214, 0.12168938666582108, 0.16309283673763275, -0.17121514678001404, -0.2497299462556839, 0.7882812023162842, -0.311846524477005, 0.07992290705442429, 0.5339338779449463, -0.4573957622051239, -0.651503324508667, 0.9911513924598694, 0.32520294189453125, 0.441742867231369, -0.12394280731678009, -0.030114227905869484, 1.099135160446167, -1.3531370162963867, 0.21249622106552124, 1.267378330230713, 0.6299690008163452, 0.7197501063346863, -0.33355218172073364, -0.24261800944805145, 0.11142747104167938, -0.376122385263443, -0.9343258738517761, -0.614163339138031, -0.5874133706092834, -0.5078219175338745, 2.2676868438720703, -0.691437840461731, -0.5353123545646667, 0.05122235417366028, -0.025385217741131783, 0.9684366583824158, -0.43014928698539734, -0.6529743671417236, -0.39711105823516846, -0.11696184426546097, -0.14316323399543762, 1.2859936952590942, -0.34339314699172974, 0.11982002854347229, -0.4706536829471588, 0.1479865163564682, 0.4154719114303589, -0.3426334857940674, -0.22719110548496246, -0.13739608228206635, -0.07561947405338287, 0.6138309836387634, 0.2597063183784485, 0.7410579919815063, 0.30046290159225464, 0.759540319442749, 0.6129630208015442, 0.2878764569759369, 0.016415636986494064, 0.5791500806808472, -0.19516097009181976, 0.03321264684200287, -0.7057026624679565, 0.1468481570482254, 0.248020201921463, 1.1174132823944092, 0.10312086343765259, 0.5354596376419067, -0.9026212096214294, 0.3022175431251526, 1.1208082437515259, 0.0025086214300245047, 0.6248421669006348, -0.6984071135520935, 0.8959470391273499, -0.3750320374965668, -0.23820458352565765, -0.16995224356651306, 0.07382043451070786, -0.33995184302330017, -0.7432084083557129, -1.0688141584396362, -0.5489645600318909, -0.15804824233055115, -0.7513840198516846, 0.7761507034301758, -0.5164296627044678, 0.1315530687570572, 0.6155083775520325, 0.32051339745521545, 0.899456799030304, 0.749177873134613, 0.2160533219575882, 0.23869290947914124, 0.6830477118492126, -1.0972061157226562, -0.10506543517112732, -1.405202031135559, 1.0104084014892578, -0.6051938533782959, 0.11077159643173218, -0.47133252024650574, -1.606229305267334, -1.1725188493728638, -0.8668580055236816, -0.11883120238780975, -0.4707088768482208, 0.5635918974876404, 0.9310911297798157, 0.3977133333683014, -1.0187913179397583, 0.9118606448173523, -0.07889550924301147, -0.029745474457740784, 0.48151662945747375, 0.05385724827647209, 0.3521502614021301, -0.473745733499527, -1.0836858749389648, 0.1304948925971985, 0.2906535565853119, -0.8591885566711426, -0.6958885788917542, -0.5100069046020508, -1.5533560514450073, 0.0616765022277832, 0.3206513226032257, -0.30094632506370544, 1.4326635599136353, -0.1576707512140274, -0.9208046793937683, 0.9781747460365295, -0.3128276765346527, -0.02250421606004238, 0.10611004382371902, 0.01892135851085186, -0.21322163939476013, -0.5089208483695984, -0.14077213406562805, 0.673866868019104, 0.6050753593444824, -0.3578961193561554, 0.04427073895931244, 0.7355797290802002, -0.025562921538949013, -0.15205898880958557, -0.24500234425067902, 0.8571280241012573, -0.7371058464050293, -0.14276209473609924, -0.02286563627421856, 0.5974356532096863, 0.02047109603881836, -0.44924211502075195, -0.2461777627468109, -1.0020776987075806, 0.4902770519256592, -0.14753152430057526, 1.0300482511520386, -1.041050910949707, -0.5462544560432434, -0.47042033076286316, -0.2533625364303589, 0.09006839245557785, -0.8305917382240295, 0.5509857535362244, -0.5738774538040161, 0.6271234154701233, -0.7088578939437866, -1.2495276927947998, 0.03621996194124222, -0.261087030172348, -0.6468018889427185, -0.14513196051120758, 0.5568057298660278, 1.278138279914856, -0.9756922721862793, -0.10654133558273315, -0.2786252498626709, -0.006999797187745571, -1.2768958806991577, 1.1653416156768799, -0.5773040056228638, -0.09405594319105148, 0.0018473033560439944, -0.23157727718353271, -0.06329718232154846, -0.3582717180252075, 0.16643956303596497, -0.12132971733808517, -0.27073678374290466, 0.5501861572265625, -0.06934904307126999, 1.894723892211914, -0.7355275750160217, 0.36529672145843506, -0.17912687361240387, -0.4944927394390106, 0.2768276631832123, 0.7383788824081421, -0.31663355231285095, -0.1576983481645584, 0.3671482801437378, 0.6647759079933167, -0.3603341579437256, -0.3566301167011261, 0.7596361637115479, 0.7104855179786682, -0.2996078431606293, 0.53773033618927, 0.7219539880752563, -0.7207522988319397, 1.0375068187713623, 0.6044028997421265, 0.9292638301849365, 0.44388437271118164, 0.33183562755584717, -0.11662250012159348, 0.7693130970001221, -0.575129508972168, -0.5030065178871155, 0.2912411093711853, 0.7383307814598083, 1.3695478439331055, 0.5489993691444397, -0.5418769121170044, -0.5696478486061096, -0.002819595392793417, 0.8253713846206665, 1.4342727661132812, 0.04546304792165756, -0.07026966661214828, -0.984115481376648, -0.5179858207702637, -0.2556129992008209, 0.1640387922525406, -0.30920225381851196, -0.6642425656318665, -0.6221178770065308, -1.0777010917663574, 0.8241341710090637, 0.3476930260658264, 1.1709836721420288, -0.5888137817382812, -0.09984748065471649, -0.42662665247917175, 0.10683247447013855, -0.5426235198974609, -1.0491477251052856, 0.5201606750488281, -0.3559618294239044, -0.22921216487884521, 0.34053584933280945, -0.4664008915424347, 0.4989396631717682, -0.36791497468948364, 1.0948729515075684, 0.06712774187326431, -0.6236822605133057, 0.20650233328342438, 0.42225903272628784, -0.18673178553581238, -1.3294775485992432, 0.6817291975021362, -0.33130398392677307, -0.3325209319591522, 0.24019357562065125, 0.4925757646560669, 0.2688242793083191, -0.06205345690250397, -0.5108616948127747, 0.16441868245601654, 0.2507173418998718, -0.09087736904621124, 0.2673553228378296, -0.17478565871715546, -0.06300440430641174, -1.1399086713790894, 0.9210224151611328, -0.11548486351966858, -0.28607115149497986, 0.28957200050354004, -0.603721559047699, -0.09845343232154846, 0.7348231673240662, -0.4408596158027649, -0.748488187789917, -0.8520308136940002, 0.4209156334400177, -0.02922896295785904, -0.1781691610813141, 0.089724101126194, 0.41253113746643066, 0.05991356819868088, 0.6434314250946045, 0.48289546370506287, 0.30403679609298706, -0.22323644161224365, 0.6633858680725098, -0.9598143100738525, 0.8515608310699463, -0.22249123454093933, 0.7188461422920227, -0.3889809250831604, -0.6200121641159058, -0.04289982095360756, -0.43217501044273376, -0.41466161608695984, -0.1390753537416458, -0.3399156928062439, 0.6320286989212036, -0.6420568227767944, -0.6784836649894714, -0.1401231735944748, -1.4667402505874634, -0.31688740849494934, 0.17510896921157837, -0.6500565409660339, -0.12456944584846497, -0.9497935771942139, -1.302057147026062, -0.1623827964067459, -0.6534711718559265, -1.2429852485656738, 0.6994960308074951, 0.04666487127542496, -0.5203712582588196, -0.579829752445221, 0.2048497051000595, -0.3563804030418396, 0.886265218257904, -0.7277517914772034, 0.9447317719459534, 0.3440765142440796, -0.27344751358032227, 0.06618104875087738, -0.0012153818970546126, 0.5876093506813049, -0.3534533977508545, 0.5667494535446167, -0.800925076007843, -0.026602232828736305, -0.1694832444190979, -0.847301721572876, 0.35533007979393005, -0.15146802365779877, 0.7801755666732788, 0.1592053920030594, -0.4663088619709015, 0.634329080581665, 1.7458910942077637, -0.6151735782623291, 0.23642121255397797, 0.018168607726693153, 1.100241780281067, 0.4232219159603119, -0.5954354405403137, 0.5335081219673157, 0.052767809480428696, -0.13126394152641296, 0.5767185091972351, 0.030242228880524635, -0.3510003387928009, -0.45046794414520264, 0.7793943285942078, 1.6566462516784668, 0.20076528191566467, -0.08177037537097931, -1.4716670513153076, 0.9255597591400146, -0.9745905995368958, -0.3765949606895447, 0.0919618010520935, 0.4890815019607544, 0.7452167272567749, -0.46221351623535156, -0.9050425291061401, -0.26606085896492004, 0.7076075077056885, 0.2928890883922577, -0.3076160252094269, -0.9192589521408081, 0.23018567264080048, 0.3418417274951935, 0.1972958892583847, 0.3952798843383789, -0.6047941446304321, 0.4642954170703888, 14.386236190795898, 1.0262675285339355, 0.38204318284988403, 0.7014328837394714, 0.692486047744751, 0.31633514165878296, -0.4751770496368408, -0.2640070915222168, -1.0385825634002686, -0.5592295527458191, 1.0675292015075684, -0.005434755235910416, 0.7727780342102051, 0.5003694891929626, -0.24242523312568665, 0.22787289321422577, -0.4411773383617401, 0.5851426124572754, 0.6073945164680481, -1.3715603351593018, 0.4762533903121948, 0.05546369403600693, 1.0951669216156006, 0.37775999307632446, 1.0876365900039673, 1.0008560419082642, 0.5418878197669983, -0.7639554142951965, 0.5199645161628723, -0.24574331939220428, 1.2352477312088013, -0.0011882545659318566, 0.4628439247608185, 0.4513663351535797, -1.2206621170043945, -0.45543479919433594, -0.5664128065109253, -1.1340951919555664, 0.06923583894968033, -0.08654358237981796, -0.6846346259117126, -0.35160553455352783, -0.5134096145629883, 0.9874970316886902, -0.27991729974746704, 0.3641246259212494, -0.5919631123542786, 0.5781647562980652, 0.5992130637168884, 0.3436996340751648, 0.28536227345466614, 0.622818112373352, -0.09021956473588943, 0.0805756151676178, 0.20408286154270172, -0.11839787662029266, 0.13677728176116943, 0.7699674963951111, -0.9127496480941772, -0.11167659610509872, -0.44189023971557617, -0.08882949501276016, -0.45619648694992065, 0.830385148525238, 0.06563237309455872, 0.2744089663028717, -0.9899964332580566, 0.23841989040374756, 0.8910256624221802, 0.06734593212604523, -0.5332414507865906, -0.1454470157623291, 0.15284916758537292, -0.3416089713573456, 0.16125282645225525, 0.44128185510635376, -0.5100070238113403, -0.5803667306900024, -0.888839602470398, -0.5142238736152649, 0.1364464908838272, -0.6425125598907471, -0.39130350947380066, 0.8689447045326233, -0.5415746569633484, -0.7538778185844421, 0.28768545389175415, -0.7165229320526123, -0.5583484768867493, 0.2863951027393341, -1.248881459236145, -0.14734135568141937, 0.1869346648454666, -0.5734633803367615, -0.09443168342113495, -0.3373975455760956, 1.0821541547775269, 0.05191532149910927, -0.3127190172672272, -0.057419318705797195, 0.30579283833503723, 0.2449820637702942, -0.14984753727912903, -0.9241591095924377, 1.1756242513656616, 0.5808109641075134, -0.2888611853122711, 0.25222572684288025, -0.0918549969792366, -0.019037475809454918, -0.6985569000244141, -0.5167087316513062, 0.673549473285675, -0.9617547988891602, -0.3198834955692291, -0.8024726510047913, -0.9068827629089355, 0.1942991316318512, 0.7720372080802917, -0.38221415877342224, 0.1746578961610794, -0.07680048793554306, -0.888109028339386, 0.033559106290340424, -0.7321399450302124, -0.16989587247371674, 0.6130241751670837, -0.8877086639404297, -0.35585206747055054, 0.021924389526247978, 0.5090318918228149, -1.0149272680282593, -0.5348996520042419, -0.347248375415802, -0.2160375416278839, -0.06607023626565933, 0.7933661341667175, 0.002288294257596135, 1.708627700805664, 0.8438063859939575, 0.3082607090473175, -0.8947013020515442, 0.11725597828626633, -1.051121473312378, 0.19855721294879913, 0.43438521027565, 0.9163676500320435, -0.1796535849571228, 0.11348381638526917, 0.853990375995636, -0.16041284799575806, -0.30998072028160095, -0.4232766330242157, -0.19935737550258636, 0.4218238890171051, -0.5426145792007446, 0.24132587015628815, -0.10967694222927094, 0.3709511458873749, -0.0947004184126854, 0.53135746717453, 0.42881256341934204, -0.4436038136482239, -0.38961076736450195, 0.4013323485851288, 0.33632582426071167, -0.36633265018463135, -0.6907166242599487, -0.0737306997179985, -1.318118691444397, 0.3773163855075836, -1.6060116291046143, 0.5922728180885315, -0.5311250686645508, 0.014177573844790459, 0.4972928762435913, 0.2869519293308258, 0.22672708332538605, 0.13934268057346344, -0.056176114827394485, -0.6986492872238159, -0.7094494700431824, -0.7374500632286072, 0.9482828378677368, 0.8180696964263916, -0.8099202513694763, 0.38056808710098267, -0.43200522661209106, 0.07588034123182297, 0.2694862484931946, 0.5025593638420105, -0.6210125684738159, -0.9967501163482666, -1.652978777885437, 0.4530480206012726, -0.15104924142360687, -0.18770276010036469, -0.8921929597854614, 0.5803534984588623, 0.7193248867988586, -0.4996691644191742, 0.4834565222263336, -0.4644929766654968, -0.37629687786102295, -0.806871771812439, 0.6460355520248413, -0.7528377771377563, 0.21755358576774597, 0.525650680065155, -0.7863892316818237, -0.5493751168251038, 0.014395678415894508, -0.15973232686519623, -1.324939250946045, -0.6499322652816772, 0.4007994830608368, -0.7258242964744568, 0.5298070907592773, -0.24532000720500946, 0.24151378870010376, -1.2679685354232788, -0.32372206449508667, 0.30341532826423645, 0.5222873091697693, -0.06258147954940796, 1.2051087617874146, 0.4169637858867645, -0.9268499612808228, -0.13553208112716675, 0.27711546421051025, 0.029969703406095505, 0.1055116206407547, 0.5053423643112183, 0.1992090344429016, -0.8466600179672241, 0.6040297746658325, 0.5383124351501465, 0.46191728115081787, -0.662209689617157, 0.2424105703830719, 0.817692220211029, -0.6154387593269348, 0.20687484741210938, 1.2461069822311401, -0.15951748192310333, -1.2189995050430298, -0.24100331962108612, -1.0119928121566772, -0.5001356601715088, -0.8244861364364624, 0.5976880788803101, 0.09621066600084305, -0.03812219947576523, -0.2753543257713318, -0.30537569522857666, 0.4520474076271057, 0.08355896919965744, -0.2543190121650696, 0.5248273015022278, -0.06723002344369888, -0.8029131293296814, 0.22017401456832886, 1.109690546989441, -0.6524333953857422, -0.6579425930976868, -0.3464586138725281, -0.45737382769584656, 0.07311250269412994, -0.005463994108140469, -0.25313204526901245, -0.9523579478263855, 0.9810242056846619, -0.015261047519743443, 0.23847250640392303, 0.1287040412425995, -0.27877452969551086, 0.12741096317768097, 0.3800446689128876, -0.05487646907567978, -0.6553198099136353, -0.2697126865386963, 1.3179869651794434, 1.1963282823562622, -1.1831759214401245, 0.08889638632535934, -0.4260743260383606, -0.5176452994346619, 1.1932390928268433, 0.6604792475700378, 0.13966098427772522, 0.5471403002738953, -0.22945798933506012, -0.3505479693412781, 0.1663646101951599, -1.1061716079711914, -0.2648315131664276, 0.890485405921936, 0.857807993888855, 1.1662988662719727, -0.014197123236954212, 0.06332818418741226, 0.7516226172447205, 0.17910100519657135, -0.20855712890625, 0.7692694664001465, 0.3565937578678131, -0.14615046977996826, -0.1633443385362625, -0.08882755041122437, 0.48921093344688416, -0.6201212406158447, -0.9363549947738647, 0.3761986792087555, 0.33466213941574097, 0.842466413974762, 0.7969882488250732, 0.5639500617980957, 0.39073798060417175, 0.1310908943414688, 0.34550970792770386, 0.4963221549987793, -0.994732141494751, -0.5153254270553589, -0.4165564477443695, -0.5345436930656433, 0.03407816216349602, -0.03647231683135033, -0.5484139323234558, -0.7107963562011719, -0.28735998272895813, 0.4684998691082001, 0.1361703872680664, 0.3955836594104767, 1.1161363124847412, 0.42640912532806396, 0.7398706078529358, -0.36752405762672424, -0.36941084265708923, -0.18442079424858093, -0.9276041984558105, 0.18033941090106964, -0.10177859663963318, -0.45083117485046387, 0.0025287745520472527, -0.1668267399072647, 0.47472822666168213]}, "authors": [{"authorId": "49416727", "name": "Yue Wang"}, {"authorId": "2064728738", "name": "Hung Le"}, {"authorId": "144049726", "name": "Akhilesh Deepak Gotmare"}, {"authorId": "26910508", "name": "Nghi D. Q. Bui"}, {"authorId": "49299019", "name": "Junnan Li"}, {"authorId": "2184854289", "name": "Steven C. H. Hoi"}], "references": [{"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "886e0962479ec6dac563666399ca4c96a468fcaa", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "fbdd496c421e050a47c4fb2e0019635d2f4b97e7", "title": "Meet in the Middle: A New Pre-training Paradigm"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "876eb375cb7b365475040046df669c039ad54202", "title": "CodeT: Code Generation with Generated Tests"}, {"paperId": "6d994b4f5a46cd14e8f09f1e9e49120546b15e31", "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "45263786d07f5751f7494fdeee3c8764836d02c4", "title": "NatGen: generative pre-training by \u201cnaturalizing\u201d source code"}, {"paperId": "0121c151c96b32cd7851e4bcda2a468b279c1e6f", "title": "CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "4b27f18bff43d605805c92696a979714ced0b805", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "a2dffcaf0aac23101f067b710f6b65753b1d5956", "title": "SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations"}, {"paperId": "55a19318cc93714802c7ac59e07651789749b20c", "title": "VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "c56aced0f0c5cfebefadb530cb08d736c3ac5c05", "title": "Retrieval Augmented Code Generation and Summarization"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "14b104d1a1c0b784e4e74454d809455cc47d0093", "title": "SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "0c21334be0228431d619a180c809b43be0065bdd", "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering"}, {"paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f", "title": "Measuring Coding Challenge Competence With APPS"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c", "title": "Unified Pre-training for Program Understanding and Generation"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563", "title": "Recipes for Safety in Open-domain Chatbots"}, {"paperId": "4083958684292f6fa2f5c7fd4f9be975e80145b6", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow"}, {"paperId": "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6", "title": "GeDi: Generative Discriminator Guided Sequence Generation"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "8ee2351221b72fca5eef4c42147ed67071903d93", "title": "IntelliCode compose: code generation using transformer"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "fbe25e4f069a19dc63daca27b7c98cff338663b9", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2cbb8de53759e75411bc528518947a3094fbce3a", "title": "Billion-Scale Similarity Search with GPUs"}, {"paperId": "62e176977d439aac2e2d7eca834a7a99016dfcaf", "title": "Probabilistic model for code with decision trees"}, {"paperId": "9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c", "title": "Towards a Big Data Curated Benchmark of Inter-project Code Clones"}, {"paperId": "110b7719f16eae8d82af0f56fca6e802c842c181", "title": "Mining source code repositories at massive scale using language modeling"}, {"paperId": "443516aeb2819d4d362ffe7d5418a54e5427a016", "title": "ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"}, {"paperId": "2cc134293669b20dce3d55a67d08fea665745e7b", "title": "Combining Parameter-efficient Modules for Task-level Generalisation"}, {"paperId": null, "title": "Code alpaca: An instruction-following llama model for code generation"}, {"paperId": "b75f12dc512e5abf59071e718da392cf4acdda0d", "title": "Learning from Self-Sampled Correct and Partially-Correct Programs"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": null, "title": "Ml-enhanced code completion improves developer productivity"}, {"paperId": null, "title": "Scaling language modeling with pathways"}, {"paperId": null, "title": "After retrieving these top-k relevant code samples, we combine them with a special token [SEP] and concatenate it to the end of the source input xi"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "pre-trained model for code generation"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}