{"paperId": "b0cd93e95fb6885db47d755a4c631158b0198047", "abstract": "We introduce a very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using DExTra, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on machine translation and language modeling tasks show that DeLighT matches the performance of baseline Transformers with significantly fewer parameters. On the WMT'14 En-Fr high resource dataset, DeLighT requires 1.8 times fewer parameters and 2 times fewer operations and achieves better performance (+0.4 BLEU score) than baseline transformers. On the WMT'16 En-Ro low resource dataset, DeLighT delivers similar performance with 2.8 times fewer parameters than baseline transformers.", "venue": "arXiv.org", "year": 2020, "citationCount": 30, "influentialCitationCount": 6, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A very deep and light-weight transformer, DeLighT, that delivers similar or better performance than transformer-based models with significantly fewer parameters, and that matches the performance of baseline Transformers with significantly less parameters."}, "embedding": {"model": "specter_v2", "vector": [0.3178653120994568, 0.6784766316413879, -0.8832939267158508, 0.1534292846918106, -0.12936995923519135, -0.3994220495223999, 0.8471550345420837, -0.5211718082427979, -0.3571474552154541, -0.32652902603149414, 0.6034850478172302, -0.6896403431892395, 0.3415129482746124, 0.17812246084213257, -0.2829568684101105, 0.1831844598054886, -0.491850882768631, 0.7308338284492493, -0.18541434407234192, -0.617293655872345, -0.37646669149398804, -0.3700374662876129, -0.19308452308177948, -0.07348371297121048, 0.6086378693580627, 0.5137888789176941, 0.28548088669776917, 0.5442963242530823, -0.532870352268219, 0.5627498030662537, 0.8223544359207153, -0.45998916029930115, 0.3126264810562134, -0.06287754327058792, -0.246510848402977, -0.015775997191667557, 0.14537374675273895, -0.5978749990463257, -0.32428574562072754, 0.7355253100395203, -0.32675451040267944, 0.07648360729217529, 0.3195205628871918, -0.8651066422462463, -0.4884251058101654, 1.0723400115966797, 0.3476599454879761, 0.5652484893798828, -0.5132284760475159, -0.48566222190856934, 1.1544297933578491, -1.55269193649292, 0.1243058443069458, 1.3873488903045654, 0.689837634563446, 0.3191617429256439, -0.353403776884079, -0.7573352456092834, 0.20604054629802704, -0.11999935656785965, -0.6018029451370239, -0.5207282304763794, -0.36747971177101135, 0.004229540005326271, 1.8261052370071411, -0.46739697456359863, 0.04249578341841698, 0.5030242204666138, 0.0632283017039299, 1.160398006439209, 0.04632653668522835, -0.44976142048835754, -0.41591835021972656, -0.1026085838675499, -0.1941850632429123, 0.8902261853218079, -0.25070202350616455, 0.12700864672660828, -0.8493476510047913, 0.048324812203645706, 0.46560510993003845, 0.0323493517935276, 0.09227248281240463, -0.5853724479675293, -0.5209888219833374, 0.6523431539535522, 0.40820184350013733, 1.1146290302276611, -0.24850361049175262, 0.8724460601806641, 0.5230125784873962, 0.5592232942581177, 0.1459469050168991, 0.314887136220932, -0.4510221779346466, 0.34850001335144043, -1.05547034740448, -0.12961646914482117, -0.17718321084976196, 0.8750590682029724, -0.2560243010520935, 0.26109519600868225, -0.7010001540184021, 0.23873543739318848, 1.3370836973190308, 0.4310678541660309, 0.4258711338043213, -0.3752906024456024, 0.5628478527069092, -0.6446192264556885, -0.21665631234645844, -0.4077218770980835, -0.15830422937870026, -0.4790830910205841, -0.8033055067062378, -1.3900673389434814, -0.5107824206352234, 0.481479674577713, -0.9850473403930664, 0.7593728303909302, -0.366697758436203, 0.2254757285118103, 0.07135184854269028, 0.10487392544746399, 0.6560856699943542, 0.5941884517669678, 0.013096824288368225, 0.024709070101380348, 0.8503435254096985, -1.2118700742721558, -0.9379429221153259, -1.1297279596328735, 0.7173875570297241, -0.6051722168922424, 0.2785790264606476, -0.2734640836715698, -1.248883605003357, -0.644273042678833, -0.7377015948295593, -0.20042145252227783, -0.41490402817726135, 0.3240063488483429, 0.8643626570701599, 0.5595016479492188, -1.0788850784301758, 0.5130213499069214, -0.38213998079299927, -0.16559018194675446, 0.27598923444747925, 0.2906830906867981, 0.23042821884155273, -0.37671515345573425, -1.381157636642456, 0.6867182850837708, 0.08596337586641312, -0.3495393991470337, -0.2813495099544525, -0.9343807101249695, -1.120384693145752, -0.04745841771364212, 0.22169071435928345, -0.7586280107498169, 1.178823709487915, 0.259919673204422, -1.4495432376861572, 0.4411896765232086, -0.2723035514354706, 0.06655146181583405, 0.4028690457344055, -0.029582522809505463, -0.3919692635536194, -0.5362645387649536, -0.1652427613735199, 0.34859442710876465, 0.5494039058685303, 0.27192142605781555, -0.020255226641893387, 0.3448919951915741, -0.18994638323783875, 0.0781455710530281, -0.2835409343242645, 1.136048436164856, -0.47487306594848633, -0.5616392493247986, 0.3907996714115143, 0.3805656433105469, -0.011029049754142761, -0.056045155972242355, -0.33608901500701904, -0.6943414807319641, 0.8455819487571716, -0.24053215980529785, 0.7758707404136658, -0.8830865621566772, -0.22658765316009521, -0.16865995526313782, 0.038056954741477966, -0.2331521213054657, -0.5924615263938904, 0.7039917707443237, -0.6396434903144836, 0.2593024969100952, 0.07378924638032913, -0.9414494037628174, 0.5376145839691162, -0.2530130445957184, -0.8723722100257874, 0.061904799193143845, -0.14562994241714478, 1.1687490940093994, -0.5013746023178101, 0.28492337465286255, 0.14292287826538086, 0.7216012477874756, -0.8917120099067688, 0.8711146712303162, -0.058422304689884186, 0.13203410804271698, 0.07951568812131882, -0.2407955527305603, 0.14101651310920715, -0.3422743082046509, 0.4582558274269104, -0.8581433892250061, 0.20320138335227966, 0.525536298751831, -0.22259119153022766, 1.7135709524154663, -0.8420619964599609, 0.5772385001182556, 0.04580533131957054, -0.5897408723831177, 0.30728521943092346, 0.26579761505126953, -0.08596715331077576, -0.3544602692127228, 0.46546468138694763, 0.6945708394050598, -0.5191864967346191, 0.8140783309936523, 0.5232963562011719, 0.4920441508293152, -0.18965309858322144, 0.2574833333492279, 0.4675237536430359, -0.4102715253829956, 0.4245014786720276, 0.41770899295806885, 0.706758439540863, 0.38743236660957336, 0.29159241914749146, -0.2670173943042755, 0.3769645690917969, -0.9352234601974487, -0.04537906125187874, 0.21755866706371307, 0.6573872566223145, 0.4934524595737457, 0.24528898298740387, -0.5220884084701538, -0.4060558080673218, -0.4291927218437195, 0.7328187227249146, 1.5444536209106445, -0.21439611911773682, -0.37685632705688477, -0.5939670205116272, -0.3255975842475891, -0.4643847644329071, 0.11009301245212555, -0.17547689378261566, -0.34133949875831604, -0.6004528403282166, -1.0591543912887573, 0.9466539025306702, 0.20772360265254974, 1.2242164611816406, 0.032769329845905304, -0.04785432666540146, -0.31408315896987915, -0.23426876962184906, -1.2552590370178223, -0.7973889708518982, 0.40006646513938904, -0.9194533824920654, -0.01727278344333172, 0.10250277817249298, -0.20673291385173798, 0.13413500785827637, -0.6085721254348755, 0.8200392723083496, -0.7237344980239868, -0.050028782337903976, -0.3831401765346527, 0.4731561243534088, -0.14194244146347046, -1.0086156129837036, 0.19959419965744019, 0.27126824855804443, -0.2718275487422943, 0.17475207149982452, 0.4242061972618103, 0.24433697760105133, 0.5555715560913086, -0.39720070362091064, 0.31344321370124817, 0.2563386559486389, -0.009353095665574074, 0.44871193170547485, 0.048726391047239304, -0.36660924553871155, -0.9367698431015015, 0.9120469689369202, 0.38756218552589417, -0.38408029079437256, 0.3682869076728821, -0.7564266920089722, -0.17505420744419098, 0.4348457157611847, -0.5043298006057739, 0.0272919163107872, -1.1763627529144287, 0.2148410975933075, -0.27814629673957825, 0.3410559296607971, -0.003195968922227621, 0.14997774362564087, 0.24863052368164062, 0.07999926060438156, 0.6375094652175903, 0.42521145939826965, -0.34290385246276855, 0.9261999726295471, -0.7077610492706299, 0.052456747740507126, 0.4645027220249176, 0.77683025598526, -0.20102286338806152, -0.12069666385650635, -0.17088249325752258, -0.1582517772912979, 0.11004434525966644, -0.11129748821258545, 0.09200893342494965, 0.25702571868896484, -0.844352662563324, -0.39586177468299866, -0.02071492187678814, -0.8954839706420898, -0.2513295114040375, 0.08365187048912048, -0.14083248376846313, -0.348991334438324, -1.1576076745986938, -1.4248251914978027, 0.13986404240131378, -0.8307298421859741, -1.591409683227539, 0.46426060795783997, -0.430368036031723, -0.21407471597194672, -0.5950866937637329, -0.36705076694488525, -0.09264018386602402, 1.1894620656967163, -0.956151008605957, 0.9827432036399841, -0.1445053517818451, -0.2258606255054474, -0.16334907710552216, 0.020663052797317505, 0.5409944653511047, 0.04554971307516098, 0.1606803685426712, -0.5946652889251709, -0.11301050335168839, -0.2750728130340576, 0.11705224961042404, -0.058800555765628815, 0.3233445882797241, 0.3445214629173279, 0.12253327667713165, -0.5297874808311462, 0.7299248576164246, 1.0239638090133667, -0.8412932753562927, 0.03493518754839897, 0.5089345574378967, 1.0111069679260254, -0.21196594834327698, -0.21449820697307587, 0.31135255098342896, 0.35659608244895935, 0.47437500953674316, 0.19130757451057434, -0.2514435052871704, -0.39780768752098083, -0.7070989012718201, 0.6748980283737183, 1.5787135362625122, 0.20329077541828156, -0.11587824672460556, -1.002976417541504, 0.35906606912612915, -0.9123125672340393, -0.486305296421051, 0.7520884275436401, 0.5584009885787964, 0.4498118460178375, -0.20251977443695068, -0.5409510731697083, 9.465045877732337e-05, 0.6182800531387329, 0.5074864625930786, -0.24147270619869232, -0.9239738583564758, 0.14581437408924103, 0.5913564562797546, 0.6701905727386475, 0.5346542596817017, 0.11636967957019806, 0.8518193960189819, 14.982481002807617, 0.9710938334465027, -0.0635712593793869, 1.0181872844696045, 0.43649792671203613, -0.0699455589056015, -0.4696020185947418, -0.06726805120706558, -1.1335986852645874, -0.08455504477024078, 1.016958475112915, 0.15457414090633392, 0.8419486284255981, -0.0008382215746678412, 0.274482786655426, 0.4474024772644043, -0.4488515555858612, 0.6983999609947205, 0.4527471363544464, -1.374024510383606, 0.5175773501396179, 0.43450868129730225, 0.06151868775486946, 0.613275945186615, 0.8357322812080383, 0.7298867106437683, 0.6359319686889648, -0.7022508382797241, 0.42557474970817566, -0.18357880413532257, 1.0925123691558838, -0.1685543656349182, 0.32672083377838135, 0.37774530053138733, -1.125678300857544, -0.23400145769119263, -0.39474454522132874, -0.9580956697463989, 0.09140084683895111, 0.4872305691242218, -0.74583500623703, -0.5005605220794678, -0.08585815131664276, 1.10585618019104, 0.30757755041122437, -0.023280246183276176, -0.4446139931678772, 0.6924858093261719, -0.3761199414730072, 0.464443564414978, 0.26972532272338867, 0.2646615207195282, 0.11458325386047363, -0.3286879062652588, 0.4564862847328186, -0.6420530676841736, -0.06415160000324249, 0.32605424523353577, -0.7576152682304382, -0.2427944391965866, -0.3423442244529724, -0.2963283658027649, 0.17897839844226837, 0.7890289425849915, 0.2747914493083954, 0.5108455419540405, -0.386029988527298, 0.47532814741134644, 0.22509999573230743, 0.23132310807704926, -0.9226198196411133, 0.11545242369174957, 0.5012925863265991, -0.25283631682395935, -0.01930410973727703, 0.22914989292621613, -0.050403524190187454, -0.6084131002426147, -0.8144803643226624, -0.6391087770462036, 0.12036404013633728, -0.7728226780891418, -0.1227232813835144, 1.0009969472885132, -0.42332664132118225, -0.608392059803009, 0.5308347940444946, -0.6126438975334167, 0.20623086392879486, 0.5650519132614136, -1.5553380250930786, -1.1237356662750244, 0.5684451460838318, -0.24722805619239807, -0.12959568202495575, -0.3165794610977173, 0.8589851260185242, 0.4486638009548187, -0.34752750396728516, 0.12225784361362457, 0.23939071595668793, 0.030445586889982224, 0.03158678114414215, -0.45262330770492554, 1.2321125268936157, 0.329387366771698, 0.03919414058327675, 0.37992408871650696, 0.31128016114234924, 0.4264700710773468, -1.0864641666412354, -0.14071010053157806, 0.8428952097892761, -0.7324382066726685, 0.11957606673240662, -0.8915119171142578, -0.20956449210643768, 0.5675743818283081, 0.7255532145500183, -0.20407257974147797, 0.4569379985332489, -0.12285540252923965, -0.7320279479026794, -0.33629313111305237, -0.9837887287139893, 0.06483689695596695, 0.7905973792076111, -0.8833890557289124, -0.24535274505615234, 0.0689409077167511, 0.5035243630409241, -0.9655914306640625, -0.7097394466400146, 0.08661233633756638, 0.0566486157476902, 0.1639830619096756, 1.1883113384246826, -0.1663808375597, 0.6508031487464905, 0.7704796195030212, -0.3273327052593231, -1.2373310327529907, -0.05786249414086342, -1.0336822271347046, 0.11747480928897858, -0.10729945451021194, 0.7384842038154602, -0.6681495308876038, 0.16565686464309692, 0.41134366393089294, 0.42865610122680664, -0.2943148612976074, -0.6435200572013855, -0.5508272051811218, 0.1052686795592308, -0.3356354236602783, 0.671764075756073, -0.01095929928123951, -0.1905873715877533, 0.2379954755306244, 0.36521539092063904, 0.36002317070961, -0.09606428444385529, -0.8737905621528625, 0.36360231041908264, 0.057409174740314484, -0.14897401630878448, -0.5947988033294678, -0.459006130695343, -1.5937505960464478, 0.06789636611938477, -1.324095606803894, -0.13145041465759277, -1.3591240644454956, -0.44975435733795166, 0.020616937428712845, -0.07925303280353546, 0.31575801968574524, 0.7148622870445251, 0.31026968359947205, -0.25753599405288696, -0.37909871339797974, 0.06622669845819473, 0.918012261390686, 1.1077321767807007, -0.981904923915863, 0.43573197722435, -0.25085926055908203, 0.025696266442537308, 0.280446857213974, 0.24491442739963531, -0.26785194873809814, -1.247092843055725, -1.6092650890350342, 0.5848673582077026, -0.3429845869541168, -0.2949015498161316, -0.3758010268211365, 0.47844234108924866, 0.27564528584480286, -0.24871666729450226, 0.20743043720722198, 0.29555168747901917, -1.1537772417068481, -0.22676987946033478, 0.6955427527427673, -0.4958030879497528, 0.5258764028549194, 0.19002611935138702, -0.8045312166213989, -0.1650296300649643, 0.9011458158493042, 0.13878022134304047, -0.8607839941978455, -0.48981937766075134, 0.47072723507881165, -0.7797147035598755, -0.07498930394649506, -0.128597229719162, -0.28912025690078735, -1.0654938220977783, -0.4003278613090515, -0.1807391196489334, 0.25644081830978394, -0.33923038840293884, 1.018528938293457, 0.14776478707790375, -1.164819359779358, 0.08065491169691086, 0.42419445514678955, -0.3960297703742981, -0.3554845452308655, -0.18079107999801636, 0.6884719133377075, -0.7383967638015747, 0.476455956697464, 0.2346978336572647, 0.39615198969841003, -0.8077400922775269, -0.28297722339630127, 0.7964938879013062, -0.7319903373718262, -0.2726767361164093, 1.062108039855957, -0.409422904253006, -1.3973751068115234, 0.010145360603928566, -1.361937403678894, -0.2088516801595688, -0.352802038192749, 0.4933507442474365, 0.1549014300107956, 0.15218228101730347, -0.07597329467535019, -0.7639766335487366, 0.29592880606651306, -0.03946185111999512, -0.5634236335754395, 0.5972679257392883, -0.40940991044044495, -0.7699993252754211, 0.23242221772670746, 0.6430785655975342, -0.6411713361740112, -0.2750684916973114, -0.8540851473808289, -0.5545112490653992, -0.12337895482778549, 0.6387120485305786, -0.6219649314880371, -0.9651556611061096, 0.7007127404212952, 0.38504451513290405, 0.0814700797200203, 0.41924551129341125, -0.3378223180770874, 0.4023202359676361, 0.7192702889442444, 0.23471693694591522, -0.4840262234210968, -0.7533576488494873, 1.368973731994629, 0.9815062880516052, -0.6180720925331116, 0.4429057538509369, -0.4621101915836334, -0.5303773880004883, 0.6470691561698914, -0.10219530761241913, 0.35226893424987793, 0.9339246153831482, 0.22997768223285675, 0.052002858370542526, 0.4940224289894104, -0.7777174115180969, -0.12987612187862396, 0.6521391868591309, 0.8199398517608643, 0.9249053001403809, 0.10256924480199814, 0.13688889145851135, 0.41271737217903137, 0.06540243327617645, 0.19571679830551147, 0.48500317335128784, 0.35485392808914185, -0.12866699695587158, -0.2562685012817383, 0.011520477011799812, 0.5077657699584961, -0.6117534637451172, -0.9529939889907837, 0.15937109291553497, 0.7012509703636169, 0.044593922793865204, 0.7208651304244995, 0.6440863013267517, -0.020471833646297455, 0.5516883730888367, 0.018778283149003983, 0.720499575138092, -0.5178630352020264, -0.30271878838539124, -0.10419101268053055, -0.7118899822235107, -0.10671761631965637, 0.06589975953102112, -0.1637376844882965, -0.5212599635124207, -0.7962584495544434, 0.14741389453411102, 0.043888386338949203, 0.6341211199760437, 1.057127833366394, 0.3634813725948334, 0.6880609393119812, -0.05226486921310425, -0.2996666729450226, -0.3611133396625519, -0.9829376339912415, -0.06173578277230263, -0.846859335899353, -0.39692315459251404, -0.2516481280326843, -0.02098841406404972, -0.43505269289016724]}, "authors": [{"authorId": "151493135", "name": "Sachin Mehta"}, {"authorId": "2320509", "name": "Marjan Ghazvininejad"}, {"authorId": "1900163", "name": "Srini Iyer"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}], "references": [{"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "57f123c95ecf9d901be3a53291f53302740451e2", "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "841d43cf4015042a4ee45745c5b6f2c59c184da5", "title": "DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "112fd54ee193237b24f2ce7fce79e399609a29c5", "title": "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives"}, {"paperId": "1fe62a928bf5cfac0f373728f3a4de3cefe0951d", "title": "On Identifiability in Transformers"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "5efadc9019ce3378a0eb6c8f939cdde6c8918b1e", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "054aa8d6d5ceb543d3f0d89b578744fd89ef3230", "title": "Pyramidal Recurrent Unit for Language Modeling"}, {"paperId": "bd8bf10edff7d2224e4eecbd63038c9e3c531590", "title": "GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking"}, {"paperId": "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "title": "An Analysis of Neural Language Modeling at Multiple Scales"}, {"paperId": "642c1b4a9da95ea4239708afc5929a5007a1870d", "title": "Tensor2Tensor for Neural Machine Translation"}, {"paperId": "9c5c89199114858eafbe50b46d77d38ffd03b28a", "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement"}, {"paperId": "58c6f890a1ae372958b7decf56132fe258152722", "title": "Regularizing and Optimizing LSTM Language Models"}, {"paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89", "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache"}, {"paperId": "424aef7340ee618132cc3314669400e23ad910ba", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "title": "Efficient softmax approximation for GPUs"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca", "title": "Regularization of Neural Networks using DropConnect"}, {"paperId": "0060745e006c5f14ec326904119dca19c6545e51", "title": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "94238dead40b12735d79ed63e29ead70730261a2", "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation"}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": null, "title": "a) Block-wise scaling (R4, R5) delivers better performance compared to uniform scaling (R1-R3)"}]}