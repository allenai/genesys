{"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "abstract": "Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.", "venue": "International Conference on Machine Learning", "year": 2021, "citationCount": 22, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head that accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks."}, "embedding": {"model": "specter_v2", "vector": [0.22738517820835114, 0.6476061344146729, -0.2542117238044739, -0.011673937551677227, -0.5315880179405212, -0.2746281921863556, 0.6647890210151672, -0.3908383548259735, -0.4755782186985016, -0.510758101940155, 1.0193411111831665, 0.06319254636764526, 0.4722777009010315, 0.3694952428340912, -0.31555527448654175, 0.048329126089811325, -0.7827721238136292, 0.39783769845962524, 0.03759435564279556, -0.2628820240497589, -0.1883639544248581, -0.9362697601318359, -0.7349492907524109, -0.09538333117961884, 0.23207323253154755, 0.42753341794013977, 0.3099246323108673, 0.8652533292770386, -0.641211748123169, 0.3320957124233246, 0.5182673931121826, -0.6805177927017212, 0.2695007920265198, 0.23108790814876556, -0.2602091431617737, -0.15518462657928467, 0.5575301647186279, -0.496737003326416, -0.7515987157821655, 0.9182846546173096, -0.04869215190410614, 0.30518874526023865, 0.6491252183914185, -0.7419321537017822, -0.48785436153411865, 1.3254748582839966, 0.6449776887893677, 0.5849182605743408, -0.377341628074646, -0.5676044821739197, 1.5178649425506592, -1.5224131345748901, 0.02308204583823681, 1.681806206703186, 0.43551918864250183, 0.31744733452796936, -0.37097111344337463, -0.8696770668029785, 1.045142650604248, 0.44221749901771545, -0.8075317740440369, -0.4240465462207794, 0.2502513825893402, -0.1302643120288849, 1.9034662246704102, -0.3865443468093872, 0.11692293733358383, 0.7108773589134216, 0.02366427704691887, 1.768278956413269, -0.2562456429004669, -0.5140147805213928, -0.5340433716773987, -0.07553130388259888, 0.4791446626186371, 0.8279939293861389, -0.7430741190910339, 0.28668224811553955, -0.9498469233512878, -0.21542584896087646, -0.06532532721757889, 0.07511178404092789, -0.10455086827278137, -0.28797367215156555, -0.30499303340911865, 0.9003897905349731, 0.34114277362823486, 0.6523054838180542, -0.20150618255138397, 0.8496174812316895, 0.5689471364021301, -0.16469357907772064, 0.05824555084109306, 0.3861365020275116, -0.11082228273153305, 0.45154517889022827, -0.9739168882369995, 0.3709540069103241, 0.04352325201034546, 1.0461866855621338, -0.18421132862567902, 0.14260333776474, -0.7521076798439026, 0.02557329647243023, 1.4910629987716675, 0.29425889253616333, 0.38970381021499634, -0.7751634120941162, 0.2541643977165222, -0.8231669664382935, -0.21105769276618958, -0.8127139806747437, -0.05288761854171753, -0.10049857199192047, -0.7392211556434631, -1.2338271141052246, -0.49181538820266724, 0.4102592468261719, -0.7997464537620544, 1.12556791305542, -0.2980445325374603, 0.3220387399196625, 0.009805925190448761, -0.015831736847758293, 0.45775067806243896, 0.9127178192138672, 0.35580623149871826, 0.32364898920059204, 0.9405921101570129, -1.0790750980377197, -0.9340391159057617, -1.26994788646698, 0.7437241673469543, -0.30095386505126953, 0.2916191518306732, 0.05478062108159065, -0.8370898365974426, -0.8449931144714355, -0.6945201754570007, -0.12954293191432953, -0.5667179822921753, 0.20572713017463684, 0.6814079880714417, 0.48936405777931213, -1.1689391136169434, 0.5800508856773376, -0.0697280764579773, 0.09463968127965927, 0.5280067920684814, 0.19072005152702332, 0.04571031033992767, -0.7553079724311829, -1.4567641019821167, 0.3382176160812378, 0.4484504461288452, -0.2824654281139374, -0.24776813387870789, -0.8080964684486389, -1.4482762813568115, 0.10414137691259384, 0.4486416280269623, -0.4970734417438507, 1.1753555536270142, 0.3133057951927185, -1.3758423328399658, 0.4094831943511963, -0.4699138104915619, 0.2296672761440277, 0.17256878316402435, -0.6117016673088074, -0.0601317398250103, -1.009690284729004, 0.0063514625653624535, 0.5981678366661072, 0.5801748633384705, 0.23857642710208893, -0.1651936173439026, -0.03658337518572807, -0.36651626229286194, -0.07181788235902786, -0.4299650490283966, 1.0171762704849243, -0.425847589969635, -0.3549392521381378, 0.027653485536575317, 0.48551562428474426, 0.016916094347834587, -0.43141186237335205, -0.7133417725563049, -1.4312806129455566, 1.1125292778015137, -0.052047308534383774, 0.7485001683235168, -0.9280968904495239, -0.41235268115997314, -0.5627328157424927, 0.13636498153209686, -0.15900245308876038, -0.8606513142585754, 0.719267725944519, -0.4760104715824127, 0.3225806653499603, -0.008123149164021015, -1.5857410430908203, 0.4642888903617859, -0.2523019015789032, -0.7188763618469238, -0.13421232998371124, 0.11369744688272476, 1.2831082344055176, -1.0015867948532104, -0.06481245905160904, 0.1618778258562088, 0.27929815649986267, -0.7928963899612427, 1.1557250022888184, -0.2821769714355469, -0.31029433012008667, 0.15902046859264374, -0.31344667077064514, 0.006235324777662754, -0.25198790431022644, 0.33029502630233765, -0.6351388692855835, 0.2790556252002716, 0.6260263323783875, -0.31619203090667725, 1.3259238004684448, -0.2763136029243469, 0.5620017647743225, -0.16426290571689606, -0.6744962930679321, -0.059688858687877655, 0.4412749409675598, -0.43425920605659485, -0.2546809911727905, 0.15576119720935822, 0.14810405671596527, -0.7070004343986511, 0.2673491835594177, 0.5883758664131165, 0.6565170288085938, -0.2731364071369171, -0.021141299977898598, 0.650542140007019, -0.2192717343568802, 0.2205674946308136, 0.6745772361755371, 0.9160439968109131, 0.4190732538700104, 0.6368541121482849, -0.05356572940945625, 0.1473490595817566, -1.1076072454452515, -0.08453848958015442, 0.3333241939544678, 0.6963916420936584, 0.5931091904640198, 0.2734017074108124, -0.6630082130432129, -0.4440106153488159, 0.08781929314136505, 0.94625324010849, 1.7685251235961914, -0.47354254126548767, -0.442219614982605, -0.40780898928642273, -0.5119332075119019, -0.34988468885421753, 0.11801481992006302, -0.6995514631271362, -0.2994860112667084, -0.5129732489585876, -0.9825767278671265, 0.7495672106742859, 0.4952871799468994, 0.7510498762130737, -0.5898585915565491, 0.004722345620393753, -0.10488799959421158, 0.11038597673177719, -0.9261800050735474, -1.019014835357666, 0.6024183630943298, -0.23027649521827698, -0.06847569346427917, -0.014399160631000996, 0.09470287710428238, -0.01783563941717148, -0.823403000831604, 0.8247935771942139, -1.2358402013778687, 0.014601041562855244, 0.09834151715040207, 0.45904722809791565, -0.6634666919708252, -0.7161743640899658, 0.2912839949131012, 0.20489946007728577, -0.23710736632347107, 0.515164852142334, 0.45608100295066833, 0.32926997542381287, -0.23071539402008057, -0.29072126746177673, 0.15883351862430573, 0.33207014203071594, 0.14451119303703308, 0.5215116143226624, -0.28692910075187683, -0.2849615514278412, -1.3558084964752197, 0.7663870453834534, 0.21088889241218567, -0.4939437806606293, 0.11293040961027145, -0.4992534816265106, -0.2544954717159271, 0.570364773273468, -0.6895852088928223, -0.09705569595098495, -0.5116617679595947, 0.3624699115753174, -0.7172589898109436, -0.1267700046300888, 0.2964622378349304, -0.12676039338111877, 0.5804073214530945, 0.05832969397306442, 0.7888506054878235, 0.21586912870407104, 0.06625165045261383, 0.9639690518379211, -1.0526915788650513, 0.6313122510910034, 0.3962683081626892, 0.26004698872566223, -0.3410901129245758, -0.21273183822631836, -0.7784408926963806, -0.3861427307128906, -0.7629369497299194, -0.1583976298570633, -0.14137844741344452, 0.17883522808551788, -0.3557283282279968, -0.7880600094795227, 0.2702919542789459, -1.4045244455337524, 0.06712432205677032, 0.24973544478416443, -0.3174302279949188, -0.08887012302875519, -0.9203578233718872, -1.10580575466156, -0.5243381857872009, -0.6963078379631042, -1.0694605112075806, 0.5372245907783508, 0.0047821830958127975, -0.35346221923828125, -0.6532824635505676, 0.08282119780778885, -0.3102332353591919, 1.1811524629592896, -0.7731346487998962, 0.8015730977058411, -0.378487229347229, -0.35710737109184265, -0.47751858830451965, 0.2660295069217682, 0.7095577716827393, 0.04605220630764961, 0.22885943949222565, -1.223009467124939, 0.32292312383651733, -0.35331985354423523, -0.17338907718658447, 0.3396998941898346, 0.5471359491348267, 0.45455554127693176, -0.4542844295501709, -0.8446961641311646, 0.28805428743362427, 1.1900635957717896, -0.5941770076751709, 0.06916320323944092, 0.11744794994592667, 1.3203387260437012, 0.2074461430311203, -0.4525696337223053, 0.40643784403800964, 0.6075090169906616, 0.35483500361442566, 0.39001092314720154, 0.3386291265487671, 0.05486111342906952, -0.5625064373016357, 0.803494393825531, 1.5909560918807983, 0.5968746542930603, -0.19383858144283295, -1.0910823345184326, 0.8837688565254211, -1.2506917715072632, -0.8625732660293579, 0.7408754229545593, 0.40393367409706116, 0.3591982424259186, -0.5806867480278015, -0.23190857470035553, -0.38036975264549255, 0.5477773547172546, 0.5476449131965637, -0.5484873056411743, -0.25254058837890625, 0.10836419463157654, 0.37107208371162415, 0.15082116425037384, 0.5967273712158203, -0.7532040476799011, 0.7227578163146973, 14.626323699951172, 0.8002033829689026, -0.04395260661840439, 0.6755185723304749, 0.39893797039985657, 0.19010092318058014, -0.32499632239341736, -0.07649853825569153, -1.613109827041626, 0.07846798002719879, 0.9043238162994385, 0.14090611040592194, 0.32410454750061035, 0.3302871882915497, -0.24138090014457703, 0.5412642359733582, -0.768912672996521, 0.7512915730476379, 0.7110425233840942, -1.209809422492981, 0.34347012639045715, 0.33390524983406067, 0.01995215006172657, 0.5998674631118774, 0.8447410464286804, 0.8892987370491028, 0.8657905459403992, -0.42339256405830383, 0.5087727904319763, 0.3059145212173462, 0.5724253058433533, 0.17635877430438995, 0.4529237747192383, 0.5767701864242554, -1.098204493522644, -0.21583569049835205, -0.4267992377281189, -0.8969083428382874, 0.37652838230133057, 0.28089454770088196, -0.6970841884613037, -0.420769602060318, 0.030446773394942284, 0.9763568639755249, 0.06394106894731522, 0.4163913130760193, -0.2417236715555191, 0.6129453778266907, -0.4039570093154907, 0.02259342186152935, 0.5186208486557007, 0.47403159737586975, 0.27038702368736267, 0.2721814215183258, 0.10808172821998596, -0.2673683166503906, 0.2898924946784973, 0.6241834759712219, -0.40384823083877563, 0.04925042390823364, -0.3825394809246063, -0.45436951518058777, 0.07099492102861404, 0.9238486289978027, 0.6414470076560974, 0.1127481460571289, -0.44813796877861023, 0.2820659875869751, 0.713516116142273, 0.1666005551815033, -0.27129343152046204, 0.03792941942811012, 0.2827959656715393, -0.5559518933296204, 0.3127892017364502, 0.7402983903884888, 0.34749725461006165, -0.30965861678123474, -0.6659097075462341, -0.26026779413223267, 0.4458673596382141, -0.5912234783172607, -1.015474796295166, 0.7772454619407654, -0.12896417081356049, -0.11045528948307037, 0.14708776772022247, -0.717089831829071, -0.22275888919830322, 0.7144656181335449, -1.2968240976333618, -0.955787181854248, 0.33276936411857605, -0.5543075799942017, -0.22250574827194214, -0.06654540449380875, 1.2417128086090088, 0.13168403506278992, -0.32799527049064636, 0.08329176902770996, -0.0037713926285505295, 0.21269595623016357, -0.0968598797917366, -1.0091347694396973, 1.1012567281723022, 0.3198779821395874, -0.13434191048145294, 0.297024130821228, 0.0768752172589302, 0.12923964858055115, -0.4966401755809784, -0.16690407693386078, 0.7362678050994873, -0.9888032078742981, -0.571499764919281, -0.7948665618896484, -0.7606789469718933, 0.6586934328079224, 0.6016927361488342, -0.23364058136940002, 0.647769033908844, 0.2752299904823303, -1.0052027702331543, 0.004187054932117462, -0.30837124586105347, -0.1960030049085617, 0.2702372372150421, -0.8141946196556091, -0.4887012839317322, -0.0708959773182869, 0.4974307715892792, -0.9454512596130371, -0.22846658527851105, -0.2569587826728821, 0.2667471468448639, 0.23864614963531494, 1.0983781814575195, -0.49895426630973816, 0.653372585773468, 0.7045661211013794, -0.27605488896369934, -0.9391463994979858, -0.3500247299671173, -0.8434738516807556, -0.22339993715286255, 0.16871728003025055, 0.4556552469730377, -0.3562197983264923, 0.4002474248409271, 0.7580938935279846, 0.25707733631134033, -0.41618552803993225, -0.567626416683197, -0.4888930916786194, -0.10640855878591537, -0.5358225703239441, 0.10685399919748306, 0.040572281926870346, -0.22312721610069275, 0.463116317987442, 0.1620018482208252, 0.4003000557422638, 0.15418492257595062, -0.9953797459602356, 0.3831081688404083, -0.2913697063922882, -0.17513197660446167, -0.6220248937606812, -0.6358425617218018, -1.3760910034179688, 0.3483187258243561, -1.2463963031768799, 0.09973330795764923, -1.072706937789917, -0.03977056220173836, 0.22824740409851074, -0.15262749791145325, 0.3868437111377716, -0.22962069511413574, -0.4410165846347809, -0.208894744515419, -0.7847481369972229, -0.5058459639549255, 1.1041699647903442, 0.533682107925415, -1.0311493873596191, 0.28921499848365784, -0.10821299999952316, -0.09634467214345932, 0.14951950311660767, 0.133037269115448, -0.6041244864463806, -0.771479606628418, -1.1860618591308594, 0.4666909873485565, -0.31781914830207825, -0.1351977437734604, -0.5334563255310059, 0.9555811285972595, 0.579659640789032, -0.29013916850090027, -0.23160874843597412, 0.6620585322380066, -0.9715650677680969, -0.3621600270271301, 0.2971336543560028, -0.7638380527496338, 0.22547906637191772, 0.21177703142166138, -0.3638400435447693, -0.34111958742141724, 0.7730753421783447, -0.06765046715736389, -1.3129897117614746, -0.7464600205421448, 0.5678661465644836, -0.5014018416404724, 0.11170938611030579, -0.2125304937362671, -0.07278630137443542, -1.1990885734558105, -0.49585282802581787, 0.06666124612092972, 0.5008983016014099, -0.41123369336128235, 0.8661791086196899, 0.6309869885444641, -1.1373049020767212, -0.0408121719956398, 0.28733739256858826, 0.11550857126712799, 0.3530869781970978, 0.7233002781867981, 0.35103723406791687, 0.09580104798078537, 0.6615750789642334, 0.49929574131965637, 0.325472354888916, -1.151956558227539, 0.27226680517196655, 0.877572238445282, -0.7203283309936523, -0.24834118783473969, 1.4272810220718384, 0.14765441417694092, -1.2409991025924683, 0.24394869804382324, -1.1724916696548462, -0.5726651549339294, -0.25403574109077454, 0.7271204590797424, 0.08225549757480621, -0.1246347650885582, -0.04236419498920441, -0.7632721066474915, 0.1547137349843979, -0.4770967662334442, -0.3259086608886719, 0.7006608247756958, 0.02689456194639206, -0.45234930515289307, 0.5967172384262085, 1.0895888805389404, -0.7408698201179504, -0.5466426014900208, -1.1093395948410034, -0.5102037787437439, 0.03509521484375, 0.35838592052459717, -0.16175499558448792, -0.4679774045944214, 0.8197317123413086, 0.2317698448896408, 0.17061205208301544, -0.0445454977452755, 0.01502396259456873, 0.23709893226623535, 0.7644203305244446, -0.20254799723625183, -0.33108770847320557, -0.5343101024627686, 1.4497190713882446, 1.2535076141357422, -0.5132686495780945, 0.0014028887962922454, -0.42495015263557434, -0.710075855255127, 0.6691234707832336, 0.2257595807313919, 0.03604546934366226, 1.0474259853363037, 0.09040545672178268, -0.07230700552463531, -0.032773442566394806, -1.071826696395874, -0.2315744310617447, 0.9544156789779663, 1.1871305704116821, 0.5617924332618713, 0.22744828462600708, 0.7629930377006531, 0.7400172352790833, 0.16405831277370453, -0.041730765253305435, 0.36179277300834656, 0.3635990023612976, -0.017465610057115555, -0.1925630122423172, 0.09617093205451965, 0.8002513647079468, -0.7660439610481262, -1.0724501609802246, 0.07826007902622223, 0.3657630383968353, 0.045678313821554184, 0.21412359178066254, 0.4764730632305145, 0.03978796675801277, 0.4701404869556427, 0.4597054123878479, 0.23471753299236298, -0.6999895572662354, 0.23392438888549805, -0.22316047549247742, -0.8598645329475403, -0.2313840240240097, -0.3832329511642456, -0.6621262431144714, 0.02505059540271759, -0.18452230095863342, 0.11543475091457367, -0.17679855227470398, 0.25150594115257263, 1.1773394346237183, 0.4888696074485779, 0.6869412064552307, -0.552523136138916, -0.5430010557174683, -0.5072760581970215, -1.2504899501800537, -0.2336619794368744, -0.6189884543418884, -0.10618124157190323, -0.18608544766902924, -0.17736521363258362, -0.20776699483394623]}, "authors": [{"authorId": "2116488139", "name": "Tam Nguyen"}, {"authorId": "150322732", "name": "T. Nguyen"}, {"authorId": "2134814752", "name": "Dung D. Le"}, {"authorId": "2170125220", "name": "Duy Khuong Nguyen"}, {"authorId": "2236167", "name": "Viet-Anh Tran"}, {"authorId": "144908066", "name": "Richard Baraniuk"}, {"authorId": "3526349", "name": "Nhat Ho"}, {"authorId": "103583159", "name": "S. Osher"}], "references": [{"paperId": "0dd163f1c10480128eeb25d77afec493c1c695eb", "title": "Transformer with Fourier Integral Attentions"}, {"paperId": "9bb59427a3e11fdc18edbe058038bf4df0fd2aff", "title": "Beyond EM Algorithm on Over-specified Two-Component Location-Scale Gaussian Mixtures"}, {"paperId": "277dd3d3d16c293d4ecc1e495dbd7f40e6706696", "title": "An Exponentially Increasing Step-size for Parameter Estimation in Statistical Models"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "b8b813111c411ae61881ab9cd25707d9de6444ec", "title": "Compositional Attention: Disentangling Search and Retrieval"}, {"paperId": "c4d8e0e72ae5bf2394041d0188f35755d6ea7456", "title": "How does momentum benefit deep neural networks architecture design? A few case studies"}, {"paperId": "fe3761961169c61e249b0845e85271e50bd3a57a", "title": "SP-GPT2: Semantics Improvement in Vietnamese Poetry Generation"}, {"paperId": "4c1d1114a15b501a90924653d263dd2222aa127e", "title": "Heavy Ball Neural Ordinary Differential Equations"}, {"paperId": "23d11338be48471b3979b13eb172ec67fc22244b", "title": "Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "26f61039857423c0aad2c44f0e7769cbb61246c9", "title": "Multi-Exit Vision Transformer for Dynamic Inference"}, {"paperId": "14b97585f136671742f6ce4151081e487b1fc1fe", "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "0c334b610b9ce71a0e3d41ed697604412e5c2aef", "title": "Momentum Residual Neural Networks"}, {"paperId": "837ac4ed6825502f0460caec45e12e734c85b113", "title": "Dynamic Neural Networks: A Survey"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "372cce47fa16c538946972e6a7ac8420e64000b0", "title": "Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "9f0272bb258506fdc0ee7d8951593914d4f9c39d", "title": "Analyzing Individual Neurons in Pre-trained Language Models"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "61325245e98920a69b40e18c069fda0c1cf00f21", "title": "MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation"}, {"paperId": "8d19137fa2d7eb68473b4f51453cb34af67b72bb", "title": "Neural Networks with Recurrent Generative Feedback"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "1c7e57ac925953dd1e48b5d175726ccf7b4f42f2", "title": "MomentumRNN: Integrating Momentum into Recurrent Neural Networks"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "731b6d6e56a8d80cc684d7574c3fd3113d958a18", "title": "Stochastically Robust Personalized Ranking for LSH Recommendation Retrieval"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "177fa110bdb2d4dc12a958be4f1cd78a6cfd750e", "title": "Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "84898960f68fa78296a102edc8ac81739f9a9408", "title": "Gaussian Transformer: A Lightweight Approach for Natural Language Inference"}, {"paperId": "53e36eb4ead1999146601db500d85faed219e8a9", "title": "Dual Dynamic Inference: Enabling More Efficient, Adaptive, and Controllable Deep Inference"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "a9d5771c96f9671e556c2bcbbd1c340b4495e00e", "title": "Sharp Analysis of Expectation-Maximization for Weakly Identifiable Models"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "374aac3e3a9e40ac80f2e74615ef4ac4dc66c17c", "title": "A Bayesian Perspective of Convolutional Neural Networks through a Deconvolutional Generative Model"}, {"paperId": "6aa8ca81c755843bb814f01b4f2a25fc8f3b781d", "title": "EnergyNet: Energy-Efficient Dynamic Inference"}, {"paperId": "9634946080bcc7c5b5e8cf09aa5e79d5d8751dcc", "title": "Optimization Algorithm Inspired Deep Neural Network Structure Design"}, {"paperId": "5e26191eb389546a67d3c80c360b8f5bfffe464c", "title": "Singularity, misspecification and the convergence rate of EM"}, {"paperId": "512b8ef0002e0bfd0ecb5ab17d533c1762eb9786", "title": "Set Transformer"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "f37ea0b173dd0403a5028c12746082d31dff60bb", "title": "SkipNet: Learning Dynamic Routing in Convolutional Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "a2d407962bb1f5fcd209114f5687d4c11bf9dfad", "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "b341e2bfc451b73927ee1b5f270d216b86fba349", "title": "A Probabilistic Framework for Deep Learning"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "abbbbb325037052e9a0983c40d43792b6cc61163", "title": "A Probabilistic Theory of Deep Learning"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91", "title": "On the importance of initialization and momentum in deep learning"}, {"paperId": "ded103d0613e1a8f51f586cc1678aee3ff26e811", "title": "Advances in optimizing recurrent networks"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e2b3ce2e0d66238fba443abece60641f98036114", "title": "Approximation of probability distributions by convex mixtures of Gaussian measures"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "09ba4b8f612f8593280ce3862d88890de9c3846b", "title": "Pattern Recognition"}, {"paperId": "52413b3078a2c877ec588e39726f539038eee1c5", "title": "Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities"}, {"paperId": "3efcb97c1de1c87832a7a1d99e91801992a938ec", "title": "Crafting Papers on Machine Learning"}, {"paperId": "7826ff60d2dfb24d2af18c5bc565c357ef9db4c1", "title": "A stochastic version of the delta rule"}, {"paperId": "9241ea3d8cb85633d314ecb74b31567b8e73f6af", "title": "Least squares quantization in PCM"}, {"paperId": "7a0748ede292a7ea83a284ecc0a5d03887b993d6", "title": "Efficient Retrieval of Matrix Factorization-Based Top-k Recommendations: A Survey of Recent Approaches"}, {"paperId": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d", "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training"}, {"paperId": null, "title": "D is the dimension of each head, which is the same as in the baselines models Details about the Long Range Arena (LRA) benchmarks can be found in the original paper (Tay et al., 2021)"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Universal language model finetuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": "0ddb71c487294246bd813abae9129b5145a16adc", "title": "Analyzing the Structure"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}