{"paperId": "b72a8884a7a2f93d60d44930dd77d0af85dd32b8", "abstract": "Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs."}, "embedding": {"model": "specter_v2", "vector": [0.3085189461708069, 1.2702990770339966, 0.0986655130982399, 0.10201600939035416, -0.397022545337677, -0.05816899612545967, 0.5307024121284485, -0.21267792582511902, -0.1816803514957428, -0.5350085496902466, 0.8099247217178345, -0.06188727915287018, 0.6273868083953857, 0.030404480174183846, -0.1452413648366928, 0.06536176800727844, -1.153518795967102, 0.05106216296553612, -0.28672146797180176, -0.1849420666694641, -0.14408648014068604, -0.6760320067405701, -1.0401593446731567, 0.34689855575561523, 0.38252559304237366, 0.21877849102020264, 0.37396883964538574, 0.9383041262626648, -0.2624005079269409, 0.5872083306312561, 0.25045526027679443, -0.3797166049480438, 0.14301848411560059, -0.38081827759742737, -0.31597578525543213, -0.18501190841197968, 0.3631564974784851, -0.5424150824546814, -0.25892022252082825, 0.614563524723053, -0.26813754439353943, 0.2083306610584259, 0.2865529954433441, -0.34189078211784363, -0.5639621019363403, 1.6000745296478271, 0.5219694375991821, 0.9131841659545898, -0.2217995822429657, -0.5246933698654175, 1.2988977432250977, -1.566632866859436, 0.009461621753871441, 1.942320704460144, 0.3418130576610565, 0.15966501832008362, -0.08675505220890045, -0.7718720436096191, 1.2937352657318115, -0.0862232968211174, -0.8567984700202942, -0.7633112668991089, 0.23503826558589935, -0.023948870599269867, 2.144348621368408, -0.5769841074943542, 0.24396948516368866, 0.3479141592979431, 0.01569157838821411, 1.6889724731445312, -0.15604199469089508, -0.9117490649223328, -0.3822782337665558, -0.021952036768198013, 0.42571789026260376, 0.6598540544509888, -0.545886754989624, 0.1628456711769104, -0.7089601159095764, -0.1149781197309494, 0.4740629494190216, -0.5183379650115967, -0.004288360010832548, 0.13522277772426605, -0.13016939163208008, 0.9099873304367065, 0.3226455748081207, 0.7994545102119446, -0.5302461385726929, 0.6892948746681213, 0.2716199457645416, 0.1398729383945465, 0.2362983673810959, 0.5353817343711853, -0.0348791629076004, 0.6428130269050598, -0.9399471879005432, 0.19117972254753113, -0.2264152318239212, 1.1640877723693848, -0.3716246485710144, 1.0610870122909546, -0.891635537147522, 0.012694962322711945, 1.6517821550369263, 0.25654685497283936, 0.6853122115135193, -0.6154345870018005, 0.20787465572357178, -0.8019359111785889, -0.21566125750541687, -0.5380765795707703, -0.11165674030780792, -0.5474106073379517, -0.6706914901733398, -1.3630081415176392, -0.27834177017211914, -0.10976874083280563, -0.7297075986862183, 0.8293620944023132, -0.470543771982193, 0.11993828415870667, 0.2196398824453354, 0.12145911902189255, 0.21573634445667267, 0.8234222531318665, 0.6976309418678284, -0.06791725009679794, 0.8857249617576599, -0.7721554636955261, -0.6087833046913147, -1.3585412502288818, 0.803107738494873, -0.3053726851940155, 0.30564171075820923, -0.17523276805877686, -1.5504475831985474, -0.8428797125816345, -0.7726666331291199, -0.08758659660816193, -0.3623705208301544, 0.557348906993866, 0.9898918867111206, 0.5371214151382446, -1.1024365425109863, 0.35960444808006287, -0.16492925584316254, 0.016435153782367706, 0.3284129202365875, 0.17011700570583344, 0.5385409593582153, -0.14032745361328125, -1.3616077899932861, -0.10808751732110977, 0.280654639005661, -0.08921454101800919, 0.1626725196838379, -0.4840616285800934, -1.2791284322738647, 0.3052864074707031, -0.049608681350946426, -0.042192403227090836, 1.253112554550171, -0.22578643262386322, -1.5140646696090698, 0.6011805534362793, -0.5608235001564026, -0.23983368277549744, -0.338960736989975, -0.39942026138305664, -0.36592742800712585, -0.5153179168701172, -0.1787591278553009, 0.5720900297164917, 0.4425641596317291, 0.13206815719604492, -0.2574518918991089, 0.04952319711446762, -0.4554186463356018, -0.22789180278778076, -0.3319786787033081, 1.0149455070495605, -0.2128322720527649, -0.36474359035491943, 0.3156655430793762, 0.34126973152160645, -0.1964617222547531, -0.8868918418884277, -0.4933386743068695, -1.297890543937683, 0.1872548907995224, -0.4008210301399231, 1.2234731912612915, -0.7236528992652893, -0.7181596755981445, -0.266659677028656, -0.2260303497314453, 0.004937045741826296, -0.9205474257469177, 0.36623281240463257, -0.6533403992652893, 0.629711389541626, 0.1424776166677475, -1.1043503284454346, -0.04006901755928993, -0.7024386525154114, -1.0604047775268555, -0.3379653990268707, 0.5382911562919617, 1.179902195930481, -0.8677102327346802, 0.15747782588005066, -0.2378711998462677, 0.12415697425603867, -1.189113974571228, 1.4336141347885132, -0.18076881766319275, 0.2225877046585083, -0.19912108778953552, -0.15160688757896423, 0.05573127046227455, -0.1749080866575241, 0.453664094209671, -0.07032831013202667, -0.2331114560365677, 0.5785776972770691, -0.25003835558891296, 0.873757004737854, -0.4037371873855591, 0.5598632097244263, -0.4223282039165497, -0.4965084195137024, 0.10886385291814804, 0.24501587450504303, -0.480180561542511, -0.5857790112495422, 0.20872832834720612, 0.27794861793518066, -0.42360424995422363, 0.5501058101654053, 0.7943781614303589, 1.1237529516220093, -0.6316795349121094, 0.09027747809886932, 0.5824209451675415, -0.12140963971614838, 0.330327570438385, 0.39738568663597107, 0.6044690012931824, 0.47960543632507324, 0.7854832410812378, -0.02140243537724018, 0.35441476106643677, -0.6692159175872803, 0.11663752794265747, 0.6812719106674194, 0.9845808148384094, 0.8139001727104187, 0.663827121257782, -0.5770658850669861, -0.3922300338745117, 0.11246784031391144, 0.7364909648895264, 1.5540568828582764, -0.6402449607849121, -0.03803413361310959, -0.6379270553588867, 0.10052646696567535, -0.5464938282966614, 0.5207861661911011, -0.07406690716743469, 0.0020481410901993513, -0.9835100173950195, -0.9300008416175842, 0.7844687104225159, 0.1748858541250229, 0.5893355011940002, -0.9616788029670715, -0.05443131551146507, -0.0538783073425293, 0.38290321826934814, -0.9853708744049072, -0.4478914141654968, 0.027484193444252014, -0.47301843762397766, 0.14865553379058838, -0.04691952094435692, -0.12442785501480103, 0.052643775939941406, -0.7670983076095581, 0.9534242749214172, -0.6396743655204773, 0.07293719053268433, 0.0931617021560669, 0.5485355854034424, -0.9743270874023438, -0.9085688591003418, 0.18592961132526398, 0.1340457946062088, -0.19247020781040192, 0.41304993629455566, 0.6088520288467407, 0.20109760761260986, -0.3982373774051666, 0.05165097117424011, 0.17870932817459106, 0.47481095790863037, -0.09558845311403275, 0.7327147722244263, -0.49434947967529297, 0.05425374209880829, -1.4458351135253906, 0.8830867409706116, 0.05609361454844475, -0.7208855748176575, 0.26573753356933594, -0.5316844582557678, -0.2742856442928314, 0.7338874936103821, -0.5437671542167664, -0.4484189748764038, -0.634013295173645, 0.30400142073631287, -0.2472364902496338, -0.4165656864643097, 0.3724519908428192, 0.25054123997688293, 0.5120190382003784, -0.039171475917100906, 0.48118844628334045, 0.09320499002933502, -0.1208697259426117, 0.4802778363227844, -0.769108772277832, 0.4126451909542084, 0.6740736365318298, -0.0072460114024579525, -0.5371252298355103, -0.357541561126709, -0.8738194704055786, -0.34662529826164246, -0.04646649211645126, -0.3349016606807709, -0.23203767836093903, 0.11037202924489975, -0.8902096152305603, -0.6085527539253235, 0.18766146898269653, -1.2564386129379272, -0.2371751219034195, 0.007064064498990774, -0.14473731815814972, 0.02740267850458622, -1.0173299312591553, -1.1276483535766602, -0.7800678610801697, -0.6916472911834717, -0.8034303188323975, 0.6292982697486877, 0.09012085199356079, -0.46039679646492004, -0.3654034435749054, -0.040072329342365265, -0.34163999557495117, 0.945522129535675, -0.8387085795402527, 0.9941630959510803, -0.296359121799469, -0.3733710050582886, -0.578704833984375, 0.5294091701507568, 0.34815099835395813, -0.5016717910766602, 0.12961933016777039, -0.7769584655761719, 0.1903569996356964, -0.5183915495872498, -0.24640744924545288, 0.02511519379913807, 0.7087946534156799, 0.4733520746231079, -0.07304422557353973, -0.3219558298587799, 0.3110184669494629, 1.0153350830078125, -0.5610368847846985, 0.32795220613479614, -0.052410174161195755, 1.0852261781692505, 0.382799357175827, -0.20940245687961578, 0.4907677173614502, 0.5363451242446899, 0.36643093824386597, 0.1663769632577896, -0.060855232179164886, -0.22072219848632812, -0.7092616558074951, 0.8432525396347046, 1.8886542320251465, 0.2630466818809509, -0.3527490496635437, -1.1108205318450928, 0.6082509160041809, -1.0205342769622803, -0.6332512497901917, 0.31307992339134216, 0.9999990463256836, 0.1293470561504364, -0.548665463924408, -0.6987634301185608, -0.5154123902320862, 0.35885512828826904, 0.33705246448516846, -0.24665363132953644, -0.9815446138381958, -0.07078821957111359, 0.29982417821884155, -0.033525846898555756, 0.6208552718162537, -0.5327786207199097, 1.1638002395629883, 14.686890602111816, 0.3716159164905548, 0.10611265152692795, 0.4539632499217987, 1.017727255821228, 0.267196923494339, -0.3205980956554413, -0.15323670208454132, -1.5464500188827515, -0.20971253514289856, 1.3176031112670898, -0.00901256687939167, 0.551598846912384, 0.3634692132472992, 0.33417731523513794, 0.36096376180648804, -0.44852885603904724, 0.48737668991088867, 0.5842958092689514, -1.036401391029358, 0.8540536761283875, 0.3320176303386688, 0.2722618281841278, 0.5796944499015808, 0.5523642897605896, 1.0339932441711426, 0.5588955879211426, -0.5382823944091797, 0.24661651253700256, 0.42797499895095825, 0.37922942638397217, 0.08153315633535385, 0.4729214906692505, 0.3814241290092468, -1.0497230291366577, -0.32394105195999146, -0.6894359588623047, -1.1439682245254517, 0.419292151927948, 0.014743790030479431, -0.48453038930892944, -0.8035836219787598, -0.4357999265193939, 0.44073352217674255, -0.25424057245254517, 0.3727036416530609, -0.45927125215530396, 1.1011642217636108, -0.09854782372713089, -0.10631394386291504, 0.25103873014450073, 0.44065845012664795, 0.5230391025543213, -0.061382319778203964, 0.33416298031806946, 0.1413036435842514, 0.02522493526339531, 0.5317161679267883, -0.4846566915512085, 0.3063162565231323, -0.4567488431930542, -0.6883207559585571, 0.1576911211013794, 0.6030201315879822, 0.5091891884803772, 0.18561290204524994, -0.3642040491104126, 0.0597575418651104, 0.9373725652694702, 0.12813463807106018, 0.05783774331212044, 0.012527916580438614, 0.1677369773387909, -0.4016866385936737, 0.052409347146749496, 0.4278112053871155, 0.02239113673567772, -0.5282160043716431, -0.8740649819374084, -0.4363427460193634, 0.2933136522769928, -0.9788540005683899, -0.5174655914306641, 1.0169737339019775, -0.27948591113090515, -0.21987877786159515, -0.45846083760261536, -0.5598623752593994, -0.3045009672641754, 0.678719699382782, -1.0909526348114014, -0.9553139805793762, 0.38351520895957947, -0.6017605066299438, 0.21173913776874542, -0.3498969078063965, 1.2048672437667847, -0.2531571388244629, -0.6559112668037415, 0.19914510846138, -0.26021191477775574, 0.04285316914319992, -0.5778874158859253, -0.6753032803535461, 1.0166549682617188, 0.8565177321434021, -0.04092581570148468, 0.20759150385856628, -0.07693523913621902, 0.08215277642011642, -0.8287957310676575, 0.02186225913465023, 1.2091760635375977, -0.5923332571983337, -0.288397878408432, -0.9819493889808655, -0.9369861483573914, 0.5686846971511841, 0.586564838886261, -0.5527331829071045, 0.1360248178243637, 0.3638954162597656, -0.009395718574523926, 0.18860961496829987, -0.1398172229528427, 0.16965851187705994, 0.32639503479003906, -0.5424886345863342, -0.21675674617290497, 0.1061948835849762, 0.7237968444824219, -0.8726354837417603, -0.32732051610946655, -0.44531992077827454, 0.0841011330485344, 0.1885511428117752, 0.6546977758407593, -0.6427100896835327, 0.5513455271720886, 1.1941421031951904, 0.14011581242084503, -0.8129101991653442, -0.6267547011375427, -0.842993974685669, 0.09064139425754547, 0.37821993231773376, 0.7000754475593567, -0.36899009346961975, -0.05366681516170502, 0.9354279637336731, 0.0465909019112587, -0.2946954071521759, -0.6292322278022766, 0.057837553322315216, 0.18817293643951416, -0.6053857207298279, 0.2325683981180191, -0.3784211575984955, -0.21606551110744476, 0.11558786779642105, 0.22780539095401764, 0.9312456846237183, -0.19343797862529755, -0.5728700757026672, 0.4722815454006195, -0.00332442345097661, 0.019584836438298225, -0.6027648448944092, -0.6732582449913025, -1.5993894338607788, 0.32878613471984863, -0.9195849299430847, 0.08874670416116714, -0.8093600869178772, -0.7692215442657471, 0.2567053437232971, -0.054170701652765274, 0.24931946396827698, 0.19858455657958984, -0.2872030735015869, -0.41364529728889465, -0.4338865280151367, -0.3632688522338867, 0.8375833034515381, 0.380845844745636, -0.5493810772895813, -0.28028714656829834, 0.09140201658010483, 0.13339781761169434, 0.3328194320201874, 0.29860416054725647, -0.5542760491371155, -0.6016125082969666, -1.5359768867492676, 0.4276559352874756, -0.1077895537018776, -0.2645795941352844, -0.7121314406394958, 0.7000992894172668, 0.6693863272666931, -0.16049867868423462, -0.12568679451942444, 0.10431719571352005, -0.3333953022956848, -0.6651943922042847, 0.14269579946994781, -0.8876894116401672, 0.48766186833381653, 0.26753169298171997, -0.5167276263237, -0.4092939794063568, 0.5000346302986145, -0.2909083366394043, -0.9629296064376831, -0.9749278426170349, 0.47492530941963196, -1.047371506690979, 0.11216932535171509, -0.3886999487876892, -0.18732276558876038, -0.9998343586921692, -0.44034233689308167, -0.05068337544798851, 0.2656145989894867, -0.7449742555618286, 1.2777565717697144, 0.4018429219722748, -0.7640843391418457, -0.15872405469417572, 0.5948891639709473, -0.33241352438926697, -0.0861550122499466, 0.4559386074542999, 0.19042369723320007, -0.2534802556037903, 0.7964810132980347, 0.540664553642273, 0.36581361293792725, -1.0446523427963257, -0.15437519550323486, 0.5896648168563843, -0.6414740681648254, -0.07250230014324188, 1.0835145711898804, -0.1371479332447052, -0.9933624267578125, -0.0031534049194306135, -1.27245032787323, -0.9034872651100159, -0.17868520319461823, 0.670752227306366, 0.09453147649765015, -0.2611294388771057, -0.40270349383354187, -0.2990542948246002, -0.006921879947185516, -0.12367218732833862, -0.16661447286605835, 0.6297552585601807, -0.26079151034355164, -0.456472247838974, 0.6358950138092041, 0.6499000191688538, -0.374116986989975, -0.5110793709754944, -0.7471689581871033, -0.37641099095344543, 0.3333861231803894, 0.605028510093689, 0.039121076464653015, -0.5298663377761841, 0.822137713432312, 0.5396776795387268, 0.30657580494880676, 0.16293953359127045, -0.07506858557462692, -0.11423283815383911, 0.5427350401878357, 0.11364584416151047, -0.628494918346405, -0.7553216814994812, 1.659568190574646, 1.4421234130859375, -0.7371946573257446, 0.25550156831741333, -0.5047322511672974, -0.8841190338134766, 0.9382303357124329, 0.2822670042514801, -0.1875932365655899, 0.9217343330383301, -0.3985283672809601, 0.04072439298033714, 0.3107079565525055, -1.1275920867919922, -0.37243399024009705, 0.9211056232452393, 0.7792330980300903, 1.1624208688735962, 0.2470143586397171, -0.045917656272649765, 1.0283629894256592, -0.06576134264469147, 0.016644008457660675, 0.5213469862937927, 0.2917180061340332, -0.1528574377298355, 0.28571245074272156, 0.12080349028110504, 0.7340954542160034, -0.6141650080680847, -1.0200241804122925, 0.24002596735954285, 0.45168203115463257, -0.016345668584108353, 0.6645163297653198, 1.0767371654510498, 0.640345573425293, 0.5090339183807373, 0.3250470757484436, 0.10938384383916855, -0.5105181932449341, -0.18568448722362518, 0.1585819125175476, -0.5597600340843201, -0.23787908256053925, 0.036018483340740204, -0.6344072222709656, -0.2362501621246338, 0.34720948338508606, 0.267926424741745, -0.31430166959762573, 0.3041711449623108, 1.0380194187164307, 0.8124269247055054, 0.4930342137813568, -0.23808500170707703, -0.7040547728538513, -0.22838065028190613, -1.0110938549041748, -0.11508484929800034, -0.6750062108039856, -0.3488312363624573, 0.27642810344696045, -0.18885496258735657, -0.18168622255325317]}, "authors": [{"authorId": "47113848", "name": "Haoran You"}, {"authorId": "2305648167", "name": "Yichao Fu"}, {"authorId": "2306259358", "name": "Zheng Wang"}, {"authorId": "2303406300", "name": "Amir Yazdanbakhsh"}, {"authorId": "2305737055", "name": "Y. Lin"}], "references": [{"paperId": "c96425efc67b0e51844b457aa7b122b73d1b3300", "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice"}, {"paperId": "20f090e35ad598fba2404e550c2462dc9da03a10", "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "1ddbd08ad8cf22a5c66c4242194c4286328533bf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"}, {"paperId": "564855d475ed9197dd7516594557ff886ff623e5", "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"}, {"paperId": "5ce76c60e0bd53d4a6aac8b3f4672188611a7149", "title": "SEA: Sparse Linear Attention with Estimated Attention Mask"}, {"paperId": "9c53679e7c2107e36e6c60c19464c607bf3459cd", "title": "EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction"}, {"paperId": "b5c104af50688a5f8b448ee2b1def977cccd99e3", "title": "Google's AI chatbot \"Bard\": a side-by-side comparison with ChatGPT and its utilization in ophthalmology."}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "92a95c5d3ea87e08ac527d8ce25383ff8c1015be", "title": "ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "017010b941d902a467f6d329ae5e74fd67e67912", "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "977351c92f156db27592e88b14dee2c22d4b312a", "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "200ef1cde362aafbf598a2b5a1c5f35504ca2289", "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "1ed66e048bb025e75aa5ea660545285212e5341f", "title": "Scaling Up Models and Data with t5x and seqio"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "0a0c204919ec72c6c335296ebf639ebc379d3ac5", "title": "Learned Queries for Efficient Local Attention"}, {"paperId": "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4", "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "01203341a8b5b7df21dec5359afe8cc388786ebf", "title": "Wiki-40B: Multilingual Language Model Dataset"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "ffb9646f66895a9753a917d6b97de6c4fb4eb437", "title": "Parallel Multi Channel convolution using General Matrix Multiplication"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "d1a6b3a5efde3783b53f822dc8dd00aaac934b95", "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9", "title": "Scaling TransNormer to 175 Billion Parameters"}, {"paperId": "ccf15b75d3ed3287c0ac524666578ed785bff1a3", "title": "Big Little Transformer Decoder"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Linformer"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "The Human Knowledge Compression Contest"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "Efficient Fine-tuning of Long-Context Large Language Models"}, {"paperId": null, "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized LLMs"}, {"paperId": null, "title": "Gem-ini: A Family of Highly Capable Multimodal Models"}, {"paperId": null, "title": "Instruction-Following"}]}