{"paperId": "838446de88c800bf2186d148514fff3a43ccbe13", "abstract": "The advent of transformers with attention mechanisms and associated pre-trained models have revolutionized the field of Natural Language Processing (NLP). However, such models are resource-intensive due to highly complex architecture. This limits their application to resource-constrained environments. While choosing an appropriate NLP model, a major trade-off exists over choosing accuracy over efficiency and vice versa. This paper presents a commentary on the evolution of NLP and its applications with emphasis on their accuracy as-well-as efficiency. Following this, a survey of research contributions towards enhancing the efficiency of transformer-based models at various stages of model development along with hardware considerations has been conducted. The goal of this survey is to determine how current NLP techniques contribute towards a sustainable society and to establish a foundation for future research.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A survey of research contributions towards enhancing the efficiency of transformer-based models at various stages of model development along with hardware considerations has been conducted to determine how current NLP techniques contribute towards a sustainable society and to establish a foundation for future research."}, "embedding": {"model": "specter_v2", "vector": [0.35961031913757324, 0.8320229649543762, -0.5145397186279297, 0.14058040082454681, -0.49018120765686035, -0.6395912766456604, 0.39694854617118835, 0.22768548130989075, -0.6682521104812622, 0.11676569283008575, 0.6132614612579346, -0.41555771231651306, 0.21194468438625336, 0.007501301355659962, -0.38645508885383606, 0.36438843607902527, -0.3570856750011444, 0.6350669264793396, -0.23819687962532043, -0.068568654358387, -0.32167020440101624, -0.5803928375244141, -0.6076228022575378, 0.07875829935073853, 0.6090694069862366, 0.29622775316238403, 0.3005773723125458, 0.6428504586219788, -0.6631187200546265, 0.5233277678489685, 0.45120033621788025, -0.43185853958129883, 0.08644290268421173, 0.4101974666118622, -0.314030259847641, -0.5777885913848877, 0.5756363868713379, -0.15533189475536346, -0.6426314115524292, 1.0171194076538086, -0.4537779688835144, 0.1650533527135849, 0.45378372073173523, -0.8501533269882202, -0.7600208520889282, 1.3091744184494019, 0.950266420841217, 1.1845238208770752, 0.09399233758449554, -0.5702025890350342, 1.7257755994796753, -1.2834008932113647, 0.3555176556110382, 1.5301820039749146, 0.7147610783576965, 0.3004087805747986, -0.05381869152188301, -0.6884941458702087, 0.22922416031360626, 0.20427370071411133, -0.830573320388794, -0.7106707692146301, -0.26504310965538025, 0.17214563488960266, 2.2297263145446777, -0.107417032122612, -0.03509330004453659, 0.16720262169837952, -0.3042681813240051, 1.6106793880462646, -0.17658190429210663, -1.225063681602478, -0.46902117133140564, 0.05412006750702858, 0.01907246932387352, 0.8380270004272461, -0.4860079884529114, -0.12906907498836517, -0.9767856597900391, -0.4069075286388397, 0.3945847749710083, -0.2781321704387665, -0.4852941036224365, 0.0579431988298893, -0.28670984506607056, 0.7504417300224304, 0.2504303753376007, 0.9297935962677002, -0.47042158246040344, 0.08336976170539856, 0.5303542613983154, 0.39311859011650085, -0.2354884296655655, 0.8258631229400635, -0.6284655332565308, 0.3023472726345062, -1.227910041809082, 0.19833558797836304, 0.2611691951751709, 1.0463615655899048, -0.376972496509552, 0.38307836651802063, -0.9926624298095703, 0.17910245060920715, 1.204039216041565, 0.517318069934845, 0.38742607831954956, -0.8237746357917786, 0.36018022894859314, -0.3050149083137512, 0.6467284560203552, -0.7312342524528503, -0.24241520464420319, -0.027062416076660156, -0.43668699264526367, -1.5280215740203857, 0.035029150545597076, 0.04927704855799675, -0.9283050298690796, 0.5319948792457581, -0.4568876326084137, -0.30908796191215515, 0.3806411027908325, 0.28440842032432556, 0.4792103171348572, 0.8330681920051575, 0.28839918971061707, -0.16967377066612244, 1.5656678676605225, -0.485064834356308, -1.1162049770355225, -1.1697536706924438, 0.6465111970901489, -0.27602627873420715, 0.3623734712600708, -0.6010176539421082, -1.351012110710144, -0.38395005464553833, -0.3493781089782715, -0.3645976185798645, -0.35287368297576904, 0.46843475103378296, 0.8601329922676086, 0.36478570103645325, -1.2328927516937256, 0.18855172395706177, 0.07575683295726776, -0.5967686176300049, 0.3005375266075134, 0.10569706559181213, 0.31765061616897583, -0.21789583563804626, -1.4740737676620483, 0.39193227887153625, 0.17992888391017914, -0.6761999130249023, -0.16988608241081238, -0.5097522139549255, -1.3929084539413452, 0.285195916891098, 0.4138840138912201, -0.43363073468208313, 1.4657920598983765, -0.11794155091047287, -1.1362696886062622, 0.5058005452156067, -0.8871251344680786, -0.13740181922912598, -0.2348276525735855, -0.22727108001708984, -0.5216374397277832, -0.28581151366233826, 0.2492973953485489, 0.388243168592453, 0.28833726048469543, 0.03950149193406105, -0.560513973236084, 0.1560491919517517, -0.0954168438911438, -0.047382961958646774, -0.27599677443504333, 1.3199657201766968, -0.3868050277233124, -0.09008466452360153, 0.436832457780838, 0.6158957481384277, -0.13960270583629608, -0.036481451243162155, -0.3247336149215698, -1.1274226903915405, 0.6182171106338501, -0.40531080961227417, 1.1768840551376343, -0.6794108152389526, -0.8891875147819519, -0.22414156794548035, -0.26007241010665894, 0.06612736731767654, -0.6271478533744812, 0.8420587778091431, -0.3338679075241089, 0.5212751626968384, -0.0005308751715347171, -0.5001855492591858, 0.04219932481646538, -0.018043451011180878, -0.905765950679779, -0.4675057828426361, 0.02891906164586544, 1.144980788230896, -0.9710798859596252, -0.04242945834994316, 0.12315241247415543, 0.23694714903831482, -0.7728861570358276, 1.1271049976348877, -0.4781899154186249, -0.1929755061864853, -0.06949702650308609, 0.14819292724132538, -0.06426122039556503, -0.26464471220970154, 0.3788626492023468, -0.6910430192947388, -0.42754927277565, 0.510217547416687, 0.05364186689257622, 1.049660086631775, -0.4761646091938019, 0.5249990820884705, 0.28653308749198914, -0.39264827966690063, -0.2421179860830307, 1.113569736480713, -0.4817267954349518, -0.3686332404613495, 0.38384342193603516, 0.6600955128669739, -0.36118873953819275, 0.050402261316776276, 1.0227675437927246, 0.9797366261482239, -0.19560091197490692, 0.30171218514442444, 0.6137171983718872, -0.21587032079696655, 0.5822156071662903, 0.41741758584976196, 0.9019804000854492, -0.03220268338918686, 0.6382453441619873, -0.2560105621814728, 0.6348739266395569, -0.2704959809780121, -0.050145223736763, 0.5959251523017883, 0.5317810773849487, 0.3859193027019501, 0.3047564625740051, -0.46602746844291687, -0.34669187664985657, 0.4341655671596527, 0.6353680491447449, 1.6422878503799438, -0.3944509029388428, -0.2719478905200958, -0.8429560661315918, -0.6646462082862854, -0.5966901779174805, 0.5444739460945129, -0.03766831010580063, 0.509573757648468, -0.6495224833488464, -0.679768979549408, 1.231830358505249, 0.2696179449558258, 0.392988920211792, -1.0029716491699219, -0.2259327918291092, -0.12341421842575073, -0.20121337473392487, -0.9570438265800476, -0.5372267365455627, 0.47461754083633423, -0.6387537121772766, -0.6189020276069641, 0.18529613316059113, -0.25698503851890564, 0.4338509738445282, -0.7006239295005798, 0.8872132897377014, -0.5213826894760132, 0.2745358347892761, 0.0832967534661293, 0.5354222655296326, -0.8843064308166504, -0.7072455286979675, 0.30748096108436584, 0.026260409504175186, -0.7261079549789429, 1.058355450630188, 0.5562241673469543, 0.2822396755218506, -0.24081195890903473, -0.4139234125614166, -0.45663002133369446, 0.2980954051017761, 0.10297505557537079, 0.7291480898857117, -0.3292112350463867, -0.08962342888116837, -1.191381573677063, 1.1295733451843262, 0.2145327776670456, -0.6173937320709229, 0.4441030025482178, -0.5482609868049622, -0.1900642067193985, 0.826476514339447, -0.5169416666030884, -0.41392019391059875, -0.5755361318588257, 0.25087451934814453, -0.0321270190179348, -0.1986096203327179, 0.7356920838356018, 0.04659320041537285, 0.1950864940881729, 0.29182174801826477, 0.8449218273162842, 0.41525426506996155, -0.15474432706832886, 0.6791648268699646, -0.28369855880737305, 0.36247164011001587, 0.34503263235092163, 0.17221476137638092, -0.37459155917167664, -0.5524714589118958, -0.6873982548713684, -0.4716234803199768, -0.300001859664917, 0.06282570213079453, -0.04947621747851372, -0.3370606601238251, -0.39391013979911804, -0.8444737195968628, -0.3640143871307373, -1.2460594177246094, 0.18348275125026703, -0.014782405458390713, -0.3372255265712738, 0.23230382800102234, -0.9300360679626465, -1.357566475868225, -0.8644389510154724, -1.0758640766143799, -0.5527113080024719, 0.4795699417591095, 0.07675710320472717, -0.5075847506523132, -0.606705367565155, -0.010813916102051735, -0.11604789644479752, 1.0700093507766724, -0.999952495098114, 1.4991657733917236, -0.4497282803058624, 0.00670947041362524, -0.32015109062194824, -0.0387234166264534, -0.008380848914384842, -0.1221231147646904, 0.06809445470571518, -0.6005384922027588, 0.5819518566131592, -0.054850608110427856, -0.17333488166332245, -0.3726789653301239, 0.6774223446846008, 0.3430193364620209, -0.3507314920425415, -0.6186148524284363, 0.06928250193595886, 1.2277501821517944, -0.6262126564979553, -0.027730468660593033, 0.2664531171321869, 0.7891972064971924, 0.35422325134277344, -0.09537173062562943, 0.1627003401517868, 0.45039188861846924, 0.24987812340259552, 0.26896458864212036, 0.19859416782855988, 0.14290760457515717, -0.44642576575279236, 0.3221726715564728, 1.8085534572601318, 0.3130723536014557, -0.5237824320793152, -1.107869029045105, 0.8073725700378418, -1.292033076286316, -0.5131884217262268, 0.4067458510398865, 0.367674320936203, 0.7777847647666931, -0.6458397507667542, -0.5671693682670593, 0.09405946731567383, 0.3763367831707001, 0.36859193444252014, -0.015324254520237446, -0.43857908248901367, 0.09773962199687958, 0.2876257300376892, 0.3777225911617279, 0.749663233757019, -0.5400362610816956, 0.8880963325500488, 14.498920440673828, 1.0078487396240234, 0.1717902272939682, 0.5732616782188416, -0.045419469475746155, 0.6102955937385559, -0.5320584774017334, -0.18908889591693878, -1.2588061094284058, -0.4396865665912628, 0.9378580451011658, -0.3017207980155945, 0.6860516667366028, 0.1448395699262619, 0.24348662793636322, 0.09407372772693634, -0.4649520516395569, 0.6288798451423645, 0.674036979675293, -1.0996097326278687, 0.827865481376648, 0.014094984158873558, -0.13617581129074097, 0.446491003036499, 0.29140061140060425, 0.7832223176956177, 0.5182561874389648, -0.6164580583572388, 0.5713434219360352, -0.048098623752593994, 0.6915011405944824, 0.08714732527732849, 0.825427234172821, 0.9684579968452454, -1.008746862411499, -0.500205397605896, -0.6385916471481323, -1.1668630838394165, 0.26391544938087463, 0.1552385538816452, -0.7397851347923279, -0.6054773330688477, -0.27695298194885254, 0.40181681513786316, 0.3207401633262634, 0.5517973303794861, -0.6511924862861633, 0.7963297367095947, -0.4385967552661896, -0.05920357629656792, 0.12658599019050598, 0.6392210721969604, 0.4556787312030792, 0.01206399966031313, 0.29516834020614624, 0.22035059332847595, 0.1371077597141266, 0.2939647436141968, -0.8225468993186951, 0.2467648684978485, -0.5050667524337769, -0.423063188791275, 0.3292980194091797, 0.786776602268219, 0.23316247761249542, 0.40050163865089417, -0.2858009934425354, 0.1948433816432953, 0.6973874568939209, 0.1886918693780899, 0.0654984787106514, -0.2665192186832428, -0.008741808123886585, -0.14189893007278442, 0.10281139612197876, 0.7699372172355652, 0.060050610452890396, -0.3370811939239502, -0.6795394420623779, -0.431281715631485, 0.46620193123817444, -0.8395959138870239, -0.880413293838501, 1.0580800771713257, -0.4269288182258606, -0.2878575026988983, 0.3252061903476715, -0.8218023180961609, 0.08272108435630798, 0.4005521833896637, -1.5876344442367554, -1.1378675699234009, 0.587471067905426, 0.10073572397232056, -0.43067243695259094, 0.18900199234485626, 1.6362378597259521, 0.08033241331577301, -0.1661192923784256, -0.12489055097103119, 0.19142630696296692, 0.48397096991539, -0.4648763835430145, -0.8300753235816956, 0.5871983170509338, 0.5112907290458679, 0.2101905792951584, 0.34987279772758484, 0.22970841825008392, -0.14402668178081512, -0.7815616130828857, -0.24135088920593262, 1.5386935472488403, -1.0417917966842651, -0.4050664007663727, -0.6142846345901489, -1.0309516191482544, 0.6252956986427307, 0.7595898509025574, -0.5793290734291077, 0.29420211911201477, 0.1231265738606453, -0.33790212869644165, 0.31811192631721497, -1.2345082759857178, 0.06563660502433777, 0.4683946967124939, -0.8625050187110901, -0.7212844491004944, 0.048730600625276566, 0.38687559962272644, -1.0070254802703857, -0.3624691963195801, -0.35127872228622437, 0.07218381762504578, 0.7180418372154236, 0.57975834608078, -0.4081505835056305, 0.08843395113945007, 0.43946391344070435, -0.0904512107372284, -0.7428829073905945, -0.10422056168317795, -0.6580275893211365, -0.4076623022556305, 0.010518823750317097, 1.278637170791626, -0.3519347906112671, 0.47327032685279846, 1.076706886291504, 0.4008459448814392, 0.10382948070764542, -0.9258280396461487, -0.1641361266374588, -0.17619588971138, -0.4510630667209625, 0.42747196555137634, -0.18144813179969788, 0.10516797751188278, 0.20982922613620758, 0.5979365110397339, 0.8931379318237305, -0.31637459993362427, -0.6631420850753784, -0.009863064624369144, -0.46173781156539917, 0.5062042474746704, -0.185041606426239, -0.02369728684425354, -1.221186876296997, 0.4004119336605072, -0.9018489122390747, 0.3300543427467346, -1.323616862297058, -0.4302419126033783, -0.06782238930463791, 0.13491560518741608, 0.47819483280181885, 0.5026732683181763, -0.6155105233192444, -0.593791127204895, -0.3542940020561218, -0.2400810271501541, 0.6347423195838928, 0.5168935656547546, -0.5888460874557495, -0.04032899811863899, 0.0909586027264595, 0.08509845286607742, 0.45058009028434753, 0.4442225992679596, -1.0391477346420288, -0.7130787372589111, -1.5082063674926758, 0.4546941816806793, -0.15351825952529907, -0.4869488775730133, -0.14150209724903107, 1.1541824340820312, 0.37207213044166565, -0.5750582814216614, 0.14148683845996857, 0.3603794574737549, -1.4409517049789429, -0.740013062953949, 0.1710229516029358, -0.5860176086425781, 0.22445079684257507, 0.1726180464029312, -0.6504579782485962, -0.5474058389663696, 0.26224786043167114, -0.30032169818878174, -1.1368378400802612, -0.3899441361427307, 0.3067147731781006, -0.901350736618042, 0.35209745168685913, -0.2565647065639496, -0.19308128952980042, -0.9355875849723816, -0.4603666663169861, -0.0533006452023983, 0.30623266100883484, -0.3608548939228058, 0.7482086420059204, 0.36910268664360046, -0.7315377593040466, 0.1354266256093979, 0.3311082720756531, -0.22364315390586853, -0.2554512023925781, -0.11204508692026138, 0.4759606122970581, -0.3004445433616638, 1.0543782711029053, 0.28992366790771484, 0.37269628047943115, -1.2046135663986206, -0.23863983154296875, 0.7224882245063782, -0.42643874883651733, -0.17973977327346802, 1.019597053527832, -0.8816854953765869, -1.2993828058242798, 0.2961251139640808, -1.5459777116775513, -1.082045316696167, -0.27823972702026367, 0.6762048006057739, 0.0326356515288353, 0.049896739423274994, -0.4125522971153259, -0.6890292167663574, -0.05289296805858612, 0.1240261048078537, -0.4257486164569855, 0.4354727566242218, -0.16504162549972534, -0.5668627023696899, 0.24544842541217804, -0.12425664067268372, -0.3564339578151703, -0.15429125726222992, -0.7509250640869141, 0.1601535826921463, -0.1874658316373825, 0.5136971473693848, -0.5055908560752869, -0.6330727934837341, 0.6986976861953735, 0.11593303829431534, 0.6604782342910767, 0.031697019934654236, -0.20584742724895477, 0.5543486475944519, 0.7630341053009033, 0.29175853729248047, -0.5702449083328247, -1.057244896888733, 1.6814966201782227, 1.2939690351486206, -0.819053053855896, 0.017204061150550842, -0.5747362971305847, -0.5089350938796997, 0.9727336764335632, 0.15535984933376312, 0.26047274470329285, 1.0246710777282715, 0.07298217713832855, 0.3309915363788605, -0.08600658923387527, -0.7068198919296265, -0.16692863404750824, 0.31334391236305237, 1.1970213651657104, 0.746823787689209, 0.1191290095448494, 0.11972247064113617, 1.026530146598816, 0.015472344122827053, 0.6229437589645386, 0.4076971113681793, 0.7437818646430969, -0.401660293340683, -0.14921216666698456, -0.02021368034183979, 0.8806840777397156, -0.9979572892189026, -1.257969617843628, -0.17636358737945557, 0.8109537959098816, 0.1331891566514969, 0.7322167754173279, 0.38437554240226746, 0.3865724205970764, 0.8006821870803833, 0.43268027901649475, 0.3657419979572296, -0.8719774484634399, -0.5092278718948364, -0.5383190512657166, -0.4041583240032196, -0.007795761805027723, -0.41296350955963135, -0.3302595317363739, -0.2669498324394226, -0.19174601137638092, -0.006220259703695774, 0.4835412800312042, 0.3562983572483063, 0.9560644030570984, 0.6911073327064514, -0.016184750944375992, -0.9030901789665222, -0.020860718563199043, -0.16446688771247864, -1.466363787651062, -0.06340400129556656, -1.0440096855163574, -0.32558366656303406, 0.3623848259449005, -0.3352465033531189, -0.2372296005487442]}, "authors": [{"authorId": "7443405", "name": "Wazib Ansar"}, {"authorId": "38994671", "name": "Saptarsi Goswami"}, {"authorId": "2276124842", "name": "Amlan Chakrabarti"}], "references": [{"paperId": "32479758dc9ff9820828a12aa7f3d066f187dc1c", "title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models"}, {"paperId": "56ad69a2629d2fe368f691d52a44c78876c0541a", "title": "A Bibliometric Study of Natural Language Processing Using Dimensions Database: Development, Research Trend, and Future Research Directions"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "bd8412c233bf3815d8e905911b58e12bd6f279da", "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model"}, {"paperId": "303f66b6c0d6d917e35c34f8123cee55f9f7bec7", "title": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning"}, {"paperId": "fd7e88a2313e176315d99fc299277e752d7703b7", "title": "Efficient Methods for Natural Language Processing: A Survey"}, {"paperId": "afb853d2ca0ed5157bcddd6636f7fac1505c0742", "title": "A novel selective learning based transformer encoder architecture with enhanced word representation"}, {"paperId": "eb4d54651c4f610749caf2bf401af3ce28ddc439", "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"}, {"paperId": "90c7ea8273612b2fa12a294f171863e30d879505", "title": "Adaptable Adapters"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "0dab58e476f3f0e6f580a295f7c4756c86f1f198", "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "c546e5447f412bf4f274e490996718641b211aa6", "title": "A Survey on Model Compression and Acceleration for Pretrained Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "b1fa4a0048901a05057944b6de10d00c2d8c6ea1", "title": "Combating the menace: A survey on characterization and detection of fake news from a data science perspective"}, {"paperId": "b8c864635656a7ec09c8dcdfc85f600cfba12ccf", "title": "Attention-guided Generative Models for Extractive Question Answering"}, {"paperId": "6b1462ee55b8a82bcb32d30c3bf0a4bc84f95c10", "title": "Active Learning by Acquiring Contrastive Examples"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "5441598e2b690a15198b7a38359e5936e4a46114", "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering"}, {"paperId": "5f0f4a3fa3cff7ffcedabbc9ed0dad2dd71f7028", "title": "Does Knowledge Distillation Really Work?"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "b769b629c8de35b16735214251d6b4e99cb55762", "title": "Generating Datasets with Pretrained Language Models"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "cbb8faf1051d858b4b0426742fdbbd0f104833ea", "title": "Automatic text summarization: A comprehensive survey"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"}, {"paperId": "7a320541d7772bedc9b7f537f6bd459675675bb0", "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "1841cf23c65ff2f27f21ba0d2268c3445f20332f", "title": "Few-Shot Text Generation with Natural Language Instructions"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "ec62a806fc52f9de0301f72dbed094b70fa057ca", "title": "Green Algorithms: Quantifying the Carbon Footprint of Computation"}, {"paperId": "3911514724fe86048884ad9685ddf84b700a90b8", "title": "Beyond Peak Performance: Comparing the Real Performance of AI-Optimized FPGAs and GPUs"}, {"paperId": "327b12c21e7b8f2126bd7d64ebc2ede0db220c88", "title": "Do We Need to Create Big Datasets to Learn a Task?"}, {"paperId": "292839cb8e5c601be8cd467184939de7873fdd44", "title": "Chasing Carbon: The Elusive Environmental Footprint of Computing"}, {"paperId": "2e06b7f72270900544284e0898aac2bb564ff58b", "title": "Cold-start Active Learning through Self-Supervised Language Modeling"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "b6451cfb71be72f8f9e0f5d2f529fea231adb382", "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer"}, {"paperId": "94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b", "title": "A Survey of Deep Active Learning"}, {"paperId": "e4593f00ca626f8adf381fadb5d261df8223c3d4", "title": "A Mixed approach of Deep Learning method and Rule-Based method to improve Aspect Level Sentiment Analysis"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "de5157a3d62ab114813379a6568f716b483feece", "title": "Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models"}, {"paperId": "b896b846ae180d804c7290d8b9ae9ffc55325866", "title": "Language-agnostic BERT Sentence Embedding"}, {"paperId": "bf4f4ebd43ac87431fdf978988c2a90b1034e994", "title": "A literature review on question answering techniques, paradigms and systems"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "d15583742c74e8d595c147d16cf1a1cdd9e1dbca", "title": "Effective Dimensionality: A Tutorial"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "22d834f7983fbd7cf2418978571f23efcd224bd9", "title": "Adversarial Filters of Dataset Biases"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "dab8424eec426ed3996e469f657371dd53e20127", "title": "CorefQA: Coreference Resolution as Query-based Span Prediction"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "093d9253a2fe765ca6577b091d3f99bab3155a7d", "title": "Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "72a056f35f542be71ddfc1f574eb8ad4227ff52e", "title": "Xilinx Adaptive Compute Acceleration Platform: VersalTM Architecture"}, {"paperId": "ab456c1ed181c5c48a34adb61395d4806a0ba949", "title": "Attention in Natural Language Processing"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "6182aed90596acd1573bd5ccbc2284b1e8a7291b", "title": "Generative Question Answering: Learning to Answer the Whole Question"}, {"paperId": "0e92976bb7883fd1f10a828af21e32b3cbe21c74", "title": "Practical Obstacles to Deploying Active Learning"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "23ded32cf9bd24fec62c7fec290f0ff968b81b42", "title": "Sentiment Analysis of Twitter Data Using Naive Bayes Algorithm"}, {"paperId": "a76706d350b8c483a3aff73e61b91d15b5687335", "title": "Universal Sentence Encoder"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "2ec7156913117949ab933f27f492d0149bc0031f", "title": "Learning Sparse Neural Networks through L0 Regularization"}, {"paperId": "af2a5a1ce4c8b25b2ba8bf9657f9445191ea40bf", "title": "Towards Lower Bounds on Number of Dimensions for Word Embeddings"}, {"paperId": "151b459fd2f47de1f24af7380aa290e79f01b0b9", "title": "Effective Dimensionality Reduction for Word Embeddings"}, {"paperId": "cb40a5e6d4fc0290452345791bb91040aed76961", "title": "Fake News Detection on Social Media: A Data Mining Perspective"}, {"paperId": "c342c71cb23199f112d0bc644fcce56a7306bf94", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d", "title": "SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents"}, {"paperId": "b05fdba8f447b37d7fa6fdd63d23c70b2f4ee01b", "title": "Aspect extraction for opinion mining with a deep convolutional neural network"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd", "title": "Bag of Tricks for Efficient Text Classification"}, {"paperId": "221ef0a2f185036c06f9fb089109ded5c888c4c6", "title": "Sequence-to-Sequence RNNs for Text Summarization"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "2bd576ce574df33c834b6032962cd5ae0be5299f", "title": "Guidelines for conducting systematic mapping studies in software engineering: An update"}, {"paperId": "02251b886862964a1a788c08980b61aacaaf4e85", "title": "Automated Rule Selection for Aspect Extraction in Opinion Mining"}, {"paperId": null, "title": "Sentiment Analysis: Mining Opinions, Sentiments, and Emotions"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "71e90c4015e3dea597dbd583f3d3d08cdc0077fb", "title": "Opinion Mining with Deep Recurrent Neural Networks"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "f3de86aeb442216a8391befcacb49e58b478f512", "title": "Distributed Representations of Sentences and Documents"}, {"paperId": "0d3edffe60f06124431cc63e40fcc1754df4c2f9", "title": "The systematic review: an overview."}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "e92d8ac1dd302bcaa856aca23597d853993161f1", "title": "How to Write a Literature Review"}, {"paperId": "acdc7037ac8efdbfdfa2feb94b01cd7de0470a4f", "title": "Rumor has it: Identifying Misinformation in Microblogs"}, {"paperId": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch"}, {"paperId": "a049555721f17ed79a97fd492c8fc9a3f8f8aa17", "title": "Self-Paced Learning for Latent Variable Models"}, {"paperId": "cfdd423c8672a7b178ea85d56079328df4eea647", "title": "Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit"}, {"paperId": "da5cd00115f7ec108de8eebf071c5f3f19807df4", "title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables"}, {"paperId": "245831b1ba9fa32fdb224555b37533010af903e6", "title": "Preferred reporting items for systematic reviews and meta-analyses: the PRISMA Statement"}, {"paperId": "bd5b9c8ef53bc8ff29441a0aa832851171c211bf", "title": "A typology of reviews: an analysis of 14 review types and associated methodologies."}, {"paperId": "98313428c86455525ee6992d850b2876973c582a", "title": "Opinion spam and analysis"}, {"paperId": "9b4876f7313b111074e79a01f570e6e9e02c0dce", "title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques"}, {"paperId": "545a4e23bf00ddbc1d3325324b4c61f57cf45081", "title": "Recurrent nets that time and count"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "d80a6a85b0c263d638877fff66ddc12963e3c34f", "title": "A trainable document summarizer"}, {"paperId": "d0be39ee052d246ae99c082a565aba25b811be2d", "title": "Learning long-term dependencies with gradient descent is difficult"}, {"paperId": "164125a65d42a791d2c1e108559344caef96d08b", "title": "Indexing by Latent Semantic Analysis"}, {"paperId": "1a3f80af28be2a2c22fdd40379a9a2396de0b276", "title": "Natural-Language Processing"}, {"paperId": "c85cff41e8b3160a04097a147d8612a10de74cbc", "title": "Exploring ChatGPT Capabilities and Limitations: A Survey"}, {"paperId": "2d1842056c4311c886ed9bf634e310490d725b33", "title": "On the Concept of Resource-Efficiency in NLP"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "15e1b9fa6d68d8465227d33de652162f564d3c9d", "title": "TexIm: A Novel Text-to-Image Encoding Technique Using BERT"}, {"paperId": "37cf223d02a61c73a21e36dd4c2b9ea4d0fa8583", "title": "An efficient methodology for aspect-based sentiment analysis using BERT through refined aspect extraction"}, {"paperId": "1e7d821219bb4955e8609edaa46b4e20db6745ed", "title": "Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "05a878000170abcd0c6f8208080470858422e17c", "title": "Sentiment Analysis on Twitter Data using KNN and SVM"}, {"paperId": "001a95d76817ab5ae98ffb29fb970418220edfaa", "title": "Active Learning with Real Annotation Costs"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": "decd9bc0385612bdf936928206d83730718e737e", "title": "Distributional Structure"}, {"paperId": null, "title": "Inference Time denotes the time required by the model to process a test input and generate a suitable response [141]"}, {"paperId": null, "title": "Speed-up Ratio helps to perform comparison of a model concerning another model [141]"}]}