{"paperId": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b", "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}", "venue": "arXiv.org", "year": 2023, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.16354", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput."}, "embedding": {"model": "specter_v2", "vector": [0.40571537613868713, 0.9053307175636292, -0.5671790838241577, 0.051175735890865326, -0.40609416365623474, -0.02570994757115841, 0.506597638130188, 0.032322000712156296, -0.49375391006469727, -0.48226815462112427, 0.6481243371963501, 0.12251236289739609, 0.7322326898574829, -0.051190584897994995, -0.34104588627815247, 0.28623053431510925, -0.6836932897567749, 0.0066635082475841045, 0.5776917934417725, -0.5505550503730774, 0.17884689569473267, -0.8940741419792175, -1.1255135536193848, 0.16836783289909363, 0.2983429729938507, 1.1249399185180664, 0.22000060975551605, 0.7234803438186646, -0.48809313774108887, 0.5676605701446533, 0.32897573709487915, -0.6020263433456421, 0.34379512071609497, 0.14606769382953644, -0.555965006351471, -0.1789821833372116, 0.3952012062072754, -0.22055299580097198, -0.6671594381332397, 1.0924409627914429, -0.01730780489742756, 0.16133418679237366, 0.2763884961605072, -0.7941150069236755, -0.4796842336654663, 0.7597621083259583, 0.6611126661300659, 0.6801585555076599, -0.7083723545074463, -0.517084538936615, 1.552872896194458, -1.3377920389175415, -0.04598596319556236, 1.2209606170654297, 0.10519595444202423, 0.3257941007614136, -0.025984803214669228, -0.622589647769928, 0.7932490110397339, 0.7736971974372864, -0.7707553505897522, -0.5981757044792175, 0.10316282510757446, 0.09203407913446426, 1.9249242544174194, -0.37555786967277527, 0.3569716513156891, 0.5845238566398621, 0.03322533145546913, 1.23359215259552, -0.05951172113418579, -0.5581894516944885, 0.050322044640779495, 0.13165730237960815, 0.4804137647151947, 0.6290733218193054, -0.4681871235370636, 0.1022922694683075, -0.9029881954193115, -0.13786134123802185, 0.23439398407936096, 0.15165555477142334, 0.23844322562217712, -0.3809724748134613, -0.05069492384791374, 0.8524944186210632, 0.6729894876480103, 0.4908044636249542, -0.06024783104658127, 1.099951982498169, 0.7585087418556213, 0.13524781167507172, 0.076655313372612, 0.24400655925273895, 0.09045524895191193, -0.18588927388191223, -1.1155493259429932, 0.006795451510697603, -0.4155665934085846, 1.2128868103027344, -0.13576249778270721, 0.5111636519432068, -0.7247270345687866, 0.1633409559726715, 1.2634083032608032, 0.4271901845932007, 0.35709401965141296, -0.6285917162895203, 0.08129861205816269, -0.9073483943939209, -0.3266134560108185, -0.701926589012146, 0.29702937602996826, -0.3384377360343933, -1.0439963340759277, -0.9243941903114319, -0.8669911623001099, 0.4833776652812958, -0.82142573595047, 0.7335835695266724, -0.6492350697517395, 0.15459005534648895, -0.16352711617946625, 0.4375703036785126, 0.6801843643188477, 0.6440203785896301, 0.03668840974569321, 0.2147350013256073, 1.3235654830932617, -1.0670874118804932, -0.6228899359703064, -0.9493691325187683, 0.35955727100372314, 0.0028218207880854607, 0.044749874621629715, 0.07212880998849869, -1.3218291997909546, -1.105512261390686, -0.8878549933433533, -0.17341244220733643, -0.5543684363365173, -0.20599830150604248, 0.5556899309158325, 0.6455464363098145, -1.3857672214508057, 0.6132916212081909, -0.4017382562160492, -0.06473968923091888, 0.8210693001747131, 0.3143789768218994, 0.5693597793579102, 0.043756935745477676, -1.3915051221847534, 0.1984461396932602, 0.13865305483341217, -0.5483383536338806, -0.29385626316070557, -0.7001737952232361, -1.1255574226379395, 0.3212222754955292, 0.500835657119751, -0.1944638043642044, 1.3603061437606812, -0.24792836606502533, -1.313679575920105, 0.7003210783004761, -0.41696423292160034, -0.09406881034374237, 0.19299207627773285, -0.2381884604692459, -0.3180140256881714, -0.4262794256210327, -0.4431956112384796, 0.6256133913993835, 0.8905490636825562, 0.08328809589147568, -0.3177758455276489, 0.1569126844406128, -0.3384566605091095, -0.18438823521137238, -0.6210145354270935, 0.7636231780052185, -0.7130047082901001, -0.1989702433347702, 0.05397549271583557, 0.6541239023208618, -0.20353730022907257, -0.1026095300912857, -0.5880928635597229, -1.0475059747695923, 0.7659950852394104, 0.36771178245544434, 0.888778567314148, -1.2647581100463867, -0.9391449093818665, 0.22951200604438782, 0.5345079302787781, 0.1254863142967224, -0.6386762261390686, 0.15481705963611603, -0.45489904284477234, 0.04822828620672226, 0.28747284412384033, -0.7400271892547607, 0.5080227255821228, -0.3639875650405884, -0.6880490183830261, -0.038757987320423126, -0.13801245391368866, 1.348663091659546, -0.9552096724510193, -0.1860455572605133, 0.1302855908870697, 0.4042743444442749, -0.975016713142395, 1.2432295083999634, -0.2163742333650589, -0.48884254693984985, 0.00040955821168608963, -0.31151673197746277, 0.099758580327034, -0.5264730453491211, 0.3928118944168091, -0.8485438823699951, 0.17581455409526825, 0.36873093247413635, -0.22185295820236206, 1.3227331638336182, -0.3552144169807434, 0.7964690923690796, -0.24576115608215332, -0.8398752808570862, 0.5272220969200134, 0.2508662939071655, -0.016466740518808365, -0.9180741310119629, 0.48976752161979675, -0.0882035344839096, -0.8522198796272278, 0.35384446382522583, 0.8806649446487427, 1.2973403930664062, -0.3314553499221802, -0.19811375439167023, 0.7540912628173828, -0.15374316275119781, 0.013839494436979294, 0.7499634027481079, 0.7989890575408936, 0.14576290547847748, 0.5418172478675842, -0.29171910881996155, 0.048197466880083084, -1.1469342708587646, -0.12726864218711853, 0.712576687335968, 0.4006969630718231, 0.5518087148666382, 0.38466107845306396, -0.5802074074745178, -0.6643548607826233, -0.25238972902297974, 0.5101467370986938, 1.428109049797058, 0.0785757377743721, -0.43593987822532654, -0.5992216467857361, -0.2356037199497223, -0.4461968243122101, 0.03675272315740585, -0.43176543712615967, -0.5938166379928589, -0.14558865129947662, -0.6578122973442078, 0.66877681016922, 0.42277783155441284, 1.010867953300476, -0.5320966839790344, -0.5556403994560242, -0.25892648100852966, 0.29909467697143555, -1.079842209815979, -0.7224002480506897, 0.7291972041130066, -0.21470697224140167, 0.15111331641674042, -0.14668752253055573, 0.07390425354242325, 0.2116016298532486, -0.6438122987747192, 1.085209608078003, -0.8138536810874939, -0.1769111305475235, 0.2593020796775818, 0.24131087958812714, -0.4236314296722412, -0.36253106594085693, 0.4971131384372711, -0.11234834790229797, -0.06612882018089294, 0.338158518075943, 0.13198523223400116, -0.3158115744590759, -0.038542598485946655, -0.22414584457874298, 0.05272220820188522, 0.15753644704818726, -0.23364654183387756, 0.48199766874313354, -0.51556396484375, -0.15021106600761414, -1.0859260559082031, 0.5825684070587158, 0.00964481569826603, -0.49768200516700745, -0.05426081269979477, -0.9437413215637207, -0.10131129622459412, 0.2077755182981491, -0.5031895637512207, 0.0824100598692894, -0.757526695728302, -0.09155696630477905, -0.6191341280937195, 0.0005060804542154074, -0.32611989974975586, 0.1837630271911621, 0.12262178957462311, 0.16424134373664856, 0.8132297396659851, 0.019160469993948936, 0.5005884170532227, 0.9831181168556213, -0.5914662480354309, 0.7053957581520081, 0.32642462849617004, 0.2364254742860794, -0.3255993127822876, -0.2003539502620697, -0.7495094537734985, -0.30570849776268005, -0.31364110112190247, 0.020598717033863068, -0.5275973081588745, 0.5384662747383118, -0.6456909775733948, -1.1940873861312866, -0.18024949729442596, -0.9718676805496216, -0.16143916547298431, -0.26605892181396484, -0.48921820521354675, -0.29078108072280884, -0.8827741146087646, -1.005041241645813, -0.5314041376113892, -1.037269115447998, -1.351435899734497, 0.21007943153381348, 0.08351615071296692, -0.3372950255870819, -0.4353356659412384, -0.37942951917648315, -0.6676298379898071, 1.3380929231643677, -0.8228477835655212, 0.3599424362182617, -0.09018594771623611, -0.44384172558784485, 0.03719528391957283, -0.18514420092105865, 0.5174559354782104, -0.36897754669189453, -0.1402197927236557, -1.0822504758834839, 0.17454035580158234, -0.7324981093406677, -0.29509007930755615, 0.42319953441619873, 0.3886410892009735, 1.1746188402175903, 0.031696587800979614, -0.5181918740272522, 0.7250283360481262, 1.5375723838806152, -0.5508983731269836, 0.4553532898426056, 0.10544851422309875, 1.0666406154632568, -0.6743632555007935, -0.414163202047348, 0.7625875473022461, 0.446118026971817, 0.5921050310134888, 0.6836054921150208, -0.3365587592124939, 0.0666704773902893, -0.20012733340263367, 0.6261336207389832, 1.2785323858261108, 0.48489147424697876, -0.2073991298675537, -0.40551891922950745, 0.868378758430481, -1.0696462392807007, -0.833844006061554, 0.598499059677124, 0.5763126015663147, 0.27598321437835693, -0.08042866736650467, -0.14911925792694092, -0.10032138973474503, 0.31672534346580505, 0.6891602873802185, -0.2919327914714813, -1.0534051656723022, -0.10511291772127151, 1.0047231912612915, 0.7410975098609924, 0.5973504781723022, -0.027609139680862427, 0.6719270944595337, 14.7561674118042, 1.074338436126709, -0.26958298683166504, 0.8469966650009155, 0.6396658420562744, 0.08241021633148193, -0.5351537466049194, -0.10978196561336517, -0.994452714920044, 0.07723722606897354, 1.062393069267273, 0.25760704278945923, 0.34122276306152344, 0.2221870869398117, -0.20224381983280182, 0.48115310072898865, -0.38630053400993347, 0.8539702892303467, 0.4363730847835541, -1.307752251625061, -0.17373330891132355, 0.27043381333351135, 0.0001996775681618601, 0.709437370300293, 1.0626121759414673, 0.7389148473739624, 0.8623383641242981, -0.24921096861362457, 0.644010603427887, 0.25901859998703003, 1.3442906141281128, -0.0022299569100141525, 0.08074618875980377, -0.10933758318424225, -1.0543330907821655, -0.004189042840152979, -0.6241812705993652, -0.7085781097412109, 0.180012047290802, 0.57980877161026, -0.195252925157547, -0.5522176027297974, 0.0676737055182457, 0.7084951996803284, 0.08003003895282745, 0.29290834069252014, -0.18286694586277008, 0.837984561920166, -0.05890914425253868, -0.25522127747535706, 0.3070310056209564, 0.8425835371017456, 0.010504687204957008, 0.06455068290233612, 0.03229421377182007, -0.031887851655483246, 0.13869218528270721, 0.4173673093318939, -0.430963933467865, -0.018472280353307724, 0.08489467203617096, 0.07503962516784668, -0.09980692714452744, 0.7745496034622192, 0.6766073703765869, -0.04835251346230507, -0.465899258852005, 0.7374219298362732, 0.48200371861457825, 0.019289856776595116, -0.8572598695755005, -0.06305905431509018, 0.4952374994754791, -0.58113032579422, 0.32087910175323486, 0.7377834916114807, 0.1275690793991089, -0.5471162796020508, -0.850884199142456, -0.2649255394935608, 0.35796675086021423, -0.9795341491699219, -0.35986876487731934, 1.0423321723937988, -0.5081768035888672, -0.28210946917533875, 0.5178448557853699, -0.5354676246643066, -0.035646967589855194, 0.20390228927135468, -1.4636133909225464, -0.29569220542907715, -0.01728765107691288, -0.15705835819244385, -0.39600157737731934, -0.055076587945222855, 1.0064432621002197, 0.23974980413913727, 0.1909787356853485, 0.48747655749320984, 0.06517349183559418, -0.19180135428905487, -0.14101065695285797, -0.7531048059463501, 1.0029469728469849, 0.2166476994752884, -0.19668717682361603, -0.39757513999938965, 0.06598930060863495, 0.6494554877281189, -0.6710463166236877, -0.2535915672779083, 0.4143980145454407, -0.5164770483970642, -0.2929859161376953, -1.0511729717254639, -0.658157467842102, 0.2847895622253418, 0.9114171266555786, 0.14727894961833954, 0.14859822392463684, -0.055613722652196884, -0.7797687649726868, -0.4050959646701813, -0.45363765954971313, -0.015031748451292515, 0.46472281217575073, -0.675724983215332, 0.09286770224571228, -0.3389558494091034, 0.5332288146018982, -0.9664365649223328, -0.6522695422172546, -0.4462946653366089, 0.14355234801769257, -0.27558550238609314, 1.1577534675598145, -0.05791695415973663, 0.7574822902679443, 1.1877877712249756, -0.20591185986995697, -0.5766075253486633, -0.4701155722141266, -0.9740819334983826, -0.17870332300662994, 0.31440386176109314, 0.3967474102973938, -0.4248584806919098, 0.7541144490242004, 0.37550655007362366, 0.26990973949432373, -0.7137818336486816, -0.7169015407562256, -0.15446056425571442, -0.21712440252304077, -0.7435411214828491, 0.3465372920036316, -0.07553662359714508, 0.09824436157941818, 0.057463664561510086, 0.25901949405670166, 0.07685030251741409, 0.08363623172044754, -0.9630869030952454, 0.09625013917684555, -0.09619119018316269, 0.023226868361234665, -0.752399742603302, -0.9879702925682068, -1.7184219360351562, -0.04177774488925934, -1.1878864765167236, 0.026985464617609978, -0.6255086660385132, -0.34269478917121887, 0.18463654816150665, -0.5919243097305298, 0.18959295749664307, 0.19237445294857025, -0.08081310242414474, -0.11438455432653427, -0.8519572615623474, -0.8246159553527832, 0.913730263710022, 0.6879251003265381, -1.0336905717849731, 0.47961050271987915, -0.20966051518917084, -0.31867310404777527, -0.12840643525123596, 0.39120161533355713, -0.4681079387664795, -0.9691697359085083, -1.0645054578781128, 0.4412561058998108, -0.09080207347869873, -0.0872323215007782, -0.8619216680526733, 0.8526437878608704, 0.4854189157485962, -0.3171192705631256, -0.07547818124294281, 0.46441134810447693, -1.1151137351989746, -0.7017306685447693, 0.44459283351898193, -0.9297453165054321, 0.2105674147605896, 0.21241414546966553, -0.6469382047653198, -0.09935987740755081, 0.8809981346130371, 0.22238057851791382, -1.0036484003067017, -1.0652471780776978, 0.6790006756782532, -0.6853445768356323, 0.42308762669563293, -0.2645154297351837, -0.41103804111480713, -1.2099977731704712, -0.6596391797065735, -0.24903909862041473, 0.2744296193122864, -0.6352860331535339, 0.982721745967865, 0.4975051283836365, -1.2323124408721924, 0.10057460516691208, 0.41157716512680054, 0.17643122375011444, -0.11717074364423752, 0.5242738127708435, 0.4145812392234802, -0.06216004490852356, 0.2315632700920105, 0.15884356200695038, 0.08418960124254227, -0.8006442785263062, 0.36519142985343933, 0.79596346616745, -0.2996836304664612, -0.23035724461078644, 1.1917515993118286, -0.29549723863601685, -0.4937741160392761, 0.1861250400543213, -1.302118182182312, -0.5261747241020203, -0.37350377440452576, 0.27317699790000916, -0.11918697506189346, 0.04964533448219299, 0.08284516632556915, -0.7791904807090759, 0.10282033681869507, -0.06374649703502655, -0.5243803262710571, 0.3394063413143158, 0.11349405348300934, -0.18170244991779327, 0.21612580120563507, 1.2317036390304565, -0.7140660285949707, -0.5128720998764038, -1.183647632598877, -0.8208004832267761, -0.023721100762486458, 0.6338797211647034, -0.010336440987884998, -1.096726655960083, 0.8534196019172668, 0.9559082984924316, 0.4070007801055908, 0.4148418605327606, 0.06621544808149338, 0.28261157870292664, 0.35923609137535095, -0.11772159487009048, -0.5031233429908752, -0.28957757353782654, 1.309064269065857, 0.5902392268180847, -0.5905219912528992, 0.13579845428466797, -0.2191922813653946, -0.5187796950340271, 0.5645712614059448, 0.2688949406147003, -0.17149774730205536, 1.187270998954773, 0.1392187774181366, 0.0358290821313858, 0.09802025556564331, -1.1769734621047974, -0.5147517323493958, 0.9258625507354736, 1.0090290307998657, 0.5063320994377136, 0.021912602707743645, 0.5495697855949402, 0.7428520917892456, 0.08758342266082764, -0.07103294879198074, 0.23064714670181274, 0.19155803322792053, -0.2450839877128601, 0.22449330985546112, 0.21607089042663574, 0.7124297022819519, -0.7752809524536133, -0.8473013639450073, 0.6107897162437439, 0.19898906350135803, 0.23387379944324493, 0.4643286168575287, 1.1989060640335083, -0.2571106553077698, 0.5858411192893982, -0.388236939907074, 0.6694352030754089, -0.19046427309513092, -0.18122711777687073, -0.2912140488624573, -0.9709419012069702, -0.4920858144760132, -0.049727488309144974, -0.3956180512905121, -0.16667412221431732, -0.3122932016849518, 0.24643175303936005, 0.13257023692131042, 0.09921234101057053, 1.0319223403930664, 0.5979941487312317, 0.9899426698684692, -0.1747385561466217, -0.38431864976882935, -0.29001957178115845, -0.6079673171043396, 0.030118733644485474, -0.39359089732170105, -0.044341519474983215, 0.04883353039622307, 0.17141826450824738, -0.38175705075263977]}, "authors": [{"authorId": "2248177403", "name": "Lucas D. Lingle"}], "references": [{"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "2d01b6afbc86cba1cb895dbcd9396b13952bf0e5", "title": "Focus Your Attention (with Adaptive IIR Filters)"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "412e266cddfd87c79087a88ba1e4d11b89a45a13", "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "5e52d654fd31f04c1bd884cd5480e6af8c95ad50", "title": "Efficient Transformers with Dynamic Token Pooling"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "af68f10ab5078bfc519caae377c90ee6d9c504e9", "title": "Flow Matching for Generative Modeling"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "6ac1fccf1e04487d439ee598f51c03ddac5144ca", "title": "N-Grammer: Augmenting Transformers with latent n-grams"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "3476a29b7af78020367123153c6e1a7ce586e7a0", "title": "Towards Robust Blind Face Restoration with Codebook Lookup Transformer"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "5ed0092c0e9244c3fb6ad40589a72ab853621db8", "title": "Efficient-VDVAE: Less is more"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "8fbc2d349d3d0945efa5e92fd3713734ce63d19e", "title": "Autoregressive Image Generation using Residual Quantization"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "12809bcb734beafeb47876f42e7b438e27fe99fe", "title": "General-purpose, long-context autoregressive modeling with Perceiver AR"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "601ab36b6f077ff57472f4a0cf2e061dd05b9b85", "title": "Discrete Representations Strengthen Vision Transformer Robustness"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "231e768f0cd280faa0f725bb353262cb4fed08d1", "title": "Hierarchical Transformers Are More Efficient Language Models"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "94bcd712aed610b8eaeccc57136d65ec988356f2", "title": "Variational Diffusion Models"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "b1250bb76f81cb120d96336997b83e3fd9174a4a", "title": "Densely connected normalizing flows"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d", "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "806adbb35ed4a95f51518f5962fd59685ad4706b", "title": "Query-Key Normalization for Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "c828f4bf1a752700dd2c4a96fdd08ba938cda43d", "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "67dea28495cab71703993d0d52ca4733b9a66077", "title": "Jukebox: A Generative Model for Music"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0", "title": "Stabilizing Transformers for Reinforcement Learning"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "a8e3093513ea5e9ee4dd40e02c6cff8546500d4a", "title": "New Loss Functions for Fast Maximum Inner Product Search"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "6be216d93421bf19c1659e7721241ae73d483baf", "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "2d08ed53491053d84b6de89aedbf2178b9c8cf84", "title": "Fast Decoding in Sequence Models using Discrete Latent Variables"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "title": "Neural Discrete Representation Learning"}, {"paperId": "e644a409b4a4c6eaedffe27efbc5c76280b34c61", "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "41f1d50c85d3180476c4c7b3eea121278b0d8474", "title": "Pixel Recurrent Neural Networks"}, {"paperId": "425ff4094c8d43020ac410334f2ee1a16df70863", "title": "Adaptive IIR Filters"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for JAX"}, {"paperId": null, "title": "H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "1f53e69f94a1020b48ea7d282221c576555c38a3", "title": "ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer"}, {"paperId": "f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c", "title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": "be0dd2e91bb104494feeb5da2761cf930564f650", "title": "Under review as a conference paper at ICLR 2016"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": "015ca32bca81dbda1e2e432445eef798582236e1", "title": "Conference Paper"}, {"paperId": null, "title": "Addressing some limitations of transformers with feedback memory, 2020b. URL https://arxiv.org/ abs/2002.09402"}, {"paperId": null, "title": "Training compute-optimal"}, {"paperId": null, "title": "return cache_var_upper_div_lower, cache_var_lower Code 4: Jax/Flax pseudocode to get cache variables for all blocks"}]}