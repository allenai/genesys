{"paperId": "4e2a3ddeaf5defb9318b43c364b9a9efe3847dcf", "abstract": "This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.", "venue": "arXiv.org", "year": 2023, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments."}, "embedding": {"model": "specter_v2", "vector": [0.6136960983276367, 0.05419889837503433, -0.4295836389064789, -0.3116680681705475, -0.7747335433959961, 0.09294412285089493, 0.7632250189781189, 0.004043631721287966, -0.6389602422714233, -0.24243642389774323, 0.6487313508987427, -0.4078247547149658, 0.22406376898288727, 0.13227586448192596, -0.19089403748512268, 0.12617580592632294, -1.102104663848877, 0.10512775927782059, -0.12444552034139633, -0.3252123296260834, -0.2011035978794098, -0.8237466812133789, -1.165040373802185, 0.3850763440132141, 0.5646346807479858, 0.5575843453407288, 0.3200781047344208, 1.205150842666626, -0.44036221504211426, 0.23434513807296753, 0.7350477576255798, -0.18769444525241852, 0.22394880652427673, 0.0762011706829071, -0.3590868413448334, -0.39615777134895325, 0.12798473238945007, -0.42016321420669556, -0.42115288972854614, 0.5369913578033447, -0.25658175349235535, 0.4602404832839966, -0.03684749826788902, -0.5364285111427307, -0.04499774053692818, 0.8778518438339233, 0.6352714896202087, 0.9404463171958923, 0.06427787989377975, -0.9155690670013428, 1.360277533531189, -1.5058484077453613, -0.24156521260738373, 1.4324558973312378, 0.3258647620677948, 0.3200969099998474, -0.26476526260375977, -0.580780029296875, 1.108589768409729, 0.27992522716522217, -0.5412718057632446, -0.24989555776119232, 0.06077264994382858, 0.11899150162935257, 1.6294807195663452, 0.008524693548679352, 0.5964688062667847, 0.5682940483093262, -0.07402531057596207, 1.3802200555801392, -0.41505566239356995, -1.0200668573379517, -0.2462964504957199, 0.269487589597702, 0.2577524781227112, 0.5135822892189026, -0.6175244450569153, 0.08817651867866516, -0.9625585079193115, -0.41927406191825867, 0.08269914239645004, 0.23946857452392578, 0.04447309672832489, -0.27713924646377563, 0.01954972743988037, 0.6127046346664429, 0.028147488832473755, 0.749403715133667, 0.05839843302965164, 0.7508918642997742, 0.6239373683929443, 0.34825003147125244, 0.3954744338989258, 0.048613291233778, -0.0032044060062617064, -0.3167244791984558, -0.8160348534584045, 0.41709890961647034, 0.09086982905864716, 0.8207314014434814, -0.37816905975341797, -0.33993884921073914, -0.7532345056533813, 0.49574586749076843, 1.372093915939331, -0.0972786694765091, 0.47342270612716675, -0.5674549341201782, 0.6017563939094543, -0.8976666331291199, 0.22868609428405762, -0.35297098755836487, 0.26727887988090515, -0.06805956363677979, -0.39716625213623047, -1.4919304847717285, -0.631280779838562, 0.13604135811328888, -0.34042006731033325, 0.4871523678302765, -0.19183452427387238, 0.4194457232952118, -0.09554531425237656, 0.06426776945590973, -0.024390660226345062, 0.6981997489929199, 0.20234431326389313, -0.03996788337826729, 0.8299656510353088, -0.8164867758750916, -0.6692159175872803, -1.4029723405838013, 0.5241774916648865, -0.20092958211898804, 0.7422699928283691, -0.02332361601293087, -1.4669374227523804, -0.7727324366569519, -1.0053685903549194, -0.030594905838370323, -0.4723242223262787, -0.37289682030677795, 1.171705961227417, 0.3737570345401764, -0.898580014705658, 0.9640457630157471, -0.4808237850666046, -0.10053960233926773, 0.3402424454689026, 0.16148392856121063, 0.5983580946922302, -0.6165940761566162, -1.4881407022476196, 0.24557042121887207, -0.053285084664821625, -0.6260464787483215, -0.00603879988193512, -0.7164342999458313, -1.050011396408081, 0.18009306490421295, 0.41957342624664307, -0.18645821511745453, 1.5809246301651, -0.2695371210575104, -1.3246660232543945, 0.08006773144006729, -0.3038293421268463, 0.1453961580991745, 0.3075859844684601, -0.40641942620277405, -0.4908936619758606, -0.27523180842399597, -0.4842701554298401, 0.06928416341543198, 0.2085447907447815, 0.10475095361471176, -0.618986964225769, 0.12529908120632172, -0.09130266308784485, 0.4492734670639038, -0.4472074508666992, 0.7048465013504028, -0.8398867845535278, 0.002547364216297865, 0.07445363700389862, 0.7890877723693848, -0.20287704467773438, 0.21306103467941284, 0.4270929992198944, -0.8046581745147705, 1.1028903722763062, -0.2267814576625824, 1.4041943550109863, -0.8972141146659851, -0.8591541647911072, 0.18849052488803864, -0.3139132261276245, 0.04665258154273033, -0.9010648131370544, 0.7160253524780273, -0.29140785336494446, 0.15968969464302063, -0.21616533398628235, -1.1671690940856934, 0.11113046854734421, -0.11109865456819534, -0.6093238592147827, -0.4422738254070282, -0.3671281039714813, 1.0144869089126587, -1.1341784000396729, -0.018591836094856262, 0.0575949102640152, 0.6034623384475708, -1.1665257215499878, 1.4950993061065674, -0.5885054469108582, 0.18019241094589233, -0.1334572732448578, -0.27292871475219727, -0.05035664513707161, -0.09569557756185532, 0.7227242588996887, -0.32453861832618713, 0.0677877813577652, 0.6398024559020996, -0.3013206720352173, 1.376253604888916, -0.8330395221710205, 0.6592703461647034, 0.363576203584671, -0.5967639088630676, 0.049463506788015366, 0.4982265830039978, -0.04190440848469734, -0.3901754915714264, 0.27722519636154175, -0.044815726578235626, -0.6536521911621094, 0.1478414088487625, 0.9617867469787598, 0.8923134207725525, -0.346089243888855, 0.376660019159317, 0.2172958105802536, -0.34331369400024414, 0.45609140396118164, 0.5047659873962402, 0.2335696667432785, 0.430716335773468, 0.36319148540496826, 0.25410741567611694, 0.2992844879627228, -1.4443583488464355, -0.35008320212364197, 0.3986964821815491, 0.40583372116088867, 0.22379857301712036, 0.44947758316993713, -0.40538468956947327, -0.37183231115341187, 0.23897333443164825, 0.49283337593078613, 1.910016417503357, -0.32388216257095337, -0.29302653670310974, -0.825686514377594, -0.12460599094629288, -0.2310783416032791, 0.2957225441932678, -0.25028133392333984, -0.17546066641807556, -0.49071595072746277, -0.7652024626731873, 0.5828201174736023, 0.5774408578872681, 0.8449353575706482, -0.21711322665214539, -0.06937265396118164, -0.12294984608888626, -0.09471239894628525, -0.7429943084716797, -0.9315313100814819, 0.2782314121723175, -0.5310726165771484, -0.04477237910032272, 0.29540395736694336, 0.20170873403549194, -0.11996978521347046, -0.3210310935974121, 0.7313870787620544, -0.2100062221288681, -0.21161089837551117, 0.8450914025306702, 0.3425227701663971, -0.7104558944702148, -0.7425652742385864, -0.19884395599365234, 0.13692224025726318, 0.06314893066883087, 0.43966934084892273, 0.16781003773212433, 0.01867215894162655, -0.31082653999328613, -0.3461468815803528, 0.386811226606369, -0.17443391680717468, -0.020897289738059044, 0.40996402502059937, -0.659970760345459, -0.3814217746257782, -1.2055549621582031, 1.1568067073822021, -0.05114399269223213, -0.5108781456947327, 0.2950704097747803, -1.0097665786743164, -0.5044074654579163, 0.334512323141098, -0.8788143396377563, -0.44384825229644775, -1.08641517162323, 0.08495453000068665, -0.2236127406358719, 0.01791752129793167, 0.316236287355423, 0.22764348983764648, 0.17022448778152466, -0.09296836704015732, 0.5265402793884277, 0.5149841904640198, -0.12831324338912964, 0.6140711903572083, -0.3611016571521759, 0.33232519030570984, -0.17484445869922638, -0.10620027780532837, -0.1509399712085724, -0.2610480785369873, -0.7081846594810486, -0.3446013629436493, -0.35425254702568054, -0.39127376675605774, -0.3985229432582855, -0.1664295196533203, -0.6762762665748596, -0.7413647770881653, -0.6263949871063232, -0.7788263559341431, -0.6436989307403564, 0.19732362031936646, -0.02078971080482006, -0.16747885942459106, -0.796881377696991, -1.006378412246704, -0.7557548880577087, -0.7595696449279785, -0.9517366886138916, 0.035442888736724854, -0.21473266184329987, -0.23062734305858612, -0.9438213109970093, -0.13982252776622772, -0.11347877979278564, 1.0652575492858887, -0.8205040097236633, 0.7052870988845825, 0.11891075223684311, -0.13784828782081604, -0.4654541313648224, 0.15061907470226288, 0.48341450095176697, -0.16405275464057922, 0.06342794001102448, -1.028364658355713, -0.09602704644203186, -0.34374257922172546, 0.12257169932126999, -0.02280307561159134, 0.02422165684401989, 1.03253972530365, 0.05865698680281639, -0.611659049987793, 0.44117817282676697, 1.045163869857788, -0.5538644194602966, -0.07534158229827881, -0.1253042072057724, 0.8877626061439514, -0.06715705990791321, 0.0417155995965004, 0.7213307619094849, 0.29313042759895325, 0.47921353578567505, 0.0973348468542099, -0.11820558458566666, 0.21720093488693237, -0.7342759966850281, 0.480737566947937, 1.5314663648605347, 0.022961383685469627, 0.15171626210212708, -0.8976869583129883, 0.4763312339782715, -1.227279543876648, -0.6664057970046997, 1.1936969757080078, 0.9867807626724243, 0.40080785751342773, -0.5725373029708862, -0.34589287638664246, -0.24666868150234222, 0.28341951966285706, 0.5274818539619446, -0.40103328227996826, -0.8018190264701843, 0.7158520817756653, 0.3294295072555542, 0.01925666257739067, 1.1883939504623413, -0.18281058967113495, 0.36722248792648315, 14.847516059875488, 0.6351727247238159, 0.06307277828454971, 0.8554635643959045, 0.5010567903518677, 0.02995237149298191, -0.47918596863746643, -0.24759672582149506, -0.9018580317497253, -0.08818121254444122, 1.529524326324463, -0.22403955459594727, 0.9850544333457947, 0.09051689505577087, 0.1712779551744461, 0.20253898203372955, -0.7020597457885742, 0.65354323387146, 0.5608027577400208, -1.042657494544983, 0.16718541085720062, 0.11945614218711853, -0.06918075680732727, 0.6360372304916382, 0.8525493741035461, 0.9619946479797363, 0.43767043948173523, -0.1960374116897583, 0.280449241399765, 0.16601704061031342, 1.2119148969650269, -0.339511513710022, 0.16046947240829468, 0.5608461499214172, -1.203966736793518, -0.6804842948913574, -0.5669701099395752, -1.1431963443756104, 0.16157442331314087, -0.19200700521469116, -0.43227720260620117, -0.6598508358001709, -0.18666709959506989, 0.45855632424354553, 0.19931307435035706, -0.046010490506887436, 0.12091173976659775, 0.7263978719711304, -0.10438577085733414, -0.04912257939577103, 0.22535307705402374, 0.2675136923789978, 0.09459806233644485, 0.2223660945892334, -0.04421893134713173, 0.0737282857298851, 0.005053726490586996, 0.25840678811073303, -0.4359884262084961, -0.08344388008117676, -0.41034242510795593, 0.02276778221130371, 0.1856665462255478, 0.6206445097923279, 1.0259391069412231, 0.006300953682512045, -0.8478187322616577, 0.33970317244529724, 0.5462459325790405, 0.2678945064544678, -0.2952200472354889, 0.21415825188159943, 0.5574727654457092, -0.20016486942768097, 0.02880239114165306, 0.5373324155807495, 0.2411787360906601, -0.6983526349067688, -0.7796037793159485, -0.371786504983902, 0.3244515359401703, -0.6507493853569031, -0.4941578209400177, 0.7394621968269348, 0.4943082928657532, -0.32199627161026, -0.002887449227273464, -0.5662584900856018, -0.5406134128570557, 0.2820374071598053, -1.2954225540161133, -0.5743616223335266, 0.3302254378795624, -0.07553432136774063, -0.2966693043708801, 0.3602951765060425, 1.4891788959503174, 0.46018293499946594, -0.20351426303386688, 0.4232875108718872, 0.16736745834350586, -0.24933195114135742, -0.23189722001552582, -0.6524406671524048, 0.6479790210723877, 0.311419814825058, -0.03605141118168831, 0.226284921169281, -0.09891701489686966, 0.2516583204269409, -0.9442281126976013, -0.20289930701255798, 0.7239625453948975, -0.790673017501831, -0.4237945079803467, -1.19657564163208, -0.8124774694442749, 0.4939371347427368, 0.42468520998954773, -0.1053759977221489, 0.7447932362556458, 0.44933223724365234, -0.552513837814331, -0.5111536979675293, -0.9039420485496521, 0.330862820148468, 0.38063693046569824, -0.8382495045661926, -0.1314905434846878, 0.003135796170681715, 0.6389300227165222, -1.1782985925674438, -0.4567093551158905, -0.5195937752723694, 0.2653254270553589, -0.08334838598966599, 1.0711506605148315, -0.029143458232283592, 0.19851557910442352, 1.2748451232910156, -0.3910275101661682, -0.5352074503898621, 0.04038761556148529, -0.8062022924423218, -0.3621656596660614, -0.046322040259838104, 0.6205628514289856, -0.2093294858932495, 0.04848317801952362, 1.2925193309783936, 0.5558401942253113, -0.7061575055122375, -0.6218147277832031, 0.4999912679195404, 0.16905158758163452, -1.1727749109268188, 0.7258764505386353, -0.30912306904792786, 0.30784860253334045, 0.017029928043484688, 0.4189266860485077, 0.5440154075622559, -0.14434321224689484, -0.8895578384399414, 0.30413180589675903, 0.26902392506599426, -0.18051964044570923, -0.1885174959897995, -0.05314042791724205, -1.4553335905075073, -0.37762776017189026, -1.0917744636535645, -0.13693007826805115, -0.6220557689666748, -0.4046832323074341, 0.036971405148506165, -0.5192030668258667, -0.20635487139225006, 0.20463179051876068, -0.4490832984447479, -0.011785547249019146, -0.9120930433273315, -0.5042791366577148, 0.835496723651886, 0.8723565340042114, -0.5843859910964966, 0.006437008269131184, 0.2852236032485962, 0.014628983102738857, 0.15786755084991455, 0.28558722138404846, -0.10086803138256073, -0.9409531354904175, -1.0831938982009888, 0.3772205710411072, 0.040492042899131775, -0.24365092813968658, -0.5072183012962341, 0.6612890362739563, 0.08704780787229538, -0.17910847067832947, -0.018900597468018532, 0.7217317819595337, -1.138952374458313, -0.7493873834609985, 0.10425716638565063, -1.0442216396331787, 0.022207435220479965, 0.253803014755249, -0.2641105353832245, -0.0052469391375780106, 0.9027019143104553, -0.05850190669298172, -0.9067720174789429, -0.4952855706214905, 0.5994694232940674, -0.7173391580581665, -0.14103206992149353, -0.47368350625038147, 0.15408296883106232, -0.46494197845458984, -0.7812730073928833, 0.02364635467529297, 0.3877370357513428, -0.7608662247657776, 0.9767454862594604, 0.533703088760376, -1.0479124784469604, 0.1913316398859024, 0.6536442041397095, -0.20323461294174194, 0.023995257914066315, 0.442814439535141, 0.3624045252799988, 0.06981192529201508, 0.3967733085155487, 0.5258328914642334, 0.19647154211997986, -1.2953944206237793, 0.08604562282562256, 0.8341948390007019, -0.6094971895217896, -0.22977863252162933, 0.9948256015777588, -0.6076955795288086, -1.0584694147109985, 0.43472012877464294, -1.2867233753204346, -0.5908275842666626, -0.6189396977424622, 0.9473122358322144, 0.06246357783675194, -0.20585724711418152, 0.4127219617366791, -0.22301757335662842, 0.20843875408172607, -0.1913483440876007, -0.7447476983070374, 0.48679208755493164, -0.6056711673736572, -0.35405540466308594, 0.8264096975326538, 0.9356905221939087, -0.5853165984153748, -0.8703657984733582, -0.9619532227516174, -0.7828873991966248, -0.5029121041297913, 0.5192078351974487, -0.373487263917923, -0.1488923281431198, 0.8027387857437134, 0.7348361611366272, 0.16842953860759735, 0.24428525567054749, -0.01699633151292801, 0.3468058109283447, 0.7590891122817993, 0.053551170974969864, -0.8334831595420837, -0.6761923432350159, 1.0666568279266357, 1.3788892030715942, -0.6786048412322998, 0.6358148455619812, 0.06300131231546402, -0.6529527902603149, 0.7746356725692749, 0.32189786434173584, 0.25622817873954773, 1.2958675622940063, 0.06494360417127609, 0.24216055870056152, 0.6123009324073792, -1.5353162288665771, 0.20685677230358124, 0.6906503438949585, 1.0601410865783691, 0.7468440532684326, 0.5135353803634644, 0.16323676705360413, 1.188158631324768, 0.28255391120910645, 0.2869398593902588, 0.28406432271003723, 0.48818808794021606, -0.19486765563488007, -0.3910406231880188, -0.18821896612644196, 0.6757445931434631, -0.7912800908088684, -0.7126243710517883, 0.604396641254425, 0.41242727637290955, 0.19470077753067017, 0.6791313290596008, 1.1047736406326294, 0.10188888758420944, 0.5482547283172607, 0.24370598793029785, 0.5426410436630249, -0.5084496140480042, -0.368317186832428, -0.06447189301252365, -0.6209090948104858, -0.26684218645095825, 0.06949877738952637, -0.512749969959259, -0.3866157829761505, -0.24169492721557617, 0.5055437088012695, 0.23748667538166046, 0.31200677156448364, 1.0274258852005005, 0.5533276200294495, 0.42828765511512756, -0.1635451465845108, -0.7123256921768188, -0.502105176448822, -0.9992802143096924, -0.0021118938457220793, -0.4927677810192108, -0.19177360832691193, 0.054571639746427536, -0.21168924868106842, -0.3949258029460907]}, "authors": [{"authorId": "2271499923", "name": "Jang-Hyun Kim"}, {"authorId": "2256572330", "name": "Junyoung Yeom"}, {"authorId": "2293774509", "name": "Sangdoo Yun"}, {"authorId": "2256580479", "name": "Hyun Oh Song"}], "references": [{"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "84a36e19f9394f22b34f79756fa9628a795e02ea", "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"}, {"paperId": "60b0476a97c00e355df28ba35422764a7fbe88e8", "title": "In-context Autoencoder for Context Compression in a Large Language Model"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "c53e3020e4b8f9e1cd7b6ed35221480a2647ea26", "title": "Meta-Learning Online Adaptation of Language Models"}, {"paperId": "c3a59e1e405e7c28319e5a1c5b5241f9b340cf63", "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory"}, {"paperId": "17170575aa8b4fa4e3eef5d366ada706a94dd836", "title": "LaMP: When Large Language Models Meet Personalization"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "f78fe02f681a0a9a6867b007bd39e3884de64a91", "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization"}, {"paperId": "37ba9c33025fb31f25436010e12c65a0bafc0e1f", "title": "Meta-Learning Fast Weight Language Models"}, {"paperId": "b3d04ab5362b3fb171b5231dcf4c675c4c64ec02", "title": "HyperTuning: Toward Adapting Large Language Models without Back-propagation"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "4afda39036206dcb3f97829dccb897f1fc80f459", "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models"}, {"paperId": "8fbd7ddf1ea30c991f3b1152a245df77caa18e16", "title": "Learning by Distilling Context"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ba233d75aa403092bda0bffc026be7913673ad69", "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "3108f96f80d129036f53684344f4058257b37c4b", "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "f61e9fd5a4878e1493f7a6b03774a61c17b7e9a4", "title": "Memory-Efficient Backpropagation Through Time"}, {"paperId": "415229903f91a1f3fc7404f5e5997fde025c221d", "title": "Deep learning and the information bottleneck principle"}, {"paperId": "0ba86604228b555475496e200f31878df3aabd6e", "title": "Never-Ending Learning"}, {"paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "An overview of bard: an early experiment with generative ai"}, {"paperId": null, "title": "Together Computer"}]}