{"paperId": "a5f26555194d50955f6b3fdafb04d4330cb272dc", "abstract": "The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This survey reviews the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs."}, "embedding": {"model": "specter_v2", "vector": [-0.21914492547512054, 0.5077775716781616, -1.0504132509231567, -0.6534931659698486, -0.3232780396938324, 0.14342743158340454, 0.21645183861255646, -0.2434825599193573, -0.7697305679321289, 0.15747086703777313, 0.6843405365943909, 0.22499656677246094, -0.2031523585319519, 0.3520960211753845, -0.10849130153656006, 0.33221349120140076, -0.905078649520874, 0.6684499979019165, -0.2409394532442093, -0.2419937402009964, -0.37491920590400696, -0.835904061794281, -0.8479668498039246, 0.15105409920215607, 0.5325290560722351, -0.013412583619356155, 0.6800786852836609, 0.9472551941871643, 0.0340462289750576, 0.18875940144062042, 1.0517985820770264, -0.4979085922241211, 0.39745602011680603, 0.330513060092926, -0.13737383484840393, -0.047357432544231415, 0.18908262252807617, -0.6546112895011902, -0.19077461957931519, 0.5908895134925842, 0.02210688777267933, 0.25234004855155945, 0.2709123194217682, -0.8319857120513916, -0.24947205185890198, 0.8700801730155945, 0.8448023200035095, 0.34084296226501465, 0.21698935329914093, -0.7417691946029663, 1.7117828130722046, -1.2471530437469482, 0.21787814795970917, 1.632609248161316, 0.19713744521141052, 0.11584657430648804, -0.21814626455307007, -0.6455014944076538, 0.5605754256248474, -0.4464842677116394, -0.8639304041862488, -0.27537065744400024, 0.11929883062839508, -0.40661847591400146, 1.3481850624084473, -0.011062693782150745, -0.13578076660633087, 0.728522539138794, -0.15398748219013214, 1.4968773126602173, -0.18651387095451355, -1.0662487745285034, -0.1903281956911087, 0.5609286427497864, 0.3425646722316742, 0.9405021071434021, -0.4967830181121826, 0.7783327698707581, -1.2227003574371338, -0.48357969522476196, 0.6466690301895142, -0.39691591262817383, -0.2817445397377014, -0.7497963309288025, -0.5720652341842651, 0.9022861123085022, -0.035368774086236954, 0.6388434767723083, -0.2184244841337204, 0.2709731459617615, 0.03938324376940727, 0.6041409373283386, -0.2647869884967804, 0.29258328676223755, -0.5410618185997009, 0.4133641719818115, -0.8008239269256592, 0.25491732358932495, 0.03306363523006439, 1.0751874446868896, -0.292489230632782, -0.23587748408317566, -1.0586490631103516, 0.5451372861862183, 2.01236629486084, -0.02805289812386036, 0.5942515134811401, -0.813499927520752, 0.3614453375339508, -0.7146201729774475, 0.8673259019851685, -0.3602776825428009, -0.15740211308002472, -0.08605014532804489, -0.5737012028694153, -1.1124348640441895, -0.29212161898612976, 0.306850403547287, -0.36020898818969727, 0.7549206614494324, -0.5804913640022278, -0.7503533959388733, 0.26445722579956055, 0.7260347008705139, 0.049361735582351685, 0.635485053062439, 0.3084181547164917, -0.08189023286104202, 0.4700002372264862, -0.7723297476768494, -0.42526599764823914, -1.055760145187378, 0.9712972044944763, -0.1027432233095169, 0.783409059047699, -0.17977620661258698, -1.066346526145935, -0.9468520879745483, -0.820221483707428, -0.0924173966050148, 0.11916442215442657, 0.8137977123260498, 1.4713882207870483, 0.5517634153366089, -1.3320854902267456, 0.7711327075958252, 0.04226722940802574, -0.32732298970222473, -0.1962633579969406, 0.3663202226161957, 0.2249685674905777, -0.5995060205459595, -1.5487512350082397, 0.229226753115654, 0.2963952124118805, -0.9377447366714478, -0.05661977082490921, -0.0960756167769432, -1.458255648612976, -0.4289468228816986, 0.04761533811688423, -0.648328959941864, 1.337730050086975, -0.3200608789920807, -1.5085645914077759, 0.6137987971305847, -0.08919373899698257, 0.3057118356227875, 0.6409057974815369, -0.17227940261363983, -1.069425344467163, -1.0100250244140625, -0.3195764422416687, 0.40472111105918884, 0.03520814701914787, -0.415987104177475, -0.5504394769668579, 0.4171505868434906, 0.04569580405950546, 0.0691971629858017, -0.3968293368816376, 1.343544363975525, -0.7293670177459717, -0.2000013291835785, -0.16122795641422272, 0.606642484664917, -0.5140619874000549, 0.21867386996746063, -0.11475956439971924, -0.8998861312866211, 0.3782583475112915, -0.09794548898935318, 1.3271673917770386, -0.6478347182273865, -0.7733055949211121, 0.031503062695264816, -0.3995122015476227, -0.26618948578834534, -1.0066487789154053, 0.5344545245170593, 0.07035938650369644, 0.19348347187042236, -0.24719616770744324, -0.9015654921531677, 0.16822107136249542, -0.2896139621734619, -0.05623362585902214, -0.3067091107368469, -0.657159149646759, 0.6775341629981995, -1.3645052909851074, -0.15268529951572418, 0.16439470648765564, 0.21950224041938782, -1.0215715169906616, 1.4331843852996826, -0.2990069091320038, 0.27923783659935, -0.34257134795188904, -0.4241204261779785, 0.00035571851185522974, -0.2652055025100708, 0.5584157109260559, -0.07964284718036652, -0.3897079825401306, 0.382445365190506, -0.5254087448120117, 1.4954019784927368, -0.48986756801605225, 0.30892181396484375, -0.07670101523399353, 0.06876835227012634, -0.015477275475859642, 0.891533613204956, -0.16535374522209167, -0.16121019423007965, -0.033179327845573425, 0.7570230960845947, -0.5446746945381165, 0.12404926866292953, 0.39929938316345215, 0.6353234052658081, -0.3605080842971802, 0.37725380063056946, 0.2018703669309616, -0.14778737723827362, 0.7248925566673279, -0.18278653919696808, 0.16954348981380463, 0.3845037519931793, 0.7438655495643616, -0.7537789940834045, 0.47701752185821533, -1.0568791627883911, 0.030758582055568695, 0.20439431071281433, 0.5202168822288513, 0.632366418838501, -0.49709001183509827, -0.8393471240997314, 0.2252923995256424, -0.12604720890522003, 0.8978005647659302, 1.597502589225769, -0.19859538972377777, -0.20136471092700958, -0.3880467414855957, -0.39900144934654236, -0.4851595163345337, 0.6856563687324524, 0.00990225188434124, 0.4823877215385437, -0.6662614941596985, -0.7089383006095886, 0.289667010307312, 0.38971036672592163, 0.5725709795951843, -0.6560645699501038, -0.42491018772125244, -0.20051586627960205, 0.08064749836921692, -0.47586724162101746, -1.1895694732666016, 0.008792701177299023, -0.6270474791526794, 0.06693031638860703, -0.40155768394470215, -0.2307996302843094, 0.22045668959617615, -1.0083802938461304, 0.8670666813850403, -0.28149697184562683, 0.3689263164997101, 0.11153186112642288, 0.5588813424110413, -0.6179856061935425, -0.7947165966033936, -0.1673983335494995, 0.5920960307121277, -0.25479361414909363, 0.18163256347179413, 0.681842029094696, 0.4644317924976349, 0.37228068709373474, -0.02123226411640644, 0.4191289246082306, 0.4196743369102478, -0.1776430606842041, 0.2469974160194397, -0.33526909351348877, -0.4123198688030243, -1.1878185272216797, 1.2177661657333374, 0.5395941138267517, -0.6528705358505249, 0.6257239580154419, -0.9337162971496582, -0.02869104966521263, 0.4582400918006897, -0.5390052795410156, -0.4092680811882019, -0.6136879920959473, 0.5065857768058777, 0.16665993630886078, -0.2772851586341858, 0.5655889511108398, -0.20260818302631378, 0.1860722452402115, -0.06268193572759628, 0.43288710713386536, 0.1262364387512207, -0.3023768663406372, 0.37823769450187683, -0.28398364782333374, 0.0961042270064354, -0.17190484702587128, 0.07931193709373474, -0.5625854730606079, -0.9142779111862183, -0.7384311556816101, -0.08494507521390915, -0.2822214365005493, -0.6026566028594971, -0.0534849539399147, -0.02377988025546074, -0.6841329336166382, -0.20511098206043243, -0.08102721720933914, -1.2047353982925415, -0.05145781859755516, 0.4789871871471405, 0.4496471583843231, -0.1358339637517929, -0.8145933747291565, -0.8526723384857178, -0.7742319703102112, -0.38498368859291077, -1.244207501411438, 0.054042864590883255, -0.15497669577598572, -0.7057891488075256, -0.5546569228172302, 0.19642996788024902, -0.006386332679539919, 0.9746175408363342, -1.279558777809143, 1.263005256652832, 0.194212406873703, -0.16450554132461548, -0.4052022695541382, 0.5530574917793274, 0.23306185007095337, -0.006944330409169197, 0.16210344433784485, -0.5422065258026123, 0.022277988493442535, -0.32195165753364563, -0.4591631293296814, -0.4350198805332184, 0.34806838631629944, 0.7162403464317322, 0.035789649933576584, -0.47058674693107605, 0.11229114979505539, 0.8799234628677368, -0.5503198504447937, -0.20609986782073975, 0.21482710540294647, 1.0209630727767944, 0.7774292826652527, -0.043296996504068375, 0.5420098900794983, 0.6133230924606323, 0.9010698199272156, -0.2208484411239624, -0.17945009469985962, 0.31674131751060486, -0.7250614166259766, 0.8450729250907898, 1.840826392173767, -0.363712877035141, -0.27633562684059143, -0.7767654061317444, 0.5321877598762512, -1.498790979385376, -0.31225836277008057, 0.7410900592803955, 1.326977014541626, 0.32128962874412537, -0.3021048605442047, -0.11005357652902603, -0.38804158568382263, 0.7372358441352844, 0.4918398857116699, 0.18181948363780975, -0.6003298759460449, 0.1894657462835312, -0.24539873003959656, 0.08844324946403503, 1.11002516746521, -0.27743732929229736, 0.47387605905532837, 14.435608863830566, 0.6817803382873535, 0.07366127520799637, 0.5656319856643677, 0.6853915452957153, 0.4050825238227844, -1.0179872512817383, -0.12367988377809525, -0.9905023574829102, 0.03297216445207596, 1.1821651458740234, -0.12974001467227936, 0.7556719183921814, 0.09518905729055405, 0.458770751953125, 0.34218552708625793, -0.9208269715309143, 0.9876630902290344, 0.1448054015636444, -0.9345130920410156, 1.063377022743225, 0.11978957802057266, 0.07603377848863602, 0.3377743065357208, 0.5355465412139893, 0.6925017237663269, 0.8570552468299866, -0.5461592674255371, 0.9748240113258362, 0.2188946157693863, 0.8820544481277466, -0.03543005883693695, 0.5181021094322205, 1.0535480976104736, -0.9137997031211853, -0.033056456595659256, -0.6493409872055054, -1.507607340812683, 0.4800034463405609, 0.2902733087539673, -0.352923184633255, -0.2898457646369934, -0.5054810047149658, 0.5670746564865112, 0.4385879337787628, 0.09397565573453903, -0.37534138560295105, 0.4968886077404022, -0.16832886636257172, -0.059943731874227524, -0.1383574903011322, 0.27795565128326416, 0.2005101889371872, -0.2298526018857956, 0.3594779372215271, -0.16102850437164307, 0.5054932236671448, 0.2803049385547638, -0.4445025622844696, 0.23640307784080505, -0.306989848613739, -0.4246261715888977, -0.02873619645833969, 0.3405737578868866, 0.9422484040260315, 0.581653356552124, -0.3956473171710968, 0.1471969187259674, 0.6229487061500549, 0.03122297301888466, 0.16689151525497437, 0.5557398796081543, 0.5084168314933777, -0.21815243363380432, -0.6224746108055115, 0.6140120029449463, 0.06561701744794846, -0.5146452784538269, -0.620603084564209, -0.5049936771392822, 0.6497213244438171, -0.6653481721878052, -0.7591084241867065, 0.9272735714912415, 0.1399315744638443, -0.7516352534294128, 0.1274164617061615, -0.7606008052825928, -0.14614050090312958, 0.4655052423477173, -1.0758841037750244, -1.0056530237197876, 0.8143569231033325, -0.37818795442581177, -0.053970325738191605, -0.05228184908628464, 1.446833610534668, 0.03284190222620964, -0.3459838926792145, 0.2003016471862793, 0.17908594012260437, -0.14956358075141907, 0.27267807722091675, -0.49002307653427124, 0.5758423209190369, 0.2105962485074997, -0.11121389269828796, 0.4977046549320221, 0.19514907896518707, -0.12925809621810913, -0.6150466203689575, -0.06349802762269974, 1.5422930717468262, -0.9427782297134399, -0.6909423470497131, -0.7750289440155029, -0.4198627769947052, 0.3974918723106384, 0.4669407308101654, -0.39386266469955444, 0.5442932844161987, 0.07796870917081833, 0.044420208781957626, 0.1270061433315277, -0.8962672352790833, 0.055596694350242615, 0.6921798586845398, -0.47932153940200806, -0.19900090992450714, -0.13373267650604248, 0.21375639736652374, -0.8365252614021301, -0.15018874406814575, -0.0016808058135211468, 0.29367658495903015, 0.08876790851354599, 0.9231064319610596, -0.5297046899795532, 0.23972465097904205, 0.5678340792655945, -0.4130173325538635, -1.1655412912368774, -0.8008241653442383, -1.0224626064300537, 0.2918608784675598, -0.31119585037231445, 1.3362617492675781, -0.2727513015270233, -0.03989638388156891, 1.334703803062439, 0.5174686908721924, -0.13831950724124908, -0.29646268486976624, -0.2700859010219574, -0.1990637332201004, -0.5598333477973938, 0.529767632484436, -0.10344789177179337, -0.13781367242336273, 0.22368021309375763, -0.3464818000793457, 0.9026367664337158, 0.16185688972473145, -0.6303115487098694, 0.33512750267982483, -0.5150730013847351, -0.14209063351154327, -0.6336838603019714, 0.20074276626110077, -1.5394896268844604, 0.013747269287705421, -1.0884954929351807, 0.25237879157066345, -1.2217625379562378, -0.2343035340309143, -0.08658158779144287, -0.5881514549255371, -0.48683810234069824, 0.44960007071495056, -0.9565945267677307, -0.6625917553901672, -0.0983118861913681, -0.4249824285507202, 0.8028094172477722, 0.9921881556510925, -0.5371894240379333, 0.045702870935201645, 0.3419225811958313, 0.09804550558328629, 0.565173864364624, 0.3748999536037445, -0.5134118795394897, -0.9205002188682556, -1.462860107421875, 0.2870299816131592, -0.048776861280202866, -0.29521921277046204, -0.3400912582874298, 0.7111997008323669, -0.3148549795150757, -0.10088103264570236, 0.16788381338119507, 0.19185686111450195, -1.0917751789093018, -0.10855823010206223, 0.32802146673202515, -1.0889923572540283, 0.18793685734272003, -0.2033090889453888, -0.5435487031936646, -0.3620094954967499, 0.6609305143356323, -0.11494188755750656, -1.1481032371520996, -1.0235729217529297, 0.7005752921104431, -0.5820845365524292, -0.444801390171051, -0.23272792994976044, 0.04171345755457878, -0.3089565932750702, -0.5950077772140503, 0.30090227723121643, 0.6453944444656372, 0.010745285078883171, 1.1489496231079102, 0.5992431044578552, -1.1684831380844116, -0.12738575041294098, 0.48847657442092896, 0.19336798787117004, -0.4730696678161621, 0.16576014459133148, 0.21700988709926605, -0.2677731215953827, 0.8693638443946838, 0.18082666397094727, 0.6599984169006348, -1.1214762926101685, -0.418712854385376, 0.5706650614738464, -0.716543972492218, -0.13822419941425323, 1.4008787870407104, -0.17442363500595093, -1.6980643272399902, 0.24247251451015472, -0.9381437301635742, -0.6280354261398315, -1.0647836923599243, 0.5123404264450073, 0.20070716738700867, -0.47583481669425964, -0.19492332637310028, -0.7008585929870605, 0.16372627019882202, 0.012938386760652065, -0.7916271090507507, 0.5570237636566162, -0.5189347267150879, -0.40294235944747925, 0.7842722535133362, 0.5389686226844788, -0.177492156624794, -0.824889600276947, -0.18251211941242218, 0.08051683008670807, -0.49951016902923584, 0.49254000186920166, -0.6402965784072876, -0.544887363910675, 0.4576687216758728, 0.7088445425033569, -0.5258625149726868, -0.23327209055423737, -0.08327416330575943, 0.3867276906967163, 0.46827375888824463, 0.9271318912506104, -0.9720507860183716, -0.9165598750114441, 0.9838948845863342, 1.6141502857208252, -1.2106350660324097, 0.15387330949306488, -0.3606899678707123, -1.1255621910095215, 0.93036949634552, 0.46122750639915466, 0.5507357120513916, 0.9725598692893982, -0.3484996557235718, 0.6685597896575928, 0.1689889281988144, -0.7246094942092896, 0.1948392689228058, 1.0977619886398315, 0.9158970713615417, 1.593343734741211, 1.1148285865783691, -0.3274087905883789, 1.1335769891738892, -0.04780860245227814, 0.8751558065414429, 0.5490907430648804, 0.5881674885749817, -0.27328360080718994, -0.6943721175193787, 0.1561264544725418, 0.6630795001983643, 0.016981039196252823, -0.5560276508331299, 0.17579364776611328, 0.15131883323192596, 0.15821616351604462, 0.4750356078147888, 0.07158297300338745, 0.014504886232316494, 0.4718075096607208, 0.21570879220962524, 0.2484249770641327, -0.792285144329071, -0.10657112300395966, -0.2677496671676636, -0.12200111895799637, -0.15157651901245117, -0.31693536043167114, -0.4170837104320526, -0.3183342516422272, 0.2288064956665039, 0.5386032462120056, 0.18108659982681274, -0.09271642565727234, 0.9497872591018677, 0.7144456505775452, -0.2753540277481079, -0.5299217700958252, -0.1884925216436386, -0.5350093841552734, -0.9678741693496704, -0.3214704990386963, -0.7784443497657776, -0.3027190864086151, -0.13461370766162872, -0.08307443559169769, -0.2880048453807831]}, "authors": [{"authorId": "2307740913", "name": "Ruili Jiang"}, {"authorId": "2266796043", "name": "Kehai Chen"}, {"authorId": "2273663144", "name": "Xuefeng Bai"}, {"authorId": "2307044191", "name": "Zhixuan He"}, {"authorId": "2257093356", "name": "Juntao Li"}, {"authorId": "2105775", "name": "Muyun Yang"}, {"authorId": "2237773157", "name": "Tiejun Zhao"}, {"authorId": "2284688853", "name": "Liqiang Nie"}, {"authorId": "2273887691", "name": "Min Zhang"}], "references": [{"paperId": "c3f1fae241a3c2449e675ab750873d800f95513c", "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"}, {"paperId": "2052a297586451686bbf959c47254cc3db13abab", "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process"}, {"paperId": "77dbbafed1edc6f159845f4a9b58886d4ab6ab62", "title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function"}, {"paperId": "973814cd535facbf4f27c3de477b05bf19366030", "title": "ORPO: Monolithic Preference Optimization without Reference Model"}, {"paperId": "66e7edf09589527ebb58418632418758cee668cd", "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models"}, {"paperId": "d27da1ba65fa958e45837120fad1c25e7017d80c", "title": "Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings"}, {"paperId": "8be81d531dfc4a1145474a1bb2f9c0cf15e19f45", "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration"}, {"paperId": "bb8e669d51503918c87e7462c7879e92a241c814", "title": "History, Development, and Principles of Large Language Models-An Introductory Survey"}, {"paperId": "bf40c8f88875f7c591dddc0936542918f4083b22", "title": "A Roadmap to Pluralistic Alignment"}, {"paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998", "title": "KTO: Model Alignment as Prospect Theoretic Optimization"}, {"paperId": "f977dac98cc603bfccae6ea991cf4b1f83bf139c", "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank"}, {"paperId": "3a589ce2b38da3083ce1b63b2785646536a364f9", "title": "Towards Efficient Exact Optimization of Language Model Alignment"}, {"paperId": "e360eb07461f2741793f99ece8b97a6c04fb2b68", "title": "MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization"}, {"paperId": "59084df7203c6be33838ba3e3854eb9bda053ed2", "title": "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint"}, {"paperId": "411114f989a3d1083d90afd265103132fee94ebe", "title": "Mixtral of Experts"}, {"paperId": "ef9e058d22d190fdd38ddee367cf6aa8d1a14bd5", "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"}, {"paperId": "ec639bacdf18bdd2e715c55b6ecd3471bd72f240", "title": "Preference as Reward, Maximum Preference Optimization with Importance Sampling"}, {"paperId": "aee47d4f45d5c02f79fff62ce4147f0d382cd87e", "title": "Aligning Large Language Models with Human Preferences through Representation Engineering"}, {"paperId": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817", "title": "A Survey of Reinforcement Learning from Human Feedback"}, {"paperId": "485f8a429cf5f70c558181187f2d62e31784deaa", "title": "Reasons to Reject? Aligning Language Models with Judgments"}, {"paperId": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976", "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"}, {"paperId": "d1f925c65d56ff4de5d317a54d47d6df34b17d4e", "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model"}, {"paperId": "1f1638ca845881545f364ca23b1fae46f729e72f", "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference"}, {"paperId": "84259db14b725853ecfe425fe85ca375b32983c2", "title": "Adversarial Preference Optimization"}, {"paperId": "e51f20efb872d0ba99a8b501259948bbb2f6963f", "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"}, {"paperId": "07cdf957a11506f87fbc030dcfaaa6399847648c", "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"}, {"paperId": "b168e265fad7cfc3fec54904f0cbc94d60f34794", "title": "AI Alignment: A Comprehensive Survey"}, {"paperId": "8dc334f0dd716585eee05d2daaff330de4916a9e", "title": "SuperHF: Supervised Iterative Learning from Human Feedback"}, {"paperId": "9262e201ecaf678429bf86481b1c1d1ae6332b44", "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks"}, {"paperId": "378a51082ddf7430f928b7dde59186c041eb4b6c", "title": "Tuna: Instruction Tuning using Feedback from Large Language Models"}, {"paperId": "0f7308fbcae43d22813f70c334c2425df0b1cce1", "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"}, {"paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f", "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"}, {"paperId": "a84d6b82947f27bc6bf7f42d69f48b40adcfb6c3", "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"}, {"paperId": "2be910eb19f2f8f2e8038d2a835bc48f868ccbf1", "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models"}, {"paperId": "d3f8d5a9c48ee9f338f25f1be5f3adc7fd5dd855", "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"}, {"paperId": "dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208", "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values"}, {"paperId": "4118c8bca76b4bfafc379d80bf91455df88614b6", "title": "Aligning Language Models with Human Preferences via a Bayesian Approach"}, {"paperId": "5001630bcc65e8e0e621b19625629a2689724743", "title": "Generative Judge for Evaluating Alignment"}, {"paperId": "e6776f5f293c18f4b2322b1479f083cb24d33343", "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"}, {"paperId": "af7669dc48c70d8cf6fccdf1322d6056a6b39dc8", "title": "Confronting Reward Model Overoptimization with Constrained RLHF"}, {"paperId": "023d462ec6ff84cee0d0716a34d11efc7cde8534", "title": "Reward Model Ensembles Help Mitigate Overoptimization"}, {"paperId": "311b5c770738fabc940b3b630664d562916df83c", "title": "Tool-Augmented Reward Modeling"}, {"paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3", "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"}, {"paperId": "e8df1cf6742b50a15500b8dd3dde3942e9c91418", "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "860c8de4fdac38695ff6860dd15312f1079c6117", "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"}, {"paperId": "749d59f887c8ac83fd4f5178465e8b03e463358c", "title": "Large Language Model Alignment: A Survey"}, {"paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5", "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"}, {"paperId": "29f032fc875576b5c3c6b1c2d76af8639bacfb88", "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "19a0777498ec3ef1e11e8349df3eb336cc19698d", "title": "Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback"}, {"paperId": "22ab4219371366a4e890382bc0ca606130840ca7", "title": "Statistical Rejection Sampling Improves Preference Optimization"}, {"paperId": "74b4b993babe99bc5f5c589c27fef0f1baba606b", "title": "Making Large Language Models Better Reasoners with Alignment"}, {"paperId": "cb587eaea753ee38013afb7e5b6bc8fba1248d04", "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"}, {"paperId": "c12db2e67d1fb289266faa5507ff112c9a062465", "title": "Efficient RLHF: Reducing the Memory Usage of PPO"}, {"paperId": "78b0c5d96a05b4be6a00702fba24c9174e8173af", "title": "Aligning Language Models with Offline Learning from Human Feedback"}, {"paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7", "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"}, {"paperId": "f2ba9e7d9624bd94a786ea5e3161a9425a21a475", "title": "Self-Alignment with Instruction Backtranslation"}, {"paperId": "7f55ef29a6f8b2771c5435bbeba29c87264fdc88", "title": "Shepherd: A Critic for Language Model Generation"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "92930ed3560ea6c86d53cf52158bc793b089054d", "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"}, {"paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e", "title": "Preference Ranking Optimization for Human Alignment"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "ccd94602e3acecf999d0c9ba62b1a8bc02e9f696", "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"}, {"paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8", "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"}, {"paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"}, {"paperId": "be8db99310602d66bba64bcf41a572c45816fbfc", "title": "Let's Verify Step by Step"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "title": "Large Language Models are not Fair Evaluators"}, {"paperId": "13fd4277388cc2a9da75e8b772e5efcf6ebe2d32", "title": "Training Socially Aligned Language Models on Simulated Social Interactions"}, {"paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99", "title": "Aligning Large Language Models through Synthetic Feedback"}, {"paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d", "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "ebf35cef5c249d90b40043fffa41f8802c27f132", "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156", "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"}, {"paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a", "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"}, {"paperId": "68c834c19cd126bbd6d25a3572d7205cfed76271", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"}, {"paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae", "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "d2170504c4ad9403bea118ae8debdfda95978546", "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers"}, {"paperId": "cb3125e4f63f3d058a2a39270ecb585e86c3d1ff", "title": "Chain of Hindsight Aligns Language Models with Feedback"}, {"paperId": "2cd72e71299c5d62d5cdb1164df5236172d418c4", "title": "Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654", "title": "Constitutional AI: Harmlessness from AI Feedback"}, {"paperId": "3eed4de25636ac90f39f6e1ef70e3507ed61a2a6", "title": "Talking about Large Language Models"}, {"paperId": "de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a", "title": "Fine-tuning language models to find agreement among humans with diverse preferences"}, {"paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "title": "Measuring Progress on Scalable Oversight for Large Language Models"}, {"paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b", "title": "Scaling Laws for Reward Model Overoptimization"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "912a39c2e0e4a35747531669cfa952d2c5627729", "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "39d05ffbc06fdca54ea6a90cd6d7fca202809aaa", "title": "Understanding Dataset Difficulty with V-Usable Information"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "49f905eb03958c7cfae52ac759ea8978b8b2a6ea", "title": "Alignment of Language Agents"}, {"paperId": "7e38476342ce1fcc8ef0dcd23686539395961769", "title": "Inductive biases for deep learning of higher-level cognition"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "f9ef88bfc78baeb24e697b05c307cf019f8a3630", "title": "Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models"}, {"paperId": "ec55f76812cacc12d29ec632d924377524a13022", "title": "Don\u2019t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "90d8f96e2cd71a50b40992020cb65bc75f352ea1", "title": "Better Rewards Yield Better Summaries: Learning to Summarise Without References"}, {"paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c", "title": "Neural Text Generation with Unlikelihood Training"}, {"paperId": "04babf1157c27c2d8abe24be5c45c6e99078daf0", "title": "Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators"}, {"paperId": "b0b96270a9bbeb9f3ec040e70114d565fbcaaed9", "title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!"}, {"paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c", "title": "Scalable agent alignment via reward modeling: a research direction"}, {"paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a", "title": "Reward learning from human preferences and demonstrations in Atari"}, {"paperId": "32bfc242d0e85f1b6f2ff838c37287f8cfddf7c2", "title": "Towards Coherent and Cohesive Long-form Text Generation"}, {"paperId": "0052b31f07eda7737b5e0e2bf3803c3a32f3f728", "title": "Supervising strong learners by amplifying weak experts"}, {"paperId": "e77213a99f05ff8525fa4ee59a3b066164b994da", "title": "Training Dialogue Systems With Human Advice"}, {"paperId": "15919637566348de8ca1e054151c24cc864b0f0e", "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "f5265e346382354887340c7b520d639162e2f598", "title": "TL;DR: Mining Reddit to Learn Automatic Summarization"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "13d156f37ecea807706fd117547ac1b805d5c5aa", "title": "Learning Language Games through Interaction"}, {"paperId": "53fcb4a07f6825439a311e61b98c007c5641e2f0", "title": "Score-based Inverse Reinforcement Learning"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "f009a64f87841b79b1e2ea1748366270455ef9c0", "title": "Model-Free Preference-Based Reinforcement Learning"}, {"paperId": "21c05d606d6899c42ae02e3b671e92faaaf130a7", "title": "Active reward learning with a novel acquisition function"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "16302319d910a1da77656133727f081c65995635", "title": "Programming by Feedback"}, {"paperId": "ad4122205be956ea90930df42507cdc0079acbc0", "title": "A Policy Iteration Algorithm for Learning from Preference-Based Feedback"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "1ee3d855069c31b7990a1ac554be9e20f408aefb", "title": "Learning non-myopically from human-generated reward"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "b3af2e367d7297775c71fa9a61b0b49fb888bc38", "title": "A Bayesian Approach for Policy Learning from Trajectory Preference Queries"}, {"paperId": "80128eac4063571da71f8e731ac6b137dd208e24", "title": "Learning from human-generated reward"}, {"paperId": "781cfd4e09dc6d721844391f415518dfa2774f1d", "title": "APRIL: Active Preference-learning based Reinforcement Learning"}, {"paperId": "31936551c1f219abd7e202a1775c9b6d756912bb", "title": "Reinforcement learning from simultaneous human and MDP reward"}, {"paperId": "cbbcf7b0ae2d7503836a5a1db42dd3104579972d", "title": "Preference-Based Policy Learning"}, {"paperId": "6efd7b0e85f735f3aa750df521a40fd5caa5a77c", "title": "Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning"}, {"paperId": "f1c72d92275e460d142db923faa6a9ecbb0a8346", "title": "Effect of human guidance and state space size on Interactive Reinforcement Learning"}, {"paperId": "c54174bd1a98b1ae1fb111b32950fce538f32007", "title": "Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch"}, {"paperId": "accb4b7a1e670ec1be3d5a2e784b8f524ff8b303", "title": "Combining manual feedback with subsequent MDP reward signals for reinforcement learning"}, {"paperId": "256c3bd45ab7452bb51721eb25d3367bb654225e", "title": "Interactively shaping agents via human reinforcement: the TAMER framework"}, {"paperId": "5fc5c5a4e489e781de434567d946e6eb65c44f60", "title": "Learning to rank for information retrieval"}, {"paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f", "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"}, {"paperId": "05d24d6f34197430d0387ad507dba0c90201364f", "title": "Cobot in LambdaMOO: An Adaptive Social Statistics Agent"}, {"paperId": "9dc1748099dd4321d42fb84bc7ee1f71e7814459", "title": "Introduction to the special issue on statistical language modeling"}, {"paperId": "7d986dac610e20441adb9161e5466c88932626e9", "title": "A study of smoothing methods for language models applied to information retrieval"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "109186b81a0575936297ae3a0ff41491124a4bf2", "title": "A social reinforcement learning agent"}, {"paperId": "3c1d0b29b2c2217d9e17b38043fb73033d3efa49", "title": "Cobot: A Social Reinforcement Learning Agent"}, {"paperId": "c6586e7c73cc1c9e9a251947425c54c5051be626", "title": "Two decades of statistical language modeling: where do we go from here?"}, {"paperId": "1e62711ba0e6385ee204b475a238c4e82811fc22", "title": "Cobot in LambdaMOO: A Social Statistics Agent"}, {"paperId": "d71c82fcb1fa2d8bb79bbd848b20f6ec53b076a9", "title": "Creating Advice-Taking Reinforcement Learners"}, {"paperId": "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "title": "An Empirical Study of Smoothing Techniques for Language Modeling"}, {"paperId": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "title": "A Maximum Likelihood Approach to Continuous Speech Recognition"}, {"paperId": "2894125ea5f8a3300bd098e7be2331f7789ff91b", "title": "Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of Resources"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "ac771182d1780c863954243809d1e144433919f9", "title": "Aligning Large Language Models with Human: A Survey"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": "7c363f962654392d67a3323cfeba4ae9cf1dec32", "title": "\u201cTechnique for the Measurement of Attitudes, A\u201d"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "0639baa6dfb35d962e46eb0c38763d769ffb0946", "title": "Models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6", "title": "GENERATIVE ADVERSARIAL NETS"}, {"paperId": null, "title": "via debate"}, {"paperId": "84082634110fcedaaa32632f6cc16a034eedb2a0", "title": "A Survey of Preference-Based Reinforcement Learning Methods"}, {"paperId": "d37d6adbd1212140fc5e1bcba1f119ab7db85bf5", "title": "Preference Learning: An Introduction"}, {"paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"}, {"paperId": "b8014d4af46d9b26cf434240dced24294b2110b6", "title": "Machine Learning manuscript No. (will be inserted by the editor) Preference-Based Reinforcement Learning: A Formal Framework and a Policy Iteration Algorithm"}, {"paperId": null, "title": "\u201cEmergent abilities of large language"}, {"paperId": null, "title": "\u201cMeasuring massive multitask"}, {"paperId": null, "title": "\u201cExtrapolating beyond suboptimal demonstrations via"}, {"paperId": null, "title": "\u201cBack to basics: Revisiting re-inforce style optimization for learning from human feedback in llms,\u201d"}, {"paperId": null, "title": "\u201cConcrete problems in ai"}, {"paperId": null, "title": "\u201cAlpagasus: Training a better alpaca model with fewer data,\u201d"}]}