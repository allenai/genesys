{"paperId": "f06191042e5b20b7d18672fbe9fc85f7b1bc4dfd", "abstract": "Due to the high memory and computational costs associated with Large Language Models, model compression via quantization and parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA), are gaining popularity. This has led to active research on quantization-aware PEFT techniques, which aim to create models with high accuracy and low memory overhead. Among quantization methods, post-training quantization (PTQ) is more commonly used in previous works than quantization-aware training (QAT), despite QAT's potential for higher accuracy. This preference is due to PTQ's low training overhead. However, PTQ-based PEFT methods often utilize high-precision parameters, making it difficult to fully exploit the efficiency of quantization. Additionally, they have limited adaptation ability due to a reduced and constrained LoRA parameter structure. To overcome these challenges, we propose L4Q, which leverages joint quantization and fine-tuning to reduce QAT's memory overhead and produce models that consist entirely of quantized weights while achieving effective adaptation to downstream tasks. By design, L4Q allows quantization parameters to reflect weight updates, while weight updates reduce quantization errors. Our experiments demonstrate that this coupled quantization and fine-tuning approach yields superior accuracy compared to decoupled fine-tuning schemes in sub-4-bit quantization. Using the LLaMA model families and instructional datasets, we showcase L4Q's capabilities in language tasks and few-shot in-context learning.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "L4Q is proposed, which leverages joint quantization and fine-tuning to reduce QAT's memory overhead and produce models that consist entirely of quantized weights while achieving effective adaptation to downstream tasks."}, "embedding": {"model": "specter_v2", "vector": [0.03330640494823456, 0.24050970375537872, -0.4373345971107483, -0.11667593568563461, -0.4101589620113373, -0.009134163148701191, 0.3776882290840149, -0.04959462583065033, -0.6664012670516968, -0.3891822099685669, 0.7970714569091797, 0.2162768393754959, 0.2538624703884125, 0.12413229048252106, -0.2054072767496109, 0.3258730471134186, -0.919433057308197, 0.6316286325454712, 0.013006736524403095, -0.6024109721183777, -0.3686904311180115, -0.8194738626480103, -0.9579687118530273, 0.27211710810661316, 0.42364072799682617, 0.5722424387931824, 0.18306635320186615, 0.8102082014083862, -0.6029633283615112, -0.31552666425704956, 0.6361198425292969, -0.38238632678985596, 0.10642573237419128, -0.16835752129554749, -0.1660756766796112, 0.00529558164998889, 0.16094335913658142, -0.6542553901672363, -0.06097279489040375, 0.8283119797706604, -0.1925867646932602, 0.3589046001434326, 0.8437124490737915, 0.060854747891426086, -0.412944495677948, 0.5306053161621094, 0.6074526309967041, 0.6166723370552063, -0.24299880862236023, -0.752734899520874, 0.9833323955535889, -1.5659184455871582, -0.12676768004894257, 1.584924340248108, 0.5269768834114075, 0.6255779266357422, -0.4685971140861511, -0.7047078013420105, 0.6195309162139893, 0.050442297011613846, -1.1022123098373413, -0.2668665945529938, -0.34207525849342346, -0.16369864344596863, 1.685634970664978, -0.37599819898605347, -0.16411375999450684, 0.5020323395729065, 0.07201071083545685, 0.9068682193756104, -0.13236252963542938, -0.784358561038971, -0.07688078284263611, 0.04917560890316963, -0.053095802664756775, 0.9008751511573792, -0.6428586840629578, 0.2548251152038574, -0.848913311958313, -0.26057514548301697, 0.5000929236412048, -0.48790115118026733, 0.15966571867465973, -0.25854748487472534, 0.15682411193847656, 0.9577837586402893, 0.1867596060037613, 0.42592841386795044, 0.22693726420402527, 0.7509267926216125, 0.7269871830940247, 0.34029966592788696, 0.33617258071899414, 0.49258407950401306, -0.15477146208286285, -0.08589630573987961, -0.768887996673584, 0.06063205376267433, -0.10832349210977554, 0.7789515852928162, -0.06101208180189133, 0.19535507261753082, -0.8265752196311951, 0.8408970832824707, 1.2201933860778809, -0.2306429147720337, 0.7189925909042358, -0.6417887210845947, 0.3094142973423004, -0.9281997084617615, 0.12544146180152893, -0.4998815953731537, -0.12918204069137573, -0.14028464257717133, -0.941156268119812, -1.3290376663208008, -0.699038565158844, 0.29764166474342346, -0.7399839162826538, 0.8901155591011047, -0.3265102505683899, 0.16903243958950043, -0.28632885217666626, 0.5539214611053467, 0.2661901116371155, 0.7416107058525085, 0.5268989205360413, -0.29471835494041443, 0.9159467816352844, -0.8419173955917358, -1.0058687925338745, -0.8945990800857544, 0.9149295687675476, -0.40541109442710876, 0.8338742852210999, -0.062491368502378464, -1.2435671091079712, -0.9294215440750122, -0.8140438795089722, -0.1523868888616562, -0.36644482612609863, 0.4057829678058624, 0.7233859896659851, 0.5410581231117249, -0.6698905229568481, 0.7505101561546326, -0.281202107667923, -0.09639351069927216, 0.24764907360076904, 0.39307132363319397, 0.26856452226638794, -0.30597737431526184, -1.6187927722930908, 0.2665608823299408, 0.364614874124527, -0.7580628395080566, -0.22181382775306702, -0.5693368911743164, -0.8590758442878723, 0.1453138142824173, 0.4440212845802307, -0.4578654170036316, 1.3286679983139038, 0.0025324425660073757, -1.8119205236434937, 0.1830957680940628, -0.059843502938747406, 0.29107698798179626, 0.2794319987297058, -0.30481359362602234, -0.5532078146934509, -0.5334778428077698, -0.6850966811180115, 0.4133332669734955, 0.6160832643508911, 0.1809743493795395, -0.05321619287133217, 0.4484531879425049, -0.3870140612125397, 0.4914233386516571, -0.7155094146728516, 0.6843287348747253, -0.9039714932441711, -0.3657962381839752, 0.3552626669406891, 0.7978647351264954, 0.07021728157997131, -0.22966133058071136, 0.06945478916168213, -1.0238611698150635, 0.5632287263870239, 0.19931359589099884, 1.482431173324585, -0.8248229026794434, -0.560712456703186, 0.04726843535900116, -0.31243202090263367, 0.19408567249774933, -1.0120272636413574, 0.5777493715286255, 0.026540981605648994, 0.6341832280158997, 0.1382714807987213, -1.3183214664459229, 0.24701058864593506, -0.1668022722005844, -0.5265656113624573, -0.1771436184644699, 0.10658008605241776, 1.035355806350708, -0.8780447840690613, 0.03463241830468178, -0.17652636766433716, 0.44363224506378174, -1.2972973585128784, 1.1969587802886963, -0.4516022205352783, 0.16248080134391785, -0.06847436726093292, -0.2729927599430084, -0.01871258206665516, -0.378741592168808, 0.2809313237667084, -0.4366028606891632, 0.20649690926074982, 0.32751455903053284, -0.6100529432296753, 1.5061949491500854, -0.5253458619117737, 0.1718226820230484, -0.09838932007551193, -0.17420896887779236, 0.04307940974831581, 0.4604060649871826, -0.30065107345581055, -0.6862901449203491, 0.27164965867996216, 0.5535131692886353, -0.82973313331604, 0.3181202709674835, 0.8608444929122925, 0.5836533904075623, -0.6045011878013611, 0.15509235858917236, 0.6889890432357788, -0.6696569323539734, 0.7836311459541321, 0.2911638915538788, 0.3581405282020569, 0.3892243504524231, 0.5933136343955994, 0.17301401495933533, 0.30100226402282715, -1.1320991516113281, -0.32282039523124695, 0.5716482996940613, 0.6022309064865112, 0.892279863357544, 0.21139678359031677, -0.5706498026847839, -0.7768847346305847, -0.2656881809234619, 0.5767819285392761, 1.9635885953903198, -0.23468944430351257, -0.3865227699279785, -0.7379195690155029, 0.056058965623378754, -0.43230608105659485, -0.03702343627810478, -0.42788586020469666, -0.45031851530075073, -0.601841926574707, -1.0217758417129517, 0.5868974924087524, 0.03803633153438568, 0.7118285894393921, -0.18061237037181854, 0.21986211836338043, 0.07047610729932785, 0.27912455797195435, -0.6272409558296204, -1.0418691635131836, 0.334769606590271, -0.6587963104248047, 0.3350459933280945, 0.046909771859645844, -0.002249320037662983, -0.11322708427906036, -0.8430564999580383, 1.0160363912582397, -0.7127476334571838, 0.026561390608549118, 0.015454482287168503, 0.29702600836753845, -0.45250967144966125, -0.9661335349082947, 0.4865466356277466, 0.5772767663002014, -0.23086921870708466, 0.31339696049690247, 0.41082218289375305, 0.42954930663108826, 0.15087580680847168, -0.46020951867103577, 0.36992430686950684, 0.24865016341209412, 0.11530209332704544, 1.1045544147491455, -0.44513264298439026, 0.1904151737689972, -1.379435420036316, 1.4345130920410156, -0.003380349837243557, -0.4714375138282776, 0.36043640971183777, -0.9139272570610046, -0.27117919921875, 0.7251504063606262, -0.8444531559944153, -0.2889847457408905, -0.9596601128578186, 0.14667226374149323, -0.4359252452850342, -0.03730140998959541, 0.4386429488658905, 0.4449242055416107, 0.07691143453121185, 0.4929710328578949, 0.2334454506635666, 0.08919524401426315, -0.4820992648601532, 0.6843868494033813, -0.6455572247505188, 0.7150650024414062, 0.1106603816151619, 0.18597330152988434, -0.4642195403575897, -0.31938454508781433, -0.47191986441612244, -0.49879735708236694, -0.5096001625061035, -0.40854644775390625, -0.11167938262224197, 0.03142731264233589, -0.7619315385818481, -0.44169023633003235, -0.512056291103363, -0.6345705986022949, -0.19463233649730682, 0.09905262291431427, -0.04211089760065079, -0.4513389766216278, -0.9936640858650208, -1.5008333921432495, -0.22699816524982452, -0.6257243156433105, -1.085577368736267, 0.10052633285522461, -0.04980206489562988, -0.46877560019493103, -0.345885306596756, 0.1506367176771164, -0.5538653135299683, 0.9705343246459961, -1.1190602779388428, 1.0868639945983887, 0.17270691692829132, 0.13754920661449432, -0.07239102572202682, 0.2836448550224304, 0.5992711186408997, -0.1872168928384781, 0.0051699369214475155, -0.8745341300964355, -0.08465151488780975, -0.44037866592407227, -0.669544517993927, 0.07853078842163086, 0.1128338873386383, 0.9544538259506226, -0.14756466448307037, -0.1720278114080429, 0.8799182772636414, 1.2876890897750854, -0.9693233966827393, -0.1853075921535492, -0.00989555660635233, 1.1516417264938354, -0.0019075641175732017, -0.24377962946891785, 0.6924922466278076, 0.11326466500759125, 0.7222652435302734, -0.15096712112426758, 0.18682648241519928, -0.48152416944503784, -0.6470533609390259, 0.5251107215881348, 2.3527252674102783, 0.41039755940437317, 0.0871211364865303, -0.838228702545166, 0.2966998815536499, -1.005585789680481, -0.4156941771507263, 0.8016349673271179, 0.8929018974304199, 0.8255029320716858, -0.5465154051780701, -0.4818408191204071, -0.5269706845283508, 0.0331852063536644, 0.19166071712970734, -0.36982834339141846, -1.2515604496002197, 0.23891770839691162, 0.13524264097213745, -0.03833146393299103, 0.6744228601455688, -0.6033865809440613, 0.49767690896987915, 14.660996437072754, 1.3131741285324097, 0.19269363582134247, 0.8543882966041565, 0.669029712677002, -0.047810379415750504, -0.320541650056839, -0.7211958765983582, -0.9982960224151611, -0.12055151164531708, 1.5046824216842651, 0.1839473396539688, 1.0297188758850098, -0.027990905568003654, 0.31052711606025696, 0.46143943071365356, -0.401480495929718, 0.8006609082221985, 0.39274388551712036, -1.2283494472503662, 0.6776111125946045, -0.009625421836972237, 0.932140588760376, 0.34793609380722046, 0.9803574681282043, 1.1750560998916626, 0.11203236132860184, -0.49721217155456543, 0.4041438102722168, 0.3694053888320923, 1.2438234090805054, -0.47595322132110596, 0.2052227407693863, 0.49654507637023926, -0.7496020197868347, -0.41841188073158264, -0.8856273889541626, -1.0094341039657593, 0.17452983558177948, -0.08171259611845016, -0.6806453466415405, -0.7400783896446228, -0.03509217128157616, 0.37960898876190186, -0.24091222882270813, 0.43833357095718384, -0.10659271478652954, 0.726101815700531, -0.06721386313438416, 0.25687164068222046, 0.49149760603904724, 0.036457501351833344, 0.10188797861337662, 0.21823672950267792, -0.01573258824646473, 0.29239341616630554, 0.2218555063009262, 0.32925692200660706, -0.6858558058738708, 0.2791089117527008, 0.10295476019382477, 0.016974138095974922, 0.04485366493463516, 0.5020459890365601, 0.9204848408699036, 0.07421962171792984, -0.43603938817977905, 0.35055989027023315, 0.922551155090332, 0.34428054094314575, -0.06605184078216553, 0.22068849205970764, 0.23360811173915863, -0.3272906541824341, -0.08561462163925171, 0.5560481548309326, -0.12076637148857117, -0.5670835375785828, -0.8417198657989502, -0.4417228102684021, 0.10203628987073898, -0.8265042304992676, -0.9126650094985962, 0.5021096467971802, 0.10410654544830322, -0.39171692728996277, -0.22413372993469238, -0.2904421091079712, 0.18501533567905426, 0.61814945936203, -1.0008065700531006, -0.3270617723464966, 0.5880422592163086, -0.5726141333580017, -0.26286160945892334, -0.1797052025794983, 1.6619352102279663, 0.36218729615211487, -0.3514636158943176, 0.3704715073108673, 0.7532669901847839, -0.4049195349216461, 0.13566698133945465, -0.5033154487609863, 0.6512018442153931, 0.11290453374385834, -0.2355862706899643, 0.3313716650009155, -0.00999633688479662, 0.20346559584140778, -0.5256520509719849, -0.24634334444999695, 0.6644141674041748, -0.4627420902252197, -0.41167163848876953, -0.8531025052070618, -1.224204421043396, -0.050191935151815414, 0.16586759686470032, -0.15118734538555145, 0.5165777802467346, 0.25832006335258484, -0.7105879783630371, 0.022724641487002373, -0.5718144178390503, 0.056294169276952744, 0.17254063487052917, -1.1257801055908203, -0.21961578726768494, -0.127126082777977, 0.32794758677482605, -1.3202221393585205, -0.8438751697540283, -0.24009424448013306, 0.0023423107340931892, 0.11999697238206863, 1.0911375284194946, -0.20268502831459045, 0.5386176705360413, 0.8615440130233765, -0.10005052387714386, -0.9564545750617981, -0.34250587224960327, -0.6372225284576416, -0.2923634350299835, -0.10305539518594742, 0.5048712491989136, 0.1577373892068863, 0.2363048493862152, 0.7308748960494995, 0.177860826253891, -0.6803154945373535, -0.8345772624015808, -0.14475734531879425, 0.21631479263305664, -0.5111838579177856, 0.25065234303474426, -0.19827528297901154, -0.2782106399536133, 0.14073285460472107, 0.30424731969833374, 0.5482370853424072, -0.3538491427898407, -1.0699632167816162, 0.15175671875476837, 0.14985644817352295, -0.22115692496299744, -0.5084190964698792, -0.04491429030895233, -1.4286991357803345, -0.2508024275302887, -0.9947460293769836, 0.005897318944334984, -0.6593756079673767, -0.19785842299461365, 0.16515393555164337, -0.23945988714694977, -0.24366343021392822, 0.3352518677711487, -0.015687620267271996, -0.3472374677658081, -0.06691776216030121, -0.7838842272758484, 1.084561824798584, 1.1367061138153076, -0.8641971349716187, -0.21592450141906738, 0.0054657235741615295, 0.5677446722984314, 0.311994343996048, 0.5642555356025696, -0.38830891251564026, -1.0963947772979736, -1.4984101057052612, 0.4760530889034271, -0.2663545608520508, -0.42716509103775024, -0.7345661520957947, 0.6810547709465027, 0.19130627810955048, 0.02927655354142189, 0.2647668421268463, 0.5722841620445251, -0.7832333445549011, -0.5268359184265137, 0.14727291464805603, -1.121660590171814, 0.24432654678821564, 0.4120190739631653, -0.4034728407859802, -0.4403563439846039, 0.3659394681453705, 0.08576802909374237, -0.8937698602676392, -0.785215437412262, 0.4999210834503174, -0.5632191896438599, 0.1574847400188446, -0.7857733964920044, 0.03965819999575615, -0.7289857864379883, -0.484318345785141, -0.0086292065680027, 0.12270551174879074, -0.41676032543182373, 0.8621780276298523, 0.5049152374267578, -1.4915523529052734, 0.029983581975102425, 0.6026155948638916, 0.2191307246685028, -0.11042389273643494, 0.5686994791030884, 0.22697415947914124, -0.1491609662771225, 0.6221470832824707, 0.2669520676136017, 0.43204325437545776, -0.7962824106216431, -0.18538038432598114, 0.8234187960624695, -0.6793069243431091, 0.23071599006652832, 1.166508436203003, -0.3459024727344513, -1.197551965713501, 0.08595895767211914, -1.1960976123809814, -0.6684778332710266, -0.29940542578697205, 0.7590181827545166, 0.012568631209433079, 0.16551975905895233, -0.27438321709632874, -0.2205841988325119, 0.19319458305835724, -0.2795490026473999, -0.49099001288414, 0.3795572817325592, -0.6098662614822388, -0.19891157746315002, 0.8906005024909973, 1.2044858932495117, -0.6882856488227844, -0.9138838052749634, -0.7776150107383728, -0.49335628747940063, -0.10598431527614594, 0.38302454352378845, -0.5985532999038696, -0.2754985988140106, 0.6788023114204407, 0.8644150495529175, -0.20466285943984985, 0.28718510270118713, -0.40513038635253906, -0.052875496447086334, 0.9171026945114136, 0.10794525593519211, -1.0281171798706055, -0.2897268235683441, 1.5155432224273682, 1.268476128578186, -0.8297431468963623, 0.40395882725715637, 0.10268401354551315, -0.5398796200752258, 0.854709267616272, 0.3931291699409485, 0.20190703868865967, 0.8596764206886292, -0.47027620673179626, 0.14675307273864746, 0.3297080993652344, -1.2232983112335205, -0.055131129920482635, 1.2585620880126953, 0.8143781423568726, 0.7791726589202881, 0.42485564947128296, 0.10761310160160065, 0.8850202560424805, 0.035114459693431854, 0.1292019933462143, 0.6736734509468079, -0.013325589708983898, -0.34906521439552307, -0.2887047529220581, -0.02228924259543419, 0.9954081773757935, -0.5506065487861633, -0.528821587562561, 0.5254054069519043, 0.38078227639198303, 0.3304886221885681, 0.3166041076183319, 0.8952271938323975, -0.08380799740552902, 0.6101401448249817, 0.13641975820064545, 0.3677464723587036, -0.7168208360671997, -0.36362922191619873, 0.19083097577095032, -0.8004173636436462, -0.3475046753883362, 0.15979312360286713, -0.10634943842887878, -0.20108750462532043, -0.21888600289821625, 0.35056212544441223, 0.024273743852972984, 0.295476496219635, 0.955812931060791, 0.40651148557662964, 0.41583240032196045, -0.5090502500534058, -0.44691216945648193, -0.8598261475563049, -1.0437912940979004, -0.059971071779727936, -0.15855252742767334, -0.383873850107193, -0.13376383483409882, 0.12595456838607788, -0.37626761198043823]}, "authors": [{"authorId": "2283134717", "name": "Hyesung Jeon"}, {"authorId": "51148698", "name": "Yulhwa Kim"}, {"authorId": "2262513070", "name": "Jae-Joon Kim"}], "references": [{"paperId": "b085968c4362fb286ad6c5ef71a5db9630da0498", "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"}, {"paperId": "3eec0c1a7dc0d364d23e2e4544bf8772f5f8ffa3", "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference"}, {"paperId": "af8123ecdff838f63e4eba0b36b8babe4c5cee65", "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"}, {"paperId": "945db0077b6d19b720f5998b3f61300013c4f885", "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "a10843d1349fff8d2a7d9722f800802187fef67f", "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"}, {"paperId": "8b32aa33601514976d88fabcb060a5cd38d34006", "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3", "title": "Fine-tuned Language Models are Continual Learners"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8cc855854384755336aa05c5376ea455137bfbce", "title": "LSQ+: Improving low-bit quantization through learnable offsets and better initialization"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "dc160709bbe528b506a37ead334f60d258413357", "title": "Learned Step Size Quantization"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "49e60f82d6ae835c56473464f67ca5c11d3e95ec", "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": null, "title": "\u201cStanford Alpaca: An Instruction-following LLaMA model\u201d"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "\u201cA framework for few-shot language model evaluation\u201d"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "\u201cRethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models\u201d"}]}