{"paperId": "4b1655a321392d0fadcf25ef5bd4040123e33244", "abstract": "Current multimodal and multitask foundation models like 4M or UnifiedIO show promising results, but in practice their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually rather small) number of modalities and tasks they are trained on. In this paper, we expand upon the capabilities of them by training a single model on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example image metadata or color palettes. A crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text. Through this, we expand on the out-of-the-box capabilities of multimodal models and specifically show the possibility of training one model to solve at least 3x more tasks/modalities than existing ones and doing so without a loss in performance. This enables more fine-grained and controllable multimodal generation capabilities and allows us to study the distillation of models trained on diverse data and objectives into a unified model. We successfully scale the training to a three billion parameter model using tens of modalities and different datasets. The resulting models and training code are open sourced at 4m.epfl.ch.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The out-of-the-box capabilities of multimodal models are expanded by training a single model on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora, enabling more fine-grained and controllable multimodal generation capabilities."}, "embedding": {"model": "specter_v2", "vector": [0.12130814790725708, 0.42269188165664673, -0.36990395188331604, -0.10103334486484528, -0.4720899164676666, 0.026209119707345963, 0.9517732262611389, -0.48553037643432617, -0.44113436341285706, -0.5413556694984436, 0.2646124064922333, 0.7106148600578308, 0.30838364362716675, 0.16563880443572998, -0.2489871084690094, 0.4098110795021057, -0.5649864077568054, 0.8566920161247253, 0.2910173833370209, -0.6142380237579346, 0.1806872934103012, -0.6716070175170898, -1.2706031799316406, 0.2096935659646988, -0.0631132572889328, 0.6910483241081238, 0.5965713858604431, 1.198358178138733, -0.067425936460495, -0.07275982201099396, 0.4417080283164978, -0.08423209935426712, 0.3888857364654541, 0.13706040382385254, -0.7853235602378845, 0.2030555009841919, 0.7345486879348755, -0.3788820505142212, -0.5148472785949707, 0.37343260645866394, 0.012023868970572948, 0.46501079201698303, 0.7791348695755005, -0.8672298192977905, -0.25047916173934937, 0.44657182693481445, 0.3966276943683624, 0.05064745619893074, 0.11164925992488861, -0.49296748638153076, 1.2018301486968994, -1.473352074623108, 0.69437175989151, 1.6946024894714355, 0.19482427835464478, 0.8520256876945496, -0.5453633069992065, -0.011170679703354836, 0.23323184251785278, 0.2763603925704956, -0.4512225091457367, -0.22378593683242798, -0.13842293620109558, -0.30927354097366333, 1.176975131034851, -0.4159427285194397, 0.0884404182434082, 1.1613900661468506, -0.29288625717163086, 1.5727300643920898, 0.11634264886379242, -1.0682096481323242, -0.3299044370651245, -0.11404752731323242, 0.22977915406227112, 0.6570599675178528, -0.45464858412742615, 0.5484240651130676, -1.1077821254730225, 0.48212897777557373, 0.6664255857467651, -0.05431726947426796, 0.35042402148246765, -0.27745330333709717, -0.9376153945922852, 0.5163195133209229, 0.7976299524307251, 0.7229737639427185, 0.05085918679833412, 0.9597593545913696, 0.5006988644599915, 0.030442573130130768, -0.6990903615951538, 0.30439889430999756, -0.0431784987449646, 0.5474109649658203, -0.7870289087295532, 0.5654105544090271, 0.16194167733192444, 0.8093125820159912, 0.2912459373474121, -0.07756029814481735, -0.6303982138633728, 0.08142475038766861, 1.5281754732131958, 0.13243256509304047, 0.3618338108062744, -0.9047532677650452, 0.27123361825942993, -0.9549561142921448, 0.36615657806396484, -0.4578171372413635, -0.3794039189815521, 0.3787102699279785, -0.3449360728263855, -0.8658873438835144, -0.12308703362941742, 0.11853675544261932, -1.5466053485870361, 0.44637224078178406, -0.19955851137638092, 0.37486374378204346, 0.1389150768518448, 1.060664176940918, 0.9372314214706421, 0.40818798542022705, 0.39202871918678284, 1.0196361541748047, 1.0919934511184692, -1.0385953187942505, -0.266614705324173, -0.7684150338172913, -0.009122410789132118, -0.35904550552368164, 0.01696139946579933, -0.24489954113960266, -1.407521367073059, -1.485325813293457, -0.7064538598060608, -0.1481429636478424, -0.8148897886276245, 0.6873091459274292, 1.2006332874298096, 0.24881994724273682, -1.0405054092407227, 0.4682362377643585, -0.14395786821842194, -0.4851738214492798, 0.33404541015625, 0.1764150708913803, -0.2803708612918854, -0.5276237726211548, -1.0084736347198486, 0.42093655467033386, 0.4308651387691498, -0.20054873824119568, -0.5735319256782532, -0.10706909000873566, -1.3024022579193115, -0.5899738669395447, 0.0324646458029747, -0.9379672408103943, 1.2041858434677124, -0.08116650581359863, -0.8020747303962708, 0.7989149689674377, -0.3904501497745514, 0.1322377771139145, 0.46408534049987793, -0.1606578826904297, -0.43097174167633057, -0.27282994985580444, -0.16546088457107544, 1.5176903009414673, 0.8549154996871948, -0.4725392460823059, 0.007608938962221146, 0.05745094642043114, -0.08143435418605804, 0.1617591381072998, -0.07344762980937958, 0.8358591198921204, -0.4310603141784668, -0.058148402720689774, 0.48448726534843445, 0.401023805141449, -0.1724509745836258, -0.3013642728328705, -0.8583638668060303, -0.8354101181030273, 0.8951203227043152, 0.3468377888202667, -0.22911517322063446, -0.7879666090011597, -0.36005768179893494, -0.2078758180141449, -0.24958214163780212, -0.4746905565261841, -1.3433483839035034, 0.5307780504226685, -0.21688853204250336, 0.511776864528656, -0.01700347475707531, -1.7138218879699707, 0.4236048758029938, -0.011943524703383446, -0.19329720735549927, -0.11498872190713882, 0.30048567056655884, 1.3181588649749756, -0.5484406352043152, -0.25938984751701355, 0.28482410311698914, 0.20954635739326477, -0.7858870029449463, 0.897182822227478, -0.7667932510375977, 0.23007354140281677, 0.11171203851699829, 0.08144472539424896, 0.0892183929681778, 0.042651474475860596, 0.0026727584190666676, -0.6659126877784729, 0.022146133705973625, 0.20988890528678894, -0.850629448890686, 2.452895402908325, 0.07789139449596405, 0.45380985736846924, -0.4070777893066406, -0.44770756363868713, 0.13793738186359406, 0.42715463042259216, -0.17700427770614624, -0.5401728749275208, 0.11629455536603928, 0.40747755765914917, -0.6544265747070312, -0.06515365093946457, 1.1834681034088135, 0.9499093294143677, -0.4828489422798157, 0.04290270060300827, 0.8543528914451599, -0.686590313911438, 0.544468343257904, 0.48207417130470276, 0.10783347487449646, 0.6811527609825134, -0.5076941251754761, -0.02366671711206436, -0.04976927861571312, -0.7683148980140686, -0.4579082727432251, 0.6300163269042969, 0.5112930536270142, 1.6767843961715698, 0.1314283013343811, -1.128961443901062, -0.3209114670753479, -0.28205451369285583, 0.4816248416900635, 1.4419527053833008, 0.6299546360969543, -0.014271204359829426, -0.5084296464920044, -0.25583696365356445, -0.3102784752845764, -0.5344595313072205, -0.9732149243354797, 0.3890873193740845, -0.3288251459598541, -0.8050643801689148, 0.5665704011917114, 0.40992486476898193, 1.2052432298660278, -1.0211987495422363, -0.5577680468559265, -0.6219034194946289, 0.005786603316664696, -0.7819087505340576, -0.4931661784648895, -0.016976209357380867, 0.12323034554719925, -0.3492068946361542, -0.24598872661590576, -0.42537057399749756, 0.2877103090286255, -0.760025680065155, 0.7769231796264648, -0.6197525858879089, -0.9444783329963684, 0.3718448579311371, 0.379268616437912, -0.6303356289863586, -0.29313942790031433, 0.20329035818576813, -0.14930328726768494, -0.19997435808181763, -0.2143389731645584, 0.824828565120697, -0.10805226862430573, 0.38215428590774536, -0.7074251174926758, 0.43687868118286133, -0.09831003844738007, -0.0009772880002856255, 1.09402596950531, -0.5103346705436707, 0.07049095630645752, -0.7197859287261963, 0.4372800588607788, 0.2269030511379242, 0.057845648378133774, -0.0038489082362502813, -0.09196200221776962, -0.5356805920600891, -0.1009567379951477, -0.5469872951507568, -0.36632296442985535, -0.09171787649393082, 0.28904786705970764, -0.5070074200630188, -1.0123450756072998, -0.13604210317134857, 0.4234383702278137, -0.09127424657344818, 0.7271363735198975, 0.24965806305408478, 0.29863086342811584, -0.10110197216272354, 0.9532524943351746, -1.4368726015090942, 0.8996330499649048, -0.007396080531179905, 0.4726407527923584, 0.23156966269016266, 0.3150619566440582, -0.5931618213653564, -0.3555164635181427, -0.5974166393280029, -0.8256438970565796, -0.5748480558395386, 1.000529408454895, -0.43611106276512146, -0.6880775690078735, 0.3006644546985626, -1.4429280757904053, -0.12000183016061783, 0.17018622159957886, -0.21961647272109985, -0.5900550484657288, -0.9351596236228943, -0.8675760626792908, -0.2541070282459259, -0.180042564868927, -0.913687527179718, 0.5773868560791016, 0.4286889135837555, -0.3880847096443176, -0.4639584422111511, 0.1762835532426834, -0.20947414636611938, 0.6819218397140503, 0.08321503549814224, 0.22277703881263733, -0.02926560677587986, -0.31713324785232544, -0.41798704862594604, 0.08941621333360672, 0.35848256945610046, -0.3930456042289734, 0.5047239661216736, -0.971099317073822, 0.5835753083229065, -0.9709441661834717, -0.7415018677711487, 0.8627175688743591, 0.3704364597797394, 0.40775996446609497, 0.5942404866218567, -0.20210257172584534, 0.2682710289955139, 1.3223583698272705, -0.8340311646461487, 0.4065764546394348, -0.10695790499448776, 1.2265896797180176, 0.4813978672027588, -0.41514766216278076, 0.5761944651603699, 0.7431274056434631, 0.30696240067481995, 0.8639296889305115, -0.37820661067962646, -0.6505520939826965, -0.56642085313797, 0.23151446878910065, 0.45655494928359985, -0.2308332920074463, 0.04663572460412979, -0.971569836139679, 0.8299379944801331, -1.173994779586792, -1.0216299295425415, 0.42096927762031555, 0.626908540725708, -0.07887600362300873, -0.6217157244682312, 0.23674918711185455, -0.4556731879711151, 0.6358346343040466, 0.34471479058265686, -0.22296065092086792, -0.536129355430603, -0.2891019284725189, 0.399931937456131, -0.3546263873577118, 0.3097890317440033, -0.7374213337898254, 0.42309120297431946, 14.524962425231934, 0.4526318311691284, 0.09937207400798798, 0.01795034110546112, 0.8404051661491394, 0.3546455204486847, -0.5112854838371277, 0.01876802369952202, -1.0879310369491577, -0.2322843372821808, 0.7667521238327026, 0.7893772721290588, 0.5951047539710999, 0.25210484862327576, -0.07564244419336319, 0.04880332946777344, -1.0202730894088745, 0.9026756286621094, 0.6604189276695251, -1.3975831270217896, 0.014637171290814877, 0.07049356400966644, 0.6054121255874634, 0.6356150507926941, 1.0971063375473022, 0.5804012417793274, 0.16134057939052582, -0.7125006318092346, 0.8862581849098206, 0.6269086599349976, 0.9543334245681763, -0.00439021922647953, 0.07495544850826263, 0.16570287942886353, -0.993780255317688, -0.19382710754871368, -0.459336519241333, -0.745026707649231, 0.43257075548171997, -0.43457141518592834, -0.5900431871414185, -0.060049109160900116, 0.01596292294561863, 0.8476115465164185, -0.08834061026573181, 0.5980250239372253, 0.03517913445830345, -0.16937647759914398, -0.07230016589164734, 0.05810748413205147, 0.19147071242332458, 0.7956622242927551, 0.2642664611339569, -0.036151476204395294, -0.040032532066106796, -0.408086359500885, 0.3908953070640564, 0.5783101320266724, -0.581917405128479, -0.4048856496810913, -0.1672162413597107, 0.01724751479923725, -0.3060466945171356, 1.3050289154052734, 0.1937507688999176, 0.5525897741317749, -0.03141510486602783, 0.48602452874183655, 0.0061919488944113255, 0.16752973198890686, -0.3398638665676117, -0.11734070628881454, 0.06612442433834076, -0.588905930519104, 0.2011643946170807, 0.4364692270755768, -0.21234416961669922, -0.42914581298828125, -0.6400830745697021, 0.20015248656272888, 0.4899293780326843, -0.7943289279937744, -0.8895621299743652, 0.9482041597366333, -0.21376006305217743, -0.3619630038738251, -0.11529432237148285, -1.042022705078125, -0.4273686707019806, 0.5594383478164673, -1.6209198236465454, -1.2136754989624023, -0.21808265149593353, 0.12689214944839478, -0.07589558511972427, 0.048955585807561874, 1.2147717475891113, -0.13732849061489105, -0.15174128115177155, -0.2843782603740692, -0.6806800961494446, 0.06658852845430374, 0.10563624650239944, -1.0820695161819458, 0.827272891998291, -0.008042307570576668, 0.26414787769317627, -0.3061836361885071, -0.19106532633304596, 0.45359259843826294, -0.7797080278396606, 0.22480788826942444, 0.4498036503791809, -1.0788750648498535, -0.7770957350730896, -0.5217401385307312, -0.19359831511974335, -0.029934516176581383, 0.9070932269096375, 0.13843892514705658, 0.1885962337255478, -0.04488983005285263, -0.6090848445892334, 0.006004725117236376, -0.9849090576171875, 0.03137124702334404, 0.27003055810928345, -0.6994414329528809, -0.3570019602775574, 0.18156984448432922, 0.18780799210071564, -1.129777193069458, -0.44146931171417236, -0.32265615463256836, 0.5366735458374023, -0.37228158116340637, 1.2280973196029663, -0.46138355135917664, 0.9148346185684204, 0.6726852059364319, -0.5205932855606079, -0.7284899950027466, 0.264279305934906, -0.8611339926719666, 0.02313019521534443, -0.10315373539924622, 0.6502832174301147, -0.05157644301652908, 0.16617178916931152, 0.7598972916603088, 0.25925371050834656, -0.43046605587005615, -0.4044578969478607, -0.13014012575149536, -0.2560310959815979, -0.512681245803833, 0.4425354599952698, -0.25708743929862976, -0.21234893798828125, 0.07446244359016418, 0.3515201210975647, 0.3400159478187561, -0.06354169547557831, -0.6411023139953613, 0.699624240398407, -0.29444482922554016, -0.35463714599609375, -0.16613928973674774, -0.8000019788742065, -1.274671196937561, -0.048730093985795975, -1.0202231407165527, 0.19359025359153748, -1.2452596426010132, -0.3569500148296356, 0.07781431823968887, -0.39605024456977844, 0.401554673910141, 0.6364335417747498, 0.16493892669677734, 0.16723637282848358, -0.5746597051620483, -0.5277149677276611, 0.5645481944084167, 1.3737118244171143, -0.7159924507141113, -0.12690502405166626, -0.2067253589630127, 0.12145736068487167, 0.32615333795547485, 0.27893808484077454, -0.0562029629945755, -0.8947288393974304, -1.2878371477127075, 0.2560652792453766, 0.23232512176036835, 0.6585568785667419, -1.2959612607955933, 0.4832041561603546, 0.5764754414558411, 0.3385273814201355, -0.17951858043670654, 0.938424825668335, -0.7067806124687195, -0.5627888441085815, 0.20042750239372253, -0.8052880764007568, 0.16162236034870148, 0.09500200301408768, -0.4634878933429718, -0.26711922883987427, 0.6341224908828735, -0.2794578969478607, -0.9757888317108154, -1.4217127561569214, 0.59236741065979, 0.1166921854019165, -0.04188303276896477, -0.09923725575208664, -0.3886135220527649, -1.2130578756332397, -0.04164868965744972, -0.7405326962471008, 0.180856853723526, -0.48116782307624817, 1.072178840637207, 1.0756607055664062, -1.1548707485198975, -0.2989885210990906, 0.3546891510486603, 0.05634105205535889, 0.3478556275367737, 1.0190987586975098, 0.4649294912815094, -0.12847569584846497, 0.005092829931527376, -0.2669123113155365, -0.03278558328747749, -0.39590343832969666, -0.19767959415912628, 0.8804886341094971, 0.02171732485294342, 0.011406457982957363, 1.3092429637908936, -0.4695015847682953, -1.00069260597229, 0.04089229181408882, -0.6358043551445007, -0.44109758734703064, -0.041219692677259445, 0.6056804060935974, 0.062007199972867966, -0.2509493827819824, -0.5159330368041992, -0.5437521934509277, 0.4407764971256256, -0.3496154546737671, -0.9928802251815796, 0.19967396557331085, 0.12728449702262878, -0.0656222552061081, 0.2496216595172882, 0.9393911361694336, -1.1788907051086426, -1.03042733669281, -0.26782384514808655, -0.7519195675849915, -0.023184098303318024, -0.2735966742038727, -0.822621762752533, -0.6670205593109131, 0.9062150120735168, 0.5281550884246826, 0.3318655490875244, 0.05874963849782944, 0.2904476523399353, 0.2869255542755127, 0.6503891944885254, -0.06339363008737564, -0.787175714969635, 0.16299262642860413, 1.0683778524398804, 1.2055165767669678, -0.896765947341919, -0.19181311130523682, -0.13348999619483948, -0.9028306603431702, 1.021147608757019, 0.5233516097068787, -0.32267552614212036, 0.5800982117652893, -0.7453219890594482, -0.09508085250854492, 0.24262304604053497, -0.5319434404373169, -0.3911351263523102, 1.2523750066757202, 1.6999810934066772, 0.24209484457969666, 0.5244689583778381, 0.26999056339263916, 0.417344868183136, 0.05581874027848244, -0.32782140374183655, 0.6070057153701782, 0.15805703401565552, -0.04071970656514168, -0.3032245934009552, 0.0075516304932534695, 0.34960758686065674, -0.22742924094200134, 0.027708159759640694, 0.19234155118465424, 0.6824519634246826, 0.6379993557929993, 0.6116182804107666, 0.7572299838066101, 0.04996318742632866, 0.44591015577316284, -0.09280291199684143, 1.0000015497207642, -0.44622159004211426, 0.09469684958457947, -0.08799421042203903, -0.7779292464256287, -0.2960429787635803, -0.6682455539703369, -0.8731223940849304, -0.7548726797103882, 0.1718308925628662, 0.5729709267616272, -0.7517637610435486, 0.7437355518341064, 1.2820830345153809, 0.4182012379169464, 0.3214385509490967, -0.25686806440353394, -1.3245383501052856, 0.09851297736167908, -0.6875972747802734, 0.04030682519078255, 0.004549933131784201, -0.04034924879670143, -0.6417356133460999, 0.09223873168230057, 0.10834357887506485]}, "authors": [{"authorId": "153825349", "name": "Roman Bachmann"}, {"authorId": "2273474116", "name": "Ouguzhan Fatih Kar"}, {"authorId": "2111623708", "name": "David Mizrahi"}, {"authorId": "2306752754", "name": "Ali Garjani"}, {"authorId": "2273661239", "name": "Mingfei Gao"}, {"authorId": "2306752983", "name": "David Griffiths"}, {"authorId": "2307218307", "name": "Jiaming Hu"}, {"authorId": "2273361790", "name": "Afshin Dehghan"}, {"authorId": "2295888299", "name": "Amir Zamir"}], "references": [{"paperId": "32112b798f70faab00e14806f51d46058cf5e597", "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models"}, {"paperId": "6c64ddd2190909de2c680dd18abc9b92e80c39f9", "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action"}, {"paperId": "13925d7d5952b1ba5960dfe2c44d977be109b636", "title": "4M: Massively Multimodal Masked Modeling"}, {"paperId": "98478ac589e5b40a20630ff54bb4eec4ab4c5f6b", "title": "GAIA-1: A Generative World Model for Autonomous Driving"}, {"paperId": "1a735015a1f7ef4f2ba2273ce5fcaaacfa9d1ea2", "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"}, {"paperId": "0fe88452660cb8a0e37f54bcd44f3cd6504354b5", "title": "Unified Model for Image, Video, Audio and Language Tasks"}, {"paperId": "83c48aa341850af478247e3b34ba1ee1db9f1236", "title": "Meta-Transformer: A Unified Framework for Multimodal Learning"}, {"paperId": "d7890d1906d95c4ae4c430b350455156d6d8aed9", "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"}, {"paperId": "61e608a70faefb8faaf92124c4a8a7a8bf1fe099", "title": "Scaling MLPs: A Tale of Inductive Bias"}, {"paperId": "861370f7c2d18bed09905fde334a19cc96e83e14", "title": "StyleDrop: Text-to-Image Generation in Any Style"}, {"paperId": "d865fe9a755473301f0560c1d3b102035a14848a", "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers"}, {"paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91", "title": "ImageBind One Embedding Space to Bind Them All"}, {"paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891", "title": "DINOv2: Learning Robust Visual Features without Supervision"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "efbe97d20c4ffe356e8826c01dc550bacc405add", "title": "Adding Conditional Control to Text-to-Image Diffusion Models"}, {"paperId": "468992bf970c37bd1fef58b78a6c2fcd8c018868", "title": "Scaling Laws for Generative Mixed-Modal Language Models"}, {"paperId": "2a3213cb3c755f036d5dfec7261d726a819c78c1", "title": "Muse: Text-To-Image Generation via Masked Generative Transformers"}, {"paperId": "30a3731f09e7a391e79a28fa736fa6bdd8331866", "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"}, {"paperId": "2f68d3934b006fcd01732adbc1ab459b2485fc8e", "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe", "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "d2425b430fbf5b8ddf9cf2309c36a80a71e5a449", "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos"}, {"paperId": "06761cb27e14aa55a6c3d98b949898aa26416698", "title": "A Unified Sequence Interface for Vision Tasks"}, {"paperId": "32b3553d7dc8a263c63d32eeec2916d1647ab178", "title": "DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder"}, {"paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"}, {"paperId": "055cd2faeebc7a9df43923d554a61ae924a4af6b", "title": "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes"}, {"paperId": "004b97aea43f9f62cc49dec20f449abfbae28811", "title": "Masked Autoencoders As Spatiotemporal Learners"}, {"paperId": "5922f437512158970c417f4413bface021df5f78", "title": "A Generalist Agent"}, {"paperId": "d2f63b56fc6bc373f5c023454c2b253326962865", "title": "DeiT III: Revenge of the ViT"}, {"paperId": "b1fc7d96d732f99510658c73a8d9da3fd7b25923", "title": "MultiMAE: Multi-modal Multi-task Masked Autoencoders"}, {"paperId": "a09cbcaac305884f043810afc4fa4053099b5970", "title": "Exploring Plain Vision Transformer Backbones for Object Detection"}, {"paperId": "140a158aa77e0e5281bf4fb3b8fa44696a2dd209", "title": "3D Common Corruptions and Data Augmentation"}, {"paperId": "7c597874535c1537d7ddff3b3723015b4dc79d30", "title": "MaskGIT: Masked Generative Image Transformer"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "c783e1fb3ce8514f981925ee590c00884660ee4e", "title": "CM3: A Causal Masked Multimodal Model of the Internet"}, {"paperId": "7d0dfbddbf5b824e7048b2d40fa5945afd18ac9c", "title": "Are Large-scale Datasets Necessary for Self-Supervised Pre-training?"}, {"paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff", "title": "Masked-attention Mask Transformer for Universal Image Segmentation"}, {"paperId": "91dc75f94da13452a54ad5c03fab2c5fda87e9ba", "title": "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks"}, {"paperId": "19c68c1e0cf0d47b96bd448e0ade1c4add0601d6", "title": "ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data"}, {"paperId": "9653c070724e44f023e8cc3ec79f0b9e6d59480d", "title": "iBOT: Image BERT Pre-Training with Online Tokenizer"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "9c7a2cd13b783bb73ad2d1ec2880bdd9b995cbdc", "title": "Vector-quantized Image Modeling with Improved VQGAN"}, {"paperId": "f1a66139fa051370018a3539f15e60f728e437ca", "title": "Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans"}, {"paperId": "19b3b074d38b250d024920732ae51a8ffa0996dd", "title": "Pix2seq: A Language Modeling Framework for Object Detection"}, {"paperId": "c6fa4dac4a8b243ef556df3da5a03d85466fcdf4", "title": "Efficiently Identifying Task Groupings for Multi-Task Learning"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "167a477de806f935d97343d40a392a04cd801997", "title": "NWT: Towards natural audio-to-video generation with representation learning"}, {"paperId": "8e33914d6051dd031a5e096962b9398fc1d16067", "title": "Vision Transformers for Dense Prediction"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "914a593b7f2e980470075a9955f1407641669a8f", "title": "Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation"}, {"paperId": "84c40cf28afdf84ec6941d92cacd49fed3c7ef9a", "title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "518cb6d4247bdebf21e2811f296b0c7372602a0a", "title": "PMI-Masking: Principled masking of correlated spans"}, {"paperId": "5b2420f9e00b311f451a4fd10168960c8622cbef", "title": "OASIS: A Large-Scale Dataset for Single Image 3D in the Wild"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "449c5660d637741f7aa7ff42549c32b43c9968bf", "title": "Gradient Surgery for Multi-Task Learning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "7406e5c8341b9e8a72080edc15ee3317ce38de22", "title": "DIODE: A Dense Indoor and Outdoor DEpth Dataset"}, {"paperId": "096803265d747a37b8b032b8e71ef2e42f4bcd41", "title": "Bfloat16 Processing for Neural Networks"}, {"paperId": "356941da708c6d5b06bce17463aca309fd33151a", "title": "Which Tasks Should Be Learned Together in Multi-task Learning?"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "2fe2cfd98e232f1396f01881853ed6b3d5e37d65", "title": "Taskonomy: Disentangling Task Transfer Learning"}, {"paperId": "e73696016b43314a7bef6015dacbe702af472d96", "title": "End-to-End Recovery of Human Shape and Pose"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "title": "Neural Discrete Representation Learning"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0", "title": "An Overview of Multi-Task Learning in Deep Neural Networks"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "f98788f32b0d33d200c9bc7d900d0ef39519c927", "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics"}, {"paperId": "922197906907dc0a5e1b51fae40d3149333ecacf", "title": "UberNet: Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory"}, {"paperId": "2976605dc3b73377696537291d45f09f1ab1fbf5", "title": "Cross-Stitch Networks for Multi-task Learning"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "cb3a2ddcf305e2ec0f6b94af13d1e631ed261bdc", "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "c1994ba5946456fc70948c549daf62363f13fa2d", "title": "Indoor Segmentation and Support Inference from RGBD Images"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "6e67e8c1f3381822725576f271a9deb21d05c0d3", "title": "Measuring colorfulness in natural images"}, {"paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c19ed5102ecd953d5c78d5a0b87eaa51658e07d8", "title": "Supplementary Material to: Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera"}, {"paperId": null, "title": "COYO-700M: Image-text pair dataset"}, {"paperId": null, "title": "Segment anything. ArXiv abs/2304.02643 (2023)"}, {"paperId": null, "title": "Python Pillow"}, {"paperId": null, "title": "PyPalette"}]}