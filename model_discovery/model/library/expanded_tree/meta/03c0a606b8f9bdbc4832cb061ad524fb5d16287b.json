{"paperId": "03c0a606b8f9bdbc4832cb061ad524fb5d16287b", "abstract": "Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges is provided."}, "embedding": {"model": "specter_v2", "vector": [0.12423751503229141, 0.5441628694534302, -0.42071279883384705, 0.1288067102432251, 0.3212881088256836, 0.20942872762680054, 0.6212106943130493, -0.2935106158256531, -0.7908644080162048, -0.44993510842323303, 0.6280295848846436, 0.09963873773813248, -0.18202240765094757, -0.17912468314170837, -0.3021560311317444, 0.25322940945625305, -0.8097766637802124, 0.31227007508277893, 0.10319000482559204, -0.5278224349021912, -0.18653470277786255, -0.25197291374206543, -1.1969925165176392, 0.2730480134487152, -0.07222259044647217, 0.37633228302001953, 0.663652777671814, 1.1184918880462646, 0.1010793000459671, 0.7995248436927795, 0.9597607851028442, 0.22139938175678253, 0.40035349130630493, 0.0963701531291008, -0.1379445493221283, -0.3889983892440796, 0.08687427639961243, -1.0803884267807007, -1.3338828086853027, 1.0423376560211182, -0.6442312598228455, 0.2184460312128067, 0.3818824589252472, -1.0905952453613281, -0.1662684828042984, 0.7605935335159302, 0.9828161001205444, 0.8113592863082886, 0.058145564049482346, -0.43652546405792236, 1.4263535737991333, -1.003207802772522, 0.46387651562690735, 1.8077547550201416, 0.43709129095077515, 0.5137766003608704, -0.18497639894485474, -0.26750922203063965, 1.1056379079818726, -0.15047438442707062, -0.718060314655304, -0.46038517355918884, -0.05855529382824898, -0.23065292835235596, 1.3080717325210571, -1.000985026359558, 0.7634379267692566, 0.9015063643455505, 0.14482632279396057, 1.8003482818603516, 0.2789636254310608, -1.1343324184417725, -0.03707926347851753, -0.07603847235441208, 0.40588584542274475, 1.1633235216140747, -0.17869752645492554, 0.9249520301818848, -1.3256001472473145, 0.11847905814647675, 1.0861424207687378, -0.237136110663414, -0.34957730770111084, -0.5186879634857178, -0.9117814898490906, 0.7146886587142944, 0.834645688533783, 0.32482874393463135, -0.5998690724372864, 0.528350830078125, 0.2567940652370453, 0.19662347435951233, -0.9969117641448975, -0.1471935659646988, 0.47387149930000305, 1.1006855964660645, -0.7565520405769348, 0.20777231454849243, 0.10998182743787766, 0.815004825592041, -0.27149152755737305, 0.1832226663827896, -0.7282943725585938, -0.13055728375911713, 2.2717125415802, 0.06401178985834122, 0.4549722373485565, -0.820661723613739, 0.2927389442920685, -0.6075522303581238, 0.855411171913147, -0.9438997507095337, -0.4046536982059479, -0.08814425021409988, -0.03739850968122482, -0.6648368835449219, 0.15136460959911346, 0.4315524399280548, -0.9927296042442322, 0.9443417191505432, -0.09740366786718369, -0.38882991671562195, 0.36065873503685, 0.9693590998649597, 0.3671134114265442, 0.3233835995197296, 0.2434581071138382, 0.486074835062027, 0.849357008934021, -0.9908010959625244, -0.8162515759468079, -1.1190134286880493, 0.21421760320663452, 0.40941059589385986, 0.18188729882240295, -0.5208470225334167, -1.010313868522644, -1.259204626083374, -1.3744642734527588, -0.20872609317302704, -0.4102437198162079, 0.3565557599067688, 1.6806641817092896, -0.10184795409440994, -1.5816915035247803, 0.6032363772392273, -0.5078871250152588, -0.3215389549732208, 0.3546878695487976, 0.35686632990837097, -0.00902773067355156, -0.2889307141304016, -0.8793196678161621, 0.7251767516136169, 0.719347357749939, 0.16408780217170715, -0.9492171406745911, 0.7642764449119568, -1.7301502227783203, -0.20107880234718323, -0.20403848588466644, -1.0645946264266968, 1.2142748832702637, -0.08333166688680649, -1.0429974794387817, 0.4770025908946991, -0.5057174563407898, -0.2028801143169403, 0.05532878637313843, -0.325515478849411, -0.8231777548789978, 0.17741243541240692, 0.02158605307340622, 1.1369296312332153, -0.16860884428024292, -0.7146456241607666, -0.41907984018325806, -0.31157904863357544, -0.2510864734649658, 0.14359022676944733, 0.07423342764377594, 1.09050452709198, -0.3385178744792938, 0.21029314398765564, 1.0934981107711792, 0.8129196166992188, -0.2855246961116791, 0.3604542016983032, -0.12016865611076355, -0.5521986484527588, 0.12596267461776733, 0.06468416005373001, 0.665040910243988, -0.4441118538379669, -0.7779079675674438, -0.11464264988899231, -0.0030972748063504696, -0.623501181602478, -0.9477553963661194, 0.3600134551525116, -0.13592106103897095, -0.13902056217193604, 0.23399031162261963, -0.6673046946525574, 0.12147273123264313, -0.18096129596233368, -0.8142659664154053, 0.3499731123447418, -0.41362524032592773, 0.909153401851654, -1.256860613822937, -0.22430434823036194, 0.2545909583568573, -0.03909413143992424, -0.5477722883224487, 0.783061683177948, -0.24899548292160034, 0.1957734078168869, -0.3701389729976654, -0.4406522810459137, -0.1336834579706192, -0.06748756021261215, 0.4163862466812134, -0.29482120275497437, -0.30984464287757874, 0.5581457018852234, -0.8576622605323792, 1.774744987487793, 0.008228098042309284, 1.2043172121047974, 0.3037082552909851, -0.2912524938583374, -0.0540018305182457, 0.46892720460891724, -0.6218243837356567, -0.626503050327301, 0.3910430371761322, 0.21236476302146912, -0.04893144592642784, 0.2689626216888428, 0.05897983908653259, 0.9477359652519226, 0.02814832516014576, 0.31784459948539734, 0.24029229581356049, -0.08807450532913208, 0.35169947147369385, 0.22443753480911255, 0.43826353549957275, 0.9773025512695312, 0.4029388129711151, -0.07965553551912308, 0.16646334528923035, -0.5533832907676697, -0.06474436074495316, 0.9109557271003723, 0.16074396669864655, 0.28536906838417053, 0.02484038472175598, -1.0438690185546875, 0.1103871688246727, -0.3858044147491455, 0.5821828842163086, 1.0859583616256714, 0.2510169744491577, 0.014502925798296928, -0.683763861656189, 0.03723379597067833, -0.4940269887447357, 0.6847402453422546, -0.6001599431037903, -0.022988470271229744, -0.3307557702064514, -0.4599488377571106, 0.657034158706665, 0.7396295070648193, 1.1695772409439087, -1.1467441320419312, -0.852735161781311, -0.07689137756824493, 0.5819203853607178, -0.8849000334739685, -0.16265347599983215, 0.1470460742712021, -0.4790315330028534, -0.7241219878196716, 0.2802059054374695, -0.49469998478889465, 0.485868901014328, -0.41261398792266846, 0.4397987127304077, -0.5497332811355591, 0.1880345344543457, 0.2682506740093231, 1.027103066444397, -0.6487944722175598, -0.357370525598526, -0.6946824789047241, 0.2538561224937439, -0.43878063559532166, 0.2737523019313812, 0.651577353477478, -0.14770202338695526, 0.11346206814050674, -0.15584483742713928, 0.5621207356452942, -0.01396849099546671, -0.02586846798658371, 0.8715903162956238, -0.11494999378919601, -0.045954808592796326, -0.4433304965496063, 0.876247763633728, 0.560818076133728, -0.9491322636604309, 0.045482803136110306, -0.6526244282722473, -0.34457093477249146, 0.3059253394603729, -0.3392680287361145, -0.5214269757270813, -0.4040106534957886, 0.6977882385253906, -0.3136911392211914, -0.6817556619644165, 0.4802784025669098, 0.4172927737236023, 0.06047927960753441, 0.38419294357299805, 0.45655927062034607, 0.10531236976385117, 0.38100123405456543, 0.41320446133613586, -0.9591648578643799, 0.6006181240081787, 0.5042997598648071, -0.2267734855413437, 0.3489774465560913, -0.1399097442626953, -0.4999469518661499, -0.6149445176124573, -0.25026074051856995, -0.24625957012176514, -0.6751305460929871, 0.14655959606170654, -0.8656999468803406, -0.8727176785469055, -0.0412687323987484, -1.1740471124649048, -0.6827086210250854, 0.37300992012023926, 0.35749712586402893, -0.7230473756790161, -0.9430710673332214, -0.979053258895874, -0.7926098704338074, -0.03787029907107353, -0.9218560457229614, -0.04954257234930992, 0.1765538603067398, -0.5775405168533325, -0.350968599319458, 0.116151824593544, -0.296939879655838, 0.3509519696235657, -0.5397399067878723, 0.6803882718086243, -0.1287732571363449, -0.2243829369544983, -0.20728866755962372, 0.27947527170181274, -0.15232102572917938, -0.5764071941375732, 0.10594385862350464, -0.5706369876861572, 0.4813203513622284, -0.24542899429798126, -0.3484014868736267, 0.32301267981529236, 0.5597854852676392, 0.3107767701148987, 0.22581696510314941, -0.846345067024231, 0.23243093490600586, 1.321933627128601, -0.3136160671710968, -0.0013371360255405307, 0.7629785537719727, 0.8276556730270386, 0.6351062655448914, -0.04411787912249565, 0.3115333318710327, 1.0860264301300049, 0.4352594316005707, 0.6251387596130371, 0.17091451585292816, -0.38281747698783875, -1.1065813302993774, 0.5617638230323792, 0.583896815776825, 0.08952397853136063, 0.17313967645168304, -1.2005345821380615, 0.8323297500610352, -1.7744210958480835, -0.7651692628860474, 0.6791943311691284, 1.022890329360962, 0.18645493686199188, 0.035301562398672104, -0.1390143781900406, -0.1728137582540512, 1.0090186595916748, 0.20402048528194427, -0.2001001536846161, -0.30384916067123413, 0.09089052677154541, -0.18745087087154388, -0.1635008603334427, 0.7540725469589233, -0.6020680069923401, 0.9074501991271973, 14.305480003356934, -0.026754876598715782, 0.3430008888244629, 0.30194583535194397, 0.00883505865931511, 0.5201004147529602, -0.36181098222732544, 0.00634264899417758, -1.4200598001480103, -0.39893025159835815, 1.0035059452056885, 0.6087891459465027, 0.9778401255607605, 0.30680111050605774, 0.15906761586666107, -0.3799337148666382, -0.8936865329742432, 0.6249047517776489, 0.41697677969932556, -0.8632152676582336, 0.5471267700195312, 0.10133467614650726, -0.1262558251619339, 0.7348343133926392, 0.4776124358177185, 0.845963180065155, 0.6749969720840454, -0.5680414438247681, 1.1327570676803589, 0.11352939158678055, 0.21621456742286682, 0.09868060797452927, 0.2794974744319916, 0.9705900549888611, -1.125201940536499, -0.4625033140182495, -0.898673415184021, -1.3410032987594604, 0.4872230291366577, -0.4153696298599243, 0.08717034012079239, -1.0211409330368042, -0.5001022219657898, 0.2829543948173523, 0.535224199295044, 0.6358444094657898, -0.7866148352622986, -0.08733788877725601, -0.6288297176361084, -0.17896230518817902, 0.16787002980709076, 0.3434930145740509, 0.27419257164001465, -0.38554805517196655, 0.050478823482990265, 0.20344147086143494, 0.17098288238048553, 0.5359440445899963, -0.006047507282346487, -0.4511862099170685, -0.7174570560455322, -0.13846631348133087, 0.20922067761421204, 0.4031423330307007, -0.10895092040300369, 0.818747878074646, -0.023279892280697823, -0.07625164091587067, 0.5096553564071655, 0.5040268898010254, -0.36399465799331665, 0.21262186765670776, 0.12426425516605377, -0.6645955443382263, 0.1233149841427803, 0.10448647290468216, -0.10054197162389755, -0.3661925196647644, -0.7682740092277527, 0.0101691959425807, 0.19456075131893158, -1.0455738306045532, -0.054446347057819366, 0.9203137755393982, -0.10831433534622192, -0.1145995631814003, 0.3437396287918091, -1.0771135091781616, -0.2115182727575302, -0.0875350832939148, -1.4264479875564575, -1.2155123949050903, 0.03739180415868759, 0.42652657628059387, -0.21833309531211853, -0.013272714801132679, 1.2204469442367554, -0.6016452312469482, -0.5578930974006653, -0.2869231104850769, -0.427213191986084, 0.14883945882320404, -0.730327844619751, -0.33098870515823364, 0.3678784668445587, 0.41485488414764404, 0.4197568893432617, 0.5247730016708374, 0.03975652903318405, -0.1074371188879013, -0.8717912435531616, 0.33638858795166016, 0.29594412446022034, -0.7805964350700378, -0.36219218373298645, -0.44691002368927, -0.24792790412902832, 0.7488414645195007, 0.5196435451507568, 0.09544994682073593, 0.0877230167388916, -0.10108864307403564, -0.45561379194259644, 0.09588536620140076, -0.8863003849983215, 0.6489890217781067, 0.5804206728935242, -0.7346479296684265, -0.9142714142799377, -0.13225631415843964, -0.14976894855499268, -0.858282744884491, 0.07846923172473907, -0.4604187309741974, 0.5518709421157837, -0.40661609172821045, 1.0186972618103027, -0.4935305714607239, -0.07323764264583588, 0.007756586652249098, -0.5038688778877258, -0.788582444190979, -0.2784741222858429, -0.5205113887786865, -0.19135363399982452, -0.2356068640947342, 0.8003860712051392, -0.5423494577407837, -0.0780407264828682, 0.735616147518158, 0.29827868938446045, 0.0003871341759804636, -0.7310708165168762, -0.008485562168061733, -0.6863368153572083, -0.6196897029876709, 0.19511765241622925, -0.6730903387069702, -0.20224393904209137, 0.07334548979997635, 0.8295331597328186, 1.2123539447784424, -0.043193381279706955, -0.4410586953163147, 0.34049952030181885, 0.0008300276822410524, 0.10281259566545486, -0.6397042870521545, -0.25535455346107483, -1.5916141271591187, 0.029988287016749382, -1.0434514284133911, 0.4474620223045349, -2.114078998565674, -0.7945185303688049, 0.2834037244319916, -0.4724957048892975, 0.11044825613498688, 0.9753144979476929, -0.7607924342155457, -0.24136735498905182, -0.2166517972946167, -0.7156776189804077, 0.7221169471740723, 1.2491345405578613, -0.32765066623687744, 0.036609113216400146, 0.048549503087997437, 0.008498071692883968, 1.0879828929901123, 0.36164337396621704, -0.35782212018966675, -0.8944284319877625, -1.4242045879364014, 0.6949223875999451, 0.03303688392043114, 0.05330374464392662, -1.0888959169387817, 1.2649813890457153, 0.340731680393219, 0.6134607195854187, -0.28167206048965454, 0.7438756823539734, -1.1911817789077759, -0.5657355785369873, 0.5783998966217041, -0.7682759761810303, -0.1464219093322754, 0.4465288519859314, 0.08093563467264175, -0.2435336410999298, 0.5768645405769348, -0.11368221044540405, -1.0958738327026367, -1.2767924070358276, 0.2991202175617218, -0.8722038865089417, -0.23412877321243286, 0.10964122414588928, -0.5183684229850769, -0.9617737531661987, -0.32504990696907043, -0.33474087715148926, 0.4921347200870514, -0.3712499737739563, 0.5464411377906799, 1.1891027688980103, -1.2170261144638062, 0.2906111776828766, 0.34549328684806824, -0.246816024184227, 0.06636466085910797, -0.08803920447826385, 0.2129531353712082, -0.324370414018631, 0.8278374671936035, -0.06012677401304245, 0.5959835648536682, -0.1906985640525818, 0.05555780604481697, 0.7375537157058716, 0.07102108746767044, -0.3585990369319916, 0.8449108600616455, -0.04091455042362213, -1.1025348901748657, 0.4949776232242584, -0.8654864430427551, -1.0965925455093384, -0.245155468583107, 0.7214685678482056, 0.2118249535560608, -0.930288553237915, -0.4133135676383972, -0.5004872679710388, 0.4905507564544678, 0.22214731574058533, -0.8404251933097839, 0.24200336635112762, -0.12655018270015717, -0.11737816780805588, 0.49060264229774475, -0.014618956483900547, -0.5485079288482666, -1.261505126953125, -0.13695071637630463, -0.2563619911670685, -0.13577044010162354, 0.5671466588973999, -0.4614507853984833, -0.4815213084220886, 0.6145684719085693, 0.4675179123878479, -0.13101661205291748, -0.0844971314072609, 0.0014590697828680277, -0.18251799046993256, 0.8187980055809021, 0.5824933052062988, -0.5751229524612427, -0.547577440738678, 1.2200018167495728, 1.4638266563415527, -0.8832515478134155, 0.19120794534683228, -0.24617286026477814, -0.3240927755832672, 1.0452154874801636, 0.6684198379516602, -0.4468117952346802, 0.6248924136161804, -0.6078171133995056, -0.10377481579780579, 0.2204219251871109, -0.34876179695129395, -0.4298941195011139, 0.5308192372322083, 1.4761346578598022, 0.14934688806533813, 0.2899963855743408, 0.5596463084220886, 0.8051207661628723, 0.025686703622341156, 0.35182857513427734, 0.2907048463821411, 0.33165672421455383, -0.8175998330116272, 0.7975423336029053, -0.01584118977189064, 0.4034199118614197, 0.038028035312891006, -0.5172417759895325, 0.3924253284931183, 1.2249009609222412, -0.12651598453521729, 0.7379858493804932, 0.6745771169662476, 0.36348652839660645, 0.4069441258907318, -0.04273414611816406, 0.9844264388084412, -0.6538639068603516, 0.33205756545066833, 0.22561673820018768, -0.328347384929657, -0.33529579639434814, -0.6120433807373047, -0.6284321546554565, -1.0326522588729858, 0.4014973044395447, 0.1451118141412735, -0.49756890535354614, 0.5758721232414246, 1.0910321474075317, 0.3175124526023865, 0.3294649124145508, -0.8166744112968445, -0.4431029260158539, -0.2379349023103714, -0.7685835957527161, 0.4458357095718384, -1.4990657567977905, 0.02084570750594139, -0.7327010631561279, -0.5193637609481812, -0.7195215821266174]}, "authors": [{"authorId": "2283332672", "name": "Yue Zhang"}, {"authorId": "2151006930", "name": "Ziqiao Ma"}, {"authorId": "2108961694", "name": "Jialu Li"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2310518904", "name": "Zun Wang"}, {"authorId": "2265383820", "name": "Joyce Chai"}, {"authorId": "2310487211", "name": "Qi Wu"}, {"authorId": "2257237631", "name": "Mohit Bansal"}, {"authorId": "2190934", "name": "Parisa Kordjamshidi"}], "references": [{"paperId": "ca91c05249eb70b84a6ad1ea5472682ff6ab2401", "title": "Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation"}, {"paperId": "f6d8eb53fa8ccdfa963afa600cd67ae23bcbfcb8", "title": "InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment"}, {"paperId": "c03341c91c6f83340f6af9aa55b5e8194a755525", "title": "DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences"}, {"paperId": "59bebcf322e2714f8f6c84e602ffc44493d6f39d", "title": "Correctable Landmark Discovery via Large Models for Vision-Language Navigation"}, {"paperId": "c5e787caef8da20a8537fefd587531335c03ce72", "title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and Reasoning Chains"}, {"paperId": "54ade9ab1e874fbe15b1bc8816dfce3c2fce7aa3", "title": "Learning to Follow and Generate Instructions for Language-Capable Navigation"}, {"paperId": "9dbd917f17390811a7d0895ad4b4e4c8276ad87e", "title": "Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation"}, {"paperId": "1772de89a739b3abedd5ade49aa82eb378cec3c0", "title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning"}, {"paperId": "01a70cb1ada213abaf966f735082a991ebe20b63", "title": "Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation"}, {"paperId": "12693672913f00a32b31fe68a3d0d3d4c40cc352", "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation"}, {"paperId": "5f69ca8747fecc743fa6d37027256fda9eac0d95", "title": "VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation"}, {"paperId": "a3570e82001666955d319647ba832df4f60a2044", "title": "A Survey on Robotics with Foundation Models: toward Embodied AI"}, {"paperId": "c1dabf63ebede62acc0543aa1ed75e1bc7deffce", "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator"}, {"paperId": "60316dcf0c41799928cd46b4579db8e74fd9bb90", "title": "A Survey for Foundation Models in Autonomous Driving"}, {"paperId": "d408272835f6bde7343a46fd9fe858c65226d742", "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments"}, {"paperId": "550c32e252722c2ad6b3ac17703654d96cb2c600", "title": "Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities"}, {"paperId": "8223f81e8cc126b83d2774fe2da19ead290c144d", "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk"}, {"paperId": "e36f79b2bfbf50cf6362cc563a6e5c261e3f0615", "title": "MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices"}, {"paperId": "6d2ab31aa75468f5458b9d96192c3f4a28f55d73", "title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"}, {"paperId": "de4c362339afc7e070bd4250de3dcb06b32a46fc", "title": "ThinkBot: Embodied Instruction Following with Thought Chain Reasoning"}, {"paperId": "c4c1c1d7312c7be6ad000f84201a72494dea33d3", "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning"}, {"paperId": "d8aca097ddc0062828948655d3a97e4204881582", "title": "Towards Learning a Generalist Model for Embodied Navigation"}, {"paperId": "451539c0d0f5f5785ff58d09ca5e67a5f129f9de", "title": "A Survey on Multimodal Large Language Models for Autonomous Driving"}, {"paperId": "13d12b26db345f62e8e512db181b96a7f8763b47", "title": "An Embodied Generalist Agent in 3D World"}, {"paperId": "eb2cbd12f749f14716296f7f415e921562c9079b", "title": "LRM: Large Reconstruction Model for Single Image to 3D"}, {"paperId": "ad4beeecbdfc7dd238eb55e2b19113f62096d95b", "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots"}, {"paperId": "60b9ffd8935d46e0d43e143186124cecfa6e5118", "title": "LangNav: Language as a Perceptual Representation for Navigation"}, {"paperId": "19933dd9e03058e686ef412262eef7696cce3e8f", "title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"}, {"paperId": "f01ff5acf9e086030c01beda6f433f99013ebbd4", "title": "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"}, {"paperId": "958ed4830ae80a189ecb9b93ab75a6ce2e3926fc", "title": "GPT-Driver: Learning to Drive with GPT"}, {"paperId": "ccd6f8b6544f112de632e49bfbe592a0a654537d", "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model"}, {"paperId": "174a3290ff0040e0f6a7a0b43bcd752c456350a0", "title": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"}, {"paperId": "f902ac89c9018a38ff33105b74d77a46434fc7cf", "title": "Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs"}, {"paperId": "9db0247728950788a2b42097d81dc0e24eed6bb2", "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent"}, {"paperId": "0455999a91507f77781b3a5a125aec0cf31398a5", "title": "Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions"}, {"paperId": "c07a039900de1b80e5ebf5bcfb1c5d456e30867a", "title": "What Is Near?: Room Locality Learning for Enhanced Robot Vision-Language-Navigation in Indoor Living Environments"}, {"paperId": "5a9d4bcffa9989cac4139b2844358884ae023e8d", "title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments"}, {"paperId": "a2e9d232ba2576a70c15c7ce19086d7aa326a70e", "title": "Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation"}, {"paperId": "d09ca4cbff21d4aaefbd3eb84eb073f1847190aa", "title": "Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation"}, {"paperId": "9466f846352cb9df46b932cb848239c5f76b878f", "title": "A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models"}, {"paperId": "8d097d73c29c2ce3af3f7a3d67e2d96f0b43908a", "title": "Dreamwalker: Mental Planning for Continuous Vision-Language Navigation"}, {"paperId": "25d8e3c541f996e366d8fa48cba248ca330e8a78", "title": "AerialVLN: Vision-and-Language Navigation for UAVs"}, {"paperId": "97d5d2c6a64e49fbf2f0ae59bb0ecc070e30e465", "title": "Bird\u2019s-Eye-View Scene Graph for Vision-Language Navigation"}, {"paperId": "82531b9ec17a1d5726c4d796bfd33a8e181d34d1", "title": "Vision Language Navigation with Knowledge-driven Environmental Dreamer"}, {"paperId": "52f166d02d7b4367fc9ee30692810b35fbd2f21f", "title": "Scaling Data Generation in Vision-and-Language Navigation"}, {"paperId": "72b96a45fa9f1ab656312cbb4a0f7a64e8063717", "title": "GridMM: Grid Memory Map for Vision-and-Language Navigation"}, {"paperId": "6703296c8c3b8d18cec0a14dfa7999e5d93e4f46", "title": "Learning Navigational Visual Representations with Semantic Map Supervision"}, {"paperId": "6f1aab7b3ca0a487e6a57571b4ef9993e6325435", "title": "Learning Vision-and-Language Navigation from YouTube Videos"}, {"paperId": "293951f26d10fed6950b2949e0f90571e0d67cd6", "title": "Tree-Structured Trajectory Encoding for Vision-and-Language Navigation"}, {"paperId": "e2a1b8a1a54a9cd87e16d3ab40cb2630f65533cf", "title": "Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation"}, {"paperId": "d7f1a876b7df0e10627bccb0c5b63faf2a1005e4", "title": "PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation"}, {"paperId": "00cb69a9f280317d1c59ac5827551ee9b10642b8", "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"}, {"paperId": "b09533b1c2f4ee3261855147b01ebe481ea8dc1b", "title": "R2H: Building Multimodal Navigation Helpers that Respond to Help Requests"}, {"paperId": "c576ccd018f6610870f0a724c1636f274d2017b4", "title": "Masked Path Modeling for Vision-and-Language Navigation"}, {"paperId": "6f821d75968bc8de070af3ce5aa7f57bc031fafb", "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models"}, {"paperId": "839fd88933c15b5f134d88c1e4936264f195eff9", "title": "A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "817f36182a9ea2605f58d728609536f474a841b9", "title": "Improving Vision-and-Language Navigation by Generating Future-View Image Semantics"}, {"paperId": "ba73a1d1ecef8f4cbc84cbbb80e6149e12a91430", "title": "ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "75136c0c63a2fe0d227e4644a3a0c293ed141414", "title": "ESceme: Vision-and-Language Navigation with Episodic Scene Memory"}, {"paperId": "82890c8e1c32c4864e64d6c64730200c096b9837", "title": "VLN-Trans: Translator for the Vision and Language Navigation Agent"}, {"paperId": "5e2bceb56f116e98baf7e418208057bc0e1c1861", "title": "ConceptFusion: Open-set Multimodal 3D Mapping"}, {"paperId": "137330d3030f75b01c88c14e1164d9b0d8c2dc70", "title": "Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation"}, {"paperId": "2e0aed964a845ad6818cd5cb26d501c41c2bee4c", "title": "HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation"}, {"paperId": "d7abc3bcf368c7c0e3487da7cecae1ac209a7284", "title": "BEVBert: Multimodal Map Pre-training for Language-guided Navigation"}, {"paperId": "325d8e9501af05e594bd668b6cd6d43ed42c8b4d", "title": "InternVideo: General Video Foundation Models via Generative and Discriminative Learning"}, {"paperId": "3dbe1b13c7fb234a72861e1614a188b5a5fbbef2", "title": "DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents"}, {"paperId": "fc5631cdd08722f51e0ce4b718de0f081ae73603", "title": "DANLI: Deliberative Agent for Following Natural Language Instructions"}, {"paperId": "da5c9d3cfdeacfc14844cad4e1aa8edbe44c1872", "title": "Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation"}, {"paperId": "c0ca3ec9cf42c067a0123990137f6ee57980ab85", "title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments"}, {"paperId": "c305ab1bdba79442bec72ec7f5c5ee7c49c2a566", "title": "Visual Language Maps for Robot Navigation"}, {"paperId": "197678d7a28be18b3de1fb03a6728dccef85d2b2", "title": "A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning"}, {"paperId": "e00867c7108395d56b823bc5c75d2f4591e2878d", "title": "Open-vocabulary Queryable Scene Representations for Real World Planning"}, {"paperId": "6b24218838b3b69481290ec5e94002a8f65aa70e", "title": "A survey of visual navigation: From geometry to embodied AI"}, {"paperId": "961a1772f3b90d9dffd2b571c6996007a1d0ccd1", "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents"}, {"paperId": "ff6d17044ac6292b9ceb87d80b11f0e269bdc543", "title": "Learning from Unlabeled 3D Environments for Vision-and-Language Navigation"}, {"paperId": "cdf54c147434c83a4a380916b6c1279b0ca19fc2", "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"}, {"paperId": "e2c75159834575fece3a62bc85f170ea997ec764", "title": "CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations"}, {"paperId": "167e4741615cee5edd6b24bdd03df2588bc2ff0b", "title": "FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation"}, {"paperId": "6ccf023ec2f8b0575cc1da990721b3dddcb3c48f", "title": "Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments"}, {"paperId": "749fc01c222e526dab89e9cb4cb280447d7d65fe", "title": "Simple and Effective Synthesis of Indoor 3D Scenes"}, {"paperId": "1000dc3b76aad7115c1c6bc3cfca653b457de50e", "title": "REVE-CE: Remote Embodied Visual Referring Expression in Continuous Environment"}, {"paperId": "559d516d83e14211123622111f404e402d371dff", "title": "Envedit: Environment Editing for Vision-and-Language Navigation"}, {"paperId": "6640d8efbe66164fe79d3a77cb3e2a3e83b8da0e", "title": "HOP: History-and-Order Aware Pretraining for Vision-and-Language Navigation"}, {"paperId": "c50924dc4eacdf80384fd6e2a4f5a5ec2af988ac", "title": "Cross-modal Map Learning for Vision and Language Navigation"}, {"paperId": "7fdea858b3ce8983851fe9a82e77dd2e7fdbc071", "title": "Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration"}, {"paperId": "529699c02bbbd302f82038e3f7a78620b9e19c19", "title": "Find a Way Forward: a Language-Guided Semantic Map Navigator"}, {"paperId": "18c302c9d51146ea91784638748e5d737da75e12", "title": "Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation"}, {"paperId": "0f01088765729402e903ec560f3246f884d324f8", "title": "Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "127ffc8697630a76b1b4149c24d1350f69205f41", "title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection"}, {"paperId": "47126c012d174d2c66dc99472d8b8f4333248ac7", "title": "Less is More: Generating Grounded Navigation Instructions from Landmarks"}, {"paperId": "cbdd3e2fcfd85d546f3ab18f644434097baa7590", "title": "SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning"}, {"paperId": "e2d328f98a6249a5e86c06c1934360e3c055141d", "title": "Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation"}, {"paperId": "fc7c20644d32542e11531e551900f305fc0953ad", "title": "SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation"}, {"paperId": "6f681faaa985ed38bc9b30777d57d9e1e3765861", "title": "History Aware Multimodal Transformer for Vision-and-Language Navigation"}, {"paperId": "bef63d4f7656393b7bceb2ec704e86577c286166", "title": "FILM: Following Instructions in Language with Modular Methods"}, {"paperId": "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424", "title": "TEACh: Task-driven Embodied Agents that Chat"}, {"paperId": "ceba50cc71df6024abcdb06158ca4b6dd1fa50fa", "title": "Waypoint Models for Instruction-guided Navigation in Continuous Environments"}, {"paperId": "cbbdcebe8ece11e883b55bcecc2f962670033263", "title": "Self-Motivated Communication Agent for Real-World Vision-Dialog Navigation"}, {"paperId": "3e7b660242f12e5b4c977c604c19176df9e4789c", "title": "Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments"}, {"paperId": "f46bce5b7dd78736133c1af1824ddb83c0ec2e55", "title": "Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI"}, {"paperId": "6664bfb49a5f3e5a5dd60b5cdafe3a63a53d834d", "title": "Semantically-aware Spatio-temporal Reasoning Agent for Vision-and-Language Navigation in Continuous Environments"}, {"paperId": "5c89bc8e91fb85f8af7761e8096d27dd740491d1", "title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "7b2ed21aa0ac773075d8b9febc0ef42403f32fbe", "title": "Multi-Grounding Navigator for Self-Supervised Vision-and-Language Navigation"}, {"paperId": "8f167ec1149921fac63b1ea855443de109bb013a", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?"}, {"paperId": "6536f36648d39f0f9f6105562f76704fcc0b19e8", "title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution"}, {"paperId": "4aa88c1406414cda3ce9cf76c8af0abaa8391760", "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat"}, {"paperId": "576e85bf4ad2cb5a6a8d2e8bd16a5fe785b9445c", "title": "A Survey on Human-aware Robot Navigation"}, {"paperId": "0fc049d7dd0f195fdead752e532c03bb11be1d80", "title": "Vision-Language Navigation with Random Environmental Mixup"}, {"paperId": "b9408b0c3445ad17f33e7a64073c1d2e6674e69a", "title": "Pathdreamer: A World Model for Indoor Navigation"}, {"paperId": "198159956ed954e858b213a0d531848559412470", "title": "Towards Navigation by Reasoning over Spatial Configurations"}, {"paperId": "25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1", "title": "Episodic Transformer for Vision-and-Language Navigation"}, {"paperId": "8351f660e67b6e792a4791b7a9ce27e6b1720236", "title": "Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation"}, {"paperId": "2df14c28dc4ebd6ce3411884f0824eaa42aaf7c8", "title": "Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information"}, {"paperId": "f52ac53244aab0a79427dfbfa724513d76a4b163", "title": "The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation"}, {"paperId": "468876b83da2eb1f98adc35af6129a326bdda8ce", "title": "SOON: Scenario Oriented Object Navigation with Graph-based Exploration"}, {"paperId": "1a8649c950e8f2208ed1ad965fa8f9c918d41460", "title": "Scene-Intuitive Agent for Remote Embodied Visual Grounding"}, {"paperId": "9c404d02aefd850ac3d5a8bdc5860738e6cd2b04", "title": "A Survey of Embodied AI: From Simulators to Research Tasks"}, {"paperId": "ee0de77c9bbe4a60fdbba09e2d37b3db842312df", "title": "Structured Scene Memory for Vision-Language Navigation"}, {"paperId": "cae06461bfc6978e9f3a3bbb0b0d0840cd762720", "title": "On the Evaluation of Vision-and-Language Navigation Instructions"}, {"paperId": "e738c4b4060e459c13646dee70b753809b82391b", "title": "A Modular Vision Language Navigation and Manipulation Framework for Long Horizon Compositional Tasks in Indoor Environment"}, {"paperId": "a9dc44231239ef010dc2617bc4c373c00e4bee72", "title": "Topological Planning with Transformers for Vision-and-Language Navigation"}, {"paperId": "604e0c54580a0530392f86b48a6183581a47b66d", "title": "VLN\u21bbBERT: A Recurrent Vision-and-Language BERT for Navigation"}, {"paperId": "a5cf0c0d57beb22fddaf0aecdd05ef9314c69ae0", "title": "Sensorimotor Representation Learning for an \u201cActive Self\u201d in Robots: A Model Survey"}, {"paperId": "cbeb7a1b00e677a94c719380421208a1899287f8", "title": "Sim-to-Real Transfer for Vision-and-Language Navigation"}, {"paperId": "191f2a22c3cd2bcebdb3c2f5ac354e2a464e0931", "title": "The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "cb10587da4c5a59d8f3321ad77f220abd87fe46b", "title": "Language and Visual Entity Relationship Graph for Agent Navigation"}, {"paperId": "5a94aaa3ad624608e46a75de49452a03de568a07", "title": "Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding"}, {"paperId": "3435e193998ec4118f51bbb608a843b0e123661b", "title": "Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule"}, {"paperId": "f0152a8fd87cb60ad30c296823829802c13a9986", "title": "Object-and-Action Aware Model for Visual Language Navigation"}, {"paperId": "7750a706a140f63a28dd3903b9a187647bf38d39", "title": "Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation"}, {"paperId": "62516303058a1322450b58e4cd778ab873b5e531", "title": "Object Goal Navigation using Goal-Oriented Semantic Exploration"}, {"paperId": "3dac85f20ffc1b6ef742f81f5a59c163b387f486", "title": "Probing the invariant structure of spatial knowledge: Support for the cognitive graph hypothesis"}, {"paperId": "dd2d81e91b051dba756df2fafb79dcb4aac7d057", "title": "Diagnosing the Environment Bias in Vision-and-Language Navigation"}, {"paperId": "6abd97eaebc2bb86fbd03b5e70db0abe8dfbfe50", "title": "RMM: A Recursive Mental Model for Dialog Navigation"}, {"paperId": "d1ac487f21829ef56c8ffdcd37ea414bce68c809", "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web"}, {"paperId": "f09e5903530e035e0d4b68fd63fb515d076d1354", "title": "Sub-Instruction Aware Vision-and-Language Navigation"}, {"paperId": "c2496cead9423897cd37be77ddd6cf2f5d03f1c8", "title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments"}, {"paperId": "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9", "title": "Multi-View Learning for Vision-and-Language Navigation"}, {"paperId": "6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52", "title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training"}, {"paperId": "4270ea0c40857b0fdfe30feab34e71d416929f4b", "title": "Semantic Information for Robot Navigation: A Survey"}, {"paperId": "f4cf4246f3882aa6337e9c05d5675a3b8463a32e", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"}, {"paperId": "7fbeca0aaeb41b597d128c322294f5d4279ef44d", "title": "Just Ask: An Interactive Learning Framework for Vision and Language Navigation"}, {"paperId": "4e2fab8c96e202d0aac3f68721e6d3f1f1f11bfd", "title": "Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks"}, {"paperId": "c8b041226ef3231d87297ac546bd1c2fd82b35ca", "title": "Talk to the Vehicle: Language Conditioned Autonomous Navigation of Self Driving Cars"}, {"paperId": "6c90693c40c5a0a77ca3862bf4406047ce9426c6", "title": "Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory"}, {"paperId": "caf39cb40e29587c59204fa486d30fc07d864593", "title": "RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation"}, {"paperId": "104f75283ae9027eb478e7984bd26b680277ce6f", "title": "Robust Navigation with Language Pretraining and Stochastic Sampling"}, {"paperId": "79bc6e1fe465aec49d7f0252f295c0ad9cdaf389", "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "1b9ce27801c077433245ad0f9e43e3c38441cecd", "title": "Vision-and-Dialog Navigation"}, {"paperId": "06036f49f15a47842b5dc41ca955976472bb07e8", "title": "Chasing Ghosts: Instruction Following as Bayesian State Tracking"}, {"paperId": "bcd411becb0f438720f84fd7d7a16ef6b94c8271", "title": "General Evaluation for Instruction Conditioned Navigation using Dynamic Time Warping"}, {"paperId": "68ccecb380ecfc0a4b294b84e3d0b6ff6884c4df", "title": "Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation"}, {"paperId": "1bdd8f6d3900e6e7fe492ea21fcdd87f3d8c857f", "title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments"}, {"paperId": "c8c76626db4246c944642e86d19665025fa7deb4", "title": "Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout"}, {"paperId": "6b48369e03fc057af7fe3aaac42303a3dd2fccb4", "title": "Learning To Follow Directions in Street View"}, {"paperId": "307f2c3ae7c795b28e14b639999e1971f669f5b8", "title": "Non-Euclidean navigation"}, {"paperId": "3c54b796cc10cb530f77caa4d18e1c80ac863822", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}, {"paperId": "29e13746fa5aed13e51558a521a39aaeaa99c1b1", "title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation"}, {"paperId": "7bf755b7ab4a768d204583a6f28ba14f9c4aa2e4", "title": "A comprehensive study for robot navigation techniques"}, {"paperId": "03fdf3abf8d6bb3ff35dc87742ad66722997caeb", "title": "Vision-Based Navigation With Language-Based Assistance via Imitation Learning With Indirect Intervention"}, {"paperId": "f75f0750a00f6f85107985c70eca9c275b5e0962", "title": "TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments"}, {"paperId": "c66b8e508718f4b7f14829e5c2cde0add31d2693", "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation"}, {"paperId": "727332fd6bd446a4dd402368988a1f34fb0649c8", "title": "Navigating cognition: Spatial codes for human thinking"}, {"paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9", "title": "Recurrent World Models Facilitate Policy Evolution"}, {"paperId": "ae09e1df06d7fdf7bece8c454085ddbf5fedec2a", "title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction"}, {"paperId": "56a0ead811a1bf15e42be8a9a007b0299636f213", "title": "Talk the Walk: Navigating New York City through Grounded Dialogue"}, {"paperId": "893186c6bc08a17cb3f9f94fa3f14e9ad20b0525", "title": "Speaker-Follower Models for Vision-and-Language Navigation"}, {"paperId": "e89a4fe6e8286eccedd702216153f0f248adb151", "title": "Gibson Env: Real-World Perception for Embodied Agents"}, {"paperId": "10e085ca54c6bc3347657e05ee5acc44c0e8adb7", "title": "Using virtual environments to investigate wayfinding in 8- to 12-year-olds and adults."}, {"paperId": "c37c23b12e00168833eccff8025a830ce27c5abc", "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"}, {"paperId": "b7bc7ee8637f63779d843d17ebd648dfd2211085", "title": "The cognitive map in humans: spatial navigation and beyond"}, {"paperId": "8337441971f941716a9e525a67f37088eb01fd13", "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments"}, {"paperId": "c8dae0af6965a180f79a35517a4ce339d3536bec", "title": "Crawling Predicts Infants\u2019 Understanding of Agents\u2019 Navigation of Obstacles"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "58dde3ad11a6ea2233b6d2ea0eab8658ddf1436a", "title": "Children's spatial thinking: does talk about the spatial world matter?"}, {"paperId": "8f3405a3fbbb6b03b9f9973805c82e05cb493819", "title": "Cognitive effects of language on human navigation"}, {"paperId": "563a49c1b8608f1454765e10e1b29bce0e6c1bf5", "title": "Evidence from an emerging sign language reveals that language supports spatial cognition"}, {"paperId": "d84dd225b0873ce9f4793243ecb30c74ed74cae6", "title": "Cognitive load of navigating without vision when guided by virtual sound versus spatial language."}, {"paperId": "851f27df3a1ec8daa693f6642194562e3fe9769a", "title": "Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions"}, {"paperId": "8c560202ceda574c65fcef53ba6a22c2e6640384", "title": "The Organization of Learning"}, {"paperId": "369c1dceb16d2ae051b8190e44ad463fda334eaa", "title": "The Hcrc Map Task Corpus"}, {"paperId": "9661f58faffe68ae080e977094b7f202d718150e", "title": "The Hippocampus as a Cognitive Map"}, {"paperId": "b01a1328a916df88471e7be032ed281da9680211", "title": "The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat."}, {"paperId": "e84d660ebf894e2fb85d7b27985bfbf07b9acd22", "title": "Cognitive maps in rats and men."}, {"paperId": "fa1e25a204a0b1ff32e25d9178446a188ac5efe2", "title": "Learning to Navigate"}, {"paperId": "3549c2005fe3a8be945deab54b7ee6a5140b47a5", "title": "Enhancing Vision and Language Navigation with Prompt-based Scene Knowledge"}, {"paperId": "7637ed79d30d0139901175ae4abedd822c217ab4", "title": "3D-LLM: Injecting the 3D World into Large Language Models"}, {"paperId": "d3eaa8481faa3bfe4430b2ca27863f6ee01aa431", "title": "Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation"}, {"paperId": "c61aa6e88e90113b56595ccbea0770b2f1edf505", "title": "a platform"}, {"paperId": "b021962b5ecd1fe2d94b5488ec0ed99004b8585a", "title": "EMMA: A Foundation Model for Embodied, Interactive, Multimodal Task Completion in 3D Environments"}, {"paperId": "bc73bc1d7f63a33e686d0bb61f64b10ae31f1a69", "title": "SEAGULL: An Embodied Agent for Instruction Following through Situated Dialog"}, {"paperId": "23cf95de13d7ad9273f4baa8e8b864f415ef88f2", "title": "SlugJARVIS: Multimodal Commonsense Knowledge-based Embodied AI for SimBot Challenge"}, {"paperId": "82e0fd80ac234fbad07fd05058e4c2ab3256ca8a", "title": "Explicit Object Relation Alignment for Vision and Language Navigation"}, {"paperId": "a00369391b59b4edfc19dd02bc2f81a93f9cb9da", "title": "NDH-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue"}, {"paperId": "3ed715e6d0cda5897b71d03691cfeae2201bccc5", "title": "Deep Learning for Embodied Vision Navigation: A Survey"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "d4a5390c0749876cadd5f863b0f20740a001de44", "title": "Navigational strategies and models"}, {"paperId": null, "title": "2023. Drivelm: Driving with graph visual question answering"}, {"paperId": null, "title": "cities without a map"}, {"paperId": null, "title": "2024. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model"}, {"paperId": null, "title": "2024b. Correctable landmark discovery via"}, {"paperId": null, "title": "2023b. Goat: Go to any thing"}, {"paperId": null, "title": "2023. Towards a holistic landscape of situated theory of mind in large language models"}, {"paperId": null, "title": "the 2021 Conference on Empirical Methods in Natural Language Processing"}, {"paperId": null, "title": "2024b. Multi-object hallucination in vision language models"}, {"paperId": null, "title": "2022. Di-alfred: Dialogue-enabled agents for embodied instruction following"}, {"paperId": null, "title": "2022. Do as i can, not as i say: Ground-ing language in robotic affordances"}, {"paperId": null, "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2022. Llm-planner: Few-shot grounded planning for embodied agents with large language models"}, {"paperId": null, "title": "2022. Integrity of visual navigation\u2014developments, challenges, and prospects"}, {"paperId": null, "title": "2024. Homerobot: Open-vocabulary"}, {"paperId": null, "title": "2022. Language models as agent models"}, {"paperId": null, "title": "2022. Target-driven structured transformer"}]}