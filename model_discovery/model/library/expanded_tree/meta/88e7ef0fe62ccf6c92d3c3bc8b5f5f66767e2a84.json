{"paperId": "88e7ef0fe62ccf6c92d3c3bc8b5f5f66767e2a84", "abstract": "Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model\u2013where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. \u201cwhile\u201d) often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives. We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@k and a novel incremental variation.", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work investigates how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task, and experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives."}, "embedding": {"model": "specter_v2", "vector": [0.48746559023857117, 0.3902547061443329, -0.5668780207633972, -0.17816220223903656, -0.19365793466567993, -0.6621008515357971, 0.5079968571662903, 0.08522016555070877, -0.4653639793395996, -0.17634643614292145, 0.4695267677307129, -0.7610361576080322, 0.4656318128108978, -0.03912115469574928, -0.5255914926528931, 0.30825871229171753, -0.6362362504005432, 0.1300320029258728, -0.32382240891456604, -0.46896713972091675, 0.12847106158733368, -0.8627009391784668, -0.668463945388794, 0.21827949583530426, 0.5934987664222717, -0.2660658061504364, -0.08740586787462234, 1.2170625925064087, -0.6891500949859619, 0.4886361062526703, 0.8000300526618958, -0.33817288279533386, 0.20345762372016907, -0.16383227705955505, -0.6386248469352722, -0.06415925920009613, 0.3381183445453644, -0.2433273047208786, -0.1505996733903885, 0.5001659393310547, -0.31396475434303284, -0.05741610378026962, 0.4385806620121002, -0.8447865843772888, -0.32043319940567017, 1.1157433986663818, 0.18773774802684784, 0.39657706022262573, 0.3383970260620117, -0.10124953836202621, 1.3016124963760376, -0.8853402733802795, 0.3776102066040039, 0.8407871723175049, 0.4924187660217285, 1.0960960388183594, -0.4687272012233734, 0.024466710165143013, 0.17497025430202484, -0.30515140295028687, -0.6269288063049316, -0.015878599137067795, -0.17111848294734955, -0.596276581287384, 2.0531935691833496, -0.3507194221019745, 0.15735001862049103, 0.3380746841430664, -0.11798132956027985, 1.1576398611068726, -0.13717927038669586, -1.2192323207855225, -0.4747058153152466, 0.44244104623794556, 0.04722479730844498, 1.0749876499176025, -0.6343849301338196, 0.38968199491500854, -0.7503175735473633, -0.06872890889644623, 0.328927218914032, 0.06374039500951767, -0.07671434432268143, -0.5474893450737, -0.8140899538993835, 0.3812185525894165, 0.299116849899292, 1.1093882322311401, 0.25438985228538513, 1.0033903121948242, 0.9093167781829834, 0.5987383723258972, -0.4747341573238373, 0.5569666028022766, -0.27276504039764404, -0.04339534044265747, -0.8577134609222412, 0.08531883358955383, 0.088904969394207, 0.9348224997520447, 0.3245924115180969, 0.3773287534713745, -1.1130527257919312, 0.056050099432468414, 1.1853396892547607, -0.09299088269472122, 0.42063429951667786, -0.3089035451412201, 0.7441020607948303, -0.2672593593597412, -0.0849730521440506, -0.03349953517317772, -0.018728718161582947, 0.17251548171043396, -0.3773941397666931, -1.2854121923446655, -0.4572259783744812, -0.2298271209001541, -0.8656982183456421, 0.9336984753608704, -0.2632237672805786, 0.045007362961769104, 0.4682016670703888, 0.40234753489494324, 0.4964563846588135, 0.7465526461601257, 0.41169285774230957, 0.38623669743537903, 0.5787257552146912, -0.637667715549469, -0.021687794476747513, -1.1480695009231567, 1.356768012046814, -0.17515096068382263, 0.6239136457443237, -0.6797118782997131, -1.3178210258483887, -1.1072428226470947, -0.8384004831314087, 0.10939770191907883, -0.6020256280899048, 0.7294579148292542, 1.1874479055404663, 0.9133346676826477, -1.4678326845169067, 0.8335511684417725, -0.11736109852790833, -0.29099223017692566, 0.15150004625320435, 0.15583159029483795, -0.00961461290717125, -0.6276834607124329, -1.1281421184539795, 0.19591495394706726, 0.49713242053985596, -0.8068557977676392, -0.3690073490142822, -0.7511903643608093, -1.5493813753128052, -0.3683564066886902, 0.11851136386394501, -0.7680090069770813, 1.2707806825637817, -0.0354931466281414, -1.3995851278305054, 0.9175124168395996, -0.09701073914766312, 0.18256112933158875, 0.4314056634902954, 0.1575736552476883, -0.1442611664533615, -0.6036449074745178, -0.13433361053466797, 0.6787058115005493, 0.49447187781333923, -0.253671258687973, -0.06232519820332527, 0.5003230571746826, 0.18197831511497498, -0.22035758197307587, -0.5096297860145569, 0.7950310707092285, 0.00460977153852582, 0.05134575441479683, 0.06376077979803085, 0.49637725949287415, 0.03636569902300835, -0.46092313528060913, -0.4968815743923187, -1.05374276638031, 0.3268972337245941, 0.02222994714975357, 0.8459222912788391, -1.0664019584655762, -0.3223864734172821, -0.24983765184879303, -0.273470401763916, -0.2407442182302475, -0.8298916220664978, 1.1212102174758911, -0.9740971922874451, 0.4996483325958252, -0.6016745567321777, -0.8307523131370544, 0.350569486618042, -0.5649335980415344, -0.6988826990127563, -0.29757237434387207, 0.12761163711547852, 1.2302842140197754, -0.854776918888092, 0.24991047382354736, 0.09661837667226791, 0.06697691977024078, -1.0957187414169312, 1.1289936304092407, -0.34169021248817444, 0.22360998392105103, 0.2558043897151947, -0.42291852831840515, -0.026212157681584358, -0.09874772280454636, 0.021794039756059647, -0.09982170164585114, -0.025830475613474846, 0.47831231355667114, 0.33210161328315735, 2.00846529006958, -0.7572061419487, 0.17542925477027893, -0.3658427894115448, -0.8607025742530823, 0.27349844574928284, 0.5201975107192993, -0.0493859201669693, 0.05743816867470741, -0.1399831920862198, 0.7524023056030273, -0.2686580717563629, -0.11559528857469559, 0.787064790725708, 0.16630873084068298, -0.45183852314949036, 0.4731994867324829, 0.4887958765029907, -0.5751981735229492, 0.8789950609207153, 0.41470617055892944, 1.0097987651824951, 0.5571807026863098, 0.3469752371311188, 0.05952382832765579, 0.6954242587089539, -0.29301437735557556, -0.6399356126785278, 0.2924574613571167, 1.0574064254760742, 0.6873756647109985, 0.29563823342323303, -0.6614249348640442, -0.4299769103527069, -0.08841727674007416, 0.9099855422973633, 1.7344908714294434, -0.4727139174938202, -0.18223395943641663, -0.7584012746810913, -0.7056277394294739, -0.20106682181358337, 0.49501562118530273, -0.572644054889679, -0.7261512875556946, -0.818748414516449, -0.8596284985542297, 0.6748175621032715, 0.6421757340431213, 0.8520824909210205, -0.37385037541389465, -0.5870998501777649, -0.2168676108121872, 0.491654634475708, -0.5460585951805115, -0.7087217569351196, 0.3838965594768524, -0.6595513820648193, 0.0845353901386261, -0.2953692674636841, -0.24482670426368713, 0.55299311876297, -0.6861230134963989, 1.0223294496536255, 0.18704545497894287, -0.5045352578163147, 0.48436659574508667, 0.5883502960205078, -0.2524509131908417, -1.118180513381958, 0.5156465172767639, -0.360557496547699, -0.5800307989120483, 0.31058263778686523, 0.41836652159690857, 0.1889471858739853, -0.11312516033649445, -0.6812088489532471, 0.3081609010696411, -0.12145571410655975, 0.27111849188804626, -0.1833779513835907, -0.3081130385398865, -0.5434846878051758, -1.1866340637207031, 0.6845065355300903, 0.39956924319267273, -0.1245027706027031, 0.4725494682788849, -0.4516243040561676, -0.40825581550598145, 0.8795326352119446, -0.4974963665008545, -0.1159137487411499, -0.8916207551956177, 0.30848097801208496, 0.2836795449256897, -0.3701196312904358, 0.5063950419425964, 0.42548683285713196, 0.1597689390182495, 0.390870064496994, 0.6230639219284058, 0.3683484196662903, 0.3164684772491455, 0.6185036301612854, -1.2559231519699097, 0.09775799512863159, -0.3006589710712433, 0.8892408609390259, -0.3477192223072052, -0.34984833002090454, -0.10893777757883072, 0.04916323721408844, 0.058381132781505585, -0.05544772744178772, -0.3446091413497925, 0.31622594594955444, -0.4851784110069275, -0.7879840731620789, -0.11582157015800476, -1.253359079360962, -0.48336267471313477, -0.04206660017371178, -0.5528448224067688, -0.20924852788448334, -0.9041340351104736, -1.1132766008377075, -0.2976875603199005, -0.30666041374206543, -1.1033213138580322, 0.5303695797920227, -0.2515910267829895, -0.48469507694244385, -0.9047912359237671, 0.3971171975135803, -0.4163023829460144, 0.8462694883346558, -0.7830002307891846, 1.4439500570297241, 0.31346794962882996, -0.1300600916147232, -0.18873310089111328, 0.4209384024143219, 0.6190704107284546, -0.1825890690088272, 0.6526679992675781, -0.45110031962394714, -0.2670562267303467, -0.5466988682746887, -0.6303880214691162, 0.06504664570093155, 0.12539051473140717, 0.5231937170028687, 0.19076547026634216, -0.5204324722290039, 0.22607725858688354, 1.8831466436386108, -0.5316004753112793, 0.30514657497406006, 0.2433498203754425, 1.2798880338668823, 0.6600275635719299, -0.39015892148017883, 0.28372615575790405, 0.4782734513282776, 0.4025183916091919, -0.2013271301984787, -0.2502151131629944, 0.08554703742265701, -0.6692710518836975, 1.0059295892715454, 1.5952211618423462, 0.2080630362033844, 0.09506437927484512, -1.7522469758987427, 0.6840283870697021, -1.1380757093429565, -0.10127855092287064, 0.47252029180526733, 0.44092920422554016, 0.680891752243042, -0.6434330940246582, -0.17612653970718384, 0.020814256742596626, 0.428111732006073, 0.0037346116732805967, 0.04221496731042862, -0.8122358918190002, 0.3891916871070862, 0.3658977150917053, 0.06750382483005524, 0.4821944236755371, -0.24677154421806335, 0.32192403078079224, 14.535750389099121, 0.7938845753669739, -0.10392770916223526, 0.3576682209968567, 0.4725276529788971, 0.4861554503440857, -0.9009564518928528, 0.09040208160877228, -0.9781239628791809, -0.31593945622444153, 1.059516429901123, -0.15864738821983337, 0.7782212495803833, 0.17457517981529236, -0.1618225872516632, 0.12625160813331604, -1.1694345474243164, 0.5083001852035522, 0.48410773277282715, -1.4696778059005737, 0.39693042635917664, 0.18467123806476593, 0.6339380741119385, 0.038826968520879745, 0.8959056735038757, 1.102161169052124, 0.24804827570915222, -0.6406959295272827, 0.6131671071052551, -0.029331186786293983, 1.2034884691238403, 0.13937242329120636, 0.5490683317184448, 0.4657744765281677, -1.0162256956100464, -0.4178334176540375, -0.44027283787727356, -1.177274465560913, 0.19232523441314697, 0.09725083410739899, -0.9463408589363098, -0.09454101324081421, -0.5436285734176636, 0.9770337343215942, -0.11955984681844711, 0.020055970177054405, -0.40637603402137756, 0.5145165920257568, 0.30535855889320374, -0.3370385766029358, 0.15846678614616394, 0.7528350949287415, -0.028064249083399773, 0.10386206954717636, 0.25211942195892334, -0.49873387813568115, 0.34864717721939087, 0.6912457346916199, -0.4613035321235657, 0.18169325590133667, -0.6701692342758179, -0.47962287068367004, -0.38503918051719666, 0.5008719563484192, 0.19760023057460785, 0.336917519569397, -0.6551616787910461, 0.1628730744123459, 0.7306115627288818, 0.28012266755104065, -0.3362305164337158, -0.13043764233589172, 0.009252319112420082, -0.18445923924446106, -0.08222588896751404, 0.3443050682544708, -0.44745969772338867, -0.5573983788490295, -0.5034586787223816, -0.28187301754951477, 0.24474123120307922, -0.7242991924285889, -0.572712779045105, 0.7853062748908997, -0.07989005744457245, -0.9333296418190002, -0.14751140773296356, -0.9428842663764954, -0.5173970460891724, 0.48370978236198425, -1.420695185661316, -0.6422551870346069, 0.39234793186187744, -0.341505765914917, -0.7746761441230774, -0.37762153148651123, 1.3083423376083374, -0.024110475555062294, -0.1571466028690338, 0.07116983085870743, 0.020658383145928383, 0.06590069085359573, -0.17788660526275635, -1.245816707611084, 0.9005750417709351, 0.24185910820960999, -0.27026668190956116, 0.6202585697174072, 0.09674039483070374, 0.15189120173454285, -0.9537948966026306, -0.11158487200737, 0.5964747667312622, -1.00873863697052, -0.31887516379356384, -0.8785076141357422, -0.6262882351875305, 0.07913526147603989, 0.8618566989898682, -0.5311990976333618, 0.9007843732833862, -0.014115969650447369, -0.6549994945526123, -0.11427202075719833, -0.8620401620864868, 0.17771607637405396, 0.7221718430519104, -1.0111110210418701, -0.29316896200180054, 0.1350039392709732, 0.26255351305007935, -1.0016790628433228, -0.43530330061912537, -0.18841899931430817, -0.10641804337501526, 0.11911652237176895, 0.7667156457901001, -0.46942728757858276, 1.5249756574630737, 0.5837909579277039, 0.18675458431243896, -0.9286374449729919, -0.002734547248110175, -0.9292493462562561, 0.4034424126148224, 0.2834669351577759, 1.1009701490402222, -0.5143752098083496, 0.5710080862045288, 1.0976179838180542, 0.28259214758872986, -0.3061211407184601, -0.43535107374191284, -0.26729637384414673, 0.3499719798564911, -0.46240851283073425, 0.4777466356754303, 0.15262775123119354, 0.3433411717414856, 0.174586683511734, 0.4958992600440979, 0.23656029999256134, -0.5917418599128723, -0.7063072323799133, 0.7931762933731079, 0.2799695134162903, -0.2815937399864197, -0.42576339840888977, -0.6151050329208374, -1.0444889068603516, 0.5117538571357727, -1.6878412961959839, 0.27657270431518555, -0.9385294914245605, -0.220167875289917, 0.2234547734260559, 0.06111699342727661, 0.5355035066604614, 0.1585712730884552, -0.4802815318107605, -0.4424445927143097, -0.8493218421936035, -0.15560655295848846, 0.9494962096214294, 0.5406169891357422, -0.8808848857879639, 0.2530359625816345, -0.4582521319389343, -0.09414797276258469, -0.2102973461151123, 0.32506290078163147, -0.4791184365749359, -1.0985016822814941, -1.5747342109680176, 0.7234952449798584, -0.00023109742323867977, 0.18682698905467987, -0.6559512615203857, 0.22267143428325653, 0.35030174255371094, -0.4084528088569641, 0.5250750780105591, -0.46039116382598877, -0.8111787438392639, -0.3280635476112366, 0.3810814917087555, -0.950461745262146, 0.4792080521583557, 0.5277296900749207, -0.8047956824302673, -0.23769371211528778, -0.008908228017389774, -0.2858698070049286, -1.2300268411636353, -0.6816040873527527, 0.21188156306743622, -0.5111039280891418, 0.142380490899086, 0.060326892882585526, -0.36728227138519287, -1.0314710140228271, -0.01860477402806282, 0.19359667599201202, 0.4396955966949463, 0.0862468034029007, 0.8773396611213684, 0.3276498019695282, -0.9819322824478149, -0.13843576610088348, 0.2500988245010376, 0.3203471899032593, -0.37768325209617615, 0.6219645738601685, 0.41817599534988403, -0.6582102179527283, 0.20678828656673431, 0.213276207447052, -0.10542041808366776, -0.8159456849098206, -0.07975763827562332, 0.4681079685688019, -0.5331928133964539, 0.19108209013938904, 1.2825777530670166, -0.6396588087081909, -1.53738534450531, -0.42821162939071655, -1.0175542831420898, -0.3181456923484802, -0.8634723424911499, 0.5219007134437561, -0.182038351893425, -0.0684749037027359, 0.12279720604419708, -0.523345410823822, 0.42037007212638855, -0.2501875162124634, -0.7744542360305786, 0.5431978702545166, 0.23385484516620636, -0.8969972133636475, 0.5972481966018677, 0.8981149196624756, -0.8390105366706848, -0.41634416580200195, -0.21455790102481842, -0.4142858386039734, -0.344347208738327, 0.007243942469358444, -0.39696362614631653, -0.6166594624519348, 1.1166898012161255, 0.3750006854534149, 0.30175960063934326, 0.0006900309235788882, -0.13429167866706848, -0.19654588401317596, 0.4465633034706116, 0.27135932445526123, -0.46988505125045776, -0.34176596999168396, 1.2072153091430664, 0.994803249835968, -0.702316403388977, 0.0652189627289772, -0.526332437992096, -0.7153707146644592, 1.1139962673187256, 0.7945217490196228, 0.21768733859062195, 0.6256644129753113, -0.3619495928287506, 0.10119592398405075, 0.31174781918525696, -0.8146855235099792, 0.21772752702236176, 0.6207197904586792, 1.3454170227050781, 1.191038966178894, 0.22773154079914093, 0.10985632240772247, 0.45420485734939575, -0.06973780691623688, 0.34099119901657104, 0.958196222782135, 1.0110753774642944, 0.2707848846912384, -0.5377628207206726, 0.0009756824001669884, 0.716736912727356, -0.27147719264030457, -0.7806432843208313, 0.29118040204048157, 0.3823103606700897, 0.24712838232517242, 0.23628288507461548, 0.2147981971502304, -0.04005977511405945, 0.5841509699821472, 0.5623038411140442, 0.7939369082450867, -1.0887147188186646, -0.37834951281547546, -0.7673218846321106, -0.549933910369873, 0.21263045072555542, -0.48663464188575745, -0.4431212246417999, -0.6582357883453369, -0.22252076864242554, 0.555499255657196, 0.20008628070354462, 0.27392104268074036, 0.8948695659637451, 0.4059143364429474, 0.5637151002883911, -0.2257789820432663, -0.5499660968780518, -0.3660939037799835, -0.8508644104003906, 0.11453583091497421, -0.3765634298324585, -0.25233858823776245, -0.29867804050445557, -0.013848173432052135, 0.2647787630558014]}, "authors": [{"authorId": "48810605", "name": "Fenia Christopoulou"}, {"authorId": "2044459", "name": "Guchun Zhang"}, {"authorId": "2346538", "name": "Gerasimos Lampouras"}], "references": [{"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a", "title": "SantaCoder: don't reach for the stars!"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "ec4c8d99eb1c028c43af6d8bbf727392d351cb59", "title": "Efficient Training of Language Models to Fill in the Middle"}, {"paperId": "06ea568379211ffa07d9605f66f26f6f736ea5e0", "title": "PanGu-Coder: Program Synthesis with Function-Level Language Modeling"}, {"paperId": "a08a3b08a5a1de6462a7da2906b1cd81691d6c18", "title": "CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "4b27f18bff43d605805c92696a979714ced0b805", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "d095f9ffcb5905bf0858ad1769d3d90e2e8737e2", "title": "Jigsaw: Large Language Models meet Program Synthesis"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "42a7015e48a1e00b70ebb442a82afb4b10017c0b", "title": "When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations"}, {"paperId": "b5f9c1cc4c74d973986bc4b352b85a6ee2f475d6", "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language"}, {"paperId": "c07651110d3b98b63607557b57808d15d99013dd", "title": "ProteinBERT: a universal deep-learning model of protein sequence and function"}, {"paperId": "0b077c9577f4297dcf3da835e253d21965bbc6e0", "title": "CoTexT: Multi-task Learning with Code-Text Transformer"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "1c6970dc9d4da9f5e94399e344fe8ba901d8fe81", "title": "PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"}, {"paperId": "f23a0e443fe931aa2fed932421bf47c1a4fcf619", "title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"}, {"paperId": "4083958684292f6fa2f5c7fd4f9be975e80145b6", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "75e924bd79d27a23f3f93d9b1ab62a779505c8d2", "title": "Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks"}, {"paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "f5a28db512357b700b62fb655ef4a90864e2fe7e", "title": "Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "463fefdbd81a4a0a32cf59bc58a9545757c8cf2e", "title": "Pre-trained Contextual Embedding of Source Code"}, {"paperId": "7102bb3fe73bd057ff161d9db5214a267c1ef312", "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"}, {"paperId": "1906e3a2fda12641a42739e3fb6a8f8b1accc8dd", "title": "SPoC: Search-based Pseudocode to Code"}, {"paperId": "56e3f8867c5371794aa7f052877b512a64ead472", "title": "Selfie: Self-supervised Pretraining for Image Embedding"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"}, {"paperId": "87078d95bee341a1767034d9432fb34937ecf65a", "title": "SciBERT: Pretrained Contextualized Embeddings for Scientific Text"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "61337bc3b39599c5d95d25ad70b7e17c8dfca514", "title": "Papers"}, {"paperId": "1b7ffb7d059f32083712127c9bef9e669d57f2b2", "title": "Language Model for Text Analytic in Cybersecurity"}, {"paperId": "1b4c19168410fb2690d285b205ab2281793db81a", "title": "A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": null, "title": "LEGAL-BERT: The muppets straight out of law school"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "be used to create text-to-code instances which are considered task-specific data for program synthesis, while code-only function snippets can expose models to generic Python programming"}, {"paperId": null, "title": "A Data Collection and"}, {"paperId": null, "title": "2022. TS-BERT: A fusion model for pre-trainning time series-text representations"}]}