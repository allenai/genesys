{"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "abstract": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 95, "influentialCitationCount": 9, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.16264", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters and experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters."}, "embedding": {"model": "specter_v2", "vector": [-0.19015824794769287, 0.06434574723243713, -0.506802499294281, -0.26595553755760193, -0.1511407047510147, -0.3757098913192749, 0.634205162525177, -0.41528499126434326, -0.7689130306243896, -0.3384603261947632, 0.2976095378398895, -0.15297625958919525, 0.2345554232597351, 0.32895612716674805, 0.051816124469041824, 0.3927597403526306, -1.116580843925476, 0.3521687686443329, -0.5675304532051086, -0.16771388053894043, -0.48672008514404297, -0.527328610420227, -1.022692322731018, -0.2894169092178345, 0.4540969729423523, 0.18398542702198029, 0.11145646125078201, 1.0026980638504028, -0.3607204556465149, -0.2101525515317917, 0.5227504968643188, -0.2102968841791153, 0.5285530090332031, 0.09171989560127258, -0.11777015030384064, 0.07909515500068665, 0.28605732321739197, -0.7631893754005432, -0.5433306097984314, 0.5187878012657166, -0.18798333406448364, 0.6295519471168518, 0.16728821396827698, -0.8770878911018372, -0.28398457169532776, 0.4859761893749237, 0.5391201972961426, 1.084890604019165, -0.18317832052707672, -0.6003228425979614, 1.0275237560272217, -1.3194807767868042, 0.2572374641895294, 1.4704408645629883, 0.6114160418510437, 0.43844491243362427, -0.19254697859287262, -0.45324140787124634, 0.8357830047607422, -0.4002600610256195, -1.053540825843811, -0.17436186969280243, -0.407641738653183, -0.09649011492729187, 2.1047797203063965, -0.428443968296051, 0.26885786652565, 0.3581184148788452, -0.23776598274707794, 1.2615296840667725, -0.13469178974628448, -0.9583590626716614, -0.31763455271720886, 0.16468052566051483, 0.36427417397499084, 0.9497358798980713, -0.17339006066322327, 0.4880955219268799, -1.3248549699783325, -0.27365171909332275, 0.27248597145080566, -0.19127027690410614, 0.21266822516918182, -0.14080743491649628, 0.07864750921726227, 0.9282868504524231, -0.09401664137840271, 0.5864116549491882, 0.06470980495214462, 0.9124387502670288, 0.7573838233947754, 0.5600997805595398, 0.860039234161377, 0.22094088792800903, -0.5783641338348389, 0.11231841892004013, -1.0813246965408325, 0.3224007785320282, 0.4194386601448059, 1.0609596967697144, -0.23245000839233398, 0.30039265751838684, -0.3236370384693146, 0.4087856709957123, 1.184941291809082, 0.27487143874168396, 0.7350839972496033, -0.2050904482603073, 0.4013684391975403, -0.7599677443504333, 0.14444756507873535, -0.39756065607070923, -0.529711127281189, -0.5277758240699768, -0.5459791421890259, -1.6531316041946411, -0.48004069924354553, -0.010385877452790737, -0.6182185411453247, 0.8868440985679626, -0.08907730132341385, 0.06318166851997375, -0.11034590005874634, 0.5454268455505371, 0.021123278886079788, 0.5553077459335327, 0.4415040612220764, 0.13535679876804352, 0.7441778182983398, -0.5364171862602234, -0.5962625741958618, -0.884232759475708, 0.9504833817481995, -0.5641642808914185, 0.7226155996322632, -0.31472793221473694, -1.1946425437927246, -0.7293550372123718, -0.7990008592605591, 0.0992564857006073, -0.6319056153297424, 0.3885037302970886, 1.4124634265899658, 0.8266932368278503, -0.9393563270568848, 0.6342228055000305, -0.6310811638832092, -0.2779146432876587, 0.32196471095085144, 0.17089210450649261, 0.3743310868740082, -0.5777740478515625, -1.2741836309432983, 0.27974817156791687, 0.2685183584690094, -0.9654388427734375, 0.20763224363327026, -0.5132400393486023, -0.9169749021530151, -0.14713981747627258, 0.2137482911348343, -0.4148361086845398, 1.0893852710723877, 0.01968476176261902, -1.0636107921600342, 0.8349767327308655, -0.21284034848213196, 0.040193863213062286, 0.6503772139549255, -0.5910375118255615, -0.8616704344749451, -0.455843448638916, -0.39230725169181824, 0.43497729301452637, 0.47365033626556396, 0.03187830001115799, -0.06953826546669006, 0.44199711084365845, -0.4998270869255066, 0.05496007204055786, -0.7304917573928833, 0.7457303404808044, -0.4040985405445099, -0.5719817280769348, 0.3826979100704193, 0.30618995428085327, 0.13275520503520966, -0.23405320942401886, -0.23648564517498016, -1.0172511339187622, 0.5151842832565308, -0.3308704197406769, 1.0225750207901, -1.0039466619491577, -0.6947466135025024, 0.1803189516067505, -0.2639354169368744, -0.03680262342095375, -0.9290598630905151, 0.85748291015625, -0.016091153025627136, 0.9606971144676208, 0.021583031862974167, -1.380810022354126, 0.3564826548099518, -0.053430818021297455, -0.823326051235199, -0.08074435591697693, 0.08580492436885834, 0.8055230379104614, -0.49381062388420105, 0.34197622537612915, -0.04351367801427841, 0.36724230647087097, -1.258399486541748, 1.1476162672042847, -0.45346373319625854, -0.11668861657381058, 0.16270442306995392, -0.17277447879314423, 0.43500036001205444, -0.21501614153385162, 0.5950618982315063, 0.01438469160348177, -0.07904890179634094, 0.44432464241981506, -0.5270695090293884, 1.2180496454238892, -0.7776075005531311, 0.32258719205856323, -0.12091074883937836, -0.38620463013648987, -0.06765903532505035, 0.23982872068881989, -0.21432818472385406, -0.2658720016479492, 0.07817234098911285, 0.5179654955863953, -0.4946116805076599, -0.06361107528209686, 0.9096149206161499, 0.6780633330345154, -0.24670904874801636, 0.3403245508670807, 0.6149282455444336, -0.514977753162384, 0.6974049210548401, 0.38801679015159607, 0.34893691539764404, 0.40771690011024475, 0.41545698046684265, -0.10436456650495529, -0.08847035467624664, -0.9975661635398865, -0.4393569231033325, 0.3659721612930298, 0.6851488351821899, 0.3681022822856903, 0.0682847872376442, -0.7267475128173828, -0.7421455383300781, -0.05181177332997322, 0.42702776193618774, 1.879012942314148, -0.3106500208377838, -0.31104597449302673, -0.6236844062805176, 0.010962745174765587, 0.22592933475971222, -0.11939439177513123, -0.17081035673618317, 0.14118722081184387, -0.7133080363273621, -1.035667061805725, 0.6426337361335754, -0.10572323203086853, 0.8025732040405273, -0.12958145141601562, -0.32146739959716797, -0.16524586081504822, 0.3575519621372223, -0.6723657846450806, -0.6733056902885437, 0.25962507724761963, -0.5765190124511719, 0.16959598660469055, 0.18959997594356537, -0.015799356624484062, -0.1691587120294571, -0.2703148126602173, 0.9246237277984619, 0.41666051745414734, -0.5310149788856506, 0.29683563113212585, 0.3027516305446625, -0.6533726453781128, -0.8497981429100037, 0.6699197292327881, 0.863176703453064, -0.5077152848243713, 0.12470079958438873, 0.5547656416893005, 0.014779430814087391, -0.10357555747032166, -0.7501516938209534, 0.043644826859235764, -0.10107008367776871, 0.038319818675518036, 0.18738126754760742, -0.4955521821975708, 0.2922472655773163, -1.1534987688064575, 0.9781992435455322, -0.11633932590484619, -0.47710785269737244, 0.37586888670921326, -0.753015398979187, -0.27352574467658997, 0.3866961598396301, -1.0289270877838135, -0.15859156847000122, -0.916944146156311, 0.3128233253955841, -0.1010139137506485, 0.204402893781662, 0.36062294244766235, 0.6478291749954224, 0.022800283506512642, 0.08078064024448395, 0.31784293055534363, 0.43353384733200073, -0.4097180664539337, 0.6475664973258972, -0.56718909740448, 0.09236925840377808, 0.05646005645394325, 0.24898284673690796, -0.18147391080856323, -0.2278779298067093, -1.0160009860992432, -0.19323238730430603, -0.4836951196193695, -0.2931153178215027, -0.1665150374174118, -0.27209165692329407, -0.6220542788505554, -0.7647691369056702, 0.0498775914311409, -0.8677785396575928, -0.33808836340904236, 0.4764518439769745, 0.03148967772722244, -0.22749316692352295, -0.9085490107536316, -1.602285385131836, -0.8752043843269348, -0.44801419973373413, -0.5715281367301941, 0.3169148564338684, -0.21389368176460266, -0.5117302536964417, -0.5692561864852905, -0.05950291454792023, -0.3632484972476959, 1.0053074359893799, -0.8755824565887451, 0.9933607578277588, -0.05907563120126724, -0.15643753111362457, -0.47134819626808167, 0.36568912863731384, 0.363094687461853, -0.5794976949691772, 0.6093351244926453, -0.8435608744621277, -0.25118201971054077, -0.6910544037818909, -0.47969380021095276, 0.12921945750713348, 0.06478262692689896, 0.6103808283805847, -0.3260948359966278, -0.466450959444046, 0.6254178881645203, 1.4375213384628296, -1.0755091905593872, -0.4335235059261322, -0.08218751102685928, 0.727804958820343, 0.2743731439113617, -0.5139891505241394, 0.8609240055084229, 0.1332891434431076, 0.31377023458480835, -0.2658528983592987, 0.04802066087722778, -0.15192022919654846, -0.852397084236145, 0.22827363014221191, 1.7947486639022827, 0.5995303988456726, -0.49479517340660095, -0.9770839810371399, 0.48228710889816284, -1.0555551052093506, -0.49266713857650757, 0.6678502559661865, 0.709805965423584, 0.5040364861488342, -0.4117637872695923, -0.028539789840579033, -0.18525253236293793, 0.3392622768878937, 0.1694190949201584, -0.5534545183181763, -0.6570066809654236, 0.05232396349310875, 0.32195308804512024, 0.18238802254199982, 0.7116875648498535, -0.3160327076911926, 0.8571010231971741, 15.003717422485352, 1.012012004852295, -0.0894409790635109, 0.8804018497467041, 0.5285831093788147, -0.41163837909698486, -0.5677382946014404, -0.4060618579387665, -1.0733275413513184, 0.25129979848861694, 1.1866298913955688, 0.012443562038242817, 1.2539201974868774, 0.0805448368191719, 0.05043434351682663, 0.08367173373699188, -0.15388935804367065, 0.6940752267837524, 0.43061065673828125, -1.2011280059814453, 0.21535369753837585, 0.09074631333351135, 0.5242352485656738, 0.5948534607887268, 0.8144251704216003, 0.7952039837837219, 0.40844422578811646, -0.47645822167396545, 0.6469387412071228, 0.01425106544047594, 1.208418607711792, -0.040035124868154526, -0.0019571564625948668, 0.7728646993637085, -0.6834710240364075, -0.037137698382139206, -0.45360609889030457, -1.4376368522644043, 0.34022119641304016, 0.4335448145866394, -0.5957527160644531, -0.6199595928192139, -0.42757099866867065, 0.31797054409980774, 0.2176588475704193, 0.07599443197250366, 0.11896221339702606, 0.9236399531364441, -0.574273407459259, -0.40242353081703186, 0.11864553391933441, 0.5633028149604797, 0.03768528997898102, 0.15910714864730835, -0.161581888794899, -0.11172204464673996, 0.26486557722091675, 0.21328584849834442, -0.7555977702140808, 0.14103485643863678, -0.2463957518339157, -0.41248396039009094, 0.4236278533935547, 0.6982967853546143, 0.3752378225326538, 0.2888394594192505, -0.3396865129470825, 0.41501057147979736, 0.9978848695755005, 0.31556200981140137, 0.027896899729967117, 0.54991614818573, 0.5348305106163025, -0.42565324902534485, -0.48610106110572815, -0.15524621307849884, -0.2961934804916382, -0.6596836447715759, -0.6056041717529297, -0.30045953392982483, 0.3732679486274719, -0.6287249326705933, -0.9288812279701233, 0.5609457492828369, -0.19010472297668457, -0.42975467443466187, 0.18208171427249908, -0.7500082850456238, 0.33516180515289307, 0.5782458782196045, -1.322664499282837, -0.6184053421020508, 0.6831273436546326, -0.5897096395492554, -0.39870598912239075, 0.00562432873994112, 1.3904895782470703, 0.1743125319480896, -0.32547876238822937, 0.3596285581588745, 0.3612368106842041, -0.23640353977680206, -0.3847191035747528, -0.37070316076278687, 1.177237629890442, 0.3084031045436859, 0.12040609866380692, 0.7496985197067261, 0.05236111581325531, 0.07595818489789963, -0.8733097910881042, 0.16294875741004944, 0.703248143196106, -0.643217921257019, -0.31285837292671204, -0.9924139380455017, -0.7364556193351746, 0.18167336285114288, 0.25462013483047485, -0.29949304461479187, 0.6979791522026062, 0.4114402234554291, -0.744024395942688, 0.1273544728755951, -0.9000484347343445, 0.00017148743791040033, 0.8394277095794678, -0.8667377233505249, -0.006970707792788744, 0.31384140253067017, 0.33017611503601074, -1.1146650314331055, -0.24479536712169647, -0.5251709818840027, 0.1636124849319458, 0.2592483162879944, 0.5781100988388062, -0.7556785941123962, 0.5431204438209534, 1.2740192413330078, -0.34538623690605164, -0.9402623772621155, 0.24438147246837616, -0.9852303862571716, 0.03631219267845154, 0.10048262774944305, 0.4006119966506958, -0.4095802903175354, 0.08633418381214142, 0.9008170366287231, 0.40352287888526917, -0.6133055686950684, -0.8104188442230225, -0.6003165245056152, 0.3184897303581238, -0.5750104188919067, 0.5351313948631287, 0.0797000601887703, 0.03674750775098801, -0.18460869789123535, 0.3827402889728546, 0.3556416630744934, -0.40442270040512085, -0.875105619430542, 0.21975386142730713, 0.03395916521549225, -0.0950428918004036, -0.5431528687477112, -0.1591189056634903, -1.458192229270935, 0.3407323956489563, -1.3356579542160034, -0.1921081691980362, -0.3553272485733032, -0.34825482964515686, -0.41587480902671814, 0.266270250082016, -0.08229931443929672, 0.46338438987731934, -0.2868362069129944, -0.34042447805404663, -0.2767249047756195, -0.07791796326637268, 0.7445171475410461, 0.5784919261932373, -0.18128591775894165, 0.2003767490386963, -0.25192791223526, 0.5672362446784973, 0.2062956690788269, 0.5925619602203369, -0.09569748491048813, -0.731809675693512, -1.5497372150421143, 0.845264196395874, -0.26686209440231323, -0.14083682000637054, -0.3649260699748993, 0.32928013801574707, -0.05290720611810684, -0.11060269922018051, 0.2675239145755768, 0.26025810837745667, -0.7317541837692261, -0.6061365604400635, 0.3713572025299072, -0.8012765049934387, 0.27803412079811096, 0.4295710325241089, -0.42795565724372864, 0.26504647731781006, 0.5899916291236877, -0.28949490189552307, -0.711078405380249, -0.6012831926345825, 0.5845860838890076, -0.6275479197502136, 0.5116041898727417, -0.4010501503944397, 0.1407167911529541, -1.0583850145339966, -0.4375635087490082, 0.19111116230487823, 0.2657712399959564, -0.2106635421514511, 1.1566483974456787, -0.024047907441854477, -1.102571964263916, 0.07892819494009018, 0.4864627420902252, -0.06076556071639061, -0.2467743307352066, 0.5760529041290283, -0.23973940312862396, -0.5738438963890076, 0.4890909492969513, 0.4668949246406555, 0.36641165614128113, -0.8978467583656311, 0.0022940884809941053, 0.5697678327560425, -0.0742793083190918, -0.006761775817722082, 1.294223666191101, -0.8799515962600708, -1.1290345191955566, 0.0375097393989563, -1.1780766248703003, 0.2150462418794632, -0.3155205249786377, 0.0918721929192543, 0.2464282214641571, 0.1322193145751953, 0.198478564620018, -0.5723218321800232, 0.059317104518413544, 0.05628621578216553, -0.603674590587616, 0.29992222785949707, -0.31120163202285767, -0.07950853556394577, 0.6600657105445862, 1.1535298824310303, -0.4428936243057251, -0.8023415207862854, -0.6917895078659058, -0.5525496602058411, -0.23482325673103333, 0.22015342116355896, -0.5414459109306335, -0.3258093595504761, 0.5210629105567932, 0.402485191822052, 0.5285577178001404, -0.28425365686416626, -0.291502445936203, 0.2749314606189728, 0.4229990243911743, 0.5584880709648132, -0.9871548414230347, -0.5971866250038147, 1.377392292022705, 1.1612931489944458, -0.8385621309280396, 0.5212700366973877, -0.44552335143089294, -0.4391297399997711, 0.4318045675754547, 0.2496558576822281, -0.015267422422766685, 1.1918768882751465, 0.014148807153105736, 0.17240524291992188, 0.25019127130508423, -0.8757079243659973, -0.05893908068537712, 0.8003556728363037, 0.5230097770690918, 0.964981734752655, 0.8620708584785461, -0.3922048807144165, 0.7569344639778137, -0.34289252758026123, 0.1963556408882141, 0.266486793756485, 0.4778823256492615, -0.07673521339893341, -0.09246759861707687, 0.19709829986095428, 0.8755518794059753, -0.5448819994926453, -0.796390175819397, 0.11926496773958206, 0.8047298192977905, 0.3174000382423401, 0.5287657380104065, 0.9578901529312134, 0.09367800503969193, 0.11189970374107361, 0.46189168095588684, 0.6946149468421936, -0.47474345564842224, -0.2444332391023636, -0.1750781536102295, -0.507994532585144, 0.23946505784988403, -0.12458652257919312, -0.4669345021247864, -0.43384039402008057, -0.7194856405258179, 0.23898054659366608, -0.3471823036670685, 0.5521273612976074, 1.160868525505066, 0.656255304813385, 0.07312977313995361, -0.09595490247011185, -0.44844770431518555, -0.807815670967102, -0.9432024955749512, -0.37906432151794434, -0.1181717962026596, -0.2555170953273773, -0.14089328050613403, -0.31753993034362793, -0.3652908504009247]}, "authors": [{"authorId": "2037383772", "name": "Niklas Muennighoff"}, {"authorId": "2531268", "name": "Alexander M. Rush"}, {"authorId": "1697211", "name": "B. Barak"}, {"authorId": "1379806208", "name": "Teven Le Scao"}, {"authorId": "120174856", "name": "Aleksandra Piktus"}, {"authorId": "2179884903", "name": "Nouamane Tazi"}, {"authorId": "1708916", "name": "Sampo Pyysalo"}, {"authorId": "145662441", "name": "Thomas Wolf"}, {"authorId": "2402716", "name": "Colin Raffel"}], "references": [{"paperId": "1a849f298dcd49fe99ebca6749c0564585aa3018", "title": "FinGPT: Large Generative Models for a Small Language"}, {"paperId": "8e817e7c898a6a52f0abd5acfb9de9e313b13ccf", "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "1567bcac0ab09269c9d0ff33c9a406132417fab9", "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "256d20b96fa0ec65a373bfe64f128eb56b4ea508", "title": "Instruction Tuned Models are Quick Learners"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "deb8f26509ae320fc975b32922416cb156c61bbd", "title": "Emergent and Predictable Memorization in Large Language Models"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "f9338e1b32cb450d95094ce71957375754823e4b", "title": "Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "3a5d6299b1cebf09af878c52320a0ff166f6372f", "title": "Measuring The Impact Of Programming Language Distribution"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "468992bf970c37bd1fef58b78a6c2fcd8c018868", "title": "Scaling Laws for Generative Mixed-Modal Language Models"}, {"paperId": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a", "title": "SantaCoder: don't reach for the stars!"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "34c2939d3147946b2ac218e7857e1bc4c8902679", "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting"}, {"paperId": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e", "title": "Training Trajectories of Language Models Across Scales"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "6cf65eb8aa66116e14a97bb8f71552359ff814ba", "title": "Will we run out of data? Limits of LLM scaling based on human-generated data"}, {"paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5", "title": "Transcending Scaling Laws with 0.1% Extra Compute"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "88a74e972898de887ad9587d4c87c3a9f03f1dc5", "title": "MTEB: Massive Text Embedding Benchmark"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "d219d971b16c708b8debc731e0c541ae218c7caf", "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese"}, {"paperId": "fb9f9e98d35340875905730e1a80221fec818944", "title": "Revisiting Neural Scaling Laws in Language and Vision"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "3a096a3d67348640084c209abb09f79447fbbc11", "title": "Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset"}, {"paperId": "45122c8f76a4e2fd0163d1f0522db37e97ea4721", "title": "Beyond neural scaling laws: beating power law scaling via data pruning"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536", "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "cf8235e0b592f52848c3dc4a9b76222c25d172cb", "title": "SGPT: GPT Sentence Embeddings for Semantic Search"}, {"paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d", "title": "Quantifying Memorization Across Neural Language Models"}, {"paperId": "55c36748f2a7c060c3313349c730b053ed03fbf7", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models"}, {"paperId": "37ddb9305c8c9120c21a2fae5a851ce8e4384a9c", "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture"}, {"paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca", "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "de1fdaf92488f2f33ddc0272628c8543778d0da9", "title": "Scaling Laws for Neural Machine Translation"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "1838cbd2eee0a555ab7e850eff1fce69d62acb95", "title": "MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b", "title": "Explaining neural scaling laws"}, {"paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb", "title": "Scaling Laws for Transfer"}, {"paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "54e4f2ef7410de9f94683cc570cb82257d27c0ff", "title": "Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "f00f2d4b8ddd55aa2cc202f44053e5f97a254175", "title": "WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "528dd0da358b4939d99eeb92548deccfeac48bd6", "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "c5f7074a264356c9a022a8dff24df79d1db8c3d3", "title": "ProGen: Language Modeling for Protein Generation"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "477d66dcd2c08243dcc69822d6da7ec06393773a", "title": "Multilingual is not enough: BERT for Finnish"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "39e801ca0dbc69c3697f118e24dac964abb63d4a", "title": "The CommitmentBank: Investigating projection in naturally occurring discourse"}, {"paperId": "92343cecdc990380de362b969eec60081959f507", "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"}, {"paperId": "572b6fe8d75032c159896c644319dacdf594e9c6", "title": "One Epoch Is All You Need"}, {"paperId": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328", "title": "Choosing Transfer Languages for Cross-Lingual Learning"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "fedbcfe03e44f4f9610e2b2164c5673516543f38", "title": "Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "9784fbf77295860b2e412137b86356d70b25e3c0", "title": "The Natural Language Decathlon: Multitask Learning as Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "97394554eb5a74c3160c6bd743fcd3e4bd6cbe28", "title": "LSDSem 2017 Shared Task: The Story Cloze Test"}, {"paperId": "32aa2517b03c871c11e521c2a3406f457833e2c3", "title": "Summarizing Source Code using a Neural Attention Model"}, {"paperId": "3e945f20718290d4639a23917d3385b0f710a593", "title": "Suggesting accurate method and class names"}, {"paperId": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b", "title": "KenLM: Faster and Smaller Language Model Queries"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": null, "title": "chinchilla\u2019s wild implications"}, {"paperId": "339b5d3316d13062d936b335aab06e9da48a5c17", "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers"}, {"paperId": "a6b6eedb0559cb44bd3fcab64151529019bae42a", "title": "Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1", "title": "Few-shot Learning with Multilingual Language Models"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "McMillan-Major, and Shmargaret Shmitchell"}, {"paperId": "cb64edcacf5bd6b1c535491454a60106e2bf1258", "title": "The Hateful Memes Challenge: Competition Report"}, {"paperId": "310b8117ae5ce3df8aa6304ad382525b9b46937e", "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+ 2020)"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "Alessandro Cappelli"}, {"paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0", "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"}, {"paperId": null, "title": "2022. Designing effective sparse expert models"}]}