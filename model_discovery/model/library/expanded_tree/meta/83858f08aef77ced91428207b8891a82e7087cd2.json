{"paperId": "83858f08aef77ced91428207b8891a82e7087cd2", "abstract": "In recent years, large-scale models can be easily scaled to trillions of parameters with sparsely activated mixture-of-experts (MoE), which significantly improves the model quality while only requiring a sub-linear increase in computational costs. However, MoE layers require the input data to be dynamically routed to a particular GPU for computing during distributed training. The highly dynamic property of data routing and high communication costs in MoE make the training system low scaling efficiency on GPU clusters. In this work, we propose an extensible and efficient MoE training system, ScheMoE, which is equipped with several features. 1) ScheMoE provides a generic scheduling framework that allows the communication and computation tasks in training MoE models to be scheduled in an optimal way. 2) ScheMoE integrates our proposed novel all-to-all collective which better utilizes intra- and inter-connect bandwidths. 3) ScheMoE supports easy extensions of customized all-to-all collectives and data compression approaches while enjoying our scheduling algorithm. Extensive experiments are conducted on a 32-GPU cluster and the results show that ScheMoE outperforms existing state-of-the-art MoE systems, Tutel and Faster-MoE, by 9%-30%.", "venue": "European Conference on Computer Systems", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes an extensible and efficient MoE training system, ScheMoE, which is equipped with several features and outperforms existing state-of-the-art MoE systems, Tutel and Faster-MoE, by 9%-30%."}, "embedding": {"model": "specter_v2", "vector": [0.3152712285518646, 0.1919536143541336, -0.5278922915458679, -0.062226489186286926, -0.25833699107170105, 0.60105299949646, -0.05985647812485695, -0.573597252368927, -0.5557394027709961, -0.46361231803894043, 0.2437591403722763, 0.5948534607887268, 0.23019497096538544, 0.36598584055900574, -0.20285658538341522, -0.0435921885073185, -1.2868480682373047, 0.4681951105594635, 0.5535617470741272, -0.21837842464447021, -0.4781813323497772, -0.3239096403121948, -1.1863126754760742, 0.5458621978759766, 0.7552211284637451, 0.9926384687423706, 0.005514516029506922, 0.9459021091461182, -0.11678564548492432, 0.06720041483640671, 0.5082032084465027, 0.1078658178448677, 0.35110047459602356, 0.3742643892765045, 0.4159587025642395, 0.3544973134994507, 0.36320120096206665, -0.29134300351142883, -0.8407106995582581, 0.5379648208618164, 0.334512859582901, 0.39536672830581665, 0.5028797388076782, -0.7818956971168518, 0.4258403778076172, 0.1829136162996292, 0.05783138796687126, 0.5556988716125488, -0.956115186214447, -0.7875049114227295, 0.5842515826225281, -1.3313995599746704, -0.01992090232670307, 1.1746011972427368, 0.6377047896385193, 0.49924415349960327, -0.38881245255470276, -0.5591628551483154, 0.5430927872657776, -0.21343235671520233, -0.43540307879447937, -0.5118997693061829, 0.04863686487078667, -0.4377294182777405, 1.6544424295425415, -0.41274964809417725, 0.11275828629732132, 0.7314245104789734, 0.2019864022731781, 1.3550103902816772, 0.04933852329850197, -0.5226157903671265, 0.1717778444290161, 0.23607027530670166, 0.31551823019981384, 0.7196317911148071, -0.23727960884571075, 0.22183765470981598, -1.3515926599502563, -0.3412308096885681, 0.4721970856189728, 0.15054748952388763, 0.4881439507007599, -0.26083192229270935, -0.10382013022899628, 0.7753718495368958, 0.5535751581192017, 0.5119510293006897, -0.4311107397079468, 0.6810181140899658, 0.8980815410614014, 0.21200110018253326, 0.4965594410896301, -0.18767336010932922, -0.035275284200906754, 0.6195189356803894, -1.0146547555923462, -0.0243111290037632, 0.05965948849916458, 0.8713085055351257, -0.3281349539756775, 0.2602732181549072, -0.3606061339378357, 0.6257203221321106, 1.161729335784912, -0.37648680806159973, 0.40021413564682007, -0.517836332321167, 0.44054391980171204, -0.6350106000900269, 0.08191271871328354, -0.6734600067138672, -0.26431339979171753, -0.4559997320175171, -1.6172946691513062, -0.917744517326355, -1.1928914785385132, -0.21759623289108276, -0.6702868938446045, 0.3175680935382843, -0.06599725782871246, 0.6964911222457886, 0.017633643001317978, 0.8189405202865601, 0.6350682377815247, 0.5627290606498718, 0.3635774850845337, 0.42923808097839355, 0.9475077390670776, -1.6671313047409058, -0.4128361940383911, -1.3273866176605225, -0.03963243588805199, -0.2701507806777954, -0.4903096556663513, -0.1740020364522934, -0.9358056783676147, -1.0821539163589478, -0.7801738977432251, 0.4351050555706024, -0.16861605644226074, 0.5802085399627686, 1.7567315101623535, 0.2256268709897995, -0.9826799035072327, 0.9746875166893005, -0.44303083419799805, 0.0050766970962285995, 0.3929458260536194, 0.6118424534797668, 0.28687402606010437, -0.1345273107290268, -1.2450284957885742, -0.10550954937934875, 0.3544359505176544, -0.4110569953918457, -0.315395712852478, -0.22752107679843903, -0.3220556080341339, -0.027561020106077194, 0.40611404180526733, -1.1818606853485107, 1.0634688138961792, -0.3010097146034241, -1.1427001953125, 0.3637080490589142, -0.08577985316514969, 0.11435451358556747, 0.34320273995399475, 0.07826416194438934, -0.7894697785377502, 0.007601715624332428, -0.6092971563339233, 0.8536693453788757, 0.7843306660652161, -0.0674796774983406, 0.030216114595532417, 0.16352349519729614, -0.6590059995651245, 0.13775227963924408, -0.5311287045478821, 0.9306643009185791, -0.6693835854530334, -0.40377047657966614, 0.6565940380096436, 0.47277358174324036, -0.7665784358978271, -0.006244034040719271, -0.2061711698770523, -0.6524115800857544, 0.7803651094436646, 0.26500508189201355, 0.7927306890487671, -0.7417354583740234, -0.7241623997688293, 0.10764189809560776, 0.1844286471605301, -0.15574081242084503, -0.6370745897293091, 0.5222653746604919, -0.37059128284454346, -0.23976275324821472, -0.03457849100232124, -1.232276439666748, 0.0337257906794548, -0.10868146270513535, -0.5470760464668274, -0.2966790199279785, 0.017555654048919678, 0.7089123129844666, -0.8315448760986328, 0.141328826546669, -0.4363642930984497, 0.3311143219470978, -1.046304702758789, 1.048021912574768, -0.4562239348888397, 0.3396562337875366, -0.27387717366218567, -0.08972109109163284, 0.2864524722099304, -0.48695892095565796, 0.5550821423530579, -0.443603515625, 0.467517226934433, 0.2064153105020523, -0.7412401437759399, 1.544208288192749, -0.30749672651290894, 0.35804763436317444, 0.3051685094833374, -0.5962051749229431, 0.03940852731466293, 1.2390081882476807, 0.6154038310050964, -0.005920914467424154, 0.8748216032981873, 0.6697359085083008, -0.42619064450263977, 0.07915367931127548, 0.8589116334915161, 1.0836354494094849, -0.2635720670223236, 0.08970910310745239, 0.6334751844406128, 0.00807728711515665, 0.32377487421035767, 0.572327196598053, 0.4438845217227936, 0.07880847901105881, -0.31074514985084534, -0.0565667487680912, -0.22339196503162384, -1.0970439910888672, -0.047633953392505646, 0.4997274577617645, 0.5966255068778992, 0.4092314541339874, 0.17613330483436584, -0.7000041604042053, -0.5218327045440674, 0.13512752950191498, 0.5743629932403564, 1.3871475458145142, -0.3015575706958771, 0.1135900467634201, -0.8833054900169373, -0.5366882085800171, -0.11800110340118408, -0.7430146336555481, -0.07079927623271942, 0.13862642645835876, -0.035664454102516174, -1.580310583114624, 0.4864911735057831, 0.19536548852920532, 0.8365830779075623, -0.571184515953064, -0.4821294844150543, -0.2636314332485199, 0.47789233922958374, -0.7389013767242432, -0.7431959509849548, 0.5739984512329102, -0.976384699344635, -0.04290955886244774, -0.2888478636741638, 0.11593949794769287, 0.5376836657524109, -0.29296550154685974, 0.7543509602546692, -0.8337773680686951, -0.5812953114509583, 0.16235218942165375, 0.6263799667358398, -0.3761017918586731, -0.23008815944194794, 0.13855916261672974, 0.059404365718364716, 0.20257039368152618, -0.02680271305143833, -0.3180937170982361, -0.05661075934767723, 0.23071467876434326, -0.2568729817867279, 0.5026172399520874, 0.3390161991119385, -0.3832146227359772, 0.9691739082336426, -0.2521445155143738, 0.3425537049770355, -1.4103221893310547, 0.913072943687439, -0.619239330291748, -0.07329253852367401, -0.15047994256019592, -0.09777025878429413, -0.6416199207305908, 0.42527031898498535, -1.0978789329528809, -0.19035282731056213, -0.6833531260490417, 0.2962215840816498, -0.909254252910614, -0.31776368618011475, -0.178110733628273, 0.5049138069152832, -0.4465217888355255, 0.6839435696601868, 0.07341153919696808, 0.0009430486825294793, -0.1346803903579712, 0.33773308992385864, -0.725389301776886, 0.4623488783836365, -0.15966391563415527, -0.1762140542268753, -0.09483575075864792, -0.15763656795024872, -0.47088178992271423, -0.020027587190270424, -0.9516608119010925, -0.2269943654537201, -0.5923055410385132, 0.029996948316693306, -0.8168346285820007, -0.7294631004333496, 0.036637984216213226, -1.0606646537780762, -0.15018603205680847, 0.6130638122558594, 0.19431659579277039, -0.08921290934085846, -1.2134054899215698, -1.507855772972107, -0.551093339920044, -1.1327095031738281, -1.3055427074432373, 0.4044990539550781, 0.18769434094429016, -0.12204640358686447, -0.2597842514514923, -0.330148845911026, -0.46365347504615784, 1.2659517526626587, -0.1918472945690155, 0.1493113785982132, 0.08183130621910095, -0.240864560008049, -0.4081898033618927, -0.1834830790758133, 0.16639038920402527, -0.7063242793083191, 0.11684427410364151, -0.9559992551803589, 0.07776262611150742, -0.3276483118534088, -0.47048279643058777, 0.696043074131012, 0.4555395841598511, 1.350796103477478, 0.3570837378501892, -0.7183505892753601, 0.5410763621330261, 1.3821656703948975, -0.5195615291595459, -0.166481152176857, -0.5773255228996277, 0.7610925436019897, 0.09791780263185501, -0.7250921726226807, 0.5853744745254517, 0.04257100820541382, 0.3416041135787964, 0.07377834618091583, -0.5541817545890808, 0.002529946155846119, 0.11902019381523132, 0.0232496690005064, 2.0739729404449463, 0.2720259130001068, -0.12910699844360352, -0.3096986711025238, 0.42848312854766846, -1.5776996612548828, -0.3870449662208557, 0.5188556909561157, 0.4480380713939667, -0.1362008899450302, -0.40383750200271606, 0.09542030841112137, -0.6707798838615417, 0.6976146697998047, 0.5534205436706543, -0.6257453560829163, -0.5445507764816284, 0.2728995978832245, 0.48379337787628174, 0.3446810841560364, 0.10390733927488327, -0.3744400143623352, 0.1423751711845398, 14.712568283081055, 1.0773744583129883, 0.036904476583004, 0.9952276349067688, 0.9853067398071289, 0.24987639486789703, -0.09941703081130981, -0.24566784501075745, -0.9413699507713318, 0.3416725695133209, 1.4853254556655884, 0.5352794528007507, 0.7153467535972595, 0.6609667539596558, -0.048671457916498184, -0.07651371508836746, -0.28797951340675354, 0.6590198874473572, 0.28285446763038635, -1.0676401853561401, -0.20580679178237915, -0.014748206362128258, 0.9262240529060364, 0.9218166470527649, 0.4197940528392792, 0.6135466694831848, 0.40580427646636963, -0.29254621267318726, -0.0171525776386261, 0.5443915724754333, 0.972837507724762, -0.16768543422222137, 0.29761767387390137, 0.5053332448005676, -1.0993478298187256, 0.3453427255153656, -0.6899574398994446, -0.624983549118042, 0.405412882566452, 0.4870360791683197, -0.11812811344861984, -0.08656967431306839, -0.08975784480571747, 0.7975045442581177, 0.4026683270931244, 0.8515000343322754, -0.012502975761890411, 0.23960024118423462, -0.21971680223941803, 0.2499791532754898, 0.08005913347005844, 0.24835093319416046, 0.10135668516159058, 0.0009218323975801468, 0.01703762635588646, -0.07028734683990479, 0.668563723564148, 0.1982700675725937, -0.731209933757782, -0.4756815731525421, 0.022360045462846756, -0.10914523899555206, -0.270408570766449, 1.1689049005508423, 0.1772599071264267, -0.11689387261867523, -0.5170916318893433, 0.5410391688346863, 0.4862445592880249, 0.23113581538200378, -0.4578886032104492, -0.2366398274898529, 0.05287478491663933, -0.6362591981887817, -0.4335757791996002, 0.5856450200080872, -0.34324875473976135, -0.7553476691246033, -0.7226938605308533, -0.3976285457611084, 0.22647015750408173, -0.6714197397232056, -1.118760347366333, 1.2996639013290405, -0.15305495262145996, -0.447684109210968, 0.7436391115188599, -0.5724066495895386, -0.736478328704834, 0.7029966711997986, -0.8179054856300354, -0.21248890459537506, 0.15332747995853424, -0.45091870427131653, -0.3435787856578827, -0.1858542263507843, 1.1032743453979492, 0.8618108034133911, -0.6973737478256226, -0.15104392170906067, -0.11596637964248657, -0.5020254850387573, 0.011740896850824356, -0.21699464321136475, 1.0644502639770508, 0.30735358595848083, -0.33615535497665405, -0.06259943544864655, -0.3600834012031555, 0.18947991728782654, -0.46473172307014465, -0.2164028137922287, 0.24705548584461212, 0.05870861932635307, -0.18854297697544098, -0.8105581402778625, -1.0652287006378174, 0.24956688284873962, 0.49077150225639343, 0.17243817448616028, 0.38163408637046814, 0.11265154182910919, -0.2084692418575287, -0.2674711346626282, -0.7298605442047119, -0.2501167058944702, 0.3199787139892578, -0.7218179106712341, 0.03983599692583084, 0.32321444153785706, 0.1230434700846672, -1.391856074333191, -0.6476999521255493, -0.1278611570596695, 0.18060702085494995, -0.3540019690990448, 1.1899783611297607, -0.0372743234038353, 0.47439080476760864, 0.8331040143966675, 0.05584048479795456, -1.0690319538116455, 0.3551796078681946, -0.7389733791351318, -0.041085775941610336, -0.3565814793109894, 0.21786728501319885, -0.4339804947376251, 0.5832617282867432, 0.5803591012954712, -0.16979806125164032, -0.8746376037597656, -0.4374716877937317, 0.1342417150735855, -0.8220610618591309, -0.48262926936149597, 0.2519979774951935, -0.003926367033272982, -0.2383461743593216, -0.28360262513160706, 0.08399344980716705, 0.6438008546829224, 0.2457766979932785, -0.8647480607032776, 0.3381912410259247, -0.22848078608512878, -0.6806926727294922, -0.5156155824661255, -0.16926342248916626, -1.47268545627594, -0.1978939175605774, -1.2969087362289429, 0.0749385729432106, -0.6397536396980286, -0.3232905864715576, -0.44512903690338135, -0.40348321199417114, -0.13637208938598633, 0.2965458333492279, -0.13256505131721497, -0.5508955121040344, -0.5180104374885559, -0.8521407246589661, 0.9503680467605591, 1.049550175666809, -0.33233964443206787, -0.16224199533462524, 0.22257113456726074, 0.5210813879966736, 0.4514390528202057, 0.5784395337104797, -0.5580050349235535, -0.6730237603187561, -1.2795000076293945, -0.12010470032691956, 0.3976486027240753, 0.13759753108024597, -1.3441967964172363, 1.1867005825042725, 0.5679922699928284, 0.0016886565135791898, -0.08610403537750244, 0.4208073019981384, -1.1573554277420044, 0.1602414846420288, 0.23244263231754303, -0.5019233822822571, -0.04603397101163864, 0.4151054620742798, -0.5031474232673645, -0.3216293156147003, 0.726227343082428, 0.02594563364982605, -0.7834208011627197, -0.905682384967804, 0.9133370518684387, -0.5517051815986633, -0.003953307401388884, -0.24205708503723145, 0.3247498869895935, -1.2056435346603394, 0.02159924991428852, -0.3379630744457245, 0.44965478777885437, -0.6512060165405273, 0.6550047397613525, 0.2743297815322876, -1.1653043031692505, 0.3124949336051941, 0.5679444074630737, -0.4642152488231659, 0.1324588656425476, 1.4976855516433716, 0.9362809062004089, -0.2620433568954468, 0.20773433148860931, -0.05396546795964241, 0.29939934611320496, -0.45620161294937134, -0.20667468011379242, 1.120683193206787, -0.47499847412109375, -0.07751765847206116, 1.258669137954712, -0.4060332477092743, -1.1269077062606812, 0.34653347730636597, -0.595284640789032, -0.2980285584926605, -0.33423787355422974, 0.5649994611740112, 0.665973424911499, 0.03506333753466606, 0.14507560431957245, -0.45769986510276794, 0.07710762321949005, 0.03070799633860588, -0.30453866720199585, 0.584860622882843, 0.059607308357954025, -0.31938958168029785, 0.40404731035232544, 0.33812958002090454, -0.7433010339736938, -1.126756191253662, -0.821111798286438, -0.8109017610549927, -0.05280214548110962, 0.4319838881492615, -0.45511728525161743, -1.2103824615478516, 0.894748866558075, 0.6519403457641602, 0.018591811880469322, 0.5109454989433289, -0.22855530679225922, 0.5365787744522095, -0.03909711912274361, 0.29347145557403564, -0.6032900810241699, -0.48546913266181946, 0.6741766333580017, 0.9224023818969727, -0.8039641380310059, 0.20545852184295654, -0.5580196380615234, -0.8643910884857178, 0.7550260424613953, 0.6991792321205139, -0.3763085901737213, 0.7898987531661987, -0.04726934805512428, -0.3244742751121521, 0.05296897515654564, -1.2238467931747437, -0.2939087152481079, 1.3822318315505981, 0.901174783706665, 0.3182505965232849, 0.5431451797485352, 0.027622349560260773, 1.0665231943130493, 0.5629501342773438, 0.1574275940656662, 0.17610694468021393, -0.05153602734208107, -0.15572375059127808, -0.09874650090932846, -0.008869897574186325, 0.9844000935554504, -0.5631774663925171, -0.30954569578170776, 0.5115982294082642, 0.29879480600357056, 0.5440008640289307, 0.6235119104385376, 0.8836625218391418, -0.1365242749452591, 0.4414776563644409, -0.35381874442100525, 0.32384562492370605, -0.5207076668739319, -0.35924142599105835, 0.5634894371032715, -0.7013274431228638, -0.24104337394237518, -0.24967506527900696, 0.31138378381729126, -0.10891426354646683, -0.5313888788223267, 0.5810134410858154, 0.3066892623901367, 0.5931787490844727, 0.6965746879577637, 0.9187223315238953, 1.3876307010650635, -0.04856079816818237, -1.311564564704895, -0.023623026907444, -0.8723607659339905, -0.4979814887046814, -0.6075170040130615, -0.6227124333381653, -0.13093116879463196, -0.13416440784931183, -0.7850872874259949]}, "authors": [{"authorId": "2268704", "name": "S. Shi"}, {"authorId": "2237187329", "name": "Xinglin Pan"}, {"authorId": "2183630484", "name": "Qiang Wang"}, {"authorId": "1992359", "name": "Chengjian Liu"}, {"authorId": "2297636539", "name": "Xiaozhe Ren"}, {"authorId": "1596797949", "name": "Zhongzhe Hu"}, {"authorId": "2297643520", "name": "Yu Yang"}, {"authorId": "2155882990", "name": "Bo Li"}, {"authorId": "2267041710", "name": "Xiaowen Chu"}], "references": [{"paperId": "a32476f93be0e8707cc1b99c2f506e60d61715a4", "title": "Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models"}, {"paperId": "479f22891a50a553ae46ca32e7bc7e195d9293fc", "title": "PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining"}, {"paperId": "dbbc5003af690799fa4fe6330fb795311cde106f", "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"}, {"paperId": "443c1bef6a7dc3db941375ae76451c884ceffb8a", "title": "A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training"}, {"paperId": "3822cb0a089a66f2ad88e7960fcae6ebdc4e4427", "title": "DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"}, {"paperId": "a34384389f74b7b2c31c696b0db0bf813e8bb301", "title": "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"}, {"paperId": "ff5eb1bd55d61ae919865f6b4dab84e6ae1974f3", "title": "Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression"}, {"paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632", "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"}, {"paperId": "6d088e8d785a57b50c2b0e465e2460e09ced48d7", "title": "Accelerating Distributed MoE Training and Inference with Lina"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "2e700ff36108119f5ed19a53bd2eaa22b42ec3d8", "title": "Tutel: Adaptive Mixture-of-Experts at Scale"}, {"paperId": "30d4380143a0e1de925a78017d0494c23b6304b3", "title": "Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers"}, {"paperId": "ffabceeeab1e6298b3633ce5a0117fe4f4fd0270", "title": "SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System"}, {"paperId": "7b9519b4488a672e5fca4d9f7282cd818d4981ca", "title": "Efficient Pipeline Planning for Expedited Distributed DNN Training"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "54b8bc5be8bbffae333dd73f2cb9d93a492d438e", "title": "HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System"}, {"paperId": "408efcc963ab871558e5e145b54ad7ae2aeb11c7", "title": "BaGuaLu: targeting brain scale pretrained models with over 37 million cores"}, {"paperId": "0dab58e476f3f0e6f580a295f7c4756c86f1f198", "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "0e23cc159fd2fb34550600d60dd9148c93636183", "title": "Taming Sparsely Activated Transformer with Stochastic Experts"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "41018bd551f07df54411a85a6edcd39c43647290", "title": "Exploiting Simultaneous Communications to Accelerate Data Parallel Distributed Deep Learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "488128bc81bb96ecdfdff8ca79fb793308d05285", "title": "Preemptive All-reduce Scheduling for Expediting Distributed DNN Training"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6bce64493906a645bb28d7f1f32f938504f617a1", "title": "Gradient Amplification: An efficient way to train deep neural networks"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "76c929af6735cdff2c4badc9a9c8f39d15ea3e70", "title": "A generic communication scheduler for distributed DNN training acceleration"}, {"paperId": "bfd6a7e5a26acbd2569fcb29595a46e3ef2f7755", "title": "MG-WFBP: Efficient Data Communication for Distributed Synchronous SGD Algorithms"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "7777d299e7b4217fc4b80234994b5a68b3031199", "title": "Fixed-Rate Compressed Floating-Point Arrays"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e", "title": "Large Scale Distributed Deep Networks"}, {"paperId": "7b9145efe64c5048a4b504bc28b789d777ce4f11", "title": "Efficient algorithms for all-to-all communications in multi-port message-passing systems"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "5c6a17850c9ad6bf6dc8992ec598cd932ce42208", "title": "SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization"}, {"paperId": "079506615d012e822fe8eebf03ea837f716149ac", "title": "Accelerating MPI All-to-All Communication with Online Compression on Modern GPU Clusters"}, {"paperId": "72567f3953c5479e09aacf48dfd888e38000b699", "title": "EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression"}, {"paperId": "502667269ea02bd3c35c47ad7c8674c9d8b8b5b0", "title": "AC-GC: Lossy Activation Compression with Guaranteed Convergence"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Onthediffi-culty of training recurrent neural networks"}, {"paperId": null, "title": "2022. Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning"}]}