{"paperId": "fb84816401a7254681c17901875b2bd3112397e4", "abstract": "Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale pre-trained models in downstream tasks by learning low-rank incremental matrices. Though LoRA and its variants effectively reduce the number of trainable parameters compared to full fine-tuning methods, they often overfit training data, resulting in sub-optimal generalization on test data. To address this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular value decomposition to parameterize low-rank incremental matrices and splits the training of pseudo singular vectors and values across two different subsets of training data. This division, embedded within separate levels of the BLO framework, mitigates the risk of overfitting to a single dataset. Tested on ten datasets covering natural language understanding and generation tasks and applied to various well-known large pre-trained models, BiLoRA significantly outperforms LoRA methods and other fine-tuning approaches, with similar amounts of trainable parameters.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "BiLoRA is introduced, an overfitting-alleviating fine-tuning approach based on bi-level optimization (BLO) that significantly outperforms LoRA methods and other fine-tuning approaches, with similar amounts of trainable parameters."}, "embedding": {"model": "specter_v2", "vector": [0.08510320633649826, 0.5010678768157959, -0.10693272948265076, -0.0228441059589386, -0.25799596309661865, -0.2509858012199402, 0.6498039364814758, -0.46912404894828796, -0.16872216761112213, -0.22015021741390228, 0.5927526950836182, 0.27364468574523926, 0.12017201632261276, 0.18831107020378113, -0.6848651170730591, 0.44278621673583984, -0.862076997756958, 0.48668691515922546, -0.30338332056999207, -0.8442012071609497, -0.41079509258270264, -0.9067879915237427, -0.742611825466156, 0.20750920474529266, 0.5520952939987183, 0.7272926568984985, 0.15023326873779297, 1.0901464223861694, -0.3214417099952698, -0.12743936479091644, 0.581220805644989, -0.525791347026825, 0.6608442068099976, -0.33226490020751953, -0.23941977322101593, -0.04936164990067482, 0.18036937713623047, -0.584496796131134, -0.19924159348011017, 0.7409551739692688, -0.34186920523643494, 0.17260555922985077, 0.5285198092460632, -0.27743610739707947, -0.48583588004112244, 0.7820546627044678, 0.6695958375930786, 0.4488316774368286, 0.0582301989197731, -0.5553181171417236, 1.373105764389038, -1.6131621599197388, 0.2256726175546646, 1.5591013431549072, 0.9197538495063782, 0.4913162887096405, -0.6488540172576904, -0.6383748054504395, 0.5754378437995911, -0.09880545735359192, -0.7115207314491272, -0.20274105668067932, -0.09709107130765915, -0.37960490584373474, 1.540850043296814, -0.4929318428039551, -0.303478866815567, 0.5647218823432922, -0.03480622544884682, 1.173429012298584, -0.20904570817947388, -0.6055208444595337, -0.43281349539756775, 0.025237856432795525, 0.10421666502952576, 0.7180765867233276, -0.25503185391426086, 0.350371778011322, -1.201664686203003, -0.11024012416601181, 0.4881112277507782, -0.6039137840270996, -0.23950959742069244, -0.18472442030906677, 0.0024522922467440367, 0.6074209213256836, 0.5586312413215637, 0.680047869682312, -0.224030002951622, 0.706151008605957, 0.5668566823005676, 0.6059379577636719, 0.08422452211380005, 0.6616666913032532, -0.48742854595184326, 0.33432695269584656, -0.46777674555778503, 0.05389497056603432, 0.1773509979248047, 0.6016587018966675, 0.03014053963124752, 0.08886867016553879, -0.7390933632850647, 0.6837352514266968, 1.2318594455718994, -0.6842902898788452, 0.8717573285102844, -0.4272320568561554, 0.4530639350414276, -0.7146024107933044, 0.07609561830759048, -0.8276058435440063, -0.260343462228775, -0.31175780296325684, -1.0011794567108154, -1.1667418479919434, -0.4409518539905548, 0.21381443738937378, -0.7792771458625793, 1.0529553890228271, -0.34724390506744385, 0.12159403413534164, 0.05140506476163864, 0.6156514883041382, 0.40516397356987, 0.7634314298629761, 0.013718991540372372, -0.03180772066116333, 0.48336416482925415, -0.976044237613678, -0.5148569941520691, -1.052380919456482, 0.5280224084854126, -0.21981212496757507, 0.6320512294769287, 0.029596151784062386, -1.1351277828216553, -1.1868396997451782, -1.014022946357727, 0.04524816572666168, -0.3187832832336426, 0.6499544382095337, 1.2155030965805054, 0.4805157482624054, -0.5326191186904907, 0.8335452675819397, -0.13326232135295868, -0.13795745372772217, 0.26262766122817993, 0.3164273202419281, -0.0039115045219659805, -0.4913299083709717, -1.5186947584152222, -0.06002156808972359, 0.5797204971313477, -0.15387186408042908, -0.230500727891922, -0.6176141500473022, -0.7299012541770935, -0.138769268989563, 0.21232818067073822, -0.7571266889572144, 1.1392847299575806, -0.27608659863471985, -1.707475185394287, 0.1613267958164215, 0.2366340160369873, 0.05547698214650154, 0.5180426239967346, -0.28684788942337036, -0.34656670689582825, -0.7742635607719421, -0.2693305015563965, 0.37033483386039734, 0.4424660801887512, -0.13586360216140747, 0.11145748198032379, 0.2902592420578003, -0.2983655333518982, 0.09420540183782578, -0.44757068157196045, 0.36810800433158875, -0.9692592024803162, -0.4102267622947693, 0.7155670523643494, 0.7114981412887573, 0.04045829176902771, -0.7014167308807373, -0.2907584309577942, -1.0195502042770386, 0.34779706597328186, -0.1630566269159317, 1.0128259658813477, -0.8365335464477539, -0.1651783138513565, -0.022489795461297035, -0.3445320129394531, -0.20164495706558228, -1.0380955934524536, 0.401044100522995, -0.3863328993320465, 0.6721076369285583, -0.0647539272904396, -1.0242077112197876, -0.058158453553915024, -0.5523004531860352, -0.6890034079551697, -0.35332295298576355, 0.30286601185798645, 1.2421146631240845, -0.9195451140403748, 0.6317012310028076, -0.2865382432937622, 0.10014235973358154, -1.1999279260635376, 1.159304141998291, -0.3836502730846405, 0.27521389722824097, -0.08641841262578964, -0.16803446412086487, 0.0860295221209526, -0.30766138434410095, 0.3120230734348297, -0.5936338901519775, 0.39476388692855835, 0.4923667907714844, -0.755464494228363, 1.722982406616211, -0.47501665353775024, 0.2744174003601074, -0.29016605019569397, -0.46525219082832336, 0.05712589621543884, 0.23641131818294525, -0.0787496268749237, -0.3388226628303528, 0.2229563593864441, 0.5753552913665771, -0.8607437014579773, 0.5033127665519714, 0.46695083379745483, 0.4020499587059021, -0.567113995552063, 0.01705564558506012, 0.9584199786186218, -0.4497900605201721, 0.8979310989379883, 0.3406331241130829, 0.5474684834480286, 0.49125567078590393, 0.5768095850944519, 0.30463436245918274, 0.6066390872001648, -0.71161288022995, 0.12599477171897888, 0.4233372211456299, 0.9634642601013184, 0.9541967511177063, 0.4403381645679474, -0.725983738899231, -0.28770920634269714, -0.32085129618644714, 0.7119976878166199, 1.7337602376937866, -0.2860049307346344, -0.28865715861320496, -0.5089929103851318, -0.23365186154842377, -0.37714365124702454, -0.042717963457107544, -0.49296048283576965, -0.3392561376094818, -0.2968975901603699, -1.5274487733840942, 0.5831567645072937, -0.0066793267615139484, 0.8880935907363892, -0.118107371032238, 0.408519983291626, 0.13185203075408936, 0.13494810461997986, -0.6447150707244873, -0.8784586787223816, 0.15938162803649902, -0.556672215461731, -0.19087107479572296, 0.13980908691883087, -0.12661106884479523, 0.08238265663385391, -0.9494168758392334, 1.0653506517410278, -0.6553716659545898, -0.3031269609928131, -0.03164680302143097, 0.29072999954223633, -0.33150896430015564, -0.9920617938041687, 0.21717457473278046, 0.5609194040298462, 0.05661320686340332, -0.04827907308936119, 0.5288246273994446, 0.4626012146472931, 0.33027586340904236, -0.5824012160301208, 0.38756364583969116, 0.1708032786846161, 0.5275037884712219, 0.9266639947891235, -0.26748546957969666, 0.2520778775215149, -1.502364158630371, 1.3196027278900146, 0.26132017374038696, -0.712973415851593, 0.3013138175010681, -0.6175205111503601, -0.27974608540534973, 0.8725380897521973, -0.7587676048278809, -0.33196231722831726, -0.8449936509132385, 0.31575149297714233, -0.12174380570650101, 0.07292348891496658, 0.48219847679138184, 0.2925427556037903, 0.12555836141109467, 0.8463218212127686, -0.019504515454173088, 0.03963469713926315, -0.4226899743080139, 0.7302957773208618, -0.8676396608352661, 0.7709617018699646, 0.2858908176422119, 0.5407326817512512, -0.4677174687385559, -0.40826424956321716, -0.33959999680519104, -0.6979809403419495, -0.3039817214012146, -0.38127028942108154, -0.15917514264583588, 0.0024863469880074263, -0.9003095030784607, -0.3145918548107147, -0.12566404044628143, -0.6375889182090759, -0.2092760056257248, 0.12659810483455658, -0.19860973954200745, -0.39333197474479675, -1.0895111560821533, -1.3663851022720337, -0.11903982609510422, -0.8432173728942871, -0.9900403022766113, 0.17135101556777954, 0.0789346843957901, -0.11137952655553818, -0.15860696136951447, 0.18527013063430786, -0.18682187795639038, 0.6494497060775757, -0.601891815662384, 0.6388808488845825, 0.16480526328086853, -0.13099534809589386, -0.3857843279838562, 0.19008484482765198, 1.0260059833526611, -0.2712063491344452, -0.08911661803722382, -0.6635451316833496, -0.11833465099334717, -0.28955042362213135, -0.6316299438476562, 0.1411849856376648, 0.18669568002223969, 0.3332427144050598, -0.002769198501482606, -0.171803280711174, 1.0153077840805054, 1.5978789329528809, -1.010042667388916, -0.19879946112632751, 0.30320584774017334, 1.1490284204483032, 0.40419936180114746, -0.1378009170293808, 0.4154531955718994, 0.2616558372974396, 0.4223718047142029, 0.016130486503243446, 0.0230360496789217, -0.4202210605144501, -0.9430196285247803, 0.661386251449585, 1.9935435056686401, 0.3554564416408539, 0.25602513551712036, -1.0320439338684082, 0.5469458699226379, -1.1594873666763306, -0.34690767526626587, 0.4296094477176666, 0.8862560391426086, 0.4959449768066406, -0.3022429943084717, -0.4886704385280609, -0.48429733514785767, 0.3073778748512268, -0.001462861429899931, -0.5686718225479126, -0.5981852412223816, -0.014476655051112175, -0.07583922892808914, 0.10394512116909027, 0.47122713923454285, -0.1715322732925415, 0.8839425444602966, 14.83092975616455, 0.8203898072242737, 0.3593329191207886, 0.9445684552192688, 0.9655053019523621, 0.10311805456876755, -0.3668220043182373, -0.21316614747047424, -1.021297812461853, -0.2461128532886505, 0.8861443996429443, 0.24268513917922974, 1.2231769561767578, 0.081325002014637, -0.035030659288167953, 0.45438551902770996, -0.6495381593704224, 0.883281946182251, 0.5529177784919739, -1.2764182090759277, 0.7085843086242676, 0.006514373701065779, 1.0035725831985474, 0.7221816778182983, 0.5510579943656921, 1.067530632019043, 0.1482393741607666, -0.28291311860084534, 0.19865824282169342, 0.5004104375839233, 1.0364606380462646, -0.07939186692237854, 0.47125041484832764, 0.34978997707366943, -0.8021305799484253, -0.6476165056228638, -0.7660464644432068, -0.9377638697624207, 0.3274781107902527, 0.11057216674089432, -0.4957689642906189, -0.3479916751384735, 0.1909397393465042, 0.7812510132789612, -0.2987862825393677, 0.18630743026733398, -0.14686664938926697, 0.5082241296768188, -0.29979583621025085, 0.604360044002533, 0.015616984106600285, -0.005527374800294638, 0.1755075305700302, -0.3651740550994873, 0.17550814151763916, -0.07258670032024384, 0.04325961321592331, 0.6237531304359436, -0.8820741772651672, 0.18898333609104156, 0.008120235987007618, -0.1409764438867569, -0.3486250042915344, 0.752048134803772, 0.8992483019828796, 0.3224261403083801, -0.39693769812583923, 0.50522381067276, 0.7552340626716614, 0.3835848569869995, -0.04078415408730507, 0.021883374080061913, 0.17047058045864105, -0.4338749945163727, -0.29684925079345703, 0.4594503343105316, -0.5164462924003601, -0.8082321286201477, -0.5654755234718323, -0.6628732085227966, 0.2173158973455429, -0.8047478199005127, -0.8683076500892639, 0.853196382522583, -0.4822860658168793, -0.5348349809646606, -0.11815976351499557, -0.7167328000068665, -0.005572642665356398, 0.5515822768211365, -1.0014368295669556, -0.5385366082191467, 0.28066685795783997, -0.7066768407821655, -0.181940957903862, -0.6557137966156006, 0.9618442058563232, 0.06263746321201324, -0.7791654467582703, 0.28853639960289, 0.4444487690925598, -0.26206275820732117, 0.10656142979860306, -0.5453269481658936, 0.9722516536712646, 0.5753408670425415, -0.12403538823127747, 0.31214213371276855, 0.03554259613156319, 0.24881207942962646, -0.8932114243507385, 0.11160502582788467, 0.4233502745628357, -0.7892768383026123, -0.17459319531917572, -0.797795295715332, -0.8076395392417908, 0.15540355443954468, 0.10253825783729553, -0.43173807859420776, 0.5017155408859253, 0.2709521949291229, -0.5837774276733398, -0.40723538398742676, -0.6341981887817383, -0.0059362249448895454, 0.32679352164268494, -0.664933979511261, -0.370334655046463, 0.19875329732894897, 0.35985442996025085, -0.983965277671814, -0.654094934463501, -0.029148003086447716, -0.009420243091881275, -0.0813383013010025, 1.2543634176254272, -0.34740501642227173, 0.6376482844352722, 0.5921071171760559, -0.0073997159488499165, -0.9897655844688416, -0.17883680760860443, -0.9263383746147156, 0.2329508364200592, 0.17488212883472443, 0.6947363018989563, -0.13085870444774628, 0.06228748336434364, 0.7952111959457397, 0.3687135577201843, -0.1970166265964508, -0.5047395825386047, -0.14396411180496216, -0.18762291967868805, -0.49247509241104126, 0.2182166874408722, -0.24641282856464386, -0.17145727574825287, 0.40974393486976624, 0.2130696177482605, 0.754469633102417, -0.32125651836395264, -0.8223708271980286, 0.6973536014556885, 0.04887542873620987, -0.6050912141799927, -0.5981477499008179, -0.09865874797105789, -1.541642189025879, -0.3656899631023407, -1.3153175115585327, 0.14117790758609772, -1.0891590118408203, -0.6008042693138123, 0.3859449326992035, -0.3276278078556061, -0.03043399751186371, 0.25467389822006226, 0.0783333033323288, -0.19880762696266174, -0.05933895707130432, -0.36316224932670593, 0.8367213010787964, 0.8073579668998718, -0.825039803981781, -0.6887035965919495, -0.025794662535190582, -0.02373332343995571, 0.37326040863990784, 0.3722286820411682, -0.19239556789398193, -0.7445015907287598, -1.3300952911376953, 0.8351289629936218, -0.13938438892364502, -0.2666877210140228, -0.8343021273612976, 0.5915936827659607, 0.525927722454071, 0.02769225463271141, 0.24972644448280334, 0.09381715208292007, -0.8177822232246399, -0.24714940786361694, 0.23462539911270142, -0.591371476650238, 0.06988953053951263, 0.24030493199825287, -0.3302234411239624, -0.5001945495605469, 0.48976972699165344, -0.16231930255889893, -0.8765882849693298, -0.5953304171562195, 0.3366261124610901, -0.34242236614227295, -0.0622825026512146, -0.6133326292037964, -0.03945069760084152, -0.7017879486083984, -0.42243656516075134, 0.038824278861284256, 0.4106578528881073, -0.428113728761673, 0.88177889585495, 0.363247275352478, -1.3656959533691406, -0.10649767518043518, 0.6341217160224915, 0.18309785425662994, 0.17140153050422668, 0.44369134306907654, 0.3904034197330475, -0.43487247824668884, 0.272833913564682, 0.3388307988643646, 0.40794041752815247, -0.23457053303718567, -0.28351879119873047, 0.8225764632225037, -0.7560999393463135, 0.2656545639038086, 1.4934877157211304, -0.17075636982917786, -1.5746192932128906, -0.05972469598054886, -0.7825875878334045, -0.4035152792930603, -0.2203345000743866, 0.5234834551811218, 0.165260910987854, 0.034184541553258896, -0.19570644199848175, -0.2750142216682434, 0.22903208434581757, -0.3005949854850769, -0.4355652928352356, 0.7860321998596191, -0.31027764081954956, -0.5585325956344604, 0.7132145166397095, 0.8794308304786682, -0.8445813059806824, -0.9173145890235901, -0.6768514513969421, -0.381949245929718, -0.3255082666873932, 0.3515966534614563, -0.6813256740570068, -0.5254887342453003, 0.25212275981903076, 0.605300784111023, -0.013515707105398178, 0.5736806392669678, -0.3040482699871063, -0.020121578127145767, 0.8159525394439697, 0.020224524661898613, -1.0206471681594849, -0.12333598732948303, 1.2483071088790894, 1.7075151205062866, -1.2107903957366943, 0.41539308428764343, -0.12552034854888916, -0.7497398257255554, 0.85786372423172, 0.3878115713596344, -0.02897442877292633, 0.6420775055885315, -0.7595496773719788, -0.12437302619218826, 0.5306816697120667, -0.9626155495643616, -0.17514120042324066, 1.5174258947372437, 0.8088912963867188, 0.859385073184967, 0.08035562932491302, -0.28821828961372375, 1.0918858051300049, -0.17643684148788452, 0.02932623028755188, 0.7802755236625671, -0.32964959740638733, 0.09097632020711899, -0.30222487449645996, -0.3023633658885956, 0.9061409831047058, -0.4953864812850952, -0.43352147936820984, 0.06287092715501785, 0.24220487475395203, 0.29678645730018616, 0.6073120832443237, 0.3958891034126282, -0.2169601172208786, 0.8913649320602417, 0.033104054629802704, 0.056088678538799286, -0.5796000957489014, -0.29102012515068054, 0.21290811896324158, -0.711051344871521, -0.4320928156375885, 0.2192045897245407, -0.4975830018520355, -0.20102538168430328, 0.27198031544685364, 0.022661851719021797, -0.30001527070999146, 0.6907046437263489, 0.8380338549613953, 0.43941348791122437, 0.5002270340919495, -0.18897196650505066, -0.5869230031967163, -0.6223174333572388, -1.2693504095077515, 0.2533671259880066, -0.1278630495071411, -0.4460046887397766, 0.014782095327973366, -0.10728469491004944, -0.1983994096517563]}, "authors": [{"authorId": "2291135314", "name": "Rushi Qiang"}, {"authorId": "2238957617", "name": "Ruiyi Zhang"}, {"authorId": "2290906338", "name": "Pengtao Xie"}], "references": [{"paperId": "3f459219d75de63b5b7a26a8c6447ec1e79a985c", "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "76b19363b10d7ea783e4a6494eae40d73c8e9628", "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models"}, {"paperId": "85e959eef45114974c8f8643e88af23936fff3d1", "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"}, {"paperId": "5be4f2074e4811fec280248e10e2f7b2faee54dd", "title": "Betty: An Automatic Differentiation Library for Multilevel Optimization"}, {"paperId": "ad471be93216ddbf8544721d50ee5aed14f07cae", "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning"}, {"paperId": "17f6f7c4973a7823364e17d501135c8f0673cab7", "title": "iDARTS: Differentiable Architecture Search with Stochastic Implicit Gradients"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "325488b29a33ba8e3f061dda9aed7b2df52f0447", "title": "A Game Theoretic Framework for Model Based Reinforcement Learning"}, {"paperId": "1187c70c4011f935642084e84186284ac0add3d0", "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "a84d359922a69916a05de7c91204b79d02c36cda", "title": "Optimizing Millions of Hyperparameters by Implicit Differentiation"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a2933c8d0e152264d1cd25ca8248b25d4b49038b", "title": "Meta-Learning with Implicit Gradients"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "99cd0211a8154383fae3498764a28fca790303b0", "title": "A new hyperparameters optimization method for convolutional neural networks"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d55d1d035e91220335edff0fe8f5d249d8c4a00b", "title": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "fda8159f997a07aec05cc2f6d3ac6c72c94988ba", "title": "A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "ecc76c03d6a3ae4233097ef8bcc9d04d8b3c9bec", "title": "Forward and Reverse Gradient-Based Hyperparameter Optimization"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "904ab6edd082c70e18a657cb4fadacafcd094ca3", "title": "Reverse-mode AD in a functional framework: Lambda the ultimate backpropagator"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": null, "title": "Low-rank adaptation of large language models"}, {"paperId": null, "title": "A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation"}]}