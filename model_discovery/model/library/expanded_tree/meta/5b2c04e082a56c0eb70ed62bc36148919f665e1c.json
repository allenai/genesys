{"paperId": "5b2c04e082a56c0eb70ed62bc36148919f665e1c", "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes SampleAttention, an adaptive structured and near-lossless sparse attention, which can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times compared with FlashAttention."}, "embedding": {"model": "specter_v2", "vector": [0.3412535786628723, 0.5075192451477051, -0.885416567325592, -0.27709588408470154, -0.4680667519569397, 0.08960825949907303, 0.7308328151702881, 0.1996242254972458, -0.47825881838798523, -0.2584998309612274, 0.41234782338142395, 0.22866737842559814, 0.8528976440429688, 0.4033290147781372, -0.17099030315876007, 0.48369017243385315, -1.2543085813522339, 0.1797834187746048, 0.20527078211307526, -0.16252762079238892, 0.14697785675525665, -0.9823760390281677, -1.0799387693405151, 0.46188804507255554, 0.2866811454296112, 0.6652756929397583, 0.7377437949180603, 0.8376304507255554, -0.664524495601654, 0.49479302763938904, 0.31909164786338806, 0.06816542148590088, 0.06157035008072853, -0.0385514535009861, -0.32922571897506714, -0.4477141499519348, 0.4110112190246582, -0.2910842299461365, -0.2489951252937317, 0.6452641487121582, -0.03420256823301315, 0.4936548173427582, 0.2072814553976059, -0.4175085425376892, -0.2727625370025635, 1.0964972972869873, 0.315815269947052, 0.6727288961410522, -0.030559886246919632, -0.59080570936203, 1.548302173614502, -1.8314627408981323, 0.3360763192176819, 1.836836576461792, 0.13921770453453064, 0.4905444383621216, -0.2276950478553772, -0.578045666217804, 1.6821565628051758, 0.34005334973335266, -0.7644243240356445, -0.625646710395813, -0.255934476852417, 0.3048454821109772, 2.236063003540039, -0.12949468195438385, 0.29133298993110657, 0.67219477891922, -0.20247086882591248, 2.091313362121582, -0.49327078461647034, -1.1373850107192993, -0.17380335927009583, -0.09789794683456421, 0.37226077914237976, 0.5390902757644653, -0.41339266300201416, 0.11114940792322159, -1.0035383701324463, -0.05049822852015495, 0.11646579205989838, 0.14413601160049438, 0.3617669641971588, -0.001801595906727016, -0.22220857441425323, 0.7622461915016174, 0.22154702246189117, 1.1151753664016724, -0.1652069091796875, 0.708027184009552, 0.3860231339931488, -0.03569085896015167, 0.08539735525846481, 0.3425733745098114, -0.15400776267051697, 0.04538390785455704, -1.4247688055038452, 0.38120245933532715, -0.011949253268539906, 1.0639519691467285, -0.260766863822937, 0.38878098130226135, -0.9876459836959839, 0.40962493419647217, 1.3467825651168823, 0.4846964478492737, 0.5950541496276855, -0.541131854057312, 0.30003154277801514, -0.8078870177268982, 0.04329107701778412, -0.5587603449821472, -0.04673786833882332, -0.12791357934474945, -0.38842806220054626, -1.334227204322815, -0.5637869238853455, 0.2545042335987091, -0.5564318299293518, 0.8090070486068726, -0.060795024037361145, 0.2770156264305115, -0.4550163745880127, 0.2845551371574402, 0.46727707982063293, 0.6116806268692017, 0.2955732047557831, 0.08287791907787323, 0.8705669045448303, -1.0881097316741943, -0.5351859331130981, -1.6460095643997192, 0.6653861403465271, -0.28151318430900574, 0.5882817506790161, -0.09661006182432175, -1.09366774559021, -0.956818699836731, -0.718054473400116, -0.2089102417230606, -0.6531169414520264, 0.053515609353780746, 0.9750220775604248, 0.0586027055978775, -0.8118345737457275, 0.3734063506126404, -0.4007245600223541, 0.06906117498874664, 0.46138468384742737, -0.0658174455165863, 0.38108474016189575, -0.6430783867835999, -1.4573999643325806, 0.07138693332672119, -0.0525348000228405, -0.4452846944332123, -0.2179226279258728, -0.5843625068664551, -1.4870423078536987, -0.14675390720367432, 0.5796060562133789, 0.07906661927700043, 1.4308353662490845, -0.22713841497898102, -1.2253230810165405, 0.6658589243888855, -1.011989951133728, 0.1106644868850708, -0.06369102001190186, -0.5688921809196472, -0.5824650526046753, -0.4375764727592468, -0.05614442378282547, 0.7070636749267578, 0.6585682034492493, -0.05177639424800873, -0.4410938322544098, 0.1694236844778061, -0.4145177900791168, -0.23389127850532532, 0.04013344272971153, 0.9548903107643127, -0.6672061681747437, -0.19375179708003998, 0.08568769693374634, 0.8616434335708618, -0.15429216623306274, -0.2838228642940521, -0.3721495568752289, -1.2362138032913208, 0.8447698950767517, 0.03628018870949745, 1.2239375114440918, -0.8911508321762085, -1.0003653764724731, -0.015038007870316505, -0.22607609629631042, 0.08449586480855942, -0.9788511991500854, 0.5599413514137268, -0.10629946738481522, 0.10326126217842102, 0.03577275574207306, -1.322646975517273, 0.2806796431541443, -0.3523675203323364, -0.7096876502037048, -0.31648197770118713, 0.09071195125579834, 1.1218514442443848, -1.1486581563949585, -0.4340795576572418, 0.06341012567281723, 0.28098374605178833, -1.3218886852264404, 1.2104614973068237, -0.9223855137825012, -0.08394230902194977, -0.4103672504425049, 0.0370294451713562, -0.18973930180072784, -0.16826167702674866, 0.37506040930747986, -0.1359148770570755, -0.021552061662077904, 0.46934887766838074, -0.5133315324783325, 1.350214958190918, -0.584934651851654, 0.7003463506698608, 0.01555757224559784, -0.7466349601745605, 0.009166371077299118, 0.07304026931524277, -0.4079720079898834, -0.7712133526802063, -0.07419230788946152, 0.23321402072906494, -0.7262044548988342, 0.009103617630898952, 1.1432275772094727, 1.2192152738571167, -0.4022769331932068, -0.08874550461769104, 0.1428532898426056, -0.19430358707904816, 0.4000868499279022, 0.5447223782539368, 0.5317472219467163, 0.40880680084228516, 0.6129851341247559, 0.03638013079762459, 0.15765522420406342, -0.9982424974441528, -0.04438962787389755, 0.9341874718666077, 0.9030767679214478, 0.5526541471481323, 0.41786763072013855, -0.7538983225822449, -0.1173061728477478, 0.5207433700561523, 0.5931122303009033, 1.8296732902526855, -0.2484893947839737, -0.2076205313205719, -0.6168854832649231, -0.1936766356229782, -0.12656471133232117, 0.2948758602142334, -0.33086907863616943, 0.21049506962299347, -0.6382827162742615, -0.5014302730560303, 0.4602859914302826, 0.25953394174575806, 0.548331081867218, -1.0874353647232056, -0.1931309551000595, -0.01267082430422306, 0.14407789707183838, -1.0127217769622803, -0.8065025210380554, 0.31323471665382385, -0.29553118348121643, -0.06036028638482094, 0.4369644224643707, 0.005056377034634352, 0.14606161415576935, -0.752616822719574, 1.1867876052856445, -0.4402691721916199, -0.5974492430686951, 0.16432181000709534, 0.4273150563240051, -0.5110735297203064, -0.21095192432403564, 0.3507547676563263, 0.2050408124923706, -0.022469624876976013, 0.5703341364860535, 0.7389318943023682, -0.20105178654193878, -0.22562962770462036, -0.19400456547737122, 0.01872607320547104, 0.22743169963359833, -0.06568939983844757, 1.0515663623809814, -0.7743788361549377, -0.03389241546392441, -1.4538438320159912, 0.4103449881076813, -0.22197823226451874, -0.45928022265434265, 0.10047996789216995, -0.7031307816505432, -0.45797938108444214, 0.6106555461883545, -0.8965033888816833, -0.18430179357528687, -0.7055130004882812, 0.09629381448030472, -0.3459484875202179, -0.42618346214294434, 0.25071999430656433, 0.10247787833213806, 0.4752460718154907, -0.05100793018937111, 0.7117307186126709, -0.09621162712574005, -0.22991804778575897, 0.6990076899528503, -0.7212104201316833, 0.5066299438476562, -0.08696135878562927, -0.2851894795894623, -0.1583208292722702, -0.4697954058647156, -1.1110774278640747, -0.4307258427143097, -0.567688524723053, 0.02724277228116989, 0.014464045874774456, -0.025303682312369347, -0.5316972732543945, -0.8857769966125488, -0.2814214825630188, -1.0042170286178589, -0.8538508415222168, 0.22811926901340485, -0.1890116184949875, -0.034033115953207016, -0.8924219608306885, -1.0989317893981934, -0.7970606684684753, -0.6869298815727234, -0.7433305978775024, 0.654802680015564, -0.1068800687789917, -0.8489028215408325, -0.6155059337615967, -0.16752156615257263, -0.5949130058288574, 1.1332662105560303, -0.7133213877677917, 0.615905225276947, -0.27318933606147766, -0.4614078402519226, -0.6776744723320007, 0.3880760371685028, 0.09829920530319214, -0.2980403006076813, 0.1458452343940735, -0.9518985152244568, 0.020890619605779648, -0.5243159532546997, -0.15546251833438873, 0.17463639378547668, 0.5718514323234558, 0.8758284449577332, -0.28706836700439453, -0.8354389667510986, 0.1619729995727539, 1.2937839031219482, -0.6657838821411133, -0.14807726442813873, -0.10296569764614105, 0.8545346856117249, -0.03161792829632759, 0.16767171025276184, 0.8163610100746155, 0.4425872266292572, 0.6237252950668335, 0.3019076883792877, -0.06238648295402527, 0.09114286303520203, -0.6345822215080261, 0.8156930208206177, 1.469010591506958, 0.6082184910774231, 0.067914217710495, -0.7265530228614807, 0.8537503480911255, -1.2275077104568481, -0.8816611766815186, 0.7567083239555359, 0.9265776872634888, 0.4600260555744171, -0.28961917757987976, -0.27294936776161194, -0.5832047462463379, 0.2997332215309143, 0.627595841884613, -0.6010738611221313, -0.7708933353424072, 0.172002911567688, 0.5738422274589539, -0.14938540756702423, 0.9256951212882996, -0.23315763473510742, 1.1134912967681885, 14.369664192199707, 1.0620568990707397, -0.05594757944345474, 0.5143981575965881, 0.8776779174804688, -0.24506349861621857, -0.3568674325942993, -0.2782110571861267, -1.708479881286621, -0.029398348182439804, 1.1533596515655518, 0.4817783832550049, 0.7486478090286255, 0.48088350892066956, 0.43368688225746155, -0.05007818713784218, -0.9054359793663025, 0.6415408849716187, 0.5885236859321594, -1.0820063352584839, 0.2962327003479004, -0.1327812373638153, 0.1676032394170761, 0.6925227642059326, 0.7537643313407898, 1.0605233907699585, 0.6044968366622925, -0.5589816570281982, 0.716468334197998, 0.5416485667228699, 0.9496893286705017, 0.13616298139095306, 0.05501202121376991, 0.48751166462898254, -1.163840413093567, 0.012823062017560005, -0.6182451248168945, -1.263548731803894, 0.09588676691055298, 0.11828780919313431, -0.31554120779037476, -0.891221284866333, -0.33311527967453003, 0.7757570743560791, 0.11659186333417892, 0.2804005742073059, 0.009414339438080788, 0.8444317579269409, -0.07002338767051697, -0.347454309463501, 0.45215585827827454, 0.8928422927856445, 0.10083995759487152, 0.42393994331359863, -0.09071451425552368, -0.24514523148536682, 0.2838117778301239, 0.6170318126678467, -0.3205229640007019, 0.20408815145492554, -0.2852839231491089, -0.08856207877397537, 0.5401483178138733, 0.743937075138092, 0.6512394547462463, 0.0607697032392025, -0.6624058485031128, 0.2954247295856476, 0.6967341303825378, 0.3620643615722656, 0.06075944006443024, -0.07802718877792358, 0.4638757109642029, -0.6712464094161987, 0.3554394543170929, 0.7789322137832642, 0.13238276541233063, -0.4784056544303894, -0.6039692759513855, -0.5598946213722229, 0.40768468379974365, -0.5916026830673218, -0.619064211845398, 0.607597291469574, -0.24480991065502167, -0.1289377063512802, -0.48962900042533875, -0.5216027498245239, -0.42118239402770996, 0.48976656794548035, -1.364227056503296, -0.6513798236846924, 0.6571413278579712, -0.2492961436510086, 0.09436378628015518, 0.32976269721984863, 1.3527330160140991, 0.10060485452413559, -0.3022589683532715, 0.44516822695732117, 0.10117243230342865, -0.043621838092803955, -0.19269011914730072, -0.7961775064468384, 0.9479168653488159, 0.4639877676963806, -0.009881602600216866, 0.3774990439414978, -0.008266353979706764, 0.18102794885635376, -0.9324513077735901, -0.16217684745788574, 1.011539101600647, -0.9160177707672119, -0.646773636341095, -0.9992579221725464, -0.704473078250885, 0.49558964371681213, 0.5290723443031311, 0.166745126247406, 0.4152653217315674, 0.43332675099372864, -0.5455247163772583, -0.22305171191692352, -0.5382277965545654, 0.0989290326833725, 0.20409661531448364, -0.7710705995559692, -0.20701412856578827, -0.3467422127723694, 0.4832824468612671, -1.0541903972625732, -0.40391090512275696, -0.7450507283210754, 0.30495670437812805, 0.17328110337257385, 0.9678173065185547, -0.3604726493358612, 0.6017147898674011, 0.994888961315155, -0.28096747398376465, -0.7444465160369873, 0.04506155103445053, -0.8807599544525146, -0.5346327424049377, 0.16763736307621002, 0.7860945463180542, -0.18777959048748016, 0.2578616142272949, 0.8855618834495544, 0.4771411120891571, -0.7537031769752502, -0.7050448656082153, -0.2030126303434372, 0.07632821053266525, -0.7775896191596985, 0.555402398109436, 0.12952515482902527, 0.22176073491573334, 0.047507066279649734, 0.2800635099411011, 0.5552459359169006, -0.146635502576828, -0.6337472200393677, 0.4658818542957306, 0.24072569608688354, -0.27933770418167114, -0.5945250391960144, -0.2889004945755005, -1.658965826034546, -0.12912054359912872, -0.8696361184120178, 0.14317329227924347, -0.7902826070785522, -0.5609818696975708, -0.027641966938972473, -0.35541602969169617, 0.3547464907169342, 0.0019479133188724518, -0.5162924528121948, -0.29061248898506165, -0.7125658392906189, -1.0505863428115845, 0.505871593952179, 0.7574825286865234, -0.6687207818031311, 0.23192577064037323, 0.0479004792869091, 0.14296254515647888, 0.006331999786198139, 0.2376476526260376, -0.09831919521093369, -0.4999594986438751, -1.2433522939682007, 0.42986223101615906, 0.00445596082136035, -0.32340916991233826, -0.5535606145858765, 0.5525387525558472, 0.21782681345939636, 0.014113325625658035, -0.24488086998462677, 0.38912633061408997, -0.9134025573730469, -0.9934384822845459, 0.3292182981967926, -1.0118827819824219, 0.037028152495622635, 0.337775319814682, -0.5792310237884521, -0.47905299067497253, 0.641028642654419, -0.12666365504264832, -1.10258150100708, -1.0946495532989502, 0.7286115288734436, -0.5499811768531799, 0.5812645554542542, -0.7049772143363953, 0.0023283385671675205, -0.9472150206565857, -0.5649478435516357, 0.10732779651880264, 0.47874319553375244, -0.6129801869392395, 1.104045033454895, 0.43884897232055664, -1.106586217880249, 0.2302444577217102, 0.4801543354988098, 0.14336833357810974, 0.42619985342025757, 0.5112031698226929, 0.2681400775909424, 0.04730649292469025, 0.7032685279846191, 0.7992432713508606, 0.2243405133485794, -0.8992469906806946, 0.24540288746356964, 0.5214957594871521, -0.6017996668815613, -0.07884320616722107, 1.1765668392181396, -0.3687998354434967, -0.8100196719169617, 0.394181489944458, -1.6530802249908447, -0.4194961488246918, -0.2570861279964447, 1.0056573152542114, -0.08324805647134781, -0.17843034863471985, 0.010125180706381798, -0.6805149912834167, 0.11450715363025665, -0.29085615277290344, -0.34122782945632935, 0.5555374622344971, -0.18116727471351624, -0.4766656160354614, 0.6826688647270203, 1.0105011463165283, -0.33659985661506653, -0.7735876441001892, -0.9558676481246948, -0.20404578745365143, -0.13502003252506256, 0.3442254364490509, -0.35574883222579956, -0.510890781879425, 0.7126988172531128, 0.5305008292198181, 0.40713006258010864, -0.2561606466770172, -0.011007514782249928, 0.21055203676223755, 0.9632928371429443, 0.07113756239414215, -0.7846434712409973, -0.6382037997245789, 1.1564037799835205, 0.9727769494056702, -1.0590364933013916, -1.4994801858847495e-05, 0.17559191584587097, -0.4402310848236084, 0.43684282898902893, 0.42661479115486145, 0.07385427504777908, 0.8400075435638428, -0.2613166272640228, -0.06067848205566406, 0.20539923012256622, -1.2033075094223022, -0.14716854691505432, 0.8147944211959839, 0.8995134234428406, 1.0422759056091309, 0.40156805515289307, 0.28577253222465515, 0.8633314967155457, 0.2239769548177719, 0.016057193279266357, -0.09562812000513077, 0.40062835812568665, -0.4492725729942322, -0.06601371616125107, 0.12372545152902603, 0.8503314256668091, -0.8943610787391663, -1.1684612035751343, 0.5127371549606323, 0.5357959270477295, 0.03666207566857338, 0.6806414127349854, 0.9349671602249146, 0.5904911756515503, 0.2878587245941162, 0.22466687858104706, 0.41504690051078796, -0.7968081831932068, 0.09495855122804642, -0.25952109694480896, -0.7247713804244995, -0.5897714495658875, 0.0627993494272232, -0.5761165022850037, -0.4264850616455078, -0.0038174947258085012, 0.5071724057197571, -0.06565944850444794, 0.09937501698732376, 1.2001286745071411, 0.5287251472473145, 0.32341718673706055, -0.3296237289905548, -0.6814557909965515, -0.10505442321300507, -1.1399339437484741, -0.11755431443452835, -0.5867642760276794, 0.012225042097270489, 0.10456317663192749, 0.0453011617064476, -0.4204602539539337]}, "authors": [{"authorId": "2239063179", "name": "Qianchao Zhu"}, {"authorId": "2268410645", "name": "Jiangfei Duan"}, {"authorId": "2298978798", "name": "Chang Chen"}, {"authorId": "2308106764", "name": "Siran Liu"}, {"authorId": "2295286977", "name": "Xiuhong Li"}, {"authorId": "2307077651", "name": "Guanyu Feng"}, {"authorId": "2308241238", "name": "Xin Lv"}, {"authorId": "47709883", "name": "Huanqi Cao"}, {"authorId": "2075319597", "name": "Chuanfu Xiao"}, {"authorId": "2298585927", "name": "Xingcheng Zhang"}, {"authorId": "2269713956", "name": "Dahua Lin"}, {"authorId": "2298434151", "name": "Chao Yang"}], "references": [{"paperId": "d98c87a7d41ba41dc804f8cb1aea74787c02038f", "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models"}, {"paperId": "75b2ae5ee35611ecfbd3dc2c3d0799cfb4fd98e4", "title": "InternLM2 Technical Report"}, {"paperId": "817f5a9504e59a6afc83cc745dff758f59e0b0a4", "title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss"}, {"paperId": "713806165610c237f551a7b68e6b09b3ded75502", "title": "SparQ Attention: Bandwidth-Efficient LLM Inference"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "9529e50807f36acf3d2e4af994b5803c47e4746a", "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"}, {"paperId": "93e58491830abe1eb965ab37ec64fa97263f6048", "title": "HyperAttention: Long-context Attention in Near-Linear Time"}, {"paperId": "539fadfb615ef84c240f4741061c44eeda540091", "title": "Scaling Laws of RoPE-based Extrapolation"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "d203c764fb5dec2b053be667c8b06e516ea6ef10", "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d", "title": "BiFormer: Vision Transformer with Bi-Level Routing Attention"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "title": "Benchmarking Large Language Models for News Summarization"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "594a89505eb803280628198920e87cfc2bb82d94", "title": "MLPerf Inference Benchmark"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "ee34b5cc38241ed5eb39d08f2eea322469103471", "title": "\"PROOF\""}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length?"}, {"paperId": "924945da8bf88764e347ecdb7eb09df6b1aac43b", "title": "\u30c9\u30ea\u30ab\u30e0\u3068moonshot"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "consistently achieve high accuracy and low latency across diverse sequence lengths and scenarios"}, {"paperId": null, "title": "Needle in a haystack\u2013pressure testing"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Memorizing trans-formers"}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}]}