{"paperId": "ac608a4a6b19b3208e560eee5daadb3cc18638a2", "abstract": "Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.", "venue": "International Conference on Learning Representations", "year": 2023, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.04542", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity."}, "embedding": {"model": "specter_v2", "vector": [0.3747105598449707, 0.8567197918891907, -0.34365248680114746, 0.028885958716273308, -0.2690730690956116, 0.1612245738506317, 0.9144359230995178, -0.4899531602859497, -0.2951062321662903, -0.48808911442756653, 0.8016226887702942, 0.5845888257026672, 0.32594460248947144, 0.16738556325435638, -0.3366137742996216, 0.04517612233757973, -0.7346607446670532, -0.04722243919968605, 0.03729424253106117, -0.3118261396884918, 0.2942151725292206, -0.9772225618362427, -1.549163818359375, 0.0993875190615654, 0.01870959810912609, 1.1047855615615845, 0.5012527108192444, 0.9250726103782654, -0.6011343002319336, 0.34238871932029724, 0.06024264171719551, -0.2327582985162735, 0.42158299684524536, -0.3205067217350006, -0.32757216691970825, 0.016974054276943207, 0.5154118537902832, -0.1385706663131714, -0.783631443977356, 1.0313138961791992, -0.1633976846933365, 0.46316176652908325, 0.5914084911346436, -0.4220850169658661, -0.5133770704269409, 0.8144175410270691, 0.5234826803207397, 0.6605144739151001, -0.2522057294845581, -0.41054439544677734, 1.4548165798187256, -1.0311123132705688, 0.47868612408638, 1.6730605363845825, -0.13306932151317596, 0.5627226233482361, -0.3226924538612366, -0.4618784785270691, 1.3569810390472412, 0.45031043887138367, -0.8245719075202942, -0.326335608959198, 0.12227745354175568, -0.09286444634199142, 1.7549153566360474, -0.3085361123085022, 0.005731221754103899, 1.0279959440231323, 0.2594511806964874, 1.7657731771469116, -0.14674101769924164, -0.9385954737663269, -0.04743482545018196, 0.30086952447891235, 0.3894396126270294, 0.5799834132194519, -0.8492838740348816, 0.33375370502471924, -0.9888952374458313, 0.12865443527698517, 0.0537775494158268, -0.22167299687862396, -0.2747001647949219, 0.017546722665429115, -0.2641204595565796, 0.8494464755058289, 0.4949880540370941, 0.6350869536399841, -0.5172598361968994, 1.0131069421768188, 0.36961671710014343, 0.05899878218770027, -0.05563727393746376, 0.5539782047271729, -0.21025918424129486, 0.17885322868824005, -0.6186901926994324, -0.01054529007524252, -0.1400166153907776, 1.2449620962142944, -0.2366408109664917, 0.57514488697052, -1.3129087686538696, 0.16048452258110046, 1.2835414409637451, 0.1879887580871582, 0.7133517265319824, -0.5584180951118469, 0.15904036164283752, -0.7512635588645935, 0.21461941301822662, -1.069981336593628, -0.011324014514684677, -0.19376914203166962, -0.6731399297714233, -0.7710325717926025, -0.6247718930244446, 0.11298363655805588, -1.0495831966400146, 0.9923794865608215, -0.39352649450302124, -0.24437569081783295, -0.8177169561386108, 0.5137484669685364, 0.04263538494706154, 0.7153893113136292, 0.334212988615036, 0.16074411571025848, 0.8206654191017151, -0.7685911655426025, -0.645448625087738, -1.1084582805633545, -0.24362397193908691, -0.25553029775619507, 0.3622930943965912, -0.1598910391330719, -1.069810152053833, -1.259785771369934, -0.992516040802002, 0.22947482764720917, -0.47114041447639465, 0.29594725370407104, 0.998934805393219, 0.439154714345932, -0.6987981796264648, 0.6073874235153198, -0.40832188725471497, -0.0283809881657362, 0.5850442051887512, 0.4187018573284149, 0.39611855149269104, 0.09842582046985626, -0.980752170085907, 0.2181285172700882, 0.5551545023918152, -0.6012961864471436, -0.09951591491699219, -0.5059423446655273, -1.2348657846450806, 0.2768615484237671, 0.3791983127593994, -0.7046508193016052, 1.038192868232727, -0.7760019898414612, -1.4789694547653198, 0.3277777135372162, -0.6049728393554688, 0.4113071858882904, 0.04687771201133728, -0.2531929016113281, -0.3344149887561798, -0.4437725245952606, -0.3147231340408325, 0.6380982995033264, 0.8536174893379211, 0.0039042942225933075, -0.3327406942844391, 0.12286262214183807, -0.5040468573570251, -0.5005675554275513, -0.36539340019226074, 0.8203092217445374, -0.4829951524734497, -0.430323988199234, 0.29768332839012146, 0.5733869075775146, 0.13241517543792725, -0.5207737684249878, -0.5599592328071594, -1.1570422649383545, 0.44952940940856934, 0.3803442120552063, 0.8964422345161438, -0.7408876419067383, -0.4394589066505432, -0.052587512880563736, 0.014967009425163269, -0.11415451020002365, -0.9402273297309875, -0.14924289286136627, -0.45202863216400146, 0.5050802230834961, 0.19385957717895508, -0.99239182472229, 0.4637250602245331, -0.6066008806228638, -0.47828859090805054, 0.1046241745352745, 0.39057058095932007, 1.0807236433029175, -0.5281441807746887, 0.12681683897972107, 0.16223062574863434, 0.03704175725579262, -0.8656843304634094, 1.3216180801391602, -0.5971157550811768, 0.09870456904172897, -0.43819108605384827, -0.17797742784023285, 0.1646735966205597, -0.5446961522102356, -0.026902688667178154, -0.3213271498680115, 0.17644305527210236, 0.30328792333602905, -0.529268205165863, 1.0321763753890991, -0.2913854122161865, 0.9581205248832703, -0.19109585881233215, -0.6942210793495178, 0.2045711725950241, 0.03631582483649254, -0.09121250361204147, -0.5863685011863708, 0.7317953705787659, -0.27113980054855347, -0.8685737252235413, -0.02730626054108143, 0.7087122797966003, 1.3030422925949097, -0.47216275334358215, 0.03339635580778122, 0.792893648147583, 0.11647721379995346, 0.06256188452243805, 0.030549338087439537, 0.6441898345947266, 0.4128726124763489, 0.7764877676963806, 0.046218644827604294, 0.06537701189517975, -1.022309422492981, 0.0771040990948677, 1.0945440530776978, 0.5825412273406982, 1.17875337600708, 0.16784673929214478, -0.755142331123352, -0.29981499910354614, -0.07553804665803909, 0.43128761649131775, 1.7471214532852173, -0.3250298798084259, -0.1417529582977295, -0.45746415853500366, -0.2820326089859009, -0.22666241228580475, 0.06497741490602493, -1.0309959650039673, -0.22919464111328125, -0.32085713744163513, -1.251499056816101, 0.3191421627998352, 0.33181482553482056, 0.8090656995773315, -0.9001531004905701, -0.28405970335006714, -0.17185576260089874, 0.5308758616447449, -0.5116861462593079, -1.0372772216796875, 0.30021971464157104, 0.1504933387041092, 0.022393247112631798, -0.0359012670814991, 0.37967950105667114, 0.09137130528688431, -0.8710909485816956, 1.0119820833206177, -1.0451020002365112, -0.5005829334259033, 0.5771153569221497, 0.2835361957550049, -1.0093655586242676, -0.35559555888175964, 0.42070096731185913, -0.14051677286624908, 0.32009249925613403, 0.3451862335205078, 0.5745055079460144, -0.0766284391283989, 0.0032984253484755754, -0.19414642453193665, -0.316118448972702, 0.07369046658277512, 0.3251097798347473, 0.8438425064086914, -0.8139438033103943, 0.012916033156216145, -1.1070624589920044, 0.5582894682884216, -0.330509752035141, -0.7319086790084839, 0.22069789469242096, -0.8927313685417175, 0.06146708130836487, 0.12607572972774506, -0.8786066174507141, -0.581839382648468, -0.48369452357292175, 0.46804672479629517, -0.5153550505638123, -0.41870924830436707, -0.0396215096116066, 0.2330440729856491, 0.25815364718437195, 0.5652252435684204, 0.3840818703174591, 0.03551870957016945, 0.3983192443847656, 0.6109423041343689, -0.78896564245224, 0.5432311296463013, 0.34854015707969666, -0.1881020963191986, -0.21766343712806702, -0.21626028418540955, -0.7480263113975525, -0.47989800572395325, -0.4180804491043091, 0.008457666262984276, -0.4482368528842926, 0.4842919707298279, -0.30592837929725647, -1.3183574676513672, -0.24521556496620178, -1.0645238161087036, 0.0981089398264885, 0.08041343837976456, -0.02844160608947277, -0.3705105185508728, -1.3816338777542114, -0.5418559908866882, -0.6803573369979858, -0.5894111394882202, -0.9607635140419006, 0.36442598700523376, 0.5961558818817139, -0.5232388973236084, -0.35581687092781067, -0.3344360888004303, -0.6322783827781677, 1.3552578687667847, -0.36875060200691223, 0.4323825538158417, -0.1714470237493515, -0.6780188679695129, -0.577050507068634, 0.5108023285865784, 0.33382564783096313, -0.13651259243488312, 0.03087947703897953, -1.287071943283081, 0.5327509641647339, -0.15870438516139984, -0.19521093368530273, 0.5339424014091492, 0.8363558650016785, 1.0317617654800415, 0.014337464235723019, -0.5459438562393188, 0.680575966835022, 1.1641230583190918, -0.6490393280982971, -0.05389146879315376, 0.44477662444114685, 0.9971747398376465, 0.22329047322273254, -0.3904501497745514, 0.8850815296173096, 0.35625290870666504, 0.5859580039978027, 0.5093798637390137, 0.2255447655916214, -0.18454110622406006, -0.32305198907852173, 0.33228981494903564, 1.0077874660491943, 0.10515471547842026, -0.23796693980693817, -0.5818440318107605, 0.5691230893135071, -1.331323266029358, -0.9082520008087158, 0.581762433052063, 0.7396215200424194, -0.017375975847244263, -0.45445704460144043, -0.5215237736701965, -0.8313028216362, 0.493937224149704, 0.5042350888252258, -0.6901839971542358, -0.4193824529647827, -0.1385960578918457, 0.5161937475204468, 0.0764959380030632, 0.6204928159713745, -0.7599228620529175, 0.5137200355529785, 14.836318969726562, 0.4557753801345825, -0.28953248262405396, 0.5452450513839722, 0.5979716181755066, 0.005024326499551535, -0.2614046335220337, -0.35361337661743164, -1.3895350694656372, 0.18433621525764465, 1.1098414659500122, 0.3140586018562317, 0.6258806586265564, 0.37119489908218384, -0.21171218156814575, 0.22355182468891144, -0.4997783601284027, 0.7728016972541809, 0.6443831920623779, -1.2538126707077026, 0.10231293737888336, 0.08105584233999252, 0.6065050959587097, 0.9253948330879211, 0.8093918561935425, 0.8113731741905212, 0.6018946170806885, -0.25965940952301025, 0.6006935834884644, 0.7541874647140503, 0.5080252289772034, 0.005747659597545862, 0.09346552938222885, -0.02390766702592373, -0.8340457081794739, -0.3822696805000305, -0.6546159386634827, -0.5782250761985779, 0.06509064137935638, -0.12799139320850372, -0.18047283589839935, -0.5913557410240173, 0.025164145976305008, 0.44155946373939514, 0.03894977644085884, 0.27703791856765747, -0.11527499556541443, 0.5249813795089722, 0.08842871338129044, -0.31256625056266785, 0.10976220667362213, 0.8789681792259216, 0.39392462372779846, 0.1657424122095108, -0.1225789412856102, -0.12814772129058838, 0.05709877982735634, 0.9409312009811401, -0.43163618445396423, -0.07591821998357773, 0.10746213048696518, 0.1075533926486969, 0.07495132833719254, 0.5978865027427673, 0.7051827311515808, 0.3836824595928192, -0.3562729060649872, 0.23517169058322906, 0.29146209359169006, 0.26887255907058716, -0.09372730553150177, -0.27253592014312744, 0.4643484055995941, -0.728428065776825, 0.492655485868454, 0.8612862825393677, -0.14284133911132812, -0.6513432860374451, -0.47370561957359314, -0.1819581240415573, 0.5730863213539124, -0.9381899833679199, -0.9887845516204834, 0.8424295783042908, -0.021434493362903595, -0.19065600633621216, -0.2115068882703781, -0.8471883535385132, -0.24216346442699432, 0.8726358413696289, -1.0008147954940796, -0.5108408331871033, 0.028148436918854713, -0.31620290875434875, -0.003572455607354641, -0.40737980604171753, 1.2726470232009888, -0.3985922932624817, -0.10605346411466599, 0.5000276565551758, -0.028941303491592407, -0.16010257601737976, -0.08411315828561783, -0.8859267830848694, 0.8775590658187866, 0.16300863027572632, -0.22642208635807037, 0.15247584879398346, 0.09529771655797958, 0.40357705950737, -0.7323628664016724, 0.10533610731363297, 0.37973007559776306, -0.9417480826377869, -0.5396684408187866, -0.6332192420959473, -0.85920649766922, 0.20362044870853424, 0.6130759716033936, 0.10666248947381973, 0.1143389493227005, 0.47535765171051025, -0.4941730201244354, -0.16115215420722961, -0.3727636933326721, -0.12469637393951416, -0.12785455584526062, -0.4393715560436249, -0.43453478813171387, -0.37144044041633606, 0.14922626316547394, -0.8490598797798157, -0.28383147716522217, -0.4237539768218994, 0.12370060384273529, 0.05015674605965614, 1.090248942375183, -0.5900648236274719, 0.3528293967247009, 0.4876652956008911, -0.27542275190353394, -0.8945233225822449, -0.5757377743721008, -0.7576925158500671, 0.012416885234415531, 0.450736403465271, 0.4816644489765167, -0.2553645670413971, -0.018642403185367584, 0.7901478409767151, 0.20521417260169983, -0.15951912105083466, -0.6376895308494568, -0.07947473227977753, -0.2433369904756546, -0.7673094272613525, 0.23348653316497803, -0.02487294189631939, -0.2206612229347229, -0.0492415688931942, 0.2722760736942291, 0.6177850961685181, -0.22687973082065582, -0.6572787165641785, 0.26265913248062134, -0.1985367238521576, -0.3396182358264923, -1.1501235961914062, -0.7088323831558228, -1.477349877357483, 0.22585105895996094, -1.1228605508804321, 0.20301002264022827, -0.5728919506072998, -0.6518347859382629, 0.6195977926254272, -0.4480830430984497, 0.205146923661232, -0.12352427840232849, -0.6486755609512329, -0.11003807187080383, -0.6611083745956421, -0.44336602091789246, 1.0428727865219116, 0.7210268378257751, -1.0618377923965454, -0.060063108801841736, 0.4092105031013489, -0.26980555057525635, 0.15942135453224182, 0.4780026078224182, -0.6135233044624329, -0.45735713839530945, -0.997824490070343, 0.2456710934638977, -0.1998182237148285, -0.25851383805274963, -0.6649132370948792, 0.7752853631973267, 0.11730467528104782, 0.344066321849823, 0.21136902272701263, 0.384747177362442, -0.8220658302307129, -0.6662135124206543, 0.08049698919057846, -0.9500948190689087, -0.04844760149717331, 0.16848288476467133, -0.17508068680763245, -0.3287906348705292, 0.4879329204559326, 0.030331188812851906, -1.2344844341278076, -0.5973920226097107, 0.5309920907020569, -0.5729442238807678, 0.19806885719299316, -0.16702939569950104, -0.12972062826156616, -1.266566514968872, -0.34645017981529236, -0.09820768982172012, 0.33326372504234314, -0.887498140335083, 1.176384449005127, 0.28098064661026, -1.1385263204574585, -0.01991654559969902, 0.6177821159362793, 0.3048897683620453, -0.036623235791921616, 0.4450037479400635, 0.1478894203901291, 0.13931716978549957, 0.320527046918869, 0.19093309342861176, 0.021046780049800873, -0.6312240958213806, 0.42100998759269714, 0.8927021026611328, 0.18522746860980988, -0.23891067504882812, 1.0684655904769897, -0.010802723467350006, -0.6487380266189575, 0.5083431005477905, -0.8640995621681213, -0.5801466107368469, 0.12354796379804611, 0.9627990126609802, 0.035272110253572464, -0.2352742999792099, 0.04071337357163429, -0.3676776885986328, 0.338490754365921, -0.15460392832756042, -0.40056565403938293, 0.7114804983139038, -0.4076971113681793, -0.32714271545410156, 0.670668363571167, 0.9178659915924072, -0.7697561979293823, -0.9708567261695862, -0.7099740505218506, -0.23822683095932007, -0.3710937201976776, 0.5063126683235168, -0.22860878705978394, -0.9262042045593262, 0.5041570663452148, 0.936181366443634, 0.24494722485542297, 0.36085182428359985, 0.4411240518093109, -0.7380456924438477, 0.6876347661018372, -0.2711816430091858, -0.6834397315979004, -0.5120562314987183, 1.2214736938476562, 1.3134732246398926, -0.9923186898231506, 0.08988332748413086, 0.06878738850355148, -0.5189054608345032, 0.9628227949142456, 0.38384610414505005, -0.14861087501049042, 1.0230809450149536, -0.4236542880535126, -0.1169513687491417, 0.18643562495708466, -1.0365675687789917, -0.4125312268733978, 1.480239987373352, 1.085365891456604, 0.5176791548728943, 0.25040966272354126, 0.3904745280742645, 0.7975246906280518, 0.01905793510377407, -0.18126121163368225, 0.450157105922699, 0.1929742991924286, -0.3748660683631897, 0.20434041321277618, -0.17977318167686462, 0.7506102919578552, -1.021632432937622, -0.3627220392227173, 0.363722026348114, 0.46541184186935425, 0.1934312880039215, 0.5262911915779114, 0.9793003797531128, -0.09487920254468918, 0.6254797577857971, 0.1679992526769638, 0.5123670697212219, -0.34592685103416443, -0.17241442203521729, -0.21032443642616272, -0.9284893274307251, -0.43520450592041016, -0.14686891436576843, -0.67079097032547, 0.07039373368024826, 0.11142623424530029, 0.22243982553482056, -0.5783105492591858, 0.48685377836227417, 1.1709303855895996, 0.6012422442436218, 0.9920387864112854, 0.0570184662938118, -0.7312830090522766, -0.19346001744270325, -1.3193449974060059, 0.24260880053043365, -0.5462414622306824, 0.2096022069454193, -0.323296457529068, -0.3256397545337677, -0.05258743464946747]}, "authors": [{"authorId": "1633166807", "name": "Lin Zheng"}, {"authorId": "2118572346", "name": "Jianbo Yuan"}, {"authorId": "2146309022", "name": "Chong Wang"}, {"authorId": "47648549", "name": "Lingpeng Kong"}], "references": [{"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "29587ba6dca7e713c3abf6f2d1045e4a5bf10586", "title": "Treeformer: Dense Gradient Trees for Efficient Attention Computation"}, {"paperId": "ebd1619e5856084cfe60b40cc141a7f69d75b523", "title": "Multi Resolution Analysis (MRA) for Approximate Self-Attention"}, {"paperId": "dc1b905c0af4dc318b63cd52fbc867c788df4b8c", "title": "Chefs' Random Tables: Non-Trigonometric Random Features"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13f10195d2a24fea28c0ea57cbd393aa76256c26", "title": "What Dense Graph Do You Need for Self-Attention?"}, {"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1", "title": "UniFormer: Unifying Convolution and Self-Attention for Visual Recognition"}, {"paperId": "2d82ee05b132d4681c3bd517afc17d608fe6e525", "title": "Simple Local Attentions Remain Competitive for Long-Context Tasks"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "a25370452533bf47549243e97852b9cdf7a0ee0e", "title": "Learning the Transformer Kernel"}, {"paperId": "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0", "title": "Hybrid Random Features"}, {"paperId": "12640af46eaf4c16125557b517a2d37fca70a82d", "title": "Ripple Attention for Visual Perception with Sub-quadratic Complexity"}, {"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "title": "ABC: Attention with Bounded-memory Control"}, {"paperId": "bb363c8c5bc1c473f0801c647c88d0c071792858", "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "066fabf6e2be7ba993abafdc6ac4c6ef1d4f2ce4", "title": "Control Variates for Slate Off-Policy Evaluation"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "08ffdec40291a2ccb5f8a6cc048b01247fb34b96", "title": "Relative Positional Encoding for Transformers with Linear Complexity"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "13da774fe604027bff2951ba82f4c3d9be7e415e", "title": "Augment Your Batch: Improving Generalization Through Instance Repetition"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "1d87bbe528850257384acb47e8848bc48668ec96", "title": "Using Large Ensembles of Control Variates for Variational Inference"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "682d194235ba3b573889836ba118502e8b525728", "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation"}, {"paperId": "5e2441b1b535dd36def485474fdec9e51a6d90b6", "title": "Control variates for stochastic gradient MCMC"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "a456265138c088a894301c0433dae938705a9bec", "title": "Deep Sets"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "a642bbbaf8822565f9b812ea279c596cc54ce4c3", "title": "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "6a667700100e228cb30a5d884258a0db921603fe", "title": "Black Box Variational Inference"}, {"paperId": "5fb8a9271af105a5065a5a855e71a7d25c7a6f1b", "title": "Variance Reduction for Stochastic Gradient Optimization"}, {"paperId": "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3", "title": "Variational Bayesian Inference with Stochastic Search"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "1187a77f857ad029168863ba0005ddf6d2b957c8", "title": "Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning"}, {"paperId": "9aa5bb7c0cc2c64205395eafdb86b4e3514f2f6d", "title": "Weighted Average Importance Sampling and Defensive Mixture Distributions"}, {"paperId": "99e60024458330c77eab695c6de7b4ed6df157fd", "title": "Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference"}, {"paperId": null, "title": "2022b). Note that increasing C also leads to better translation quality, although we found the performance gain is slightly less effective than that of increasing |E|"}, {"paperId": "5a9bc55f6332e38f62eb509b684147a1d4f10fd9", "title": "Focal Attention for Long-Range Interactions in Vision Transformers"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c", "title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}, {"paperId": null, "title": "2021b) to re-evaluate all attention baselines and report"}, {"paperId": null, "title": "4we retain the repeated augmentation technique in training PVT to be consistent with the original training protocol"}, {"paperId": null, "title": "2021a), an approach that combines Performer and sparse attention. The details can be found in Appendix G. Here we implement the sparse module as a simple local attention to ensure a fair comparison"}, {"paperId": null, "title": "2021) is a lightweight benchmark that assesses the ability"}, {"paperId": null, "title": "Books3"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Monte Carlo theory, methods and examples"}]}