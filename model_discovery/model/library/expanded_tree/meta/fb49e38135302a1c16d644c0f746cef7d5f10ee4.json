{"paperId": "fb49e38135302a1c16d644c0f746cef7d5f10ee4", "abstract": "We view the landscape of large language models (LLMs) through the lens of the recently released BLOOM model to understand the performance of BLOOM and other decoder-only LLMs compared to BERT-style encoder-only models. We achieve this by evaluating the smaller BLOOM model variants (\\textit{350m/560m} and \\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards. We make the following observations: (1) BLOOM performance does not scale with parameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning BLOOM models show that the 560m variant performs similarly to or better than the 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning experiments show that BLOOM is at par or worse than monolingual GPT-2 models, and (3) Toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that the text generated by BLOOM is at least 17\\% less toxic than GPT-2 and GPT-3 models.", "venue": "arXiv.org", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2211.14865", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Evaluating the smaller BLOOM model variants on several NLP benchmark datasets and popular leaderboards shows that the 560m variant performs similarly to or better than the 1b7 variant, and toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that it is at least 17\\% less toxic than GPT-2 and G PT-3 models."}, "embedding": {"model": "specter_v2", "vector": [0.20030105113983154, 0.7896697521209717, -0.39650025963783264, 0.16447213292121887, -0.4212464392185211, -0.7105462551116943, 0.8182440996170044, -0.03712231665849686, -0.38357192277908325, 0.0715148076415062, 0.49840331077575684, -0.41844046115875244, 0.1283377707004547, 0.14041467010974884, -0.33205580711364746, 0.17924104630947113, -0.5265713334083557, 0.7117583751678467, -0.31538352370262146, -0.27023428678512573, -0.3205275535583496, -0.9483946561813354, -0.2574814558029175, 0.15502026677131653, 0.6351120471954346, 0.14051435887813568, 0.46501055359840393, 0.7723271250724792, -0.37837252020835876, 0.38843584060668945, 0.2066074013710022, -0.8284183740615845, 0.36044466495513916, -0.1244218498468399, -0.15684258937835693, -0.09010016173124313, 0.20771080255508423, -0.42960667610168457, -0.06327170133590698, 1.0366710424423218, -0.010805241763591766, -0.044883642345666885, 0.5419678092002869, -0.6048240065574646, -0.829520583152771, 1.2440681457519531, 0.6755586266517639, 0.4926820695400238, 0.1797409951686859, 0.0027653295546770096, 1.4225929975509644, -1.0136775970458984, 0.6419143080711365, 1.791263222694397, 0.7365186214447021, 0.5206333994865417, -0.28809043765068054, -0.7175878286361694, 0.45803746581077576, -0.13011741638183594, -0.8872039318084717, -0.12031061202287674, -0.4499584138393402, 0.09308446943759918, 1.7664424180984497, -0.546760082244873, -0.22674992680549622, 0.9019627571105957, -0.20829656720161438, 1.6865004301071167, -0.4838387966156006, -0.8404425382614136, -0.6074824333190918, 0.07400497049093246, -0.1853175163269043, 0.46671342849731445, -0.32565975189208984, 0.4163323938846588, -0.7390058636665344, -0.14954829216003418, 0.23759499192237854, -0.6455699801445007, -0.4917614758014679, 0.4271492660045624, -0.5350589752197266, 0.5541132688522339, -0.07345207780599594, 0.8650533556938171, -0.21567147970199585, 0.5065233707427979, 0.5645698308944702, 0.48083963990211487, -0.08899009972810745, 0.6580301523208618, -0.2322865128517151, 0.05948483198881149, -0.8507207632064819, 0.6744111180305481, 0.2656806707382202, 1.0443686246871948, -0.3725201189517975, -0.026123760268092155, -1.2412644624710083, 0.013194858096539974, 1.3879539966583252, 0.10499422997236252, 0.49008503556251526, -0.9577940702438354, 0.35977640748023987, -0.9085776805877686, 0.42863938212394714, -0.38633495569229126, -0.22365298867225647, -0.12926509976387024, -0.3721865713596344, -1.5799665451049805, -0.050989486277103424, -0.3406299650669098, -0.5920401215553284, 0.9694351553916931, -0.3903502821922302, -0.28212815523147583, 0.3816837668418884, 0.3320535719394684, 0.8226444125175476, 1.0043938159942627, 0.26573044061660767, -0.09345366060733795, 0.7014777064323425, -0.4054871201515198, -0.9376081824302673, -0.9760743975639343, 1.2207956314086914, -0.23669548332691193, 0.21147087216377258, 0.1567024290561676, -1.4939333200454712, -0.49356046319007874, -0.5803213119506836, -0.2868499755859375, -0.24075838923454285, 0.7314926981925964, 0.8487336039543152, 0.6386560201644897, -0.8776142597198486, 0.5283857583999634, 0.05197708681225777, -0.4811665117740631, 0.05390390381217003, -0.36381205916404724, 0.34789326786994934, -0.6212695837020874, -1.4094775915145874, 0.283737450838089, 0.5530007481575012, -0.7355332970619202, -0.32570478320121765, -0.7112267017364502, -0.9897929430007935, -0.2230074405670166, 0.4989033341407776, -0.2660556435585022, 1.824291467666626, 0.315329909324646, -1.3459914922714233, 0.5353822112083435, -0.4335179030895233, 0.053210850805044174, 0.31315547227859497, 0.05426083877682686, -0.4122970998287201, -0.7146731615066528, -0.0914468765258789, 0.772932767868042, 0.013737376779317856, -0.1335049420595169, -0.25891947746276855, 0.4931841790676117, -0.11858315020799637, -0.035324081778526306, -0.17408549785614014, 1.3091787099838257, -0.4144579768180847, -0.24854333698749542, -0.0930071622133255, 0.7180595993995667, 0.011917011812329292, -0.09153538942337036, -0.6605181694030762, -1.4605388641357422, 0.5995479226112366, -0.1367359161376953, 0.9247429966926575, -0.6470738053321838, -0.8695124983787537, -0.6599217653274536, -0.5602340698242188, 0.030024349689483643, -0.878778338432312, 0.9943525791168213, -0.055526673793792725, 0.6938080191612244, -0.38758790493011475, -1.2740708589553833, 0.25439098477363586, -0.11734950542449951, -0.47177737951278687, -0.4175076484680176, 0.16472861170768738, 1.176544189453125, -0.9046884179115295, 0.02010030671954155, -0.2827562689781189, -0.10244929045438766, -1.004676103591919, 0.8743312954902649, -1.0045207738876343, 0.27173131704330444, 0.05531471222639084, -0.32366737723350525, -0.08966583013534546, -0.15605808794498444, 0.19745472073554993, -0.3390854299068451, -0.28224825859069824, 0.5173654556274414, -0.33525997400283813, 1.6614768505096436, -0.11467345058917999, 0.1481323540210724, -0.09893476963043213, -0.19393010437488556, -0.028185933828353882, 0.8597097992897034, -0.5451807379722595, -0.24077264964580536, 0.2536581754684448, 0.6685834527015686, -0.1989252269268036, 0.0830291137099266, 0.5857356786727905, 0.5911567807197571, -0.1072288528084755, 0.5669515132904053, 0.6191594004631042, -0.4149526357650757, 1.0719428062438965, 0.366055965423584, 0.7582734823226929, 0.3351423740386963, 0.32451316714286804, -0.2813096046447754, 0.21766065061092377, -0.3752162456512451, -0.05803997814655304, 0.5133183002471924, 0.7044786810874939, 0.6780698895454407, 0.5549298524856567, -0.6435216665267944, -0.17199347913265228, 0.15827150642871857, 0.3685515224933624, 1.2285176515579224, -0.29183700680732727, -0.4633389115333557, -0.7717925906181335, -0.29979026317596436, -0.23463714122772217, 0.6524174809455872, -0.22263102233409882, 0.09715740382671356, -0.5323220491409302, -1.2821167707443237, 0.9642210006713867, -0.013178098946809769, 0.5128178000450134, -0.7228001952171326, -0.12074852734804153, -0.03575891628861427, -0.0871124416589737, -0.887334406375885, -0.7185707092285156, 0.20148223638534546, -0.18158425390720367, -0.06909273564815521, -0.05532103776931763, 0.02099824696779251, -0.0048390538431704044, -0.9435088634490967, 0.7903180122375488, -0.8224872946739197, -0.20583972334861755, -0.13006927073001862, 0.35017016530036926, -0.8685306310653687, -1.289262056350708, 0.050657711923122406, -0.0668787881731987, -0.4143359363079071, 0.3027518391609192, 0.7883638739585876, 0.5407047867774963, -0.08210960030555725, -0.5751780867576599, 0.19052840769290924, 0.05299612134695053, -0.10857605189085007, 0.5584405064582825, -0.17006182670593262, -0.07608772069215775, -1.1006176471710205, 1.5036858320236206, 0.33773115277290344, -0.6323477625846863, 0.4465944468975067, -0.36522912979125977, -0.4602522552013397, 0.5476064682006836, -0.7555851340293884, -0.7509866952896118, -1.0502393245697021, 0.012506217695772648, 0.16704276204109192, -0.1459047943353653, 0.6084650754928589, -0.014143689535558224, 0.49683138728141785, 0.547123372554779, 0.6303241848945618, 0.25713080167770386, -0.42439785599708557, 0.8951523900032043, -0.7408856153488159, 0.5591908097267151, 0.05929504334926605, 0.02401011250913143, -0.13370254635810852, -0.2870784401893616, -0.5157074928283691, -0.45406362414360046, 0.11729566752910614, 0.06834027916193008, -0.08051622658967972, 0.3328092694282532, -0.4809116721153259, -0.782463014125824, 0.1746535748243332, -1.6059497594833374, -0.19284428656101227, 0.03760790079832077, -0.4245848059654236, -0.06699229776859283, -0.8904855251312256, -1.402250051498413, -0.4915917217731476, -0.5616039633750916, -0.8865821361541748, 0.7636317014694214, -0.21340036392211914, -0.5377828478813171, -0.46418312191963196, 0.5498262047767639, -0.5763751268386841, 0.702451765537262, -0.7148436903953552, 1.0156826972961426, 0.057955265045166016, -0.03954213857650757, -0.24823135137557983, 0.4305309057235718, 0.31696203351020813, -0.21410711109638214, 0.63377445936203, -0.48203209042549133, 0.3233969509601593, -0.2255546599626541, -0.6642958521842957, -0.19036906957626343, 0.6202937960624695, 0.34988096356391907, -0.17890428006649017, -0.6246242523193359, 0.34366968274116516, 1.2335107326507568, -0.42290693521499634, -0.04517502710223198, 0.0738331526517868, 0.7792172431945801, 0.6106746196746826, -0.3267805278301239, 0.6674969792366028, 0.4597178101539612, 0.2602197527885437, 0.16860228776931763, 0.34910139441490173, 0.1953759491443634, -0.8123835325241089, 0.7774387001991272, 1.275964379310608, 0.5450202226638794, -0.6408166885375977, -1.3002225160598755, 0.4801141619682312, -1.213409185409546, -0.4615877866744995, 0.20870915055274963, 0.5021027326583862, 0.8793551921844482, -0.7039164900779724, -0.5310124158859253, -0.2676405608654022, 0.31023505330085754, 0.08237611502408981, -0.08040132373571396, -0.585404634475708, -0.2104356288909912, -0.059673137962818146, 0.02925664559006691, 0.4004233479499817, -0.152933731675148, 0.967056155204773, 14.629594802856445, 1.0071585178375244, 0.20583510398864746, 0.6974365711212158, 0.5857875943183899, 0.011243768036365509, -0.7583162784576416, -0.400619775056839, -1.5148952007293701, -0.15527497231960297, 1.1474051475524902, -0.24079574644565582, 0.43665844202041626, -0.11620582640171051, 0.4225757420063019, 0.20965790748596191, -0.46138933300971985, 0.6134319305419922, 0.43450847268104553, -1.3083394765853882, 0.6408612728118896, 0.15055982768535614, 0.5090875029563904, 0.8953616619110107, 0.5120616555213928, 0.6777093410491943, 0.783759355545044, -0.4588219225406647, 0.7384428977966309, 0.03057178482413292, 0.6466248035430908, 0.05606868490576744, 0.4262753427028656, 1.076209545135498, -0.4202594757080078, -0.37615466117858887, -0.8241875171661377, -1.0718740224838257, 0.5944979190826416, 0.24475793540477753, -0.408864825963974, -0.44397422671318054, -0.16522975265979767, 0.7581580281257629, 0.3928664028644562, 0.37794092297554016, -0.3680307865142822, 0.5409988164901733, -0.18535953760147095, -0.06251931935548782, 0.4112543761730194, 0.4300202429294586, 0.46290701627731323, 0.024808868765830994, 0.5281038284301758, -0.006296391133219004, 0.2351384311914444, 0.9310480952262878, -0.8596134185791016, 0.2395821362733841, -0.40912237763404846, -0.5115932822227478, 0.13144797086715698, 0.5516114830970764, 0.551167368888855, 0.35802748799324036, -0.3611515164375305, 0.16393616795539856, 0.4463654160499573, 0.04054958373308182, 0.14889195561408997, 0.15605100989341736, 0.10414905101060867, -0.5191522240638733, -0.12843260169029236, 0.7991403341293335, -0.07327261567115784, -0.14011314511299133, -0.6274964809417725, -0.6139104962348938, 0.06402257829904556, -0.9555601477622986, -0.7966297268867493, 0.8254196047782898, -0.2530682682991028, -0.26497146487236023, 0.08482183516025543, -0.8418266773223877, -0.2755619287490845, 0.6592054963111877, -1.3257148265838623, -0.8828845620155334, 0.6871595978736877, -0.40451955795288086, -0.4713529646396637, 0.21640507876873016, 1.2265206575393677, -0.1432129293680191, -0.6093320250511169, 0.1348922997713089, 0.5657336115837097, -0.13878171145915985, -0.16608159244060516, -0.7306728959083557, 1.2489478588104248, 0.3598676025867462, -0.1602816879749298, 0.13465473055839539, 0.09635470062494278, -0.19582562148571014, -0.8202519416809082, -0.24326099455356598, 1.1939717531204224, -1.1823984384536743, -0.41084322333335876, -0.6280487179756165, -0.768845796585083, 0.10624436289072037, 0.8550035357475281, -0.7148294448852539, 0.0022544506937265396, -0.07734262943267822, -0.38859567046165466, 0.01409666147083044, -0.9737427830696106, 0.1032717302441597, 0.4743785262107849, -0.39138415455818176, -0.1295616328716278, 0.38419392704963684, 0.6318710446357727, -0.9648165702819824, -0.49825000762939453, -0.26718008518218994, 0.09305606037378311, 0.1408240795135498, 0.24535591900348663, -0.25412219762802124, 0.6702238321304321, 0.8589515089988708, 0.14447465538978577, -0.9936428070068359, -0.022168517112731934, -1.3598655462265015, 0.3395806849002838, 0.3025364875793457, 1.133167028427124, -0.44595715403556824, -0.037826452404260635, 0.7680098414421082, 0.13176856935024261, -0.3639700710773468, -0.6770029067993164, -0.6650131344795227, 0.5605246424674988, -0.7801550626754761, 0.6396438479423523, -0.1639970988035202, 0.3549848198890686, -0.032201990485191345, 0.32739555835723877, 0.5170436501502991, -0.29652121663093567, -1.054275631904602, 0.34286051988601685, -0.06279219686985016, -0.23517441749572754, -0.5777567028999329, 0.025546377524733543, -1.4672342538833618, -0.15605519711971283, -1.1649583578109741, 0.2967982292175293, -1.1120972633361816, -0.3946893811225891, 0.14384758472442627, 0.25169846415519714, 0.14845819771289825, 0.6931840181350708, -0.5782221555709839, -0.4372415840625763, -0.4876251220703125, -0.05812276154756546, 0.9054431319236755, 1.287241816520691, -0.7853477001190186, 0.21286430954933167, -0.22063377499580383, -0.08907359838485718, 0.2343156635761261, 0.360111802816391, -0.5540651082992554, -0.8080427050590515, -1.8378722667694092, 0.7086884379386902, 0.09379029273986816, -0.39532336592674255, -0.6636296510696411, 0.579981803894043, 0.6184545159339905, -0.6064527630805969, 0.043617796152830124, 0.03504189848899841, -0.6603645086288452, -0.5344573855400085, 0.20860488712787628, -0.783187985420227, 0.33729422092437744, 0.23254436254501343, -0.7113155126571655, -0.23976776003837585, 0.27142494916915894, -0.2764996886253357, -1.3108783960342407, -0.05560535565018654, 0.49890944361686707, -0.8332834243774414, 0.1515471190214157, -0.4131247103214264, -0.12201046943664551, -1.160247802734375, -0.3956616222858429, 0.5163186192512512, 0.42774197459220886, -0.22142428159713745, 1.0641722679138184, 0.13143429160118103, -0.6783581376075745, -0.29308581352233887, 0.2585456967353821, 0.06793797761201859, -0.17335516214370728, 0.24369844794273376, 0.3902130722999573, -0.3919750452041626, 0.8112332224845886, 0.7049548029899597, 0.2717359662055969, -0.5761953592300415, -0.19817136228084564, 0.8470423221588135, -0.46991991996765137, -0.22673475742340088, 1.4730409383773804, -0.22555245459079742, -1.3176294565200806, 0.40794914960861206, -1.4713530540466309, -0.4276275932788849, -0.750968873500824, 0.7759397029876709, 0.10168642550706863, -0.04124211147427559, -0.08378264307975769, -0.13796138763427734, -0.028302934020757675, -0.1210845559835434, -0.6078827977180481, 0.25047236680984497, -0.07824021577835083, -0.5405420064926147, 0.15465731918811798, 0.49083659052848816, -0.7754999995231628, -0.5284385085105896, -0.4741497337818146, -0.20868317782878876, 0.3115755021572113, 0.3981225788593292, -0.8012573719024658, -0.1345212459564209, 0.6865987181663513, 0.0996757298707962, 0.11920669674873352, 0.11669788509607315, -0.11829870939254761, 0.48410987854003906, 0.7661725878715515, 0.2643558382987976, -0.541386604309082, -0.9374070167541504, 1.3906737565994263, 1.247527003288269, -1.0261409282684326, -0.19724781811237335, -0.22103071212768555, -0.8411339521408081, 0.6801175475120544, 0.19441865384578705, 0.5738677382469177, 0.8235755562782288, -0.11271601170301437, 0.1835821568965912, 0.09672868996858597, -1.2761846780776978, 0.025893094018101692, 0.5410839319229126, 0.948442280292511, 0.8316792845726013, 0.24531501531600952, -0.38238614797592163, 1.019540548324585, 0.08186206966638565, 0.4239571690559387, 0.719245433807373, 0.18614083528518677, -0.09437286853790283, -0.3760232627391815, 0.13567306101322174, 0.6431645154953003, -0.6074392795562744, -0.9632492065429688, -0.11849960684776306, 0.5465535521507263, 0.4819655120372772, 0.7938276529312134, 0.3057595491409302, 0.32850897312164307, 0.46797236800193787, 0.1326170712709427, 0.4958929419517517, -0.5464164614677429, -0.36150529980659485, -0.1891392320394516, -0.43065154552459717, 0.06204355135560036, 0.038519080728292465, -0.819858968257904, -0.09096436947584152, -0.17318075895309448, 0.1779414713382721, 0.04680062457919121, 0.2897806763648987, 1.1717889308929443, 0.5184937715530396, -0.15293359756469727, -0.7154648900032043, -0.317791610956192, -0.6333829164505005, -1.284269094467163, -0.11085930466651917, -0.6336350440979004, -0.5639864802360535, 0.262060284614563, -0.1592472642660141, -0.17851029336452484]}, "authors": [{"authorId": "51116864", "name": "Parag Dakle"}, {"authorId": "3167650", "name": "Sai Krishna Rallabandi"}, {"authorId": "30088877", "name": "Preethi Raghavan"}], "references": [{"paperId": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e", "title": "Training Trajectories of Language Models Across Scales"}, {"paperId": "75f7e9e2b59fb640ef9d1dff94097175daf46c4d", "title": "Large Language Models Struggle to Learn Long-Tail Knowledge"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "0de580957d23dd65e31b6c95e6bc5d15bc15c57d", "title": "Impact of Tokenization on Language Models: An Analysis for Turkish"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "eafe58b4a09844418bc8972f890e5d24e9df2fb7", "title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"}, {"paperId": "2672777d25562c9df6fc13b653181db62d39bece", "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"}, {"paperId": "20da8033ed8b696e2e27ec40b1aa8a0ab82b964c", "title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems"}, {"paperId": "02f033482b8045c687316ef81ba7aaae9f0a2e1c", "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "4f68b92981dc0363803105042b4eab19cc388cbb", "title": "ParaQA: A Question Answering Dataset with Paraphrase Responses for Single-Turn Conversation"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "cb596bffc5c5042c254058b62317a57fa156fea4", "title": "Unifying Vision-and-Language Tasks via Text Generation"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "0d4b5c9a071557f4eb12f63f785dbc89071d4272", "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"}, {"paperId": "a2d534fda2eafabf5ac19934ce500cd975e33030", "title": "Adapting a Language Model for Controlled Affective Text Generation"}, {"paperId": "df057d8d4346e938ea6e377065a47ee03e9e67be", "title": "The Multilingual Amazon Reviews Corpus"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "54bf77d9ef075db48c180d995c0fa7b1903cf13c", "title": "Uncertainty-aware Self-training for Few-shot Text Classification"}, {"paperId": "ba960d5b53f3795be5d9600da2adea63754bfc9f", "title": "Few-Shot Generative Conversational Query Rewriting"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "bd2fbe5f6c7f26851c004e659a1e38a8f5005d80", "title": "Fine-grained Sentiment Classification using BERT"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "814978b4306e40ede6e125fe762bf9d632329a69", "title": "Aspect-Based Sentiment Analysis using BERT"}, {"paperId": "997855e1f17d34dd3922d953a587742d198844e6", "title": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations"}, {"paperId": "81fbf08beb80b01abaa6ad6a07b48c3034ead8a6", "title": "Is Multilingual BERT Fluent in Language Generation?"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "39e801ca0dbc69c3697f118e24dac964abb63d4a", "title": "The CommitmentBank: Investigating projection in naturally occurring discourse"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "809cc93921e4698bde891475254ad6dfba33d03b", "title": "How Multilingual is Multilingual BERT?"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "7191680b572ee7145f1a9d95ff11ab1ff44259f3", "title": "WWW'18 Open Challenge: Financial Opinion Mining and Question Answering"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0", "title": "Deep Learning Scaling is Predictable, Empirically"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "8ae1af4a424f5e464d46903bc3d18fe1cf1434ff", "title": "End-to-end Neural Coreference Resolution"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "92cfbec6f710a483f1f91800121ea25053458d7c", "title": "A Conversational Movie Search System Based on Conditional Random Fields"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "d2161251488dbba08616a9cdd4223a0ac1190cef", "title": "Building a question answering test collection"}, {"paperId": "1d19708290ef3cc3f43c2c95b07acdd4f52f5cda", "title": "The ATIS Spoken Language Systems Pilot Corpus"}, {"paperId": "4a05f611a1697fdf41e21ab8f9cb7dc2048833ee", "title": "RoBERTa-based Traditional Chinese Medicine Named Entity Recognition Model"}, {"paperId": "080df61ee1c15ff3c8e5d0d82d60bfd80e372e38", "title": "Probing Toxic Content in Large Pre-Trained Language Models"}, {"paperId": "0639baa6dfb35d962e46eb0c38763d769ffb0946", "title": "Models"}, {"paperId": null, "title": "ELECTRA: Pretraining text encoders as discriminators rather than generators"}, {"paperId": null, "title": "improves the BERT and RoBERTa models using disentangled attention and enhanced mask de-coder"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2019) presented a study investigating BERT pre-training that measures the impact of key hyperparameters and training data size"}, {"paperId": null, "title": "Improving language understanding with unsupervised learning"}, {"paperId": "db8885a0037fe47d973ade79d696586453710233", "title": "The Sixth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "351ec42df2b60c6042addf96e6b98673bbaf4dfd", "title": "The Fourth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "title": "The Third PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "136326377c122560768db674e35f5bcd6de3bc40", "title": "The Second PASCAL Recognising Textual Entailment Challenge"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}]}