{"paperId": "6c761cfdb031701072582e434d8f64d436255da6", "abstract": "Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.", "venue": "arXiv.org", "year": 2021, "citationCount": 195, "influentialCitationCount": 12, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic."}, "embedding": {"model": "specter_v2", "vector": [0.42630720138549805, 0.7386593818664551, -0.13686443865299225, -0.21064940094947815, -0.07573380321264267, -0.38838666677474976, 0.6967997550964355, -0.038194987922906876, -0.8170326352119446, -0.31067001819610596, 0.6370585560798645, 0.12178240716457367, 0.5399134159088135, -0.015015127137303352, -0.3209740221500397, 0.23207037150859833, -0.6051380634307861, 0.38081538677215576, -0.12632761895656586, -0.7510061860084534, -0.4987938106060028, -0.9364760518074036, -0.4524025619029999, 0.0994131788611412, 0.4837827682495117, 0.2384980171918869, 0.26804181933403015, 0.6476524472236633, -0.594611644744873, -0.0962342768907547, 0.6919227242469788, -0.5362243056297302, 0.3581770360469818, 0.030433209612965584, -0.40891024470329285, -0.02657257951796055, 0.4216483533382416, -0.3318951725959778, -0.43653935194015503, 0.953481912612915, -0.32370978593826294, 0.33172234892845154, 0.7454447746276855, -0.5023371577262878, -0.6246730089187622, 1.228072166442871, 0.9177207350730896, 0.8080141544342041, -0.393330842256546, -0.5653082132339478, 1.2258225679397583, -1.5233491659164429, 0.11817862838506699, 1.3959041833877563, 0.6404613256454468, 0.5438660979270935, -0.4320307672023773, -0.7694991827011108, 0.43133342266082764, -0.1363249570131302, -0.5232036113739014, -0.5073401927947998, 0.09658139944076538, -0.01807938702404499, 2.2049293518066406, -0.6566936373710632, -0.15812411904335022, 0.5627076029777527, -0.06871245801448822, 1.293755054473877, -0.11789362132549286, -0.9512683153152466, -0.8952402472496033, 0.2668766975402832, 0.08655091375112534, 1.0272754430770874, -0.8853905200958252, 0.4094279408454895, -0.862919807434082, 0.0911962017416954, 0.41508129239082336, -0.210999995470047, -0.25583866238594055, -0.09107042849063873, -0.5493147969245911, 0.8336852788925171, 0.7091890573501587, 1.1444391012191772, -0.0949181541800499, 0.32397550344467163, 0.7496317028999329, 0.5803619027137756, -0.11514738947153091, 0.6702749133110046, -0.3759785592556, 0.6039761900901794, -0.8035324215888977, -0.06120124086737633, -0.198907271027565, 0.7637966275215149, 0.13328537344932556, 0.47950953245162964, -0.7946696877479553, 0.351699560880661, 1.201583981513977, 0.20267315208911896, 0.9204888343811035, -0.47162628173828125, 0.37517496943473816, -0.5489934682846069, -0.2464563548564911, -0.6592882871627808, -0.04850019887089729, -0.3491845428943634, -0.8681370615959167, -1.5225105285644531, -0.3302779495716095, 0.05052433907985687, -1.058739423751831, 1.1991994380950928, -0.4603863060474396, -0.18575169146060944, 0.43758752942085266, 0.32818496227264404, 0.5029548406600952, 0.7871825098991394, 0.5352779030799866, 0.21588215231895447, 0.8984075784683228, -0.8654206991195679, -0.8240633606910706, -1.1577928066253662, 0.7550987601280212, -0.2149513214826584, 0.41574329137802124, -0.40134382247924805, -0.9758874773979187, -0.982073187828064, -0.7273790240287781, -0.11238406598567963, -0.7827431559562683, 0.44821226596832275, 0.8514006733894348, 0.4564504623413086, -1.0115506649017334, 1.0221569538116455, 0.08489204198122025, -0.43974119424819946, 0.3037780523300171, 0.2716062366962433, -0.05609850957989693, -0.2815380394458771, -1.6701552867889404, 0.2555422782897949, 0.6670860052108765, -0.49032801389694214, -0.3754761219024658, -0.6075621247291565, -1.3356293439865112, -0.0024672909639775753, 0.12147515267133713, -0.4013080298900604, 1.2488802671432495, -0.5716941952705383, -1.3926821947097778, 1.0590999126434326, -0.5067383646965027, -0.2985430359840393, -0.05800379440188408, -0.1804225593805313, -0.684184730052948, -0.7448688745498657, 0.01334459614008665, 0.62911057472229, 0.5765172839164734, 0.052585355937480927, 0.01831568032503128, 0.18430864810943604, -0.4251275360584259, 0.10475289821624756, -0.906975507736206, 0.7790022492408752, -0.2069346308708191, -0.34577882289886475, 0.43501466512680054, 0.7916697263717651, 0.10368746519088745, -0.5938817858695984, -0.4543597400188446, -1.3106895685195923, 0.6518459320068359, 0.17650780081748962, 0.6052899360656738, -1.1885701417922974, -0.6706225275993347, -0.1685505211353302, -0.2903601825237274, 0.17552898824214935, -1.210820198059082, 0.9777722358703613, -0.7141271233558655, 0.36147454380989075, -0.006113243754953146, -1.35750150680542, 0.05015213415026665, -0.15996114909648895, -0.8677823543548584, -0.3520292043685913, 0.32907575368881226, 1.2772058248519897, -1.1345558166503906, 0.049674972891807556, -0.016240209341049194, 0.32864826917648315, -0.7848010063171387, 1.2905884981155396, -0.3003828227519989, -0.150150328874588, 0.15731331706047058, -0.3217467963695526, -0.006127862725406885, 0.0046423496678471565, 0.1888238489627838, -0.6094160676002502, -0.0702594593167305, 0.6791483759880066, -0.18259747326374054, 1.5918103456497192, -0.32210707664489746, 0.45759353041648865, -0.2102183848619461, -0.6745067238807678, 0.07888127863407135, 0.7422608733177185, -0.12937231361865997, -0.3227878510951996, 0.5355448722839355, 0.4387083947658539, -0.6621202230453491, 0.5022870898246765, 0.8161636590957642, 0.10824462026357651, -0.12441577017307281, 0.11357085406780243, 0.8671169281005859, -0.25941717624664307, 0.5152649283409119, 0.4125423729419708, 0.6861712336540222, 0.32159918546676636, 0.5030339360237122, -0.05242980271577835, 0.5021328926086426, -0.4578971564769745, -0.07534443587064743, 0.08027228713035583, 0.6268185973167419, 0.6827264428138733, 0.05802663788199425, -0.3469771146774292, 0.013776109553873539, -0.31150537729263306, 0.8332853317260742, 1.7009475231170654, -0.6385470628738403, -0.29660776257514954, -0.5438663363456726, -0.4298027753829956, -0.45220324397087097, 0.1694088876247406, -0.40111613273620605, -0.29100170731544495, -0.4461148977279663, -0.9467238187789917, 0.6878283619880676, 0.2501351535320282, 1.0375932455062866, -0.7003352046012878, 0.19642174243927002, 0.029572848230600357, 0.18946893513202667, -0.7883291244506836, -0.9093373417854309, 0.6761445999145508, -0.8105891346931458, -0.14326927065849304, 0.026592887938022614, -0.3874002993106842, 0.44338059425354004, -0.6497732996940613, 0.8880276083946228, -0.8804846405982971, 0.13710767030715942, 0.030332358554005623, 0.3985731601715088, -0.7723264098167419, -0.8807353973388672, 0.4808967709541321, 0.4264853596687317, -0.008320718072354794, 0.5035585761070251, 0.35440951585769653, 0.40924665331840515, -0.04535573348402977, -0.4852709174156189, -0.08531080186367035, 0.1564166247844696, 0.2133830189704895, 0.8930274844169617, 0.08892315626144409, 0.07082826644182205, -1.609879732131958, 0.9769976735115051, 0.3256549537181854, -0.143123596906662, 0.055380940437316895, -0.5906090140342712, -0.23045584559440613, 0.8399311900138855, -0.6633177399635315, -0.42155829071998596, -0.3533729612827301, 0.22822533547878265, -0.4176335334777832, -0.2601991593837738, 0.5285813212394714, 0.021026132628321648, 0.46011966466903687, 0.6295064687728882, 0.5358144640922546, 0.36616334319114685, 0.015403317287564278, 0.7185646295547485, -0.87371426820755, 0.6043598055839539, 0.4720032215118408, 0.8029455542564392, -0.34663763642311096, -0.5730631351470947, -0.5761381387710571, -0.6976518630981445, -0.607111394405365, -0.2496078610420227, 0.0422353520989418, 0.15694652497768402, -0.4908446967601776, -0.5114883780479431, -0.01774715632200241, -0.7716566324234009, -0.33039531111717224, -0.25451403856277466, -0.44240084290504456, -0.15202422440052032, -0.9679074287414551, -1.0971653461456299, -0.4942544102668762, -0.8357207775115967, -0.36976131796836853, 0.22738811373710632, 0.1705569177865982, -0.18948206305503845, -0.9154468178749084, 0.15146100521087646, -0.435851126909256, 1.3172837495803833, -0.7796757817268372, 0.9454061985015869, -0.041569534689188004, 0.2841421067714691, -0.2264789640903473, 0.22189848124980927, 1.0959056615829468, 0.08163158595561981, -0.01546655222773552, -0.861107349395752, 0.015984605997800827, -0.32955673336982727, -0.4974648952484131, 0.3198102116584778, 0.21920372545719147, 0.2706840932369232, 0.11329276114702225, -0.46462056040763855, 0.6971607208251953, 1.649717926979065, -0.7604660391807556, -0.03271546587347984, 0.16043314337730408, 0.6627591252326965, 0.08907603472471237, -0.4025232493877411, -0.05079931765794754, 0.5275861620903015, 0.008982294239103794, 0.20192940533161163, -0.18399111926555634, -0.04742668196558952, -0.892451822757721, 0.7763184905052185, 1.7136579751968384, 0.43529412150382996, -0.20570921897888184, -1.1113557815551758, 0.9649553298950195, -0.8604201674461365, -0.4665730893611908, 0.5130257606506348, 0.48674240708351135, 0.8280318379402161, -0.4016512334346771, -0.5481322407722473, -0.020889582112431526, 0.16920502483844757, 0.23129646480083466, -0.31280553340911865, -0.7232539653778076, -0.2518705427646637, 0.5123733282089233, 0.31687891483306885, 0.47425326704978943, -0.8897612690925598, 1.2103168964385986, 14.3599214553833, 0.564713180065155, -0.0505724661052227, 0.7084329128265381, 0.09552278369665146, 0.609409749507904, -0.23159648478031158, -0.1519758403301239, -1.1482301950454712, -0.6182875037193298, 0.8942394852638245, 0.15049955248832703, 0.6700210571289062, 0.043468624353408813, -0.1649908721446991, 0.4306243360042572, -0.7697369456291199, 0.6148286461830139, 0.5169315338134766, -0.8880237936973572, 0.61310213804245, 0.14720217883586884, 0.339180052280426, 0.8161971569061279, 0.8489483594894409, 1.1118032932281494, 0.49038437008857727, -0.2939448654651642, 0.10390260070562363, 0.23371511697769165, 0.7554031014442444, 0.4073241651058197, 0.5779350996017456, 0.7259612679481506, -0.7344759702682495, -0.28478124737739563, -0.6785860657691956, -0.9261721968650818, 0.22658275067806244, 0.1614329069852829, -0.7737302184104919, -0.28392675518989563, -0.2117754966020584, 1.0963225364685059, 0.12434666603803635, 0.04271884635090828, -0.47592097520828247, 0.8674830794334412, -0.4491252899169922, 0.27613165974617004, 0.39267098903656006, 0.3792141377925873, 0.3973281681537628, 0.1473945677280426, 0.15917351841926575, -0.06505925208330154, 0.04219815507531166, 0.36885198950767517, -0.7612109184265137, 0.4204859435558319, -0.2305845320224762, -0.5919327735900879, -0.6196979284286499, 0.9760061502456665, 0.724000871181488, 0.3878771662712097, -0.2784731388092041, 0.6633776426315308, 0.6757453083992004, 0.264892041683197, -0.010240384377539158, -0.15984408557415009, 0.1089579239487648, -0.031088754534721375, -0.024588244035840034, 0.49527063965797424, 0.1310616433620453, -0.5457759499549866, -0.8441866040229797, -0.13865533471107483, 0.33519357442855835, -0.7282431125640869, -1.158339023590088, 0.9982326626777649, -0.5628194212913513, -0.456563264131546, 0.11031820625066757, -1.0866841077804565, -0.18123523890972137, 0.7980643510818481, -1.8550386428833008, -0.5606094598770142, 0.33335572481155396, -0.09248503297567368, -0.6829482316970825, -0.4954831898212433, 1.5283583402633667, 0.018649857491254807, -0.3818363845348358, 0.13780082762241364, 0.08722908794879913, 0.47418075799942017, 0.03339427709579468, -0.9810882210731506, 0.5536143779754639, 0.27660998702049255, 0.06325731426477432, 0.29154708981513977, -0.06186021864414215, 0.17914822697639465, -0.6516493558883667, -0.3814919888973236, 1.0389883518218994, -0.7659385800361633, -0.3904000520706177, -0.5728560090065002, -0.7879496216773987, 0.34575575590133667, 0.8996074795722961, -0.5049476027488708, 0.6415457129478455, 0.2399396449327469, -0.6569602489471436, -0.1737121343612671, -0.765159547328949, -0.051967084407806396, 0.6117219924926758, -0.7018105983734131, -0.6169177889823914, 0.16583818197250366, 0.12664613127708435, -0.7568070292472839, -0.46497371792793274, -0.1699848473072052, -0.3005163073539734, 0.24006065726280212, 0.8371429443359375, -0.7094278335571289, 0.48723453283309937, 0.8012462258338928, -0.1853509098291397, -1.2554166316986084, -0.1627621054649353, -0.6815025210380554, 0.2596023976802826, 0.31669214367866516, 0.9561811685562134, -0.5303276181221008, 0.342558354139328, 0.5357512831687927, 0.1546311378479004, -0.21967560052871704, -0.5296826958656311, -0.5543535947799683, 0.11742718517780304, -0.3262214660644531, 0.0574510432779789, 0.16890360414981842, -0.16269142925739288, 0.5599909424781799, 0.737095057964325, 0.28670063614845276, -0.26074880361557007, -0.8424623012542725, 0.2807719111442566, -0.20457328855991364, 0.008275424130260944, -0.3122592866420746, -0.19126780331134796, -1.2994545698165894, 0.2575743794441223, -1.8416005373001099, -0.1371147483587265, -1.308957815170288, -0.4252900779247284, 0.04081885144114494, -0.3487895727157593, 0.6292049884796143, 0.2167617529630661, -0.4150497019290924, -0.39673110842704773, -0.48761484026908875, -0.04825228452682495, 1.010676622390747, 0.857962965965271, -0.799584150314331, -0.31078851222991943, -0.16009558737277985, -0.07346510142087936, 0.2414824515581131, 0.7426497936248779, -0.5812284350395203, -0.882877767086029, -1.6424180269241333, 0.3422418236732483, -0.3331339359283447, -0.30516207218170166, -0.6738675832748413, 0.6197712421417236, 0.6108741760253906, -0.23769362270832062, 0.2549416422843933, 0.37721070647239685, -1.1530224084854126, -0.6830421090126038, 0.18213391304016113, -0.5817996263504028, 0.22315022349357605, 0.2434428632259369, -0.5129913091659546, -0.7409595847129822, 0.5355343222618103, -0.06185233220458031, -1.1483979225158691, -0.5794193148612976, 0.7012420892715454, -0.617850661277771, 0.17371219396591187, -0.2577438950538635, -0.26142287254333496, -0.9537715911865234, -0.2740105092525482, -0.03185316175222397, 0.48834651708602905, -0.5528295636177063, 0.898612916469574, 0.5297685265541077, -1.2241765260696411, -0.11390543729066849, 0.7032767534255981, 0.09759245067834854, -0.14686129987239838, 0.4158646762371063, 0.30409589409828186, -0.0035526365973055363, 0.6121048927307129, 0.2826843559741974, 0.3110431432723999, -0.7942814230918884, -0.0630105584859848, 1.1820640563964844, -0.6949191093444824, -0.009095974266529083, 1.1470762491226196, -0.3137936592102051, -1.6185593605041504, -0.1897978037595749, -1.1269044876098633, -0.6718119978904724, -0.17250898480415344, 0.3300832211971283, -0.22875502705574036, -0.0801548883318901, -0.5184903144836426, -0.2404225617647171, 0.2373921275138855, -0.02654866874217987, -0.5372709631919861, 0.8307851552963257, 0.1125522032380104, -0.4739038944244385, 0.5609979033470154, 1.0248126983642578, -0.7736234664916992, -0.5022485256195068, -1.1487929821014404, -0.4586946964263916, 0.11357306689023972, 0.4836811125278473, -0.44355207681655884, -0.793626606464386, 0.865704357624054, 0.4852609932422638, 0.21566608548164368, 0.4582958519458771, -0.40073633193969727, 0.3755175769329071, 0.6778960227966309, -0.1544845700263977, -0.8589192628860474, -0.47367149591445923, 1.8527122735977173, 1.220951795578003, -1.0479531288146973, -0.3147406578063965, -0.6507114768028259, -0.5920848250389099, 0.8696501851081848, 0.10114123672246933, -0.0866055116057396, 1.3544756174087524, -0.4189775884151459, 0.20417609810829163, 0.3218834400177002, -0.8139255046844482, -0.4926321804523468, 0.8757228255271912, 1.1907274723052979, 0.9355672001838684, 0.17298996448516846, 0.24192692339420319, 0.8800146579742432, -0.29834267497062683, 0.05320797488093376, 0.25471341609954834, 0.23507031798362732, -0.25879332423210144, -0.3019273281097412, 0.2198331654071808, 0.762457549571991, -0.5584598779678345, -0.7035624384880066, 0.02020583488047123, 0.5464186668395996, 0.36452704668045044, 0.6025463342666626, 0.5601173639297485, -0.020570950582623482, 0.9386076331138611, 0.5493203401565552, 0.43360674381256104, -0.9034345746040344, -0.5668220520019531, -0.4605005085468292, -0.7843506932258606, -0.020596327260136604, -0.3950725495815277, -0.6203557252883911, -0.40574124455451965, 0.20838698744773865, 0.41209831833839417, -0.07594376057386398, 0.5137797594070435, 1.0536551475524902, 0.24235549569129944, 0.32615578174591064, -0.44315123558044434, -0.034226007759571075, -0.5103868842124939, -1.446027398109436, -0.08026999235153198, -0.35084661841392517, -0.19203822314739227, 0.06913363188505173, -0.0761910155415535, -0.08089479804039001]}, "authors": [{"authorId": "49559893", "name": "Katikapalli Subramanyam Kalyan"}, {"authorId": "1613102522", "name": "A. Rajasekharan"}, {"authorId": "46505742", "name": "S. Sangeetha"}], "references": [{"paperId": "79926aa63d4daee6af06a8e9a7c2480b31cb7ed9", "title": "Spanish Pre-trained BERT Model and Evaluation Data"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "ff2f48fe6438adcaf860aac0f41c584568beafb5", "title": "CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision"}, {"paperId": "2ee03e28208a9310a9be4032c2b04ebdddb83cc7", "title": "FLEX: Unifying Evaluation for Few-Shot NLP"}, {"paperId": "4237cbebe788a97174f48dc398082739bbffe95b", "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "8b954e1654c6b759a957fd11e66c111e6105fb3f", "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"}, {"paperId": "94b38a1cf2905a9f8dd90f5e22f904a07a59b6bc", "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA"}, {"paperId": "f07bcf49a92e09437359be788bbe3f9237c5ec40", "title": "A Closer Look at How Fine-tuning Changes BERT"}, {"paperId": "cf5e670a79847d9be0eb185fb372d99d30d4d98f", "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains"}, {"paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a", "title": "Towards Understanding and Mitigating Social Biases in Language Models"}, {"paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, {"paperId": "a6a7724763d8adba466519489b0b9d209e7f2d15", "title": "BARTScore: Evaluating Generated Text as Text Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "a93632237958800217341d7bad847200afdd60e3", "title": "Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better"}, {"paperId": "c6fd846b9b8f9eb0a492d6d6242fffce987c4580", "title": "Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "2384c92bbde47f5dbc8d8f175aa67e0f95c413d4", "title": "FastSeq: Make Sequence Generation Faster"}, {"paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f", "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "981995fd64611f475179b280f4e9c241051ac185", "title": "Knowledge Inheritance for Pre-trained Language Models"}, {"paperId": "077c713bccd9d2c7fde68d4cbde06ab0f07a6855", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"}, {"paperId": "0934952763f1549e75b142261e73f2b27a2f495b", "title": "RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model"}, {"paperId": "28459083ba624020c8f1c1ed7c3a075f48b4e709", "title": "KLUE: Korean Language Understanding Evaluation"}, {"paperId": "0b077c9577f4297dcf3da835e253d21965bbc6e0", "title": "CoTexT: Multi-task Learning with Code-Text Transformer"}, {"paperId": "6563251e69e4378c189d0a0c94d8d19508d552c8", "title": "MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"}, {"paperId": "c553280c1fc1d0bc7b94683bb75910e309b0d579", "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "b4f30496a8fa212a40461ca1bdef32169e998902", "title": "Efficient pre-training objectives for Transformers"}, {"paperId": "279a19b9eba7afd513394c7a733834b0f41f97fb", "title": "mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs"}, {"paperId": "c5dd0c12353d683179fae1df079d5c5ae0e2cd23", "title": "Dual-View Distilled BERT for Sentence Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"}, {"paperId": "7b99c51d562e33309a46601c846abbe72a65c6a4", "title": "What to Pre-Train on? Efficient Intermediate Task Selection"}, {"paperId": "2435c04832d486975304a094e55ecbab8acf8a5f", "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders"}, {"paperId": "0e027f8206a4d04f0b50b88dfe31e7f2f46e2d60", "title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation"}, {"paperId": "2b9762e91305986ac8a2d624d0a69521304405f3", "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"}, {"paperId": "b6647280615f667dd7418bfb9b13d828a22c1cfe", "title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning"}, {"paperId": "1cf2e9e198feef3893da2800a7949f6880ddc084", "title": "ExplainaBoard: An Explainable Leaderboard for NLP"}, {"paperId": "3acedae6febb3d9567e896696f966c92ff406a17", "title": "Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages"}, {"paperId": "6fd41ee12ae828bb7de965bf7e06bb1bd5259fe7", "title": "IndT5: A Text-to-Text Transformer for 10 Indigenous Languages"}, {"paperId": "96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61", "title": "MuRIL: Multilingual Representations for Indian Languages"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "969287b8a96e242793b11f0dbb99ec341228106f", "title": "Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"}, {"paperId": "8994bce4b85a8b4087584661c49f8776f868f7dd", "title": "Interpretable Bias Mitigation for Textual Data: Reducing Genderization in Patient Notes While Maintaining Classification Performance"}, {"paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c", "title": "Unified Pre-training for Program Understanding and Generation"}, {"paperId": "beb6a2d33b1979d9d2010fc5721fd307e015c342", "title": "Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "cbd4f06a08eb4223b97c9079007a87dda4339afe", "title": "Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models"}, {"paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"}, {"paperId": "fcfc9648561a221750b8085790ad9ba1bebb1800", "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models"}, {"paperId": "86b369759bff13ec23b45ca23cc5461292a75415", "title": "WangchanBERTa: Pretraining transformer-based Thai Language Models"}, {"paperId": "0e8afb694ad0a84c5deafd4eedfb75e953776e65", "title": "Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "9f9fe98b60d75c6407726efff8193e8bee3ee13b", "title": "KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "c342798bafc1eaaa60c652fc90fd738941542133", "title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation"}, {"paperId": "0d4b5c9a071557f4eb12f63f785dbc89071d4272", "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"}, {"paperId": "aea9f18d70a78d39ca927f2baa143e084c486086", "title": "AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding"}, {"paperId": "1e4cda8be54999ced1324777fa462a85e2c9746c", "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "21d347db8adc949b0ad08e9c42b66a63739c866f", "title": "ParsiNLU: A Suite of Language Understanding Challenges for Persian"}, {"paperId": "092442a694b811dff5b7715fba9e363e0ce4108c", "title": "SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis"}, {"paperId": "5eea6a9de39c41715e105f5943ac0fcb98fa245c", "title": "Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base"}, {"paperId": "aa8f5faf8890b6846d4da9cb7e60a8df6e96ba4a", "title": "KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization"}, {"paperId": "5fe78eb0f142902237df11cb67c455787a759172", "title": "GLGE: A New General Language Generation Evaluation Benchmark"}, {"paperId": "70b0c85638d195dbde56cbedc94ae4363b272b58", "title": "A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"}, {"paperId": "b70ee890b5e0ec7d6db04cfd5471b3bbbd0320fc", "title": "Self-Supervised Learning from Contrastive Mixtures for Personalized Speech Enhancement"}, {"paperId": "a79b520571f7373cbeb8c6ffc02f6a719b3bce38", "title": "CODER: Knowledge-infused cross-lingual medical term embedding for term normalization"}, {"paperId": "96c22a88ec3b9d3799daa41098555ab665c24ea8", "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning"}, {"paperId": "17aa716dae728e994a2539bf4952c05ad513bd7a", "title": "CharBERT: Character-aware Pre-trained Language Model"}, {"paperId": "1109d62ebd2b29a7dc148bc30dd6cfc803a63dec", "title": "IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP"}, {"paperId": "d7b93f247f9e46115d6b78a67a458c44de0f9039", "title": "iNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages"}, {"paperId": "28ba41305e8268592ec829600f5a3b54cd10fbca", "title": "Learning from Unlabelled Data for Clinical Semantic Textual Similarity"}, {"paperId": "70efbd71c840e78d0ecee101d389db0aaea93652", "title": "exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources"}, {"paperId": "210cf704dddaa922e4eafe634dbabf707d6683bc", "title": "LIMIT-BERT : Linguistics Informed Multi-Task BERT"}, {"paperId": "302bb91ed02b90896e0ee78a80f303672d9c3b1b", "title": "RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark"}, {"paperId": "b68b2e81ae2de647394ec05ee62ecf108bf2b50a", "title": "Eliciting Knowledge from Language Models Using Automatically Generated Prompts"}, {"paperId": "9927a15ddf5313d97c98f0111fd191caf507ce72", "title": "HateBERT: Retraining BERT for Abusive Language Detection in English"}, {"paperId": "9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "615204452304331004532c5800399ef55d58b4c7", "title": "Self-Alignment Pretraining for Biomedical Entity Representations"}, {"paperId": "03935e520c612ac9f137d9e9ef388e0c08568b60", "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus"}, {"paperId": "5ad57623099f1fb6045a67fee313fee2573ef5ec", "title": "A Benchmark for Lease Contract Review"}, {"paperId": "1d95011355628f7aef068ab1914198e43258c530", "title": "BERTimbau: Pretrained BERT Models for Brazilian Portuguese"}, {"paperId": "473921de1b52f98f34f37afd507e57366ff7d1ca", "title": "CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"}, {"paperId": "708dcd8456426cd609c89a86344e0007c04c80bf", "title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models"}, {"paperId": "6d6595766a35f12a6ad671d05634b5e2159d4f3e", "title": "Bio-Megatron: Larger Biomedical Domain Language Model"}, {"paperId": "0c775d7ed34fb4690b4291490778649ae75c48d2", "title": "TurboTransformers: an efficient GPU serving system for transformer models"}, {"paperId": "1c6f94fb3d888167355afb580f04d55cd517ebc6", "title": "On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "2274b14cd3f513bee527a92f5859d14aea093aaa", "title": "DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "574c7c7260a795d2906f03b5229ace3f54ee0ab3", "title": "An Unsupervised Sentence Embedding Method by Mutual Information Maximization"}, {"paperId": "8b0a0f6d1cd6f3aa9b54be45d5127bb016a98171", "title": "The birth of Romanian BERT"}, {"paperId": "4083958684292f6fa2f5c7fd4f9be975e80145b6", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "03f22e693a0c00bae8a66a64a2fecb0f11a4b034", "title": "IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"}, {"paperId": "2b01b3334ce950c76c9c3c2c9146a7f0ce79cc50", "title": "Conceptualized Representation Learning for Chinese Biomedical Text Mining"}, {"paperId": "5b2ec7a534cab750c6b00fe491500681ae3b1527", "title": "PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data"}, {"paperId": "9baab08fbe37369856688b2abe5b3c90cce1682c", "title": "Compression of Deep Learning Models for Text: A Survey"}, {"paperId": "2a218786f4615b82389f78472e7ff22e6ce57490", "title": "ConvBERT: Improving BERT with Span-based Dynamic Convolution"}, {"paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"}, {"paperId": "16cca900a850eee1b832937281bb08c84beb86ff", "title": "Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "7c462df4adefcf90af3c27ce7f7a8d83efbff2b0", "title": "Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models"}, {"paperId": "09bfe057c9285577242636950c6835b8731a07fb", "title": "Multi-task learning for natural language processing in the 2020s: where are we going?"}, {"paperId": "4ceff7472c04ee6d76bce89d61ba4b445d8dbf74", "title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "214a0eede75f546b631d6d28871bd9028a66fc46", "title": "Playing with Words at the National Library of Sweden - Making a Swedish BERT"}, {"paperId": "c0091585ae4f9cccd4c1dba5aa7409c0886553fa", "title": "Transferability of Natural Language Inference to Biomedical Question Answering"}, {"paperId": "6150a2dab1b63b246eb2cd418fcdb5a6b6b8ae62", "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e", "title": "Self-Supervised Learning: Generative or Contrastive"}, {"paperId": "3578a7792904e6af3db8ffefdff86ab6a387c7c3", "title": "FinBERT: A Pretrained Language Model for Financial Communications"}, {"paperId": "8b9d77d5e52a70af37451d3db3d32781b83ea054", "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"}, {"paperId": "82453548b97f78ab2cdb9a8626ff858db9ce5a82", "title": "Pre-training Polish Transformer-based Language Models at Scale"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "be158d5ab493b2f2dae736a2ca92afcd66ed5be4", "title": "ParsBERT: Transformer-based Model for Persian Language Understanding"}, {"paperId": "5d4de0fa45aeddc31142e6a24666d06ed7923f1e", "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"}, {"paperId": "72cdd6ebe0221fb568ef20534f44ba5b35190a56", "title": "BERTweet: A pre-trained language model for English Tweets"}, {"paperId": "126fb7df6bcab2b70000dfe5b940ada63ae1ba6a", "title": "COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a", "title": "An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"}, {"paperId": "f527bcef68aeda601aae314fe5c75185c716e579", "title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding"}, {"paperId": "8ae8e5e840c5f43d4d72cbc4595690fba01aa799", "title": "LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"}, {"paperId": "c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c", "title": "Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"}, {"paperId": "673e970fd835c7dd1bb1e071c5a37e9df99b7c8e", "title": "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?"}, {"paperId": "11c226588d4d6a255967fe9c1cee92b081b405cd", "title": "Low Resource Multi-Task Sequence Tagging - Revisiting Dynamic Conditional Random Fields"}, {"paperId": "00cd2650a89734105fa0c0aba3bf07935b318290", "title": "GLUECoS: An Evaluation Benchmark for Code-Switched NLP"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "64b4d2c2181e3fb4ecacf797bf5f35db203c1437", "title": "MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "title": "Training with Quantization Noise for Extreme Model Compression"}, {"paperId": "5b015296730273921889e54a0a31e3b173017026", "title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue"}, {"paperId": "1c8aea2bfb61f4661b6907018a5a8bca390900dd", "title": "PALM: Pre-training an Autoencoding&autoregressive Language Model for Context-conditioned Generation"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "25a49187e0d1e3ebebda71c7e77f31bc49358044", "title": "Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "10467a1466aeec246ac0a577bfc311ec4de110de", "title": "Alternating Language Modeling for Cross-Lingual Pre-Training"}, {"paperId": "297ad41c0e7264e67ae078921e2a57436293ce72", "title": "XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"}, {"paperId": "dbfbfcc2633ef46c53e2343525ee87c700f2cfc3", "title": "Extending Multilingual BERT to Low-Resource Languages"}, {"paperId": "b2fd96a52ded7a64f60c1e54f5bb488c787629c0", "title": "What Happens To BERT Embeddings During Fine-tuning?"}, {"paperId": "ba4a34680e09e77984624c95f5245d91b54373f6", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286", "title": "A Survey on Contextual Embeddings"}, {"paperId": "0dde065405210ebc399c58ab6b7e843a18caad51", "title": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"}, {"paperId": "a622332550eaf535cf0f0f6c3a3f3ba197c39cac", "title": "PhoBERT: Pre-trained language models for Vietnamese"}, {"paperId": "501a8b86428563539667e8117cd8409674ef97c3", "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"}, {"paperId": "738215a396f6eee1709c6b521a6199769f0ce674", "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"}, {"paperId": "efe638a32c6bd9ad24a233784008bfe5b33cfc83", "title": "Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "1359d2ef45f1550941e22bf046026c89f6edf315", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding"}, {"paperId": "81ed0e757ae2d66a43d73407ad6f7e0359adf6d7", "title": "PMIndia - A Collection of Parallel Corpora of Languages of India"}, {"paperId": "dc5da5ac3aff86e4b0156c52d9641d05dc1eeace", "title": "MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "495da6f19baa09c6db3697d839e10432cdc25934", "title": "Multilingual Denoising Pre-training for Neural Machine Translation"}, {"paperId": "634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b", "title": "RobBERT: a Dutch RoBERTa-based Language Model"}, {"paperId": "e344ae0473651d04bf322849f1c71a5edc75f887", "title": "Modified Bidirectional Encoder Representations From Transformers Extractive Summarization Model for Hospital Information Systems Based on Character-Level Tokens (AlphaBERT): Development and Performance Evaluation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "a4d5e425cac0bf84c86c0c9f720b6339d6288ffa", "title": "BERTje: A Dutch BERT Model"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "477d66dcd2c08243dcc69822d6da7ec06393773a", "title": "Multilingual is not enough: BERT for Finnish"}, {"paperId": "069e0d896da7c79faeee4cf057548d5da7ce885e", "title": "FlauBERT: Unsupervised Language Model Pre-training for French"}, {"paperId": "04690b6e72d1d236d91196b93fe64a48f4666a52", "title": "Automated Brain Image Classification Based on VGG-16 and Transfer Learning"}, {"paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968", "title": "How Can We Know What Language Models Know?"}, {"paperId": "b61c6405f4de381758e8b52a20313554d68a9d85", "title": "CamemBERT: a Tasty French Language Model"}, {"paperId": "46b8201f1b84950f141cbbb5eeccaa1437159ff4", "title": "A Massive Collection of Cross-Lingual Web-Document Pairs"}, {"paperId": "2bd5b4aed18400bf1a1cc866d9b8d931aa047290", "title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"}, {"paperId": "68c1bf884f0fc0e86641466a1f1fa67e79f16a17", "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76", "title": "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"}, {"paperId": "4099c4d272c12081b562392606e6d567e4ae7031", "title": "Masked Language Model Scoring"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "12dc43176fd607557d6cc8d46af8b8d77c121b47", "title": "Domain-Relevant Embeddings for Medical Question Similarity"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "222b9a7b8038120671a1610e857d3edbc7ac5550", "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models"}, {"paperId": "aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47", "title": "Cross-Lingual Natural Language Generation via Pre-Training"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "1a00229c25dcc740fd0388ac1e98c42eaa52912e", "title": "Pre-trained Language Model for Biomedical Question Answering"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations"}, {"paperId": "e14c93e69cbf9645abb70cef09391f21f644d6d8", "title": "Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity"}, {"paperId": "65f788fb964901e3f1149a0a53317535ca85ed7d", "title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "7102bb3fe73bd057ff161d9db5214a267c1ef312", "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "3caf34532597683c980134579b156cd0d7db2f40", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP"}, {"paperId": "772717eb2e369cd68c11b7da7aa779450dced9d0", "title": "SenseBERT: Driving Some Sense into BERT"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "d56c1fc337fb07ec004dc846f80582c327af717c", "title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"}, {"paperId": "8aa1d9145640b4a63258b82bc8180c3683d072b5", "title": "KU_ai at MEDIQA 2019: Domain-specific Pre-training and Transfer Learning for Medical NLI"}, {"paperId": "1cd8167b2a6be4fdc0f2bfeec4e6f23a9bbb7090", "title": "Tuning Multilingual Transformers for Language-Specific Named Entity Recognition"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "57633ff5c6f0708be25e651f51eef29d2fbfe48b", "title": "BEHRT: Transformer for Electronic Health Records"}, {"paperId": "92343cecdc990380de362b969eec60081959f507", "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"}, {"paperId": "f48f90464d9694e2ea18767f14842c64c9a1e8fb", "title": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "347bac45298f37cd83c3e79d99b826dc65a70c46", "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "3b3f47170ec5c4fabac510585b33aeb87b384396", "title": "Variational Pretraining for Semi-supervised Text Classification"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "cc94d15ba408c260c8fe4fa4f1cb6797a996dd21", "title": "Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "145b8b5d99a2beba6029418ca043585b90138d12", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "162515d87256f13888d9d7ba95275ac4b6c35396", "title": "Combating Adversarial Misspellings with Robust Word Recognition"}, {"paperId": "a9c3a009d754a110379574b069b48c0b4c75db40", "title": "Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"}, {"paperId": "e61a3a5ba2b93458774f2ccbe480f3cf6cd74fa1", "title": "Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?"}, {"paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2", "title": "Publicly Available Clinical BERT Embeddings"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "06a1bf4a7333bbc78dbd7470666b33bd9e26882b", "title": "Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling"}, {"paperId": "b47381e04739ea3f392ba6c8faaf64105493c196", "title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "421fc2556836a6b441de806d7b393a35b6eaea58", "title": "Contextual String Embeddings for Sequence Labeling"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25", "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"}, {"paperId": "8c207ece66e0a63627869c49fb37c6811072539b", "title": "The brWaC Corpus: A New Open Resource for Brazilian Portuguese"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3", "title": "The IIT Bombay English-Hindi Parallel Corpus"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "786f95cada23d4639aa1a8b922cdb9fb9a9c03fa", "title": "Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b", "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning"}, {"paperId": "95cd83603a0d2b6918a8e34a5637a8f382da96f5", "title": "MIMIC-III, a freely accessible critical care database"}, {"paperId": "800366078f063a637e6a4880c0c49c217c7905ea", "title": "The United Nations Parallel Corpus v1.0"}, {"paperId": "36f652172792f8aab1cf3c4441a72a1bf79d17c8", "title": "Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"}, {"paperId": "9a501e501a60b431b6031f81dc2c19b390b0aff3", "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "title": "A Convolutional Neural Network for Modelling Sentences"}, {"paperId": "d770060812fb646b3846a7d398a3066145b5e3c8", "title": "Do Deep Nets Really Need to be Deep?"}, {"paperId": "032d67d27ecacbf6c5b82eb67e5d02d81fb43a7a", "title": "A Survey on Multi-view Learning"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "1b97b4623cf2f183340e548e0aa53abf0f2963d8", "title": "Representing General Relational Knowledge in ConceptNet 5"}, {"paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972", "title": "A Survey on Transfer Learning"}, {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725", "title": "Why Does Unsupervised Pre-training Help Deep Learning?"}, {"paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9", "title": "Model compression"}, {"paperId": "45a23651bcc5a6cc993d722e71b0d301a6dc9dee", "title": "Open Mind Common Sense: Knowledge Acquisition from the General Public"}, {"paperId": "a9693b7b57f203940889de6d3f979c70c09202ed", "title": "Shuffled-token Detection for Refining Pre-trained RoBERTa"}, {"paperId": "50068fbea4d1cafcf4c99873ab272c701c08dfcb", "title": "OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models"}, {"paperId": "76db3624f1278a4f4f1e69b343bfff7bba306d47", "title": "XLM-T: A Multilingual Language Model Toolkit for Twitter"}, {"paperId": "acf2dd4e2853f90832c01c556a2e716e7c720bc2", "title": "G ENIE A Leaderboard for Human-in-the-Loop Evaluation of Text Generation"}, {"paperId": "1554887c6bd76c443a477b27dbcab35877787b27", "title": "LightSeq: A High Performance Inference Library for Transformers"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "2ada5bea5e567313fdac9541725fac0cdc49bc36", "title": "BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding"}, {"paperId": "b9478e237b58160c65acd2c41894493d27e2c277", "title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"}, {"paperId": "cbd78779af4e83fe101ba3f7ba4d4786388d12d8", "title": "Semantic Re-tuning with Contrastive Tension"}, {"paperId": "5322e5936e4a46195b1a92001467a2350fe72782", "title": "KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records"}, {"paperId": "2af4e8d14d03cd9031ae4a6b1ef39fce2ab3f504", "title": "NetBERT: A Pre-trained Language Representation Model for Computer Networking"}, {"paperId": null, "title": "\u201cLegal-bert: The muppets straight out of law school,\u201d"}, {"paperId": "888c3a3788c52d6637d45dc4238691083884589d", "title": "Investigating Learning Dynamics of BERT Fine-Tuning"}, {"paperId": "17d5884215b5afa53545cd7cb6135de5478da4ec", "title": "CERT: Contrastive Self-supervised Learning for Language Understanding"}, {"paperId": "5c5751d45e298cea054f32b392c12c61027d2fe7", "title": "S2ORC: The Semantic Scholar Open Research Corpus"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Electra: Pretraining text encoders as discriminators rather than generators"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "2a0870bc2ecfd17dfb1b96cc34613bb73bb4506a", "title": "BLACK BOX ATTACKS ON TRANSFORMER LANGUAGE MODELS"}, {"paperId": "e1e43d6bdb1419e08af833cf4899a460f70da26c", "title": "AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets"}, {"paperId": null, "title": "\u201cBertviz: A tool for visualizing multihead self-attention in the bert model,\u201d"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cWordnet,\u201d"}, {"paperId": "1f1eaf19e38b541eec8a02f099e3090536a4c936", "title": "The Unified Medical Language System (UMLS): integrating biomedical terminology"}, {"paperId": null, "title": "Bert and pals : Projected attention layers for efficient adaptation in multitask learning"}, {"paperId": null, "title": "Sentilare: Sentimentaware language representation learning with linguistic knowledge"}, {"paperId": null, "title": "\u201cUmlsbert: Clinical domain knowledge augmentation of con-38"}]}