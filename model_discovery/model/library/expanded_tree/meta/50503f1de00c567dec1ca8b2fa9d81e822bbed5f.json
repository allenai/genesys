{"paperId": "50503f1de00c567dec1ca8b2fa9d81e822bbed5f", "abstract": "As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.", "venue": "arXiv.org", "year": 2024, "citationCount": 6, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper aims to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer, and identifies a class of DP problems for which these models can be more efficient than the standard Transformer."}, "embedding": {"model": "specter_v2", "vector": [0.30384719371795654, 0.6144521832466125, -0.4124823212623596, -0.2791813611984253, -0.35895317792892456, -0.6150239706039429, 0.5848712921142578, 0.14033770561218262, -0.288303941488266, -0.3976048231124878, 0.20366182923316956, -0.4261639714241028, 0.08324030041694641, -0.11671538650989532, -0.2789889872074127, 0.1229652538895607, -0.5724306702613831, 0.8439135551452637, -0.022266974672675133, -0.4941134452819824, -0.22655247151851654, -0.3665388226509094, -1.0534707307815552, 0.2475709319114685, 0.42244258522987366, 0.4071577191352844, -0.041444502770900726, 0.563633143901825, -0.36213696002960205, 0.6190652251243591, 0.6465363502502441, -0.32219192385673523, 0.11223436892032623, 0.6507863998413086, -0.403735876083374, -0.18012045323848724, 0.2420371174812317, -0.6123984456062317, -0.4940292537212372, 0.6868014931678772, -0.34033846855163574, 0.17199300229549408, 0.19947165250778198, -1.2521100044250488, -0.42922210693359375, 0.8978856205940247, 0.7462209463119507, 0.8859230279922485, 0.0750114843249321, -0.47932401299476624, 1.9526501893997192, -1.0066165924072266, 0.3042496144771576, 1.3816620111465454, 0.2345743626356125, 0.5894750356674194, -0.6543944478034973, -0.4684484601020813, 0.8354372382164001, -0.14225314557552338, -0.7511426210403442, -0.421654611825943, -0.21794739365577698, -0.013927130028605461, 2.136636972427368, -0.32703033089637756, -0.003529108827933669, 0.3120808005332947, -0.1161336675286293, 1.683760643005371, 0.3863452970981598, -0.9281092882156372, -0.2593146860599518, 0.1256013810634613, 0.39047887921333313, 0.9169485569000244, -0.4651353061199188, 0.32176753878593445, -1.16400945186615, -0.2050049602985382, 0.3046695590019226, 0.08919869363307953, -0.24153250455856323, -0.3726404309272766, -0.012991512194275856, 0.9198173880577087, -0.0576859675347805, 0.9524983763694763, -0.05803675204515457, 0.48898935317993164, 0.4701596200466156, 0.5251585245132446, -0.06789541989564896, 0.563940167427063, -0.4179573357105255, 0.2651323080062866, -0.7467207908630371, 0.6722529530525208, 0.3512728810310364, 0.7592766880989075, -0.48136910796165466, -0.02491985633969307, -0.7968948483467102, 0.04271969944238663, 1.1979098320007324, 0.330263614654541, 0.11209002882242203, -0.8812993168830872, 0.3128942847251892, -0.4703385531902313, 0.3614092767238617, -0.4822186231613159, -0.3061925172805786, 0.16570480167865753, -0.10779702663421631, -1.618151068687439, -0.052057236433029175, 0.20023074746131897, -0.6899078488349915, 0.6482226848602295, -0.4094086289405823, -0.42249956727027893, 0.001253273687325418, 0.4460364282131195, 0.5195509195327759, 0.7953169345855713, 0.31344783306121826, -0.2617480158805847, 0.8452829718589783, -0.31657230854034424, -0.8979103565216064, -1.4277433156967163, 0.9166727662086487, -0.012478441931307316, 0.43577396869659424, -0.10869213938713074, -1.0320967435836792, -0.48732757568359375, -0.6404998302459717, -0.02454063855111599, -0.45099741220474243, -0.10279955714941025, 1.399156093597412, 0.5576634407043457, -1.2291263341903687, 0.28415966033935547, -0.29085272550582886, -0.1143350899219513, 0.056607216596603394, 0.35932981967926025, 0.3459841012954712, -0.8401395082473755, -1.2999745607376099, 0.3152916133403778, 0.1352008730173111, -0.6584120988845825, -0.006111515685915947, -0.5263501405715942, -1.3970452547073364, 0.06864746659994125, 0.4844359755516052, -0.299892783164978, 1.7593895196914673, 0.1907898336648941, -1.1224172115325928, 0.6916162371635437, -0.3050428032875061, 0.10326237976551056, -0.04522491246461868, -0.2668871283531189, -0.417601078748703, -0.3977952301502228, 0.08675795793533325, 0.46511876583099365, 0.4205561578273773, -0.08722128719091415, -0.5662739276885986, 0.12946933507919312, 0.21834169328212738, -0.20935344696044922, 0.19039398431777954, 1.2874442338943481, -0.013876576907932758, 0.09422869235277176, 0.2792294919490814, 0.5879810452461243, -0.059672098606824875, -0.11536239087581635, -0.11765556037425995, -1.3146001100540161, 0.5488101243972778, 0.08278205245733261, 1.254205346107483, -0.8687182664871216, -0.6782189011573792, -0.2830963134765625, -0.0447043813765049, -0.3550097644329071, -0.4366459846496582, 0.9095957279205322, -0.7960832715034485, 0.3620751202106476, -0.13721369206905365, -0.5983796119689941, 0.5066349506378174, 0.2741954028606415, -1.0672212839126587, -0.10130145400762558, -0.011284234002232552, 0.8707958459854126, -0.8381630182266235, -0.01920492760837078, 0.22446012496948242, -0.00475445669144392, -1.0385808944702148, 1.3279404640197754, -0.5714545845985413, -0.15729883313179016, -0.08110128343105316, -0.07700201869010925, 0.08314675837755203, -0.5258108377456665, 0.6698787212371826, -0.35748985409736633, -0.07959945499897003, 0.37664440274238586, -0.38953566551208496, 1.0448154211044312, -0.2521750032901764, 0.2798132598400116, -0.08325677365064621, -0.718662679195404, -0.19885548949241638, 0.4355286657810211, -0.17742380499839783, -0.591877818107605, 0.06789689511060715, 0.6053189635276794, -0.4070737957954407, 0.25417351722717285, 0.5178247690200806, 0.49151402711868286, -0.5548669695854187, 0.3723335564136505, 0.22522422671318054, -0.7433064579963684, 0.7321990728378296, 0.4092034697532654, 0.7796213626861572, 0.28572723269462585, 0.4178568422794342, 0.00759767135605216, 0.32024097442626953, -0.433763325214386, -0.046868275851011276, 0.42220839858055115, 1.0196415185928345, 0.5207239985466003, 0.6184604167938232, -0.798919141292572, -0.1256393939256668, 0.15908005833625793, 0.5389488339424133, 2.014733076095581, -0.09274733066558838, -0.5955215692520142, -0.3941551744937897, -0.42626726627349854, -0.48170119524002075, 0.5608013272285461, 0.03161342069506645, 0.09318843483924866, -0.8968903422355652, -0.8841845989227295, 0.7102654576301575, 0.6504504084587097, 0.6445943713188171, -0.19828882813453674, -0.500025749206543, -0.06647968292236328, -0.03958024084568024, -0.835519552230835, -0.32076987624168396, 0.787713885307312, -0.7403411269187927, -0.45118093490600586, 0.2980867922306061, -0.09670859575271606, 0.33013394474983215, -0.8502253890037537, 0.801826000213623, -0.5506484508514404, 0.012777948752045631, 0.5202404856681824, 0.7030317783355713, -0.25187671184539795, -0.6132915019989014, -0.19255515933036804, -0.011264549568295479, -0.5659151673316956, 0.5899519324302673, 0.3704870939254761, 0.3115553855895996, 0.11969459056854248, -0.4553123116493225, 0.10161511600017548, 0.1025538519024849, 0.24605347216129303, 0.7411831617355347, -0.006819929461926222, -0.674443781375885, -1.478109359741211, 1.0679911375045776, 0.33990123867988586, -0.7016871571540833, 0.4618583917617798, -1.1619702577590942, -0.0408935546875, 0.5545595288276672, -0.7000460028648376, -0.08003328740596771, -0.5510126352310181, 0.5341311097145081, -0.18148182332515717, -0.020582180470228195, 0.3312721848487854, 0.16362811625003815, 0.25202420353889465, 0.060776907950639725, 0.641510009765625, -0.011869896203279495, -0.08890238404273987, 0.6985800862312317, -0.9489009976387024, 0.13583381474018097, -0.29361391067504883, 0.3255564272403717, -0.5594879388809204, -0.2738645374774933, -0.3134876787662506, -0.5027168393135071, -0.09269482642412186, 0.38417360186576843, -0.0022134296596050262, -0.2353648990392685, -0.5281410217285156, -1.0321887731552124, 0.013426135294139385, -1.2083237171173096, -0.2852639853954315, -0.034035030752420425, -0.7551910281181335, -0.2120550572872162, -1.0758429765701294, -1.5681730508804321, -0.37078291177749634, -0.524340808391571, -0.8540930151939392, 0.8007892966270447, 0.005982313770800829, -0.2974730134010315, -0.28032466769218445, -0.052889250218868256, -0.20928232371807098, 1.0104401111602783, -0.7560595870018005, 1.6234757900238037, -0.4506745934486389, -0.5335943102836609, -0.03863328695297241, 0.22863130271434784, 0.05451878160238266, -0.5177900791168213, 0.43480220437049866, -0.7673807740211487, 0.10612282902002335, -0.15898452699184418, -0.2180119901895523, -0.46197429299354553, 0.4016331136226654, 0.5438448786735535, -0.45083343982696533, -0.7059839367866516, -0.1001759022474289, 1.3862812519073486, -0.37766778469085693, -0.02587595395743847, 0.3576517701148987, 0.8230060338973999, 0.3614634573459625, 0.07872363179922104, 0.4587397277355194, 0.6919585466384888, 0.27782130241394043, 0.0319485068321228, 0.41710755228996277, 0.5714280605316162, -0.5090832710266113, 0.479586660861969, 1.4182112216949463, 0.2623044550418854, -0.16069087386131287, -1.3701934814453125, 0.4621641933917999, -1.4356868267059326, -0.23467975854873657, 0.6012057065963745, 0.5765721797943115, 0.6371942758560181, -0.33076322078704834, -0.34700289368629456, 0.15709631145000458, 0.3962760269641876, 0.49836766719818115, -0.15483762323856354, -0.8772388696670532, 0.30604586005210876, 0.9010424017906189, 0.2581501007080078, 0.4297294616699219, -0.32515236735343933, 0.5631646513938904, 14.857816696166992, 0.8605437278747559, -0.04619084671139717, 0.543383777141571, 0.2836630940437317, 0.5423884987831116, -0.5479016304016113, -0.08116140961647034, -1.0887439250946045, -0.33954209089279175, 1.1237354278564453, -0.08242245763540268, 0.613767683506012, 0.15640513598918915, -0.37457919120788574, 0.008089093491435051, -0.8877179026603699, 0.733379065990448, 0.07108324766159058, -0.9633703231811523, 0.2816496789455414, 0.06733890622854233, -0.23868180811405182, -0.038535237312316895, 0.6309530138969421, 0.9302306771278381, 0.8265848755836487, -0.5184169411659241, 0.5355718731880188, -0.2102462649345398, 0.9981272220611572, -0.28036588430404663, 0.4544222950935364, 0.603171706199646, -0.8000689148902893, -0.6053934693336487, -0.16915076971054077, -1.271111249923706, -0.00932361651211977, 0.10887593030929565, -0.402850866317749, -0.7201825380325317, -0.26279255747795105, 0.5389655232429504, 0.26331785321235657, 0.1459195762872696, -0.443352073431015, 0.652477502822876, -0.5136076807975769, 0.16050662100315094, 0.004896652419120073, 0.6678085327148438, 0.1351962387561798, -0.39734917879104614, 0.0401901975274086, 0.23796769976615906, 0.23106659948825836, 0.18211066722869873, -0.08579237014055252, -0.09226927161216736, -0.37745723128318787, -0.6450387239456177, 0.271679550409317, 0.6093047261238098, 0.1475440412759781, 0.6406693458557129, -0.3225249648094177, -0.05261775478720665, 0.5178859829902649, -0.1287369579076767, 0.012607001699507236, 0.07785428315401077, 0.18911120295524597, -0.2533809542655945, 0.046874500811100006, 0.531228244304657, 0.2031392902135849, -0.48157933354377747, -0.8457947969436646, -0.8206730484962463, 0.7994434237480164, -0.6343221068382263, -0.8115007281303406, 0.4576081931591034, -0.0018174592405557632, -0.3733557462692261, -0.0555211678147316, -0.6929928064346313, 0.07052184641361237, -0.2150491625070572, -1.129716396331787, -0.9014819264411926, 0.5681473016738892, -0.022366827353835106, -0.4590870141983032, 0.580597460269928, 1.477981686592102, 0.051436178386211395, -0.2652300000190735, -0.193085178732872, -0.06504938006401062, -0.04198357090353966, -0.2743678092956543, -1.2454235553741455, 0.6000522971153259, 0.3465725779533386, 0.19241861999034882, 0.5561243891716003, 0.548221230506897, 0.23560674488544464, -1.129525065422058, 0.3201982080936432, 1.250422716140747, -1.030011534690857, -0.20527835190296173, -0.7960270047187805, -0.6446658968925476, 0.4452632963657379, 0.15978631377220154, -0.26554539799690247, 0.24421294033527374, 0.14906947314739227, -0.6598557233810425, -0.3407154977321625, -1.042394995689392, -0.18253237009048462, 0.7219942808151245, -0.9760546088218689, -0.6459763646125793, 0.018144328147172928, 0.4302716851234436, -1.408300518989563, -0.531925618648529, -0.3993667960166931, 0.030891021713614464, 0.2919074594974518, 0.695787250995636, -0.23924417793750763, 0.722965657711029, 0.6399432420730591, -0.40009811520576477, -0.7010046243667603, -0.0024865244049578905, -0.9543182849884033, -0.16854876279830933, 0.11705195903778076, 0.54981529712677, -0.6573123931884766, -0.027612727135419846, 1.58659827709198, 0.59751957654953, -0.2714415192604065, -0.6050871014595032, -0.12644194066524506, -0.23194384574890137, -0.5283788442611694, 0.5253490805625916, -0.20938192307949066, -0.0002637769503053278, 0.3497996926307678, 0.7082473039627075, 0.5058220028877258, -0.22086498141288757, -0.23317691683769226, -0.0009692449821159244, -0.3653270900249481, 0.005584906321018934, -0.09302228689193726, -0.2333739995956421, -1.3594930171966553, 0.14501675963401794, -0.9183855056762695, 0.2612285017967224, -1.1735289096832275, -0.6347351670265198, -0.08267438411712646, 0.10900475084781647, 0.3427695035934448, 0.6946293115615845, -0.8774448037147522, -0.540244460105896, -0.6448314785957336, -0.396931916475296, 0.4799228608608246, 0.6051080822944641, -0.7900739312171936, 0.41724100708961487, 0.0949031412601471, -0.17845191061496735, 0.16218551993370056, 0.05168948695063591, -0.6372153759002686, -0.8329586386680603, -1.1520698070526123, 0.8488750457763672, 0.11637775599956512, -0.346564918756485, -0.025538455694913864, 0.8584241271018982, 0.11501162499189377, -0.5173384547233582, 0.35775062441825867, 0.30120375752449036, -1.3885189294815063, -0.37268486618995667, 0.24644522368907928, -0.6652495861053467, 0.3914993107318878, 0.32063615322113037, -0.45151299238204956, -0.3860655426979065, 0.6806671619415283, -0.07239038497209549, -0.9149322509765625, -0.72225421667099, 0.3398118317127228, -1.026151180267334, 0.23083311319351196, -0.23877055943012238, -0.25272515416145325, -1.19557785987854, -0.07694805413484573, 0.10435501486063004, 0.42548781633377075, -0.309175968170166, 0.9529444575309753, 0.2779669463634491, -0.7497214674949646, 0.5421473979949951, 0.22788560390472412, -0.02757040224969387, 0.3340953290462494, 0.13657917082309723, 0.19422951340675354, -0.4221920967102051, 0.7359476685523987, 0.18142803013324738, 0.2573709487915039, -1.1149746179580688, -0.03403116762638092, 0.7578722834587097, -0.6180813908576965, -0.32385092973709106, 1.1443836688995361, -0.32240667939186096, -1.003206729888916, 0.09693969041109085, -1.6955897808074951, -0.5222917795181274, -0.44384458661079407, 0.31779181957244873, -0.05471445620059967, -0.08235730975866318, 0.24336101114749908, -0.915428876876831, 0.23186549544334412, -0.06014236807823181, -0.3050088882446289, 0.8986560702323914, -0.08976271748542786, -0.725796103477478, 0.41550835967063904, 0.15013478696346283, -0.46438831090927124, -0.16021990776062012, -0.722700834274292, -0.03815624862909317, -0.2541724145412445, 0.3260953426361084, -0.5397294759750366, -0.6185013651847839, 0.6913979649543762, 0.6066169142723083, 0.4065752625465393, 0.23971858620643616, -0.11097901314496994, 0.1408669501543045, 0.9095955491065979, 0.6616946458816528, -0.5811110138893127, -0.9673594236373901, 1.1114399433135986, 1.0555598735809326, -0.5514337420463562, 0.2308768928050995, -0.37579458951950073, -0.49648526310920715, 0.9246062636375427, 0.39895099401474, 0.1615290641784668, 0.8049759864807129, 0.24172179400920868, -0.15558940172195435, -0.253767728805542, -1.289719820022583, -0.06535395979881287, 0.4015164077281952, 1.2077337503433228, 0.6005838513374329, 0.4124433100223541, -0.033218346536159515, 1.1788535118103027, 0.08256444334983826, 0.6110327839851379, 0.4676640033721924, 1.0064080953598022, -0.08637755364179611, -0.4125000536441803, 0.18487811088562012, 0.7984393239021301, -0.8129335045814514, -0.8268241286277771, 0.215424582362175, 0.9995787739753723, 0.1647420972585678, 0.5394189357757568, 0.4568978250026703, 0.21523866057395935, 0.632337212562561, 0.3258172571659088, 0.9613628387451172, -0.762708842754364, -0.4159652888774872, -0.4711698889732361, -0.19222044944763184, -0.10853829234838486, -0.3702331781387329, -0.29405245184898376, -0.34363171458244324, -0.42765480279922485, 0.03481403365731239, 0.1425761580467224, -0.04554973170161247, 0.9759811162948608, 0.274341344833374, -0.0554104708135128, -0.3615144193172455, -0.0355224646627903, -0.48892053961753845, -0.49709072709083557, -0.18482254445552826, -1.1565722227096558, -0.535722017288208, -0.15542303025722504, -0.33375874161720276, -0.38478097319602966]}, "authors": [{"authorId": "2281776780", "name": "Kai Yang"}, {"authorId": "2284871344", "name": "Jan Ackermann"}, {"authorId": "2266802709", "name": "Zhenyu He"}, {"authorId": "2205796820", "name": "Guhao Feng"}, {"authorId": "1988294358", "name": "Bohang Zhang"}, {"authorId": "2285593979", "name": "Yunzhen Feng"}, {"authorId": "2279751598", "name": "Qiwei Ye"}, {"authorId": "2266469594", "name": "Di He"}, {"authorId": "2257314569", "name": "Liwei Wang"}], "references": [{"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "f2f68ed280d27bd25d61782224f8a465db8f43bd", "title": "Sumformer: Universal Approximation for Efficient Transformers"}, {"paperId": "42cf52baff90952944da0409ec52ff7611ed55dc", "title": "Representational Strengths and Limitations of Transformers"}, {"paperId": "c2260403fd5cb2de73491323433e48b6ec36872c", "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "69c85405cc1986a41f6387d869aa1648a5668d6f", "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "title": "Transformers Learn Shortcuts to Automata"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "ac2e15fbfe3ea338725f5d33d17a5a687609c431", "title": "On The Computational Complexity of Self-Attention"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "955191363c3676f71766af3d14d1e6bbc0f040d6", "title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "5437e8adab596d7294124c0e798708e050e25321", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"}, {"paperId": "216b05e812896e790d5b5a084614e2523daa198e", "title": "Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a", "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"}, {"paperId": "349eb17c5b61924db8ccc5816c863c6674c8b565", "title": "Saturated Transformers are Constant-Depth Threshold Circuits"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea", "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "10c86505de83647c7b4157595ab10f64e97c94ef", "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d", "title": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f", "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "3694381e74445a8b9f8cb8d373e39626e47191b5", "title": "On the Turing Completeness of Modern Neural Network Architectures"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "title": "Fixing Weight Decay Regularization in Adam"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "75f57a8e71ebc9f05b6c9983032a75fc1edb1597", "title": "Parity, circuits, and the polynomial-time hierarchy"}, {"paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922", "title": "Attention is Turing-Complete"}, {"paperId": null, "title": "Linformer"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Fast trans-formers with clustered attention"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}]}