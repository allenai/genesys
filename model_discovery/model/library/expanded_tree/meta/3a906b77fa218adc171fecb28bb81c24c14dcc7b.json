{"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.", "venue": "ACM Computing Surveys", "year": 2021, "citationCount": 1728, "influentialCitationCount": 39, "openAccessPdf": {"url": "https://arxiv.org/pdf/2101.01169", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding."}, "embedding": {"model": "specter_v2", "vector": [0.7306454181671143, 0.43502622842788696, 0.1663842499256134, -0.09542658925056458, -0.29978930950164795, 0.11250530183315277, 0.7659407258033752, -0.30550897121429443, -0.32304662466049194, -0.7366185784339905, 0.139085054397583, 0.687279224395752, 0.4628960192203522, -0.1311369389295578, -0.3505077064037323, -0.12344478815793991, -0.6686432361602783, 0.17149722576141357, 0.7985007762908936, -0.4076292812824249, -0.03907323628664017, -0.26786255836486816, -1.554154634475708, 0.3811590373516083, 0.04441959038376808, 0.9072210192680359, 0.2356860190629959, 0.9432967901229858, -0.12501631677150726, 0.8006109595298767, 0.6649182438850403, -0.06510695070028305, 0.23239165544509888, -0.04114682599902153, -0.40334564447402954, 0.14754438400268555, 0.7325493693351746, -0.19348587095737457, -0.9317339062690735, 0.8277907967567444, -0.5190113186836243, 0.30825796723365784, 0.5763738751411438, -1.0409603118896484, -0.31362590193748474, 0.33954086899757385, 0.8359267115592957, 0.970323920249939, -0.5494722127914429, -0.5464496612548828, 1.4731183052062988, -1.2956138849258423, -0.09887516498565674, 1.6654936075210571, 0.6171311736106873, 0.3587602972984314, -0.3397657871246338, -0.2985993027687073, 0.9988288283348083, 0.3332131803035736, -0.529653787612915, -0.39588141441345215, 0.24084025621414185, -0.24878272414207458, 1.7528107166290283, -0.37509045004844666, 0.5160836577415466, 0.4845418334007263, 0.11166144162416458, 1.3437702655792236, 0.19439493119716644, -0.8089423179626465, -0.13251952826976776, -0.18119299411773682, 0.389398455619812, 1.1162710189819336, -0.6098117232322693, 0.3000856637954712, -1.0767909288406372, 0.2475268542766571, 0.781603991985321, 0.3133123815059662, 0.17810150980949402, -0.43059268593788147, -0.1558484584093094, 0.7039559483528137, 1.0124833583831787, 0.48292168974876404, -0.47448402643203735, 1.1261897087097168, 0.2999860942363739, 0.09592081606388092, -0.33755654096603394, 0.08266565948724747, 0.4169383943080902, 0.913007378578186, -0.5952900052070618, 0.11814353615045547, -0.26621249318122864, 0.8240857720375061, 0.028186488896608353, 0.4541804790496826, -0.6963833570480347, 0.30469459295272827, 1.4477956295013428, 0.1792583018541336, 0.1642289012670517, -0.7524989247322083, -0.2035813331604004, -0.5792920589447021, -0.04969620332121849, -0.8140529990196228, 0.11190099269151688, -0.411211222410202, -0.531011700630188, -0.7709682583808899, -0.2021944522857666, 0.8159838318824768, -1.3077784776687622, 0.5311648845672607, -0.6856354475021362, 0.1995965540409088, -0.238957017660141, 0.49714070558547974, 0.04178500548005104, 0.5709855556488037, 0.5463184118270874, 0.41135504841804504, 1.0901095867156982, -0.8428364396095276, -0.24976910650730133, -0.9611659646034241, -0.22747959196567535, 0.12847192585468292, 0.3005160093307495, -0.23184798657894135, -0.9436002373695374, -1.3353482484817505, -0.6834235787391663, -0.0519401952624321, -0.7278118133544922, 0.053277187049388885, 1.0092638731002808, 0.15651455521583557, -1.335429310798645, 0.7706174254417419, -0.3825410008430481, -0.37512004375457764, 0.8649899363517761, 0.38414865732192993, 0.5189096927642822, -0.2563193440437317, -0.9226678609848022, 0.36330094933509827, -0.2500458061695099, -0.30896511673927307, -0.8988115191459656, -0.1883625090122223, -1.1466418504714966, 0.2960544526576996, 0.07237409800291061, -0.690186083316803, 1.1674569845199585, -0.6322430372238159, -0.6886667013168335, 0.7072315812110901, -0.6495181322097778, -0.28293535113334656, 0.00832443218678236, -0.36751386523246765, 0.045377954840660095, -0.1373603194952011, -0.1833232194185257, 0.6433603763580322, 0.9198296666145325, -0.37523698806762695, -0.4978312849998474, -0.03673899546265602, -0.28555411100387573, -0.07271523028612137, -0.3072296380996704, 0.9498695731163025, -0.5408202409744263, -0.00952062476426363, 0.5824928283691406, 0.5600624084472656, -0.29408127069473267, -0.16998730599880219, 0.18332503736019135, -1.1946243047714233, 0.7285774350166321, 0.3164696991443634, 0.27767711877822876, -0.913162887096405, -0.806511640548706, -0.1255473643541336, 0.04717235267162323, -0.401328444480896, -0.860417366027832, 0.41710275411605835, -0.5160059332847595, -0.0013982432428747416, -0.10932022333145142, -0.7532384991645813, -0.09925667196512222, 0.028605099767446518, -0.9336898326873779, 0.3050292134284973, 0.6097774505615234, 1.3900959491729736, -0.9481984376907349, -0.0557369627058506, 0.34826910495758057, 0.235279843211174, -0.5482288599014282, 0.9352886080741882, -0.30532318353652954, -0.4475080966949463, -0.04801388829946518, 0.10010547935962677, -0.21215219795703888, -0.4534931182861328, 0.17981168627738953, -0.6577765941619873, 0.053681399673223495, 0.28795844316482544, -0.061198651790618896, 1.0229378938674927, 0.1186036542057991, 1.032414436340332, -0.5744152069091797, -1.0191806554794312, 0.33276239037513733, 0.25481751561164856, 0.04471219703555107, -0.6851372718811035, 0.3345666229724884, -0.12443330883979797, -0.8066725134849548, 0.3629383444786072, 0.5749013423919678, 0.9014699459075928, -0.1708841472864151, -0.156898632645607, 0.898554801940918, -0.40875670313835144, 0.34220778942108154, 0.5396537780761719, 0.5661792159080505, 0.5417119264602661, -0.09529483318328857, -0.11246410012245178, -0.0061707827262580395, -0.8708907961845398, -0.06476185470819473, 0.7813303470611572, 0.4316413402557373, 1.2839088439941406, 0.39032623171806335, -0.8827263116836548, -0.5816177129745483, -0.1706005185842514, 0.823772668838501, 1.3466205596923828, -0.05737588554620743, -0.06307036429643631, -0.7825530767440796, -0.21683718264102936, -0.6172415614128113, -0.44478365778923035, -0.7898725867271423, -0.1522292196750641, -0.24022161960601807, -0.8929803371429443, 0.7097669839859009, 0.8845563530921936, 1.0515596866607666, -0.6944806575775146, -0.547379195690155, -0.29323819279670715, 0.3565264046192169, -0.7904523015022278, -0.5497518181800842, 0.5531770586967468, -0.5290749073028564, -0.46595776081085205, -0.04758553206920624, -0.5038055777549744, 0.4286253750324249, -0.2294389009475708, 0.8827871680259705, -0.577502429485321, -0.6928094625473022, 0.47988948225975037, 0.8218192458152771, -0.9613491892814636, -0.07560046017169952, -0.2828264534473419, -0.1326889991760254, 0.21032603085041046, 0.5665507912635803, 0.17945913970470428, 0.032021477818489075, 0.19009257853031158, -0.26766490936279297, -0.18256698548793793, 0.03755274415016174, 0.2116249054670334, 1.0324115753173828, -0.3414130210876465, 0.03349251672625542, -0.9145892262458801, 0.7385446429252625, 0.4597494304180145, -0.6779538989067078, 0.3521316349506378, -0.5207406282424927, -0.2853638529777527, 0.3002983629703522, -0.5618374347686768, -0.07524723559617996, -0.026175152510404587, 0.6106011271476746, -1.0469400882720947, -0.23462797701358795, -0.22575122117996216, 0.562740683555603, -0.10571601986885071, 0.40703579783439636, 0.5720067620277405, 0.22491054236888885, 0.48371249437332153, 0.36572495102882385, -0.9659231305122375, 0.8474352359771729, 0.3418405055999756, 0.09829694777727127, 0.21717359125614166, 0.028006993234157562, -0.9449642300605774, -0.5034134387969971, -0.7253511548042297, -0.23330263793468475, -0.8399109840393066, 0.20355646312236786, -0.709359884262085, -1.0685275793075562, 0.3909336030483246, -0.8658149242401123, -0.16321414709091187, -0.236474871635437, -0.585565447807312, -0.5629348158836365, -1.1604090929031372, -0.8890631794929504, -0.7453312873840332, -0.368993878364563, -0.6811736226081848, 0.20581522583961487, 0.6897287964820862, 0.16236154735088348, -0.4100464880466461, -0.14566461741924286, -0.5393204689025879, 0.7256548404693604, -0.15546564757823944, 0.47125720977783203, 0.028012413531541824, -0.5051577091217041, 0.05799952894449234, 0.014137689024209976, 0.5758013129234314, -0.20416192710399628, 0.2945804297924042, -1.4197425842285156, 0.3880099356174469, -0.31651633977890015, -0.3499344289302826, 0.7728425860404968, 0.4659280776977539, 0.6225963234901428, 0.31444570422172546, -0.3780670166015625, 0.12584704160690308, 1.4833279848098755, -0.387460321187973, 0.32591596245765686, 0.07722005993127823, 0.8410174250602722, 0.019083349034190178, -0.39879992604255676, 0.22287024557590485, 0.3034507632255554, -0.07654566317796707, 0.6694309711456299, -0.3340480625629425, -0.5262134671211243, -0.8275245428085327, 0.2495066225528717, 0.6596172451972961, -0.01751948520541191, 0.19074025750160217, -1.0635894536972046, 1.1869182586669922, -1.3552186489105225, -0.8950474262237549, 0.651313304901123, 0.43928462266921997, -0.140107661485672, -0.20229658484458923, -0.1351158320903778, -0.23025323450565338, 0.7147588133811951, 0.2878284752368927, -0.4721660614013672, -0.35670846700668335, -0.2955785095691681, 0.6100763082504272, 0.2385440468788147, 0.4878252148628235, -0.7358527779579163, 0.5669718980789185, 14.782527923583984, 0.4024943709373474, -0.34927740693092346, 0.4175684452056885, 0.633686900138855, 0.5137726664543152, -0.31004685163497925, 0.243050217628479, -0.9247841835021973, -0.36987656354904175, 0.8464410901069641, 0.45053818821907043, 0.5625870227813721, 0.3500349819660187, -0.3229639232158661, 0.36158570647239685, -0.7921609282493591, 1.0921632051467896, 0.6249945163726807, -1.2583715915679932, 0.33437344431877136, 0.08639253675937653, 0.09780798107385635, 0.6996251940727234, 0.9467034339904785, 0.6004849076271057, 0.593808650970459, -0.5273101925849915, 0.7137314677238464, 0.4725671708583832, 0.6545495390892029, 0.35948705673217773, -0.09343450516462326, 0.14178304374217987, -1.261001706123352, -0.5478458404541016, -0.721145510673523, -0.8809800744056702, -0.011458978056907654, -0.14252059161663055, -0.6496835947036743, -0.6418104767799377, 0.13468043506145477, 0.9710823893547058, -0.09358955919742584, 0.45953214168548584, -0.22123943269252777, 0.522853434085846, -0.39341598749160767, -0.02810039557516575, 0.362577348947525, 0.5401642322540283, -0.020290886983275414, -0.08395665138959885, -0.2535104751586914, 0.3345719873905182, 0.23829782009124756, 0.4731207489967346, -0.4126870036125183, -0.558249294757843, -0.30885419249534607, -0.1732398122549057, -0.45427262783050537, 0.8107501864433289, -0.05517726019024849, 0.3473397493362427, -0.18270039558410645, 0.13860180974006653, 0.2334032505750656, 0.4137939214706421, -0.3969310224056244, -0.20201076567173004, 0.11625922471284866, -0.1848880499601364, 0.570405900478363, 0.6547842025756836, -0.08088724315166473, -0.4869430959224701, -0.7013446092605591, 0.020404929295182228, 0.48944324254989624, -1.054533839225769, -0.6255138516426086, 1.2481493949890137, -0.32282909750938416, -0.1476912647485733, 0.4865058958530426, -0.9783006310462952, -0.18430976569652557, 0.27202779054641724, -1.5677541494369507, -0.8814100027084351, -0.38323384523391724, 0.13956598937511444, 0.09062688797712326, -0.08114063739776611, 0.9632591009140015, -0.1959240734577179, -0.041326142847537994, -0.37004879117012024, -0.6306242942810059, 0.2719495892524719, -0.6126285791397095, -0.9025225043296814, 0.7669110894203186, 0.4790482223033905, 0.3802505135536194, -0.08135128021240234, 0.13447622954845428, 0.511260449886322, -0.4644690454006195, 0.12755154073238373, 0.5820924043655396, -0.6909775733947754, -0.655646562576294, -0.6982396841049194, -0.865714430809021, 0.47600623965263367, 0.45473718643188477, 0.16681648790836334, -0.1800462007522583, 0.029809515923261642, -0.7498425841331482, -0.39352738857269287, -0.626096785068512, 0.013838849030435085, 0.39641058444976807, -0.9756057262420654, -0.6202206015586853, -0.15254780650138855, -0.017376046627759933, -0.6629001498222351, -0.17799000442028046, -0.32683035731315613, 0.34453994035720825, -0.3308068811893463, 1.2172373533248901, -0.7232760190963745, 0.4389761686325073, 0.5400425791740417, -0.26359322667121887, -0.5076141357421875, -0.5683391690254211, -0.8075385689735413, 0.1634174883365631, 0.085667185485363, 0.11952243745326996, -0.5004779100418091, 0.2879556715488434, 0.7245553135871887, 0.23770837485790253, -0.588274359703064, -0.620003879070282, 0.08595478534698486, -0.3255058825016022, -0.3740718960762024, 0.05246232450008392, -0.5722957253456116, -0.4196889102458954, 0.3489343523979187, 0.7054552435874939, 0.4194517135620117, -0.05846799165010452, -0.45493757724761963, -0.056976065039634705, -0.4374719262123108, 0.1857086420059204, -0.43323302268981934, -0.7116416692733765, -1.4937769174575806, -0.03309791162610054, -0.8448060750961304, 0.2681747376918793, -1.3283405303955078, -0.4915187656879425, 0.02998005412518978, -0.646336555480957, 0.38415804505348206, 0.5742666125297546, -0.23141632974147797, 0.07463304698467255, -0.45836785435676575, -0.6191303133964539, 0.623768150806427, 0.7987357974052429, -1.00874924659729, 0.15457646548748016, 0.12428677082061768, -0.024819856509566307, 0.5633546113967896, 0.22488237917423248, -0.5097028017044067, -1.0348689556121826, -1.0130010843276978, 0.2565998136997223, -0.24531905353069305, 0.3336414098739624, -1.2310377359390259, 1.0472525358200073, 0.29052507877349854, 0.11970188468694687, 0.10715905576944351, 0.8831610083580017, -0.8451840281486511, -0.6118746399879456, 0.3041255474090576, -0.8399034738540649, -0.1414022445678711, 0.20804336667060852, -0.25001752376556396, -0.47463396191596985, 0.8635097742080688, 0.24291808903217316, -1.4408178329467773, -1.004026174545288, 0.4377341568470001, -0.501103937625885, 0.06389055401086807, 0.06899536401033401, -0.4109688103199005, -1.0269482135772705, -0.2729678153991699, -0.3037947416305542, 0.2795502245426178, -0.6016455292701721, 0.8869773149490356, 1.273403286933899, -0.9851365089416504, 0.07942086458206177, 0.5295655131340027, -0.25397393107414246, 0.11543386429548264, 0.48258551955223083, 0.3257288634777069, -0.11299204081296921, 0.6082140207290649, -0.18281009793281555, -0.03835462033748627, -0.8324847221374512, 0.14754356443881989, 1.4078644514083862, -0.14212268590927124, -0.25689318776130676, 1.275354027748108, 0.06302601099014282, -0.5227460265159607, 0.1736600697040558, -1.2435517311096191, -0.7373785376548767, 0.015059943310916424, 0.39063331484794617, -0.18541854619979858, -0.11921972781419754, -0.16614143550395966, -0.5683733820915222, 0.4484722316265106, 0.00216272403486073, -0.539073646068573, 0.7022827863693237, 0.13294082880020142, -0.14352399110794067, 0.7122235298156738, 0.41498100757598877, -1.209660530090332, -1.3415565490722656, -0.8425878286361694, -0.6473813652992249, -0.37116003036499023, 0.21240121126174927, -0.08825219422578812, -0.8420426249504089, 1.0387014150619507, 0.8350573182106018, 0.48117008805274963, 0.5183658003807068, 0.1001678928732872, -0.17384403944015503, 0.5027984976768494, -0.06479160487651825, -0.5760484933853149, -0.14096195995807648, 1.2730085849761963, 1.3363298177719116, -0.35074272751808167, 0.08680063486099243, -0.5349756479263306, -0.6038026213645935, 0.941573977470398, 0.2522546052932739, -0.7806705832481384, 1.0453009605407715, -0.3929021954536438, -0.14553718268871307, -0.027248919010162354, -1.0175658464431763, -0.5759540796279907, 0.7277078628540039, 1.5518540143966675, 0.22020499408245087, -0.08843632787466049, 0.7950190305709839, 0.751001238822937, 0.4688498377799988, 0.36765074729919434, 0.4771568477153778, 0.4008442759513855, -0.4135723412036896, 0.3875674903392792, -0.16008348762989044, 0.5104372501373291, -0.48064079880714417, -0.3426485061645508, 0.19023801386356354, 0.39315804839134216, 0.1989443153142929, 0.5713750720024109, 1.1450474262237549, 0.09240594506263733, 0.7913708090782166, 0.08918064832687378, 0.8873112201690674, -0.42933911085128784, -0.3847576677799225, -0.10622522979974747, -0.9992439150810242, -0.06037935987114906, -0.6071772575378418, -1.0101016759872437, -0.25224387645721436, 0.4200648069381714, 0.25318506360054016, -0.1568664163351059, 0.3167390525341034, 0.7683423161506653, 0.3838324248790741, 0.7907281517982483, -0.30357128381729126, -0.3708639442920685, -0.3745730221271515, -0.762484073638916, 0.26290029287338257, -0.4019171893596649, 0.19783151149749756, -0.5733180642127991, 0.11448466777801514, 0.41902363300323486]}, "authors": [{"authorId": "152973423", "name": "Salman Hameed Khan"}, {"authorId": "40894826", "name": "Muzammal Naseer"}, {"authorId": "145684318", "name": "Munawar Hayat"}, {"authorId": "3323621", "name": "Syed Waqas Zamir"}, {"authorId": "2358803", "name": "F. Khan"}, {"authorId": "145103012", "name": "M. Shah"}], "references": [{"paperId": "b4ce19f3b3819accb160acffabffa849f18f4758", "title": "Anchor DETR: Query Design for Transformer-Based Detector"}, {"paperId": "6296aa7cab06eaf058f7291040b320b5a83c0091", "title": "Generative Adversarial Networks"}, {"paperId": "3425495ee3b6ead009f35aeb70edeac4e6eb2d10", "title": "Patches Are All You Need?"}, {"paperId": "1e88d5afe19aea324d33541f60a90b7036894c32", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration"}, {"paperId": "8c5f059ed54ffa777f1ac8478655c24d4ca0abf9", "title": "StyleFormer: Real-time Arbitrary Style Transfer via Parametric Style Composition"}, {"paperId": "19b3b074d38b250d024920732ae51a8ffa0996dd", "title": "Pix2seq: A Language Modeling Framework for Object Detection"}, {"paperId": "7a9a708ca61c14886aa0dcd6d13dac7879713f5f", "title": "SwinIR: Image Restoration Using Swin Transformer"}, {"paperId": "9933a5af7895354087baf6c96b64dc8a8973eaed", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs"}, {"paperId": "66775d9f16b3f4ca43dba2b31c7c42ca6dcba72b", "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "ba1b51e872cdf7744a50b1b2e76ee8b85a0d0dfd", "title": "P2T: Pyramid Pooling Transformer for Scene Understanding"}, {"paperId": "b70bb1855e217edffb5dfa0632e8216860821870", "title": "Efficient Self-supervised Vision Transformers for Representation Learning"}, {"paperId": "0ba85645402a4dd5d3b1567c86494a1bd06e9c1d", "title": "Long-Short Temporal Contrastive Learning of Video Transformers"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9", "title": "On Improving Adversarial Transferability of Vision Transformers"}, {"paperId": "f43b98fcc2d56c60fc71bce96374c1e6b8e12c66", "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer"}, {"paperId": "2835951fabf12804e17d5a525b2be2bee70e7910", "title": "Uformer: A General U-Shaped Transformer for Image Restoration"}, {"paperId": "9dcaf5ab101ba551ac334f3ede177a444e154643", "title": "Referring Transformer: A One-step Approach to Multi-task Visual Grounding"}, {"paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"}, {"paperId": "42a7015e48a1e00b70ebb442a82afb4b10017c0b", "title": "When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations"}, {"paperId": "9d1934ea1bd69d928d17e05d44495d42edf8601d", "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60", "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"}, {"paperId": "adb4302eb7c420a46d770afe2448d4508c65fe58", "title": "ResT: An Efficient Transformer for Visual Recognition"}, {"paperId": "03db529f0bfae6d0b64b0feef565196327fe8d50", "title": "Intriguing Properties of Vision Transformers"}, {"paperId": "68f080e0ac836ea230cb5316fbed273c70422d75", "title": "Segmenter: Transformer for Semantic Segmentation"}, {"paperId": "5924ffeb05f8de7c6df37a47b3a74c63555caaf1", "title": "Visual Grounding with Transformers"}, {"paperId": "db33c408174eef1e40661e8279afbbbf6db2352c", "title": "Self-Supervised Learning with Swin Transformers"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "0768aba7d87ddda3482fd7892b189f84711ede47", "title": "Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "cc9f3a61ea4eaabf43cbb30cd1dd718074932679", "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "ef89d5899eff8d5e62f85018b3f11889d920a1aa", "title": "TransVG: End-to-End Visual Grounding with Transformers"}, {"paperId": "5b68522f58b61e7235b852677337ef3725075fd9", "title": "Co-Scale Conv-Attentional Image Transformers"}, {"paperId": "14c52ffa7ea9c1971d5d82ea369c946c98d056a9", "title": "LocalViT: Bringing Locality to Vision Transformers"}, {"paperId": "4b06c7e29280b1c6bc05c9df39023b48fef02c93", "title": "Escaping the Big Data Paradigm with Compact Transformers"}, {"paperId": "103f9830c3b7efff0233b5755c49da4818d2b8cf", "title": "Handwriting Transformers"}, {"paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae", "title": "An Empirical Study of Training Self-Supervised Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "44930df2a3186edb58c4d6f6e5ed828c5d6a0089", "title": "Attention, please! A survey of neural attention models in deep learning"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "421fba3813d04684b42dd667e16ed22a64f50752", "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search"}, {"paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "2984ab83ade26639c3a82d29628d0d9e4abbebb0", "title": "Incorporating Convolution Designs into Visual Transformers"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "a11676f2864b2d923bb9facc9f6548c812f9e005", "title": "M6: A Chinese Multimodal Pretrainer"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "fa08b41ccdfc5d8771adfbc34c176fa237d4646c", "title": "Is Space-Time Attention All You Need for Video Understanding?"}, {"paperId": "30f326353dfeed21216c1cf98d3c42d794fa054e", "title": "Colorization Transformer"}, {"paperId": "b4ce7f92a8b987b5e76d580bf5076e2495f06883", "title": "TransReID: Transformer-based Object Re-Identification"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "94b69cf199fa0b6c842e17fe5d6174a9d161c3df", "title": "Video Transformer Network"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "d29430adccb805ab57b349afa8553954347b3197", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "ff50b46b4e1cc0fd9beb832fc3468785b635a824", "title": "PCT: Point cloud transformer"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "6102a12b22ec3e44f9bbdae76185ca6e8a358f83", "title": "End-to-End Human Pose and Mesh Reconstruction with Transformers"}, {"paperId": "30e77fb57578bf6c7fd8f44d63a9467e6f7804dd", "title": "SceneFormer: Indoor Scene Generation with Transformers"}, {"paperId": "0acd7ff5817d29839b40197f7a4b600b7fba24e4", "title": "Transformer Interpretability Beyond Attention Visualization"}, {"paperId": "a9dc44231239ef010dc2617bc4c373c00e4bee72", "title": "Topological Planning with Transformers for Vision-and-Language Navigation"}, {"paperId": "aed0f85b14e2c52e0c1850c93503bd637ff8119b", "title": "Parameter Efficient Multimodal Transformers for Video Representation Learning"}, {"paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a", "title": "Pre-Trained Image Processing Transformer"}, {"paperId": "2ac7999cce9f415ee87643f56631b55ed26aa10e", "title": "End-to-End Video Instance Segmentation with Transformers"}, {"paperId": "42aa775a29ff4b3a1cec178301708b8c54266ae7", "title": "Attention-Based Transformers for Instance Segmentation of Cells in Microstructures"}, {"paperId": "e1d082562981a9f51649c60663aa484ee623dbb0", "title": "Point Transformer"}, {"paperId": "80089ad641bae28b0e57771afef181b60011069e", "title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "dedcdc1fb3a6def9772dce674d89150923dd75b9", "title": "Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "a2cd073b57be744533152202989228cb4122270a", "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "737e2998f1db3b2ad8aaa0390ac439402ce1fb23", "title": "Correction to: Single Image Super-Resolution via a Holistic Attention Network"}, {"paperId": "96a2148efe6471866d4e51e64f81090d13a8715e", "title": "Spatial Temporal Transformer Network for Skeleton-based Action Recognition"}, {"paperId": "0fee1138854bd786697dcdb1f052b079d077b9e9", "title": "CrossTransformers: spatially-aware few-shot transfer"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6e1efe22d5696269aff7addcb438f77ff6cc2508", "title": "A Universal Representation Transformer Layer for Few-Shot Image Classification"}, {"paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e", "title": "Self-Supervised Learning: Generative or Contrastive"}, {"paperId": "38f93092ece8eee9771e61c1edaf11b1293cae1b", "title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning"}, {"paperId": "3e86f5a0e2a97894de1cf1f1587799ac79bad0f2", "title": "VirTex: Learning Visual Representations from Textual Annotations"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "c209d9c0d49b2377860acad2acbcc13523a40b7f", "title": "Learning Texture Transformer Network for Image Super-Resolution"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "76a9f336481b39515d6cea2920696f11fb686451", "title": "Quantifying Attention Flow in Transformers"}, {"paperId": "d1ac487f21829ef56c8ffdcd37ea414bce68c809", "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web"}, {"paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33", "title": "Exploring Self-Attention for Image Recognition"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"}, {"paperId": "a1b8a8df281bbaec148a897927a49ea47ea31515", "title": "Improved Baselines with Momentum Contrastive Learning"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52", "title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "285678583be4f8d07470c1629194d8eb0f1a5347", "title": "Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation"}, {"paperId": "7664caa2086c30ea38c9ed09c3678785e101f8c8", "title": "AdversarialNAS: Adversarial Neural Architecture Search for GANs"}, {"paperId": "14fdc18d9c164e5b0d6d946b3238c04e81921358", "title": "Analyzing and Improving the Image Quality of StyleGAN"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "0be0313db9a4fd54a2e3a4f427772498a1419db8", "title": "Video Multitask Transformer Network"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "53c18ed46512ea4cdfceeaa43360e543b512e1ab", "title": "Deep Amortized Clustering"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "025a0dc4a2a98742f1b410b6318a46de2c854b22", "title": "Learning Video Representations using Contrastive Bidirectional Transformer"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "28e1db22e72be9d4fbfa830f31c471a69eab2d86", "title": "FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029", "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "b82153bf85d5d1edd3f170aace830e5328ca9ed0", "title": "Fusion of Detected Objects in Text for Visual Question Answering"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "84c0528cb2aa4bdacad989b5b43441161dd4ecda", "title": "A Short Note on the Kinetics-700 Human Action Dataset"}, {"paperId": "97f4d09175705be4677d675fa27e55defac44800", "title": "Contrastive Multiview Coding"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "9b09d296059909490096e34e9df2d95314787ad5", "title": "Learning Representations by Maximizing Mutual Information Across Views"}, {"paperId": "6be216d93421bf19c1659e7721241ae73d483baf", "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2"}, {"paperId": "bc1138738f24c4a23d865d7786fc4c8229e4662a", "title": "Cross-Domain Transferability of Adversarial Perturbations"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "1cae417456711c4da184f5efcd1b7464a7a0661a", "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "1b632712cd0d1f14784ba938f135960f71a52e5c", "title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding"}, {"paperId": "24d70dbe19baa72a0c8481d5006d3632712ba688", "title": "Video Instance Segmentation"}, {"paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "title": "Local Relation Networks for Image Recognition"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks"}, {"paperId": "69455376f5ad52cac5b72d5e8c6cf03fb466b55c", "title": "Cross-Modal Self-Attention Network for Referring Image Segmentation"}, {"paperId": "a8427ce5aee6d62800c725588e89940ed4910e0d", "title": "An Attentive Survey of Attention Models"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "79e523beb1e1411a241edde0464b07c2ebc231d1", "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling"}, {"paperId": "c5875654fecca84e61a7287b8fbf30d4caccada6", "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples"}, {"paperId": "4c94ee7df6bc2bfcac76703be4f059a79010f7e5", "title": "Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "3694381e74445a8b9f8cb8d373e39626e47191b5", "title": "On the Turing Completeness of Modern Neural Network Architectures"}, {"paperId": "7a4c5dc91af23805f4f6ecea30ca6e5d085e2108", "title": "Residual Dense Network for Image Restoration"}, {"paperId": "171a027fc6c7f4194569170accc48187c8bb5aaa", "title": "Grounded Video Description"}, {"paperId": "e28a054b11b0b861b5055595bbc69a8d51cac0e4", "title": "Few-Shot Learning via Embedding Adaptation With Set-to-Set Functions"}, {"paperId": "9bd25f99bfc73af7e6d76f83d92f8270eab7be1d", "title": "Video Action Transformer Network"}, {"paperId": "5132500b23d2da47129b3f4f68dd30947a29e502", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation"}, {"paperId": "f7fa3b8ed5f3f75ace4fe8447ccd9abfbb19e621", "title": "Self-Supervised Spatiotemporal Feature Learning via Video Rotation Prediction."}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "2e46eac625e70261e43fa765c22a2828e5dd2659", "title": "An Introductory Survey on Attention Mechanisms in NLP Problems"}, {"paperId": "cf336d272a30d6ad6141db67faa64deb8791cd61", "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs"}, {"paperId": "7b2c24d41ced8c03c026a6bbc0e8ec1417793f5c", "title": "Cross and Learn: Cross-Modal Self-Supervision"}, {"paperId": "512b8ef0002e0bfd0ecb5ab17d533c1762eb9786", "title": "Set Transformer"}, {"paperId": "2e5c62a871dbce54816ba30797c5d85cad0e5f37", "title": "SRFeat: Single Image Super-Resolution with Feature Discrimination"}, {"paperId": "1bdd30a8acc75c58a1bdd4daa4545d5f3971a826", "title": "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks"}, {"paperId": "ecbfdcbc09146c87fca594b9dcdf55f9c3504ce3", "title": "Video Jigsaw: Unsupervised Learning of Spatiotemporal Context for Video Action Recognition"}, {"paperId": "9775f8964a2eea1c9e35a02b1b906487396ea1f5", "title": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks"}, {"paperId": "45b7b5514a65126d39a51d5a68da53e7aa244c1f", "title": "Understanding and Simplifying One-Shot Architecture Search"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "9baf01eb53abda6a169110477f2c7a3492559368", "title": "Learning and Using the Arrow of Time"}, {"paperId": "8dac02f61e12560607f857cee3c1d5abaf40ecd0", "title": "Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks"}, {"paperId": "175f74a09241b6cb5101a2a09978095720db7d5f", "title": "Image Super-Resolution via Dual-State Recurrent Networks"}, {"paperId": "83f80b0c517f60eaec01442eb58a6ffb8bd8ab60", "title": "Look into Person: Joint Body Parsing & Pose Estimation Network and a New Benchmark"}, {"paperId": "35ed258aede3df17ee20a6635364cb5fd2461049", "title": "End-to-End Dense Video Captioning with Masked Transformer"}, {"paperId": "45dd2a3cd7c27f2e9509b023d702408f5ac11c9d", "title": "Stacked Cross Attention for Image-Text Matching"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "c7d6f073d89f31e6584450713013c1fd85138090", "title": "A Guide to Convolutional Neural Networks for Computer Vision"}, {"paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "title": "Efficient Neural Architecture Search via Parameter Sharing"}, {"paperId": "e22979cdf147a63be74f3816ef59ef11f3508919", "title": "Learning Image Representations by Completing Damaged Jigsaw Puzzles"}, {"paperId": "dce916351ef589afa7a63452648dd8acba931e92", "title": "Panoptic Segmentation"}, {"paperId": "8b35c00edfa4edfd7a99d816e671023d2c000d55", "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "4ace72f12491a7c06967a6011c4bef004192d767", "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks"}, {"paperId": "79828e6e9f137a583082b8b5a9dfce0c301989b8", "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes"}, {"paperId": "de40803a3d17dd406e25922695266d6ed580e371", "title": "Unsupervised Representation Learning by Sorting Sequences"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "7ba5d3808e117e7a68dc40331ce1d483ceeedcb2", "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution"}, {"paperId": "a55970013b984f344dfbbbba677d89dce0ba5f81", "title": "Image Super-Resolution via Deep Recursive Residual Network"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8674494bd7a076286b905912d26d47f7501c4046", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"}, {"paperId": "9b5f696f73c1264ccb8e97d3b738a2342ecd6bee", "title": "Look, Listen and Learn"}, {"paperId": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6", "title": "The Kinetics Human Action Video Dataset"}, {"paperId": "96dd1fc39a368d23291816d57763bc6eb4f7b8d6", "title": "Dense-Captioning Events in Videos"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "4a73a1840945e87583d89ca0216a2c449d50a4a3", "title": "Deformable Convolutional Networks"}, {"paperId": "a456265138c088a894301c0433dae938705a9bec", "title": "Deep Sets"}, {"paperId": "5ba2218b708ca64ab556e39d5997202e012717d5", "title": "Audio Set: An ontology and human-labeled dataset for audio events"}, {"paperId": "e10a5e0baf2aa87d804795af071808a9377cc80a", "title": "Towards Automatic Learning of Procedures From Web Instructional Videos"}, {"paperId": "dad895b572e01a6a83bddbac1662e9bc629aa693", "title": "MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification"}, {"paperId": "fddc32f3880688238847077fd927ab3025db7a6a", "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis"}, {"paperId": "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921", "title": "StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks"}, {"paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878", "title": "Feature Pyramid Networks for Object Detection"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3", "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51", "title": "Conditional Image Generation with PixelCNN Decoders"}, {"paperId": "405c31c85a324942811f3c9dc53ce3528f9284df", "title": "Towards a Neural Statistician"}, {"paperId": "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "title": "Generative Adversarial Text to Image Synthesis"}, {"paperId": "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87", "title": "Context Encoders: Feature Learning by Inpainting"}, {"paperId": "091e4d3c85dc0a8212afea875cd3b162d273d46b", "title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis"}, {"paperId": "21334d1aac5422da88780f8e24e181bfa15ef0e1", "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding"}, {"paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, {"paperId": "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0", "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles"}, {"paperId": "8201e6e687f2de477258e9be53ba7b73ee30d7de", "title": "Colorful Image Colorization"}, {"paperId": "6d4e3616d0b27957c4107ae877dc0dd4504b69ab", "title": "Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification"}, {"paperId": "915c4bb289b3642489e904c65a47fa56efb60658", "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "9b686d76914befea66377ec79c1f9258d70ea7e3", "title": "ShapeNet: An Information-Rich 3D Model Repository"}, {"paperId": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0", "title": "SSD: Single Shot MultiBox Detector"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "8388f1be26329fa45e5807e968a641ce170ea078", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"paperId": "e65142010431ffc089b272a1174214e00693e503", "title": "Generation and Comprehension of Unambiguous Object Descriptions"}, {"paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd", "title": "You Only Look Once: Unified, Real-Time Object Detection"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator"}, {"paperId": "92c141447f51b6732242376164ff961e464731c8", "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}, {"paperId": "bac377d3a051899dbe0d7249ed5d3d0b22d57310", "title": "Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments"}, {"paperId": "7c8a51d04522496c43db68f2582efd45eaf59fea", "title": "3D ShapeNets: A deep representation for volumetric shapes"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "44040913380206991b1991daf1192942e038fe31", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad", "title": "Intriguing properties of neural networks"}, {"paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02", "title": "Auto-Encoding Variational Bayes"}, {"paperId": "da9e411fcf740569b6b356f330a1d0fc077c8d7c", "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "be9a17321537d9289875fe475b71f4821457b435", "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "e03bbca03dc10c4dfb10eca7439aa1a19233aa5a", "title": "Semantic object classes in video: A high-definition ground truth database"}, {"paperId": "9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d", "title": "A non-local algorithm for image denoising"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "title": "Backpropagation Applied to Handwritten Zip Code Recognition"}, {"paperId": "850d753aed56ed1a0facc3e87734aa1dfd197e82", "title": "DALL-E: CREATING IMAGES FROM TEXT"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "5366919840236059252c7f8f510dfb36df9e3206", "title": "TransGAN: Two Transformers Can Make One Strong GAN"}, {"paperId": "c34943674c442b25e7bf4288701860934077ddf1", "title": "Efficient Transformer for Single Image Super-Resolution"}, {"paperId": "083ca4bd4d5b231a1d7a0715ec55cc57a0f44b13", "title": "Aggregating Nested Transformers"}, {"paperId": "9f7f81b1c82828a45a52df8f0c6a92636af76c7e", "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention"}, {"paperId": "62409eeec47c64463090216582210e2b0d4f3101", "title": "Transformer in Convolutional Neural Networks"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6", "title": "GENERATIVE ADVERSARIAL NETS"}, {"paperId": "c19ed5102ecd953d5c78d5a0b87eaa51658e07d8", "title": "Supplementary Material to: Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera"}, {"paperId": null, "title": "Abhinav Gupta, and Kaiming He"}, {"paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82", "title": "Deep Learning"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "So , Chen Liang , and uoc V . Le . 2019 . he Evolved Transformer"}, {"paperId": null, "title": "OpenAI's GPT-3 Language Model: A Technical Overview"}, {"paperId": null, "title": "Aaai 2020 keynotes turing award winners event"}, {"paperId": null, "title": "Atention is all you need . In NeurIPS . ACM Comput . Surv ."}, {"paperId": null, "title": "Revisiting the Unreasonable Effectiveness of Data"}]}