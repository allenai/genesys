{"paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8", "abstract": "Vision Transformer (ViT) has emerged as a competitive alternative to convolutional neural networks for various computer vision applications. Specifically, ViTs\u2019 multi-head attention layers make it possible to embed information globally across the overall image. Nevertheless, computing and storing such attention matrices incurs a quadratic cost dependency on the number of patches, limiting its achievable efficiency and scalability and prohibiting more extensive real-world ViT applications on resource-constrained devices. Sparse attention has been shown to be a promising direction for improving hardware acceleration efficiency for NLP models. However, a systematic counterpart approach is still missing for accelerating ViT models. To close the above gap, we propose a first-of-its-kind algorithm-hardware codesigned framework, dubbed VITALITY, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, VITALITY unifies both low-rank and sparse components of the attention in ViTs. At the algorithm level, we approximate the dot-product softmax operation via first-order Taylor attention with row-mean centering as the low-rank component to linearize the cost of attention blocks and further boost the accuracy by incorporating a sparsity-based regularization. At the hardware level, we develop a dedicated accelerator to better leverage the resulting workload and pipeline from VITALITY\u2019s linear Taylor attention which requires the execution of only the low-rank component, to further boost the hardware efficiency. Extensive experiments and ablation studies validate that VITALITY offers boosted end-to-end efficiency (e.g., 3\u00d7 faster and 3\u00d7 energy-efficient) under comparable accuracy, with respect to the state-of-the-art solution. We make the codes available on https://github.com/GATECH-EIC/ViTaLiTy", "venue": "International Symposium on High-Performance Computer Architecture", "year": 2022, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2211.05109", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A first-of-its-kind algorithm-hardware codesigned framework, dubbed VITALITY, for boosting the inference efficiency of ViTs, which unifies both low-rank and sparse components of the attention in ViTs."}, "embedding": {"model": "specter_v2", "vector": [0.640781819820404, 0.6714519262313843, -0.3310930132865906, 0.32392680644989014, -0.5275474190711975, 0.2221800684928894, 0.7914674878120422, -0.28982338309288025, -0.05678630992770195, -1.1074589490890503, 0.4561973512172699, 0.027106383815407753, 0.4029126763343811, 0.006771205924451351, -0.3304523527622223, -0.08964122086763382, -0.43566617369651794, -0.11623205244541168, 0.1888975352048874, -0.36864736676216125, 0.29075828194618225, -0.4082799553871155, -1.267697811126709, 0.2542400658130646, 0.024884790182113647, 1.5346368551254272, 0.2492971569299698, 0.7074896097183228, -0.24219268560409546, 0.7421010732650757, 0.807816207408905, -0.12931567430496216, 0.7036722302436829, -0.005944231990724802, -0.04278003051877022, -0.3188510239124298, 0.876329779624939, -0.26833266019821167, -0.623808741569519, 0.9760861396789551, -0.2094389796257019, 0.27095627784729004, 0.5659465789794922, -0.7221271395683289, -0.6602102518081665, 0.09886437654495239, -0.0010664453729987144, 0.908892810344696, -0.4971329867839813, -0.2380596101284027, 1.0472577810287476, -1.5858231782913208, 0.10550641268491745, 1.2089564800262451, 0.577700674533844, 0.14955314993858337, 0.2057347595691681, -0.17166271805763245, 0.5487252473831177, 0.3405156433582306, -0.460453599691391, -0.681516706943512, 0.071373850107193, -0.12968288362026215, 1.8600001335144043, -0.6133731603622437, 0.12666967511177063, 0.602501630783081, 0.38538622856140137, 1.3250459432601929, 0.13200528919696808, -0.8508982062339783, -0.08767438679933548, -0.17360398173332214, 0.22006972134113312, 0.795427143573761, -0.3097745180130005, 0.2582148313522339, -1.1112122535705566, 0.11968224495649338, 0.5040829181671143, 0.1393335461616516, 0.49867963790893555, -0.1394469439983368, -0.12442224472761154, 0.6276839375495911, 0.8928298354148865, 0.44930365681648254, -0.36573857069015503, 1.0725282430648804, 0.6027603149414062, -0.02224189043045044, -0.17441770434379578, 0.11788583546876907, 0.10342255234718323, 0.7415387034416199, -0.9758887887001038, -0.23139552772045135, -0.19422809779644012, 0.863974392414093, -0.3728356659412384, 0.5435481071472168, -0.797096312046051, 0.08397779613733292, 1.1269893646240234, 0.44773030281066895, 0.18269005417823792, -0.544334888458252, -0.13283951580524445, -0.9399793744087219, -0.4315175712108612, -0.9684346914291382, 0.34779688715934753, -0.5688085556030273, -1.2822575569152832, -0.6232787370681763, -0.5842229127883911, 0.7834076285362244, -1.2643101215362549, 0.26233169436454773, -0.3643494248390198, 0.10087818652391434, -0.10937726497650146, 0.663081169128418, 0.7654862403869629, 0.654204785823822, -0.02617710269987583, 0.37126579880714417, 1.5258551836013794, -1.2433247566223145, -0.31388649344444275, -1.2383782863616943, -0.22244492173194885, -0.3510606586933136, 0.2876970171928406, 0.3405168354511261, -1.2161471843719482, -1.1601824760437012, -0.6619645953178406, -0.4640994966030121, -0.4849482774734497, 0.4992935359477997, 1.0523710250854492, -0.050165850669145584, -1.1746939420700073, 0.477152019739151, -0.4691895544528961, -0.17562635242938995, 0.8352044224739075, 0.23000302910804749, 0.46071210503578186, -0.2510286867618561, -0.8054648041725159, 0.20936642587184906, -0.35085001587867737, -0.16033297777175903, -0.7683814167976379, -0.619661808013916, -1.1032439470291138, 0.47553500533103943, 0.06488019227981567, -0.6231234669685364, 1.1120307445526123, -0.16518865525722504, -1.0098298788070679, 0.658877968788147, -0.6127369403839111, 0.018954109400510788, -0.10701701790094376, 0.027540801092982292, -0.03763185068964958, -0.1382317990064621, 0.08082135021686554, 0.5447800755500793, 1.454505443572998, 0.08311133086681366, -0.09546387195587158, 0.129365473985672, -0.46052947640419006, -0.34727758169174194, -0.5029112100601196, 0.7276070713996887, -0.9403776526451111, -0.07066147029399872, 0.5412096381187439, 0.6944281458854675, -0.35810598731040955, 0.05650799721479416, -0.2392251044511795, -0.7432426810264587, 0.7814740538597107, 0.13263612985610962, 0.21349073946475983, -1.1870458126068115, -0.879545271396637, 0.07298751920461655, 0.2737492322921753, -0.18889957666397095, -0.67695552110672, 0.22047439217567444, -0.42070212960243225, -0.287492036819458, 0.3841138780117035, -1.0163651704788208, -0.18064144253730774, -0.5387678146362305, -1.103079080581665, 0.09050960093736649, 0.2847708761692047, 1.4115498065948486, -0.6182966828346252, 0.17642492055892944, 0.06773614883422852, 0.7715887427330017, -1.070346713066101, 0.8249963521957397, -0.2391970455646515, -0.029065389186143875, 0.012153929099440575, 0.2214668095111847, -0.10251998901367188, -0.7979804277420044, 0.4888213276863098, -1.0679974555969238, 0.18929187953472137, 0.3104631006717682, -0.07699353992938995, 1.04698646068573, -0.3707403242588043, 0.9083091616630554, 0.06569723784923553, -0.9490861892700195, 0.6271148324012756, -0.01018449105322361, -0.38750016689300537, -0.9617786407470703, 0.6288033723831177, 0.32333311438560486, -0.9352742433547974, 0.3362249433994293, 0.6885294318199158, 1.1171720027923584, -0.09115301072597504, -0.2624149024486542, 0.7094561457633972, -0.11927212029695511, 0.014084515161812305, 0.6107670664787292, 0.706755518913269, 0.30037030577659607, 0.2841763496398926, -0.11945302784442902, 0.07229234278202057, -0.906582772731781, -0.128498375415802, 0.8602367043495178, 0.7635259628295898, 0.8351705074310303, 0.7202365398406982, -0.90857994556427, -0.48877108097076416, 0.06842044740915298, 0.5520442128181458, 1.1668496131896973, -0.16178300976753235, 0.056896038353443146, -0.37512341141700745, -0.05530903860926628, -0.6879392862319946, -0.7971409559249878, -0.3024015426635742, -0.20470070838928223, -0.17948898673057556, -0.9124836921691895, 0.5087657570838928, 0.3256707191467285, 1.0012235641479492, -0.650094211101532, -0.548991858959198, -0.7563535571098328, 0.3840702176094055, -1.22018563747406, -0.3764553666114807, 0.697655975818634, -0.4382888972759247, -0.05888732150197029, 0.07870713621377945, -0.33463528752326965, 0.5695410370826721, -0.49174928665161133, 0.805167555809021, -0.7220921516418457, -0.7306027412414551, 0.3629647493362427, 0.4592515528202057, -0.44476476311683655, -0.11535251140594482, 0.21071718633174896, 0.00459787342697382, -0.042386215180158615, 0.312979131937027, -0.13750562071800232, -0.17081087827682495, -0.41366487741470337, -0.17105422914028168, -0.1406085044145584, 0.3795299828052521, 0.15699288249015808, 1.1144962310791016, -0.5803062319755554, -0.36705705523490906, -0.7299180030822754, 0.4392356872558594, 0.05273821949958801, -0.6022558212280273, -0.3433065712451935, -0.7043081521987915, -0.24759283661842346, 0.6235323548316956, -0.7128442525863647, -0.009803093038499355, -0.2838345170021057, 0.1279168576002121, -1.0947024822235107, -0.13557811081409454, -0.05217012017965317, 0.4376663565635681, -0.48462533950805664, 0.6958929896354675, 0.620148241519928, 0.3332499861717224, 0.2696089744567871, 0.43432536721229553, -1.1479119062423706, 1.1570111513137817, 0.20861347019672394, 0.4334649443626404, 0.3532208800315857, 0.4310794472694397, -0.8021878004074097, -0.7687323689460754, -0.49681439995765686, -0.015238964930176735, -0.5334203839302063, 0.24012748897075653, -0.8275370001792908, -1.2065035104751587, -0.09020546078681946, -0.7592586278915405, -0.050022922456264496, -0.10907342284917831, -0.4198899269104004, -0.39519771933555603, -0.9390382170677185, -1.2568702697753906, -0.39845651388168335, -1.3077061176300049, -1.2578442096710205, 0.48948749899864197, 0.43012985587120056, 0.21487602591514587, -0.32266557216644287, -0.4878023564815521, -0.4943004548549652, 1.2580814361572266, -0.3880007266998291, 0.42875272035598755, 0.0177354346960783, -0.6443902254104614, 0.02785434015095234, -0.36221545934677124, 0.46279144287109375, -0.7167113423347473, 0.20442594587802887, -1.233913779258728, 0.35088685154914856, -0.3081994950771332, -0.3788582682609558, 0.5305952429771423, 0.70168536901474, 0.4333842694759369, 0.10771111398935318, -0.4694992005825043, 0.9358169436454773, 1.7320388555526733, -0.8614810705184937, 0.39167454838752747, 0.1563718467950821, 1.1461495161056519, -0.17964394390583038, -0.10686501115560532, 0.6987757682800293, 0.07418681681156158, 0.20958921313285828, 0.6519774198532104, -0.5129296779632568, -0.5673640966415405, -0.5992135405540466, 0.4913991391658783, 1.191033959388733, 0.6459188461303711, 0.29015129804611206, -0.8515874743461609, 0.8982834815979004, -0.9875237941741943, -0.7007900476455688, 0.4459657669067383, 0.3579537272453308, -0.15243437886238098, -0.2841189503669739, -0.6250930428504944, -0.29710453748703003, 0.20545026659965515, 0.6297624111175537, -0.4907363951206207, -0.7735435962677002, 0.09259859472513199, 0.7423144578933716, 0.6003512740135193, 0.46986255049705505, -0.15387791395187378, 0.6130437850952148, 14.5996675491333, 1.1111347675323486, -0.3502916693687439, 0.5488055944442749, 0.744335949420929, 0.331961065530777, -0.12337692826986313, 0.26101166009902954, -1.4468075037002563, -0.5338056683540344, 0.7358728647232056, 0.7283297181129456, 0.6085424423217773, 0.5897874236106873, -0.33671727776527405, 0.16869446635246277, -0.3665250539779663, 0.9302266240119934, 0.9183516502380371, -1.5938133001327515, 0.10855207592248917, 0.29533177614212036, 0.4156794846057892, 0.8201256394386292, 1.201907753944397, 0.6533935070037842, 0.3415112793445587, -0.31231293082237244, 0.4285508990287781, 0.28813913464546204, 1.206636905670166, 0.22707869112491608, 0.19544290006160736, 0.2625032961368561, -1.2380503416061401, -0.028602920472621918, -0.664722740650177, -1.0728622674942017, -0.049850624054670334, 0.48819202184677124, -0.21178610622882843, -0.43936946988105774, 0.311514288187027, 1.1413487195968628, -0.01069505326449871, 0.33033323287963867, -0.2598438858985901, 0.31771552562713623, -0.43485209345817566, 0.3526637554168701, 0.37281736731529236, 0.48523133993148804, -0.1432875096797943, -0.10040617734193802, 0.09177513420581818, -0.03433181345462799, 0.14727894961833954, 0.6129937767982483, -0.41811221837997437, -0.46142497658729553, -0.19920194149017334, -0.08823052793741226, -0.4216495752334595, 1.3572490215301514, -0.04530346766114235, 0.3548283278942108, -0.25630491971969604, 0.33281928300857544, 0.4092496335506439, 0.03255273401737213, -0.3352963924407959, -0.31234046816825867, 0.13055066764354706, -0.44853582978248596, 0.6186425685882568, 0.773198664188385, -0.45252275466918945, -0.6540119051933289, -0.44545310735702515, -0.6419121026992798, 0.12270180135965347, -0.9002694487571716, -0.5564448237419128, 0.8875886797904968, -0.5849779844284058, -0.22569015622138977, 0.5094579458236694, -0.9213511943817139, -0.3516545593738556, 0.4911820590496063, -1.622848629951477, -0.8529689311981201, -0.014984301291406155, -0.21831601858139038, -0.3516767919063568, -0.18572452664375305, 0.5859431028366089, 0.2171248346567154, 0.014392267912626266, 0.2353557050228119, -0.2028478980064392, -0.11114303022623062, -0.24453002214431763, -0.28317689895629883, 1.1067287921905518, 0.6489037275314331, 0.006394464988261461, 0.10052315890789032, 0.19457073509693146, 0.374740868806839, -1.2515254020690918, 0.06750447303056717, 0.47135189175605774, -0.5964878797531128, -0.17304478585720062, -0.7630317807197571, -0.5665190815925598, 0.2985060513019562, 0.4099910855293274, 0.4339442551136017, -0.24984966218471527, 0.1974523961544037, -1.0182040929794312, -0.30437996983528137, -0.520365834236145, -0.21548208594322205, 0.17389225959777832, -0.996552586555481, 0.02150142565369606, 0.010768536478281021, 0.047692567110061646, -1.2768326997756958, -0.4007094204425812, -0.15127964317798615, 0.2737768888473511, -0.16973379254341125, 1.4298430681228638, 0.2919365465641022, 1.1421538591384888, 0.6589062809944153, -0.17182615399360657, -0.09826938062906265, -0.013539846055209637, -0.8119524717330933, -0.21447597444057465, 0.08223306387662888, 0.09975513815879822, -0.34667426347732544, 0.5649061799049377, 0.3965313136577606, 0.2173195481300354, -0.6411014199256897, -0.5872123837471008, -0.05505292862653732, -0.4552304744720459, -0.6178929805755615, -0.24452537298202515, -0.2011045515537262, -0.13262899219989777, 0.08305729180574417, 0.11593591421842575, 0.7232375144958496, 0.23816955089569092, -0.42376115918159485, 0.4377310276031494, -0.06389141082763672, -0.36871597170829773, -0.2560705542564392, -0.7414885759353638, -1.6415971517562866, -0.31023913621902466, -1.166053295135498, -0.042775709182024, -0.975727379322052, -0.4449564814567566, 0.1601363569498062, -0.20720195770263672, 0.5633231401443481, 0.4552755355834961, 0.34835341572761536, -0.23433317244052887, -0.5208044052124023, -0.5296778678894043, 0.8190072178840637, 0.8001658916473389, -0.7266882061958313, 0.2792045474052429, -0.40704306960105896, -0.18354259431362152, 0.4408760666847229, 0.03943975269794464, -0.47151339054107666, -0.7214221954345703, -1.0674670934677124, 0.650171160697937, -0.07279717177152634, 0.2362462878227234, -1.1188576221466064, 1.253039002418518, 0.6596038937568665, 0.03683658689260483, 0.028049873188138008, 0.39267075061798096, -0.8650369644165039, -0.6357501149177551, 0.4768200218677521, -0.4177108407020569, -0.19314473867416382, 0.49433043599128723, -0.6353896856307983, -0.2985445559024811, 0.7499726414680481, 0.22115908563137054, -0.7099066972732544, -0.8964807391166687, 0.49390527606010437, -0.34250208735466003, 0.06103653460741043, -0.4261682629585266, -0.2855454385280609, -1.3391274213790894, -0.40063759684562683, 0.04170990735292435, 0.08498930186033249, -0.5066539645195007, 0.7023097276687622, 0.7429898977279663, -1.0713703632354736, 0.3431777060031891, 0.5524434447288513, -0.3151555359363556, 0.14652033150196075, 0.521744966506958, 0.7451026439666748, -0.3081907033920288, 0.3948412239551544, -0.05426350235939026, -0.005116550251841545, -0.6166519522666931, 0.3977316617965698, 0.66402268409729, -0.491386353969574, -0.36802276968955994, 1.0796798467636108, -0.023944497108459473, -0.705727219581604, 0.11844497174024582, -1.123740315437317, -0.42106613516807556, -0.1326034963130951, 0.5715134739875793, -0.18111753463745117, 0.15149341523647308, 0.06569759547710419, -0.5888130068778992, 0.5490875840187073, -0.16968019306659698, -0.4109412431716919, 0.5301989912986755, 0.3148265480995178, -0.4816160798072815, -0.11201809346675873, 0.755048930644989, -0.6169476509094238, -0.8668177723884583, -1.2058939933776855, -0.8920011520385742, -0.12246084213256836, 0.7114298939704895, 0.0019997665658593178, -1.2270435094833374, 0.9425610303878784, 0.6730480790138245, 0.06788592040538788, 0.6362081170082092, 0.07495598495006561, 0.16750523447990417, 0.6892662644386292, -0.17786216735839844, -0.3146706223487854, -0.4398368000984192, 1.4099427461624146, 0.8742591142654419, -0.7136326432228088, 0.36134064197540283, -0.3902357816696167, -0.5392358303070068, 0.741796612739563, 0.16655920445919037, -0.48884111642837524, 0.7199817895889282, 0.23072633147239685, -0.23231783509254456, -0.07320786267518997, -0.8308242559432983, -0.3668521046638489, 1.027307152748108, 1.319642186164856, 0.3597109317779541, -0.4787178933620453, 0.6173021197319031, 0.5772016644477844, 0.3861111104488373, 0.12859901785850525, 0.23479430377483368, 0.13491372764110565, -0.000283372268313542, 0.39799150824546814, -0.44458889961242676, 0.829037606716156, -0.8654981851577759, -0.9983729720115662, 0.6496540307998657, 0.2650378942489624, -0.03136197477579117, 0.4431033432483673, 0.929388701915741, 0.04972756654024124, 0.6242281198501587, -0.5407450199127197, 0.5814111828804016, -0.2800399661064148, -0.3986418545246124, 0.08516452461481094, -1.1160324811935425, -0.49696117639541626, 0.044084567576646805, -0.44889816641807556, -0.1819402277469635, -0.19421398639678955, 0.5308396816253662, -0.3639196455478668, 0.43851983547210693, 0.7416285276412964, 0.6063596606254578, 0.860319197177887, -0.3728089928627014, -0.9801979660987854, -0.2921293377876282, -0.4789315164089203, 0.3387525975704193, -0.9058213233947754, 0.06940443813800812, 0.08500562608242035, -0.24421831965446472, 0.0612017847597599]}, "authors": [{"authorId": "40436575", "name": "Jyotikrishna Dass"}, {"authorId": "2167290089", "name": "Shang Wu"}, {"authorId": "30984015", "name": "Huihong Shi"}, {"authorId": "28987646", "name": "Chaojian Li"}, {"authorId": "2189592928", "name": "Zhifan Ye"}, {"authorId": "47196693", "name": "Zhongfeng Wang"}, {"authorId": "3138925", "name": "Yingyan Lin"}], "references": [{"paperId": "cbff35378657225ece138c33e6a23afb3b46b41f", "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences"}, {"paperId": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a", "title": "Unified Visual Transformer Compression"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5", "title": "QuadTree Attention for Vision Transformers"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "b9cb9eead20a88bfb8946917b11083251e6851a8", "title": "Learning to Match Features with Seeded Graph Matching Network"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b03cf6324ecf7a295a4aeae5970c88d1a1c3f336", "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection"}, {"paperId": "a3ef6ee560e93e6f58be2b28f27aed0eb86dc463", "title": "Fine-tune BERT with Sparse Self-Attention Mechanism"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "69455376f5ad52cac5b72d5e8c6cf03fb466b55c", "title": "Cross-Modal Self-Attention Network for Referring Image Segmentation"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "b8a919f4a2aaa97bef19aa43e01f8bc347693b73", "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training"}, {"paperId": "7d2a78a1f713b71c3a337247d042c5c2f0b2da84", "title": "EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "977424d679d4300a506d31374135413d5d9270b2", "title": "Visual Transformer Pruning"}, {"paperId": null, "title": "GeForce RTX 2080 TI Graphics Card \u2014 NVIDIA"}, {"paperId": null, "title": "Pixel 3"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Pytorch image models"}, {"paperId": "deace80ef638b4987bd473cbbf28bde73e15bde5", "title": "\u57fa\u4e8eNVIDIA Jetson TX2\u7684\u9053\u8def\u573a\u666f\u5206\u5272"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "the hardware level"}, {"paperId": null, "title": "\u201c Synopsys design compiler . \u201d [ Online ]"}, {"paperId": null, "title": "Visual correspondence hallucination , \u201d in International Conference on Learning Representations , 2022 . [ Online ]"}]}