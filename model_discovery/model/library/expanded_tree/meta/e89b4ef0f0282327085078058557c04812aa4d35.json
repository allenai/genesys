{"paperId": "e89b4ef0f0282327085078058557c04812aa4d35", "abstract": "The recent surge of large language models (LLMs) highlights their ability to perform in-context learning, i.e.,\"learning\"to perform a task from a few demonstrations in the context without any parameter updates. However, their capabilities of in-context learning are limited by the model architecture: 1) the use of demonstrations is constrained by a maximum sentence length due to positional embeddings; 2) the quadratic complexity of attention hinders users from using more demonstrations efficiently; 3) LLMs are shown to be sensitive to the order of the demonstrations. In this work, we tackle these challenges by proposing a better architectural design for in-context learning. We propose SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations. We evaluate SAICL in a meta-training framework and show that SAICL achieves comparable or better performance than full attention while obtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a strong Fusion-in-Decoder (FiD) baseline which processes each demonstration independently. Finally, thanks to its linear nature, we demonstrate that SAICL can easily scale to hundreds of demonstrations with continuous performance gains with scaling.", "venue": "arXiv.org", "year": 2023, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.02690", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations."}, "embedding": {"model": "specter_v2", "vector": [0.18915317952632904, 0.3975592851638794, -0.1254790723323822, -0.47522783279418945, -0.5635926723480225, -0.3074244260787964, 0.7703213095664978, -0.28607967495918274, -0.6081955432891846, -0.08750233799219131, 0.8125121593475342, -0.10541804134845734, 0.2693217396736145, 0.7489948272705078, -0.6964044570922852, 0.14896583557128906, -1.0876659154891968, 0.2323170155286789, -0.0216049961745739, -0.2732486426830292, -0.25454315543174744, -0.8618459105491638, -0.6909497380256653, 0.14010752737522125, 0.35661426186561584, -0.07681253552436829, 0.8124783635139465, 1.2257773876190186, 0.060246653854846954, 0.1625126302242279, 0.041157372295856476, -0.023052603006362915, 0.25363850593566895, 0.03300858289003372, -0.02021673507988453, -0.29437416791915894, 0.6874427795410156, -0.9141994118690491, -0.4977002739906311, 0.5217579007148743, -0.02708805352449417, 0.2729286551475525, 0.4830985963344574, -0.5190565586090088, -0.5620895028114319, 0.8859427571296692, 0.6602717638015747, 0.6690540909767151, 0.2584235966205597, -0.46614617109298706, 1.4343379735946655, -1.702837586402893, 0.35394003987312317, 1.436270833015442, 0.1752491295337677, 0.6389452219009399, -0.25008177757263184, -0.3676201403141022, 1.0834113359451294, 0.12589815258979797, -0.4878957271575928, -0.13632796704769135, 0.07904920727014542, 0.13961371779441833, 1.8465335369110107, -0.5657836198806763, 0.2300921082496643, 1.2547123432159424, -0.2905953526496887, 1.620629906654358, -0.3804413676261902, -0.9779922962188721, -0.252877414226532, 0.12265338748693466, 0.28395798802375793, 0.731255054473877, -0.389011949300766, 0.5961032509803772, -0.9316826462745667, 0.2054273933172226, 0.39710870385169983, 0.028375228866934776, -0.1346835345029831, -0.33413246273994446, -0.9056274890899658, 0.5388239622116089, 0.522235631942749, 0.8135380744934082, -0.13272792100906372, 1.0245614051818848, 0.6107096672058105, 0.4118722379207611, -0.3652520477771759, 0.35606709122657776, -0.3236631155014038, 0.16929508745670319, -0.8588315844535828, 0.3146662414073944, -0.11406886577606201, 1.0152089595794678, -0.4316868185997009, -0.11763108521699905, -0.41297417879104614, -0.005904472433030605, 1.427006483078003, 0.34778502583503723, 0.5186589360237122, -0.6431190967559814, 0.8380382061004639, -0.7786033749580383, 0.6223172545433044, -0.33622461557388306, -0.3212330937385559, -0.04460524767637253, -0.11645038425922394, -1.0421361923217773, -0.4075219929218292, 0.0637635886669159, -0.20686694979667664, 1.1679381132125854, 0.046780072152614594, 0.4894527792930603, 0.4139164686203003, 0.39696821570396423, 0.6209405064582825, 0.5714110136032104, 0.12183887511491776, 0.22843579947948456, 0.6264382600784302, -1.1830005645751953, -0.504609227180481, -0.9343453049659729, 0.9369738101959229, 0.03775494545698166, 0.8566850423812866, -0.45538458228111267, -0.8339685797691345, -1.3331166505813599, -1.3514803647994995, -0.09561995416879654, -0.5218151211738586, 0.30584239959716797, 0.616531491279602, 0.29452869296073914, -1.1740050315856934, 0.8284271955490112, -0.6191545724868774, -0.13996349275112152, -0.008218794129788876, 0.17164134979248047, -0.0974486842751503, -0.5845099687576294, -1.0574464797973633, 0.3047255575656891, 0.2036534696817398, -0.20732802152633667, -0.5446928143501282, -0.7675095796585083, -1.6844059228897095, -0.27525466680526733, 0.200616717338562, -0.47269314527511597, 1.5830719470977783, 0.048180606216192245, -1.1176636219024658, 0.33211851119995117, -0.27505314350128174, 0.35478636622428894, 0.29423797130584717, -0.40657806396484375, -0.820188045501709, -0.35452374815940857, 0.008072138763964176, 1.0768320560455322, 0.0497271753847599, -0.1481267809867859, -0.22984880208969116, -0.21768321096897125, 0.0792759507894516, 0.05748752877116203, -0.02566680870950222, 0.5992140173912048, -0.222712442278862, -0.09813462197780609, 0.023759067058563232, 0.622943639755249, -0.004747799132019281, -0.2899794280529022, -0.4001545011997223, -1.2357795238494873, 1.3181933164596558, 0.15728172659873962, 0.5630956292152405, -1.0173429250717163, -0.6220765709877014, -0.6270183324813843, -0.4488500952720642, -0.013277772814035416, -1.1451970338821411, 0.9467775821685791, -0.07726122438907623, 0.20617181062698364, 0.09377758204936981, -1.6318727731704712, 0.3102971911430359, -0.10269970446825027, -0.4252545237541199, -0.4577384293079376, 0.2554008662700653, 1.2750645875930786, -1.5026042461395264, -0.005474469158798456, 0.20061403512954712, 0.45835718512535095, -1.0228899717330933, 1.293184757232666, -0.5572265386581421, 0.7840022444725037, -0.23029491305351257, -0.6408591866493225, -0.28737515211105347, -0.45634543895721436, 0.472909539937973, -0.40148279070854187, 0.2879367768764496, 0.7072147130966187, -0.3615993559360504, 1.8972203731536865, -0.8378824591636658, 0.6329418420791626, -0.09021051228046417, -0.6277687549591064, 0.03609027713537216, 0.4831696152687073, -0.678057074546814, -0.31513991951942444, -0.0811758041381836, 0.24735389649868011, -0.3883555829524994, -0.4644017219543457, 0.6679741740226746, 1.182296633720398, -0.3197631537914276, 0.17374102771282196, 0.5199005007743835, -0.24884319305419922, 0.6173166036605835, 0.7778964638710022, 0.569625973701477, 1.1294472217559814, 0.289681077003479, 0.259788453578949, 0.36577779054641724, -1.1219980716705322, -0.5091376900672913, 0.41333669424057007, 0.5595870614051819, 0.7535679340362549, 0.2996763288974762, -0.6743847727775574, -0.35638463497161865, -0.05482683703303337, 1.052304983139038, 1.4372738599777222, -0.032588571310043335, -0.106206975877285, -0.828546404838562, -0.8649994134902954, -0.24535833299160004, 0.487115740776062, -0.714262843132019, 0.2539964020252228, -0.7175561785697937, -0.2348257452249527, 0.4876742959022522, 0.8981705904006958, 0.8947944641113281, -0.9129320979118347, -0.5640011429786682, -0.23290586471557617, 0.34467774629592896, -0.9981151223182678, -0.9112932085990906, 0.22219504415988922, -0.5374923944473267, -0.5383773446083069, 0.16963043808937073, -0.2692566215991974, 0.3397620618343353, -0.6566115617752075, 0.8933360576629639, -0.6041381359100342, -0.31907084584236145, 0.3528038561344147, 0.3916172981262207, -0.30825817584991455, -0.7916184663772583, 0.05467740818858147, 0.03394463658332825, -0.5063257813453674, 0.12077351659536362, 0.38473284244537354, -0.2231084704399109, -0.29217323660850525, -0.31205812096595764, 0.31879791617393494, 0.14655566215515137, -0.06668171286582947, 0.4736841917037964, -0.8740233778953552, 0.3791411519050598, -1.1972568035125732, 0.8402504324913025, 0.17659629881381989, -0.36943599581718445, 0.2254951298236847, -0.5289196372032166, -0.52382493019104, 0.6367077827453613, -0.6614391803741455, -0.19266179203987122, -0.8400979042053223, 0.28910812735557556, -0.31945258378982544, -0.5396472811698914, 0.17748744785785675, 0.0988226905465126, 0.5743648409843445, 0.29111167788505554, 0.08087655901908875, 0.4439619183540344, -0.14345349371433258, 0.8862361311912537, -0.9412997961044312, 0.31898725032806396, 0.31066346168518066, 0.11707871407270432, -0.460838258266449, -0.07555615156888962, -0.4852210581302643, -0.48268237709999084, -0.23008811473846436, -0.5504472255706787, -0.4707340896129608, 0.1989487260580063, -0.518447756767273, -0.9260963797569275, -0.5837300419807434, -1.1497983932495117, -0.6141871213912964, 0.10535722225904465, -0.13983401656150818, -0.16774968802928925, -0.8119032979011536, -0.7851352095603943, -0.2789120078086853, -0.21785734593868256, -1.2815020084381104, 0.6624590158462524, 0.13704323768615723, -0.770343005657196, -1.0521339178085327, 0.2893364727497101, -0.35297179222106934, 0.7777383923530579, -0.5185028910636902, 0.43413689732551575, -0.21480505168437958, -0.20804473757743835, -0.3228065073490143, 0.3244660496711731, 0.4693882465362549, 0.0032657852862030268, 0.09406908601522446, -0.8697252869606018, -0.046355072408914566, -0.5389567017555237, -0.644031822681427, -0.19053712487220764, 0.025732997804880142, 0.5165195465087891, 0.11765991896390915, -0.6389660835266113, 0.08914674818515778, 1.2094993591308594, -0.47618013620376587, -0.16058948636054993, -0.20339418947696686, 0.948958158493042, 0.2565661072731018, 0.09847166389226913, 0.04171931743621826, 0.3360772430896759, 0.7207722663879395, 0.00929608941078186, 0.23276768624782562, -0.22126922011375427, -1.0796706676483154, 0.8258016109466553, 1.3181191682815552, 0.6441410183906555, 0.3230486512184143, -0.6057515144348145, 0.6399358510971069, -1.4213658571243286, -0.5206172466278076, 0.8616816997528076, 0.7216538190841675, 0.22824005782604218, -0.5821349620819092, -0.07621629536151886, -0.4340769350528717, 0.19007991254329681, 0.44779708981513977, -0.5485586524009705, -0.5083568096160889, 0.5069417357444763, 0.17366841435432434, -0.6055527329444885, 1.1562408208847046, -0.41547971963882446, 0.12514182925224304, 14.439083099365234, 0.39387157559394836, 0.13751175999641418, 0.6186481714248657, 0.5152099132537842, 0.06659446656703949, -0.25535112619400024, -0.3248005509376526, -1.2960177659988403, 0.01274010632187128, 1.077565312385559, 0.7430864572525024, 0.458503395318985, 0.05004020407795906, 0.43924105167388916, 0.14659720659255981, -1.636253833770752, 0.6630088090896606, 0.7806215882301331, -1.0531675815582275, 0.20947682857513428, -0.037732526659965515, 0.21251718699932098, 0.44159919023513794, 0.967722475528717, 1.1117209196090698, 0.20158632099628448, -0.3666864037513733, 0.31406018137931824, 0.016051020473241806, 1.188660979270935, 0.03186141327023506, 0.24176712334156036, 0.82313072681427, -0.6679909825325012, -0.23101381957530975, -0.22511351108551025, -1.1975737810134888, 0.48256102204322815, -0.40048202872276306, -0.9387348890304565, -0.28636297583580017, -0.27376386523246765, 0.8757185935974121, 0.02510182186961174, 0.04294685274362564, -0.7720939517021179, 0.753793478012085, -0.014109516516327858, -0.4768589437007904, 0.5651998519897461, 0.31160783767700195, 0.11654385179281235, 0.19045601785182953, -0.05815451964735985, -0.04845556244254112, 0.13922707736492157, 0.6847753524780273, -0.2774849832057953, -0.11891093850135803, -0.5271917581558228, -0.01214440818876028, 0.05616844817996025, 0.5810149312019348, 0.9198910593986511, 0.18530143797397614, -0.6168848872184753, 0.3578181266784668, 0.8536049723625183, 0.5228977799415588, -0.22220084071159363, 0.050464943051338196, -0.07511672377586365, -0.6378107070922852, 0.22619092464447021, 0.39829641580581665, 0.42670345306396484, -0.5180147886276245, -0.18569478392601013, -0.1568751484155655, -0.08018053323030472, -0.9966035485267639, -0.8009343147277832, 0.5582121014595032, 0.18460945785045624, -0.7113457918167114, -0.117927186191082, -0.6600440144538879, -0.5762852430343628, 0.2749660611152649, -1.34207284450531, -0.8199215531349182, 0.16062498092651367, -0.42343994975090027, 0.07859533280134201, 0.035583339631557465, 1.305444359779358, 0.21959292888641357, -0.3540438115596771, 0.2206893265247345, 0.0010921305511146784, -0.18203392624855042, 0.1431567221879959, -1.1733570098876953, 0.9484051465988159, -0.07350216060876846, 0.08574722707271576, 0.2837297320365906, 0.03002767264842987, 0.2668074071407318, -0.6522483825683594, -0.07573743164539337, 0.6888614296913147, -1.106399655342102, -0.4094958007335663, -0.7829854488372803, -0.5956970453262329, 0.6192481517791748, 0.8005450367927551, -0.06328881531953812, 0.600651741027832, 0.23480895161628723, -0.7147191762924194, -0.15198014676570892, -0.7792490720748901, 0.394737184047699, 0.505003035068512, -0.36673083901405334, -0.681921660900116, 0.03271351382136345, 0.8699564337730408, -0.9888483285903931, -0.24005413055419922, -0.32572057843208313, 0.05670127645134926, -0.09820708632469177, 0.8966193795204163, -0.437258243560791, 0.7354305386543274, 0.6413486003875732, -0.05338013544678688, -0.9754829406738281, 0.37655365467071533, -0.8977252840995789, -0.19525134563446045, -0.04566144943237305, 0.8752862215042114, -0.2952592968940735, 0.2810696065425873, 0.6216711401939392, 0.36357536911964417, -0.7661439776420593, -0.35694214701652527, -0.5035960078239441, 0.02713777683675289, -0.2530505359172821, 0.1673121303319931, -0.24958497285842896, 0.13247829675674438, 0.5640871524810791, 0.6969980597496033, 0.695685863494873, -0.03861335664987564, -0.9376606345176697, 0.7877857685089111, 0.4153454601764679, -0.47038814425468445, -0.3847796618938446, -0.30060648918151855, -1.7926533222198486, 0.11928632110357285, -1.2993311882019043, 0.0021453809458762407, -1.1022945642471313, -0.3300345540046692, -0.027732403948903084, -0.14810821413993835, -0.2776631712913513, -0.0753859132528305, -0.6029122471809387, -0.11665761470794678, -0.6391653418540955, -1.0436522960662842, 0.7498075366020203, 1.157160758972168, -0.6179473400115967, 0.19545452296733856, -0.1141187995672226, 0.36761674284935, 0.09475665539503098, 0.213798388838768, -0.11628944426774979, -1.2168270349502563, -1.3005850315093994, 0.28343629837036133, 0.0028129133861511946, 0.1125212237238884, -0.5790773034095764, 0.5720658302307129, 0.7817709445953369, -0.04221254959702492, -0.3909938931465149, 0.5485182404518127, -0.900132417678833, -0.6348393559455872, 0.29624250531196594, -1.079339861869812, 0.3318330943584442, 0.22551825642585754, 0.013189728371798992, -0.28485751152038574, 0.4779900908470154, -0.4109388291835785, -0.7386864423751831, -1.0652494430541992, 0.20090007781982422, -0.11451112478971481, 0.16092440485954285, -0.0649532750248909, 0.14506743848323822, -0.9185920357704163, -0.4597512483596802, -0.14545512199401855, 0.7138839364051819, -0.5216516852378845, 0.7845499515533447, 0.556165337562561, -1.121793270111084, 0.06061672791838646, 0.2292690873146057, 0.033844515681266785, 0.3229697644710541, 0.8559659719467163, 0.33816054463386536, -0.3000543713569641, 0.37024593353271484, 0.5131288766860962, -0.025318866595625877, -1.1906037330627441, 0.23054815828800201, 0.7494990229606628, -0.5813433527946472, -0.31004464626312256, 1.0744991302490234, -0.3125822842121124, -1.5376203060150146, 0.518337607383728, -1.154817819595337, -0.5666142106056213, -0.6749745011329651, 0.9166486859321594, 0.00914038997143507, -0.1978413611650467, 0.3406426012516022, -0.33947184681892395, 0.46744436025619507, -0.2657712399959564, -0.7329363822937012, 0.3311913013458252, -0.16524779796600342, 0.05525892227888107, 0.8579665422439575, 1.2285640239715576, -0.4412303566932678, -1.1972205638885498, -0.5933221578598022, -0.6406103372573853, -0.04841497540473938, -0.39015740156173706, -0.7417096495628357, -0.08941346406936646, 0.8944736123085022, 0.25151327252388, 0.07066880911588669, -0.3676941394805908, 0.27748382091522217, 0.08535964041948318, 1.1027618646621704, 0.02774495631456375, -0.46013936400413513, 0.008129899390041828, 1.017898440361023, 1.6167789697647095, -1.274800181388855, 0.17832835018634796, -0.19142670929431915, -0.5997923016548157, 0.7386847138404846, 0.5844722390174866, -0.4181402325630188, 0.870560348033905, -0.20692428946495056, 0.22060486674308777, 0.1957908719778061, -0.9212493896484375, 0.5450396537780762, 0.5165194869041443, 1.4282951354980469, 0.723702609539032, 0.7262144684791565, 0.3772347569465637, 0.6428356170654297, 0.15308216214179993, 0.0031712211202830076, 0.34055379033088684, 0.24411948025226593, 0.05599392578005791, -0.35289663076400757, 0.20998650789260864, 0.28516021370887756, -0.15371067821979523, -0.49191364645957947, 0.44659897685050964, 0.8327130675315857, 0.17397619783878326, 0.6334949135780334, 0.660368800163269, 0.36039507389068604, 0.7217515110969543, 0.24907173216342926, 0.8298585414886475, -0.8426326513290405, -0.18040871620178223, -0.5746325254440308, -0.6296814680099487, -0.2710438668727875, -0.37567973136901855, -0.6602320671081543, -0.504081666469574, 0.4731083810329437, 0.45913761854171753, -0.22501426935195923, 0.39668750762939453, 1.1051623821258545, 0.7443647384643555, 0.6741659641265869, -0.36716240644454956, -0.8886179327964783, -0.6492097973823547, -1.3356174230575562, 0.3967178165912628, -0.15383736789226532, 0.03987279161810875, -0.367636114358902, -0.2781522274017334, -0.3681526780128479]}, "authors": [{"authorId": "123970124", "name": "Tianle Cai"}, {"authorId": "2112769495", "name": "Kaixuan Huang"}, {"authorId": "100811091", "name": "Jason Lee"}, {"authorId": "2145361472", "name": "Mengdi Wang"}], "references": [{"paperId": "35922cd0d6b17e45320917338e9f98cb5c1a4f6f", "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters"}, {"paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc", "title": "Reasoning with Language Model Prompting: A Survey"}, {"paperId": "0942bd8fad71282994ff4e9a779c09745da68edc", "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "eecb45aa040064cbc0b37fd100706c02e7dc880e", "title": "Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "b9ec37d028fae61752c33a55fb88bd27e6cb8c4d", "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners"}, {"paperId": "b65b7f480a61d3dd31d8117b349cabc87c8ccf6c", "title": "Bidirectional Language Models Are Also Few-shot Learners"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "81986b8a3d3fe6c5be06fc4527953fb514ad12e8", "title": "Improving In-Context Few-Shot Learning via Self-Supervised Training"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7", "title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab", "title": "AdaPrompt: Adaptive Model Training for Prompt-based NLP"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "f9838a3be5c94bb2674a0e224de349b50e18f3c4", "title": "Learning To Retrieve Prompts for In-Context Learning"}, {"paperId": "d9cdf21e73519edc593bdf1a00fcd778764b13f6", "title": "Training Neural Networks with Fixed Sparse Masks"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "eea7bca03bda3ee2448cd012bbcb2b33822861d8", "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493", "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"}, {"paperId": "9d81bc8bebf1beb936427c224afb219b54a64f1e", "title": "Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "ea8c46e193d5121e440daf96edfd15a47151c293", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "48530f3d6425f2f150f07ccdd61ba951951a0a7d", "title": "Simple, Scalable Adaptation for Neural Machine Translation"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d89ee98810039d2061ed42ee8026da49c503d16b", "title": "Learning multiple visual domains with residual adapters"}, {"paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725", "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "a131c44951b7ace0892dd830dd0a040b99ed0803", "title": "Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning"}, {"paperId": "0eedbc38bc215fdbe4e5bcde8aeac08fb3ce9f44", "title": "Parallel Context Windows Improve In-Context Learning of Large Language Models"}, {"paperId": null, "title": "Investigating fusion methods for in-context learning. preprint under review, 2022a"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": null, "title": "What makes good in-context examples for gpt3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "2021) constrains the effective context, and aggressive truncation is needed"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "or adding a few additional tunable parameters and fixing the original model"}, {"paperId": "cb15518b8a848dc68d1c5fab02414d205ccdcb67", "title": "Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}