{"paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2", "abstract": "Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $L$ attention layers, we prove certain critical points of the training objective implement $L$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 72, "influentialCitationCount": 14, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.00297", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work makes the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression, and proves the global minimum of the training objective implements a single iteration of preconditioned gradient descent."}, "embedding": {"model": "specter_v2", "vector": [0.2797127962112427, 0.6324957013130188, -0.6871870756149292, -0.296548992395401, -0.4900954067707062, 0.21110104024410248, 0.1309116780757904, -0.2975099980831146, -0.2714480757713318, -0.2176152616739273, 0.2114778608083725, -0.023157283663749695, 0.1527017503976822, -0.040469564497470856, -0.4764094054698944, -0.23942548036575317, -0.9836746454238892, -0.047289326786994934, -0.3718482553958893, -0.13368654251098633, -0.14688588678836823, -0.3800313472747803, -0.8121597766876221, 0.17422160506248474, 0.5052346587181091, 0.8505105972290039, -0.12299709022045135, 0.49828851222991943, -0.324464350938797, 0.7586971521377563, 0.6699166297912598, -0.1474536806344986, 0.41723427176475525, 0.21313510835170746, -0.25369229912757874, -0.15027408301830292, 0.7358763217926025, -0.34761956334114075, -0.33372992277145386, 0.8551524877548218, -0.42158621549606323, -0.046684201806783676, 0.28307074308395386, -1.1401026248931885, -0.13849519193172455, 0.6803346276283264, 0.47021034359931946, 0.5711678266525269, -0.3035427927970886, -0.7783725261688232, 1.8575000762939453, -1.3363981246948242, -0.0797390267252922, 0.8276993036270142, 0.8070763349533081, 0.44694167375564575, -0.5360249280929565, -0.35190823674201965, 1.0318750143051147, 0.08681647479534149, -0.4811999499797821, -0.07438767701387405, -0.15766797959804535, 0.03856116533279419, 1.556223750114441, -0.3749472200870514, 0.1171754002571106, 0.5529058575630188, 0.11188605427742004, 1.530877947807312, 0.4214557111263275, -0.7644931674003601, -0.1743689328432083, 0.4770757555961609, 0.37462595105171204, 0.8892776966094971, -0.40604427456855774, 0.5152496695518494, -0.9254541993141174, -0.11835295706987381, 0.24708068370819092, 0.35602739453315735, 0.052158720791339874, -0.6744857430458069, 0.029343418776988983, 0.6545136570930481, 0.6733502149581909, 0.8812949061393738, -0.5288719534873962, 1.0900853872299194, 0.5813741087913513, 0.6853373050689697, 0.01010855007916689, 0.34922897815704346, -0.15890520811080933, 0.3027319610118866, -0.6800644397735596, -0.03697481378912926, -0.06946289539337158, 0.4894537329673767, -0.018421761691570282, 0.15930698812007904, -0.4538956880569458, 0.31310051679611206, 1.1835224628448486, 0.2836132049560547, 0.4267142713069916, -0.5710115432739258, 0.5559746026992798, -0.3427657186985016, 0.2956760823726654, -0.47182196378707886, -0.22588805854320526, -0.44583696126937866, -0.6983010172843933, -0.6888024806976318, -0.25040656328201294, -0.07201375812292099, -0.4212762415409088, 0.7726768255233765, -0.24050182104110718, 0.14217233657836914, -0.024348586797714233, 0.6861535906791687, -0.3397076427936554, 0.529525101184845, 0.0323282852768898, -0.03939525783061981, 0.3620043098926544, -0.5872591137886047, -0.5504105687141418, -0.9787152409553528, 0.7537485957145691, 0.06342380493879318, 0.7589492201805115, -0.017834579572081566, -0.8185665607452393, -0.8022066950798035, -1.1585172414779663, 0.48355790972709656, -0.3197805881500244, 0.0092427097260952, 1.0598294734954834, 0.9276158809661865, -0.9912740588188171, 0.9642170667648315, -0.6377151012420654, -0.055898766964673996, 0.6191157102584839, 0.6324954628944397, 0.2973076105117798, -0.2970317304134369, -1.0966565608978271, 0.4798257052898407, 0.0007961678202264011, -0.3987976908683777, -0.6116567254066467, -0.9182227849960327, -0.9981496930122375, 0.010503282770514488, 0.508809506893158, -0.45923611521720886, 1.2616350650787354, -0.704168438911438, -1.2099307775497437, 0.5661466717720032, -0.01257756445556879, 0.014503024518489838, 0.757597029209137, -0.5274714827537537, -0.33135896921157837, -0.5037363171577454, -0.038534242659807205, -0.1408388465642929, 0.357353150844574, -0.5761803388595581, -0.3684042692184448, -0.19210998713970184, -0.3783329427242279, -0.11262780427932739, -0.2921324372291565, 0.5721349120140076, 0.25443035364151, 0.07582274079322815, 0.2754247188568115, 0.835770845413208, -0.44282665848731995, -0.18719640374183655, -0.38044217228889465, -0.8557493686676025, 0.7279210090637207, 0.18094082176685333, 1.0219560861587524, -0.9176991581916809, -0.8424564599990845, 0.515836238861084, -0.10686872899532318, -0.24916493892669678, -0.6690642237663269, 0.5629050135612488, -0.39684736728668213, 0.40794411301612854, -0.06435150653123856, -0.7811041474342346, 0.3240015506744385, -0.09497585892677307, -0.9712549448013306, -0.0803125873208046, -0.11103297770023346, 0.7021670937538147, -1.0224876403808594, 0.24377727508544922, 0.20719578862190247, 0.34387272596359253, -1.0077183246612549, 1.3855513334274292, -0.08772365003824234, -0.0985737293958664, -0.35922086238861084, -0.4352479577064514, -0.028521787375211716, -0.4734799265861511, 0.5249534845352173, -0.2392120659351349, 0.2644495368003845, 0.36955031752586365, -0.6679280400276184, 0.95207279920578, -1.2365552186965942, 0.3113952577114105, 0.07321272045373917, -1.2135121822357178, -0.12626433372497559, 0.5482279062271118, 0.15491893887519836, -0.42679864168167114, 0.06259024143218994, 0.266388475894928, -0.9410920739173889, 0.08639764040708542, 0.4384574294090271, 0.38070905208587646, -0.1092890277504921, 0.37538832426071167, 0.4818612039089203, -0.17124974727630615, -0.06993937492370605, 0.07075363397598267, 0.597578227519989, 0.6350939273834229, 0.3859711289405823, 0.010621999390423298, 0.11090248823165894, -1.0511876344680786, -0.0037718492094427347, 0.8800032734870911, 0.8935964703559875, 0.4667438268661499, 0.5236151218414307, -0.7064700722694397, -0.18393930792808533, -0.15569797158241272, 0.552929699420929, 1.5378175973892212, -0.4315914809703827, -0.4278869032859802, -0.8450464010238647, -0.664283275604248, -0.2713829576969147, 0.5635149478912354, -0.465405136346817, 0.06356284022331238, -0.2500283420085907, -1.4887441396713257, 0.5665987730026245, 0.812261700630188, 1.036354660987854, -0.10131743550300598, -0.3041592836380005, -0.060037847608327866, 0.48317649960517883, -0.6891630291938782, -0.4793106019496918, 0.6698427200317383, -0.839647114276886, -0.3411746025085449, 0.18660655617713928, -0.15941894054412842, 0.5096865296363831, -1.0160179138183594, 0.691335916519165, -0.1317228227853775, -0.10592525452375412, 0.7702881693840027, 0.9566648602485657, -0.34722700715065, -0.48226332664489746, 0.3559127151966095, -0.048573434352874756, 0.01505941804498434, 0.07136593014001846, -0.04054282605648041, 0.12867559492588043, 0.14862821996212006, -0.2852541506290436, 0.10249978303909302, 0.026627449318766594, 0.44392725825309753, 0.3263602554798126, -0.3104937970638275, 0.07654553651809692, -1.41416597366333, 1.1016367673873901, 0.1220359057188034, -0.5912997722625732, 0.4157661497592926, -1.1742843389511108, -0.09485682100057602, 0.7764465808868408, -0.7090288996696472, -0.025883764028549194, -0.8007552027702332, 0.19354148209095, -0.16661207377910614, -0.1377294361591339, 0.20886114239692688, 0.15669487416744232, 0.1550815850496292, 0.601399838924408, 0.15377579629421234, 0.17435185611248016, 0.18335895240306854, 0.6064019203186035, -0.9379686713218689, -0.08043863624334335, -0.30521878600120544, 0.27018794417381287, -0.40180641412734985, 0.017013071104884148, -0.5446734428405762, -0.6713376641273499, -0.052529025822877884, 0.05475470796227455, -0.17992858588695526, -0.3599691092967987, -0.3766557574272156, -1.1044743061065674, -0.2509348392486572, -0.20128318667411804, -0.8687252998352051, -0.32823145389556885, -0.5544657707214355, -0.6127646565437317, -1.4519047737121582, -1.1928199529647827, -0.6496849656105042, -0.2623675465583801, -1.0591789484024048, 0.2825958728790283, -0.1633603721857071, -0.06548798829317093, -0.9179961085319519, -0.43937528133392334, -0.5321655869483948, 1.276233434677124, -0.4115269184112549, 0.6861629486083984, -0.08928672969341278, -0.3945469260215759, -0.0712023451924324, 0.34371817111968994, 0.6959792375564575, -0.13680538535118103, -0.16225874423980713, -1.1310826539993286, 0.22854270040988922, -0.18701986968517303, -0.6163626909255981, -0.11786109209060669, 0.15886549651622772, 1.032658576965332, -0.34810248017311096, -0.613865852355957, 0.20698028802871704, 1.7295619249343872, -0.6323197484016418, 0.10532326251268387, 0.24496516585350037, 1.01145339012146, -0.04530256614089012, -0.12714123725891113, 0.2528625726699829, -0.04981078580021858, 0.3115168511867523, 0.01242909487336874, -0.13659273087978363, 0.30413544178009033, -0.7221122980117798, 0.5176268815994263, 1.0580953359603882, 0.19647644460201263, 0.4686061143875122, -0.9395668506622314, 0.4209449887275696, -1.4080641269683838, -0.717931866645813, 1.0283819437026978, 0.9122708439826965, 0.295877069234848, -0.3115088939666748, -0.23884323239326477, 0.29990530014038086, 0.16889217495918274, 0.27062955498695374, -0.16403059661388397, -0.44634371995925903, 0.4102049767971039, 0.8245574235916138, 0.5640043616294861, 1.0706117153167725, -0.1137402206659317, 0.31269070506095886, 15.074810981750488, 0.7282168865203857, -0.2726859152317047, 0.6663053035736084, 0.9150481820106506, 0.5495538115501404, -0.41955918073654175, 0.07091663032770157, -0.8565950393676758, -0.1024588793516159, 0.9013535380363464, 0.4723460376262665, 0.9377126693725586, 0.3432159125804901, -0.26422959566116333, 0.2259192317724228, -0.8826712369918823, 0.35623306035995483, 0.1841953545808792, -1.0367939472198486, 0.05998888611793518, 0.22831155359745026, 0.3509064018726349, 0.4141949415206909, 0.9134343266487122, 0.6976772546768188, 0.7396833896636963, -0.5054370760917664, 0.30873429775238037, 0.22656996548175812, 1.1150752305984497, -0.2194550484418869, 0.3552890717983246, 0.4430323541164398, -0.5158160924911499, -0.35728487372398376, -0.30136504769325256, -0.9911217093467712, -0.03632282838225365, 0.4855084717273712, -0.6495499610900879, -0.349165678024292, -0.09511620551347733, 0.773948073387146, 0.5375029444694519, 0.018904443830251694, -0.6249350309371948, 0.8478862643241882, -0.44648000597953796, 0.05337905511260033, 0.06434998661279678, 0.3997668921947479, -0.03374703228473663, -0.1801072657108307, 0.1472708284854889, -0.2509814500808716, 0.530971109867096, 0.2454119473695755, -0.4169010818004608, 0.2649766802787781, -0.3048706352710724, 0.032466523349285126, -0.3099268674850464, 0.7595010995864868, 0.7566109895706177, 0.37250638008117676, -0.21818692982196808, 0.11747539788484573, 0.6593974232673645, 0.33027157187461853, -0.04540397971868515, -0.11340441554784775, 0.4948708713054657, -0.3835084140300751, -0.17969532310962677, 0.7453798055648804, -0.34097597002983093, -0.719639003276825, -0.6647082567214966, -0.46781468391418457, 0.4882388114929199, -0.8631700873374939, -1.1933091878890991, 0.4270573556423187, -0.22034746408462524, -0.23904432356357574, 0.6457235217094421, -0.9161359667778015, -0.23963046073913574, 0.17103958129882812, -1.2595106363296509, -0.5982925891876221, 0.31633463501930237, -0.16125090420246124, -0.7217533588409424, 0.20500953495502472, 1.2202181816101074, 0.12637372314929962, -0.4610341191291809, 0.321956604719162, 0.0783279687166214, 0.011701238341629505, -0.22402721643447876, -1.5462886095046997, 0.5419867634773254, 0.19381141662597656, 0.18995578587055206, 0.5138380527496338, -0.05445621535181999, 0.5948042869567871, -0.540591835975647, 0.09860221296548843, 0.47335994243621826, -0.9593927264213562, -0.07734158635139465, -0.6615385413169861, -0.6048156023025513, 0.840861976146698, 0.4611092805862427, -0.11601259559392929, 0.5064735412597656, 0.6539871096611023, -0.6084333062171936, -0.8853691816329956, -0.9004509449005127, 0.18688486516475677, 0.7707765102386475, -0.5981132388114929, -0.40240994095802307, -0.15031257271766663, -0.26360154151916504, -1.142754077911377, -0.689986526966095, -0.3173604905605316, 0.021325087174773216, -0.2748374044895172, 0.9564856290817261, -0.5200523734092712, 0.7073266506195068, 0.7155722975730896, -0.17936307191848755, -1.0421336889266968, 0.20481926202774048, -0.8931757807731628, 0.4622604250907898, 0.27924782037734985, 0.5609676241874695, -0.961371660232544, 0.2530103325843811, 0.8157640099525452, 0.39054176211357117, -0.5302929878234863, -0.5688449740409851, 0.20484916865825653, -0.10873083025217056, -0.6770613789558411, 0.5068885684013367, 0.09972207993268967, -0.06951774656772614, -0.049280550330877304, 0.4401153326034546, 0.6367781162261963, -0.03088510036468506, -0.5876439213752747, 0.360300749540329, 0.008029461838304996, 0.06637725234031677, -0.4508240818977356, -0.5719057321548462, -1.5317491292953491, 0.0698370635509491, -1.3433120250701904, 0.09475046396255493, -1.0372889041900635, -0.6926175951957703, -0.4608217477798462, -0.6177833676338196, 0.3771415948867798, 0.13695821166038513, -0.7090659737586975, -0.48956936597824097, -0.8352601528167725, -0.44052788615226746, 0.4351688623428345, 0.4541277289390564, -0.5939674377441406, 0.1994069367647171, 0.34249022603034973, -0.2041318565607071, 0.015464777126908302, 0.5372592210769653, -0.7211220860481262, -0.9086154699325562, -0.640751838684082, 0.722331166267395, -0.24639366567134857, -0.13035689294338226, -0.6215397715568542, 0.4201723337173462, 0.0024273416493088007, 0.2292613834142685, 0.7883657217025757, 0.08689064532518387, -1.5366389751434326, -0.373381108045578, 0.2037244737148285, -1.081138253211975, -0.0362667553126812, -0.15414081513881683, -0.6200512051582336, -0.1308080404996872, 0.542801022529602, -0.015406902879476547, -0.8852542638778687, -0.3440023958683014, 0.2851276993751526, -0.768146276473999, 0.4128890037536621, -0.40226972103118896, 0.0673854649066925, -0.8598085045814514, -0.14976781606674194, -0.06572815775871277, 0.6393410563468933, -0.21400807797908783, 0.4496192932128906, 0.23146328330039978, -1.0707075595855713, 0.5268736481666565, 0.37437155842781067, -0.22678513824939728, -0.05557677894830704, 0.357200026512146, 0.3753693401813507, -0.22928784787654877, -0.2876509428024292, 0.06218782067298889, 0.011890915222465992, -0.6204144358634949, -0.12334888428449631, 1.2429540157318115, -0.8194502592086792, -0.36318638920783997, 0.6372553706169128, -0.3811414837837219, -1.1236414909362793, 0.29099658131599426, -1.5073912143707275, -0.263388067483902, -0.6939650177955627, 0.5711369514465332, 0.04667652025818825, -0.2188204824924469, 0.6802485585212708, -0.32061967253685, 0.5598248839378357, 0.05760494992136955, -0.3553656339645386, 0.5037165284156799, 0.06153483688831329, -0.26071247458457947, 1.0040550231933594, 0.4849681854248047, -0.40858951210975647, -0.8377081155776978, -0.5386657118797302, -0.212758406996727, -0.4642252027988434, 0.20858344435691833, -0.4788822531700134, -1.0687485933303833, 0.885924756526947, 0.9495227932929993, 0.2995130121707916, 0.24709588289260864, -0.4239696264266968, -0.2875611484050751, 0.674548327922821, 0.31012341380119324, -0.7567580342292786, -0.41453808546066284, 0.5405095815658569, 1.1618589162826538, -0.5391767621040344, 0.5498529672622681, -0.3333008587360382, -0.8098703026771545, 0.9899790287017822, 0.3970052897930145, -0.3423674702644348, 0.8835628628730774, 0.06488491594791412, 0.5599117875099182, 0.10794106870889664, -0.7973822355270386, -0.026885487139225006, 0.5714002847671509, 1.2863696813583374, 0.5468149185180664, 0.3054801821708679, 0.5050615668296814, 0.9802072048187256, 0.18408821523189545, 0.3735046684741974, 0.631545901298523, 0.5373578667640686, 0.014121792279183865, -0.1385621726512909, 0.24393367767333984, 0.7620305418968201, -1.0702067613601685, -0.21802276372909546, 0.4377075135707855, 0.8064454197883606, 0.014419496059417725, 0.35591956973075867, 0.23501160740852356, -0.04954957216978073, 0.8576614856719971, 0.37516218423843384, 0.624485433101654, -0.3545922338962555, -0.8609445095062256, -0.7728838324546814, -0.645106315612793, -0.3374018669128418, -0.42006632685661316, -0.2498612105846405, -0.379110187292099, 0.12243860960006714, 0.2944246828556061, 0.15534920990467072, 0.055661510676145554, 0.9950552582740784, 0.30750328302383423, 0.4447735548019409, -0.05675118416547775, -0.27850162982940674, -0.9273115396499634, -0.6163362860679626, 0.14682070910930634, -0.4563206136226654, 0.1800355315208435, -0.03648403659462929, -0.45342355966567993, -0.6128430366516113]}, "authors": [{"authorId": "9036928", "name": "Kwangjun Ahn"}, {"authorId": "2149478480", "name": "Xiang Cheng"}, {"authorId": "1764651", "name": "Hadi Daneshmand"}, {"authorId": "3072326", "name": "S. Sra"}], "references": [{"paperId": "c35c41dc9c93eff38ec8913cb4d639b06a915d33", "title": "Linear attention is (maybe) all you need (to understand transformer optimization)"}, {"paperId": "7368c3cdf7cbed194e96dc4da53ed61f185e3d82", "title": "Replacing softmax with ReLU in Vision Transformers"}, {"paperId": "cbec8bf16a459b0ae38856f604a6a14cd1343477", "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention"}, {"paperId": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8", "title": "Trained Transformers Learn Linear Models In-Context"}, {"paperId": "15288293edeae26dad6e37218cc1c0fc96316635", "title": "Do Transformers Parse while Predicting the Masked Word?"}, {"paperId": "d4c60620570801a231a7756f931dda1740288fb9", "title": "Looped Transformers as Programmable Computers"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737", "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"}, {"paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a", "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "098b96b713aa8a324090662c7f35213512f4525e", "title": "Residual Connections Encourage Iterative Inference"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "3346e72b31399527d037d6ed091a2ab96225e0fd", "title": "Preliminary"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "17594df98c222217a11510dd454ba52a5a737378", "title": "On the computational power of neural nets"}, {"paperId": "38a4f14f8c4933a1aa8781da7d5c746879bbbc85", "title": "Physics of Language Models: Part 1, Context-Free Grammar"}, {"paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922", "title": "Attention is Turing-Complete"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "ee6280a6aef1248c8ccd64dfbcf7e72f0c1bb4f4", "title": "Scaled Least Squares Estimator for GLMs in Large-Scale Problems"}, {"paperId": "8213dbed4db44e113af3ed17d6dad57471a0c048", "title": "The Nature of Statistical Learning Theory"}, {"paperId": "e5cca121e9852018c141ac3bc2cf2c35fea1dc9a", "title": "The Probabilistic Method"}]}