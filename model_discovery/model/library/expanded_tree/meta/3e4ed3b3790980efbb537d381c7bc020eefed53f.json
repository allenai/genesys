{"paperId": "3e4ed3b3790980efbb537d381c7bc020eefed53f", "abstract": "Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. MambaMixer connects selective mixers using a weighted averaging mechanism, allowing layers to have direct access to early features. As a proof of concept, we design Vision MambaMixer (ViM2) and Time Series MambaMixer (TSM2) architectures based on the MambaMixer block and explore their performance in various vision and time series forecasting tasks. Our results underline the importance of selective mixing across both tokens and channels. In ImageNet classification, object detection, and semantic segmentation tasks, ViM2 achieves competitive performance with well-established vision models and outperforms SSM-based vision models. In time series forecasting, TSM2 achieves outstanding performance compared to state-of-the-art methods while demonstrating significantly improved computational cost. These results show that while Transformers, cross-channel attention, and MLPs are sufficient for good performance in time series forecasting, neither is necessary.", "venue": "arXiv.org", "year": 2024, "citationCount": 12, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The results show that while Transformers, cross-channel attention, and MLPs are sufficient for good performance in time series forecasting, neither is necessary."}, "embedding": {"model": "specter_v2", "vector": [0.42658665776252747, 0.06537000089883804, -0.4399895966053009, 0.002929212525486946, -0.48653072118759155, 0.05120502784848213, 1.1314483880996704, -0.23213842511177063, -0.3649132549762726, -0.35943660140037537, 0.31034740805625916, 0.3142434060573578, 0.7321336269378662, 0.3457708954811096, 0.13611887395381927, 0.030289839953184128, -0.9657872319221497, 0.17186667025089264, 0.05003133416175842, -0.18142615258693695, 0.1891283094882965, -0.4076956808567047, -1.1590614318847656, -0.10529281944036484, -0.11997837573289871, 1.388317584991455, 0.1881152093410492, 0.9832159876823425, -0.8764688968658447, 0.43889760971069336, 0.3972894549369812, 0.20357944071292877, 0.5739638805389404, -0.290857195854187, -0.49301886558532715, -0.0626581683754921, 1.0150576829910278, -0.41392913460731506, -0.9027411937713623, 0.8405066132545471, -0.5744524002075195, 0.06719620525836945, 0.2848312258720398, -0.9631077647209167, 0.6112925410270691, 0.6153663992881775, 0.8180930018424988, 0.7877994775772095, -0.7691192030906677, -0.3843323588371277, 1.3163946866989136, -0.7298351526260376, 0.009717550128698349, 1.543328881263733, 0.5063284039497375, 0.6444080471992493, -0.4083292484283447, -0.8850921988487244, 1.2785611152648926, 0.5314409732818604, -0.4292583763599396, -0.38520103693008423, 0.27077949047088623, -0.5115591287612915, 1.7329370975494385, -0.09594374895095825, 0.66148442029953, 0.7581477761268616, 0.19428136944770813, 1.4628028869628906, 0.128777414560318, -0.5721626877784729, 0.1995677947998047, -0.25795847177505493, 0.5581067800521851, 0.6210910081863403, -0.5074430108070374, 0.20574025809764862, -1.0778582096099854, 0.1858183592557907, 0.536761462688446, 0.2684212327003479, 0.6842631101608276, -0.014098726212978363, -0.5703760385513306, 0.48889654874801636, 0.5235458016395569, 0.4952266216278076, -0.4550466537475586, 1.2282464504241943, 0.6151642799377441, 0.20913106203079224, -0.13443346321582794, 0.6621605753898621, -0.10271010547876358, 0.5449040532112122, -0.8522821664810181, 0.12361873686313629, -0.1941700130701065, 0.7866609692573547, -0.3484019339084625, 0.4757584035396576, -0.6155865788459778, 0.020212402567267418, 1.3095202445983887, 0.044575635343790054, 0.6274551749229431, -0.7407099604606628, 0.046600788831710815, -0.7894023656845093, 0.23964816331863403, -0.7756151556968689, -0.2796032428741455, -0.512824296951294, -0.6807107925415039, -0.8426504135131836, -0.3509373664855957, 0.7422951459884644, -1.0648112297058105, 0.8488583564758301, -0.5330219864845276, 0.7450499534606934, 0.010604197159409523, 0.051428914070129395, 0.1762298047542572, 0.8886678218841553, 0.7335099577903748, -0.16296817362308502, 1.107928991317749, -1.0305110216140747, -0.48759371042251587, -1.1368212699890137, -0.22475898265838623, 0.10393168777227402, -0.13559289276599884, -0.10106100887060165, -1.2563940286636353, -1.228264331817627, -0.8400517106056213, 0.3210920989513397, -0.6769611239433289, -0.20322483777999878, 0.8766366839408875, 0.011961488984525204, -0.6589512228965759, 0.8614004850387573, -0.8922746777534485, -0.45712077617645264, 0.8705419898033142, 0.012669481337070465, 0.6900941729545593, 0.21883618831634521, -1.0538091659545898, 0.6766077280044556, 0.11272027343511581, -0.5028945803642273, -0.8342960476875305, -0.28963491320610046, -0.6110284328460693, 0.19294460117816925, -0.1316823661327362, -0.47030502557754517, 1.1239023208618164, -0.5362023115158081, -1.037603497505188, 0.4032912254333496, -0.08338466286659241, -0.49262750148773193, 0.274774432182312, -0.1628452092409134, -0.43887320160865784, -0.22031334042549133, -0.4490112066268921, 0.7647529244422913, 0.8198758959770203, -0.044209521263837814, -0.6035301089286804, 0.0018947095377370715, -0.6201157569885254, -0.23871979117393494, -0.771453320980072, 0.7219253778457642, -0.10241249948740005, -0.20184661448001862, 0.6796901822090149, 0.6115453243255615, -0.288296103477478, -0.24725210666656494, -0.408336877822876, -0.7224481701850891, 0.691157341003418, -0.029761899262666702, 0.6007251143455505, -0.8777251243591309, -0.6640435457229614, -0.30938467383384705, -0.050678156316280365, -0.11575920134782791, -1.0496939420700073, 0.5135159492492676, -0.17591196298599243, -0.08991283178329468, 0.017641276121139526, -1.1320083141326904, -0.049130361527204514, -0.3226509392261505, -0.8330566883087158, -0.03810325264930725, 0.1505116969347, 0.9539650082588196, -0.8146488070487976, -0.0330360047519207, 0.1358077973127365, 0.003410928649827838, -0.6423053741455078, 1.0115606784820557, -0.4425337016582489, 0.1432797610759735, -0.13195112347602844, 0.010559829883277416, -0.11316138505935669, -0.3074842095375061, 0.7297974824905396, -0.6025199890136719, -0.2380085438489914, 0.8353453278541565, -0.4174732565879822, 1.5708247423171997, -0.2116377055644989, 0.8810723423957825, -0.26807790994644165, -1.185918927192688, 0.4742276966571808, 0.34945914149284363, 0.16260629892349243, -0.6146162152290344, 0.4124995470046997, 0.3025220036506653, -1.0031187534332275, 0.33042722940444946, 0.6816139221191406, 0.9206700921058655, -0.4668731987476349, -0.46115314960479736, 0.8571367859840393, -0.4580892324447632, 0.33829304575920105, 0.30440881848335266, 0.7376977801322937, 0.2927044630050659, 0.1528909057378769, -0.12107782065868378, -0.13491424918174744, -0.7125087976455688, 0.2392449975013733, 0.4849255681037903, 0.006618674844503403, 0.8528808355331421, 0.38633641600608826, -0.9719822406768799, -0.6528272032737732, 0.04136999696493149, 0.7113435864448547, 0.9109866619110107, 0.19744513928890228, 0.0056909541599452496, -0.7231901288032532, -0.4446907043457031, -0.3128551244735718, -0.3195841610431671, -0.20182670652866364, -0.1410963088274002, -0.3659016489982605, -0.6879938244819641, 0.4103703200817108, 0.5709861516952515, 1.059885025024414, -0.6470580101013184, -0.6438837647438049, -0.014867771416902542, 0.10342987626791, -0.6315825581550598, -0.2932194173336029, 0.47194793820381165, -0.6177406907081604, -0.06699678301811218, 0.2854645848274231, -0.15574492514133453, -0.18498101830482483, -0.503493070602417, 0.9865102767944336, -0.9496241807937622, -0.48724451661109924, 0.2378576695919037, 0.8936709761619568, -1.0339680910110474, -0.581317663192749, 0.3295830488204956, 0.1507769227027893, 0.008933410979807377, 0.38361233472824097, 0.08307983726263046, -0.37133413553237915, -0.20117254555225372, -0.06237758323550224, 0.3674972355365753, 0.05804000422358513, 0.20670373737812042, 0.7345708608627319, -0.5612912774085999, 0.3378952741622925, -0.5470605492591858, 0.09501568228006363, -0.13232670724391937, -0.9275897741317749, 0.2656845152378082, -0.6140087246894836, -0.3924887776374817, 0.16728514432907104, -0.6837230324745178, 0.08938983082771301, -0.2711860239505768, 0.664670467376709, -0.9757842421531677, -0.47961461544036865, -0.047795966267585754, 0.6102248430252075, 0.333117812871933, 0.2712736129760742, 0.3883127272129059, 0.3660393953323364, 0.039732642471790314, 0.326530396938324, -0.9269177913665771, 0.8475682735443115, 0.7749589085578918, 0.004235506057739258, 0.2949397563934326, 0.20126959681510925, -0.7331686615943909, -0.38058480620384216, -0.6171904802322388, -0.13500718772411346, -0.47429952025413513, 0.46332958340644836, -0.668201744556427, -0.7459666728973389, 0.4747961759567261, -1.0581536293029785, -0.22825676202774048, -0.03815174475312233, -0.03591593727469444, -0.45280298590660095, -1.4794403314590454, -1.0658390522003174, -0.693011462688446, -0.3540818989276886, -0.7338558435440063, -0.14592288434505463, 0.393867552280426, -0.48898178339004517, -0.4746837019920349, -0.08393274247646332, -0.6362451314926147, 1.2004477977752686, -0.23263703286647797, 0.31060972809791565, -0.3373279571533203, -0.5558870434761047, -0.5101910829544067, 0.39342671632766724, 0.5569299459457397, -0.38976994156837463, 0.2706800401210785, -1.2445546388626099, 0.31507110595703125, -0.5126528143882751, -0.4205464720726013, 0.7635486125946045, 0.7537627220153809, 0.8402811288833618, 0.09662416577339172, -0.37449225783348083, 0.3778177797794342, 1.4234473705291748, -0.3758053183555603, -0.005342458840459585, 0.13157249987125397, 0.9678968787193298, 0.06009913235902786, -0.3509350121021271, 0.5625402331352234, 0.28221815824508667, 0.1706821769475937, 0.8432115912437439, -0.40436485409736633, -0.33805662393569946, -0.4458916485309601, 0.03773552179336548, 1.3687878847122192, 0.4398559033870697, 0.15817692875862122, -0.9241045117378235, 1.1012572050094604, -1.206897258758545, -0.893933892250061, 0.8941074013710022, 0.6434563398361206, -0.07854419201612473, -0.42921358346939087, 0.1938169002532959, -0.07606671750545502, 0.6296381950378418, 0.4617142081260681, -0.5938149094581604, -0.42631015181541443, 0.06610043346881866, 0.3962537944316864, 0.4273757040500641, 0.3212215006351471, -0.8647271990776062, 0.4935357868671417, 14.812135696411133, 0.7301385998725891, -0.6191869378089905, 0.39042213559150696, 0.814533531665802, -0.1255122274160385, -0.09685374051332474, 0.11879747360944748, -1.0344747304916382, 0.14730167388916016, 1.3961853981018066, 0.6838402152061462, 0.6371906399726868, 0.20579621195793152, -0.3724846839904785, 0.5695808529853821, -0.7065898180007935, 0.9046928882598877, 0.09503019601106644, -1.5103821754455566, -0.08789694309234619, -0.17309696972370148, 0.4837751090526581, 0.609319269657135, 0.8602370619773865, 0.8313831090927124, 0.3358587920665741, -0.21696309745311737, 0.8472709059715271, 0.3370777666568756, 1.0222501754760742, 0.4580000340938568, -0.2922043800354004, 0.13195231556892395, -1.1767390966415405, -0.4868012070655823, -0.1882966309785843, -1.0746654272079468, 0.17461133003234863, -0.286868155002594, -0.20742076635360718, -0.6716175079345703, -0.24059444665908813, 0.7204570174217224, 0.42838039994239807, 0.37831607460975647, 0.21917331218719482, 0.818240761756897, -0.06804795563220978, -0.032819218933582306, 0.41795629262924194, 0.6805810332298279, 0.2074449360370636, -0.04631563276052475, -0.12124856561422348, -0.13891352713108063, 0.49914997816085815, 0.17922574281692505, -0.34039923548698425, -0.5800121426582336, 0.055662091821432114, -0.5096908211708069, 0.26478517055511475, 0.9712381958961487, 0.05797844007611275, -0.3215878903865814, -0.5114732384681702, 0.1687111258506775, 0.18158569931983948, 0.32695239782333374, -0.34733864665031433, -0.05969519540667534, 0.5066385865211487, -0.8032464385032654, 0.1458638459444046, 0.664238691329956, -0.5035703182220459, -0.45715826749801636, -1.0591710805892944, -0.21546438336372375, 0.5884339809417725, -0.39684823155403137, -0.7256242632865906, 1.0507079362869263, -0.17736934125423431, -0.525087833404541, 0.4895656704902649, -0.634253203868866, -0.42114415764808655, 0.48274537920951843, -1.380489468574524, -0.6216110587120056, -0.36864468455314636, -0.39323410391807556, 0.09908676147460938, -0.32582786679267883, 1.2724162340164185, 0.03258080035448074, -0.5580121874809265, -0.3654654622077942, -0.057130586355924606, -0.26082685589790344, -0.4568388760089874, -0.6983634233474731, 1.2385594844818115, 0.18485312163829803, 0.06857040524482727, -0.5034879446029663, 0.11056111752986908, 0.4095546305179596, -0.36712267994880676, -0.08570612967014313, 0.5190792083740234, -0.553519606590271, -0.25229108333587646, -0.5426846146583557, -0.7732903957366943, 0.28992921113967896, 0.33001139760017395, 0.18690726161003113, -0.10618191212415695, 0.03460007905960083, -0.7495312690734863, -0.40879762172698975, -0.6111128926277161, -0.2694961428642273, 0.08761006593704224, -0.627223014831543, -0.2971750497817993, -0.18928490579128265, 0.10444657504558563, -0.6764072179794312, -0.18563613295555115, -0.38757291436195374, 0.5009914040565491, 0.032684072852134705, 1.1551930904388428, -0.7338986992835999, 0.8825700283050537, 1.0525965690612793, -0.44288134574890137, -0.8148801922798157, -0.1348511427640915, -0.7947385311126709, -0.2096201777458191, 0.17924624681472778, 0.04869156330823898, -0.6273825168609619, 0.27439871430397034, 0.3721894323825836, 0.021120678633451462, -0.35356149077415466, -0.5195700526237488, -0.43458130955696106, -0.17291419208049774, -0.33126476407051086, 0.5516542196273804, 0.1197899878025055, -0.11173882335424423, 0.07436507195234299, 0.4827195703983307, 0.22666136920452118, -0.10221526026725769, -0.37819644808769226, 0.07942935824394226, -0.2638366222381592, 0.029550805687904358, -0.9561217427253723, -0.8804754018783569, -1.4073492288589478, -0.04392063990235329, -0.9378935098648071, 0.2588399350643158, -0.8927097320556641, -0.41793107986450195, -0.14557383954524994, -1.0541660785675049, -0.03889365494251251, 0.26771116256713867, -0.23057565093040466, -0.055906716734170914, -0.31004658341407776, -0.594195544719696, 0.6618519425392151, 0.557247519493103, -0.9906050562858582, -0.0284110177308321, -0.01393709797412157, 0.33008483052253723, 0.3860796391963959, 0.45315784215927124, -0.04707420617341995, -0.5773656964302063, -0.9447108507156372, -0.13282085955142975, 0.2308538258075714, 0.12773430347442627, -1.292703628540039, 0.47543463110923767, -0.02282741293311119, -0.12132256478071213, 0.02001088112592697, 0.8797901272773743, -0.7877919673919678, -0.4398547410964966, 0.6616929769515991, -1.1967267990112305, -0.38001376390457153, 0.27362850308418274, -0.3076130747795105, -0.257511168718338, 1.2372610569000244, 0.21683195233345032, -1.1302613019943237, -1.0567467212677002, 0.5814948081970215, -0.3498692512512207, -0.2588658630847931, 0.08354098349809647, 0.033758096396923065, -1.1895397901535034, -0.11224211007356644, -0.12980616092681885, 0.3784399926662445, -0.7778398990631104, 0.7599222660064697, 0.2716871500015259, -1.233644723892212, 0.002361118793487549, 0.3601333200931549, -0.11279071122407913, 0.018800051882863045, 0.5200222730636597, 0.5254742503166199, -0.18992553651332855, 0.5861573219299316, -0.42320477962493896, 0.03892766684293747, -0.5819573998451233, 0.24563555419445038, 0.8289510011672974, -0.20411106944084167, 0.08264423906803131, 1.2946935892105103, -0.2416689693927765, -0.37450283765792847, 0.45821520686149597, -1.066851258277893, -0.5554641485214233, 0.1323491632938385, 0.5441965460777283, 0.6179016828536987, -0.21998968720436096, 0.2729226350784302, -0.5283564925193787, -0.0023256482090801, 0.09587090462446213, -0.6033918857574463, 0.429376482963562, -0.1525864601135254, 0.05774571746587753, 1.1771130561828613, 0.8220001459121704, -1.348724365234375, -1.4169573783874512, -0.8734479546546936, -0.6671927571296692, -0.2569006085395813, 0.41136497259140015, -0.16780905425548553, -0.8514736890792847, 0.7993481755256653, 1.1704416275024414, 0.834266722202301, 0.33900219202041626, -0.20703130960464478, 0.19126997888088226, 0.2250107377767563, -0.1990801841020584, -0.6656109094619751, -0.3418046534061432, 0.9423649907112122, 1.124176263809204, -0.8983787894248962, 0.034227386116981506, 0.0231857281178236, -0.4536784887313843, 0.6514736413955688, 0.4137391746044159, -0.595715343952179, 0.7116448283195496, -0.6203954219818115, 0.2306460589170456, 0.09209412336349487, -1.26460862159729, -0.2915377914905548, 0.7232884764671326, 0.8692806959152222, 0.12558263540267944, 0.5945541262626648, 0.29919132590293884, 0.44511908292770386, 0.5087253451347351, -0.20812267065048218, 0.14728839695453644, 0.5704303979873657, -0.19128476083278656, 0.35129106044769287, -0.1832222193479538, 0.7472997903823853, -0.4586121439933777, -0.22452685236930847, 0.349511057138443, 0.1734042465686798, 0.029684150591492653, 0.5174150466918945, 1.567280650138855, 0.008695272728800774, 0.3916202485561371, 0.2911495864391327, 0.6379702687263489, -0.27556121349334717, -0.07400532066822052, -0.11106229573488235, -0.8538005352020264, -0.3228691518306732, -0.44094425439834595, -0.990077793598175, -0.013502448797225952, 0.04737330973148346, 0.20218691229820251, -0.24903804063796997, 0.8124319314956665, 1.0416654348373413, 0.3724989593029022, 1.0650511980056763, 0.22010073065757751, -0.8979523777961731, -0.6384285688400269, -0.9398024082183838, -0.032940156757831573, -0.2665357291698456, 0.41906535625457764, -0.3559190332889557, 0.05806629732251167, 0.04722888022661209]}, "authors": [{"authorId": "46211294", "name": "Ali Behrouz"}, {"authorId": "2253701357", "name": "Michele Santacatterina"}, {"authorId": "2188653630", "name": "Ramin Zabih"}], "references": [{"paperId": "ba4c5a116d07b37dea1046b6d16a60cb2d01cd47", "title": "Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges"}, {"paperId": "9898fa5e103efc283fa013ac4e5acddff8de04e8", "title": "Efficient Modulation for Vision Networks"}, {"paperId": "da9178eae82d1ca5492aaecd0151ba49481cb8b1", "title": "Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation"}, {"paperId": "5867382590f9f0ff8caf15804d20bde10845b2d2", "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan"}, {"paperId": "0a32e6ff6eaac83ff325bae4557a8362222979aa", "title": "Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding"}, {"paperId": "76ee7943528dc087766fd66e4312884d95ad3c32", "title": "ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions"}, {"paperId": "6c1578d9eff8f9d25ddf0398a77ffcc888a4593b", "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling"}, {"paperId": "26e6cd121c5fdb147df83cb848e4813c926737c8", "title": "The Hidden Attention of Mamba Models"}, {"paperId": "241110426ec211c3e6c8faa2af6a92aa5cd6d477", "title": "Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling"}, {"paperId": "c8f5f6d981d96ebc84ba0a2665f00be3d1cfc27e", "title": "SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention"}, {"paperId": "2dda6da7375bf5e8bcf60f87b17ba10757f3bc57", "title": "Graph Mamba: Towards Learning on Graphs with State Space Models"}, {"paperId": "906d0688e1c683d5fec70e88e71ea1291c666b78", "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data"}, {"paperId": "b32fdc4b6d62edb015ed1242940c0005109fb7e1", "title": "DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging"}, {"paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97", "title": "VMamba: Visual State Space Model"}, {"paperId": "c1a04730c83967d0bb904b02263b17893cb50bad", "title": "U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation"}, {"paperId": "fe9722f5d29f38d3a9ecd65f6c1fe55b81903791", "title": "MossFormer2: Combining Transformer and RNN-Free Recurrent Network for Enhanced Time-Domain Monaural Speech Separation"}, {"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "434d751d355d7a7c20efa570e785c76286245e77", "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"}, {"paperId": "5ad84fe07a569b6cc339d77cd2c265c1b79d644d", "title": "Convolutional State Space Models for Long-Range Spatiotemporal Modeling"}, {"paperId": "c85268696fe1435605ae66a18653cfdcf8153753", "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, {"paperId": "412027f0a7db3188ae12aca4a1a0b51430a737e3", "title": "On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "198fb07ae84ca36f98a362058fb8a90e214bea06", "title": "RepViT: Revisiting Mobile CNN From ViT Perspective"}, {"paperId": "918617dbc02fa4df1999599bcf967acd2ea84d71", "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "7a816dd242c4c3f652a448dba54daa53f89a9e4f", "title": "Soft Merging of Experts with Adaptive Routing"}, {"paperId": "9c703f8d5ad3ba10564f25ffae62cf8a71f1f6cb", "title": "Lightweight Vision Transformer with Bidirectional Interaction"}, {"paperId": "32d0ad59162b07bf469b6889930ef1d10e66f7f8", "title": "Brain encoding models based on multimodal transformers can transfer across language and vision"}, {"paperId": "6b655b74da908e2e0d7c37996d36d5ff4a28f030", "title": "Sequence Modeling with Multiresolution Convolutional Memory"}, {"paperId": "52cc542b60c99e8866a7fc515c10f443089005f2", "title": "FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization"}, {"paperId": "a7d68b1702af08ce4dbbf2cd0b083e744ae5c6be", "title": "Effectively Modeling Time Series with Simple Discrete State Spaces"}, {"paperId": "59694c8dce4f13db2f486eb8102459a3f7c23da6", "title": "TSMixer: An all-MLP Architecture for Time Series Forecasting"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "95d6d5ecb6ec57cf913e955107070496450cfd4c", "title": "MossFormer: Pushing the Performance Limit of Monaural Speech Separation Using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc", "title": "Pretraining Without Attention"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "dad15404d372a23b4b3bf9a63b3124693df3c85e", "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"}, {"paperId": "4b5d5382d66ac8f9f160cf7b57507a26b7334b2e", "title": "QDPN - Quasi-dual-path Network for single-channel Speech Separation"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "3e448df5aa191f7a3945d0fd609c8bc5966a2333", "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"}, {"paperId": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4", "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "dbf6e95cb618f207f029276a6df11f4a9a6313d4", "title": "Inception Transformer"}, {"paperId": "defecf3dc299214f4cb76a093c6eed2297eaa46f", "title": "FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting"}, {"paperId": "259c681c76335540e13081efad584efdf9101868", "title": "DaViT: Dual Attention Vision Transformers"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "58a3fedc03ab9f5908c077c115eff4c8d2d87660", "title": "ActionFormer: Localizing Moments of Actions with Transformers"}, {"paperId": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06", "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting"}, {"paperId": "48e84128b0f288f544176138805a97fbe592a1dd", "title": "DynaMixer: A Vision MLP Architecture with Dynamic Mixing"}, {"paperId": "3425495ee3b6ead009f35aeb70edeac4e6eb2d10", "title": "Patches Are All You Need?"}, {"paperId": "c8831d0629f0eaf7f723317d71bbd60b8eb3c39f", "title": "UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5", "title": "QuadTree Attention for Vision Transformers"}, {"paperId": "15b0e710a9b8069d898ae6a0963d627e0fb86bd8", "title": "MPViT: Multi-Path Vision Transformer for Dense Prediction"}, {"paperId": "1e88d5afe19aea324d33541f60a90b7036894c32", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "b4713949345551ebb6763f83004f9da68b760b6f", "title": "ASFormer: Transformer for Action Segmentation"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "485c08025157973bb52a935c6aa3bee74f990c01", "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?"}, {"paperId": "c2d5426ee019d3d894b9d4416dc866b65fe64312", "title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models"}, {"paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede", "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f", "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "20251a6c246d203d663a3c4253f8fc32ccb42ed6", "title": "Compute and Memory Efficient Universal Sound Source Separation"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "1834a1ff37052895c42906ceb163d9306badc00b", "title": "FcaNet: Frequency Channel Attention Networks"}, {"paperId": "51c9d4d2f50ac5707c1f889aa97f08350d549132", "title": "Attention Is All You Need In Speech Separation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "e2e055f18b01eac6eb14f7462c3708ad9f073316", "title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "fb93ca1e004cbdcb93c8ffc57357189fa4eb6770", "title": "ResNeSt: Split-Attention Networks"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "4a7a9a3353ac0615f5635bae8ae3ebcb1821a80e", "title": "Voice Separation with an Unknown Number of Multiple Speakers"}, {"paperId": "c4cb16ba887828671732c9fec0d6283cedd22ee6", "title": "Wavesplit: End-to-End Speech Separation by Speaker Clustering"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "6a9d69fb35414b8461573df333dba800f254519f", "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"}, {"paperId": "02adbef22d70b6dba0b82c174e79ff4d8d1e6beb", "title": "Dual-Path RNN: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "b4673e744d0ded47fe6df3b6314f79a41359578b", "title": "MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation"}, {"paperId": "5623a462fa870ae98a134d32b46e3a30a7c39865", "title": "Depthwise Convolution is All You Need for Learning Multiple Visual Domains"}, {"paperId": "0d4b28a4a244a569d5a1345464ae2be48595fff0", "title": "Conv-TasNet: Surpassing Ideal Time\u2013Frequency Magnitude Masking for Speech Separation"}, {"paperId": "ad655c25e052fa4eeed53421344aca6f239c4c9d", "title": "Dual Attention Network for Scene Segmentation"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "5aeef2c4f3eb125ec1db9c20392f95e64ef62b41", "title": "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "e5c98541d7ba1cdf92a853d731c4bb1b531aa5d9", "title": "TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "4eebe0d12aefeedf3ca85256bc8aa3b4292d47d9", "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c", "title": "Densely Connected Convolutional Networks"}, {"paperId": "88512be44744615f4baa8e14f600f036db4c2433", "title": "Semantic Understanding of Scenes Through the ADE20K Dataset"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "99e9c9641c38f1a50b97d8677aa97b5a7f8a26c2", "title": "Computing with Quasiseparable Matrices"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "3332dc72fbe3907e45e8a500c6a1202ad5092c0f", "title": "Deep clustering: Discriminative embeddings for segmentation and separation"}, {"paperId": "d559dd84fc473fca7e91b9075675750823935afa", "title": "Sparse Convolutional Neural Networks"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "bc900a187b6f0115417f4b7dfd2cf44c62875bf8", "title": "Learning to recognize objects in egocentric activities"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "4cef3559ba4120ed064bc397d94da4a625feb5d5", "title": "An Improved Error Model for Noisy Channel Spelling Correction"}, {"paperId": "23d16d8681cf2ef14de47f1b555f58107c0907eb", "title": "Learning Temporal Higher-order Patterns to Detect Anomalous Brain Activity"}, {"paperId": "4b56eef2862f7f553686f1dd190c56017122a6a0", "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"}, {"paperId": "689bc24f71f8f22784534c764d59baa93a62c2e0", "title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer"}, {"paperId": "cf0f8f585c8822e3c6bcd9527d546eefc8486aea", "title": "S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces"}, {"paperId": "30dcc0e191a376fea0e7a46f94c53872c029efc9", "title": "Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Augmix: A simple method to improve robustness and uncertainty under data shift"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": "7a8cfe0b13a3253ad7a49e823b586bae9f01c98f", "title": "State Space Modeling Of Time Series"}, {"paperId": null, "title": "Object categorization"}, {"paperId": null, "title": "mamba-based"}, {"paperId": null, "title": "Vision mamba: E ffi cient visual representation learning with bidirectional state space model"}, {"paperId": null, "title": "M5 accuracy competition: Results, findings and conclusions"}, {"paperId": null, "title": "Separate and di ff use: Using a pretrained di ff u-sion model for better source separation"}, {"paperId": null, "title": "A neural state-space model approach to e ffi cient speech"}, {"paperId": null, "title": "Zigma: Zigzag mamba di ff usion model"}, {"paperId": null, "title": "Gri ffi n"}, {"paperId": null, "title": "Unsupervised representation learning of brain activity via bridging voxel activity and functional connectivity"}]}