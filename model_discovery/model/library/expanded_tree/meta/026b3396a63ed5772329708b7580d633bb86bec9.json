{"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "abstract": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 255, "influentialCitationCount": 28, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.13048", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks."}, "embedding": {"model": "specter_v2", "vector": [0.4920809268951416, 0.7213784456253052, -0.11084993928670883, -0.04128255695104599, -0.15471895039081573, -0.5979769825935364, 0.6582261323928833, 0.027324607595801353, -0.7516061067581177, -0.23799565434455872, 0.6484910845756531, -0.5559056997299194, 0.5689331889152527, 0.05043695122003555, -0.036965541541576385, 0.08890306949615479, -0.4543425142765045, 0.08703707158565521, 0.0808839350938797, -0.4078401029109955, 0.2393127977848053, -0.3237573206424713, -0.9246684312820435, 0.043096233159303665, -0.10142000019550323, 0.7838701009750366, 0.23230548202991486, 0.8216485381126404, -0.5875758528709412, 0.9523884654045105, 0.729559063911438, -0.3938453197479248, 0.36657747626304626, 0.14010393619537354, -0.5191994309425354, -0.5902203321456909, 0.2610090672969818, -0.07850423455238342, -0.438045471906662, 0.8914768695831299, -0.3185179531574249, 0.445957213640213, 0.18416728079319, -0.46986040472984314, -0.4363398253917694, 1.424404263496399, 0.6471716165542603, 0.9857165217399597, -0.22887130081653595, -0.2508654296398163, 1.58995521068573, -1.4300497770309448, 0.29675188660621643, 1.3934650421142578, 0.9406053423881531, 0.3140527009963989, -0.05570942908525467, -0.5839992165565491, 0.8634674549102783, -0.0028332723304629326, -0.561777651309967, -0.37755873799324036, -0.09502317011356354, 0.1663656383752823, 2.181359052658081, -0.44056078791618347, 0.3981451094150543, 0.4230929911136627, -0.15306125581264496, 1.251116394996643, -0.337083101272583, -0.5088439583778381, -0.44783151149749756, -0.20392566919326782, 0.32855522632598877, 0.8286638259887695, -0.6837449073791504, 0.1357545256614685, -0.6366615295410156, -0.21960996091365814, 0.3435383141040802, 0.1827438622713089, -0.26282092928886414, 0.08234738558530807, -0.37357279658317566, 0.7731134295463562, 0.4791317582130432, 0.8256288170814514, -0.6771541833877563, 0.6275722980499268, 0.746588945388794, 0.35533344745635986, -0.12391036003828049, 0.7285816669464111, -0.36748313903808594, 0.033451128751039505, -0.9768313765525818, 0.10434475541114807, -0.06337171792984009, 0.5491966605186462, -0.29310178756713867, 0.4617256224155426, -0.6109802722930908, 0.25730931758880615, 1.281338095664978, -0.12419068813323975, 0.6143921613693237, -0.8122798204421997, 0.1490035504102707, -0.42874783277511597, -0.08468573540449142, -0.5989286303520203, -0.1784142255783081, -0.04293527454137802, -0.8895186185836792, -1.5374963283538818, -0.47592800855636597, 0.20161104202270508, -0.7856719493865967, 0.7047103643417358, -0.3575424551963806, 0.2316414713859558, 0.029827965423464775, 0.036337342113256454, 0.5049804449081421, 0.9901928305625916, 0.42353904247283936, -0.055297646671533585, 1.1175730228424072, -0.4495692551136017, -0.6209214925765991, -0.8945708274841309, 0.4983356297016144, 0.10467962175607681, 0.24529479444026947, -0.058071136474609375, -1.2076966762542725, -0.7651598453521729, -0.514261782169342, -0.12210686504840851, -0.4471847414970398, -0.202670618891716, 0.9410878419876099, 0.1634741872549057, -1.2237499952316284, 0.8807755708694458, -0.04079034551978111, -0.09779386222362518, 0.30191633105278015, 0.047064438462257385, 0.3235095143318176, -0.07269313931465149, -1.425224781036377, 0.3986988663673401, 0.34391260147094727, -0.06462699174880981, -0.12079378962516785, -1.0038971900939941, -1.1821718215942383, 0.2989392876625061, 0.18636400997638702, -0.27611708641052246, 1.6429678201675415, 0.025869330391287804, -1.1879059076309204, 0.7409789562225342, -0.4847790598869324, -0.16298849880695343, 0.05187848582863808, -0.03324391692876816, -0.28455033898353577, -0.3612643778324127, -0.42424559593200684, 0.28153446316719055, 0.4207513928413391, 0.38809603452682495, -0.3833257853984833, 0.18711717426776886, -0.38821980357170105, -0.5600151419639587, -0.28990060091018677, 0.9752302765846252, -0.08566831797361374, -0.12345018237829208, 0.333264023065567, 0.800654947757721, -0.03834370896220207, -0.1571536660194397, -0.21304696798324585, -1.464722990989685, 0.6052494049072266, -0.1887342780828476, 0.8045946359634399, -0.7163735032081604, -0.6423163414001465, -0.10048040002584457, 0.2551285922527313, 0.10710014402866364, -0.8853869438171387, 0.654853343963623, -0.8254122734069824, 0.5219646096229553, 0.04303544759750366, -0.7338098883628845, -0.172124445438385, -0.12522700428962708, -0.9658645391464233, -0.08348295092582703, 0.17477574944496155, 1.200506567955017, -1.1635395288467407, 0.10696398466825485, 0.21337798237800598, 0.5135930180549622, -0.6653183698654175, 1.2442930936813354, -0.15104198455810547, -0.34992197155952454, -0.02554580755531788, -0.4205129146575928, 0.03011913225054741, -0.4347131848335266, 0.6371104121208191, -0.6724207997322083, -0.3334487974643707, 0.7389684915542603, -0.09567573666572571, 0.8690969347953796, -0.43773746490478516, 0.6931145787239075, 0.05980067327618599, -0.8103214502334595, 0.40935131907463074, 0.41911548376083374, -0.26983538269996643, -0.5112464427947998, 0.3275272846221924, 0.10746170580387115, -0.7026339173316956, 0.37934669852256775, 0.8688786625862122, 0.6693523526191711, -0.17770282924175262, 0.22808964550495148, 0.4227977395057678, -0.24776820838451385, 0.5326617360115051, 0.6000209450721741, 0.8731611967086792, 0.15605944395065308, 0.5914644598960876, 0.07364613562822342, 0.4386713206768036, -0.9502819776535034, 0.3447842299938202, 0.5218526124954224, 0.7764843702316284, 0.708466649055481, 0.8062179088592529, -0.7132467031478882, -0.697751522064209, 0.30072125792503357, 0.6448144316673279, 1.4283617734909058, -0.6335853338241577, -0.13412407040596008, -0.3767526149749756, -0.24882709980010986, -0.3530583083629608, 0.009573109447956085, -0.2173123061656952, -0.0002376743359491229, -0.6480888724327087, -0.8597167730331421, 1.1405316591262817, 0.41132399439811707, 0.6488510370254517, -1.013054370880127, -0.20778530836105347, -0.28598493337631226, 0.044753897935152054, -0.9171714782714844, -0.40011709928512573, 0.4492890238761902, -0.8060086369514465, -0.33526140451431274, 0.2416481077671051, 0.08781987428665161, 0.0481034591794014, -0.6476105451583862, 1.0703972578048706, -0.4616656005382538, -0.15556611120700836, 0.2353416085243225, 0.5576275587081909, -0.3258998692035675, -0.7469843029975891, 0.08828597515821457, 0.2352675497531891, -0.22030708193778992, 0.24359852075576782, 0.41913652420043945, -0.046933628618717194, -0.33605995774269104, -0.3375728130340576, -0.22852152585983276, 0.0671502873301506, 0.05276787653565407, 0.6188352108001709, -0.5015785694122314, -0.05725204199552536, -1.3045878410339355, 1.1063196659088135, 0.09028495848178864, -0.8053610920906067, 0.4192117750644684, -0.8559693694114685, -0.07588725537061691, 0.7099404335021973, -0.4842357635498047, -0.3027791380882263, -0.8094854950904846, 0.017574457451701164, -0.4422016739845276, 0.11368970572948456, 0.3522585332393646, 0.5060739517211914, 0.6244857311248779, -0.09863292425870895, 0.5952304601669312, 0.37317371368408203, -0.028684396296739578, 0.4287797510623932, -0.6495645642280579, 0.512663722038269, 0.7162914276123047, 0.17035987973213196, -0.5293863415718079, -0.13851924240589142, -0.8183803558349609, -0.42113354802131653, -0.4444509744644165, 0.17478211224079132, -0.1160721480846405, -0.04747913032770157, -0.6194044351577759, -1.4856904745101929, 0.004893861245363951, -1.1362032890319824, -0.1820671260356903, -0.08335907757282257, -0.3651939034461975, 0.14783449470996857, -0.75376957654953, -1.4733762741088867, -0.8847798705101013, -1.0681920051574707, -0.5278908014297485, 0.06691958755254745, 0.10320216417312622, -0.3254978656768799, -0.4694870710372925, 0.025300221517682076, -0.45831698179244995, 0.8865347504615784, -0.8322077393531799, 0.9014695286750793, -0.15969699621200562, 0.04010557755827904, -0.17590570449829102, -0.08667173981666565, 0.6614894866943359, -0.25122132897377014, 0.36763718724250793, -0.783485472202301, 0.35804271697998047, -0.3362468183040619, -0.2624305486679077, 0.2446378767490387, 0.2914867401123047, 0.7037456631660461, 0.0059338402934372425, -0.3712186813354492, 0.2280791848897934, 1.2573425769805908, -0.7019518613815308, 0.2547384798526764, 0.03271457180380821, 0.7794498205184937, -0.13616012036800385, -0.37274402379989624, 0.5169216990470886, 0.14800739288330078, 0.02222270891070366, 0.20866140723228455, 0.06064259260892868, 0.1003907173871994, -0.8575980067253113, 0.4065665900707245, 1.3908613920211792, 0.094779372215271, -0.13082346320152283, -1.0875667333602905, 0.5343889594078064, -1.0797147750854492, -0.9529744386672974, 0.5512179732322693, 0.5640263557434082, 0.44906431436538696, -0.5108367800712585, -0.4770318567752838, 0.06227051839232445, 0.31615838408470154, 0.42447590827941895, -0.5372580885887146, -0.8937231302261353, 0.052349675446748734, 0.5029383897781372, 0.5487980842590332, 0.5066583156585693, -0.01941511034965515, 0.7784266471862793, 14.907551765441895, 0.48798128962516785, -0.195053830742836, 0.4371465742588043, 0.41632235050201416, 0.31546691060066223, -0.6276058554649353, -0.16901955008506775, -1.3790274858474731, -0.35327011346817017, 1.2596898078918457, 0.2296704798936844, 0.5940515398979187, 0.12601664662361145, 0.4039452373981476, 0.24517057836055756, -0.465810090303421, 0.8430113196372986, 0.38653191924095154, -1.3024042844772339, 0.47054415941238403, 0.048426538705825806, -0.26746001839637756, 0.8937721252441406, 0.6824991106987, 0.8728440999984741, 0.8800395727157593, -0.4936266541481018, 0.30546706914901733, 0.3869011700153351, 0.69862961769104, 0.06207658350467682, 0.592503011226654, 0.4733588397502899, -1.0316163301467896, -0.44750306010246277, -0.6304985284805298, -1.1829049587249756, 0.1659267246723175, 0.27879756689071655, -0.6626858711242676, -0.7102653980255127, 0.15789391100406647, 1.1863892078399658, 0.22950027883052826, 0.041789859533309937, -0.3709885776042938, 0.6242883205413818, -0.21272626519203186, -0.05540037900209427, 0.3524334132671356, 0.3297463357448578, -0.03371967747807503, 0.06222006306052208, 0.04273762181401253, 0.05838219076395035, -0.2737977206707001, 0.5584193468093872, -0.8681458830833435, -0.2754884958267212, -0.3820861876010895, -0.6441647410392761, 0.21403883397579193, 0.9758867621421814, 0.3375869691371918, 0.452515184879303, -0.18776042759418488, 0.16873799264431, 0.7122260928153992, 0.06047144904732704, -0.12057793885469437, -0.23664939403533936, 0.41303104162216187, -0.41958487033843994, 0.446012407541275, 0.5604137182235718, 0.13669519126415253, -0.5656650066375732, -0.8175530433654785, -0.33278632164001465, 0.40550294518470764, -0.9015066623687744, -0.7186257839202881, 0.8773064613342285, -0.5921416878700256, -0.1365610957145691, 0.022973885759711266, -0.8812881708145142, -0.09127177298069, 0.5947628021240234, -1.6708238124847412, -0.592189371585846, 0.3505645990371704, -0.03364045172929764, -0.48152270913124084, 0.16159573197364807, 1.0927306413650513, -0.22044110298156738, -0.30686458945274353, -0.1532372534275055, -0.15259376168251038, 0.4022749364376068, -0.7539185881614685, -0.6109727025032043, 1.0688711404800415, 0.3005116879940033, -0.048394910991191864, 0.42069071531295776, 0.057421669363975525, 0.2832706868648529, -0.8793137073516846, 0.05996137484908104, 0.9013967514038086, -0.8864693641662598, -0.17874568700790405, -0.7821303606033325, -0.7842217683792114, 0.7450225353240967, 0.22067005932331085, -0.20211313664913177, 0.137502983212471, 0.2561006546020508, -1.0320653915405273, -0.11481498181819916, -0.7077873945236206, 0.08096146583557129, 0.5616738796234131, -0.9837324023246765, -0.5198633670806885, -0.4882217049598694, 0.5163243412971497, -0.5442948937416077, -0.5308547019958496, -0.255467027425766, 0.251456618309021, -0.02202514186501503, 0.9467166662216187, -0.5534024834632874, 0.4548768401145935, 0.755608856678009, 0.206487774848938, -0.46125081181526184, -0.09651648253202438, -0.7834345698356628, 0.04317784681916237, 0.13275063037872314, 0.7700441479682922, -0.7036970257759094, 0.3244164288043976, 0.7840166687965393, 0.410989910364151, -0.21946415305137634, -0.6448330879211426, -0.03736065328121185, -0.19228163361549377, -0.6800398230552673, 0.39308440685272217, 0.023540819063782692, 0.0700402557849884, 0.27743831276893616, 0.375875324010849, 0.33873942494392395, -0.526102602481842, -0.9387596249580383, -0.3283829987049103, -0.5638729929924011, 0.239321768283844, -0.5509636998176575, -0.5370332598686218, -1.2674158811569214, 0.14823362231254578, -1.1114906072616577, -0.11831599473953247, -1.1574541330337524, -0.454307496547699, 0.17267268896102905, -0.330742210149765, 0.5486995577812195, 0.4910965859889984, -0.4868531823158264, -0.44347724318504333, -0.6668341755867004, -0.3600335419178009, 0.5973532199859619, 0.48621228337287903, -0.7547966241836548, -0.013193152844905853, -0.08826691657304764, -0.09399303048849106, 0.16906999051570892, 0.40828701853752136, -0.3699553310871124, -0.8016449213027954, -1.0834033489227295, 0.9769467711448669, 0.08987949788570404, -0.2067943811416626, -0.22103165090084076, 0.7984588742256165, 0.4629305899143219, -0.2647762894630432, -0.050952669233083725, 0.35022950172424316, -0.9212641716003418, -0.5828147530555725, 0.39746707677841187, -0.7581263184547424, 0.5565061569213867, 0.1442541927099228, -0.4939509630203247, -0.17443417012691498, 0.47979655861854553, -0.09766937792301178, -1.2995655536651611, -0.6947737336158752, 0.49512603878974915, -1.076916217803955, 0.1274138242006302, -0.4118039608001709, -0.392343670129776, -0.7459882497787476, -0.3704030215740204, 0.2578134536743164, 0.6617152690887451, -0.7066614031791687, 0.868479311466217, 0.8412255644798279, -0.5453192591667175, 0.018459182232618332, 0.21077096462249756, -0.14754043519496918, -0.12053459137678146, 0.3495532274246216, 0.2814374268054962, -0.12028156220912933, 0.44268402457237244, 0.4039544463157654, 0.4421508014202118, -1.1761245727539062, 0.19211021065711975, 0.9404461979866028, -0.5732964873313904, -0.5550191402435303, 1.0596669912338257, -0.6705541014671326, -1.4021071195602417, 0.18315915763378143, -1.5250566005706787, -0.9250361919403076, 0.13441263139247894, 0.7034568190574646, -0.003445104230195284, -0.04422588646411896, 0.17032550275325775, -0.3868989646434784, 0.23524782061576843, -0.06927602738142014, -0.26695936918258667, 0.8621305823326111, 0.08040677756071091, -0.6551430821418762, 0.6854146122932434, 0.6793763041496277, -0.9700515866279602, -0.4240315556526184, -0.9737822413444519, -0.14183147251605988, 0.2642673850059509, 0.6028908491134644, -0.07654233276844025, -0.8322564959526062, 0.7390264868736267, 0.3545849323272705, 0.3470565676689148, 0.13110627233982086, -0.24678020179271698, 0.2601854205131531, 0.9544355869293213, -0.21780455112457275, -0.5509650111198425, -0.5747915506362915, 1.7155689001083374, 1.0720257759094238, -0.5283389091491699, 0.21336576342582703, -0.10985767096281052, -0.614361584186554, 0.8561188578605652, 0.026030490174889565, 0.15174147486686707, 0.9441317319869995, -0.023405609652400017, -0.03241007402539253, 0.006044647190719843, -1.113878607749939, -0.05048744007945061, 0.2080819010734558, 0.8937317132949829, 0.9160353541374207, -0.17730101943016052, 0.22977100312709808, 0.6123219132423401, 0.0790330097079277, 0.42875292897224426, 0.431474894285202, 0.4537014067173004, -0.014013993553817272, 0.12020199000835419, 0.31488487124443054, 0.6675393581390381, -0.7850977778434753, -1.1775016784667969, 0.1844339668750763, 0.4012221395969391, -0.11723493039608002, 0.6130907535552979, 1.1472891569137573, 0.10797262191772461, 0.7579565048217773, 0.2954036593437195, 0.15634678304195404, -0.35798484086990356, -0.792137861251831, -0.6782152056694031, -0.46555185317993164, -0.206305131316185, -0.22701281309127808, -0.5066626667976379, -0.6086015105247498, -0.45959287881851196, 0.42726436257362366, 0.4026225507259369, 0.24154745042324066, 0.8461851477622986, 0.32887402176856995, 0.42132192850112915, -0.3185975253582001, -0.3065812587738037, -0.655364453792572, -1.0296906232833862, -0.12562714517116547, -0.6471682190895081, 0.02424611523747444, 0.23998022079467773, -0.3561405539512634, -0.5294817090034485]}, "authors": [{"authorId": "145560079", "name": "Bo Peng"}, {"authorId": "79046907", "name": "Eric Alcaide"}, {"authorId": "1404060481", "name": "Quentin G. Anthony"}, {"authorId": "2044198106", "name": "Alon Albalak"}, {"authorId": "88727011", "name": "Samuel Arcadinho"}, {"authorId": "103476203", "name": "Stella Biderman"}, {"authorId": "47709883", "name": "Huanqi Cao"}, {"authorId": "2193630544", "name": "Xin Cheng"}, {"authorId": "2218291950", "name": "Michael Chung"}, {"authorId": "2425906", "name": "Matteo Grella"}, {"authorId": "101433524", "name": "G. Kranthikiran"}, {"authorId": "33913193", "name": "Xuming He"}, {"authorId": "9397636", "name": "Haowen Hou"}, {"authorId": "1724788", "name": "Przemyslaw Kazienko"}, {"authorId": "2905929", "name": "Jan Koco\u0144"}, {"authorId": "35171548", "name": "Jiaming Kong"}, {"authorId": "2208962106", "name": "Bartlomiej Koptyra"}, {"authorId": "1598440603", "name": "Hayden Lau"}, {"authorId": "2209207087", "name": "Krishna Sri Ipsit Mantri"}, {"authorId": "2218334866", "name": "Ferdinand Mom"}, {"authorId": "2186861874", "name": "Atsushi Saito"}, {"authorId": "47274259", "name": "Xiangru Tang"}, {"authorId": "2153213619", "name": "Bolun Wang"}, {"authorId": "1388112865", "name": "J. S. Wind"}, {"authorId": "2218377273", "name": "Stansilaw Wozniak"}, {"authorId": "2218987422", "name": "Ruichong Zhang"}, {"authorId": "2144400204", "name": "Zhenyuan Zhang"}, {"authorId": "2110483969", "name": "Qihang Zhao"}, {"authorId": "1994721202", "name": "P. Zhou"}, {"authorId": "144549416", "name": "Jian Zhu"}, {"authorId": "144649570", "name": "Rui Zhu"}], "references": [{"paperId": "ddcfdcab9e339e38bfb27e862c41e38169f809d9", "title": "Recasting Self-Attention with Holographic Reduced Representations"}, {"paperId": "594d8e1696619f3cebb7c6bffdad8e0a5592f006", "title": "Scaling Transformer to 1M tokens and beyond with RMT"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "8fc90497d9043fdf35e71302b7c2e79bb907144f", "title": "Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "5848737f78397f72ceae2ba6f3419a6a8502b8ba", "title": "ChatGPT: Jack of all trades, master of none"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "59707fbd3308257628470d94e56c8165bf4e1cff", "title": "FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "996445d847f06e99b0bd259345408a0cf1bce87e", "title": "Locating and Editing Factual Associations in GPT"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2", "title": "Primer: Searching for Efficient Transformers for Language Modeling"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"}, {"paperId": "d81de5819afe1aefa5e44bf6367c8615cb3b26fc", "title": "Six Attributes of Unhealthy Conversations"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "4450493ecb806cb889b341ae8e430886f2549a61", "title": "GoEmotions: A Dataset of Fine-Grained Emotions"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "04234cd1cad396f76b96042227041abc9e525b0a", "title": "HEAD-QA: A Healthcare Dataset for Complex Reasoning"}, {"paperId": "7edacd94dc1509803d9bbcc1d92fea780d71cb3e", "title": "MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "2b5f65d3e6245cf0efe9971b06bfa4f98d5ca227", "title": "Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies"}, {"paperId": "a5b66ee341cb990f7f70a124b5fab3316d3b7e27", "title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"paperId": "1c2efb418f79b5d29913e014a1dfd78865221c39", "title": "Deep learning for time series classification: a review"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "e77099681374e940ea45821fd7e406394721552f", "title": "Stable Recurrent Models"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "921196c32213a229245a9705ee4768bc941e7a26", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "title": "Simple Recurrent Units for Highly Parallelizable Recurrence"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "title": "Quasi-Recurrent Neural Networks"}, {"paperId": "6f35b070c250507dbec8a7365cf01b25eb66d792", "title": "Ex Machina: Personal Attacks Seen at Scale"}, {"paperId": "df0402517a7338ae28bc54acaac400de6b456a46", "title": "WaveNet: A Generative Model for Raw Audio"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "85b68477a6e031d88b963833e15a4b4fc6855264", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"}, {"paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80", "title": "Identity Mappings in Deep Residual Networks"}, {"paperId": "80c05cae76e05fab33ff5622157731d0a5549723", "title": "Quantifying the Vanishing Gradient and Long Distance Dependency Problem in Recursive Neural Networks and Recursive LSTMs"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e", "title": "On the difficulty of training recurrent neural networks"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "e9fac1091d9a1646314b1b91e58f40dae3a750cd", "title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "30becc9209c02cd4f3f6aed86ea9bd37de50be9d", "title": "Improving the Gating Mechanism of Recurrent Neural Networks"}, {"paperId": null, "title": "Rethinking attention with per-formers"}, {"paperId": "4698a7e950443e4ea05b6489b3b1218436b86ed7", "title": "Multi-Level Sentiment Analysis of PolEmo 2.0: Extended Corpus of Multi-Domain Consumer Reviews"}, {"paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df", "title": "A Comprehensive Survey on Graph Neural Networks"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "SARCASMANIA: Sarcasm Exposed!"}, {"paperId": "be31cff7d8ad7ec2b90765f754dcaf4a241df151", "title": "Preventing Gradient Explosions in Gated Recurrent Units"}, {"paperId": null, "title": "Hinton"}, {"paperId": null, "title": "Manuscript (sections 2 and 3 contributions to abstract; revision and proofreading)"}, {"paperId": null, "title": "OpenAI. 2023."}, {"paperId": null, "title": "Contri-butions to Appendix H Krishna Sri Ipsit Mantri Figure 4"}, {"paperId": null, "title": "Tables 1 and 5. Experiments for table 5"}, {"paperId": null, "title": "Manuscript (proofreading and re-vision)"}, {"paperId": null, "title": "2022. Di-agonal state spaces are as effective as structured state spaces"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "2022. Introducing chatgpt"}, {"paperId": null, "title": "2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo"}, {"paperId": null, "title": "Chat Experiments Jan Koco\u00b4n (lead), Przemys\u0142aw Kazienko, Bart\u0142omiej Tang, Stanis\u0142aw Wo\u00b4zniak, Zhenyuan Zhang Ethics and Broader Impacts"}, {"paperId": null, "title": "2022. Training compute-optimal"}, {"paperId": null, "title": "2022b. Chain of thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2022. Datasheet for the pile"}]}