{"paperId": "f7ed2546ef817a2ccfd4eda2e54ff6c98dc8da6d", "abstract": "While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper provides a set of guidelines for users to maximize the runtime performance of their transformer models by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU."}, "embedding": {"model": "specter_v2", "vector": [-0.11170056462287903, 0.44685226678848267, -0.6097100377082825, 0.1295565962791443, -0.05467156693339348, 0.014763155952095985, 0.6219894289970398, -0.6244454383850098, -0.3912504315376282, -0.8037044405937195, 0.18834835290908813, -0.10465085506439209, 0.609785258769989, -0.24130745232105255, -0.1427844762802124, -0.07697204500436783, -0.459634393453598, 0.34150582551956177, 0.19569237530231476, 0.024827489629387856, -0.1984335035085678, 0.3982399106025696, -1.368614912033081, -0.07623744755983353, 0.017417652532458305, 1.3509587049484253, -0.029983391985297203, 0.9455448389053345, -0.21994617581367493, 0.30662670731544495, 0.823879599571228, -0.2685977816581726, 0.5735028386116028, 0.4606136977672577, 0.1568240076303482, -0.32288724184036255, 0.7540180683135986, -0.5463241338729858, -0.9156692028045654, 0.7421002984046936, 0.016952935606241226, 0.07268582284450531, 0.1786869317293167, -1.0567971467971802, 0.26902636885643005, -0.30437561869621277, 0.3222160339355469, 0.8600688576698303, -1.1615219116210938, -0.07013941556215286, 0.875961184501648, -1.1014306545257568, -0.005595689173787832, 1.0778013467788696, 0.6668049097061157, -0.015472698025405407, -0.33971548080444336, -0.7136954069137573, -0.016655905172228813, -0.219869926571846, -0.605596125125885, -0.5333511829376221, 0.3150654435157776, -0.19289520382881165, 1.8504570722579956, -0.5577330589294434, -0.06254294514656067, 0.4427899718284607, 0.1843879073858261, 1.185998558998108, 0.07941451668739319, -0.6508540511131287, -0.054322678595781326, 0.0005129303899593651, 0.6072684526443481, 0.976817786693573, 0.2698979675769806, 0.34318530559539795, -0.8548003435134888, -0.5326555967330933, 0.7072794437408447, 0.0667901411652565, 0.7928375601768494, -0.5047552585601807, 0.013120311312377453, 0.9929892420768738, 0.5607393980026245, 0.42459890246391296, -0.08272777497768402, 1.136796474456787, 0.8153681755065918, 0.062062498182058334, -0.3420602083206177, 0.42713385820388794, -0.13366495072841644, 0.6019134521484375, -1.012628436088562, -0.10288504511117935, 0.08202149718999863, 1.0926988124847412, -0.2375459372997284, 0.5618842244148254, -0.18752551078796387, -0.018710041418671608, 1.2340067625045776, 0.03095749020576477, 0.17642077803611755, -0.34353095293045044, 0.19885915517807007, -0.5954153537750244, -0.23388661444187164, -0.20949150621891022, -0.17373201251029968, -0.7003322839736938, -0.9921751618385315, -0.8138680458068848, -0.40776264667510986, 0.0011324675288051367, -0.7850863337516785, 0.1577974408864975, -0.4301622211933136, 0.39808836579322815, 0.10597220808267593, 0.14077109098434448, 0.7331467866897583, 0.48048654198646545, 0.2470286637544632, 0.585178792476654, 1.0120415687561035, -1.3195945024490356, -0.07501489669084549, -1.0017080307006836, 0.40706539154052734, -0.3416426181793213, 0.17798638343811035, 0.12233027815818787, -1.1838184595108032, -1.17699134349823, -1.0451823472976685, -0.12750115990638733, -0.3504828214645386, 0.3800750970840454, 1.2808165550231934, 0.29722949862480164, -1.2389113903045654, 0.8605962991714478, -0.6424075961112976, 0.017769576981663704, 0.3508678674697876, 0.5046120285987854, 0.7151824235916138, -0.2275681048631668, -0.8485211133956909, -0.0006679868092760444, 0.29037371277809143, -1.0079915523529053, -0.4891282021999359, -0.8604728579521179, -0.632955014705658, 0.16476017236709595, -0.3873122036457062, -1.0133147239685059, 0.9320645928382874, 0.053193654865026474, -0.8946828842163086, 0.8080922365188599, -0.18996146321296692, -0.10588648170232773, 0.0452813096344471, 0.2568356990814209, -0.48589661717414856, -0.24566560983657837, -0.6950129270553589, 0.6456720232963562, 0.8626520037651062, -0.1636880338191986, 0.05821041762828827, -0.06586739420890808, -0.09399599581956863, -0.23207426071166992, -0.41751712560653687, 0.9109812378883362, -0.6888867616653442, -0.31164345145225525, 0.4480847716331482, 0.2873760461807251, -0.3256380259990692, 0.08358322083950043, -0.2968032658100128, -0.642939567565918, 1.1138330698013306, 0.2221091240644455, 0.6653488278388977, -0.9215465784072876, -1.3319355249404907, 0.22773371636867523, 0.21346506476402283, -0.1088467687368393, -0.36679399013519287, 0.25061938166618347, -0.41657891869544983, -0.11474324017763138, 0.4255756437778473, -0.9252556562423706, 0.24166125059127808, -0.5486056208610535, -0.5519993901252747, -0.15410932898521423, -0.08954323828220367, 0.788187563419342, -0.4081694185733795, 0.49207550287246704, -0.18612560629844666, 0.6599087715148926, -1.1747509241104126, 1.2571525573730469, -0.05396459624171257, -0.10302688926458359, 0.17332032322883606, -0.02749226987361908, 0.21658438444137573, -0.561415433883667, 0.3346453607082367, -1.1147198677062988, 0.1350291669368744, 0.5566151142120361, -0.26264432072639465, 1.3992016315460205, -0.6251645684242249, 0.3586759865283966, 0.35779955983161926, -0.6057472229003906, 0.6178486347198486, 0.1540999859571457, -0.3564718961715698, -0.6057928204536438, 0.43265771865844727, 0.6437744498252869, -0.39079174399375916, 0.7511212825775146, 0.910024881362915, 0.8800949454307556, -0.008224726654589176, 0.13164904713630676, 0.6229022145271301, -0.4844067692756653, 0.21106179058551788, 0.08947629481554031, 0.5762220025062561, 0.17031928896903992, -0.3205939829349518, -0.4630351662635803, 0.08697789162397385, -0.874366819858551, -0.46833670139312744, 0.3505668342113495, 0.37687045335769653, 0.639741837978363, 0.5862534642219543, -0.8285117745399475, -0.7109963893890381, -0.1756553202867508, 0.290979266166687, 1.36329984664917, -0.4588277339935303, 0.3105607032775879, -0.5257538557052612, -0.5047576427459717, -0.20445513725280762, -0.1342533826828003, 0.050319358706474304, -0.1513577252626419, -0.5487968921661377, -1.4276227951049805, 1.0819885730743408, 0.19931089878082275, 0.9367929100990295, 0.012330933474004269, -0.9057130217552185, -0.6940041184425354, 1.1059439182281494, -0.8875030279159546, -0.4423955976963043, 0.8824470639228821, -1.2073636054992676, -0.1598074734210968, 0.12046042084693909, -0.07955113053321838, 0.21060697734355927, -0.4352606236934662, 0.9752716422080994, -0.4347149431705475, -0.47783204913139343, -0.024782728403806686, 1.0612660646438599, -0.4096105694770813, -0.45934009552001953, 0.23070770502090454, -0.04862970858812332, -0.5121668577194214, 0.2020045667886734, 0.0592409111559391, -0.10442475229501724, -0.14084818959236145, -0.32207125425338745, 0.11834661662578583, 0.03999018296599388, 0.10163377970457077, 0.9410930275917053, -0.20810960233211517, -0.20952056348323822, -0.9023465514183044, 0.862182080745697, 0.23737764358520508, -0.5189718008041382, 0.07525277137756348, -0.9198618531227112, 0.35047829151153564, 0.9444563984870911, -0.7093569040298462, -0.0495196096599102, -0.8074852228164673, 0.04700734093785286, -0.9319083094596863, -0.11714643985033035, -0.5148861408233643, 0.21354730427265167, -0.45035773515701294, 0.3548467755317688, 0.3084597885608673, 0.4879046678543091, -0.06083238869905472, 0.3706086277961731, -1.2872004508972168, 0.2609056234359741, -0.131150484085083, -0.03647514060139656, 0.06566937267780304, 0.3874354362487793, -0.5771201252937317, -0.3371897041797638, -0.05392197519540787, -0.21068379282951355, -0.12381433695554733, -0.18439312279224396, -0.6819525957107544, -0.5435555577278137, 0.07677267491817474, -0.7837163805961609, -0.1765834242105484, 0.24516311287879944, -0.24525988101959229, -0.13499586284160614, -1.3690738677978516, -1.3116333484649658, -0.06332095712423325, -1.3953211307525635, -1.8830572366714478, 0.4665769338607788, 0.5252170562744141, 0.05490223318338394, -0.8826925754547119, -0.5692557692527771, -0.3435979187488556, 1.2985422611236572, -0.35341668128967285, 0.9685748219490051, -0.3330078125, -0.04943894222378731, -0.06880032271146774, -0.6233183741569519, 0.24647444486618042, -0.865326464176178, 0.4436642825603485, -1.3909187316894531, 0.160801962018013, -0.2793750464916229, -0.33063679933547974, 0.20128877460956573, 0.31317612528800964, 1.1110814809799194, 0.07346102595329285, -0.4395977258682251, 0.9870226383209229, 1.7681488990783691, -0.6835775971412659, -0.028852447867393494, -0.021425949409604073, 1.1281030178070068, -0.47965550422668457, -0.5694453120231628, 0.6379728317260742, 0.005866780411452055, 0.4910620152950287, 0.4542698860168457, -0.46290361881256104, -0.45658642053604126, -0.48559826612472534, 0.1677338182926178, 1.163370132446289, 0.7991380095481873, 0.2830365300178528, -0.8437358140945435, 0.08565494418144226, -0.7142677307128906, -0.17777927219867706, 0.4591185450553894, 0.793356716632843, 0.041450999677181244, 0.08080543577671051, -0.1926056444644928, -0.13513407111167908, 0.32506895065307617, 0.6660100221633911, -0.7904745936393738, -1.0907399654388428, 0.19365294277668, 1.0316975116729736, 0.8401232957839966, 0.32409417629241943, -0.1146530881524086, 0.1573021411895752, 14.625933647155762, 1.1011545658111572, -0.40835416316986084, 0.5895678997039795, 0.9703028202056885, 0.20186761021614075, -0.29525259137153625, -0.0424211360514164, -1.5349514484405518, -0.03190778195858002, 1.56219482421875, 0.3916306495666504, 0.4393984079360962, 0.793020486831665, 0.026011794805526733, 0.10823968052864075, -0.4652996361255646, 0.9970054626464844, 0.49338918924331665, -1.535336971282959, 0.2610238790512085, 0.17087557911872864, 0.47510096430778503, 0.7514281272888184, 0.7126845717430115, 0.5048685073852539, 0.431338906288147, -0.5092172622680664, 0.47099560499191284, 0.35785797238349915, 1.0609277486801147, -0.5584147572517395, 0.40912896394729614, 0.31223517656326294, -1.123829960823059, 0.3246423900127411, -0.5009005665779114, -0.9754387140274048, -0.3425474762916565, 0.4061412811279297, -0.7345996499061584, -0.5720095634460449, -0.2592248022556305, 0.8446374535560608, -0.1983829289674759, 0.5784115195274353, -0.10985146462917328, 0.4395725727081299, -0.40494802594184875, 0.10563287138938904, 0.32677969336509705, 0.3635790944099426, -0.2506735622882843, 0.10369165986776352, 0.12831231951713562, -0.43567371368408203, 0.07492873817682266, 0.5185447335243225, -0.5354333519935608, -0.6002369523048401, -0.32085561752319336, -0.1052345559000969, -0.03495281934738159, 1.098331332206726, 0.04037601500749588, 0.38747960329055786, -0.3422788083553314, 0.19118309020996094, 0.43137234449386597, -0.18786458671092987, -0.6507102251052856, -0.17108231782913208, 0.6965909600257874, -0.3936299681663513, -0.28627467155456543, 0.3078635632991791, -0.9690658450126648, -0.6036654114723206, -0.9234837889671326, -0.5781142711639404, 0.5807141065597534, -0.8216302394866943, -0.5698533058166504, 0.8797122836112976, -0.31701260805130005, -0.029637053608894348, 0.8994104266166687, -1.2893236875534058, -0.5482149720191956, 0.404268741607666, -1.4917157888412476, -0.817659318447113, 0.2773802876472473, -0.22660639882087708, -0.5467617511749268, -0.1452067345380783, 1.0930986404418945, 0.2381870597600937, -0.44786468148231506, 0.4308716952800751, 0.09957347065210342, 0.05271770805120468, -0.5616211891174316, -0.45937737822532654, 1.2328169345855713, 0.2692932188510895, -0.21953599154949188, -0.02049366757273674, 0.04952433705329895, 0.7080757021903992, -1.137523889541626, -0.09511768072843552, 0.28651195764541626, -0.26459044218063354, 0.33632615208625793, -0.6098851561546326, -0.31148824095726013, 0.31674954295158386, 0.45721739530563354, -0.04912557452917099, 0.23864240944385529, -0.07878797501325607, -0.9044634103775024, -0.27556899189949036, -0.9052740931510925, -0.08105218410491943, 0.569636344909668, -0.8978962898254395, 0.006893749348819256, 0.09622584283351898, 0.013483732007443905, -1.1988104581832886, -0.6960573792457581, -0.016117330640554428, 0.01701246201992035, -0.6125776767730713, 1.3403974771499634, 0.009813031181693077, 0.5649333596229553, 1.0629466772079468, -0.10507023334503174, -0.37883707880973816, 0.4551860988140106, -0.36244329810142517, -0.29647690057754517, -0.3576585650444031, 0.28331390023231506, -0.645452618598938, 0.7806850075721741, 1.0823308229446411, -0.019066564738750458, -0.7030826807022095, -0.28457146883010864, -0.2153206765651703, -0.15566135942935944, -0.6647177934646606, 0.3897121846675873, -0.13950227200984955, -0.41723141074180603, -0.021467572078108788, 0.7383771538734436, 0.49393555521965027, 0.30337706208229065, -0.4598987102508545, 0.3172827959060669, -0.2618758976459503, -0.379873126745224, -0.8968901038169861, -0.6150351762771606, -1.5119705200195312, 0.05216142162680626, -1.4289121627807617, -0.5079609155654907, -0.4816344380378723, -0.5638967752456665, -0.5896281599998474, -0.18371059000492096, 0.24352331459522247, 0.7752079963684082, 0.006209913641214371, -0.5611059665679932, -0.3194237947463989, -0.33473455905914307, 0.6533522009849548, 0.7628617286682129, -0.342430979013443, 0.2516142725944519, -0.19026494026184082, 0.3525351583957672, 0.7016635537147522, 0.4363675117492676, -0.1744033396244049, -0.8655152320861816, -1.4157079458236694, 0.17275749146938324, -0.12800168991088867, 0.07788901776075363, -1.1612831354141235, 0.7220784425735474, 0.5731142163276672, 0.036709077656269073, 0.1566007137298584, 0.2661331593990326, -1.0498443841934204, -0.19282513856887817, 0.6624673008918762, -0.320468008518219, 0.5471841096878052, 0.7730045914649963, -0.8060660362243652, 0.08991082012653351, 0.3812798857688904, 0.46972954273223877, -0.6133870482444763, -1.0528346300125122, 0.443244606256485, -0.11004085093736649, 0.061103396117687225, -0.1744985282421112, -0.08656208962202072, -1.1832929849624634, 0.01505968626588583, -0.11613059043884277, 0.0017409452702850103, 0.04755029082298279, 0.6712778210639954, 0.3197478652000427, -1.0670750141143799, 0.6373714208602905, 0.6073794364929199, -0.41922521591186523, -0.1250016987323761, 0.3738265037536621, 0.5937681794166565, -0.9260857701301575, 0.605410635471344, -0.13988851010799408, 0.2612502872943878, -0.7204576730728149, -0.13126754760742188, 0.8815528750419617, -0.8339213132858276, -0.2990040183067322, 1.296967625617981, -0.3346061110496521, -0.86008220911026, 0.2812552750110626, -1.091844916343689, -0.26689252257347107, -0.3460450768470764, 0.4883498549461365, 0.08312990516424179, 0.6886482238769531, 0.2819065451622009, -0.3357390761375427, -0.07126549631357193, -0.022842463105916977, -0.003159756539389491, 0.400393545627594, 0.3655494749546051, -0.31660693883895874, 0.26883912086486816, 0.9818831086158752, -0.9935764074325562, -0.9592185020446777, -0.721957802772522, -0.2997983694076538, -0.1000903770327568, 0.6753087043762207, -0.16822272539138794, -1.3407866954803467, 0.9219146966934204, 0.6363826990127563, -0.07874207198619843, 0.40225523710250854, -0.2805461883544922, 0.41904014348983765, 0.26662689447402954, 0.5908576846122742, -0.19152234494686127, -0.6346443295478821, 1.3873047828674316, 0.6392956972122192, -0.46351444721221924, 0.41801536083221436, -0.13106879591941833, -0.5203770995140076, 1.1137295961380005, 0.5088184475898743, -0.33203068375587463, 1.119213581085205, 0.46201345324516296, -0.6229471564292908, 0.1809910535812378, -0.8084547519683838, 0.07475927472114563, 0.8626296520233154, 0.6540184617042542, 0.6858556866645813, 0.19760191440582275, 0.24717427790164948, 1.063043475151062, -0.0006246757111512125, -0.20583517849445343, 0.45819291472435, 0.5675522089004517, 0.10553275048732758, -0.03364808112382889, -0.2763117253780365, 0.9290914535522461, -0.5204746127128601, -0.9178178310394287, 0.2685573995113373, 0.8005936741828918, 0.2667869031429291, 0.18499942123889923, 0.8423755764961243, -0.15844649076461792, 0.592034637928009, 0.038492120802402496, 0.7131101489067078, -0.2987157106399536, -0.4846506416797638, -0.052744410932064056, -0.5160070061683655, -0.14873141050338745, 0.28570371866226196, -0.19226893782615662, -0.6648276448249817, -0.5282076597213745, 0.6043599843978882, -0.27714648842811584, 0.5581703186035156, 0.5034157633781433, 0.9238658547401428, 0.7235817909240723, -0.1618100255727768, -0.8936980366706848, -0.20823781192302704, -0.33765989542007446, -0.0864829272031784, -0.7135352492332458, -0.6152663826942444, -0.3574768006801605, -0.26104357838630676, -0.49418365955352783]}, "authors": [{"authorId": "1404060481", "name": "Quentin G. Anthony"}, {"authorId": "2281637468", "name": "Jacob Hatef"}, {"authorId": "2251007290", "name": "Deepak Narayanan"}, {"authorId": "103476203", "name": "Stella Biderman"}, {"authorId": "32136590", "name": "Stas Bekman"}, {"authorId": "2281786903", "name": "Junqi Yin"}, {"authorId": "1685408", "name": "A. Shafi"}, {"authorId": "1802958", "name": "H. Subramoni"}, {"authorId": "2222521323", "name": "Dhabaleswar K. Panda"}], "references": [{"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "ece77610adfb0fb162dd22ef694f2777393c319a", "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "b92e970af7977f590661c4aaeda5b49b7d500881", "title": "A Comprehensive Evaluation of Novel AI Accelerators for Deep Learning Workloads"}, {"paperId": "22b58dce1a13382418b8372bbd50ed3b2533f899", "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "fe66bcf273a3bf750cefa7e84ed98ca57dbec161", "title": "Comparative evaluation of deep learning workloads for leadership-class systems"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "7e3d583d3131161be6e5eac67626d3e53e002cf0", "title": "A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes"}, {"paperId": "0c775d7ed34fb4690b4291490778649ae75c48d2", "title": "TurboTransformers: an efficient GPU serving system for transformer models"}, {"paperId": "f6ee999393ceacd00fdcc8e30f42002d7eb66c82", "title": "Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear Algebra Computations"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ed32fef79581afa6c33a3afe34c2ee723156a055", "title": "Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply"}, {"paperId": "0142889cda151bbe0b28237acf0b3a21c01248b5", "title": "XSP: Across-Stack Profiling and Analysis of Machine Learning Models on GPUs"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "230d65fa05ce3fb4370e7e47c0cd531a951c7d2d", "title": "A survey of techniques for optimizing deep learning on GPUs"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f5dcdf616c3af51210631d39981de7c131682887", "title": "Benchmarking TPU, GPU, and CPU Platforms for Deep Learning"}, {"paperId": "ae64adf6fc9df5a3b24bd2c5152cda68323deb81", "title": "Modeling Deep Learning Accelerator Enabled GPUs"}, {"paperId": "986396ffd8ab1b78f7a9c6ae9e2596e146c83236", "title": "Harnessing GPU Tensor Cores for Fast FP16 Arithmetic to Speed up Mixed-Precision Iterative Refinement Solvers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": null, "title": "Together Computer"}, {"paperId": null, "title": "Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Attention score computation: b \u00b7 a/t batched matrix multiplications (BMMs), each of size ( s, ha ) \u00d7 ( ha , s ) . Output is of size ( b \u00b7 at , s, s ) . providers such as AWS and Cirrascale"}, {"paperId": null, "title": "Matrix Multiplication Background"}, {"paperId": null, "title": "OLCF6 Technical Requirements and Benchmarks"}, {"paperId": null, "title": "Let\u2019s talk about a detail that occurs during PyTorch 2.0\u2019s codegen - tiling"}, {"paperId": null, "title": "The most dramatic optimization to nanoGPT so far ( 25% speedup) is to simply increase vocab size from 50257 to 50304 (nearest multiple of 64)"}, {"paperId": null, "title": "Post-attention linear projection: a single matrix multiplication of size ( b \u00b7 s, ht ) \u00d7 ( ht , h ) . Output is of size ( b \u00b7 s, h )"}]}