{"paperId": "81521a80f7aa939281863c15b42e446ff5a0e65a", "abstract": "In deep learning, different kinds of deep networks typically need different optimizers, which have to be chosen after multiple trials, making the training process inefficient. To relieve this issue and consistently improve the model training speed across deep networks, we propose the ADAptive Nesterov momentum algorithm, Adan for short. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the gradient's first- and second-order moments in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an \u03f5-approximate first-order stationary point within O(\u03f5-3.5) stochastic gradient complexity on the non-convex stochastic problems (e.g.deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan consistently surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, eg\u00a0ResNet, ConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, GPT-2, MAE, etc, and also shows great tolerance to a large range of minibatch size, e.g.from 1k to 32k. Code is released at https://github.com/sail-sg/Adan, and has been used in multiple popular deep learning frameworks or projects.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2022, "citationCount": 80, "influentialCitationCount": 17, "openAccessPdf": {"url": "http://arxiv.org/pdf/2208.06677", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Adan can use half of the training cost of SoTA optimizers to achieve higher or comparable performance on ViT, GPT-2, MAE, etc, and also shows great tolerance to a large range of minibatch size, e.g. from 1k to 32k."}, "embedding": {"model": "specter_v2", "vector": [-0.4200740456581116, 0.5663944482803345, -0.46597516536712646, 0.04105416312813759, 0.3041541874408722, 0.2704645097255707, 0.4431254267692566, -0.36784830689430237, -0.7469049692153931, 0.014909092336893082, 0.1672103852033615, -0.10274489969015121, 0.5672734975814819, -0.02147561125457287, -0.23966741561889648, -0.4009847640991211, -0.8520261645317078, 0.2128915935754776, -0.10794278234243393, -0.3021943271160126, -0.28660231828689575, -0.4366123080253601, -0.7727031111717224, -0.15557977557182312, 0.1933358758687973, 0.9325467944145203, -0.1669975072145462, 1.106228232383728, -0.5649576783180237, 0.4135168790817261, 0.8523353338241577, -0.4362911880016327, 0.6858841180801392, -0.021588247269392014, -0.6020108461380005, -0.04669536277651787, 0.02504504844546318, -0.6834807395935059, -0.4998486042022705, 0.9742409586906433, -0.4295521676540375, 0.4623775780200958, 0.33345770835876465, -0.8816123008728027, 0.4456002116203308, 0.2575953006744385, 0.20961830019950867, 1.0788679122924805, -0.9427844285964966, -0.3025233745574951, 1.015434741973877, -0.8315749764442444, -0.02277146466076374, 1.1191587448120117, 0.6888194680213928, 0.5896969437599182, -0.06336810439825058, -0.5612831711769104, 0.7606706619262695, 0.020636549219489098, -0.2865266501903534, -0.18542972207069397, 0.3492356836795807, -0.020536398515105247, 1.6856778860092163, -0.6259353160858154, 0.35608142614364624, 0.699662983417511, 0.44246453046798706, 1.4969878196716309, 0.2806966006755829, -0.6378834843635559, 0.011474443599581718, 0.5843822360038757, 0.3076446056365967, 0.8174639940261841, -0.3573754131793976, 0.6869356632232666, -1.0767759084701538, -0.08795501291751862, 0.1892574280500412, 0.10304254293441772, 0.4730755388736725, -0.41100114583969116, -0.017969485372304916, 1.0234583616256714, 0.5933824181556702, 0.19352614879608154, -0.5936173796653748, 1.042611837387085, 0.6212073564529419, 0.20335133373737335, 0.24941052496433258, 0.14695678651332855, -0.2665872275829315, 0.12798115611076355, -0.4611413776874542, 0.10220646858215332, -0.16243627667427063, 0.37116318941116333, -0.32780295610427856, 0.535198986530304, -0.5473524928092957, 0.08944869041442871, 1.5316592454910278, -0.16738861799240112, 0.5043094754219055, -0.6037111878395081, 0.08146864175796509, -0.4452311098575592, 0.07283784449100494, -0.1776926964521408, -0.4116620421409607, -0.6985030770301819, -1.3313624858856201, -0.1466739922761917, -0.4872520864009857, 0.04965151846408844, -0.8659799695014954, 0.47149330377578735, 0.354260116815567, 0.23590688407421112, 0.3869131803512573, 0.7143775224685669, -0.01734004355967045, 0.9089595675468445, -0.017286228016018867, -0.18858225643634796, 0.5194664001464844, -1.2573310136795044, -0.6469636559486389, -0.9751718044281006, 0.30662426352500916, 0.011739734560251236, 0.07462166994810104, 0.1422807276248932, -1.115478515625, -0.6305620670318604, -1.0098543167114258, 0.23663917183876038, -0.08913473039865494, 0.1577574908733368, 1.560221552848816, 0.5007531046867371, -0.4888858199119568, 1.1073840856552124, -1.0591723918914795, 0.0664287731051445, 0.612334668636322, 0.23830297589302063, 0.5380885004997253, 0.4015113413333893, -0.6969521045684814, 0.17161288857460022, 0.5445616841316223, -0.20389556884765625, -0.3465922772884369, -0.5126778483390808, -0.5267715454101562, 0.07193601876497269, 0.0735439658164978, -0.8581112623214722, 1.292529582977295, -0.24182379245758057, -1.778351068496704, 0.4084815979003906, -0.24993906915187836, -0.39485570788383484, 0.9038662314414978, -0.21915096044540405, -0.3373619616031647, -0.6196614503860474, -0.6099669337272644, 0.3895648717880249, 0.5860591530799866, 0.002845869166776538, -0.373672217130661, 0.09908994287252426, -0.1787395030260086, -0.25071650743484497, -0.5633816719055176, 0.5500330328941345, -0.2305951565504074, -0.24139456450939178, 0.42631950974464417, 0.4194641411304474, -0.3477051854133606, 0.04311637580394745, -0.20820260047912598, -0.9728071689605713, 0.7529982924461365, 0.14257566630840302, 0.7692524194717407, -0.8818524479866028, -0.6711133122444153, -0.11799473315477371, 0.20106978714466095, 0.026637692004442215, -0.5386290550231934, -0.17127087712287903, -0.1910957545042038, -0.033783718943595886, -0.1449664682149887, -0.8555401563644409, -0.06845865398645401, -0.11095988750457764, -0.8663870692253113, 0.0674438327550888, 0.1010519489645958, 0.9590237736701965, -0.9651594161987305, 0.17591634392738342, -0.12257987260818481, 0.4087718725204468, -1.3186203241348267, 1.181389570236206, -0.23696796596050262, 0.3458236753940582, -0.1277506798505783, -0.42726048827171326, 0.29588043689727783, -0.39423245191574097, 0.44570547342300415, -0.2908574044704437, 0.125971719622612, 0.752149760723114, -0.9934858679771423, 0.8013644218444824, -0.48806971311569214, 0.6634760499000549, 0.26639172434806824, -0.693291187286377, -0.0018917927518486977, 0.19885948300361633, 0.002205413533374667, -0.34424322843551636, 0.5725020170211792, 0.17349480092525482, -0.7246190309524536, 0.445422500371933, 0.368856817483902, 0.800897479057312, 0.24406622350215912, 0.1715884804725647, 0.567270815372467, -0.17402884364128113, 0.1692240834236145, 0.21094150841236115, 0.0020333915017545223, 0.43678757548332214, 0.5283724665641785, -0.02444334328174591, 0.1992577612400055, -0.9126460552215576, 0.3258841931819916, 0.4666942358016968, 0.09329678863286972, 0.20671959221363068, 0.27038928866386414, -1.0327826738357544, -0.35414281487464905, -0.464299738407135, 0.4344879686832428, 1.2766727209091187, -0.11138901114463806, 0.04465110972523689, -0.4923878014087677, -0.4428100883960724, -0.37732651829719543, 0.07906581461429596, -0.1806885302066803, -0.09126646816730499, -0.426456093788147, -1.6331559419631958, 0.6137099266052246, 0.33930766582489014, 1.3195674419403076, -0.2510997951030731, -0.36127516627311707, -0.3676515221595764, 0.5938653945922852, -0.5677856802940369, -0.24349839985370636, 0.3995506465435028, -1.2891594171524048, 0.12465930730104446, 0.18697771430015564, 0.2836777865886688, 0.024607855826616287, -0.8068252205848694, 0.6930636167526245, -0.6318328976631165, 0.022632405161857605, -0.1311596930027008, 0.6546493172645569, -1.0580976009368896, -0.6818244457244873, 0.14832089841365814, 0.6444053649902344, 0.018614361062645912, -0.0617111437022686, 0.16789613664150238, 0.06091487407684326, 0.044028837233781815, -0.3364972174167633, 0.28119412064552307, -0.0019505966920405626, 0.4148876965045929, 0.7275449633598328, -0.18104572594165802, -0.08405298739671707, -0.7482248544692993, 1.0435903072357178, 0.15689617395401, -0.9025363922119141, -0.09615775942802429, -1.0732120275497437, 0.29133909940719604, 0.15175917744636536, -0.7743125557899475, -0.18971344828605652, -0.7435716390609741, 0.01080591045320034, -0.5526461005210876, 0.02075977623462677, -0.17714528739452362, 0.9248017072677612, -0.22640156745910645, 0.5242009162902832, -0.04927496612071991, 0.16287486255168915, 0.01606258936226368, 0.31024181842803955, -0.9200630784034729, 0.28224724531173706, 0.49641409516334534, 0.1355484426021576, 0.09130048006772995, 0.05162753537297249, -0.40941768884658813, -0.7655338048934937, -0.37698087096214294, 0.03335295617580414, -0.349439412355423, -0.13351355493068695, -0.39216428995132446, -1.002351999282837, 0.2233668714761734, -0.6888532042503357, -0.3086014688014984, -0.006546357646584511, 0.27767619490623474, -0.29526326060295105, -1.1640022993087769, -1.2964608669281006, -0.8920139670372009, -1.013612985610962, -1.0409741401672363, -0.20873935520648956, 0.4391665458679199, 0.140920490026474, -0.6585716009140015, 0.06685779243707657, -0.45797020196914673, 1.0011296272277832, -0.8201994299888611, 0.36364999413490295, 0.27254682779312134, -0.2926693558692932, 0.06642136722803116, 0.3262232542037964, 0.8529292941093445, -0.644538402557373, 0.28705450892448425, -0.8921472430229187, 0.06088780611753464, -0.17849893867969513, -0.5262086391448975, 0.4693030118942261, 0.636062741279602, 0.6196757555007935, -0.2658963203430176, -0.21869778633117676, 0.8821114301681519, 1.2897132635116577, -0.6609498858451843, 0.05035395547747612, 0.39983370900154114, 0.9216314554214478, -0.35505932569503784, 0.03214074671268463, 0.7349640130996704, -0.12868472933769226, 0.092572882771492, 0.524503231048584, -0.07065919041633606, 0.18207302689552307, -0.37385010719299316, 0.3397974669933319, 1.4812763929367065, 0.19955992698669434, 0.2758718430995941, -0.708497941493988, 0.5679163932800293, -0.9647067785263062, -0.7280780076980591, 0.6747872829437256, 0.6692262291908264, 0.0994599461555481, 0.05716444179415703, -0.08799533545970917, -0.007883924059569836, 0.2221689671278, 0.6218317151069641, -0.6349850296974182, -0.9592928290367126, 0.10054146498441696, 0.13433079421520233, 0.5690413117408752, 0.664921224117279, -0.392629474401474, 0.7155270576477051, 15.065305709838867, 0.7899580597877502, -0.36903300881385803, 0.7941511273384094, 0.7396818399429321, 0.025059141218662262, -0.015534046106040478, -0.28033098578453064, -1.1304287910461426, -0.25993993878364563, 0.7491906881332397, 0.21346533298492432, 0.9119617938995361, 0.6588389873504639, -0.22063489258289337, 0.0981425940990448, -0.21298497915267944, 0.8437349796295166, 0.028524942696094513, -1.4031108617782593, -0.034173402935266495, 0.13470911979675293, 0.6924629807472229, 1.2336561679840088, 0.7935980558395386, 0.6913967728614807, 0.4830167591571808, -0.05197092518210411, 0.49648794531822205, 0.10478989779949188, 0.44151249527931213, -0.23198239505290985, 0.5023459792137146, 0.6431134939193726, -0.7724562883377075, -0.2748253345489502, -0.7452206611633301, -1.4568408727645874, 0.19060853123664856, 0.5447714924812317, -0.22655591368675232, -0.4930736720561981, 0.1651940643787384, 0.6683904528617859, 0.6328108906745911, 0.520150363445282, -0.2577211856842041, 0.5438160300254822, -0.5677149891853333, -0.2783685624599457, 0.4876119792461395, -0.14888423681259155, 0.5991566777229309, -0.3536052107810974, -0.07437089085578918, -0.10503898561000824, 0.028523318469524384, 0.5196793079376221, -0.5875318646430969, -0.2841026186943054, 0.2332257479429245, -0.3755238354206085, -0.44757339358329773, 0.901637852191925, 0.5812911987304688, 0.21643111109733582, 0.21272654831409454, 0.5306058526039124, 0.7354878187179565, 0.09378105401992798, -0.5584080815315247, -0.16156384348869324, 0.7578015923500061, -0.9411322474479675, -0.2444230318069458, 0.3737685978412628, -0.49011141061782837, -0.8034591674804688, -0.4191981256008148, -0.3691015839576721, 0.36772358417510986, -0.6808526515960693, -0.6359689235687256, 0.990555465221405, -0.5610269904136658, -0.17866116762161255, 0.5800151228904724, -0.9651829600334167, -0.5101361274719238, 0.1898634135723114, -1.3222116231918335, -0.07368479669094086, 0.4516913890838623, -0.1823612004518509, -0.3326247036457062, -0.08256737142801285, 0.7411019802093506, 0.0319615975022316, -0.8436759114265442, 0.06570743769407272, 0.18922898173332214, -0.18522228300571442, -0.5754899382591248, -0.7533831000328064, 0.9296797513961792, 0.4526706635951996, 0.1503230482339859, -0.08207308501005173, 0.16278912127017975, 0.5433692932128906, -0.3774804174900055, 0.08926412463188171, 0.04384361580014229, -0.8382768630981445, 0.05078588053584099, -0.4703744351863861, -0.6401519775390625, 0.28509506583213806, 0.2503386437892914, 0.03076743707060814, -0.299832284450531, 0.24744054675102234, -0.43629056215286255, -0.4332823157310486, -0.6466923356056213, 0.009158809669315815, 0.4031185209751129, -0.5161457061767578, -0.292108952999115, 0.11487916111946106, 0.22076515853405, -1.2653632164001465, -0.24112941324710846, 0.19760021567344666, 0.26475533843040466, -0.4809231162071228, 1.1298339366912842, -0.6940951347351074, 0.3897259533405304, 0.9775986671447754, 0.17776280641555786, -0.7684696316719055, 0.0886516124010086, -0.9789415001869202, -0.3018622398376465, -0.005419117398560047, 0.40927669405937195, -0.756297767162323, 0.6313519477844238, 0.4838712215423584, 0.5519301295280457, -0.6572005748748779, -1.0097661018371582, -0.21990571916103363, -0.009036675095558167, -1.019449234008789, 0.2507442533969879, -0.02760745957493782, -0.3420352041721344, -0.14880631864070892, 0.19682425260543823, 0.7808540463447571, -0.07469102740287781, -0.7307835221290588, 0.2667672038078308, -0.07345831394195557, -0.19539223611354828, -0.8556959629058838, -0.6290552020072937, -1.7490365505218506, -0.2668960988521576, -1.2317163944244385, -0.3990922272205353, -0.8748769760131836, -0.5227755308151245, -0.09857053309679031, -0.5601527690887451, 0.007317961659282446, 0.26877814531326294, -0.33294451236724854, -0.5408318638801575, -0.46425625681877136, -0.38395798206329346, 1.3392285108566284, 0.6805703043937683, -0.9352970123291016, -0.5290515422821045, 0.48539894819259644, 0.4313758909702301, 0.7468156218528748, 0.7243967652320862, -0.6108856201171875, -0.5004010200500488, -1.2023570537567139, 0.5524923205375671, -0.3617704212665558, 0.02949822135269642, -1.134584903717041, 0.616133451461792, 0.08166105300188065, 0.14808203279972076, 0.12588420510292053, 0.18096910417079926, -0.7928282618522644, -0.5137640833854675, 0.5919325947761536, -0.47963181138038635, 0.03422725945711136, 0.362539678812027, -0.5418190360069275, -0.00047783972695469856, 0.7669073939323425, 0.23252825438976288, -0.2990977168083191, -0.5296050906181335, 0.6448412537574768, -0.6453892588615417, 0.17851954698562622, -0.3341708183288574, -0.07001842558383942, -0.9879966378211975, -0.046956680715084076, -0.07076016813516617, 0.5046778917312622, -0.5275769829750061, 0.6557226181030273, 0.29034045338630676, -1.152107834815979, 0.36495018005371094, 0.31086698174476624, -0.3375168442726135, -0.015835464000701904, 0.6457465887069702, 0.613762378692627, -0.5637943148612976, 0.27309417724609375, 0.015999168157577515, -0.005285314284265041, -0.572047233581543, 0.0601852685213089, 1.1857168674468994, -0.7473684549331665, -0.4578399062156677, 1.3256598711013794, -0.2021227777004242, -1.3751294612884521, 0.43347594141960144, -0.9565289616584778, -0.2821030914783478, -0.6549965739250183, 0.5554763078689575, 0.5729166865348816, -0.3458769619464874, 0.2740003764629364, -0.4886864125728607, 0.052709806710481644, 0.001862692297436297, -0.48175713419914246, 0.4463896155357361, -0.30850544571876526, -0.24967819452285767, 0.882347822189331, 0.5486512780189514, -0.6847853064537048, -1.2716830968856812, -0.6686879992485046, -0.6942785382270813, -0.17566335201263428, 0.7282875180244446, -0.2757062613964081, -1.1749838590621948, 0.9977186322212219, 1.0653722286224365, 0.0020395591855049133, 0.4913385510444641, -0.32028016448020935, 0.13142849504947662, 0.9982020258903503, 0.10180175304412842, -0.9446200728416443, -0.5575635433197021, 1.2000242471694946, 1.0522581338882446, -0.9931261539459229, 0.696904718875885, -0.16294771432876587, -0.915738582611084, 0.7157080173492432, 0.3834516704082489, -0.19641047716140747, 0.7972783446311951, -0.5665953159332275, 0.29634854197502136, 0.314961701631546, -0.8423375487327576, -0.2169874757528305, 0.9007413983345032, 0.620029628276825, 0.2841069996356964, 0.24720130860805511, 0.20997561514377594, 0.8822980523109436, 0.13357995450496674, 0.016885124146938324, 0.41561755537986755, 0.41014957427978516, 0.003740459680557251, 0.24880748987197876, 0.03496219217777252, 0.7779111862182617, -0.8949877023696899, -0.07131055742502213, 0.5506624579429626, 0.5583561658859253, -0.29725921154022217, 0.4871625304222107, 0.7296916246414185, 0.15913282334804535, 0.17482507228851318, -0.24067877233028412, 0.3883223533630371, 0.2011292278766632, -0.14273637533187866, -0.13691458106040955, -0.8841727375984192, -0.5441541075706482, -0.20127278566360474, -0.18484237790107727, -0.3976169228553772, -0.6261461973190308, 0.1035090833902359, 0.14602452516555786, 0.6592231392860413, 1.104328989982605, 0.4483937621116638, 0.8585777282714844, -0.026102356612682343, -0.8612975478172302, -0.5060118436813354, -0.7030119895935059, -0.17340883612632751, -0.7296579480171204, -0.021181315183639526, -0.27006006240844727, -0.31836000084877014, -0.7122655510902405]}, "authors": [{"authorId": "2543387", "name": "Xingyu Xie"}, {"authorId": "2153245275", "name": "Pan Zhou"}, {"authorId": "3092681", "name": "Huan Li"}, {"authorId": "33383055", "name": "Zhouchen Lin"}, {"authorId": "143653681", "name": "Shuicheng Yan"}], "references": [{"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "378fb84294837d1c8582b55886b8c3c65ebe9d8f", "title": "Optimization Induced Equilibrium Networks: An Explicit Optimization Perspective for Understanding Equilibrium Models"}, {"paperId": "d2f63b56fc6bc373f5c023454c2b253326962865", "title": "DeiT III: Revenge of the ViT"}, {"paperId": "0b28ed4853de3f8cc75c782b8ac1e9e8432d6592", "title": "Mugs: A Multi-Granular Self-Supervised Learning Framework"}, {"paperId": "07f46ed2ebba37fd639dc060503e012ea752fe01", "title": "Towards Efficient and Scalable Sharpness-Aware Minimization"}, {"paperId": "3387e9dedb7accc3c248d194b012cab0ab5ab0b8", "title": "Context Autoencoder for Self-Supervised Representation Learning"}, {"paperId": "f4ad6f3bc68b66d9d8a6eaa826d0bb813ef6b636", "title": "Understanding AdamW through Proximal Methods and Scale-Freeness"}, {"paperId": "cf9fcf10af8200f12952e1fa9a929496dbbce810", "title": "Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the O(\u03b5-7/4) Complexity"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "c57b9bc112e40c3d1beb71cc087fccec07106796", "title": "A Novel Convergence Analysis for Algorithms of the Adam Family"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "f12975df157fe4462054ba1484dd4b4e5aaf5a17", "title": "Large-Scale Deep Learning Optimizations: A Comprehensive Survey"}, {"paperId": "a82ae40ecc5ea5e33b52c87c9464510cab7bf9d9", "title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks"}, {"paperId": "f454f6b5f2ca9749ddf442eb5134612ef7f758c1", "title": "ResNet strikes back: An improved training procedure in timm"}, {"paperId": "80438f2ef228776b22118915f1a27e2cf4a0f4e0", "title": "Tianshou: a Highly Modularized Deep Reinforcement Learning Library"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "42a7015e48a1e00b70ebb442a82afb4b10017c0b", "title": "When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations"}, {"paperId": "1fae35d17aca0ec5ceb866824a17c14a6f6b0ba1", "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks"}, {"paperId": "7e3d583d3131161be6e5eac67626d3e53e002cf0", "title": "A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "36730b26770982567b11f05fe932a7ebd94134ea", "title": "Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks"}, {"paperId": "031960a54caf7059388d4d491a579ac18ebcc2f2", "title": "Adam+: A Stochastic Method with Adaptive Variance Reduction"}, {"paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4", "title": "Why are Adaptive Methods Good for Attention Models?"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "1e04ca1998c04040c9c10685fc0daa4ecc13855b", "title": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "a2cd073b57be744533152202989228cb4122270a", "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "3f6a7fffbf3c96c99743f1e01f8dd86d9a9851fa", "title": "Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum"}, {"paperId": "e11e8d81c68cc782f564ed78e595b66790719804", "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights"}, {"paperId": "3e9a102d175b226951760a90c27bbdaacb2ea5c4", "title": "Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations"}, {"paperId": "6ce274c0a930c6e2c91d24c7b5776a81b1e022f0", "title": "Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "b8b0e4270e10f3d185fcd20c9d1b2673a6ace96e", "title": "Momentum Improves Normalized SGD"}, {"paperId": "1448562ccd5bf4bab01d4cae4b2e4549ad3b8e8a", "title": "Lower bounds for non-convex stochastic optimization"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "2bf7c350a8280e7c593d46a60127f99b21517121", "title": "On the Variance of the Adaptive Learning Rate and Beyond"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "1906e3a2fda12641a42739e3fb6a8f8b1accc8dd", "title": "SPoC: Search-based Pseudocode to Code"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "03af562fb8e69677865dbe94910e464443dd4623", "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate"}, {"paperId": "877f550f299b76d9ad96dfe3204841065e094493", "title": "Sharp Analysis for Nonconvex SGD Escaping from Saddle Points"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "551505614e025831a80c2d67bd854b1441eac03c", "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization"}, {"paperId": "c215b9ac79f07c8a43782b224f4416943837ffa8", "title": "On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization"}, {"paperId": "2b7201745450633619f6ad871cdfd2a28d35d0e2", "title": "Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks"}, {"paperId": "0e662587c790e5d11475f5b0bce3638b4d1effa0", "title": "Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced"}, {"paperId": "c983653841b6987d9959318f074a595783838576", "title": "On the Convergence of Adam and Beyond"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "0054eb42d8ad63b1988fe32df6284762619dcada", "title": "Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "30b8a48ec5d6189ce37e062a57a9b52a355213e2", "title": "Stability and Generalization of Learning Algorithms that Converge to Global Optima"}, {"paperId": "5fef384d40543761c537e0111de37a3ed94db388", "title": "Characterization of Gradient Dominance and Regularity Conditions for Neural Networks"}, {"paperId": "1e3d18beaf3921f561e1b999780f29f2b23f3b7d", "title": "Large Batch Training of Convolutional Networks"}, {"paperId": "353f07a8c8a35a6b121d891d81e1f4ebec0d849a", "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "e6f6757f177fc959957e1ec8d53fc2d4d7cd1ad9", "title": "Diverse Neural Network Learns True Target Functions"}, {"paperId": "8fbb115c578e8bfbcc1615bd7af990396abf6776", "title": "Identity Matters in Deep Learning"}, {"paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56", "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "d44efdc542f2cc5e196f04bc76bc783bfd7084af", "title": "Incorporating Nesterov Momentum into Adam"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "86efe7769f2b8a0e15ca213ab09881e6705caeb0", "title": "Convolutional Neural Networks for Speech Recognition"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "d0b0c3e5a1e768490bc9b759685930541957508b", "title": "Introductory Lectures on Convex Optimization - A Basic Course"}, {"paperId": "43c05444fbc239321f6676f3cd539cac34fde7b8", "title": "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction"}, {"paperId": "de887b3547a6a774563100a28b9390d394b90132", "title": "Proximal Algorithms"}, {"paperId": "709e40a2614846a48d8e25f114eb2fa6ece853cc", "title": "Improvements to Deep Convolutional Neural Networks for LVCSR"}, {"paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1", "title": "MuJoCo: A physics engine for model-based control"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "34ddd8865569c2c32dec9bf7ffc817ff42faaa01", "title": "A Stochastic Approximation Method"}, {"paperId": "c99be6b5cd24ae05f60256989efbefc7252c7717", "title": "Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms"}, {"paperId": "d29c75b16df7318d9c5b27693575f686f28cc830", "title": "Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the O ( (cid:15) \u2212 7 / 4 ) Complexity"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "7b6aa14207eb54964fa97899460dc6367766b1ef", "title": "Towards Understanding Why Lookahead Generalizes Better Than SGD and Beyond"}, {"paperId": "c7f93c7a6c8646b6df94ff7f5eaf230f3f285e1f", "title": "RMSprop converges with proper hyper-parameter"}, {"paperId": "8b2ffb143ed51bae6093a07650d994ac05be6242", "title": "Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization"}, {"paperId": null, "title": "Accelerated optimization for machine learning"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "90e2842c06b8d4a1fa3c146aabf07a63de4bdc2f", "title": "Adaptive Methods for Nonconvex Optimization"}, {"paperId": "be0dd2e91bb104494feeb5da2761cf930564f650", "title": "Under review as a conference paper at ICLR 2016"}, {"paperId": null, "title": "Lecture 6.5-rmsprop: Divide the gradient by a run- ning average of its recent magnitude"}, {"paperId": null, "title": "On an approach to the construction of optimal methods of minimization of smooth convex 205 functions"}, {"paperId": "8d3a318b62d2e970122da35b2a2e70a5d12cc16f", "title": "A method for solving the convex programming problem with convergence rate O(1/k^2)"}, {"paperId": "4b53e3f719ff983eef867c6d8deac5dbe38aecb4", "title": "Some methods of speeding up the convergence of iteration methods"}]}