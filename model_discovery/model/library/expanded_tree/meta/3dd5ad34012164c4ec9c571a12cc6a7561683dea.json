{"paperId": "3dd5ad34012164c4ec9c571a12cc6a7561683dea", "abstract": "In many scientific fields, large language models (LLMs) have revolutionized the way with which text and other modalities of data (e.g., molecules and proteins) are dealt, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one to two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 250 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper comprehensively survey over 250 scientific LLMs, discusses their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality, and investigates how LLMs have been deployed to benefit scientific discovery."}, "embedding": {"model": "specter_v2", "vector": [0.09309888631105423, 0.2858501970767975, -0.36161932349205017, -0.01454001385718584, -0.26578983664512634, -0.19067494571208954, 1.1075599193572998, -0.04628991708159447, -0.5436378717422485, 0.25749024748802185, 0.30073392391204834, -0.22465315461158752, 0.2602919936180115, 0.4713270366191864, -0.36332690715789795, -0.3631187677383423, -0.8708376884460449, 0.684527575969696, -0.34899815917015076, 0.1065022200345993, -0.11623191833496094, -0.3823357820510864, -0.7043648362159729, 0.20276157557964325, -0.16236674785614014, 0.07504747807979584, 0.16254262626171112, 0.8711363673210144, -1.0278630256652832, 0.6472010016441345, 0.599943995475769, -0.3907376229763031, -0.019645163789391518, -0.19825150072574615, -0.418490469455719, -0.2621931731700897, 0.23407036066055298, 0.1599581390619278, -0.7670578360557556, 0.596221923828125, -0.02569105289876461, 0.3036968410015106, 0.6766481995582581, -0.5492808818817139, -0.3759315013885498, 0.9642345905303955, 0.5222553610801697, 0.552954375743866, -0.2593010365962982, -0.3459224998950958, 1.4621444940567017, -1.419834852218628, 0.7183926105499268, 1.6642643213272095, 0.20479722321033478, 0.47596943378448486, -0.4137345850467682, -1.0173542499542236, 0.19375428557395935, -0.19089391827583313, -0.9694625735282898, -0.2950994670391083, -0.05640573427081108, -0.6223829388618469, 1.859224796295166, -0.29907652735710144, -0.3996778428554535, 0.5954069495201111, 0.12682628631591797, 1.2303471565246582, 0.08181697130203247, -0.8609893321990967, -0.1373647302389145, 0.2052835375070572, -0.08190376311540604, 0.7575615644454956, -0.5392138361930847, 0.2680498957633972, -1.1503711938858032, -0.4223993718624115, 0.34433725476264954, -0.3489568531513214, -0.2724219262599945, 0.09164412319660187, -0.7929453253746033, 0.7097761034965515, 0.17606490850448608, 1.042712926864624, -0.18965467810630798, 0.3574743866920471, 0.18505899608135223, 0.4738393723964691, 0.02728225290775299, 0.6113760471343994, -0.4717315435409546, 0.7089784741401672, -0.7842991948127747, 0.393816739320755, 0.16635511815547943, 0.8869882225990295, -0.28395891189575195, -0.027679376304149628, -0.6977495551109314, 0.24853016436100006, 1.5022273063659668, -0.10611308366060257, 0.8783351182937622, -0.5307532548904419, 0.25886210799217224, -0.40394166111946106, 0.2628449499607086, -0.3756415545940399, -0.48183953762054443, -0.20832747220993042, -0.5908324122428894, -1.109257459640503, -0.3136928677558899, 0.23198957741260529, -0.45449942350387573, 0.7473771572113037, -0.2360505312681198, 0.18662837147712708, 0.49911999702453613, 0.5512124300003052, 1.1463712453842163, 0.8874752521514893, 0.9078192710876465, -0.07888074964284897, 1.1790467500686646, -0.7213660478591919, -0.6480791568756104, -1.1543318033218384, 0.8433146476745605, -0.24365843832492828, 0.08276426792144775, -0.37142422795295715, -0.9026914834976196, -0.7968820333480835, -0.41727879643440247, -0.28803279995918274, -0.7727940082550049, 0.48124173283576965, 1.1723685264587402, -0.08453918248414993, -0.8148298859596252, 0.5507422089576721, 0.3946473300457001, -0.16807107627391815, -0.20781198143959045, 0.1337769627571106, -0.10407362878322601, -0.4104982316493988, -1.4479161500930786, 0.12305881828069687, 0.4478684961795807, -0.7406655550003052, -0.6250733137130737, -0.45545342564582825, -1.0870763063430786, -0.13616758584976196, 0.09708946198225021, -0.6393614411354065, 1.1321227550506592, 0.05737011134624481, -0.9635531306266785, 1.2434173822402954, -0.429690420627594, -0.002886327216401696, 0.2948108911514282, 0.31517741084098816, -0.7505856156349182, -0.549085259437561, -0.25649791955947876, 0.7452998161315918, 0.134186789393425, -0.34231555461883545, 0.14516527950763702, 0.10062666982412338, -0.17332927882671356, -0.23494982719421387, 0.0670146718621254, 1.2252042293548584, -0.5004942417144775, -0.41048675775527954, 0.0989060327410698, 0.5626272559165955, -0.20858027040958405, -0.0927099660038948, -0.4034859538078308, -1.033954381942749, 0.3331933319568634, 0.08471941947937012, 0.8560826778411865, -0.7962414026260376, -0.42343828082084656, -0.692396342754364, 0.35336023569107056, -0.3358118236064911, -0.8437564969062805, 0.8395658731460571, -0.6727762818336487, 0.563218891620636, -0.4115487039089203, -1.294806718826294, -0.17721495032310486, -0.17144979536533356, -0.4870193898677826, -0.06915455311536789, 0.20475178956985474, 1.1613397598266602, -0.9008181691169739, -0.03152783215045929, -0.17569294571876526, 0.3863765299320221, -0.5362908244132996, 1.2201286554336548, -0.8672387599945068, 0.19512926042079926, -0.14540083706378937, -0.1599556803703308, -0.0801510438323021, -0.6670089960098267, 0.4963178336620331, -0.303916871547699, -0.7337560653686523, 0.24570998549461365, -0.13993199169635773, 1.3305293321609497, -0.06188620626926422, 0.3839963674545288, -0.011384118348360062, -0.6550488471984863, 0.09555298089981079, 0.7639650106430054, 0.08527199178934097, -0.15993396937847137, 0.37289535999298096, 0.4064491093158722, -0.5879163146018982, -0.06556358188390732, 0.22093135118484497, 0.2773221433162689, 0.19554492831230164, 0.20282571017742157, 0.5253714919090271, -0.1367645114660263, 0.5519488453865051, 0.5495002865791321, 0.43431055545806885, 0.34127679467201233, 0.467914342880249, -0.13972733914852142, 0.7031254172325134, -0.7541161179542542, -0.28820282220840454, 0.6103829741477966, 0.8920745849609375, 0.5734304189682007, 0.37712395191192627, -0.6444702744483948, -0.1464710682630539, 0.5610926151275635, 0.5950873494148254, 1.5268926620483398, -0.2009783387184143, -0.41816118359565735, -0.04687283933162689, -0.05840688943862915, -0.13263830542564392, 0.09497296810150146, -0.6266855597496033, 0.05750640854239464, -0.33431121706962585, -1.3059697151184082, 0.6686264872550964, -0.23725853860378265, 0.28474676609039307, -0.8614216446876526, 0.06815796345472336, -0.35076770186424255, 0.22626736760139465, -0.5641522407531738, -0.4513281285762787, 0.47974005341529846, -0.7462018728256226, -0.07620636373758316, -0.2068498134613037, -0.21253469586372375, -0.04739458113908768, -0.6483129262924194, 1.2276345491409302, -0.901023805141449, -0.5666179656982422, -0.008169317618012428, 1.0101442337036133, -0.8282900452613831, -1.014923334121704, -0.5045642852783203, 0.04822747781872749, -0.14223532378673553, 0.5695882439613342, 0.6990803480148315, 0.14007818698883057, -0.1629926860332489, -0.7640870809555054, 0.36304211616516113, 0.3912753760814667, 0.09527541697025299, 0.8715753555297852, -0.23112688958644867, -0.022930825129151344, -1.2460960149765015, 0.7578194737434387, -0.2532818913459778, -0.7316440343856812, 0.344186007976532, -0.6528998613357544, -0.15009154379367828, 0.5070038437843323, -0.8316954374313354, -0.2895112931728363, -0.6689401268959045, 0.44435593485832214, -0.24644915759563446, -0.32695579528808594, 0.8815502524375916, 0.4327814280986786, 0.3403831124305725, 0.25835245847702026, 0.6042571663856506, 0.24306562542915344, -0.7408638596534729, 0.4833076596260071, -0.5133398771286011, 0.17675115168094635, 0.48520565032958984, 0.08441423624753952, -0.1625073254108429, 0.01420197356492281, -0.7887946963310242, -0.43754541873931885, -0.6178283095359802, -0.08679278194904327, -0.15702565014362335, 0.17620691657066345, -0.9191241264343262, -0.7186205387115479, 0.02816649340093136, -0.8093835115432739, 0.030981354415416718, 0.6091148257255554, 0.060332637280225754, 0.33237242698669434, -0.9217590093612671, -1.2067021131515503, -0.6836316585540771, -0.49548274278640747, -0.9440468549728394, 0.5550642013549805, -0.012497873976826668, -0.07784389704465866, -0.72611403465271, 0.006501880940049887, -0.13106022775173187, 0.5961687564849854, -0.7235547304153442, 1.0553420782089233, -0.18455711007118225, 0.05475832521915436, -0.25795143842697144, 0.19948838651180267, 0.04776616767048836, -0.1519457846879959, 0.2992108464241028, -0.7755662798881531, 0.3188225030899048, -0.16141866147518158, -0.5087993741035461, 0.30719509720802307, 0.5468199849128723, 0.8164111971855164, 0.21931393444538116, -0.7570064663887024, 0.10713167488574982, 1.1548517942428589, -0.6859859824180603, -0.6364304423332214, -0.031050849705934525, 0.732824444770813, 0.39988598227500916, -0.48785528540611267, 0.5876441597938538, 0.017252366989850998, 0.2588714063167572, -0.04729973524808884, -0.14667163789272308, 0.28286710381507874, -0.42797502875328064, 0.5439903736114502, 1.1728241443634033, 0.3408987522125244, -0.3838253617286682, -1.2210525274276733, 0.38064277172088623, -0.9302619695663452, -0.6728903651237488, 0.3115694522857666, 0.7730281949043274, 0.8273977637290955, -0.25801530480384827, -0.43295666575431824, -0.6119367480278015, 0.20416171848773956, 0.22730210423469543, -0.46995216608047485, -0.18099769949913025, -0.5741370916366577, 0.24726077914237976, 0.19659341871738434, 0.44739001989364624, -0.5785027146339417, 0.4255329370498657, 14.76436996459961, 0.595811128616333, -0.12949486076831818, 0.3434274196624756, 0.8206747174263, 0.26012954115867615, -0.2926541268825531, -0.2361806482076645, -1.3498976230621338, -0.19514043629169464, 1.45557701587677, 0.058706264942884445, 0.4187949299812317, 0.34598609805107117, 0.3770792484283447, 0.3044469952583313, -0.6367074251174927, 0.8992817997932434, 0.3608168065547943, -1.5490981340408325, 0.5791486501693726, 0.15074898302555084, 0.061794351786375046, 0.8920099139213562, 0.5815223455429077, 0.8957468271255493, 0.4306422770023346, -0.8176417350769043, 0.21601803600788116, 0.5716211199760437, 0.7758542895317078, 0.5274432301521301, 0.6036282777786255, 0.7918103337287903, -0.7869226932525635, 0.03082898072898388, -0.722399115562439, -1.038167119026184, -0.18596430122852325, 0.34058788418769836, -0.9339351654052734, -0.12484821677207947, -0.44933000206947327, 0.9497132897377014, 0.030030760914087296, 0.14548055827617645, 0.22830399870872498, 0.9659170508384705, 0.0885118916630745, 0.051452260464429855, 0.4719535708427429, 0.03973071649670601, 0.2626911401748657, 0.18817605078220367, 0.029461191967129707, -0.09867417067289352, 0.30660203099250793, 0.6253783702850342, -0.9531811475753784, 0.5316011309623718, -0.43461769819259644, -1.2212636470794678, -0.4247773289680481, 0.7677801251411438, 0.5939761996269226, 0.1270834505558014, -0.8029685616493225, 0.045804694294929504, 0.5117594003677368, 0.27755361795425415, -0.1284409910440445, 0.1913859099149704, 0.4933421313762665, -0.5069562196731567, -0.37343287467956543, 0.6275008916854858, 0.13823352754116058, -0.5636929869651794, -0.9082300066947937, -0.24657420814037323, 0.6594162583351135, -0.799537181854248, -1.3438408374786377, 0.8511863350868225, -0.4858951270580292, -0.48334506154060364, 0.42056170105934143, -1.1064445972442627, 0.3769840896129608, 1.0763086080551147, -1.547170877456665, -0.6147302389144897, 0.3851659893989563, -0.5440663695335388, -0.24302294850349426, -0.38024231791496277, 1.3820371627807617, 0.2954937815666199, -0.7017897963523865, -0.3557073175907135, 0.47510579228401184, 0.01945909485220909, 0.1558997929096222, -0.572701632976532, 0.5749240517616272, -0.2778995931148529, 0.05124462768435478, 0.8935569524765015, 0.28463736176490784, -0.0400284118950367, -1.0865345001220703, 0.01211758702993393, 1.2169380187988281, -0.9339548349380493, -0.07736440002918243, -0.7469112873077393, -1.2189056873321533, 0.08906140178442001, 0.2717617452144623, -0.5019871592521667, 0.7203670144081116, -0.06164812296628952, -0.6448957324028015, 0.47756096720695496, -0.4437679648399353, -0.08499816060066223, 0.5604382157325745, -0.6209269762039185, -0.22347137331962585, 0.7331243753433228, 0.3173561692237854, -0.5550686120986938, -0.5650989413261414, -0.18039365112781525, -0.12458398193120956, 0.4748123288154602, 1.1509746313095093, -1.336667776107788, 0.9685104489326477, 0.5780602693557739, 0.03708472475409508, -0.4081227481365204, -0.3292393684387207, -0.6449732780456543, -0.24802619218826294, -0.2555442154407501, 0.9033534526824951, -0.2892279624938965, -0.08673642575740814, 0.7043270468711853, 0.490059494972229, -0.4786735475063324, -0.5158118605613708, -0.42423945665359497, 0.2098662257194519, -0.29975444078445435, -0.05572253465652466, 0.24947193264961243, -0.1833404004573822, 0.3674100935459137, 0.13789433240890503, 0.6960012316703796, -0.43556317687034607, -0.1567404717206955, 0.35553357005119324, -0.48355942964553833, -0.21470464766025543, -0.6288994550704956, -0.16024728119373322, -1.4295363426208496, 0.2675853669643402, -1.3640917539596558, -0.10859233886003494, -0.9842466115951538, 0.0407542958855629, 0.11408717930316925, -0.5415059328079224, 0.17065909504890442, 0.462436705827713, -0.35678553581237793, -1.1001423597335815, -0.16449566185474396, -0.17635534703731537, 0.6954056024551392, 0.8723381757736206, -0.8611010313034058, 0.19422167539596558, -0.3294681906700134, 0.28399768471717834, -0.010929389856755733, 0.3024933934211731, -0.5528854727745056, -0.7737093567848206, -1.0539876222610474, 0.35505616664886475, 0.11609816551208496, -0.14587880671024323, -0.38339582085609436, 0.41558757424354553, -0.05645895376801491, -0.16722245514392853, 0.16281114518642426, 0.5180695056915283, -0.6184464693069458, 0.23814097046852112, 0.3035377860069275, -1.2108455896377563, 0.08041182905435562, 0.10779035836458206, -0.38588935136795044, -0.5731843709945679, 0.4870651066303253, 0.1262168139219284, -1.2784128189086914, -0.25427573919296265, 0.6623110175132751, -0.5679347515106201, -0.04680762067437172, -0.13114851713180542, 0.4768362045288086, -0.6022852659225464, -0.19471143186092377, -0.20979419350624084, 0.4159197509288788, -0.5714528560638428, 0.994979977607727, 0.35105252265930176, -0.656298816204071, -0.2584221661090851, 0.15893711149692535, 0.38982394337654114, -0.5078591108322144, 0.3799437880516052, 0.3747929632663727, -0.19330112636089325, 1.1800849437713623, 0.5952872037887573, 0.6346108913421631, -1.125943660736084, 0.15288205444812775, 0.9189043641090393, -0.4555700123310089, -0.24211187660694122, 1.4888157844543457, 0.19193126261234283, -1.1651073694229126, 0.20412755012512207, -1.0089129209518433, -0.9509562253952026, -0.3297579884529114, 0.8473978042602539, -0.2918145954608917, -0.08302860707044601, -0.3199370503425598, -0.46504107117652893, -0.22239750623703003, 0.17499031126499176, -0.32582375407218933, 0.7132781744003296, -0.01925158128142357, -0.5650764107704163, 1.0399887561798096, 1.0164884328842163, -0.44079113006591797, -0.023167859762907028, -0.8962292075157166, -0.11554025113582611, 0.10732161998748779, 0.4928605556488037, -0.7122899293899536, -0.5040570497512817, 0.4401978850364685, 0.3431069552898407, 0.09607837349176407, -0.21967779099941254, 0.13219445943832397, -0.2294546365737915, 0.7025386095046997, -0.2909908890724182, -0.27541443705558777, -0.942080020904541, 1.2261735200881958, 1.4100112915039062, -1.3296208381652832, 0.24021145701408386, -0.22791065275669098, -0.7801740169525146, 0.9250370264053345, 0.2129373699426651, 0.7484887838363647, 0.9641279578208923, -0.5584213137626648, 0.10272839665412903, -0.38076913356781006, -1.2548929452896118, 0.007560434751212597, 1.0072975158691406, 0.17012928426265717, 1.5131995677947998, 0.3599362075328827, -0.3872583508491516, 0.8431378602981567, 0.25354036688804626, 0.35263490676879883, 0.36319443583488464, 0.5556361675262451, -0.0025431611575186253, -0.23162759840488434, 0.18977665901184082, 0.5682579278945923, -0.6369441151618958, -0.7571401596069336, -0.3896673619747162, 0.3819412887096405, 0.38345369696617126, 0.5351659655570984, 0.2003766894340515, 0.31339186429977417, 0.1365218162536621, 0.9303947687149048, -0.13255207240581512, -0.6643092036247253, -0.09429393708705902, -0.19108009338378906, -0.5792576670646667, 0.3254021406173706, -0.7132011651992798, -0.4703991711139679, -0.4415760934352875, 0.13722550868988037, 0.5611319541931152, 0.31624919176101685, 0.5594898462295532, 1.1450217962265015, 0.7257302403450012, 0.08796525746583939, -0.7576069235801697, 0.052761711180210114, -0.14491085708141327, -1.2658681869506836, -0.22182190418243408, -0.5259408354759216, -0.1875220388174057, -0.4435698091983795, -0.431395024061203, 0.055240608751773834]}, "authors": [{"authorId": "2293820617", "name": "Yu Zhang"}, {"authorId": "29963551", "name": "Xiusi Chen"}, {"authorId": "2057050247", "name": "Bowen Jin"}, {"authorId": "2307033488", "name": "Sheng Wang"}, {"authorId": "2307565578", "name": "Shuiwang Ji"}, {"authorId": "2283212563", "name": "Wei Wang"}, {"authorId": "2257136881", "name": "Jiawei Han"}], "references": [{"paperId": "4fcb8b6c466937025d315be6a83b624b10e860b4", "title": "MAmmoTH2: Scaling Instructions from the Web"}, {"paperId": "cf4d2cc2270e9b48f5fc94ce26ee702697b9c79d", "title": "Advancing Multimodal Medical Capabilities of Gemini"}, {"paperId": "6d227a30452f773cea678fa8872ed43566c4f394", "title": "Capabilities of Gemini Models in Medicine"}, {"paperId": "d276a83b9bd5aaf75c6d921841959118e0d59870", "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers"}, {"paperId": "db8c398681aba69c605f3528dcbb9f853a382f14", "title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments"}, {"paperId": "cd2277d7d5776972287b586b67e3d290af60d5d6", "title": "Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare"}, {"paperId": "7bcdfc0759561118cd79667379c5d174e2a747f2", "title": "SaProt: Protein Language Modeling with Structure-aware Vocabulary"}, {"paperId": "a6ddb4a4a49e799d5cf36574c592ef9c543da7b7", "title": "Transparent medical image AI via an image-text foundation model grounded in medical literature."}, {"paperId": "c5e40c7de3ad5cce2bd01fce0f71de313a8ee837", "title": "PMC-LLaMA: toward building open-source language models for medicine."}, {"paperId": "cafe794035266eaef5d53e5d37aa486c71db9703", "title": "Rho-1: Not All Tokens Are What You Need"}, {"paperId": "a30bd328bc36a3f75aa18f653919611b1a8ea23d", "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs"}, {"paperId": "a76bc48006448ed6b8a32397b4748f60fdcd0921", "title": "Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry"}, {"paperId": "d209bca639ac7e3ec43ede41e48f05718db90d1b", "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios"}, {"paperId": "9be020d2865f682af9ed4d3d8c6c5719eedd9b9e", "title": "Self-supervised learning on millions of primary RNA sequences from 72 vertebrates improves sequence-based RNA splicing prediction"}, {"paperId": "c90318dd9ce7a1b0a45d71ceeb555cee3896a618", "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey"}, {"paperId": "892dba6e3240433b30c8ac776c3b6dad61ebdfa9", "title": "Automated Statistical Model Discovery with Language Models"}, {"paperId": "f95d2c974e6aa2497b85342296b863d0d10c0892", "title": "Large Language Model for Participatory Urban Planning"}, {"paperId": "f75d602332b7c2040259f73f23bdece1e0d9f0bb", "title": "ProLLaMA: A Protein Language Model for Multi-Task Protein Language Processing"}, {"paperId": "13dc81fce2c73de67dbe3829a32ec23d663cec89", "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI."}, {"paperId": "99f6acfc14ee633787f987768b31b240a87c2c0f", "title": "Me-LLaMA: Foundation Large Language Models for Medical Applications"}, {"paperId": "b798cf6af813638fab09a8af6ad0f3df6c241485", "title": "Benchmarking Retrieval-Augmented Generation for Medicine"}, {"paperId": "a3d749bc119f5c8425779e4e72e650720db2fe4b", "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset"}, {"paperId": "7c8a6552fe0e3b33456afdac79614e7dc04f0b4d", "title": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback"}, {"paperId": "13b8934468665ecb586f491d7f9f6c460cb095e5", "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains"}, {"paperId": "1823b8aecd62ccfca0cb6caa8e2a1159754afc5e", "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset"}, {"paperId": "af1d4a4141eaf52384ca85f1d468443f8f3d5c06", "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning"}, {"paperId": "c3e029b4a44c5784f296303f8cab2d7feb2d4ffa", "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text"}, {"paperId": "35b142ea69598e6241f0011312128031df55895c", "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"}, {"paperId": "6e160a96e81aad37189230c630a796b2ef11bd69", "title": "A Survey of Pre-trained Language Models for Processing Scientific Text"}, {"paperId": "660f5c57d459671f3f6436f116fef0bf011c1748", "title": "ChemDFM: Dialogue Foundation Model for Chemistry"}, {"paperId": "ba7399f5dcbf99179a9cf10762ed628895e5afee", "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains"}, {"paperId": "f58a49ea99e9a2754dd050927b9d830e3743f844", "title": "Towards 3D Molecule-Text Interpretation in Language Models"}, {"paperId": "c6e162aedf6a5ab0135e3b991577d77ca06673f9", "title": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning"}, {"paperId": "c890b28a001c885d1f7aa05f5d24ead9bf6ae058", "title": "MARG: Multi-Agent Review Generation for Scientific Papers"}, {"paperId": "60f47874beb5f00364f8621410d1d34c37d11007", "title": "AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets"}, {"paperId": "04a340b15945c70e642227bf249639b171beb3f8", "title": "PLLaMa: An Open-source Large Language Model for Plant Science"}, {"paperId": "9b093787f9be3d480cd11f8ec6ca5b0e44050d6d", "title": "Solving olympiad geometry without human demonstrations"}, {"paperId": "ddc1899e59a8e4fda60f5a175fef710a63abcef9", "title": "DrugAssist: A Large Language Model for Molecule Optimization"}, {"paperId": "3713112311efbcf785de17fa86e5bf42e4360f77", "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model"}, {"paperId": "08e3ba7c046964aad7a875e6795c8e01d2a11e55", "title": "PolyNC: a natural and chemical language model for the prediction of unified polymer properties"}, {"paperId": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51", "title": "Large Language Models on Graphs: A Comprehensive Survey"}, {"paperId": "796cda4533fc601e25943db0579d2dab20ea2145", "title": "BIOCLIP: A Vision Foundation Model for the Tree of Life"}, {"paperId": "0c8a630657a2cf5dea41472a9b5e20544ce2bd56", "title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks"}, {"paperId": "2a86d281bef364e2ea2d4fc61fde46ca25b955f1", "title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs"}, {"paperId": "288e64e8adb23d81e291a2cb51e3a56b315023b7", "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning"}, {"paperId": "0c26024316c823e326bed05fd1611de365c0ebb3", "title": "TableLlama: Towards Open Large Generalist Models for Tables"}, {"paperId": "0a29191d66a129709980cbe3c937aa9e98707dc8", "title": "FORGE: Pre-Training Open Foundation Models for Science"}, {"paperId": "c74e1671cee1e19c49a5aafbedfe403af471b8ec", "title": "GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences"}, {"paperId": "c67a58bb5eb9cb6557a6032bb058a5cab978907f", "title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare"}, {"paperId": "c86de166504e73465a64a8ac89335d63cf800b1c", "title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT"}, {"paperId": "15a2682ba1b479dea284062dd097a9a349a2eceb", "title": "AlpaCare: Instruction-tuned Large Language Models for Medical Application"}, {"paperId": "55fca5ba3b7a76d14fa60fbb02e0b02f2fe9e35d", "title": "\"Why Should I Review This Paper?\" Unifying Semantic, Topic, and Citation Factors for Paper-Reviewer Matching"}, {"paperId": "468fc94845b52c6e96ba1f3c3884d0653d5421b4", "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding"}, {"paperId": "bf93fe733932fd25780ef84911b8a507bec1c372", "title": "Neural scaling of deep chemical models"}, {"paperId": "43f3520f9d957634c9e389ce8c7dac99619e4f66", "title": "UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web"}, {"paperId": "2129c6edc2593bf4adb5bc2772fdb042bdf14070", "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design"}, {"paperId": "525d4aee811dcfdfd11afe7d0ae9204f03c8a74e", "title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions"}, {"paperId": "48b29405b5a0d37dce376e5f2e02411dc1afa5f4", "title": "CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training"}, {"paperId": "25738c43c0c4788d803981eaf5d397691aba0958", "title": "MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter"}, {"paperId": "b16c7d45183b9d595ab64301be019741b1528860", "title": "Llemma: An Open Language Model For Mathematics"}, {"paperId": "3f413dca2607d68301143770e599b59d461a569e", "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks"}, {"paperId": "7370c206a551f64d2dd12f38f73b28f1725b9def", "title": "ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets"}, {"paperId": "c3382fd533b9dd7f8ed7ba7766159079bc1d3935", "title": "BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"}, {"paperId": "b1e90b67675b6d7ae88b563a93cb4d375857cb15", "title": "CellPLM: Pre-training of Cell Language Model Beyond Single Cells"}, {"paperId": "f2209eb5ac6747319a29b87dedabb97770be3243", "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis"}, {"paperId": "8946891e94831adc8cddb0d32311cce2445c96d2", "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"}, {"paperId": "d6354e91d8dcf73bff50097b76a81de874f7bd7a", "title": "OceanGPT: A Large Language Model for Ocean Science Tasks"}, {"paperId": "c5fa173e6f52dbc2e5156e2e7c9e6d3c0f1c4aa0", "title": "DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients"}, {"paperId": "b06e1a2c84fb3bff03b10283bc863f007f5483b6", "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics"}, {"paperId": "9298cb4086ba3abaedcfc25f4a1ee1ff69de9a15", "title": "Enhancing Representation in Radiography-Reports Foundation Model: A Granular Alignment Algorithm Using Masked Contrastive Learning"}, {"paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}, {"paperId": "4c5b4a8e31d3119c1e3b5753693ff283c9717218", "title": "DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation"}, {"paperId": "4c6fb350e7769cb730a15c62927b6e9b563d0157", "title": "DARWIN Series: Domain Specific Large Language Models for Natural Science"}, {"paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7", "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"}, {"paperId": "e3fd89a7f6b28973cfc68bfc51caebd8fb93f0bc", "title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine"}, {"paperId": "5df24ed6fdf10d1e92885687abce7bd5e56f3f85", "title": "CMB: A Comprehensive Medical Benchmark in Chinese"}, {"paperId": "0f0024bfef037b97b324b97150ee022c178d6282", "title": "A visual\u2013language foundation model for pathology image analysis using medical Twitter"}, {"paperId": "2e3dcf5a5d58ac210d0d87e9f918540a8373211a", "title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"}, {"paperId": "a50d4fd8f584276c0fd8560255884edd57aa926e", "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue"}, {"paperId": "dd7e55cf8a0fd9a8f9588f7bc384061692d80191", "title": "A Symbolic Characters Aware Model for Solving Geometry Problems"}, {"paperId": "df0ddb588a200d095743e9d26fc4a9318619766e", "title": "Towards Generalist Foundation Model for Radiology"}, {"paperId": "6eb3dd2b64db74f9743802acbb9875bdffb9d246", "title": "Text-guided Foundation Model Adaptation for Pathological Image Classification"}, {"paperId": "c9dbdae8146b9f97e254f5d26fd6efde96eaa703", "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner"}, {"paperId": "de7e5fee8cf03bd485b1104d3e40e8ab45d76c0a", "title": "Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers"}, {"paperId": "b0883ddf7fb07aabfeb2b2f593e7ac302aa42372", "title": "PRIOR: Prototype Representation Joint Learning from Medical Images and Reports"}, {"paperId": "b7482bf179bd230ea5ea130bf3243b1fd73658ec", "title": "M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization"}, {"paperId": "d1f49ea7287120bb7a1e214862388222ce76d9e8", "title": "Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems"}, {"paperId": "6c95835ffdf5ee65c35f50fd00a56f54faf15eda", "title": "Crystal Structure Generation with Autoregressive Large Language Modeling"}, {"paperId": "bffe2951b1ec9ebde3677a7aadc8b91212a9fdfc", "title": "BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval"}, {"paperId": "bfd2b76998a0521c12903ef5ced517adf70ad2ba", "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution"}, {"paperId": "0f4780f3f42dbe9755d54495ae17244cc88a7483", "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome"}, {"paperId": "e795f62df9ccac2a39e126f95404e5364d55193c", "title": "FuXi: a cascade machine learning forecasting system for 15-day global weather forecast"}, {"paperId": "e17e34fc53ce5e1d039d5dce056cb9c7691eb568", "title": "Quilt-1M: One Million Image-Text Pairs for Histopathology"}, {"paperId": "32f541216112de78037d8e0f95ddc152eb6f05fa", "title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization"}, {"paperId": "2cea5bbb6a4e45667df475e22023422c0f2c7008", "title": "Large-Scale Cell Representation Learning via Divide-and-Conquer Contrastive Learning"}, {"paperId": "f22d71c7ce9720ba1f717a4f1181488200e78198", "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"}, {"paperId": "a42fc49a300136d60aaebb668369010ee7746150", "title": "Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images"}, {"paperId": "6e6bf202d2bb3d22c08863b160ea93fbb2d6b0cc", "title": "Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias"}, {"paperId": "7d1e59ce254bea5228da634dbe7c5c4160df6f98", "title": "Transfer learning enables predictions in network biology"}, {"paperId": "9e8af0791e8c87452c8cff25dab5448a29c218d4", "title": "ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback"}, {"paperId": "20d7965c0b282a0cd7f990e435d0f6bc9535bbc6", "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks"}, {"paperId": "b17969990b3745a494f8fcadae6c8cd0426dc3ec", "title": "Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding"}, {"paperId": "002bf0720404e5dc6bf43eff64f116ec755b405f", "title": "SciMON: Scientific Inspiration Machines Optimized for Novelty"}, {"paperId": "2db3eb03b0a9fedde37066673532804d3e224a4a", "title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files"}, {"paperId": "bf9fc4c2f834b314328f30a585abb755bfa0877e", "title": "W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting"}, {"paperId": "ac76a385400d495c80a6f03599865e319774fb86", "title": "Interactive and Explainable Region-guided Radiology Report Generation"}, {"paperId": "38a9609a5bd874534527df9b00f2897927e57be9", "title": "MedAlpaca - An Open-Source Collection of Medical Conversational AI Models and Training Data"}, {"paperId": "354dcdebf3f8b5feeed5c62090e0bc1f0c28db06", "title": "Augmenting large language models with chemistry tools"}, {"paperId": "8489b55d992e6a3ac54aec7094a42ec8e333012f", "title": "SELFormer: molecular representation learning via SELFIES language models"}, {"paperId": "b04283494a9ca6a456eb6912fe56d26aac1c4800", "title": "Towards Automated Urban Planning: When Generative and ChatGPT-like AI Meets Urban Planning"}, {"paperId": "f4a62db4dd86129561a16b0a18cc09985580554c", "title": "FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead"}, {"paperId": "374140289b1479c824f0643e1d4179737893ca45", "title": "PK-Chat: Pointer Network Guided Knowledge Driven Generative Dialogue Model"}, {"paperId": "4a7f6c4e71e20311ade4e76e8d0945d499c31fcd", "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"}, {"paperId": "c47d0e4aae45f63defa22456e8e13f249f3cf034", "title": "Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis"}, {"paperId": "a990b0604f64f3277e6b44299711f1ce44b8460a", "title": "Multiple sequence alignment-based RNA language model and its application to structural inference"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "8f3138f7ee5127faab265793be8ae278bc49d9b1", "title": "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"}, {"paperId": "5814bd146b37e13115af4330caf3a751159a156f", "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "d55b69a533dea69c8b2673cde8de90c6626ee789", "title": "A Text-guided Protein Design Framework"}, {"paperId": "e7dcdfb7734d59b97f825cce8b3105a2d9b14d10", "title": "The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study"}, {"paperId": "e962f95e03a50ff2f3a0fe7840daebac04578c46", "title": "Structure-informed Language Models Are Protein Designers"}, {"paperId": "b822f2abca1da6f990b2bd47ed43da0671bfc6f8", "title": "Unifying Molecular and Textual Representations via Multi-task Language Modelling"}, {"paperId": "4ea1f64c13280ef13f506eef4b3dd2395d1cf171", "title": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts"}, {"paperId": "835c305e52769a8433f8383e91d33ba6c66ad55b", "title": "Large language models generate functional protein sequences across diverse families"}, {"paperId": "898842b1c1fb4689ffa2f20c0836961a962b7691", "title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?"}, {"paperId": "1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92", "title": "Ankh \u2625: Optimized Protein Language Model Unlocks General-Purpose Modelling"}, {"paperId": "84729ec815f0607a4a2370c0969e8c3ba82a9411", "title": "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing"}, {"paperId": "774e71df81488d83eddb706313fe9480fc1232d2", "title": "MGeo: Multi-Modal Geographic Language Model Pre-Training"}, {"paperId": "5425de16356015c2f26d2a50684c6c46d6998f51", "title": "MIMIC-IV, a freely accessible electronic health record dataset"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "958bb3831589246fe5b6b58cf99e3b65c58d027f", "title": "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing"}, {"paperId": "c49a0912595a1cc70aab63524f64ed08c92194a8", "title": "Evolutionary-scale prediction of atomic level protein structure with a language model"}, {"paperId": "72fce949725b20428e5f56247fef5c6bd1ce6154", "title": "UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression"}, {"paperId": "577e96243eaffa374a977fa46cf1680877b2b509", "title": "BARTSmiles: Generative Masked Language Models for Molecular Representations"}, {"paperId": "0d68e244faa9a5944228b571d91bc7bed88083cd", "title": "SciRepEval: A Multi-Format Benchmark for Scientific Document Representations"}, {"paperId": "62a45ab7b676f3877d41f66f6c9ddf1ec44a1c5f", "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics"}, {"paperId": "161f4f53cbddaa7523c11cb7173789f1bb567559", "title": "Bidirectional generation of structure and properties through a single molecular foundation model"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "e1801f68d42f4a1a96d580b73706733883bf6af8", "title": "ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples"}, {"paperId": "8a5837b9245035972f97d6796f62790b6e7e9d71", "title": "SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation"}, {"paperId": "cdd9c1d23f9e89d5113f3e31821bb174c6a6afed", "title": "MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "ad3dfb2514cb0c899fcb9a14d229ff2a6018892f", "title": "Deep Bidirectional Language-Knowledge Graph Pretraining"}, {"paperId": "4867e6c7d190e37b9199a67ebeac62180b59aa32", "title": "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning"}, {"paperId": "4e61c7f801a3dcbef288678e1ed28afcf2b75535", "title": "Large-Scale Distributed Training of Transformers for Chemical Fingerprinting"}, {"paperId": "c8459d9f59c3d6909bae8f9b9fcf622f55c2e6ea", "title": "polyBERT: a chemical language model to enable fully machine-driven ultrafast polymer informatics"}, {"paperId": "44279244407a64431810f982be6d0c7da4429dd7", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"}, {"paperId": "81adb80e390f25d4a2d764b1063eeaa2c334d441", "title": "Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"}, {"paperId": "28ff0816f19a5e3e37eac5569de41872fd262f0a", "title": "Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"}, {"paperId": "1c7a4e8d9f4fcf19a5d1caa078c66ca39cb75dd2", "title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language"}, {"paperId": "f9c95ea9bcd529309c1d743f0926b5137f8ed60f", "title": "Learning inverse folding from millions of predicted structures"}, {"paperId": "72c53ffacd4ad86391dd70d3b18c2b9e80ba2956", "title": "TransPolymer: a Transformer-based language model for polymer property predictions"}, {"paperId": "dcb31b98ec58f3fff9f94f148e2952595f017fd9", "title": "ProtGPT2 is a deep unsupervised language model for protein design"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "26133033149afb4b45e5d0a4bd1dc712a236810e", "title": "ProGen2: Exploring the Boundaries of Protein Language Models"}, {"paperId": "d593b9b8d63426f0d6a795dd7f2294619bc03610", "title": "NaturalProver: Grounded Mathematical Proof Generation with Language Models"}, {"paperId": "9ebd20a621ea9f458d4919c8e247932ddc7664fd", "title": "The Diminishing Returns of Masked Language Models to Science"}, {"paperId": "5476911896491f32ca123e7030e9bae1116d8de2", "title": "BatteryBERT: A Pretrained Language Model for Battery Database Enhancement"}, {"paperId": "a1aeb5442e31276b197696f49b3243112a4049ce", "title": "Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing"}, {"paperId": "07264347e959913a6ea37953d9c0e30ed4efb3ba", "title": "Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions"}, {"paperId": "a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f", "title": "LinkBERT: Pretraining Language Models with Document Links"}, {"paperId": "a376e6b118a10a8cb8a2920f6b83a70f087579f5", "title": "Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"}, {"paperId": "182b92dafafd619a472418818fda0947efc100a8", "title": "ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps"}, {"paperId": "eee7997106834442f1704e4681a9a761df6696a1", "title": "Unified Deep Learning Model for Multitask Reaction Predictions with Explanation"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "10194e9d1d6b8ca8870445c990d4933c1dac1125", "title": "FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators"}, {"paperId": "6958612fea7f220757b4165b8e12d4b62b4baa80", "title": "A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals"}, {"paperId": "72d5bd1095f9719742212f81a2a3790583e95796", "title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings"}, {"paperId": "f30e95be411456a709e7cb9a8b3a3e557bd0356a", "title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences"}, {"paperId": "b66e776502b8484a09f8759c8dc3737ebe714f34", "title": "Controllable protein design with language models"}, {"paperId": "6538421d45c27cbb9b655d86d1974d7886f8895e", "title": "Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning"}, {"paperId": "2fca2821ac2beb60fa0e26866e8f063261713951", "title": "Joint Learning of Localized Representations from Medical Images and Reports"}, {"paperId": "0f1c956f84a68ea4d6f5ebcf3c4d1d4e1f41d8f3", "title": "Language models can learn complex molecular distributions"}, {"paperId": "cee993b4bbfe23d47be828bf7887a882975a1c98", "title": "Building astroBERT, a language model for Astronomy & Astrophysics"}, {"paperId": "ee2289b89a651157139da24674cbb201f479b9bb", "title": "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "4a3c9ec4c4bf37aec0b0d9c83c74e3589e8343b4", "title": "GAKG: A Multimodal Geoscience Academic Knowledge Graph"}, {"paperId": "01bd2ae31f36ad05e94aaf7aa87b836c0982f622", "title": "SciClops: Detecting and Contextualizing Scientific Claims for Assisting Manual Fact-Checking"}, {"paperId": "a50ee01a66aa92b664a78b2a0628ac346f53e31d", "title": "ClimateBert: A Pretrained Language Model for Climate-Related Text"}, {"paperId": "1404477ac8050b0a37658b8e14ee76026683aa64", "title": "The State of the Art"}, {"paperId": "39df1a17da84f02bfcb8751de8965b798653a5ee", "title": "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"}, {"paperId": "b15469d0ab3dc3a9dec037d761817b3fe546bed6", "title": "Pre-trained Language Models in Biomedical Domain: A Systematic Survey"}, {"paperId": "0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d", "title": "GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"}, {"paperId": "dae0eed35bf60a1f307cfa9ec99dac5cf3671085", "title": "FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining"}, {"paperId": "8cdb9f975aaff5adb51cfa164199010bb9b9b6d1", "title": "Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "2406cf39805c70264c4226b7325a09b506c70921", "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor"}, {"paperId": "3f9f7f690e003176316d0ee56fbcbfed4b6b0948", "title": "Chemformer: a pre-trained transformer for computational chemistry"}, {"paperId": "0cfce36622a017a86c2248ebe5dfdd0f8d643df7", "title": "Language models enable zero-shot prediction of the effects of mutations on protein function"}, {"paperId": "dc1402d05c6a18843d1fc6b31f1bd4fcdaa8ad30", "title": "Benchmarking for biomedical natural language processing tasks with a domain specific ALBERT"}, {"paperId": "a280b17dc450866982dead4302d6cb6e92ea59f6", "title": "Automated Chemical Reaction Extraction from Scientific Literature"}, {"paperId": "f829674dceb91b76c9350b90d9432530eb1f7ca1", "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education"}, {"paperId": "291133a657498920451481d3bf784ebbafda8d6e", "title": "GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning"}, {"paperId": "6003d268e9b5230dbc3e320497b50329d6186816", "title": "SciFive: a text-to-text transformer model for biomedical literature"}, {"paperId": "c07651110d3b98b63607557b57808d15d99013dd", "title": "ProteinBERT: a universal deep-learning model of protein sequence and function"}, {"paperId": "d9317660e2a538d9c018028956fd114d55330f82", "title": "Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"}, {"paperId": "fb1c90806fc5ec72987f58110aa255edbce6620d", "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"}, {"paperId": "386bfd0e411dee4f512a8737c55dd84846981182", "title": "TABBIE: Pretrained Representations of Tabular Data"}, {"paperId": "58fe64beb45b18f63cbc001849a0dee3e4e60482", "title": "Improving Biomedical Pretrained Language Models with Knowledge"}, {"paperId": "1991ea2ec85113cadf38faea840f4b5cf73ae0c7", "title": "ELECTRAMed: a new pre-trained language representation model for biomedical NLP"}, {"paperId": "1066d94640d1374cc1f21b49083095e1cbcdc67f", "title": "Capturing Row and Column Semantics in Transformer Based Question Answering over Tables"}, {"paperId": "17c2bb358169541f2d0a769f80779f46d1cd3d37", "title": "MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"}, {"paperId": "0c2b09dafba1d030682d9e1149748fd2fca95f47", "title": "Extraction of organic chemistry grammar from unsupervised learning of chemical reactions"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "beb6a2d33b1979d9d2010fc5721fd307e015c342", "title": "Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression"}, {"paperId": "dfeb14849634814da58df726a97f986da2e2cacc", "title": "OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "93b6b79b4ef6c345f31722ce7c829385c6dce0d6", "title": "Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"}, {"paperId": "deee48c5e0ac0407a1e002905caaf2b174bdb0e6", "title": "MSA Transformer"}, {"paperId": "40558e802642e0d1cdfbaaaec1be339c129e5dfc", "title": "Faculty Opinions recommendation of Learning the language of viral evolution and escape."}, {"paperId": "9eaceb2b21876a39f948c6841deee2c2767fba2f", "title": "LBERT: Lexically-aware Transformers based Bidirectional Encoder Representation model for learning Universal Bio-Entity Relations"}, {"paperId": "903ad24a5e924086869f6f9f0ae2992ae2971048", "title": "BioMedBERT: A Pre-trained Biomedical Language Model for QA and IR"}, {"paperId": "e4c5e81e6e337bb94af3eb719df5f029b40434fa", "title": "Molecular representation learning with language models and domain-relevant auxiliary tasks"}, {"paperId": "70b0c85638d195dbde56cbedc94ae4363b272b58", "title": "A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"}, {"paperId": "a79b520571f7373cbeb8c6ffc02f6a719b3bce38", "title": "CODER: Knowledge-infused cross-lingual medical term embedding for term normalization"}, {"paperId": "76ad0d37bd3845431b3ca9d07f8db74c82752298", "title": "Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art"}, {"paperId": "b9868f59ed09866143261e43e7596d1c71326399", "title": "Clinical concept extraction using transformers"}, {"paperId": "615204452304331004532c5800399ef55d58b4c7", "title": "Self-Alignment Pretraining for Biomedical Entity Representations"}, {"paperId": "24a12899ce97bd4a56f7c6b49d3979b9465f0190", "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training"}, {"paperId": "95ce6f77e26b496ffb705a0a3b54f2fb7a6d2452", "title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction"}, {"paperId": "5ba77a5bdeffb62aa0902ae68997bbc38db8a722", "title": "MedICaT: A Dataset of Medical Images, Captions, and Textual References"}, {"paperId": "6d6595766a35f12a6ad671d05634b5e2159d4f3e", "title": "Bio-Megatron: Larger Biomedical Domain Language Model"}, {"paperId": "c4ce6aca9aed41d57d588674484932e0c2cd3547", "title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers"}, {"paperId": "a2cde1da31a61adab24e702999680108ab58e5ff", "title": "COMETA: A Corpus for Medical Entity Linking in the Social Media"}, {"paperId": "6dd9f99cecd38504b667d320eb2a6267a9fee35d", "title": "Contrastive Learning of Medical Visual Representations from Paired Images and Text"}, {"paperId": "8b2cbb2f101b025c16e12d0d7628f65e5378e10d", "title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing"}, {"paperId": "fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7", "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"}, {"paperId": "44507bde6e9caf60b41c60d703e7972b520d48a6", "title": "Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems"}, {"paperId": "c43d9cade31600400a0f62beb5bbcc1b548e009e", "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "2b01b3334ce950c76c9c3c2c9146a7f0ce79cc50", "title": "Conceptualized Representation Learning for Chinese Biomedical Text Mining"}, {"paperId": "309fd887dc776db182ba054c979b37738a2bee07", "title": "Earthquake transformer\u2014an attentive deep-learning model for simultaneous earthquake detection and phase picking"}, {"paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"}, {"paperId": "b01b962620a1627dbdd0d202eda9e7185fa8f2e0", "title": "Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "5b0b876a815f8a07876052c03c4a733d595f72fb", "title": "On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining"}, {"paperId": "5d4de0fa45aeddc31142e6a24666d06ed7923f1e", "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"}, {"paperId": "126fb7df6bcab2b70000dfe5b940ada63ae1ba6a", "title": "COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter"}, {"paperId": "a5b1d1cab073cb746a990b37d42dc7b67763f881", "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data"}, {"paperId": "3502a542b2e98d9094e1880a30f652d4170b9534", "title": "TLDR: Extreme Summarization of Scientific Documents"}, {"paperId": "a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031", "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "52cb05d721688cb766c6e282e9d55c3b8e3dc0cf", "title": "TaPas: Weakly Supervised Table Parsing via Pre-training"}, {"paperId": "3dd61d97827e3f380bf9304101149a3f865051fc", "title": "Injecting Numerical Reasoning Skills into Language Models"}, {"paperId": "355601688c5c567bc5900c2ed09941538bfed960", "title": "Molecule Attention Transformer"}, {"paperId": "b5cdbc2793ece23babc840c697a2cc4c6f59d77e", "title": "[PubMed]."}, {"paperId": "70139335657559df6f0de540f9a0bd4f9f0d8bac", "title": "Explaining Relationships Between Scientific Documents"}, {"paperId": "42f0bae2dacba44e9b5d8f050da3cbe41b9fc437", "title": "Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation"}, {"paperId": "d1f407b16fb8d99487baee37ed0805676c58e7ac", "title": "MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c33b69855e03958f1d9d3cff7abd2eb2184ecd52", "title": "Fine-Tuning Bidirectional Encoder Representations From Transformers (BERT)\u2013Based Models on Large-Scale Electronic Health Record Notes: An Empirical Study"}, {"paperId": "3d99747cc3e13d22f21e02c35e82b57d2e351e2a", "title": "SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "57633ff5c6f0708be25e651f51eef29d2fbfe48b", "title": "BEHRT: Transformer for Electronic Health Records"}, {"paperId": "753b7a701adc1b6072378bd048cfa8567885d9c7", "title": "Invariant Risk Minimization"}, {"paperId": "347bac45298f37cd83c3e79d99b826dc65a70c46", "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"}, {"paperId": "c3229debfda1b015c88404cf98f1074237d80809", "title": "Pre-training of Graph Augmented Transformers for Medication Recommendation"}, {"paperId": "8338a903d8078481ff8af777475f7394d00e9d57", "title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"}, {"paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2", "title": "Publicly Available Clinical BERT Embeddings"}, {"paperId": "5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f", "title": "Probing Biomedical Embeddings from Language Models"}, {"paperId": "95ebe37c856e914b760bc5db63561f461ec444cc", "title": "Structural Scaffolds for Citation Intent Classification in Scientific Publications"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "f1a17b7c4cae4513731f6d81b433e338cf4114eb", "title": "PadChest: A large chest x-ray image dataset with multi-label annotated reports"}, {"paperId": "89a816719613e220a64ab2590c938c23bbfe187e", "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"}, {"paperId": "fa795716fb68ccbe52a8479c4d28da340d932222", "title": "PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data"}, {"paperId": "32915f4e54d3a8add04241ffb5da0379494a6358", "title": "RNAcentral: a hub of information for non-coding RNA sequences"}, {"paperId": "92a79ba7bb783b64069ca006206b05458a696b36", "title": "PubChem 2019 update: improved access to chemical data"}, {"paperId": "a564fabf130ff6e2742cfba90c7a4018937d764d", "title": "Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"}, {"paperId": "b21b927c251c415b601b6d7f785a42cc5c292635", "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction"}, {"paperId": "f2588de5173fb047192dbb93d62ce6636bdf46bd", "title": "Lessons from Natural Language Inference in the Clinical Domain"}, {"paperId": "649def34f8be52c8b66281af98ae884c09aef38b", "title": "Construction of the Literature Graph in Semantic Scholar"}, {"paperId": "678fd7c48efe21434148b4b3482c2b8b3ee618fc", "title": "Deep Neural Solver for Math Word Problems"}, {"paperId": "aaf046c4da99ee6184f3fd31961a9967272152f9", "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network"}, {"paperId": "cbd569036fc72ae7ff747350b91816440282596b", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "d0ab11de3077490c80a08abd0fb8827bac84c454", "title": "MoleculeNet: a benchmark for molecular machine learning"}, {"paperId": "eba56a16763a489367dd5ca1ead995d75dbe8b1a", "title": "What's What: The (Nearly) Definitive Guide to Reaction Role Assignment"}, {"paperId": "d71af418eeb9f5a68062929bae12af74773ffcb2", "title": "The ChEMBL database in 2017"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "95cd83603a0d2b6918a8e34a5637a8f382da96f5", "title": "MIMIC-III, a freely accessible critical care database"}, {"paperId": "9f5d1ac4bd9900238ac238fa221191aa436e87a1", "title": "A Large Public Corpus of Web Tables containing Time and Context Metadata"}, {"paperId": "082ae4ab466b36029c8d0f28a5ec75a665e7a715", "title": "A Full-Text Learning to Rank Dataset for Medical Information Retrieval"}, {"paperId": "f7629d88403e22cde5c0f3f5b3df7e0a9623b7b9", "title": "Overview of the Coupled Model Intercomparison Project Phase 6 (CMIP6) experimental design and organization"}, {"paperId": "5972c3d8507359a6cff6ef17c4af206ec76b32bb", "title": "ZINC 15 \u2013 Ligand Discovery for Everyone"}, {"paperId": "53a7a77005e93fb884ebad4fb958bc774b97bf9f", "title": "A global reference for human genetic variation"}, {"paperId": "c87dccf7c21e67679389f23f86f039cd96720c3f", "title": "Solving Geometry Problems: Combining Text and Diagram Interpretation"}, {"paperId": "b41e95c8c97846d5ca4c11ef79d7814499cc9663", "title": "Compositional Semantic Parsing on Semi-Structured Tables"}, {"paperId": "8ebc4145aef6a575cbaffcfeec56b20586db573a", "title": "An Overview of Microsoft Academic Service (MAS) and Applications"}, {"paperId": "795dc87b4727b303d3672539e4578d41dfd3aeb3", "title": "UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches"}, {"paperId": "89655dc3c3a794cb25e055aed79424c66301d70f", "title": "Quantum chemistry structures and properties of 134 kilo molecules"}, {"paperId": "696753d59185436ec95ecf3021c413f353be4874", "title": "NCBI disease corpus: A resource for disease name recognition and concept normalization"}, {"paperId": "98476502ec663771d41d2f8c948fd176257f17bd", "title": "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation"}, {"paperId": "be415644afa5de5180f42c1d736e6be8bf7cf254", "title": "Open-source platform to benchmark fingerprints for ligand-based virtual screening"}, {"paperId": "5dcb131e1b335b14f712e31bc6a487e697f1b29b", "title": "GENCODE: The reference human genome annotation for The ENCODE Project"}, {"paperId": "bd2062eb413816144bcfc1c8511f73712c67ac74", "title": "PubMed and beyond: a survey of web tools for searching biomedical literature"}, {"paperId": "acd13b907c9acf16c6bf7c0a1cff0d7f2402c669", "title": "OpenStreetMap: User-Generated Street Maps"}, {"paperId": "f020b61789112fe7241b871907268f0bdc5c84fa", "title": "ArnetMiner: extraction and mining of academic social networks"}, {"paperId": "83ca945514b8b623ba63335b432ae5da70b7d226", "title": "An introduction to Wu's method for mechanical theorem proving in geometry"}, {"paperId": "3f7983818b76a5f1b5daf9b605877ed401c8e73c", "title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules"}, {"paperId": "a2d2e3bbae2cb3905bff421fdd90bc130af1b4e3", "title": "Rapid and sensitive protein similarity searches."}, {"paperId": "dddd1de22beca06119ca83122afd82059abbf48a", "title": "Towards a Human-Computer Collaborative Scientific Paper Lifecycle: A Pilot Study and Hands-On Tutorial"}, {"paperId": "2b18e63caff82f1a688714aed099b8856db92581", "title": "Domain-Agnostic Molecular Generation with Self-feedback"}, {"paperId": "cfdc3c11d8b031457b6d7cf9a633e45ff54222de", "title": "A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions"}, {"paperId": "bbb9d3c982a2a15b481e51bc0302a967818b6448", "title": "UniMath: A Foundational and Multimodal Mathematical Reasoner"}, {"paperId": "9904557ddf52402170da288cf3a88a4f28847f0d", "title": "forecasting with 3D neural"}, {"paperId": "e5bea15998eea829f7acc92710954283443d582d", "title": "A Japanese Masked Language Model for Academic Domain"}, {"paperId": "52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7", "title": "LILA: A Unified Benchmark for Mathematical Reasoning"}, {"paperId": "57651d65078818821234d13544ac1f29858dcd67", "title": "Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries"}, {"paperId": "ef74ded91ac870426a057ba4027cf9890ad43197", "title": "The Impact of Domain-Specific Pre-Training on Named Entity Recognition Tasks in Materials Science"}, {"paperId": null, "title": "Trec-covid: constructing a pandemic information retrieval test collection"}, {"paperId": null, "title": "Prottrans: Toward understanding the language of life through self-supervised learning"}, {"paperId": "5c5751d45e298cea054f32b392c12c61027d2fe7", "title": "S2ORC: The Semantic Scholar Open Research Corpus"}, {"paperId": null, "title": "The era5 global reanalysis Quarterly"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "1f1eaf19e38b541eec8a02f099e3090536a4c936", "title": "The Unified Medical Language System (UMLS): integrating biomedical terminology"}, {"paperId": "58a93e9cd60ce331606d31ebed62599a2b7db805", "title": "The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000"}, {"paperId": null, "title": "2022. Large-scale chemical language representations capture molecular structure and properties"}, {"paperId": null, "title": "2021b. Mapping the space of chemical reactions using attention-based neural networks"}, {"paperId": null, "title": "2022. Mwp-bert: Numeracy-augmented pre-training for math word problem solving"}, {"paperId": null, "title": "2023. Reviewergpt? exploratory study on using large language models for paper reviewing"}, {"paperId": null, "title": "2022. A search engine for discovery of scientific challenges and directions"}, {"paperId": null, "title": "2023. Survey of hallucination in natural language generation"}, {"paperId": null, "title": "2023b. Pat-ton: Language model pretraining on text-rich networks"}, {"paperId": null, "title": "2024. Foundation model for advancing healthcare: Challenges, opportunities, and future directions"}, {"paperId": null, "title": "2023d. Masked vision and language pre-training with unimodal and multimodal contrastive losses for medical visual question answering"}, {"paperId": null, "title": "2022. Omnitab: Pretraining with natural and synthetic data for few-shot table-based question answering"}, {"paperId": null, "title": "2024b. Chemllm: A chemical large language model"}, {"paperId": null, "title": "2022. Single-sequence protein structure prediction using a language model and deep learning"}, {"paperId": null, "title": "2023. Xraygpt: Chest radiographs summarization using medical vision-language models"}, {"paperId": null, "title": "2024. Mathematical discoveries from program search with large language models"}, {"paperId": null, "title": "2023b. Molfm: A multi-modal molecular foundation model"}, {"paperId": null, "title": "2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"}, {"paperId": null, "title": "2022a. scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data"}, {"paperId": null, "title": "2023g. Scibench: Evaluating college-level scientific problem-solving abilities of large language models"}, {"paperId": null, "title": "2022. Matscibert: A materials domain language model for text mining and information extraction"}, {"paperId": null, "title": "2023. A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing"}, {"paperId": null, "title": "2023b. Qilin-med: Multi-stage knowledge injection advanced medical large language model"}, {"paperId": null, "title": "2022a. Biobart: Pretrain-ing and evaluation of a biomedical generative language model"}, {"paperId": null, "title": "2024. Large-scale foundation model on single-cell transcriptomics"}, {"paperId": null, "title": "2023a. Dnagpt: A generalized pretrained tool for multiple dna sequence analysis tasks"}, {"paperId": null, "title": "2023. A systematic survey of chemical pre-trained models"}, {"paperId": null, "title": "2023a. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning"}, {"paperId": null, "title": "2024. Biomedlm: A 2.7 b parameter language model trained on biomedical text"}, {"paperId": null, "title": "2022. Molgpt: molecular generation us-ing a transformer-decoder model"}, {"paperId": null, "title": "2023. Catalyst energy prediction with catberta: Unveiling feature exploration strategies through large language models"}, {"paperId": null, "title": "Li, et al. 2023b"}, {"paperId": null, "title": "2023. Doctorglm: Fine-tuning your chi-nese doctor is not a herculean task"}, {"paperId": null, "title": "2023. Chatcli-mate: Grounding conversational ai in climate science"}, {"paperId": null, "title": "2024. Researchagent: Iterative research idea generation over scientific literature with large language models"}, {"paperId": null, "title": "2022. Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning"}, {"paperId": null, "title": "2024. Bimedix: Bilingual medical mixture of experts"}, {"paperId": null, "title": "2022. Translation between molecules and natural language"}, {"paperId": null, "title": "2023b. Hu-atuogpt, towards taming language model to be a doctor"}, {"paperId": null, "title": "2023c. Scientific discovery in the age of artificial intelligence"}, {"paperId": null, "title": "2023e. Meditron-70b: Scaling medical pretraining for large language models"}]}