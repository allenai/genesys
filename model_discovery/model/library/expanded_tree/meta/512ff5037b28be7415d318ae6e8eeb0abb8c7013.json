{"paperId": "512ff5037b28be7415d318ae6e8eeb0abb8c7013", "abstract": "Models based on the attention mechanism, i.e., transformers, have shown extraordinary performance in natural language processing (NLP) tasks. However, their memory footprint, inference latency, and power consumption are still prohibitive for efficient inference at edge devices, even at data centers. To tackle this issue, we present an algorithm-architecture co-design named DTATrans. We find empirically that the tolerance to the noise varies from token to token in attention-based NLP models. This finding leads us to dynamically quantize different tokens with mixed levels of bits. Furthermore, we find that the overstrict quantization method causes a dilemma of the model accuracy and model compression ratio, which impels us to explore a method to compensate for the model accuracy when the compression ratio is high. Thus, in DTATrans, we design a compression framework that: 1) dynamically quantizes tokens while they are forwarded in the models; 2) jointly determines the ratio of each precision; and 3) compensate the model accuracy by exploiting lightweight computing on the 0-bit tokens. Moreover, due to the dynamic mixed-precision tokens caused by our framework, previous matrix-multiplication accelerators (e.g., systolic array) cannot effectively exploit the benefit of the compressed attention computation. We thus design our transformer accelerator with the variable-speed systolic array (VSSA) and propose an effective optimization strategy to alleviate the pipeline-stall problem in VSSA without hardware overhead. We conduct experiments with existing attention-based NLP models, including BERT and GPT-2 on various language tasks. Our results show that DTATrans outperforms the previous neural network accelerator Eyeriss by <inline-formula> <tex-math notation=\"LaTeX\">$16.04\\times $ </tex-math></inline-formula> in terms of speedup and <inline-formula> <tex-math notation=\"LaTeX\">$3.62\\times $ </tex-math></inline-formula> in terms of energy saving. Compared with the state-of-the-art attention accelerator SpAtten, our DTATrans achieves at least <inline-formula> <tex-math notation=\"LaTeX\">$3.62\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$4.22\\times $ </tex-math></inline-formula> energy efficiency improvement.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "year": 2023, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The transformer accelerator with the variable-speed systolic array (VSSA) is designed and an effective optimization strategy to alleviate the pipeline-stall problem in VSSA without hardware overhead is proposed."}, "embedding": {"model": "specter_v2", "vector": [0.5187064409255981, 0.49632933735847473, -0.6867552995681763, 0.2498999983072281, -0.49491918087005615, -0.038893718272447586, 0.3065583109855652, 0.3165576159954071, -0.6855940818786621, -0.5161998271942139, 0.42942920327186584, -0.14799830317497253, 0.4859146475791931, -0.1906430721282959, -0.04132017493247986, 0.1435849517583847, -0.7438650727272034, 0.21388700604438782, -0.07829086482524872, -0.13334020972251892, 0.21322473883628845, -0.3748941719532013, -1.10370671749115, 0.17634861171245575, 0.5119181275367737, 1.026715874671936, 0.14845679700374603, 0.7885766625404358, -0.7091305255889893, 0.2792675495147705, 0.5564578175544739, -0.3208993673324585, 0.29042932391166687, 0.5170798301696777, -0.15817251801490784, -0.633243203163147, 0.42893186211586, -0.3073500096797943, -0.5150225758552551, 1.261789083480835, -0.30587318539619446, 0.2569911777973175, 0.2765055000782013, -0.8762427568435669, -0.5571115016937256, 0.9405079483985901, 0.41477376222610474, 0.9347019791603088, -0.6638815999031067, -0.2840133309364319, 1.4164061546325684, -1.128683090209961, -0.08811385929584503, 1.310563087463379, 0.5865538716316223, 0.1602008193731308, -0.1231054961681366, -0.47497886419296265, 0.3190193772315979, 0.33636900782585144, -1.0607318878173828, -0.8592533469200134, -0.12092027813196182, 0.03164229542016983, 2.0442557334899902, -0.2640683650970459, 0.10946302860975266, 0.22283905744552612, 0.0626484602689743, 1.5220975875854492, -0.21164429187774658, -0.983284592628479, 0.07655090093612671, -0.578425943851471, 0.4103924632072449, 0.9272476434707642, -0.07360927015542984, 0.1778278648853302, -1.1946971416473389, -0.24344369769096375, 0.33483055233955383, 0.03645112365484238, 0.5093348026275635, 0.21759437024593353, -0.09472183883190155, 0.5171865224838257, 0.29671043157577515, 0.6502466201782227, -0.42808404564857483, 0.8160134553909302, 1.0227925777435303, -0.03842931240797043, -0.057183511555194855, 0.12350375205278397, -0.1405159831047058, 0.04036380723118782, -1.3846044540405273, 0.20738770067691803, -0.2679108679294586, 0.8961495161056519, -0.3732186257839203, 0.8159824013710022, -1.0907974243164062, -0.05009070038795471, 1.1155972480773926, 0.6380289793014526, 0.4086952209472656, -0.5099795460700989, 0.3793903887271881, -0.6372066736221313, 0.14823928475379944, -0.42534908652305603, -0.073239266872406, -0.3016933798789978, -1.2460262775421143, -1.0467467308044434, -0.38053521513938904, 0.32566654682159424, -1.0399231910705566, 0.4711892306804657, -0.48357415199279785, -0.013778994791209698, 0.3488697111606598, 0.15364138782024384, 0.4862855076789856, 0.6027453541755676, 0.4610816538333893, 0.0835396945476532, 1.6064308881759644, -0.9333215951919556, -0.8992478251457214, -1.1147496700286865, 0.5200367569923401, -0.3385910987854004, 0.22494271397590637, -0.19089147448539734, -1.5824073553085327, -0.9363284707069397, -0.5459256172180176, -0.4207753539085388, -0.27866488695144653, 0.10359442234039307, 0.7132335901260376, 0.2506413161754608, -1.3128334283828735, 0.6835073232650757, -0.6973878741264343, 0.2487218677997589, 0.7105254530906677, 0.31948497891426086, 0.8513849973678589, 0.13553139567375183, -1.3102986812591553, 0.0497058629989624, 0.0333753265440464, -0.3892088830471039, 0.044372670352458954, -0.6649537682533264, -1.1583606004714966, 0.5436956286430359, 0.25291258096694946, -0.4997957646846771, 1.39519464969635, 0.1817382574081421, -1.082013726234436, 0.353586345911026, -0.7830487489700317, -0.21149791777133942, -0.3127492070198059, -0.13963009417057037, -0.4922742545604706, -0.2719220221042633, 0.05462370812892914, 0.5952687859535217, 0.39117631316185, 0.24161511659622192, -0.4768710732460022, 0.04805940389633179, -0.43157076835632324, -0.19370079040527344, -0.35105666518211365, 1.2248305082321167, -0.5529923439025879, -0.4039803445339203, 0.4275345206260681, 0.735556423664093, -0.3531952202320099, 0.23152340948581696, -0.5890753865242004, -1.0091508626937866, 0.47797447443008423, 0.06054321303963661, 1.2066290378570557, -1.0803771018981934, -1.1135677099227905, -0.04621182382106781, -0.06572261452674866, 0.16111597418785095, -0.47627711296081543, 0.43736711144447327, -0.16229388117790222, 0.31514379382133484, 0.14286985993385315, -0.8482965230941772, 0.4103148877620697, -0.22707277536392212, -0.986526608467102, -0.49212124943733215, 0.03401976823806763, 1.3113449811935425, -0.7779271602630615, -0.2960457503795624, -0.2562269866466522, 0.47065234184265137, -1.0840959548950195, 0.9224411249160767, -0.4228712022304535, -0.18747907876968384, -0.15029698610305786, 0.04724648967385292, 0.17194336652755737, -0.353090763092041, 0.09482096135616302, -0.6737041473388672, -0.39162677526474, 0.6253111362457275, -0.23529312014579773, 1.3164578676223755, -0.2810189127922058, 0.48168206214904785, 0.10335379093885422, -0.6307573914527893, 0.36975419521331787, 0.48030391335487366, -0.4995424449443817, -0.7935160398483276, 0.40337073802948, 0.2867875397205353, -0.23141029477119446, 0.08876687288284302, 1.4107091426849365, 1.1898930072784424, -0.4704415500164032, 0.14907397329807281, 0.2290264368057251, -0.1879337579011917, 0.5811589360237122, 0.3336038291454315, 0.6507712602615356, -0.005140198860317469, 0.5133264660835266, -0.16156543791294098, 0.42476335167884827, -0.7367498278617859, 0.04701457917690277, 0.8403966426849365, 0.30291539430618286, 0.39026162028312683, 0.5411955714225769, -0.8396764993667603, -0.5285868644714355, 0.1479189395904541, 0.379482239484787, 1.531347632408142, -0.4306483864784241, -0.1327870786190033, -0.674751877784729, -0.10701609402894974, -0.4294855296611786, -0.14287863671779633, 0.17956002056598663, -0.004455099813640118, -0.41987791657447815, -0.9068841934204102, 1.4405207633972168, 0.6730054020881653, 1.0437346696853638, -0.7990671396255493, -0.32168176770210266, -0.431516170501709, 0.18068471550941467, -1.09220552444458, -0.5626645088195801, 0.5951916575431824, -0.4869595170021057, 0.15256468951702118, 0.536700427532196, -0.18808594346046448, 0.22040164470672607, -0.9035925269126892, 0.6839768290519714, -0.599702775478363, -0.25313979387283325, -0.3636256456375122, 0.5105047821998596, -0.6932135820388794, -0.7016170620918274, 0.2812262177467346, -0.06681188195943832, -0.4529321789741516, 0.8686516284942627, 0.43313151597976685, 0.0351184643805027, -0.7258394360542297, -0.2905289828777313, -0.1959289014339447, 0.2417825311422348, -0.0788627490401268, 0.9095764756202698, -0.18740326166152954, -0.14108814299106598, -1.0047662258148193, 0.9676413536071777, 0.5001336932182312, -0.7513697147369385, 0.10888192057609558, -0.6253777146339417, 0.046424031257629395, 0.9796764254570007, -0.4279008209705353, -0.12574192881584167, -0.7921981811523438, -0.02731509879231453, -0.5112264156341553, -0.1551642119884491, 0.12049312889575958, 0.4061944782733917, 0.045074403285980225, 0.20570723712444305, 0.9148473739624023, 0.20225642621517181, 0.3522498309612274, 0.491182416677475, -0.40884751081466675, 0.5323240756988525, 0.10561782121658325, 0.04331699758768082, -0.051458850502967834, 0.004820718429982662, -0.7727544903755188, -0.292589008808136, -0.011727497912943363, 0.07048497349023819, 0.17063814401626587, -0.008336533792316914, -0.5694367289543152, -1.061555027961731, -0.1579197347164154, -1.2852990627288818, -0.08402685821056366, -0.20965330302715302, -0.31452205777168274, -0.14905929565429688, -1.0015850067138672, -1.7002707719802856, -0.7495699524879456, -1.3792484998703003, -1.1577702760696411, 0.7177675366401672, 0.06482864916324615, -0.3419773578643799, -0.4373067319393158, -0.477206826210022, -0.5196542143821716, 1.22347092628479, -0.8702787756919861, 0.8274559378623962, -0.16645506024360657, -0.3836899995803833, 0.012768268585205078, -0.2937457859516144, 0.10665560513734818, -0.468772292137146, 0.4953324496746063, -0.9814876914024353, 0.6548991799354553, -0.2584092915058136, -0.20251576602458954, 0.13311420381069183, 0.5920912623405457, 1.0693646669387817, -0.13934241235256195, -0.60002601146698, 0.3647330701351166, 1.312886357307434, -0.398440420627594, 0.3036455810070038, -0.08430168032646179, 0.8321699500083923, -0.49160075187683105, -0.14110995829105377, 0.7254099249839783, 0.055826447904109955, 0.46686089038848877, 0.06236855685710907, -0.06687552481889725, -0.08071531355381012, 0.02517862245440483, 0.19469550251960754, 1.8562012910842896, 0.5478698015213013, -0.21853476762771606, -1.0442968606948853, 0.6444821357727051, -1.0649420022964478, -0.6380079388618469, 0.43469366431236267, 0.5494449734687805, 0.4657398760318756, -0.06797818094491959, -0.5689842700958252, 0.35137802362442017, 0.4519638121128082, 0.6765755414962769, -0.28966817259788513, -1.4075751304626465, 0.37284281849861145, 0.7245795130729675, 0.41261976957321167, 0.6664270162582397, -0.2935325503349304, 0.7028518319129944, 14.577516555786133, 1.2732456922531128, -0.1483219861984253, 0.31571531295776367, 0.4750020503997803, 0.6221531629562378, -0.16495008766651154, -0.29345041513442993, -1.5145716667175293, 0.007623299956321716, 1.2660086154937744, -0.042703866958618164, 0.26803264021873474, 0.26607221364974976, 0.14899395406246185, 0.2836720049381256, -0.4479758143424988, 0.5256718993186951, 0.34133729338645935, -1.3857183456420898, 0.5369054079055786, -0.03341138735413551, -0.016586435958743095, 0.4497365951538086, 0.7793809175491333, 0.6545455455780029, 0.412944495677948, -0.30614805221557617, 0.4188168942928314, 0.15999139845371246, 1.0939065217971802, 0.10435821115970612, 0.3352298438549042, 0.42903950810432434, -1.2489780187606812, -0.02846563048660755, -0.6215055584907532, -1.4379467964172363, 0.14424438774585724, 0.36382320523262024, -0.5578380227088928, -0.5762629508972168, -0.3339778184890747, 0.4336778521537781, 0.5662763118743896, 0.3907771706581116, -0.2801026999950409, 0.5318664908409119, -0.20788754522800446, -0.24683935940265656, 0.08483249694108963, 0.657677948474884, -0.01463612262159586, 0.2053270936012268, 0.02083209715783596, -0.09353554993867874, 0.11638164520263672, 0.7329301834106445, -0.4651797413825989, -0.5698390007019043, -0.10103188455104828, -0.31472527980804443, 0.26571837067604065, 0.8805838227272034, 0.02689279243350029, 0.1953449249267578, -0.38322150707244873, 0.12126271426677704, 0.6299731135368347, 0.022499237209558487, -0.301067590713501, -0.49735918641090393, 0.2961622476577759, -0.676442563533783, 0.15773023664951324, 0.5829454660415649, -0.5730326175689697, -0.3128775954246521, -0.564787745475769, -0.3560173511505127, 0.12054885923862457, -0.6385290026664734, -0.4309701919555664, 1.0647708177566528, -0.49624401330947876, -0.08510217070579529, 0.43150970339775085, -0.6837553977966309, -0.08072590827941895, 0.5090596675872803, -1.6146610975265503, -0.6662729978561401, 0.5969182848930359, -0.32545796036720276, -0.3055577576160431, 0.26818886399269104, 1.4279191493988037, 0.2634943723678589, -0.2963690161705017, 0.1225084438920021, 0.13257664442062378, 0.2978784441947937, -0.5468499660491943, -0.6619573831558228, 1.3742990493774414, 0.529424786567688, -0.15366166830062866, 0.048270586878061295, 0.10296159237623215, 0.1209459900856018, -0.8419265747070312, -0.40287044644355774, 1.0942661762237549, -0.40713924169540405, -0.13740162551403046, -0.9029832482337952, -0.8733713626861572, 0.38113242387771606, 0.42509177327156067, 0.11756449192762375, 0.13576707243919373, 0.1480698138475418, -0.42466428875923157, -0.10374881327152252, -0.6471385359764099, 0.30645209550857544, 0.32851994037628174, -0.9089532494544983, -0.21833878755569458, -0.20768199861049652, 0.4744527041912079, -1.4346046447753906, -0.5561994910240173, -0.10974454879760742, 0.15981915593147278, 0.17543035745620728, 1.156084656715393, -0.010103058069944382, 0.9990305304527283, 0.8391659259796143, 0.0971192792057991, -0.3143596649169922, 0.02135641872882843, -0.6659684777259827, -0.22720086574554443, 0.09483979642391205, 0.4538128077983856, -0.31228023767471313, 0.9376979470252991, 1.0192584991455078, -0.0895906314253807, -0.4648359715938568, -0.7730998992919922, -0.1098484992980957, -0.2214955985546112, -0.6056817770004272, 0.5487228035926819, -0.22188197076320648, 0.26926419138908386, -0.04185856878757477, 0.7801689505577087, 0.5933467149734497, -0.15118034183979034, -0.3610347509384155, -0.04196130484342575, 0.055021580308675766, 0.18320469558238983, -0.7016306519508362, -0.6488096117973328, -1.5112234354019165, -0.022351816296577454, -1.0336352586746216, 0.16282571852207184, -0.7215134501457214, -0.5742455720901489, -0.25821882486343384, -0.02137693762779236, 0.34853890538215637, 0.23813633620738983, -0.31831300258636475, -0.6865278482437134, -0.6511589884757996, -0.47978368401527405, 0.8929101824760437, 0.8674497604370117, -0.7158928513526917, 0.06784848868846893, -0.27418315410614014, 0.10770983248949051, 0.4535772204399109, 0.33371421694755554, -0.46822214126586914, -0.6728233098983765, -1.4180277585983276, 0.17678479850292206, 0.03884343057870865, -0.22196289896965027, -0.805402934551239, 1.2961009740829468, 0.37997007369995117, -0.3922686278820038, -0.0826638862490654, 0.4176476001739502, -0.9060767889022827, -0.9210840463638306, 0.746248185634613, -0.9533476233482361, 0.27986642718315125, 0.6279023885726929, -0.8597201704978943, -0.24661986529827118, 0.5426458120346069, -0.3870079815387726, -0.6038134694099426, -0.8065958023071289, 0.5544940829277039, -0.6849634051322937, 0.429337739944458, -0.619972825050354, -0.1287568360567093, -1.4114291667938232, 0.16844657063484192, 0.19018420577049255, 0.21044832468032837, -0.5826600193977356, 0.5816983580589294, 0.39126521348953247, -0.8948258757591248, 0.1756042093038559, 0.7751851081848145, -0.3078940212726593, 0.3603803217411041, 0.37759754061698914, 0.4223491847515106, -0.591073215007782, 0.7886144518852234, 0.2584637999534607, 0.05703502148389816, -1.0616205930709839, 0.12424848973751068, 0.347525954246521, -0.5445985198020935, -0.3120950758457184, 0.8746849298477173, -0.5896663069725037, -0.6676320433616638, 0.12378653138875961, -1.6931513547897339, -0.5091577768325806, -0.5940479636192322, 0.6161097288131714, 0.10298147797584534, 0.1719820648431778, 0.026297511532902718, -0.5390749573707581, -0.005703503265976906, -0.21737051010131836, -0.32789304852485657, 0.12495119869709015, 0.12544474005699158, -0.5240979790687561, 0.017083728685975075, 0.22974686324596405, -0.619545578956604, -0.38207268714904785, -0.92473965883255, -0.29833391308784485, -0.10966095328330994, 0.523970901966095, 0.015038229525089264, -0.6619906425476074, 0.8625279068946838, 0.24431046843528748, 0.6064244508743286, 0.18200552463531494, -0.47584378719329834, 0.4905805289745331, 0.5373563766479492, 0.012724252417683601, -0.5624974966049194, -0.9641976952552795, 1.5220530033111572, 0.9496899843215942, -0.6194847226142883, 0.2881535589694977, -0.5259087085723877, -0.4588027894496918, 0.7693597078323364, 0.34125369787216187, -0.05871199816465378, 0.8471749424934387, 0.6741729974746704, -0.15160883963108063, -0.002173981163650751, -1.0000789165496826, -0.3304094672203064, 0.42991194128990173, 1.0057696104049683, 0.7231932878494263, 0.06048028543591499, 0.12421729415655136, 0.9965143799781799, 0.17401792109012604, 0.3674643933773041, 0.20631881058216095, 0.5135458707809448, -0.3037824034690857, -0.08273106068372726, -0.282492995262146, 0.9016081690788269, -0.8248834609985352, -1.2948230504989624, 0.6406844854354858, 0.6672600507736206, 0.1142011508345604, 0.5471187829971313, 1.2414518594741821, 0.18840989470481873, 0.5152337551116943, 0.00303351110778749, 0.45612597465515137, -0.42338067293167114, -0.4365606904029846, -0.19090840220451355, -0.856451690196991, -0.023178966715931892, -0.12269090116024017, -0.24373748898506165, -0.6190372109413147, -0.5071905255317688, 0.3223625421524048, 0.2809165120124817, 0.32457372546195984, 0.8403488397598267, 0.8895879983901978, 0.8992782235145569, -0.6921544075012207, -0.49634796380996704, -0.39233705401420593, -0.6364011764526367, -0.07108943909406662, -0.6788180470466614, -0.09221884608268738, 0.36354389786720276, 0.052916012704372406, -0.24507471919059753]}, "authors": [{"authorId": "2122900858", "name": "Tao Yang"}, {"authorId": "2068197824", "name": "Fei Ma"}, {"authorId": "2202216946", "name": "Xiaoling Li"}, {"authorId": "1491233985", "name": "Fangxin Liu"}, {"authorId": "2109819730", "name": "Yilong Zhao"}, {"authorId": "2116778755", "name": "Zhezhi He"}, {"authorId": "2148655643", "name": "Li Jiang"}], "references": [{"paperId": "13f7a106bb3814ad1fab25fd1356e99e91f402d3", "title": "Q-ViT: Fully Differentiable Quantization for Vision Transformer"}, {"paperId": "31a6e5c57d7f6c7afe6252c500e444b5c2675f2b", "title": "BISWSRBS: A Winograd-based CNN Accelerator with a Fine-grained Regular Sparsity Pattern and Mixed Precision Quantization"}, {"paperId": "d045133e6e022684329ff944d67f91888be1bc3b", "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "82d94353c60a8e0ff7ed4eb9f0dca65ac2ccbfcb", "title": "Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization"}, {"paperId": "503503ec4395ab0e36d4f0a190772f7785649319", "title": "Towards Fully 8-bit Integer Inference for the Transformer Model"}, {"paperId": "9bb5665fe48e7122beda73a53316de9f7f243b19", "title": "APQ: Joint Search for Network Architecture, Pruning and Quantization Policy"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "dff076e78b3f333772be076c65a96e3b7a70ba92", "title": "DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration"}, {"paperId": "7b9b756ab509cb9f52dbac95e3e901d571f0784f", "title": "A Survey of the Usages of Deep Learning for Natural Language Processing"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "0e0a24ca2869e56ffe47a87d360b78b6994c8f58", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration"}, {"paperId": "132ae47905b1a648c095da54b8533e87cf642897", "title": "Fully Quantized Transformer for Machine Translation"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "7edacd94dc1509803d9bbcc1d92fea780d71cb3e", "title": "MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "9ef78ff09225149271c1216416b9bd3dee31f1c9", "title": "Hardware-oriented Approximation of Convolutional Neural Networks"}, {"paperId": "ffdaa12ef011de9dbf43be46d45a3abcc8288965", "title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e", "title": "Practical Bayesian Optimization of Machine Learning Algorithms"}, {"paperId": null, "title": "Gaussian Processes For Machine Learning"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "3364bc50921a9566d61ef8cb73baa82341725e4b", "title": "CACTI 6.0: A Tool to Model Large Caches"}]}