{"paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d", "abstract": "Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.", "venue": "arXiv.org", "year": 2023, "citationCount": 75, "influentialCitationCount": 16, "openAccessPdf": {"url": "https://arxiv.org/pdf/2305.14342", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "S Sophia, Second-order Clipped Stochastic Optimization is proposed, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner and has a run-time bound that does not depend on the condition number of the loss."}, "embedding": {"model": "specter_v2", "vector": [0.10713518410921097, 0.8691641092300415, -0.2501888573169708, -0.4935644268989563, -0.24435098469257355, 0.2993124723434448, 0.44813403487205505, -0.37290969491004944, -0.6268186569213867, -0.49336323142051697, 0.42797476053237915, 0.21771149337291718, 0.5472121834754944, 0.4192540645599365, -0.2780529260635376, 0.05863150954246521, -0.9983943700790405, 0.47000378370285034, 0.01617974042892456, -0.3778516352176666, -0.379104346036911, -0.6810318827629089, -0.4537036716938019, 0.07312595844268799, 0.3955579400062561, 0.7015131115913391, 0.25959399342536926, 1.1395395994186401, -0.3792550265789032, 0.3238442540168762, 0.5650767087936401, -0.6619987487792969, 0.39202451705932617, -0.31841665506362915, -0.288665235042572, 0.0902559831738472, 0.16630780696868896, -0.5347782373428345, -0.3889135718345642, 1.0059828758239746, -0.23882019519805908, 0.1756906509399414, 0.5097910165786743, -0.42913708090782166, -0.1790512353181839, 0.8742259740829468, 0.3974524736404419, 0.46990087628364563, -0.23314084112644196, -0.4732553958892822, 1.233076810836792, -1.5742613077163696, 0.3201521635055542, 1.7592707872390747, 0.6759652495384216, 0.44119489192962646, -0.12754309177398682, -0.2115778923034668, 1.0254877805709839, -0.20042547583580017, -0.8200991153717041, -0.21485306322574615, -0.0185856930911541, -0.13817714154720306, 1.8370860815048218, -0.4757430851459503, -0.08366148918867111, 0.6779220104217529, 0.27413704991340637, 1.6940656900405884, -0.013073275797069073, -0.21875877678394318, -0.27686166763305664, 0.40331804752349854, 0.2079247683286667, 0.7614297270774841, -0.7078232169151306, 0.2589031159877777, -0.7380460500717163, -0.06762294471263885, -0.031918201595544815, -0.2955978810787201, 0.2904545068740845, -0.053566813468933105, 0.22466984391212463, 0.6313299536705017, 0.40603336691856384, 0.5985764265060425, -0.1983567476272583, 0.9301498532295227, 0.4310501217842102, 0.17221945524215698, 0.5163912773132324, 0.10999125987291336, -0.3051064908504486, 0.1988709717988968, -1.1143237352371216, 0.15197058022022247, 0.10346123576164246, 0.6104850172996521, -0.46282443404197693, 0.2769457995891571, -0.6212669014930725, 0.27488404512405396, 1.6046463251113892, -0.06094219535589218, 0.5296108722686768, -0.33557286858558655, 0.3710857927799225, -0.8139545321464539, -0.2851264178752899, -0.8847476840019226, -0.6022630333900452, -0.47003528475761414, -0.8828292489051819, -1.070799469947815, -0.6326285004615784, 0.02923966385424137, -0.8866129517555237, 0.909765899181366, 0.013290505856275558, 0.2112097293138504, 0.17432457208633423, 0.6583082675933838, 0.435394823551178, 0.8167794346809387, 0.03704327717423439, 0.11781208217144012, 0.8367679715156555, -1.0911790132522583, -0.5425141453742981, -0.9466179013252258, 0.8247278332710266, 0.09012328088283539, 0.37951239943504333, 0.10752007365226746, -1.152693510055542, -0.5910724997520447, -0.976768434047699, -0.16657856106758118, -0.41408994793891907, 0.4898718595504761, 0.9291931986808777, 0.5574337244033813, -0.6421438455581665, 1.056142807006836, -0.42394497990608215, 0.0953468605875969, 0.42287468910217285, 0.22438067197799683, 0.22637754678726196, -0.08103420585393906, -1.4112342596054077, 0.31919556856155396, 0.23524776101112366, -0.2200477123260498, -0.06192560866475105, -1.0656208992004395, -0.784523606300354, -0.03008980304002762, 0.20807014405727386, -0.5038107633590698, 1.1519421339035034, -0.29164355993270874, -2.1307425498962402, 0.674433171749115, -0.35899990797042847, -0.338623046875, 0.8840032815933228, -0.2630285918712616, -0.4333387613296509, -0.5511152148246765, -0.6528056859970093, 0.55460125207901, 0.708791196346283, 0.08029991388320923, -0.21716822683811188, 0.077548548579216, -0.5152741074562073, 0.05790982395410538, -0.1751217544078827, 0.805837094783783, -0.522388756275177, -0.30549508333206177, 0.34447211027145386, 0.4867769777774811, -0.3287452161312103, -0.5015677809715271, -0.5132392048835754, -1.0201148986816406, 0.6518732309341431, -0.1134428009390831, 0.8023207783699036, -0.9675934910774231, -0.4293249845504761, -0.054677169770002365, -0.07173404097557068, -0.10818377137184143, -0.6601206064224243, 0.3147750198841095, -0.3102414309978485, 0.4577946364879608, -0.4517587721347809, -1.6113004684448242, 0.23190435767173767, -0.3994625508785248, -0.4944092631340027, -0.10440322756767273, 0.5935724973678589, 0.8204337358474731, -0.8037064075469971, 0.15417812764644623, -0.523879885673523, 0.6311531662940979, -1.2475389242172241, 0.8695591688156128, -0.4478216767311096, 0.2999185025691986, 0.0071253362111747265, -0.459888756275177, 0.3715158998966217, -0.04155139997601509, 0.38764020800590515, -0.15307661890983582, 0.2529045641422272, 0.5872688293457031, -0.8274313807487488, 1.1640583276748657, -0.33558496832847595, 0.412690132856369, -0.05477461218833923, -0.32787373661994934, 0.17706552147865295, 0.08065684884786606, -0.0632588341832161, -0.1793648600578308, 0.3240422010421753, 0.5993910431861877, -0.8618778586387634, 0.45087090134620667, 0.37245410680770874, 0.7278662919998169, -0.23582886159420013, 0.07896507531404495, 0.6539599299430847, -0.5424736738204956, 0.5643463730812073, 0.5176090002059937, 0.16103968024253845, 0.5162121057510376, 0.28701069951057434, -0.03849581629037857, 0.42644181847572327, -0.9685713052749634, 0.1529167890548706, 0.4374936819076538, 0.6585444808006287, 0.727357029914856, 0.5572655200958252, -0.9352571964263916, -0.21633197367191315, 0.20174480974674225, 0.8293735384941101, 1.5479563474655151, -0.5877509713172913, -0.24972033500671387, -0.8609257936477661, -0.3945962190628052, -0.14672483503818512, 0.012205511331558228, -0.2951347827911377, -0.12907558679580688, -0.5609202980995178, -1.5963828563690186, 0.6800962686538696, -0.014191255904734135, 0.9855943322181702, -0.00511507410556078, 0.1271672397851944, -0.17766693234443665, 0.5901495218276978, -1.0872893333435059, -0.9853072762489319, 0.2731587886810303, -0.6685982346534729, 0.22676919400691986, 0.0857529565691948, 0.20367039740085602, 0.25224924087524414, -1.138689398765564, 0.6645148992538452, -0.6634928584098816, -0.26097244024276733, -0.12509366869926453, 0.5241166949272156, -0.818136990070343, -0.8698709011077881, 0.34412840008735657, 0.5101052522659302, 0.4556678831577301, -0.10812923312187195, 0.39594709873199463, 0.1844128519296646, -0.1047598123550415, -0.24186356365680695, 0.1805339902639389, -0.0026589371263980865, -0.0886077806353569, 0.673336923122406, -0.22594937682151794, -0.2650078237056732, -1.3124079704284668, 0.9774521589279175, 0.3452219069004059, -0.7936272025108337, 0.0629810020327568, -0.804053544998169, -0.026185978204011917, 0.6986569166183472, -0.7768505215644836, -0.4213828146457672, -0.5167204141616821, -0.0660228580236435, -0.16568949818611145, -0.011978382244706154, 0.3518484830856323, 0.31372392177581787, 0.27374160289764404, 0.24194148182868958, 0.35391589999198914, 0.11365862190723419, -0.5005617141723633, 0.6079413294792175, -1.1533243656158447, 0.21364812552928925, 0.5035656690597534, 0.1349770724773407, -0.05390043184161186, 0.03383829444646835, -0.8437031507492065, -0.5599367022514343, -0.2759009003639221, -0.4945646822452545, -0.20201215147972107, 0.19089734554290771, -0.3434962034225464, -0.7446302771568298, 0.013926757499575615, -0.636245846748352, -0.2876345217227936, 0.2147619128227234, 0.040010906755924225, -0.44318434596061707, -1.0608155727386475, -1.4287523031234741, -0.5221899747848511, -0.8431692123413086, -1.1741689443588257, 0.45498359203338623, 0.010913211852312088, -0.030724946409463882, -0.6368771195411682, -0.007959598675370216, -0.48373597860336304, 0.9949486255645752, -0.9007993340492249, 0.42057570815086365, -0.23632271587848663, 0.1164219006896019, -0.3949163854122162, 0.5035467743873596, 0.9330215454101562, -0.4912126660346985, 0.5023828744888306, -0.57905113697052, 0.11689123511314392, -0.23071236908435822, -0.13716910779476166, 0.3949055075645447, 0.43512630462646484, 0.3479214012622833, -0.09100230038166046, -0.3364889323711395, 0.9016772508621216, 1.2973134517669678, -1.0708868503570557, -0.14546145498752594, -0.18362583220005035, 0.9498001933097839, 0.08479654788970947, -0.28408554196357727, 0.675091564655304, 0.30145275592803955, 0.3235332667827606, 0.0383896604180336, 0.0031354965176433325, -0.016737964004278183, -0.670985758304596, 0.7053776979446411, 1.5913214683532715, 0.516994297504425, -0.04913189262151718, -1.0658137798309326, 0.462862104177475, -0.9850824475288391, -0.8333116173744202, 0.4140947163105011, 0.6468220353126526, 0.18359318375587463, -0.3296162784099579, -0.09710517525672913, -0.5777087211608887, 0.06946568936109543, 0.36711522936820984, -0.546188235282898, -0.88620525598526, -0.28669071197509766, -0.03880227729678154, 0.18892113864421844, 0.8920341730117798, -0.4567723274230957, 1.0036110877990723, 14.774889945983887, 0.5843763947486877, -0.2209557741880417, 0.6388285756111145, 1.1291882991790771, -0.03603813424706459, -0.05767018720507622, -0.13573655486106873, -1.3847569227218628, 0.03800242394208908, 1.2467436790466309, 0.399840384721756, 0.5953907370567322, 0.36131903529167175, 0.5449217557907104, 0.29938286542892456, -0.454173743724823, 0.6825107336044312, 0.47317999601364136, -1.044862151145935, 0.14542287588119507, 0.14528438448905945, 0.6740247011184692, 1.066847562789917, 0.791618287563324, 0.7871844172477722, 0.6615471839904785, -0.17903780937194824, 0.37758445739746094, 0.5849224328994751, 0.4314301609992981, -0.18327417969703674, 0.29109954833984375, 0.5028793215751648, -0.7807999849319458, -0.36526790261268616, -0.47019678354263306, -1.0076358318328857, 0.41999486088752747, 0.6846089363098145, -0.7843868136405945, -0.5656465888023376, -0.04531661793589592, 0.7530040740966797, 0.07605065405368805, 0.15735739469528198, -0.08086010813713074, 0.7787179946899414, -0.7279214262962341, -0.11666064709424973, 0.4748777747154236, -0.0936150923371315, 0.518328845500946, 0.32889819145202637, 0.035250481218099594, -0.054250024259090424, 0.17749716341495514, 0.3713042736053467, -0.7767506241798401, 0.3032066226005554, 0.442686527967453, -0.3826075494289398, -0.687027096748352, 0.728527307510376, 0.6423224210739136, -0.13632018864154816, -0.2576742172241211, 0.6010124683380127, 0.7005602121353149, 0.17889979481697083, -0.35957086086273193, 0.10588464885950089, 0.541370689868927, -0.582331120967865, 0.0549345463514328, 0.2903244197368622, -0.046645887196063995, -0.6543318629264832, -0.3254108428955078, -0.42997848987579346, 0.18705007433891296, -0.3608901798725128, -0.5700096487998962, 0.646242618560791, -0.4185936450958252, 0.12864342331886292, 0.23954516649246216, -0.6139165759086609, -0.09519921988248825, 0.49348706007003784, -1.2073078155517578, -0.3308018743991852, 0.7929714918136597, -0.4979681372642517, -0.28696534037590027, 0.15030628442764282, 0.7459874749183655, 0.37797287106513977, -0.7808767557144165, 0.29903700947761536, 0.5215625762939453, -0.021818634122610092, -0.3309698700904846, -0.7352955937385559, 1.2317495346069336, 0.3495522439479828, -0.23020856082439423, -0.14595040678977966, 0.1477026492357254, 0.21053054928779602, -0.7067040801048279, -0.09533794969320297, 0.829706072807312, -0.6626078486442566, -0.22973239421844482, -0.9926455616950989, -0.6131584644317627, 0.0288248173892498, 0.17548096179962158, -0.14076775312423706, 0.16356241703033447, -0.08939765393733978, -0.5311950445175171, -0.32664403319358826, -0.578487753868103, 0.042419057339429855, 0.39345481991767883, -0.45755839347839355, 0.19990158081054688, 0.6206644773483276, 0.4128301441669464, -1.0263514518737793, -0.5250811576843262, -0.1999712735414505, 0.0460846833884716, -0.0176219642162323, 0.8571311235427856, -0.7834762334823608, 0.5063769817352295, 0.9822807908058167, -0.13238611817359924, -0.8092195987701416, 0.06945350021123886, -1.1944705247879028, 0.14390547573566437, -0.043598730117082596, 0.9156095385551453, -0.5323590636253357, 0.28700506687164307, 0.8227881193161011, 0.45186713337898254, -0.7755202651023865, -0.6139769554138184, -0.5531675219535828, 0.04603726044297218, -0.5775184035301208, 0.14120438694953918, -0.3132573068141937, -0.0900559350848198, -0.09009646624326706, 0.040367741137742996, 0.7631615400314331, -0.34076374769210815, -1.073865532875061, 0.4987379014492035, -0.05857224762439728, -0.3202795684337616, -0.7840319275856018, -0.5351709723472595, -1.7187303304672241, -0.024232052266597748, -1.1594481468200684, 0.028473133221268654, -0.8535844683647156, -0.4342936873435974, 0.3452425003051758, -0.2518993616104126, -0.1496390551328659, -0.004266233649104834, -0.4919508397579193, -0.00023900112137198448, -0.4386068284511566, -0.2899784743785858, 1.034757375717163, 0.8508160710334778, -0.9013852477073669, -0.30362677574157715, 0.3416554927825928, 0.327324241399765, 0.35387396812438965, 0.5915826559066772, -0.6052712202072144, -0.7310900688171387, -1.324753761291504, 0.6567012667655945, -0.33573096990585327, -0.24233044683933258, -0.9057912230491638, 0.6145130395889282, 0.43576061725616455, -0.34494516253471375, 0.20318545401096344, 0.48032763600349426, -0.569861650466919, -0.484480082988739, 0.41503119468688965, -0.7853003144264221, 0.2272908240556717, -0.20482195913791656, -0.5413379669189453, -0.2096126675605774, 0.6805104613304138, 0.19070877134799957, -0.8619710803031921, -0.3902462124824524, 0.8752708435058594, -0.530592679977417, 0.207266703248024, -0.5254060626029968, 0.02701733447611332, -0.5400965809822083, -0.3311859965324402, -0.39802441000938416, 0.32390105724334717, -0.5126107335090637, 0.9381881952285767, 0.4186798632144928, -0.9447699189186096, -0.33257442712783813, 0.3039023280143738, -0.3849828243255615, -0.1826992928981781, 0.6427070498466492, 0.9003354907035828, -0.3350536823272705, 0.4574279487133026, 0.777601957321167, 0.13686352968215942, -0.9004635214805603, -0.10326921194791794, 1.0602432489395142, -0.9077336192131042, -0.31536200642585754, 1.6946134567260742, -0.22124037146568298, -1.432411789894104, 0.1774231493473053, -1.2379150390625, -0.13031262159347534, -0.26728665828704834, 0.6094240546226501, 0.1654532104730606, 0.004395946394652128, -0.2515694499015808, -0.5623019933700562, 0.10004094243049622, -0.2637665271759033, -0.7752748131752014, 0.6322756409645081, -0.35617783665657043, -0.38105809688568115, 0.6338945031166077, 1.3256810903549194, -0.7764068841934204, -1.0357316732406616, -0.8050995469093323, -0.3030240535736084, 0.15676501393318176, 0.4599315822124481, -0.26777005195617676, -0.7173213958740234, 0.8516668081283569, 0.7314774394035339, -0.1864384412765503, 0.10897254943847656, -0.42392808198928833, -0.050056979060173035, 0.7546171545982361, -0.064590223133564, -1.1394270658493042, -0.6041153073310852, 1.4112995862960815, 1.1937720775604248, -0.9883419275283813, 0.5101094245910645, -0.188354030251503, -1.05270516872406, 0.7575530409812927, -0.12131001800298691, -0.26057785749435425, 1.2470386028289795, -0.42405927181243896, 0.1566222757101059, 0.3087780475616455, -0.9969749450683594, -0.2570419907569885, 1.2065961360931396, 0.6809121370315552, 0.7682807445526123, 0.3076409101486206, 0.16975528001785278, 0.8263498544692993, -0.0592365488409996, -0.2914223074913025, 0.4245540201663971, 0.23402881622314453, -0.13726532459259033, 0.2081012725830078, 0.10083648562431335, 0.6160024404525757, -0.8776702880859375, -0.6385257840156555, -0.08206158131361008, 0.28419890999794006, 0.019244402647018433, 0.6893841028213501, 0.3852143883705139, 0.12153876572847366, 0.43633249402046204, 0.0638490840792656, 0.13218297064304352, -0.2938259541988373, -0.36776551604270935, -0.08845357596874237, -0.7666571140289307, -0.5905587077140808, 0.16850122809410095, -0.5073988437652588, -0.5008494853973389, -0.047391992062330246, 0.5790390372276306, -0.2148313671350479, 0.7064298987388611, 1.1661490201950073, 0.7895569205284119, 0.5004827380180359, -0.07982508838176727, -0.7359273433685303, -0.6542637944221497, -0.9955111742019653, -0.03238651156425476, -0.4195306897163391, -0.24124076962471008, 0.24591045081615448, -0.13798585534095764, -0.47626158595085144]}, "authors": [{"authorId": "2118903632", "name": "Hong Liu"}, {"authorId": "46947755", "name": "Zhiyuan Li"}, {"authorId": "145385471", "name": "David Leo Wright Hall"}, {"authorId": "145419642", "name": "Percy Liang"}, {"authorId": "2114186424", "name": "Tengyu Ma"}], "references": [{"paperId": "592e2a4c8bb3e72b1f6d671d6642907fa81b1782", "title": "Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458", "title": "Symbolic Discovery of Optimization Algorithms"}, {"paperId": "5b064570a83231471072b9c03bc068077cefb868", "title": "Robustness to Unbounded Smoothness of Generalized SignSGD"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "31f79d616785e4ce096953d6524b1032031cc82f", "title": "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information"}, {"paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9", "title": "How to Train BERT with an Academic Budget"}, {"paperId": "92c6e4c5d72a39c2d89faf21ff8e9089c0ab2032", "title": "Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "0aa575ebb8cfe9a2b1e71f0d4219fde7b9907132", "title": "A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to Regularization"}, {"paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4", "title": "Why are Adaptive Methods Good for Attention Models?"}, {"paperId": "1e04ca1998c04040c9c10685fc0daa4ecc13855b", "title": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients"}, {"paperId": "6b4dbafb62cbd516b11a74c5637e77a7a9b59bb0", "title": "On the Promise of the Stochastic Generalized Gauss-Newton Method for Training DNNs"}, {"paperId": "20438e2a38a0c4723fbd9de50b44b7335f6f43cb", "title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "0d37c762336ce69801c7fda5eb140d716ece0859", "title": "The Implicit and Explicit Regularization Effects of Dropout"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6e5d89c2b3b5ead2c3ab389534de62a28c1e8e6e", "title": "PyHessian: Neural Networks Through the Lens of the Hessian"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "36afb8fbf8a2d6712a9533984e853e3804243a9a", "title": "Limitations of the Empirical Fisher Approximation"}, {"paperId": "e658741baefd2c4da4742bb43155f69d4cbd79fa", "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "409b9abbeecbc91d00187e73d441f556c8c6b598", "title": "An Investigation into Neural Net Optimization via Hessian Eigenvalue Density"}, {"paperId": "e495612653904beca446fb313cd315536793df4c", "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "0679950558d72791f16031dd08c39367d8dd47b8", "title": "Shampoo: Preconditioned Stochastic Tensor Optimization"}, {"paperId": "c983653841b6987d9959318f074a595783838576", "title": "On the Convergence of Adam and Beyond"}, {"paperId": "42d74488ea64d5420257f547f6d0a7a2909d87c0", "title": "Kronecker-factored Curvature Approximations for Recurrent Neural Networks"}, {"paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54", "title": "signSGD: compressed optimisation for non-convex problems"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "58c6f890a1ae372958b7decf56132fe258152722", "title": "Regularizing and Optimizing LSTM Language Models"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "aaed2a884af95852580fdedda4ea768f2effeb46", "title": "Practical Gauss-Newton Optimisation for Deep Learning"}, {"paperId": "e4da4fb310df2bfa5b9aab217982723634bda4bc", "title": "Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients"}, {"paperId": "0c7a93498360ce6c0ff1a7c37fd9b6aa0c054f85", "title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations"}, {"paperId": "8151fbde700614ab25d6165b9ce5f76456c180d4", "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "d44efdc542f2cc5e196f04bc76bc783bfd7084af", "title": "Incorporating Nesterov Momentum into Adam"}, {"paperId": "573277bf56dd1cd73f0bf27115f9a6a974c25358", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "189ef6bea81fe8ee3ed3214b9394f7e2027eff98", "title": "New Insights and Perspectives on the Natural Gradient Method"}, {"paperId": "ef098e0154da4b210a6ee11b84ca30bd3e445ac6", "title": "Iterative solution of nonlinear equations in several variables"}, {"paperId": "6daf97525dd40fa7f207d17bb1c48e5c59239c8d", "title": "Improved Bounds on Sample Size for Implicit Matrix Trace Estimators"}, {"paperId": "e8f95ccfd13689f672c39dca3eccf1c484533bcc", "title": "Revisiting Natural Gradient for Deep Networks"}, {"paperId": "e5a685f40338f9c2f3e68e142efa217aad16dd56", "title": "No more pesky learning rates"}, {"paperId": "a8921166af9d21cdb8886ddb9a80c703abe3dde5", "title": "Hessian Matrix vs. Gauss-Newton Hessian Matrix"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "4c46347fbc272b21468efe3d9af34b4b2bad6684", "title": "Deep learning via Hessian-free optimization"}, {"paperId": "0b14178e7d79ac426d0a39700e1ac8b2c6f2e752", "title": "Convex Optimization"}, {"paperId": "91a8c8058e41dae7863c30386638bdfbe77ab6b7", "title": "Cubic regularization of Newton method and its global performance"}, {"paperId": "ffa94bba647817fa5e8f8d3250fc977435b5ca76", "title": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent"}, {"paperId": "e1053197256c6c3c0631377ec23a3f7dc1cb4781", "title": "Numerical methods for unconstrained optimization and nonlinear equations"}, {"paperId": "6583e799ce560a21d5fc8bf726a9fa4accbf036b", "title": "The Convergence of a Class of Double-rank Minimization Algorithms 1. General Considerations"}, {"paperId": "a476d242350d5b56f0787ece2dcab4644c6e9ad6", "title": "APPROXIMATE CONFIDENCE INTERVALS"}, {"paperId": "b504620e0313c8fd0968fcdc66811182d1b34df2", "title": "Eva: Practical Second-order Optimization with Kronecker-vectorized Approximation"}, {"paperId": null, "title": "sizes. Generally, larger models use smaller peak learning rate. For Lion, we use"}, {"paperId": null, "title": "Neural Network Training Dynamics. 2022"}, {"paperId": null, "title": "Mistral \u2013 a journey towards reproducible language model training"}, {"paperId": null, "title": "These models are trained on a TPU"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Memory efficient adaptive optimization"}, {"paperId": null, "title": "pre-training. GPT-2 models are trained on OpenWebText (Gokaslan"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": "00a949fb5dc504b5114b26ed6e0eda4d5d76fd69", "title": "TRUST REGION METHODS"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent"}, {"paperId": "b0ced8ba22674b3b948c22deb0f43df93c82f87f", "title": "Improved Preconditioner for Hessian Free Optimization"}, {"paperId": "02597de11d6b808ed0a4019f411f9ac7a9d426cb", "title": "RPROP - A Fast Adaptive Learning Algorithm"}, {"paperId": "5537987153925c5968038dc3ed8e195a72c99d5f", "title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines"}, {"paperId": "589d377b23e2bdae7ad161b36a5d6613bcfccdde", "title": "Improving the convergence of back-propagation learning with second-order methods"}, {"paperId": null, "title": "psychological crime thriller film"}]}