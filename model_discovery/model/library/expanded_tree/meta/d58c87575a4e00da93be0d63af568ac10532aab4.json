{"paperId": "d58c87575a4e00da93be0d63af568ac10532aab4", "abstract": "Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.06744", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task."}, "embedding": {"model": "specter_v2", "vector": [0.16839224100112915, 1.151766300201416, 0.20501476526260376, 0.0959133505821228, -0.6718137860298157, -0.19207318127155304, 0.8774062395095825, -0.23311875760555267, -0.19676543772220612, -0.13325276970863342, 0.45467016100883484, -0.5385944247245789, -0.048739057034254074, -0.2828773260116577, -0.0641166940331459, -0.2073703110218048, -0.5412513017654419, 0.8280907273292542, -0.24360890686511993, -0.38686731457710266, -0.43543219566345215, -1.1254048347473145, -0.7726795077323914, -0.41049066185951233, 0.5436345338821411, 0.5457825064659119, 0.2384745180606842, 0.809226930141449, -0.7796555161476135, 0.18776486814022064, 0.20713387429714203, -0.8624105453491211, 0.26091912388801575, -0.3705964684486389, -0.3581499457359314, 0.08953443169593811, 0.03726411238312721, -0.5612313747406006, -0.4795777499675751, 0.6910881996154785, 0.1344219148159027, 0.05764147639274597, 0.7475467920303345, -0.8530555367469788, -0.7917640805244446, 1.2884758710861206, 0.6035875082015991, 0.6676156520843506, -0.15697994828224182, -0.5780117511749268, 1.3412327766418457, -1.0353329181671143, 0.21076570451259613, 1.926063060760498, 0.12992681562900543, 0.6466028690338135, -0.5266786813735962, -0.7404090166091919, 0.6862142086029053, 0.21200300753116608, -0.903300404548645, 0.3436039388179779, -0.012475456111133099, -0.16002874076366425, 1.5645545721054077, -0.44559308886528015, 0.41948339343070984, 0.7672722935676575, -0.38901764154434204, 1.4681931734085083, 0.022880760952830315, -0.8373875617980957, -0.2826738655567169, -0.08205494284629822, -0.18528611958026886, 1.0126383304595947, -0.30481278896331787, 0.3521236181259155, -0.7330871224403381, -0.04264185577630997, 0.5359072089195251, -0.46448007225990295, 0.2100672423839569, -0.16481605172157288, 0.0548132099211216, 0.9571275115013123, 0.22686776518821716, 0.6773234009742737, 0.11516599357128143, 0.9491603970527649, 0.14387951791286469, 0.1625213772058487, 0.06303902715444565, 0.17994922399520874, 0.013123520649969578, 0.1389448195695877, -1.0225038528442383, 0.33569711446762085, -0.07956306636333466, 0.5893853306770325, -0.0831632912158966, 0.8156653642654419, -0.9831398725509644, 0.22899004817008972, 1.743679404258728, 0.294081449508667, 0.9123708605766296, -0.6755002737045288, 0.36942747235298157, -0.5669406056404114, -0.03368569537997246, -0.5476925373077393, 0.13199879229068756, -0.02984531968832016, -0.5639029145240784, -1.5401493310928345, -0.4730391204357147, 0.12014846503734589, -1.4602484703063965, 0.9607139229774475, -0.0674903392791748, 0.06847360730171204, 0.006178399547934532, 0.3233465254306793, 0.45923295617103577, 0.5969025492668152, 0.5569028854370117, 0.2462470829486847, 0.8119052648544312, -0.2859303057193756, -0.7572417855262756, -0.9395818114280701, 0.6195815801620483, -0.021560179069638252, 0.07779013365507126, -0.08318974822759628, -1.455893874168396, -0.9664888978004456, -0.8110868334770203, -0.16159342229366302, -0.49294334650039673, 0.07643188536167145, 1.1191847324371338, 1.1580333709716797, -0.871394157409668, 0.3776875138282776, -0.3429957330226898, -0.09566809237003326, 0.6366248726844788, 0.1636366993188858, 0.5432063341140747, -0.5849695801734924, -1.615192174911499, 0.30944985151290894, 0.5194426774978638, -0.7434995770454407, -0.40248557925224304, -0.7040843963623047, -1.183333158493042, 0.052873026579618454, 0.26203471422195435, -0.8547391891479492, 1.5477648973464966, 0.3502901792526245, -1.311662197113037, 0.42952340841293335, -0.25396212935447693, 0.03327873721718788, 0.45040449500083923, 0.0750412568449974, -0.39888083934783936, -0.6375131607055664, 0.04095364734530449, 0.7387298941612244, 0.4632844626903534, -0.22112879157066345, -0.04168621450662613, 0.42655470967292786, -0.3603394031524658, -0.18322165310382843, -0.3010351359844208, 0.4162350594997406, -0.2995205819606781, -0.49281710386276245, 0.5543426871299744, 0.6675873398780823, 0.07413043826818466, -0.3560957908630371, -0.7644820213317871, -1.355338454246521, 0.46908193826675415, 0.012273511849343777, 0.735588014125824, -0.6824371218681335, -0.7814420461654663, 0.14959993958473206, 0.1753818243741989, -0.0929972231388092, -0.5880244374275208, 0.7999794483184814, -0.47045445442199707, 0.5066267251968384, -0.1189216822385788, -1.0903290510177612, 1.0321500301361084, 0.16325846314430237, -0.8290664553642273, -0.6514261364936829, 0.2187974900007248, 1.3860783576965332, -0.9471316337585449, -0.2814165949821472, 0.22322218120098114, 0.3446778655052185, -0.8236513733863831, 1.1029130220413208, -0.46392086148262024, 0.16086451709270477, -0.42940646409988403, -0.36251649260520935, 0.14156243205070496, -0.14403636753559113, 0.31254813075065613, -0.11785352230072021, 0.18223965167999268, 0.6237179636955261, -0.6980108618736267, 1.5517723560333252, -0.18673604726791382, 0.28818944096565247, -0.805296003818512, -0.7841669917106628, 0.5964372754096985, 0.33139926195144653, -0.13729211688041687, -0.5414138436317444, 0.34374451637268066, 0.42441675066947937, -0.22088003158569336, 0.25499624013900757, 0.6184110641479492, 0.1730852723121643, -0.5383622050285339, 0.4423312246799469, 0.912849485874176, -0.7912587523460388, 1.061698317527771, 0.6730527877807617, 0.6846770644187927, 0.33220750093460083, 0.19113542139530182, 0.2544664442539215, 0.047022975981235504, -0.8820884227752686, -0.2517412602901459, 0.5743081569671631, 0.8514258861541748, 0.8336914777755737, 0.5218691229820251, -0.39827466011047363, -0.2834259569644928, -0.48504915833473206, 0.679658055305481, 1.2977737188339233, -0.4286700189113617, -0.42478129267692566, -0.6618650555610657, -0.11461897939443588, -0.19586975872516632, 0.5353219509124756, -0.5119330286979675, -0.3902273178100586, -0.15262426435947418, -1.1048204898834229, 0.887628972530365, 0.4198521375656128, 1.008643388748169, -0.1493562012910843, 0.12157277017831802, 0.005954337771981955, 0.06810867041349411, -0.5412299633026123, -0.13091200590133667, 0.26915621757507324, -0.010385728441178799, -0.1335354894399643, -0.17511852085590363, 0.1896119862794876, 0.11089896410703659, -0.9637734293937683, 0.9629915356636047, -0.6748865246772766, -0.3793254494667053, -0.11482500284910202, 0.5417867302894592, -0.7549601793289185, -0.8022700548171997, 0.2316780686378479, -0.07522592693567276, -0.4218951463699341, 0.3500564992427826, 0.5444619059562683, -0.02029045857489109, -0.059484753757715225, -0.6338359117507935, 0.34627285599708557, -0.12484430521726608, 0.14448542892932892, 0.354920893907547, -0.23524053394794464, -0.10579496622085571, -0.771511971950531, 1.1116923093795776, 0.29234611988067627, -0.4067768454551697, 0.23848775029182434, -0.7127301096916199, -0.09673193097114563, 0.08676574379205704, -0.47203710675239563, -0.5034175515174866, -1.1911557912826538, 0.01615050807595253, -0.06220869719982147, -0.24320088326931, 0.19354356825351715, 0.30817416310310364, 0.30094966292381287, -0.01623314991593361, 0.48257991671562195, -0.19629709422588348, 0.13331730663776398, 1.0463500022888184, -0.9495629668235779, 0.19038988649845123, -0.175850972533226, 0.4473032057285309, -0.6111403703689575, -0.06155866011977196, -0.26003575325012207, -0.45617642998695374, 0.3459590971469879, -0.03825055807828903, -0.25704431533813477, 0.4001956880092621, -0.5322951078414917, -0.8730814456939697, -0.03114893101155758, -0.6618680953979492, -0.17258071899414062, -0.4174835681915283, -0.39893418550491333, -0.0987468883395195, -1.0226364135742188, -0.9895853996276855, -0.5678147673606873, -0.2273186445236206, -1.0197542905807495, 0.5332743525505066, 0.20750242471694946, -0.5228167176246643, -0.8978858590126038, 0.11606887727975845, -0.07106612622737885, 0.7771478891372681, -0.7341654896736145, 1.0636969804763794, 0.0492735281586647, -0.3774830996990204, -0.4066474437713623, 0.20921607315540314, 0.26427313685417175, -0.22237268090248108, 0.24315641820430756, -0.655414342880249, 0.2791619300842285, -0.7084840536117554, -0.8352097868919373, 0.11514316499233246, 0.30455875396728516, 0.7386888861656189, 0.14758434891700745, -0.49399539828300476, 0.1894216686487198, 1.2693380117416382, -0.607563853263855, 0.02513432875275612, -0.028428012505173683, 1.0791022777557373, -0.17289631068706512, -0.4005141258239746, 0.5664346814155579, 0.807915449142456, 0.37097737193107605, 0.03108333982527256, -0.3292329013347626, -0.13621678948402405, -0.9457855820655823, 0.06284596771001816, 1.3858661651611328, 0.3759305477142334, -0.5307766199111938, -1.3124299049377441, 0.7304612994194031, -0.9421098828315735, -0.3358929455280304, 0.5285531878471375, 0.3818843364715576, 0.45627203583717346, -0.46873849630355835, -0.7425074577331543, 0.3516536056995392, 0.22866524755954742, 0.25284233689308167, -0.16781695187091827, -0.7465783357620239, 0.017277345061302185, 0.7963933944702148, 0.03560682013630867, 0.6832370758056641, 0.0904223844408989, 0.7097366452217102, 14.728764533996582, 1.2037217617034912, 0.29760777950286865, 0.6475780010223389, 0.6331473588943481, 0.27955862879753113, -0.7571926712989807, -0.13054229319095612, -1.1369445323944092, -0.0922146588563919, 0.7883517742156982, 0.06747915595769882, 0.3055383563041687, -0.13312242925167084, -0.20216332376003265, 0.003325509140267968, -0.1599525660276413, 0.5417004823684692, 0.5060460567474365, -1.0159410238265991, 0.611718475818634, 0.45392170548439026, 0.6568895578384399, 0.16082985699176788, 0.9973214864730835, 1.1603001356124878, 0.9543748497962952, -0.43138545751571655, 0.6068457365036011, 0.2814755141735077, 0.7192316651344299, -0.06838428229093552, 0.22196437418460846, 0.3866613805294037, -0.7520989775657654, -0.23161180317401886, -0.4914781153202057, -0.8913226127624512, 0.4156566262245178, 0.1315501183271408, -0.6740027666091919, -0.33242589235305786, -0.37420448660850525, 0.32136985659599304, 0.14447779953479767, -0.03633907437324524, -0.5854683518409729, 0.960347056388855, -0.22124774754047394, 0.20026737451553345, 0.47657841444015503, 0.3416132628917694, -0.19476079940795898, -0.4564219117164612, 0.565908670425415, -0.08493785560131073, 0.1635851413011551, 0.875344455242157, -0.38288894295692444, -0.5111042261123657, -0.06770266592502594, -0.2614577114582062, -0.1142912283539772, 0.6428965330123901, 0.42128339409828186, 0.131158709526062, -0.17455455660820007, 0.16590313613414764, 0.5552942752838135, 0.2314244508743286, -0.46354252099990845, 0.2905898988246918, 0.22554323077201843, -0.5942156314849854, 0.26808351278305054, 0.33372312784194946, -0.0032660197466611862, -0.16884084045886993, -1.138576865196228, -0.0898120254278183, 0.25329717993736267, -1.174536108970642, -0.9204028248786926, 0.7020288109779358, -0.17685693502426147, 0.02330978587269783, -0.3389120101928711, -0.5179408192634583, -0.0330544151365757, 0.7838070392608643, -1.4868508577346802, -0.870515763759613, 0.3315524160861969, -0.3606824576854706, -0.5741937160491943, -0.1168217658996582, 1.406085729598999, -0.09803517907857895, -0.4132063388824463, 0.09140072762966156, 0.10393030941486359, 0.024355117231607437, -0.474931925535202, -1.2443162202835083, 1.2148185968399048, 0.0531640388071537, 0.2214752584695816, 0.10251615941524506, 0.019221490249037743, 0.0784253478050232, -0.5414099097251892, 0.12462050467729568, 0.9168417453765869, -0.7161505818367004, -0.21726064383983612, -0.9265844821929932, -0.541345477104187, 0.254863977432251, 0.5686861872673035, -0.4275612235069275, 0.6677600145339966, 0.13975310325622559, -0.40060749650001526, 0.10508957505226135, -0.7487010955810547, 0.2397364377975464, 0.27323049306869507, -0.3746607303619385, -0.32836803793907166, 0.0908021405339241, 0.05379938706755638, -1.212912678718567, -0.5499508380889893, -0.3979586064815521, -0.15017317235469818, 0.00016611696628388017, 1.1733195781707764, -0.24787619709968567, 0.6989114880561829, 1.0529305934906006, 0.05244923010468483, -1.0286377668380737, -0.4071325659751892, -0.7829796671867371, 0.0025718293618410826, 0.6580737233161926, 0.534731924533844, -0.3700248897075653, 0.3866848647594452, 0.9880767464637756, 0.2936253547668457, -0.3869301974773407, -0.8084511160850525, -0.08882827311754227, 0.25611433386802673, -0.7477019429206848, 0.6846070885658264, -0.006361795123666525, -0.3382529020309448, 0.1373603492975235, 0.4753519296646118, 0.3838422894477844, -0.351814329624176, -0.9283022880554199, 0.28261569142341614, 0.10253110527992249, -0.0741579532623291, -0.5815814137458801, -0.7449876070022583, -1.5506011247634888, 0.010501042939722538, -1.229689598083496, -0.23425504565238953, -1.6767456531524658, -0.5724137425422668, -0.1636468768119812, -0.09268885850906372, 0.3688119053840637, 0.15061989426612854, 0.00934555009007454, -0.361172616481781, -0.9307052493095398, -0.09584151208400726, 1.135227084159851, 0.9744503498077393, -0.9085309505462646, 0.5767826437950134, -0.3317933976650238, -0.19752852618694305, -0.07060158997774124, 0.2600034475326538, -0.44617968797683716, -1.1807249784469604, -1.4414056539535522, 0.40882283449172974, -0.2208065688610077, -0.001322460826486349, -0.7689225673675537, 0.38262322545051575, 0.502848744392395, 0.06080713868141174, 0.0523466020822525, 0.6652882099151611, -0.492468923330307, -0.4908859133720398, 0.2630203664302826, -0.6193514466285706, -0.1920153796672821, 0.07003701478242874, -0.6192604899406433, -0.16322481632232666, 0.5665410757064819, -0.3038671314716339, -1.0256917476654053, -0.3281797468662262, 0.4236803948879242, -0.7270657420158386, 0.33599400520324707, -0.47994494438171387, -0.6096196174621582, -0.8807584047317505, -0.3680320382118225, -0.061973221600055695, 0.24477489292621613, -0.7722791433334351, 0.7683108448982239, 0.4041280448436737, -1.2204896211624146, -0.01999364234507084, 0.5972471833229065, -0.1538105010986328, -0.14484795928001404, 0.4886879622936249, 0.0022873429115861654, 0.06327641755342484, 0.7557278871536255, 0.37997952103614807, 0.36717480421066284, -0.5151470899581909, -0.31753474473953247, 0.7451329827308655, -0.5715165734291077, 0.06108401343226433, 1.4729955196380615, -0.4832388758659363, -1.0606061220169067, -0.10782234370708466, -1.4011423587799072, -0.3493555784225464, -0.4678572118282318, 0.3266175389289856, 0.03897085785865784, 0.024913860484957695, 0.15737272799015045, -0.012310902588069439, 0.3529195785522461, 0.03838115930557251, -0.8515852093696594, 0.19489921629428864, -0.17444366216659546, -0.18312454223632812, 0.5773847103118896, 0.4376699924468994, -1.1047906875610352, -0.7327361106872559, -0.48724567890167236, -0.5368189811706543, -0.4343239665031433, 0.4135154187679291, -0.6095677614212036, -0.6518442630767822, 0.869828999042511, 0.5999599695205688, 0.7645156979560852, 0.06268429011106491, -0.11612638831138611, 0.2967677116394043, 0.35834965109825134, -0.018358824774622917, -0.4313235580921173, -0.5595617890357971, 1.3286619186401367, 0.9367997050285339, -0.8278887867927551, -0.08983983099460602, -0.23704834282398224, -0.7998445630073547, 0.5776062607765198, 0.2643645107746124, 0.30054253339767456, 0.5464749932289124, -0.24757465720176697, 0.15764577686786652, -0.01111938338726759, -0.9507031440734863, -0.7198443412780762, 0.9058443903923035, 1.4568533897399902, 0.9215206503868103, 0.2672197222709656, 0.21223241090774536, 0.9854516386985779, -0.363761305809021, 0.06570735573768616, 0.13148970901966095, 0.48961716890335083, -0.08251506835222244, -0.5300222039222717, 0.024906739592552185, 0.7874729037284851, -0.26017874479293823, -0.7742556929588318, -0.010698406957089901, 0.5294256806373596, 0.7555777430534363, 0.772603452205658, 0.7474756836891174, 0.25568273663520813, 0.48005837202072144, 0.3869249224662781, 1.0411994457244873, -0.3374251425266266, 0.043094128370285034, -0.06763263046741486, -0.686215341091156, 0.0999317541718483, -0.033555347472429276, -0.4099889099597931, -0.5499030351638794, -0.27777305245399475, 0.17124712467193604, 0.05397891253232956, 0.5641568899154663, 1.3710392713546753, 0.21519778668880463, 0.900796115398407, -0.48269689083099365, -0.1357715129852295, -0.2184440642595291, -1.089725136756897, -0.1661083996295929, -0.5241492390632629, -0.47361165285110474, -0.5301064252853394, 0.18050239980220795, -0.23524820804595947]}, "authors": [{"authorId": "2141320070", "name": "Minsoo Kim"}, {"authorId": "2144376191", "name": "Sihwa Lee"}, {"authorId": "40585524", "name": "Janghwan Lee"}, {"authorId": "2158125346", "name": "S. Hong"}, {"authorId": "2180828053", "name": "Duhyeuk Chang"}, {"authorId": "66936521", "name": "Wonyong Sung"}, {"authorId": "2506452", "name": "Jungwook Choi"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "a60469b864bcf770ef940a512a000a791677b075", "title": "Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers"}, {"paperId": "31b81d8cff7551537a12a1a133c100f131baaa11", "title": "Understanding Int4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "f7655ff2b53e4c87645d85297f994d33e7e4a92b", "title": "Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "3f6243097a58e386aea1215fed4f372dee07a100", "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "64a8a7eb8360f4003af85c2c8a3bf245fb2f94ae", "title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cf36236015c9f93f15bfafbf282f69e08bdc9c16", "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "2a0ae7182b13789056e13dc1887904c923a92675", "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b79e5e4622a95417deec313cd543617b19611bea", "title": "Deep Learning using Rectified Linear Units (ReLU)"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "49e60f82d6ae835c56473464f67ca5c11d3e95ec", "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d418295cd3027c43eccc5592ae5b8303ba8192be", "title": "Trained Ternary Quantization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": "8264257f573696fc0a1ef7531c825041832197f8", "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"}, {"paperId": "81051b830a4f5606106765902a51ba281c9230f9", "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"}, {"paperId": null, "title": "Scaling instruction-finetuned language models, 2022"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "A framework for few-shot language model evaluation, September 2021"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}]}