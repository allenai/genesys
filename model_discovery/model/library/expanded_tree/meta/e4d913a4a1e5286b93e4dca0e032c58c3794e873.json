{"paperId": "e4d913a4a1e5286b93e4dca0e032c58c3794e873", "abstract": "Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel method is proposed, AB-LoRA, that can effectively estimate the importance score of each LoRA rank and gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks."}, "embedding": {"model": "specter_v2", "vector": [0.14469875395298004, 0.5042393207550049, -0.2756352722644806, 0.00933156069368124, -0.3778735101222992, -0.24943706393241882, 0.7533379793167114, -0.7068399786949158, -0.4246566593647003, -0.26620304584503174, 0.7686707377433777, 0.1724713295698166, 0.007832014933228493, 0.14662274718284607, -0.059846293181180954, 0.08499712496995926, -1.0154458284378052, 0.8483503460884094, 0.04750583693385124, -0.8229146599769592, -0.656467080116272, -0.545809805393219, -0.707213819026947, 0.1268904209136963, 0.40030544996261597, 0.5905482769012451, 0.49231305718421936, 0.7152010798454285, -0.5754780769348145, -0.04257962480187416, 0.4932976961135864, -0.4195895791053772, 0.4769745171070099, -0.12878212332725525, 0.028943784534931183, 0.01642431691288948, 0.03451741486787796, -0.5738764405250549, -0.12780004739761353, 0.7993414998054504, -0.12332562357187271, 0.15941424667835236, 0.4042574167251587, -0.4920863211154938, 0.05928109213709831, 0.82158362865448, 0.5021451711654663, 0.7510966658592224, -0.6245574355125427, -0.7644640207290649, 1.333654522895813, -1.538940668106079, -0.2260354459285736, 1.5914161205291748, 0.49357739090919495, 0.5449629426002502, -0.2620755434036255, -0.8771823644638062, 1.0593616962432861, -0.10037823766469955, -0.9546407461166382, -0.7056315541267395, 0.088449627161026, -0.03516804426908493, 1.942760705947876, -0.5116420984268188, -0.0845692977309227, 0.4442019760608673, 0.08845215290784836, 1.0235949754714966, -0.369182825088501, -0.7476562261581421, -0.3229248821735382, -0.04181463643908501, -0.38521450757980347, 0.5705468058586121, -0.5897675156593323, 0.1789206713438034, -1.0975728034973145, -0.395138144493103, 0.4108130931854248, -0.5214787125587463, 0.09129797667264938, -0.3075724244117737, 0.0029374973382800817, 0.6957735419273376, 0.1725115031003952, 0.6365179419517517, -0.2749744951725006, 0.715957522392273, 0.5462112426757812, 0.5513836145401001, 0.4033898115158081, 0.4472409188747406, -0.5864361524581909, 0.530036985874176, -0.6922951936721802, -0.03404567763209343, 0.1329909861087799, 0.7639806270599365, -0.3050566613674164, 0.1344442367553711, -0.8865212798118591, 0.918465256690979, 1.3977359533309937, -0.10825604200363159, 0.7128884792327881, -0.5212593674659729, 0.2751246392726898, -0.9507614374160767, 0.042845647782087326, -0.12186407297849655, -0.25203123688697815, -0.5522814989089966, -0.8930390477180481, -1.2492179870605469, -0.591714084148407, 0.6092575788497925, -0.5832918286323547, 1.2185883522033691, -0.33817896246910095, -0.2017327845096588, -0.41686445474624634, 0.3356545567512512, -0.09283794462680817, 0.41982075572013855, 0.5376696586608887, -0.38967710733413696, 0.9657993316650391, -1.3091099262237549, -0.7656747698783875, -1.2175393104553223, 0.3532513678073883, -0.5993618965148926, 0.6345546245574951, 0.05066459998488426, -1.1943442821502686, -0.5888721942901611, -0.8836998343467712, 0.2577059268951416, -0.2518783509731293, 0.4550424814224243, 0.9635235071182251, 0.2178577333688736, -1.0203334093093872, 0.46166184544563293, -0.04859466105699539, -0.09839025884866714, 0.2143603414297104, 0.4874896705150604, 0.4236368238925934, -0.10367246717214584, -1.6274861097335815, 0.2044801414012909, 0.13174250721931458, -0.6045652031898499, -0.3575347661972046, -0.672147274017334, -0.5002467036247253, 0.04261483624577522, 0.2896135449409485, -0.816342294216156, 1.4974393844604492, -0.2944471836090088, -1.9995218515396118, 0.44348400831222534, 0.010196443647146225, 0.26781120896339417, 0.32600289583206177, -0.1245540976524353, -0.741595447063446, -0.6402982473373413, -0.279020220041275, 0.35025471448898315, 0.8897944092750549, 0.2681690752506256, -0.2643730938434601, 0.25744321942329407, -0.18418732285499573, 0.23779445886611938, -0.6437942981719971, 0.8192844390869141, -0.8918379545211792, -0.39422452449798584, 0.4900530278682709, 0.8215941190719604, -0.10978788137435913, -0.31299111247062683, 0.0027579194866120815, -1.2043328285217285, 0.2630236744880676, -0.09723108261823654, 1.323488473892212, -0.7553569674491882, -0.1898472160100937, -0.09387653321027756, -0.14570944011211395, -0.3079388737678528, -0.9864535331726074, 0.5523199439048767, -0.1815119981765747, 0.09727241098880768, 0.12710407376289368, -1.1173988580703735, 0.049783796072006226, -0.5183627605438232, -0.7850098609924316, -0.16132009029388428, 0.12576694786548615, 0.7946073412895203, -0.8043942451477051, 0.36684247851371765, -0.35170286893844604, 0.39340612292289734, -1.1678944826126099, 1.3545197248458862, -0.3833863139152527, 0.30496343970298767, -0.17163003981113434, -0.09604290872812271, -0.05430522933602333, -0.3385899066925049, 0.6818286776542664, -0.4371452033519745, 0.3186553418636322, 0.7213684320449829, -0.2864798903465271, 1.453585147857666, -0.7803604006767273, 0.1566753089427948, 0.10256143659353256, -0.3192804753780365, -0.03643586114048958, 0.23419567942619324, -0.1001773551106453, -0.6330352425575256, 0.4501737654209137, 0.6117905378341675, -0.6852520108222961, 0.7601131796836853, 0.5822471380233765, 0.5417116284370422, -0.6515018939971924, -0.03443334996700287, 0.8341749310493469, -0.15502022206783295, 0.5765137076377869, 0.4296667277812958, 0.5949358940124512, 0.24320100247859955, 0.8275665640830994, -0.2261616289615631, 0.7465196251869202, -1.0232032537460327, -0.0420299731194973, 0.19173090159893036, 0.6596037745475769, 0.38811585307121277, 0.186788871884346, -0.8504992127418518, -0.5276961326599121, -0.43370360136032104, 0.5780430436134338, 2.172452449798584, -0.3717624247074127, -0.34816160798072815, -0.594440758228302, -0.027583131566643715, -0.5295886397361755, -0.10907918959856033, -0.18158982694149017, -0.21674250066280365, -0.7314631938934326, -1.215991497039795, 0.5742485523223877, -0.11635274440050125, 0.5841490030288696, -0.19245177507400513, 0.20642882585525513, 0.15365241467952728, 0.13658683001995087, -0.818276584148407, -1.1997836828231812, 0.3515254259109497, -0.7156943678855896, 0.17346534132957458, 0.02204183302819729, -0.0957421362400055, 0.0077098519541323185, -0.9028600454330444, 1.2113478183746338, -0.6936187148094177, 0.08657613396644592, -0.09786374121904373, 0.6477376222610474, -0.27318325638771057, -1.0390435457229614, 0.5335308313369751, 0.5410303473472595, 0.023273706436157227, 0.12432873249053955, 0.17906597256660461, 0.23713746666908264, 0.41040343046188354, -0.23952652513980865, 0.30514219403266907, 0.26019659638404846, 0.01596449501812458, 1.0071827173233032, -0.3368072807788849, 0.2925243079662323, -1.5646084547042847, 1.343832015991211, 0.30308929085731506, -0.8152381181716919, 0.2581872045993805, -0.8807364106178284, -0.3862115144729614, 0.7439711689949036, -0.9895885586738586, -0.29676568508148193, -0.9240339398384094, 0.3122138977050781, -0.19701053202152252, 0.3598678410053253, -0.10137949138879776, -0.13151675462722778, 0.29794591665267944, 0.3945492208003998, 0.3634987473487854, -0.0900362953543663, -0.5282522439956665, 0.5981454253196716, -0.9108780026435852, 0.5193712115287781, 0.1274055391550064, 0.44451358914375305, -0.2855651080608368, -0.5452927947044373, -0.11150019615888596, -0.32093051075935364, -0.47132426500320435, -0.14842821657657623, -0.23961791396141052, -0.29346251487731934, -1.135987401008606, -0.28649482131004333, 0.1295534074306488, -0.6996626257896423, -0.2810492217540741, 0.1884518563747406, -0.32315775752067566, -0.3441891074180603, -1.3176872730255127, -1.4837929010391235, -0.25029659271240234, -1.1196123361587524, -1.021191120147705, 0.26357704401016235, -0.2500811219215393, -0.2255096435546875, -0.23204641044139862, 0.12747082114219666, -0.36539509892463684, 1.388365387916565, -1.370883584022522, 1.2289577722549438, -0.001634366693906486, -0.34164005517959595, 0.04332227259874344, 0.5146584510803223, 0.4362536370754242, -0.2394082397222519, -0.2613857090473175, -1.0939000844955444, -0.15140117704868317, -0.4735551178455353, 0.21664759516716003, -0.05620824918150902, 0.31476572155952454, 0.6998245716094971, -0.1836971640586853, -0.4699386656284332, 0.8429800271987915, 1.0975362062454224, -0.7455126047134399, -0.23365919291973114, 0.6401615738868713, 0.8581113219261169, 0.30983904004096985, -0.1778772473335266, 0.7535383105278015, 0.3784668445587158, 0.7893434762954712, 0.07748016715049744, -0.08215120434761047, -0.3936308026313782, -0.6885117292404175, 0.8939118981361389, 2.251157522201538, 0.5064859390258789, 0.04162324219942093, -0.6767880320549011, 0.5220008492469788, -1.1463594436645508, -0.26623645424842834, 1.1040939092636108, 0.9508079886436462, 0.4114986062049866, -0.3880968391895294, -0.2537373900413513, -0.4139208495616913, 0.4259204864501953, 0.2470559924840927, -0.451029509305954, -0.8982324600219727, 0.09962648898363113, 0.12781812250614166, 0.020603176206350327, 0.6422377228736877, -0.23154589533805847, 0.9366390705108643, 14.475537300109863, 1.1541690826416016, 0.4768677353858948, 1.202575922012329, 0.5152488350868225, -0.09834042936563492, -0.5624269247055054, -0.2941814661026001, -1.1316298246383667, -0.03588809818029404, 1.180873990058899, 0.11534542590379715, 1.486303687095642, 0.06352949142456055, 0.13839839398860931, 0.6447158455848694, -0.30901649594306946, 0.5205058455467224, 0.48399171233177185, -1.0260181427001953, 0.8657582998275757, 0.09101878851652145, 0.6200925707817078, 0.6981316208839417, 0.505963921546936, 1.041426420211792, 0.27510929107666016, -0.22489391267299652, 0.550399124622345, 0.0002113047958118841, 0.9726926684379578, -0.3517450988292694, 0.30832111835479736, 0.22615577280521393, -0.9734646081924438, -0.47709113359451294, -0.6324899196624756, -0.975619375705719, 0.3602296710014343, 0.4333164691925049, -0.6120017170906067, -0.9102898240089417, 0.20432330667972565, 0.358029842376709, 0.032932184636592865, 0.20740307867527008, -0.27698537707328796, 0.5099865794181824, -0.18762192130088806, 0.5915972590446472, -0.002893165685236454, 0.2878096401691437, 0.33365675806999207, -0.35530325770378113, 0.12809379398822784, -0.19034262001514435, -0.06851152330636978, 0.411129355430603, -0.8417993187904358, 0.1810639351606369, 0.12426149100065231, -0.08438380062580109, 0.3400314152240753, 0.9419649839401245, 0.7284734845161438, 0.33365198969841003, -0.3161749541759491, 0.4603384733200073, 0.7457041144371033, 0.259733110666275, -0.18140454590320587, 0.24547819793224335, 0.49749356508255005, -0.22550155222415924, -0.2933395504951477, 0.6914190649986267, 0.12971365451812744, -0.5855885148048401, -0.7543800473213196, -0.6772828698158264, 0.39234232902526855, -0.7332115769386292, -0.8119530081748962, 1.0951294898986816, -0.14323605597019196, -0.492266446352005, 0.13209065794944763, -0.30749425292015076, -0.13053561747074127, 0.5134625434875488, -1.3646318912506104, -0.8329460620880127, 0.6795016527175903, -0.23988144099712372, -0.09213187545537949, -0.6677056550979614, 1.1304924488067627, 0.6127582788467407, -0.3983900547027588, 0.5953461527824402, 0.3296772837638855, -0.5882439613342285, 0.2647179663181305, -0.2835818827152252, 0.8802931904792786, 0.35532519221305847, -0.2727549374103546, 0.3134017884731293, 0.20729872584342957, 0.6702843904495239, -0.6590203046798706, 0.0656203180551529, 1.0063977241516113, -0.7037916779518127, -0.11561638861894608, -0.847817599773407, -0.7138919830322266, 0.17227157950401306, 0.2855314314365387, -0.4265526533126831, 0.2984386086463928, 0.2963545620441437, -0.820122480392456, -0.4612729251384735, -0.6419743895530701, -0.1700228452682495, 0.6182869076728821, -0.9819534420967102, -0.0384664423763752, 0.025057677179574966, 0.45718473196029663, -1.059857726097107, -0.518000602722168, -0.08896901458501816, 0.012050838209688663, 0.3495846688747406, 1.236923098564148, -0.2713784873485565, 0.09010523557662964, 0.40064331889152527, -0.18406103551387787, -0.9600403308868408, -0.4639488756656647, -1.056381106376648, 0.11964566260576248, 0.23569722473621368, 0.7059904336929321, -0.4813481569290161, -0.1831621378660202, 0.5507389903068542, 0.30021587014198303, -0.23200367391109467, -0.8024195432662964, -0.13341279327869415, 0.12774981558322906, -0.3320119082927704, 0.6680580377578735, -0.20413105189800262, -0.38246166706085205, 0.4221870005130768, 0.24804511666297913, 0.9541081786155701, -0.24325573444366455, -0.9003227353096008, 0.29808536171913147, 0.035720858722925186, -0.5094338655471802, -0.8550379276275635, 0.09298619627952576, -1.2141144275665283, -0.11971250176429749, -1.1573522090911865, 0.0811806321144104, -0.5964088439941406, -0.20255450904369354, 0.15398195385932922, -0.46120715141296387, -0.10224613547325134, 0.55389004945755, -0.23257535696029663, -0.34180283546447754, -0.08539697527885437, -0.6672782301902771, 1.0642495155334473, 0.8698868155479431, -0.9128187894821167, 0.030367402359843254, 0.1497911959886551, 0.17423684895038605, 0.37957245111465454, 0.5120730400085449, -0.496075838804245, -0.856537401676178, -1.5308341979980469, 0.6632769703865051, -0.55101078748703, -0.5148546099662781, -0.31234270334243774, 0.6725216507911682, 0.39610815048217773, -0.11802167445421219, 0.42997482419013977, 0.18491779267787933, -1.0111019611358643, -0.1429908126592636, 0.17171171307563782, -0.6101368069648743, 0.5781403183937073, 0.2779408097267151, -0.4797084331512451, -0.25754621624946594, 0.8591741919517517, 0.110690176486969, -0.9370821714401245, -0.9723743200302124, 0.46783557534217834, -0.7436357140541077, 0.16630996763706207, -0.7354275584220886, 0.24663659930229187, -0.8064596652984619, -0.4118843078613281, 0.2802320122718811, 0.2964290380477905, -0.3185403347015381, 1.019789457321167, 0.05682360380887985, -1.3369656801223755, 0.034024640917778015, 0.7258837819099426, 0.22411280870437622, -0.2990952432155609, 0.29951319098472595, 0.45369458198547363, -0.08166051656007767, 0.6118284463882446, 0.2996239960193634, 0.42910096049308777, -0.6634920835494995, -0.27314189076423645, 0.9692897200584412, -0.9192154407501221, 0.04905397444963455, 1.3422123193740845, 0.1725267618894577, -1.5514642000198364, 0.010670322924852371, -0.6868454217910767, -0.5913161039352417, -0.35110199451446533, 0.7113150358200073, 0.09136371314525604, 0.0663202553987503, -0.17132097482681274, -0.7886810898780823, 0.2756837010383606, -0.12288710474967957, -0.4102143943309784, 0.5555798411369324, -0.6027756929397583, -0.7341710329055786, 0.3712897002696991, 0.49894097447395325, -0.5006076693534851, -0.6765364408493042, -0.8834523558616638, -0.3851955831050873, -0.022478099912405014, 0.5562792420387268, -0.5904853343963623, -0.6843367218971252, 0.3663620352745056, 0.7791010141372681, -0.28972098231315613, 0.6896473169326782, -0.09710675477981567, -0.07425343245267868, 0.4877068102359772, 0.5461878776550293, -0.8848440647125244, -0.4633767604827881, 1.1886521577835083, 1.3393014669418335, -0.8120091557502747, 0.2688010036945343, 0.14303283393383026, -0.5008360147476196, 0.6831015944480896, 0.345878928899765, 0.2777708172798157, 0.8097884058952332, -0.19110676646232605, 0.18875454366207123, 0.34447264671325684, -1.330672025680542, -0.1636609435081482, 1.0554779767990112, 0.6541063785552979, 0.7121310830116272, 0.638580858707428, -0.11878398805856705, 1.0015314817428589, -0.06204357370734215, -0.18103554844856262, 0.40254953503608704, -0.24393346905708313, -0.17270298302173615, -0.5289032459259033, -0.06755534559488297, 0.8172361850738525, -0.6608757972717285, -0.7549825310707092, 0.2832285165786743, 0.32147416472435, -0.006523594260215759, 0.5642284154891968, 0.5632661581039429, 0.005490301642566919, 0.6380583047866821, 0.04587489739060402, 0.39121052622795105, -0.5706065893173218, -0.24029260873794556, 0.3820432722568512, -0.6134186387062073, -0.34904298186302185, 0.0746675431728363, -0.26688244938850403, -0.057528164237737656, -0.27575838565826416, 0.02191772684454918, -0.11753978580236435, 0.40852493047714233, 0.9121953845024109, 0.4973210096359253, 0.6790924668312073, -0.19414225220680237, -0.6931858658790588, -0.753839910030365, -1.129015326499939, -0.06212740018963814, -0.6786307692527771, -0.3960016965866089, -0.11569774150848389, -0.20899644494056702, -0.5692487359046936]}, "authors": [{"authorId": "2293351624", "name": "Zequan Liu"}, {"authorId": "2293318377", "name": "Jiawen Lyn"}, {"authorId": "2293408368", "name": "Wei Zhu"}, {"authorId": "2293390999", "name": "Xing Tian"}, {"authorId": "2293317722", "name": "Yvette Graham"}], "references": [{"paperId": "acd7adcdb914fb2e18a19792af47e59e78cab3f0", "title": "Overview of the PromptCBLUE Shared Task in CHIP2023"}, {"paperId": "ce2272a439ba89f1ea0a822c8d19078732d75e5d", "title": "PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"}, {"paperId": "f637047c326329dfc4fdb945cd1264214d47c30a", "title": "Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning"}, {"paperId": "0d7f24578340aae6df610ed95aaa276b9c3ddcd3", "title": "VeRA: Vector-based Random Matrix Adaptation"}, {"paperId": "8ce219059d777c2333ee21cb2af2aad71275c98f", "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "623415074702068fdf0b6a4883fa8561e0c35c36", "title": "Multi-task entity linking with supervision from a taxonomy"}, {"paperId": "e6e5c6468466fd2ece4e7653f1b4cb4f965b458c", "title": "F-PABEE: Flexible-Patience-Based Early Exiting For Single-Label and Multi-Label Text Classification Tasks"}, {"paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"}, {"paperId": "6a2d96d2a7adde6349f15c1e680b67d114e7b67c", "title": "Unified Demonstration Retriever for In-Context Learning"}, {"paperId": "873a581320d928249609d3c07229d5af182a379c", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"}, {"paperId": "45fe20ec9a351e6f6d24316cdfef30a84062ce5b", "title": "Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation"}, {"paperId": "8973ad5fc1264594a1fda3bd9e04258074cea9cc", "title": "Neural Architecture Search: Insights from 1000 Papers"}, {"paperId": "a981a57848e19adb80c4a29471fbb798ac050a8f", "title": "Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts"}, {"paperId": "50a260631a28bfed18eccf8ebfc75ff34917518f", "title": "Convolutional Bypasses Are Better Vision Transformer Adapters"}, {"paperId": "41129978a894dfc9726664444d6d0f7f468416cd", "title": "Sparse Structure Search for Parameter-Efficient Tuning"}, {"paperId": "960d40497717ad22a7ebb84db238fa2415fc89cc", "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning"}, {"paperId": "a7b92f9c62cdf35c37d9ccdb20e43807eb86900a", "title": "IDPG: An Instance-Dependent Prompt Generation Method"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "d05141dc0900140f7146bb71e1f7402cf896ea87", "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "84310f76cf8909f87c6a7f2ed30ae28214cc9eab", "title": "LeeBERT: Learned Early Exit for BERT with cross-level optimization"}, {"paperId": "4690eb050572a279f94560b6bbdccaae577b45f5", "title": "MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "87f759eb8c2a8d718b5d1e27ea28bf73c15a922c", "title": "Automatic Student Network Search for Knowledge Distillation"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "7a5d40ba5e1beec91bf45067102dbb0d7e6a420a", "title": "Mining Infrequent High-Quality Phrases from Domain-Specific Corpora"}, {"paperId": "44928ee327335883eacce30d3baeb7dfdaa1fed2", "title": "AutoRC: Improving BERT Based Relation Classification Models via Architecture Search"}, {"paperId": "9b39de93ab9b5c74ed726d1ee8e31a927e4f6292", "title": "AutoTrans: Automating Transformer Design via Reinforced Architecture Search"}, {"paperId": "15e5f85bf12c901efea51dee91dcea5d96572c96", "title": "Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation"}, {"paperId": "056935031bc5cf0aeeaa0946320de26e14a1817e", "title": "Revisiting Few-sample BERT Fine-tuning"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "7fb301ea25f02dc7f4f7ee1360137503ee942c8c", "title": "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models"}, {"paperId": "38b64492ac1b5d6c8166c7952073e760bfb8f46a", "title": "Stabilizing Differentiable Architecture Search via Perturbation-based Regularization"}, {"paperId": "3cb1103dd1fedddc7524888eed519ebe09ac7f0c", "title": "Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "2bb4b6b876581c0c21ef58d3c38809715173ca7a", "title": "PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "bc61e3b5612ebbf7363ecfb70f6dce433d6893ed", "title": "The Dr-KGQA System for Automatically Answering Medication Related Questions in Chinese"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d55d1d035e91220335edff0fe8f5d249d8c4a00b", "title": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "90d4087dc495db8990ac36bff466d1bddc477df8", "title": "Methods"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "22effc238e65136003690f9a0cdcfb73b72b6f0f", "title": "SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning"}, {"paperId": "c136f3882f1d466a494e9c52d424b4197b16be04", "title": "Learned Adapters Are Better Than Manually Designed Adapters"}, {"paperId": "4e6881ed38b637d377bfa54350a48f98db469b04", "title": "FastNER: Speeding up Inferences for Named Entity Recognition Tasks"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "4243ba22834129c254543198b7f36d364a1a0eb2", "title": "BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting"}, {"paperId": "8bd3198940c205bad825c63b1950dfc2963a38e8", "title": "NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks"}, {"paperId": "4d63a23a299350d6e17d86c1c1b83687469194af", "title": "LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "ab461008fe8540054c7aa5f42ff55a9a39367704", "title": "PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting"}, {"paperId": null, "title": "to its bottleneck architecture."}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "50920f95cee87913dea19656e83ae06b51f9ba4a", "title": "Discovering Better Model Architectures for Medical Query Understanding"}, {"paperId": "a0033c2b38d289fd71194eb830b14d0db8f5a18b", "title": "Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning"}, {"paperId": "74512e17ba6e9617260bf9a65503ffa22bc66e3a", "title": "Global Attention Decoder for Chinese Spelling Error Correction"}, {"paperId": "fc5e25efe0a30230b553c817e38bc8308e127a08", "title": "paht_nlp @ MEDIQA 2021: Multi-grained Query Focused Multi-Answer Summarization"}, {"paperId": "6fbd2bcf27c3df92c824406888cd9b25ceff31e3", "title": "GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning"}, {"paperId": "5d1132b0f6eee0cdc1d6e2539520c5c0c49bfa8a", "title": "AutoNLU: Architecture Search for Sentence and Cross-sentence Attention Modeling with Re-designed Search Space"}, {"paperId": null, "title": "Parameterized hypercomplex graph neural networks for graph classification"}, {"paperId": "ac14aa5c8ba41037446ae04ace3fdcabf1ecd710", "title": "Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2023. GPT-4 Technical Report"}, {"paperId": null, "title": "2023. Stanford al-paca: An instruction-following llama model"}, {"paperId": null, "title": "2023. QLoRA: Efficient Fine-tuning of Quantized LLMs"}, {"paperId": null, "title": "2023d. Acf: Aligned contrastive fine-tuning for language and vision tasks"}, {"paperId": null, "title": "2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models"}, {"paperId": null, "title": "2022. Adaptable adapters"}, {"paperId": null, "title": "and achieve better performances"}, {"paperId": null, "title": "2023b. Extracting decision trees from medical texts: An overview of the text2dt track in chip2022"}]}