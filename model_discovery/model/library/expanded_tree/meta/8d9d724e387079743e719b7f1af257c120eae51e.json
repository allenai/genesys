{"paperId": "8d9d724e387079743e719b7f1af257c120eae51e", "abstract": "Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models' language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models' sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes, together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception."}, "embedding": {"model": "specter_v2", "vector": [0.0712895616889, 0.7482767105102539, -0.1296011060476303, -0.26084962487220764, -0.10038193315267563, -0.053312502801418304, 0.3641276955604553, -0.09012404829263687, -0.4332646429538727, -0.4172179400920868, 0.4814130365848541, -0.5175457000732422, 0.06681825965642929, -0.007846773602068424, -0.2719557285308838, 0.14033901691436768, -0.6751076579093933, 0.5028771758079529, 0.07198004424571991, -0.7058910131454468, 0.013412204571068287, -0.8391217589378357, -1.0344822406768799, 0.2505947947502136, 0.926064670085907, -0.12405095249414444, 0.7978207468986511, 1.249759554862976, -0.14204411208629608, 0.2119165062904358, 0.35686805844306946, -0.13764867186546326, -0.02919100970029831, -0.15591149032115936, -0.08128876239061356, -0.19391049444675446, 0.7996551394462585, -0.5822004675865173, -0.5066869258880615, 0.5386346578598022, -0.12841230630874634, -0.08037860691547394, 0.6000059843063354, -0.6284759044647217, -0.7839500904083252, 0.6772221326828003, 0.7823014259338379, 1.0085577964782715, -0.04191461205482483, -0.28809231519699097, 1.2504395246505737, -0.8133654594421387, 0.11909275501966476, 1.44293212890625, 0.22110192477703094, 0.47093072533607483, 0.004963739309459925, -0.3029208183288574, 0.2498735934495926, -0.139205664396286, -0.5670081973075867, -0.3791610300540924, -0.16820773482322693, -0.02290518954396248, 1.658272385597229, -0.3893313705921173, -0.31987157464027405, 0.29294711351394653, 0.1637038290500641, 1.3562227487564087, 0.5449090600013733, -1.2856783866882324, -0.17741701006889343, 0.3428296744823456, 0.598130464553833, 0.8629564642906189, -0.2656128704547882, 0.536450982093811, -0.9814473390579224, -0.040428370237350464, 1.1244897842407227, -0.8617300391197205, -0.3620864152908325, -0.15711350739002228, -0.9441401362419128, 0.6174301505088806, 0.2681603729724884, 0.7587183713912964, -0.07164168357849121, 0.4203556776046753, 0.4778885543346405, 0.6684876084327698, -0.28341659903526306, 0.3483429551124573, -0.4726838767528534, 0.4279947280883789, -0.4958566725254059, -0.09106321632862091, -0.03747028857469559, 1.153882384300232, -0.16976380348205566, 0.24080012738704681, -0.7658160924911499, 0.2436128407716751, 1.4888368844985962, 0.4146993160247803, 0.6138398051261902, -0.9038568735122681, 0.3665979504585266, -0.3529380261898041, 0.3861244320869446, -0.7092822194099426, -0.441744863986969, -0.14796824753284454, -0.3941647410392761, -1.0164155960083008, 0.24589885771274567, 0.346650630235672, -0.7771164178848267, 0.8239014744758606, -0.1534421145915985, -0.46481409668922424, -0.0299586933106184, 0.4621289074420929, 0.6835414171218872, 0.4361662268638611, 0.5035911202430725, 0.18467913568019867, 1.1125789880752563, -0.43593230843544006, -0.8427444696426392, -0.9723318815231323, 0.6647518873214722, -0.37650078535079956, 1.2011308670043945, -0.19003742933273315, -1.1271666288375854, -1.3888027667999268, -0.8709499835968018, -0.11472981423139572, -0.2169930785894394, 0.9280892014503479, 1.3021191358566284, 0.3504745364189148, -1.1944023370742798, 0.6267499327659607, -0.2475094050168991, -0.78971928358078, 0.17969286441802979, 0.16374613344669342, 0.07545138150453568, -0.36563655734062195, -1.3863928318023682, 0.4214484691619873, 0.24321189522743225, -0.991077184677124, 0.19279909133911133, -0.38782110810279846, -1.4514211416244507, -0.2351151406764984, 0.2464817315340042, -0.3763160705566406, 1.1469517946243286, -0.07261504977941513, -1.075961947441101, 0.8684993386268616, -0.4793619215488434, 0.32919129729270935, -0.03776983916759491, -0.12242291867733002, -0.6114226579666138, -0.12799771130084991, 0.03350362926721573, 0.6902187466621399, 0.1024591252207756, -0.501327633857727, -0.21707771718502045, 0.06547553837299347, -0.150675967335701, -0.16452421247959137, -0.16731685400009155, 1.0115971565246582, -0.18965154886245728, -0.5820577144622803, 0.26912960410118103, 0.5526456236839294, 0.45626792311668396, 0.05363958328962326, -0.17439977824687958, -1.362725853919983, 0.1375645399093628, -0.3291129767894745, 1.6204111576080322, -0.6651524305343628, -0.569845974445343, -0.3736301064491272, -0.08779226243495941, -0.19890031218528748, -0.7460938692092896, 0.41848883032798767, -0.2639181613922119, 0.18866300582885742, -0.5964092016220093, -0.9300162196159363, 0.06329672783613205, -0.020588692277669907, -0.23496277630329132, -0.15911805629730225, -0.06818538904190063, 1.1449404954910278, -0.6762165427207947, -0.08417539298534393, -0.02374647930264473, 0.029577389359474182, -0.9248033761978149, 1.0937269926071167, -0.46952489018440247, 0.24298222362995148, 0.3675716519355774, -0.49373218417167664, -0.1316923350095749, -0.5508943796157837, 0.021831953898072243, -0.5264849662780762, -0.1600104123353958, 0.30475375056266785, -0.3202512860298157, 1.3662558794021606, -0.06614307314157486, 0.8139314651489258, -0.407985657453537, -0.4875898063182831, -0.1090523898601532, 0.36930328607559204, -0.6686899662017822, -0.3544684648513794, 0.5613923668861389, 0.48654675483703613, -0.030269986018538475, -0.0900038406252861, 0.2440790981054306, 0.9415087103843689, -0.1983356475830078, 0.5963295698165894, 0.41666755080223083, -0.29250794649124146, 0.4313592314720154, 0.48840609192848206, 0.05722879245877266, 0.42915043234825134, 0.8223675489425659, -0.7230904698371887, 0.368971049785614, -0.39462852478027344, -0.6716605424880981, 0.3529742360115051, 0.4677041172981262, 0.897206723690033, 0.142900750041008, -0.8279792070388794, -0.39353564381599426, -0.14991255104541779, 0.4901328682899475, 1.85245680809021, -0.22697123885154724, -0.1713877171278, -0.9196049571037292, 0.08563697338104248, -0.523618757724762, 0.8567401766777039, -0.7925654053688049, -0.26087769865989685, -0.834993302822113, -0.4384881556034088, 0.5026240944862366, 0.48771780729293823, 0.8633237481117249, -0.6398345232009888, -0.6213201880455017, -0.2517545819282532, 0.3302304446697235, -0.8065124750137329, -1.0896319150924683, -0.18181534111499786, -0.46307337284088135, -0.3198752701282501, 0.12049822509288788, -0.3998784124851227, 0.1323087513446808, -0.2967415153980255, 0.9422897696495056, -0.26144352555274963, -0.16984109580516815, 0.49852702021598816, 0.7379565834999084, -0.7820907235145569, -1.3285647630691528, -0.05200387164950371, 0.21563781797885895, -0.3259965479373932, 0.5341467261314392, 0.8498857617378235, 0.22443163394927979, 0.33566007018089294, -0.42319828271865845, 0.20954422652721405, 0.08708373457193375, -0.04715421050786972, 0.43905648589134216, -0.5529260039329529, -0.10740084201097488, -0.8490816354751587, 1.0520951747894287, 0.6407334208488464, -0.3586834669113159, 0.4865877032279968, -0.5808398723602295, -0.1236107274889946, 0.22599808871746063, -0.7831284999847412, -0.20368726551532745, -0.9589037299156189, 0.7205032706260681, 0.4537125527858734, -0.4564443528652191, 0.06733006983995438, -0.05520511046051979, -0.08306872844696045, 0.6483028531074524, 0.4905446469783783, 0.7201777100563049, -0.05318325385451317, 0.4716501235961914, -0.2402387261390686, 0.49400246143341064, 0.27928438782691956, -0.2882334887981415, -0.3302549421787262, -0.46486130356788635, -0.7892295718193054, -0.32692819833755493, 0.1556997448205948, -0.46291986107826233, -0.01244044303894043, 0.6196240782737732, -0.5258908271789551, -0.8696863055229187, 0.07502175867557526, -1.1856067180633545, -0.33482784032821655, 0.3877485990524292, 0.0323486328125, -0.4806724190711975, -0.8198497295379639, -0.9708651900291443, 0.005598815158009529, -0.12684974074363708, -1.1497176885604858, 0.4269670844078064, 0.14660581946372986, -0.6563050746917725, -0.44362881779670715, 0.0646129623055458, -0.09720422327518463, 1.0704004764556885, -0.8063837289810181, 1.3500921726226807, -0.0874236598610878, -0.13021081686019897, 0.04834967851638794, 0.30866584181785583, 0.4038645327091217, 0.2728540003299713, 0.31521421670913696, -0.5158199667930603, 0.16295269131660461, -0.02489963173866272, -0.024349505081772804, -0.057748179882764816, 0.5663001537322998, 0.10952933132648468, 0.1468804031610489, -0.2177800089120865, 0.05713855102658272, 1.0157055854797363, -0.7999256253242493, 0.08485329896211624, 0.2710895836353302, 1.2380074262619019, 0.7402674555778503, -0.16957241296768188, 0.24901805818080902, 1.069195032119751, 0.394612580537796, 0.02308007702231407, -0.10760264098644257, -0.5154750943183899, -0.3505246341228485, 0.7348883748054504, 1.6362390518188477, -0.2904737591743469, 0.10309578478336334, -1.3508293628692627, 0.2799395024776459, -0.7819404006004333, -0.4969044029712677, 0.4998491704463959, 1.054419994354248, 0.6540585160255432, -0.4296552538871765, -0.3039727509021759, -0.03389199450612068, 0.6132574081420898, -0.01609111577272415, -0.21549025177955627, -0.51058429479599, -0.12914490699768066, -0.03866647556424141, -0.1884785294532776, 1.0959947109222412, -0.2828122675418854, 0.8870121836662292, 14.919245719909668, 0.640070378780365, 0.04950743913650513, 0.40879008173942566, 0.4817080497741699, 0.7006503343582153, -0.4335296154022217, -0.2975563108921051, -0.9604475498199463, -0.10433738678693771, 1.0568175315856934, 0.13283970952033997, 0.8572909832000732, -0.3305568993091583, 0.20551952719688416, 0.09655057638883591, -0.5444268584251404, 0.7137529850006104, 0.686895489692688, -0.9465661644935608, 0.830153226852417, 0.037732068449258804, 0.135515958070755, 0.3019806146621704, 0.27239561080932617, 0.4940396547317505, 0.07762549817562103, -0.37530165910720825, 0.42345893383026123, 0.1799662858247757, 1.0631086826324463, -0.0701030045747757, -0.028643865138292313, 0.5764405131340027, -0.7608112692832947, -0.002012268640100956, -0.5949698686599731, -1.0820716619491577, 0.02450660429894924, -0.4348422884941101, -0.18739715218544006, -0.9076760411262512, -0.35422196984291077, 0.3654917776584625, -0.2635115683078766, 0.20416764914989471, -0.5154076218605042, 0.845386266708374, 0.09929647296667099, -0.29738134145736694, -0.17220132052898407, 0.6107621788978577, 0.45751723647117615, -0.2687094211578369, -0.06807775050401688, 0.2474362701177597, -0.007861302234232426, 0.5315314531326294, -0.06846027821302414, 0.20422488451004028, -0.0640769749879837, -0.4274041950702667, 0.112001433968544, 0.18351203203201294, 0.14958901703357697, 0.2262469083070755, -0.3117259740829468, 0.16489183902740479, 0.5638379454612732, 0.0538635328412056, -0.1257457286119461, -0.12529700994491577, 0.36225852370262146, -0.23119057714939117, -0.05249223858118057, 0.16848716139793396, -0.3980926275253296, -0.3573509156703949, -0.6463183760643005, -0.7363207340240479, -0.0476105660200119, -1.1996012926101685, -0.5264113545417786, 0.9472245573997498, 0.06562921404838562, -0.4695499539375305, 0.5624592304229736, -1.248045563697815, -0.44270434975624084, 0.06477387249469757, -1.0019639730453491, -0.9466073513031006, -0.007821613922715187, -0.2802852988243103, 0.0283969659358263, -0.34825459122657776, 1.2007930278778076, -0.3149650990962982, 0.03937339782714844, 0.15290172398090363, -0.24810636043548584, -0.24855351448059082, 0.21168608963489532, -1.0428835153579712, 0.9810979962348938, 0.15961110591888428, 0.10707232356071472, 0.6077045202255249, 0.4609340727329254, 0.10585282742977142, -0.6802460551261902, 0.6278473734855652, 0.7763119339942932, -1.1579986810684204, -0.22173312306404114, -0.7221255302429199, -1.1422743797302246, 0.06262325495481491, 0.9110548496246338, -0.42779457569122314, 0.1629202514886856, -0.41781026124954224, -0.18421906232833862, -0.04467065632343292, -0.6353217363357544, 0.2174350917339325, 0.6202940940856934, -0.6911509037017822, -0.7708728909492493, -0.034646086394786835, 0.8032568693161011, -0.9091030955314636, -0.45618027448654175, -0.43958932161331177, 0.020507419481873512, -0.05156537517905235, 0.5167869329452515, -1.0784705877304077, 0.43689650297164917, 0.49551597237586975, -0.11927412450313568, -1.2254118919372559, -0.5093103051185608, -1.0079233646392822, 0.27717727422714233, 0.07540448009967804, 0.936355471611023, -0.536973237991333, -0.5113033652305603, 0.8985495567321777, 0.15827207267284393, -0.37084969878196716, -0.6016413569450378, 0.19106370210647583, 0.3057253956794739, -0.5515608191490173, 0.5825288891792297, -0.2732706367969513, -0.27939802408218384, 0.32841670513153076, 0.5126478672027588, 0.9027485847473145, -0.14793133735656738, -0.5648515224456787, 0.30041074752807617, -0.30500882863998413, -0.19356468319892883, -0.5244060158729553, -0.15647560358047485, -1.3941906690597534, -0.3687903881072998, -0.8289833068847656, 0.09354142099618912, -1.2230639457702637, -0.686097264289856, 0.3260113298892975, 0.012603016570210457, -0.024262685328722, -0.17344819009304047, -0.7256150841712952, -0.690325915813446, -0.2761925458908081, -0.22219246625900269, 0.7472733855247498, 1.2771714925765991, -0.3076893091201782, 0.14122915267944336, 0.06952919065952301, -0.22964796423912048, 0.2511042058467865, 0.6668170690536499, -0.29113611578941345, -0.7258215546607971, -1.6988141536712646, 0.47717538475990295, -0.43376943469047546, -0.3288094401359558, -0.48105955123901367, 0.8745891451835632, 0.40092039108276367, -0.3132939040660858, 0.16697721183300018, -0.2800249457359314, -0.7301937341690063, -0.9047783017158508, 0.41545718908309937, -0.8228024840354919, 0.4664084315299988, 0.5472491979598999, -0.6105927228927612, 0.04061536863446236, 0.6810760498046875, 0.005179825704544783, -0.969161331653595, -0.6857617497444153, 0.2689584493637085, -0.5706614851951599, 0.6165316104888916, -0.035079747438430786, -0.10075222700834274, -1.1355464458465576, -0.16393373906612396, -0.00969131663441658, 0.21504254639148712, -0.20055590569972992, 1.0271308422088623, 0.23901408910751343, -1.0718954801559448, -0.04770940542221069, 0.7085028886795044, 0.2702697515487671, -0.346377432346344, 0.10519300401210785, 0.15287941694259644, -0.5179989337921143, 0.5319814682006836, 0.1528899073600769, 0.04133076220750809, -0.9175537824630737, -0.29178720712661743, 0.48801374435424805, 0.32376372814178467, -0.07646718621253967, 0.9525259137153625, -0.34129101037979126, -1.3485137224197388, 0.11264392733573914, -1.2948260307312012, -0.6900754570960999, -0.5468525290489197, 0.9300031065940857, 0.3803030252456665, -0.5283104777336121, -0.384714275598526, -0.2694452106952667, 0.8510696887969971, 0.009059285745024681, -0.16996054351329803, 0.37207934260368347, -0.4151277542114258, -0.5344278216362, 0.7519544959068298, 0.2752269208431244, -0.2637063264846802, -0.7081813216209412, -0.32747045159339905, -0.4648318290710449, 0.017075419425964355, 0.10105355083942413, -0.4547367990016937, -0.41236573457717896, 1.0253493785858154, 0.5126959085464478, 0.2986816465854645, -0.07219807058572769, 0.31992170214653015, -0.004272146616131067, 0.4326174557209015, 0.2622056305408478, -0.5361018776893616, -0.8264167904853821, 1.118834376335144, 1.6022517681121826, -1.413791537284851, 0.219504714012146, -0.3297547698020935, -1.086208462715149, 0.874678373336792, 0.6449931263923645, 0.031123585999011993, 0.6317141056060791, -0.32845446467399597, 0.4646437466144562, 0.08844318240880966, -0.7075883150100708, -0.15308986604213715, 1.0855973958969116, 0.9545325636863708, 0.8448271155357361, 0.49365684390068054, -0.07735908776521683, 0.6873527765274048, -0.286136269569397, 0.31488385796546936, 0.6500059962272644, 0.2876792252063751, -0.738099217414856, 0.4192344844341278, 0.04476636275649071, 0.45359376072883606, 0.05871979519724846, -0.5681256651878357, 0.28917196393013, 0.8612824082374573, -0.02341109700500965, 0.42065438628196716, 0.657589316368103, 0.4400138556957245, 0.7973567843437195, 0.051629092544317245, 0.6978801488876343, -0.6410889625549316, 0.03384295850992203, -0.3263457417488098, -0.8268932104110718, 0.013412682339549065, -0.22306793928146362, -0.3906332552433014, -0.8508961796760559, 0.20044812560081482, 0.10625895857810974, -0.2861466705799103, 0.18177175521850586, 1.2087594270706177, 0.7815847396850586, 0.22194615006446838, -0.7076216340065002, -0.3286742866039276, -0.5993072986602783, -1.3674203157424927, 0.3102848529815674, -0.7132917642593384, -0.08746476471424103, -0.5024946331977844, -0.42301633954048157, -0.14529871940612793]}, "authors": [{"authorId": "2264636324", "name": "Changjiang Gao"}, {"authorId": "2046010", "name": "Shujian Huang"}, {"authorId": "2264297601", "name": "Jixing Li"}, {"authorId": "1838162", "name": "Jiajun Chen"}], "references": [{"paperId": "01a659968a6511e590f0e37a81eb25e53fa2c752", "title": "Explaining How Transformers Use Context to Build Predictions"}, {"paperId": "ae41e68c5f1d6da16b84a10a8a898aa6fc7d4889", "title": "Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization"}, {"paperId": "9ad02d98c3421cf93be11c81e7b19faab12aea4a", "title": "Quantifying Context Mixing in Transformers"}, {"paperId": "eaee0b647d336c6fc8b844812675ec35cddf14a1", "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "46c87b00d9ad2f69eb282fc2faa908dcf94eb77d", "title": "Attention weights accurately predict language representations in the brain"}, {"paperId": "2b31936832bdadb4b751c5294f73888571c37db0", "title": "Deep language algorithms predict semantic comprehension from brain activity"}, {"paperId": "bb1c9cb431e771660cffdda1d80a7f15ff40c764", "title": "Measuring the Mixing of Contextual Information in the Transformer"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "a6e259c8813ae419610dab4190a8358adcc7b2b5", "title": "Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects"}, {"paperId": "042c6d99bdf381b55048b8bb48c8479dbcfbcd5a", "title": "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "e1afe296314493791b1c2c0af88cfe97279bdc69", "title": "Relative Importance in Sentence Processing"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "54d1079968596ea2ffe17ef3eeb854ef488b4882", "title": "Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention"}, {"paperId": "644a33399711b31f8a5a1b464f6ffd7c2264fedc", "title": "The neural architecture of language: Integrative modeling converges on predictive processing"}, {"paperId": "04ef54bd467d5e03dee7b0be601cf06d420bffa0", "title": "Emergent linguistic structure in artificial neural networks trained by self-supervision"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "6add8d6f1ba7e78924dad8ba61df9808ed4d9ca6", "title": "Human Sentence Processing: Recurrence or Attention?"}, {"paperId": "76a9f336481b39515d6cea2920696f11fb686451", "title": "Quantifying Attention Flow in Transformers"}, {"paperId": "9a21740d87976bf76f4a9668a9da631035302fb2", "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"}, {"paperId": "3d4dfbdcb11d7b495e066435a9a98f02eb0cb369", "title": "Attention Interpretability Across NLP Tasks"}, {"paperId": "8b127b9c7e37f075559ed5a51428527205dcf3f5", "title": "Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks"}, {"paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9", "title": "Attention is not not Explanation"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad", "title": "Is Attention Interpretable?"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "title": "Attention is not Explanation"}, {"paperId": "49400b3a3ea01772e321e3e010b7b891c3d6cb88", "title": "Extracting Syntactic Trees from Transformer Encoder Self-Attentions"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "10ed243c62c7c613195dc8c414c1de1848f78c4c", "title": "A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy"}, {"paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0", "title": "Deep Learning Scaling is Predictable, Empirically"}, {"paperId": "c79ae75f34ee7c3c99ee456041dccac5dc1df310", "title": "Determinants of Scanpath Regularity in Reading"}, {"paperId": "fe2fb4dc0df7101295a914876023c0534797fd9c", "title": "Scanpaths reveal syntactic underspecification and reanalysis strategies"}, {"paperId": "7876a8b5c72679448aae017f1a3c6d9149aa5bc1", "title": "How do the efficiencies of reading subcomponents relate to looking back in text"}, {"paperId": "d751f14cd132e7dfaa052af26e5744a9c3c844fb", "title": "A Cross-lingual Comparison of Human and Model Relative Word Importance"}, {"paperId": null, "title": "The reading brain project: An open science data-sharing initiative"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "94238dead40b12735d79ed63e29ead70730261a2", "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Layerwise trivial pattern reliance"}, {"paperId": null, "title": "10: Attention divergence on the original and noise-attached text of 7B and 13B models"}, {"paperId": null, "title": "2022. Mul-titask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": null, "title": "2023a. Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens"}, {"paperId": null, "title": "Table 6 shows the list of layers in 7B Alpaca and Vi-cuna with significant change in human resemblance after instruction tuning compared with LLaMA, and Table 7 shows the list in 13B"}, {"paperId": null, "title": "the 2022 Conference on Empirical Methods in Natural Language Processing"}, {"paperId": null, "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Preliminary fMRI results Some preliminary analysis on the fMRI data in the Reading Brain dataset is done, and the results com-Figure"}]}