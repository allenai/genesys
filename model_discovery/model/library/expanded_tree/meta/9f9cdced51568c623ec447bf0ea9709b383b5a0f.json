{"paperId": "9f9cdced51568c623ec447bf0ea9709b383b5a0f", "abstract": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a light-weight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.", "venue": "arXiv.org", "year": 2023, "citationCount": 7, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.17002", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "A light-weight black-box tuning method (NMTune) is proposed to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models."}, "embedding": {"model": "specter_v2", "vector": [0.0747452825307846, 0.44600340723991394, -0.3350328803062439, -0.12425068765878677, -0.5357392430305481, -0.07711832225322723, 0.7945219874382019, -0.4513542950153351, -0.8668767809867859, -0.3971666991710663, 0.16319461166858673, 0.2501407861709595, 0.7007914185523987, 0.22957296669483185, -0.5526947379112244, 0.1635715812444687, -0.6092034578323364, 0.08211349695920944, -0.006114248186349869, -0.5392036437988281, -0.5960635542869568, -0.9492887258529663, -0.8928261995315552, 0.21280255913734436, 0.21921631693840027, 0.9803171753883362, -0.09848324209451675, 0.6278138756752014, -0.26433923840522766, 0.05954587087035179, 0.10536264628171921, -0.3795003592967987, 0.757747232913971, 0.03564071282744408, -0.18845432996749878, 0.5818637609481812, 0.46625375747680664, -0.48766568303108215, -0.6821367740631104, 1.210678219795227, -0.026560457423329353, 0.38788992166519165, 0.5647438764572144, -0.8091098666191101, -0.9314382672309875, 0.20528459548950195, 0.37999674677848816, 0.5657267570495605, -0.8321264982223511, -0.14280882477760315, 0.7975714206695557, -1.4644135236740112, 0.3113821744918823, 1.3812198638916016, 0.6951026916503906, 0.7627612948417664, -0.448691725730896, -0.937480628490448, 0.4152829051017761, 0.1498262584209442, -0.8120436072349548, -0.05612915754318237, -0.10452958196401596, -0.5689669251441956, 1.2894586324691772, -0.9470186233520508, -0.5345893502235413, 1.00704026222229, 0.04704606905579567, 0.8727277517318726, 0.11083123832941055, -0.6109704375267029, -0.2988694906234741, 0.21295157074928284, 0.2935570180416107, 0.759978175163269, -0.356350302696228, 0.8030892014503479, -0.8362244367599487, 0.06741799414157867, 0.399397075176239, -0.19305536150932312, 0.23915043473243713, -0.27144208550453186, -0.02883227914571762, 0.5940532088279724, 0.7772326469421387, 0.18730396032333374, 0.14728078246116638, 0.8594831228256226, 0.9394855499267578, 0.6257485747337341, 0.1711084544658661, 0.22714197635650635, -0.05730167403817177, 0.3376407027244568, -0.4362708330154419, -0.14355435967445374, -0.32114386558532715, 1.2065761089324951, 0.2383429855108261, -0.0505547858774662, -0.5637578964233398, 0.3502863943576813, 1.0130081176757812, -0.3400305211544037, 0.6252495050430298, -0.5358381867408752, 0.40936800837516785, -0.872608482837677, 0.277325302362442, -0.5082136392593384, -0.24880705773830414, -0.4817811846733093, -1.0214227437973022, -0.8403796553611755, -0.08149345964193344, 0.11586181074380875, -1.4529471397399902, 0.9614400863647461, -0.5909360647201538, 0.35897427797317505, 0.062263213098049164, 0.740788996219635, 0.40184494853019714, 0.5375933647155762, 0.5261562466621399, 0.5425036549568176, 0.7447239756584167, -1.025041103363037, -0.37670600414276123, -0.6948741674423218, 0.6696176528930664, -0.6976176500320435, 0.8978572487831116, -0.30233490467071533, -0.7423640489578247, -1.4122815132141113, -0.7831375002861023, -0.13338495790958405, -0.7628200650215149, 0.6458665728569031, 0.45543453097343445, 0.45739901065826416, -0.7275379300117493, 1.0128000974655151, -0.17648090422153473, -0.3189307749271393, 0.8529040217399597, 0.24840138852596283, 0.026101676747202873, -0.5635202527046204, -1.2219046354293823, 0.40458378195762634, 0.32116714119911194, -0.5930785536766052, -0.6012105345726013, -0.5562687516212463, -0.8994473218917847, -0.3986169993877411, 0.5923820734024048, -0.5965167880058289, 1.2891556024551392, -0.6805084347724915, -0.8255268335342407, 0.8319092988967896, 0.3344893753528595, 0.1290624439716339, 0.9083065390586853, -0.15470871329307556, -0.46008822321891785, -0.7787914872169495, -0.17092742025852203, 0.7385926842689514, 0.7865243554115295, -0.4228813052177429, 0.19939543306827545, 0.21493278443813324, -0.33662667870521545, -0.04453298822045326, -0.22288745641708374, 0.46655547618865967, -0.48094311356544495, -0.47971025109291077, 0.4233299493789673, 0.47462084889411926, 0.25795066356658936, 0.02591054141521454, -0.34532299637794495, -0.9593034386634827, 1.00372314453125, 0.3616735339164734, 0.11255615949630737, -1.2655344009399414, -1.386313557624817, 0.06846567988395691, -0.3850421905517578, 0.17727813124656677, -1.1572216749191284, 0.41606947779655457, -0.2334260493516922, 0.4791768193244934, 0.11596589535474777, -1.3184512853622437, 0.23703083395957947, -0.1807669699192047, -0.6730031967163086, 0.040630046278238297, 0.4199208915233612, 1.0759871006011963, -0.7895985245704651, 0.6807274222373962, -0.2594164311885834, 0.20936155319213867, -1.0358326435089111, 1.000397801399231, -0.6061236262321472, 0.4027223587036133, 0.28875041007995605, -0.13148781657218933, 0.323393315076828, -0.40950489044189453, -0.23120617866516113, -0.8575530648231506, 0.43991103768348694, 0.3146957755088806, -0.40772736072540283, 1.6200553178787231, -0.37240085005760193, 0.5251805186271667, -0.12302903085947037, -0.7133652567863464, 0.22895978391170502, 0.3850536048412323, -0.17447064816951752, -0.2753954231739044, 0.6134016513824463, 0.25799819827079773, -0.4934738874435425, 0.3793593943119049, 0.8947532176971436, 0.4896441698074341, -0.44545960426330566, 0.3116931915283203, 0.9552265405654907, -0.30381542444229126, 0.13838660717010498, -0.12907227873802185, 0.37784498929977417, 0.4881347417831421, 0.06915172189474106, 0.05083994194865227, 0.102028988301754, -0.7459894418716431, -0.19838738441467285, 0.9353477954864502, -0.06580605357885361, 1.0999900102615356, 0.32354825735092163, -0.7936256527900696, -0.7414143085479736, -0.31090253591537476, 0.6647302508354187, 1.872678518295288, 0.12748384475708008, 0.09918525069952011, -0.8454474806785583, -0.8049104809761047, -0.24947009980678558, -0.3700491786003113, -0.8110235929489136, -0.3761483132839203, -0.07410462945699692, -1.254991054534912, 0.8025849461555481, 0.1687077283859253, 1.2519723176956177, -0.16592900454998016, 0.4407513737678528, -0.252543568611145, 0.4443487226963043, -0.6972578167915344, -1.1448014974594116, 0.3974663317203522, 0.16506674885749817, -0.12662795186042786, -0.005564514081925154, -0.17376567423343658, 0.3530019521713257, -0.44574540853500366, 0.9007057547569275, -0.4738905131816864, -0.4098690450191498, 0.291596382856369, 0.32504063844680786, -0.9997377395629883, -0.9268167614936829, 0.37451648712158203, 0.0408560112118721, 0.012980769388377666, 0.7073178887367249, 0.5398732423782349, -0.05894479528069496, 0.6801602244377136, -0.8675769567489624, -0.21111612021923065, -0.08586513996124268, 0.5107144117355347, 0.8991576433181763, -0.039781078696250916, 0.5583235621452332, -1.2125133275985718, 1.2139965295791626, -0.027755849063396454, -0.4232981503009796, 0.17917120456695557, -0.5828064680099487, -0.5364949107170105, 0.6693807244300842, -1.0603073835372925, -0.34541264176368713, -0.7869469523429871, 0.05600925534963608, -0.5499740839004517, -0.2321302592754364, -0.16756488382816315, 0.46679171919822693, 0.06286890059709549, 1.0269966125488281, 0.07075675576925278, 0.5219777226448059, -0.10952425003051758, 0.7339125275611877, -0.8389500379562378, 0.7186923027038574, -0.1413826197385788, 0.24830856919288635, 0.2566942572593689, -0.42637962102890015, -0.4775102436542511, -0.5104344487190247, -0.3455280661582947, -0.24416206777095795, -0.2648324966430664, 0.07693896442651749, -0.9646909832954407, -0.3104894161224365, 0.037185102701187134, -0.7436317205429077, -0.3813016414642334, -0.25107690691947937, 0.03131169080734253, -0.5811957120895386, -1.3545138835906982, -1.1394906044006348, -0.04618893936276436, -0.057409290224313736, -1.370284080505371, 0.3146851658821106, 0.02087661251425743, -0.17249837517738342, -0.408894807100296, -0.5377528667449951, -0.432440847158432, 0.9871915578842163, -0.8203632831573486, 0.6995585560798645, -0.21934956312179565, -0.23123060166835785, -0.11226215213537216, -0.2872728109359741, 1.2043925523757935, -0.23322166502475739, -0.05756726115942001, -1.4040755033493042, -0.053545646369457245, -0.3029197156429291, -0.8346146941184998, 0.6785255074501038, 0.05148516967892647, 0.47964194416999817, 0.5144495368003845, -0.06636451184749603, 0.7955575585365295, 1.5002961158752441, -0.8087879419326782, -0.08055189996957779, 0.22959065437316895, 0.9389914870262146, 0.32930028438568115, -0.12712395191192627, 0.13514283299446106, -0.13612353801727295, -0.1515154093503952, 0.04086372256278992, -0.28011658787727356, -0.6022987961769104, -0.6963541507720947, 0.2967185080051422, 1.1255160570144653, 0.5774964094161987, 0.20338013768196106, -1.048495888710022, 0.6891289353370667, -1.268151044845581, -0.23498520255088806, 0.9322561025619507, 0.4547446370124817, 0.48358723521232605, -0.28712037205696106, -0.1213626116514206, -0.5089561939239502, 0.7003687620162964, 0.16876202821731567, -0.4757603108882904, -0.21928340196609497, 0.17710603773593903, 0.023984864354133606, 0.558784544467926, 0.20024435222148895, -0.760208785533905, 0.6194710731506348, 14.494234085083008, 1.0413658618927002, -0.3020961582660675, 0.984705924987793, 0.9147651791572571, 0.36121082305908203, -0.038762416690588, -0.4248185455799103, -0.9048748016357422, 0.05741997808218002, 0.4249074459075928, 0.29868608713150024, 0.8013186454772949, 0.30669111013412476, -0.028163256123661995, 0.19783689081668854, -0.5587316751480103, 1.0368582010269165, 0.32515332102775574, -1.15914785861969, 0.23130033910274506, -0.07257456332445145, 1.2716643810272217, 1.0727059841156006, 1.187809705734253, 0.8742900490760803, 0.36756259202957153, -0.3731556534767151, 0.5455427169799805, 0.05394861847162247, 1.1879589557647705, -0.023125620558857918, 0.5901646614074707, 0.4279597997665405, -0.2880619466304779, -0.09787043929100037, -1.1203057765960693, -0.8099982738494873, 0.01971493475139141, 0.2070368081331253, -0.4212047755718231, -0.42843443155288696, -0.026702305302023888, 0.545994222164154, -0.06644000858068466, 0.37611472606658936, -0.15019221603870392, 0.5099537372589111, -0.14885836839675903, 0.34196335077285767, 0.45314350724220276, 0.18587936460971832, 0.14663559198379517, -0.04898235946893692, 0.10558601468801498, -0.41020381450653076, 0.4468950033187866, 0.5131082534790039, -1.0418715476989746, -0.2754935324192047, -0.23325318098068237, 0.33203884959220886, -0.4900834262371063, 0.2541678249835968, 0.45636776089668274, 0.22561495006084442, -0.6558416485786438, 0.27908608317375183, 0.42506828904151917, 0.6444317102432251, -0.29255878925323486, 0.3020247519016266, 0.4297002851963043, -0.5504919290542603, -0.22830957174301147, 0.5244803428649902, -0.5082213282585144, -0.5194898247718811, -0.512640118598938, -0.09849461168050766, 0.0634966567158699, -0.8011800646781921, -1.4564809799194336, 0.9270806908607483, -0.41279518604278564, -0.6473292112350464, 0.5433735847473145, -0.6974177956581116, 0.02343563735485077, 0.7877553105354309, -1.4725245237350464, -0.8929502367973328, 0.14423877000808716, -0.4394626021385193, -0.3988989591598511, -0.5455475449562073, 0.986601710319519, 0.43230941891670227, -0.2775081694126129, 0.45933058857917786, 0.4093102216720581, 0.2711239755153656, 0.5673253536224365, -0.5631002187728882, 1.0589646100997925, 0.13378408551216125, -0.26826897263526917, -0.042200133204460144, -0.11341413855552673, 0.2158919721841812, -0.06894879788160324, -0.6558915376663208, -0.09533579647541046, -0.5850609540939331, -0.3074604868888855, -0.5014357566833496, -0.9911991357803345, 0.3360203802585602, 0.48038962483406067, 0.33631864190101624, 0.3786625564098358, 0.18008360266685486, -0.9226714968681335, -0.14757350087165833, -0.844272255897522, 0.0842878445982933, 0.5194646716117859, -1.0563677549362183, -0.21749050915241241, -0.006226902827620506, 0.043598275631666183, -1.280774712562561, -0.5289819836616516, -0.16727466881275177, -0.1803174912929535, -0.27776339650154114, 0.9718461036682129, -0.5312008261680603, 0.45422375202178955, 0.7260168194770813, -0.15295206010341644, -1.3012696504592896, 0.05571398884057999, -0.8156508207321167, 0.7440614700317383, 0.38451525568962097, 0.35176628828048706, -0.3964892625808716, 0.43871524930000305, 0.6159636974334717, 0.2169351875782013, -0.09798559546470642, -0.4904564321041107, -0.6170352101325989, 0.17665980756282806, -0.8762508630752563, 0.20744933187961578, 0.061068564653396606, -0.5948538780212402, -0.010529224760830402, 0.31512513756752014, 0.3357275724411011, -0.23740169405937195, -1.1558712720870972, 0.5033186078071594, 0.231328547000885, -0.11453362554311752, -0.21825574338436127, -0.4238540828227997, -1.5507241487503052, 0.007932527922093868, -1.3847041130065918, 0.07751516252756119, -0.2108103334903717, -0.8221719264984131, -0.13484540581703186, -0.37598368525505066, -0.31254586577415466, 0.3852485120296478, 0.20099036395549774, 0.07111961394548416, -0.1383625715970993, -0.31340092420578003, 0.8383179306983948, 1.1851481199264526, -0.7996697425842285, -0.12462030351161957, 0.15123829245567322, -0.0057446314021945, 0.5846018195152283, 0.5680283904075623, -0.6401426196098328, -0.9969297051429749, -1.3083432912826538, 0.0505404993891716, -0.6175493001937866, 0.027702899649739265, -0.7121463418006897, 0.5992165207862854, 0.3837633430957794, 0.08709203451871872, 0.1368512064218521, 0.4319843053817749, -1.3893345594406128, -0.5488421320915222, -0.05502717196941376, -0.9201133251190186, -0.14520198106765747, 0.0848497673869133, -0.04966285824775696, -0.4412735402584076, 0.5935275554656982, 0.42186540365219116, -1.0158796310424805, -0.8852213025093079, 0.4486340284347534, -0.13421791791915894, 0.4621366560459137, -0.19326041638851166, 0.2184392660856247, -1.5003507137298584, -0.4319247305393219, -0.6213861107826233, 0.2153521180152893, -0.19245095551013947, 0.8130900263786316, -0.04673184081912041, -1.5595507621765137, 0.28199005126953125, 0.7220480442047119, 0.20575203001499176, 0.1761435717344284, 0.504131019115448, 0.3515280485153198, -0.38376399874687195, -0.03669576346874237, 0.0648794174194336, 0.0653398334980011, -0.3935021758079529, 0.13110356032848358, 1.0532041788101196, -0.26585155725479126, 0.09178546816110611, 1.259621500968933, -0.0979999229311943, -1.2766987085342407, 0.5122700333595276, -0.9652994275093079, 0.03552074357867241, -0.14115235209465027, 0.5086483955383301, 0.22074154019355774, 0.3519746661186218, -0.17041081190109253, -0.049106426537036896, 0.12360786646604538, -0.28664666414260864, -0.5604182481765747, 0.38644933700561523, -0.43046513199806213, 0.1324446201324463, 0.6250134110450745, 1.3041510581970215, -0.9901023507118225, -1.4431220293045044, -1.1801916360855103, -0.48992326855659485, -0.32075998187065125, 0.08220463246107101, -0.47309091687202454, -0.541144847869873, 0.8234323263168335, 0.8061956167221069, 0.34794944524765015, 0.09894605726003647, -0.29846763610839844, 0.1130591481924057, 0.6786055564880371, -0.21683356165885925, -1.2750167846679688, -0.2405163049697876, 1.2434567213058472, 1.1532095670700073, -1.412078619003296, -0.03936373442411423, -0.5865367650985718, -0.6084688901901245, 0.8120723366737366, 0.6357527375221252, -0.425314724445343, 1.257628083229065, -0.5199028849601746, 0.5412668585777283, 0.4462822675704956, -0.6429010033607483, -0.5514044761657715, 0.9617536664009094, 1.2745596170425415, 0.20441512763500214, 0.06742627918720245, 0.34870943427085876, 0.8251699209213257, 0.07200329005718231, 0.04932728782296181, 0.6870786547660828, 0.19999650120735168, 0.0128219835460186, -0.1496514081954956, -0.3467836081981659, 0.787453830242157, -0.8646603226661682, -0.03080037608742714, 0.2813723385334015, 0.8268275856971741, 0.8453737497329712, 0.4900342524051666, 0.8302134275436401, -0.3981649577617645, 0.8898442387580872, 0.433467835187912, 0.37503716349601746, -0.5872191190719604, -0.18677346408367157, 0.12047024071216583, -0.8022212386131287, 0.09569750726222992, -0.27357566356658936, -0.3230917751789093, 0.004312385339289904, -0.013620125129818916, 0.2978968024253845, -0.522893488407135, 0.45531323552131653, 0.9216133952140808, 0.4291726350784302, 0.7561721205711365, -0.33722037076950073, -0.4574474096298218, -0.6935057044029236, -1.1126378774642944, 0.003638025838881731, 0.29785865545272827, -0.0053849369287490845, -0.3604854345321655, -0.06441192328929901, 0.0930178090929985]}, "authors": [{"authorId": "2242179580", "name": "Hao Chen"}, {"authorId": "2145270616", "name": "Jindong Wang"}, {"authorId": "2257395734", "name": "Ankit Shah"}, {"authorId": "2249539790", "name": "Ran Tao"}, {"authorId": "2257325005", "name": "Hongxin Wei"}, {"authorId": "1576441343", "name": "Xingxu Xie"}, {"authorId": "67154907", "name": "Masashi Sugiyama"}, {"authorId": "2240348802", "name": "Bhiksha Raj"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "8665c864d71df1e918d2010778fc06712f4e5550", "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations"}, {"paperId": "604b4b4d55587ab6cdc7a6a70e66a193439f4445", "title": "On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training"}, {"paperId": "f9570989919338079088270a9cf1a7afc8db8093", "title": "DataComp: In search of the next generation of multimodal datasets"}, {"paperId": "6baabc2f8824a781ded3d620fb847b6d717f5b4d", "title": "Towards Efficient Task-Driven Model Reprogramming with Foundation Models"}, {"paperId": "fdaacabb69ca054d1d9acf2f1409c083672adc4a", "title": "Exploring Vision-Language Models for Imbalanced Learning"}, {"paperId": "cfb5ef050cad787196d60a55a9583107d02e58e3", "title": "BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning"}, {"paperId": "0ae93d5c8528328c9e59fa92e86ea9c6de4a62d0", "title": "The Role of Pre-training Data in Transfer Learning"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "16de2006e2960ba410772c6b6d460b83c0a5cc4b", "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning"}, {"paperId": "e35c225e7b6da0568d8e723eb5380294e76e5cc0", "title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models"}, {"paperId": "d15091e73f7295ba8c0bdbabe0b7188307c96039", "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "fdd69dd6ad2be28c6864b7330ccc1311e284cc5c", "title": "USB: A Unified Semi-supervised Learning Benchmark"}, {"paperId": "e221c769957a8eef2836a64cede88401129d86e9", "title": "Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP"}, {"paperId": "098669d74f7a84b711718dccef7070052f22ca65", "title": "Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models"}, {"paperId": "a4c99a1f69909443b3ca895cdd3dc78070c03377", "title": "FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning"}, {"paperId": "576299b1e8a0e624dbfa0e7d29eb588d527a80aa", "title": "When does dough become a bagel? Analyzing the remaining mistakes on ImageNet"}, {"paperId": "c37d0b258386293097fa3f71f971dc5dfceb4684", "title": "Data Contamination: From Memorization to Exploitation"}, {"paperId": "273f0da6c9064cc0c1b30eda702e33f79c9f60bb", "title": "Selective-Supervised Contrastive Learning with Noisy Labels"}, {"paperId": "d237e110e233e13474e68ccd6580971926616577", "title": "Robust Training under Label Noise by Over-parameterization"}, {"paperId": "885b701960e880391a2a311ed885c92f79e024cc", "title": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better"}, {"paperId": "29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d", "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution"}, {"paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d", "title": "Quantifying Memorization Across Neural Language Models"}, {"paperId": "2bc8100b244cc4bf5f2ba2ee5a4d62f13bdf44bc", "title": "Investigating Why Contrastive Learning Benefits Robustness Against Label Noise"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "303cee259e056ee2c27f5609ff5b0001d0e4317b", "title": "Margin Calibration for Long-Tailed Visual Recognition"}, {"paperId": "da4261a957eaa96bf626e9641ef68ebed1d5333f", "title": "RedCaps: web-curated image-text data created by the people, for the people"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"}, {"paperId": "ab8510d2675a94c9d29e49d949820a2eb136df10", "title": "Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations"}, {"paperId": "34ddedc56636cffd6582570049747bd5dfa0ae5b", "title": "Mitigating Memorization of Noisy Labels via Regularization between Representations"}, {"paperId": "4b842ba29f6d244b9f456056a2d7efab9e4903a5", "title": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "a01ac66f5f66a2b23152f631b920972e4407275c", "title": "Self-supervised Learning is More Robust to Dataset Imbalance"}, {"paperId": "071f5565484ee844fe69b0e32e502e86ceb5196a", "title": "Deep Long-Tailed Learning: A Survey"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "f454f6b5f2ca9749ddf442eb5134612ef7f758c1", "title": "ResNet strikes back: An improved training procedure in timm"}, {"paperId": "9289826beb6206eeaf500105f7329d6d5a495d8a", "title": "Robust fine-tuning of zero-shot models"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "c1816ce71d81cbcdc257661c5ff75f7004820f22", "title": "Mandoline: Model Evaluation under Distribution Shift"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "c99cf0a45e0e01efec62f3bc747774d1efb08cc1", "title": "To Smooth or Not? When Label Smoothing Meets Noisy Labels"}, {"paperId": "0d0cf5f64c052aa7edc5bb638203616a620557f6", "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning"}, {"paperId": "0d5406775fab3e71848908327fb5504df5f60f92", "title": "ImageNet-21K Pretraining for the Masses"}, {"paperId": "8a9d84d86ac0d76e63914802f9738325c3bece9c", "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "6dad270ed83f5aa0a93ff3cb9e78fd069c78c249", "title": "FINE Samples for Learning with Noisy Labels"}, {"paperId": "2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c", "title": "Understanding deep learning (still) requires rethinking generalization"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "5b0d1cf668c92f23fa9be2e5efe6d8c8ab9376b3", "title": "Provably End-to-end Label-Noise Learning without Anchor Points"}, {"paperId": "09903d10c025ecebc0238e8f6ec91245255b40c3", "title": "Learning Noise Transition Matrix from Only Noisy Labels via Total Variation Regularization"}, {"paperId": "08bbe9ee1271ff8e0fd7bdbb1d87a995cc4509c6", "title": "Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "5ffe9b1d8219438f0343995ad3ea1a888e3d9f8e", "title": "Learning From Noisy Labels With Deep Neural Networks: A Survey"}, {"paperId": "0593d3da080f886fa020541a1e1c675f4fdd37c6", "title": "On Robustness and Transferability of Convolutional Neural Networks"}, {"paperId": "c67a535e74bd9f323abb66046d0ed3ae8f8cea69", "title": "Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources"}, {"paperId": "c9b08639a28ab70a06ba9a09eaae98b2fe3dc6c9", "title": "Early-Learning Regularization Prevents Memorization of Noisy Labels"}, {"paperId": "022622e024890d6e044ac50e2da6b44c59bdf418", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"}, {"paperId": "07cca761749bfe21c2d096ff60f32b574d5c84c4", "title": "Normalized Loss Functions for Deep Learning with Noisy Labels"}, {"paperId": "515a4cff44d879b56adec7d16a0b70610ece665d", "title": "On Adversarial Bias and the Robustness of Fair Machine Learning"}, {"paperId": "f5c8464032a936451b222be1984cabf42d6adfa8", "title": "Are we done with ImageNet?"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ce435482acc0e195be8d8f002b2655b4c7b08be6", "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "299847adf3ee558a760475ffa364facac3ebbb16", "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence"}, {"paperId": "bc51622358d8eea83248ef29402fe10640d07ba6", "title": "Big Transfer (BiT): General Visual Representation Learning"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d", "title": "Self-Training With Noisy Student Improves ImageNet Classification"}, {"paperId": "cbaaa1154c491f9da2f050d3c22970e15bb7b52b", "title": "Confident Learning: Estimating Uncertainty in Dataset Labels"}, {"paperId": "dcc4c760c3f1cb17f953c487190b735030c33b78", "title": "Decoupling Representation and Classifier for Long-Tailed Recognition"}, {"paperId": "3ba8d3060731d64cd46d27e933cbdfb8b7853f4b", "title": "Symmetric Cross Entropy for Robust Learning With Noisy Labels"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7", "title": "Natural Adversarial Examples"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "3f1edf11bc3ca1f0ed116cf72ff51d092ad8644b", "title": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian"}, {"paperId": "3b37790c56d310e6d88e22103321235be28d9f02", "title": "Do Image Classifiers Generalize Across Time?"}, {"paperId": "0ddada21984cd7e1a8aa9af41bddbd2660865f1a", "title": "Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02", "title": "Learning Robust Global Representations by Penalizing Local Predictive Power"}, {"paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"paperId": "42ed4a9994e6121a9f325f5b901c5b3d7ce104f5", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "3217278e346fefbd34f0727321059c7ea5792612", "title": "Moment Matching for Multi-Source Domain Adaptation"}, {"paperId": "8a30ce1b79d65658d58c154b944e50253a9d79cf", "title": "Introducing Eurosat: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification"}, {"paperId": "2a96afaf3261a87f0daa51699b4b3cf169e092c4", "title": "Rotation Equivariant CNNs for Digital Pathology"}, {"paperId": "57471cee5c2ab45639fdc4d1f2f93ed8ef574308", "title": "Multi-Label Transfer Learning for Multi-Relational Semantic Similarity"}, {"paperId": "8a8cfa45b4c0d071fbffa091c02670b19c94b693", "title": "Do Better ImageNet Models Transfer Better?"}, {"paperId": "1e1855ca80e8ac3de0e169871f320416902e9ad1", "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels"}, {"paperId": "cf8c493079702ec420ab4fc9c0fabb56b2a16c84", "title": "SciTaiL: A Textual Entailment Dataset from Science Question Answering"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "e061d23b68e7d4aac5aece4794c044c80e638dca", "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels"}, {"paperId": "9c88c2357abcd58cc330179c1965fe0a8c067ebc", "title": "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification"}, {"paperId": "59e94c9f21937643678ff494901f3d8b22af4e2f", "title": "Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "179765729fc1e269393617795507607c29a66a8e", "title": "Remote Sensing Image Scene Classification: Benchmark and State of the Art"}, {"paperId": "33d6aa6c41ce3000161d9b5eea910a5b78e14330", "title": "Robust Loss Functions under Label Noise for Deep Neural Networks"}, {"paperId": "3eda43078ae1f4741f09be08c4ecab6229046a5c", "title": "NewsQA: A Machine Comprehension Dataset"}, {"paperId": "bc550ee45f4194f86c52152c10d302965c3563ca", "title": "Training deep neural-networks using a noise adaptation layer"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80", "title": "Identity Mappings in Deep Residual Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "5d6ae67e569f974360b107060c23cbb8a13b0687", "title": "Learning from massive noisy labeled data for image classification"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "354c029c88be2bbc27dfd2e2e729c0ae622511e6", "title": "YFCC100M"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "8e3f12804882b60ad5f59aad92755c5edb34860e", "title": "Food-101 - Mining Discriminative Components with Random Forests"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "a83cec6a91701bd8500f8c43ad731d4353c71d55", "title": "3D Object Representations for Fine-Grained Categorization"}, {"paperId": "18c125ce0f64e85577f7d30132cf0e92ec664bf4", "title": "Describing Textures in the Wild"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "522d65a3db7431015aeaa201a7fc4450a57e40c3", "title": "Fine-Grained Visual Classification of Aircraft"}, {"paperId": "84b50ebe85f7a1721800125e7882fce8c45b5c5a", "title": "Cats and dogs"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "639174f32a71ecfe9041ad05ff30eb39bd4977bf", "title": "ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "3bf2e6941dbb87ac0d2c771c159e1e27366a26e3", "title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"}, {"paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "015ca32bca81dbda1e2e432445eef798582236e1", "title": "Conference Paper"}]}