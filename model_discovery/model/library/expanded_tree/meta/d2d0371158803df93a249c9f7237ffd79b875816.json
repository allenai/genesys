{"paperId": "d2d0371158803df93a249c9f7237ffd79b875816", "abstract": "Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.11197", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models."}, "embedding": {"model": "specter_v2", "vector": [0.5242244601249695, 0.786296010017395, -0.4522469639778137, 0.04442134499549866, 0.018375515937805176, -0.09596895426511765, 0.6653376221656799, -0.32011279463768005, -0.19273845851421356, -0.09831807017326355, 0.6603144407272339, 0.0533449724316597, 0.6912499666213989, 0.26743048429489136, -0.4108322858810425, 0.11601550132036209, -0.8689797520637512, -0.06509366631507874, 0.2183489352464676, -0.35391536355018616, 0.05572338029742241, -0.8408169150352478, -0.90882807970047, 0.20413155853748322, 0.40310773253440857, 0.4627443552017212, 0.35322555899620056, 0.6667408347129822, -0.5436121821403503, 0.7111827731132507, 0.3543493151664734, 0.04710639640688896, 0.34325405955314636, -0.6801713705062866, -0.3854883909225464, -0.28787726163864136, 0.12768137454986572, -0.05198463797569275, -0.6329700350761414, 0.691239058971405, -0.34587597846984863, 0.3204902708530426, 0.2888270914554596, -0.22517256438732147, 0.4412490129470825, 1.0413384437561035, 0.6958752274513245, 0.8077225685119629, -0.041472624987363815, -0.37700894474983215, 1.213710904121399, -1.148921251296997, 0.21198119223117828, 1.244004726409912, 0.32636961340904236, 0.6699268221855164, -0.3974713683128357, -0.6724019646644592, 1.0001224279403687, 0.3130611181259155, -0.6169223189353943, -0.7246954441070557, 0.15331609547138214, -0.16744761168956757, 1.9424052238464355, -0.13230350613594055, 0.2508595883846283, 0.5305880308151245, 0.08013283461332321, 1.3745684623718262, -0.26798567175865173, -0.6181021332740784, 0.07177416980266571, -0.19997473061084747, 0.23835676908493042, 1.0706508159637451, -0.36836400628089905, 0.5952747464179993, -1.2416484355926514, -0.12872625887393951, 0.6648203730583191, 0.17678292095661163, 0.21666380763053894, -0.3893229067325592, -0.29218950867652893, 0.6880483031272888, 0.42071452736854553, 0.9119337797164917, -0.3021615445613861, 0.9323766827583313, 0.5985293388366699, 0.0983157828450203, 0.15136879682540894, 0.3562287986278534, -0.052245110273361206, 0.2586287558078766, -0.757456362247467, -0.20781272649765015, -0.2182530164718628, 0.7531378865242004, -0.009399062022566795, 0.8910958766937256, -0.5249528288841248, 0.3400043845176697, 1.4438855648040771, -0.28740453720092773, 0.6085340976715088, -0.767359733581543, 0.09781143069267273, -0.32610490918159485, 0.006434034090489149, -0.47552961111068726, -0.24286693334579468, -0.5523251891136169, -1.017500638961792, -1.1622003316879272, -0.7996442914009094, 0.12889423966407776, -0.5776021480560303, 1.0993443727493286, -0.2985835671424866, 0.502651572227478, -0.18674489855766296, 0.2104015052318573, 0.2261151671409607, 0.9320665001869202, 0.3501076400279999, -0.28177687525749207, 0.8448706269264221, -0.9367218613624573, -0.6366669535636902, -1.359326720237732, 0.7838628888130188, 0.3410317003726959, -0.3028004765510559, -0.45717039704322815, -1.629756212234497, -1.0886642932891846, -0.8574946522712708, 0.48665907979011536, -0.045051317662000656, 0.016515731811523438, 0.8741472959518433, 0.15840137004852295, -0.8292262554168701, 0.8848330974578857, -0.7763012647628784, -0.23490649461746216, 0.5219172835350037, 0.17326685786247253, 0.2854609787464142, -0.04704022407531738, -1.3322194814682007, 0.5867293477058411, 0.34876352548599243, -0.3898700177669525, -0.39251044392585754, -0.7093333005905151, -1.4443206787109375, 0.2566758692264557, 0.2081550806760788, -0.9339485764503479, 1.2612190246582031, -0.4642956256866455, -1.5766792297363281, 0.46899572014808655, -0.5132349729537964, -0.2056887149810791, -0.1068447008728981, 0.0316561758518219, -0.2624579071998596, -0.3203466832637787, -0.3685046434402466, 0.37378308176994324, 0.5710806846618652, -0.1496313363313675, -0.11480654031038284, 0.04220297560095787, -0.708340048789978, -0.20217038691043854, -0.16385021805763245, 0.9642696380615234, -0.1854638159275055, -0.27363526821136475, 0.48983851075172424, 0.6975812315940857, -0.27968108654022217, -0.5517845749855042, -0.2264346480369568, -1.0481626987457275, 0.34205302596092224, 0.11720140278339386, 1.1502119302749634, -0.8254950642585754, -0.46855297684669495, -0.4023647606372833, -0.22224843502044678, 0.10136401653289795, -0.7332028150558472, 0.7677435874938965, -0.6468424797058105, 0.011006185784935951, -0.51396644115448, -0.9956492781639099, -0.25769534707069397, -0.2951902449131012, -0.47711417078971863, -0.05366601422429085, 0.41179603338241577, 1.222496747970581, -1.1152596473693848, -0.045567262917757034, -0.06826487928628922, 0.06532023847103119, -1.0793920755386353, 1.409293532371521, -0.27501073479652405, 0.20618633925914764, 0.0907662957906723, -0.5599305033683777, -0.20502030849456787, -0.4331344664096832, 0.6056683659553528, -0.49565404653549194, -0.04186196252703667, 0.8832042217254639, -0.41634535789489746, 1.7286999225616455, -0.3507962226867676, 0.6190102100372314, -0.1251906156539917, -1.113189458847046, 0.39743128418922424, 0.44540902972221375, 0.15163898468017578, -0.4663339853286743, 0.18900200724601746, -0.15130148828029633, -0.5770930051803589, 0.28334808349609375, 0.3717747926712036, 0.7225131392478943, -0.2794927656650543, 0.23566460609436035, 0.7903012633323669, -0.04600541666150093, 0.43026748299598694, 0.6933913826942444, 0.9909005761146545, 0.44963976740837097, 0.895287275314331, -0.0428006611764431, 0.36442074179649353, -1.0354810953140259, 0.41653260588645935, 0.3092251718044281, 0.5247481465339661, 0.7347962260246277, 0.15478931367397308, -0.5360709428787231, -0.4912480115890503, 0.07849331945180893, 0.9502764344215393, 1.4164742231369019, -0.2076018750667572, -0.10874342173337936, -0.8411267995834351, -0.3152657449245453, -0.5927451252937317, 0.20605617761611938, -0.2877778708934784, -0.7350262999534607, -0.7685264945030212, -0.7853384613990784, 0.5457751154899597, 0.5023630261421204, 0.9872905611991882, -0.669843852519989, -0.3646853566169739, 0.22582294046878815, 0.2394123375415802, -0.676109790802002, -0.7398054003715515, 0.55373615026474, -0.8603941202163696, 0.014600569382309914, 0.1782057285308838, -0.1739361584186554, 0.09350491315126419, -0.516516387462616, 1.0828962326049805, -0.7127565145492554, -0.10703156143426895, 0.07371588796377182, 0.5684267282485962, -0.5565717220306396, -0.862111508846283, 0.4667101800441742, 0.13244330883026123, 0.01712820678949356, 0.02895306423306465, 0.49499964714050293, -0.291332483291626, 0.19311875104904175, -0.12676112353801727, 0.39491698145866394, 0.03228702396154404, 0.19722464680671692, 0.08441717177629471, -0.6939810514450073, 0.47189560532569885, -1.1737158298492432, 0.2706569731235504, -0.11284000426530838, -0.26242533326148987, -0.017493370920419693, -0.3405548334121704, -0.0037814711686223745, 0.2221730649471283, -0.5625482797622681, -0.46188321709632874, -0.6491212248802185, 0.2930625081062317, -0.2690018117427826, -0.305705189704895, 0.11477851122617722, 0.4829462468624115, 0.1818998008966446, 0.35160622000694275, 0.8623022437095642, 0.6572326421737671, 0.09277627617120743, 0.18290987610816956, -0.9368533492088318, 0.4995911121368408, 0.632464587688446, 0.06196373328566551, 0.02479945309460163, -0.23373743891716003, -0.6453351974487305, -0.5025953650474548, -0.5344685316085815, -0.13904745876789093, -0.3130580484867096, 0.2016710489988327, -0.6947460770606995, -1.0258374214172363, -0.12413340061903, -1.2446157932281494, -0.49364981055259705, -0.09219866245985031, -0.30616116523742676, -0.592827558517456, -0.7171109914779663, -1.1500345468521118, -0.9556584358215332, -0.5781579613685608, -0.5503939986228943, 0.16069404780864716, -0.019494593143463135, -0.6734067797660828, -0.5867636203765869, 0.2231592833995819, -0.4188799262046814, 0.9205120205879211, -0.888930082321167, 0.4970037341117859, 0.01079762727022171, -0.533866822719574, -0.19683295488357544, 0.4417271316051483, 0.36118417978286743, -0.2044951319694519, -0.19890613853931427, -0.7006193995475769, 0.09322487562894821, -0.27211570739746094, 0.050018567591905594, 0.356302946805954, 0.5511292219161987, 0.48748037219047546, -0.1687171310186386, -0.5092880725860596, 0.289374440908432, 1.1969419717788696, -0.19701749086380005, 0.07927112281322479, -0.12107305228710175, 0.7835046648979187, 0.20847897231578827, -0.47992199659347534, 0.6173705458641052, 0.2717352509498596, 0.688385546207428, 0.37852296233177185, -0.12357445806264877, -0.05865122750401497, -0.6817043423652649, 0.8625299334526062, 1.838253140449524, 0.42144423723220825, 0.36478444933891296, -0.5680957436561584, 0.6274628043174744, -1.449018120765686, -1.0910817384719849, 0.8452283143997192, 0.6014697551727295, 0.3815050423145294, -0.6225326657295227, -0.06709466129541397, -0.326316773891449, 0.40327179431915283, 0.39846259355545044, -0.6695398092269897, -0.6892226934432983, -0.0552523098886013, 0.26293089985847473, 0.1282871812582016, 0.6151453256607056, -0.04953925684094429, 0.7214108109474182, 15.024456977844238, 0.46144160628318787, -0.009012795984745026, 0.9814490079879761, 0.6036495566368103, -0.0970861092209816, -0.11112666875123978, 0.2168647050857544, -1.121293544769287, 0.2314576804637909, 1.2870433330535889, 0.24081097543239594, 0.6342539191246033, -0.31351080536842346, 0.3299279808998108, 0.2529967129230499, -0.593273401260376, 0.6618146300315857, 0.41964179277420044, -1.5267448425292969, 0.15504823625087738, -0.12215718626976013, 0.5332624912261963, 0.5384141802787781, 0.3895202577114105, 0.6600328087806702, 0.5383480787277222, -0.1168970987200737, 0.43059980869293213, 0.49876466393470764, 0.7812159061431885, 0.32477837800979614, -0.005545584484934807, 0.8207237124443054, -0.8634548187255859, -0.3482986092567444, -0.2297848016023636, -1.0607049465179443, 0.6507043242454529, -0.14705181121826172, -0.19048282504081726, -0.39248842000961304, -0.4637628495693207, 0.9700646996498108, 0.5261698961257935, 0.47452399134635925, -0.17005202174186707, 0.9962611198425293, 0.021138830110430717, -0.0049850232899188995, 0.10886147618293762, 0.38492295145988464, 0.15422989428043365, 0.12425971031188965, -0.024257836863398552, 0.23643259704113007, -0.16117000579833984, 0.6310462355613708, -0.21754339337348938, -0.4044686555862427, -0.39727193117141724, -0.042454659938812256, 0.01849956065416336, 0.4666529893875122, 1.128704309463501, -0.22171643376350403, -0.3874521255493164, 0.22446925938129425, 0.432407021522522, 0.29992392659187317, -0.12459481507539749, 0.18617597222328186, 0.49672219157218933, -0.3850558400154114, -0.09824692457914352, 0.1925961673259735, -0.6377022862434387, -0.6128183603286743, -1.164090633392334, -0.3865028917789459, 0.4285098910331726, -0.9963025450706482, -0.30225035548210144, 0.7491429448127747, -0.16813722252845764, -0.33530858159065247, -0.1137622520327568, -0.3163411021232605, -0.4167378842830658, 0.47486403584480286, -0.7969034314155579, -0.12548081576824188, 0.2621643841266632, -0.5499762892723083, 0.00136679841671139, -0.5020269751548767, 1.0652164220809937, 0.0009658750495873392, -0.8950209617614746, 0.06105297431349754, -0.1357497125864029, -0.26914286613464355, -0.6369684338569641, -0.5251973867416382, 0.9211736917495728, 0.31189849972724915, -0.07066738605499268, 0.2638971209526062, 0.10012856870889664, 0.10491675138473511, -0.7177398800849915, -0.06291460990905762, 0.7715353965759277, -0.7933010458946228, -0.4774738848209381, -0.7267278432846069, -1.1251024007797241, 0.3024120628833771, 0.5921149253845215, -0.34454581141471863, 0.23643319308757782, 0.035728901624679565, -0.3533304035663605, -0.08480606973171234, -0.4046980142593384, 0.059899672865867615, 0.7586844563484192, -0.8304407596588135, -0.44177502393722534, -0.3235592246055603, 0.420422226190567, -0.5303549766540527, -0.3645486831665039, -0.09493698924779892, -0.11790269613265991, -0.05155249685049057, 0.9793862104415894, -0.8476195931434631, 0.45292577147483826, 0.685271143913269, 0.3037966191768646, -0.8841793537139893, -0.5220891833305359, -1.0447396039962769, -0.4822368323802948, -0.04529115930199623, 0.3708041310310364, -0.6820778250694275, 0.6857178211212158, 0.47719675302505493, -0.18972618877887726, -0.387444406747818, -0.8317728638648987, -0.2693721652030945, -0.4099206030368805, -0.3170320391654968, 0.1887490451335907, -0.006309042684733868, 0.23212146759033203, 0.41140908002853394, 0.5407790541648865, 0.2246546596288681, -0.130021870136261, -0.717200517654419, 0.23015399277210236, 0.0009822890860959888, 0.06298740953207016, -0.8288361430168152, -0.5595974922180176, -1.686766266822815, 0.11390066891908646, -1.2896167039871216, -0.026619525626301765, -0.9410297274589539, -0.23626883327960968, 0.34668004512786865, -0.4110850691795349, 0.12116250395774841, 0.2535146176815033, -0.3806280791759491, -0.45250189304351807, -0.3773166239261627, -0.611446738243103, 1.0868576765060425, 0.6363685727119446, -0.8206704258918762, -0.01729309931397438, 0.011674782261252403, 0.0595664419233799, 0.09444447606801987, 0.6199617981910706, -0.4309540092945099, -0.5252553224563599, -1.0229192972183228, 0.39897793531417847, 0.174844890832901, 0.0217296089977026, -0.6183691620826721, 0.8462048172950745, 0.31179991364479065, -0.4239804446697235, -0.17191573977470398, 0.45715105533599854, -0.706059992313385, -0.2437318116426468, 0.8071253895759583, -0.78924560546875, 0.09682635217905045, 0.23859335482120514, -0.5016993880271912, -0.32693931460380554, 0.6126773357391357, 0.12680260837078094, -1.107176661491394, -0.47494837641716003, 0.3747786283493042, -0.9859112501144409, 0.002053573727607727, -0.12295953929424286, -0.33208101987838745, -0.8928088545799255, -0.4586469531059265, -0.06592296063899994, 0.3488098978996277, -0.6690945029258728, 0.7595634460449219, 0.41289830207824707, -1.270526647567749, -0.015621699392795563, 0.5716896057128906, -0.07800039649009705, 0.06638678908348083, 0.8011674880981445, 0.40044328570365906, -0.06475681811571121, 0.7495087385177612, 0.3098960816860199, 0.11353961378335953, -0.6004521250724792, 0.04306264966726303, 0.9723864197731018, -0.2614620625972748, 0.15555495023727417, 0.7402471899986267, -0.008205735124647617, -0.8819162845611572, -0.03824141249060631, -0.9005756974220276, -0.32340922951698303, -0.2453080713748932, 0.19464211165905, 0.3589836061000824, -0.3574754297733307, 0.13095661997795105, -0.48367545008659363, 0.05063888058066368, 0.058425236493349075, -0.48351362347602844, 0.5364927649497986, -0.19065745174884796, -0.18681946396827698, 0.9680543541908264, 0.7535302639007568, -1.21931791305542, -0.9983941912651062, -0.9784310460090637, -0.3688759207725525, 0.08839090913534164, 0.4107397496700287, -0.19101642072200775, -0.6104593873023987, 0.8770948648452759, 0.5208199620246887, 0.33611708879470825, -0.01610991731286049, 0.08854207396507263, -0.06028525158762932, 0.20399950444698334, 0.026427922770380974, -0.6300087571144104, -0.4045379161834717, 1.3957005739212036, 1.4263486862182617, -0.7031942009925842, 0.036305706948041916, -0.19113211333751678, -0.4280823767185211, 0.2889576256275177, 0.25924140214920044, -0.09496041387319565, 0.7640842795372009, -0.4427167773246765, -0.11856482923030853, 0.22610071301460266, -1.5240755081176758, -0.23633027076721191, 0.6930608153343201, 0.7115863561630249, 0.7734168767929077, 0.18091081082820892, 0.17002050578594208, 0.7129548192024231, 0.45588183403015137, 0.09345878660678864, 0.18865738809108734, 0.3780824542045593, -0.31999099254608154, 0.26093095541000366, 0.11603471636772156, 0.5151044726371765, -0.2947605550289154, -0.44597765803337097, 0.20458194613456726, -0.07049589604139328, -0.23548907041549683, 0.46744513511657715, 1.1869558095932007, 0.1078101098537445, 0.5550048351287842, 0.3278576731681824, 0.6045126914978027, -0.9011110663414001, -0.426615834236145, -0.18447235226631165, -0.6941558718681335, -0.3864929974079132, -0.040450360625982285, -0.9584187269210815, -0.26959285140037537, 0.07947508990764618, 0.03399889916181564, -0.09772244840860367, 0.6208407878875732, 1.078271746635437, 0.943539559841156, 0.6716537475585938, 0.3035963773727417, -0.7515392303466797, -0.5824332237243652, -1.2326737642288208, 0.41453996300697327, -0.3993399441242218, -0.2161998748779297, -0.20026478171348572, -0.05539816990494728, -0.23008593916893005]}, "authors": [{"authorId": "46177458", "name": "Liliang Ren"}, {"authorId": "2152797401", "name": "Yang Liu"}, {"authorId": "2146294891", "name": "Shuo Wang"}, {"authorId": "2110197273", "name": "Yichong Xu"}, {"authorId": "8652308", "name": "Chenguang Zhu"}, {"authorId": "143869012", "name": "Chengxiang Zhai"}], "references": [{"paperId": "01e721df7fcf8b664deb2cdc97ff58d65553af6b", "title": "State Spaces Aren\u2019t Enough: Machine Translation Needs Attention"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "690a37a2ba67b44b012bf9aa92e6a7f7670f487f", "title": "Dynamic Sparse Attention for Scalable Transformer Acceleration"}, {"paperId": "cd16a961b16482d4aede6227acaa95183c5c555c", "title": "Language Model Pre-Training with Sparse Latent Typing"}, {"paperId": "ee42f8a43bdf2cf1e39a0e08bd7b448034cb4a0d", "title": "Why neural networks find simple solutions: the many regularizers of geometric complexity"}, {"paperId": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc", "title": "Liquid Structural State-Space Models"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "81b234a1e6da7bc8131e8585a9455dca5dd68754", "title": "Transkimmer: Transformer Learns to Layer-wise Skim"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "802a5d24c78f713e282b003d99b4afd924bd7568", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2", "title": "Primer: Searching for Efficient Transformers for Language Modeling"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "060eb1ad5da6a059320c244532ad5c9c0ab52485", "title": "Implicit Gradient Regularization"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328", "title": "Sparse GPU Kernels for Deep Learning"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "67a9dde04f367efc903b6d06097df9bdd9887ae7", "title": "Recurrent Independent Mechanisms"}, {"paperId": "2bf7c350a8280e7c593d46a60127f99b21517121", "title": "On the Variance of the Adaptive Learning Rate and Beyond"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "da6e404d8911b0e5785019a79dc8607e0b313dc4", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning"}, {"paperId": "29e944711a354c396fad71936f536e83025b6ce0", "title": "Categorical Reparameterization with Gumbel-Softmax"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "45f5e49f5e75f4b8e3ca9296c6aa45a3fcda2e76", "title": "Towards Flatter Loss Surface via Nonmonotonic Learning Rate Scheduling"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs, 2018"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "The human knowledge compression"}, {"paperId": "56c16d9e2a5270ba6b1d83271e2c10916591968d", "title": "Human Memory: A Proposed System and its Control Processes"}]}