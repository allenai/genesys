{"paperId": "713806165610c237f551a7b68e6b09b3ded75502", "abstract": "The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.", "venue": "arXiv.org", "year": 2023, "citationCount": 10, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is shown that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks."}, "embedding": {"model": "specter_v2", "vector": [0.19116675853729248, 0.6020863652229309, -0.5511789917945862, 0.2894309163093567, -0.5695604085922241, -0.19594153761863708, 1.063388705253601, 0.06220874935388565, -0.49541565775871277, 0.11199788749217987, 0.387285977602005, -0.2007473260164261, 0.7032092213630676, 0.18209275603294373, -0.3185853958129883, 0.1631506085395813, -1.081077218055725, 0.6108874082565308, 0.19738569855690002, 0.1152825877070427, -0.020485972985625267, -0.736970841884613, -1.2958492040634155, 0.12688040733337402, 0.21410270035266876, 0.6522802114486694, 0.21444293856620789, 1.1100211143493652, -0.5430417060852051, 0.34423956274986267, 0.31866884231567383, -0.1700982302427292, -0.3167945146560669, -0.37201082706451416, -0.8230589628219604, -0.2712649703025818, 0.3827019929885864, -0.19980137050151825, -0.2580952048301697, 0.7309041619300842, -0.19442015886306763, 0.5139102339744568, 0.4498484432697296, -0.6035593152046204, -0.055050816386938095, 0.9911909699440002, 0.45524126291275024, 1.0022755861282349, -0.17495208978652954, -0.3835271894931793, 1.1078721284866333, -1.3848602771759033, 0.24314026534557343, 1.9052809476852417, 0.558982789516449, 0.17730434238910675, -0.2665756046772003, -0.3319927752017975, 0.7713524699211121, 0.4538457691669464, -0.805067241191864, -0.6823276877403259, -0.5266953706741333, 0.09189727157354355, 2.3849892616271973, 0.0012533428380265832, 0.0011141481809318066, 0.4574800133705139, -0.15436075627803802, 1.7234747409820557, -0.4485509693622589, -0.854103147983551, -0.1590219885110855, 0.061720654368400574, 0.01907234638929367, 0.6377044320106506, -0.36648645997047424, -0.0005579566350206733, -0.8545724153518677, -0.337657630443573, 0.12747131288051605, -0.5615872144699097, 0.43095046281814575, 0.2615480422973633, -0.18260207772254944, 0.6883571147918701, 0.013326141983270645, 0.5485730767250061, -0.17289403080940247, 0.7863539457321167, 0.47008734941482544, -0.18391254544258118, 0.4714109003543854, 0.29210540652275085, -0.06289012730121613, -0.23796626925468445, -0.9819406867027283, 0.27434080839157104, 0.05171912908554077, 1.0122554302215576, -0.19968439638614655, 0.5123243927955627, -0.9681745767593384, 0.2635297477245331, 1.1281609535217285, 0.07787548005580902, 0.7139392495155334, -0.5333948135375977, 0.1743285059928894, -1.090095043182373, -0.06695163249969482, -0.24704574048519135, 0.08782582730054855, 0.12661993503570557, -0.5696861743927002, -1.3586444854736328, -0.5999904274940491, 0.12097484618425369, -0.6513658165931702, 0.545314610004425, -0.05689038708806038, 0.5382897257804871, 0.12684744596481323, 0.2796039581298828, 0.3915523588657379, 0.8608745336532593, 0.5448184013366699, -0.06607446074485779, 1.093092918395996, -0.8316206932067871, -0.2762773036956787, -1.244112491607666, 0.44235873222351074, -0.23294858634471893, -0.08367029577493668, -0.22705794870853424, -1.6952159404754639, -0.8143028616905212, -0.45564714074134827, -0.07649556547403336, -0.4353598654270172, 0.24760690331459045, 1.2237175703048706, 0.14598558843135834, -0.6408438086509705, 0.8079806566238403, -0.23368537425994873, -0.0815187618136406, 0.6168470978736877, 0.014347154647111893, 0.4736403524875641, -0.22181110084056854, -1.5305389165878296, 0.19141866266727448, 0.4153428077697754, -0.3938475549221039, -0.11990775913000107, -0.7450174689292908, -1.1815847158432007, 0.002572908764705062, 0.4817200005054474, -0.28594109416007996, 1.7590736150741577, -0.11373211443424225, -1.4709419012069702, 0.5921522974967957, -0.48728933930397034, -0.1996488869190216, 0.46652740240097046, -0.2838181257247925, -0.2837941348552704, -0.5014029145240784, -0.3700443208217621, 0.5871623754501343, 0.6242008209228516, 0.45616328716278076, -0.5886751413345337, 0.42415502667427063, -0.4354303181171417, -0.14866438508033752, 0.01774127222597599, 0.7163143157958984, -0.865550696849823, -0.06896848231554031, 0.04762381315231323, 0.5153850317001343, -0.10661961883306503, -0.691326379776001, -0.6419362425804138, -1.3900493383407593, 0.4796527028083801, -0.17116184532642365, 0.9039060473442078, -0.536297082901001, -0.6712235808372498, -0.28673872351646423, -0.6289536952972412, 0.2521219551563263, -0.5571116209030151, 0.4201910197734833, -0.09868025779724121, 0.348275750875473, -0.14681541919708252, -1.254152774810791, 0.05542733520269394, -0.278045654296875, -0.4152494966983795, -0.20990465581417084, 0.4144153594970703, 1.1166616678237915, -1.140289068222046, -0.26246872544288635, -0.15008322894573212, 0.3795877695083618, -1.1827774047851562, 1.3452972173690796, -0.7769514322280884, -0.0028404551558196545, -0.31165653467178345, -0.24789997935295105, 0.1273871213197708, -0.17937538027763367, 0.8855893015861511, -0.13142982125282288, -0.45989617705345154, 0.8213464021682739, -0.4203011095523834, 1.2944437265396118, -0.46093666553497314, 0.6281106472015381, -0.38300442695617676, -0.5035825967788696, -0.12538938224315643, 0.30153223872184753, -0.6493377089500427, -0.5978328585624695, 0.19124065339565277, 0.3711157441139221, -0.7225860953330994, 0.10867390036582947, 1.0509763956069946, 0.9680376052856445, -0.4376460909843445, 0.27972212433815, 0.5315552353858948, -0.03528091311454773, 0.6617172360420227, 0.40054720640182495, 0.29944637417793274, 0.7269240021705627, 0.5973788499832153, -0.0757916197180748, 0.5087357759475708, -0.631568193435669, -0.13911721110343933, 0.7198558449745178, 1.0559372901916504, 0.6849476099014282, 0.5645692944526672, -0.6861000657081604, -0.4658306837081909, 0.40671783685684204, 0.4452079236507416, 1.4993579387664795, -0.28885817527770996, 0.15487758815288544, -0.9116173386573792, -0.41229867935180664, -0.10144117474555969, 0.2332579791545868, -0.2896900177001953, -0.03360476717352867, -0.7121123671531677, -1.0147022008895874, 0.8844193816184998, 0.22273513674736023, 0.7258741855621338, -1.0715739727020264, -0.3350144326686859, -0.07389021664857864, 0.19505588710308075, -0.6729840040206909, -0.8542686104774475, 0.23521903157234192, -0.4399333596229553, 0.07316715270280838, 0.14617802202701569, 0.08801423758268356, 0.21073508262634277, -0.951176643371582, 1.049392580986023, -0.4296901524066925, -0.5374905467033386, -0.1669052690267563, 0.6644031405448914, -0.6096909642219543, -0.9977018237113953, 0.5980204939842224, 0.32591530680656433, -0.16933979094028473, 0.3118457496166229, 0.6816588640213013, 0.25146716833114624, -0.326133668422699, -0.13878382742404938, 0.2884633541107178, 0.03460411727428436, -0.11435433477163315, 0.7742292881011963, -0.4961901605129242, 0.0957060381770134, -1.419622540473938, 0.39650478959083557, -0.23600241541862488, -0.7893255949020386, 0.05279223248362541, -0.6334269046783447, -0.13103458285331726, 0.49891427159309387, -0.7581686973571777, -0.5880882740020752, -1.0516613721847534, -0.08674623817205429, -0.209241583943367, -0.441989004611969, 0.23462651669979095, 0.37193068861961365, 0.7711448073387146, -0.28664568066596985, 0.5072379112243652, 0.4197683334350586, -0.8238738775253296, 0.6144404411315918, -0.5894695520401001, 0.7956365942955017, 0.30087417364120483, -0.1727539300918579, -0.4395471513271332, 0.01809215359389782, -0.8941842913627625, -0.47814953327178955, -0.41727185249328613, -0.32597479224205017, -0.0449979342520237, 0.03161506727337837, -0.411034494638443, -1.1380352973937988, -0.008735212497413158, -1.4336178302764893, -0.4345666766166687, 0.5124039053916931, -0.06134914979338646, -0.19157204031944275, -1.0288541316986084, -1.192720651626587, -0.8938771486282349, -0.8747693300247192, -0.7870065569877625, 0.3194872736930847, 0.1276485025882721, -0.7433011531829834, -0.7939148545265198, 0.22633500397205353, -0.6896024942398071, 0.8742676377296448, -0.7704674005508423, 0.9000574946403503, 0.05171225219964981, -0.3171338140964508, -0.3110780417919159, 0.4377627968788147, 0.04633927717804909, -0.587343692779541, 0.13543641567230225, -1.0857182741165161, 0.06623683124780655, -0.9435598850250244, -0.2985496520996094, 0.3355996310710907, 0.5441983342170715, 0.7895944714546204, -0.33737868070602417, -0.7586934566497803, 0.27442434430122375, 1.2239594459533691, -0.7945238351821899, 0.03291754797101021, 0.06742308288812637, 0.8265373110771179, -0.028111465275287628, 0.04450688138604164, 0.6545742154121399, 0.06271491199731827, 0.646198570728302, 0.2665005326271057, -0.18352900445461273, -0.0008657013531774282, -0.6010568141937256, 0.5582972168922424, 1.4634920358657837, 0.3561113476753235, -0.35180050134658813, -0.8028052449226379, 0.7220824956893921, -1.2798947095870972, -0.8116374015808105, 0.5012102723121643, 0.8647627234458923, 0.6554915904998779, -0.3817364275455475, -0.08845626562833786, -0.5173272490501404, 0.367990642786026, 0.1837443858385086, -0.6176915168762207, -1.0338834524154663, 0.14280647039413452, 0.3507055938243866, 0.07652189582586288, 0.8205648064613342, -0.2972604036331177, 0.6963587999343872, 14.594513893127441, 1.014655351638794, 0.23219862580299377, 0.537140965461731, 0.8487129211425781, 0.08456285297870636, -0.3194234073162079, -0.24132446944713593, -1.5171780586242676, -0.11951087415218353, 1.4639389514923096, -0.027101630344986916, 0.45112547278404236, 0.3060060739517212, 0.152583047747612, 0.008593258447945118, -0.7450875639915466, 0.4408529996871948, 0.45343127846717834, -1.65799880027771, 0.4499119818210602, 0.07682318240404129, 0.2464723438024521, 0.6378219127655029, 0.5977895855903625, 0.8723496794700623, 0.698767900466919, -0.5645144581794739, 0.853825569152832, 0.41119319200515747, 1.0608752965927124, -0.41970187425613403, 0.6218751072883606, 0.4303683340549469, -0.8606969118118286, 0.18619194626808167, -0.6205318570137024, -1.0575352907180786, 0.12024255096912384, 0.03451569005846977, -1.0741451978683472, -0.36498624086380005, -0.16241146624088287, 0.6549793481826782, 0.22421419620513916, 0.35733315348625183, -0.08381188660860062, 0.6359663009643555, 0.2726234793663025, -0.12950709462165833, 0.461422324180603, 0.5308224558830261, 0.1022893488407135, 0.4042958617210388, 0.1576627790927887, -0.20078158378601074, -0.019055286422371864, 0.6420756578445435, -0.6576308608055115, -0.3572838604450226, -0.10832646489143372, -0.08166273683309555, 0.3974006772041321, 0.7341553568840027, 0.6847529411315918, 0.07533127814531326, -0.29832905530929565, 0.47750699520111084, 0.6975164413452148, -0.13741658627986908, -0.24140232801437378, 0.14560523629188538, 0.6565241813659668, -0.661643922328949, 0.47635236382484436, 0.9406300783157349, 0.4954970180988312, -0.14432862401008606, -1.1535950899124146, -0.38817983865737915, 0.5313963294029236, -0.5390404462814331, -0.6330815553665161, 0.8797380924224854, -0.12174042314291, -0.10684768110513687, -0.4799228608608246, -0.5341264605522156, -0.43520820140838623, 0.8626571893692017, -1.4422236680984497, -0.4746535122394562, 0.6969935894012451, -0.7410953044891357, -0.14048168063163757, 0.1092960387468338, 1.4331574440002441, -0.09524349123239517, -0.5226569771766663, 0.10792754590511322, 0.19467833638191223, -0.19385167956352234, -0.38990291953086853, -0.6791433691978455, 1.3504819869995117, 0.4160347878932953, -0.08280815929174423, -0.034316688776016235, -0.08177420496940613, 0.1627204716205597, -0.8833499550819397, -0.38381749391555786, 0.9875295162200928, -1.045653223991394, -0.6496846675872803, -0.8203111886978149, -0.7871459722518921, 0.4246574640274048, 0.5529757142066956, -0.21832019090652466, 0.11822360008955002, 0.18252921104431152, -0.25725895166397095, 0.15079332888126373, -0.33200663328170776, -0.08197436481714249, 0.13158459961414337, -0.5728290677070618, 0.01802646368741989, -0.41985049843788147, 0.39194217324256897, -1.1245448589324951, -0.572636067867279, -0.7496001124382019, 0.07717523723840714, 0.2209598869085312, 1.024311900138855, -0.5711079239845276, 0.4897117018699646, 1.0772807598114014, 0.20569674670696259, -0.8262153267860413, -0.12606120109558105, -1.0491536855697632, -0.4619835913181305, 0.4459475576877594, 0.7741835713386536, -0.11250845342874527, 0.13653823733329773, 0.37901219725608826, 0.3398352861404419, -0.24628084897994995, -0.7450661063194275, -0.4724917709827423, 0.2462897151708603, -0.634075939655304, 0.7014269232749939, -0.018687298521399498, -0.05003887787461281, 0.15425269305706024, 0.24324974417686462, 0.8199483752250671, -0.03791191056370735, -0.4341883361339569, 0.47762930393218994, -0.09276995807886124, -0.28149086236953735, -0.500184953212738, -0.004430265165865421, -1.033207893371582, 0.2365669459104538, -1.0621494054794312, 0.1599503755569458, -0.6735562086105347, -0.11054711043834686, -0.12300138175487518, 0.02715686336159706, 0.18384955823421478, 0.04167163744568825, -0.24166269600391388, -0.6074019074440002, -0.7199962139129639, -0.7302743196487427, 0.6391150951385498, 0.615449070930481, -0.5269899368286133, 0.22493518888950348, 0.015497509390115738, 0.4479049742221832, 0.1985233724117279, 0.40870437026023865, -0.5066085457801819, -0.4853827953338623, -1.4459903240203857, 0.3270292580127716, 0.1101856529712677, -0.10762878507375717, -0.32283371686935425, 0.5061867833137512, 0.220658540725708, -0.21334849298000336, -0.078415647149086, 0.12206067144870758, -0.32206985354423523, -0.38774484395980835, 0.31679052114486694, -0.7681577801704407, 0.06939419358968735, 0.7371435761451721, -0.6888485550880432, -0.13584883511066437, 0.5372607707977295, -0.12451498210430145, -1.1690316200256348, -0.8147693872451782, 0.6073673367500305, -0.8257644772529602, 0.4225832223892212, -0.8599557280540466, 0.0733008161187172, -0.7497081160545349, -0.23179763555526733, 0.2707614600658417, 0.26239046454429626, -0.6142054200172424, 1.0447781085968018, 0.05329996347427368, -1.082780361175537, -0.11529742181301117, 0.48540565371513367, -0.1674373745918274, 0.18284551799297333, 0.6264671087265015, 0.5909118056297302, 0.040132082998752594, 0.9373701214790344, 0.4491696357727051, 0.23539118468761444, -1.045947551727295, -0.09412164986133575, 0.6052343249320984, -0.4333154857158661, -0.23485711216926575, 1.0953160524368286, -0.6334483027458191, -0.869847297668457, 0.04821813479065895, -1.1706092357635498, -0.5814903974533081, -0.12920546531677246, 0.9462098479270935, 0.30048996210098267, 0.06600432842969894, -0.08491208404302597, -0.7945032119750977, -0.3512463867664337, -0.19412028789520264, -0.3365653157234192, 0.5770490169525146, -0.2302747666835785, -0.46187129616737366, 0.6980873346328735, 0.7746531963348389, -0.6662246584892273, -0.8148732781410217, -0.748863160610199, -0.0925026684999466, 0.10007017850875854, 0.5836437940597534, -0.24537408351898193, -0.2016746997833252, 0.8033941984176636, -0.024527018889784813, 0.34961432218551636, -0.37194979190826416, -0.027182146906852722, 0.18017412722110748, 0.4097365140914917, -0.08584114164113998, -0.5802245140075684, -0.6962690353393555, 1.3803805112838745, 0.9884772896766663, -0.8290709257125854, 0.20082317292690277, 0.1868264526128769, -0.740408182144165, 0.684490442276001, 0.19224609434604645, 0.35575607419013977, 0.42363491654396057, -0.23488576710224152, -0.2586972713470459, 0.3902563452720642, -1.409104347229004, -0.33673253655433655, 0.9453209042549133, 0.7835323214530945, 0.8216255903244019, 0.22432754933834076, 0.546143651008606, 0.8360256552696228, 0.2759542763233185, 0.00084513402543962, 0.4665701687335968, 0.2804012596607208, -0.18560966849327087, -0.1989644467830658, -0.018246129155158997, 0.54035484790802, -0.876214325428009, -0.9512516260147095, 0.34580159187316895, 0.6164347529411316, 0.28161004185676575, 0.3237692713737488, 1.4217565059661865, 0.7219849824905396, 0.3570072650909424, 0.1930977702140808, 0.5577816367149353, -0.5778685808181763, -0.020673096179962158, -0.23186257481575012, -0.22414441406726837, -0.20837847888469696, 0.08124002814292908, -0.7813153862953186, -0.42653974890708923, -0.6971428990364075, 0.38563215732574463, 0.2500632107257843, 0.0030856363009661436, 1.2480539083480835, 0.6124836206436157, 0.6483636498451233, -0.36307525634765625, -0.7140636444091797, -0.2787432074546814, -1.2250041961669922, -0.062015336006879807, -0.5476331114768982, -0.11125312745571136, 0.10557760298252106, 0.03494911640882492, -0.15306086838245392]}, "authors": [{"authorId": "40859606", "name": "Luka Ribar"}, {"authorId": "66190473", "name": "Ivan Chelombiev"}, {"authorId": "2273469656", "name": "Luke Hudlass-Galley"}, {"authorId": "2249530912", "name": "Charlie Blake"}, {"authorId": "2249537360", "name": "Carlo Luschi"}, {"authorId": "145474032", "name": "Douglas Orr"}], "references": [{"paperId": "cf188f980d987d70358e414f44505e8427496d08", "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs"}, {"paperId": "b842b83a7ff5dff8e3b83915d8c15423b6085728", "title": "Gemma: Open Models Based on Gemini Research and Technology"}, {"paperId": "95240dda409e28acccdc5cf619ad0c036cf4292d", "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "188336f606e76fda9e219b954d1750ad26646fdb", "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "4ac782a4d5db8ef988d2e6f4a24dda0db027c4d5", "title": "Graphcore"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "0de0a44b859a3719d11834479112314b4caba669", "title": "A Multiscale Visualization of Attention in the Transformer Model"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "ae726fcc91664d3e710ba32429d2f3befb15cf17", "title": "The Correlation Ratio as a New Similarity Measure for Multimodal Image Registration"}, {"paperId": "2c455f0da2bd86a9b9ea432d1485049073d7c63d", "title": "Remarks on Some Nonparametric Estimates of a Density Function"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": null, "title": "NVIDIA"}, {"paperId": "84163fa8cd0ba18039021328eeaa00370bcf0a04", "title": "Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "The unreasonable effectiveness of recurrent neural networks"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Storing K twice One limitation of a theoretical model of data transfer is that it does not account for the granularity of memory access"}, {"paperId": null, "title": "Find the top-k positions in the approximate attention scores and fetch the corresponding full key and value vectors"}, {"paperId": null, "title": "Bandwidth-Efficient LLM Inference large language models"}, {"paperId": null, "title": "Introducing meta llama 3: The most capable openly available llm to date"}, {"paperId": null, "title": "Data-free"}, {"paperId": null, "title": "The needle in a haystack test"}, {"paperId": null, "title": "can; for my good uncle Gloucester H2O: can; LM-INFINITE: 'll not stand to prate, but to the purpose"}, {"paperId": null, "title": "mechanisms (Multi-Head and Grouped-Query), layer normalisation implementations, activation functions and execution of modules in parallel"}, {"paperId": null, "title": "Estimate the total score of the top k, and interpolate with V_mean alpha = gather(s_hat, -1, i2).sum(-1, keepdim= True ) y = alpha * y_ + (1 - alpha) * V_mean return"}, {"paperId": null, "title": "Step 1: Find the indices of r largest components of | q | 1 and only fetch K along the dimensions corresponding to these indices"}]}