{"paperId": "014c00319cb23c6322ea5218049661a4ce222946", "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module. TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision). Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B). Our code and models are available at https://github.com/HazyResearch/TART .", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.07536", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes TART, which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module in a task-agnostic manner and improves performance across different model families, tasks, and even across different modalities."}, "embedding": {"model": "specter_v2", "vector": [0.4761834740638733, 0.5689967274665833, -0.32457980513572693, -0.21787041425704956, -0.5419406294822693, -0.1926603764295578, 0.8155604600906372, -0.20724256336688995, -0.3301405608654022, -0.4632718563079834, 0.3373526930809021, -0.2946242392063141, 0.14439772069454193, 0.16655875742435455, -0.2705771028995514, 0.4092710018157959, -0.4661121666431427, 0.753919780254364, 0.32948368787765503, -0.6055824756622314, -0.42140793800354004, -0.6831270456314087, -0.7502845525741577, 0.5436024069786072, 0.296587198972702, 0.39957696199417114, 0.025504911318421364, 1.1940895318984985, -0.17746172845363617, 0.4489209055900574, 0.6145641207695007, -0.5281308889389038, 0.07711899280548096, 0.30793559551239014, -0.1585860550403595, -0.03806423395872116, 0.01950281858444214, -0.14793746173381805, -0.4714978039264679, 0.497309148311615, -0.24321208894252777, 0.15482595562934875, 0.7387081384658813, -0.9572335481643677, -0.7205542325973511, 0.9156779050827026, 0.6456516981124878, 0.6265742182731628, -0.28196272253990173, -0.3893760144710541, 1.3563697338104248, -1.4000171422958374, -0.07350800186395645, 1.484678864479065, 0.5955474972724915, 0.773655116558075, -0.6122604012489319, -0.696662425994873, 0.7734134793281555, -0.08024738729000092, -0.6648000478744507, -0.4562447965145111, -0.3440724313259125, -0.09529469162225723, 2.0737802982330322, -0.561654269695282, -0.3625306487083435, 0.5340626835823059, -0.1640395224094391, 1.2142099142074585, -0.35168758034706116, -0.7736629247665405, -0.32415103912353516, 0.33375364542007446, -0.12961311638355255, 0.5723486542701721, -0.5749449133872986, 0.09390050917863846, -0.851006805896759, -0.06627506762742996, 0.4971983730792999, -0.11314781755208969, -0.00997941941022873, -0.3012787103652954, -0.3364544212818146, 0.6475213766098022, 0.5079352855682373, 0.7890362739562988, -0.1633402556180954, 0.830201268196106, 0.5759692192077637, 0.4699479341506958, -0.21020853519439697, 0.663749635219574, -0.707207977771759, 0.3507338762283325, -0.5673027038574219, 0.4786578118801117, 0.13391909003257751, 1.0919936895370483, -0.3668898642063141, -0.08967357873916626, -0.7120693922042847, 0.377947598695755, 1.41499924659729, 0.08355920761823654, 0.5346908569335938, -0.7528252601623535, 0.33950138092041016, -0.801863431930542, 0.08543518930673599, -0.20740312337875366, -0.2874646782875061, -0.11758984625339508, -0.41807863116264343, -1.1576309204101562, -0.26381784677505493, 0.2754313051700592, -0.7436771392822266, 1.070162296295166, -0.5731592178344727, -0.13695725798606873, 0.39971619844436646, 0.1446106880903244, 0.6974872350692749, 0.5189256072044373, 0.5996149778366089, 0.18636485934257507, 0.9602596759796143, -0.8038710951805115, -0.36661267280578613, -1.482534646987915, 0.5591346025466919, -0.2939648926258087, 0.7203240394592285, -0.1662413626909256, -1.1670126914978027, -0.8214522004127502, -0.6390523314476013, -0.17146441340446472, -0.7702014446258545, 0.30869901180267334, 1.0829651355743408, 0.49309611320495605, -0.8564023971557617, 0.25759217143058777, -0.007828625850379467, -0.4543420970439911, 0.34069210290908813, 0.21861699223518372, -0.008962247520685196, -0.5700738430023193, -1.2716896533966064, 0.19469216465950012, 0.3709161579608917, -0.6286432147026062, -0.37662798166275024, -0.6377711892127991, -0.9802905917167664, -0.06363800168037415, 0.4530153274536133, -0.8516634106636047, 1.842126727104187, -0.14526182413101196, -1.175097107887268, 0.5595709681510925, -0.3156156837940216, 0.05670255795121193, 0.43060874938964844, -0.03150476515293121, -0.7197622060775757, -0.7412744164466858, -0.11247431486845016, 0.572352409362793, 0.4342893362045288, -0.1392824500799179, -0.5424486398696899, 0.4780822992324829, 0.2651519179344177, -0.08621763437986374, -0.0656026229262352, 0.8566343784332275, -0.5973788499832153, -0.22054722905158997, 0.14480085670948029, 0.8343403339385986, -0.04038209468126297, -0.0034607017878443003, -0.14858834445476532, -1.0285983085632324, 0.7922835946083069, -0.07763268798589706, 0.5824874639511108, -1.1877318620681763, -0.6879717111587524, -0.3170119524002075, -0.0807701051235199, -0.2839035391807556, -0.7261908650398254, 0.5976316928863525, -0.3455907106399536, 0.2538506090641022, -0.46856844425201416, -1.0245718955993652, 0.2764758765697479, -0.17578108608722687, -0.6256174445152283, -0.5343897342681885, 0.5967069268226624, 1.200859785079956, -1.04278564453125, 0.08094320446252823, -0.22889341413974762, 0.45315730571746826, -1.3883644342422485, 1.3421599864959717, -0.8593496084213257, 0.11672165244817734, -0.15306633710861206, -0.10722962766885757, -0.1994950771331787, -0.5104635953903198, 0.29727548360824585, -0.4815625846385956, 0.3464471101760864, 0.4968346953392029, 0.04826648160815239, 1.5479177236557007, -0.898638129234314, 0.4664311707019806, -0.3168115019798279, -0.6319005489349365, -0.05320049822330475, 0.6186466813087463, -0.5771425366401672, -0.40231677889823914, 0.07842055708169937, 0.594153106212616, -0.584364116191864, 0.028067979961633682, 0.5096099376678467, 0.4783775806427002, -0.2933923006057739, 0.07104358077049255, 0.700648844242096, -0.5943041443824768, 0.2715807855129242, 0.216317281126976, 1.072650671005249, 0.4311782419681549, 0.4866490364074707, 0.33374732732772827, 0.5431361794471741, -0.8122849464416504, -0.32098135352134705, 0.5406908392906189, 0.8567008376121521, 0.9075634479522705, 0.34794193506240845, -0.580148458480835, -0.16815713047981262, -0.3969212770462036, 0.5693982243537903, 2.0082626342773438, -0.4230429530143738, -0.11286439746618271, -0.34771066904067993, -0.34689050912857056, -0.33968842029571533, 0.4498443901538849, -0.7350107431411743, -0.36647239327430725, -0.29835593700408936, -1.2185803651809692, 0.6383473873138428, 0.6314323544502258, 0.9902777671813965, -0.4885241389274597, 0.10130994766950607, 0.05559663474559784, -0.021822188049554825, -0.9561185836791992, -0.44825974106788635, 0.6042497754096985, -0.28008338809013367, -0.12621547281742096, 0.4218013286590576, -0.1714548021554947, 0.16074839234352112, -0.8019618988037109, 0.9601871967315674, -0.9282774329185486, -0.18711230158805847, 0.42687657475471497, 0.4588378071784973, -0.5763012766838074, -1.042260766029358, 0.34391891956329346, -0.22365298867225647, -0.1408088207244873, 0.4631827175617218, 0.12471179664134979, 0.35195547342300415, 0.3760686218738556, -0.4503125250339508, 0.24672506749629974, 0.2094740867614746, -0.0082997502759099, 0.7755767107009888, -0.0849699005484581, 0.08159167319536209, -1.5658003091812134, 0.8536744713783264, -0.18620486557483673, -0.13894838094711304, 0.38723334670066833, -0.597307562828064, -0.4832252264022827, 0.7388452887535095, -1.002148151397705, -0.5754216313362122, -0.7871620059013367, 0.38606834411621094, -0.14826197922229767, -0.32466524839401245, 0.23290497064590454, 0.04970184713602066, 0.43345850706100464, 0.6747782230377197, 0.40252867341041565, -0.025898337364196777, -0.11326242238283157, 0.8686306476593018, -0.8776900768280029, 0.734878659248352, 0.09297855198383331, 0.5495583415031433, -0.24906522035598755, -0.2685869038105011, -0.13060028851032257, -0.5939250588417053, -0.2415553480386734, -0.036766886711120605, -0.13778135180473328, 0.2085571587085724, -0.744956910610199, -0.7966909408569336, -0.29589101672172546, -1.0268492698669434, -0.6602742075920105, -0.012588560581207275, -0.6212746500968933, -0.46920421719551086, -1.6207215785980225, -1.0387426614761353, -0.3087148368358612, -0.25076282024383545, -0.8255483508110046, 0.5021931529045105, 0.18297640979290009, -0.2576763331890106, -0.8142889142036438, -0.013138760812580585, -0.024379797279834747, 1.2606440782546997, -0.6898270845413208, 1.070117712020874, -0.12368800491094589, -0.12760505080223083, -0.33302831649780273, -0.20801396667957306, 0.2746012508869171, -0.1758948564529419, 0.2089199721813202, -1.5188047885894775, 0.5700768232345581, -0.40560147166252136, -0.4037603735923767, -0.08826874196529388, 0.07953733205795288, 0.7620723843574524, 0.16449686884880066, -0.5044482946395874, 0.37152186036109924, 1.3166476488113403, -0.5345250964164734, -0.07493142038583755, 0.26850512623786926, 0.8953123092651367, 0.32862213253974915, -0.22275996208190918, 0.3283557593822479, 0.6966545581817627, 0.4210810959339142, 0.11331535875797272, 0.18176531791687012, 0.028506366536021233, -0.7940796613693237, 0.6434492468833923, 1.0083776712417603, 0.06434875726699829, 0.0036788673605769873, -1.2264440059661865, 0.4511176645755768, -1.5526459217071533, -0.3328058123588562, 1.0116417407989502, 0.7094415426254272, 0.6613069176673889, -0.2678420841693878, -0.43500927090644836, -0.21928447484970093, 0.14358022809028625, -0.1601964235305786, -0.45618417859077454, -0.6437049508094788, 0.20604686439037323, 0.22665928304195404, -0.0024745340924710035, 0.46888095140457153, -0.4150341749191284, 0.5977357625961304, 14.50455379486084, 0.880355715751648, 0.3392171561717987, 0.9185695648193359, 0.3712728023529053, 0.4925459325313568, -0.40747255086898804, -0.09486018121242523, -1.0374679565429688, -0.7619410157203674, 1.2620460987091064, 0.2781267762184143, 0.8969820737838745, 0.13205279409885406, -0.2153889387845993, 0.487180233001709, -0.7984840273857117, 0.5207604765892029, 0.395748108625412, -0.9353628754615784, 0.6113618016242981, -0.10883121192455292, 0.023357940837740898, 0.39365527033805847, 0.9858957529067993, 1.223972201347351, 0.7373457551002502, -0.6414008736610413, 0.6194550395011902, 0.27410608530044556, 0.9479719996452332, 0.3058462142944336, 0.33785930275917053, 0.32571613788604736, -1.1043659448623657, -0.7446510195732117, -0.3574048578739166, -0.8570161461830139, 0.079161636531353, -0.20447683334350586, -0.80108243227005, -0.6324663758277893, -0.00014831926091574132, 0.8059867024421692, -0.1556912511587143, 0.3526584506034851, -0.40607646107673645, 0.500324010848999, 0.01707838475704193, 0.5620009303092957, -0.2028014361858368, 0.980091392993927, 0.17697888612747192, 0.016947152093052864, -0.06940232217311859, -0.3900967240333557, -0.12842734158039093, 0.6773693561553955, -0.615379810333252, -0.09076900780200958, -0.2473914474248886, -0.1489843726158142, -0.25607556104660034, 0.5513747334480286, 0.44037389755249023, 0.4389895498752594, -0.7285850048065186, 0.10960697382688522, 0.6708707213401794, 0.48880434036254883, -0.3967841863632202, 0.18142789602279663, 0.17788401246070862, -0.32017242908477783, 0.015748266130685806, 0.9541622996330261, 0.188156858086586, -0.5715675354003906, -0.6429660320281982, -0.47267693281173706, 0.48262646794319153, -0.9857785701751709, -1.141313910484314, 0.8367145657539368, -0.15652640163898468, -0.426072895526886, 0.10100262612104416, -0.6603900194168091, -0.14686129987239838, 0.2136368304491043, -1.5826948881149292, -1.2070592641830444, 0.27977609634399414, -0.12719877064228058, -0.016125014051795006, -0.18639597296714783, 1.493093729019165, 0.2037728875875473, -0.14846138656139374, 0.08084675669670105, -0.24009215831756592, -0.04343307763338089, 0.16137340664863586, -0.9984638094902039, 1.0243741273880005, -0.03972989320755005, -0.08659064769744873, 0.042158763855695724, 0.32725709676742554, 0.5143592357635498, -0.5725599527359009, -0.30319860577583313, 0.9066805243492126, -1.2896170616149902, -0.1581396460533142, -0.7264057993888855, -0.7295836806297302, 0.6902938485145569, 0.2873366177082062, -0.11826422065496445, 0.5200153589248657, 0.27501749992370605, -1.3967251777648926, -0.2895606458187103, -0.9296590685844421, 0.13610634207725525, 0.4106602072715759, -0.7990545034408569, -0.4915062189102173, 0.14664535224437714, 0.7271958589553833, -1.0663732290267944, -0.5341747999191284, -0.21460267901420593, 0.22812609374523163, 0.008466619998216629, 1.1684433221817017, -0.29473885893821716, 0.6047114133834839, 0.6940622925758362, -0.4437660574913025, -0.6631844639778137, -0.08525294810533524, -0.7868860960006714, 0.026264535263180733, 0.18368816375732422, 1.1692562103271484, -0.830876886844635, -0.3519136905670166, 0.976396918296814, 0.42073917388916016, -0.17135702073574066, -0.31758740544319153, 0.02455875091254711, 0.17496037483215332, -0.7575452923774719, 0.4847668409347534, -0.1319914013147354, -0.2937938868999481, 0.1904682219028473, 0.7496146559715271, 0.9419611096382141, -0.3354399502277374, -0.8847184181213379, 0.5907805562019348, -0.28933772444725037, -0.7777938842773438, -0.4271147847175598, -0.38602936267852783, -0.8753358721733093, -0.09009809046983719, -1.1100120544433594, 0.3105221688747406, -0.9865813851356506, -0.468906432390213, -0.03291817009449005, -0.4001718759536743, 0.3922399878501892, 0.42255672812461853, -0.294269859790802, -0.406371533870697, -0.6838998794555664, -0.4760165512561798, 0.5382280349731445, 0.772399365901947, -0.8463752269744873, 0.1126728281378746, 0.3199758231639862, -0.2150646597146988, 0.39306554198265076, 0.4739460349082947, -0.5352229475975037, -0.9357175827026367, -1.4198864698410034, 0.8121833801269531, -0.13216756284236908, 0.0887850970029831, -0.7674214839935303, 0.7533469796180725, 0.36434563994407654, -0.20925624668598175, 0.8760404586791992, 0.46202442049980164, -1.2538533210754395, -0.5924381613731384, 0.0845460295677185, -0.9629591703414917, 0.19615678489208221, 0.7448070645332336, -0.3188915252685547, -0.2546941041946411, 0.7770487666130066, -0.078194759786129, -0.9018010497093201, -0.5472084879875183, 0.06990363448858261, -0.6893075108528137, 0.1607511043548584, -0.5957645773887634, 0.30721575021743774, -1.2883222103118896, -0.49521604180336, -0.08894560486078262, 0.3631281554698944, -0.4273635745048523, 0.943328857421875, 0.24431733787059784, -0.9384931325912476, 0.04901418462395668, 0.30853524804115295, 0.4065263569355011, 0.34429365396499634, 0.5101708769798279, 0.3167164623737335, -0.10762035101652145, 0.3117513954639435, 0.2609265148639679, 0.3342963457107544, -0.5053613781929016, -0.02325664460659027, 1.0864728689193726, -0.5388259291648865, -0.12325959652662277, 1.1631876230239868, -0.013150164857506752, -1.500266194343567, 0.10049237310886383, -1.267396092414856, -0.6831623911857605, -0.6370669603347778, 0.5416715741157532, -0.2980230748653412, -0.21491850912570953, 0.2175968438386917, -0.2473442107439041, 0.3828481137752533, -0.1581190526485443, -0.5187915563583374, 0.4339868426322937, -0.18192334473133087, -0.36798804998397827, 0.7411231398582458, 0.3920677900314331, -0.6068105697631836, -0.666824996471405, -0.9955775141716003, -0.41052255034446716, 0.07694415748119354, 0.10728269815444946, -0.7658154964447021, -0.5895392298698425, 0.6829408407211304, 0.08395294100046158, 0.11867552995681763, 0.5245442986488342, 0.11713547259569168, -0.35146042704582214, 0.8971542119979858, -0.03497537970542908, -0.1497161090373993, -0.5393178462982178, 1.03031325340271, 1.2557495832443237, -1.1596614122390747, -0.05579204857349396, -0.4680127203464508, -0.7520350813865662, 1.0111994743347168, 0.5317206382751465, 0.16989803314208984, 1.152736783027649, -0.22611413896083832, 0.011292114853858948, 0.20506997406482697, -1.4904862642288208, -0.061110615730285645, 0.8630959987640381, 1.4507625102996826, 0.7113833427429199, 0.4725136160850525, 0.6603342890739441, 0.9700766801834106, 0.031746357679367065, 0.33609071373939514, 0.587185263633728, 0.24854706227779388, 0.003921937197446823, -0.28652793169021606, 0.19466263055801392, 0.712029218673706, -0.47420915961265564, -0.6273922920227051, 0.4177037477493286, 0.5769935846328735, 0.4069761037826538, 0.7461707592010498, 0.5935350060462952, 0.2196991741657257, 0.7751650810241699, 0.34937584400177, 0.37516728043556213, -0.9171285033226013, -0.21732883155345917, -0.44943156838417053, -0.6732119917869568, 0.028628654778003693, -0.4513160288333893, -0.49295440316200256, -0.5731518864631653, 0.13969114422798157, 0.4542589485645294, -0.1866898387670517, 0.24063648283481598, 1.1533055305480957, 0.290817528963089, 0.7469227313995361, -0.648551881313324, -0.298103392124176, -0.10296052694320679, -0.9039360880851746, 0.33283185958862305, -0.8693078756332397, -0.08669249713420868, -0.235067218542099, -0.3300219178199768, -0.04743129387497902]}, "authors": [{"authorId": "144383716", "name": "K. Bhatia"}, {"authorId": "1381444249", "name": "A. Narayan"}, {"authorId": "2081393182", "name": "Chris De Sa"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}], "references": [{"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "154493f69d7db3d49da0e51df0192c6ad5f1724a", "title": "Larger language models do in-context learning differently"}, {"paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9", "title": "Active Prompting with Chain-of-Thought for Large Language Models"}, {"paperId": "5848737f78397f72ceae2ba6f3419a6a8502b8ba", "title": "ChatGPT: Jack of all trades, master of none"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "fb49e88c6bd676516898e911e42b4f8479e6f1bf", "title": "Ask Me Anything: A simple strategy for prompting language models"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b", "title": "Rationale-Augmented Ensembles in Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b", "title": "Can Foundation Models Wrangle Your Data?"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be", "title": "STaR: Bootstrapping Reasoning With Reasoning"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d", "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "e1227daa4877599e13de41a5207a222e1b197456", "title": "RAFT: A Real-World Few-Shot Text Classification Benchmark"}, {"paperId": "3ba529f732d3c4a31e9ce57f1c78ddf911846bf4", "title": "WRENCH: A Comprehensive Benchmark for Weak Supervision"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "a0185d4f32dde88aa1749f3a8000ed4721787b65", "title": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b611a8095630557229dc5fb6b07c272f1cd614da", "title": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "da6e404d8911b0e5785019a79dc8607e0b313dc4", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "db6b56557d4b8c722e1fc504af4a361cfe1e22d8", "title": "Contributions to the study of SMS spam filtering: new collection and results"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "6dc9b109c995873c33f13722d0541a87f3362ac0", "title": "Large Language Models are Zero-Shot Clinical Information Extractors"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "20f63033e8775cbab0692aed92d38da7e725d64e", "title": "Understanding Machine Learning - From Theory to Algorithms"}, {"paperId": null, "title": "Mnist handwritten digit database"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "6f7c1d384c87e4527ff004de0bde80b07bbd3f58", "title": "Probability theory: the logic of science"}, {"paperId": null, "title": "H2. Fine-tuning a\ufb00ects both the representations and reasoning but the improvement in reasoning abilities primarily leads to better performance"}, {"paperId": null, "title": "Prompting vs. finetuning vs. alternatives,"}, {"paperId": null, "title": "Resources for inference We use 8 NVIDIA A100 GPU\u2019s ( $3.5 per GPU/hr) for 480 hours for inference runs costing a total of $13"}, {"paperId": null, "title": "We now analyze each of the task adaptation approaches through the lens of the above hypotheses"}, {"paperId": null, "title": "H1. LLM representations have enough information to perform the task in-context, but they lack the reasoning abilities to perform the task well"}, {"paperId": null, "title": "H3. Fine-tuning and adapters are not task-agnostic because the task-speci\ufb01c training hurts their ability to transfer reasoning"}]}