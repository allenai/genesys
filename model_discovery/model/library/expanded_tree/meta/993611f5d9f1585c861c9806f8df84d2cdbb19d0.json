{"paperId": "993611f5d9f1585c861c9806f8df84d2cdbb19d0", "abstract": "Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.", "venue": "arXiv.org", "year": 2024, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces APT that adaptively prunes and tunes parameters for the LMs and dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency."}, "embedding": {"model": "specter_v2", "vector": [-0.1842188835144043, 0.4939133822917938, -0.5253739356994629, -0.00627138651907444, 0.008190066553652287, -0.16533531248569489, 0.7770240902900696, -0.563423216342926, -0.7192330956459045, -0.31513479351997375, 0.3758166432380676, -0.4487841725349426, 0.49924883246421814, 0.26783376932144165, -0.005877424497157335, 0.25804224610328674, -0.9077829718589783, 0.357510507106781, -0.058425065129995346, -0.35100212693214417, -0.4528143107891083, -0.4848455786705017, -0.914810299873352, 0.10442556440830231, 0.5154274106025696, 0.6745375394821167, 0.19021263718605042, 0.7302361130714417, -0.5812119245529175, 0.24145442247390747, 0.20271417498588562, -0.3649941682815552, 0.1757735162973404, 0.0636967197060585, -0.2990451455116272, -0.23024171590805054, 0.3141627311706543, -0.436053991317749, -0.17046551406383514, 0.6944522857666016, -0.2651083469390869, 0.4210232198238373, 0.36315250396728516, -0.5373268723487854, -0.060698773711919785, 0.8833605647087097, 0.5267891883850098, 0.9714469909667969, -0.9057311415672302, -0.581702470779419, 0.7870150804519653, -1.4109832048416138, -0.02173953503370285, 1.4794691801071167, 0.6962616443634033, 0.4413147270679474, -0.07946246862411499, -0.9452006220817566, 0.9179846048355103, -0.050426825881004333, -1.0132397413253784, -0.6146125793457031, -0.323520302772522, 0.18078522384166718, 2.451930284500122, -0.43331843614578247, -0.18562492728233337, 0.5149226188659668, -0.31147655844688416, 1.3320759534835815, -0.13976338505744934, -0.7001476287841797, -0.48093724250793457, 0.31831464171409607, 0.18000531196594238, 0.8020273447036743, -0.46707025170326233, 0.20256009697914124, -0.9085066318511963, -0.3354201018810272, 0.3560243546962738, -0.3730952739715576, 0.20339296758174896, 0.04470750316977501, -0.43607279658317566, 0.8470788598060608, 0.20237207412719727, 0.9251046180725098, -0.02118876576423645, 0.3323083817958832, 0.5132120251655579, 0.4959380328655243, 0.14772264659404755, 0.5410419702529907, -0.7092951536178589, 0.5137547850608826, -0.9575097560882568, -0.03691122308373451, 0.007872350513935089, 0.914513111114502, -0.26794347167015076, 0.5532101988792419, -0.9011994004249573, 0.6991853713989258, 1.2852615118026733, 0.3636329472064972, 0.6090009212493896, -0.7359293699264526, 0.5092694163322449, -0.6866714358329773, 0.184691920876503, -0.22716031968593597, -0.4095524549484253, -0.4277363121509552, -0.7931848168373108, -1.3263871669769287, -0.34191277623176575, -0.31248706579208374, -0.7564266920089722, 0.8973070979118347, 0.10096458345651627, 0.28816285729408264, -0.020710861310362816, 0.36252689361572266, 0.4601127505302429, 0.6743095517158508, 0.46036791801452637, -0.05417049676179886, 1.1253879070281982, -1.0940204858779907, -0.6128898859024048, -1.008777379989624, 0.529310405254364, -0.5061923861503601, 0.40290510654449463, -0.14102639257907867, -1.172508716583252, -0.9437170028686523, -0.9754443168640137, -0.30849403142929077, -0.6659404039382935, 0.7358983159065247, 0.914094865322113, 0.5659196376800537, -0.6715906858444214, 0.49236157536506653, -0.1519598513841629, -0.1914084255695343, 0.3657858967781067, 0.32165321707725525, 0.5343578457832336, -0.023363545536994934, -1.3916748762130737, 0.42794835567474365, 0.3778228163719177, -0.4477386474609375, -0.0806463211774826, -0.6144959330558777, -0.6658355593681335, 0.04605467617511749, 0.4047187566757202, -0.9243325591087341, 1.562767505645752, -0.09021061658859253, -1.8574424982070923, 0.6804801225662231, -0.38947075605392456, -0.23662997782230377, 0.14601872861385345, -0.2528463900089264, -0.771701455116272, -0.33644312620162964, -0.6476598381996155, 0.7860156893730164, 1.0356062650680542, 0.3625994920730591, -0.3528437614440918, 0.34972184896469116, -0.09776664525270462, 0.2227337509393692, -0.22611302137374878, 0.9831865429878235, -0.6262309551239014, -0.5265668630599976, 0.540839433670044, 0.6283013820648193, 0.12699294090270996, -0.4455772340297699, -0.22968333959579468, -1.008857011795044, 0.5269253849983215, -0.08252474665641785, 1.250740885734558, -0.8864821791648865, -0.705069363117218, -0.007920026779174805, -0.3177354633808136, 0.233192577958107, -0.7597306370735168, 0.12604442238807678, 0.03927043452858925, 0.5314664244651794, 0.17536962032318115, -1.2556275129318237, 0.1891651153564453, -0.4725184440612793, -1.0421695709228516, -0.43066948652267456, -0.034795984625816345, 0.9479196071624756, -0.5710622072219849, 0.009901298210024834, -0.036864664405584335, 0.5954166054725647, -1.1825087070465088, 0.9493420124053955, -0.8613824844360352, 0.30410197377204895, -0.1304679960012436, -0.2197464555501938, -0.01322578452527523, -0.3117038607597351, 0.3205234706401825, -0.22406233847141266, -0.19822265207767487, 0.7571420669555664, -0.6698369979858398, 1.2271207571029663, -0.7701865434646606, 0.35137444734573364, 0.29300108551979065, -0.1428270787000656, -0.2121427208185196, 0.38553380966186523, -0.5075257420539856, -0.452549546957016, 0.3251591920852661, 0.6008374691009521, -0.7209293842315674, 0.5496681332588196, 0.6577825546264648, 1.040892243385315, -0.17240825295448303, 0.3026452660560608, 0.5486788749694824, -0.43824946880340576, 0.3708973228931427, 0.2473447024822235, 0.3386252522468567, 0.3583281338214874, 0.3036890923976898, -0.0527569055557251, 0.6350337862968445, -0.9320289492607117, -0.11650881916284561, 0.801337718963623, 0.966821551322937, 0.3848261535167694, 0.10657534748315811, -0.5261112451553345, -0.12438283115625381, -0.18914276361465454, 0.47177207469940186, 2.0266072750091553, -0.5047386288642883, -0.0865337923169136, -0.7414777874946594, -0.2787559926509857, -0.34375864267349243, 0.2766760289669037, -0.12188377976417542, 0.15720482170581818, -0.7088751792907715, -1.1306509971618652, 0.9133532643318176, -0.33599209785461426, 1.3111594915390015, -0.5930269956588745, -0.14301882684230804, -0.2956283986568451, 0.12274667620658875, -0.6726211309432983, -0.7229897379875183, 0.24952849745750427, -0.9343522191047668, 0.047434207051992416, 0.17996281385421753, -0.07662993669509888, 0.019932983443140984, -0.5251495838165283, 1.0395400524139404, -0.3621065318584442, 0.01915396936237812, -0.0914168655872345, 0.777800440788269, -0.6204432249069214, -0.6447110772132874, 0.5552787780761719, 0.29552504420280457, -0.10653586685657501, 0.1999867856502533, 0.33907026052474976, 0.11356130987405777, 0.23622877895832062, -0.14306215941905975, 0.12043111026287079, 0.2530354857444763, 0.06601367145776749, 0.9330079555511475, -0.15092171728610992, 0.14492563903331757, -1.567778468132019, 0.7180787324905396, 0.263242244720459, -0.682815432548523, 0.3874319791793823, -0.6050958037376404, -0.26619553565979004, 0.42087432742118835, -0.808955729007721, -0.8437018394470215, -1.1370489597320557, 0.14431896805763245, -0.22408416867256165, 0.04370293766260147, 0.05455887317657471, 0.22105269134044647, 0.11521059274673462, 0.3825450837612152, 0.15643349289894104, -0.1335095316171646, -0.5285713076591492, 0.7426779866218567, -0.681527316570282, 0.4787442088127136, 0.333438515663147, 0.30358847975730896, -0.3329290747642517, -0.6005218029022217, -0.9118679165840149, -0.7232245802879333, -0.40736129879951477, -0.22170250117778778, 0.33350279927253723, -0.16442939639091492, -0.640126645565033, -0.5575509667396545, 0.10642608255147934, -0.8213890194892883, -0.5691831707954407, 0.3564642369747162, 0.057893406599760056, -0.018056316301226616, -1.1568881273269653, -1.2237045764923096, -0.38448387384414673, -1.008127212524414, -0.7871436476707458, 0.24909387528896332, -0.02749883197247982, -0.5010519027709961, -0.3783583641052246, -0.0689919963479042, -0.2652990221977234, 1.2646596431732178, -1.2558808326721191, 1.016196370124817, 0.03266742080450058, -0.17273573577404022, 0.0660770982503891, 0.1875944882631302, 0.49446848034858704, -0.7387741208076477, 0.13298314809799194, -1.2786521911621094, 0.20651575922966003, 0.028794610872864723, -0.21506008505821228, 0.2652178108692169, 0.32552656531333923, 0.9786295890808105, -0.23173849284648895, -0.4975414574146271, 0.7936424612998962, 0.8840996623039246, -0.9085181355476379, -0.19357675313949585, 0.0783548355102539, 0.7253459095954895, -0.05998145416378975, -0.09915801882743835, 0.343939870595932, 0.14947670698165894, 0.29731351137161255, -0.20990993082523346, 0.08719994872808456, -0.22074104845523834, -0.515933632850647, 0.5887812376022339, 1.6905344724655151, 0.4940136969089508, 0.0835399255156517, -0.6787543892860413, 0.25972676277160645, -0.9139532446861267, -0.5185598731040955, 0.7384358048439026, 1.2254542112350464, 0.7881184220314026, -0.21751224994659424, -0.38946428894996643, -0.5003544092178345, 0.23131488263607025, 0.322799414396286, -0.5026436448097229, -0.8342100381851196, 0.07552690804004669, 0.4217435419559479, 0.07042863965034485, 0.6374230980873108, -0.7462088465690613, 0.9743021130561829, 14.649572372436523, 1.0260920524597168, 0.04419693350791931, 0.9277850389480591, 0.5815017223358154, 0.05764078348875046, -0.03432314097881317, -0.4099500775337219, -1.485321283340454, -0.41787728667259216, 1.1391870975494385, 0.34000298380851746, 1.1723778247833252, 0.04079287499189377, -0.028230436146259308, 0.12152472883462906, -0.07865073531866074, 0.6354667544364929, 0.3048500716686249, -1.1957533359527588, 0.38194528222084045, -0.14430783689022064, 0.5272008180618286, 1.0176951885223389, 0.8115931153297424, 1.3651734590530396, 0.4366532564163208, -0.4127676784992218, 0.35764336585998535, 0.05100489780306816, 0.5140008330345154, -0.14355166256427765, 0.1872347742319107, 0.5999836325645447, -0.4834810495376587, -0.20836614072322845, -0.5896211266517639, -0.9756184220314026, -0.0927533432841301, 0.15029926598072052, -0.6445402503013611, -0.872501790523529, -0.060410864651203156, 0.29251229763031006, -0.2035064846277237, 0.4159996807575226, -0.4453386068344116, 0.9652386903762817, -0.5356881022453308, 0.04756707698106766, 0.2278391569852829, 0.3821559250354767, 0.36061087250709534, 0.07490326464176178, 0.07099291682243347, -0.10500089824199677, -0.144266739487648, 0.4659271836280823, -0.8344770073890686, -0.04896625503897667, -0.011562495492398739, -0.2585563063621521, 0.3391803801059723, 0.9071395993232727, 0.5564925074577332, 0.2300899773836136, -0.3589264452457428, 0.23531292378902435, 0.6835609078407288, 0.3625810444355011, -0.4304138123989105, 0.2023736983537674, 0.3545401990413666, -0.8201612830162048, 0.16080708801746368, 0.3093666434288025, -0.20494338870048523, -0.7182000279426575, -0.865898847579956, -0.5942463874816895, 0.42962655425071716, -0.8385521173477173, -0.5440238118171692, 0.7089066505432129, -0.30203521251678467, -0.17473101615905762, 0.14039281010627747, -0.5136793851852417, -0.13280466198921204, 0.6487654447555542, -1.3878626823425293, -0.7505379319190979, 0.7456278800964355, -0.2710113227367401, -0.32961586117744446, -0.2567356824874878, 1.4502180814743042, 0.12371263653039932, -0.6371896266937256, 0.26265624165534973, 0.2235526144504547, -0.22360390424728394, -0.1057218536734581, -0.5089614391326904, 0.7707732915878296, 0.3414507508277893, 0.29615482687950134, 0.6492195129394531, -0.1893278807401657, 0.25593283772468567, -0.6609826683998108, -0.10834331810474396, 0.9222044944763184, -0.8228054642677307, -0.12516358494758606, -0.7891973853111267, -0.7613157033920288, 0.436484694480896, 0.5299623012542725, -0.43044358491897583, 0.15038040280342102, 0.482970267534256, -0.7679377794265747, -0.19682256877422333, -0.640518844127655, 0.16704773902893066, 0.4767053425312042, -0.789072573184967, -0.28341230750083923, -0.025254717096686363, 0.327825129032135, -1.1274977922439575, -0.5900272130966187, -0.09634608030319214, 0.03944443538784981, 0.19184765219688416, 1.1990742683410645, -0.1218857616186142, -0.0412871427834034, 0.8399926424026489, -0.011102945543825626, -0.8879337310791016, 0.2745625376701355, -0.9381142854690552, -0.25933218002319336, 0.031411416828632355, 1.0696394443511963, -0.507639467716217, -0.15332196652889252, 0.6194865107536316, 0.34637144207954407, -0.2295076996088028, -0.6290296912193298, -0.25301167368888855, 0.13633666932582855, -0.5366056561470032, 0.4193853735923767, -0.10574168711900711, -0.2556956708431244, 0.5153598785400391, 0.6298457980155945, 0.8432525992393494, -0.3659918010234833, -0.4983612596988678, 0.44314974546432495, 0.03925248607993126, -0.32649803161621094, -0.7783313989639282, 0.27671900391578674, -1.4389894008636475, -0.03987112641334534, -0.958605170249939, -0.2869814932346344, -0.5381429195404053, -0.6105902194976807, -0.3024224042892456, -0.17290160059928894, 0.23070335388183594, 0.4535984992980957, -0.1768665313720703, -0.6832457780838013, -0.39918819069862366, -0.5201435089111328, 0.6014377474784851, 0.6395531892776489, -0.6426964402198792, -0.1991029530763626, 0.4778071343898773, 0.39147573709487915, 0.6826082468032837, 0.8287690877914429, -0.38663336634635925, -0.8273911476135254, -1.6129924058914185, 0.4289158880710602, -0.40588605403900146, -0.3698427081108093, -0.5938464403152466, 0.7120456099510193, 0.4218358099460602, 0.05799056217074394, -0.0563710518181324, 0.3281819820404053, -0.7042005062103271, -0.5616850852966309, 0.05836108699440956, -0.4632694721221924, 0.024138424545526505, 0.6991099119186401, -0.8257076740264893, -0.07419214397668839, 0.5046488046646118, 0.05132400617003441, -0.8965494632720947, -0.8547006249427795, 0.32110705971717834, -0.682102620601654, 0.4168718159198761, -0.7867865562438965, 0.24807080626487732, -1.1837095022201538, -0.1780032217502594, 0.05988822877407074, 0.4654521346092224, -0.7257673144340515, 0.9207491278648376, 0.09670446068048477, -0.7750931978225708, 0.20464003086090088, 0.6907044053077698, -0.36793434619903564, -0.10836255550384521, 0.3978973627090454, 0.8143454194068909, -0.3099345564842224, 0.7489624619483948, 0.5752318501472473, 0.45596227049827576, -0.8840207457542419, -0.24334846436977386, 1.0606762170791626, -0.46220526099205017, -0.10397316515445709, 1.2064263820648193, -0.359098345041275, -1.4738999605178833, 0.5145159363746643, -1.395356297492981, -0.5332279205322266, -0.19224458932876587, 0.6989468336105347, 0.20922918617725372, 0.13248310983181, 0.0837000384926796, -0.2623099982738495, -0.009688843972980976, -0.03966536745429039, -0.6385588049888611, 0.3308382034301758, -0.4764559864997864, -0.4516541063785553, 0.6940332055091858, 0.6611869931221008, -0.2948944568634033, -0.6722908616065979, -0.8055446147918701, -0.23273660242557526, 0.20893512666225433, 0.7280260324478149, -0.9935756921768188, -0.5990064144134521, 0.816981852054596, 0.397196888923645, -0.13444563746452332, 0.07274865359067917, -0.5129156708717346, 0.08476488292217255, 1.1969709396362305, 0.11923445016145706, -0.9761517643928528, -0.7615787982940674, 1.5292109251022339, 1.1349396705627441, -1.047816514968872, 0.41749313473701477, -0.01432319637387991, -0.6771731972694397, 0.6723565459251404, 0.2554311454296112, 0.3120853006839752, 1.0526803731918335, -0.5142342448234558, 0.15728342533111572, 0.35008856654167175, -0.784403920173645, -0.4326779246330261, 0.9447747468948364, 0.447115421295166, 0.666541337966919, 0.32704514265060425, 0.29031455516815186, 0.9292005896568298, -0.05962327867746353, -0.10955402255058289, -0.04934956133365631, 0.2095269411802292, -0.3168052136898041, 0.05577333644032478, -0.03421090915799141, 0.6929094791412354, -0.7880938053131104, -0.7452812194824219, 0.13657166063785553, 0.9248806238174438, 0.3472992479801178, 0.4894959330558777, 0.6095502376556396, 0.39854317903518677, 0.45949169993400574, 0.5066195726394653, 0.6808363795280457, -0.5178272724151611, -0.4984127879142761, 0.04769626632332802, -0.45111992955207825, -0.07804708927869797, 0.03123103268444538, -0.45804834365844727, -0.2024897187948227, -0.3438516855239868, 0.21061818301677704, -0.19348564743995667, 0.39097779989242554, 1.1576173305511475, 0.5789440274238586, 0.11609727144241333, -0.437576562166214, -0.39427515864372253, -0.46527865529060364, -1.2827986478805542, 0.17925545573234558, -1.0319867134094238, -0.2847364842891693, -0.0925396978855133, -0.12203194200992584, -0.5646570920944214]}, "authors": [{"authorId": "2280917335", "name": "Bowen Zhao"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}, {"authorId": "2253406610", "name": "Qingqing Cao"}], "references": [{"paperId": "97f3cf88bc592a323e62dffe20e522de44957076", "title": "CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"paperId": "024a25b2445ecb3a181c5e2f39fbf8b73a4c1a6f", "title": "Emergent Modularity in Pre-trained Transformers"}, {"paperId": "017010b941d902a467f6d329ae5e74fd67e67912", "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"}, {"paperId": "6007263dd3d14373be5f84fb6ccb0be3f7fce903", "title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning"}, {"paperId": "76b19363b10d7ea783e4a6494eae40d73c8e9628", "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "92c1430d29f1b4b26077370d09fbc9dc06fe901c", "title": "Task-Specific Skill Localization in Fine-tuned Language Models"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "5d3cf0909ba206cd6bc2e86610f77ca25d9b2d1c", "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "0bcedac1643e22b39df3c820c4f9b767a910877c", "title": "Structural Pruning via Latency-Saliency Knapsack"}, {"paperId": "7bd551537b67a31a733ac12a8de69968bc190f66", "title": "SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "03d19fde1df67c7ea8dedc750dcd3a6291032577", "title": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0", "title": "A Fast Post-Training Pruning Framework for Transformers"}, {"paperId": "d9cdf21e73519edc593bdf1a00fcd778764b13f6", "title": "Training Neural Networks with Fixed Sparse Masks"}, {"paperId": "4b4cf6cc67f23635449e59222c055dfd87ab34bd", "title": "When to Prune? A Policy towards Early Structural Pruning"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "651114de017abc76b8e1aa2c1c30a370b463b6ee", "title": "RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "40235eded15f44c8c4a7f48468adcc7df4e171fb", "title": "Rethinking Network Pruning \u2013 under the Pre-train and Fine-tune Paradigm"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb", "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "b2c3f631999857d26a9abc4895ca6a9531d54a8e", "title": "Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "d7db793f9824a179518a3eb4ea3bd8bc620f6b6f", "title": "Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": "2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c", "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "89654f548e541a8ef233be6585316ce6cc201535", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "73b5c30c7cd689906d1b074e052e2a2cfc1f0675", "title": "Section"}, {"paperId": "6cbb968ef5d81f33134c7022b12241d1bbef47ff", "title": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference"}, {"paperId": null, "title": "of quantized"}, {"paperId": null, "title": "2023. Qlora:"}, {"paperId": null, "title": "trained on the objective L , the salience scoring function S is defined as:"}, {"paperId": null, "title": "Hypeparameter GLUE-small GLUE-big SQuAD CNN/DM Alpaca Learning rate 2e-4 2e-4 2e-4 1e-4 1e-4 Batch size 32 32 32 16 32"}, {"paperId": null, "title": "the 2021 Conference of the North American Chapter"}, {"paperId": null, "title": "time-consuming, we did not do the TTA evaluation as small models. We do not conduct any hyper-parameters search for any training for fair comparison"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}]}