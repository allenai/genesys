{"paperId": "28a840d36aeabb165804c293f9b91a47d118f3b9", "abstract": "Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision Transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. Based on this basic design, we develop a series of isotropic models with a Transformer-style simple architecture and CNN-style hierarchical models with better performance. Isotropic GFNet models exhibit favorable accuracy/complexity trade-offs compared to recent vision Transformers and pure MLP models. Hierarchical GFNet models can inherit successful designs in CNNs and be easily scaled up with larger model sizes and more training data, showing strong performance on both image classification (e.g., 85.0% top-1 accuracy on ImageNet-1 k without any extra data or supervision, and 87.4% accuracy with ImageNet-21 k pre-training) and dense prediction tasks (e.g., 54.3 mIoU on ADE20 k val). Our results demonstrate that GFNet can be a very competitive alternative to Transformer-based models and CNNs in terms of efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2023, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper presents the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture that learns long-term spatial dependencies in the frequency domain with log-linear complexity and can be a very competitive alternative to Transformer-based models and CNNs in terms of efficiency, generalization ability and robustness."}, "embedding": {"model": "specter_v2", "vector": [0.41172268986701965, 0.432447224855423, -0.04941590875387192, 0.15926921367645264, 0.40267372131347656, 0.4286387860774994, 0.7009223103523254, -0.4083852767944336, -0.6058013439178467, -0.6609755158424377, 0.8162606358528137, 0.6738307476043701, 0.36615538597106934, -0.5382217168807983, -0.158794105052948, -0.13553336262702942, -0.9460365772247314, -0.5472007989883423, 0.546188235282898, -0.1817009299993515, 0.15961535274982452, -0.4156789779663086, -1.4581022262573242, 0.46245500445365906, -0.2703549265861511, 1.4231921434402466, 0.45125675201416016, 1.3695101737976074, 0.011903127655386925, 0.7980256676673889, 0.5398325324058533, 0.3071248233318329, 0.4550958573818207, -0.21188460290431976, -0.31650039553642273, -0.25146886706352234, 0.6645398736000061, -0.5423345565795898, -0.9007053971290588, 1.0841394662857056, -0.1344146579504013, 0.31899192929267883, 0.7404395341873169, -0.1849966198205948, -0.21739467978477478, 0.22288504242897034, 0.687184751033783, 0.9337372183799744, -0.4103507101535797, -0.21236059069633484, 1.0870848894119263, -1.155368447303772, 0.15846747159957886, 1.60175621509552, 0.5198163986206055, 0.42777255177497864, -0.03133349120616913, -0.34302929043769836, 0.7829895615577698, -0.04202655330300331, -0.574711263179779, -0.14377999305725098, 0.053259819746017456, -0.5697284936904907, 1.9670491218566895, -0.741620659828186, 0.08623172342777252, 0.7817937731742859, 0.5986562967300415, 0.754560649394989, 0.21878454089164734, -0.9408628940582275, -0.37894564867019653, -0.03099834732711315, 0.4355008900165558, 0.7160235047340393, -0.4921601414680481, 0.6571240425109863, -1.1655503511428833, 0.15900319814682007, 1.313130259513855, -0.159010112285614, 0.02280040644109249, 0.3179526627063751, -0.3887901306152344, 0.5356032252311707, 1.0139477252960205, 0.47558140754699707, -0.48947885632514954, 1.0407437086105347, 0.23574014008045197, 0.22098389267921448, -0.11610198020935059, 0.4824560880661011, 0.18992061913013458, 0.8024572134017944, -0.38476601243019104, 0.07895460724830627, -0.32632720470428467, 0.7443069219589233, 0.21809659898281097, 0.3307431638240814, -0.16560205817222595, -0.011700610630214214, 1.119341254234314, 0.032358501106500626, 0.3315514922142029, -0.6318760514259338, -0.4035108983516693, -0.8514090180397034, -0.21441970765590668, -1.1868916749954224, -0.34396713972091675, -0.8268324136734009, -1.2172884941101074, -0.4484293758869171, -0.2804349660873413, 0.52078777551651, -0.8238622546195984, 0.5751389861106873, -0.4938269853591919, 0.030354302376508713, -0.16171026229858398, 0.4841068983078003, 0.5047239065170288, 0.31180500984191895, 0.42119452357292175, 0.6758056879043579, 1.492348074913025, -1.0681095123291016, -0.3978204131126404, -0.7687599658966064, -0.37202414870262146, -0.15377666056156158, 0.07683458924293518, -0.15283618867397308, -0.8531873822212219, -1.7220268249511719, -0.8006908893585205, -0.09532647579908371, -0.7531887292861938, 0.23774464428424835, 1.4385557174682617, -0.10750981420278549, -0.8547574877738953, 1.1212226152420044, -0.21817472577095032, -0.3535981774330139, 0.5674677491188049, 0.1059059426188469, 0.30975285172462463, 0.3114525079727173, -1.0894014835357666, 0.3893260359764099, 0.3316477835178375, -0.4475436806678772, -0.3903428912162781, -0.6015489101409912, -0.7854783535003662, 0.15393996238708496, -0.040952760726213455, -1.2493573427200317, 0.9483096599578857, -0.41525551676750183, -0.656292736530304, 0.5670853853225708, -0.006307224743068218, -0.42321956157684326, -0.013407601043581963, -0.35885190963745117, -0.5437228083610535, -0.003224270883947611, -0.21219994127750397, 0.8836095333099365, 0.8720245957374573, -0.1847137063741684, -0.5297284722328186, 0.0007609075983054936, -0.35362523794174194, -0.2826552093029022, -0.6729977130889893, 0.8013556599617004, -0.337565541267395, -0.6678951382637024, 0.3552139699459076, 0.7534431219100952, 0.028006866574287415, -0.014015290886163712, 0.10366930067539215, -0.8187482357025146, 0.7301254868507385, 0.26945242285728455, 0.5704623460769653, -1.204827070236206, -1.141310453414917, -0.1396985799074173, 0.21032775938510895, -0.5520025491714478, -0.7262327671051025, 0.15996821224689484, -0.38241949677467346, 0.10118895024061203, 0.253993421792984, -0.48464927077293396, -0.5364284515380859, -0.6902820467948914, -0.8098719716072083, -0.05144428089261055, 0.38977694511413574, 0.8173195123672485, -1.0122286081314087, 0.012140551581978798, 0.012323628179728985, 0.5011025071144104, -1.040908932685852, 1.183733344078064, -0.3635301887989044, 0.14820300042629242, -0.0854245200753212, -0.2541975677013397, 0.10985709726810455, -0.5943335294723511, 0.015724601224064827, -0.9304477572441101, -0.07669249176979065, 0.4361107349395752, -0.1677418053150177, 1.3843326568603516, -0.2532474398612976, 0.9019630551338196, -0.07793288677930832, -0.16591988503932953, 0.05698161572217941, 0.2096148282289505, 0.0833970382809639, -0.6753034591674805, 0.3907313942909241, -0.0015426070895045996, -0.6945978999137878, 0.38112086057662964, 0.17100420594215393, 1.2624590396881104, -0.08915261179208755, -0.12833143770694733, 1.1950114965438843, -0.09328161925077438, -0.07619388401508331, 0.2882173955440521, 0.7974556088447571, 0.3919166922569275, 0.38240718841552734, -0.49496087431907654, -0.025510253384709358, -0.9482672810554504, -0.32024258375167847, 1.1092016696929932, 0.6495753526687622, 1.3216087818145752, -0.0193161703646183, -0.9533215165138245, -0.46657198667526245, -0.09845057874917984, 0.3887954652309418, 0.9938982129096985, -0.14463099837303162, 0.16021579504013062, -0.4760143458843231, -0.21160884201526642, -0.2167678028345108, -0.6552860736846924, -0.6550186276435852, -0.23823706805706024, -0.23846380412578583, -0.7523319125175476, 0.460771381855011, 0.532552182674408, 1.6521949768066406, -0.7328082919120789, -0.6130107641220093, -0.24520380795001984, 0.8425837159156799, -0.8263052701950073, -0.6275397539138794, 0.3896377682685852, -0.24192394316196442, -0.2559072971343994, -0.06559095531702042, -0.11949456483125687, 0.3035886585712433, -0.2475079596042633, 0.686512291431427, -0.5869088768959045, -0.6684237122535706, 0.4824368953704834, 0.686542272567749, -0.8040351271629333, -0.0253454577177763, 0.2030794322490692, 0.14629009366035461, 0.02982128970324993, 0.279474675655365, -0.054518938064575195, -0.749137818813324, 0.08766798675060272, -0.24476183950901031, -0.03186020255088806, 0.21505573391914368, 0.20952573418617249, 0.5002292394638062, 0.37409693002700806, 0.1871655285358429, -1.042894721031189, 0.6262930035591125, 0.20483465492725372, -0.28875431418418884, -0.3652224838733673, -0.5513309240341187, -0.3028424382209778, 0.5487412214279175, -0.5226491093635559, -0.4316519498825073, -0.4100832939147949, 0.27575287222862244, -0.5133168697357178, -0.6694939732551575, -0.1274702250957489, 0.4083916246891022, -0.6095680594444275, 0.9929904937744141, -0.08328408747911453, 0.17016707360744476, 0.09714478254318237, 0.14912040531635284, -1.080356240272522, 1.23219895362854, 0.4670676589012146, 0.3087528944015503, 0.5757861137390137, 0.11831364780664444, -1.0266202688217163, -0.892297625541687, -0.9361300468444824, -0.4793013334274292, -0.771388590335846, 0.6887127757072449, -0.818809449672699, -1.0534875392913818, 0.4520680010318756, -0.9782655835151672, -0.22522711753845215, 0.029423970729112625, 0.19750282168388367, -0.4811713993549347, -0.9861136078834534, -0.80633544921875, -0.7039389610290527, -0.2926413416862488, -0.240015909075737, -0.004263896029442549, 0.48874881863594055, -0.2720251977443695, -0.5873721837997437, -0.5209813117980957, -0.7348398566246033, 1.3702610731124878, -0.19788479804992676, 0.44124361872673035, 0.39447999000549316, -0.6543707251548767, -0.21597692370414734, -0.10716527700424194, 0.7688210010528564, -0.18504483997821808, 0.06958556175231934, -1.261508584022522, 0.4949466586112976, -0.04148770868778229, -0.1413683146238327, 1.0912827253341675, 0.6033105850219727, 0.7616419792175293, 0.2128644436597824, -0.18002021312713623, 0.6757753491401672, 1.7063559293746948, -0.8166003823280334, 0.14015448093414307, 0.2856491804122925, 0.7621200084686279, -0.009331031702458858, -0.49157586693763733, 0.5050531625747681, 0.17895261943340302, 0.05773086100816727, 0.5372263193130493, -0.6717844605445862, -1.1322518587112427, -0.8811935782432556, -0.10021321475505829, 0.5186178684234619, -0.06838054209947586, 0.3793676495552063, -0.7679809927940369, 0.7735720276832581, -1.0926132202148438, -0.8792552947998047, 0.7201625108718872, 0.7260511517524719, -0.14650477468967438, 0.003763172309845686, -0.2126295119524002, -0.5293852090835571, 0.4722592532634735, 0.4049972891807556, -0.2502795457839966, -0.10037054121494293, -0.4704872667789459, 0.47289493680000305, 0.7211624979972839, 0.6419592499732971, -1.1304770708084106, 0.6199673414230347, 14.630744934082031, 0.3425094187259674, -0.16390439867973328, 0.41491806507110596, 0.5455312728881836, 0.6400399208068848, -0.09102421998977661, 0.18749430775642395, -1.288819670677185, -0.4184342622756958, 0.6200070977210999, 0.46415525674819946, 0.7336540222167969, 0.23460564017295837, -0.18790732324123383, 0.005115262232720852, -0.1535993069410324, 0.9329963326454163, 0.33159783482551575, -1.3777962923049927, 0.28439861536026, 0.2566382586956024, 0.35228511691093445, 0.46819302439689636, 0.8073225021362305, 0.21883061528205872, 0.3239082098007202, -0.42776036262512207, 0.31358152627944946, 0.7633298635482788, 0.8504524827003479, 0.29307684302330017, -0.09328095614910126, 0.4472618103027344, -1.1976937055587769, -0.45706260204315186, -0.867717981338501, -1.0837103128433228, -0.05808139219880104, 0.3503466844558716, -0.005847102031111717, -0.5708664655685425, 0.45743072032928467, 0.5876770615577698, -0.025898972526192665, 0.5260288119316101, -0.07772362977266312, 0.07734248787164688, -0.26684707403182983, 0.1627342253923416, 0.15793418884277344, 0.9288463592529297, 0.11305171996355057, 0.0971524715423584, -0.17480334639549255, 0.4556414484977722, -0.06439464539289474, 0.5746411681175232, -0.3436026871204376, -0.4157080352306366, -0.2437027245759964, -0.1388958841562271, -0.46649855375289917, 0.8382319211959839, 0.3145529329776764, 0.35191044211387634, -0.32088279724121094, 0.17763808369636536, 0.3289678692817688, 0.5012144446372986, -0.24659930169582367, -0.1048121377825737, 0.4013074040412903, -0.11239385604858398, 0.11989584565162659, 0.07882263511419296, -0.3152470588684082, -0.5493882894515991, -0.8631386160850525, 0.00968252494931221, 0.47323253750801086, -0.7848438620567322, -0.7855738401412964, 1.0858070850372314, -0.39639246463775635, -0.0060171992518007755, 0.6481215953826904, -1.3070622682571411, -0.29584595561027527, 0.5967972874641418, -1.4022266864776611, -0.6344707608222961, -0.43702924251556396, -0.2479742169380188, -0.3153022229671478, -0.2617245316505432, 0.927361786365509, -0.17677266895771027, 0.20102369785308838, -0.03351547196507454, -0.8137995600700378, -0.05846996232867241, -0.14146633446216583, -0.5925310254096985, 0.7885327339172363, 0.3523508906364441, -0.1678064912557602, 0.27566099166870117, 0.03254307061433792, 0.08010143786668777, -0.719305157661438, 0.06095463037490845, 0.3518645167350769, -0.47281572222709656, -0.2661971151828766, -0.8127880096435547, -0.7163324952125549, -0.1374579221010208, 0.6748567223548889, 0.20415009558200836, -0.36883899569511414, 0.15039768815040588, -0.949535608291626, -0.21444444358348846, -0.45084935426712036, -0.17823265492916107, 0.5291678309440613, -1.0018939971923828, -0.04390690475702286, 0.014536858536303043, -0.11203642934560776, -0.7309446930885315, -0.5557851195335388, 0.08646097034215927, 0.45836496353149414, -0.7959794402122498, 1.293416142463684, -0.3934561014175415, 0.39345505833625793, 0.6177653670310974, -0.1357407420873642, -0.4329059422016144, -0.5946398973464966, -0.8188517093658447, 0.02135290764272213, -0.15946029126644135, 0.2693033516407013, -0.9485328197479248, 0.11911571025848389, 0.27497854828834534, 0.17474766075611115, -0.33517932891845703, -0.37481462955474854, -0.1749875694513321, -0.474910169839859, -0.45599305629730225, -0.06331271678209305, -0.30198797583580017, -0.37324288487434387, 0.09400109201669693, 0.4982924163341522, 0.7863671183586121, 0.21739009022712708, -0.6692215204238892, 0.22549614310264587, -0.47686487436294556, -0.442295640707016, -0.8077925443649292, -0.9350635409355164, -1.8467527627944946, -0.14513500034809113, -0.8146919012069702, -0.491327166557312, -0.9611427187919617, -0.6158429980278015, 0.5411250591278076, -0.47065505385398865, -0.10372298955917358, 0.5014021992683411, -0.13424541056156158, -0.4404982924461365, -0.18159157037734985, -0.2843427360057831, 0.623038649559021, 1.0127980709075928, -0.6132692098617554, 0.2117026448249817, 0.14530935883522034, -0.24196982383728027, 0.6849988698959351, 0.2487533986568451, -0.2272700071334839, -0.6502953171730042, -1.0124293565750122, 0.30077654123306274, -0.5355380177497864, 0.404246062040329, -1.7059698104858398, 1.3220946788787842, 0.6106786727905273, 0.5725868344306946, -0.08117090165615082, 0.42661991715431213, -0.8922511339187622, -0.7042612433433533, 0.22047662734985352, -0.6378512978553772, -0.0626484826207161, 0.16269958019256592, -0.03774142265319824, -0.44672641158103943, 1.1826187372207642, 0.350215882062912, -1.4809483289718628, -0.8946897387504578, 0.5413386821746826, -0.34258970618247986, -0.18757812678813934, -0.35705995559692383, -0.24403157830238342, -1.4211503267288208, -0.44980764389038086, -0.3286375105381012, 0.12669561803340912, -0.6123592257499695, 0.9014065265655518, 0.7073981165885925, -1.1603186130523682, 0.18156692385673523, 0.7418379783630371, -0.3551080524921417, 0.07643788307905197, 0.8348248600959778, 0.23386937379837036, -0.17457623779773712, 0.8896386027336121, -0.15737774968147278, 0.3081226944923401, -0.3037683069705963, 0.3601376712322235, 1.1098567247390747, 0.22990931570529938, -0.3475576937198639, 1.2228901386260986, 0.3923356533050537, -0.2651975750923157, 0.24156823754310608, -1.0587447881698608, -0.42179396748542786, -0.09265483915805817, 0.40263861417770386, -0.04407985135912895, -0.10676782578229904, -0.08417557924985886, -0.26132267713546753, 0.6230623722076416, 0.09639309346675873, -0.5545614957809448, 0.32854118943214417, 0.18041451275348663, -0.10447584837675095, 0.46801143884658813, 0.999127209186554, -0.7653053402900696, -1.1268905401229858, -0.898991048336029, -0.7179979085922241, -0.3320760726928711, 0.6032523512840271, -0.10957954078912735, -0.7859115600585938, 0.7780672907829285, 0.824008047580719, 0.5038728713989258, 0.896541178226471, -0.04513705149292946, -0.26389411091804504, 0.5191509127616882, -0.10572196543216705, -0.6563576459884644, -0.20575478672981262, 1.5046749114990234, 1.3326212167739868, -0.465197890996933, 0.18642376363277435, -0.5658896565437317, -0.061561569571495056, 0.9884290099143982, 0.649851381778717, -0.6904292106628418, 1.518547534942627, -0.32253900170326233, 0.1007656678557396, 0.03844332695007324, -1.020895004272461, -0.6417790055274963, 0.922313928604126, 1.013959527015686, 0.40689653158187866, -0.17166420817375183, 0.3878694474697113, 0.5624366998672485, 0.10627532750368118, -0.15736836194992065, 0.006477273069322109, 0.24841704964637756, -0.348899781703949, 0.6216773390769958, 0.09387367963790894, 0.6520959734916687, -0.2971155643463135, -0.266801118850708, 0.2357458919286728, 0.506126880645752, 0.3063167631626129, 0.2092684507369995, 1.0539642572402954, -0.04316095635294914, 0.7263200283050537, -0.14098525047302246, 0.8909171223640442, -0.4331852197647095, -0.3277135491371155, 0.2770928740501404, -0.622915506362915, -0.13685527443885803, -0.4468540549278259, -0.6639506220817566, -0.2939075827598572, 0.38915157318115234, 0.10491745918989182, -0.4397755265235901, 0.35626837611198425, 0.5170259475708008, 0.4811570942401886, 0.8039687871932983, -0.07544418424367905, -0.6280983090400696, -0.19029934704303741, -1.1035034656524658, 0.09827917814254761, -0.6530584096908569, 0.26112958788871765, -0.6525210738182068, -0.5070391893386841, -0.2329900860786438]}, "authors": [{"authorId": "39358728", "name": "Yongming Rao"}, {"authorId": "2118223312", "name": "Wenliang Zhao"}, {"authorId": "1490318512", "name": "Zhengbiao Zhu"}, {"authorId": "48128428", "name": "Jie Zhou"}, {"authorId": "1697700", "name": "Jiwen Lu"}], "references": [{"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "9f1b0e4c42a5a85d4c023030557ade4419f82ecf", "title": "Scaling Up Your Kernels to 31\u00d731: Revisiting Large Kernel Design in CNNs"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "f75cddf2d42ed01b34686704eb3504becef67442", "title": "CycleMLP: A MLP-like Architecture for Dense Prediction"}, {"paperId": "260ad39a1dac4b451019e2bf17925f4df8e3b69a", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "9b6af0e358e76d22f209c75b1702c3e6ea7815b1", "title": "Global Filter Networks for Image Classification"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "60707f6d2bffeab09e8f1d073fce4fc06ab89ec1", "title": "S2-MLP: Spatial-Shift MLP Architecture for Vision"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "6b6ffb94626e672caffafc77097491d9ee7a8682", "title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "0d5406775fab3e71848908327fb5504df5f60f92", "title": "ImageNet-21K Pretraining for the Masses"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "7c3ce1b3ad598a282546e03e2dc8b52c338caed6", "title": "Transformer Tracking"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "d29430adccb805ab57b349afa8553954347b3197", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "f5c8464032a936451b222be1984cabf42d6adfa8", "title": "Are we done with ImageNet?"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "13da774fe604027bff2951ba82f4c3d9be7e415e", "title": "Augment Your Batch: Improving Generalization Through Instance Repetition"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "f15aecec2a5672beaae77853ea0eea560505df8e", "title": "FDA: Fourier Domain Adaptation for Semantic Segmentation"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "fb7972f30812c7dd056d7943c3e3f00af022d607", "title": "Dynamic Convolution: Attention Over Convolution Kernels"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7", "title": "Natural Adversarial Examples"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "49b64383fe36268410c430352637ed23b16820c5", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"}, {"paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"paperId": "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1", "title": "Panoptic Feature Pyramid Networks"}, {"paperId": "b383c8dc570d86d4dc2f9d22ca054cfe245bef6d", "title": "FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "c90b38b0a64b22808ecd0e48170923b3bd459469", "title": "Single-Image Depth Estimation Based on Fourier Domain Analysis"}, {"paperId": "9309ddee47f24660690b5cb4be4fd1918189785f", "title": "Channel Gating Neural Networks"}, {"paperId": "04957e40d47ca89d38653e97f728883c0ad26e5d", "title": "Cascade R-CNN: Delving Into High Quality Object Detection"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "2c031aea0d2eec0ff0e1947bb3c94dc81b46e3a5", "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "aba48504f4f9563eafa44e0cfb22e1345d767c80", "title": "Dynamic Filter Networks"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb", "title": "Explaining and Harnessing Adversarial Examples"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "a83cec6a91701bd8500f8c43ad731d4353c71d55", "title": "3D Object Representations for Fine-Grained Categorization"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "474468920cf66823c86b362f0e04b2fba410d59c", "title": "Professor"}, {"paperId": "d71ddd89c327c8121cf302b75adf4e144552e332", "title": "Digital Image Processing Algorithms and Applications"}, {"paperId": "d1476fde47a7b3683629be974336d978bc76010a", "title": "Digital Image Processing: Principles and Applications"}, {"paperId": "6dc61f37ecc552413606d8c89ffbc46ec98ed887", "title": "Acceleration of stochastic approximation by averaging"}, {"paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "title": "Backpropagation Applied to Handwritten Zip Code Recognition"}, {"paperId": "0e6beb95b5150ce99b108acdefabf70ccd3fee30", "title": "An algorithm for the machine calculation of complex Fourier series"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "986e339f3ab16c44b57b0544fe7aecb96679eeb3", "title": "Improve Vision Transformers Training by Suppressing Over-smoothing"}, {"paperId": "b00dc5743fb6e57e3024cb9826f909f8b795f758", "title": "MorphMLP: A Self-Attention Free, MLP-Like Backbone for Image and Video"}, {"paperId": "976a609cf540d1ded373b872d34779f7164d840a", "title": "Rethinking the Design Principles of Robust Vision Transformer"}, {"paperId": "279696f90d13b1327ec7adb73e711e7d8f5db761", "title": "Token Labeling: Training a 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "ZhengZhu (Member,IEEE)receivedthePhDdegree from the Institute of Automation, Chinese Academy of Sciences"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "and 1992, respectively, and the PhD degree from the Institute of Pattern Recognition and Arti\ufb01cial Intelligence, Huazhong University"}, {"paperId": null, "title": "several international conferences and journals"}, {"paperId": null, "title": "[ [ than 40 journal and conference papers computervisionandroboticsproblems,suchasfacerecognition, humanposeestimation,andservocontrol.Hehasmorethan3,000GoogleScholarcitationstohiswork"}, {"paperId": null, "title": "invariousjournalsandconferencesincluding IEEE"}]}