{"paperId": "7f1a473834eea608980e4e04cce21be18d65b9b6", "abstract": "As an efficient alternative to conventional full finetuning, parameter-efficient finetuning (PEFT) is becoming the prevailing method to adapt pretrained language models. In PEFT, a lightweight module is learned on each dataset while the underlying pretrained language model remains unchanged, resulting in multiple compact modules representing diverse skills when applied to various domains and tasks. In this paper, we propose to compose these parameter-efficient modules through linear arithmetic operations in the weight space, thereby integrating different module capabilities. Specifically, we first define addition and negation operators for the module, and then further compose these two basic operators to perform flexible arithmetic. Our approach requires \\emph{no additional training} and enables highly flexible module composition. We apply different arithmetic operations to compose the parameter-efficient modules for (1) distribution generalization, (2) multi-tasking, (3) unlearning, and (4) domain transfer. Additionally, we extend our approach to detoxify Alpaca-LoRA, the latest instruction-tuned large language model based on LLaMA. Empirical results demonstrate that our approach produces new and effective parameter-efficient modules that significantly outperform existing ones across all settings.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 58, "influentialCitationCount": 9, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.14870", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes to compose parameter-efficient modules through linear arithmetic operations in the weight space, thereby integrating different module capabilities and extends this approach to detoxify Alpaca-LoRA, the latest instruction-tuned large language model based on LLaMA."}, "embedding": {"model": "specter_v2", "vector": [0.23114800453186035, 0.2949341833591461, -0.3810056746006012, -0.1755407601594925, -0.10780046880245209, -0.2748096287250519, 0.42439645528793335, -0.20328110456466675, -0.8720930814743042, -0.02042313478887081, 0.2601815462112427, 0.08429287374019623, 0.17617134749889374, 0.022270172834396362, -0.24906481802463531, 0.2799827456474304, -0.7981915473937988, 0.6061645746231079, 0.18619920313358307, -0.6128884553909302, -0.5446120500564575, -0.5152995586395264, -0.91599041223526, -0.07242748886346817, 0.6855515837669373, 0.6280577778816223, 0.46224915981292725, 0.88676518201828, -0.3880805969238281, -0.0775468721985817, 0.2593921422958374, -0.31629693508148193, 0.1999989151954651, 0.12937991321086884, -0.342418372631073, -0.007636346388608217, 0.3955693244934082, -0.23510438203811646, -0.32055529952049255, 0.832382321357727, -0.14387555420398712, 0.5170736312866211, 0.7883597612380981, -0.35682186484336853, -0.19765105843544006, 0.6191996335983276, 0.8124663829803467, 0.6937699913978577, -0.537161111831665, -0.46865978837013245, 0.966870903968811, -1.3105144500732422, -0.27520644664764404, 1.3040803670883179, 0.4841034412384033, 0.8222740292549133, -0.4241699278354645, -0.6835660338401794, 0.5150592923164368, -0.27046000957489014, -0.6458414196968079, 0.07296478003263474, -0.23689468204975128, -0.1157323569059372, 1.9662196636199951, -0.5942162871360779, -0.2567480504512787, 0.6300469636917114, 0.2118867188692093, 1.4792357683181763, 0.055387675762176514, -0.48248666524887085, -0.4293046295642853, 0.11246766895055771, 0.25847935676574707, 0.949365496635437, -0.32490015029907227, 0.0164292361587286, -0.6783148646354675, -0.009068626910448074, 0.5154883861541748, -0.00043634598841890693, 0.2641984224319458, -0.4645114541053772, -0.6229802370071411, 0.7479977607727051, 0.3393775522708893, 0.7778849601745605, -0.31526392698287964, 0.6589833498001099, 0.7077857255935669, 0.47477778792381287, -0.0767086073756218, 0.7030771374702454, -0.5467021465301514, 0.3314022421836853, -0.5246596932411194, 0.33947041630744934, -0.1136125698685646, 0.9582825899124146, -0.007877052761614323, 0.2207399606704712, -0.8015548586845398, 0.31353744864463806, 1.2272107601165771, 0.2218467891216278, 0.8008170127868652, -0.5458469986915588, 0.5705758929252625, -0.6151137351989746, 0.07847802340984344, -0.20148156583309174, -0.42280134558677673, -0.41058850288391113, -0.8443588614463806, -1.4316303730010986, -0.49519938230514526, 0.010494416579604149, -0.8860631585121155, 1.1122081279754639, -0.47473376989364624, 0.0053623029962182045, -0.08541858941316605, 0.23181767761707306, 0.2975655198097229, 0.4132097065448761, 0.92520672082901, 0.3980681896209717, 0.910236656665802, -0.8938026428222656, -0.6304367184638977, -1.2058558464050293, 0.5101973414421082, -0.2595250606536865, 0.6303972005844116, -0.06055097654461861, -1.3266336917877197, -1.2802790403366089, -1.1133118867874146, -0.02156246453523636, -0.8677193522453308, 0.37400317192077637, 1.1099456548690796, 0.5185547471046448, -0.6628904938697815, 0.7466409206390381, 0.041615188121795654, -0.06902410089969635, 0.20725587010383606, 0.6328719854354858, 0.1428358554840088, -0.3456892967224121, -1.6821293830871582, 0.3322712481021881, 0.9408493638038635, -0.8738265633583069, -0.28252360224723816, -0.6306002736091614, -0.9640558958053589, 0.027535896748304367, 0.21779732406139374, -0.9598695039749146, 1.2113981246948242, -0.5048777461051941, -1.304128885269165, 0.506041944026947, 0.1755325198173523, 0.023743895813822746, 0.2457452416419983, -0.25367406010627747, -0.5653637647628784, -0.35895460844039917, -0.6113622784614563, 0.8810504674911499, 0.8338725566864014, 0.27715128660202026, 0.024493560194969177, 0.4906844198703766, -0.07769564539194107, 0.42318060994148254, -0.6804870963096619, 0.6562311053276062, -0.29351183772087097, -0.34760144352912903, 0.34552061557769775, 0.7688654065132141, 0.24450542032718658, -0.4306980073451996, 0.017754053696990013, -1.1874873638153076, 0.4037192463874817, 0.26147159934043884, 1.047240972518921, -0.94129478931427, -0.4075193703174591, 0.015987377613782883, -0.09020449966192245, -0.08382607996463776, -1.258409857749939, 0.13441653549671173, -0.41175222396850586, 0.20645290613174438, -0.08622927963733673, -1.3835843801498413, 0.3149624764919281, -0.2140364944934845, -0.36580246686935425, -0.176284059882164, 0.26250720024108887, 1.0192400217056274, -0.8303858041763306, 0.17430898547172546, -0.32358965277671814, 0.2847263514995575, -1.324721336364746, 1.1810235977172852, -0.6990978121757507, 0.020991072058677673, -0.08133396506309509, -0.2205902338027954, 0.0593685582280159, -0.3044189214706421, 0.41098856925964355, -0.5424428582191467, 0.6812019944190979, 0.33392828702926636, -0.40885689854621887, 1.832356333732605, -0.6364335417747498, 0.0860876590013504, -0.12045153230428696, -0.3691893219947815, 0.36312463879585266, 0.3641721308231354, -0.12344049662351608, -0.3100106120109558, 0.3953242003917694, 0.5756364464759827, -0.4738382399082184, 0.22559763491153717, 1.1898537874221802, 0.5712686777114868, -0.6792812943458557, 0.17932593822479248, 0.8295373320579529, -0.4923393428325653, 0.5793896913528442, 0.14061801135540009, 0.3512226641178131, 0.31117069721221924, 0.41389480233192444, -0.13688410818576813, 0.6930891871452332, -1.1766093969345093, -0.29879698157310486, 0.45803162455558777, 0.35542038083076477, 0.6085125207901001, -0.018519485369324684, -0.5431438684463501, -0.400766521692276, -0.48615384101867676, 0.751059889793396, 2.1746327877044678, -0.2756933569908142, 0.37615829706192017, -0.8292980194091797, -0.5344558358192444, 0.08165694773197174, -0.08800993859767914, -0.4788602292537689, -0.32369694113731384, -0.7122302651405334, -1.210801124572754, 0.6871869564056396, 0.49005863070487976, 1.2510790824890137, -0.5672348737716675, -0.13522592186927795, -0.18638208508491516, 0.06958308815956116, -0.5192620158195496, -1.1056623458862305, 0.3352503776550293, -0.624284565448761, -0.049433063715696335, -0.04644250124692917, -0.06832171976566315, 0.26047343015670776, -0.6700589060783386, 1.1602421998977661, -0.8997305035591125, -0.021481182426214218, 0.3621833622455597, 0.7396085858345032, -0.4687343239784241, -0.8715826869010925, 0.5817957520484924, 0.45361781120300293, 0.02938007563352585, 0.5224995017051697, 0.5810601711273193, -0.053348250687122345, 0.35016703605651855, -0.643466055393219, 0.013928067870438099, 0.5465260744094849, 0.14583976566791534, 1.0188491344451904, -0.26135244965553284, 0.5129019021987915, -1.6033480167388916, 1.401832103729248, -0.17582453787326813, -0.05270015075802803, 0.28090405464172363, -0.39248451590538025, -0.35524114966392517, 0.478617787361145, -1.135288953781128, -0.35378310084342957, -0.8338497877120972, 0.32775214314460754, -0.4213707447052002, -0.19763505458831787, 0.32274380326271057, 0.21545271575450897, 0.1245235875248909, 0.6363950967788696, 0.609133243560791, 0.46778902411460876, -0.041165247559547424, 0.6316268444061279, -0.8843677043914795, 0.5762301683425903, -0.23701032996177673, 0.12630444765090942, -0.493013471364975, -0.3957096040248871, -0.1686418652534485, -0.447394460439682, -0.588718593120575, -0.39220064878463745, -0.10655621439218521, 0.19835664331912994, -0.7954652905464172, -0.9209650754928589, 0.08807282149791718, -0.6894853115081787, -0.46256372332572937, 0.10540307313203812, -0.20137910544872284, -0.34538015723228455, -1.2738752365112305, -0.9533697962760925, -0.052154283970594406, -0.7105457782745361, -1.156704068183899, -0.06049824878573418, 0.15461482107639313, -0.41148701310157776, -0.9807828664779663, -0.109283447265625, -0.31894633173942566, 1.6619147062301636, -0.7955508828163147, 0.9935117363929749, -0.2771952748298645, 0.08682964742183685, -0.42934417724609375, 0.1477368175983429, 0.9908140301704407, -0.26774105429649353, -0.3400720953941345, -1.0387604236602783, -0.2769538462162018, -0.4063554108142853, -0.6451682448387146, 0.329092800617218, -0.023775476962327957, 1.049659013748169, 0.1492878496646881, -0.20532575249671936, 0.5248011350631714, 1.1488828659057617, -1.025771975517273, -0.057905953377485275, 0.3084471523761749, 0.849949836730957, 0.11249472200870514, -0.4097364842891693, 0.2764774560928345, 0.37339600920677185, 0.18288061022758484, -0.07760129123926163, 0.17268164455890656, -0.33682340383529663, -0.6093279123306274, 0.746281087398529, 1.5563651323318481, 0.3061316907405853, -0.035246215760707855, -0.7401489615440369, 0.3401919901371002, -0.8863780498504639, -0.3823581039905548, 1.1106092929840088, 0.8783352375030518, 0.8023691773414612, -0.3581225574016571, -0.1967761367559433, -0.5417354702949524, 0.028830355033278465, 0.37390023469924927, -0.48403987288475037, -0.8601201772689819, -0.025013186037540436, 0.4954436123371124, 0.2423824518918991, 0.38273119926452637, -0.8489078879356384, 0.5293989777565002, 14.424811363220215, 1.1255379915237427, 0.08809324353933334, 0.7163400053977966, 0.6040732860565186, 0.40654996037483215, -0.5894673466682434, -0.7340225577354431, -1.12673020362854, -0.028370769694447517, 1.2261123657226562, 0.5055931210517883, 0.7473182678222656, 0.14400643110275269, -0.24006439745426178, 0.29024189710617065, -0.6024072170257568, 0.4935096502304077, 0.6478857398033142, -0.9078595638275146, 0.5197004079818726, -0.20973704755306244, 0.768070638179779, 0.34335067868232727, 1.1274863481521606, 1.29520583152771, 0.164364293217659, -0.5278185606002808, 0.32359108328819275, 0.46942541003227234, 0.9137129187583923, -0.11089704930782318, 0.3796735405921936, 0.3836648464202881, -0.6968383193016052, -0.4593895971775055, -0.7330146431922913, -0.7944089770317078, -0.07070985436439514, -0.21540220081806183, -0.5440230369567871, -0.6159182786941528, -0.5038151144981384, 0.6758933663368225, -0.13120268285274506, 0.3770499527454376, -0.343779593706131, 0.5138272047042847, 0.11404461413621902, 0.023939374834299088, 0.18872718513011932, 0.5080522894859314, 0.2232203632593155, 0.15551066398620605, -0.06587986648082733, -0.4035668969154358, 0.21597789227962494, 0.5181323885917664, -0.8627046942710876, -0.10697702318429947, 0.1403215229511261, -0.09930510073900223, 0.04519923776388168, 0.9264514446258545, 0.7357944846153259, 0.31740111112594604, -0.5683771967887878, 0.4410727620124817, 0.654740571975708, 0.4657799303531647, -0.2792467474937439, -0.1679231971502304, 0.41273343563079834, -0.537104606628418, -0.1257624626159668, 0.37490755319595337, -0.12084250152111053, -0.42689722776412964, -0.9359656572341919, -0.48797059059143066, 0.4539724290370941, -0.720920979976654, -0.9537197351455688, 0.608476996421814, -0.020239943638443947, -0.40226805210113525, 0.2874542772769928, -1.0692191123962402, -0.3219623267650604, 0.8232094049453735, -1.5081473588943481, -0.5585833191871643, 0.29930010437965393, -0.17891129851341248, -0.44848424196243286, -0.6749644875526428, 1.3709114789962769, 0.40630874037742615, -0.475001722574234, 0.5000678896903992, 0.026047799736261368, -0.18839477002620697, 0.27492550015449524, -0.9001325964927673, 0.5783907771110535, 0.07454311102628708, -0.5462676882743835, 0.3391040563583374, -0.10979395359754562, 0.21140886843204498, -0.6757097840309143, -0.3218405842781067, 0.6821414232254028, -0.7230458855628967, -0.24439825117588043, -0.7508867383003235, -0.9036303162574768, 0.4142150580883026, 0.4579046070575714, -0.4500117897987366, 0.5554984211921692, 0.08615073561668396, -0.8659346103668213, -0.1300780177116394, -0.9349330067634583, -0.22726836800575256, 0.11567119508981705, -0.8274702429771423, -0.46684131026268005, 0.20952372252941132, 0.17567560076713562, -1.2973535060882568, -0.6880457401275635, -0.323749303817749, -0.15708060562610626, 0.21788117289543152, 1.292657732963562, -0.5738973021507263, 0.3685102164745331, 0.8350769877433777, -0.012461811304092407, -0.9944804906845093, -0.08538850396871567, -0.4436121881008148, 0.06084714084863663, -0.30547428131103516, 0.762167751789093, -0.5872876644134521, 0.04106114059686661, 0.6260737776756287, 0.12581774592399597, -0.6337703466415405, -0.39271557331085205, -0.19743336737155914, 0.42311781644821167, -0.41055402159690857, 0.5608610510826111, -0.1984710991382599, -0.2594980299472809, 0.04540228843688965, 0.7827984690666199, 0.45993632078170776, -0.41444966197013855, -0.8484442830085754, 0.245773583650589, 0.03262384608387947, -0.5677312612533569, -0.6470760703086853, -0.14268289506435394, -1.5514981746673584, 0.18030020594596863, -1.3810768127441406, -0.09874279052019119, -0.4285602271556854, -0.11469970643520355, -0.1855544000864029, -0.6127938032150269, 0.15600788593292236, 0.05696950480341911, -0.3254912495613098, -0.38844531774520874, -0.46578288078308105, -0.49395477771759033, 0.7112739682197571, 1.1418811082839966, -0.7594495415687561, 0.0026934556663036346, 0.11968924850225449, 0.3040339946746826, 0.24910584092140198, 0.8825165629386902, -0.048668619245290756, -1.014061689376831, -1.411849856376648, 0.5706300139427185, -0.522964358329773, -0.27337369322776794, -0.7685785293579102, 0.7089415192604065, 0.25721636414527893, 0.04591945558786392, 0.35552552342414856, 0.45313000679016113, -0.8442421555519104, -0.9283100366592407, 0.3502297103404999, -0.6327677369117737, 0.5065716505050659, 0.6597630381584167, -0.4558869004249573, -0.35783880949020386, 0.47132396697998047, 0.010417439974844456, -0.8639600872993469, -0.9496351480484009, 0.5104964375495911, -0.4722050130367279, 0.2140055000782013, -0.1716318577528, 0.17878690361976624, -1.374081015586853, -0.22004178166389465, -0.14075317978858948, 0.5306052565574646, -0.7002077698707581, 0.863115668296814, 0.6643109917640686, -1.109081506729126, -0.08445826172828674, 0.9308469891548157, 0.035998012870550156, -0.014324463903903961, 0.8430168032646179, 0.5613815188407898, -0.526881217956543, 0.4699946641921997, -0.03932766988873482, 0.5488737225532532, -0.5803495645523071, -0.17776601016521454, 1.3965826034545898, -0.7248221635818481, 0.2573820948600769, 1.3534053564071655, -0.237161785364151, -1.6049764156341553, 0.17352241277694702, -0.9891920685768127, -0.6496706008911133, -0.2917928397655487, 0.6017866134643555, -0.05884137004613876, -0.16957426071166992, -0.024700962007045746, -0.24090899527072906, 0.2136181741952896, -0.19239909946918488, -0.2873193919658661, 0.1967732459306717, -0.19045352935791016, -0.20974311232566833, 0.4559153914451599, 0.8299902081489563, -0.8151246309280396, -0.7013522386550903, -0.7994003891944885, -0.4842059016227722, 0.3202541470527649, 0.06272267550230026, -0.6823796033859253, -0.6916691660881042, 0.6616979241371155, 0.6081401705741882, -0.22264984250068665, 0.2603427469730377, -0.3349611163139343, 0.041147880256175995, 0.8775560259819031, -0.09587901830673218, -0.8008348941802979, -0.14923332631587982, 1.4391199350357056, 1.0897870063781738, -0.955846905708313, 0.13533394038677216, -0.17233134806156158, -0.670306384563446, 1.0073827505111694, 0.5354911684989929, 0.003729132702574134, 0.8468436598777771, -0.4656888544559479, -0.30082300305366516, 0.426614910364151, -0.8414874076843262, -0.21815885603427887, 1.300167441368103, 0.8406283259391785, 0.8393181562423706, 0.7580526471138, 0.11772129684686661, 1.0400081872940063, 0.022036582231521606, -0.17760728299617767, 0.4925236701965332, 0.17933964729309082, -0.45444589853286743, -0.28313636779785156, 0.10029905289411545, 0.7686067819595337, -0.48119670152664185, -0.3389662504196167, 0.609456479549408, 0.5793498158454895, 0.4630110263824463, 0.16060011088848114, 0.9715948700904846, 0.14274701476097107, 0.6892564296722412, 0.6032693386077881, 0.664710521697998, -0.6463304162025452, -0.4440244436264038, -0.5757237672805786, -0.9122977256774902, -0.021919934079051018, -0.2564062476158142, -0.22538533806800842, -0.46624690294265747, -0.16698767244815826, 0.7971726655960083, -0.34212616086006165, 0.3942556083202362, 1.1118218898773193, 0.3856842517852783, 0.9678910374641418, -0.4172476828098297, -0.3398019075393677, -0.7691882848739624, -1.2994928359985352, -0.044527195394039154, -0.1032356470823288, -0.4392622709274292, -0.38158777356147766, 0.04887675121426582, -0.4809092581272125]}, "authors": [{"authorId": "6062056", "name": "Jinghan Zhang"}, {"authorId": "2108956946", "name": "Shiqi Chen"}, {"authorId": "2217264323", "name": "Junteng Liu"}, {"authorId": "2109932032", "name": "Junxian He"}], "references": [{"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "1f346f74e8eabececa4896d734ab9b261f30830d", "title": "Modular Deep Learning"}, {"paperId": "629bc57782bb4326a3eb5f89314e350729c5f417", "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "3ff08b5ca57e786d8af7b204ef94c9972bd9a61e", "title": "Dataless Knowledge Fusion by Merging Weights of Language Models"}, {"paperId": "71ba5f845bd22d42003675b7cea970ca9e590bcc", "title": "Editing Models with Task Arithmetic"}, {"paperId": "02c873a69b8702e3848f942fefb7437ec012422c", "title": "Exploring Mode Connectivity for Pre-trained Language Models"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "a9e20180153f6c139a4b6f2791b535fa6ffc3959", "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca", "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"}, {"paperId": "06b20a1c6883464fcb2855adc146874fe7937c41", "title": "Merging Models with Fisher-Weighted Averaging"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "5baa3e00d66bc42db7e3908f0b70875cff9d0193", "title": "What is being transferred in transfer learning?"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "b611a8095630557229dc5fb6b07c272f1cd614da", "title": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "665f89a20b05472d82df0a12f2dd63e8fcc4f3ea", "title": "Hidden factors and hidden topics: understanding rating dimensions with review text"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "7e2530784eeae241e997627795819cf42ba8562f", "title": "AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models"}, {"paperId": null, "title": "Optimizing language models for dialogue. OpenAI Blog"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Unitary team,\u201cdetoxify,"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Cola: The corpus of linguistic acceptability (with added annotations)"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Advances in Neural Information Processing Systems , volume 30"}, {"paperId": null, "title": "First quora dataset release: Question pairs"}, {"paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "title": "The Third PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": null, "title": "Less is more for alignment"}, {"paperId": null, "title": "Baize: An open-source chat model with parameter-ef\ufb01cient tuning on self-chat data"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}]}