{"paperId": "1dd5beb70fe2a4ace695d3fc7f1fb0c53f757d87", "abstract": "Transformer-based pre-trained models have gained much advance in recent years, becoming one of the most important backbones in natural language processing. Recent work shows that the attention mechanism inside Transformer may not be necessary, both convolutional neural networks and multi-layer perceptron based models have also been investigated as Transformer alternatives. In this paper, we consider a graph recurrent network for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation decoupled from other tokens. The original model performs well in domain-specific text classification under supervised training, however, its potential in learning transfer knowledge by self-supervised way has not been fully exploited. We fill this gap by optimizing the architecture and verifying its effectiveness in more general language understanding tasks, for both English and Chinese languages. As for model efficiency, instead of the quadratic complexity in Transformer-based models, our model has linear complexity and performs more efficiently during inference. Moreover, we find that our model can generate more diverse outputs with less contextualized feature redundancy than existing attention-based models.", "venue": "arXiv.org", "year": 2022, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2209.03834", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A graph recurrent network for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation decoupled from other tokens, which can generate more diverse outputs with less contextualized feature redundancy than existing attention-based models."}, "embedding": {"model": "specter_v2", "vector": [0.17898619174957275, 0.6939390301704407, -0.05162893980741501, -0.18154571950435638, 0.009743629954755306, -0.2007247507572174, 0.5549880862236023, -0.13934381306171417, -0.490528404712677, -0.100371815264225, 0.7513135671615601, -0.25617069005966187, 0.677402138710022, -0.2386797070503235, -0.0011219949228689075, 0.21010588109493256, -0.7161232829093933, -0.03955554962158203, -0.03350866213440895, -0.3245154023170471, -0.11681089550256729, -0.7660588026046753, -0.6887543201446533, -0.05253066495060921, 0.26723021268844604, 0.06755752116441727, 0.5978785753250122, 0.8712595701217651, -0.4883647859096527, 0.9400205612182617, 0.5404379963874817, -0.6689757108688354, -0.08713578432798386, -0.38759565353393555, -0.8431830406188965, 0.05112137272953987, 0.08208513259887695, -0.14823122322559357, -0.6485846638679504, 0.8746515512466431, -0.32225608825683594, 0.41308608651161194, 0.2518499195575714, -0.43028613924980164, -0.39863061904907227, 1.3280998468399048, 0.6988136172294617, 0.2895882725715637, -0.16372831165790558, -0.7253384590148926, 1.7185168266296387, -1.156091332435608, 0.35981082916259766, 1.3271527290344238, 0.25810638070106506, 0.6788740754127502, 0.18062351644039154, -0.6960687041282654, 1.003453016281128, 0.15181735157966614, -0.6252775192260742, -0.023769492283463478, 0.30391114950180054, -0.028154166415333748, 2.1506834030151367, -0.46815669536590576, 0.13841158151626587, 0.2418753057718277, -0.21875202655792236, 1.3330177068710327, -0.2755241394042969, -0.6262301206588745, -0.9419103860855103, -0.23141932487487793, 0.6843922138214111, 1.227979063987732, -0.2798340618610382, 0.1693277806043625, -0.7024376392364502, 0.40831807255744934, 0.6169358491897583, 0.25440725684165955, 0.0321660190820694, 0.17534707486629486, -0.24968236684799194, 0.7688553333282471, 0.43846288323402405, 0.9752231240272522, -0.17508749663829803, 0.6808297634124756, 0.6745517253875732, 0.6241229772567749, 0.09560064226388931, 0.2231571078300476, 0.24111653864383698, 0.5264414548873901, -0.2732468545436859, -0.09974486380815506, -0.2951861321926117, 0.7292807102203369, 0.35051682591438293, 0.5995675325393677, -0.5872058868408203, 0.1630208045244217, 1.43385910987854, -0.14661942422389984, 0.41855260729789734, -0.465350866317749, 0.18767325580120087, -0.37598246335983276, -0.5069373846054077, -0.6649234890937805, -0.08181048929691315, -0.3217303454875946, -1.0198904275894165, -1.596603274345398, -0.24631845951080322, 0.18474848568439484, -0.8757214546203613, 0.925790548324585, -0.6935703158378601, 0.09718170762062073, 0.40218639373779297, 0.12521877884864807, 0.6916025280952454, 0.6870679259300232, 0.3400145471096039, 0.13905608654022217, 1.0282701253890991, -1.0018892288208008, -0.7475490570068359, -0.9272142052650452, 1.1102970838546753, -0.023368917405605316, 0.0652705579996109, -0.4824729561805725, -0.9140221476554871, -1.0722414255142212, -0.5939732193946838, 0.10059633105993271, -0.6569958925247192, 0.17420826852321625, 1.060588002204895, 0.5582402348518372, -1.23701012134552, 1.0533771514892578, 0.020095381885766983, -0.48237907886505127, 0.0706535130739212, 0.05137702077627182, -0.04479886591434479, -0.6107431054115295, -1.6459273099899292, 0.616712749004364, 0.3103495240211487, -0.19218051433563232, -0.2950299084186554, -0.4207763373851776, -1.6134997606277466, 0.01188609004020691, 0.11524540930986404, -0.5701824426651001, 1.1236162185668945, 0.008081728592514992, -1.6644474267959595, 0.8996641039848328, -0.5695343613624573, -0.159028559923172, -0.06356868147850037, -0.41325268149375916, -0.4079159200191498, -0.7313307523727417, -0.06420211493968964, -0.05631447583436966, 0.15513524413108826, 0.2525622248649597, -0.08455352485179901, 0.003140474669635296, -0.47121351957321167, 0.025694966316223145, 0.05220453813672066, 1.1841663122177124, -0.4422333538532257, -0.11997516453266144, 0.10494845360517502, 0.8497136831283569, -0.056215912103652954, -1.0986402034759521, -0.611412525177002, -1.666285753250122, 0.7782332897186279, -0.04051530733704567, 0.9889850616455078, -0.7601810097694397, -0.73456209897995, -0.28430479764938354, 0.01563299261033535, 0.09367996454238892, -0.9781265258789062, 0.8580467700958252, -0.6488755345344543, 0.4940507411956787, 0.17013652622699738, -1.0053337812423706, -0.12659181654453278, -0.37667521834373474, -0.6313247680664062, -0.1381087452173233, 0.38604724407196045, 1.1948851346969604, -1.2186026573181152, 0.29636844992637634, 0.26630574464797974, 0.10065095126628876, -0.620373010635376, 1.6044445037841797, -0.13171204924583435, -0.09194695949554443, -0.502778172492981, -0.5178050994873047, 0.1719527542591095, -0.3949443995952606, 0.4726327359676361, -0.23815831542015076, -0.20789900422096252, 0.7286601066589355, -0.09233562648296356, 1.2597473859786987, -0.1994098722934723, 0.551803708076477, -0.37133529782295227, -1.0719112157821655, 0.6818333864212036, 0.3499952554702759, -0.30372291803359985, -0.3956531882286072, 0.07135146111249924, -0.017843332141637802, -0.626753568649292, 0.5346955060958862, 0.6344426274299622, 0.5527632236480713, -0.5188726186752319, 0.599008321762085, 0.5802046656608582, -0.11089165508747101, 0.45220279693603516, 0.5841416120529175, 0.9003719687461853, 0.22493024170398712, 0.3299538195133209, 0.08770253509283066, 0.6716492772102356, -0.7529581189155579, -0.02736719697713852, 0.6931895017623901, 0.9173712730407715, 0.8723790049552917, 0.3437427878379822, -0.5124568939208984, -0.11826132237911224, -0.027712929993867874, 1.2914260625839233, 1.6502612829208374, -0.1906009167432785, -0.46537870168685913, -0.6358889937400818, -0.3352280855178833, -0.6150894165039062, 0.20740903913974762, -0.04932988062500954, -0.30166956782341003, -0.6624454855918884, -0.9851107001304626, 0.8490463495254517, 0.2162507325410843, 1.2885082960128784, -0.6091658473014832, 0.12177275866270065, -0.17613418400287628, 0.2084321528673172, -0.8523619771003723, -0.7378260493278503, 0.22079020738601685, -0.381658136844635, -0.36919113993644714, -0.19639045000076294, -0.1654295176267624, 0.11697949469089508, -0.33075186610221863, 0.8376346230506897, -0.10135001689195633, -0.17451359331607819, 0.44037264585494995, 0.5782176852226257, -0.4935671091079712, -0.6912562847137451, 0.2676814794540405, -0.12447131425142288, -0.10849722474813461, 0.37092024087905884, 0.733722448348999, 0.09267500042915344, -0.041549697518348694, -0.1277569979429245, 0.24890512228012085, -0.04549787938594818, 0.1856611669063568, 0.08810640126466751, 0.061250701546669006, -0.003776171477511525, -1.4122672080993652, 0.9797166585922241, 0.46071717143058777, -0.33033517003059387, 0.4806130528450012, -0.5839821696281433, -0.10547013580799103, 0.607312798500061, -0.2611299157142639, -0.4469093084335327, -0.8241636753082275, 0.468997985124588, -0.07737182825803757, -0.04423828423023224, 0.717911422252655, -0.022164003923535347, 0.573149561882019, -0.41731956601142883, 0.5901894569396973, 0.6667824387550354, -0.09131819754838943, 0.7298951148986816, -0.9960288405418396, 0.40585923194885254, 0.5269677042961121, 0.49903997778892517, -0.4743458032608032, -0.24120716750621796, -0.9691389203071594, -0.7078775763511658, -0.6920557618141174, -0.1603933721780777, -0.42101961374282837, 0.31909820437431335, -0.7920305728912354, -0.8129013180732727, 0.4732806980609894, -1.3031009435653687, -0.2853609323501587, 0.3611515462398529, -0.11373955756425858, 0.04768861085176468, -0.7911672592163086, -1.1020407676696777, -0.6590641736984253, -0.32070454955101013, -0.6356997489929199, 0.1993933469057083, 0.07791376858949661, -0.06655887514352798, -0.9726361036300659, -0.04317516088485718, -0.36732351779937744, 1.007597804069519, -0.5634528994560242, 0.9777849316596985, -0.23374097049236298, -0.5098164677619934, -0.019587378948926926, -0.06903421133756638, 0.673428475856781, 0.13593265414237976, -0.0471004918217659, -0.6033557653427124, -0.014578338712453842, -0.4789876639842987, -0.07113274186849594, 0.4175763726234436, 0.1891470104455948, 0.5104956030845642, 0.2853817939758301, -0.5125047564506531, 0.108230359852314, 1.5220565795898438, -0.6864803433418274, 0.3251442611217499, 0.0869903415441513, 1.351255178451538, 0.38787317276000977, -0.5100854635238647, -0.05885940045118332, 0.7091760039329529, 0.2206822782754898, 0.00128644285723567, -0.6467781662940979, -0.33957648277282715, -0.6756900548934937, 0.9616742134094238, 1.7701749801635742, -0.07398892194032669, -0.2278861105442047, -0.9368737936019897, 0.8023239374160767, -1.3434569835662842, -0.9501144886016846, 0.39276036620140076, 0.34372612833976746, 0.11969026178121567, -0.610882043838501, -0.21659432351589203, -0.18744899332523346, 1.0550655126571655, 0.42564690113067627, -0.11965277791023254, -0.7818711400032043, 0.043809615075588226, 0.4675838351249695, 0.43522030115127563, 0.8980787992477417, -0.09427027404308319, 1.189815878868103, 14.306796073913574, 0.5105093121528625, -0.03274241462349892, 0.16903507709503174, 0.41089433431625366, 0.9070717096328735, -0.21877549588680267, 0.13372723758220673, -1.335349678993225, -0.6086663007736206, 0.9193809032440186, -0.5742858052253723, 0.6052896976470947, 0.0034811238292604685, 0.09284845739603043, 0.3480673134326935, -0.5487657189369202, 0.6697247624397278, 0.6210396885871887, -1.186165452003479, 0.8255004286766052, 0.31107282638549805, -0.22703729569911957, 0.6511684060096741, 0.5356128215789795, 0.9867790937423706, 0.8078835606575012, -0.6285660862922668, 0.020140081644058228, 0.22044478356838226, 0.6729661226272583, -0.07810056209564209, 0.5270023345947266, 0.36114054918289185, -1.2430212497711182, 0.01128742378205061, -0.770173192024231, -1.303835391998291, 0.22167916595935822, 0.18979167938232422, -0.5347371697425842, 0.19231580197811127, -0.5755424499511719, 1.447097897529602, 0.02534422278404236, 0.14559318125247955, -0.7246033549308777, 0.7081258893013, 0.18290859460830688, -0.36743587255477905, 0.3090788424015045, 0.2422461062669754, 0.40600666403770447, 0.2516620457172394, 0.239755779504776, 0.049869682639837265, 0.027193671092391014, 0.43819519877433777, -0.5926379561424255, -0.08681830018758774, -0.8479178547859192, -0.42446091771125793, -0.2706878185272217, 0.4722290635108948, 0.6192711591720581, 0.21644224226474762, -0.49657776951789856, 0.25188493728637695, 0.8684077262878418, 0.23010991513729095, -0.20957539975643158, -0.3517194390296936, 0.21869447827339172, -0.21805916726589203, 0.06471794098615646, 0.25328588485717773, 0.12543931603431702, -0.7040812373161316, -1.002945065498352, -0.01770324632525444, 0.5683780312538147, -0.7652646899223328, -0.8354679942131042, 1.1198893785476685, -0.45514482259750366, -0.14943507313728333, -0.18240244686603546, -1.0156283378601074, -0.7112718224525452, 0.7098557949066162, -1.7652482986450195, -0.738559365272522, 0.1302623450756073, 0.16718274354934692, -0.5256346464157104, -0.17400169372558594, 1.2212250232696533, -0.16799071431159973, -0.80215984582901, -0.13715152442455292, -0.19519910216331482, 0.569128692150116, -0.5493534803390503, -0.8661890029907227, 0.6064282059669495, 0.4735383987426758, -0.08907680213451385, 0.4113367199897766, -0.17628242075443268, 0.13412395119667053, -0.9607707858085632, -0.48020270466804504, 1.15126371383667, -0.6242461800575256, -0.25555548071861267, -0.9869241118431091, -1.0636751651763916, 0.45092687010765076, 0.8351970314979553, -0.5958689451217651, 0.19882626831531525, 0.5542252659797668, -0.5250431895256042, -0.21702304482460022, -0.4108973443508148, 0.3108103573322296, 0.7704962491989136, -1.0774705410003662, -0.4115602374076843, -0.20121635496616364, 0.3491179049015045, -0.4153810739517212, -0.22121666371822357, -0.274281769990921, -0.24727973341941833, 0.09888294339179993, 0.7833207249641418, -0.538145124912262, 0.36848780512809753, 0.6527449488639832, 0.36056071519851685, -0.9262480735778809, -0.34736010432243347, -1.2126867771148682, 0.7563668489456177, 0.5431784391403198, 0.6903660893440247, -0.7132520079612732, 0.21087400615215302, 0.7133740782737732, 0.2896294593811035, -0.0293029323220253, -0.5526845455169678, -0.38940444588661194, -0.13574784994125366, -0.42019593715667725, 0.10395687818527222, 0.10292509943246841, 0.12145958840847015, 0.6505797505378723, 0.4384554922580719, 0.48400020599365234, -0.08709347993135452, -0.7006396055221558, 0.39509400725364685, -0.29822400212287903, 0.5579850077629089, -0.6916024684906006, -0.4647832214832306, -1.3798973560333252, 0.6343424320220947, -1.6033202409744263, -0.15372489392757416, -1.575575828552246, -0.3033246099948883, 0.5792863965034485, -0.33592018485069275, 0.25814834237098694, 0.06720475852489471, -0.7542317509651184, -0.3629826605319977, -0.8759825229644775, -0.74066162109375, 0.8610987663269043, 0.8728660941123962, -0.729134202003479, 0.2158534973859787, -0.12395410239696503, -0.22477084398269653, 0.04264671355485916, 0.5186331272125244, -0.42652878165245056, -0.8061153888702393, -1.4414900541305542, 0.10948223620653152, -0.06022864207625389, -0.04242825135588646, -0.25458037853240967, 0.8896633386611938, 0.5013289451599121, -0.40650755167007446, -0.21342991292476654, -0.14560292661190033, -0.37947630882263184, -0.41913920640945435, 0.3921949565410614, -0.630174458026886, 0.5102196931838989, 0.011103990487754345, -0.43179383873939514, -0.6772883534431458, 0.8901405334472656, -0.15310145914554596, -1.4376463890075684, -0.8232054710388184, 0.5817435383796692, -1.0233550071716309, 0.10335460305213928, -0.4653511941432953, -0.27173277735710144, -1.0430312156677246, -0.6215075254440308, 0.14608483016490936, 0.493396520614624, -0.5944510102272034, 1.0267735719680786, 0.7828401327133179, -1.0658111572265625, -0.3465142548084259, 0.3102327883243561, -0.164491668343544, -0.23996299505233765, 0.4119426906108856, -0.11690738797187805, 0.02763233706355095, 0.4894658923149109, 0.5007141828536987, 0.2939358055591583, -1.0611600875854492, -0.19520775973796844, 0.954012393951416, -1.0243521928787231, -0.47946110367774963, 0.6552311778068542, -0.1611524224281311, -1.1605172157287598, 4.34547291661147e-06, -0.9230784773826599, -0.720169723033905, -0.4735644459724426, 0.37630245089530945, -0.00874270685017109, -0.31023070216178894, -0.0648859515786171, -0.3078899085521698, 0.4597330689430237, -0.19998131692409515, -0.28369706869125366, 0.7184785008430481, -0.33578553795814514, -0.6248753070831299, 0.6620624661445618, 0.82440584897995, -0.9258816242218018, -0.1840204894542694, -0.867927074432373, -0.055359914898872375, 0.151641845703125, 0.3431709110736847, 0.13353748619556427, -0.689410924911499, 0.9254948496818542, 0.04365150257945061, 0.5907981395721436, 0.09655949473381042, -0.24691884219646454, 0.049628399312496185, 0.7016122937202454, -0.05348753556609154, -0.59437096118927, -0.39764267206192017, 2.0040156841278076, 1.2574340105056763, -0.6250730156898499, 0.14616353809833527, -0.6612676382064819, -0.6766196489334106, 1.0710512399673462, 0.11709025502204895, -0.22358477115631104, 1.0372480154037476, -0.4562740921974182, 0.33770477771759033, -0.052468013018369675, -1.1290605068206787, -0.8503029942512512, 0.5848217606544495, 1.122514247894287, 1.1519505977630615, 0.21953792870044708, 0.3041301369667053, 0.687860369682312, 0.11141186207532883, -0.04189329221844673, 0.7680787444114685, 0.057842861860990524, -0.02638254687190056, -0.17438946664333344, 0.5211566686630249, 0.13412180542945862, -0.6294336915016174, -0.6752929091453552, 0.006844410207122564, 0.834347128868103, -0.5288974642753601, 0.7231988906860352, 0.7013218998908997, 0.026462825015187263, 0.8862310647964478, 0.29821497201919556, 0.45908811688423157, -0.8289138674736023, -0.5628079175949097, -0.43582943081855774, -0.2659190595149994, -0.23585057258605957, -0.6183649897575378, -0.5821911096572876, -0.4743623435497284, -0.05608512461185455, 0.28062233328819275, 0.39349275827407837, 0.08819580078125, 0.9939847588539124, 0.26127493381500244, 0.5293052792549133, -0.0025145811960101128, 0.2040678709745407, -0.1616079956293106, -1.0271137952804565, -0.1421232372522354, -0.22415274381637573, -0.07220703363418579, -0.08133714646100998, -0.21324576437473297, -0.34961435198783875]}, "authors": [{"authorId": "2108102904", "name": "Yile Wang"}, {"authorId": "2145500840", "name": "Linyi Yang"}, {"authorId": "2272668", "name": "Zhiyang Teng"}, {"authorId": "143849609", "name": "M. Zhou"}, {"authorId": "39939186", "name": "Yue Zhang"}], "references": [{"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "36e69a05b315c7f2c51379a261b70154482a4c74", "title": "A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "2365410a710b421b2295cdca0074946cb50bb1d4", "title": "Are Pretrained Convolutions Better than Pretrained Transformers?"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9", "title": "How to Train BERT with an Academic Budget"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "23ee26733a4aac635cc36683d89852f979995b34", "title": "Revisiting Representation Degeneration Problem in Language Modeling"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "d97cd476bef990352ae921e4c7c5ae1222e9b8da", "title": "A Mixture of h - 1 Heads is Better than h Heads"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "2c2cc3decc0d5091965975eb4bb6f5dc802bcbf7", "title": "IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization"}, {"paperId": "8e85df955707cb7959299d83b6b8c8a28b2b53dd", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "d16ab5c19ed33a263b6412ac41a4ea1f068d254a", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing"}, {"paperId": "0dde065405210ebc399c58ab6b7e843a18caad51", "title": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "6ebfbc954b9975d2f2651f380b9bdf46ae963178", "title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "55e960535f643637161b2e99a8c21a92c0d13757", "title": "Representation Degeneration Problem in Training Natural Language Generation Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "2ff41a463a374b138bb5a012e5a32bc4beefec20", "title": "Pre-Training with Whole Word Masking for Chinese BERT"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "6f0637f44ac481905d732be7a2e15d8f699aeeb3", "title": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "305f8c705c034b6179fc4db19eb6163265432baa", "title": "Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model"}, {"paperId": "549c1a581b61f9ea47afc6f6871845392eaebbc4", "title": "LCQMC:A Large-scale Chinese Question Matching Corpus"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "3e79a574d776c46bbe6d34f41b1e83b5d0f698f2", "title": "Sentence-State LSTM for Text Representation"}, {"paperId": "b4812702a7c1c47e2af34d8752b2103505089fc2", "title": "A Graph-to-Sequence Model for AMR-to-Text Generation"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "2784000e1a3554374662f4d18cb5ad52f59c8de6", "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation"}, {"paperId": "c3a3c163f25b9181f1fb7e71a32482a7393d2088", "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "c41eb895616e453dcba1a70c9b942c5063cc656c", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "346d900f2f73b706ec539caecb4800d131fc9f6a", "title": "An empirical study of sentiment analysis for chinese documents"}, {"paperId": "885476e9d1b2ea67405f7f9b46c2c8536657a204", "title": "Scalable Term Selection for Text Categorization"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "2a8bb26654c015f663670d1e0745f870071d5507", "title": "Isotropy in the Contextual Embedding Space: Clusters and Manifolds"}, {"paperId": null, "title": "ELECTRA: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}]}