{"paperId": "388df4f4d6577c92a43a9a578ebed844a1b4cc17", "abstract": "Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline. Our code is open sourced at https://github.com/thu-ml/Jetfire-INT8Training.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes Jetfire, an efficient and accurate INT8 training method specific to transformers, which achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers."}, "embedding": {"model": "specter_v2", "vector": [0.4396747052669525, 0.45641207695007324, -0.608405351638794, -0.17545317113399506, -0.5036974549293518, 0.042164500802755356, 0.2670445740222931, 0.014560569077730179, -0.3616902530193329, -0.4446883797645569, 0.38099876046180725, -0.1367315798997879, 0.5844089984893799, 0.057321347296237946, -0.17170660197734833, -0.14179937541484833, -0.9184767007827759, -0.12718576192855835, 0.011200977489352226, -0.22067803144454956, 0.12967100739479065, -0.5861120820045471, -0.7761735916137695, 0.3692452907562256, 0.4604730010032654, 1.2625068426132202, -0.31094756722450256, 0.8579159379005432, -0.6489588618278503, 0.20435971021652222, 0.25278252363204956, -0.5591328144073486, 0.43992558121681213, 0.23753301799297333, -0.4225098788738251, -0.00886610709130764, 0.6619746088981628, -0.5214188694953918, -0.587949812412262, 1.0522650480270386, -0.06912999600172043, 0.21061338484287262, 0.5186432600021362, -0.5358619689941406, -0.16266539692878723, 0.5322535634040833, 0.37988388538360596, 1.0200327634811401, -0.9895244836807251, -0.6565263867378235, 1.1781270503997803, -1.6220829486846924, -0.16036628186702728, 1.1658401489257812, 0.7244462370872498, 0.21394743025302887, -0.5704742670059204, -0.6654097437858582, 0.14352241158485413, 0.3316023051738739, -0.7971460223197937, -0.3968852162361145, -0.40426400303840637, -0.08593056350946426, 1.8383443355560303, -0.45790210366249084, 0.3002101182937622, 0.2972300350666046, 0.40486225485801697, 1.3041951656341553, 0.1766255795955658, -0.5200046896934509, -0.06358294934034348, -0.06872744113206863, 0.451477587223053, 0.7798684239387512, -0.6371377110481262, 0.508216381072998, -1.0644409656524658, 0.20389945805072784, 0.34462282061576843, 0.08030761778354645, 0.48545199632644653, -0.22690916061401367, -0.2032153159379959, 0.6737480759620667, 0.7521758079528809, 0.6410346031188965, -0.104780413210392, 0.8493304252624512, 0.8667194247245789, 0.19404694437980652, 0.1888647973537445, 0.07795123755931854, -0.15873983502388, 0.17791980504989624, -1.1681734323501587, -0.09318912029266357, -0.3994605541229248, 0.6699864864349365, 0.33546963334083557, 0.4048789143562317, -1.0700604915618896, 0.1369098424911499, 0.9687392115592957, 0.395224392414093, 0.1531667560338974, -0.6458356380462646, 0.2010013461112976, -0.4345181882381439, -0.22237588465213776, -0.49723491072654724, 0.14225918054580688, -0.387602299451828, -1.1587731838226318, -1.0082082748413086, -0.8149259686470032, 0.3440842032432556, -1.4535000324249268, 0.21176226437091827, -0.15563778579235077, 0.6099674105644226, -0.005085267126560211, 0.4658879041671753, 0.5651791095733643, 0.5829558968544006, 0.3151320517063141, -0.05750199034810066, 1.2245190143585205, -1.2584232091903687, -0.7225663661956787, -0.6424927711486816, 0.4109242260456085, -0.39699703454971313, -0.0750516876578331, 0.17675213515758514, -0.9941959381103516, -1.3255836963653564, -0.9792829751968384, -0.16424494981765747, -0.5316976308822632, 0.004994622431695461, 1.192479133605957, 0.6720184683799744, -1.1319764852523804, 0.8105794191360474, -0.2767810523509979, 0.2560354173183441, 0.6977691650390625, 0.5507509708404541, 0.41681957244873047, -0.11366327852010727, -1.3158613443374634, 0.21751581132411957, 0.267265260219574, -0.1186174526810646, -0.4711313843727112, -0.9873865842819214, -0.8005136847496033, 0.05687083303928375, 0.0928587093949318, -0.07361208647489548, 1.5624381303787231, -0.19930794835090637, -0.9918044209480286, 0.529746949672699, -0.34606751799583435, -0.15680281817913055, 0.2431788295507431, -0.03968140482902527, -0.2893143594264984, -0.12076328694820404, -0.3060704469680786, 1.0064735412597656, 0.7901899814605713, 0.19793859124183655, -0.38968056440353394, 0.21643558144569397, -0.19743166863918304, 0.09952794760465622, -0.6597029566764832, 0.6698715090751648, -0.4053824841976166, -0.574469268321991, 0.5012322664260864, 0.7603120803833008, -0.21047605574131012, -0.04537591338157654, -0.320762038230896, -0.923814594745636, 0.9182647466659546, 0.26616737246513367, 0.7419149279594421, -1.190322995185852, -0.9795901775360107, 0.09746258705854416, 0.22028161585330963, 0.08900953084230423, -0.6290200352668762, 0.3308211863040924, -0.603955090045929, 0.18735362589359283, 0.2719414532184601, -1.0138978958129883, 0.41743552684783936, -0.15101061761379242, -0.9714528918266296, -0.21170352399349213, 0.06625958532094955, 1.1413856744766235, -0.7789802551269531, -0.0689331442117691, 0.08502193540334702, 0.8076238632202148, -1.1381289958953857, 0.9086881875991821, -0.19732233881950378, 0.15319454669952393, 0.053084004670381546, -0.23468995094299316, 0.4583266079425812, -0.34464573860168457, 0.33545586466789246, -0.9102029204368591, 0.05519431084394455, 0.7099812030792236, -0.5306338667869568, 1.2920182943344116, -0.22872166335582733, 0.4204495847225189, -0.2031090408563614, -0.7139537930488586, 0.21265046298503876, 0.3759002387523651, 0.2090751677751541, -0.7233714461326599, 0.5555461645126343, 0.3709358870983124, -0.6001189351081848, 0.7550361752510071, 1.14737069606781, 0.5077797770500183, -0.18438632786273956, 0.04535907134413719, 0.8208969831466675, -0.35700517892837524, 0.5914585590362549, 0.43869486451148987, 0.5587836503982544, 0.045067813247442245, 0.16047529876232147, -0.09469323605298996, 0.07113220542669296, -0.9730648994445801, -0.05668449401855469, 0.4473290741443634, 0.43778541684150696, 0.5744404792785645, 0.4177814722061157, -0.8925020098686218, -0.5713648796081543, -0.07882019132375717, 0.3280619978904724, 1.4935919046401978, -0.2204609215259552, -0.24241916835308075, -0.6698544025421143, -0.591594398021698, -0.25500163435935974, -0.32220351696014404, -0.4040055274963379, -0.38342154026031494, -0.19498753547668457, -1.0162333250045776, 0.6054017543792725, 0.4361080527305603, 1.504910945892334, -0.5257521867752075, -0.41812050342559814, -0.5099634528160095, 0.2553368806838989, -0.8128643035888672, -0.6376970410346985, 1.1431727409362793, -0.8115522861480713, -0.15739190578460693, 0.09137985855340958, -0.20360004901885986, 0.45122361183166504, -0.6968229413032532, 0.9567916989326477, -0.2876242399215698, -0.2830936312675476, -0.20705176889896393, 0.30843886733055115, -0.2505732476711273, -0.3758767545223236, 0.39400532841682434, -0.07398754358291626, -0.06203601136803627, 0.42911088466644287, 0.15289850533008575, -0.14430978894233704, -0.04908556491136551, -0.26636800169944763, 0.20669765770435333, 0.2959353029727936, 0.19114665687084198, 1.0063201189041138, 0.04071340709924698, -0.2664916515350342, -1.2348676919937134, 0.39620697498321533, 0.44490090012550354, -0.2035522758960724, -0.07150914520025253, -0.7129600048065186, -0.3099491596221924, 0.3268948197364807, -0.2611883580684662, -0.0853801816701889, -0.6082615852355957, 0.0218195803463459, -0.7285050749778748, 0.007959183305501938, -0.12377335876226425, 0.7287114262580872, -0.2139534205198288, 0.5909442901611328, 0.3447028398513794, -0.22876597940921783, 0.11386168003082275, 0.6260002851486206, -0.8184437155723572, 0.4883368909358978, 0.21412549912929535, 0.3779065012931824, -0.25250449776649475, -0.17996948957443237, -0.3767092525959015, -0.5569794178009033, -0.6435102820396423, -0.5143222212791443, -0.2800559997558594, 0.2864581048488617, -0.5280043482780457, -0.7499651908874512, -0.2118041068315506, -0.8795899748802185, -0.547461211681366, -0.11053972691297531, -0.29145511984825134, -0.1456371694803238, -1.0952016115188599, -1.3666300773620605, -0.4472014307975769, -1.233432650566101, -1.1582077741622925, 0.7532038688659668, 0.1100393608212471, -0.021686678752303123, -0.27511993050575256, -0.4583618938922882, -0.5808752775192261, 0.8861156702041626, -0.7523581981658936, 0.3445250988006592, 0.13212744891643524, -0.34393978118896484, 0.1294749677181244, -0.13362133502960205, 0.8203203678131104, -0.20395179092884064, 0.14850780367851257, -1.1350830793380737, -0.0030708876438438892, -0.42305630445480347, -0.6400870084762573, 0.6073781847953796, 0.05042542517185211, 0.955755352973938, 0.05015430599451065, -0.31328830122947693, 0.748335063457489, 1.2542030811309814, -0.6004709601402283, 0.38557544350624084, -0.07200463861227036, 0.7773581743240356, -0.49671271443367004, -0.02051769196987152, 0.3777637183666229, -0.007035963702946901, 0.27711111307144165, 0.07238603383302689, -0.18433116376399994, -0.17401482164859772, -0.1182725727558136, 0.5513307452201843, 1.3732719421386719, 0.7129335999488831, 0.4350728690624237, -0.9933295249938965, 0.7083117961883545, -0.6292130947113037, -0.5906164050102234, 0.6968933939933777, 0.45601555705070496, 0.516430139541626, -0.07936111837625504, -0.20486792922019958, 0.43724942207336426, 0.18649153411388397, 0.43756332993507385, -0.4192957580089569, -0.8984639644622803, 0.4299139976501465, 1.149113416671753, 0.5306171774864197, 0.3323369324207306, -0.6911647319793701, 0.3122676610946655, 14.905038833618164, 0.9586591124534607, -0.4022248089313507, 0.515939474105835, 1.0252236127853394, 0.6023040413856506, -0.20416796207427979, -0.14784874022006989, -1.1845381259918213, -0.17510585486888885, 1.0714794397354126, 0.5298672318458557, 0.6457735300064087, 0.25693216919898987, -0.3063075840473175, 0.12887616455554962, -0.5339953899383545, 0.6787282824516296, 0.2144571989774704, -1.331821322441101, 0.16132870316505432, 0.1536424309015274, 0.4309529960155487, 0.3965701460838318, 0.9493528008460999, 1.0789486169815063, 0.09467273950576782, -0.33055466413497925, 0.3829263150691986, -0.08893513679504395, 1.3598014116287231, -0.16192394495010376, 0.4800998866558075, 0.09718170762062073, -1.167066216468811, -0.10390232503414154, -0.6150954961776733, -1.0190123319625854, -0.11518814414739609, 0.4693513512611389, -1.159193992614746, -0.36076703667640686, 0.14216706156730652, 0.8365153074264526, -0.09697932004928589, 0.4443255364894867, -0.281840980052948, 0.7436012029647827, -0.1463395208120346, 0.2841629385948181, 0.40081650018692017, 0.35201966762542725, -0.32001015543937683, 0.13925033807754517, -0.07279103994369507, 0.13018900156021118, 0.32153749465942383, 0.04013633728027344, -0.49754810333251953, -0.05425126478075981, 0.24627768993377686, -0.22049397230148315, -0.49859750270843506, 1.3030112981796265, -0.008451621048152447, -0.02977164089679718, -0.20676591992378235, 0.3725754916667938, 0.32666561007499695, 0.02981634996831417, -0.7211493849754333, -0.10726550221443176, 0.11482661962509155, -0.7585697770118713, 0.41709622740745544, 0.5990892648696899, -0.2952026128768921, -0.5505623817443848, -0.9324151277542114, -0.44083231687545776, 0.44848668575286865, -0.6526057720184326, -0.44071367383003235, 0.7677455544471741, -0.4139421284198761, -0.449029803276062, 0.44693776965141296, -0.8221903443336487, 0.03349296376109123, 0.3587881326675415, -1.5563242435455322, -0.4364875853061676, -0.017162270843982697, -0.4436495006084442, -0.5274598002433777, 0.10464166849851608, 1.211434006690979, 0.6559538245201111, -0.043325792998075485, 0.1298559010028839, -0.07808297127485275, 0.11631316691637039, -0.14438031613826752, -0.9542877674102783, 0.9878094792366028, 0.23567311465740204, 0.08624632656574249, 0.26192575693130493, -0.14900413155555725, 0.3495042324066162, -0.708125114440918, -0.45781320333480835, 0.3966001868247986, -0.35980311036109924, -0.19520141184329987, -0.9247603416442871, -0.5652230978012085, 0.35651421546936035, 0.5184657573699951, 0.16405613720417023, 0.4618082642555237, 0.22976186871528625, -0.5642761588096619, -0.546187698841095, -0.5208054184913635, -0.14733904600143433, 0.3835449814796448, -0.8843041062355042, -0.09309891611337662, -0.1196049302816391, -0.10661280900239944, -1.2618495225906372, -0.9047511219978333, -0.04004710167646408, -0.07414919883012772, -0.21664674580097198, 1.048872470855713, 0.13552942872047424, 0.6435468196868896, 0.7181341052055359, -0.2500891089439392, -0.799393892288208, 0.3141684830188751, -0.7389477491378784, 0.05518582463264465, -0.12470300495624542, 0.30947816371917725, -0.09058047086000443, 0.8537755012512207, 0.23911987245082855, 0.16361433267593384, -0.5053248405456543, -0.6312447786331177, 0.08793982863426208, -0.2908032536506653, -0.4778027832508087, 0.08902885764837265, -0.1751730889081955, -0.21378368139266968, 0.2532089650630951, 0.5759044289588928, 0.11537191271781921, -0.26590266823768616, -0.35053902864456177, 0.4427958130836487, 0.23513443768024445, -0.18518385291099548, -0.30577340722084045, -0.7664154171943665, -1.763688325881958, -0.18458734452724457, -1.3880902528762817, 0.08958926796913147, -0.6958374977111816, -0.1304115504026413, -0.390421986579895, -0.6003800630569458, 0.38281694054603577, 0.08411317318677902, 0.6026322841644287, -0.4294643700122833, -0.5449331402778625, -0.7726532816886902, 0.8845399618148804, 1.110233187675476, -0.9384134411811829, 0.3189748227596283, -0.3166272044181824, 0.11561879515647888, -0.09219588339328766, 0.198795884847641, -0.4764957129955292, -1.0314083099365234, -0.9248714447021484, 0.3370214104652405, -0.27891862392425537, -0.1874283105134964, -0.9145393371582031, 0.61165851354599, 0.5776152610778809, 0.18656839430332184, -0.23217180371284485, 0.3995227813720703, -0.826487123966217, -0.6366223692893982, 0.48733845353126526, -0.4462483525276184, -0.008764728903770447, 0.5052198767662048, -0.7282242774963379, -0.3475985527038574, 0.748839795589447, 0.22199063003063202, -0.5526399612426758, -0.9972962737083435, 0.6157227158546448, -0.21061035990715027, -0.01817978173494339, -0.4095604717731476, -0.14834558963775635, -1.0258737802505493, -0.16570329666137695, -0.18910600244998932, 0.3198288381099701, -0.7087697386741638, 0.3993789851665497, 0.4204568564891815, -1.2289384603500366, 0.4766978323459625, 0.7357731461524963, -0.41230690479278564, 0.1124797835946083, 0.5508576035499573, 0.8203818202018738, -0.48042410612106323, 0.3158213794231415, -0.09752073884010315, -0.07105597853660583, -0.7138499617576599, 0.029141932725906372, 1.0201338529586792, -0.700108528137207, 0.060203712433576584, 1.0956902503967285, -0.4626927077770233, -1.0625381469726562, 0.30716270208358765, -1.4663804769515991, -0.5691214799880981, -0.11577270179986954, 0.638005793094635, 0.02581162191927433, 0.5182839035987854, -0.023152044042944908, -0.4879785180091858, 0.1424408257007599, -0.0703149363398552, -0.46320515871047974, 0.6063230037689209, 0.5307696461677551, -0.36140432953834534, 0.3339506685733795, 1.0011162757873535, -0.8383145928382874, -0.7955893278121948, -0.5952638387680054, -0.5576741695404053, -0.2058020383119583, 0.380794495344162, -0.3152942955493927, -1.2430390119552612, 1.1868422031402588, 0.9279045462608337, 0.3554164171218872, 0.578183650970459, -0.8319029808044434, 0.4272369146347046, 0.8180716037750244, 0.10427884757518768, -0.6747603416442871, -0.19654859602451324, 1.3605180978775024, 0.6443644762039185, -0.5317795276641846, 0.48642975091934204, -0.3772468864917755, -0.4431028664112091, 0.9053553342819214, 0.361960232257843, -0.19454388320446014, 1.088195562362671, -0.08612575381994247, 0.16779528558254242, 0.1916503608226776, -0.7193965911865234, -0.39321163296699524, 0.9631597399711609, 0.8727208375930786, 0.7490416169166565, 0.0002553020021878183, 0.5231037139892578, 0.4701721668243408, -0.044529497623443604, -0.07247842848300934, -0.04832572489976883, 0.4335576295852661, -0.08018513023853302, -0.1794036328792572, 0.12412039190530777, 0.9637959003448486, -0.773300290107727, -0.7414191961288452, 0.5082758069038391, 0.5001729130744934, 0.7036587595939636, 0.3487835228443146, 1.0139951705932617, -0.23960880935192108, 0.684517502784729, 0.07100164890289307, 0.7214305996894836, -0.4437338411808014, -0.6600878834724426, 0.020008275285363197, -0.8948190212249756, -0.2065492868423462, -0.36739474534988403, -0.10283166915178299, -0.4958200454711914, -0.4288744628429413, 0.24383170902729034, 0.3017725944519043, 0.4122495651245117, 0.9461716413497925, 0.26801738142967224, 0.8762176036834717, -0.20822030305862427, -0.6059363484382629, -0.48050007224082947, -0.9806721210479736, -0.023237265646457672, -0.625371515750885, -0.12445323914289474, -0.21134699881076813, 0.22077979147434235, -0.15609021484851837]}, "authors": [{"authorId": "2220346402", "name": "Haocheng Xi"}, {"authorId": "2292214203", "name": "Yuxiang Chen"}, {"authorId": "2292367492", "name": "Kang Zhao"}, {"authorId": "2292162993", "name": "Kaijun Zheng"}, {"authorId": "2276707", "name": "Jianfei Chen"}, {"authorId": "2287800407", "name": "Jun Zhu"}], "references": [{"paperId": "334e8b7597b69da621c9114e3f695c6315c9267e", "title": "FP8-LM: Training FP8 Large Language Models"}, {"paperId": "370ece25c851b4eefb8a1b81f3110958e8ca0b20", "title": "Training and inference of large language models using 8-bit floating point"}, {"paperId": "56b828717f32251a5e0f0be9c0113077f23c8429", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}, {"paperId": "7505337d88e4d36e2e37891c542947a2e3c9e009", "title": "Training Transformers with 4-bit Integers"}, {"paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "title": "SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "b3c64a046d6cfc0f55a2aebc5176bbab69a7e021", "title": "Stable and low-precision training for large-scale vision-language models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "7240d82352e1eea633edc925c0903a72920e642b", "title": "Distribution Adaptive INT8 Quantization for Training CNNs"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "c6b5ecf84e304cda4dbd1687cf1902790d65cc6e", "title": "A Statistical Framework for Low-bitwidth Training of Deep Neural Networks"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "962bdacb3941053788d69696826c9b4307652cce", "title": "Towards Unified INT8 Training for Convolutional Neural Network"}, {"paperId": "0a5d987ddb5463062babceca90ba974db0cf96e7", "title": "HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "dc160709bbe528b506a37ead334f60d258413357", "title": "Learned Step Size Quantization"}, {"paperId": "9a1093af92d315def21b90918faf08665157051a", "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers"}, {"paperId": "fae2a5101789afd51c1ececb28c75537c88734ec", "title": "Scalable Methods for 8-bit Training of Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "c904d42a6aeb81c385dbc94d95380734dee43fbc", "title": "NVIDIA Tensor Core Programmability, Performance & Precision"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c", "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"}, {"paperId": "9fae65e8c1af810dbdecfd9a8c6302d1145c3666", "title": "Logarithmic Unbiased Quantization: Practical 4-bit Training in Deep Learning"}, {"paperId": "2cf369e94aae96b599aec9581827466b9b85545b", "title": "Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning"}, {"paperId": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9", "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc", "title": "Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"}, {"paperId": null, "title": "Open clone of openai\u2019s unreleased webtext dataset"}, {"paperId": null, "title": "Nvidia h100 tensor core gpu architecture"}, {"paperId": null, "title": "2. Acceleration result on other hardware Besides RTX 4090, we tested our linear operator and end-to-end speed up result on the RTX 3090 GPUs"}, {"paperId": null, "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization"}]}