{"paperId": "b8c236dc5963dac36b0d8e419beb5876e3a18f96", "abstract": "Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks (DNNs), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide NN kernel theory to improve signal propagation in vanilla DNNs (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control. And so the question remains: is it possible to train deep vanilla transformers? We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on WikiText-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.", "venue": "International Conference on Learning Representations", "year": 2023, "citationCount": 18, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.10322", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "Several approaches are designed that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers, including the interaction with positional encoding and causal masking."}, "embedding": {"model": "specter_v2", "vector": [0.37195053696632385, 0.9852110147476196, -0.5218338370323181, 0.22549328207969666, 0.0987090989947319, -0.23379062116146088, 0.84938645362854, -0.4018077850341797, -0.19287332892417908, -0.4972230792045593, 0.5083050727844238, -0.12882786989212036, 0.5127735733985901, -0.192937970161438, -0.3810403347015381, -0.05610214173793793, -0.9265698790550232, -0.4179481863975525, 0.8451020121574402, -0.37730011343955994, -0.11852099746465683, -0.5675768256187439, -0.513570249080658, 0.11761872470378876, -0.04717586562037468, 0.8402131795883179, -0.34941306710243225, 0.9861932396888733, -0.31307709217071533, 0.9224135279655457, 0.4191274046897888, -0.4551825225353241, 0.49507570266723633, 0.11608945578336716, -0.3979663848876953, -0.5218581557273865, 0.2293853610754013, -0.42140841484069824, -0.8165450096130371, 0.8564828038215637, -0.31973135471343994, -0.06991439312696457, 0.0448058620095253, -0.7681595087051392, -0.39785873889923096, 1.201724886894226, 0.657312273979187, 0.6436406373977661, -0.7844398021697998, -0.23328335583209991, 1.3862923383712769, -1.0149978399276733, 0.04611930996179581, 0.9841874837875366, 0.7378605008125305, 0.3778730034828186, -0.5539016723632812, -0.4783019423484802, 0.8024069666862488, 0.3172137439250946, -0.6536436676979065, -0.4064425528049469, 0.27612781524658203, -0.005779622588306665, 1.3782591819763184, -0.3270822763442993, 0.1871776431798935, 0.8484265208244324, 0.12001890689134598, 1.2872376441955566, 0.09073687344789505, -0.5453505516052246, -0.023527899757027626, 0.3139444589614868, 0.25281018018722534, 0.5463339686393738, -0.519012451171875, 0.6844262480735779, -0.8839429616928101, -0.13126881420612335, 0.8392164707183838, 0.006830722559243441, -0.02257610857486725, -0.5046878457069397, -0.35380569100379944, 0.7869625091552734, 0.5661900639533997, 0.7075532078742981, -0.4683820903301239, 1.4270453453063965, 0.8552007079124451, 0.4396555423736572, -0.3442854881286621, 0.3803975284099579, -0.10540129244327545, 0.19804181158542633, -1.0247880220413208, 0.01741735450923443, 0.06953192502260208, 0.7771733999252319, -0.14310497045516968, 0.7980778813362122, -0.5421533584594727, 0.03070557303726673, 1.3368338346481323, -0.0519266314804554, 0.09739279001951218, -0.9782046675682068, -0.15235759317874908, -0.9483795762062073, -0.015018009580671787, -0.7855547666549683, 0.1767873764038086, -0.707070529460907, -0.909210741519928, -0.7280556559562683, -0.1496843695640564, 0.09463267773389816, -1.0017644166946411, 0.7076499462127686, -0.793407678604126, 0.7481151223182678, -0.07210437953472137, 0.379353404045105, 0.3061269223690033, 0.5623828172683716, 0.10970401763916016, 0.29737406969070435, 0.6014904975891113, -0.8079827427864075, -0.4151093065738678, -1.075738549232483, 0.3013753294944763, -0.2109372466802597, 0.07251186668872833, -0.07140028476715088, -1.0469048023223877, -0.9124776124954224, -1.1723781824111938, 0.47864094376564026, -0.40568360686302185, -0.27169638872146606, 1.3086392879486084, 0.5370110273361206, -1.0675605535507202, 1.0962700843811035, -0.52375328540802, 0.16162583231925964, 0.7680312991142273, 0.3325398862361908, 0.4196939766407013, -0.12051226198673248, -1.505720853805542, 0.29641860723495483, 0.12823963165283203, -0.2534272372722626, -0.6923038959503174, -1.0416412353515625, -0.8881312012672424, -0.1206563338637352, 0.342286616563797, -0.39135321974754333, 1.283615231513977, -0.07246772199869156, -1.2291606664657593, 0.42772775888442993, 0.016057442873716354, -0.28938713669776917, 0.36246299743652344, -0.09222638607025146, -0.412880539894104, -0.252880722284317, -0.1469704508781433, 0.40304335951805115, 0.8544140458106995, -0.5318052172660828, 0.018205730244517326, 0.2235858291387558, 0.14637142419815063, -0.7226023077964783, -0.14036911725997925, 0.5907303690910339, -0.2930603325366974, -0.2047027349472046, 0.07794084399938583, 0.7305772304534912, -0.19277779757976532, -0.22750847041606903, -0.6626229882240295, -0.6349319815635681, 0.5536854267120361, 0.3183867633342743, 0.9323644042015076, -1.057328462600708, -0.9520119428634644, 0.4500008225440979, 0.16446149349212646, -0.07448044419288635, -0.6506578922271729, 0.28215375542640686, -0.5169826745986938, 0.13664333522319794, -0.029982540756464005, -0.4103277921676636, 0.182081401348114, -0.1322953999042511, -0.8350998759269714, -0.16259852051734924, 0.14386793971061707, 1.0295509099960327, -0.9830949306488037, 0.09825440496206284, 0.20452862977981567, 0.4428379237651825, -0.9418805837631226, 1.0992071628570557, -0.10879118740558624, -0.43857812881469727, 0.27858811616897583, -0.3514851927757263, 0.02461572363972664, -0.14499928057193756, -0.0036625564098358154, -0.6596742272377014, 0.3277489244937897, 0.4223375916481018, -0.5667532682418823, 1.2261284589767456, -0.5310562252998352, 0.746363639831543, 0.2637272775173187, -0.9578540325164795, 0.09238190948963165, 0.2535743713378906, -0.11003705114126205, -0.31767281889915466, 0.19214846193790436, 0.16548390686511993, -0.5636748671531677, 0.5940688848495483, 0.7522202730178833, 0.6288660168647766, -0.23590339720249176, -0.19677478075027466, 0.6890861392021179, -0.3391779661178589, 0.1451684534549713, 0.6914897561073303, 1.0218651294708252, 0.37745940685272217, 0.06470098346471786, -0.02840716950595379, -0.014988021925091743, -0.8033802509307861, -0.05171668156981468, 0.6378061771392822, 0.522490918636322, 0.7991267442703247, 0.8265237808227539, -0.34878307580947876, -0.3853406310081482, -0.3937709629535675, 0.3889928162097931, 1.4894015789031982, -0.39270657300949097, -0.17199063301086426, -0.48044195771217346, -0.3622663617134094, -0.240829735994339, 0.3030211627483368, -0.7063306570053101, -0.5943739414215088, -0.27416878938674927, -1.1037492752075195, 1.117691159248352, 0.5904005765914917, 0.9184292554855347, -0.16868044435977936, -0.3724624216556549, 0.10523992776870728, 0.938290536403656, -0.6414880156517029, -0.39561954140663147, 0.9564691185951233, -0.3727792203426361, -0.2162528932094574, -0.07442283630371094, 0.03856253996491432, -0.09638430923223495, -0.7713897228240967, 0.703334629535675, -0.6982671022415161, -0.19279207289218903, 0.2739046514034271, 0.7365915179252625, -0.526625394821167, -0.5512246489524841, 0.3246886134147644, -0.21109256148338318, -0.011616161093115807, 0.1826881766319275, 0.005518089048564434, -0.23916544020175934, 0.13299179077148438, -0.28952643275260925, -0.47412317991256714, 0.1336754560470581, 0.1905280500650406, 0.26207154989242554, -0.27901774644851685, 0.16322551667690277, -1.4490742683410645, 0.7636968493461609, 0.05811863765120506, -0.2328660637140274, -0.4097083508968353, -0.8483270406723022, -0.17763714492321014, 0.6242812871932983, -0.631243109703064, -0.025550279766321182, -1.0795406103134155, 0.19717594981193542, -0.48852500319480896, -0.1460472047328949, -0.23167291283607483, 0.45125672221183777, -0.2403411567211151, 0.4922443926334381, 0.4902176856994629, 0.19269268214702606, 0.3858598470687866, 1.1457897424697876, -1.0513659715652466, 0.890098512172699, 0.23686721920967102, 0.6514629125595093, 0.17516395449638367, -0.11305350065231323, -0.7122939825057983, -0.4882689416408539, -0.16107770800590515, -0.036134615540504456, -0.21112926304340363, 0.1236572414636612, -0.3948265314102173, -0.9280872344970703, 0.07753503322601318, -0.8122677206993103, -0.5766671299934387, -0.32067081332206726, -0.574492871761322, -0.18920142948627472, -1.3122246265411377, -1.318880319595337, -0.4559837281703949, -0.15235628187656403, -0.8967006206512451, -0.20401547849178314, 0.1252700388431549, -0.631606936454773, -0.8030418157577515, -0.4676843583583832, -0.7003399133682251, 1.583458662033081, -0.5902294516563416, 0.45565441250801086, -0.06050839275121689, -0.40298977494239807, -0.06788181513547897, -0.2516125738620758, 0.7654908299446106, -0.5417495965957642, 0.1396501362323761, -1.3332593441009521, 0.5568206906318665, -0.38131198287010193, -0.14964187145233154, 0.4351809620857239, 0.3688995838165283, 0.5781312584877014, -0.12926237285137177, -0.2767152488231659, 0.682690441608429, 1.4536755084991455, -0.33758413791656494, 0.4311150014400482, 0.40529510378837585, 1.1391559839248657, -0.05285998433828354, -0.5530728697776794, 0.3959597647190094, 0.11699119955301285, 0.05734476074576378, 0.6566619873046875, -0.06262538582086563, -0.19031809270381927, -0.7273847460746765, 0.35887235403060913, 1.1085752248764038, 0.3553079664707184, 0.5584327578544617, -0.5801364183425903, 0.9130925536155701, -1.0885101556777954, -0.8386048078536987, 0.5367683172225952, 0.6120620965957642, 0.43743062019348145, 0.21143248677253723, -0.7695260047912598, 0.3588956892490387, 0.1513238251209259, 0.9516834020614624, -0.3209364116191864, -0.7604772448539734, 0.2219342291355133, 0.7523518204689026, 0.9696779251098633, 0.6327222585678101, -0.19222241640090942, 0.5976473689079285, 14.963546752929688, 0.739942729473114, -0.2475290447473526, 0.5452855825424194, 0.3205847442150116, 0.4179054796695709, -0.4921407997608185, 0.22529681026935577, -0.9725127220153809, 0.033173736184835434, 1.137854814529419, 0.4786438047885895, 0.6177960634231567, 0.2033357322216034, -0.10732831805944443, 0.23763428628444672, -0.4797477126121521, 0.3292962908744812, 0.20580150187015533, -1.6263331174850464, -0.007429271936416626, -0.1866215467453003, 0.25337398052215576, 0.4699975252151489, 0.861177921295166, 0.6414403915405273, 0.8226924538612366, -0.2604210376739502, 0.5981625318527222, 0.29017147421836853, 1.0532114505767822, 0.10366737842559814, 0.3281841576099396, -0.08774897456169128, -0.7792955040931702, 0.043258536607027054, -0.4030377268791199, -1.0738171339035034, 0.05712969973683357, 0.2848206162452698, -0.1663960963487625, -0.7279884219169617, -0.3003004491329193, 0.8736740350723267, 0.02506989985704422, 0.3972559869289398, -0.3932177722454071, 1.0014927387237549, -0.1605643332004547, 0.27096036076545715, 0.5298727750778198, 0.8415432572364807, 0.07556182146072388, 0.07357974350452423, -0.29155853390693665, -0.32972732186317444, -0.15774087607860565, 0.7629836201667786, -0.30940937995910645, -0.3994576036930084, -0.1259554624557495, -0.21776224672794342, -0.11494800448417664, 0.7673863768577576, 0.3948322534561157, 0.2749544084072113, -0.39765027165412903, 0.5644055008888245, 0.08444203436374664, 0.5163665413856506, -0.5609861016273499, -0.8911505341529846, 0.5010485649108887, -0.14330297708511353, 0.03310958668589592, 0.7823862433433533, -0.6506812572479248, -0.25094524025917053, -0.9857596158981323, 0.0918886587023735, 0.07939057052135468, -0.8886620402336121, -0.4810390770435333, 0.6257668137550354, -0.428821325302124, -0.2103281468153, 0.6258078217506409, -0.7214011549949646, -0.6497845649719238, 0.16395631432533264, -1.2448393106460571, -0.6217039823532104, 0.1238284632563591, -0.1626080572605133, -0.7733289003372192, -0.24970267713069916, 1.0852149724960327, 0.3497565686702728, -0.08170612156391144, 0.1342080682516098, -0.14668408036231995, 0.18636471033096313, 0.025869492441415787, -1.1543697118759155, 0.8858518004417419, 0.26375076174736023, -0.11825735867023468, 0.4430443346500397, 0.260297566652298, 0.6953331232070923, -0.3438515365123749, -0.16690576076507568, 0.20140406489372253, -0.77022784948349, 0.438173770904541, -0.4036858081817627, -0.86448734998703, 0.6466132998466492, 0.7973194718360901, 0.09281142801046371, 0.030182965099811554, -0.014659401029348373, -0.9090191125869751, -0.5086162686347961, -0.4424947202205658, 0.00143904029391706, 0.34398895502090454, -0.7848142385482788, -0.4041426181793213, -0.6891027688980103, 0.4327111840248108, -0.8730119466781616, -0.5562726259231567, -0.3423655033111572, -0.16977475583553314, -0.5071233510971069, 1.2360669374465942, -0.22798241674900055, 0.6244073510169983, 0.7397375702857971, -0.02213289402425289, -0.9749006032943726, 0.029811164364218712, -1.1905077695846558, 0.12258496880531311, 0.5450582504272461, 0.6487321853637695, -0.6927998065948486, 0.7674161195755005, 0.44037434458732605, 0.015386905521154404, -0.39120763540267944, -0.442569375038147, 0.033493272960186005, 0.06764006614685059, -1.2467308044433594, 0.1596197783946991, 0.04110754281282425, -0.1287728101015091, 0.1784762442111969, 0.29934653639793396, 0.20893028378486633, 0.26431769132614136, -1.064414620399475, 0.009862719103693962, -0.1236530989408493, -0.06743704527616501, -0.6406072974205017, -0.7951415777206421, -1.4265871047973633, -0.17912501096725464, -1.3000184297561646, -0.21058304607868195, -1.106866717338562, -0.844606876373291, 0.04381151869893074, -0.5590162873268127, 0.052286356687545776, 0.5927154421806335, -0.0865044891834259, -0.41760319471359253, -0.8511277437210083, -0.43995484709739685, 0.41185349225997925, 0.5642702579498291, -0.8690994381904602, 0.06287040561437607, 0.16560553014278412, -0.4418732821941376, 0.5636952519416809, 0.48306843638420105, -0.42517727613449097, -0.737829327583313, -0.9895234107971191, 0.44065970182418823, -0.2937406003475189, 0.39224132895469666, -0.8183102607727051, 0.7915250062942505, 0.3951735198497772, 0.02191811613738537, 0.1392083317041397, 0.278706431388855, -1.1091768741607666, -0.6381141543388367, 0.14978301525115967, -0.7396921515464783, -0.3102831244468689, 0.15357095003128052, -0.364092618227005, 0.01332425232976675, 0.7748138904571533, 0.4623226523399353, -0.931725800037384, -1.0134499073028564, 0.12906819581985474, -0.7262549996376038, 0.3281632363796234, -0.2917852997779846, -0.31524085998535156, -1.1208620071411133, -0.2543804943561554, -0.4334465563297272, 0.354446679353714, -0.6086668372154236, 0.9313700795173645, 0.32301706075668335, -1.2260847091674805, 0.4473336338996887, 0.45474421977996826, -0.1909172683954239, -0.019196273759007454, 0.4260585606098175, 0.44036635756492615, -0.4501778781414032, 0.23575648665428162, 0.0708731934428215, 0.31351637840270996, -0.22989490628242493, -0.18100666999816895, 0.8003825545310974, -0.23701141774654388, -0.32666701078414917, 1.2498284578323364, -0.4283788204193115, -0.9212455153465271, 0.5593560934066772, -1.366045594215393, -0.36309269070625305, -0.07467174530029297, 0.3531251549720764, 0.3686842918395996, -0.3672705292701721, 0.3757835924625397, -0.36034005880355835, 0.2614594101905823, -0.00531514547765255, -0.4016946256160736, 0.5802348852157593, 0.10863513499498367, -0.3041205406188965, 0.6111436486244202, 0.8842009902000427, -0.6639609336853027, -0.907217264175415, -0.8881326913833618, -0.33169010281562805, -0.15835249423980713, 0.5948972105979919, -0.24731649458408356, -0.9017834663391113, 1.1464314460754395, 0.3420553505420685, 0.46708738803863525, -0.29537853598594666, -0.10209380090236664, 0.09783772379159927, 0.24184724688529968, 0.03349880129098892, -0.6349729895591736, -0.3098715543746948, 1.017853021621704, 0.6743409633636475, -0.20014922320842743, -0.041280969977378845, -0.3263649344444275, -0.5126315951347351, 0.7327746152877808, 0.3609752953052521, -0.38653483986854553, 1.0027607679367065, 0.11304832249879837, 0.3016023337841034, 0.10609109699726105, -1.0018630027770996, -0.24399667978286743, 0.21771791577339172, 1.294248342514038, 0.3877851068973541, 0.024813249707221985, 0.744452714920044, 0.955940306186676, 0.03233153373003006, 0.15929551422595978, 0.7853546142578125, 0.22481036186218262, -0.10529256612062454, 0.28076857328414917, -0.053718604147434235, 0.8918057680130005, -1.1166983842849731, -0.6604370474815369, 0.36548930406570435, 0.5983736515045166, 0.24264885485172272, 0.3966999053955078, 0.6050232648849487, -0.3623805046081543, 0.9547179937362671, 0.15579810738563538, 0.41430550813674927, -0.30624330043792725, -0.21238836646080017, -0.21686510741710663, -0.5985531806945801, -0.4318622350692749, -0.10759934037923813, -0.2513563930988312, -0.4627929925918579, -0.2866961359977722, 0.3120737373828888, -0.09091978520154953, 0.09071791172027588, 0.761279821395874, 0.09054878354072571, 1.3729764223098755, -0.11638098955154419, -0.7217732071876526, -0.46879157423973083, -0.7229758501052856, -0.08082278072834015, -0.7075514793395996, -0.01564408652484417, -0.0714431032538414, -0.5069824457168579, -0.19848541915416718]}, "authors": [{"authorId": "1810714948", "name": "Bobby He"}, {"authorId": "145704247", "name": "James Martens"}, {"authorId": "46266081", "name": "Guodong Zhang"}, {"authorId": "3436640", "name": "Aleksandar Botev"}, {"authorId": "2065040422", "name": "Andy Brock"}, {"authorId": "2157770601", "name": "Samuel L. Smith"}, {"authorId": "1725303", "name": "Y. Teh"}], "references": [{"paperId": "5eeb80dc67590422db64ca95ec0aded24799cfb6", "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse"}, {"paperId": "eb7d596fb4ab48c2db1b0ee6d8127df7a5b5ebeb", "title": "The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization"}, {"paperId": "082aa5c92202719a5740c8e2edee65cc4bf3ccfa", "title": "Pre-training via Denoising for Molecular Property Prediction"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "d7664cd386f70a9838c3c3c83128f4d22bb9c8f7", "title": "Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "309037d7fa4bf15768b23ba77c959ceb8cd18c87", "title": "Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "b2fa8f0995a7dbe251a9946981867c3aea452652", "title": "On the validity of kernel approximations for orthogonally-initialized neural networks"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "758cf7cd62bb8b62a6a4bc550a34e0a574bbbcb2", "title": "Stable ResNet"}, {"paperId": "e00484961fb2f30d2d48a5f9853fa3ebab140cac", "title": "Improving Transformer Optimization Through Better Initialization"}, {"paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510", "title": "Infinite attention: NNGP and NTK for deep attention networks"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "e52051204cb1179584f3b008c9d38848b52c1f28", "title": "ReZero is All You Need: Fast Convergence at Large Depth"}, {"paperId": "f92b1f08d1da3aa0e0b6060780124d68a3506abf", "title": "Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "633d727ec370e729c0e398cd92032ab05ef20b35", "title": "Lipschitz Constrained Parameter Initialization for Deep Transformers"}, {"paperId": "727fbaaa672bf723790d515938cd77c189eb281e", "title": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "2f308a7372908034537ea59d8d746823aca39ab9", "title": "The Random Matrix Theory of the Classical Compact Groups"}, {"paperId": "4cf963e5fd88825ac62ad6cce364447e5d2dfb2b", "title": "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention"}, {"paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "title": "Learning Deep Transformer Models for Machine Translation"}, {"paperId": "9c81b183774e3915de02bd126093b68bf4aea0dc", "title": "How to Initialize your Network? Robust Initialization for WeightNorm & ResNets"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "e663797275a20c0ab960772fe74e86c855b33767", "title": "On the Impact of the Activation Function on Deep Neural Networks Training"}, {"paperId": "96c82727dd5a80fef93007f888bb8569feb6bd85", "title": "Fixup Initialization: Residual Learning Without Normalization"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "ad6309d1ea001098189425f54d069ef12abcb583", "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10, 000-Layer Vanilla Convolutional Neural Networks"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d23a1cd6c73e3e43295c3585b3db147eb1c3ee91", "title": "How to Start Training: The Effect of Initialization and Architecture"}, {"paperId": "0679950558d72791f16031dd08c39367d8dd47b8", "title": "Shampoo: Preconditioned Stochastic Tensor Optimization"}, {"paperId": "fb350d3b03e9308ccbd131d3d45dd44e383e6227", "title": "Gaussian Process Behaviour in Wide Deep Neural Networks"}, {"paperId": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0", "title": "Deep Neural Networks as Gaussian Processes"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e79fa48078e9c1794cb67bfb1aab9557f263820f", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is the question?"}, {"paperId": "4fdc7df2c737141a1bf5aec27a438b77d01f8af0", "title": "Deep Information Propagation"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "6e997fec1412abb4b630d0e6d4df95813a01e093", "title": "Exponential expressivity in deep neural networks through transient chaos"}, {"paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80", "title": "Identity Mappings in Deep Residual Networks"}, {"paperId": "dade3d81749385e08ec0f163c806308c94b56f32", "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity"}, {"paperId": "cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e", "title": "On the difficulty of training recurrent neural networks"}, {"paperId": null, "title": "case, \u03c3 = 1 by construction for activations transformed by DKS/TAT"}, {"paperId": "0c36366413f08c6f635af6897627a3f113f6115a", "title": "Catformer: Designing Stable Transformers via Sensitivity Analysis"}, {"paperId": "339b5d3316d13062d936b335aab06e9da48a5c17", "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers"}, {"paperId": "5c43d84705cc320414ebad1525599146bcd68ca3", "title": "Infinite attention: NNGP and NTK for deep attention networks"}, {"paperId": null, "title": "Scalable second order optimization for deep learning"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Social iqa: Commonsense reasoning about social interactions"}, {"paperId": null, "title": "2019). This means that we can use our methods in Section 3.3 in combination with these positional encoders. Let us denote the unscaled query-key dot product as S = XWQ(XWK)>"}, {"paperId": null, "title": "2021), and Xi denotes row i of the incoming representation X \u2208 RT\u00d7d to the attention block"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Bayesian learning for neural networks , volume 118"}, {"paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0", "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"}, {"paperId": null, "title": "produces by d 0 to get a \u03a3 0 with ones on the diagonal. 5 Constraint (iii) is satis\ufb01ed as lower triangular matrices are closed under multiplication and inversion"}]}