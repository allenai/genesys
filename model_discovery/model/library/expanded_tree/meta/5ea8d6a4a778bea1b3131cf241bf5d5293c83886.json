{"paperId": "5ea8d6a4a778bea1b3131cf241bf5d5293c83886", "abstract": "Vision Transformers have achieved impressive performance in many vision tasks. While the token mixer or attention block has been studied in great detail, much less research has been devoted to the channel mixer or feature mixing block (FFN or MLP), which accounts for a significant portion of of the model parameters and computation. In this work, we show that the dense MLP connections can be replaced with a block diagonal MLP structure that supports larger expansion ratios by splitting MLP features into groups. To improve the feature clusters formed by this structure we propose the use of a lightweight, parameter-free, channel covariance attention (CCA) mechanism as a parallel branch during training. This enables gradual feature mixing across channel groups during training whose contribution decays to zero as the training progresses to convergence. In result, the CCA block can be discarded during inference, enabling enhanced performance at no additional computational cost. The resulting $\\textit{Scalable CHannEl MixEr}$ (SCHEME) can be plugged into any ViT architecture to obtain a gamut of models with different trade-offs between complexity and performance by controlling the block diagonal MLP structure. This is shown by the introduction of a new family of SCHEMEformer models. Experiments on image classification, object detection, and semantic segmentation, with different ViT backbones, consistently demonstrate substantial accuracy gains over existing designs, especially for lower complexity regimes. The SCHEMEformer family is shown to establish new Pareto frontiers for accuracy vs FLOPS, accuracy vs model size, and accuracy vs throughput, especially for fast transformers of small size.", "venue": "", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work shows that the dense MLP connections can be replaced with a block diagonal MLP structure that supports larger expansion ratios by splitting MLP features into groups and proposes the use of a lightweight, parameter-free, channel covariance attention (CCA) mechanism as a parallel branch during training."}, "embedding": {"model": "specter_v2", "vector": [0.7176216244697571, 0.6712995767593384, -0.4642521142959595, 0.13406211137771606, -0.7356361150741577, 0.10936678200960159, 0.711105465888977, -0.16793514788150787, -0.06279954314231873, -0.9397940635681152, 0.5090591311454773, 0.33096006512641907, 0.38621529936790466, 0.39898917078971863, 0.19255292415618896, -0.40076708793640137, -0.7252434492111206, 0.061039041727781296, 0.015063588507473469, 0.16299588978290558, 0.08348692953586578, -0.7210608720779419, -0.9509373307228088, 0.07203478366136551, -0.030801810324192047, 1.500262975692749, 0.1352929174900055, 1.0190544128417969, -0.22627927362918854, 0.5477546453475952, 0.3743131160736084, -0.0320383757352829, 0.35574638843536377, -0.5175467729568481, -0.1546710729598999, 0.25465431809425354, 0.8138891458511353, -0.061281539499759674, -0.5850105881690979, 0.9628207683563232, 0.0003779868420679122, -0.06343626976013184, 0.4002060294151306, -1.1678712368011475, 0.14798946678638458, 0.8157373666763306, 0.544143795967102, 0.7502613067626953, -0.927958607673645, -0.5243045687675476, 1.0576671361923218, -1.1787126064300537, -0.07697147130966187, 1.7656499147415161, 0.9503328800201416, 0.1202385202050209, -0.3915817439556122, -0.7185946106910706, 0.8608618974685669, 0.4662399888038635, -0.7323329448699951, -0.620490312576294, 0.20051686465740204, 0.04804965853691101, 1.3978040218353271, -0.6250457167625427, 0.1878875195980072, 0.5917647480964661, 0.2097749412059784, 1.3887672424316406, 0.41867703199386597, -0.37369054555892944, 0.2505936622619629, 0.20056217908859253, 0.38563695549964905, 0.48640257120132446, -0.38120904564857483, 0.15111932158470154, -1.1325953006744385, 0.20079781115055084, 0.2229694277048111, -0.23663628101348877, 0.39191868901252747, -0.35828226804733276, -0.004050588700920343, 0.8293861150741577, 0.674741804599762, 0.3755372166633606, -0.036442048847675323, 1.1652793884277344, 0.7203598022460938, 0.34479403495788574, 0.05857421085238457, 0.2935444414615631, -0.09196662157773972, 0.49155178666114807, -0.8203244805335999, -0.1955469697713852, 0.04022780433297157, 0.906596839427948, -0.21824577450752258, 0.6460785865783691, -0.6853313446044922, 0.16548149287700653, 1.6789414882659912, 0.30541086196899414, 0.24614188075065613, -0.9344452023506165, -0.15912461280822754, -0.921714186668396, -0.20306247472763062, -0.6471545100212097, 0.008235801942646503, -0.7324821352958679, -1.0393568277359009, -1.016729474067688, -0.38471826910972595, 0.44579049944877625, -1.3769872188568115, 0.45183220505714417, -0.3777073919773102, 0.5768842697143555, 0.14475473761558533, 0.08121239393949509, 0.3223704993724823, 0.3595085144042969, 0.8992078304290771, 0.16957232356071472, 1.234844446182251, -0.965142011642456, -0.37213557958602905, -0.7904496192932129, -0.2582065463066101, -0.34791478514671326, -0.3294283449649811, 0.16564005613327026, -0.8993655443191528, -1.4989091157913208, -0.6424815654754639, -0.03677833825349808, -0.26876792311668396, 0.12159297615289688, 1.2900712490081787, 0.8606669306755066, -1.1298021078109741, 0.500043511390686, -0.37889429926872253, -0.4432557225227356, 1.172385334968567, 0.396993488073349, 0.9628373980522156, 0.04980077967047691, -0.7349138855934143, 0.21271714568138123, 0.02512640319764614, -0.16359873116016388, -0.8219488263130188, -0.5611074566841125, -0.7488495707511902, 0.3811267018318176, 0.025231463834643364, -0.7135518193244934, 1.1408709287643433, -0.6946619749069214, -1.3190844058990479, 0.18725338578224182, -0.4285898506641388, -0.1868928223848343, 0.326799213886261, -0.004745871294289827, -0.09792148321866989, -0.12254974246025085, -0.4259031414985657, 0.7129718065261841, 1.391984224319458, -0.1277123987674713, -0.6046662926673889, 0.24696579575538635, -0.46409499645233154, 0.0652201846241951, -0.2794848680496216, 0.8623632788658142, -0.3484033942222595, -0.013085220009088516, 0.8203710317611694, 0.4792863130569458, -0.3794838786125183, -0.1781691461801529, -0.002347873756662011, -0.8989361524581909, 1.102157473564148, 0.33966711163520813, 0.29553279280662537, -0.9293506741523743, -0.8451735973358154, 0.11483818292617798, 0.24448443949222565, -0.15475402772426605, -0.8269803524017334, 0.12905199825763702, -0.1239771768450737, -0.2264668047428131, 0.10679255425930023, -1.0408446788787842, 0.18449914455413818, -0.3455812931060791, -0.6087191104888916, 0.12507390975952148, 0.23247307538986206, 1.1182070970535278, -0.6041367650032043, 0.03428918123245239, 0.1994055211544037, 0.5308058857917786, -1.0719630718231201, 0.9501308798789978, 0.033329661935567856, 0.3008352220058441, -0.147604301571846, 0.10377407819032669, 0.012628832831978798, -0.55382239818573, 0.8464072346687317, -0.8053408265113831, 0.18287070095539093, 0.48263251781463623, -0.6461418271064758, 1.2630596160888672, -0.29224082827568054, 0.9783870577812195, 0.13071924448013306, -1.0080485343933105, 0.38376927375793457, 0.6291566491127014, -0.16731800138950348, -0.7192869782447815, 0.8691760301589966, 0.37184029817581177, -0.6294623017311096, 0.5270100235939026, 0.6832146048545837, 1.042292833328247, -0.36819103360176086, -0.5030059218406677, 1.0847874879837036, -0.49448737502098083, 0.1229342669248581, 0.49952536821365356, 0.3747645914554596, -0.018436292186379433, 0.3555431067943573, -0.20721407234668732, 0.034047629684209824, -1.0993465185165405, -0.028519637882709503, 0.9851759076118469, 0.1406354308128357, 0.8969528675079346, 0.519736111164093, -0.5760512351989746, -0.655144214630127, -0.11755555868148804, 0.3358516991138458, 1.0156408548355103, 0.22775937616825104, -0.04580221325159073, -0.6891149878501892, -0.24737036228179932, -0.3371971845626831, -0.9215721487998962, -0.281111478805542, -0.21605582535266876, -0.1672881841659546, -1.0573790073394775, 0.7783175110816956, 0.7159615159034729, 1.2652546167373657, -0.4850628674030304, -0.43224456906318665, -0.12473265081644058, 0.14059484004974365, -0.8574602603912354, -0.19524280726909637, 0.635810136795044, -0.4740180969238281, -0.24613553285598755, -0.034885473549366, -0.039250440895557404, 0.15775497257709503, -0.47514376044273376, 0.6421578526496887, -0.8983845114707947, -0.5985735058784485, 0.4306034743785858, 0.49098047614097595, -0.9672086238861084, -0.3912211060523987, 0.17777281999588013, -0.12310325354337692, 0.33429446816444397, 0.22888754308223724, 0.2871895730495453, -0.2684713900089264, -0.22152923047542572, 0.17068594694137573, -0.11540844291448593, 0.19083856046199799, 0.4238579273223877, 0.7202110290527344, -0.5340628623962402, -0.25248345732688904, -0.8479428291320801, 0.6589356064796448, 0.04045187681913376, -0.3837365210056305, -0.05378713458776474, -0.7905858755111694, -0.3311247229576111, 0.22411932051181793, -0.5429814457893372, -0.28582295775413513, -0.47573450207710266, 0.4087764024734497, -0.7223027348518372, -0.5010024309158325, -0.1873476207256317, 0.4828738272190094, -0.28512728214263916, 0.2647415101528168, 0.500470757484436, 0.18262304365634918, -0.003583230311051011, 0.7284060716629028, -0.6293370127677917, 0.7096109986305237, 0.3218759000301361, -0.05249767750501633, 0.227878138422966, 0.06097038462758064, -0.929114043712616, -0.37391722202301025, -0.9513033032417297, -0.19395476579666138, -0.3834208548069, 0.35462629795074463, -0.9483776688575745, -1.0680723190307617, 0.46515050530433655, -0.7405868768692017, -0.21701054275035858, -0.11171855032444, -0.056026116013526917, -0.3881486654281616, -1.27020263671875, -1.0938791036605835, -0.5135462284088135, -0.5859492421150208, -1.0297834873199463, 0.3538608253002167, 0.6929017305374146, -0.33914026618003845, -0.4028579592704773, -0.19986587762832642, -0.4829493761062622, 1.45023512840271, -0.613227903842926, 0.19602102041244507, -0.19185574352741241, -0.5625712275505066, 0.2679829001426697, -0.2659189999103546, 0.7172639966011047, -0.5659649968147278, -0.0420549139380455, -1.549329161643982, 0.019238848239183426, -0.4144108295440674, -0.4662545919418335, 0.798526406288147, 0.703454852104187, 0.6620932817459106, 0.10461712628602982, -0.38824132084846497, 0.7506229281425476, 1.3633846044540405, -0.4961264431476593, 0.21365486085414886, 0.009643175639212132, 1.1714732646942139, -0.16174645721912384, -0.7313555479049683, 0.3941896855831146, 0.29833996295928955, 0.1545252501964569, 0.1614323854446411, -0.2992228865623474, -0.9454964995384216, -0.33389565348625183, 0.18703876435756683, 1.0739325284957886, 0.23898375034332275, 0.06741321086883545, -0.7890511155128479, 0.6003109216690063, -1.0770893096923828, -0.7965260744094849, 0.5567896366119385, 0.28391143679618835, -0.31167319416999817, -0.40542247891426086, -0.27944719791412354, 0.13950975239276886, 0.4128088355064392, 0.385117769241333, -0.13179612159729004, -0.6692646741867065, 0.22827963531017303, 0.7715508937835693, 0.49139079451560974, 0.6016318202018738, -0.8674282431602478, 0.5591201186180115, 14.801722526550293, 0.7679187059402466, -0.536291778087616, 0.6291229128837585, 0.7105055451393127, 0.6364532113075256, -0.12774479389190674, 0.23425252735614777, -1.111190915107727, -0.14487658441066742, 1.2735917568206787, 0.34298545122146606, 0.49977630376815796, 0.5345364809036255, -0.27538159489631653, 0.43016287684440613, -0.27120083570480347, 0.8221853375434875, 0.5202670693397522, -1.1836459636688232, -0.007932992652058601, -0.012457551434636116, 0.4175621271133423, 0.6160330176353455, 0.9651381969451904, 0.7794700860977173, 0.4192388951778412, -0.19861775636672974, 0.9456464648246765, -0.2582832872867584, 0.8590565323829651, 0.2770942449569702, -0.05581941828131676, -0.26432451605796814, -1.5757205486297607, -0.5557864904403687, -0.3669275641441345, -1.0371264219284058, 0.08080992102622986, -0.16042964160442352, -0.5371792316436768, -0.45742109417915344, 0.07741322368383408, 0.8919539451599121, -0.14763259887695312, 0.790073573589325, -0.21799086034297943, 1.103092074394226, -0.2223556786775589, 0.14220848679542542, 0.3231305181980133, 0.8588328957557678, 0.055121488869190216, 0.15976645052433014, -0.43646636605262756, -0.28805387020111084, 0.45452144742012024, -0.01564241200685501, -0.5539186000823975, -0.6934841275215149, 0.22503644227981567, -0.05422648414969444, -0.08926606923341751, 1.1156257390975952, -0.516703724861145, -0.23103520274162292, 0.00589626794680953, 0.15652036666870117, 0.2479979693889618, 0.2937037944793701, -0.3521850109100342, -0.19763454794883728, 0.24251383543014526, -1.0793970823287964, 0.7059654593467712, 0.6896880865097046, -0.5017126202583313, -0.5500110983848572, -1.1628626585006714, -0.48395833373069763, 0.4015975296497345, -0.5701034069061279, -0.4181016981601715, 0.925481379032135, -0.2596331238746643, -0.05490595102310181, 0.773594856262207, -0.7217863202095032, -0.223828986287117, 0.2160143107175827, -1.7436317205429077, -1.1257212162017822, -0.2920837998390198, -0.06363694369792938, -0.07070688903331757, 0.05825541168451309, 1.0351712703704834, -0.060611966997385025, -0.05072527751326561, 0.03512009233236313, -0.09280897676944733, 0.12181594967842102, -0.3377002775669098, -0.6621991991996765, 0.8228214383125305, 0.4347267746925354, 0.1173660010099411, -0.25320976972579956, 0.025988008826971054, 0.632026731967926, -0.44395482540130615, 0.0712149366736412, 0.5118869543075562, -0.686025857925415, -0.32428643107414246, -0.7755044102668762, -0.5588104128837585, 0.2583567202091217, 0.41485023498535156, 0.29385533928871155, -0.042173366993665695, 0.4026496708393097, -1.1364561319351196, -0.4337296485900879, -0.555469274520874, -0.5412164926528931, 0.12123735249042511, -0.9568815231323242, -0.3871620297431946, -0.5900983810424805, -0.13684260845184326, -0.9989830851554871, -0.20166486501693726, -0.3650761544704437, 0.5225143432617188, -0.1542227417230606, 1.0842291116714478, -0.15922659635543823, 0.1929830014705658, 0.7889571785926819, -0.32744356989860535, -0.5369666814804077, -0.1984778642654419, -0.890738308429718, -0.05387655273079872, 0.363756000995636, 0.05085375905036926, -0.5719610452651978, 0.5069985389709473, 0.24146869778633118, 0.019103342667222023, -0.020063750445842743, -0.7683242559432983, -0.01376856118440628, -0.4630703926086426, -0.8114498853683472, 0.12270087748765945, -0.21939145028591156, -0.1873224526643753, 0.07473703473806381, 0.5250213146209717, 0.6086940169334412, 0.3922567665576935, -0.533876895904541, 0.2928137481212616, -0.1532997190952301, -0.3130483031272888, -0.4602733850479126, -0.7887847423553467, -1.402302861213684, -0.43568649888038635, -1.0266889333724976, 0.32514089345932007, -0.6124306321144104, -0.4420190751552582, -0.37515023350715637, -0.7667168974876404, -0.04264770820736885, 0.36358919739723206, 0.20417016744613647, 0.14339929819107056, -0.3139316737651825, -0.8599028587341309, 0.650734543800354, 0.4037545621395111, -0.8831064105033875, -0.034400343894958496, -0.060627274215221405, -0.31521567702293396, 0.5782122015953064, 0.35110482573509216, -0.6049734354019165, -0.7763147354125977, -0.8648598790168762, 0.056886836886405945, -0.1223326325416565, 0.12799812853336334, -1.3936634063720703, 0.8144245743751526, 0.5704800486564636, 0.11639145016670227, 0.6284964084625244, 0.3854006826877594, -0.8357688784599304, -0.4601825475692749, 0.6674466133117676, -1.05601167678833, -0.5272772312164307, -0.018023133277893066, -0.22137899696826935, -0.005457463674247265, 1.3179959058761597, 0.390964537858963, -0.9825823903083801, -1.1910239458084106, 0.303573876619339, -0.4861431419849396, 0.11321930587291718, -0.3087480366230011, -0.06565166264772415, -1.1014288663864136, -0.02147054299712181, -0.1786840260028839, 0.0023817692417651415, -0.7428353428840637, 0.9885517954826355, 0.3661711513996124, -0.6006671190261841, 0.07877646386623383, 0.4201483130455017, -0.21656791865825653, -0.19917604327201843, 0.48747867345809937, 0.7927955389022827, -0.1757861077785492, 0.39996716380119324, -0.26067936420440674, -0.19806963205337524, -0.809727132320404, -9.230437717633322e-05, 0.7691712379455566, -0.22201399505138397, -0.22853757441043854, 1.3446475267410278, -0.36417925357818604, -0.41183170676231384, 0.5473220348358154, -1.3207693099975586, -0.47914743423461914, 0.15549403429031372, 0.5779268741607666, 0.2532047927379608, -0.17732897400856018, 0.18409037590026855, -0.48561495542526245, 0.28604811429977417, 0.05741459131240845, -0.39810821413993835, 0.2034316211938858, -0.41501539945602417, 0.004639884922653437, 0.326457679271698, 0.7469912171363831, -1.1915764808654785, -1.211975336074829, -0.6059384346008301, -0.6810299158096313, -0.16103869676589966, 0.46843042969703674, -0.1541542112827301, -0.8235920667648315, 1.0009998083114624, 1.2712184190750122, 0.3824436664581299, 0.14710800349712372, 0.03420744463801384, -0.20270387828350067, 0.42022404074668884, 0.21698422729969025, -0.44924870133399963, -0.38595160841941833, 0.930551290512085, 0.6274145841598511, -0.5826736092567444, 0.10024183243513107, -0.31350499391555786, -0.5210415124893188, 0.704666793346405, 0.04680671542882919, -0.405792772769928, 0.9220913052558899, -0.23557206988334656, 0.44164377450942993, 0.15240630507469177, -0.8209223747253418, -0.5987045764923096, 0.7692682147026062, 1.3576840162277222, 0.19984667003154755, 0.0705643743276596, 0.3259667754173279, 0.42813777923583984, 0.28575441241264343, 0.05342746526002884, 0.38203194737434387, 0.39920294284820557, -0.3278774917125702, 0.25404730439186096, -0.1570950597524643, 0.4827868640422821, -0.7191885709762573, -0.28025659918785095, 0.32198089361190796, 0.30221933126449585, 0.5120112299919128, 0.6480780839920044, 1.0135936737060547, 0.3405880033969879, 0.4714577794075012, -0.07871375232934952, 0.6701104640960693, -0.24980999529361725, -0.39373382925987244, 0.28821587562561035, -0.7565016746520996, -0.44640153646469116, -0.5995217561721802, -0.6475258469581604, -0.05607271194458008, 0.12141352146863937, 0.24995242059230804, -0.0684206560254097, 0.5924206376075745, 0.9279189109802246, 0.5456048846244812, 0.9816147089004517, -0.044040802866220474, -1.0532563924789429, -0.26345670223236084, -0.7606302499771118, 0.02619859017431736, -0.5397940278053284, 0.19138124585151672, -0.16233153641223907, -0.10643695294857025, 0.3936905860900879]}, "authors": [{"authorId": "2269143317", "name": "Deepak Sridhar"}, {"authorId": "2110461237", "name": "Yunsheng Li"}, {"authorId": "2269143167", "name": "Nuno Vasconcelos"}], "references": [{"paperId": "c85268696fe1435605ae66a18653cfdcf8153753", "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "d2f63b56fc6bc373f5c023454c2b253326962865", "title": "DeiT III: Revenge of the ViT"}, {"paperId": "259c681c76335540e13081efad584efdf9101868", "title": "DaViT: Dual Attention Vision Transformers"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "75c642ebdcfbd4c16d6c3161130b72ff9af5c311", "title": "Three things everyone should know about Vision Transformers"}, {"paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5", "title": "QuadTree Attention for Vision Transformers"}, {"paperId": "e5cb26148791b57bfd36aa26ce2401e231d01b57", "title": "Vision Transformer with Deformable Attention"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "333212e246fb65f7c9d43862021e78f007c48449", "title": "A Survey of Visual Transformers"}, {"paperId": "a66686e60a3eda0c606e036403cf0a07a5962595", "title": "Mobile-Former: Bridging MobileNet and Transformer"}, {"paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede", "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"}, {"paperId": "a9c214e846188adb645021cd7b1964b8ea1fef6f", "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "492276505cf4a5c6c5c421bbdd9d01557438578d", "title": "Gaussian Context Transformer"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "14c52ffa7ea9c1971d5d82ea369c946c98d056a9", "title": "LocalViT: Bringing Locality to Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"}, {"paperId": "2984ab83ade26639c3a82d29628d0d9e4abbebb0", "title": "Incorporating Convolution Designs into Visual Transformers"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1", "title": "Panoptic Feature Pyramid Networks"}, {"paperId": "d9a826707851cf9b25ae941111197399a34a6df4", "title": "Quantifying the separability of data classes in neural networks"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "5582bebed97947a41e3ddd9bd1f284b73f1648c2", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}]}