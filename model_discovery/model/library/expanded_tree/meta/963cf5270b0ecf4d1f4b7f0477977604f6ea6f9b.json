{"paperId": "963cf5270b0ecf4d1f4b7f0477977604f6ea6f9b", "abstract": "Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generalization over dense training. We show that an ensemble of sub-models can be formed from the subsampled pathways within a network, which can achieve better performance than its densely attended counterpart. We perform experiments on a variety of NLP, computer vision and graph learning tasks in both generative and discriminative settings to provide empirical evidence for our claims and show the effectiveness of the proposed method.", "venue": "Knowledge Discovery and Data Mining", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.01705", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is shown that an ensemble of sub-models can be formed from the subsampled pathways within a network, which can achieve better performance than its densely attended counterpart."}, "embedding": {"model": "specter_v2", "vector": [0.269095778465271, 1.1217797994613647, -0.15913067758083344, 0.11186151206493378, -0.022767486050724983, -0.27644625306129456, 0.7022514939308167, -0.2693774700164795, 0.144526869058609, 0.15772148966789246, 0.6745579242706299, 0.16631123423576355, 0.24829436838626862, -0.24073249101638794, -0.6377848982810974, 0.11291354894638062, -0.813804030418396, 0.25590020418167114, 0.5131426453590393, -0.24598431587219238, -0.27278539538383484, -0.7025112509727478, -1.028298020362854, 0.15946142375469208, 0.6562305092811584, 0.5230053663253784, 0.1389535516500473, 0.7271699905395508, -0.33627328276634216, 0.48537713289260864, 0.618227481842041, -0.6068401336669922, 0.10836675763130188, -0.4019579589366913, -0.574542224407196, -0.2008313685655594, 0.5587884187698364, 0.12222909182310104, -1.0656930208206177, 1.0052250623703003, -0.34347909688949585, -0.07906798273324966, 0.4925500154495239, -0.6889762282371521, -0.263714998960495, 1.4449374675750732, 0.795489490032196, 1.055069923400879, -0.1911839097738266, -0.7429719567298889, 1.7600945234298706, -0.9853336811065674, 0.46866992115974426, 1.5670925378799438, 0.4800035357475281, 0.4991459250450134, -0.8320093750953674, -0.7602095603942871, 0.7825366258621216, 0.43165841698646545, -0.8049208521842957, -0.22002530097961426, 0.07607534527778625, -0.04664888232946396, 1.772162675857544, -0.7850319147109985, 0.09843490272760391, 0.677898108959198, 0.27224740386009216, 1.560978651046753, 0.19972728192806244, -0.3896515667438507, -0.4718860983848572, 0.17603424191474915, 0.36887651681900024, 0.9252134561538696, -0.20536544919013977, 0.1940116435289383, -1.0742312669754028, -0.08482363075017929, 0.4953833818435669, -0.07644915580749512, -0.1974705010652542, -0.25823962688446045, -0.1072954535484314, 0.4166905879974365, 0.8474411368370056, 1.0627036094665527, -0.5643683075904846, 0.8794702291488647, 0.41819676756858826, 0.5441445112228394, -0.2000323086977005, 0.3722221553325653, 0.035572368651628494, 0.5881673097610474, -0.7740240097045898, 0.05396832525730133, 0.21770526468753815, 1.0091688632965088, 0.5167497396469116, 0.13223141431808472, -0.1434192806482315, 0.10686740279197693, 1.2688206434249878, -0.4874458611011505, 0.6469041705131531, -0.603508710861206, 0.25436967611312866, -0.22011280059814453, -0.2580271363258362, -0.9058070182800293, -0.20882520079612732, -0.6520925760269165, -0.860722541809082, -1.3550198078155518, -0.23982536792755127, 0.15170307457447052, -0.37873759865760803, 0.921056866645813, -0.23043397068977356, 0.14674142003059387, -0.2935231626033783, 0.36225277185440063, 0.3155825734138489, 0.3974061608314514, 0.32089757919311523, 0.46176472306251526, 0.6814706325531006, -0.7076289057731628, -0.351630300283432, -1.0395302772521973, 0.36040589213371277, 0.24831125140190125, 0.18682970106601715, -0.3726232647895813, -1.1049630641937256, -0.9798087477684021, -0.8783352375030518, 0.2923397719860077, -0.3088117837905884, -0.08089721202850342, 1.2480149269104004, 0.1735192835330963, -1.022909164428711, 1.4033229351043701, -0.20965227484703064, -0.5349751710891724, 0.8211386203765869, 0.2849077582359314, 0.20836026966571808, -0.3448244035243988, -1.353985071182251, 0.07467631995677948, 0.24661360681056976, -0.5216732025146484, -0.7216103076934814, -0.5667338967323303, -1.0596047639846802, 0.3672125041484833, 0.6893891096115112, -0.8754196763038635, 0.955933153629303, -0.24804584681987762, -0.6218027472496033, 0.8579190373420715, -0.44952186942100525, -0.2622981667518616, 0.09124072641134262, 0.07838128507137299, -0.27954405546188354, -0.2137582153081894, 0.1688496172428131, 0.2388722151517868, 0.37116193771362305, -0.6930427551269531, -0.3323812484741211, -0.3535713255405426, -0.32064276933670044, -0.3777029812335968, -0.14505904912948608, 0.43154123425483704, -0.5415796041488647, -0.041718170046806335, 0.3823506534099579, 0.6434797644615173, 0.05311768129467964, -0.5317519307136536, -0.7511045932769775, -1.3383644819259644, 0.36301398277282715, 0.21393495798110962, 0.8165720701217651, -0.7788290977478027, -0.6412068009376526, 0.05290272459387779, 0.4269242286682129, -0.4869914948940277, -0.7217597961425781, 0.8356061577796936, -0.6231685876846313, 0.6595081090927124, -0.17350907623767853, -0.8376235961914062, -0.029719961807131767, -0.013556639663875103, -0.5570454597473145, -0.3547671437263489, 0.1912386417388916, 0.9744022488594055, -1.0572303533554077, -0.1279391497373581, -0.3926871120929718, 0.19088682532310486, -0.5751305222511292, 0.8903211355209351, -0.4481584131717682, 0.0005449876189231873, -0.33341822028160095, -0.18088366091251373, -0.26237499713897705, -0.26904916763305664, 0.1679231822490692, -0.5878345966339111, 0.3766236901283264, 0.6873152852058411, -0.26448142528533936, 1.2498273849487305, -0.33063140511512756, 0.6010231971740723, -0.18855208158493042, -0.9558825492858887, -0.09078540652990341, 0.2984965443611145, -0.022418757900595665, -0.41509464383125305, 0.07591844350099564, 0.03046230413019657, -0.31152525544166565, 0.3682914078235626, 0.2518558204174042, 0.8308437466621399, -0.1527501791715622, 0.4108382761478424, 1.1010085344314575, -0.11355571448802948, 0.6073967218399048, 1.0047569274902344, 1.1592466831207275, 0.5714671015739441, 0.35009151697158813, -0.21707594394683838, 0.14509063959121704, -0.7418081760406494, 0.09784581512212753, 0.6531482934951782, 0.6493419408798218, 0.5341128706932068, 0.7948808670043945, -0.5095869898796082, -0.36801037192344666, 0.07785557955503464, 0.6636789441108704, 1.352748155593872, -0.31031084060668945, -0.27247652411460876, -0.8368809223175049, -0.627838134765625, -0.3635336756706238, 0.20647647976875305, -0.7678858041763306, -0.4401523768901825, -0.088053859770298, -0.9695129990577698, 0.6096502542495728, 0.3089539408683777, 1.2301139831542969, -0.6844599843025208, 0.046259600669145584, 0.09766695648431778, 0.47739285230636597, -0.816807210445404, -0.6993711590766907, 0.5838483572006226, -0.38289937376976013, -0.41992896795272827, 0.19629916548728943, -0.12865671515464783, 0.15917712450027466, -0.6021154522895813, 1.1772406101226807, -0.4005555212497711, -0.568534791469574, 0.5357120037078857, 0.8500657081604004, -0.4646468758583069, -0.4483204782009125, 0.37441369891166687, 0.1092480793595314, 0.10030597448348999, 0.2549031376838684, 0.023015424609184265, -0.26922476291656494, 0.1952703893184662, -0.2326527237892151, -0.32054004073143005, 0.023846611380577087, 0.17510195076465607, 0.6287362575531006, 0.5303868055343628, 0.06189139559864998, -1.699546456336975, 0.7450157999992371, -0.3126242756843567, -0.1704225093126297, -0.004001375753432512, -0.8846566081047058, -0.2480982095003128, 0.6107763648033142, -0.714407742023468, -0.487351655960083, -0.8645408153533936, 0.8615700006484985, -0.0721069946885109, -0.48801571130752563, 0.20379573106765747, -0.14221897721290588, -0.15687361359596252, 0.5346806645393372, 0.39989715814590454, 0.12078855186700821, 0.2617977559566498, 0.5439727306365967, -1.3528681993484497, 0.6300975680351257, 0.09189462661743164, 0.439831405878067, -0.025243164971470833, 0.059274978935718536, -0.8051028251647949, -0.8426832556724548, -0.3896344304084778, -0.004468213766813278, -0.18565820157527924, -0.1533142477273941, -0.40840235352516174, -0.9439137578010559, -0.22392742335796356, -0.6544901132583618, -0.6154355406761169, -0.008035264909267426, -0.6364043354988098, -0.034517787396907806, -1.0576788187026978, -1.125576138496399, -0.4748263359069824, -0.5476505756378174, -0.4637846052646637, 0.1604880541563034, 0.32502150535583496, -0.4033221900463104, -0.9095306992530823, -0.09809211641550064, -0.4971938729286194, 0.9156056046485901, -0.5368981957435608, 0.5773890614509583, -0.05804499611258507, -0.6616023182868958, -0.1266040802001953, -0.19255413115024567, 0.3767927587032318, -0.14341479539871216, -0.19759753346443176, -0.7581055760383606, 0.4438459575176239, -0.48484131693840027, -0.39428970217704773, 0.46439847350120544, 0.46752840280532837, 1.0409835577011108, -0.18421503901481628, -0.9320476651191711, 0.28358906507492065, 1.7847776412963867, -0.7337300777435303, 0.25754112005233765, 0.23510751128196716, 0.8994607925415039, 0.5507800579071045, -0.6670730710029602, 0.01526233833283186, 0.895109236240387, -0.2867991328239441, 0.45577433705329895, -0.04375135898590088, -0.14308376610279083, -1.1205450296401978, 0.08716292679309845, 0.7679755091667175, 0.31042391061782837, 0.10151266306638718, -1.0113476514816284, 1.1341853141784668, -1.1974455118179321, -0.957388162612915, 0.6551575064659119, 0.5644228458404541, 0.24045173823833466, -0.37432536482810974, -0.258588582277298, -0.18089745938777924, 0.35357195138931274, 0.09528596699237823, -0.7499869465827942, -0.024580219760537148, -0.08313258737325668, 0.586207926273346, 0.49907466769218445, 0.44675111770629883, -0.13318534195423126, 0.8022149801254272, 14.829076766967773, 0.6258344650268555, 0.48569923639297485, 0.5399436950683594, 0.721325159072876, 0.6466179490089417, -0.5667657852172852, 0.30516040325164795, -1.2958465814590454, -0.09647748619318008, 0.9773871898651123, -0.04975418746471405, 0.739683985710144, 0.07121188938617706, -0.1312684565782547, 0.1508888304233551, -0.5015506744384766, 0.28603705763816833, 0.3903247117996216, -1.0850754976272583, 0.5217233896255493, 0.40949028730392456, 0.3512043058872223, 0.6601384282112122, 0.6413647532463074, 0.622031569480896, 1.040731430053711, -0.2273639589548111, 0.13318729400634766, 0.5148800015449524, 0.8682714700698853, 0.17100095748901367, 0.17339713871479034, 0.5485670566558838, -0.8564294576644897, -0.39314746856689453, -0.8453384637832642, -0.6647001504898071, -0.045929424464702606, 0.22093529999256134, -0.46561479568481445, -0.5257154703140259, 0.21972864866256714, 0.9514033794403076, -0.18967777490615845, 0.4510074555873871, -0.41407322883605957, 0.8312575221061707, -0.07288864254951477, -0.028233511373400688, 0.16418862342834473, 0.6738389730453491, 0.2344920039176941, -0.14021512866020203, 0.02769835852086544, -0.038306716829538345, -0.08880943059921265, 0.810463547706604, -0.6825292706489563, -0.28767314553260803, -0.531430184841156, -0.3738059997558594, -0.07764624804258347, 1.0783337354660034, 0.5242816209793091, 0.22323134541511536, -0.1706388145685196, 0.3991476595401764, 0.32980766892433167, 0.33532723784446716, 0.01824597641825676, -0.3102346360683441, -0.025945007801055908, 0.07351764291524887, -0.12206239253282547, 0.7631593346595764, -0.13254515826702118, -0.3703191876411438, -1.1540608406066895, -0.15620021522045135, 0.5814248919487, -1.1152551174163818, -0.9353988766670227, 1.0844467878341675, -0.5629193782806396, -0.05255196616053581, 0.26317140460014343, -0.7343108654022217, -0.4838607609272003, 0.3431479334831238, -1.1082463264465332, -0.7792158126831055, 0.006260397844016552, -0.03963741287589073, -0.44235464930534363, -0.25687405467033386, 1.0425374507904053, -0.17277918756008148, -0.2791173458099365, -0.1475382149219513, -0.40342164039611816, -0.14994484186172485, -0.3857324719429016, -1.352437973022461, 0.7941958904266357, 0.1452188491821289, 0.23334991931915283, 0.44999948143959045, 0.33859243988990784, 0.18427959084510803, -0.49011608958244324, -0.0672261118888855, 0.6839753985404968, -0.9583459496498108, -0.031420715153217316, -0.45239660143852234, -1.0747103691101074, 0.6332394480705261, 0.7214149832725525, -0.10471968352794647, 0.30766206979751587, 0.14325740933418274, -0.5478088855743408, -0.32831788063049316, -0.8264246582984924, -0.025074683129787445, 1.0208927392959595, -0.8458932638168335, -0.3718642294406891, -0.18503119051456451, -0.006029834505170584, -0.5177969336509705, -0.444021999835968, -0.5415725111961365, 0.08085732907056808, -0.3306525647640228, 0.7721182703971863, -0.3877957761287689, 0.6154401302337646, 0.8598495721817017, 0.17855338752269745, -1.1262983083724976, -0.2271525114774704, -1.2146393060684204, -0.0745013877749443, 0.6603090763092041, 0.7645039558410645, -0.8346301317214966, 0.5050948858261108, 0.698997974395752, 0.31898626685142517, -0.36159417033195496, -0.4626651704311371, -0.04353873431682587, -0.20486252009868622, -0.4282282292842865, 0.40354105830192566, 0.1962558627128601, -0.083476223051548, 0.4039475619792938, 0.8873149752616882, 0.5171531438827515, 0.5388727188110352, -0.7649431228637695, 0.09889646619558334, -0.44821614027023315, -0.03946218267083168, -0.5428755283355713, -0.37470459938049316, -1.2308250665664673, 0.11291644722223282, -1.2757737636566162, -0.2980470061302185, -1.3688324689865112, -0.3693235218524933, 0.006071544252336025, -0.362979531288147, -0.04655781015753746, 0.5801370739936829, -0.350618451833725, -0.8139810562133789, -0.7062219977378845, -0.5421018600463867, 0.5654386878013611, 0.912281334400177, -0.8323016166687012, 0.15376177430152893, 0.09337488561868668, -0.8111183047294617, 0.27452144026756287, 0.579010009765625, -0.7596223950386047, -0.8270471692085266, -0.9377104043960571, 0.6237182021141052, -0.28365397453308105, 0.055450689047575, -0.6254284381866455, 1.0144445896148682, 0.8911076784133911, -0.11373847723007202, 0.06848534941673279, 0.028278585523366928, -0.9188674092292786, -0.471152126789093, 0.12817564606666565, -0.7123380899429321, -0.07297845184803009, -0.06943981349468231, 0.0378548800945282, -0.10877574980258942, 0.6771875619888306, -0.17468750476837158, -1.8511954545974731, -0.3175450563430786, 0.7723174691200256, -0.5218600630760193, 0.28099536895751953, -0.35290589928627014, -0.31295648217201233, -1.036101222038269, -0.44416964054107666, -0.44396281242370605, 0.6443941593170166, -0.4326978921890259, 0.704383373260498, 0.28712648153305054, -0.8272414207458496, 0.14511434733867645, 0.2467750906944275, -0.0788121223449707, 0.18666444718837738, 0.7182406187057495, 0.22048410773277283, 0.1220720186829567, 0.347208708524704, 0.44194847345352173, 0.4058482348918915, -0.485380083322525, -0.03095075860619545, 1.0011489391326904, -0.5419617295265198, -0.3845214545726776, 1.1010087728500366, -0.1154833734035492, -0.9344815015792847, 0.21723243594169617, -1.0553042888641357, -0.839706540107727, -0.04883977770805359, 0.44552090764045715, -0.008165387436747551, -0.6674094200134277, 0.03186173364520073, -0.299152672290802, 0.39089566469192505, -0.11948131769895554, -0.12508133053779602, 0.8674449920654297, -0.47733694314956665, -0.6038658618927002, 0.6559327244758606, 0.4763254225254059, -0.8146485686302185, -0.7787531614303589, -1.0759837627410889, -0.24717728793621063, -0.053442396223545074, 0.5229878425598145, -0.43591657280921936, -0.6228445768356323, 0.804691731929779, 0.18896278738975525, 0.908592164516449, -0.031942274421453476, 0.3788435161113739, -0.01990310102701187, 0.5293266773223877, 0.21600604057312012, -0.43154439330101013, -0.24079082906246185, 0.8044865727424622, 1.2951951026916504, -0.35461854934692383, -0.039045266807079315, -0.37058860063552856, -0.7609270811080933, 0.8572394847869873, 0.5506588816642761, -0.37266066670417786, 0.900279700756073, -0.34097573161125183, -0.4738742709159851, -0.3686102628707886, -1.3375658988952637, -0.6161707639694214, 0.6207238435745239, 1.3666844367980957, 0.4087558388710022, -0.16549067199230194, 0.3368176817893982, 1.1032038927078247, -0.08825629949569702, -0.15113018453121185, 0.7017513513565063, -0.29977554082870483, -0.09340263158082962, 0.18977871537208557, 0.47616443037986755, 0.6512452363967896, -0.7890011668205261, -0.2792075574398041, -0.130878284573555, 0.5310302376747131, 0.23382778465747833, 0.5475035905838013, 0.5434907078742981, -0.08647371828556061, 0.6851564645767212, -0.02343589812517166, 0.64774489402771, -0.7010990381240845, -0.01918392814695835, -0.20619012415409088, -0.5349645614624023, -0.447978675365448, -0.2796887457370758, -0.7491223216056824, -0.318429559469223, -0.05579527094960213, -0.07513130456209183, -0.0488591305911541, 0.3195812404155731, 1.0188015699386597, 0.6249957084655762, 0.7355284690856934, 0.010864345356822014, -0.341769814491272, -0.22512409090995789, -1.0013147592544556, -0.18749520182609558, -0.6371570229530334, -0.18043343722820282, -0.3292824327945709, -0.6071017384529114, -0.5090686678886414]}, "authors": [{"authorId": "31521267", "name": "Md Shamim Hussain"}, {"authorId": "1693515", "name": "Mohammed J. Zaki"}, {"authorId": "1762215", "name": "D. Subramanian"}], "references": [{"paperId": "1900de2b966ca55ee5ca24ec94d5debe66e80c5b", "title": "Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "b92898a28bfad42a053726c2707cc05686cd332a", "title": "GRPE: Relative Positional Encoding for Graph Transformer"}, {"paperId": "2d82ee05b132d4681c3bd517afc17d608fe6e525", "title": "Simple Local Attentions Remain Competitive for Long-Context Tasks"}, {"paperId": "ec34748a74c84f67edde7cc763922fa6d4486022", "title": "Transformer Acceleration with Dynamic Sparse Attention"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "fd753314bfa805a0c831cc0a693a1de2defbc387", "title": "Global Self-Attention as a Replacement for Graph Convolution"}, {"paperId": "a9c214e846188adb645021cd7b1964b8ea1fef6f", "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41", "title": "Do Transformers Really Perform Bad for Graph Representation?"}, {"paperId": "9389af659f14239319186dff1cef49e8ece742c8", "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "91ac65431b2dc46919e1673fde67671c29446812", "title": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "5c27f7106d2f0c95fc51fb98ece672e00185ffa1", "title": "Scheduled DropHead: A Regularization Method for Transformer Models"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "910aea4a020c329afb8b8a948abaafdf9ebcab3f", "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "b8989afff14fb630ca58b6afa917fb42574228ee", "title": "Averaging Weights Leads to Wider Optima and Better Generalization"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "b134d0911e2e13ac169ffa5f478a39e6ef77869a", "title": "Snapshot Ensembles: Train 1, get M for free"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "title": "Efficient softmax approximation for GPUs"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca", "title": "Regularization of Neural Networks using DropConnect"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning"}, {"paperId": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"}, {"paperId": "257dc8ae2a8353bb2e86c1b7186e7d989fb433d3", "title": "Neural Network Ensembles"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "2ff74d426e712522030057624510c03713fa77ba", "title": "Linear Transformers Are Secretly Fast Weight Memory Systems"}, {"paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e", "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training"}, {"paperId": "dd6de9423afcc0f821fee2bd0363a4091c7f8cd3", "title": "Distribution Augmentation for Generative Modeling"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles"}, {"paperId": null, "title": "KDD \u201923, August 6\u201310, 2023,"}, {"paperId": null, "title": "For image generation on CIFAR-10, we use a 16-layer transformer with similar architectural and hyperparameter settings as in the"}, {"paperId": null, "title": "For WikiText-103 we use a value of \ud835\udf0e = 0 . 2 in the first layer and linearly increase it to \ud835\udf0e = 0 . 35 in the deepest layer"}, {"paperId": null, "title": "We tune the SSA parameter \ud835\udf0e (in Eq. 3) in different layers for the best validation set results"}, {"paperId": null, "title": "All datasets used in this work are publicly available. The dataset sources are listed in Table 6. Code: The code is"}]}