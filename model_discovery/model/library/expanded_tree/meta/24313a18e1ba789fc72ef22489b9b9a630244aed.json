{"paperId": "24313a18e1ba789fc72ef22489b9b9a630244aed", "abstract": "Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be context-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that CAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A Context-Adaptive Positional Encoding (CAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors is proposed, which enhances model performances in terms of trained length and length generalization."}, "embedding": {"model": "specter_v2", "vector": [0.0832110196352005, 0.31261980533599854, -0.7183836698532104, -0.5935803651809692, -0.3759971857070923, -0.44149065017700195, 0.3752799928188324, 0.14746150374412537, -0.8039342761039734, 0.05504407733678818, 0.7820510268211365, -0.13098978996276855, 0.17411468923091888, -0.005263666156679392, -0.3482649028301239, -0.43703997135162354, -0.9358377456665039, 0.10708535462617874, 0.12067164480686188, 0.014128636568784714, 0.37458789348602295, -0.36900243163108826, -0.8358762264251709, 0.23032870888710022, 0.20445875823497772, 0.5519527792930603, 0.2601451277732849, 0.7542716264724731, -0.6927591562271118, 0.18831123411655426, 0.2823365330696106, -0.35351529717445374, 0.36160847544670105, -0.012808199971914291, -0.3568500876426697, -0.6774978041648865, 0.2900361716747284, -0.4214673638343811, -0.47240501642227173, 0.6545997858047485, -0.32886481285095215, 0.27757441997528076, 0.18804480135440826, -0.5214712619781494, -0.4358324706554413, 1.6211439371109009, 0.8401037454605103, 0.666326105594635, -0.14602114260196686, -0.6817666888237, 1.4824391603469849, -1.1592662334442139, 0.15961594879627228, 0.8760111927986145, 0.8227887153625488, 0.33633244037628174, 0.1480095088481903, -0.33246102929115295, 0.3031318485736847, 0.43790653347969055, -0.6754463911056519, -0.16671893000602722, 0.18230102956295013, -0.2230704426765442, 1.3114707469940186, -0.07785657048225403, 0.09499741345643997, 0.5795119404792786, -0.2570338547229767, 1.220391035079956, 0.16056770086288452, -0.9726274609565735, -0.10271706432104111, -0.23033902049064636, 0.45171716809272766, 0.4516664445400238, -0.7265934944152832, 0.39601534605026245, -1.0117764472961426, 0.019271794706583023, 0.4905441701412201, -0.26401883363723755, 0.4891628324985504, 0.19507922232151031, -0.5056067109107971, 0.14388364553451538, 0.17464885115623474, 0.7195786833763123, -0.2757274806499481, 0.5650142431259155, 0.6363910436630249, 0.19204294681549072, -0.06542377918958664, 0.3812634348869324, 0.008628091774880886, -0.20882394909858704, -1.1124554872512817, 0.3600226044654846, -0.40484723448753357, 1.0420091152191162, -0.18071283400058746, 0.2498929500579834, -0.8639597296714783, 0.12022905796766281, 1.257709264755249, -0.2713727355003357, 0.31332704424858093, -1.0616109371185303, 0.37570565938949585, -0.6581284999847412, 0.39269500970840454, -0.15998752415180206, 0.012664979323744774, -0.19149382412433624, -0.2806490957736969, -1.1621068716049194, 0.023165903985500336, 0.1390872746706009, -0.42242544889450073, 0.6960356831550598, -0.4015188217163086, 0.547755241394043, -0.06319219619035721, 0.13758881390094757, 0.18359263241291046, 0.5570685863494873, 0.20743805170059204, -0.05608655512332916, 0.602524995803833, -0.8508379459381104, -0.733293890953064, -0.7743690013885498, 0.8131148219108582, -0.4409228563308716, 0.2660377025604248, -0.4692607820034027, -1.3318918943405151, -0.8264642357826233, -1.0334917306900024, 0.1304265856742859, -0.5288758277893066, 0.25078263878822327, 0.765878438949585, 0.13802635669708252, -0.9102924466133118, 1.238982915878296, -0.31925323605537415, 0.022879963740706444, 0.33868494629859924, -0.3039354383945465, 0.45243531465530396, 0.04043055325746536, -1.8393172025680542, 0.10197886824607849, 0.7531355619430542, -0.447235107421875, 0.30308249592781067, -0.781764566898346, -0.9520542621612549, 0.21978487074375153, 0.3433487117290497, -0.03505968302488327, 1.176223635673523, 0.5190528035163879, -1.2664542198181152, 0.3076537549495697, 0.018931878730654716, 0.0041110217571258545, 0.6235347986221313, -0.11015634983778, -0.4546591639518738, -0.2777850925922394, 0.06357257813215256, 0.5615787506103516, -0.07244306802749634, -0.1842120736837387, -0.44139352440834045, 0.35610198974609375, -0.1776687502861023, 0.16035416722297668, -0.15349596738815308, 0.9282020330429077, -0.16415588557720184, -0.3878987729549408, 0.49013105034828186, 0.7742189764976501, -0.0206215251237154, -0.2527749836444855, -0.18284985423088074, -1.0066587924957275, 0.7142212986946106, -0.11612945050001144, 1.2733408212661743, -0.6312122344970703, -1.149954915046692, -0.20289139449596405, -0.47741684317588806, -0.1794128268957138, -0.49393707513809204, 0.680851399898529, 0.05924128741025925, 0.4663658142089844, -0.1229831725358963, -0.764145016670227, 0.12153086066246033, 0.04947247728705406, -0.4704079329967499, -0.30542075634002686, -0.43735557794570923, 1.1092000007629395, -1.1653900146484375, -0.05052405595779419, 0.1777327060699463, 0.1114334985613823, -0.6442030072212219, 1.1042423248291016, -0.40973207354545593, 0.21553504467010498, 0.295636385679245, -0.27406045794487, -0.10701579600572586, -0.11902061104774475, 0.06103866174817085, -0.09109805524349213, -0.42422425746917725, 0.9573296308517456, -0.6890338659286499, 1.484093189239502, -0.07495131343603134, 0.4760684370994568, -0.20536164939403534, 0.009981115348637104, 0.557032585144043, 0.4181773364543915, 0.2507995069026947, -0.33506420254707336, 0.3387020230293274, 0.2345440685749054, -0.643140435218811, 0.04146223142743111, 1.0764819383621216, 1.1510653495788574, -0.5390449166297913, 0.3324456512928009, 0.08458183705806732, 0.24917499721050262, 0.7481805682182312, 0.47622382640838623, 0.21105925738811493, 0.48754045367240906, 0.3660227060317993, -0.01666485331952572, 0.11322197318077087, -0.960067093372345, 0.1081668883562088, 0.6482384204864502, 0.44073113799095154, 0.9097129702568054, 0.3796725273132324, -0.5310928225517273, -1.0245555639266968, 0.18563078343868256, 0.34175965189933777, 1.7266722917556763, 0.05308150500059128, -0.46942299604415894, -0.9375964403152466, -0.24039410054683685, -0.3071521818637848, 0.04469126835465431, -0.4388698935508728, -0.036084141582250595, -0.8622974753379822, -0.46001771092414856, 1.3516391515731812, 0.9609566330909729, 1.0079234838485718, -0.749325692653656, -0.47574958205223083, 0.008108999580144882, 0.06065728887915611, -0.536102831363678, -0.7272953987121582, 0.25519508123397827, -0.39589473605155945, 0.03335167467594147, 0.23915158212184906, -0.15160587430000305, -0.21067719161510468, -0.8241397142410278, 0.5424529910087585, -0.4544066786766052, 0.13142727315425873, -0.17266881465911865, 0.14800430834293365, -0.5805162787437439, -0.9700616002082825, 0.5466196537017822, -0.13541236519813538, -0.4503248929977417, 0.5888054370880127, 0.5692163705825806, -0.2574259042739868, 0.11540199816226959, -0.3662821352481842, 0.054800089448690414, 0.08985647559165955, 0.018032997846603394, 0.8528026938438416, -0.3836999833583832, 0.19134129583835602, -1.0608329772949219, 0.9163336753845215, 0.41277191042900085, -0.3398483693599701, 0.36672890186309814, -0.9563865065574646, -0.31428658962249756, 0.4460448622703552, -0.3629247546195984, -0.34913989901542664, -1.0416042804718018, -0.09278009086847305, -0.08600839227437973, -0.5858734846115112, 0.39439114928245544, 0.1390448659658432, 0.3574484884738922, 0.2634793519973755, 0.5534489750862122, 0.16465207934379578, -0.43117424845695496, 0.676359236240387, -0.2843650281429291, 0.8973436951637268, -0.07314002513885498, -0.45953062176704407, -0.47959384322166443, -0.14504697918891907, -0.4516053795814514, -0.3848547041416168, -0.5372616648674011, -0.9401191473007202, -0.29091501235961914, -0.07293632626533508, -0.12985004484653473, -0.7974201440811157, 0.13909727334976196, -0.9479502439498901, -0.4498291015625, -0.13352347910404205, -0.11320406943559647, -0.30291685461997986, -0.7806236743927002, -1.4803105592727661, -0.3198775053024292, -0.41864895820617676, -0.9239381551742554, -0.05537531524896622, -0.23704221844673157, -0.28339627385139465, -0.48515981435775757, -0.27802732586860657, -0.4981575608253479, 0.934985339641571, -0.4911787509918213, 0.6819645762443542, -0.29020094871520996, 0.009234273806214333, -0.4922218322753906, 0.4274123013019562, 0.588282585144043, -0.09393016993999481, 0.453777939081192, -0.6366226673126221, 0.3726755380630493, 0.019840657711029053, -0.11968585848808289, 0.051375433802604675, 0.13848057389259338, 1.1484047174453735, -0.4809027314186096, -0.2018468677997589, 0.3220297694206238, 1.03437340259552, -0.42013275623321533, 0.2725861966609955, 0.4288957417011261, 0.9349122643470764, 0.28966227173805237, 0.1460408866405487, 0.8999882340431213, -0.022255048155784607, 0.570290207862854, 0.16303151845932007, 0.30132627487182617, -0.1696675717830658, -0.546576738357544, 0.043796829879283905, 1.7319388389587402, -0.1325281411409378, -0.09148260205984116, -0.9485905170440674, 0.4755006432533264, -1.1494274139404297, -1.3771755695343018, 0.5998547673225403, 0.8329276442527771, 0.6940946578979492, -0.32365164160728455, -0.0774165689945221, 0.3388602137565613, 0.5459548234939575, 0.7359500527381897, -0.07418782263994217, -0.5648059844970703, -0.027471274137496948, -0.08993744105100632, 0.45986801385879517, 0.715242862701416, -0.42235642671585083, 0.5440567135810852, 15.050657272338867, 0.9484674334526062, -0.24421098828315735, 0.3028554916381836, 0.5473986268043518, 0.3940020203590393, -0.18224941194057465, -0.3509576618671417, -1.4993776082992554, 0.6029709577560425, 1.3124786615371704, -0.06685884296894073, 0.3383226692676544, 0.24406154453754425, 0.22404080629348755, 0.26354047656059265, -0.5653402209281921, 0.3455301523208618, 0.32756859064102173, -1.6435261964797974, 0.026879094541072845, -0.05329718440771103, 0.40210211277008057, 0.19163864850997925, 0.8664053678512573, 0.2585015892982483, 0.07304280251264572, -0.3334227204322815, 1.1274755001068115, 0.14131519198417664, 1.03978431224823, -0.36513277888298035, 0.7089131474494934, 0.28549879789352417, -1.024396538734436, 0.049722108989953995, -0.7733929753303528, -1.1576467752456665, 0.39627549052238464, 0.030268020927906036, -0.9516199231147766, -0.3080560564994812, -0.3782903254032135, 1.0462204217910767, 0.11270472407341003, 0.7014266848564148, -0.5448629856109619, 1.091868281364441, 0.5408568382263184, 0.014386076480150223, 0.7386032938957214, 0.3526442050933838, 0.15603184700012207, 0.2612779438495636, -0.016016939654946327, 0.04307432100176811, -0.005924389697611332, 0.13646291196346283, -0.5187607407569885, 0.07852446287870407, -0.14774170517921448, -0.038403384387493134, 0.6605274081230164, 0.563209056854248, 0.25659286975860596, 0.21194055676460266, -0.07070163637399673, -0.03371339291334152, 0.5143780708312988, 0.07058604061603546, -0.4748007655143738, -0.5326564311981201, 0.37591636180877686, -0.23346024751663208, -0.17164313793182373, 0.3913114666938782, -0.8731213212013245, -0.05886776000261307, -0.7731336355209351, 0.01263202354311943, 0.2227189689874649, -0.521989643573761, -0.5244230031967163, 0.9219255447387695, -0.05630900710821152, -0.6690266728401184, 0.1117594763636589, -0.7156392335891724, -0.14684683084487915, 0.36253082752227783, -1.0014516115188599, -0.5101954936981201, 0.4026238024234772, -0.5021429657936096, -0.33489054441452026, 0.3875083923339844, 0.8927825689315796, 0.2514643371105194, -0.031730469316244125, 0.43842142820358276, 0.655449628829956, 0.3538081645965576, 0.1713002324104309, -1.1787601709365845, 0.5498340129852295, 0.38430535793304443, -0.16863511502742767, 0.6289693117141724, -0.08007165789604187, 0.1875007003545761, 0.07153075188398361, -0.3802807927131653, 0.9385005235671997, -0.7535735964775085, -0.37539300322532654, -0.9170790314674377, -1.2744200229644775, 0.5492857694625854, 0.6361965537071228, -0.4493248462677002, 0.5891878604888916, 0.036390140652656555, -0.3611801266670227, -0.12280578166246414, -0.7542542815208435, -0.006214823108166456, 0.7064837217330933, -0.5791036486625671, -0.5630553960800171, -0.26872026920318604, 0.6340251564979553, -1.4308193922042847, -1.1313722133636475, -0.3526983857154846, 0.28657180070877075, -0.3475506901741028, 1.1796295642852783, -0.23675842583179474, 0.8871099352836609, 1.0054718255996704, -0.04585082456469536, -0.6712081432342529, -0.23177668452262878, -0.5794622898101807, 0.2151951789855957, 0.3965124487876892, 0.845867395401001, -0.199594646692276, 0.6146863698959351, 0.8486337065696716, -0.395672470331192, -0.7851086258888245, -0.4599039554595947, -0.5903118252754211, 0.25303134322166443, -0.5784400105476379, 1.0438839197158813, -0.019428608939051628, 0.15061020851135254, -0.2101859599351883, 0.2709793448448181, 0.5196962356567383, -0.42553195357322693, -0.6263192296028137, -0.3470935821533203, 0.2241324633359909, 0.1211620569229126, -0.8491524457931519, -0.5457094311714172, -1.559261441230774, -0.04918985813856125, -1.1412696838378906, 0.4572047293186188, -0.8655650019645691, -0.6189898252487183, -0.3252856731414795, -0.7074736952781677, 0.14436857402324677, 0.19631078839302063, -0.3619149327278137, -0.6334885358810425, -0.24248294532299042, -0.7043471336364746, 0.6075271368026733, 0.8715608716011047, -0.544475257396698, 0.1918892115354538, 0.1254374235868454, 0.5341273546218872, 0.14910836517810822, 0.7500338554382324, -0.47460687160491943, -0.6041563749313354, -1.364626407623291, 0.37567538022994995, -0.1681414544582367, -0.32666051387786865, -0.5597553253173828, 0.8479950428009033, 0.05756407976150513, -0.210352823138237, -0.47974395751953125, 0.2780916094779968, -0.6594929099082947, -0.45377638936042786, 0.2046506106853485, -1.431616187095642, 0.24790842831134796, -0.38166239857673645, -0.6285613775253296, -0.3045433461666107, 0.1494818925857544, -0.11441884934902191, -0.8987135887145996, -0.8524729013442993, 0.4714234173297882, -0.48659437894821167, 0.13420678675174713, -0.2114820033311844, -0.26961565017700195, -1.1606444120407104, -0.022845333442091942, -0.14760936796665192, 0.24601684510707855, -0.03759375214576721, 0.7059324383735657, 0.49403834342956543, -1.3653626441955566, 0.17599762976169586, 0.3627568483352661, 0.05760083720088005, -0.12510284781455994, 0.3309422731399536, 0.325016587972641, -0.5923942923545837, 0.7025598287582397, -0.011570440605282784, 0.39276424050331116, -1.1640607118606567, -0.1861090064048767, 0.24445146322250366, -0.18304362893104553, -0.1936470866203308, 1.1623886823654175, -1.3037947416305542, -0.7838969826698303, 0.49871084094047546, -1.251196026802063, -0.645315945148468, -0.35297903418540955, 0.8147642612457275, 0.8457990884780884, -0.10451478511095047, -0.2047307789325714, -0.3879219591617584, -0.03680342063307762, -0.1755490005016327, -0.587913990020752, 0.3775654435157776, -0.3320560157299042, -0.35804086923599243, 0.6817951202392578, 0.4578864872455597, -0.4529181718826294, -1.0981650352478027, -0.5309218168258667, -0.008861738257110119, -0.7078142762184143, -0.2253246158361435, -0.3266037404537201, -0.2799832820892334, 0.7941535711288452, 0.11745216697454453, 0.5696970820426941, -0.21408767998218536, -0.06987597793340683, 0.40661701560020447, 0.49005842208862305, 0.16223296523094177, -0.7759774923324585, -0.47565287351608276, 1.2808417081832886, 0.9943689107894897, -0.5593632459640503, 0.16870902478694916, 0.059299957007169724, -0.5435389280319214, 1.1125651597976685, 0.322308748960495, 0.2571941912174225, 0.4734647870063782, -0.12434373050928116, 0.2829047739505768, 0.4250887334346771, -0.6701111793518066, 0.3476576805114746, 0.31349366903305054, 1.0326879024505615, 0.8287861943244934, 0.4142039120197296, -0.17066366970539093, 0.8632172346115112, -0.18721503019332886, -0.02170233614742756, 0.6707672476768494, 0.7363790273666382, -0.6845808029174805, -0.6417979598045349, 0.1066131591796875, 0.6829250454902649, -0.688559353351593, -0.684990644454956, 0.14671190083026886, 0.4567311108112335, 0.41570061445236206, 0.6031203269958496, 0.768562376499176, -0.09091729670763016, 0.16626569628715515, 0.5449773669242859, 0.328123539686203, -0.5014995336532593, -0.5383243560791016, -0.4318492114543915, -0.7859997153282166, -0.01642192341387272, -0.15386854112148285, -0.8192722797393799, -0.7015618681907654, -0.5835522413253784, 0.4875776469707489, 0.4795691967010498, 0.27758294343948364, 0.7958135008811951, 0.49437785148620605, 0.32377752661705017, -0.3016338050365448, -0.31805989146232605, -0.3818157911300659, -1.0260515213012695, -0.06482936441898346, -0.28655052185058594, -0.48511964082717896, 0.14056271314620972, 0.12060147523880005, -0.1274014711380005]}, "authors": [{"authorId": "2294163935", "name": "Chuanyang Zheng"}, {"authorId": "2302856496", "name": "Yihang Gao"}, {"authorId": "2285182555", "name": "Han Shi"}, {"authorId": "2162225343", "name": "Minbin Huang"}, {"authorId": "2302794180", "name": "Jingyao Li"}, {"authorId": "2302818457", "name": "Jing Xiong"}, {"authorId": "153457264", "name": "Xiaozhe Ren"}, {"authorId": "2302795642", "name": "Michael Ng"}, {"authorId": "2302886882", "name": "Xin Jiang"}, {"authorId": "2302853377", "name": "Zhenguo Li"}, {"authorId": "2249753981", "name": "Yu Li"}], "references": [{"paperId": "98372f2e164a4ae44c390a72a39bd6d7675cae89", "title": "xLSTM: Extended Long Short-Term Memory"}, {"paperId": "ebc746fe7d2d580912498a98ec76a1e60020c95d", "title": "Long Context Alignment with Short Instructions and Synthesized Positions"}, {"paperId": "37b1ce339678f63315c82841c6824dd739269636", "title": "Length Generalization of Causal Transformers without Position Encoding"}, {"paperId": "7b3c8d1aacf2c3b356d7ee26d6e7fc2a8914bb45", "title": "Fewer Truncations Improve Language Modeling"}, {"paperId": "c79fe572b44b0ad904bd30bdfd78d5d3c591e342", "title": "Naive Bayes-based Context Extension for Large Language Models"}, {"paperId": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6", "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference"}, {"paperId": "bd0788f6908c0b3a5eb576e4f53999954391c9a7", "title": "Can't Remember Details in Long Documents? You Need Some R&R"}, {"paperId": "275b005c33a315ad603f236cd5766efe07ef6a54", "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding"}, {"paperId": "d53fe76bd2795a19ddf52d012917782f6f6f2c1e", "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models"}, {"paperId": "f016f079ee63a0487756f895c1d93ff0110d3ecd", "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models"}, {"paperId": "cf7ab5df804575bad88a9fcf0fbf7707bf500944", "title": "Training-Free Long-Context Scaling of Large Language Models"}, {"paperId": "2330035c7586a0dc0b1f09e9c00106b295acf543", "title": "Long-Context Language Modeling with Parallel Context Encoding"}, {"paperId": "e35074176fdbfae90772cc69a17f95cbd65d18f1", "title": "On the Expressive Power of a Variant of the Looped Transformer"}, {"paperId": "50503f1de00c567dec1ca8b2fa9d81e822bbed5f", "title": "Do Efficient Transformers Really Save Computation?"}, {"paperId": "c9603ec967879c24973b5bd48861df2e5555932e", "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"}, {"paperId": "f288e2238ac8725baa7ca9874bbc3fed1e89a632", "title": "Data Engineering for Scaling Language Models to 128K Context"}, {"paperId": "8f490b938586d8e1b892304dd5209b2295c93ed7", "title": "Transformers Can Achieve Length Generalization But Not Robustly"}, {"paperId": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d", "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory"}, {"paperId": "1c143a753d4eb82da5526132149f6e4ed5271b63", "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens"}, {"paperId": "9d932de1d2f51067b6481745f28a2db345293d48", "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation"}, {"paperId": "5215a3cfd67fdc6eb0201822dd0004bd4b830f91", "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation"}, {"paperId": "33230a9d2e4d0ef9c923813a07107b2e3bc56605", "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models"}, {"paperId": "7294c426b8a95975ca932eaf8f700acdd3d950b2", "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models"}, {"paperId": "ee802ccb7fc3a322b824310ae6f29fc6a1e4314b", "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"}, {"paperId": "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b", "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"}, {"paperId": "963c34859da99cd4296925d3ca6b1b58f65d4057", "title": "Structured Packing in LLM Training Improves Long Context Utilization"}, {"paperId": "6cea8d0fa51ad72f4f07b154cbf8c2c7214294cd", "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks"}, {"paperId": "801935217ef90357cf4a9220de9d46aafd819190", "title": "BAL: Balancing Diversity and Novelty for Active Learning"}, {"paperId": "8cf15664e0200c27900365e7b62e0f7377d3e96c", "title": "A Survey of Reasoning with Foundation Models"}, {"paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60", "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "889617f6dc6fe2de4f0fb004c3c827db1b4c771a", "title": "Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation"}, {"paperId": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0", "title": "CLEX: Continuous Length Extrapolation for Large Language Models"}, {"paperId": "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b", "title": "What Algorithms can Transformers Learn? A Study in Length Generalization"}, {"paperId": "d395c771f6537259610497ba218cce5b9bfc2c50", "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries"}, {"paperId": "539fadfb615ef84c240f4741061c44eeda540091", "title": "Scaling Laws of RoPE-based Extrapolation"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "73290ecbec2f38d1d647ddef1ada69cee41725b3", "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"}, {"paperId": null, "title": "CoCA: Fusing position embedding with Collinear Constrained Attention for fine-tuning free context window extending"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "2f0203386f3dcbffb47c9f7fe2d19d373d9dda2f", "title": "Exploring Transformer Extrapolation"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "af385c0fdd0eda2bbf429bea6fedffc327c8a180", "title": "Randomized Positional Encodings Boost Length Generalization of Transformers"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "261549439aebdda72b648ecc462448fd24857ac1", "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models"}, {"paperId": "db0c03a5adff4d16ce501e6fb2f37717e42ea093", "title": "TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "0efa4d128dd140a2d3ad36b9f452fc3b80223667", "title": "Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b", "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "64522a5b3476e9f201f6a5b3e312ef0005c562f1", "title": "SHAPE: Shifted Absolute Position Embedding for Transformers"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f", "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "e8984c6e6c24aab26c332728a5fff616dfb3adbb", "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "990504fc9e5a1f3619ace4fa7f5bf667069018b1", "title": "Original Contribution: Multilayer feedforward networks with a nonpolynomial activation function can approximate any function"}, {"paperId": "6e785a402a60353e6e22d6883d3998940dcaea96", "title": "Three models for the description of language"}, {"paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0", "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers"}, {"paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf", "title": "On Position Embeddings in BERT"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning"}]}