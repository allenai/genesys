{"paperId": "85a3a45f13c35ca460132db0f4ba713cb955ed1b", "abstract": "Training large AI models such as deep learning recommendation systems and foundation language (or multi-modal) models costs massive GPUs and computing time. The high training cost has become only affordable to big tech companies, meanwhile also causing increasing concerns about the environmental impact. This paper presents CoMERA, a Computing- and Memory-Efficient training method via Rank-Adaptive tensor optimization. CoMERA achieves end-to-end rank-adaptive tensor-compressed training via a multi-objective optimization formulation, and improves the training to provide both a high compression ratio and excellent accuracy in the training process. Our optimized numerical computation (e.g., optimized tensorized embedding and tensor-vector contractions) and GPU implementation eliminate part of the run-time overhead in the tensorized training on GPU. This leads to, for the first time, $2-3\\times$ speedup per training epoch compared with standard training. CoMERA also outperforms the recent GaLore in terms of both memory and computing efficiency. Specifically, CoMERA is $2\\times$ faster per training epoch and $9\\times$ more memory-efficient than GaLore on a tested six-encoder transformer with single-batch training. With further HPC optimization, CoMERA may significantly reduce the training cost of large language models.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "CoMERA achieves end-to-end rank-adaptive tensor-compressed training via a multi-objective optimization formulation, and improves the training to provide both a high compression ratio and excellent accuracy in the training process."}, "embedding": {"model": "specter_v2", "vector": [0.06706452369689941, -0.06945354491472244, -0.21760232746601105, -0.194305419921875, 0.010537096299231052, 0.2039155215024948, 0.6632227301597595, -0.5502963066101074, -0.6317757368087769, -0.2211228460073471, 0.9694216251373291, -0.027087580412626266, 0.2196333259344101, 0.20042692124843597, -0.16712696850299835, -0.19179317355155945, -1.071835994720459, 0.4304634928703308, -0.49128833413124084, -0.3676506280899048, 0.12939801812171936, -0.09844868630170822, -1.3904664516448975, 0.3111582100391388, 0.3009157180786133, 0.8818907141685486, -0.26289352774620056, 1.1238679885864258, -0.45863816142082214, 0.3853173851966858, 1.0421106815338135, -0.2481091022491455, 0.5594505071640015, 0.0766420066356659, -0.543437123298645, -0.022133708000183105, 0.1674996316432953, -0.8904451727867126, -0.42376118898391724, 0.6932472586631775, -0.7261672616004944, 0.34552374482154846, 0.6895543932914734, -0.48585018515586853, -0.12518255412578583, 0.33207079768180847, 0.7011204957962036, 0.8047087788581848, -0.3570861220359802, -0.44466811418533325, 1.4423190355300903, -1.7303730249404907, 0.056485168635845184, 0.6490069627761841, 0.279465913772583, 0.48315832018852234, -0.11846469342708588, -0.3366749882698059, 0.38926202058792114, -0.11888747662305832, -0.6236249804496765, -0.07265844941139221, 0.12270954251289368, -0.7894343733787537, 1.5078825950622559, -0.17263466119766235, 0.23370879888534546, 0.40455514192581177, -0.029542062431573868, 1.3003144264221191, 0.2587750256061554, -0.6782938241958618, 7.41322073736228e-05, -0.11173395067453384, 0.35981428623199463, 0.7452632188796997, -0.2802220582962036, 0.556094229221344, -1.0621633529663086, -0.13301874697208405, 0.303988516330719, 0.4297831058502197, 0.4474841058254242, -0.061874713748693466, -0.7885274291038513, 0.7167646884918213, 0.43552035093307495, 0.8392618298530579, -0.6704047918319702, 0.9973767399787903, 0.7505151629447937, 0.062129758298397064, 0.14748568832874298, 0.3513982594013214, 0.10160033404827118, 0.04156390577554703, -0.8011770844459534, 0.42288169264793396, 0.07260462641716003, 0.38156554102897644, -0.18499279022216797, 0.45503735542297363, -0.34429997205734253, -0.11991819739341736, 1.1537123918533325, 0.1645391285419464, 0.43829333782196045, -0.702751100063324, 0.08470503240823746, -0.6484911441802979, 0.07755257189273834, -0.984582245349884, -0.19256243109703064, -0.46236735582351685, -1.4135477542877197, -0.9725682139396667, -0.7431306838989258, 0.13123628497123718, -0.4919127821922302, 0.007606958970427513, -0.3788827955722809, 0.43772247433662415, 0.1754189431667328, 1.1162711381912231, 0.17792227864265442, 0.8315520882606506, -0.17326471209526062, 0.1696503460407257, 0.9375182390213013, -1.4145437479019165, -0.7737874984741211, -1.202023983001709, 0.6171146035194397, -0.026789501309394836, 0.28897419571876526, 0.14538155496120453, -1.1975563764572144, -0.6930108666419983, -0.9345137476921082, 0.013233502395451069, -0.3150181472301483, 0.46915435791015625, 1.7357295751571655, -0.08882767707109451, -0.5945099592208862, 0.576942503452301, -0.48909834027290344, 0.32962730526924133, 0.12074967473745346, 0.3001309335231781, 0.2822635769844055, -0.49776744842529297, -1.1366114616394043, 0.26381078362464905, 0.10987894237041473, -0.29617905616760254, -0.4000833034515381, -0.3594733476638794, -0.5396876335144043, -0.009541483595967293, -0.03373146057128906, -0.42003726959228516, 1.060074806213379, 0.042252667248249054, -1.4503178596496582, 0.2941301763057709, -0.07373152673244476, 0.08845604211091995, 0.16792820394039154, -0.46950316429138184, -0.5211507678031921, -0.2183336615562439, -0.19047318398952484, 0.1854209005832672, 0.5943337082862854, -0.0022540614008903503, 0.10639362037181854, 0.06120745465159416, -0.3682991564273834, 0.1813572645187378, -0.6023343205451965, 0.8570176959037781, -0.5643664002418518, -0.2760394215583801, 0.12669230997562408, 0.3590344786643982, -0.6480836868286133, 0.004868373274803162, -0.10011966526508331, -0.6188914775848389, 0.5085362792015076, 0.09304870665073395, 1.1665045022964478, -1.1316269636154175, -0.00837169960141182, 0.037810638546943665, 0.3393210172653198, -0.17326754331588745, -0.25370800495147705, 0.4018767476081848, -0.28158479928970337, -0.0440242700278759, -0.20301105082035065, -0.8945283889770508, -0.0980830118060112, 0.1124003604054451, -0.9367040395736694, -0.2834196984767914, 0.08500806987285614, 0.7574118971824646, -0.7377230525016785, 0.1792798936367035, -0.3487420678138733, 0.34786397218704224, -1.3757578134536743, 1.2798064947128296, -0.44650596380233765, 0.35691720247268677, -0.36036160588264465, -0.5620468258857727, -0.11576184630393982, -0.7522131204605103, 0.3475634455680847, -0.5553229451179504, -0.29245176911354065, 0.41292333602905273, -0.7910913825035095, 1.3707225322723389, -0.1848852038383484, 0.5195503234863281, -0.07519754022359848, -0.552611768245697, 0.2463970184326172, 0.15066349506378174, 0.21127642691135406, -0.12652716040611267, 0.35990098118782043, 0.8360397815704346, -0.6655638813972473, 0.1443800926208496, 0.9254883527755737, 0.7221410870552063, -0.4190399646759033, 0.3985913097858429, 0.4137364327907562, -0.2425350397825241, 0.5577824115753174, 0.6729807257652283, 0.32064831256866455, 0.257698655128479, -0.058277029544115067, -0.12767918407917023, 0.34581395983695984, -0.7340005040168762, -0.33108118176460266, 0.24997958540916443, 0.5834053158760071, 0.6417918801307678, 0.16150829195976257, -1.2690149545669556, -0.7241686582565308, 0.5579853057861328, 0.6540380120277405, 1.5202200412750244, -0.3415718376636505, -0.362947016954422, -0.39140865206718445, -0.16010962426662445, -0.0348198339343071, -0.23829081654548645, -0.30047115683555603, -0.07480329275131226, -0.37690186500549316, -1.1150068044662476, 0.7495226860046387, 0.3009885251522064, 1.078145146369934, -0.040078818798065186, -0.21915142238140106, -0.5867947340011597, 0.302497923374176, -0.8031740784645081, -0.796355128288269, 0.18509510159492493, -0.5856413245201111, 0.45599040389060974, -0.17731761932373047, 0.278969407081604, 0.2115529179573059, -0.5128512978553772, 0.559043824672699, -0.20659443736076355, 0.17162823677062988, 0.25687700510025024, 0.7077426910400391, -0.2846422791481018, -0.4177204370498657, 0.06543917208909988, 0.6844589710235596, 0.39652666449546814, 0.27690860629081726, 0.3978763222694397, 0.0937381312251091, -0.010376322083175182, -0.48686525225639343, 0.7470124959945679, 0.047721993178129196, 0.18134446442127228, 0.6717878580093384, -0.645095705986023, -0.49807384610176086, -0.9365595579147339, 0.8726094365119934, 0.10959769785404205, -0.4918661415576935, -0.230470210313797, -0.9066396355628967, 0.0009408995392732322, 0.5142617225646973, -0.7310658097267151, -0.10931488126516342, -0.8934294581413269, 0.15785127878189087, -0.30480098724365234, -0.13287058472633362, 0.6873547434806824, 0.8985771536827087, -0.09285376220941544, 0.6449730396270752, 0.14553475379943848, 0.37351861596107483, -0.32695379853248596, 0.47591346502304077, -0.4920395016670227, 0.5505030751228333, -0.011084766127169132, 0.4326809048652649, 0.0928187146782875, 0.17900483310222626, -0.27115368843078613, -0.9244430661201477, -0.4398941993713379, -0.33828720450401306, -0.7111889719963074, 0.475636750459671, -0.8062875270843506, -0.6223965883255005, -0.1086808443069458, -0.8491553664207458, 0.03596946969628334, 0.3672674596309662, 0.2704223096370697, -0.04813866317272186, -1.0933948755264282, -1.5596855878829956, -0.39612722396850586, -1.209652304649353, -1.2524173259735107, 0.42292508482933044, -0.17056532204151154, 0.3489612638950348, -0.4911152124404907, -0.30682140588760376, -0.0660495012998581, 1.2631593942642212, -0.5357373356819153, 0.4055798649787903, 0.08975405991077423, -0.3156510591506958, -0.24492312967777252, -0.049881063401699066, 0.6390212178230286, -0.9759233593940735, 0.2733525335788727, -0.37692129611968994, 0.19843925535678864, -0.5690137147903442, -0.22447516024112701, 0.042840149253606796, -0.1365119367837906, 0.7459549307823181, -0.1577736735343933, -0.2615223824977875, 0.5363696813583374, 1.229422688484192, -0.9988839626312256, -0.03681941330432892, -0.11753560602664948, 1.3349533081054688, -0.28039583563804626, -0.3051592707633972, 0.8849267959594727, 0.1412435621023178, 0.5136759281158447, 0.2308117002248764, -0.30934908986091614, 0.043912023305892944, -0.39114391803741455, 0.5985996127128601, 2.5122697353363037, -0.16704408824443817, 0.5681993365287781, -1.25678288936615, 0.3827207684516907, -0.8103868365287781, -0.6386926174163818, 0.3518368601799011, 0.5403210520744324, 0.12126440554857254, -0.3312206268310547, -0.015521307475864887, -0.16912312805652618, 0.04902137815952301, 0.2437746673822403, -0.5397392511367798, -0.7939947247505188, 0.08499714732170105, 0.15706537663936615, 0.18164590001106262, 0.4469563066959381, -0.13655036687850952, 0.20502158999443054, 14.892084121704102, 0.5227102637290955, -0.3042190968990326, 0.7305075526237488, 0.8605676293373108, 0.26510950922966003, -0.36891740560531616, -0.1528744250535965, -1.0101996660232544, -0.22038951516151428, 1.4832230806350708, 0.20133338868618011, 0.8728067874908447, 0.6251002550125122, -0.3764970898628235, 0.6945679783821106, -0.26846250891685486, 0.7849091291427612, 0.5854468941688538, -1.7418930530548096, 0.16578854620456696, 0.6393147110939026, 0.5499352216720581, 1.0786340236663818, 0.7679038643836975, 0.8236939311027527, 0.1883586347103119, -0.25308823585510254, 0.27544647455215454, 0.4370356500148773, 1.1507734060287476, 0.04013613983988762, 0.47769802808761597, 0.3958871364593506, -0.8651902675628662, -0.2151166796684265, -0.4129822850227356, -1.661405086517334, 0.12448842823505402, 0.3372017741203308, -0.29528576135635376, -0.2984703779220581, -0.08570010960102081, 1.3125447034835815, 0.40699368715286255, 0.07260337471961975, 0.33275046944618225, 0.9128641486167908, -0.23898281157016754, -0.11203490942716599, 0.5039876699447632, -0.08321335911750793, 0.23279213905334473, -0.0967731922864914, 0.011601247824728489, 0.07201670855283737, 0.2974872887134552, 0.3341026306152344, -0.33908358216285706, -0.05085635185241699, -0.43743857741355896, -0.5698347687721252, -0.5749420523643494, 1.004679560661316, 0.797677218914032, 0.05333143100142479, -0.6375923752784729, 0.2836516499519348, 0.5367313623428345, 0.1425672471523285, -0.1283501386642456, -0.05263731628656387, 0.4377936124801636, -0.4718071520328522, -0.10683966428041458, -0.020179051905870438, -0.32362470030784607, -0.971371054649353, -0.7160779237747192, -0.585345983505249, 0.2682141959667206, -0.7658707499504089, -1.0103929042816162, 0.9871912598609924, -0.22060154378414154, -0.5409116148948669, 0.1580314040184021, -1.1355156898498535, 0.17584742605686188, 0.3435962200164795, -1.0657113790512085, -0.4036318063735962, 0.3893336057662964, -0.3262767195701599, -0.7594748735427856, -0.14604146778583527, 1.2327851057052612, 0.7093703150749207, -0.5041012167930603, -0.24816405773162842, 0.3826643228530884, 0.16609713435173035, -0.8585926294326782, -0.6663738489151001, 0.543846845626831, 0.40517279505729675, -0.09281187504529953, 0.2994427978992462, 0.07774539291858673, 0.414936363697052, -1.4784550666809082, 0.021441679447889328, 0.3350262939929962, -0.5763029456138611, 0.0473535992205143, -0.9690268635749817, -0.7426401376724243, 0.17649567127227783, 0.09705405682325363, 0.19324958324432373, 0.6096354126930237, 0.18602368235588074, -0.6653239130973816, -0.4098523259162903, -0.356087327003479, 0.015306072309613228, 0.317837655544281, -0.8550288677215576, -0.050981104373931885, 0.3779962956905365, 0.46613988280296326, -1.0631853342056274, -0.8967675566673279, -0.32870975136756897, 0.2762313485145569, 0.06793710589408875, 1.0911486148834229, -0.3305136561393738, 1.029343605041504, 0.5668294429779053, -0.32683560252189636, -0.4342513978481293, -0.21268129348754883, -0.8473854064941406, -0.10048925131559372, -0.5746572017669678, 0.3877238929271698, -0.09546062350273132, 0.1874818503856659, 1.1134464740753174, 0.13967281579971313, -0.501315712928772, -0.596898078918457, 0.04884301498532295, -0.1288461685180664, -0.6375190615653992, 0.19518637657165527, 0.02926887944340706, 0.15361621975898743, 0.3272063434123993, -0.20252111554145813, 0.5152926445007324, -0.30021190643310547, -0.5330707430839539, 0.21878132224082947, 0.0570213720202446, -0.20893414318561554, -0.30641162395477295, -0.5274325013160706, -1.5265378952026367, -0.3441944420337677, -1.4241626262664795, -0.26546362042427063, -0.7693483233451843, -0.21351666748523712, 0.05954398214817047, -0.21065588295459747, 0.23057341575622559, 0.42653533816337585, 0.08547510206699371, -0.3638111650943756, -0.05064364895224571, -0.7534809112548828, 0.9297107458114624, 1.1047853231430054, -0.4439120590686798, 0.3062938153743744, -0.29397785663604736, 0.41315025091171265, 0.12243972718715668, 0.19687336683273315, -0.04358774796128273, -0.3737415671348572, -1.2705345153808594, 0.7497255802154541, -0.01716870814561844, -0.1740190088748932, -1.2425620555877686, 0.6227883696556091, 0.17916470766067505, -0.4692511260509491, 0.30100131034851074, 0.3780635893344879, -0.9477458000183105, -0.05204148590564728, 0.6478649377822876, -0.9603776335716248, 0.5905821919441223, 0.10900980234146118, -0.5746774673461914, -0.20763103663921356, 0.7428791522979736, -0.2787809669971466, -0.604243814945221, -0.45385435223579407, 0.5545843839645386, -0.6575433015823364, -0.156379833817482, -0.5902668237686157, 0.21357402205467224, -0.8886771202087402, -0.2802475094795227, 0.15036185085773468, 0.04588785395026207, -0.4827515780925751, 0.6578824520111084, 0.4877443313598633, -1.5243310928344727, -0.007352893706411123, 0.43508973717689514, -0.06984926760196686, -0.20964668691158295, 0.4015304148197174, 0.6243661046028137, -0.35262465476989746, 0.3707132935523987, 0.2242199331521988, 0.29276663064956665, -0.40322086215019226, 0.08675440400838852, 0.7910152077674866, -0.6842451691627502, -0.4402531683444977, 0.9264761805534363, -0.5416920185089111, -0.9480463862419128, -0.20579127967357635, -0.9552097916603088, -0.6796246767044067, -0.8657564520835876, 0.7328798174858093, -0.15791425108909607, -0.11534416675567627, -0.2999132573604584, -0.47255051136016846, 0.04270976781845093, -0.23154044151306152, -0.5276082754135132, 0.722251832485199, 0.08879377692937851, -0.6350685358047485, 0.892092227935791, 0.8339471817016602, -0.521412193775177, -0.5361387133598328, -0.8783973455429077, -0.5769751071929932, -0.3612704277038574, 0.774359405040741, -0.44287124276161194, -0.8615831732749939, 0.8091910481452942, 0.7578470706939697, 0.25019559264183044, 0.4986274838447571, -0.39603954553604126, 0.37830066680908203, 0.6858867406845093, -0.09350945055484772, -0.5726945996284485, -0.06012633815407753, 1.4322519302368164, 1.21123206615448, -0.638247549533844, 0.7361579537391663, 0.1508943736553192, -1.1292684078216553, 0.7879899740219116, 0.1646161526441574, -0.19635584950447083, 1.030514121055603, -0.16023725271224976, -0.3057347238063812, -0.11815383285284042, -1.0646250247955322, -0.06773269176483154, 1.3622546195983887, 0.5693718791007996, 0.7173746228218079, 0.4369508624076843, -0.15759779512882233, 0.8566564917564392, 0.31823834776878357, 0.3107410669326782, 0.6274776458740234, 0.4072452783584595, 0.12817147374153137, 0.21046492457389832, -0.12060479819774628, 0.9692990183830261, -0.6589137315750122, -0.4784018397331238, 0.7948013544082642, 0.12308581173419952, -0.028859088197350502, 0.17962616682052612, 0.796193540096283, -0.2327403724193573, 0.3073202669620514, -0.2546685039997101, -0.017370380461215973, -0.20052939653396606, -0.31246131658554077, -0.10536754131317139, -0.5141235589981079, -0.07065501809120178, -0.3692372143268585, -0.499424546957016, -0.7004474401473999, -0.41354885697364807, 0.30694374442100525, 0.27963173389434814, 0.6132787466049194, 1.1649404764175415, 0.9113194942474365, 0.33746734261512756, -0.31099191308021545, -0.9065401554107666, -0.3785676062107086, -0.7469692230224609, -0.010029761120676994, -0.09524313360452652, -0.3356887996196747, -0.49384617805480957, -0.42399969696998596, -0.39590033888816833]}, "authors": [{"authorId": "2303357991", "name": "Zi Yang"}, {"authorId": "22177625", "name": "Samridhi Choudhary"}, {"authorId": "2302820531", "name": "Xinfeng Xie"}, {"authorId": "2303603805", "name": "Cao Gao"}, {"authorId": "2302806267", "name": "Siegfried Kunzmann"}, {"authorId": "2302789034", "name": "Zheng Zhang"}], "references": [{"paperId": "c1fa6255cc9fc3128f74befc7855e255bc7a2c6e", "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"}, {"paperId": "ec079312f82d98196ec6053f037462956935a73f", "title": "Training Neural Networks from Scratch with Parallel Low-Rank Adapters"}, {"paperId": "dade80f06d5928998ef8e50bd6c2b6081a8c443d", "title": "Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "7bb3927de47d3ab7c33e26d470f7a2e93b8d6ab1", "title": "Towards Compact Neural Networks via End-to-End Training: A Bayesian Tensor Approach with Automatic Rank Determination"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "a6f4917d043494d2ebaebe6b65cb35e6a07fda41", "title": "Importance Estimation for Neural Network Pruning"}, {"paperId": "6e13e111e85d499d781386b182fd855fbb053771", "title": "Deep Learning Recommendation Model for Personalization and Recommendation Systems"}, {"paperId": "9269eca8b9bf244e45b186936f821d05b36d8b2a", "title": "Bayesian Tensorized Neural Networks with Automatic Rank Selection"}, {"paperId": "0b623b9af026cb87a550af22f994d3da184d7753", "title": "Compression and Interpretability of Deep Neural Networks via Tucker Tensor Layer: From First Principles to Tensor Valued Back-Propagation"}, {"paperId": "1ae89db458cb0dae764f42c74d97b262beff4e2a", "title": "Tensorized Embedding Layers for Efficient Model Compression"}, {"paperId": "cd14707ba72d0d13c8bf42bfd9d072122db7c711", "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks"}, {"paperId": "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression"}, {"paperId": "2db62ac8bc735133f746cc10439f419abf3b3a2c", "title": "Tensor-Train Recurrent Neural Networks for Video Classification"}, {"paperId": "1215aa0ba37d67ede562314b1cb01b52aa7ea475", "title": "Compressing recurrent neural network with tensor train"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "5b5415352b9e7e11941339502adc04e9f6c9bd1c", "title": "Effective Quantization Methods for Recurrent Neural Networks"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "4ca3b996d888d7178dbbf9855bb2ab253bdfa43d", "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "e6f2f3a5cc7c7213835b9aede15715b5830520e1", "title": "Tensorizing Neural Networks"}, {"paperId": "efb5032e6199c80f83309fd866b25be9545831fd", "title": "Compressing Neural Networks with the Hashing Trick"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "b7cf49e30355633af2db19f35189410c8515e91f", "title": "Deep Learning with Limited Numerical Precision"}, {"paperId": "62e348e26976c3ef77909b9af9788ebc2509009a", "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition"}, {"paperId": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions"}, {"paperId": "839bad8fab15d0ee73203e36585783b80c6be184", "title": "Tensors: Geometry and Applications"}, {"paperId": "6ff0ab1e9064dba97bb8e5ae0b0f1110b5565e06", "title": "Tensor-Train Decomposition"}, {"paperId": "87e43e9eba01a4eb03436c9946bf6aa031a5d5af", "title": "Tensor Decompositions and Applications"}, {"paperId": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9", "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks"}, {"paperId": null, "title": "AI and compute"}, {"paperId": "dc39536089a5fead271ae6e44617407e7148536b", "title": "Multi-objective Optimization"}, {"paperId": null, "title": "Display advertising challenge"}, {"paperId": null, "title": "ChatGPT and generative AI are booming, but the costs can be extraordinary"}, {"paperId": null, "title": "Low-rank economic tensor-train adaptation for ultra-low-parameter fine-tuning of large language models"}, {"paperId": null, "title": "Update: ChatGPT runs 10K Nvidia training GPUs with potential for thousands more"}]}