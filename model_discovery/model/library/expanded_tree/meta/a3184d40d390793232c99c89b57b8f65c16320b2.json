{"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "abstract": "Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.", "venue": "arXiv.org", "year": 2021, "citationCount": 60, "influentialCitationCount": 18, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters is trained, which is the largest Chinese dense pre-trained model so far and outperforms the state-of-the-art models on 68 NLP datasets."}, "embedding": {"model": "specter_v2", "vector": [0.005578898824751377, 0.8429402112960815, -0.1071949377655983, 0.34908467531204224, -0.41438111662864685, -0.6942662596702576, 0.6205815076828003, -0.27072882652282715, -0.43775060772895813, 0.058948028832674026, 0.5663776397705078, -0.6389228701591492, 0.5744606256484985, -0.1578368842601776, -0.37503552436828613, 0.1042666956782341, -0.6622346639633179, 0.49649670720100403, -0.4554128348827362, -0.461449533700943, -0.42038366198539734, -0.768672525882721, -0.39598822593688965, -0.11862578243017197, 0.850814700126648, 0.2923087179660797, 0.22327008843421936, 0.9140359163284302, -0.31507235765457153, 0.07643475383520126, 0.29552769660949707, -0.412761926651001, 0.40766701102256775, 0.15822724997997284, -0.5079506635665894, -0.09789806604385376, -0.24268658459186554, -0.3878591060638428, -0.4120515286922455, 0.7557662129402161, -0.04912490025162697, 0.26512548327445984, 0.4434368312358856, -0.3989870548248291, -0.8574187159538269, 1.1110438108444214, 0.6696991920471191, 0.6128555536270142, -0.195477694272995, -0.39624926447868347, 1.0665231943130493, -1.2795175313949585, 0.37304362654685974, 1.3863739967346191, 0.25836920738220215, 0.5597667098045349, 0.040272895246744156, -0.8951427936553955, 0.38653364777565, 0.09734859317541122, -0.6766679883003235, 0.1393813043832779, -0.12617400288581848, 0.06869141012430191, 1.731615424156189, -0.7201356291770935, 0.18351486325263977, 0.7067563533782959, -0.32871508598327637, 1.439754605293274, -0.12202345579862595, -0.8672322630882263, -0.666053295135498, 0.24887259304523468, 0.008589530363678932, 1.0343363285064697, -0.3827511966228485, 0.5077031850814819, -0.48747944831848145, 0.06542853266000748, 0.26890280842781067, -0.27322399616241455, -0.08340678364038467, 0.41782113909721375, -0.3665742576122284, 1.0745224952697754, 0.5163136124610901, 0.9133434891700745, -0.0410282202064991, 0.46090859174728394, 0.6118539571762085, 0.3780784606933594, 0.19258235394954681, 0.5715479850769043, -0.3459996283054352, 0.32867977023124695, -0.8522436618804932, 0.22862763702869415, 0.04867033287882805, 0.9473430514335632, -0.1009666919708252, 0.40759333968162537, -0.9031239151954651, -0.3456573188304901, 1.0408321619033813, 0.1726943999528885, 0.6067312955856323, -0.6207980513572693, 0.3599523603916168, -0.7631970643997192, -0.05339496582746506, -0.37733691930770874, -0.18382662534713745, -0.31944048404693604, -0.9706469774246216, -1.4488774538040161, -0.5755442976951599, -0.5504273772239685, -1.1591376066207886, 0.8944117426872253, -0.1393386870622635, 0.2606097459793091, 0.42505595088005066, 0.18367363512516022, 0.6092524528503418, 1.0235458612442017, 0.07242236286401749, 0.08116959035396576, 1.037331461906433, -0.9522097706794739, -0.7533271312713623, -0.7418504357337952, 0.8369545340538025, -0.16697169840335846, 0.04894048348069191, -0.04820564389228821, -1.0209572315216064, -0.7828345894813538, -0.7210655212402344, -0.2501260042190552, -0.6859452724456787, 0.34279578924179077, 0.8885794878005981, 0.825687825679779, -0.6324164271354675, 0.6228947043418884, -0.19129639863967896, -0.07358013838529587, 0.406554251909256, -0.11933211237192154, 0.19053268432617188, -0.4917275607585907, -1.592778205871582, 0.30366581678390503, 0.7829405665397644, -0.5985702872276306, -0.40818899869918823, -0.7231357097625732, -1.1026402711868286, 0.22441132366657257, 0.5123966932296753, -0.6195462942123413, 1.2676444053649902, 0.12079107016324997, -1.3742947578430176, 0.5990890860557556, -0.4387422800064087, -0.17176087200641632, 0.23792994022369385, -0.4999798834323883, -0.37987494468688965, -0.46002480387687683, -0.11966684460639954, 0.4817639887332916, 0.44223448634147644, 0.009695801883935928, 0.1375381499528885, 0.556027889251709, -0.24860893189907074, -0.24635520577430725, -0.3589268922805786, 0.8895254731178284, -0.5470149517059326, -0.4878510534763336, 0.047101665288209915, 0.4699835777282715, -0.23648707568645477, -0.2922825515270233, -0.8825715184211731, -1.0843307971954346, 0.9947963953018188, -0.15211732685565948, 0.6123011708259583, -0.98988276720047, -0.6422176361083984, -0.030627833679318428, -0.13131752610206604, 0.33749493956565857, -0.790815532207489, 0.5281303524971008, -0.5796014666557312, 0.8924236297607422, 0.01667799800634384, -1.1455655097961426, 0.26695242524147034, 0.0003298248630017042, -0.9991999268531799, -0.5786322355270386, 0.3868212103843689, 1.2915235757827759, -0.7611522674560547, 0.14180888235569, -0.017130713909864426, 0.3903076648712158, -1.138954758644104, 0.9842627048492432, -0.8039231896400452, 0.4725252687931061, -0.6231204867362976, -0.2668136656284332, 0.11545529961585999, -0.2818286716938019, 0.09810175001621246, -0.15445807576179504, -0.056548070162534714, 0.3873342275619507, -0.3837279975414276, 1.3979710340499878, -0.22456082701683044, 0.5588716268539429, -0.22309157252311707, -0.8021608591079712, 0.1497846245765686, 0.8079954385757446, -0.4958398938179016, -0.13109219074249268, 0.3839183449745178, 0.3770124912261963, -0.6225144863128662, 0.029394179582595825, 0.7296226620674133, 0.43187281489372253, -0.389117956161499, 0.6505522131919861, 0.47069841623306274, -0.47002461552619934, 0.49931541085243225, 0.7296749949455261, 0.4175948202610016, 0.3632906377315521, 0.45449021458625793, 0.05981988087296486, 0.31841182708740234, -0.6524878144264221, 0.09900498390197754, 0.4514995813369751, 0.7077623009681702, 0.6736849546432495, 0.4457927346229553, -0.7002483010292053, -0.42594677209854126, 0.12276626378297806, 0.8065457344055176, 1.316336989402771, -0.43309348821640015, -0.23808462917804718, -0.7115154266357422, -0.47180432081222534, -0.449186772108078, 0.3385365307331085, -0.3292177617549896, 0.06451870501041412, -0.5706254243850708, -1.1167206764221191, 0.8733265399932861, 0.09266071766614914, 1.2348121404647827, -0.47235623002052307, 0.08120055496692657, -0.3168434202671051, -0.014359739609062672, -1.016485333442688, -0.6970853209495544, 0.16273440420627594, -0.17988505959510803, -0.03331035003066063, -0.061078742146492004, -0.028064439073204994, 0.2732163667678833, -0.7992373108863831, 0.8080894947052002, -0.2673227787017822, -0.44419729709625244, 0.03804754838347435, 0.20836235582828522, -0.8730330467224121, -0.8442789912223816, 0.15470091998577118, 0.22288306057453156, -0.5717921257019043, 0.2355004996061325, 0.6576979160308838, 0.14560167491436005, -0.1321200728416443, -0.5807127356529236, 0.046238526701927185, -0.017482677474617958, 0.20904140174388885, 0.583339512348175, 0.16418468952178955, 0.029933929443359375, -1.3645925521850586, 0.7359091639518738, 0.23852744698524475, -0.5135823488235474, 0.26092931628227234, -0.2801661193370819, -0.1926468461751938, 0.7690909504890442, -0.8290513753890991, -0.5821357369422913, -1.0318512916564941, 0.1162295788526535, -0.11027597635984421, -0.10959700495004654, 0.6300285458564758, 0.2717984914779663, 0.22543570399284363, 0.35901305079460144, 0.6475921869277954, 0.028797615319490433, 0.02174456976354122, 0.8243504762649536, -1.1266461610794067, 0.20693814754486084, 0.00047355619608424604, 0.8034650087356567, -0.21455729007720947, -0.33578234910964966, -0.6135368943214417, -0.8642071485519409, 0.034777186810970306, -0.2053709477186203, 0.12254875153303146, 0.4529360234737396, -0.6705388426780701, -0.7708906531333923, 0.007438439875841141, -1.1506739854812622, -0.29079046845436096, -0.08842917531728745, -0.2788918614387512, -0.018164943903684616, -0.7622897624969482, -1.209923505783081, -0.3948593735694885, -0.8462820649147034, -0.8793888688087463, 0.48214954137802124, 0.3199600279331207, -0.08221984654664993, -0.9114807844161987, 0.09654112905263901, -0.20908334851264954, 1.0374469757080078, -0.44120296835899353, 0.8917021751403809, -0.17846105992794037, -0.4600394070148468, -0.11134210973978043, 0.09101912379264832, 0.6374384164810181, -0.4736282229423523, 0.4408396780490875, -0.6875903010368347, 0.1871320903301239, -0.4107132852077484, -0.8094798922538757, 0.4071175754070282, 0.18195301294326782, 0.5680503249168396, -0.044217467308044434, -0.6275001764297485, 0.5598934292793274, 1.4196009635925293, -0.7166813611984253, 0.253409743309021, 0.09328612685203552, 0.9210788011550903, -0.15634892880916595, -0.34366101026535034, 0.3674115240573883, 0.31974101066589355, -0.018496669828891754, -0.15070116519927979, -0.06836190074682236, 0.17791122198104858, -0.7827703356742859, 0.5358929634094238, 1.3546737432479858, 0.29817619919776917, -0.34411361813545227, -1.2263888120651245, 0.6721553206443787, -0.7483404278755188, -0.4831096827983856, 0.6718823313713074, 0.2871188819408417, 0.65138179063797, -0.3925217390060425, -0.8454128503799438, -0.16069296002388, 0.3739946186542511, 0.10783784836530685, -0.47343939542770386, -0.7277340292930603, -0.03412449732422829, 0.7555667757987976, 0.03629563748836517, 0.46569570899009705, -0.2821725904941559, 1.070906162261963, 14.73363208770752, 1.1988554000854492, 0.04276763275265694, 0.6681156158447266, 0.6980263590812683, 0.4233582019805908, -0.45674192905426025, -0.2389264702796936, -1.2675808668136597, -0.49945852160453796, 0.8494547009468079, -0.15711621940135956, 0.2980649173259735, -0.04507504403591156, 0.0852959007024765, 0.3030656576156616, -0.0994134247303009, 0.2770783007144928, 0.35935524106025696, -1.2391395568847656, 0.6301258206367493, 0.43712612986564636, 0.7165417075157166, 0.7382405400276184, 1.0296937227249146, 1.165473222732544, 0.7157111167907715, -0.5354596376419067, 0.2003546953201294, 0.13846327364444733, 0.7861493825912476, -0.034861970692873, 0.3985253572463989, 0.9942033886909485, -0.7464842796325684, -0.27097198367118835, -0.6094211935997009, -1.0274286270141602, 0.43172121047973633, 0.3998839855194092, -0.3930650055408478, -0.13744594156742096, -0.35091906785964966, 0.719902515411377, 0.459015816450119, 0.15504637360572815, -0.6652858257293701, 0.8595158457756042, -0.4472499489784241, 0.05783143639564514, 0.4221518039703369, 0.44833481311798096, 0.5201207995414734, -0.3312457203865051, 0.5641223788261414, 0.3872942626476288, 0.1821328103542328, 0.8283558487892151, -0.8935193419456482, -0.28782352805137634, -0.23667950928211212, -0.38338205218315125, -0.15941891074180603, 0.8772244453430176, 0.5404844284057617, 0.15863417088985443, -0.17637716233730316, 0.22213345766067505, 0.494111031293869, 0.1958569586277008, -0.27416589856147766, -0.03542556241154671, 0.0027452909853309393, -0.6771324276924133, 0.21750345826148987, 0.2641634941101074, -0.05922350287437439, -0.7953431010246277, -0.8181489706039429, -0.47659701108932495, 0.059640079736709595, -0.7644598484039307, -0.6804746985435486, 1.0287230014801025, -0.6081029772758484, -0.2926936149597168, 0.1328529417514801, -0.8641177415847778, -0.6421110033988953, 0.7154518961906433, -1.6957250833511353, -0.8556007742881775, 0.7363530397415161, -0.1450035721063614, -0.5659326910972595, -0.09672670811414719, 1.5855473279953003, -0.13293425738811493, -0.5642883777618408, 0.01230876799672842, 0.3527677059173584, 0.29816314578056335, -0.29185718297958374, -0.9366912841796875, 1.0012420415878296, 0.4004102051258087, -0.10491211712360382, 0.05485181137919426, 0.0709356814622879, -0.028992395848035812, -0.7075240015983582, -0.2725207209587097, 0.8444433212280273, -1.0805790424346924, -0.3536984920501709, -0.8593441843986511, -0.6564090847969055, 0.43229973316192627, 0.6872453689575195, -0.6408628821372986, 0.41687971353530884, 0.1962776631116867, -0.6053336262702942, 0.2944546043872833, -0.9419716000556946, 0.04270361736416817, 0.12004970014095306, -0.6526809930801392, -0.4168489873409271, 0.3957878351211548, 0.7091991305351257, -1.1098835468292236, -0.6122835278511047, -0.2752934992313385, -0.13121891021728516, 0.15327861905097961, 0.8574067950248718, -0.0379319004714489, 0.7757602334022522, 1.0503888130187988, 0.4446491599082947, -0.9316632747650146, 0.1835206300020218, -1.1855608224868774, 0.17584377527236938, 0.1429937481880188, 0.7801085710525513, -0.4973364770412445, 0.3484930694103241, 1.0207492113113403, 0.3785448968410492, -0.24694359302520752, -0.5836538076400757, -0.2823221981525421, 0.6094664335250854, -0.548943281173706, 0.30772489309310913, 0.08507128804922104, 0.053734470158815384, 0.14059557020664215, 0.29911115765571594, 0.5978648662567139, -0.1863701492547989, -0.7329695224761963, 0.49976491928100586, 0.0028566508553922176, -0.25606924295425415, -0.24150240421295166, -0.0819336324930191, -1.5963822603225708, 0.1061033383011818, -1.2007722854614258, -0.2712422311306, -0.713388204574585, -0.3248918354511261, -0.004577043000608683, 0.2977248728275299, 0.2821184992790222, 0.02242027223110199, -0.24650835990905762, -0.279024600982666, -0.9349481463432312, -0.12913647294044495, 1.0454936027526855, 1.1503256559371948, -0.6382344365119934, 0.05185971036553383, -0.13448113203048706, -0.2363077849149704, 0.3002280294895172, 0.4285667836666107, -0.37995094060897827, -0.6413731575012207, -1.5203245878219604, 0.335757851600647, -0.26629477739334106, -0.08045031130313873, -0.4740397334098816, 0.659570038318634, 0.9021572470664978, -0.19749009609222412, -0.06571917235851288, 0.1564471274614334, -0.7724075317382812, -0.7346234917640686, 0.4762420654296875, -0.4959707260131836, 0.04581243172287941, 0.5600256323814392, -0.4742882251739502, -0.34940916299819946, 0.48178333044052124, -0.2585887014865875, -1.1442513465881348, -0.48416998982429504, 0.6162586808204651, -0.7845516800880432, 0.11779100447893143, -0.22272050380706787, 0.18029169738292694, -1.3851449489593506, -0.5505529046058655, -0.2792662978172302, 0.6370587348937988, -0.8021731376647949, 0.587012529373169, 0.34177806973457336, -0.8860093951225281, -0.3542100787162781, 0.5973553657531738, -0.4235743284225464, 0.29314833879470825, 0.6657242178916931, 0.07006367295980453, -0.33187854290008545, 0.8402067422866821, 0.7223451137542725, 0.621556282043457, -0.6445937156677246, 0.1684996336698532, 0.7640956044197083, -1.1081119775772095, -0.2992037534713745, 1.1489946842193604, -0.39092135429382324, -1.438529372215271, -0.013783262111246586, -1.1239147186279297, -0.5578315258026123, -0.5621366500854492, 0.7081225514411926, -0.19263356924057007, -0.519037127494812, 0.09983540326356888, 0.08260743319988251, 0.3103753924369812, -0.13600409030914307, -0.5112783908843994, 0.5838136672973633, -0.17216241359710693, -0.3659576177597046, 0.09700082242488861, 0.7064090371131897, -0.7029601335525513, -0.3556147813796997, -0.9618334770202637, -0.4429478049278259, 0.21073268353939056, 0.35631194710731506, -0.5710601806640625, -0.5474643707275391, 0.8869819045066833, 0.20052248239517212, 0.379512757062912, 0.36954930424690247, -0.35521116852760315, 0.725625216960907, 0.9232229590415955, 0.05863117799162865, -0.36573538184165955, -0.4228854775428772, 1.7748349905014038, 1.0221503973007202, -1.0784180164337158, 0.2292480170726776, -0.51750248670578, -0.8535692095756531, 0.9303488731384277, 0.07668959349393845, 0.08739984035491943, 1.0850707292556763, -0.43405306339263916, -0.05562906712293625, 0.2645445168018341, -1.0091408491134644, -0.5281504392623901, 0.7755497694015503, 0.8787304759025574, 0.6304646730422974, 0.23238658905029297, 0.20541024208068848, 1.0191714763641357, -0.1357499212026596, 0.16350746154785156, 0.6380552053451538, 0.10291602462530136, -0.08584824204444885, -0.30887559056282043, -0.04006347432732582, 0.33620160818099976, -0.9519020915031433, -0.9325236082077026, 0.18154866993427277, 0.7933098077774048, 0.4195908308029175, 0.728991687297821, 0.502879798412323, 0.3163323998451233, 0.414339542388916, 0.26090821623802185, 0.4047947824001312, -0.6791781783103943, -0.35443058609962463, -0.1327800750732422, -0.7790846824645996, 0.20504197478294373, -0.14653822779655457, -0.2880032956600189, -0.4615984857082367, -0.44628024101257324, 0.11171174794435501, 0.1781572848558426, 0.3288259208202362, 1.347846269607544, 0.5196815133094788, 0.2979607582092285, -0.19643743336200714, -0.36719924211502075, -0.1504385620355606, -1.0114530324935913, -0.1731884926557541, -0.7448983788490295, -0.23807644844055176, 0.1312246471643448, -0.09968677163124084, -0.34459933638572693]}, "authors": [{"authorId": "104463827", "name": "Shuohuan Wang"}, {"authorId": "2117103617", "name": "Yu Sun"}, {"authorId": "2068340960", "name": "Yang Xiang"}, {"authorId": "47039787", "name": "Zhihua Wu"}, {"authorId": "2152123817", "name": "Siyu Ding"}, {"authorId": "2117587198", "name": "Weibao Gong"}, {"authorId": "144588144", "name": "Shi Feng"}, {"authorId": "40861754", "name": "Junyuan Shang"}, {"authorId": "2117889541", "name": "Yanbin Zhao"}, {"authorId": "2054086618", "name": "Chao Pang"}, {"authorId": "2144130913", "name": "Jiaxiang Liu"}, {"authorId": "2109214103", "name": "Xuyi Chen"}, {"authorId": "2140025135", "name": "Yuxiang Lu"}, {"authorId": "2109563578", "name": "Weixin Liu"}, {"authorId": "2108249583", "name": "Xi Wang"}, {"authorId": "2153241479", "name": "Yangfan Bai"}, {"authorId": "2157776938", "name": "Qiuliang Chen"}, {"authorId": "2116548646", "name": "Li Zhao"}, {"authorId": "2155911527", "name": "Shiyong Li"}, {"authorId": "2075416480", "name": "Peng Sun"}, {"authorId": "3046102", "name": "Dianhai Yu"}, {"authorId": "2148985098", "name": "Yanjun Ma"}, {"authorId": "50007795", "name": "Hao Tian"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "2112663486", "name": "Tian Wu"}, {"authorId": "144424265", "name": "Wei Zeng"}, {"authorId": "2154591487", "name": "Ge Li"}, {"authorId": "2153578299", "name": "Wen Gao"}, {"authorId": "144270731", "name": "Haifeng Wang"}], "references": [{"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "355fe8111c250dee86a7d6a5cd818b6f61e41279", "title": "End-to-end Adaptive Distributed Training on PaddlePaddle"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "24e775b20adf21e9b5b95c6a9b7a5c164d055849", "title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"}, {"paperId": "1cb134e23264d0796c08f7187c6af735455e6c9a", "title": "Follow Your Path: a Progressive Method for Knowledge Distillation"}, {"paperId": "04833b92c9002f241b8f8b956d018759eebc85b3", "title": "Tailor: Generating and Perturbing Text with Semantic Controls"}, {"paperId": "4237cbebe788a97174f48dc398082739bbffe95b", "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "70db308d3635bf20bfbc66e177e6744362e4a9cb", "title": "ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression"}, {"paperId": "04b40daa1ca74bdbb578beb314bf662538ecd18e", "title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "9d81bc8bebf1beb936427c224afb219b54a64f1e", "title": "Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right"}, {"paperId": "7599401233a40cc8c51b77d0419d2732c35439b2", "title": "Annealing Knowledge Distillation"}, {"paperId": "1e5b05838e16244310db554b04ff6541f05acb0b", "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "fb5d827bb514fa285723557cc39a8200d31d4d1a", "title": "Robo-writers: the rise and risks of language-generating AI"}, {"paperId": "040ad14a2c97e51510889ae6a0c3c23b29da801d", "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe", "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"}, {"paperId": "b5b006dc558cb7fbd532d67e989173b536e8ac80", "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers"}, {"paperId": "70e11c102a6231049307449a7181127889946606", "title": "Chinese Medical Question Answer Matching Based on Interactive Sentence Representation Learning"}, {"paperId": "2f78c1b9c1773a76a33c6ac169051481ea88ccc8", "title": "Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation"}, {"paperId": "9ffcb3624f2637b5d0fe28c61ec8472293cfebc7", "title": "All the News That\u2019s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "9b975cd0e9cb330300062916c72df3d63e1db207", "title": "CoCon: A Self-Supervised Approach for Controlled Text Generation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d7d5bb7c5424a725d3c2b7d352aa299f0f90a5e5", "title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis"}, {"paperId": "6b16b80ccc3f67d862c218eea353cd429d77f7d1", "title": "A Controllable Model of Grounded Response Generation"}, {"paperId": "b49556626ceb024bcbf095aa85aa7ef147606d3a", "title": "Aspect-Controlled Neural Argument Generation"}, {"paperId": "1cc0b98b938b984e5da85f86c1a24099b9b4b582", "title": "SegaBERT: Pre-training of Segment-aware BERT for Language Understanding"}, {"paperId": "414f232eda907f7fe7eb5b56f0efcde6c78b0d0b", "title": "DuReaderrobust: A Chinese Dataset Towards Evaluating the Robustness of Machine Reading Comprehension Models"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "2081ac22151c1075fcc6533f0935c29d486bfa6f", "title": "A Sentence Cloze Dataset for Chinese Machine Reading Comprehension"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "9e9188927859c3351f6e4b2c07a30b64387de578", "title": "Distill BERT to Traditional Models in Chinese Machine Reading Comprehension (Student Abstract)"}, {"paperId": "d16ab5c19ed33a263b6412ac41a4ea1f068d254a", "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "0accb5cb9d06b4cdc4ceda316bc51c8ba95e6838", "title": "PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8de7f044a673d1f5e3b454d0663811f91aa9811a", "title": "On the Efficacy of Knowledge Distillation"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "e04a80263d252a3d8a382ba37a249b9345620570", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation"}, {"paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97", "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8", "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "04a7021fe6be6bddcfae476493fcc7571e7c613c", "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"}, {"paperId": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "7334f45c06555d4b6bf7e6b4437574c11369697e", "title": "Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge"}, {"paperId": "2ff41a463a374b138bb5a012e5a32bc4beefec20", "title": "Pre-Training with Whole Word Masking for Chinese BERT"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "e278e072774f23675266881750e20bca74804cb9", "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1b07a24b81834116f6ad1d0232485ba81b9445f3", "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"}, {"paperId": "7b0ed3d67375a4542133c992f4e55fd4ade0cd90", "title": "Knowledge Distillation via Route Constrained Optimization"}, {"paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3", "title": "ERNIE: Enhanced Representation through Knowledge Integration"}, {"paperId": "425482094edea7deef11287bc27d73b84ee7c32e", "title": "Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "889ad3c713bd7f1b3a8e9b07e136ec4a88651893", "title": "Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "b0e716728e940eb2164892b7e284940157a2cebe", "title": "Plan-And-Write: Towards Better Automatic Storytelling"}, {"paperId": "be2e66b8b28bfad2cbfa3087176b79ec5ab1ec04", "title": "Character-based BiLSTM-CRF Incorporating POS and Dictionaries for Chinese Opinion Target Extraction"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "549c1a581b61f9ea47afc6f6871845392eaebbc4", "title": "LCQMC:A Large-scale Chinese Question Matching Corpus"}, {"paperId": "2e29be79de2bb255784b65f4ecd59824b8cc21fe", "title": "CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "c997d481606f0346164511cabe74c6d1ef3f6be5", "title": "DRCD: a Chinese Machine Reading Comprehension Dataset"}, {"paperId": "c0fdddc750f58373ad6b1e30660812ef9903b7fe", "title": "Matching Article Pairs with Graphical Decomposition and Convolutions"}, {"paperId": "e2c8726d092aea573e69f5b0a2654225883cfacf", "title": "Horovod: fast and easy distributed deep learning in TensorFlow"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "9589244bbff8c5b5e57f52f99776cda332e6ba48", "title": "A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text"}, {"paperId": "995b7affd684b910d5a1c520c3af00fd20cc39b0", "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications"}, {"paperId": "96bc7e517759afa2972278ef206796154a295c98", "title": "Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "bdf28e3cadbabda3261bd904c37edea66ab84766", "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering"}, {"paperId": "f92272e33b11a0d2f47b5b65446c0f1a913cfd17", "title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "b6a0f30260302a2001da9999096cfdd89bc1f7fb", "title": "The Hungarian method for the assignment problem"}, {"paperId": "45612980cad4d8a7a013b214c310989e26d24948", "title": "Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": "040dc42c38f74a904e1a5e51f35fa6a8c70b4e8c", "title": "NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task-Next Sentence Prediction"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "4ae2960d3c6ac489b3b072666fb0b91d0480a170", "title": "A Span-Extraction Dataset for Chinese Machine Reading Comprehension"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Iflytek: a multiple categories chinese text classifier"}, {"paperId": "7afb83134d5b7914131e10b229d30dc2593266f6", "title": "The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"}, {"paperId": "8ce1512e77fa6a646513a60d78e0081afe870c07", "title": "Dataset for the First Evaluation on Chinese Machine Reading Comprehension"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, the world\u2019s largest and most powerful generative language mode"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": null, "title": "Chnsenticorp"}, {"paperId": null, "title": "PyTorch distributed: Experiences on accelerating data"}]}