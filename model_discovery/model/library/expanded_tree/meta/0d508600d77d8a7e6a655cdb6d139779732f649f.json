{"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "abstract": "The attention module, which is a crucial component in Transformer, cannot scale efficiently to long sequences due to its quadratic complexity. Many works focus on approximating the dot-then-exponentiate softmax function in the original attention, leading to sub-quadratic or even linear-complexity Transformer architectures. However, we show that these methods cannot be applied to more powerful attention modules that go beyond the dot-then-exponentiate style, e.g., Transformers with relative positional encoding (RPE). Since in many state-of-the-art models, relative positional encoding is used as default, designing efficient Transformers that can incorporate RPE is appealing. In this paper, we propose a novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention. Based upon the observation that relative positional encoding forms a Toeplitz matrix, we mathematically show that kernelized attention with RPE can be calculated efficiently using Fast Fourier Transform (FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity. Interestingly, we further demonstrate that properly using relative positional encoding can mitigate the training instability problem of vanilla kernelized attention. On a wide range of tasks, we empirically show that our models can be trained from scratch without any optimization issues. The learned model performs better than many efficient Transformer variants and is faster than standard Transformer in the long-sequence regime.", "venue": "Neural Information Processing Systems", "year": 2021, "citationCount": 43, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention using Fast Fourier Transform (FFT), and demonstrates that properly using relative positional encoding can mitigate the training instability problem of vanilla Kernelized attention."}, "embedding": {"model": "specter_v2", "vector": [0.4775310456752777, 0.9280959367752075, -0.49792638421058655, -0.24472887814044952, -0.523957371711731, 0.04118133336305618, 0.3223465085029602, -0.2696455717086792, -0.5029044151306152, -0.5267825722694397, 0.4498293101787567, 0.2296341210603714, -0.012974626384675503, 0.03632183000445366, -0.326423317193985, -0.11835312843322754, -0.7347063422203064, 0.05037000775337219, 0.2164735049009323, -0.34239283204078674, 0.23409385979175568, -0.7973126173019409, -0.978229284286499, 0.02095046266913414, 0.45044928789138794, 1.1140931844711304, 0.41568151116371155, 0.8172854781150818, -0.1814553588628769, 0.7087525725364685, 0.4577922821044922, -0.43375635147094727, 0.21217627823352814, 0.1774362176656723, -0.561296820640564, -0.4010753333568573, 0.24982771277427673, -0.3910287320613861, -0.6247508525848389, 0.8330003023147583, -0.409657746553421, 0.3630627989768982, 0.48012617230415344, -0.6963058114051819, -0.5072962641716003, 0.9627644419670105, 0.5534614324569702, 0.7879526019096375, -0.6750019788742065, -0.7914021611213684, 1.4669944047927856, -1.2850003242492676, -0.2719886600971222, 1.1337144374847412, 0.5220443606376648, 0.1849285066127777, -0.01783529482781887, -0.31035926938056946, 0.8196213245391846, 0.5745430588722229, -0.7611098289489746, -0.42065849900245667, 0.2165513038635254, -0.09250254929065704, 1.6147023439407349, -0.41420403122901917, 0.10806111991405487, 0.22975046932697296, 0.02916027046740055, 1.3345359563827515, -0.22418279945850372, -0.7661159038543701, -0.3772462010383606, 0.0870286151766777, 0.6872334480285645, 0.6700474619865417, -0.8322073817253113, 0.1333368718624115, -0.9335811734199524, -0.03897349163889885, 0.42007240653038025, -0.07232051342725754, 0.17377452552318573, -0.21551881730556488, -0.3318367004394531, 0.5619317293167114, 0.5536478757858276, 0.44782644510269165, -0.43876761198043823, 1.1958026885986328, 0.534060537815094, 0.21619321405887604, 0.17407606542110443, 0.26154080033302307, -0.20023120939731598, 0.08270949125289917, -0.9660504460334778, -0.013427959755063057, -0.39645737409591675, 0.8945376873016357, -0.06466469913721085, 0.6148300766944885, -0.5006703734397888, 0.04144798964262009, 1.025090217590332, 0.1754665970802307, 0.17572267353534698, -0.5556262731552124, 0.026801904663443565, -0.7084239721298218, 0.012931954115629196, -0.9522632360458374, 0.04330206289887428, -0.2746361494064331, -0.8023963570594788, -1.0204992294311523, -0.44698867201805115, 0.4643321931362152, -0.6839150786399841, 0.6152985692024231, -0.578081488609314, 0.39884498715400696, -0.17164096236228943, 0.3705027401447296, 0.29493477940559387, 0.7551360726356506, 0.18615563213825226, 0.15190865099430084, 1.2009503841400146, -1.1531912088394165, -0.7170765995979309, -0.9569761157035828, 0.3616059422492981, -0.3206236660480499, 0.19714392721652985, -0.1769331842660904, -1.1307919025421143, -1.1137386560440063, -0.8834771513938904, -0.07350926101207733, -0.41907191276550293, 0.5237479209899902, 1.1660377979278564, 0.1449434608221054, -1.1139466762542725, 0.7510260343551636, -0.3632652163505554, 0.15959395468235016, 0.6715356707572937, 0.4348655045032501, 0.42687079310417175, 0.07949284464120865, -1.4162144660949707, 0.5314605832099915, 0.2687281668186188, -0.3150154650211334, -0.025370903313159943, -0.916577935218811, -1.0792760848999023, 0.3109867572784424, 0.3708021938800812, -0.09042945504188538, 1.3252496719360352, -0.23870712518692017, -1.4020185470581055, 0.47542768716812134, -0.39149588346481323, -0.08530320972204208, 0.01603115163743496, -0.363778293132782, -0.3354406952857971, -0.2982542812824249, -0.342698335647583, 0.3675019145011902, 1.037055253982544, 0.36872759461402893, -0.3193488121032715, 0.43502679467201233, -0.3681986629962921, -0.22574809193611145, -0.5369940400123596, 1.1577575206756592, -0.22506923973560333, -0.44989141821861267, 0.1629527062177658, 0.46439653635025024, 0.11851607263088226, -0.3166949152946472, -0.3067121207714081, -1.1186628341674805, 0.716580867767334, 0.0036189458332955837, 1.204714059829712, -1.0025641918182373, -0.5131357312202454, -0.29346707463264465, 0.11282636225223541, -0.11776091903448105, -0.4296410381793976, 0.17122390866279602, -0.7873784303665161, -0.20131519436836243, 0.18587958812713623, -0.6990119814872742, 0.3526977598667145, -0.7139298915863037, -0.8727208971977234, 0.07300540804862976, -0.006409045308828354, 1.351050853729248, -0.7157354354858398, 0.20717427134513855, -0.0005162456654943526, 0.20569194853305817, -0.9297316074371338, 1.399608850479126, 0.028397981077432632, -0.1787382811307907, 0.06946326792240143, -0.3417815864086151, 0.005440667737275362, -0.6813017129898071, 0.05935240536928177, -0.9253694415092468, -0.044461846351623535, 0.5325454473495483, -0.43166375160217285, 1.1395906209945679, -0.21410532295703888, 1.0885940790176392, -0.06977663189172745, -0.9101269245147705, 0.10571110248565674, 0.302697092294693, -0.050259314477443695, -0.4663638770580292, 0.41615408658981323, 0.20206165313720703, -0.7973941564559937, 0.437905877828598, 0.7622801065444946, 1.09765625, -0.5277891755104065, 0.22039851546287537, 0.5766116380691528, 0.20315775275230408, -0.18160654604434967, 0.33673661947250366, 0.36756059527397156, 0.3171176016330719, 0.706335723400116, -0.3090636134147644, 0.09500693529844284, -0.7961425185203552, -0.29942992329597473, 0.4164600074291229, 0.6148242354393005, 0.5792174935340881, 0.3432486355304718, -0.891346275806427, -0.5671529173851013, -0.0014795104507356882, 0.5074727535247803, 1.741791844367981, -0.1315327137708664, -0.18686020374298096, -0.730903148651123, -0.017037907615303993, -0.5409711599349976, -0.15488915145397186, -0.6951725482940674, -0.4335384964942932, -0.6148788332939148, -1.0438698530197144, 0.9169662594795227, 0.612829327583313, 0.9001263976097107, -0.33752140402793884, -0.7386550307273865, -0.13702596724033356, 0.33417198061943054, -0.9517182111740112, -0.9917529821395874, 0.6449500918388367, -0.18251405656337738, -0.06465121358633041, -0.09725279361009598, -0.05867770314216614, 0.14196865260601044, -0.6652193665504456, 0.47983506321907043, -1.1041752099990845, -0.42880958318710327, 0.2581099271774292, 0.5251131653785706, -0.7626742124557495, -0.11820400506258011, 0.6092821359634399, 0.07062191516160965, 0.16548213362693787, 0.32865238189697266, 0.3452371656894684, -0.03314989060163498, 0.09273692220449448, -0.030254719778895378, 0.32759881019592285, 0.28455570340156555, 0.015999889001250267, 0.39148056507110596, -0.43176546692848206, 0.11144698411226273, -0.9517684578895569, 0.7649027705192566, 0.5686267614364624, -0.5757068395614624, 0.061932556331157684, -0.5495985746383667, -0.23679912090301514, 0.3282855451107025, -0.8515279293060303, -0.059266623109579086, -0.6658356189727783, 0.20037388801574707, -0.4777701795101166, -0.00697691272944212, 0.14359815418720245, 0.5033174157142639, -0.19342947006225586, 0.3542678654193878, 0.5261568427085876, 0.23943036794662476, 0.0638563334941864, 0.51481032371521, -0.9198690056800842, 0.6280943155288696, 0.33074939250946045, 0.35613274574279785, -0.2960844933986664, -0.2800813317298889, -0.7552834153175354, -0.6082900762557983, -0.4105391800403595, -0.29212304949760437, -0.31343692541122437, 0.3725525438785553, -0.5144309401512146, -1.215772271156311, 0.24552021920681, -1.0603567361831665, -0.3811063766479492, -0.059241149574518204, 0.05481981113553047, -0.44098272919654846, -1.2741162776947021, -1.2569364309310913, -0.5619925260543823, -0.7799847722053528, -0.9872449636459351, 0.19230428338050842, 0.038823917508125305, -0.40997618436813354, -0.44043976068496704, -0.4362451434135437, -0.5554790496826172, 1.6385622024536133, -0.7127323746681213, 1.0079712867736816, -0.07052022218704224, -0.45961520075798035, -0.11081023514270782, 0.028822090476751328, 0.554829478263855, -0.20494364202022552, 0.20997971296310425, -0.9761996865272522, 0.3467801809310913, -0.45761099457740784, -0.12427029758691788, 0.18854887783527374, 0.608137309551239, 0.7817835807800293, -0.06721574068069458, -0.33377009630203247, 0.4551490247249603, 1.1984132528305054, -0.3242136240005493, 0.2667481303215027, 0.3270875811576843, 1.2173477411270142, -0.0812540203332901, -0.07140450924634933, 0.6966155767440796, 0.4346117079257965, 0.7223522067070007, 0.27434661984443665, -0.1734016388654709, 0.09559141844511032, -0.148361474275589, 0.3981270492076874, 1.6201870441436768, 0.10670909285545349, 0.4861122667789459, -0.8244210481643677, 0.6817643642425537, -1.2125946283340454, -1.007422685623169, 0.8223869800567627, 0.6260207295417786, 0.08335661143064499, -0.3956177830696106, -0.2860601544380188, -0.16208048164844513, 0.4957757294178009, 0.49872371554374695, -0.29642102122306824, -0.7305763363838196, 0.24396154284477234, 0.548418402671814, 0.15092366933822632, 0.5908910036087036, -0.8095084428787231, 0.8663877844810486, 14.966655731201172, 0.9450086355209351, -0.2787832021713257, 0.6990790963172913, 0.46870964765548706, 0.41126659512519836, 0.02186429686844349, -0.18856267631053925, -1.160506010055542, -0.019038040190935135, 1.1480345726013184, -0.23088382184505463, 0.5480586886405945, 0.5477585196495056, -0.2861025333404541, 0.4273794889450073, -0.6242508888244629, 0.9258356094360352, 0.3558705151081085, -1.4946074485778809, -0.10512586683034897, 0.18614663183689117, -0.03347688540816307, 0.1972411870956421, 1.0633162260055542, 0.7099594473838806, 0.19296914339065552, -0.5309370160102844, 0.6581618189811707, 0.36469629406929016, 1.05868661403656, -0.3885294497013092, 0.1763794869184494, 0.19725237786769867, -1.4531813859939575, -0.1684882789850235, -0.6259279847145081, -1.0099403858184814, 0.18877092003822327, 0.24901165068149567, -0.16775035858154297, -0.42762109637260437, -0.07775061577558517, 0.861854612827301, 0.29980525374412537, 0.40699082612991333, -0.1400761753320694, 0.45872586965560913, -0.03606203570961952, 0.0347406379878521, 0.39577457308769226, 0.5685474276542664, -0.01899314671754837, -0.09471146017313004, 0.06486769765615463, 0.05624190717935562, -0.12455885112285614, 0.5428259372711182, -0.13516752421855927, -0.13895481824874878, -0.06283356994390488, -0.3692268431186676, -0.03503740206360817, 0.7891844511032104, 0.6000953316688538, 0.29472383856773376, -0.1833696961402893, 0.3667258620262146, 0.7936781644821167, 0.1666315346956253, -0.8982141613960266, -0.5509620308876038, 0.5543806552886963, -0.5094125270843506, 0.18112881481647491, 0.5656701922416687, -0.0873422622680664, -0.4867394268512726, -0.8545005917549133, -0.18833006918430328, 0.29176533222198486, -0.7450958490371704, -0.37346112728118896, 1.0234301090240479, -0.22902382910251617, -0.40500593185424805, 0.5979542136192322, -0.676277756690979, -0.10006620734930038, 0.46895498037338257, -1.277693271636963, -0.6461685299873352, 0.11849121004343033, -0.2892889082431793, -0.2988501489162445, 0.12727245688438416, 1.1282159090042114, 0.45416784286499023, -0.1761510819196701, 0.4087372124195099, -0.316581130027771, 0.11258779466152191, -0.09924987703561783, -0.5936525464057922, 0.9996885657310486, 0.16888310015201569, -0.38842445611953735, 0.22989758849143982, -0.11482958495616913, 0.6293186545372009, -0.6049980521202087, -0.10663041472434998, 0.5817636847496033, -0.8579276204109192, 0.002376916352659464, -0.7248461842536926, -0.8928492069244385, 0.5323120355606079, 0.4353242814540863, -0.1648484766483307, 0.256972074508667, -0.020263368263840675, -0.6048523783683777, -0.39559176564216614, -0.20814579725265503, -0.2528057098388672, 0.02561667189002037, -0.8845371007919312, -0.43526995182037354, -0.2630600333213806, 0.41243577003479004, -1.1005092859268188, -0.4388103783130646, -0.1454547494649887, 0.1776650995016098, 0.08606799691915512, 1.2202280759811401, -0.310137540102005, 0.48954296112060547, 0.8865646123886108, -0.023955604061484337, -0.7201634049415588, -0.44201263785362244, -0.8335066437721252, -0.1480368673801422, 0.010950637049973011, 0.39586275815963745, -0.4591589868068695, 0.46655064821243286, 0.6807311773300171, 0.179216206073761, -0.6187300086021423, -0.5511660575866699, -0.04872780665755272, -0.5980611443519592, -0.7905642986297607, -0.01547722052782774, -0.05887759104371071, 0.022223113104701042, 0.013801706954836845, -0.1975550502538681, 0.4867142140865326, -0.026320938020944595, -0.5687239170074463, 0.1386571228504181, -0.18501757085323334, -0.09713616222143173, -0.7246781587600708, -1.0325210094451904, -1.8182013034820557, 0.07370217889547348, -1.2022194862365723, -0.0050577279180288315, -0.675147294998169, -0.4926767349243164, 0.054186929017305374, -0.31860077381134033, 0.25112706422805786, 0.09251787513494492, -0.26208654046058655, -0.43405240774154663, -0.6904751062393188, -0.5141685605049133, 0.8134056925773621, 0.5995001196861267, -0.6302828788757324, 0.17913886904716492, 0.11832912266254425, 0.0002225666685262695, 0.36217689514160156, 0.2647062838077545, -0.6242427229881287, -0.7842612266540527, -1.0761263370513916, 0.3974660634994507, -0.04288516193628311, -0.07080212235450745, -0.8328161835670471, 1.1107243299484253, 0.4257858693599701, -0.26571646332740784, 0.04179196432232857, 0.565102219581604, -0.9807130098342896, -0.49893730878829956, 0.303027480840683, -0.7644924521446228, 0.2717055082321167, 0.11169637739658356, -0.5837451815605164, -0.2531816065311432, 0.8709560632705688, 0.29518669843673706, -0.6171413660049438, -1.0537872314453125, 0.6568562388420105, -0.6950554251670837, 0.1391020119190216, -0.04759636148810387, -0.35796478390693665, -1.1657639741897583, -0.3937280774116516, -0.08295565098524094, -0.05206448212265968, -0.6270646452903748, 0.6930280327796936, 0.5764532685279846, -1.2758421897888184, 0.24345554411411285, 0.2778942286968231, -0.028098735958337784, -0.279569536447525, 0.673342227935791, 0.7746050953865051, -0.21820203959941864, 0.5535871386528015, 0.04559687525033951, 0.2512514293193817, -0.9743708372116089, 0.24274548888206482, 0.803796112537384, -0.18486158549785614, -0.2924681603908539, 1.29684579372406, -0.2863740622997284, -0.8673396110534668, 0.431174099445343, -1.2911462783813477, -0.5847876667976379, 0.08546225726604462, 0.7640187740325928, 0.2119641900062561, -0.10286938399076462, -0.13278068602085114, -0.7794052362442017, 0.5772829055786133, -0.04904894158244133, -0.21533940732479095, 0.42542022466659546, 0.16519246995449066, -0.5619499087333679, 0.5877360105514526, 0.9733738303184509, -0.8765907287597656, -0.6687610745429993, -0.8296465277671814, -0.39420604705810547, -0.1108916848897934, 0.1668536365032196, 0.16741858422756195, -0.7904390692710876, 1.0048092603683472, 0.597098708152771, 0.29114219546318054, 0.4074751138687134, 0.06977790594100952, -0.15510177612304688, 0.7921221852302551, 0.1441120058298111, -0.43303683400154114, -0.33959564566612244, 1.4805171489715576, 0.8191505074501038, -0.29156389832496643, 0.10253188759088516, -0.6777741312980652, -0.6819335222244263, 0.9987902641296387, 0.33718055486679077, -0.15549908578395844, 0.9551740884780884, 0.02179579623043537, 0.15611813962459564, 0.17410153150558472, -1.0186034440994263, -0.17282328009605408, 1.152990460395813, 0.8854445815086365, 0.3722764551639557, 0.23263296484947205, 0.6678141951560974, 0.942822277545929, -0.0019674019422382116, -0.22953617572784424, 0.30890515446662903, 0.37902992963790894, -0.4862052798271179, 0.3619447648525238, 0.1457301825284958, 0.8117181658744812, -0.94039386510849, -0.6507282853126526, 0.5666502118110657, 0.5063261985778809, -0.17249417304992676, 0.13364532589912415, 0.8025034070014954, -0.08530960977077484, 0.5667256712913513, 0.14657659828662872, 0.647136390209198, -0.28632187843322754, -0.1795235574245453, -0.20194043219089508, -0.8601893782615662, -0.2542724907398224, -0.1068405732512474, -0.07706096768379211, -0.16576999425888062, -0.15593120455741882, 0.06691938638687134, 0.21975396573543549, 0.14901314675807953, 1.0019375085830688, 0.536187469959259, 1.1110670566558838, -0.3644747734069824, -0.8434591293334961, -0.3220447599887848, -0.6558834910392761, -0.015715640038251877, -0.7799049615859985, -0.13545803725719452, -0.1061982661485672, -0.007690420839935541, -0.041835322976112366]}, "authors": [{"authorId": "2108801920", "name": "Shengjie Luo"}, {"authorId": "2119028865", "name": "Shanda Li"}, {"authorId": "123970124", "name": "Tianle Cai"}, {"authorId": "1391126980", "name": "Di He"}, {"authorId": "2067915907", "name": "Dinglan Peng"}, {"authorId": "150311931", "name": "Shuxin Zheng"}, {"authorId": "35286545", "name": "Guolin Ke"}, {"authorId": "24952249", "name": "Liwei Wang"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}], "references": [{"paperId": "94bcd712aed610b8eaeccc57136d65ec988356f2", "title": "Variational Diffusion Models"}, {"paperId": "b1250bb76f81cb120d96336997b83e3fd9174a4a", "title": "Densely connected normalizing flows"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "9cf6f42806a35fd1d410dbc34d8e8df73a29d094", "title": "Maximum Likelihood Training of Score-Based Diffusion Models"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "title": "Fixing Weight Decay Regularization in Adam"}, {"paperId": "e644a409b4a4c6eaedffe27efbc5c76280b34c61", "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "79d1b330f0ef51f63ecb9b291dd5a05de5a858c0", "title": "Toeplitz and Circulant Matrices: A Review"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Mixed precision training, 2018"}, {"paperId": null, "title": "We use five for the top four high-resource tasks, MNLI-m/-mm, QQP, and QNLI, to save the fine-tuning costs. Ten is used for other tasks"}, {"paperId": null, "title": "five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. The pre-training corpora consist of Bookcorpus[51] and English wikipedia"}, {"paperId": null, "title": "The model architecture consists of 6 decoder layers. The number of attention head is set to 8. The hidden dimension is set to 512. The dimension of feed-forward layer is set to 2048"}]}