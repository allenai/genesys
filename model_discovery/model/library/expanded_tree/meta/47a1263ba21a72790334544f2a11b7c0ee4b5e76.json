{"paperId": "47a1263ba21a72790334544f2a11b7c0ee4b5e76", "abstract": "Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.", "venue": "ACM Trans. Multim. Comput. Commun. Appl.", "year": 2023, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2302.00389", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodals tasks and prepares a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning."}, "embedding": {"model": "specter_v2", "vector": [0.29077786207199097, 0.36537495255470276, -0.234479159116745, -0.3753507733345032, -1.0864940881729126, 0.04885207489132881, 0.5446712970733643, -0.09824517369270325, -0.39921048283576965, -0.4611653983592987, 1.0792449712753296, 0.4615585207939148, 0.09435290843248367, 0.10170125961303711, 0.031893759965896606, 0.18900473415851593, -0.7189686894416809, 0.2567537724971771, -0.18074236810207367, -0.8312163352966309, -0.3022935092449188, -0.6786109805107117, -1.4116178750991821, 0.9188641309738159, -0.1342213749885559, 0.764322817325592, 0.14441585540771484, 1.2194503545761108, -0.3609156310558319, 0.6350332498550415, 0.6203105449676514, -0.18943525850772858, -0.14020942151546478, -0.12603648006916046, -0.6181308031082153, 6.331741315079853e-05, 0.4398152232170105, -0.7281133532524109, -0.7215214371681213, 0.294551283121109, -0.011246380396187305, 0.3490528166294098, 0.6385635137557983, -0.6144803166389465, -0.19897548854351044, 0.703563392162323, 0.4358493685722351, 0.21209965646266937, 0.36940714716911316, -0.8463611602783203, 1.6095702648162842, -1.441333293914795, 0.00023771394626237452, 1.8989571332931519, 0.2838289439678192, 0.9736633896827698, -0.055935367941856384, -0.5159434676170349, 0.3877858817577362, 0.14125168323516846, -0.3249225914478302, -0.32394111156463623, 0.12108821421861649, -0.11082520335912704, 1.0211237668991089, -0.29372310638427734, -0.23861771821975708, 1.012713074684143, -0.30897101759910583, 1.5876703262329102, -0.13509516417980194, -0.9185041189193726, -0.4402683973312378, -0.04820866510272026, 0.4999835193157196, 0.8708491921424866, -1.2427020072937012, 0.48405614495277405, -1.131508231163025, -0.037770338356494904, 0.3440971076488495, 0.16521045565605164, -0.41891297698020935, -0.1621117740869522, -0.7507708668708801, 0.9445646405220032, 0.7963759899139404, 0.7108548879623413, -0.1436542123556137, 0.6124259233474731, 0.7690284252166748, 0.4823114275932312, -0.8125985264778137, 0.3048423230648041, 0.17996183037757874, 0.5052017569541931, -0.46848052740097046, -0.03894553706049919, -0.1501358300447464, 0.6858736276626587, -0.23847748339176178, 0.09689036756753922, -0.934380054473877, 0.611338198184967, 1.6593433618545532, -0.2583867609500885, 0.4785749912261963, -0.6394678354263306, 0.4525209069252014, -0.6991108655929565, -0.012621471658349037, -0.3894885182380676, -0.05406630039215088, 0.05403975769877434, -0.47897905111312866, -1.2724249362945557, -0.3150310814380646, 0.2608945667743683, -1.0605452060699463, 0.94278883934021, -0.656209409236908, -0.6233641505241394, 0.4418394863605499, 0.4783107042312622, 1.1339360475540161, 0.791607141494751, 0.5930687785148621, 0.5717816948890686, 1.4093297719955444, -0.8615180253982544, -0.7553972005844116, -0.6825449466705322, 0.43286338448524475, -0.08820359408855438, 0.3201069235801697, -0.07645242661237717, -0.9498827457427979, -1.200286865234375, -0.7278297543525696, -0.2782467007637024, -0.32789942622184753, 0.2632189095020294, 1.0549888610839844, 0.45080119371414185, -1.1133835315704346, 0.29407137632369995, -0.0441318042576313, -0.49949395656585693, -0.08260975033044815, 0.33068612217903137, -0.23647655546665192, -0.664830207824707, -1.085152506828308, 0.24897609651088715, 0.2432015836238861, -0.38530629873275757, -0.515777051448822, 0.09125134348869324, -1.7338526248931885, -0.15934017300605774, -0.34178468585014343, -0.6362268924713135, 1.0645285844802856, -0.33556485176086426, -1.074350357055664, 0.47854676842689514, -0.6874725222587585, 0.5051978230476379, -0.06384871155023575, -0.23707899451255798, -0.9116953015327454, 0.06451793015003204, -0.16759246587753296, 1.4074729681015015, 0.32325980067253113, -0.46782398223876953, -0.22312398254871368, 0.08995996415615082, -0.15248441696166992, 0.25455719232559204, -0.44960373640060425, 1.216666579246521, -0.3158561885356903, 0.08782763034105301, 0.7041007280349731, 1.0901716947555542, 0.24194617569446564, -0.01627201959490776, -0.1612662971019745, -0.794464647769928, 0.8669778108596802, 0.16842621564865112, 0.30786362290382385, -1.2111114263534546, -0.42454931139945984, -0.3007547855377197, 0.030080588534474373, -0.7567318081855774, -1.6237179040908813, 0.9368083477020264, -0.5784382224082947, -0.266427606344223, -0.17613595724105835, -1.3575375080108643, 0.07715301215648651, -0.09675292670726776, -0.7944123148918152, -0.20868781208992004, 0.09114658832550049, 1.1996116638183594, -0.6431390643119812, -0.11036855727434158, -0.040058914572000504, 0.3717774450778961, -0.4859994947910309, 1.4199950695037842, -0.18386098742485046, 0.1659095734357834, 0.24633899331092834, 0.21506769955158234, 0.05436192825436592, -0.23658870160579681, 0.03887224569916725, -0.9399711489677429, 0.2539594769477844, 0.3144247829914093, -0.1443457305431366, 1.8853601217269897, -0.19521939754486084, 0.9164366126060486, -0.35114315152168274, -0.3704379200935364, 0.33390721678733826, 0.7409343123435974, -0.008458144962787628, -0.7200808525085449, 0.6529980301856995, 0.2982576787471771, -0.5537437796592712, -0.09513919055461884, 0.8720934987068176, 0.2746238708496094, -0.45452597737312317, 0.0624823197722435, 0.8592114448547363, 0.03407798334956169, 0.3641258180141449, 0.34748223423957825, 0.4720601737499237, -0.04279892519116402, -0.042351409792900085, 0.6113006472587585, 0.26345542073249817, -1.1966522932052612, -0.5516748428344727, 0.26349812746047974, 0.678806483745575, 1.2730016708374023, 0.3814857304096222, -0.22911278903484344, 0.15209554135799408, -0.44508200883865356, 0.6841844320297241, 1.600422978401184, 0.00011511351476656273, -0.09235578775405884, -0.14295771718025208, -0.3380259871482849, -0.5553010702133179, 0.0011273488635197282, -0.7193114161491394, -0.39663374423980713, 0.04282797873020172, -0.6514956951141357, 0.7478747963905334, 0.832792341709137, 0.7907806038856506, -0.6804008483886719, -0.3026258051395416, -0.557198703289032, -0.19616711139678955, -0.7224162817001343, -0.5202665328979492, 0.16643226146697998, -0.5518525242805481, -0.3641052842140198, -0.40422913432121277, -0.15740880370140076, 0.05443266034126282, -0.693830132484436, 0.794021487236023, -0.9951953887939453, 0.014259041287004948, 0.8347972631454468, 0.2776621878147125, -0.34263503551483154, -0.5215852856636047, -0.15280839800834656, -0.3653533458709717, 0.18537509441375732, 0.47233614325523376, 0.7330648303031921, 0.15677687525749207, 0.16912861168384552, -0.7031936049461365, -0.08032631874084473, 0.21598529815673828, 0.21799777448177338, 0.6195545792579651, -0.6499620079994202, 0.2988361120223999, -0.6455841660499573, 0.909584105014801, -0.2457883507013321, 0.10789018124341965, 0.29264116287231445, 0.19811740517616272, -0.47302380204200745, -0.1623310148715973, -0.5394696593284607, -0.13185001909732819, -0.3482513129711151, 0.14592748880386353, -0.37472277879714966, -0.5180252194404602, 0.3496015965938568, 0.13643386960029602, 0.16338442265987396, 0.25915759801864624, 0.38020405173301697, 0.5157120227813721, 0.19903750717639923, 0.6000820994377136, -0.5569218993186951, 0.5573987364768982, 0.20098918676376343, -0.011051738634705544, 0.09949512779712677, -0.31256234645843506, -0.4091513752937317, -0.5462586879730225, -0.6007307171821594, -0.054873958230018616, -0.6242461204528809, 0.4967384338378906, -0.5866838693618774, -1.073156714439392, -0.4319365322589874, -1.161367416381836, 0.3906663954257965, -0.21837736666202545, 0.06840387731790543, -0.2947239279747009, -1.0563491582870483, -0.7910948395729065, -0.8380901217460632, -0.21116185188293457, -1.0219345092773438, 0.3340059816837311, 0.21946772933006287, -0.35301971435546875, -0.380599707365036, 0.06181686744093895, -0.02211788482964039, 0.6803960800170898, -0.39523088932037354, 0.7400581240653992, 0.03945877403020859, -0.1168648973107338, -0.49424487352371216, -0.19762106239795685, 0.577168881893158, 0.035596929490566254, -0.035237476229667664, -1.1289958953857422, 0.3466850221157074, -0.25525590777397156, -0.7159035801887512, 0.5918674468994141, 0.1639295220375061, 0.17840710282325745, 0.7693912386894226, -0.05229460075497627, 0.17653398215770721, 1.403836727142334, -0.13799592852592468, 0.0038669744972139597, -0.20221282541751862, 1.0089523792266846, 0.8480435013771057, -0.43404704332351685, 0.44997334480285645, 0.7473199367523193, 0.38380229473114014, 0.43458402156829834, -0.28543707728385925, -0.5608158111572266, -0.5973455309867859, 0.6779122948646545, 1.1454814672470093, 0.3544812500476837, -0.029507122933864594, -0.8674221634864807, 0.7955545783042908, -1.3316701650619507, -0.4179297983646393, 0.6589624881744385, 0.35804662108421326, 0.09661272913217545, -1.0984606742858887, 0.03798363730311394, 0.006149969529360533, 0.5490067005157471, 0.4361265003681183, 0.01822088100016117, -0.6320692300796509, -0.17254169285297394, 0.3570781350135803, -0.4124597907066345, 0.4883195459842682, -0.5163620710372925, -0.10418920964002609, 14.298980712890625, 0.23122233152389526, 0.053944386541843414, 0.5247402787208557, 0.240453839302063, 0.294009804725647, -0.8208182454109192, -0.36418774724006653, -0.8682202100753784, -0.5094761848449707, 0.9015231132507324, 1.0466365814208984, 0.4285126030445099, -0.13523277640342712, -0.5334670543670654, 0.07816445082426071, -1.3425557613372803, 1.2285232543945312, 0.8817698955535889, -0.8262198567390442, 0.555324375629425, -0.1037944108247757, 0.10021807253360748, 0.284393310546875, 1.1536239385604858, 0.8393686413764954, -0.009417417459189892, -0.6988919973373413, 0.553334653377533, 0.6271958947181702, 0.7662279605865479, -0.08899011462926865, 0.41270023584365845, 0.08891300112009048, -1.1574299335479736, -0.804121732711792, -0.3162371516227722, -0.5005226731300354, 0.47153860330581665, -0.7353219985961914, -0.20203199982643127, -0.24159365892410278, -0.471878319978714, 0.7951067686080933, 0.12169652432203293, 0.4944605529308319, -0.1889144480228424, 0.1431802213191986, 0.02512475661933422, 0.02916199341416359, 0.3796992599964142, 0.8982324004173279, 0.3389643430709839, -0.18970124423503876, 0.021734651178121567, -0.1699436604976654, -0.08132044970989227, 0.43740272521972656, -0.5946388244628906, -0.10018689930438995, -0.4718486964702606, -0.10549470782279968, -0.3881683647632599, 0.5821982622146606, 0.7295332551002502, 0.8123558163642883, -0.5485458970069885, 0.29590272903442383, 0.25333037972450256, 0.5820052027702332, -0.28774115443229675, -0.19193243980407715, -0.022615032270550728, -0.5215062499046326, 0.0944342091679573, 0.6892990469932556, 0.06402497738599777, -0.3769303262233734, -0.6913406848907471, -0.2196531891822815, 0.9484132528305054, -0.9692242741584778, -1.1263853311538696, 1.243148684501648, -0.21388548612594604, -0.7063212990760803, 0.08694937825202942, -0.8971038460731506, -0.19525502622127533, 0.3944653570652008, -1.5221532583236694, -0.9241780638694763, -0.23353996872901917, 0.3112800121307373, -0.43290066719055176, -0.7148081064224243, 1.2718937397003174, 0.38936954736709595, -0.17333954572677612, 0.2608119547367096, -0.7343004941940308, 0.006700039375573397, -0.04625947028398514, -0.6271789073944092, -0.29326677322387695, -0.014890503138303757, 0.22313252091407776, -0.014071451500058174, -0.19342204928398132, 0.5022132396697998, -0.7002310752868652, 0.17614831030368805, 0.5473232865333557, -0.6067786812782288, -0.6823068857192993, -0.5146991610527039, -0.6951833963394165, 0.23321975767612457, 0.8151329755783081, -0.3091852366924286, 0.6975716948509216, 0.28560006618499756, -1.0301357507705688, -0.22454386949539185, -0.8413861393928528, 0.08368481695652008, -0.04051811248064041, -1.1168187856674194, -0.5980401039123535, -0.10333572328090668, 0.15939632058143616, -0.6444147825241089, -0.1915561407804489, 0.08618802577257156, 0.263703316450119, -0.05063702166080475, 0.9552522897720337, -0.7724193930625916, 0.8044877052307129, 0.5279857516288757, -0.93211430311203, -0.34627795219421387, 0.3243042528629303, -0.47482243180274963, -0.06595534831285477, 0.1523790806531906, 0.7091716527938843, -0.13541240990161896, 0.1307068169116974, 0.7796394228935242, 0.2816515564918518, -0.4126947820186615, -0.5324694514274597, 0.157346710562706, -0.39530041813850403, -0.46020931005477905, 0.18424756824970245, -0.4125867784023285, -0.25671273469924927, 0.4078122675418854, 0.36745303869247437, 0.5486363172531128, -0.40676721930503845, -0.5667190551757812, 0.24825069308280945, -0.05756406858563423, -0.08646351099014282, -0.3626402020454407, -0.3820236027240753, -1.5484097003936768, 0.08540024608373642, -1.2520804405212402, 0.04499930515885353, -1.2979322671890259, -0.25385454297065735, 0.6610681414604187, -0.5011963844299316, 0.2996853291988373, 0.5536003708839417, -0.34234678745269775, -0.10089995712041855, -0.7101134061813354, -0.7863578200340271, 1.159924030303955, 1.158700942993164, -0.7801618576049805, 0.1284220665693283, -0.10990370810031891, -0.20245687663555145, 0.029032889753580093, 0.0978628545999527, -0.3096070885658264, -1.0875433683395386, -1.2933884859085083, 0.20892129838466644, 0.3346937894821167, 0.21547964215278625, -0.9322561025619507, 0.7955056428909302, 0.8799062967300415, 0.009547376073896885, -0.29644230008125305, 1.3026670217514038, -1.206381916999817, -0.55214923620224, 0.11226560920476913, -1.1085747480392456, 0.15884876251220703, 0.32415884733200073, -0.7230974435806274, -1.0253698825836182, 0.5298619866371155, -0.016183778643608093, -0.6026061177253723, -0.9578180909156799, 0.43131324648857117, -0.33348900079727173, -0.18054597079753876, -0.05311598628759384, -0.27250298857688904, -1.0175721645355225, -0.8674284815788269, -0.2746020555496216, 0.41919881105422974, -0.9643355011940002, 0.7178146839141846, 1.434140920639038, -1.5116643905639648, -0.28087282180786133, 0.43439793586730957, 0.6874727010726929, -0.4012327492237091, 0.7687487006187439, 0.35906878113746643, 0.0931425467133522, 0.12027700990438461, -0.13934575021266937, 0.030218735337257385, -0.9005815386772156, -0.41466453671455383, 1.1777836084365845, -0.031321946531534195, 0.30940940976142883, 1.5430036783218384, -0.12658755481243134, -1.3418899774551392, 0.40640395879745483, -0.5624973177909851, -0.9392691254615784, 0.13699044287204742, 0.6970313787460327, -0.05145882070064545, -0.5825017094612122, -0.3708682358264923, -0.27893441915512085, 0.6053307056427002, -0.19286958873271942, -1.1414885520935059, 0.2623714804649353, 0.10000881552696228, -0.3480497896671295, 0.8036304712295532, 1.2958186864852905, -0.8775424957275391, -0.5729620456695557, -0.4891323447227478, -0.37674760818481445, -0.010731014423072338, -0.06390409916639328, -0.49144136905670166, -1.1440162658691406, 1.1423773765563965, 0.8997350335121155, 0.1156117171049118, 0.6071094870567322, 0.5022780895233154, 0.28694021701812744, 0.6980845332145691, -0.5612013936042786, -0.2067325860261917, 0.044370461255311966, 1.4015017747879028, 1.6396126747131348, -1.0313332080841064, -0.4476035237312317, -0.29243481159210205, -0.8468820452690125, 1.2034937143325806, 0.2891995310783386, 0.22185127437114716, 0.8957979083061218, -0.5849788784980774, 0.32787325978279114, 0.04852196201682091, -0.9383895397186279, -0.3863997459411621, 1.2398836612701416, 1.5460928678512573, 0.8595461249351501, 0.22554777562618256, 0.5535168647766113, 0.9037789106369019, 0.03927767276763916, -0.11411361396312714, 0.16579528152942657, 0.08666419982910156, 0.08858150988817215, -0.24891246855258942, 0.09277166426181793, 0.4565732777118683, -0.061333637684583664, -0.3309496343135834, 0.305406779050827, 0.4236707389354706, 0.15117384493350983, 1.0041406154632568, 0.9175961017608643, -0.14308369159698486, 0.8848645091056824, 0.0470731183886528, 0.827168345451355, -0.6922006011009216, -0.2716740667819977, -0.2265816181898117, -0.6270374059677124, -0.18281984329223633, -0.656938910484314, -0.5618653297424316, -0.5966953635215759, 0.5012828707695007, 0.8957878947257996, -0.17929764091968536, 0.3146613836288452, 0.9447935819625854, 0.3656502068042755, 0.6446672677993774, -0.3653390109539032, -0.08800071477890015, -0.1004922091960907, -1.0390764474868774, -0.051497217267751694, -0.588128924369812, 0.23745056986808777, -0.4337427616119385, -0.013728871941566467, -0.557632327079773]}, "authors": [{"authorId": "2065683787", "name": "Muhammad Arslan Manzoor"}, {"authorId": "2129589403", "name": "S. Albarri"}, {"authorId": "2203864747", "name": "Ziting Xian"}, {"authorId": "3451645", "name": "Zaiqiao Meng"}, {"authorId": "1683562", "name": "Preslav Nakov"}, {"authorId": "3279808", "name": "Shangsong Liang"}], "references": [{"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "b287a2765e5bceb732de39dafdf70594dc9cd664", "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"}, {"paperId": "1f2129feb5b1c8ba1b07ad5e7e0bd965fe281ec5", "title": "MS\u00b2-GNN: Exploring GNN-Based Multimodal Fusion Network for Depression Detection"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "effbb421f3e4a9318dd0e0c55805fcec2369d4f9", "title": "Multimodal learning with graphs"}, {"paperId": "2765cef1c00991ec0cdf37c197caf7a30b7c5c45", "title": "Visual Question Answering"}, {"paperId": "68d0fc739612be741460af3bcf9e609bfda113e8", "title": "Multi-Relational Graph Representation Learning with Bayesian Gaussian Process Network"}, {"paperId": "5e6af7a6f10f19e15e109c6d72176412fc0519f5", "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training"}, {"paperId": "66ee488cf3dad5bb83804124367460edddd3c271", "title": "Vision-and-Language Pretrained Models: A Survey"}, {"paperId": "93c1dffe2bae737da8f342fd749aa783df572a14", "title": "XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding"}, {"paperId": "8c57aa39866850d0da58c4e8febe9e117058c2e6", "title": "A Survey of Data Representation for Multi-Modality Event Detection and Evolution"}, {"paperId": "24ed74ed29c057cba8b52fff4edd2c0d7f408716", "title": "VLP: A Survey on Vision-language Pre-training"}, {"paperId": "04248a087a834af24bfe001c9fc9ea28dab63c26", "title": "A Survey of Vision-Language Pre-Trained Models"}, {"paperId": "4c09ac7b09628aa2aad12aea8dd6c2aef6c83aa0", "title": "Revisiting Parameter-Efficient Tuning: Are We Really There Yet?"}, {"paperId": "fa350b1089db1f8ab97bb72287b37ed4748c89cf", "title": "Multi-Modal Knowledge Graph Construction and Application: A Survey"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "8f2bca9d684005675e294b33c26481e36f528cdb", "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "592f3bc0fa50a25020f544860722868ef64dd457", "title": "FMFN: Fine-Grained Multimodal Fusion Networks for Fake News Detection"}, {"paperId": "dc9a76b7cb690e6a95f0f07bb3d4fabb7181b68d", "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction"}, {"paperId": "c8f92e2033630bec76d4e3d3c02b11088e30dda9", "title": "Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text"}, {"paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02", "title": "FLAVA: A Foundational Language And Vision Alignment Model"}, {"paperId": "56c0872ab619f54d1a2c8841e404aac31a7bb0f3", "title": "Multi-source Multimodal Data and Deep Learning for Disaster Response: A Systematic Review"}, {"paperId": "175a7185d2089cd8746d4ddae2ecb13755bfcf48", "title": "Benchmarking Multimodal AutoML for Tabular Data with Text Fields"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "cb8dcaf8e5fe7256577c6bc83e11dd64d8f3ae31", "title": "Towards artificial general intelligence via a multimodal foundation model"}, {"paperId": "b1aa394d8d03cd17ee3d3626cbb98b2e4f6b1933", "title": "How to find a good image-text embedding for remote sensing visual question answering?"}, {"paperId": "de0efc73850606f0c44eefe06854c9c23000f15c", "title": "KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation"}, {"paperId": "fe0969ea1875c4c76bcdb205d818daaee7a84bfc", "title": "MURAL: Multimodal, Multitask Retrieval Across Languages"}, {"paperId": "ec33756aa312a33a5979d4d542da2eeabaa8d1a0", "title": "Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization"}, {"paperId": "238bb889ec2c817646c438fac26ccb3fac86a5a0", "title": "M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining"}, {"paperId": "cc8a0ea78ebd29848d0e4ace0d4a4c5d274ef4f2", "title": "Gaussian Process with Graph Convolutional Kernel for Relational Learning"}, {"paperId": "ed7391f944d413d8d2e67d8d03bc1d4f8e83a327", "title": "Detecting Propaganda Techniques in Memes"}, {"paperId": "1b49598ad829cb93a0847df99a3f96f4fb602f7f", "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers"}, {"paperId": "18982c55e60d3157e07d0fbdaba0bf7df2fab8bd", "title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal Pretraining"}, {"paperId": "4b8f5c922932377dd21d804183c2a870b6d628bc", "title": "Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "af86df6a0af3226a1b4b5eb27c17c9e45367f896", "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning"}, {"paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d", "title": "Multimodal Few-Shot Learning with Frozen Language Models"}, {"paperId": "0415cc9ae16b80623c39a7b3ffeee03a1e9d79d7", "title": "Leveraging Category Information for Single-Frame Visual Sound Source Separation"}, {"paperId": "2a9b33f66ccc3806af58bdab2319559f4f9d2c5e", "title": "A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets"}, {"paperId": "b69fb6e4d4b62b61ae224106ff1094ece2c813b6", "title": "Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "e3575ca62373639152adf84ca0b33c52a4f892ef", "title": "M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training"}, {"paperId": "fee129b617268c770fd92440853001ed8ac3641f", "title": "Multimodal sentimental analysis for social media applications: A comprehensive review"}, {"paperId": "981995fd64611f475179b280f4e9c241051ac185", "title": "Knowledge Inheritance for Pre-trained Language Models"}, {"paperId": "99bd323be33f950d13df3569ca223ccffb308df1", "title": "Self-Supervised Multimodal Opinion Summarization"}, {"paperId": "f38a55900ec5c3bc54db5359ddf6d6a55204dfb7", "title": "Predicting the Survival of Cancer Patients With Multimodal Graph Neural Network"}, {"paperId": "0f800c305279093ad4401eb030dde5c977e9a370", "title": "Recent Advances and Trends in Multimodal Deep Learning: A Review"}, {"paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"}, {"paperId": "420c897bc67e6f438db522d919d925df1a10aa8c", "title": "AMMU: A survey of transformer-based biomedical pretrained language models"}, {"paperId": "889c9c37634766b3543424ac6955811f83f260e0", "title": "Compressing Visual-linguistic Model via Knowledge Distillation"}, {"paperId": "70a79ded7818ba8ae807102b00643e331e344ee8", "title": "UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training"}, {"paperId": "7f2209573e95406dbd739d20f9a95f603583194d", "title": "Multi-modal neural machine translation with deep semantic interactions"}, {"paperId": "809b231e915c35db47cb81abfd8600f4c0f9fa10", "title": "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain"}, {"paperId": "f59572c43c1d145349a1d4a06a7ff2155159ce12", "title": "Multi-modal generative adversarial networks for traffic event detection in smart cities"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "a87bb9e70d1127b6a9c0e721e48c0b58c997c70e", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "a76d9ba0f2f10a0968c01f647d577a863da3a221", "title": "Market strategies used by processed food manufacturers to increase and consolidate their power: a systematic review and document analysis"}, {"paperId": "d2893b533aff2d3d0b4aa40716ffa7262e528f4d", "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements"}, {"paperId": "151b2f167b957a071fa3694dd58fce880373a288", "title": "Learning Robust Patient Representations from Multi-modal Electronic Health Records: A Supervised Deep Learning Approach"}, {"paperId": "d42f77ee45e267cd0aa66103bbec8abf7f6d4767", "title": "LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding"}, {"paperId": "7217b5d8d0fb753532026cc36b0aaa056960c6f8", "title": "Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network"}, {"paperId": "7912b8bb86a6d32ed355651d05ff0cbf37e9504e", "title": "KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning"}, {"paperId": "8dca64d7cc92dc1f899306772fc0d5f46ed0bc39", "title": "DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization"}, {"paperId": "03fa50a1b62db4bc5d98bea6ca5bcc1a611af51a", "title": "SWAFN: Sentimental Words Aware Fusion Network for Multimodal Sentiment Analysis"}, {"paperId": "70b0c85638d195dbde56cbedc94ae4363b272b58", "title": "A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"}, {"paperId": "60dd8f53b98d875428d31fa89ac6db8b161ba5b0", "title": "An Improved Attention for Visual Question Answering"}, {"paperId": "e993f052fc1a860011e34e7043af9b3b1a2c8897", "title": "Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos"}, {"paperId": "9927a15ddf5313d97c98f0111fd191caf507ce72", "title": "HateBERT: Retraining BERT for Abusive Language Detection in English"}, {"paperId": "dedcdc1fb3a6def9772dce674d89150923dd75b9", "title": "Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "553093238c2e2ccdb76d8b55045aa35258db8e1c", "title": "An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation"}, {"paperId": "9d8398d9c36d25ad1363d9a5dd922dbbfa7be3be", "title": "Referring Expression Comprehension: A Survey of Methods and Datasets"}, {"paperId": "4ceff7472c04ee6d76bce89d61ba4b445d8dbf74", "title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"}, {"paperId": "02eaaf87f9cae34cca398fed146079e6eeb1f868", "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"}, {"paperId": "dae130ba6f363dc8f83e67cd09896af983f7b7d2", "title": "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation"}, {"paperId": "bc996a4dbf9d4234eacdd0b930a94de1d158e256", "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "3e86f5a0e2a97894de1cf1f1587799ac79bad0f2", "title": "VirTex: Learning Visual Representations from Textual Annotations"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "ba684a9966995e5a8c6efef46aeb57bd387ff51f", "title": "Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis"}, {"paperId": "e4432c316b1766067047bbca228180dd73f8fc89", "title": "Oversampling effect in pretraining for bidirectional encoder representations from transformers (BERT) to localize medical BERT and enhance biomedical BERT."}, {"paperId": "5546e6073f3b82967b12c87d6b90ba722c4b85c6", "title": "Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d", "title": "Experience Grounds Language"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "598a2ee223e2949c3b28389e922c1892b4717d2a", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"}, {"paperId": "c9918cdf3c578fe18ea121b60b4b1961586e904f", "title": "Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text"}, {"paperId": "b9779ddeb6a8a9de0f7e104d8742728aa14578d6", "title": "InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining"}, {"paperId": "4c7c70760029e813ef76ea5a578174d2d3ec1490", "title": "A Survey on Deep Learning for Multimodal Data Fusion"}, {"paperId": "81bbe5e2c8c82294a4ff5e60de543ae79a22a194", "title": "Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition"}, {"paperId": "9915315f5cae822e98c94382ce3b0a6f9a7f8e5e", "title": "12-in-1: Multi-Task Vision and Language Representation Learning"}, {"paperId": "4e2fab8c96e202d0aac3f68721e6d3f1f1f11bfd", "title": "Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks"}, {"paperId": "a4f0556a6225135c4525c5fda2fcab00c91609a3", "title": "Recurrent Neural Network Transducer for Audio-Visual Speech Recognition"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a12d22ff91ce159a0d3558ed5aaed115115beabd", "title": "Fine-Grained Analysis of Propaganda in News Article"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "4267112ddb9959252cf25bb4b7692858434393a7", "title": "Representation Learning for Electronic Health Records"}, {"paperId": "dfba89013c4ca7d90d86ac91fe98586756256440", "title": "LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading"}, {"paperId": "d00cbb0c05c1dc922126fe72c1078b773d01c688", "title": "ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction"}, {"paperId": "2cd27f7a7498e812c4859c15e0371c8b7ec019e2", "title": "Proppy: Organizing the news based on their propagandistic content"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "ad732e16296b62ba1de9a22bd01452590b52a2fc", "title": "VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f8a48678094adbe421d61d0045361bfc635a2900", "title": "Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods"}, {"paperId": "e17330208d553f184ab91e1fc63fa18a087d9d8a", "title": "Video-Driven Speech Reconstruction using Generative Adversarial Networks"}, {"paperId": "37cc2c54cc60ee1301baca2d95bf003c76dd07d5", "title": "Multimodal Abstractive Summarization for How2 Videos"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "58c793e278cdbf669a615b2c2479cd69ff785d63", "title": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents"}, {"paperId": "c192c7d1d94e7a64de7e18e2f2fdffbf2909fcff", "title": "Deep Multimodal Representation Learning: A Survey"}, {"paperId": "2f42ec0d6fe1172122333b36337f09ff5d3d400b", "title": "MVAE: Multimodal Variational Autoencoder for Fake News Detection"}, {"paperId": "2dc698077cb178286c737484dcf67c5ab19314d0", "title": "Language-Conditioned Graph Networks for Relational Reasoning"}, {"paperId": "68a2eb5890eb67989df5fb42b929d10e5e8e5a47", "title": "Multi-Target Embodied Question Answering"}, {"paperId": "908c6b1577a1f5309ae183daf2e24363039f22a8", "title": "Good News, Everyone! Context Driven Entity-Aware Captioning for News Images"}, {"paperId": "9e475a514f54665478aac6038c262e5a6bac5e64", "title": "nuScenes: A Multimodal Dataset for Autonomous Driving"}, {"paperId": "4f30434e86a54a214c56e76106f801a1ad740b1f", "title": "Probing the Need for Visual Context in Multimodal Machine Translation"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "e9b13731027418ed38103d1dfc8a70f6881bc684", "title": "Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "f56cb5dc32b5b280546998418fda7769d0858629", "title": "How2: A Large-scale Dataset for Multimodal Language Understanding"}, {"paperId": "fcecc4ef2c32dbedda61648febb39a0f905c367e", "title": "Deep Audio-Visual Speech Recognition"}, {"paperId": "a46174aa635759070984ed7062c9402695bce830", "title": "LRS3-TED: a large-scale dataset for visual speech recognition"}, {"paperId": "5c107e9d3e09bc33c416cd214d3760ba8c32a470", "title": "EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection"}, {"paperId": "006fdeff6e1a81c404317ee4056d6cc72f9c0e50", "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "04187519dc8c468f2b5b17442413ada7830068e5", "title": "Deep Lip Reading: a comparison of models and an online application"}, {"paperId": "2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9", "title": "CrisisMMD: Multimodal Twitter Datasets from Natural Disasters"}, {"paperId": "fe018f22600d07cbd0452a070e03708886470015", "title": "The Sound of Pixels"}, {"paperId": "f7cc85bed2a3d0b0ef1c0e0258f5b60ee4bb4622", "title": "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-attention for Visual Question Answering"}, {"paperId": "93b4cc549a1bc4bc112189da36c318193d05d806", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform"}, {"paperId": "1a2599e467e855f845dcbf9282f8bdbd97b85708", "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions"}, {"paperId": "006dfcaefde7201e785a7bd02a2077680cdb19b8", "title": "Adaptive online event detection in news streams"}, {"paperId": "90873a97aa9a43775e5aeea01b03aea54b28bfbd", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"}, {"paperId": "74b284a66e75b65f5970d05bac000fe91243ee49", "title": "Video Captioning via Hierarchical Reinforcement Learning"}, {"paperId": "8b35c00edfa4edfd7a99d816e671023d2c000d55", "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks"}, {"paperId": "0197f278e2dedd67ec5067f47037b8cdd3ae8509", "title": "Deep Multimodal Learning: A Survey on Recent Advances and Trends"}, {"paperId": "c499a1738be0915df2fa1f97c1f8671da1630bee", "title": "Cascade recurrent neural network for image caption generation"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "edf0d924b740e436ca4641ccdc8ec3983132fa18", "title": "Lip2Audspec: Speech Reconstruction from Silent Lip Movements Video"}, {"paperId": "057b80e235b10799d03876ad25465208a4c64caf", "title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion"}, {"paperId": "feca3f41b5b4cc13368d53f3168cc55a2420ec16", "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning"}, {"paperId": "20d27c336bef081adf7faffd42721b77a3c92508", "title": "Multimodal Fusion with Recurrent Neural Networks for Rumor Detection on Microblogs"}, {"paperId": "9e24181d33f8f06f79c0e9e109269f03a13a0cc5", "title": "Video2vec Embeddings Recognize Events When Examples Are Scarce"}, {"paperId": "81e6fd757c2a1098554fdb2d7b6726e32088b667", "title": "On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy"}, {"paperId": "b2f521c02c6ed3080c5fe123e938cdf4555e6fd2", "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"}, {"paperId": "f338fe8ed2e8d87feea5c6538a5a05a06b3696db", "title": "AMC: Attention Guided Multi-modal Correlation Learning for Image Search"}, {"paperId": "915b5b12f9bdebc321e970ecd713458c3479d70e", "title": "An Analysis of Visual Question Answering Algorithms"}, {"paperId": "32dda6ba64ff03979da500567943c5c4999bcf9a", "title": "On the Importance of Super-Gaussian Speech Priors for Machine-Learning Based Speech Enhancement"}, {"paperId": "5c87c275ddde2e0b75264fe9dad7b130db410601", "title": "Vid2speech: Speech reconstruction from silent video"}, {"paperId": "dc07e7bf1cf25aab2c39c85ab03bf085bbca31b5", "title": "Attentive Explanations: Justifying Decisions and Pointing to the Evidence"}, {"paperId": "6c8353697cdbb98dfba4f493875778c4286d3e3a", "title": "Self-Critical Sequence Training for Image Captioning"}, {"paperId": "92d72fc425716930dbab3b9565a25ac53434210a", "title": "Topic-based content and sentiment analysis of Ebola virus on Twitter and in the news"}, {"paperId": "88513e738a95840de05a62f0e43d30a67b3c542e", "title": "SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning"}, {"paperId": "88c307c51594c6d802080a0780d0d654e2e2891f", "title": "Visual question answering: A survey of methods and datasets"}, {"paperId": "1389564ab24d4b63c921a1ed564e5410b5199f7d", "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos"}, {"paperId": "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "title": "Generative Adversarial Text to Image Synthesis"}, {"paperId": "95cd83603a0d2b6918a8e34a5637a8f382da96f5", "title": "MIMIC-III, a freely accessible critical care database"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "492f57ee9ceb61fb5a47ad7aebfec1121887a175", "title": "Gated Graph Sequence Neural Networks"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "354c029c88be2bbc27dfd2e2e729c0ae622511e6", "title": "YFCC100M"}, {"paperId": "d6829332d0596659272451920d9ff778b0b400af", "title": "TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech"}, {"paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator"}, {"paperId": "e6d850a9e31799460a3c71c4addfbcd106755630", "title": "AVEC 2014: 3D Dimensional Affect and Depression Recognition Challenge"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "c3370cf462ed32e2f13dac7d7f2de49a4546fcbe", "title": "The MIT Stata Center dataset"}, {"paperId": "463df0d0d0731e62315b5c9322e2f8e8757014f7", "title": "Multimodal Saliency and Fusion for Movie Summarization Based on Aural, Visual, and Textual Attention"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "c1994ba5946456fc70948c549daf62363f13fa2d", "title": "Indoor Segmentation and Support Inference from RGBD Images"}, {"paperId": "d3963ea65b028504f85eb6c4b04a28f8331b6eac", "title": "Indoor scene segmentation using a structured light sensor"}, {"paperId": "27daf5bb692f8fba491f349ab11821e59f974c72", "title": "AVEC 2011-The First International Audio/Visual Emotion Challenge"}, {"paperId": "c069629a51f6c1c301eb20ed77bc6b586c24ce32", "title": "The Caltech-UCSD Birds-200-2011 Dataset"}, {"paperId": "a78273144520d57e150744cf75206e881e11cc5b", "title": "Multimodal Deep Learning"}, {"paperId": "e9090508b9073763c6693983577137db2a41a25b", "title": "Multimodal fusion for multimedia analysis: a survey"}, {"paperId": "6e8c107f5f852640daaf324111fc9cf35b5e4acc", "title": "A new approach to cross-modal multimedia retrieval"}, {"paperId": "6240ce60e12b817ad8e4f660386ded68225f18d5", "title": "The SEMAINE corpus of emotionally coloured character interactions"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "843959ffdccf31c6694d135fad07425924f785b1", "title": "Extracting and composing robust features with denoising autoencoders"}, {"paperId": "e4e0fd56309e28b28bb47c9a72ad6111c76bb8b9", "title": "The AMI Meeting Corpus: A Pre-announcement"}, {"paperId": "1ee66e6ff219e4314560d06cb7c3c96450e36623", "title": "Comparison of automatic shot boundary detection algorithms"}, {"paperId": "76b532e2cb573fdf29f3ae68dc1372f3319c93c2", "title": "Active Appearance Models"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "eef41ae597a20ea377461d522fd5100da6a7a9b7", "title": "Hearing lips and seeing voices"}, {"paperId": "9d846ed3e6cd0e60f50ebc547318815ebb2bb9ca", "title": "VQA-GNN: Reasoning with Multimodal Semantic Graph for Visual Question Answering"}, {"paperId": "a9693b7b57f203940889de6d3f979c70c09202ed", "title": "Shuffled-token Detection for Refining Pre-trained RoBERTa"}, {"paperId": "6a2b1b9ee2f9bd8b8773a950f635d59d037948e3", "title": "Multi-Gate Attention Network for Image Captioning"}, {"paperId": "654247d5b184495fca18c6aa7e840e4f4559fef0", "title": "Do We Really Need Explicit Position Encodings for Vision Transformers?"}, {"paperId": null, "title": "ELECTRA: pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "6e94479bcb7f3180acc426261646caba07dd3e8d", "title": "Analyzing Compositionality in Visual Question Answering"}, {"paperId": "8b55402ffee2734bfc7d5d7595500916e1ef04e8", "title": "nocaps: novel object captioning at scale"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Fashion 200K Benchmark"}, {"paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82", "title": "Deep Learning"}, {"paperId": null, "title": "2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"}, {"paperId": null, "title": "Journal, 978-1-4503-XXXX-X/18/06"}]}