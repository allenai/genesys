{"paperId": "dfd1927e00519848b124449373168d00eb140bde", "abstract": "With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient search mechanism that dynamically finds the best quantization configurations, and (3) a quantization-aware delta compression mechanism that rearranges weights to minimize checkpoint differences, thereby maximizing compression. We instantiate these contributions in DynaQuant - a framework for DL workload checkpoint compression. Our experiments show that DynaQuant consistently achieves a better tradeoff between accuracy and compression ratios compared to prior works, enabling a compression ratio up to 39x and withstanding up to 10 restores with negligible accuracy impact for fault-tolerant training. DynaQuant achieves at least an order of magnitude reduction in checkpoint storage overhead for training failure recovery as well as transfer learning use cases without any loss of accuracy.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.11800", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "DynaQuant consistently achieves a better tradeoff between accuracy and compression ratios compared to prior works, enabling a compression ratio up to 39x and withstanding up to 10 restores with negligible accuracy impact for fault-tolerant training."}, "embedding": {"model": "specter_v2", "vector": [0.2703397572040558, 0.31457287073135376, -0.9914218187332153, 0.07148829847574234, -0.47992411255836487, 0.1990172564983368, 0.2629755735397339, 0.11954062432050705, -0.4166436195373535, -0.4635648727416992, 0.1163359060883522, -0.26867592334747314, 0.384580135345459, 0.06626929342746735, -0.44130709767341614, 0.24388296902179718, -0.6992509961128235, 0.262033075094223, 0.29358595609664917, -0.06510136276483536, 0.12326006591320038, 0.13222643733024597, -1.5891757011413574, 0.3274572193622589, 0.5139979124069214, 1.4180755615234375, -0.6290079951286316, 1.0176042318344116, -0.31913256645202637, 0.279377281665802, 0.36781948804855347, 0.02556663192808628, 0.22718876600265503, 0.16932864487171173, -0.546980082988739, 0.36454927921295166, 0.5295653343200684, -0.49570736289024353, -0.45291662216186523, 0.4530496895313263, -0.15615583956241608, 0.05445292964577675, 0.23492678999900818, -0.9753944873809814, -0.047839146107435226, -0.24458250403404236, 0.3123789131641388, 0.7432103753089905, -0.819185733795166, -0.3638271689414978, 0.6105749011039734, -1.242652416229248, -0.16637347638607025, 0.9920307397842407, 1.1450316905975342, 0.32746776938438416, -0.5964722037315369, -0.211934894323349, 0.10830719023942947, 0.11241405457258224, -0.4855012595653534, -0.6184223890304565, -0.30744466185569763, -0.39529848098754883, 1.634781002998352, -0.28794845938682556, -0.09132516384124756, 0.16936589777469635, 0.4276416301727295, 1.035424828529358, 0.024849705398082733, -0.767818033695221, 0.1283189356327057, -0.22164297103881836, 0.41721150279045105, 0.6760349273681641, -0.023234829306602478, 0.26113811135292053, -1.1634823083877563, -0.13881856203079224, 0.1526382714509964, 0.4655378758907318, 0.44779446721076965, -0.4089873433113098, 0.3949477970600128, 0.33910906314849854, 0.3791245222091675, 0.1804104596376419, -0.23591233789920807, 1.3858706951141357, 1.0144321918487549, 0.6561295390129089, 0.6020205616950989, -0.0986761674284935, 0.20522907376289368, -0.05236778035759926, -1.031143307685852, -0.27738523483276367, -0.1860995590686798, 0.5537923574447632, 0.18543580174446106, 0.20610973238945007, -0.587005615234375, 0.43113768100738525, 0.7443625926971436, -0.4265119135379791, 0.46501749753952026, -0.6493924260139465, 0.5100998878479004, -0.33583223819732666, -0.37008312344551086, -0.1240450069308281, 0.17381560802459717, -0.40553152561187744, -1.158249855041504, -0.44468194246292114, -1.2382937669754028, -0.10787433385848999, -0.9544835090637207, 0.10258140414953232, -0.3398406505584717, 0.5491471290588379, 0.3684826195240021, 0.36484163999557495, 0.07256242632865906, 0.25047171115875244, 0.09769453853368759, 0.07353617250919342, 0.703974187374115, -0.9647030830383301, -0.049888644367456436, -1.0422942638397217, 0.6209070682525635, -0.3157583475112915, 0.603945255279541, -0.2617291510105133, -1.323623538017273, -1.116830825805664, -1.141802430152893, 0.1444219946861267, -0.0849362388253212, -0.01848471909761429, 1.0191214084625244, 0.2339095026254654, -1.0270766019821167, 1.403913974761963, -0.6825668215751648, -0.08433874696493149, 0.6525250673294067, 0.3890881836414337, 0.47958919405937195, -0.5970625281333923, -0.7074159383773804, -0.08639070391654968, -0.02142503671348095, -0.7334863543510437, -0.2350342571735382, -1.1145128011703491, -0.4811108112335205, 0.2296193391084671, 0.12928622961044312, -0.7403706908226013, 1.4308645725250244, -0.04772277548909187, -0.8197291493415833, 0.5356597900390625, 0.27885016798973083, 0.1724630445241928, 0.7446461915969849, -0.17222940921783447, -0.27520933747291565, -0.05764109268784523, -0.6457715034484863, 0.5562618970870972, 0.8159268498420715, -0.21544237434864044, -0.09182509779930115, -0.005473238416016102, -0.7194119095802307, 0.32024404406547546, -0.6538086533546448, 0.39671963453292847, -0.7002048492431641, 0.010083092376589775, 0.4830649793148041, 0.6990734338760376, 0.1714119017124176, 0.0026533238124102354, -0.1963617354631424, -0.6793795228004456, 1.2382445335388184, 0.2963501811027527, 0.6520415544509888, -1.2171076536178589, -1.1621429920196533, -0.014564271084964275, -0.049021512269973755, -0.22435759007930756, -0.5072903037071228, 0.9691242575645447, -0.2956981956958771, 0.3675287365913391, 0.034331392496824265, -1.4078774452209473, 0.08417358249425888, -0.014106563292443752, -1.142066478729248, -0.3663199543952942, 0.2670067250728607, 0.9463689923286438, -0.18083031475543976, 0.07974302023649216, -0.46012037992477417, 0.4479886591434479, -0.8836401700973511, 1.0821672677993774, -0.23217248916625977, -0.082376629114151, 0.4948257505893707, 0.21911372244358063, 0.13435442745685577, -0.3421882092952728, 0.6397926211357117, -0.4784103035926819, 0.2572743594646454, 0.41071686148643494, 0.13902193307876587, 1.0735994577407837, -0.5801398754119873, 0.26781415939331055, -0.22624404728412628, -0.9003680944442749, 0.2889830470085144, 0.24765342473983765, 0.03583298996090889, -0.47841551899909973, 0.07764218747615814, 0.5389657020568848, -0.8600643277168274, 0.5275263786315918, 1.244137167930603, 0.4892265498638153, -0.4991890788078308, 0.21089985966682434, 0.8663250803947449, -0.5177856087684631, 0.3510171175003052, 0.18316011130809784, 0.9487199187278748, 0.232036754488945, -0.5007931590080261, 0.3320283889770508, -0.5592923760414124, -0.625490665435791, -0.20541606843471527, 0.43813297152519226, 0.4768087863922119, 0.8469328284263611, 1.0906801223754883, -0.7864123582839966, -0.7159717679023743, 0.17368176579475403, 0.7325515151023865, 1.5485841035842896, -0.20418940484523773, 0.24713097512722015, -0.8208493590354919, -0.7351308465003967, 0.09114200621843338, -0.11522097885608673, -0.319915771484375, -0.9750708937644958, -0.3817230463027954, -1.2494304180145264, 0.9799551963806152, 0.3000268042087555, 1.0870332717895508, -0.6122194528579712, -0.558253288269043, -0.5301558971405029, 0.8267672061920166, -0.653822124004364, -0.539168655872345, 1.0992321968078613, -1.1699633598327637, -0.27856263518333435, 0.4871567189693451, -0.11947409808635712, 0.2778935730457306, -0.531592845916748, 1.116213321685791, 0.2808692753314972, -0.539581298828125, 0.26966559886932373, 0.658744752407074, -0.3524690270423889, -0.6920284032821655, 0.4474082887172699, 0.4447278678417206, -0.3691057860851288, 0.06058146432042122, -0.062156740576028824, -0.07645910233259201, -0.3306921422481537, -0.6625440716743469, -0.09487316012382507, 0.1466490626335144, -0.07540522515773773, 1.1422526836395264, -0.43989312648773193, 0.22025059163570404, -1.378248691558838, 1.3943238258361816, -0.21576541662216187, -0.15509165823459625, 0.2283717393875122, -1.4458236694335938, -0.20347070693969727, 0.6874973177909851, -0.548160195350647, 0.11632099002599716, -1.3276790380477905, 0.13424962759017944, -1.2869292497634888, -0.2576802372932434, 0.20222489535808563, 1.158825159072876, -0.26409491896629333, 0.5410333871841431, 0.4574466943740845, 0.6402145624160767, -0.21643035113811493, 0.11711293458938599, -0.9247092008590698, 0.7952547669410706, 0.06459766626358032, -0.23908869922161102, -0.4594561457633972, 0.31659790873527527, -0.3806239664554596, -0.5224146246910095, 0.04280148074030876, -0.24662497639656067, -0.42098841071128845, -0.17261262238025665, -0.2411871701478958, -0.7768880128860474, -0.46320459246635437, -0.5839359164237976, -0.7955553531646729, -0.4135710597038269, -0.49406951665878296, -0.47766345739364624, -1.1891247034072876, -1.5643041133880615, -0.14142495393753052, -1.0865004062652588, -1.3038434982299805, 0.3855246305465698, 0.03488771617412567, -0.08081558346748352, -0.4323559105396271, -0.1459226757287979, -0.7856287360191345, 0.9599475264549255, -0.283780574798584, 1.103110909461975, 0.3542076647281647, 0.28199467062950134, 0.17991088330745697, -0.16616782546043396, 0.8898984789848328, -0.6731210947036743, 0.3717675805091858, -0.985422670841217, -0.03855683654546738, -0.5520750880241394, -1.2414562702178955, 0.4712068736553192, -0.09557253122329712, 0.9265753626823425, -0.12846705317497253, -0.155626118183136, 0.9422222971916199, 1.7178864479064941, -0.9787814617156982, 0.07718502730131149, -0.08312533050775528, 0.7274468541145325, -0.1205943152308464, -0.5029165148735046, 0.44644004106521606, -0.44711068272590637, 0.2846207022666931, 0.4589817523956299, -0.25342708826065063, -0.7013562321662903, -0.6208275556564331, 0.35743510723114014, 1.6129741668701172, 0.9542197585105896, 0.098673515021801, -1.2052055597305298, 0.16797196865081787, -1.2182905673980713, -0.020137546584010124, 0.9231009483337402, 0.6806893944740295, 0.4238296151161194, -0.2405603528022766, -0.3534896671772003, 0.23009786009788513, 0.07705099135637283, 0.3700515031814575, -0.9649225473403931, -1.0926722288131714, 0.5859007239341736, 0.4805608093738556, 0.3822925388813019, 0.17919030785560608, -0.10922541469335556, 0.11136265099048615, 14.387218475341797, 0.8769851326942444, -0.5695170164108276, 0.8314589858055115, 1.0836232900619507, 0.13088089227676392, -0.05664520338177681, -0.5020186305046082, -0.9150684475898743, 0.03309619054198265, 1.4596346616744995, 0.3496474325656891, 0.3114812672138214, 0.41558969020843506, -0.2160978764295578, -0.08058304339647293, -0.44707244634628296, 0.6719776391983032, 0.2589583098888397, -1.7323983907699585, 0.16648982465267181, 0.11997096240520477, 0.628529965877533, 0.6634459495544434, 1.228688359260559, 0.9801222681999207, 0.23572379350662231, -0.2620551288127899, 0.40369144082069397, 0.5494188666343689, 1.8852331638336182, -0.6139169931411743, 0.580392062664032, 0.5765787363052368, -0.4119245111942291, -0.02999689057469368, -0.7062139511108398, -0.8659217953681946, -0.02974766679108143, 0.23089361190795898, -0.8588771820068359, -0.3185654878616333, 0.013655995018780231, 0.451456755399704, -0.40800032019615173, 0.21194609999656677, 0.15212973952293396, 0.9863538146018982, -0.002889418974518776, 0.7422356605529785, 0.06756801903247833, 0.5328882932662964, -0.657845675945282, 0.2975434362888336, 0.07985460013151169, -0.4852234423160553, 0.44080811738967896, -0.01846049539744854, -1.087201476097107, -0.619970977306366, -0.20255199074745178, 0.015980927273631096, -0.03174968808889389, 0.5993784666061401, 0.19252048432826996, 0.16132642328739166, -0.0688619315624237, 0.7039976716041565, 0.9913073182106018, 0.15493164956569672, 0.02230600081384182, -0.07119164615869522, 0.5449041724205017, -0.2071780562400818, -0.1923763006925583, 0.6420326232910156, -0.7261874675750732, -0.33500316739082336, -0.6752175688743591, -0.17153704166412354, 0.11820925027132034, -0.5595752000808716, -0.7560239434242249, 0.7575895190238953, -0.26002267003059387, -0.24642066657543182, 0.05776311084628105, -0.3237893283367157, -0.493574321269989, 0.44319698214530945, -1.3885464668273926, -0.2027309089899063, 0.008240941911935806, -0.5223527550697327, -0.712917685508728, -0.06972941756248474, 1.3241291046142578, 0.3527742028236389, -0.677543580532074, 0.2041665017604828, 0.2698012888431549, -0.547858476638794, 0.06465700268745422, -0.7078896164894104, 1.0422289371490479, 0.16091388463974, -0.3209647834300995, -0.33515796065330505, -0.23343978822231293, 0.1859961301088333, -0.6733219623565674, -0.8266892433166504, 0.3084266185760498, -0.14268995821475983, -0.48588138818740845, -0.5156469345092773, -1.0233569145202637, 0.3483312726020813, 0.1972026526927948, 0.5471780896186829, 0.5157482028007507, 0.1372617930173874, -0.6641327738761902, -0.3753248155117035, -0.9619760513305664, -0.25267907977104187, 0.3620855212211609, -0.9674787521362305, -0.016186963766813278, 0.13673071563243866, 0.5439404249191284, -1.270442247390747, -0.796699583530426, -0.1638331264257431, -0.27610936760902405, -0.6666842103004456, 0.6968772411346436, -0.2107210010290146, 1.119023084640503, 1.2746965885162354, 0.03872856870293617, -0.9813109636306763, 0.5359383821487427, -0.32180505990982056, -0.269029825925827, 0.17945416271686554, -0.08485500514507294, 0.009504665620625019, 1.0741076469421387, 0.44035640358924866, -0.550729513168335, -0.9275295734405518, -0.547558069229126, -0.06456474214792252, -0.08725482225418091, -0.5331952571868896, 0.2171284258365631, 0.036040209233760834, -0.05165104568004608, 0.01565992459654808, 0.7492459416389465, 0.12319224327802658, -0.03533565625548363, -0.5899595022201538, 0.014708934351801872, -0.25067979097366333, -0.19280990958213806, -0.20201872289180756, -0.4615195393562317, -1.0903105735778809, -0.17215247452259064, -1.0275306701660156, -0.07657628506422043, -0.7400385141372681, -0.5983651280403137, -0.6019099950790405, -0.1893528699874878, 0.289201021194458, 0.6221954822540283, 0.3277141749858856, -0.22113364934921265, 0.100867360830307, -0.7753663659095764, 0.8767158389091492, 0.6529765129089355, -0.15088219940662384, 0.15932393074035645, -0.1673273891210556, 0.1376097947359085, 0.1871102899312973, 0.7162041068077087, -0.4412550628185272, -0.8558384776115417, -1.3756887912750244, 0.17705270648002625, 0.23027350008487701, -0.06623339653015137, -1.3788998126983643, 0.7361204624176025, 0.30936336517333984, 0.11216806620359421, 0.23459334671497345, 0.030572103336453438, -0.7926697731018066, -0.42800652980804443, 0.39804038405418396, -0.6305398344993591, 0.45013850927352905, 0.6545007228851318, -0.31265419721603394, -0.34320902824401855, 0.5564969182014465, 0.1664111614227295, -0.05720582231879234, -0.847572922706604, 0.44501084089279175, -0.19569925963878632, -0.1643083244562149, -0.3549548089504242, 0.05627183988690376, -1.4281429052352905, -0.028437111526727676, -0.1857781857252121, 0.08474690467119217, 0.052950043231248856, 0.6085026264190674, 0.4567525386810303, -1.4669443368911743, 0.5195813775062561, 0.5252806544303894, -0.045129578560590744, 0.2903801500797272, 0.7621726393699646, 0.7309802770614624, -0.9280954599380493, 0.07030540704727173, -0.13608957827091217, 0.2690550684928894, -0.487927109003067, -0.07948043197393417, 0.7724600434303284, -0.6407617330551147, -0.06673379987478256, 0.8217639923095703, -0.9343942403793335, -1.0936282873153687, 0.367823988199234, -1.3526510000228882, -0.0951247289776802, -0.16577069461345673, 0.2047395557165146, 0.2874127924442291, 1.0270739793777466, 0.46425870060920715, -0.2769095003604889, -0.19903184473514557, 0.025297652930021286, -0.3751680254936218, 0.2997766435146332, 0.028118865564465523, -0.12821358442306519, 0.7492614984512329, 1.0579980611801147, -1.0387628078460693, -1.2539547681808472, -0.7601711750030518, -0.24454791843891144, 0.226812943816185, 0.5734992623329163, -0.44346264004707336, -0.8129357695579529, 0.9472869634628296, 0.6381068825721741, 0.2545321583747864, 0.44679194688796997, -0.4816614091396332, 0.22918741405010223, 0.6155714988708496, -0.05711737275123596, -0.5665273666381836, -0.13962717354297638, 1.018375039100647, 1.2672806978225708, -0.7371805310249329, 0.525902271270752, -0.034102026373147964, -0.6471031308174133, 0.9834088683128357, 0.6921178698539734, -0.2614332437515259, 0.7806478142738342, 0.06277615576982498, -0.11765439808368683, 0.11583086103200912, -1.443934679031372, 0.18659475445747375, 0.5332609415054321, 0.8721997737884521, 0.5800957083702087, 0.05304192379117012, 0.5016186833381653, 0.7079924941062927, 0.2551306486129761, 0.2663070857524872, 0.4406570792198181, 0.7663435935974121, -0.29203397035598755, -0.021142667159438133, -0.0273776613175869, 1.151294231414795, -0.6545266509056091, -0.25848937034606934, 0.4106675088405609, 0.5995360016822815, 0.5604653358459473, 0.3503958284854889, 1.4449411630630493, -0.7330941557884216, 1.0415585041046143, 0.1878432035446167, 0.39894992113113403, -0.628595232963562, -0.6197038888931274, -0.43336430191993713, -0.8589794039726257, -0.1698557436466217, 0.5040340423583984, -0.2859281897544861, -0.44753795862197876, -0.8622593879699707, 1.097496747970581, 0.11238617449998856, 0.262263685464859, 0.4339822232723236, 0.9637367129325867, 1.1982766389846802, -0.1199568435549736, -0.9018807411193848, -0.9966336488723755, -0.7700066566467285, -0.25692296028137207, -0.5051271319389343, -0.32495561242103577, 0.05537747964262962, -0.048529017716646194, -0.15039287507534027]}, "authors": [{"authorId": "70203043", "name": "Amey Agrawal"}, {"authorId": "2072887029", "name": "Sameer Reddy"}, {"authorId": "92954142", "name": "S. Bhattamishra"}, {"authorId": "2220287616", "name": "Venkata Prabhakara Sarath Nookala"}, {"authorId": "10723041", "name": "Vidushi Vashishth"}, {"authorId": "7804757", "name": "Kexin Rong"}, {"authorId": "144312193", "name": "Alexey Tumanov"}], "references": [{"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "9296ba66fff9af385197a7f9a3aba092ea04aff3", "title": "Design of a Quantization-Based DNN Delta Compression Framework for Model Snapshots and Federated Learning"}, {"paperId": "10e5216e9f077cde88b8fbde04cbca5bef6ccb64", "title": "A Sensitivity-based Pruning Method for Convolutional Neural Networks"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "4b4cf6cc67f23635449e59222c055dfd87ab34bd", "title": "When to Prune? A Policy towards Early Structural Pruning"}, {"paperId": "1119cb7a2d9626796db19e13e829f0e5fc3cba26", "title": "Distribution-aware Adaptive Multi-bit Quantization"}, {"paperId": "a82ac23ae7db7ad88e9d4ecd526cb15fb5d68b99", "title": "Layer Importance Estimation with Imprinting for Neural Network Quantization"}, {"paperId": "6a18f2a287e57949372b88a3a9b7bcf42c9d43f4", "title": "SeReNe: Sensitivity-Based Regularization of Neurons for Structured Sparsity in Neural Networks"}, {"paperId": "de983239063bf87fafc549e651ea133088b1831f", "title": "Hessian-Aware Pruning and Optimal Neural Implant"}, {"paperId": "6dbe18f41615ec60b10e4698a0c82fd8c0b05f5a", "title": "HAWQV3: Dyadic Neural Network Quantization"}, {"paperId": "ce298d749f20ae5024c83c39b22d2c4b943bc2e0", "title": "Hessian-Driven Unequal Protection of DNN Parameters for Robust Inference"}, {"paperId": "a9bebc22b682906a9ebb4d4cce4b31d230131385", "title": "Adaptive Gradient Quantization for Data-Parallel SGD"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "10827fb697b6f426b0d6f4905053aafa51a8786f", "title": "A Decade Survey of Transfer Learning (2010\u20132020)"}, {"paperId": "9c0e855382de7e708c8eea7b4d5cf792bcd4a326", "title": "Degree-Quant: Quantization-Aware Training for Graph Neural Networks"}, {"paperId": "33889c33811c3fc5bfa4a14fdff6604842ea8e1e", "title": "Delta-DNN: Efficiently Compressing Deep Neural Networks via Exploiting Floats Similarity"}, {"paperId": "8e1127b8eeaa2ac06d840ec4cebbc56f99a6b78b", "title": "On Efficient Constructions of Checkpoints"}, {"paperId": "0b5c5571ab9dd88ebf010e8dc6898d8078d8e877", "title": "Progressive Skeletonization: Trimming more fat from a network at initialization"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "5e2df05a1a45b57d2fc0748bccc45865b9da6913", "title": "DeepFreeze: Towards Scalable Asynchronous Checkpointing of Deep Learning Models"}, {"paperId": "72d3ddf1f7210d7e70144bbc09f770ec411fe909", "title": "Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence"}, {"paperId": "e31318ede6342e5726c8081b4273bd01adc13283", "title": "Post-training Piecewise Linear Quantization for Deep Neural Networks"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "0acc25f3993bd9431418ae275aa12a536b740b77", "title": "ZeroQ: A Novel Zero Shot Quantization Framework"}, {"paperId": "26aa2fe7d026f0c66c3181097b939c67c2ff22b2", "title": "Adaptive Loss-Aware Quantization for Multi-Bit Networks"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "0a5d987ddb5463062babceca90ba974db0cf96e7", "title": "HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks"}, {"paperId": "d8a4b8d9d71a5139062d142cb0bed0a0f274641c", "title": "Post-Training 4-bit Quantization on Embedding Tables"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "134736a458faa7fe298ac0563de95475ba433e87", "title": "How much can k-means be improved by using better initialization and repeats?"}, {"paperId": "ca8c77f776b92d5a80ced6111f960d85aa649f5c", "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch with Relative-Error Guarantees"}, {"paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec", "title": "A Study of BFLOAT16 for Deep Learning Training"}, {"paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae", "title": "Searching for MobileNetV3"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "47a1edfb88f5b4a7ba1e9f6aed327f67f942f6d6", "title": "Low-bit Quantization of Neural Networks for Efficient Inference"}, {"paperId": "54ddd67944520f249b906ba4e817188686eae94d", "title": "Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads"}, {"paperId": "54c4642d017830e1faddbb49f0377228d2b01493", "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"}, {"paperId": "f789425a7af1d012675118d7d10cd50afad09074", "title": "Post training 4-bit quantization of convolutional networks for rapid-deployment"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "7b7c343c21b9130c5f3908e6435006b555373680", "title": "Learning Compression from Limited Unlabeled Data"}, {"paperId": "4be90a098e461d8730232529c1f8edefc87b85b6", "title": "Optimization based Layer-wise Magnitude-based Pruning for DNN Compression"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "28b79a6a752433958fa4d7ac82ac304b7087400a", "title": "Deep Learning with Low Precision by Half-Wave Gaussian Quantization"}, {"paperId": "3db8730c203f88d7f08a6a99e8c02a077dc9b011", "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference"}, {"paperId": "6d749712ef4f02a87f4e362058469a1da46c8bcc", "title": "Towards the Limit of Network Quantization"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "eb21fe36822a05a14b5ab548696e8fa8d555d6d4", "title": "Convolutional Neural Networks using Logarithmic Data Representation"}, {"paperId": "9ef78ff09225149271c1216416b9bd3dee31f1c9", "title": "Hardware-oriented Approximation of Convolutional Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"}, {"paperId": "67c191bcce6821f736798cb9b31472bcdd1e52a6", "title": "Neural Networks with Few Multiplications"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "ac5a2d9efc4113b280dbf19159fc61deecc52126", "title": "Clustering in the Presence of Background Noise"}, {"paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972", "title": "A Survey on Transfer Learning"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "4610879d3babc80e86601dec7aef4219abf26233", "title": "Clusterability: A Theoretical Study"}, {"paperId": "ab25e57c716c34c02fe8f78738ffbd44fe7732fa", "title": "k-means++: the advantages of careful seeding"}, {"paperId": "5b9e3b5b27a4b09ffb7492c2b2572dab063e8a0a", "title": "A new pruning heuristic based on variance analysis of sensitivity information"}, {"paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning"}, {"paperId": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"}, {"paperId": "03dc759f8dec25d22086afa34970e984eddb5418", "title": "Run-length encodings (Corresp.)"}, {"paperId": "a068df4c5f829bd0b85bf72fd919006c563c6345", "title": "A method for the construction of minimum-redundancy codes"}, {"paperId": null, "title": "Stanford Alpaca: An Instruction-following LLaMA model"}, {"paperId": "7ddb669ca1032e820f1ddc8f9fbb2847e55983e7", "title": "Comparing Fisher Information Regularization with Distillation for DNN Quantization"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Homemade BookCorpus"}, {"paperId": "a59da4639436f582e483347a4833e7659fd3e598", "title": "CuPy : A NumPy-Compatible Library for NVIDIA GPU Calculations"}, {"paperId": "28135fd3e80dda50a673cd556f10b9b972005d27", "title": "Binarized Neural Networks"}, {"paperId": null, "title": "TorchVision: PyTorch's Computer Vision library"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": "a3d37fe50a66e4c67bbffa98d6aaa53c39b108c8", "title": "This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation."}, {"paperId": "ff1b068b21ff4b721672c42f08476659a123020e", "title": "This paper is included in the Proceedings of the 19th USENIX Conference on File and Storage Technologies."}, {"paperId": null, "title": "The simplest, fastest repository for training/finetuning medium-sized gpts"}, {"paperId": null, "title": "Hugging Face"}]}