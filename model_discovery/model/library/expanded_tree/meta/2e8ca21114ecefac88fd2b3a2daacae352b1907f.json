{"paperId": "2e8ca21114ecefac88fd2b3a2daacae352b1907f", "abstract": "Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.", "venue": "arXiv.org", "year": 2024, "citationCount": 6, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, are reviewed, designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements."}, "embedding": {"model": "specter_v2", "vector": [0.4509138762950897, 0.30153271555900574, -0.13761135935783386, -0.4538155496120453, -0.2812856435775757, -0.23123860359191895, 0.5110517740249634, 0.10855622589588165, -0.5989802479743958, 0.16164903342723846, 0.5078889727592468, -0.16893264651298523, 0.4326205849647522, 0.1558927744626999, -0.19539931416511536, 0.07033555209636688, -1.2523516416549683, 0.023506227880716324, 0.21054226160049438, -0.07242288440465927, 0.03813090920448303, -0.33922722935676575, -0.9824926853179932, 0.6593376994132996, -0.010356559418141842, 0.34656476974487305, 0.591058075428009, 1.0124741792678833, -0.4934800863265991, 0.677378237247467, 0.5312923192977905, -0.12543179094791412, 0.011679543182253838, -0.009125152602791786, -0.07674973458051682, -0.6488867998123169, 0.3551487922668457, -0.74161297082901, -0.7279565334320068, 0.4809715151786804, -0.22495898604393005, 0.3403242528438568, -0.07311441004276276, -0.4581526219844818, -0.22202955186367035, 1.477049708366394, 0.6876694560050964, 0.9302841424942017, 0.04209545627236366, -0.6469748020172119, 1.71721613407135, -1.4695639610290527, 0.22298648953437805, 1.9265474081039429, 0.5061385631561279, 0.4780004322528839, -0.29979828000068665, -0.6605428457260132, 1.368529200553894, 0.134357750415802, -0.904148280620575, -0.43109214305877686, -0.07175058126449585, 0.1991967409849167, 1.9633089303970337, -0.16770370304584503, -0.00916302390396595, 0.9367035031318665, 0.008313898928463459, 1.3090400695800781, -0.02813713811337948, -1.1829560995101929, -0.01073733065277338, 0.07579795271158218, 0.4984579384326935, 0.5930376648902893, -0.3893495798110962, 0.48730987310409546, -1.0254676342010498, -0.3505808115005493, 0.6294868588447571, -0.22195307910442352, 0.1713583767414093, 0.002764770295470953, -0.5007233619689941, 0.6725255846977234, 0.04276934266090393, 0.7801167368888855, -0.08028902858495712, 0.6377783417701721, 0.19334913790225983, 0.29471153020858765, -0.17786556482315063, 0.35265839099884033, -0.25913137197494507, 0.36601924896240234, -0.6868553161621094, 0.4981018900871277, -0.007612297311425209, 0.9104111790657043, -0.4499633312225342, 0.227557972073555, -0.4809824526309967, 0.2946981191635132, 1.5088822841644287, -0.0971142053604126, 0.1771746128797531, -0.8231018781661987, 0.5052453875541687, -0.7523330450057983, 0.39064693450927734, -0.4454941153526306, -0.283986896276474, -0.3275286555290222, 0.11672976613044739, -1.3233447074890137, -0.0741131603717804, -0.033559516072273254, -0.0123098473995924, 1.1672589778900146, -0.16113601624965668, 0.18856187164783478, 0.14754723012447357, 0.07857382297515869, 0.20972800254821777, 0.6896647214889526, 0.3763386011123657, -0.21127885580062866, 0.74452143907547, -0.45619526505470276, -0.4361528754234314, -1.6336795091629028, 0.8691906332969666, 0.0700894221663475, 0.22959156334400177, -0.390243798494339, -1.158210039138794, -0.8802697062492371, -1.0361145734786987, -0.06694738566875458, -0.6238319873809814, -0.18919989466667175, 1.0383071899414062, 0.018301432952284813, -1.2332966327667236, 0.5596691370010376, -0.011034411378204823, -0.14688287675380707, 0.014484921470284462, 0.05802769958972931, 0.3737812638282776, -0.6717807650566101, -1.5933088064193726, 0.30254819989204407, 0.4854568839073181, -0.3327096700668335, -0.1330115646123886, -0.18818789720535278, -1.6218491792678833, 0.06077350303530693, 0.444806307554245, -0.5822016596794128, 1.4268524646759033, -0.05239639803767204, -0.9253019094467163, 0.3578483462333679, -0.5379186868667603, 0.0467756949365139, -0.0604279562830925, -0.021067237481474876, -0.8650657534599304, -0.3043738305568695, -0.5791128873825073, 0.7237103581428528, -0.06257527321577072, -0.3222109079360962, -0.5766057372093201, -0.042940814048051834, 0.15624374151229858, -0.059557441622018814, 0.10063648223876953, 1.0225948095321655, -0.29467085003852844, -0.2687698304653168, 0.6358602643013, 0.6266598701477051, -0.2527965009212494, -0.00688105309382081, 0.07462814450263977, -0.8176251649856567, 0.6418467164039612, -0.045321181416511536, 1.527079701423645, -0.702156126499176, -1.0677733421325684, -0.23357290029525757, -0.21480858325958252, -0.305614173412323, -1.1388843059539795, 0.8998892307281494, -0.26500028371810913, 0.3593079149723053, -0.22470155358314514, -0.8532842993736267, -0.25996679067611694, 0.0670018121600151, -0.6315629482269287, -0.37074044346809387, -0.18736650049686432, 0.9489022493362427, -1.2674336433410645, -0.32200825214385986, 0.05425134673714638, 0.20081713795661926, -0.8646186590194702, 1.1292929649353027, -0.8612571954727173, 0.15148188173770905, -0.298374205827713, -0.1972527652978897, -0.04059875011444092, -0.2839578688144684, 0.6698701977729797, -0.1448979675769806, -0.3161255717277527, 0.4912998378276825, -0.3648778796195984, 1.3720580339431763, -0.45784416794776917, 0.755757749080658, -0.10231401026248932, -0.13795068860054016, -0.27584385871887207, 0.5038958787918091, -0.38762417435646057, -0.2819940149784088, 0.15190929174423218, 0.09592155367136002, -0.36584511399269104, -0.2602701485157013, 0.5659134387969971, 0.8915428519248962, -0.36247336864471436, 0.3040179908275604, 0.036060020327568054, -0.015289080329239368, 0.6546008586883545, 0.4399118423461914, 0.49623265862464905, 0.5228617191314697, 0.8000591397285461, 0.19044655561447144, 0.5650768280029297, -1.1392030715942383, -0.044033341109752655, 0.8699842095375061, 0.8128041625022888, 0.5051381587982178, 0.39839881658554077, -0.5501914024353027, -0.3982243835926056, 0.5132718682289124, 0.4282452166080475, 1.6689335107803345, -0.16217587888240814, -0.18167538940906525, -0.7157974243164062, -0.05473064258694649, -0.27919501066207886, 0.6310997009277344, -0.321492999792099, 0.17397919297218323, -0.7022884488105774, -0.5010688900947571, 0.8983414769172668, 0.9307314157485962, 0.45192307233810425, -0.6786180734634399, -0.24639616906642914, 0.11706290394067764, 0.23047715425491333, -1.1411513090133667, -0.3801952600479126, 0.23596660792827606, -0.8931289315223694, -0.2998349666595459, 0.8061647415161133, -0.1810877025127411, 0.19411219656467438, -0.5770810842514038, 0.7608907222747803, -0.5100637078285217, -0.19425491988658905, 0.06801582872867584, 0.5470395684242249, -0.4372099041938782, -0.7888883352279663, -0.2560422420501709, 0.021412665024399757, -0.34744715690612793, 0.6688945889472961, 0.5643142461776733, 0.06078904867172241, 0.05317726358771324, -0.49786078929901123, 0.3855985105037689, 0.2238779515028, -0.056483011692762375, 0.7355100512504578, -0.46224263310432434, 0.35212987661361694, -1.3978639841079712, 0.6135410070419312, 0.09067311882972717, -0.5957834720611572, 0.3840991258621216, -0.673256516456604, -0.12299053370952606, 0.6519638299942017, -0.6875327229499817, -0.5573931336402893, -0.8708933591842651, 0.13535481691360474, -0.09181779623031616, -0.6389588713645935, 0.462277352809906, 0.13564413785934448, 0.23173770308494568, 0.008129275403916836, 0.5099626779556274, 0.24220937490463257, -0.1581854522228241, 0.49661582708358765, -0.48334434628486633, 0.3173345625400543, 0.30537691712379456, -0.48823150992393494, -0.3150465786457062, -0.17139334976673126, -0.7712891101837158, -0.4139176905155182, -0.45758160948753357, -0.4912617802619934, -0.06189584732055664, -0.2117796391248703, -0.5506092309951782, -0.7143586277961731, -0.11664500087499619, -1.0828602313995361, -0.38578787446022034, 0.5697817206382751, -0.06842964887619019, 0.11683998256921768, -0.7879318594932556, -1.3848559856414795, -0.8236274719238281, -0.057349979877471924, -0.6924260258674622, 0.16260464489459991, -0.1960357427597046, -0.8079981803894043, -0.9987272024154663, -0.2103547751903534, -0.2529473602771759, 0.7819961309432983, -0.6490804553031921, 1.3356765508651733, -0.3060307204723358, -0.48790520429611206, -0.5058125853538513, 0.27916455268859863, -0.09958455711603165, -0.3323332667350769, 0.05879983678460121, -1.1805669069290161, 0.225282683968544, -0.04135163500905037, -0.07264667749404907, -0.10247577726840973, 0.17534637451171875, 0.8216469287872314, -0.23679973185062408, -0.7138130068778992, -0.03925647959113121, 1.4315686225891113, -0.173639178276062, -0.18803860247135162, -0.24822810292243958, 0.8109409213066101, 0.3693298101425171, 0.036460407078266144, 0.7943065166473389, 0.08167869597673416, 0.329641729593277, 0.28499090671539307, 0.4092477858066559, 0.1095847338438034, -0.5614442825317383, 0.5107848048210144, 1.04586923122406, 0.5351453423500061, -0.1222744882106781, -0.8824771046638489, 0.3477843999862671, -1.308092713356018, -0.9255638718605042, 0.8740916848182678, 0.9067425727844238, 0.6366283297538757, -0.23515334725379944, -0.7742555737495422, -0.2580873370170593, 0.4330918788909912, 0.6628183722496033, -0.5229096412658691, -0.5888957977294922, -0.049573108553886414, -0.13051913678646088, -0.30043742060661316, 1.1945242881774902, -0.30236583948135376, 0.5622238516807556, 14.650675773620605, 0.4153372049331665, -0.02067881077528, 0.3164379596710205, 0.560971200466156, 0.02909237891435623, -0.47062912583351135, -0.11419393867254257, -1.5171622037887573, -0.23917710781097412, 1.4263041019439697, 0.3966842591762543, 0.8060060739517212, 0.12153377383947372, 0.5299327373504639, -0.27680832147598267, -1.2071199417114258, 0.7482053637504578, 0.6865942478179932, -1.1083765029907227, 0.3652980923652649, -0.085110142827034, 0.01056760549545288, 0.17463886737823486, 0.39054322242736816, 0.8259637355804443, 0.2910813093185425, -0.48559120297431946, 0.7046036720275879, 0.12071538716554642, 0.866795539855957, 0.17708739638328552, 0.3292064964771271, 0.881732702255249, -1.127250075340271, -0.6438771486282349, -0.6353044509887695, -1.2650700807571411, 0.13499434292316437, -0.30349406599998474, -0.7447779178619385, -0.8469425439834595, -0.6173123121261597, 0.6755017638206482, -0.2181074470281601, 0.3019196689128876, -0.2468380630016327, 0.5978882312774658, 0.11307420581579208, -0.16245673596858978, 0.35753363370895386, 0.327228307723999, 0.283771812915802, -0.0421450100839138, -0.002065009204670787, 0.3062724173069, 0.12024261802434921, 0.3204936683177948, -0.15374058485031128, -0.13752371072769165, -0.6747564077377319, -0.36152899265289307, 0.468966007232666, 0.26988527178764343, 0.5689370036125183, 0.25595271587371826, -0.48493361473083496, -0.06208541989326477, 0.7286208868026733, 0.5996399521827698, -0.2583663761615753, 0.351115345954895, 0.27427583932876587, -0.6294143199920654, 0.1155647486448288, 0.6591601371765137, -0.35161951184272766, -0.29929810762405396, -0.7607928514480591, -0.4295409917831421, 0.5841794610023499, -0.9822059869766235, -0.5948646068572998, 0.7608983516693115, 0.15584230422973633, -0.19038191437721252, -0.15301842987537384, -0.9090582728385925, -0.18305008113384247, 0.4506961703300476, -1.0431994199752808, -0.7179805040359497, 0.3637729585170746, -0.39650413393974304, 0.15185964107513428, 0.44772687554359436, 1.5767607688903809, -0.09587129205465317, -0.7655954360961914, -0.02631634846329689, 0.2150164693593979, -0.006172996014356613, -0.23339441418647766, -0.5411852598190308, 0.5033944249153137, 0.5397542119026184, -0.17269699275493622, 0.9045110940933228, -0.023312827572226524, -0.03225200995802879, -0.6980894804000854, -0.04049308970570564, 1.115849256515503, -0.9536073803901672, -0.5446951985359192, -0.9802723526954651, -1.0420199632644653, 0.7714716196060181, 0.4654506742954254, -0.35391533374786377, 0.5569604635238647, 0.1004289984703064, -0.7795570492744446, -0.28763413429260254, -0.6256937980651855, 0.3826655447483063, 0.6257058382034302, -0.9677176475524902, -0.38992470502853394, -0.48997732996940613, 0.46507176756858826, -0.9261325597763062, -0.29017388820648193, -0.2720581591129303, 0.29761117696762085, -0.009077604860067368, 0.9941425323486328, -0.6648485660552979, 0.4592379629611969, 0.8103467226028442, -0.45894014835357666, -0.3723156750202179, -0.2950942814350128, -0.41488927602767944, -0.29521188139915466, 0.03967928886413574, 0.9741984009742737, -0.48199865221977234, -0.17427663505077362, 0.9773625731468201, 0.3355443775653839, -0.7382693290710449, -0.5813612341880798, 0.08601102977991104, -0.04746308550238609, -0.8806275129318237, 0.42517340183258057, -0.2225359082221985, 0.06922899931669235, 0.25564515590667725, 0.5246319770812988, 1.2987946271896362, -0.35926079750061035, -0.3321547210216522, -0.07755366712808609, 0.15011514723300934, -0.24271216988563538, -0.47275790572166443, -0.20846949517726898, -1.5889530181884766, 0.024601800367236137, -0.658287763595581, 0.10629494488239288, -1.0888020992279053, -0.5727260708808899, -0.07605349272489548, -0.4166984260082245, -0.03176211565732956, 0.30870816111564636, -0.8388550877571106, -0.6946173310279846, -0.7191411852836609, -0.6278253197669983, 0.2131926417350769, 0.40121665596961975, -0.3304547369480133, 0.3223567605018616, 0.14291533827781677, 0.24525126814842224, 0.1325165182352066, 0.36980417370796204, -0.27481967210769653, -1.1085847616195679, -1.5189650058746338, 0.7277225255966187, 0.3028095066547394, -0.1858915090560913, -0.41265669465065, 0.732390820980072, -0.18037234246730804, -0.019125351682305336, -0.20172449946403503, 0.7848709225654602, -0.9996679425239563, -0.45563143491744995, 0.3539181649684906, -1.4576561450958252, 0.21935074031352997, 0.41919901967048645, -0.12980185449123383, -0.27722805738449097, 0.38635820150375366, -0.12022847682237625, -1.20392644405365, -1.1138421297073364, 0.2948229908943176, -0.7564405798912048, -0.09426681697368622, -0.07746776193380356, -0.22592219710350037, -0.8912156224250793, -0.4007655680179596, 0.08652570098638535, 0.4714239537715912, -0.32143962383270264, 0.9396362900733948, 0.9617452025413513, -0.6464901566505432, 0.19976848363876343, 0.3914724886417389, 0.12675799429416656, 0.105134017765522, 0.6337003707885742, 0.19987599551677704, -0.22600482404232025, 0.9366474747657776, 0.5084704160690308, 0.4948613941669464, -1.3427797555923462, 0.243733212351799, 0.7274805903434753, -0.15589599311351776, 0.010740936733782291, 1.19943368434906, -0.04812106862664223, -0.8568369746208191, 0.6144075989723206, -1.5120551586151123, -0.8431583046913147, -0.44584885239601135, 1.0628581047058105, -0.00670289620757103, -0.36225569248199463, 0.22260881960391998, -0.4330989122390747, 0.14894630014896393, 0.05136739835143089, -0.6759097576141357, 0.2686143219470978, -0.688010036945343, -0.4893783926963806, 1.0589646100997925, 0.6457188129425049, -0.33729901909828186, -0.9161441326141357, -0.7095611095428467, -0.13972891867160797, -0.35742220282554626, 0.2187957763671875, -0.5940982103347778, -0.10235918313264847, 0.8970832228660583, 0.15880292654037476, 0.009852001443505287, -0.28153103590011597, 0.018491070717573166, 0.01844223588705063, 0.898219883441925, 0.23751042783260345, -0.3049106001853943, -0.6943934559822083, 1.2482969760894775, 1.509956955909729, -0.9459020495414734, 0.02918301895260811, -0.19453223049640656, -0.445314884185791, 1.032556414604187, 0.7271991968154907, 0.5560653209686279, 0.7845929265022278, -0.11366082727909088, 0.2982231378555298, 0.4313596785068512, -1.4301398992538452, 0.333283394575119, 0.42299994826316833, 0.809734046459198, 0.9203361868858337, 0.1706465780735016, 0.2951032221317291, 0.9433791041374207, 0.2533873915672302, 0.11346776783466339, 0.5383988618850708, 0.6995670199394226, -0.40363579988479614, -0.12076089531183243, 0.021724874153733253, 0.4181378483772278, -0.5388303995132446, -1.0171138048171997, 0.18854406476020813, 0.6615126132965088, 0.26173126697540283, 0.9154229760169983, 0.8232017755508423, 0.4028836488723755, -0.039574746042490005, 0.633173942565918, 0.6150525212287903, -0.9045640230178833, -0.13033480942249298, -0.2405838668346405, -0.20863087475299835, -0.179132878780365, -0.04992895573377609, -0.4815094769001007, -0.5319172739982605, 0.006945671048015356, 0.33889076113700867, -0.22142021358013153, 0.0909799188375473, 1.0670366287231445, 0.4375959634780884, 0.25897058844566345, -0.46941256523132324, 0.16585488617420197, -0.6018684506416321, -1.032736897468567, 0.14464397728443146, -0.7851484417915344, -0.14205482602119446, 0.249633327126503, -0.045211706310510635, -0.16355539858341217]}, "authors": [{"authorId": "2108048327", "name": "Xindi Wang"}, {"authorId": "1904419", "name": "Mahsa Salmani"}, {"authorId": "2282534833", "name": "Parsa Omidi"}, {"authorId": "2283447900", "name": "Xiangyu Ren"}, {"authorId": "2066076226", "name": "Mehdi Rezagholizadeh"}, {"authorId": "50782111", "name": "A. Eshaghi"}], "references": [{"paperId": "00e18c603e60d861c4e99c541e4d65ef442d5945", "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory"}, {"paperId": "5851121df5ce46be5faea265c868ec0beabfce96", "title": "Efficient Large Language Models: A Survey"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "4ea5ca620122e6a9a2b000444d36491cebf49c7c", "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"}, {"paperId": "4d76206515d6b33903937474273885476fc2771e", "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs"}, {"paperId": "6d8896632ca2af8310273f6774e1dfb3140770f7", "title": "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"}, {"paperId": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0", "title": "CLEX: Continuous Length Extrapolation for Large Language Models"}, {"paperId": "908dad62c0e43d80e3e3cb3c0402f7c71c70499c", "title": "MemGPT: Towards LLMs as Operating Systems"}, {"paperId": "4c0428917aeee6aa7bd434f337d039f35996b736", "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "b12541867632737e826b7b01c7fbe1c4222d8655", "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models"}, {"paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80", "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "0456cd227edb95e596e3915ebcfd1133bcc8d725", "title": "Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729", "title": "A Comprehensive Overview of Large Language Models"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"paperId": "aa44b28b7c4c4a56d1f59ab4669215b667822c25", "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "017010b941d902a467f6d329ae5e74fd67e67912", "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "52045d4d4ae305aebb9e92fbbcf23104242c4d31", "title": "A Study on ReLU and Softmax in Transformer"}, {"paperId": "fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0", "title": "A Survey on Efficient Training of Transformers"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "980e55d9226cac302d0fae7732da4e67b8bc952c", "title": "Parallel Context Windows for Large Language Models"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "a7ca1bce0af7fe4703f5c3296db2dcc8dc112f20", "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference"}, {"paperId": "eecb45aa040064cbc0b37fd100706c02e7dc880e", "title": "Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "1966c4df2cda0fb8daf7f36366d909a021b6d5c1", "title": "SimA: Simple Softmax-free Attention for Vision Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "27be9c039bdafb058741fa1fdd2764def4721541", "title": "DeepTensor: Low-Rank Tensor Decomposition with Deep Network Priors"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f", "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "db46b0de44c5113c47f0ec5392eb91d0726497bf", "title": "A Simple and Effective Positional Encoding for Transformers"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "49e5b09480189fc9b2316a54f9d1e55cf0097c8b", "title": "Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d0d3f4d1003db0fb637519ef5d8bb140e7df8355", "title": "May the source be with you."}, {"paperId": "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8", "title": "Blockwise Parallel Transformers for Large Context Models"}, {"paperId": "343d24c4dcfaff2132373d218561a23fbd53e934", "title": "OWQ: Lessons learned from activation outliers for weight quantization in large language models"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": null, "title": "Flash-Decoding for long-context"}, {"paperId": null, "title": "Hun-gry Hungry Hippos: Towards language modeling with state space models"}]}