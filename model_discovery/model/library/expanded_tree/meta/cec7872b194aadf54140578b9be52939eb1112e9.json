{"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "abstract": "We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.", "venue": "International Conference on Learning Representations", "year": 2021, "citationCount": 162, "influentialCitationCount": 11, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, C OCO object detection and COCO instance segmentation, while being more computationally efficient."}, "embedding": {"model": "specter_v2", "vector": [0.1689475178718567, 0.38010478019714355, -0.43250131607055664, -0.01412552036345005, 0.18441705405712128, 0.26869338750839233, 0.6539462208747864, 0.008695710450410843, -0.7714000344276428, -0.41040241718292236, 0.4427356421947479, 0.7862643599510193, 0.7317257523536682, -0.12379982322454453, -0.34409984946250916, 0.11909201741218567, -0.8894413113594055, -0.3417870104312897, 0.8268879055976868, -0.21420317888259888, 0.36117687821388245, -0.43720653653144836, -1.4497830867767334, 0.545383095741272, -0.0859040766954422, 0.9431689977645874, 0.28770512342453003, 1.1521830558776855, -0.24453075230121613, 0.6054822206497192, 0.32289958000183105, 0.11912143975496292, 0.07540512084960938, 0.2176736742258072, -0.15232713520526886, -0.2847461402416229, 0.48658323287963867, -0.05596751347184181, -0.5872181057929993, 0.839706540107727, -0.2677736282348633, 0.24869123101234436, 0.2592524588108063, -0.3001808822154999, -0.08894767612218857, 0.7043759226799011, 0.48270681500434875, 0.8987202048301697, -0.4852266311645508, -0.448199987411499, 1.2637232542037964, -1.2054545879364014, 0.4400266706943512, 1.6728880405426025, 0.7538090348243713, 0.11426328867673874, -0.37327271699905396, -0.29390108585357666, 0.9684987664222717, 0.16602449119091034, -0.6564644575119019, -0.4436381459236145, 0.003642867784947157, -0.32772520184516907, 1.8536454439163208, -0.5353178381919861, -0.042727191001176834, 0.38279855251312256, 0.38633275032043457, 1.468991994857788, 0.10842432081699371, -0.9484052062034607, -0.07785990834236145, 0.18440841138362885, 0.5553341507911682, 0.3700687885284424, -0.351015567779541, 0.3222244381904602, -0.7864806056022644, 0.14511774480342865, 0.3435978293418884, 0.18397414684295654, 0.6344889998435974, -0.09105703979730606, -0.46751782298088074, 0.3852979242801666, 1.144803524017334, 0.6127175688743591, -0.4759097397327423, 1.3271516561508179, 0.4597708582878113, 0.032595012336969376, -0.018906187266111374, 0.3078215420246124, 0.3588084578514099, 0.6787668466567993, -0.5280010104179382, 0.057099126279354095, -0.058090753853321075, 1.0271763801574707, 0.12400023639202118, 0.05275340378284454, -0.3237611651420593, 0.2443649172782898, 1.1023811101913452, -0.09825009107589722, 0.10432808101177216, -0.6282179355621338, -0.059219248592853546, -0.5270394682884216, -0.20991387963294983, -1.1329821348190308, -0.0029467518907040358, -0.5841071605682373, -0.3912459909915924, -0.38357484340667725, -0.353831946849823, 0.22613580524921417, -0.5761498808860779, 0.26903441548347473, -0.21136920154094696, 0.1854061335325241, -0.008879539556801319, 0.40589818358421326, 0.29830607771873474, 0.685543417930603, 0.6233094334602356, 0.6883097290992737, 1.129390001296997, -1.3752508163452148, -0.2553478479385376, -1.0418683290481567, 0.0853879451751709, -0.1489163637161255, 0.025706887245178223, -0.5047563910484314, -0.7499213814735413, -1.394970417022705, -1.0370839834213257, -0.08781974017620087, -1.2267811298370361, 0.35932445526123047, 1.0003165006637573, 0.036919910460710526, -1.0085253715515137, 0.8715017437934875, -0.3871414065361023, -0.2427934855222702, 0.8899065852165222, 0.28496089577674866, 0.5355059504508972, 0.0699300467967987, -0.9849404692649841, 0.030617605894804, 0.418161004781723, -0.28680604696273804, -0.3312153220176697, -0.3170166611671448, -0.8411706686019897, 0.07813182473182678, 0.6586406826972961, -0.34096571803092957, 1.017421841621399, -0.7461728453636169, -0.5806660056114197, 0.5808565020561218, -0.04292993247509003, -0.06898098438978195, 0.20550383627414703, -0.3666505515575409, -0.4248279929161072, -0.1330077201128006, -0.14343410730361938, 0.9568048119544983, 0.69544517993927, -0.5053807497024536, -0.5104377865791321, -0.0340634249150753, 0.06654416024684906, -0.16364578902721405, -0.35136985778808594, 0.9503248929977417, -0.8115569949150085, -0.508909285068512, 0.17099986970424652, 0.5011987090110779, -0.2429112046957016, 0.18941333889961243, -0.3435567319393158, -0.9027412533760071, 1.0236858129501343, 0.6658951640129089, 0.5224546790122986, -1.1919395923614502, -1.0655664205551147, -0.19472309947013855, 0.13638880848884583, -0.19752441346645355, -0.7283692955970764, 0.040400147438049316, -0.27989086508750916, 0.2799161672592163, 0.1265481561422348, -0.9281394481658936, -0.23763608932495117, -0.3173817992210388, -0.3640138506889343, -0.04928887262940407, 0.27166786789894104, 1.3166885375976562, -1.085208773612976, -0.20926959812641144, -0.032220758497714996, 0.5267378091812134, -0.6967953443527222, 0.8776508569717407, -0.8811320066452026, -0.35238125920295715, -0.3536401689052582, 0.06463451683521271, -0.14087222516536713, -0.32915985584259033, 0.36202335357666016, -0.5930173397064209, -0.21321174502372742, 0.37977686524391174, -0.09142275154590607, 0.9746529459953308, -0.23750518262386322, 1.0386226177215576, 0.2352677434682846, -0.40643930435180664, -0.05170004442334175, -0.10366759449243546, -0.1641983538866043, -0.5512693524360657, 0.45859336853027344, -0.18551884591579437, -0.8928231000900269, 0.22774352133274078, 0.6319319009780884, 1.4138528108596802, -0.37863680720329285, 0.20278224349021912, 0.46713608503341675, 0.2139599621295929, 0.10080866515636444, 0.4236767888069153, 0.6324762105941772, 0.8967469334602356, 0.331382155418396, -0.4331551492214203, -0.006290482822805643, -0.7101775407791138, 0.06880135089159012, 0.8574332594871521, 0.3973159193992615, 0.843400776386261, 0.6963579654693604, -0.9484261274337769, -0.6482446789741516, 0.4934026300907135, 0.5047719478607178, 1.6981278657913208, 0.23156961798667908, 0.31142958998680115, -0.8785015344619751, -0.7007249593734741, -0.2888757884502411, -0.3940602242946625, -0.8119500279426575, 0.2600197494029999, -0.6816467642784119, -1.1175804138183594, 0.7803464531898499, 0.7039704918861389, 1.2221431732177734, -1.1887134313583374, -0.7659565210342407, -0.08990935236215591, 0.7217921018600464, -1.0671874284744263, -0.6199889779090881, 0.5138161182403564, -0.28963980078697205, -0.3261899948120117, 0.3804000914096832, -0.27554142475128174, 0.5212403535842896, -0.5339161157608032, 1.046589732170105, -0.35168755054473877, -0.7382181882858276, 0.25593769550323486, 0.36793556809425354, -0.7855510115623474, -0.20289158821105957, 0.2942769527435303, -0.041539717465639114, 0.12287934124469757, 0.384613573551178, 0.38147255778312683, -0.4080488383769989, 0.3666420876979828, -0.362417072057724, -0.004361569881439209, 0.3924751877784729, -0.11803694814443588, 0.9257908463478088, -0.15263107419013977, 0.3616926074028015, -1.5365878343582153, 0.2185680866241455, -0.075304314494133, -0.5149970054626465, 0.07371836155653, -0.4980250895023346, -0.40196457505226135, 0.42480114102363586, -0.671353816986084, -0.4016149938106537, -0.43450474739074707, 0.36478373408317566, -0.46460768580436707, -0.7196722030639648, -0.1612495332956314, 0.3680996000766754, -0.21388274431228638, 0.8064401745796204, -0.13003478944301605, 0.18925537168979645, 0.0677347332239151, 0.4073164761066437, -0.9259653687477112, 1.0035223960876465, -0.0010457969037815928, -0.25583386421203613, 0.43155351281166077, -0.15928666293621063, -0.7961300015449524, -0.4740937650203705, -0.7556193470954895, -0.7023255825042725, -0.590505063533783, 0.6934019923210144, -0.28846484422683716, -1.0324681997299194, -0.1649782806634903, -0.8804904818534851, -0.4756138324737549, 0.08916313946247101, -0.3944353461265564, 0.10568410158157349, -1.218610167503357, -1.0595624446868896, -0.5765765905380249, -0.4075021743774414, -0.918093204498291, -0.009121379815042019, 0.529936671257019, 0.02127227559685707, -0.6459656953811646, -0.5086174607276917, -0.6208268404006958, 1.1801079511642456, -0.012896370142698288, 0.25896507501602173, -0.016870828345417976, -0.8046925663948059, -0.3311183750629425, -0.3645970821380615, 0.49323123693466187, -0.44243183732032776, 0.20366798341274261, -1.2152633666992188, 0.482534795999527, -0.5036905407905579, -0.42416173219680786, 0.6523775458335876, 0.1999109387397766, 0.9825646877288818, 0.25297024846076965, -0.7364081740379333, 0.22710749506950378, 1.6567875146865845, -0.7339503169059753, 0.1707868129014969, 0.1875324249267578, 0.7990469336509705, 0.342329204082489, -0.2423858642578125, 0.28385016322135925, 0.21211673319339752, 0.203449547290802, 0.7804550528526306, -0.4177194833755493, -0.6972826719284058, -0.5169114470481873, 0.013749844394624233, 0.7676752209663391, 0.23497039079666138, 0.2319296896457672, -0.8159151077270508, 0.8121073842048645, -1.3297721147537231, -0.7088526487350464, 0.7913152575492859, 0.5460779666900635, -0.2699517607688904, -0.06722160428762436, -0.3742726147174835, -0.8390400409698486, 0.8870781660079956, 0.6988849639892578, -0.7474700212478638, -0.23085452616214752, -0.2658151090145111, 0.11780063807964325, 0.40244635939598083, 0.6671628952026367, -0.9612770676612854, 0.8070642948150635, 14.82478141784668, 0.575802206993103, -0.26734238862991333, 0.5173265337944031, 1.0372034311294556, 0.43638163805007935, 0.019032472744584084, 0.1095384955406189, -1.4665257930755615, -0.2001439481973648, 0.8433803915977478, 0.8765567541122437, 0.4618309438228607, 0.4802258610725403, -0.16270053386688232, -0.170737624168396, -0.46413153409957886, 0.5246207118034363, 0.6925231218338013, -1.513110876083374, -0.2993088364601135, 0.07075121998786926, 0.41192659735679626, 0.8704461455345154, 0.8329736590385437, 0.7302878499031067, 0.5271545648574829, -0.25041472911834717, 0.47084692120552063, 0.4153449237346649, 0.917864978313446, 0.3742285668849945, 0.15175184607505798, 0.24017611145973206, -1.1109260320663452, -0.5273002982139587, -0.8035058379173279, -0.8175742030143738, -0.24190694093704224, 0.013778741471469402, -0.3971179723739624, -0.7250362038612366, 0.28480690717697144, 0.7684474587440491, -0.27855226397514343, 0.38159874081611633, -0.10618870705366135, 0.13625799119472504, -0.062029629945755005, -0.22906659543514252, 0.4687632918357849, 0.8270915746688843, 0.19653065502643585, 0.4608481228351593, -0.5581094622612, 0.3848598003387451, 0.15014569461345673, 0.7788928747177124, -0.7783848643302917, -0.22834640741348267, -0.18804161250591278, -0.01957104168832302, -0.38653329014778137, 1.09255051612854, -0.074598029255867, 0.08233474940061569, -0.36788302659988403, 0.2009558379650116, 0.2722376883029938, 0.47970816493034363, -0.222246453166008, -0.3827712833881378, 0.24161548912525177, -0.12411200255155563, 0.2030278891324997, 0.5620928406715393, -0.35023626685142517, -0.35166919231414795, -0.8331328630447388, 0.16323527693748474, 0.4274531900882721, -0.8004007339477539, -0.5693028569221497, 1.204511046409607, -0.29639938473701477, -0.14845818281173706, 0.6239928603172302, -1.1392080783843994, -0.46314650774002075, 0.2539639472961426, -1.3807220458984375, -0.605349063873291, -0.12368161976337433, -0.61298006772995, -0.19058914482593536, 0.09343746304512024, 0.9310968518257141, 0.02672356553375721, -0.0701049193739891, -0.25478899478912354, -0.2131112515926361, 0.3126031458377838, -0.21295998990535736, -0.8777990341186523, 0.6365147829055786, 0.3066868782043457, -0.3337865173816681, 0.012454342097043991, -0.23215648531913757, 0.0418209545314312, -0.2413635104894638, -0.06623883545398712, 0.3251045346260071, -0.7538527846336365, -0.6623234748840332, -0.8855411410331726, -0.765296995639801, 0.2947297990322113, 0.7897903919219971, 0.4493660032749176, 0.05853477492928505, 0.1512681543827057, -0.7714277505874634, -0.4237082600593567, -0.7843286395072937, 0.014477918855845928, 0.24908475577831268, -0.8033460974693298, -0.30381283164024353, -0.12436741590499878, 0.24064286053180695, -0.7848170399665833, -0.36951395869255066, -0.19532319903373718, 0.3926835358142853, -0.4724523723125458, 1.2579413652420044, -0.2460143268108368, 0.6144579648971558, 0.47452035546302795, -0.08960428833961487, -0.35759666562080383, 0.11665008962154388, -0.6707366108894348, 0.1482180655002594, -0.07659916579723358, 0.4717569649219513, -0.29979851841926575, 0.6120954155921936, 0.6785605549812317, 0.22075901925563812, -0.5868254899978638, -0.2679698169231415, 0.12878108024597168, -0.17839285731315613, -0.7549461126327515, 0.16712597012519836, 0.071372851729393, -0.16318072378635406, -0.1818029284477234, 0.7914842367172241, 0.699043869972229, 0.2651943564414978, -0.49609139561653137, 0.1407407522201538, -0.029941318556666374, -0.17595483362674713, -0.6517663598060608, -0.9268728494644165, -1.5413382053375244, -0.06597278267145157, -0.7025582790374756, 0.04858556389808655, -0.7047516703605652, -0.5479341745376587, -0.24866726994514465, -0.697071373462677, 0.26393309235572815, 0.31225043535232544, 0.09016447514295578, -0.4309958815574646, -0.503923237323761, -1.1491990089416504, 0.4243931174278259, 0.7344667911529541, -0.5442403554916382, -0.04305131360888481, 0.10553823411464691, -0.012483014725148678, 0.5779558420181274, 0.28520119190216064, -0.41077977418899536, -0.4854919910430908, -1.0297061204910278, 0.527977705001831, -0.2669493854045868, 0.26107072830200195, -1.4160934686660767, 1.319319725036621, 0.5608146786689758, 0.31140315532684326, -0.16306228935718536, 0.4492533206939697, -0.9253454208374023, -0.8243627548217773, 0.13336390256881714, -1.0426928997039795, -0.14744700491428375, 0.19690854847431183, -0.10693004727363586, -0.25102096796035767, 0.8500514030456543, 0.21307724714279175, -1.203894853591919, -1.1838290691375732, 0.27206146717071533, -0.3961236774921417, -0.11484032869338989, 0.06549789011478424, -0.3015300929546356, -1.3657268285751343, -0.0944925844669342, -0.5450125932693481, 0.48633429408073425, -0.5577093362808228, 1.0482757091522217, 0.4293537735939026, -0.9621531367301941, 0.26354122161865234, 0.28905248641967773, -0.2664260268211365, 0.48124560713768005, 0.7352728843688965, 0.3836904466152191, -0.6109798550605774, 0.48310574889183044, 0.06389109045267105, 0.09573434293270111, -0.582679808139801, 0.3324306011199951, 1.0183531045913696, 0.010734702460467815, -0.26296597719192505, 0.9253582954406738, 0.001940643647685647, -0.9340121746063232, 0.43432727456092834, -1.0902599096298218, -0.7598267793655396, -0.06445217877626419, 0.5744108557701111, 0.1701960265636444, -0.28008386492729187, -0.25903409719467163, -0.47141680121421814, 0.30459704995155334, -0.15228892862796783, -0.44422784447669983, 0.5055767893791199, -0.15292254090309143, -0.3666244447231293, 0.4886961281299591, 0.7238392233848572, -0.591962456703186, -1.3773553371429443, -1.2704166173934937, -0.38336631655693054, -0.28583061695098877, 0.35216253995895386, -0.3033396303653717, -0.6046524047851562, 0.8401369452476501, 0.6359959840774536, 0.5753785371780396, 0.037089165300130844, 0.10859325528144836, -0.2412099540233612, 0.7465680837631226, -0.39264726638793945, -0.8836472630500793, 0.0026639297138899565, 0.9787468910217285, 1.2344502210617065, -0.5784872174263, 0.2853213846683502, -0.36810797452926636, -0.5600280165672302, 1.0087641477584839, 0.7544993758201599, -0.4808802604675293, 1.3299400806427002, -0.11076334863901138, 0.28002995252609253, 0.17369554936885834, -0.9960852265357971, -0.2314070612192154, 0.5933433175086975, 0.9634130597114563, 0.27757933735847473, 0.13904809951782227, 0.5787968635559082, 0.5225428342819214, 0.6161506772041321, -0.2803196609020233, 0.2943919599056244, 0.5077230930328369, -0.2962827682495117, 0.1755058616399765, -0.12657295167446136, 0.5423508882522583, -0.9490277767181396, -0.11410300433635712, 0.1914559155702591, 0.5351468920707703, 0.651941180229187, 0.468225359916687, 1.0414155721664429, 0.06056876480579376, 0.4401373565196991, -0.1274975836277008, 0.0005167091730982065, -0.49234405159950256, -0.45040419697761536, 0.01818890869617462, -0.7764565348625183, -0.2955007255077362, -0.2959437966346741, -0.7227280139923096, -0.41626492142677307, -0.06828271597623825, 0.20046591758728027, -0.4223019778728485, 0.6234471797943115, 0.7332671284675598, 0.6148388385772705, 1.0782843828201294, -0.1823154240846634, -0.8407056331634521, -0.07317326217889786, -0.7515427470207214, 0.3175397217273712, -0.7415875792503357, -0.04371945559978485, -0.03960295394062996, 0.00932520441710949, -0.12182756513357162]}, "authors": [{"authorId": "4689792", "name": "Irwan Bello"}], "references": [{"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b", "title": "Global Self-Attention Networks for Image Recognition"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "5156381d63bb3e873533b08f203cb56c2d79b6c9", "title": "Object-Centric Learning with Slot Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "a0185d4f32dde88aa1749f3a8000ed4721787b65", "title": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33", "title": "Exploring Self-Attention for Image Recognition"}, {"paperId": "fb93ca1e004cbdcb93c8ffc57357189fa4eb6770", "title": "ResNeSt: Split-Attention Networks"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "5f69762f2ec3eecb6011d14f024ef0dec3eba23c", "title": "TResNet: High Performance GPU-Dedicated Architecture"}, {"paperId": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"}, {"paperId": "d34aaf35f433dbb51681a91a2eed37a3926346e9", "title": "Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "bc51622358d8eea83248ef29402fe10640d07ba6", "title": "Big Transfer (BiT): General Visual Representation Learning"}, {"paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d", "title": "Self-Training With Noisy Student Improves ImageNet Classification"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae", "title": "Searching for MobileNetV3"}, {"paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "title": "Local Relation Networks for Image Recognition"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "29309743870c825f9645a4803af727402462e513", "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks"}, {"paperId": "b7339c1deeb617c894cc08c92ed8c2d4ab14b4b5", "title": "A2-Nets: Double Attention Networks"}, {"paperId": "cd8ddaaf56e38dddafdeac3f9643b9b5e9d35d54", "title": "Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks"}, {"paperId": "22aab110058ebbd198edb1f1e7b4f69fb13c0613", "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis"}, {"paperId": "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2", "title": "CBAM: Convolutional Block Attention Module"}, {"paperId": "10bb4ef7a6719ea132e00f0ab5680919a4131d99", "title": "BAM: Bottleneck Attention Module"}, {"paperId": "bff271886a64964a0a3f4ccdb6e0abb85abfbea0", "title": "Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention"}, {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "6a0aaefce8a27a8727d896fa444ba27558b2d381", "title": "Relation Networks for Object Detection"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7", "title": "FiLM: Visual Reasoning with a General Conditioning Layer"}, {"paperId": "87fc28cbb193a3bc100e13a4a57a8dc9ce7e31a3", "title": "Efficient Attention using a Fixed-Size Memory Representation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "d7878c2044fb699e0ce0cad83e411824b1499dc8", "title": "Neural Combinatorial Optimization with Reinforcement Learning"}, {"paperId": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "563783de03452683a9206e85fe6d661714436686", "title": "HyperNetworks"}, {"paperId": "53839f5d018e063f1c2b6b1bd391352702ca34c1", "title": "A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size Representations"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "9653d5c2c7844347343d073bbedd96e05d52f69b", "title": "Pointer Networks"}, {"paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "bd5fc28c7356915ec71abafbe86b7596c60720aa", "title": "The conference paper"}, {"paperId": "039ad1ad259a9bd98e24b0738ba048282188d184", "title": "The GPU Computing Era"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": null, "title": "Revisiting resnets: Improved training methodologies and scaling rules"}, {"paperId": "378f559dc5aff6f2813fdf866182903e73a553ad", "title": "A Training details"}, {"paperId": null, "title": "LambdaResNets use the block allocations from He et al"}, {"paperId": null, "title": "We use the same dataset of 130M filtered and balanced JFT images"}, {"paperId": null, "title": "2020); object detection and object-centric tasks (Wang et al., 2018"}, {"paperId": null, "title": "Global relative attention replaces convolutions at low"}, {"paperId": null, "title": "2021), the weiht decay is reduced to 4e-5"}, {"paperId": null, "title": "Multiple choices for the feature function"}, {"paperId": null, "title": "2018) and additionally replace the max pooling layer in the stem by a strided"}, {"paperId": null, "title": "They are recently enjoying a resurgence of popularity with many works modifying the popular Transformer architecture for sequential processing applications (Katharopoulos et al., 2020"}, {"paperId": null, "title": "Block configurations and lambda layers placement of LambdaResNets in the Pareto curves"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "What if my task doesn\u2019t require position-based interactions? Computational costs in the lambda layer are dominated by position-based interactions"}]}