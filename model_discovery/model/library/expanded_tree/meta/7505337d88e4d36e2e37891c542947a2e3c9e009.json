{"paperId": "7505337d88e4d36e2e37891c542947a2e3c9e009", "abstract": "Quantizing the activation, weight, and gradient to 4-bit is promising to accelerate neural network training. However, existing 4-bit training methods require custom numerical formats which are not supported by contemporary hardware. In this work, we propose a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic. Training with an ultra-low INT4 precision is challenging. To achieve this, we carefully analyze the specific structures of activation and gradients in transformers to propose dedicated quantizers for them. For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers. For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. Our algorithm achieves competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and image classification. Unlike previous 4-bit training methods, our algorithm can be implemented on the current generation of GPUs. Our prototypical linear operator implementation is up to 2.2 times faster than the FP16 counterparts and speeds up the training by up to 35.1%.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 14, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.11987", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a training method for transformers with all matrix multiplications implemented with the INT4 arithmetic to achieve competitive accuracy on a wide range of tasks including natural language understanding, machine translation, and image classification."}, "embedding": {"model": "specter_v2", "vector": [0.23380699753761292, 0.45893245935440063, -0.5650150179862976, -0.20320819318294525, 0.2520643472671509, 0.24862124025821686, 0.3454044759273529, -0.10791905224323273, -0.7389875054359436, -0.5328526496887207, 0.32058024406433105, -0.1327461302280426, 0.7954752445220947, -0.28292638063430786, -0.32196351885795593, 0.13530848920345306, -1.1718242168426514, -0.2378433644771576, -0.008549829944968224, -0.16651350259780884, 0.09711454063653946, -0.3067921996116638, -0.8525903820991516, 0.6917533278465271, 0.2616695761680603, 1.294345736503601, -0.4149031341075897, 0.7592057585716248, -0.8791601657867432, 0.5554108023643494, 0.25488802790641785, -0.4442840814590454, 0.5450707674026489, 0.23219890892505646, -0.5220416784286499, -0.26452475786209106, 0.6576122045516968, -0.40620556473731995, -0.3605956435203552, 1.1607974767684937, -0.36897963285446167, 0.23777613043785095, 0.21485833823680878, -0.6072964072227478, -0.3946482539176941, 0.8590234518051147, 0.34682223200798035, 0.6868042349815369, -0.715618371963501, -0.5499051213264465, 1.2112581729888916, -1.4494879245758057, -0.12483832240104675, 1.1854839324951172, 0.6927926540374756, 0.055996619164943695, -0.14962205290794373, -0.553932249546051, 0.32521912455558777, 0.43107888102531433, -0.6711598634719849, -0.150307759642601, -0.2445337027311325, 0.08953747898340225, 1.602892518043518, -0.1438933163881302, 0.5048009157180786, 0.35585203766822815, 0.13846468925476074, 1.3428936004638672, 0.2886188328266144, -0.8415873050689697, 0.00047792651457712054, -0.17560921609401703, 0.20935286581516266, 0.9935375452041626, -0.33906984329223633, -0.010678098537027836, -1.5439021587371826, 0.11220280826091766, 0.542566180229187, 0.07435088604688644, 0.28626811504364014, -0.18549694120883942, -0.6815577149391174, 0.6311998963356018, 0.9669238924980164, 0.6763826608657837, -0.422476589679718, 1.0606484413146973, 0.7547968626022339, 0.43999582529067993, -0.03086049109697342, 0.061147019267082214, 0.0520949587225914, 0.48560652136802673, -0.9434278607368469, -0.050620436668395996, -0.3306816518306732, 0.66326904296875, -0.03608006238937378, 0.5471704006195068, -0.6761971712112427, 0.12976022064685822, 0.8878772854804993, 0.08133291453123093, 0.44638291001319885, -0.7020326256752014, 0.20252011716365814, -0.44120752811431885, -0.17608000338077545, -0.41198885440826416, 0.004784901160746813, -0.601483941078186, -1.0932484865188599, -0.5715605020523071, -0.394315630197525, 0.07393131405115128, -1.3546154499053955, 0.3847355842590332, -0.4753776490688324, 0.38303133845329285, -0.2109074741601944, 0.5051240921020508, 0.256265252828598, 0.8982434868812561, 0.055398404598236084, 0.3880501687526703, 1.0269994735717773, -0.977705717086792, -0.4257493019104004, -0.8938615322113037, 0.6363692879676819, -0.23578381538391113, 0.12207550555467606, 0.235741525888443, -1.39246666431427, -1.0883718729019165, -0.9538591504096985, -0.135391503572464, -0.6446841955184937, 0.28811702132225037, 1.1556955575942993, 0.4812126159667969, -1.326552391052246, 1.0655845403671265, -0.29601579904556274, -0.058963239192962646, 0.6801628470420837, 0.6666359901428223, 0.18809759616851807, -0.27624285221099854, -1.0466595888137817, 0.39499756693840027, 0.301491379737854, -0.206984743475914, 0.16718211770057678, -0.6034697890281677, -0.8648644685745239, 0.04628665745258331, -0.12846025824546814, -0.33332791924476624, 0.9769148826599121, -0.15461842715740204, -1.1667976379394531, 0.5855275988578796, -0.20254257321357727, -0.378876268863678, -0.04191448166966438, -0.23016327619552612, -0.35018524527549744, -0.3304162621498108, -0.3949292004108429, 0.8663671016693115, 0.6954739689826965, 0.335523396730423, 0.00987073965370655, 0.06589815765619278, -0.3636876940727234, 0.03293555974960327, -0.7064920663833618, 0.9975875020027161, -0.30886149406433105, -0.4135820269584656, 0.964713990688324, 0.6250197291374207, -0.07337933033704758, -0.059675879776477814, -0.424906849861145, -0.7903676629066467, 0.48783618211746216, 0.4110293686389923, 0.5625503063201904, -1.042868733406067, -0.607697606086731, 0.2204519361257553, -0.13254135847091675, -0.12112525850534439, -0.393187552690506, 0.135470911860466, -0.5524132251739502, 0.4196624755859375, 0.025297001004219055, -0.800164520740509, -0.010316797532141209, -0.34486252069473267, -0.8906556367874146, 0.14242541790008545, 0.2450840324163437, 1.1436052322387695, -0.6302562952041626, 0.10770286619663239, -0.06762254238128662, 0.2538084387779236, -1.1187609434127808, 1.0154013633728027, -0.19475072622299194, -0.28003379702568054, 0.385831743478775, 0.079338438808918, 0.21377545595169067, -0.43206238746643066, 0.2758156955242157, -0.832097053527832, -0.3149622678756714, 0.6748478412628174, -0.39952147006988525, 1.6168147325515747, -0.08053285628557205, 0.6842803359031677, -0.07065118849277496, -0.7692523002624512, 0.17276309430599213, 0.33357828855514526, -0.02579323574900627, -0.2497042566537857, 0.4760943055152893, 0.487183153629303, -0.5769376754760742, 0.5668579936027527, 0.8961418867111206, 0.6742193102836609, -0.25037965178489685, 0.01919289492070675, 0.715724527835846, -0.3449653089046478, 0.17816346883773804, 0.14477823674678802, 0.43808379769325256, -0.002466177335008979, 0.2741018831729889, -0.022195395082235336, 0.20385394990444183, -0.9088718891143799, -0.04613107815384865, 0.5986695885658264, 0.2964252233505249, 0.8688610196113586, 0.46269574761390686, -0.9189820289611816, -0.3312141001224518, -0.1723487377166748, 0.4412241280078888, 1.5298477411270142, -0.16242757439613342, -0.15626008808612823, -0.5007529258728027, -0.14007119834423065, -0.1758957803249359, -0.3537600040435791, -0.39206358790397644, 0.05889078974723816, -0.5479066967964172, -1.192359447479248, 0.8981109857559204, 0.40232956409454346, 1.3085204362869263, -0.361245334148407, -0.47442826628685, -0.9353941082954407, 0.5527657270431519, -0.7256185412406921, -0.516907811164856, 0.8998705148696899, -0.8666617274284363, 0.46422913670539856, 0.03723790869116783, -0.2847832143306732, 0.413906067609787, -0.7896479964256287, 0.6512147784233093, -0.40961939096450806, -0.16047626733779907, -0.49348825216293335, 0.7484942078590393, -0.39724498987197876, -0.36056360602378845, 0.04169927537441254, 0.056976787745952606, -0.11500772833824158, 0.3896499574184418, 0.030192891135811806, -0.12226986885070801, -0.30781999230384827, -0.36174970865249634, 0.17694883048534393, 0.39718517661094666, 0.17462584376335144, 0.4826171100139618, -0.3558531701564789, -0.21667899191379547, -0.9547250866889954, 0.5999943614006042, 0.24955439567565918, 0.07020285725593567, -0.1385054886341095, -0.4512689411640167, -0.10983562469482422, 0.5276330709457397, -0.5338876247406006, 0.14244772493839264, -0.6963266134262085, 0.156929150223732, -0.5212962031364441, -0.00925517175346613, 0.03707287460565567, 0.8159574270248413, -0.3184886872768402, 0.7894173264503479, 0.11689230799674988, 0.29464778304100037, 0.13089731335639954, 0.7605000734329224, -0.8252283334732056, 0.8146385550498962, 0.22758908569812775, 0.27601298689842224, 0.1823316514492035, -0.0494871623814106, -0.5478373169898987, -0.1173485815525055, -0.3661331534385681, -0.09157705307006836, -0.16131682693958282, 0.3042271137237549, -0.36483317613601685, -0.8435844779014587, 0.09898897260427475, -1.252834439277649, -0.08875647187232971, -0.0868305191397667, -0.09720482677221298, -0.020804302766919136, -1.0874848365783691, -1.6283602714538574, -0.35244619846343994, -0.8877248764038086, -1.2568204402923584, 0.21446086466312408, 0.0005727838724851608, -0.32175734639167786, 0.008138169534504414, -0.680671215057373, -0.6819352507591248, 1.1575431823730469, -0.6362587213516235, 0.6317688822746277, 0.27892637252807617, -0.3471112847328186, -0.02542170137166977, -0.18271689116954803, 0.7087965607643127, -0.3716054856777191, 0.14641548693180084, -0.856815755367279, 0.4833173155784607, -0.2692806124687195, -0.6634114980697632, 0.5021190643310547, 0.143311008810997, 0.9754598736763, -0.11178924888372421, 0.025963326916098595, 0.7459838390350342, 1.068303108215332, -0.9164907932281494, 0.32846176624298096, 0.05606886371970177, 0.8708968758583069, -0.0936921089887619, -0.5399293303489685, 0.4505743682384491, -0.24595879018306732, 0.2634390890598297, 0.44931769371032715, -0.137376606464386, -0.13392813503742218, -0.1401498168706894, 0.6257706880569458, 1.3293743133544922, 0.4227932393550873, 0.4379297196865082, -0.7684717178344727, 0.49926817417144775, -0.6624524593353271, -0.6745346188545227, 0.4441389739513397, 0.6998652815818787, 0.45472583174705505, 0.1274602860212326, -0.43648219108581543, 0.18092843890190125, 0.2832096517086029, 0.7468457818031311, -0.17902827262878418, -1.1951348781585693, -0.03387916833162308, 0.972331166267395, 0.8806915879249573, 0.5077700018882751, -0.45688822865486145, 0.46529337763786316, 15.023791313171387, 0.6435856223106384, -0.7870211601257324, 0.3252270221710205, 0.8110965490341187, 0.11603574454784393, -0.25905728340148926, -0.24338622391223907, -0.8341314792633057, -0.08395285904407501, 0.9383463263511658, 0.5570401549339294, 0.6455469131469727, 0.32421159744262695, -0.17010290920734406, 0.3966902792453766, -0.4227256178855896, 0.8991547226905823, 0.4366145133972168, -1.6604382991790771, 0.17225518822669983, -0.06908876448869705, 0.4563482403755188, 0.6216192245483398, 1.0427135229110718, 0.49396318197250366, -0.006610123906284571, -0.7689293026924133, 0.6345563530921936, -0.12382733821868896, 1.1843422651290894, 0.2570340037345886, 0.23136548697948456, 0.16465073823928833, -0.7899701595306396, 0.032123345881700516, -0.6114749908447266, -1.2912243604660034, -0.026828475296497345, 0.39103013277053833, -1.056382656097412, -0.4572865962982178, -0.0745604932308197, 0.821824848651886, 0.24638313055038452, 0.45678701996803284, -0.2017793208360672, 0.6709874868392944, -0.40631240606307983, -0.03525512292981148, 0.34910982847213745, 0.16006223857402802, -0.10524653643369675, 0.1830201894044876, 0.28681063652038574, -0.3685493767261505, -0.057588547468185425, 0.5213652849197388, -0.805635929107666, 0.04805536940693855, 0.021340899169445038, -0.20213556289672852, -0.3506065607070923, 0.7463725209236145, 0.11805281043052673, 0.13862112164497375, -0.22302283346652985, 0.8165414333343506, 0.6591395735740662, 0.19023233652114868, -0.5085022449493408, -0.2414560317993164, 0.3638996481895447, -0.6245021224021912, 0.15836510062217712, 0.2817361354827881, -0.9700972437858582, -0.5288684368133545, -0.5617690682411194, -0.33930251002311707, 0.38307464122772217, -0.758354127407074, -0.23849111795425415, 0.7594478130340576, -0.5555101037025452, -0.3350035548210144, 0.6041563153266907, -1.1689246892929077, -0.27688825130462646, 0.5509253740310669, -1.3771470785140991, -0.3515376150608063, -0.04864048212766647, -0.44483041763305664, -0.48947909474372864, 0.13080650568008423, 1.25053870677948, 0.41955602169036865, -0.1225946843624115, -0.11313735693693161, -0.3570772409439087, -0.09349070489406586, -0.19597682356834412, -1.1033213138580322, 1.11727774143219, 0.47117018699645996, 0.11131075024604797, 0.5367889404296875, -0.279636412858963, 0.4603583812713623, -0.8255322575569153, -0.09803060442209244, 0.5804641246795654, -0.10151554644107819, -0.301872193813324, -0.7948635816574097, -0.8505734205245972, 0.3941781222820282, 0.5026613473892212, 0.09448912739753723, 0.2292104810476303, 0.21183738112449646, -0.8404785990715027, -0.4068939685821533, -0.289836049079895, 0.24472388625144958, 0.4235008656978607, -0.7056913375854492, -0.08008009195327759, -0.17762763798236847, -0.04888797178864479, -1.168289065361023, -0.38742610812187195, -0.02296295203268528, 0.07206064462661743, -0.5477204322814941, 1.3542765378952026, -0.4488219916820526, 0.7734319567680359, 0.5055426359176636, -0.017297498881816864, -0.7268047332763672, 0.09322123974561691, -0.8873100876808167, 0.22335265576839447, -0.5781608819961548, 0.4388348162174225, -0.29262658953666687, 1.1208230257034302, 0.23614192008972168, 0.024364417418837547, -0.6205470561981201, -0.6862893104553223, 0.06392452120780945, -0.24546471238136292, -0.9000101685523987, 0.08152368664741516, -0.17632435262203217, 0.0852963775396347, 0.09946660697460175, 0.31533268094062805, 0.279367595911026, -0.36109092831611633, -0.5778864622116089, 0.04694563150405884, 0.27655208110809326, -0.16972416639328003, -0.6559696197509766, -0.9355972409248352, -1.8670353889465332, -0.13224108517169952, -1.4912266731262207, -0.08755332976579666, -0.7100691199302673, -0.7074635028839111, -0.25529032945632935, -0.40393322706222534, 0.2773682773113251, 0.3239614963531494, 0.43428289890289307, -0.6118959188461304, -0.5769377946853638, -0.18799029290676117, 0.7624108791351318, 0.521996796131134, -0.7263040542602539, 0.271886944770813, -0.16797520220279694, 0.3236703872680664, 0.20650358498096466, 0.47726985812187195, -0.006848826538771391, -0.833206057548523, -1.3166351318359375, 0.4283736050128937, -0.4725501239299774, -0.012488012202084064, -1.1236813068389893, 0.7454327344894409, 0.3629753887653351, 0.09554639458656311, -0.10208119451999664, 0.6159272789955139, -0.6179249882698059, -0.6260111927986145, 0.5073474049568176, -0.7824535965919495, 0.22115343809127808, 0.2923896312713623, -0.8743520379066467, -0.15546555817127228, 0.53813636302948, 0.11735183000564575, -0.5490989089012146, -0.8770105242729187, 0.21243511140346527, -0.2305855005979538, 0.13228602707386017, -0.42806676030158997, -0.1266254484653473, -1.1994307041168213, -0.21636568009853363, 0.16253094375133514, -0.025658076629042625, -0.2693420946598053, 0.29565349221229553, 0.6419020295143127, -1.1678941249847412, 0.2474759817123413, 0.7058637142181396, -0.4688605070114136, 0.0527372807264328, 0.17575405538082123, 0.6395339369773865, -0.9359931349754333, 0.20142382383346558, -0.13189849257469177, 0.12295086681842804, -0.7131704092025757, -0.2883284389972687, 0.9028747081756592, -0.2593022286891937, -0.19099529087543488, 1.400985598564148, -0.7207704782485962, -0.7521237730979919, 0.24864667654037476, -1.6806553602218628, -0.21365615725517273, -0.5372807383537292, 0.382590651512146, 0.12668274343013763, 0.6854779124259949, 0.30456429719924927, -0.4552965760231018, -0.1958402544260025, 0.01841449737548828, -0.5847470164299011, 0.14256872236728668, 0.7400173544883728, -0.7038080096244812, 0.5456728935241699, 1.0721209049224854, -0.3800254464149475, -0.2830928862094879, -0.7420080304145813, -0.3223029673099518, -0.3705779016017914, 0.30475103855133057, -0.008765889331698418, -0.8334352970123291, 1.1494694948196411, 0.32947155833244324, 0.5155238509178162, 0.3128136098384857, -0.4119853377342224, 0.4769967198371887, 0.6422279477119446, -0.11955208331346512, -0.5045471787452698, -0.2926938831806183, 1.3075162172317505, 0.8266339898109436, -0.5285901427268982, 0.3914523720741272, -0.7758246660232544, -0.45581233501434326, 1.0325955152511597, 0.17751182615756989, -0.12049597501754761, 0.7706322073936462, -0.07843948900699615, 0.0780162662267685, 0.17381785809993744, -0.5629853010177612, -0.03326531872153282, 0.9417828917503357, 0.5510649085044861, 1.133760929107666, 0.21195398271083832, 0.22629719972610474, 0.5251725316047668, -0.2718353569507599, 0.03759758546948433, 0.14677557349205017, 0.5800889134407043, -0.1684710532426834, -0.11176185309886932, -0.1623179167509079, 0.7326818704605103, -0.7249782085418701, -0.9065369963645935, 0.46303287148475647, 0.26256173849105835, 0.4791375398635864, 0.1821899563074112, 1.014596939086914, -0.175770103931427, 0.6440491080284119, 0.16019420325756073, 0.6313387751579285, -0.5418972969055176, -0.5646225810050964, -0.5963172912597656, -0.7676327228546143, -0.18185195326805115, -0.04168591648340225, -0.40659472346305847, -0.6518150568008423, -0.4550964832305908, 0.5602246522903442, 0.011342895217239857, 0.37579238414764404, 0.6025540828704834, 0.49674978852272034, 0.5521337389945984, -0.3523792624473572, -0.4552665650844574, -0.7076711654663086, -0.830077052116394, 0.16195081174373627, -0.295251727104187, -0.20417772233486176, 0.09637964516878128, -0.026567617431282997, -0.2686198651790619]}, "authors": [{"authorId": "2220346402", "name": "Haocheng Xi"}, {"authorId": "2215360655", "name": "Changhao Li"}, {"authorId": "2276707", "name": "Jianfei Chen"}, {"authorId": "2155220672", "name": "Jun Zhu"}], "references": [{"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "9bf72f03a0fd82990c63831cc5f2cf9056cf2ab6", "title": "SQuAT: Sharpness- and Quantization-Aware Training for BERT"}, {"paperId": "3f6243097a58e386aea1215fed4f372dee07a100", "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "2e5aafa18a0034940bcc48e5bac5851df5627f61", "title": "How Do Adam and Training Strategies Help BNNs Optimization?"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c", "title": "Understanding deep learning (still) requires rethinking generalization"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "c6b5ecf84e304cda4dbd1687cf1902790d65cc6e", "title": "A Statistical Framework for Low-bitwidth Training of Deep Neural Networks"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "a2cd073b57be744533152202989228cb4122270a", "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "1fab639219c601ddaf73430249035c827fbc1d7f", "title": "ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions"}, {"paperId": "693cce5d9764f9e9e0c9c583bf840ac019e2179f", "title": "Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "962bdacb3941053788d69696826c9b4307652cce", "title": "Towards Unified INT8 Training for Convolutional Neural Network"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "0a5d987ddb5463062babceca90ba974db0cf96e7", "title": "HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks"}, {"paperId": "a5fdfb231e74986db783457a1af48c8d1817a902", "title": "Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "146c95362597f956de9fbab4d3598266f11d6d35", "title": "Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers"}, {"paperId": "d5096770ab37926e3921ef08ec2795fd895d2e06", "title": "Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks"}, {"paperId": "8864f4000a4a5a950761dbcdf56eab85b840f57f", "title": "Cheetah: Mixed Low-Precision Hardware & Software Co-Design Framework for DNNs on the Edge"}, {"paperId": "2171c4ed2ae6bd3a896bdf5c1a761e3efa9b2023", "title": "Deep Learning Training on the Edge with Low-Precision Posits"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "dc160709bbe528b506a37ead334f60d258413357", "title": "Learned Step Size Quantization"}, {"paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235", "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"}, {"paperId": "9a1093af92d315def21b90918faf08665157051a", "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"}, {"paperId": "a8e1b91b0940a539aca302fb4e5c1f098e4e3860", "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "fae2a5101789afd51c1ececb28c75537c88734ec", "title": "Scalable Methods for 8-bit Training of Neural Networks"}, {"paperId": "f603b1cc0d3f5297c54003709296a973eb72b20f", "title": "Faster Neural Network Training with Approximate Tensor Operations"}, {"paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "title": "A Call for Clarity in Reporting BLEU Scores"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "339f5523a0cdeea315dc5affc27379ca57398264", "title": "Training DNNs with Hybrid Block Floating Point"}, {"paperId": "49e60f82d6ae835c56473464f67ca5c11d3e95ec", "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks"}, {"paperId": "acdf151b8efc2c6b05662d69f27531afc557dc85", "title": "Training and Inference with Integers in Deep Neural Networks"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "407ead18083e68626e82e07db1a9289ff0b7e862", "title": "Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "3b529b765907a6df4234d8f2d1033e638ed44aed", "title": "LX. Thoughts on inverse orthogonal matrices, simultaneous signsuccessions, and tessellated pavements in two or more colours, with applications to Newton's rule, ornamental tile-work, and the theory of numbers"}, {"paperId": "9fae65e8c1af810dbdecfd9a8c6302d1145c3666", "title": "Logarithmic Unbiased Quantization: Practical 4-bit Training in Deep Learning"}, {"paperId": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9", "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc", "title": "Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Randnla: randomized numerical linear algebra"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "Transformer Engine"}, {"paperId": null, "title": "compared"}]}