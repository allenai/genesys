{"paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df", "abstract": "Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.", "venue": "arXiv.org", "year": 2023, "citationCount": 113, "influentialCitationCount": 18, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.03078", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "The Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods."}, "embedding": {"model": "specter_v2", "vector": [0.37573492527008057, 0.7087950110435486, -0.8150899410247803, -0.13841117918491364, -0.4824628233909607, 0.4373690187931061, 0.19258582592010498, -0.0743212103843689, -0.6968579888343811, -0.4168131351470947, 0.5314499139785767, 0.046903274953365326, 0.45076385140419006, 0.17043596506118774, -0.18319568037986755, 0.2657831609249115, -0.824628472328186, 0.4154915511608124, -0.06688421964645386, -0.577751874923706, -0.08974483609199524, -0.596246600151062, -1.0203824043273926, 0.20700180530548096, 0.33010509610176086, 0.8504502773284912, 0.29244354367256165, 0.6981651782989502, -0.7512776851654053, 0.37352699041366577, 0.7657209038734436, -0.5468243360519409, 0.10378163307905197, -0.2252652645111084, -0.1194077804684639, -0.14896713197231293, 0.36475321650505066, -0.4330996572971344, -0.2903655767440796, 0.9732141494750977, -0.3732716143131256, 0.4538702368736267, 0.5698865652084351, -0.517571210861206, -0.20840559899806976, 0.6601801514625549, 0.2238081693649292, 0.4888904392719269, -0.14449161291122437, -0.7087595462799072, 1.5499931573867798, -1.3467119932174683, -0.0034508288372308016, 1.8352142572402954, 0.6894044876098633, 0.3468455970287323, -0.40692970156669617, -0.6587420701980591, 0.6122584939002991, 0.012454042211174965, -0.7736741304397583, -0.29633915424346924, -0.44651153683662415, 0.20112961530685425, 2.013873338699341, -0.19253845512866974, 0.13063979148864746, 0.828567624092102, -0.017656642943620682, 1.2063219547271729, -0.18453067541122437, -0.7208694815635681, -0.3855540454387665, -0.030271144583821297, 0.19745399057865143, 1.0777193307876587, -0.2569642663002014, 0.26744478940963745, -0.8218547701835632, -0.48496192693710327, 0.33672481775283813, 0.0396394357085228, 0.327712744474411, -0.19616581499576569, 0.07089193165302277, 0.8487699627876282, 0.37572160363197327, 0.6770891547203064, 0.008284949697554111, 1.0335543155670166, 0.49921944737434387, 0.19730909168720245, 0.4601788818836212, 0.07781172543764114, -0.29049909114837646, 0.06757461279630661, -1.16605544090271, 0.10462936758995056, -0.1044883131980896, 0.6994806528091431, -0.2321578860282898, 0.38683584332466125, -0.4841037392616272, 0.5311785340309143, 1.66852605342865, 0.2949199378490448, 0.8892786502838135, -0.47057652473449707, 0.39288365840911865, -0.6374001502990723, -0.07637927681207657, -1.046137809753418, -0.36810746788978577, -0.31612861156463623, -0.8359617590904236, -1.340988039970398, -0.7865291833877563, 0.33680376410484314, -0.547023355960846, 0.8239039182662964, -0.3459090292453766, 0.4990275502204895, -0.28465917706489563, 0.545052170753479, 0.469734251499176, 0.7841617465019226, 0.470358282327652, 0.2984229326248169, 0.9058021903038025, -0.6560361385345459, -0.4292088747024536, -1.1217048168182373, 0.9331682324409485, -0.37286269664764404, 0.40628284215927124, -0.14915631711483002, -1.0634486675262451, -1.0479611158370972, -0.760536789894104, -0.22140197455883026, -0.43580254912376404, 0.43489870429039, 0.9804959297180176, 0.4302467703819275, -0.5168216824531555, 1.1174516677856445, -0.3696601688861847, -0.046708427369594574, 0.5275035500526428, 0.38314080238342285, 0.27879324555397034, -0.36055171489715576, -1.2931753396987915, 0.3613664209842682, 0.33902421593666077, -0.5637977719306946, 0.3811127841472626, -0.7935364246368408, -1.3860397338867188, -0.018301744014024734, 0.034670572727918625, -0.4609803557395935, 1.0924323797225952, -0.12915648519992828, -1.5279314517974854, 0.805450975894928, -0.45318594574928284, -0.10443378239870071, 0.17966578900814056, -0.1744614988565445, -0.7272348403930664, -0.3844427168369293, -0.664902925491333, 0.7621183395385742, 0.8544308543205261, 0.2019929736852646, -0.32167303562164307, 0.3987448513507843, -0.6534924507141113, -0.03196841478347778, -0.38247328996658325, 0.8427639007568359, -0.456716388463974, -0.34658390283584595, 0.47412654757499695, 0.6382832527160645, 0.12401699274778366, -0.07324542850255966, -0.10710794478654861, -0.9531117081642151, 0.3236580491065979, 0.0007386465440504253, 0.8722054958343506, -1.0108779668807983, -0.6248813271522522, 0.22358447313308716, 0.08028324693441391, -0.011197018437087536, -1.0982770919799805, 0.44583961367607117, -0.4695647954940796, 0.37757888436317444, -0.06975243240594864, -1.4685859680175781, 0.5347608923912048, 0.00164752674754709, -0.784534215927124, 0.2656482458114624, 0.13518637418746948, 0.9117524027824402, -1.0470002889633179, -0.13136406242847443, -0.08296018093824387, 0.4965696632862091, -1.3052207231521606, 0.9350602030754089, -0.2615594267845154, -0.273105651140213, -0.18694785237312317, -0.31389549374580383, 0.09837671369314194, -0.22608886659145355, 0.3752647936344147, -0.6867494583129883, 0.32528573274612427, 0.2804119884967804, -0.6290956735610962, 1.3604063987731934, -0.5648154020309448, 0.370467871427536, -0.3863328993320465, -0.23073607683181763, 0.3421468734741211, 0.07081954926252365, -0.184538796544075, -0.44177377223968506, 0.478376179933548, 0.7317419648170471, -0.5235532522201538, 0.44454601407051086, 0.9539327025413513, 0.5720289945602417, -0.41639888286590576, 0.09741568565368652, 0.7067955136299133, -0.30839258432388306, 0.46789097785949707, 0.456622838973999, 0.3397167921066284, 0.4064033031463623, 0.6111311912536621, 0.3047023117542267, 0.39655444025993347, -1.0006874799728394, -0.012645687907934189, 0.5886545181274414, 0.6508388519287109, 1.0924614667892456, 0.2393326461315155, -0.6347567439079285, -0.581135094165802, 0.3411112129688263, 0.5539287328720093, 1.6181309223175049, -0.3537828028202057, -0.28176721930503845, -0.23265011608600616, -0.036141254007816315, 0.1363552212715149, 0.05370263382792473, -0.1186915934085846, -0.3691454827785492, -0.3483949601650238, -1.2252899408340454, 0.6824387907981873, -0.007014093454927206, 0.6356640458106995, -0.5551807284355164, -0.03729216009378433, -0.24218250811100006, 0.2405766397714615, -0.8736829161643982, -0.6786543130874634, 0.32515984773635864, -0.7234770655632019, 0.051820237189531326, 0.1973789781332016, 0.3134414851665497, 0.22283443808555603, -0.9891538023948669, 1.102988362312317, -0.394749253988266, -0.34147027134895325, -0.052725572139024734, 0.39875054359436035, -0.19831185042858124, -0.7976745367050171, 0.2636313736438751, 0.41679123044013977, 0.11657106876373291, 0.3412998616695404, 0.46458837389945984, 0.09947194904088974, -0.3577043116092682, -0.1587105691432953, 0.17988437414169312, 0.04732200503349304, -0.1733100563287735, 0.7555398941040039, -0.5466777086257935, -0.12404124438762665, -1.382437825202942, 0.8495901823043823, -0.18750406801700592, -0.21918562054634094, -0.23991502821445465, -0.6437090635299683, -0.30647456645965576, 0.6902475357055664, -0.5823351144790649, -0.11875417828559875, -0.8459352850914001, -0.023118719458580017, -0.432964026927948, -0.374695360660553, 0.15359795093536377, 0.6039411425590515, 0.30803272128105164, 0.20714439451694489, 0.3015386462211609, 0.15056078135967255, -0.48972374200820923, 0.5956365466117859, -0.6273996233940125, 0.3910909593105316, 0.09100286662578583, 0.05140959098935127, -0.29343798756599426, -0.02647479809820652, -0.5104490518569946, -0.7649775147438049, -0.5160072445869446, -0.20615695416927338, -0.07804112881422043, -0.19710025191307068, -0.6157227158546448, -0.6538617014884949, -0.49393513798713684, -0.6170046329498291, -0.027251332998275757, 0.053718216717243195, -0.0515047162771225, -0.3126002848148346, -0.8299925923347473, -1.2767410278320312, -0.5642644166946411, -0.6360806822776794, -0.945833683013916, 0.5456345677375793, -0.07915007323026657, -0.6875727772712708, -0.6282529234886169, -0.32983893156051636, -0.45706307888031006, 0.8435831069946289, -0.4474784731864929, 0.8660178184509277, -0.062303390353918076, 0.6624577641487122, -0.5529912114143372, 0.038325510919094086, 0.5375654101371765, -0.3849181830883026, 0.23478035628795624, -0.581787109375, -0.08087420463562012, -0.525600254535675, -0.5759049654006958, 0.16553480923175812, 0.040564145892858505, 0.7511544823646545, 0.16927941143512726, -0.2964716851711273, 0.8661660552024841, 1.3968565464019775, -1.4470794200897217, -0.21333278715610504, -0.2925737798213959, 0.8579313158988953, -0.3931242823600769, -0.49073362350463867, 0.7417635321617126, 0.03632678836584091, 0.451463907957077, 0.18472370505332947, -0.15008360147476196, -0.2606019973754883, -0.8051573634147644, 0.8333448171615601, 2.0165257453918457, 0.606532633304596, 0.07241155207157135, -0.7046371102333069, 0.2593517601490021, -1.0598698854446411, -0.503331184387207, 0.2771765887737274, 0.686385452747345, 0.6641305088996887, -0.20445789396762848, -0.4053017795085907, -0.35514387488365173, 0.11910364031791687, 0.5320808291435242, -0.3131498396396637, -1.1454156637191772, -0.0741548240184784, 0.46398866176605225, 0.34000006318092346, 0.6197479963302612, -0.24236483871936798, 0.6229794025421143, 14.64343547821045, 1.2027777433395386, 0.10054938495159149, 0.6398627758026123, 0.807054877281189, 0.14430829882621765, -0.20851460099220276, -0.43540188670158386, -1.3990740776062012, 0.1984909623861313, 1.3916380405426025, 0.45966964960098267, 0.5695925951004028, 0.06730587780475616, 0.19443044066429138, 0.3098495900630951, -0.5869507193565369, 1.0135564804077148, 0.659218430519104, -0.8414222002029419, 0.3824014365673065, 0.0484888069331646, 0.412916362285614, 0.7310261130332947, 0.9767287969589233, 0.9829740524291992, 0.31299343705177307, -0.6529330611228943, 0.5356216430664062, 0.7097328305244446, 1.222962737083435, -0.1863902062177658, 0.43460094928741455, 0.715669572353363, -0.7900081872940063, -0.26167649030685425, -1.095129132270813, -1.3008816242218018, -0.07116948068141937, 0.2784842550754547, -0.323503315448761, -0.47165414690971375, -0.3670378029346466, 1.1126976013183594, -0.0719406008720398, 0.21950766444206238, 0.036094408482313156, 0.8961893320083618, -0.4334374964237213, 0.1424427032470703, 0.5727866888046265, 0.10939832031726837, 0.20984651148319244, 0.19659781455993652, 0.14513789117336273, -0.21381515264511108, 0.27463507652282715, 0.7516266107559204, -0.6862722039222717, 0.06368828564882278, 0.10380744934082031, -0.19649243354797363, -0.20562216639518738, 0.9060462117195129, 0.692939043045044, 0.08335930109024048, -0.4488484859466553, 0.5399274826049805, 0.8612807989120483, 0.12142222374677658, -0.26899680495262146, 0.12071643024682999, 0.3453949987888336, -0.7688881158828735, 0.15102198719978333, 0.5361180305480957, 0.20314601063728333, -0.7895268797874451, -0.9681525826454163, -0.5932193994522095, 0.3554118871688843, -0.5772004127502441, -1.0899697542190552, 0.44249919056892395, -0.2914015054702759, -0.23200850188732147, 0.08198259770870209, -0.7739023566246033, 0.16166244447231293, 0.6390059590339661, -1.6097747087478638, -0.20084446668624878, 0.7556020617485046, -0.7014142870903015, -0.43729138374328613, -0.25633665919303894, 1.3898367881774902, 0.22207558155059814, -0.490818053483963, 0.33713412284851074, 0.3223963677883148, -0.33464735746383667, -0.26951292157173157, -0.5949003100395203, 0.7435481548309326, -0.02699263021349907, 0.09824331849813461, 0.2725270688533783, -0.12329448014497757, 0.27280193567276, -0.8436163067817688, -0.2438337355852127, 0.7985415458679199, -0.5681581497192383, -0.16005901992321014, -0.8859307765960693, -0.6044096350669861, 0.13543154299259186, 0.07753515988588333, 0.10908523947000504, 0.4907013773918152, 0.16250431537628174, -0.6645439267158508, 0.05110633000731468, -0.6540069580078125, 0.07673190534114838, 0.19116348028182983, -0.5893942713737488, -0.33307284116744995, -0.08406860381364822, -0.04991215839982033, -1.0923351049423218, -0.6388698816299438, -0.17333903908729553, 0.0436306931078434, -0.015410007908940315, 1.1676117181777954, -0.545764148235321, 0.8104742765426636, 1.1167452335357666, -0.36234375834465027, -0.9537588357925415, -0.0653161108493805, -0.6875624656677246, -0.43910977244377136, -0.30392399430274963, 0.5444823503494263, -0.22615405917167664, 0.32627904415130615, 0.46249282360076904, 0.47031447291374207, -0.8592734336853027, -0.6251595616340637, -0.13585880398750305, -0.06763206422328949, -0.6926964521408081, 0.19936451315879822, -0.26852765679359436, -0.17550484836101532, 0.0001918760099215433, 0.29128730297088623, 0.2988947629928589, -0.1522868424654007, -0.7955729961395264, 0.018069667741656303, 0.09834592044353485, -0.2518858015537262, -0.741631805896759, -0.529909610748291, -1.5659228563308716, 0.29916900396347046, -1.5969115495681763, -0.3532988727092743, -0.6603848934173584, -0.3634563386440277, 0.11603567749261856, -0.47625941038131714, 0.1033640131354332, 0.19579242169857025, 0.23404626548290253, -0.13455888628959656, -0.3987370431423187, -0.5895772576332092, 0.9561132192611694, 0.8805536031723022, -0.5515169501304626, 0.6871455311775208, 0.0030384105630218983, 0.44917452335357666, 0.5397666692733765, 0.4762875437736511, -0.7712145447731018, -0.7636426687240601, -1.3155107498168945, 0.9188018441200256, -0.30400726199150085, -0.1135949045419693, -0.6990217566490173, 0.38731321692466736, 0.4408169686794281, 0.11415962129831314, 0.19204524159431458, 0.569185197353363, -1.0709940195083618, -0.7702375054359436, 0.5759255290031433, -0.8812263011932373, 0.14455163478851318, 0.17466913163661957, -0.32183560729026794, -0.3016761243343353, 0.22386212646961212, 0.061882298439741135, -0.7282185554504395, -0.619296133518219, 0.5647428631782532, -0.3737036883831024, 0.3374514877796173, -0.9186915755271912, 0.08905860036611557, -0.5765160322189331, -0.49264681339263916, -0.114344522356987, 0.0010880808113142848, -0.5887749195098877, 0.97211092710495, 0.6362543106079102, -1.3556760549545288, -0.04927319288253784, 0.4741601347923279, -0.05450519546866417, -0.572465717792511, 0.2792451083660126, 0.31003740429878235, -0.3047625720500946, 0.3699215352535248, 0.4639662802219391, 0.3318755626678467, -0.6394268870353699, -0.2267402857542038, 0.8853901624679565, -0.40411677956581116, -0.5827353596687317, 1.1635040044784546, -0.8984754085540771, -1.2350592613220215, 0.21515882015228271, -1.5362492799758911, -0.41902583837509155, -0.3394696116447449, 0.5336522459983826, -0.010857192799448967, 0.12183938920497894, -0.3835448920726776, -0.38380610942840576, 0.2916100323200226, 0.0795290544629097, -0.08364883065223694, 0.4207397997379303, -0.08686240762472153, -0.4584517478942871, 0.7664092183113098, 1.5409836769104004, -0.6378705501556396, -0.4460855722427368, -0.7967647314071655, -0.3725978136062622, -0.21426700055599213, 0.591971218585968, -0.2342866063117981, -0.7471134066581726, 0.6804925799369812, 0.5981133580207825, -0.033738624304533005, -0.04043363407254219, -0.44483229517936707, 0.6020205020904541, 0.8837982416152954, -0.13760197162628174, -0.8541092872619629, -0.6406669020652771, 1.4515634775161743, 1.0349081754684448, -0.959399402141571, 0.4052097797393799, -0.07537337392568588, -0.5388028025627136, 0.9228737354278564, -0.08901006728410721, -0.4380382001399994, 0.8635804057121277, -0.24269574880599976, -0.38560950756073, 0.261051744222641, -1.099080204963684, -0.32539963722229004, 1.4201675653457642, 0.4561172425746918, 1.1486072540283203, 0.2416037619113922, -0.06887003779411316, 0.9361011385917664, -0.19553984701633453, 0.002609414281323552, 0.15868903696537018, 0.1984364241361618, -0.3238159716129303, -0.057902101427316666, 0.03137617185711861, 1.1643891334533691, -0.5731080174446106, -0.7095177173614502, 0.4460597336292267, 0.3029804527759552, 0.5683521032333374, 0.5060621500015259, 0.9130628705024719, 0.2600061595439911, 0.5804916620254517, 0.24175715446472168, 0.6210060119628906, -0.7779679894447327, -0.31675225496292114, -0.34284454584121704, -0.7419100403785706, -0.49271994829177856, -0.023354528471827507, -0.20190399885177612, -0.7582329511642456, -0.43043747544288635, 0.7045485377311707, -0.24442628026008606, 0.10744632035493851, 1.0674902200698853, 0.4559065103530884, 0.6596025824546814, -0.47393473982810974, -0.30839893221855164, -0.7996125817298889, -0.9604101181030273, -0.3564826548099518, 0.027164220809936523, -0.2006407231092453, -0.1300225555896759, -0.09679720550775528, -0.5801684856414795]}, "authors": [{"authorId": "3239480", "name": "Tim Dettmers"}, {"authorId": "2219555303", "name": "Ruslan Svirschevski"}, {"authorId": "52257721", "name": "Vage Egiazarian"}, {"authorId": "2006108901", "name": "Denis Kuznedelev"}, {"authorId": "1502248377", "name": "Elias Frantar"}, {"authorId": "9543395", "name": "Saleh Ashkboos"}, {"authorId": "2113838061", "name": "Alexander Borzunov"}, {"authorId": "1713648", "name": "T. Hoefler"}, {"authorId": "3311387", "name": "Dan Alistarh"}], "references": [{"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3", "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"}, {"paperId": "9d6acac70b2d1fdb861a08b00766ef263109cd7f", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks"}, {"paperId": "91cc5192a2c389a6b71b5adb6643c4eb14302f1f", "title": "FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference"}, {"paperId": "c5757decb2d7fd2f785b1353542b8b290642e285", "title": "Towards Accurate Post-training Network Quantization via Bit-Split and Stitching"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0c0dfe47afcec2e229015f3c8f213d4c88e86b28", "title": "Up or Down? Adaptive Rounding for Post-Training Quantization"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "594a89505eb803280628198920e87cfc2bb82d94", "title": "MLPerf Inference Benchmark"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "89f57741a47cde722487dbbc8d067dfcbaa28cf2", "title": "OpenVINO Deep Learning Workbench: Comprehensive Analysis and Tuning of Neural Networks Inference"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "0de0a44b859a3719d11834479112314b4caba669", "title": "A Multiscale Visualization of Attention in the Transformer Model"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f", "title": "Et al"}, {"paperId": "dd01291bcb89b3332e6fae969f9a15256325f004", "title": "PiQA: an algebra for querying protein data sets"}, {"paperId": "59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b", "title": "The Penn Treebank: Annotating Predicate Argument Structure"}, {"paperId": "b4829f2e7968be42bead06b1710abee6f5a0afc2", "title": "A Comprehensive Study on Post-Training Quantization for Large Language Models"}, {"paperId": null, "title": "Kyle O\u2019Brien"}, {"paperId": null, "title": "The refined web dataset"}, {"paperId": null, "title": "and Yuxiong He"}, {"paperId": null, "title": "and Luke Zettlemoyer"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "851c2e1942642537499c743c324f10624b7b77ac", "title": "Accurate Post Training Quantization With Small Calibration Sets"}, {"paperId": null, "title": "and Iason Gabriel"}, {"paperId": null, "title": "Journal of Machine Learning Research"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "OpenAI blog"}]}