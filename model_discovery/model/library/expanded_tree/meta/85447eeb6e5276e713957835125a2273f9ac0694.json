{"paperId": "85447eeb6e5276e713957835125a2273f9ac0694", "abstract": "Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the\"real\"ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL? (2) What algorithmic solutions do successful models implement to perform ICLL? (3) What architectural changes can improve ICLL in less performant models? We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that their ability to do so relies on specialized\"n-gram heads\"(higher-order variants of induction heads) that compute input-conditional next-token distributions. Finally, we show that hard-wiring these heads into neural models improves performance not just on ICLL, but natural language modeling -- improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.", "venue": "arXiv.org", "year": 2024, "citationCount": 13, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks, and there is evidence that their ability to do so relies on specialized\"n-gram heads\"(higher-order variants of induction heads) that compute input-conditional next-token distributions."}, "embedding": {"model": "specter_v2", "vector": [0.5641880035400391, 0.21895264089107513, -0.2950907349586487, -0.07006058841943741, -0.5901842713356018, -0.5688599944114685, 0.6200645565986633, -0.12730465829372406, -0.7032482028007507, 0.24318163096904755, 0.5460214614868164, -0.5790872573852539, 0.380048543214798, 0.3290627896785736, -0.495534211397171, -0.05435541644692421, -1.175420880317688, -0.018244000151753426, -0.4479845464229584, 0.016122344881296158, -0.0356338769197464, -0.5241892337799072, -0.7613556385040283, -0.08022242784500122, 0.10787089169025421, 0.14528875052928925, 0.4490453898906708, 0.9943966269493103, -0.6592246890068054, 0.6999918222427368, 0.659196138381958, -0.2537383437156677, -0.029674969613552094, -0.11501338332891464, -0.47724002599716187, -0.5158101916313171, 0.4149862825870514, -0.19697701930999756, -0.4928746223449707, 0.578624963760376, -0.24901771545410156, 0.18674522638320923, 0.3460956811904907, -0.2082909196615219, -0.07890169322490692, 1.1753127574920654, 1.0433958768844604, 0.6272706985473633, 0.053114019334316254, -0.31238871812820435, 1.8325945138931274, -1.271658182144165, 0.4077696204185486, 1.3185991048812866, 0.634162425994873, 0.6170355081558228, -0.3693541884422302, -0.5668074488639832, 0.8726866245269775, -0.044287703931331635, -0.8023455142974854, -0.3317644000053406, 0.056926604360342026, 0.23951272666454315, 1.815808653831482, -0.3067716360092163, 0.3133352994918823, 0.551513135433197, -0.200400710105896, 1.5168615579605103, -0.12693729996681213, -1.2463188171386719, -0.24609415233135223, 0.10582498461008072, 0.2581293284893036, 0.8601054549217224, -0.4525241553783417, 0.3697185218334198, -0.39538341760635376, 0.046704430133104324, 0.5007162094116211, 0.15935280919075012, 0.09249444305896759, -0.028559677302837372, -0.28567901253700256, 0.7041986584663391, 0.40753254294395447, 1.1732274293899536, -0.1753278523683548, 1.0643503665924072, 0.47943615913391113, 0.16293078660964966, -0.3138597309589386, 0.6439323425292969, -0.27953535318374634, 0.27502670884132385, -0.7989131212234497, 0.2751685678958893, 0.060236167162656784, 0.624753475189209, -0.07019835710525513, 0.32177260518074036, -0.34177979826927185, 0.23196212947368622, 1.1413434743881226, 0.09623424708843231, 0.7717142105102539, -0.6815668940544128, 0.335714191198349, -0.4720453917980194, 0.10960498452186584, -0.2468155175447464, -0.4148973226547241, -0.42185643315315247, -0.2918958365917206, -1.4104417562484741, -0.318731427192688, 0.0537373423576355, -0.23814615607261658, 1.1452239751815796, -0.25399693846702576, 0.47651854157447815, 0.21337442100048065, 0.22017285227775574, -0.16295328736305237, 0.7458522915840149, 0.4080762267112732, -0.3220162093639374, 0.587790846824646, -0.42177706956863403, -0.45129895210266113, -0.9985901117324829, 0.9187178611755371, 0.21252232789993286, 0.6159340143203735, -0.3445975184440613, -1.0706275701522827, -0.8401705622673035, -0.8071476221084595, 0.07147198170423508, -0.6523127555847168, 0.1106674000620842, 1.074150562286377, 0.5490136742591858, -1.1192225217819214, 0.6904959678649902, -0.04801953583955765, -0.0642167329788208, -0.10133624821901321, 0.25852370262145996, 0.26521438360214233, -0.46765828132629395, -1.1831305027008057, 0.2044442594051361, 0.27693331241607666, -0.36840254068374634, -0.21071560680866241, -0.256256639957428, -1.231811761856079, -0.17918123304843903, 0.3681302070617676, -0.44540104269981384, 1.52389395236969, -0.42444097995758057, -1.0549209117889404, 0.5007337331771851, -0.3765426278114319, -0.06156865879893303, 0.02328117936849594, -0.2937781512737274, -0.6108800768852234, -0.5228157043457031, -0.1757918745279312, 0.39158299565315247, 0.11594630777835846, -0.2509262263774872, -0.08482397347688675, -0.10976333171129227, -0.34466883540153503, -0.2595513164997101, 0.17977169156074524, 0.7500936985015869, -0.32658249139785767, -0.14664681255817413, 0.10692950338125229, 0.5394936800003052, -0.34347862005233765, -0.616304874420166, -0.3811589181423187, -1.2671340703964233, 0.8121861815452576, 0.09221174567937851, 1.2817573547363281, -0.6583524346351624, -0.9645296335220337, -0.07147030532360077, -0.48444363474845886, 0.1780252605676651, -0.94927579164505, 0.7160636782646179, -0.5389564633369446, 0.5648560523986816, -0.3568666875362396, -0.9302981495857239, -0.2580374777317047, -0.02992914989590645, -0.7618284225463867, -0.5401089191436768, 0.3732837736606598, 0.9594127535820007, -1.245124101638794, 0.0005999192944727838, 0.04227413237094879, 0.1210814118385315, -1.0347394943237305, 1.1673558950424194, -0.5645235180854797, 0.06841873377561569, -0.09276353567838669, -0.2802322506904602, -0.3806224465370178, -0.34041279554367065, 0.8281875252723694, 0.08213754743337631, -0.4769919514656067, 0.830628514289856, -0.32264044880867004, 1.446569800376892, -0.9204928874969482, 0.46439531445503235, -0.15012311935424805, -0.5939167141914368, -0.2852088510990143, 0.31204167008399963, -0.39420872926712036, -0.21094723045825958, 0.032608069479465485, 0.3718584477901459, -0.488984614610672, 0.16937851905822754, 0.6800711750984192, 0.8184375762939453, -0.16070306301116943, 0.303914874792099, 0.6135057210922241, -0.28199896216392517, 0.5476289987564087, 0.5405182838439941, 0.7518062591552734, 0.630550742149353, 0.6085225939750671, 0.32362231612205505, 0.5571743845939636, -1.0955595970153809, -0.12719538807868958, 0.7021761536598206, 0.7478916645050049, 0.4396098256111145, 0.37108102440834045, -0.6059980988502502, -0.3268049955368042, 0.016384225338697433, 0.5748940706253052, 1.271426796913147, -0.5813477635383606, -0.47065791487693787, -1.0494487285614014, -0.3884214162826538, -0.5552240014076233, 0.5328103303909302, -0.1916549801826477, 0.07033868134021759, -0.7088294625282288, -0.6931207776069641, 0.9589638113975525, 0.3853258788585663, 0.8273330926895142, -0.8832413554191589, -0.0641857162117958, 0.07439090311527252, 0.15969285368919373, -0.7663290500640869, -0.665535569190979, 0.3973107635974884, -0.7679955363273621, -0.2645505368709564, 0.3077608644962311, -0.2857799828052521, 0.0954156294465065, -0.7707914710044861, 0.7950588464736938, -0.17543326318264008, -0.4306986629962921, 0.4341752231121063, 0.8548301458358765, -0.5597076416015625, -0.7747296094894409, 0.17954638600349426, 0.2005493938922882, -0.23338454961776733, 0.26330795884132385, 0.3158045709133148, 0.18148747086524963, -0.5576319694519043, -0.42500898241996765, -0.0906430259346962, 0.04221482202410698, 0.4218950569629669, 0.5319669842720032, -0.6149855256080627, 0.15585742890834808, -1.6165624856948853, 0.6326960921287537, 0.1333397924900055, -0.6273836493492126, 0.49070417881011963, -0.9630847573280334, 0.05386093631386757, 0.7994658350944519, -0.40015125274658203, -0.20623090863227844, -0.6423616409301758, 0.20535507798194885, -0.1790321320295334, -0.4358452260494232, 0.3222492039203644, 0.09656786918640137, 0.842354953289032, 0.03142528980970383, 0.42381832003593445, 0.2680261731147766, -0.11552312970161438, 0.6531463861465454, -0.8697714805603027, 0.5292209982872009, -0.05194498598575592, 0.1504027098417282, -0.5139281153678894, -0.02140437252819538, -0.5997158288955688, -0.3155010938644409, -0.3167944550514221, 0.05321081727743149, -0.23888693749904633, -0.11215686053037643, -0.539764940738678, -0.7722461223602295, -0.061439149081707, -0.9744571447372437, -0.689238429069519, 0.19491912424564362, -0.41093912720680237, -0.1599474549293518, -1.0794156789779663, -1.068976879119873, -1.0327516794204712, -0.4599688947200775, -0.7374712228775024, 0.25245437026023865, 0.14106722176074982, -0.46584203839302063, -0.938592791557312, 0.20441409945487976, -0.7085134387016296, 0.7557042241096497, -0.8518354892730713, 0.8136206865310669, 0.1525081992149353, -0.23380032181739807, -0.0004406266089063138, 0.37436145544052124, 0.3787115216255188, -0.2922801971435547, 0.3233780562877655, -1.021633505821228, 0.41580045223236084, -0.7934404015541077, -0.5479885339736938, 0.1006331816315651, 0.2120632529258728, 0.8064975738525391, -0.2630341947078705, -0.5800676941871643, 0.26860925555229187, 1.5431203842163086, -0.26512208580970764, 0.26502588391304016, -0.12607444822788239, 0.6671874523162842, 0.18295741081237793, -0.37951382994651794, 0.12112003564834595, 0.07863637804985046, 0.21740210056304932, -0.2194511890411377, 0.23866936564445496, 0.2665150761604309, -1.0515068769454956, 0.693430483341217, 1.1893385648727417, 0.2343258410692215, 0.3040960729122162, -1.0128870010375977, 0.6298395991325378, -1.2046535015106201, -0.5856269001960754, 0.7947977781295776, 0.7837832570075989, 0.7853760123252869, -0.6894209384918213, -0.16214871406555176, -0.06622244417667389, 0.25897711515426636, 0.09315695613622665, -0.3390372395515442, -0.5347913503646851, -0.1820174753665924, 0.6622415781021118, 0.1965346485376358, 0.7139924168586731, -0.42327427864074707, 0.7875459790229797, 14.975180625915527, 0.6301896572113037, -0.2543264925479889, 0.5156411528587341, 0.6243277788162231, 0.15343812108039856, -0.37324294447898865, 0.1990250200033188, -1.1631250381469727, -0.3741629123687744, 1.4889777898788452, 0.6606408357620239, 0.7863574624061584, 0.17723700404167175, 0.13219639658927917, 0.09850554168224335, -0.8203312158584595, 0.7967928647994995, 0.3497973084449768, -1.453472375869751, 0.32190725207328796, 0.23742219805717468, 0.2135620266199112, 0.7185757756233215, 0.9070518016815186, 0.8673133254051208, 0.8517537117004395, -0.31691616773605347, 0.6120322942733765, 0.43601250648498535, 1.3804386854171753, -0.004932718351483345, 0.19886496663093567, 0.5154356956481934, -0.644855260848999, -0.4927370846271515, -0.29262304306030273, -1.0898957252502441, -0.17110003530979156, -0.09429362416267395, -0.6939871311187744, -0.449144184589386, -0.22859413921833038, 0.7570159435272217, 0.1828104853630066, 0.09599323570728302, -0.5243071913719177, 1.0207265615463257, 0.0427834652364254, -0.3478275537490845, 0.1847631335258484, 0.19649747014045715, 0.02944992668926716, 0.3305431306362152, -0.07459071278572083, 0.09267155826091766, 0.1483238935470581, 0.4437144100666046, -0.5787137150764465, 0.07679161429405212, -0.8764721751213074, -0.31963178515434265, 0.17505128681659698, 0.7127540111541748, 0.8873543739318848, 0.3321421444416046, -0.3228411078453064, -0.03401175141334534, 0.6868288516998291, 0.3607617914676666, 0.2322162687778473, -0.14428581297397614, 0.348384827375412, -0.5250712633132935, -0.06514544785022736, 0.3943493962287903, 0.08446203172206879, -0.5310465693473816, -0.8757596611976624, -0.3585783541202545, 0.28649064898490906, -0.9856064915657043, -0.9224316477775574, 0.5378412008285522, -0.4753256142139435, -0.23878641426563263, -0.24017296731472015, -1.0715630054473877, -0.4246476888656616, 0.37685540318489075, -1.3215924501419067, -0.48209577798843384, 0.8292512893676758, -0.13141989707946777, -0.5430189371109009, -0.059638187289237976, 1.4199384450912476, -0.2718442678451538, -0.6346516013145447, 0.05139080435037613, 0.4996849298477173, 0.33589062094688416, -0.4051334857940674, -1.2447607517242432, 0.510499119758606, 0.41451141238212585, 0.3351498544216156, 0.8996455669403076, -0.27869752049446106, 0.42697903513908386, -0.7310463190078735, -0.08543995767831802, 0.838180422782898, -1.150962471961975, -0.36825671792030334, -0.6965453028678894, -0.2973381280899048, 0.5592228174209595, 0.3339090049266815, -0.2942025363445282, 0.5506013631820679, 0.3090674579143524, -0.7292837500572205, -0.38810157775878906, -0.591292142868042, 0.5230727791786194, 0.8619937300682068, -0.8403066992759705, -0.24295297265052795, -0.46045902371406555, 0.5160967707633972, -0.5197189450263977, -0.202434241771698, -0.5879833698272705, 0.23613014817237854, 0.20825175940990448, 0.6777070164680481, -0.7507818937301636, 0.808718204498291, 0.7243736386299133, -0.13131189346313477, -0.6953115463256836, 0.04310358315706253, -1.1315842866897583, -0.013051928021013737, 0.29043009877204895, 0.7690485119819641, -0.5916796326637268, -0.16876435279846191, 0.7330725193023682, 0.2722315788269043, -0.4503428637981415, -0.7503388524055481, -0.17147597670555115, 0.08168179541826248, -0.8381410837173462, 0.5210451483726501, 0.05922413244843483, 0.034748394042253494, 0.35302621126174927, 0.7912143468856812, 0.4199334681034088, -0.4714321792125702, -0.7566756010055542, 0.5156376957893372, 0.09404969960451126, -0.20615486800670624, -0.5817410945892334, -0.5096601247787476, -1.1787461042404175, 0.3469107747077942, -1.3180067539215088, 0.034043099731206894, -0.6208773255348206, -0.38672083616256714, -0.3300168812274933, -0.3305840790271759, 0.14280875027179718, 0.22805003821849823, -0.8508207201957703, -0.4797684848308563, -0.8136228919029236, -0.605043888092041, 0.423272967338562, 0.35727518796920776, -0.6502398252487183, 0.07834244519472122, -0.2023521512746811, 0.04123329371213913, 0.15696501731872559, 0.31726107001304626, -0.15601782500743866, -0.8120163083076477, -1.1489667892456055, 0.4123380482196808, 0.16148413717746735, -0.00535919051617384, -0.6596474647521973, 0.3676793873310089, 0.359154611825943, -0.08390109241008759, 0.07668157666921616, 0.03470524400472641, -0.6833764314651489, -0.4972384572029114, 0.2758379876613617, -1.0724866390228271, 0.3939790427684784, -0.0912763774394989, -0.3180002272129059, -0.10168337821960449, 0.24506875872612, -0.5008040070533752, -1.260935664176941, -0.5312650203704834, 0.15073056519031525, -0.9053988456726074, 0.10605861991643906, -0.6633662581443787, 0.12008810043334961, -0.5839390754699707, -0.26174670457839966, 0.19693723320960999, 0.3230883479118347, -0.45046889781951904, 0.5844711065292358, 0.23021267354488373, -0.6339309215545654, 0.20603103935718536, 0.151349276304245, -0.027542468160390854, 0.12899655103683472, 0.37404653429985046, 0.16075390577316284, -0.23993158340454102, 0.6303538084030151, 0.7232245206832886, 0.20378664135932922, -0.8060362339019775, -0.018071509897708893, 0.8132016062736511, -0.9242647290229797, 0.1912471503019333, 0.7532655000686646, -0.5209553837776184, -1.3470630645751953, -0.031446464359760284, -1.347922444343567, -0.5018489360809326, -0.8681151866912842, 0.6887075304985046, -0.2582801282405853, -0.15426835417747498, 0.3454504907131195, -0.12302092462778091, 0.0823124349117279, 0.06917652487754822, -0.6867402195930481, 0.5180938839912415, -0.016811028122901917, -0.4533575177192688, 0.9019400477409363, 0.7943581938743591, -0.40360191464424133, -0.5412266254425049, -0.9241958856582642, -0.07569960504770279, 0.14417405426502228, 0.18253745138645172, -0.49965548515319824, -0.2755993902683258, 0.7712603211402893, 0.39709773659706116, 0.031851060688495636, -0.048468951135873795, -0.18735361099243164, 0.10305075347423553, 0.969441831111908, 0.2592497766017914, -0.6091275215148926, -0.5956956744194031, 1.44013512134552, 1.2702934741973877, -0.6142963767051697, 0.02255246415734291, 0.07899468392133713, -0.25272202491760254, 0.9265612363815308, 0.21290872991085052, -0.056419502943754196, 0.8843463063240051, -0.12655571103096008, 0.17543074488639832, 0.45972684025764465, -1.32615065574646, -0.13273824751377106, 0.3154975473880768, 1.1753345727920532, 1.1128475666046143, 0.5972666144371033, 0.12109751254320145, 1.182768702507019, 0.0751798078417778, 0.22596292197704315, 0.5310677886009216, 0.7121529579162598, 0.1404445320367813, -0.4400029480457306, 0.007353917229920626, 0.5392025709152222, -0.7540848255157471, -0.7916448712348938, 0.06232612207531929, 0.6455928683280945, 0.2275390625, 0.4608224630355835, 0.9319577813148499, 0.1537901908159256, 0.2957823872566223, 0.7607770562171936, 0.5299459099769592, -0.6012774109840393, -0.4270710051059723, -0.39827650785446167, -0.462822824716568, -0.01674678362905979, -0.31507641077041626, -0.9373743534088135, -0.5717630386352539, 0.2717173397541046, -0.02961481548845768, 0.11405006051063538, 0.39974847435951233, 1.121593952178955, 0.5604367256164551, 0.1655176877975464, -0.14694522321224213, -0.2977783977985382, -0.524658203125, -0.941619336605072, 0.3380243480205536, -0.5284572243690491, -0.04648279398679733, 0.05659467354416847, 0.026452254503965378, -0.15480318665504456]}, "authors": [{"authorId": "1992708068", "name": "Ekin Aky\u00fcrek"}, {"authorId": "2257409822", "name": "Bailin Wang"}, {"authorId": "2280418575", "name": "Yoon Kim"}, {"authorId": "2269458328", "name": "Jacob Andreas"}], "references": [{"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60", "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"}, {"paperId": "e956e7598a8349683da7b16fc6d9ce64b5ae72e8", "title": "Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "e764ad9ccf0b31a0c91a9220290930f083ad062a", "title": "Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability"}, {"paperId": "d4c1517ca5e550ce43515b54e475082fba80bd56", "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions"}, {"paperId": "9bb3deca32af8d632e0d916c587cca6c185a6576", "title": "Uncovering mesa-optimization algorithms in Transformers"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "be48aae5d57efeff477199e55a536466280c853a", "title": "MLRegTest: A Benchmark for the Machine Learning of Regular Languages"}, {"paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f", "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction"}, {"paperId": "154493f69d7db3d49da0e51df0192c6ad5f1724a", "title": "Larger language models do in-context learning differently"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "40047a74b707743157051d38f76061ba5ff9aab4", "title": "Compositional Semantic Parsing with Large Language Models"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "955191363c3676f71766af3d14d1e6bbc0f040d6", "title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "cb16b85891172572cd856142880b503db0c2bc61", "title": "What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a", "title": "Few-Shot Semantic Parsing with Language Models Trained on Code"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9b9dc2b3d95d2f4e4269a9818c14c70c1f801384", "title": "An Interpretability Illusion for BERT"}, {"paperId": "3dcfa05a1c162e6cab927c5b08d0444f7b6691f4", "title": "Probing Classifiers: Promises, Shortcomings, and Advances"}, {"paperId": "d06e84ac9e912b415719f0e7f3163d59e0a329cd", "title": "RNNs Can Generate Bounded Hierarchical Languages with Optimal Memory"}, {"paperId": "10c86505de83647c7b4157595ab10f64e97c94ef", "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "d15e715d99a52f1c8a1ae6ec0ca853b13e04bbc7", "title": "On the Linguistic Capacity of Real-Time Counter Automata"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "a28ee1bea680c745636d7e09218fffda5d544ffe", "title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "a1b35b15a548819cc133e3e0e4cf9b01af80e35d", "title": "Sequential Neural Networks as Automata"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d821ce08da6c0084d5eacbdf65e25556bc1b9bc3", "title": "Does String-Based Neural MT Learn Source Syntax?"}, {"paperId": "2bb1df67e235015d867bc2d3fdbf12028976a299", "title": "Faster and Smaller N-Gram Language Models"}, {"paperId": "235544af245d5cc3253705435f9784648889438a", "title": "Links between probabilistic automata and hidden Markov models: probability distributions, learning models and induction algorithms"}, {"paperId": "1be8778de4c6eb623871fe08d0998016bd60936f", "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "title": "An Empirical Study of Smoothing Techniques for Language Modeling"}, {"paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time"}, {"paperId": "c42500dfe2683a333e7fd84d69e4bea2fcdf4acc", "title": "Probabilistic inductive inference"}, {"paperId": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "title": "A tutorial on hidden Markov models and selected applications in speech recognition"}, {"paperId": "009233749be702cb3ee7afb96cc7707c63f1cd52", "title": "A theory of the learnable"}, {"paperId": "d36efb9ad91e00faa334b549ce989bfae7e2907a", "title": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"}, {"paperId": "20cc59e8879305cbe18409c77464eff272e1cf55", "title": "Language Identification in the Limit"}, {"paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725", "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}, {"paperId": null, "title": "Identifying languages from stochastic examples"}, {"paperId": "dd2fbc39d90d96dadc56594b61e3e4b2be651686", "title": "An n log n algorithm for minimizing states in a finite automaton"}, {"paperId": null, "title": "SlimPa-jama: A 627B token cleaned and deduplicated version of RedPajama"}, {"paperId": null, "title": "In-Context Language Learning: Architectures and Algorithms"}]}