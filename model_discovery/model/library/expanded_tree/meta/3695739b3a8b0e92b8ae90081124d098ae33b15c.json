{"paperId": "3695739b3a8b0e92b8ae90081124d098ae33b15c", "abstract": "Transformer-based Large Language Models (LLMs) have made a significant impact on various domains. However, LLMs' efficiency suffers from both heavy computation and memory overheads. Compression techniques like sparsification and quantization are commonly used to mitigate the gap between LLM's computation/memory overheads and hardware capacity. However, existing GPU and transformer-based accelerators cannot efficiently process compressed LLMs, due to the following unresolved challenges: low computational efficiency, underutilized memory bandwidth, and large compilation overheads. This paper proposes FlightLLM, enabling efficient LLMs inference with a complete mapping flow on FPGAs. In FlightLLM, we highlight an innovative solution that the computation and memory overhead of LLMs can be solved by utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory hierarchy). We propose a configurable sparse DSP chain to support different sparsity patterns with high computation efficiency. Second, we propose an always-on-chip decode scheme to boost memory bandwidth with mixed-precision support. Finally, to make FlightLLM available for real-world LLMs, we propose a length adaptive compilation method to reduce the compilation overhead. Implemented on the Xilinx Alveo U280 FPGA, FlightLLM achieves 6.0\u00d7 higher energy efficiency and 1.8\u00d7 better cost efficiency against commercial GPUs (e.g., NVIDIA V100S) on modern LLMs (e.g., LLaMA2-7B) using vLLM and SmoothQuant under the batch size of one. FlightLLM beats NVIDIA A100 GPU with 1.2\u00d7 higher throughput using the latest Versal VHK158 FPGA.", "venue": "Symposium on Field Programmable Gate Arrays", "year": 2024, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3626202.3637562", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "In FlightLLM, an innovative solution that the computation and memory overhead of LLMs can be solved by utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory hierarchy) is highlighted, enabling efficient LLMs inference with a complete mapping flow on FPGAs."}, "embedding": {"model": "specter_v2", "vector": [0.3231812119483948, 0.1600663810968399, -0.6679391264915466, 0.20702263712882996, -0.5331118702888489, 0.19389590620994568, 0.4252866804599762, 0.19017350673675537, -0.1519261747598648, -0.31662023067474365, 0.4908455014228821, -0.33541378378868103, 0.7314378023147583, -0.27341634035110474, -0.048484139144420624, 0.1482267528772354, -0.7578721642494202, -0.039183907210826874, -0.1299222856760025, -0.0913507267832756, -0.020432310178875923, -0.3961885869503021, -1.0021519660949707, 0.277078777551651, 0.29796332120895386, 0.962139904499054, 0.08180321007966995, 0.7377343773841858, -0.6410560011863708, 0.5782268047332764, 0.7160090208053589, -0.11187873780727386, 0.3218880593776703, -0.000807527860160917, 0.013666207902133465, -0.04201367869973183, 0.06326587498188019, -0.4396494925022125, -0.35729315876960754, 1.0768311023712158, -0.2993772625923157, 0.14567674696445465, 0.1363745778799057, -0.5515304803848267, -0.3500983417034149, 0.6300170421600342, 0.30631569027900696, 0.5840034484863281, -0.42587050795555115, -0.41622453927993774, 1.057381272315979, -1.3980331420898438, -0.007450892124325037, 1.1857932806015015, 0.34252041578292847, -0.02457776665687561, -0.11225404590368271, -0.6325501799583435, 0.06640640646219254, -0.21407465636730194, -1.186262607574463, -0.6057486534118652, -0.20695212483406067, -0.14122046530246735, 1.9570289850234985, -0.18145404756069183, 0.28603336215019226, 0.31427299976348877, 0.37105539441108704, 0.9275637865066528, 0.20922230184078217, -1.0240602493286133, 0.03921961784362793, -0.1443864107131958, 0.2803518772125244, 1.0503151416778564, -0.06332239508628845, 0.46367979049682617, -0.9138919115066528, -0.3461757302284241, 0.3216070234775543, -0.32070043683052063, 0.5722376108169556, -0.21341601014137268, -0.2185600996017456, 0.9814286828041077, -0.06702495366334915, 0.3911624252796173, 0.19640770554542542, 0.6835541725158691, 0.9862278699874878, 0.015625804662704468, -0.014811483211815357, 0.03413611650466919, 0.21313375234603882, 0.16635046899318695, -1.3078604936599731, 0.13104067742824554, -0.23568423092365265, 0.848841667175293, -0.3543621301651001, 0.8882734179496765, -0.4060317873954773, -0.12848031520843506, 1.0861451625823975, 0.6493529677391052, 0.5875875353813171, -0.2375747412443161, 0.16689424216747284, -0.9956994652748108, -0.4569410979747772, -0.5288384556770325, 0.20587006211280823, -0.4504479765892029, -1.0611969232559204, -0.8341833353042603, -1.005859375, 0.34510910511016846, -0.7782919406890869, 0.5133633017539978, -0.5897623896598816, 0.27595254778862, 0.38122686743736267, 0.2591607868671417, 0.7800469398498535, 0.7705520391464233, 0.14313840866088867, -0.07408792525529861, 1.1662042140960693, -1.365541934967041, -0.6537541747093201, -1.1126922369003296, 0.34320706129074097, -0.46598020195961, 0.02204255387187004, -0.07464597374200821, -1.3842624425888062, -1.0746406316757202, -1.0148001909255981, -0.20019595324993134, -0.3595070540904999, 0.376013845205307, 1.1669652462005615, 0.6087660193443298, -0.9505986571311951, 0.38870561122894287, -0.5524470806121826, 0.19826772809028625, 0.30297020077705383, 0.4460471272468567, 0.7224588394165039, -0.0657360702753067, -1.4589407444000244, -0.02151883766055107, 0.2075187861919403, -0.8109632730484009, -0.23484034836292267, -0.4905891716480255, -1.3338212966918945, 0.11673544347286224, -0.04314296320080757, -0.30921322107315063, 0.8465132713317871, -0.09182564914226532, -1.2829736471176147, 0.6670247912406921, -0.3061649203300476, -0.46555399894714355, -0.21622103452682495, 0.19439923763275146, -0.59938645362854, -0.5114544034004211, -0.21425224840641022, 0.36663511395454407, 0.931070864200592, 0.37351205945014954, -0.134425088763237, 0.5050764083862305, -0.3304286301136017, -0.043184373527765274, -0.3530352711677551, 1.2228562831878662, -0.48586171865463257, -0.5487378239631653, -0.05635102465748787, 0.47826096415519714, -0.4609668254852295, -0.21067868173122406, -0.41073328256607056, -0.40089091658592224, 0.8464029431343079, -0.15193378925323486, 1.4457272291183472, -0.9719483852386475, -1.081536889076233, 0.13393466174602509, 0.008127463981509209, 0.12473057210445404, -0.37150073051452637, 0.5197601914405823, -0.6269932985305786, 0.046284157782793045, 0.04856415465474129, -0.6865226030349731, 0.0013756122207269073, -0.5014475584030151, -1.0621901750564575, -0.0670647993683815, -0.07345467805862427, 0.8900379538536072, -0.7014448046684265, 0.04475868493318558, 0.0562744103372097, 0.40585026144981384, -1.019244909286499, 0.8910742998123169, -0.21043197810649872, -0.26439663767814636, -0.04665224626660347, 0.2074224352836609, 0.38202860951423645, -0.5466347336769104, 0.6715021729469299, -0.5604113936424255, -0.4192705750465393, 0.3821086585521698, -0.3237769901752472, 1.4715913534164429, -0.4887898564338684, 0.5342956185340881, 0.08184664696455002, -0.5321106314659119, 0.6117475628852844, -0.03407862037420273, -0.27698659896850586, -0.5548791289329529, 0.44332489371299744, 0.5355773568153381, -0.48216378688812256, 0.5813906192779541, 0.9355617165565491, 0.6854085922241211, -0.617125928401947, 0.4012989401817322, 0.308685302734375, -0.36088985204696655, 0.7040998339653015, 0.4322131276130676, 0.5758966207504272, 0.032029151916503906, 0.8995473384857178, -0.5695899128913879, 0.7069455981254578, -0.8815950155258179, -0.23722755908966064, 0.44538259506225586, 0.6815879344940186, 0.8499107956886292, 0.28598159551620483, -0.8793835639953613, -0.4116984009742737, 0.10269978642463684, 0.4887481927871704, 0.9927515387535095, -0.20318204164505005, -0.6140908002853394, -0.6663119196891785, -0.13318029046058655, -0.4021715521812439, -0.43758803606033325, 0.27155205607414246, -0.2211669683456421, -0.5338755249977112, -1.2142307758331299, 0.8510120511054993, 0.12242664396762848, 0.7267709970474243, -0.5474566221237183, -0.8037985563278198, -0.47727301716804504, 0.5460994839668274, -1.0105996131896973, -0.6765884757041931, 0.7204676866531372, -0.4820680320262909, 0.3321864604949951, 0.35557594895362854, -0.08367671817541122, 0.3411843776702881, -0.8891460299491882, 0.8991755247116089, -0.58104008436203, -0.5814710259437561, -0.1996563971042633, 0.7357980608940125, -0.3636547029018402, -1.0950008630752563, 0.0335603766143322, 0.1478639841079712, -0.2687388062477112, 0.20636261999607086, 0.3356361985206604, 0.5726711750030518, -0.5392367839813232, -0.2299451231956482, 0.22074617445468903, 0.17163939774036407, 0.21539467573165894, 0.9342027902603149, -0.03368190675973892, -0.4302058219909668, -0.8106715083122253, 0.976392924785614, 0.43051522970199585, -0.8971915245056152, 0.014073538593947887, -0.6118893623352051, -0.026978129521012306, 0.9578608274459839, -0.5269650816917419, -0.05349664017558098, -0.5504761338233948, -0.08499690890312195, -0.6920769214630127, -0.03991798684000969, 0.11023406684398651, 0.21472369134426117, -0.28205424547195435, 0.22201362252235413, 0.7774780988693237, 0.4026431143283844, 0.15960420668125153, 0.30752032995224, -0.7030094265937805, 0.3790077269077301, -0.13080613315105438, 0.019750326871871948, -0.028800128027796745, 0.07073560357093811, -0.7913122773170471, -0.2626146078109741, -0.11374608427286148, -0.014307119883596897, -0.020228715613484383, -0.03698265552520752, -0.946627140045166, -0.37726110219955444, -0.27327263355255127, -0.8463589549064636, 0.03437754511833191, -0.026172300800681114, -0.33442193269729614, -0.26258915662765503, -0.6436038017272949, -1.2322124242782593, -0.40938085317611694, -1.2767084836959839, -1.6544228792190552, 0.6834863424301147, -0.13087990880012512, -0.40023770928382874, -0.5946686267852783, -0.3351794481277466, -0.5480433702468872, 0.9253936409950256, -0.4932861924171448, 0.8571211099624634, -0.055757761001586914, -0.3618590533733368, 0.015602363273501396, 0.1727549135684967, 0.44504478573799133, -0.35275477170944214, 0.2213575541973114, -0.646624743938446, 0.12735770642757416, -0.14934934675693512, -0.45366159081459045, 0.20557543635368347, 0.34603387117385864, 1.0943740606307983, 0.31866133213043213, -0.5499588847160339, 0.6886212825775146, 1.4169847965240479, -0.4595831334590912, 0.2630680799484253, -0.11099253594875336, 0.8103595972061157, -0.6541585922241211, 0.05399816110730171, 0.9458950757980347, -0.5117584466934204, 0.6281020045280457, 0.1429959386587143, -0.23456624150276184, -0.19165077805519104, -0.15280507504940033, 0.6766135096549988, 1.962831974029541, 0.831387460231781, 0.09616304188966751, -0.7613234519958496, 0.4531129002571106, -0.7776477932929993, -0.5690379738807678, 0.6103922724723816, 0.6912022233009338, 0.3321860730648041, 0.10218756645917892, -0.382530152797699, 0.23057183623313904, 0.569313108921051, 0.6906523704528809, -0.01147752720862627, -1.335062026977539, 0.12579822540283203, 0.990123450756073, 0.39845362305641174, 0.7366781830787659, -0.44517627358436584, 0.47090771794319153, 14.824417114257812, 1.6709511280059814, -0.43095862865448, 0.39150580763816833, 0.7416475415229797, 0.4703969955444336, -0.2748374938964844, 0.017153136432170868, -1.6927516460418701, 0.07560667395591736, 1.5257363319396973, 0.01640261523425579, 0.26342344284057617, 0.5050455331802368, -0.04789986088871956, 0.2491990476846695, 0.07975756376981735, 0.7574556469917297, 0.6271110773086548, -1.740869402885437, 0.4088399410247803, 0.2136603146791458, 0.1833140254020691, 0.3284399211406708, 0.8069593906402588, 0.6147580742835999, 0.13413651287555695, -0.6146570444107056, 0.6910004615783691, -0.066565603017807, 1.3919258117675781, -0.5236225128173828, 0.26270052790641785, 0.5102312564849854, -1.2800558805465698, 0.06718498468399048, -0.6838747262954712, -1.2370778322219849, 0.1275518238544464, 0.7026150226593018, -0.8313290476799011, -0.45227888226509094, -0.5862399935722351, 0.9735603928565979, 0.43535763025283813, 0.12063907086849213, 0.14985105395317078, 0.5236785411834717, -0.1086563766002655, 0.09960170835256577, 0.3047747015953064, 0.13713283836841583, 0.17605485022068024, 0.20877709984779358, 0.39124032855033875, -0.21368032693862915, 0.5608130693435669, 0.5541724562644958, -0.17199867963790894, -0.177185520529747, -0.2292918860912323, -0.6995929479598999, -0.35028567910194397, 0.9126922488212585, -0.11281399428844452, 0.2904440462589264, -0.6479927897453308, 0.19010983407497406, 0.3862324357032776, -0.47759169340133667, -0.668441891670227, -0.10518323630094528, 0.3813333511352539, -0.3954451382160187, 0.17952868342399597, 0.4708360731601715, -0.20024092495441437, -0.7895835638046265, -0.8978436589241028, -0.749593198299408, 0.14119654893875122, -0.304824560880661, -0.44338250160217285, 0.9572007060050964, -0.4236204922199249, -0.4178723394870758, 0.4875218868255615, -0.7217761874198914, -0.002060551196336746, 0.28882506489753723, -1.2735795974731445, -0.7327589988708496, 0.7084181308746338, -0.3709433972835541, -0.12292158603668213, -0.1437954306602478, 1.5254648923873901, 0.21322643756866455, -0.02037942409515381, -0.11099093407392502, -0.009727233089506626, -0.08772209286689758, -0.41396576166152954, -0.2013092190027237, 1.2746320962905884, 0.5474669337272644, 0.13413718342781067, 0.4381178915500641, -0.0765075758099556, -0.23107759654521942, -1.0156947374343872, -0.3247428834438324, 0.59844970703125, -0.23466193675994873, 0.10807189345359802, -1.3168247938156128, -0.3945501148700714, 0.13375425338745117, 0.4707610011100769, 0.2750789225101471, 0.22675900161266327, -0.31716999411582947, -0.292669415473938, -0.0742531344294548, -0.4898049831390381, -0.3537711203098297, 0.09818227589130402, -0.8009413480758667, 0.16209818422794342, -0.02238777093589306, 0.2533109784126282, -1.4306391477584839, -0.5923779606819153, -0.10674835741519928, 0.02596289850771427, -0.2625390589237213, 1.27122163772583, -0.02380749024450779, 1.0432279109954834, 0.8007335066795349, -0.46481049060821533, -0.3612609803676605, 0.2805132269859314, -1.00762140750885, -0.261949360370636, -0.19312532246112823, 0.3287102282047272, -0.14087547361850739, 0.3770083785057068, 0.13273191452026367, 0.1403055638074875, -0.6402640342712402, -0.5927711725234985, -0.2725922167301178, -0.14194758236408234, -0.6239986419677734, 0.32872191071510315, -0.22924084961414337, 0.08146736770868301, 0.014881115406751633, -0.04605451971292496, 0.5428923964500427, -0.11520188301801682, 0.019806774333119392, 0.08872883766889572, 0.30395564436912537, -0.5400435328483582, -0.28387370705604553, -0.7629709243774414, -1.2764196395874023, 0.3981286585330963, -1.3853453397750854, -0.24347594380378723, -0.21954485774040222, -0.06419318914413452, -0.05524776875972748, -0.041042350232601166, 0.06848054379224777, 0.7292613387107849, 0.043191321194171906, -0.5735518336296082, -0.7964456081390381, -0.4859127104282379, 0.7745494246482849, 0.6156195402145386, -0.7535439133644104, 0.4978366792201996, -0.16490313410758972, 0.32486265897750854, 0.2701828181743622, 0.11651855707168579, -0.41595402359962463, -0.9303116798400879, -1.0073233842849731, 0.3310474455356598, 0.19648686051368713, -0.30538567900657654, -1.0196794271469116, 0.5247112512588501, 0.23532645404338837, -0.3414941728115082, 0.2552661895751953, 0.2968205511569977, -0.7287188172340393, -0.32326799631118774, 0.7044483423233032, -0.5472238659858704, 0.2985120713710785, 0.7580744028091431, -0.9365944862365723, -0.4931607246398926, 0.4418192207813263, 0.27583861351013184, -0.9494704604148865, -0.8766757249832153, 0.46358436346054077, -0.783821165561676, 0.4089348316192627, -0.222807839512825, 0.2511383593082428, -0.9493445158004761, -0.17903484404087067, -0.11113985627889633, -0.2844921350479126, -0.1764635443687439, 0.92060786485672, 0.5799124240875244, -1.1189967393875122, 0.4091337323188782, 0.953264594078064, -0.4093762934207916, -0.41877347230911255, 0.3694545328617096, 0.20620624721050262, -0.613900899887085, 0.8808163404464722, 0.10921614617109299, 0.14063617587089539, -1.2722700834274292, 0.031304799020290375, 0.4606665372848511, -0.8665013909339905, -0.3011074662208557, 0.9777396321296692, -0.23597265779972076, -0.7290493845939636, -0.5254147052764893, -1.1903003454208374, -0.19365432858467102, -0.6844042539596558, 0.7881194353103638, -0.1863897740840912, 0.5558503866195679, -0.19899725914001465, -0.5556084513664246, 0.13795264065265656, 0.11404352635145187, -0.27220749855041504, 0.16267746686935425, 0.05862453579902649, -0.7049154043197632, 0.07383617013692856, 0.9455875754356384, 0.026775546371936798, -0.06515344977378845, -0.2438288927078247, -0.9518189430236816, -0.17256580293178558, 0.5769926309585571, 0.11969040334224701, -0.7718244791030884, 0.8948034048080444, 0.5264943242073059, 0.06153789535164833, 0.34546348452568054, -0.6142414808273315, 0.6386773586273193, 0.20938651263713837, 0.4053570032119751, -0.3814191520214081, -0.7371922731399536, 1.4515600204467773, 0.5769659876823425, -0.49681147933006287, 0.806983232498169, -0.6561054587364197, -0.5367119908332825, 1.1952873468399048, 0.15164192020893097, -0.12961910665035248, 0.9267311096191406, 0.8603321313858032, -0.2535870671272278, 0.33184224367141724, -0.8612052202224731, -0.17918358743190765, 0.8418224453926086, 0.7173311114311218, 0.8258126378059387, 0.12960544228553772, -0.1043616235256195, 0.9146918058395386, 0.14191663265228271, 0.2707549035549164, 0.6369094252586365, 0.6450299024581909, -0.1406756043434143, -0.5005544424057007, -0.3616958558559418, 0.581422746181488, -0.7779234647750854, -1.190463662147522, 0.6926078796386719, 0.48554885387420654, 0.2339341789484024, 0.04981035366654396, 0.8901816010475159, 0.0470530167222023, 0.07344228029251099, -0.09077233076095581, 0.6074299812316895, -0.6760246157646179, -0.22323963046073914, 0.30112341046333313, -0.6587967872619629, -0.0550987683236599, -0.020356539636850357, -0.1602480709552765, -0.7588772773742676, -0.06517168879508972, 0.495197057723999, -0.012922775000333786, 0.3510712683200836, 0.7888411283493042, 0.538240373134613, 0.6770447492599487, -0.4536867141723633, -0.3775738477706909, -0.4645620882511139, -0.12089294195175171, -0.3602150082588196, -0.8062915205955505, -0.11220528930425644, 0.024889279156923294, 0.23414389789104462, -0.12267876416444778]}, "authors": [{"authorId": "115524834", "name": "Shulin Zeng"}, {"authorId": "2264141921", "name": "Jun Liu"}, {"authorId": "144290348", "name": "Guohao Dai"}, {"authorId": "2150441889", "name": "Xinhao Yang"}, {"authorId": "48737592", "name": "Tianyu Fu"}, {"authorId": "2278587594", "name": "Hongyi Wang"}, {"authorId": "2279337796", "name": "Wenheng Ma"}, {"authorId": "1508493242", "name": "Hanbo Sun"}, {"authorId": "2242132627", "name": "Shiyao Li"}, {"authorId": "2278457016", "name": "Zixiao Huang"}, {"authorId": "2278587533", "name": "Yadong Dai"}, {"authorId": "2278583295", "name": "Jintao Li"}, {"authorId": "2279113764", "name": "Zehao Wang"}, {"authorId": "2278586664", "name": "Ruoyu Zhang"}, {"authorId": "2278430615", "name": "Kairui Wen"}, {"authorId": "6636914", "name": "Xuefei Ning"}, {"authorId": "2241604835", "name": "Yu Wang"}], "references": [{"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "dcf2e723ee9c3270c98ff768b139cca75d29242e", "title": "A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture"}, {"paperId": "79b53d37ec3a0002a25809ee6986385b26b2b182", "title": "COSA:Co-Operative Systolic Arrays for Multi-head Attention Mechanism in Neural Network using Hybrid Data Reuse and Fusion Methodologies"}, {"paperId": "3f5e63168d0ae1af41c3434e9e3e7e84dda9a5d8", "title": "FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "429a414f745150aa46ec0a7bd479faa09698b7a6", "title": "CTA: Hardware-Software Co-design for Compressed Token Attention Mechanism"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "b0c5c673c690c644a7d4af73adb783bd98486181", "title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences"}, {"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0", "title": "A Fast Post-Training Pruning Framework for Transformers"}, {"paperId": "1900de2b966ca55ee5ca24ec94d5debe66e80c5b", "title": "Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism"}, {"paperId": "9a65967286e8dcf221a1f658bfefc0dc940950a2", "title": "FILM-QNN: Efficient FPGA Acceleration of Deep Neural Networks with Intra-Layer, Mixed-Precision Quantization"}, {"paperId": "9b644023204759900686b856348d3f849f8c68c5", "title": "N3H-Core: Neuron-designed Neural Network Accelerator via FPGA-based Heterogeneous Computing Cores"}, {"paperId": "af77acc405764ea8cfcd835d937c8d4f88628966", "title": "Logic Shrinkage: Learned FPGA Netlist Sparsity for Efficient Neural Network Inference"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "21d0613c3e7fe2cb31f34441c1604edc9882fa45", "title": "NVIDIA A100 Tensor Core GPU: Performance and Innovation"}, {"paperId": "f7e83bf07e88bb586b72ba9f5d8d0ffe90c116b7", "title": "FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "44b67c61fea56e7b132a447d19a4cdb064b42470", "title": "MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "f1d8d5a5f2dcf7472311ee9e0c7658affcaacbc0", "title": "Shuhai: Benchmarking High Bandwidth Memory On FPGAS"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "e70d609ce18cd61799b087bf3a5e14c1ce70a41a", "title": "Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "50841809c5e8537174c6bc7ab72b71be2e297248", "title": "Bandwidth and Locality Aware Task-stealing for Manycore Architectures with Bandwidth-Asymmetric Memory"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c", "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"}, {"paperId": null, "title": "Sanger:ACo-DesignFrameworkforEnablingSparseAttentionUsingReconfigurableArchitecture"}, {"paperId": null, "title": "Alveo U280 Data Center Accelerator Card Data Sheet"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Angel-eye:AcompletedesignflowformappingCNNontoembeddedFPGA"}, {"paperId": null, "title": "Deep Learning with INT8 Optimization on Xilinx Devices"}, {"paperId": null, "title": "2023. SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": null, "title": "on FPGAs FPGA \u201924, March 3\u20135, 2024, Monterey, CA, USA Accelerating Transformer-based Text Generation"}, {"paperId": null, "title": "2022. Xilinx Board Utility Tool"}, {"paperId": null, "title": "2022. Emergentabilitiesoflargelanguagemodels"}, {"paperId": null, "title": "FPGA \u201924, March 3\u20135, 2024, Monterey, CA, USA"}, {"paperId": null, "title": "2023. RedPajama:AnOpenSourceRecipetoReproduceLLaMA"}, {"paperId": null, "title": "2023. Practitioners\u2019 Expectations on Code Completion"}, {"paperId": null, "title": "Xilinx"}, {"paperId": null, "title": "2023. LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation"}, {"paperId": null, "title": "2023. Largelanguagemodels in medicine"}]}