{"paperId": "076477b1e35da20c3eb0153183a966ff2710d7cd", "abstract": "As a primary component of Transformers, attention mechanism suffers from quadratic computational complexity. To achieve efficient implementations, its hardware accelerator designs have aroused great research interest. However, most existing accelerators only support a single type of application and a single type of attention, making it difficult to meet the demands of diverse application scenarios. Additionally, they mainly focus on the dynamic pruning of attention matrices, which requires the deployment of pre-processing units, thereby reducing overall hardware efficiency. This paper presents CoDA which is an algorithm, dataflow and architecture co-design framework for versatile and efficient attention accelerators. The designed accelerator supports both NLP and CV applications, and can be configured into the mode supporting low-rank attention or low-rank plus sparse attention. We apply algorithmic transformations to low-rank attention to significantly reduce computational complexity. To prevent an increase in storage overhead resulting from the proposed algorithmic transformations, we carefully design the dataflows and adopt a block-wise fashion. Down-scaling softmax is further supported by architecture and dataflow co-design. Moreover, we propose a softmax sharing strategy to reduce the area cost. Our experiment results demonstrate that the proposed accelerator outperforms the state-of-the-art designs in terms of throughput, area efficiency and energy efficiency.", "venue": "IEEE transactions on computers", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "CoDA is an algorithm, dataflow and architecture co-design framework for versatile and efficient attention accelerators and demonstrates that the proposed accelerator outperforms the state-of-the-art designs in terms of throughput, area efficiency and energy efficiency."}, "embedding": {"model": "specter_v2", "vector": [0.21866314113140106, 0.4587562680244446, -0.4918845295906067, 0.37568509578704834, -0.004554162733256817, 0.09601994603872299, 0.3660134971141815, 0.06231963634490967, -0.1368466019630432, -0.5477852821350098, 0.6979970932006836, -0.1030229702591896, 0.41927865147590637, 0.045745640993118286, 0.12396097928285599, 0.25640401244163513, -0.6782696843147278, 0.08038844168186188, 0.015507256612181664, -0.1984454244375229, 0.25754106044769287, -0.35909590125083923, -1.1902291774749756, 0.22894829511642456, 0.23275455832481384, 1.026673674583435, 0.5719531178474426, 0.8357623219490051, -0.3774230480194092, 0.40067076683044434, 0.394511342048645, 0.2128790318965912, 0.35391438007354736, 0.29863420128822327, -0.273784875869751, -0.49602481722831726, 0.43449437618255615, -0.1986350268125534, -0.2964732050895691, 1.0155880451202393, -0.06720660626888275, 0.21322298049926758, 0.25914159417152405, -0.7186392545700073, -0.38305583596229553, 0.42626023292541504, 0.0649246945977211, 1.0200568437576294, -0.504499614238739, -0.5019502639770508, 1.1584709882736206, -1.5684244632720947, -0.12579700350761414, 1.4391237497329712, 0.37828773260116577, 0.04014142230153084, 0.08196038752794266, -0.26383036375045776, 0.5753300189971924, 0.2648427486419678, -0.6474890112876892, -0.6459234952926636, 0.08989161998033524, -0.11111140996217728, 1.8583660125732422, -0.19219201803207397, -0.08384556323289871, 0.17658504843711853, 0.343129426240921, 1.2394359111785889, -0.11084632575511932, -0.8762800097465515, 0.20454680919647217, -0.368929922580719, 0.5579925775527954, 0.48592138290405273, 0.021175377070903778, -0.23716922104358673, -0.9666264057159424, -0.20469123125076294, 0.22683344781398773, 0.21809756755828857, 0.37962228059768677, -0.0649021565914154, 0.04653333127498627, 0.7744265794754028, 0.483211487531662, 0.5334601998329163, -0.24615032970905304, 1.001673698425293, 0.8607040047645569, -0.008700255304574966, -0.29406043887138367, 0.2882513105869293, 0.025246193632483482, 0.5116763710975647, -0.8714289665222168, -0.030911067500710487, -0.13009828329086304, 1.184950828552246, -0.7735034823417664, 0.7153717875480652, -0.8936735987663269, 0.01543780043721199, 1.0413745641708374, 0.31543099880218506, 0.5106210708618164, -0.08091859519481659, 0.26847806572914124, -0.8096762299537659, -0.34113970398902893, -0.7405118346214294, -0.11618348956108093, -0.45856472849845886, -1.303528070449829, -0.6948724389076233, -0.7932592034339905, 0.33171001076698303, -1.0847887992858887, 0.3428158164024353, -0.6922072768211365, 0.010849124751985073, 0.12857404351234436, 0.4052846431732178, 0.6691253781318665, 0.5358715057373047, 0.18929815292358398, 0.24958164989948273, 1.5599244832992554, -1.457305669784546, -0.6605424880981445, -1.142003059387207, 0.014791286550462246, -0.5521264672279358, 0.2888358235359192, -0.08505947887897491, -1.453544020652771, -0.9164215326309204, -0.9155491590499878, -0.4953084886074066, -0.38864997029304504, 0.4913488030433655, 1.1125801801681519, 0.4388449192047119, -1.06403386592865, 0.17186886072158813, -0.3787257671356201, -0.11181189864873886, 0.59526526927948, 0.49262043833732605, 0.8831421136856079, 0.03682167828083038, -0.9974365234375, 0.13077570497989655, -0.08747339993715286, -0.47009149193763733, -0.2686125338077545, -0.6396603584289551, -0.6837681531906128, 0.850866436958313, 0.1383020430803299, -0.7020968198776245, 1.292834758758545, -0.14534428715705872, -1.1588408946990967, 0.23866279423236847, -0.2671794891357422, -0.005851804278790951, -0.2999712824821472, -0.1162685826420784, -0.46844837069511414, -0.3846021592617035, -0.25566166639328003, 0.39192435145378113, 0.9823543429374695, 0.6610581278800964, -0.4351348876953125, 0.29110607504844666, -0.14775624871253967, 0.04959449544548988, -0.5293112993240356, 1.1680998802185059, -0.8005620241165161, -0.31608104705810547, 0.5299702882766724, 0.6427857875823975, -0.4817011058330536, -0.19715839624404907, -0.47928351163864136, -0.6488359570503235, 0.8088935017585754, 0.49603113532066345, 1.1370625495910645, -1.1479384899139404, -0.8618547916412354, 0.1396091878414154, 0.06611662358045578, 0.08474713563919067, -0.28971248865127563, 0.0033449765760451555, -0.45111706852912903, -0.17307332158088684, 0.6173425912857056, -0.6958776116371155, -0.16853396594524384, -0.7754546403884888, -1.023592472076416, -0.1686471402645111, 0.12486641854047775, 0.9212766885757446, -0.3815104067325592, 0.11948142200708389, -0.348715603351593, 0.29645681381225586, -1.341690182685852, 0.4861757457256317, -0.37735986709594727, -0.151528000831604, -0.23200927674770355, 0.20448747277259827, 0.35095542669296265, -0.7370136976242065, 0.7687349915504456, -0.930648684501648, -0.05912747606635094, 0.38739195466041565, -0.5103809833526611, 1.2049353122711182, -0.5109560489654541, 0.8042190074920654, 0.38257578015327454, -0.535240650177002, 0.31537926197052, 0.21236012876033783, -0.4573100805282593, -0.6241143345832825, 0.6325452923774719, 0.2517237961292267, -0.5181083679199219, 0.4076356589794159, 1.2438710927963257, 1.57793128490448, -0.687383234500885, -0.13464750349521637, 0.4977576732635498, 0.011414016596972942, 0.2956710755825043, 0.36196237802505493, 0.7766962051391602, -0.3033156096935272, 0.4618304967880249, -0.6974121332168579, 0.748457670211792, -0.9463775753974915, 0.13387185335159302, 0.7095338702201843, 0.6422356367111206, 0.5276172161102295, 0.5304549932479858, -0.7570264935493469, -0.18211251497268677, 0.08407635986804962, 0.5000957250595093, 1.7208373546600342, -0.1320868730545044, 0.06300568580627441, -0.21105051040649414, -0.22029441595077515, -0.3895227313041687, -0.5074048638343811, 0.21597570180892944, 0.15168902277946472, -0.5480549931526184, -0.9664850234985352, 0.5228075981140137, 0.46933862566947937, 0.7072678804397583, -0.8476055860519409, -0.7623831033706665, -0.6409812569618225, 0.4850477874279022, -0.9914758801460266, -0.6831492781639099, 0.7475204467773438, -0.4862518906593323, -0.04429116100072861, 0.22477367520332336, -0.43781182169914246, 0.6914238929748535, -0.42777562141418457, 0.8599422574043274, -0.766791582107544, -0.5040770173072815, -0.23667128384113312, 0.43454796075820923, -0.6509425640106201, -0.22419072687625885, 0.4961211681365967, -0.06687447428703308, -0.3740466237068176, 0.42432931065559387, 0.015776118263602257, 0.06349870562553406, -0.4120240807533264, -0.06654760241508484, -0.10446851700544357, 0.4492553770542145, 0.0438472144305706, 0.8978580236434937, -0.3753494322299957, -0.12233321368694305, -1.005656361579895, 1.087886095046997, 0.10154859721660614, -0.7112585306167603, -0.3654491603374481, -0.5230833292007446, -0.15658366680145264, 0.6621101498603821, -0.870514452457428, -0.15994276106357574, -0.39093539118766785, 0.16759620606899261, -0.7130675911903381, 0.22476400434970856, 0.12057992070913315, 0.2740270793437958, -0.2612071633338928, 0.2928732633590698, 0.45380499958992004, 0.1744472235441208, 0.11225646734237671, 0.4970265030860901, -0.5104811191558838, 0.7451390624046326, 0.23587997257709503, 0.028380917385220528, 0.15195155143737793, 0.17957520484924316, -0.6348215341567993, -0.17954346537590027, -0.0943598523736, 0.16322916746139526, 0.0364941731095314, -0.11601531505584717, -0.9484505653381348, -1.1833223104476929, -0.5002772212028503, -0.9803447127342224, 0.4356793761253357, 0.066437728703022, -0.14715667068958282, -0.14384318888187408, -1.3185999393463135, -1.1062431335449219, -0.30114656686782837, -1.7207368612289429, -1.4559106826782227, 0.36436355113983154, 0.38238316774368286, -0.35932376980781555, 0.009794368408620358, -0.45881181955337524, -0.9607763290405273, 1.3056567907333374, -0.6564470529556274, 0.8758581280708313, -0.28933921456336975, -0.5722299814224243, -0.1520971953868866, -0.13474000990390778, 0.011252222582697868, -0.6868125200271606, 0.16280728578567505, -1.076416254043579, 0.3628758490085602, -0.17428845167160034, -0.13016997277736664, 0.34978315234184265, 0.5977091193199158, 1.1173021793365479, 0.19374151527881622, -0.7979005575180054, 0.6691361665725708, 1.311828374862671, -0.5900704860687256, 0.20047764480113983, -0.10726173222064972, 0.8073087334632874, -0.5174394845962524, -0.10941624641418457, 0.7456479072570801, -0.3015103340148926, 0.5304774641990662, 0.39189425110816956, -0.07497507333755493, -0.11312295496463776, 0.20674313604831696, 0.09625088423490524, 1.6122820377349854, 0.8950706720352173, -0.021259907633066177, -0.5865222215652466, 0.7247875332832336, -1.007932186126709, -0.5933557748794556, 0.27082112431526184, 0.3209887146949768, 0.21095754206180573, 0.2154713273048401, -0.6797191500663757, 0.14953011274337769, 0.5246816277503967, 0.5159661173820496, -0.32349294424057007, -1.5498411655426025, 0.33158963918685913, 0.8509293794631958, 0.46712395548820496, 0.642955482006073, -0.2820657789707184, 0.37271466851234436, 14.714256286621094, 1.0386115312576294, -0.19694925844669342, 0.6680363416671753, 0.4579532742500305, 0.03299635648727417, 0.08745567500591278, 0.014079206623136997, -1.6425745487213135, 0.04143407195806503, 0.817068338394165, 0.15198369324207306, 0.08661091327667236, 0.8044226169586182, -0.2975923717021942, 0.1517379879951477, -0.3060316741466522, 0.5143318772315979, 0.6118833422660828, -1.660719394683838, -0.22286051511764526, 0.1069079264998436, 0.3304028809070587, 0.46157950162887573, 0.7919487953186035, 0.8479298949241638, 0.28417548537254333, -0.06528676301240921, 0.282081663608551, 0.04304133355617523, 0.8950316905975342, -0.08720146119594574, 0.1905096173286438, 0.16993416845798492, -1.200173020362854, -0.22746264934539795, -0.45924344658851624, -1.1761643886566162, 0.010617171414196491, 0.7745391726493835, -0.284616082906723, -0.7458629012107849, -0.14451803267002106, 0.47455471754074097, 0.3219284117221832, 0.6552250981330872, -0.35458052158355713, -0.030759049579501152, -0.2147745043039322, -0.2593992352485657, -0.12818431854248047, 0.9727972149848938, 0.033699654042720795, -0.031431034207344055, 0.03008442558348179, -0.2837340831756592, 0.15510430932044983, 0.45398131012916565, -0.5955200791358948, -0.40250101685523987, 0.13060912489891052, -0.01259113010019064, 0.17448578774929047, 1.2556508779525757, 0.2023404836654663, 0.4434719979763031, -0.6116310358047485, 0.2122904360294342, 0.3453434109687805, -0.2205212265253067, -0.5144138336181641, -0.6075016260147095, 0.5007284879684448, -0.7184776067733765, 0.36437684297561646, 0.6381330490112305, -0.6143566370010376, -0.4201815128326416, -0.6718880534172058, -0.4650338888168335, 0.1800362765789032, -0.6978471875190735, -0.3698872923851013, 0.8388631939888, -0.5539000034332275, -0.4839668273925781, 0.5452810525894165, -0.8456231951713562, -0.4903598427772522, 0.7581393122673035, -1.3057870864868164, -0.4274759292602539, 0.12586408853530884, -0.4102034270763397, -0.22836889326572418, -0.06589885801076889, 1.0846302509307861, 0.30622515082359314, -0.25966668128967285, 0.2200295776128769, -0.06057929992675781, -0.38348034024238586, -0.2579413950443268, -0.11941416561603546, 1.1310474872589111, 0.5383990406990051, -0.4582071900367737, -0.04458373412489891, 0.11210229247808456, 0.1703016310930252, -1.1887017488479614, -0.24257946014404297, 0.7026006579399109, -0.1773442178964615, -0.25651779770851135, -1.0084885358810425, -0.44052988290786743, 0.1387147754430771, 0.3419998288154602, 0.18757832050323486, 0.05852638557553291, 0.28208664059638977, -0.4106248617172241, -0.31097960472106934, -0.5780377388000488, 0.0866505429148674, 0.05544551834464073, -0.7590791583061218, 0.005489049479365349, 0.12994056940078735, 0.42091917991638184, -1.6217384338378906, -0.20324601233005524, -0.17074500024318695, -0.0016462369821965694, 0.2980623245239258, 1.3841824531555176, 0.14661374688148499, 0.897379457950592, 0.5856556296348572, 0.173471599817276, -0.21234099566936493, -0.16096757352352142, -0.7332335114479065, -0.5372005701065063, -0.03428950905799866, 0.42506518959999084, -0.1120377629995346, 0.8747773170471191, 0.45291590690612793, -0.02291356399655342, -0.4103408753871918, -0.44243818521499634, 0.018552662804722786, -0.49664902687072754, -0.398800790309906, 0.24203401803970337, -0.1832791566848755, 0.4085150361061096, 0.11883831024169922, 0.4923057556152344, 0.6950245499610901, -0.4945121109485626, -0.0837324857711792, 0.26105833053588867, -0.19984737038612366, -0.3753581643104553, -0.6526247262954712, -0.4999847710132599, -1.4532103538513184, -0.2722988724708557, -1.0933125019073486, -0.04558103531599045, -0.362331748008728, -0.49188360571861267, 0.1373472511768341, -0.1379420906305313, 0.2524734437465668, 0.3078700602054596, -0.0674443170428276, -0.21644096076488495, -0.4699503481388092, -0.4514731764793396, 0.6569270491600037, 0.7156074047088623, -0.6319507360458374, -0.3240688145160675, -0.053561750799417496, -0.21057932078838348, 0.5211612582206726, 0.46742674708366394, -0.35973840951919556, -0.7257357835769653, -1.0695158243179321, 0.5081102252006531, -0.022332947701215744, -0.5700406432151794, -1.1206079721450806, 1.4077205657958984, 0.35715073347091675, -0.05792989581823349, -0.0928742066025734, 0.1775231659412384, -0.9457645416259766, -0.8631722331047058, 0.600440502166748, -0.4777081310749054, 0.39466023445129395, 0.8580693006515503, -0.7604475617408752, -0.04670478776097298, 0.41703861951828003, -0.10133481025695801, -0.5132343173027039, -0.8469440937042236, 0.18186980485916138, -0.7716541290283203, 0.3153531849384308, -0.14672105014324188, 0.0743262842297554, -1.425271987915039, -0.023912567645311356, -0.02053089626133442, 0.2838672399520874, -0.9249987602233887, 0.8114132881164551, 0.32723474502563477, -1.025072455406189, 0.26600220799446106, 0.8908702731132507, -0.5628204941749573, -0.02582518942654133, 0.29332372546195984, 0.7434908151626587, -0.596963107585907, 0.8628219962120056, -0.0975097268819809, 0.1540582925081253, -0.9773921370506287, 0.12861400842666626, 0.15730562806129456, -0.5267365574836731, -0.15936292707920074, 0.9576661586761475, -0.4202408790588379, -0.46947401762008667, 0.20844578742980957, -1.4445003271102905, -0.5572521090507507, -0.28896716237068176, 0.6669714450836182, 0.047278210520744324, 0.2586112320423126, 0.24185077846050262, -0.8944819569587708, -0.1763245165348053, -0.26101160049438477, -0.09493511915206909, 0.07091960310935974, 0.0784282386302948, -0.28931644558906555, -0.42815518379211426, 0.44951722025871277, -0.25989365577697754, -0.052639659494161606, -0.6712281107902527, -0.5643842220306396, -0.14221979677677155, 0.7339736223220825, 0.09251520037651062, -1.1747074127197266, 0.6399855017662048, 0.5785452723503113, 0.2589992880821228, 0.849004328250885, -0.2611921429634094, -0.11980161815881729, 0.4246944189071655, -0.1006491631269455, -0.46811121702194214, -0.5370168089866638, 1.6338473558425903, 1.101446270942688, -0.3599439263343811, 0.8330792784690857, -0.7668229341506958, -0.4023786783218384, 0.8770425319671631, 0.4479163587093353, -0.3252982497215271, 1.1588197946548462, 0.5105151534080505, -0.4136872887611389, 0.2361721247434616, -0.8938189744949341, -0.46226415038108826, 0.6830463409423828, 0.7848977446556091, 0.5790790915489197, -0.22278442978858948, -0.18054090440273285, 0.9780191779136658, 0.36198434233665466, 0.1284439116716385, 0.5332651138305664, 0.4029897451400757, -0.02633536607027054, -0.16723810136318207, -0.453226774930954, 0.7836577892303467, -0.9444981217384338, -0.9976807832717896, 0.7734560966491699, 0.5904957056045532, 0.06783581525087357, 0.38836196064949036, 1.2005460262298584, -0.0250355526804924, 0.5481526255607605, -0.44318145513534546, 0.2447492778301239, -0.542988657951355, -0.4814817011356354, 0.13870136439800262, -0.923581063747406, -0.418978214263916, 0.39943602681159973, -0.11923915147781372, -0.5152772068977356, -0.33470505475997925, 0.23017945885658264, -0.3018411099910736, 0.6279186606407166, 0.6197767853736877, 1.022835612297058, 1.3625380992889404, -0.3567594289779663, -0.8524139523506165, -0.2363414168357849, -0.7573234438896179, 0.15082308650016785, -0.8120675683021545, -0.2157980501651764, 0.28306007385253906, 0.16481028497219086, -0.3465921878814697]}, "authors": [{"authorId": "2145236706", "name": "Wenjie Li"}, {"authorId": "2197866625", "name": "Aokun Hu"}, {"authorId": "2072804799", "name": "Ningyi Xu"}, {"authorId": "2241531132", "name": "Guanghui He"}], "references": [{"paperId": "05939c8de0760181e51dfa2112dc9558c941ae2b", "title": "Quantization and Hardware Architecture Co-Design for Matrix-Vector Multiplications of Large Language Models"}, {"paperId": "8846cf3a03d6a8f1097008f8881b0ec9496ead60", "title": "A Precision-Scalable Deep Neural Network Accelerator With Activation Sparsity Exploitation"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "2a44c6b7f291f625314a82ba3131e605009fd533", "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "512ff5037b28be7415d318ae6e8eeb0abb8c7013", "title": "DTATrans: Leveraging Dynamic Token-Based Quantization With Accuracy Compensation Mechanism for Efficient Transformer Architecture"}, {"paperId": "429a414f745150aa46ec0a7bd479faa09698b7a6", "title": "CTA: Hardware-Software Co-design for Compressed Token Attention Mechanism"}, {"paperId": "7ce692010c494e20ecd838ca51b10c22ae58c5d5", "title": "An Energy-Efficient Transformer Processor Exploiting Dynamic Weak Relevances in Global Attention"}, {"paperId": "690a37a2ba67b44b012bf9aa92e6a7f7670f487f", "title": "Dynamic Sparse Attention for Scalable Transformer Acceleration"}, {"paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8", "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention"}, {"paperId": "468bee032a25f148a4ed6822c0bfb4780d8087f1", "title": "GQNA: Generic Quantized DNN Accelerator With Weight-Repetition-Aware Activation Aggregating"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "c67b1a62b868a758791c88d5465c7b6d53510fc3", "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9", "title": "How to Train BERT with an Academic Budget"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "bc4bf86b9bd3bc4311ca64485a02323024f81ad4", "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning"}, {"paperId": null, "title": "\u201cEf\ufb01cient transformers: A survey,\u201d"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "\u201cScaling instruction-\ufb01netuned language model,\u201d"}]}