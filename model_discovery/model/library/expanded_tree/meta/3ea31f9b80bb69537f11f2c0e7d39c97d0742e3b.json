{"paperId": "3ea31f9b80bb69537f11f2c0e7d39c97d0742e3b", "abstract": "Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT. In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n^d) width and O(1) depth can approximate the self-attention layer arbitrarily well, where d is the input feature dimension. Lastly, under some assumptions, we provide an explicit construction of MPNN + VN with O(1) width and O(n) depth approximating the self-attention layer in GT arbitrarily well. On the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly strong baseline, outperforming GT on the recently proposed Long Range Graph Benchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation on a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer and MPNN on the climate modeling task.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 25, "influentialCitationCount": 5, "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.11956", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "The inverse connection is studied and it is shown that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT."}, "embedding": {"model": "specter_v2", "vector": [0.1814700961112976, 1.123977541923523, -0.5119815468788147, 0.35081085562705994, 0.2047434002161026, -0.2637527883052826, 0.956865131855011, -0.1494477242231369, -0.13777483999729156, 0.07795590907335281, 0.39391666650772095, -0.14048482477664948, 0.1416613608598709, -0.7621775269508362, -0.35922953486442566, -0.24660666286945343, -0.6609258651733398, 0.1914491057395935, 0.23848137259483337, -0.21243330836296082, 0.361567884683609, -0.5693732500076294, -0.8374049067497253, -0.5444523096084595, 0.3182876706123352, 0.6002453565597534, 0.1601821333169937, 1.0449386835098267, -0.31199532747268677, 0.3307324945926666, 0.6521494388580322, -0.40397655963897705, 0.33645251393318176, -0.1270282119512558, -0.6110761761665344, -0.14941991865634918, 0.3582526743412018, 0.35352346301078796, -0.7540081143379211, 0.9443846344947815, -0.28767287731170654, 0.5534197092056274, 0.11599278450012207, -1.264361023902893, -0.2809167504310608, 0.9817903637886047, 0.3281668424606323, 0.4358939528465271, -0.2889702320098877, -0.40936079621315, 1.6923521757125854, -0.9491432905197144, 0.11333569884300232, 1.2511982917785645, 0.5020384192466736, 0.26268163323402405, -0.15260937809944153, -0.5268424153327942, 0.5153729915618896, 0.011145208030939102, -0.7559215426445007, 0.25529515743255615, 0.2883157730102539, 0.12174499779939651, 1.7553949356079102, -0.30363667011260986, 0.2516116797924042, 0.46761295199394226, 0.09695204347372055, 0.8327818512916565, 0.10015983134508133, -0.36212390661239624, -0.35862991213798523, -0.028865749016404152, 0.42675986886024475, 1.2500735521316528, -0.2950229346752167, 0.5538807511329651, -0.8929813504219055, -0.21656666696071625, 0.2820180654525757, 0.06062043085694313, -0.08748766779899597, -0.2595689296722412, -0.22480367124080658, 0.6596177816390991, 0.8450071811676025, 0.39995715022087097, -0.13280799984931946, 1.2014399766921997, 0.0695885717868805, 0.23738397657871246, 0.015676910057663918, 0.37724360823631287, 0.3643817603588104, 0.7094377279281616, -0.6222689151763916, 0.0697341188788414, 0.10091532766819, 0.5650846362113953, 0.2818194329738617, 0.36455342173576355, -0.042486388236284256, -0.24348965287208557, 1.6508363485336304, -0.05395236611366272, 0.26614928245544434, -0.9132480025291443, 0.09963201731443405, 0.016354897990822792, -0.3132733106613159, -1.4239296913146973, -0.5563272833824158, -0.5516575574874878, -0.826077938079834, -0.8691465258598328, -0.4627442955970764, 0.4889437258243561, -0.5431385040283203, 0.8954881429672241, -0.1456945836544037, 0.5004063844680786, -0.08593600243330002, 0.3770976662635803, 1.052329421043396, 0.7210460901260376, 0.023346154019236565, 0.47768649458885193, 0.5726728439331055, -1.1863828897476196, -0.32557642459869385, -1.0843391418457031, 0.610020101070404, 0.1067359521985054, 0.3178998529911041, -0.3560931384563446, -0.8807744979858398, -1.0997270345687866, -1.0195410251617432, 0.2746797204017639, -0.9150307774543762, -0.3582264482975006, 1.0215277671813965, 0.3929760158061981, -1.1863980293273926, 1.0110772848129272, -0.13282690942287445, -0.7222129106521606, -0.178440660238266, 0.41030311584472656, -0.057856328785419464, -0.31732407212257385, -1.5280030965805054, 0.5440700054168701, 0.6359120607376099, -0.13822390139102936, -0.38873541355133057, -0.6714992523193359, -1.0938889980316162, 0.21246173977851868, 0.4588071405887604, -0.9391528964042664, 0.7703231573104858, 0.3481943607330322, -0.9067121148109436, 0.8663268089294434, 0.23592624068260193, -0.07112652063369751, 0.5285771489143372, 0.4849320650100708, -0.5793276429176331, -0.5292395949363708, -0.19527630507946014, 0.5244237780570984, 0.0983513742685318, -0.47689613699913025, -0.09895025193691254, -0.09800262004137039, -0.21683070063591003, -0.22141124308109283, -0.28487977385520935, 0.4238893389701843, -0.3326705992221832, 0.3832888603210449, 0.3462945222854614, 0.4809727370738983, -0.13039013743400574, -0.12996260821819305, -0.642555296421051, -1.0651010274887085, 0.6698272824287415, 0.4058249294757843, 0.7876030206680298, -0.6435931921005249, -0.5943469405174255, 0.1144024208188057, 0.6654463410377502, -0.5138968825340271, -0.6702768802642822, 0.822344958782196, -0.5360605716705322, 0.529683530330658, 0.03213983401656151, -0.5917814373970032, 0.13243894279003143, 0.30124935507774353, -0.2701268196105957, 0.07392985373735428, -0.07702477276325226, 1.0686055421829224, -1.3061550855636597, -0.09020645916461945, 0.17800326645374298, -0.04818231612443924, -0.539223313331604, 1.066785216331482, -0.28355956077575684, -0.37876829504966736, -0.04668635129928589, -0.627419114112854, 0.3556608557701111, -0.5512217879295349, 0.5266603231430054, -0.16593147814273834, 0.04528394341468811, 0.7064861059188843, -0.4702678620815277, 1.1251593828201294, 0.03200746700167656, 0.3206709623336792, 0.003248601220548153, -0.6937514543533325, 0.6524770855903625, -0.1207648441195488, -0.20822708308696747, -0.4776994585990906, -0.04997621104121208, -0.03666478022933006, -0.5563958883285522, 0.2838486433029175, 0.6010486483573914, 1.0205652713775635, -0.20869474112987518, 0.2822197675704956, 0.5675197839736938, -0.17867499589920044, 0.25637832283973694, 0.5067483186721802, 0.7581791877746582, 0.4279659688472748, 0.3966922461986542, -0.30290523171424866, 0.07079093903303146, -0.6813190579414368, -0.1416761428117752, 0.598362922668457, 0.6002629399299622, 0.7294420599937439, 0.6363500356674194, -0.7754622101783752, -0.24370962381362915, -0.133888378739357, 1.0248116254806519, 1.2380270957946777, 0.1028013825416565, -0.45972707867622375, -0.5635398626327515, -0.4845847189426422, -0.2270520031452179, 0.06754614412784576, -0.7332963347434998, -0.6721811890602112, -0.35800787806510925, -1.3150420188903809, 0.8866066336631775, 0.2558022737503052, 1.3353134393692017, -0.3742041289806366, -0.04058296978473663, -0.43825361132621765, 0.2797820270061493, -0.7065480351448059, -0.3187605142593384, 0.22100403904914856, -0.16857558488845825, -0.4759366810321808, 0.25182846188545227, -0.0020870594307780266, 0.210768461227417, -0.3447152078151703, 0.9742991328239441, -0.08446343243122101, -0.00042830334859900177, 0.2719631791114807, 0.861510694026947, -0.3258110582828522, -0.47051140666007996, 0.48861128091812134, 0.1741592288017273, -0.20429743826389313, 0.16238711774349213, 0.1161733940243721, -0.13515113294124603, 0.13383729755878448, -0.3777805268764496, 0.15374262630939484, -0.2302166372537613, -0.019939102232456207, 0.09757501631975174, 0.17294460535049438, -0.40150970220565796, -1.0592968463897705, 0.8805892467498779, -0.1307280957698822, -0.6421290636062622, 0.14502444863319397, -1.1975361108779907, -0.376559853553772, 0.38948509097099304, -0.3674628734588623, 0.320607453584671, -1.126348853111267, 0.5445464253425598, -0.12477385252714157, -0.052832696586847305, 0.26486316323280334, 0.018960850313305855, -0.2764730155467987, 0.2085464894771576, 0.4918650686740875, 0.29347798228263855, 0.17218859493732452, 0.5940603613853455, -1.1732069253921509, 0.8274603486061096, 0.2632884979248047, 0.6121876835823059, -0.18500548601150513, 0.6500027775764465, -0.6794783473014832, -0.6968126893043518, 0.03460993245244026, 0.06900838762521744, -0.533051073551178, 0.19896715879440308, -0.22928619384765625, -1.2975435256958008, -0.01044310349971056, -0.6702851057052612, -0.65988689661026, 0.38747987151145935, -0.611398458480835, 0.08517094701528549, -0.644243597984314, -0.8897559642791748, -0.48007234930992126, -0.3504576086997986, -0.5347328186035156, -0.046584729105234146, 0.36527419090270996, -0.18037398159503937, -0.9785358905792236, -0.1300709992647171, -0.3272998631000519, 0.8333101868629456, 0.0826125517487526, 0.7647631764411926, 0.0308236014097929, -0.6010149717330933, -0.026906555518507957, -0.17702724039554596, 0.07128912955522537, -0.19601966440677643, 0.3653368651866913, -0.5495682954788208, 0.3800743818283081, -0.6502451300621033, -0.022506559267640114, 0.15618778765201569, 0.3481559753417969, 0.5287315845489502, -0.07564801722764969, -0.6913732290267944, 0.3534213602542877, 1.5933717489242554, -1.0792484283447266, 0.46166205406188965, 0.6243732571601868, 1.2526626586914062, 0.48507750034332275, -0.39168083667755127, 0.15861473977565765, 0.8689702749252319, 0.1766444593667984, 0.920252799987793, -0.539072573184967, -0.24600659310817719, -0.9874470233917236, 0.28049927949905396, 1.2983235120773315, 0.2608895003795624, -0.40825575590133667, -1.1969488859176636, 0.4920593798160553, -1.3970657587051392, -0.783060610294342, 0.16297060251235962, 0.6527867317199707, -0.30810773372650146, -0.17294740676879883, 0.003824670799076557, -0.030950725078582764, 0.6551017761230469, 0.6988173723220825, -0.2677465081214905, -0.4037756621837616, -0.2113601416349411, 0.1127367690205574, 0.7336522340774536, 0.8684409260749817, 0.11233509331941605, 0.7656505703926086, 15.131014823913574, 0.5731003284454346, 0.07208649814128876, 0.04084101319313049, 0.46813589334487915, 0.5310072302818298, -0.2545402944087982, 0.13612885773181915, -1.1370348930358887, 0.10981301218271255, 1.2192323207855225, -0.04606929421424866, 0.6899946928024292, 0.1734778881072998, 0.017734268680214882, 0.20694951713085175, -0.4440020024776459, 0.4812546968460083, 0.5101238489151001, -1.2347638607025146, 0.2711971402168274, 0.5087416768074036, 0.16706526279449463, 0.790239155292511, 0.7041467428207397, 0.5018234252929688, 0.8058916926383972, -0.44886842370033264, 0.05588933452963829, 0.2629970908164978, 0.5917515754699707, -0.22343645989894867, 0.42582449316978455, 0.46376192569732666, -1.1112065315246582, -0.10720528662204742, -0.9531503915786743, -1.321427822113037, -0.16650721430778503, 0.3496294915676117, -0.5282716155052185, 0.08003813773393631, -0.07883239537477493, 0.8467422723770142, 0.08127716928720474, 0.5292264223098755, -0.5300983190536499, 0.2879408895969391, -0.3078402876853943, -0.01774820126593113, 0.5128126740455627, 0.19988375902175903, 0.09524784982204437, -0.27957406640052795, 0.25618091225624084, -0.1688348352909088, 0.365500271320343, 0.5266157388687134, -0.715910017490387, -0.30455705523490906, -0.5398917198181152, -0.3790769577026367, 0.004667319823056459, 1.1667249202728271, 0.39307600259780884, 0.5016966462135315, -0.12755542993545532, -0.0007864029030315578, 0.5457984209060669, 0.08031827211380005, -0.5594507455825806, -0.19517970085144043, 0.45614102482795715, -0.11766874045133591, -0.41620495915412903, 0.4731062352657318, -0.07461488991975784, -0.17344923317432404, -1.0594123601913452, -0.15602444112300873, 0.5898704528808594, -0.9804521203041077, -0.6859897375106812, 0.9061037302017212, -0.7858663201332092, -0.2307974398136139, 0.3289850652217865, -0.9988633394241333, -0.608043909072876, 0.3059941828250885, -1.825762391090393, -0.8322475552558899, -0.29562556743621826, -0.21419969201087952, -1.00908625125885, 0.20584915578365326, 0.9291493892669678, -0.11690979450941086, -0.5307873487472534, -0.2624739110469818, -0.3319549560546875, -0.03302094712853432, -0.2787711024284363, -1.1874284744262695, 0.9258725643157959, 0.40448394417762756, 0.02301490120589733, 0.036826908588409424, 0.22273869812488556, 0.2001470923423767, -0.6217918395996094, -0.15929044783115387, 0.39884018898010254, -1.0039348602294922, 0.20383238792419434, -0.9216029644012451, -0.8151329755783081, 0.4853071868419647, 0.7714920043945312, 0.006516993511468172, 0.10576631873846054, -0.09592515975236893, -0.6450701951980591, -0.21713419258594513, -0.89982670545578, 0.14294500648975372, 0.8196556568145752, -0.5238354206085205, -0.20210158824920654, 0.05279558151960373, 0.38396212458610535, -0.8558894991874695, -0.6742229461669922, -0.11987415701150894, 0.1117805540561676, -0.1523137092590332, 1.1144877672195435, -0.6069715619087219, 0.5958811640739441, 0.4147006571292877, 0.08259189873933792, -0.9232082962989807, -0.5321664214134216, -1.2218958139419556, 0.009824628941714764, 0.30028223991394043, 0.7567660212516785, -0.9736066460609436, 0.8348453640937805, 0.5714991092681885, 0.40619638562202454, -0.27955755591392517, -0.27727726101875305, -0.29523375630378723, -0.2973083257675171, -0.13118977844715118, 0.5858157277107239, 0.36623769998550415, 0.03153287619352341, 0.22959379851818085, 0.25962933897972107, 0.9567025303840637, 0.36351311206817627, -0.43853098154067993, 0.41553986072540283, -0.6049090623855591, -0.2011784315109253, -0.696003258228302, -0.7337393164634705, -1.1411936283111572, 0.08698639273643494, -1.339038372039795, -0.3390319049358368, -1.5356401205062866, -0.30540382862091064, 0.3828503489494324, -0.450309157371521, -0.022364286705851555, 0.3130623698234558, -0.21024486422538757, -0.48975175619125366, -0.9193904995918274, -0.5124304890632629, 0.5496661067008972, 0.7538051605224609, -0.6751983761787415, 0.32931414246559143, -0.19238103926181793, -0.2442305088043213, 0.19108478724956512, 0.5345804691314697, -0.7702320218086243, -0.6729992628097534, -1.0148338079452515, 0.7519550919532776, -0.23289889097213745, 0.07275185734033585, -0.7622953057289124, 0.8572959899902344, 0.2669008672237396, -0.03659731522202492, 0.07397828996181488, 0.2926367521286011, -0.8064819574356079, -0.15334342420101166, 0.5121117830276489, -0.5797762274742126, 0.6021313071250916, -0.2396097481250763, -0.16301633417606354, 0.09151163697242737, 0.9910103678703308, -0.042568691074848175, -1.1723605394363403, -0.5201007127761841, 0.632265567779541, -0.3633998930454254, 0.10494907200336456, -0.27079322934150696, -0.5915632247924805, -1.5831968784332275, -0.3989810645580292, -0.22485819458961487, 0.8365967869758606, -0.10956031084060669, 0.8538303971290588, 0.22596070170402527, -1.2078231573104858, 0.10620902478694916, -0.001374404295347631, 0.1851012259721756, -0.2930849492549896, 0.24851690232753754, 0.22301974892616272, -0.33097711205482483, 0.11756733059883118, -0.005571982357650995, 0.46466705203056335, -0.6394219398498535, 0.34768640995025635, 0.9344906210899353, -0.6043096780776978, -0.4114731252193451, 0.7305505275726318, -0.15491312742233276, -1.4358099699020386, 0.05853484198451042, -0.8055645227432251, -0.5129618048667908, -0.48507678508758545, 0.33644089102745056, 0.512512743473053, -0.531278669834137, 0.24052825570106506, -0.37629374861717224, 0.4528261125087738, 0.49020683765411377, -0.016261618584394455, 0.6293714046478271, -0.19442428648471832, -0.7060601711273193, 0.2181456983089447, 0.5478987693786621, -0.7442551851272583, -0.35813167691230774, -0.7905864119529724, -0.06417816877365112, -0.17521335184574127, 0.4723454713821411, -0.041895076632499695, -0.6823437809944153, 0.7748702168464661, 0.32148388028144836, 0.9424187541007996, 0.12838852405548096, -0.21134881675243378, 0.20381662249565125, 0.761539876461029, 0.0008612688980065286, -0.7033548951148987, -0.3918343186378479, 0.7872134447097778, 0.983011782169342, -0.6295506954193115, 0.33579325675964355, -0.4822801649570465, -0.2777846157550812, 0.8849006295204163, 0.6640342473983765, -0.29627761244773865, 0.7768164873123169, -0.15116000175476074, -0.23740027844905853, -0.11559980362653732, -1.0260674953460693, -0.2484402358531952, 0.4925128221511841, 1.1748690605163574, 0.28741851449012756, 0.13332538306713104, 0.24980314075946808, 0.2889796197414398, -0.15423829853534698, -0.2984165549278259, 0.8132208585739136, -0.0365946926176548, -0.3309292793273926, 0.0037192897871136665, 0.157924085855484, 0.23759986460208893, -0.4109429121017456, -0.3348599970340729, -0.533464252948761, 1.0105303525924683, -0.10716255754232407, 0.6774289608001709, 0.659156858921051, -0.19401207566261292, 0.40661686658859253, -0.1839923858642578, 0.33689600229263306, -0.2756313681602478, -0.24899911880493164, -0.6285784244537354, -0.4578342139720917, -0.2983786463737488, -0.185118168592453, -0.06896790117025375, -0.5480972528457642, -0.7892463803291321, 0.13702546060085297, 0.318042129278183, 0.2678683400154114, 0.5386661887168884, 0.3405696451663971, 0.5959860682487488, 0.03024468757212162, -0.49071788787841797, 0.02095908485352993, -0.590644359588623, -0.24160051345825195, -0.7089006304740906, -0.4607276916503906, -0.36786070466041565, -0.5963883996009827, -1.1204912662506104]}, "authors": [{"authorId": "2053992712", "name": "Chen Cai"}, {"authorId": "51015909", "name": "T. Hy"}, {"authorId": "2023052", "name": "Rose Yu"}, {"authorId": "49416382", "name": "Yusu Wang"}], "references": [{"paperId": "01de6d0c00e7e77050a90945246b2b4acde497a2", "title": "NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification"}, {"paperId": "8ad8a29861f0bdb829bed960f068a37fbc5ff11d", "title": "Graph Neural Networks for Link Prediction"}, {"paperId": "30258c205060af5ce958dc6c9e184c9498ee48ed", "title": "Attending to Graph Transformers"}, {"paperId": "b12488cf6cf9c630ab2d344964bf2233bbd9dee8", "title": "Transformers Meet Directed Graphs"}, {"paperId": "5eda60d4940d4185df45c5703e103458171d465d", "title": "Pure Transformers are Powerful Graph Learners"}, {"paperId": "f7a3d9bcf052f2b4ef7d59dcca4013ea11081d0f", "title": "Long Range Graph Benchmark"}, {"paperId": "8ecf2c170913282748fd53ab34ed3a56ad964668", "title": "Exponential Separations in Symmetric Neural Networks"}, {"paperId": "277dd73bfeb5c46513ce305136b0e71fcd2a311c", "title": "Recipe for a General, Powerful, Scalable Graph Transformer"}, {"paperId": "407238453abd02a9edf48032eaa19819d6d150ff", "title": "Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets"}, {"paperId": "eb984b142db9965b10a3b5ae5813eeb3e0f6e676", "title": "Sign and Basis Invariant Networks for Spectral Graph Representation Learning"}, {"paperId": "ea0e4a9778e33b7f8e7b3246d63071330950995a", "title": "Structure-Aware Transformer for Graph Representation Learning"}, {"paperId": "b92898a28bfad42a053726c2707cc05686cd332a", "title": "GRPE: Relative Positional Encoding for Graph Transformer"}, {"paperId": "2b8a207189bc02d73d1dce850bcde24dbd984483", "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention"}, {"paperId": "04ac11f8db83406b03d92aa4571fc3e6c176c1e3", "title": "Understanding over-squashing and bottlenecks on graphs via curvature"}, {"paperId": "6c761cfdb031701072582e434d8f64d436255da6", "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"}, {"paperId": "fd753314bfa805a0c831cc0a693a1de2defbc387", "title": "Global Self-Attention as a Replacement for Graph Convolution"}, {"paperId": "c7c625fc31168b64ed3191f916e446e08baaf9d2", "title": "Universal Approximation of Functions on Sets"}, {"paperId": "a6337d9ebb0b7de84588806110157806f9c0383b", "title": "GraphiT: Encoding Graph Structure in Transformers"}, {"paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41", "title": "Do Transformers Really Perform Bad for Graph Representation?"}, {"paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c", "title": "Rethinking Graph Transformers with Spectral Attention"}, {"paperId": "ab30672c8c5e4787f6a5985f26a8f281f0db2fb8", "title": "How Attentive are Graph Attention Networks?"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "9389af659f14239319186dff1cef49e8ece742c8", "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs"}, {"paperId": "4f0462f76c4a6000d89638bfdcc7b7d0a7016c07", "title": "Meta-Learning Dynamics Forecasting Using Task Inference"}, {"paperId": "d40c77c010c8dbef6142903a02f2a73a85012d5d", "title": "A Survey on Vision Transformer"}, {"paperId": "849b88ddc8f8cabc6d4246479b275a1ee65d0647", "title": "A Generalization of Transformer Networks to Graphs"}, {"paperId": "5dcbce2dc927e6c96c6be88d2585a835cc9913eb", "title": "Improvements of the Daily Optimum Interpolation Sea Surface Temperature (DOISST) Version 2.1"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c6d550c3fcecf27b979be84c4cd444cc1c72bf47", "title": "A Note on Over-Smoothing for Graph Neural Networks"}, {"paperId": "3bfa808ce20b2736708c3fc0b9443635e3f133a7", "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "597bd2e45427563cdf025e53a3239006aa364cfc", "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs"}, {"paperId": "575941d954765bd0142c4cf7211513a09c08b3e1", "title": "Revisiting \"Over-smoothing\" in Deep GCNs"}, {"paperId": "989e681731eb27ac1e12b2d60c8612c6982fe8c7", "title": "Towards Physics-informed Deep Learning for Turbulent Flow Prediction"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "5d8b0520b61bc77975e81d4636490dbe3f22238f", "title": "On Universal Equivariant Set Networks"}, {"paperId": "38293873cce681b857b3c1d73f5590f641f3dc42", "title": "PairNorm: Tackling Oversmoothing in GNNs"}, {"paperId": "4ce9c20642dce5eb7930966053a1e3da4ef617f2", "title": "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "6541afa4b4061a7d5c8387514bedea9dc249fd80", "title": "Invariant and Equivariant Graph Networks"}, {"paperId": "de9550945b2f631c541c299114a770c4f47f9616", "title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs"}, {"paperId": "3a58efcc4558727cc5c131c44923635da4524f33", "title": "Relational inductive biases, deep learning, and graph networks"}, {"paperId": "36652428740cd30d245d55889f01a7fb04a91c93", "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning"}, {"paperId": "2581b3e44592b3b3741474c8d6f483a90c29f139", "title": "Deep learning for physical processes: incorporating prior scientific knowledge"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "007112213ece771be72cbecfd59f048209facabd", "title": "A simple neural network module for relational reasoning"}, {"paperId": "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367", "title": "Neural Message Passing for Quantum Chemistry"}, {"paperId": "d997beefc0922d97202789d2ac307c55c2c52fba", "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "bf52f9d48ee4161d362628f8c0e476618577280c", "title": "Daily High-Resolution-Blended Analyses for Sea Surface Temperature"}, {"paperId": "8da1dda34ecc96263102181448c94ec7d645d085", "title": "Approximation by superpositions of a sigmoidal function"}, {"paperId": null, "title": "2022), we divide the region into 11 square batches of equal size"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "2021), different attentions are used conditional on whether an edge is presented in the graph or not, detailed below. One may wonder whether we can approximate such a framework in MPNN + VN"}, {"paperId": null, "title": "the two-step method (center & scale) PairNorm is proposed to reduce the oversmoothing issues"}, {"paperId": null, "title": "Deep sets. Advances in neural information processing systems"}, {"paperId": null, "title": "We train all our models with 100 epochs with batch size 20, initial learning rate 10 \u22123 , and Adam optimizer"}, {"paperId": null, "title": "Connection to Over-Smoothing Phenomenon"}]}