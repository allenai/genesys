{"paperId": "dbb30953d9303658586083bbd4c196f9c0895f88", "abstract": "In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - the approach of incorporating a depth-wise recurrence similar to Universal Transformers and a chunk-wise temporal recurrence like Temporal Latent Bottleneck are studied."}, "embedding": {"model": "specter_v2", "vector": [0.027587711811065674, 0.6603004336357117, -0.43074703216552734, -0.14669905602931976, -0.48169443011283875, -0.4277198910713196, 0.6196348667144775, -0.08585429191589355, 0.06013692170381546, 0.10920680314302444, 0.36804908514022827, -0.4836433529853821, 0.08815392851829529, -0.1908186376094818, -0.26652073860168457, -0.048943765461444855, -1.3531516790390015, 0.5632627010345459, 0.20036527514457703, -0.21238605678081512, 0.1194482371211052, -0.14346927404403687, -1.2515171766281128, 0.3295418918132782, 0.4844271242618561, 0.5389710068702698, -0.5679250359535217, 0.47687891125679016, -0.45474544167518616, 1.4206269979476929, 0.23984616994857788, -0.40733280777931213, 0.06039730831980705, -0.016965998336672783, -0.6997549533843994, -0.5106570720672607, 0.09829504042863846, -0.42156922817230225, -0.42337721586227417, 0.3294129967689514, -0.33177465200424194, -0.06267116218805313, 0.2745218873023987, -1.0221452713012695, 0.14410564303398132, 1.3349617719650269, 0.3893972933292389, 0.7202860116958618, -0.11887187510728836, -0.7512898445129395, 1.6171954870224, -1.0467482805252075, 0.3778184652328491, 1.5762509107589722, 0.4743043780326843, 0.5048011541366577, -0.42562124133110046, -0.727642297744751, 0.6406033635139465, 0.1716119796037674, -1.0095018148422241, -0.5287892818450928, -0.086355060338974, 0.012460500001907349, 1.5234808921813965, -0.4747631847858429, -0.10956528782844543, 0.09671857208013535, 0.060842253267765045, 1.377967119216919, 0.020826533436775208, -0.3998247981071472, -0.16787484288215637, 0.2230813056230545, 0.22361478209495544, 0.943846583366394, -0.3279631435871124, -0.0020035039633512497, -1.058315634727478, -0.07867515087127686, 0.365494966506958, 0.12939688563346863, 0.11509963870048523, -0.5711898803710938, -0.23068265616893768, 0.4875010550022125, 0.08681437373161316, 0.899340808391571, -0.1404428333044052, 0.5237483978271484, 0.44296032190322876, 0.6280735731124878, -0.3902066648006439, 0.4492805302143097, -0.23742663860321045, 0.275071382522583, -0.4922357499599457, 0.32868894934654236, -0.15606126189231873, 0.6217033267021179, -0.36705392599105835, 0.2420923262834549, -0.8452153205871582, -0.05474831908941269, 1.2860703468322754, 0.04831543564796448, 0.4606207311153412, -0.8431724905967712, 0.21739444136619568, 0.03265094757080078, 0.45419833064079285, -0.04265134036540985, -0.17462177574634552, 0.004808266181498766, -0.44073253870010376, -1.1150448322296143, -0.14789406955242157, 0.376754492521286, -0.5150468945503235, 0.7756525278091431, -0.5704758763313293, 0.006909467745572329, 0.4056185781955719, 0.15602494776248932, 0.33463844656944275, 0.5794224739074707, 0.27185577154159546, -0.3136788606643677, 0.4170670509338379, -0.41124391555786133, -0.4134865701198578, -0.8129761219024658, 1.2638838291168213, 0.5210708975791931, -0.14629411697387695, -0.28040140867233276, -1.4629757404327393, -0.6864411234855652, -0.8549233078956604, 0.20822910964488983, -0.1963297426700592, -0.3876255750656128, 1.0161489248275757, 0.10264363884925842, -1.190298318862915, 0.4054545760154724, -0.40472397208213806, -0.1731555461883545, 0.030696989968419075, 0.19630268216133118, 0.31760847568511963, -0.33505430817604065, -1.1813749074935913, 0.07558152824640274, 0.07246746867895126, -0.514920711517334, -0.4779221713542938, -0.6503089666366577, -1.3215253353118896, 0.03091861493885517, 0.5979715585708618, -0.181386798620224, 1.4323800802230835, 0.5260354280471802, -1.1589887142181396, 0.7370729446411133, -0.4110541045665741, -0.3913736045360565, 0.13184122741222382, 0.07007460296154022, -0.34876546263694763, -0.6295304298400879, 0.15034708380699158, 0.31175217032432556, 0.21197809278964996, -0.47020572423934937, -0.16681073606014252, -0.2840369939804077, -0.1489384025335312, -0.4034002125263214, 0.2860557436943054, 0.838236927986145, 0.07481461018323898, 0.6279814839363098, 0.29777586460113525, 1.0724389553070068, -0.08235010504722595, -0.30340486764907837, -0.21954046189785004, -1.409004807472229, 0.5167482495307922, 0.382641077041626, 1.5603206157684326, -0.763827383518219, -0.5112746953964233, -0.31071096658706665, -0.07368283718824387, -0.3948407471179962, -0.49603307247161865, 0.7590503692626953, -0.6482280492782593, 0.6558259129524231, -0.20547530055046082, -0.8790817260742188, -0.3059808313846588, -0.2813316285610199, -1.3215411901474, -0.22880740463733673, 0.06942498683929443, 0.9815393090248108, -0.6756070852279663, -0.2354038506746292, -0.01443958654999733, -0.2809225916862488, -0.6675931215286255, 1.382916808128357, -0.3198414742946625, -0.15951450169086456, -0.27579882740974426, 0.0672604963183403, -0.1711413711309433, -0.3499566912651062, 0.6971630454063416, -0.025266146287322044, -0.6969322562217712, 0.6946038603782654, -0.5537887811660767, 0.8973921537399292, -0.05145074427127838, -0.022814761847257614, -0.03919220715761185, -0.7651770114898682, 0.10877969115972519, 0.3136237859725952, -0.46103596687316895, -0.9208452105522156, -0.010951627977192402, 0.27959778904914856, -0.3338102400302887, 0.14981403946876526, 0.5592142939567566, 0.5281321406364441, -0.6977897882461548, 0.33499908447265625, 0.37079933285713196, -0.4029933214187622, 0.6388636827468872, 0.6531345844268799, 0.9518651366233826, 0.26567158102989197, 0.25717079639434814, -0.09125754237174988, 0.4733414947986603, -0.6564286351203918, 0.5100243091583252, 0.532367467880249, 0.6013540625572205, 0.5425804853439331, 0.9988827109336853, -0.6667901277542114, -0.47773733735084534, 0.004899727646261454, 0.21061886847019196, 1.26534903049469, 0.05098693445324898, -0.759558379650116, -0.25870147347450256, -0.19378134608268738, -0.45352715253829956, 0.5087122321128845, -0.33114349842071533, -0.2030823826789856, -0.725763201713562, -0.6344757676124573, 1.083414077758789, 0.6594034433364868, 1.0738096237182617, -0.7481229901313782, -0.40989550948143005, -0.42941519618034363, 0.4186601936817169, -0.49436551332473755, 0.09968718141317368, 0.44813641905784607, -0.8183928728103638, -0.20934976637363434, 0.6321364641189575, -0.22095149755477905, -0.10631678253412247, -0.8746225833892822, 0.6731879711151123, -0.17412732541561127, -0.3108557462692261, 0.22966796159744263, 1.0377706289291382, -0.24435022473335266, -0.5897886157035828, 0.15264587104320526, -0.1678624302148819, -0.47767728567123413, 0.29672327637672424, 0.6236767172813416, 0.24608168005943298, 0.32288312911987305, -0.4055822193622589, -0.1570042073726654, -0.10385135561227798, 0.18334433436393738, 0.37418413162231445, 0.11732687801122665, -0.22508062422275543, -1.2005462646484375, 0.6553874611854553, 0.22378060221672058, -0.9842379689216614, 0.5782617330551147, -0.8328209519386292, -0.1381078064441681, 0.35733240842819214, -0.33026739954948425, -0.08146078139543533, -1.5715268850326538, 0.8390876650810242, -0.0524984709918499, 0.05014105886220932, 0.4350433349609375, 0.22412611544132233, 0.6043990850448608, -0.025550635531544685, 0.5031505823135376, 0.4822567105293274, 0.2173365205526352, 0.35479530692100525, -0.8934639096260071, 0.274257093667984, 0.1076393872499466, 0.25105562806129456, -0.3385765552520752, 0.128672793507576, -0.3452472686767578, -0.4399857521057129, 0.007974408566951752, 0.21815107762813568, 0.035964421927928925, -0.26606953144073486, -0.5952649116516113, -1.3929734230041504, 0.4479585886001587, -0.7897945642471313, -0.8161889910697937, 0.09214793890714645, -0.9833276867866516, -0.07667909562587738, -1.381682276725769, -1.2968008518218994, -1.0892163515090942, -0.27028828859329224, -0.767678439617157, 0.5835874676704407, -0.12211544811725616, -0.2897166609764099, -0.3605252206325531, 0.09612729400396347, -0.4962976574897766, 0.759175181388855, -0.5624286532402039, 1.331758737564087, -0.01612805388867855, -0.7275835871696472, 0.014803593046963215, 0.4573681950569153, -0.05085575208067894, -0.5152660608291626, 0.3345256447792053, -0.38517001271247864, 0.32027870416641235, -0.030326301231980324, -0.655739426612854, -0.1807367354631424, 0.13663743436336517, 0.9179837703704834, 0.16773802042007446, -0.7145847678184509, -0.19463038444519043, 1.1614792346954346, 0.06088665500283241, 0.6798838376998901, 0.5169052481651306, 0.6509713530540466, 0.7147711515426636, 0.34877389669418335, 0.43644285202026367, 0.5413970947265625, 0.21804071962833405, 0.18161548674106598, 0.3260694146156311, 0.5794821977615356, -0.6061937808990479, 0.7220287322998047, 1.044663429260254, 0.3746906518936157, -0.07656230032444, -1.2192416191101074, 0.6888806223869324, -1.3466604948043823, -0.7724961042404175, 0.8099107146263123, 0.9525517821311951, 0.398285448551178, -0.3021043837070465, -0.28705930709838867, 0.5258405208587646, 0.5005278587341309, 0.27675965428352356, -0.2601097822189331, -0.5929210186004639, 0.11702729016542435, 0.49358007311820984, 0.13608984649181366, 0.618703305721283, 0.1562494933605194, 0.6403639316558838, 15.109910011291504, 0.4553665816783905, -0.04075285419821739, 0.1821492314338684, 0.8967437148094177, 0.45537081360816956, -0.3170687258243561, -0.1241203099489212, -1.2934324741363525, -0.4553912878036499, 1.5726358890533447, 0.014174168929457664, 0.5594473481178284, -0.008385386317968369, -0.5243624448776245, -0.06401025503873825, -0.7498993277549744, 0.18921007215976715, 0.2647651433944702, -1.1530507802963257, 0.5371989011764526, 0.17754504084587097, -0.0833149403333664, 0.1899576187133789, 1.0264602899551392, 0.8925546407699585, 0.6872157454490662, -0.6015183925628662, 0.6654627919197083, 0.18049024045467377, 1.2210899591445923, -0.2578391134738922, 0.8573002815246582, 0.1821189820766449, -0.7897370457649231, -0.6243113875389099, -0.09918513149023056, -1.1930127143859863, -0.12696315348148346, -0.32256272435188293, -0.4294612407684326, -0.6993820071220398, -0.012994460761547089, 0.6690475344657898, 0.4016846716403961, 0.16754291951656342, -0.5600966215133667, 0.5434210300445557, 0.01707502081990242, 0.054896410554647446, -0.22720296680927277, 0.57393878698349, 0.025394132360816002, -0.8381955623626709, 0.13087448477745056, 0.011444596573710442, -0.21924123167991638, 0.33719778060913086, -0.4812794327735901, -0.2513909637928009, -0.7250010371208191, -0.6739903092384338, 0.4085181951522827, 0.5704994201660156, 0.5060523748397827, 0.48272183537483215, -0.02264871820807457, -0.2719256579875946, 0.48030373454093933, -0.07194467633962631, -0.2560516595840454, -0.20138290524482727, 0.42560774087905884, 0.03766762837767601, -0.24014295637607574, 0.6194626092910767, -0.205257385969162, -0.5021717548370361, -0.8719043731689453, -0.3434014618396759, 0.49669596552848816, -0.7794609665870667, -0.31262531876564026, 0.702763557434082, 0.027880053967237473, -0.2714290916919708, -0.24931111931800842, -0.6048041582107544, -0.42489689588546753, -0.04784722626209259, -1.0305535793304443, -0.7422943115234375, 0.22638405859470367, 0.026880761608481407, -0.5662826299667358, 0.2167038768529892, 1.2228261232376099, -0.30609187483787537, -0.6638871431350708, 0.05093201622366905, -0.3091104328632355, -0.08106938004493713, -0.09994463622570038, -1.1349637508392334, 0.5533192157745361, 0.28428447246551514, -0.22378751635551453, 0.8980143666267395, 0.1697121113538742, 0.2959541976451874, -1.0015454292297363, 0.012233165092766285, 0.938453197479248, -1.258069396018982, 0.26626795530319214, -0.9666395783424377, -0.9806942939758301, 0.6910635232925415, 0.2718917727470398, -0.3367461860179901, -0.037351567298173904, 0.3774649202823639, -0.5086031556129456, -0.6882363557815552, -0.8785068988800049, 0.12379376590251923, 0.8333275318145752, -0.8972352147102356, -0.45173296332359314, -0.43159955739974976, 0.3788979947566986, -0.7998217940330505, -0.3987674415111542, 0.15417318046092987, 0.02623683586716652, -0.034862831234931946, 0.5057601928710938, -0.19260387122631073, 0.8030789494514465, 0.4701343774795532, 0.05921247974038124, -0.22489532828330994, -0.01564081758260727, -1.0196515321731567, 0.11528497189283371, 0.8183587789535522, 1.0442439317703247, -0.9327899217605591, 0.05574563890695572, 1.2529956102371216, 0.35982757806777954, -0.2324388325214386, -0.6560614109039307, 0.15322962403297424, 0.07708297669887543, -0.2628556489944458, 0.6151864528656006, -0.2546105682849884, 0.2067856639623642, 0.3267180323600769, 0.49118179082870483, 0.6793842911720276, -0.5736589431762695, -0.346028596162796, 0.32219165563583374, -0.2965163588523865, 0.41532132029533386, -0.2405044138431549, -0.5368995070457458, -0.8340121507644653, -0.11679448187351227, -0.7279208898544312, 0.31133174896240234, -0.8712072968482971, -0.4395720958709717, -0.11278966069221497, -0.4425226151943207, 0.18200546503067017, 0.9823017716407776, -0.5071533918380737, -1.000769019126892, -0.5135915875434875, -1.0234073400497437, 0.65580153465271, 0.19553659856319427, -0.6285172700881958, -0.007637511007487774, -0.30005189776420593, -0.15039703249931335, -0.03194515407085419, 0.39029332995414734, -0.4894179403781891, -0.6902045011520386, -0.7408252954483032, 0.606763482093811, 0.18785661458969116, -0.33029860258102417, -0.43008488416671753, 0.6632251739501953, 0.46190300583839417, -0.2927250266075134, -0.14453212916851044, 0.0012954405974596739, -0.6839509606361389, -0.2612066864967346, 0.1562822312116623, -0.8003422617912292, 0.30519869923591614, 0.23842179775238037, -0.856742799282074, -0.40474414825439453, 0.7023293375968933, -0.17600959539413452, -0.7628186345100403, -0.42184242606163025, 0.15453080832958221, -1.0203243494033813, -0.028785541653633118, -0.06244455277919769, -0.0346684604883194, -1.2744380235671997, 0.07397382706403732, 0.33693796396255493, 0.8664013743400574, -0.03899271413683891, 0.7486417293548584, 0.18153001368045807, -0.7798355221748352, 0.5080686807632446, 0.3520107567310333, -0.05398847162723541, 0.3098430931568146, 0.26254111528396606, 0.3250158429145813, -0.08616579324007034, 0.5734926462173462, 0.43010157346725464, 0.28607308864593506, -0.961958646774292, 0.025121308863162994, 0.7032690048217773, -0.6854604482650757, -0.1093626543879509, 0.9086841344833374, -0.0700775757431984, -0.84840989112854, 0.2699851989746094, -1.2510257959365845, -0.9130178093910217, -0.3130488693714142, 0.8565861582756042, 0.18780723214149475, -0.574970543384552, 0.30554309487342834, -0.6555489897727966, 0.4399983286857605, -0.37484124302864075, -0.43559426069259644, 0.831754744052887, 0.008526593446731567, -1.0130677223205566, 1.1653121709823608, 0.025655563920736313, -0.40619027614593506, -0.22157885134220123, -0.4184625744819641, -0.08142674714326859, 0.09434561431407928, 0.055508702993392944, -0.37311413884162903, -0.14518727362155914, 0.6116902828216553, 0.42971619963645935, 0.6526817679405212, -0.11880040168762207, 0.04352633282542229, -0.3564832806587219, 0.5832632780075073, 0.6929942965507507, -0.11408384889364243, -0.47087985277175903, 0.9363855719566345, 1.2561126947402954, -0.560703456401825, 0.13902728259563446, -0.13230454921722412, -0.7547296285629272, 0.8191512227058411, 0.7151262164115906, 0.013220526278018951, 0.3010134696960449, 0.11384094506502151, -0.007788042537868023, -0.18637390434741974, -1.576446771621704, -0.21251177787780762, -0.259176105260849, 0.9481829404830933, 0.5697289109230042, 0.06402710825204849, 0.14401347935199738, 0.9043149352073669, 0.4000440835952759, 0.2130434364080429, 0.65416419506073, 0.72660893201828, -0.11534332484006882, -0.49288246035575867, 0.2654799818992615, 0.5147930383682251, -0.7063929438591003, -0.4700758755207062, -0.095023974776268, 0.9939090609550476, -0.1747112274169922, 0.7455227971076965, 0.9796087741851807, 0.02270709164440632, 0.3517877459526062, 0.563182532787323, 0.4629845917224884, -0.5803012251853943, -0.17174743115901947, -0.3891861140727997, -0.27017149329185486, -0.2495756298303604, -0.2728428542613983, -0.7713163495063782, -0.6508058905601501, -0.19115665555000305, 0.1029336079955101, 0.6134382486343384, -0.16795367002487183, 0.9240353107452393, 0.4553748369216919, 0.22611680626869202, -0.17871132493019104, 0.012644032947719097, -0.2236500233411789, -0.8876183032989502, -0.07619632035493851, -1.0211142301559448, -0.18760766088962555, -0.2757742702960968, -0.5331155061721802, -0.7018023133277893]}, "authors": [{"authorId": "123467107", "name": "Jishnu Ray Chowdhury"}, {"authorId": "2140493460", "name": "Cornelia Caragea"}], "references": [{"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "8420fddf489bd7c5b822bd904aa11ff3742bfb78", "title": "On the Long Range Abilities of Transformers"}, {"paperId": "2fc074288f66711e4ee37350d364e74c1c401163", "title": "Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability"}, {"paperId": "434d751d355d7a7c20efa570e785c76286245e77", "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"}, {"paperId": "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b", "title": "What Algorithms can Transformers Learn? A Study in Length Generalization"}, {"paperId": "bcd84a2b8f9ae40a908f375425f113c82f8dd739", "title": "Sparse Universal Transformer"}, {"paperId": "9e3e56957e249cdebdd8673fd1174980ed694560", "title": "Efficient Beam Tree Recursion"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "d2d0371158803df93a249c9f7237ffd79b875816", "title": "Sparse Modular Activation for Efficient Sequence Modeling"}, {"paperId": "0a067fab18c67d4a386efa846c080f8afff5e8f3", "title": "Block-State Transformers"}, {"paperId": "d40dbe668d5b68419e934dfa4c5851ffa1c24aa2", "title": "Exposing Attention Glitches with Flip-Flop Language Modeling"}, {"paperId": "526706ca6db9800d103bf87b104e98bc4ae4feed", "title": "Beam Tree Recursive Cells"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "465ec2212d865e875e64638b3dd1ecaac21c5ddd", "title": "Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "1f346f74e8eabececa4896d734ab9b261f30830d", "title": "Modular Deep Learning"}, {"paperId": "24576dcca716c82f66b8cc3c85ecfae18be41edd", "title": "Adaptive Computation with Elastic Input Sequence"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "fb6d75a4f3b1af2058f59957116c178a47b56f05", "title": "Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions"}, {"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "title": "Transformers Learn Shortcuts to Automata"}, {"paperId": "f024706d6e60aa87312f790f579e0e4e4d59d7a3", "title": "Neural Attentive Circuits"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "d6a0dfd5f39222d8924b7727a0a49f81fa247d71", "title": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning"}, {"paperId": "216b05e812896e790d5b5a084614e2523daa198e", "title": "Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "e528466e2aff981511d4ca6e063211297c0b4175", "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"}, {"paperId": "8ccaf0c0fbd5e4079f36fa720cf23890be10dd66", "title": "Dynamic Inference with Neural Interpreters"}, {"paperId": "349eb17c5b61924db8ccc5816c863c6674c8b565", "title": "Saturated Transformers are Constant-Depth Threshold Circuits"}, {"paperId": "db016d2b6d2577c47d62f9de2a7d1ddbf226386a", "title": "Modeling Hierarchical Structures with Continuous Recursive Neural Networks"}, {"paperId": "b50815251c948f00baedccaf5f56c281ffa7650f", "title": "Staircase Attention for Recurrent Processing of Sequences"}, {"paperId": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "837ac4ed6825502f0460caec45e12e734c85b113", "title": "Dynamic Neural Networks: A Survey"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "67ee20536c30a225b86902af2f091e28e5e19b40", "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling"}, {"paperId": "ad833d73b13e21189512df62dde4db37a980a333", "title": "Neurocoder: Learning General-Purpose Computation Using Stored Neural Programs"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "70557ea6b65846fc30729ceed224acd4ac64ca5d", "title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "287e85aca777d6d3d73e1484ba9c0f09d40f578a", "title": "Ordered Memory"}, {"paperId": "4585611042d2be0d997ee135e3fe219d668db9ec", "title": "Depth-Adaptive Transformer"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "9a618cca0d2fc78db1be1aed70517401cb3f3859", "title": "Deep Equilibrium Models"}, {"paperId": "41dd3dd3b9e39c91eb975d91d2d019cb9024015b", "title": "Recursive Routing Networks: Learning to Compose Modules for Language Understanding"}, {"paperId": "b0e2fe0fe9f4fc4ce05d5f637baff96a7e966c01", "title": "Cooperative Learning of Disjoint Syntax and Semantics"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "997c55547aeca733dfc5dfebd12412612ecba022", "title": "The Importance of Being Recurrent for Modeling Hierarchical Structure"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "eae22611309fd49da6afca531a3237ea5ea1a0c6", "title": "Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition"}, {"paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "title": "Neural GPUs Learn Algorithms"}, {"paperId": "04d1a26c2516dc14a765112a63ec60dc3cb3de72", "title": "Tree-Structured Composition in Neural Networks without Tree-Structured Architectures"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "152d82025f02916019e4cfcc943dceecc159cda4", "title": "Self-Delimiting Neural Networks"}, {"paperId": "0bd2602df71e89c8961562175c8759e625e99389", "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data"}, {"paperId": "c7c4f25ad05454dac375a9112e944b2fc0aa75d2", "title": "Early Exit with Disentangled Representation and Equiangular Tight Frame"}, {"paperId": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9", "title": "Scaling TransNormer to 175 Billion Parameters"}, {"paperId": null, "title": "Pon-dernet: Learning to ponder"}, {"paperId": null, "title": "Skyformer: Re-model self-attention with gaussian kernel and nystr\u00a8om method"}, {"paperId": null, "title": "Addressing some limitations of transformers with feed-back memory"}, {"paperId": null, "title": "Do transformer modi\ufb01cations transfer across implementations and applications?"}, {"paperId": null, "title": "Teaching ALBERT to ponder"}, {"paperId": null, "title": "Recurrent Transformers with Dynamic d8e1344e27a5b08cdfd5d027d9b8d6de-Paper"}, {"paperId": null, "title": "Ef\ufb01cient transformers: A survey"}, {"paperId": null, "title": "e9a32fade47b906de908431991440f7c-Paper-Conference"}, {"paperId": null, "title": "BERxiT: Early exiting for BERT with better \ufb01ne-tuning and extension"}]}