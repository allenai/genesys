{"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "abstract": "Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "citationCount": 21, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://aclanthology.org/2022.acl-long.515.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work shows that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and it outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss."}, "embedding": {"model": "specter_v2", "vector": [0.4843541383743286, 1.0920573472976685, -0.4799349904060364, 0.41589581966400146, -0.3075440526008606, -0.20717045664787292, 1.0844279527664185, -0.08721356093883514, -0.433188259601593, 0.3615615665912628, 0.5370732545852661, -0.5546373128890991, 0.4987313151359558, -0.08721954375505447, -0.18130820989608765, -0.14272357523441315, -0.55743008852005, 0.6430126428604126, -0.06375100463628769, 0.1668938845396042, 0.2744210362434387, -0.6261014938354492, -0.8161731958389282, 0.12716901302337646, 0.1853443831205368, 0.5508940815925598, 0.5129537582397461, 0.8302112817764282, -0.4649159908294678, 0.7342555522918701, 0.35447531938552856, -0.09332572668790817, -0.2610255479812622, -0.126125305891037, -0.7678869366645813, -0.4697410762310028, 0.17614296078681946, 0.06661316007375717, -0.25891274213790894, 0.7281676530838013, -0.24450860917568207, 0.25253432989120483, 0.06308577954769135, -0.41658684611320496, -0.790248692035675, 1.0127497911453247, 0.3794955015182495, 1.1026549339294434, -0.10424473136663437, -0.3536384403705597, 1.4825937747955322, -1.215963363647461, 0.014299724251031876, 1.4804291725158691, 0.20098379254341125, 0.5785571336746216, -0.41456684470176697, -0.40827032923698425, 1.220285177230835, 0.5619834065437317, -1.079131007194519, -0.0884491577744484, -0.24902425706386566, 0.21265371143817902, 1.9653245210647583, -0.2005300372838974, 0.4752674996852875, 0.6218254566192627, -0.14453710615634918, 1.3802556991577148, -0.024054592475295067, -1.1755949258804321, -0.35523292422294617, -0.06180775910615921, 0.4879724383354187, 0.6404762268066406, -0.27216318249702454, 0.43331342935562134, -0.6440802812576294, -0.1868842989206314, 0.14139807224273682, -0.05026686191558838, -0.07880762219429016, -0.10670343041419983, -0.30384761095046997, 0.8205153346061707, 0.11667047441005707, 0.8892874121665955, -0.23034538328647614, 0.7167064547538757, 0.07080616056919098, 0.013608007691800594, -0.04490363225340843, 0.5979230999946594, -0.036992475390434265, 0.43744274973869324, -0.8722946643829346, 0.007720490451902151, -0.29611021280288696, 1.3594013452529907, -0.21560271084308624, 0.4557662308216095, -0.7196441888809204, 0.14632147550582886, 0.9749430418014526, 0.5473986864089966, 0.5924293398857117, -0.4161399006843567, -0.12947799265384674, -0.464755117893219, 0.20266610383987427, -0.7756492495536804, -0.21224305033683777, 0.11349830031394958, -0.5646626353263855, -1.343483567237854, -0.5564862489700317, 0.14951612055301666, -0.7170084118843079, 0.8055067658424377, -0.48193055391311646, -0.012038730084896088, -0.45874571800231934, 0.318740576505661, 0.09861596673727036, 0.5577787756919861, 0.5178719162940979, 0.05242833122611046, 1.1547739505767822, -0.6605181097984314, -0.9035298228263855, -1.4914077520370483, 0.2772960364818573, -0.1037466824054718, 0.7733317017555237, -0.24976639449596405, -1.1408644914627075, -0.7481343746185303, -0.7534374594688416, -0.03137218579649925, -0.7095394730567932, 0.059922825545072556, 0.8616316318511963, 0.5649168491363525, -0.8127607703208923, 0.014647604897618294, -0.4117242395877838, -0.27689871191978455, 0.5347010493278503, 0.26115578413009644, 0.4190467298030853, -0.34831681847572327, -1.2714647054672241, 0.8670951128005981, 0.15046724677085876, -0.3211480379104614, -0.5986035466194153, -0.7190988659858704, -1.0919060707092285, 0.04575824365019798, 0.4746456742286682, -0.562704861164093, 1.0520285367965698, -0.4446800947189331, -0.9819788336753845, 0.6305726170539856, -0.5197873711585999, -0.11017399281263351, 0.0004110165173187852, -0.685911238193512, -0.47645699977874756, -0.3965431749820709, 0.1080184280872345, 0.4018060564994812, 0.41319960355758667, 0.09513111412525177, -0.37707436084747314, 0.09380295872688293, -0.552487850189209, -0.5652917623519897, -0.10856466740369797, 0.7743052840232849, -0.43197980523109436, 0.027164507657289505, -0.15284036099910736, 0.5136439204216003, 0.06077495589852333, -0.49581795930862427, -0.3169209361076355, -0.7772026658058167, 0.7137085795402527, 0.12841971218585968, 1.2989400625228882, -0.6811678409576416, -0.4628015160560608, -0.3562186062335968, -0.38294515013694763, -0.14370523393154144, -0.5195799469947815, 0.14518442749977112, -0.5432374477386475, 0.8517253398895264, -0.39236631989479065, -0.7644089460372925, 0.10894817113876343, -0.12908188998699188, -0.945419192314148, -0.04993012547492981, 0.537923276424408, 0.9770891666412354, -0.798445999622345, -0.26043668389320374, -0.08787084370851517, 0.01213894784450531, -1.0347044467926025, 1.4181326627731323, -0.4151856601238251, 0.1783720850944519, 0.009850251488387585, -0.09238120168447495, 0.2050454467535019, -0.45368245244026184, 0.47467055916786194, -0.05102455988526344, -0.3914740979671478, 0.29471254348754883, -0.09452113509178162, 0.6832518577575684, -0.5513564348220825, 0.9661222696304321, 0.14169266819953918, -0.9234444499015808, 0.21243798732757568, 0.20241717994213104, -0.6045129299163818, -0.6209404468536377, 0.12069123983383179, 0.066123366355896, -0.47314193844795227, 0.034622274339199066, 0.6863517761230469, 0.9402317404747009, -0.39025965332984924, 0.14441703259944916, 0.532564103603363, -0.42680537700653076, 0.04981639236211777, 0.5083388090133667, 0.651689887046814, 0.2511152923107147, 0.8389355540275574, -0.18585273623466492, 0.1405583918094635, -0.7917031645774841, -0.27465224266052246, 0.9272949695587158, 0.7516937851905823, 0.849117636680603, 0.5415226221084595, -0.8763628005981445, -0.6606914401054382, 0.3238575756549835, 0.5448808073997498, 1.4523612260818481, -0.18036524951457977, 0.1502208411693573, -0.6799534559249878, -0.07293885946273804, -0.6408438682556152, 0.40202388167381287, -0.7221454381942749, -0.2284650206565857, -0.7602394223213196, -1.177266240119934, 0.94417405128479, 0.4478479325771332, 0.9257203936576843, -0.9010147452354431, -0.27362075448036194, -0.28146088123321533, 0.1895551085472107, -0.9388371109962463, -0.5735220313072205, 0.2203584760427475, -0.04760311543941498, -0.26194992661476135, 0.32002758979797363, -0.1464814394712448, 0.05424676090478897, -0.6566471457481384, 1.122369647026062, -0.33618754148483276, -0.17971323430538177, 0.5068551301956177, 0.6077302694320679, -0.9311473965644836, -0.1007796972990036, 0.4454430937767029, -0.18622374534606934, 0.27056390047073364, 0.6688598990440369, 0.49029645323753357, -0.08206232637166977, -0.13147591054439545, -0.513982355594635, -0.053359873592853546, 0.09797847270965576, 0.5589960217475891, 0.598033607006073, -0.8213884830474854, -0.09901268035173416, -1.1729319095611572, 0.5139827132225037, 0.06706324219703674, -0.7046536207199097, 0.14705778658390045, -0.5900930166244507, -0.47815921902656555, 0.06988359987735748, -0.7698648571968079, -0.316057026386261, -0.6688950061798096, 0.3266710937023163, -0.36687803268432617, -0.22986149787902832, 0.26907384395599365, -0.13272550702095032, 0.5267773866653442, -0.12008331716060638, 0.6887890100479126, 0.004313770215958357, -0.0051981620490550995, 0.7443163394927979, -0.7812595963478088, 0.48449209332466125, 0.223594531416893, -0.22172336280345917, 0.18284520506858826, -0.32712990045547485, -0.9200599193572998, -0.7650623917579651, 0.08690562099218369, -0.18083100020885468, 0.013851696625351906, 0.4247782230377197, -0.40990495681762695, -0.8715822696685791, -0.3246644139289856, -1.385486364364624, -0.5411720871925354, 0.15030650794506073, -0.2892639935016632, -0.22046677768230438, -1.1272693872451782, -0.7939118146896362, -0.5967350602149963, -0.5214687585830688, -0.5678080320358276, 0.11405383050441742, -0.14990657567977905, -0.6847481727600098, -0.4958510100841522, -0.018376978114247322, -0.2572167217731476, 0.9540590643882751, -0.6229627728462219, 1.0279253721237183, -0.34911781549453735, -0.46475714445114136, -0.21552030742168427, 0.27047643065452576, -0.09159130603075027, -0.28268876671791077, -0.024010872468352318, -0.9384371042251587, 0.6541993021965027, -0.3036051094532013, -0.036718063056468964, 0.002869540126994252, 0.37219908833503723, 0.842132031917572, -0.14896872639656067, -0.552596926689148, 0.0485370047390461, 0.8713623285293579, -0.642301082611084, 0.3953767418861389, 0.5180208683013916, 0.9494384527206421, -0.05999305471777916, -0.18411506712436676, 0.4072095453739166, 0.12003524601459503, 0.6457282304763794, 0.3836289346218109, -0.2001066654920578, 0.12161310762166977, -0.46209388971328735, 0.14685818552970886, 0.7868337631225586, 0.09806942194700241, -0.053746793419122696, -1.3024235963821411, 0.8958593606948853, -1.1646095514297485, -0.979732096195221, 0.7425940632820129, 0.6194968819618225, 0.39774999022483826, -0.5081053376197815, -0.48383331298828125, -0.3127039074897766, 0.9203606247901917, 0.165531724691391, -0.21417886018753052, -0.7287939190864563, -0.18868109583854675, 0.37489449977874756, 0.20773066580295563, 1.1145193576812744, -0.3508591055870056, 0.8140764832496643, 15.140811920166016, 0.6319491267204285, -0.10474303364753723, 0.9340561628341675, 0.569379985332489, -0.2425537258386612, -0.16554096341133118, 0.018015718087553978, -1.1746493577957153, -0.016138341277837753, 1.1168133020401, 0.08731658011674881, 0.592861533164978, 0.15445613861083984, 0.08387400954961777, 0.15867844223976135, -0.6863322257995605, 0.4563820958137512, 0.6066766381263733, -1.100839614868164, 0.5860654711723328, 0.2822868824005127, 0.13537855446338654, 0.3015688955783844, 0.7487761974334717, 0.6241769790649414, 0.71489417552948, -0.5865520238876343, 0.9309808611869812, 0.4223569631576538, 0.8599412441253662, -0.1833002120256424, 0.41783958673477173, 0.3179950714111328, -0.7266796827316284, -0.5762292146682739, -0.6571348905563354, -1.0298267602920532, 0.41113170981407166, -0.08452826738357544, -0.056593235582113266, -0.8275452852249146, -0.11748604476451874, 0.25832146406173706, 0.08492046594619751, 0.1435941457748413, -0.4398367702960968, 0.8611190915107727, 0.2509032189846039, -0.1374594271183014, 0.4093499481678009, 0.7347843647003174, 0.2695270776748657, 0.3762343227863312, -0.07068078964948654, 0.3810172975063324, -0.08397752791643143, 0.7004829049110413, -0.5133172273635864, -0.4168119430541992, -0.4242713451385498, -0.27499252557754517, 0.4892953038215637, 0.444562166929245, 0.5293705463409424, 0.22716231644153595, -0.5563322305679321, -0.15944044291973114, 0.45285534858703613, -0.0012336226645857096, -0.5857517123222351, -0.230230912566185, 0.061111930757761, -0.12759046256542206, 0.49679577350616455, 0.6207370162010193, -0.3073784112930298, -0.2543955147266388, -1.0213661193847656, -0.5483791828155518, 0.5510573387145996, -0.9076967239379883, -0.5259898900985718, 0.9707723259925842, -0.47315514087677, 0.0006632065051235259, 0.5123375654220581, -0.6412976384162903, -0.08938734233379364, 0.49117788672447205, -1.0626195669174194, -0.7641357779502869, 0.2612924873828888, -0.09783107042312622, -0.3369649648666382, -0.15262067317962646, 1.280436396598816, 0.13103029131889343, -0.45314228534698486, -0.21880221366882324, -0.47043323516845703, 0.0346103236079216, -0.36809682846069336, -0.8853526711463928, 0.4667355418205261, 0.14569821953773499, -0.17533445358276367, 0.39757049083709717, 0.08178853988647461, 0.3244330585002899, -0.5818968415260315, 0.020842796191573143, 0.7023823857307434, -1.3165079355239868, 0.030888210982084274, -0.7683411836624146, -0.8102712035179138, 0.8423961400985718, 0.7088603377342224, 0.00769767677411437, 0.24599456787109375, 0.34117522835731506, -0.473154217004776, -0.09230277687311172, -0.3970308005809784, 0.12113434076309204, 0.16108818352222443, -0.909250795841217, -0.8191946148872375, -0.3422609865665436, 0.7173289060592651, -0.7884570956230164, -0.10715841501951218, -0.5510300993919373, 0.08302322775125504, 0.2632404863834381, 0.9065374732017517, -0.5208503007888794, 0.36896586418151855, 0.6774899959564209, 0.029009103775024414, -0.6879984140396118, -0.3572376072406769, -0.8920430541038513, -0.16443517804145813, 0.269666463136673, 0.8598555326461792, -0.4501033127307892, -0.06135692819952965, 0.8246557712554932, 0.04031388834118843, -0.12007986009120941, -0.9140605330467224, -0.20557688176631927, 0.08448213338851929, -0.401827871799469, 0.7745428085327148, 0.06142149120569229, 0.17867842316627502, 0.4138363003730774, 0.2785697281360626, 0.9548110365867615, -0.48355400562286377, -0.3317073583602905, -0.046128641813993454, -0.04216884449124336, 0.3788788616657257, -0.6567285060882568, -0.6510189771652222, -1.463824987411499, 0.10106637328863144, -0.5952441096305847, 0.09617156535387039, -0.541261613368988, -0.17362858355045319, 0.4084964394569397, -0.10044309496879578, 0.13097544014453888, 0.17818216979503632, -0.39153003692626953, -0.5247195959091187, -0.6700799465179443, -0.7265076041221619, 0.6964853405952454, 0.5189016461372375, -1.0158802270889282, 0.2773187458515167, -0.3038615882396698, -0.1562085896730423, 0.30605554580688477, 0.280775249004364, -0.6465824842453003, -0.2536819875240326, -1.4177193641662598, 0.3594443202018738, -0.11746184527873993, -0.013849250972270966, -0.25525155663490295, 0.7941288948059082, 0.3767431080341339, -0.09600692242383957, -0.05189507082104683, 0.305916965007782, -0.8133913278579712, -0.669976532459259, 0.27748337388038635, -1.023169755935669, 0.4253135919570923, 0.4993899464607239, -0.45719751715660095, -0.392410546541214, 0.7243650555610657, -0.02529863268136978, -1.1376657485961914, -0.42408716678619385, 0.2525237798690796, -1.3380261659622192, 0.5432826280593872, -0.29978442192077637, -0.13393862545490265, -1.0285724401474, -0.3588956296443939, -0.35873496532440186, 0.1715342402458191, -0.6804916858673096, 0.7566428184509277, 0.14915725588798523, -0.96969074010849, 0.2248195856809616, 0.1704195737838745, -0.17215268313884735, 0.022274892777204514, 0.10877406597137451, 0.5175295472145081, -0.035771891474723816, 1.0423786640167236, 0.3540763556957245, 0.2650592625141144, -0.8826669454574585, 0.6669132709503174, 0.8257082104682922, -0.1927090734243393, -0.14554454386234283, 0.5560570359230042, -0.2053031176328659, -0.9954656958580017, 0.09976326674222946, -0.9678059220314026, -0.5331271290779114, -0.07094363123178482, 0.9014673829078674, 0.16817396879196167, -0.15377318859100342, 0.24307475984096527, -0.5060684680938721, 0.22401733696460724, 0.26290953159332275, -0.46002888679504395, 0.6006535887718201, -0.595969557762146, -0.5892889499664307, 0.5903320908546448, 0.09134364873170853, -0.3728107810020447, -0.240836963057518, -0.7067210078239441, 0.23111240565776825, -0.07366392016410828, 0.5076262950897217, -0.33157777786254883, -0.6297030448913574, 0.9363359808921814, 0.47120723128318787, 0.5855900645256042, -0.023052992299199104, 0.11425282061100006, -0.5231548547744751, 0.7793869972229004, 0.15301458537578583, -0.49169448018074036, -0.6200956106185913, 1.3357709646224976, 1.6116565465927124, -0.6486946940422058, 0.3752123713493347, -0.039693694561719894, -0.6120268106460571, 0.5508810877799988, 0.5203642249107361, 0.22029747068881989, 0.8040536046028137, -0.039214201271533966, -0.11884085088968277, 0.1005348339676857, -1.110195279121399, -0.3296140134334564, 0.28910166025161743, 1.1225303411483765, 0.5552806258201599, 0.2414703369140625, 0.2322671264410019, 0.9218804240226746, 0.26355525851249695, 0.12956856191158295, 0.5038117170333862, 0.42809706926345825, -0.3560776710510254, 0.11731082946062088, -0.36387011408805847, 0.1007542535662651, -0.7302723526954651, -1.1176526546478271, -0.17346329987049103, 0.9625678062438965, -0.006423426792025566, 0.7918965816497803, 1.4265103340148926, 0.5642926096916199, 0.5493106842041016, 0.3083798587322235, 0.15363170206546783, -0.6901496648788452, -0.06019195169210434, -0.2838805615901947, -0.42533230781555176, -0.19883665442466736, -0.22785526514053345, -0.7560021877288818, 0.0375576950609684, -0.28993210196495056, -0.08466780185699463, 0.3566591143608093, -0.20025236904621124, 0.9772008657455444, 0.7811490297317505, 0.44085589051246643, -0.5361402034759521, -0.5489439964294434, 0.11028090119361877, -1.3543356657028198, 0.24965371191501617, -0.9148654937744141, 0.1399366706609726, -0.3510447144508362, -0.29933464527130127, -0.24159634113311768]}, "authors": [{"authorId": "1818378366", "name": "Hao Peng"}, {"authorId": "11348687", "name": "Jungo Kasai"}, {"authorId": "143958923", "name": "Nikolaos Pappas"}, {"authorId": "1755465", "name": "Dani Yogatama"}, {"authorId": "47039405", "name": "Zhaofeng Wu"}, {"authorId": "47648549", "name": "Lingpeng Kong"}, {"authorId": "4671928", "name": "Roy Schwartz"}, {"authorId": "144365875", "name": "Noah A. Smith"}], "references": [{"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "78ea232dbabc67ca4d6d4a7c1bbf568e9b47cb8a", "title": "Coordination Among Neural Modules Through a Shared Global Workspace"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "0cd82dfae930ac4b57c0e959f744f2d10bf87649", "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "ac6535d096fc79dde2d9ce0329e0626b79ede7f0", "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "07a9f47885cae97efb7b4aa109392128532433da", "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "28a8604d0ca174d885f5bffde6b6b1d3f955f5bd", "title": "Self-Attentive Associative Memory"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "e20ff55e87e2b3ef02ae0529880bb705f5efbcae", "title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks"}, {"paperId": "a56ebc39b8c527774be705cccdcb5f66c7302e0c", "title": "Rational Recurrences"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "title": "A Call for Clarity in Reporting BLEU Scores"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "27981998aaef92952eabef2c1490b926f9150c4f", "title": "Memory Architectures in Recurrent Neural Network Language Models"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "title": "Simple Recurrent Units for Highly Parallelizable Recurrence"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2", "title": "Hierarchical Attention Networks for Document Classification"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "e837b79de602c69395498c1fbbe39bbb4e6f75ad", "title": "Learning to Transduce with Unbounded Memory"}, {"paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "c394b410daf326e178b28502a9ce2e94782ba33c", "title": "Children\u2019s understanding of the relationship between addition and subtraction"}, {"paperId": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "title": "Moses: Open Source Toolkit for Statistical Machine Translation"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": null, "title": "Hyperparameters used in the language modeling experiments"}, {"paperId": null, "title": "2020, inter alia). All queries attend to a subset of n < N \u201cglobal tokens,"}, {"paperId": null, "title": "Table 13 briefly describes the tasks. The readers are referred to Wang et al. (2019) for futher details. 12https://github.com/pytorch/fairseq/ blob/master/examples/roberta/README.glue"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": null, "title": "fling. Other preprocessing"}, {"paperId": null, "title": "Openweb-text corpus"}, {"paperId": null, "title": "When a good translation is wrong in context: Contextaware machine translation improves on deixis, ellipsis, and lexical cohesion"}, {"paperId": null, "title": "QNLI is compiled by GLUE\u2019s"}, {"paperId": null, "title": "2018) and use the dev2010 subset for development and tst2010-2012 for testing. The tokenization is also the same as Miculicich et al. (2018): we tokenize and truecase Spanish"}, {"paperId": null, "title": "First Quora Dataset Release: Question Pairs"}, {"paperId": null, "title": "The preprocessing and data splits of WMT14 EN-DE"}, {"paperId": null, "title": "News dataset available. https://commoncrawl.org/2016/10/ news-dataset-available/, 2016"}, {"paperId": null, "title": "2019), and RealNews (Zellers et al., 2019). Our data differs from RoBERTa\u2019s pretraining data, which we do not have access to. We replace their CC-News (Nagel, 2016) with RealNews, and drop"}, {"paperId": "be0dd2e91bb104494feeb5da2761cf930564f650", "title": "Under review as a conference paper at ICLR 2016"}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": null, "title": "High accuracy protein structure prediction using deep learning"}]}