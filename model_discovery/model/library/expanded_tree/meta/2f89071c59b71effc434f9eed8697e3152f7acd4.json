{"paperId": "2f89071c59b71effc434f9eed8697e3152f7acd4", "abstract": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.", "venue": "AAAI Conference on Artificial Intelligence", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work investigates how attention heads impact fairness and performance in pre-trained transformer-based language models and proposes a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities."}, "embedding": {"model": "specter_v2", "vector": [-0.03652820736169815, 1.0968210697174072, -0.5412744283676147, -0.0016152566531673074, -0.37440571188926697, 0.14117754995822906, 0.5594745874404907, -0.5315150618553162, -0.1070752814412117, 0.12415952980518341, 0.4107937514781952, -0.47818753123283386, 0.011517004109919071, -0.13590221107006073, 0.21891358494758606, -0.00758087495341897, -0.9507116079330444, 0.7636780738830566, -0.35255199670791626, -0.2052009254693985, -0.41692379117012024, -0.46192705631256104, -0.9569842219352722, -0.4267050325870514, 0.7951339483261108, 0.512990415096283, -0.7198887467384338, 0.4749297499656677, -0.3115151524543762, 0.9238048791885376, 0.5408782362937927, -1.0515652894973755, 0.3224847912788391, -0.28280141949653625, -0.32187169790267944, -0.23879390954971313, 0.7098842263221741, -0.4114486575126648, -0.42044752836227417, 0.7443813681602478, 0.1361774206161499, 0.2083793729543686, 0.16337543725967407, -0.9736056923866272, -0.251105934381485, 1.0452381372451782, 0.43928802013397217, 0.8561621904373169, -0.475474089384079, -0.6464694142341614, 1.043215036392212, -0.9877349138259888, 0.06141439452767372, 1.9034143686294556, 0.6195409893989563, 0.5252747535705566, -0.41412612795829773, -1.0783896446228027, 0.04085051640868187, -0.10815607756376266, -0.8122595548629761, -0.6909155249595642, 0.029072098433971405, -0.112280935049057, 1.223635196685791, 0.11669165641069412, -0.22303344309329987, 0.21491333842277527, -0.27100691199302673, 1.0051467418670654, 0.318671315908432, -0.6077733039855957, -0.21053126454353333, 0.09350275248289108, 0.4299062490463257, 1.1161960363388062, -0.05971439182758331, 0.32553285360336304, -1.0681895017623901, -0.9617664217948914, 0.0729026272892952, -0.21714617311954498, 0.3529883325099945, -0.24242118000984192, 0.36397334933280945, 1.0439735651016235, -0.02848723717033863, 0.6625023484230042, 0.0379529669880867, 0.48443594574928284, 0.5252718925476074, 0.11501133441925049, 0.3529159128665924, 0.08237598091363907, 0.09812560677528381, 0.441328227519989, -1.194958209991455, 0.20110854506492615, -0.010077678598463535, 0.5551384687423706, -0.22485944628715515, 0.6480542421340942, -1.083740234375, 0.09541691094636917, 1.518367886543274, 0.38096076250076294, 0.3174390196800232, -0.7906949520111084, 0.09212309867143631, -0.562637209892273, -0.06623590737581253, -0.6416276097297668, -0.01378921139985323, -0.1957748681306839, -0.9291517734527588, -1.3245325088500977, -0.27914589643478394, 0.28613513708114624, -0.9633583426475525, 0.7238131165504456, -0.13390885293483734, 0.05762791261076927, -0.20489098131656647, 0.42711341381073, 0.7058603167533875, 0.0612948052585125, 0.5713251233100891, 0.05538535118103027, 0.8565055131912231, -0.5163232088088989, -0.5134435296058655, -0.9714024066925049, 0.3601613938808441, -0.2561829090118408, -0.10813350975513458, -0.31866350769996643, -1.2884465456008911, -0.7524729371070862, -0.37345147132873535, -0.14264944195747375, 0.03067902848124504, 0.02479291521012783, 0.9404374361038208, 0.9703201651573181, -1.0902947187423706, 0.4408643841743469, -0.2483081966638565, -0.4678279161453247, 0.6819601058959961, 0.40085795521736145, 0.5007258057594299, -0.07291212677955627, -1.3981332778930664, 0.4022725522518158, -0.03389556333422661, -0.6629740595817566, -0.006450664717704058, -0.4697335362434387, -1.1761951446533203, -0.11854685097932816, 0.20798349380493164, -0.48506370186805725, 1.0152699947357178, -0.0032506759744137526, -0.8434494733810425, 1.0289808511734009, -0.5383797287940979, -0.2790747582912445, 0.5017519593238831, 0.009297693148255348, -0.5389339923858643, -0.39944398403167725, 0.05641993507742882, 0.0775328129529953, 0.7055560350418091, -0.19814372062683105, 0.05364757403731346, -0.048420440405607224, -0.4331379532814026, -0.36336684226989746, 0.03281967714428902, 1.358201503753662, -0.20267066359519958, -0.013406770303845406, 0.30573728680610657, 0.5741072297096252, -0.12173786014318466, -0.10533536970615387, -0.6991405487060547, -0.8037484288215637, 0.5296410918235779, -0.7082839012145996, 1.4746613502502441, -0.4698953330516815, -0.771777868270874, 0.05644042044878006, 0.2602394223213196, 0.019162410870194435, -0.008412294089794159, 0.25021907687187195, -0.2098722904920578, 0.7039110064506531, -0.2706097662448883, -1.1113426685333252, 0.28730538487434387, -0.3084302246570587, -0.7359693050384521, -0.005919085815548897, -0.3487088978290558, 0.92672199010849, -0.6647228598594666, -0.24053868651390076, -0.05429203808307648, 0.12522493302822113, -0.8281266689300537, 1.076396107673645, 0.1454506814479828, 0.10494285821914673, 0.07180973887443542, 0.08900200575590134, 0.4582197964191437, -0.007395877502858639, 0.41873589158058167, -0.01492499653249979, -0.2675204277038574, 0.5567262172698975, -0.1892460286617279, 0.7058998942375183, 0.10607058554887772, 0.16180290281772614, 0.27933889627456665, -0.570239782333374, 0.20088313519954681, 0.3699967861175537, -0.30299562215805054, -0.634329617023468, 0.28630566596984863, 0.5078402757644653, -0.3017112612724304, 0.6482817530632019, 0.7136569619178772, 0.386832594871521, -0.3852710723876953, 0.1483369916677475, 0.48388001322746277, 0.01058778166770935, 0.47038936614990234, 0.3917907476425171, 0.21680252254009247, 0.20029081404209137, 0.22139161825180054, -0.36861464381217957, 0.40074554085731506, -0.9904803037643433, 0.32173317670822144, 0.7144196033477783, 0.3791559040546417, 0.7585968375205994, 0.45407888293266296, -0.7467604279518127, -0.06877487897872925, -0.04355492442846298, 0.42564988136291504, 1.641377568244934, -0.3000956177711487, -0.33953937888145447, -0.3819427788257599, -0.5137410759925842, -0.21821117401123047, 0.1810378134250641, -0.3892168700695038, -0.2777327597141266, -0.5168861150741577, -1.224058985710144, 1.243527889251709, -0.215313121676445, 0.8810144066810608, -0.21523892879486084, -0.4029466211795807, -0.6436221599578857, 0.3016749620437622, -0.45323505997657776, -0.3529323935508728, 0.2167840600013733, -0.5066031813621521, -0.13598865270614624, -0.10846120119094849, -0.10682356357574463, -0.10904179513454437, -1.0465781688690186, 1.1140187978744507, -0.19740116596221924, -0.2687048614025116, -0.12651506066322327, 0.9858022332191467, -0.5029661655426025, -0.9686137437820435, -0.08551518619060516, 0.023264138028025627, -0.18997325003147125, 0.3985058665275574, 0.5779153108596802, 0.2607857584953308, -0.022733746096491814, 0.030787046998739243, 0.0002527967153582722, -0.18339136242866516, 0.10834559798240662, -0.11945489048957825, -0.21615338325500488, -0.3083032965660095, -1.160007357597351, 0.7243168354034424, 0.14712902903556824, -0.5786256194114685, 0.4048005938529968, -0.7102877497673035, -0.32528921961784363, 0.20209819078445435, -0.21932445466518402, -0.45815956592559814, -1.6464996337890625, 0.3063763678073883, 0.1466175615787506, 0.17714568972587585, -0.010927832685410976, 0.2332438975572586, 0.14020290970802307, -0.039025288075208664, 0.4021849036216736, 0.3523504137992859, -0.3594655990600586, 0.31743237376213074, -0.49943700432777405, 0.1125418096780777, 0.03507338836789131, 0.11194002628326416, -0.18996530771255493, -0.10135621577501297, -0.849086582660675, -0.18536895513534546, 0.07135798782110214, 0.2403576672077179, 0.11828387528657913, -0.27588504552841187, -0.5547850131988525, -0.5477892756462097, 0.34770065546035767, -0.5810810327529907, -0.22299593687057495, 0.08290548622608185, -0.364638090133667, 0.15796762704849243, -0.9087653160095215, -1.3914670944213867, -1.15666925907135, -0.9912116527557373, -1.0678074359893799, 0.5308765172958374, 0.06518712639808655, -0.5714081525802612, -0.44763511419296265, -0.23275525867938995, 0.018780967220664024, 0.870526909828186, -0.6685630083084106, 1.1282240152359009, -0.3606458008289337, 0.1410495489835739, 0.1102757602930069, 0.01283347699791193, 0.09617535769939423, -0.20249620079994202, 0.3227062523365021, -0.7966449856758118, 0.1958027482032776, -0.3812171220779419, -0.28129544854164124, 0.030268581584095955, 0.2615116238594055, 0.6735811829566956, -0.37251222133636475, -0.4515690207481384, 0.21544981002807617, 0.6855397820472717, -0.7869094610214233, 0.018765568733215332, -0.15610332787036896, 1.1290563344955444, 0.3595926761627197, -0.19458934664726257, 0.6060589551925659, 0.3909587860107422, 0.37007787823677063, -0.09228114783763885, -0.32502588629722595, -0.026825493201613426, -0.5253614187240601, 0.3503723740577698, 1.3525457382202148, 0.12034352123737335, 0.1540122628211975, -1.0713518857955933, 0.4857102334499359, -1.0390584468841553, -1.0549423694610596, 0.5211202502250671, 1.028743028640747, 0.6350016593933105, -0.4139067828655243, 0.10686688870191574, 0.052572961896657944, 0.6140121817588806, 0.42722001671791077, 0.04072820022702217, -0.8706112504005432, 0.15036965906620026, 0.9185236692428589, 0.3094122111797333, 0.6225478649139404, -0.3546176850795746, 0.4841752350330353, 15.421773910522461, 0.8319158554077148, -0.1650484800338745, 0.8077576160430908, 1.0392966270446777, 0.2902095913887024, -0.43810132145881653, -0.07390058040618896, -1.0539569854736328, 0.30079641938209534, 1.1252764463424683, -0.11225134134292603, 0.6525048613548279, 0.16858366131782532, -0.14044685661792755, 0.0955260843038559, 0.04153001308441162, 0.44414201378822327, 0.6105070114135742, -0.7837682366371155, 0.40204668045043945, 0.35186493396759033, 0.33634355664253235, 0.31950995326042175, 0.6378912925720215, 0.8017618656158447, 0.5722962021827698, -0.6846317052841187, 0.7319193482398987, -0.03142068535089493, 0.7530516982078552, -0.4873183071613312, 0.3982507288455963, 0.4697820842266083, -0.5444353222846985, 0.008965834975242615, -0.5545037388801575, -1.33148992061615, -0.09652657806873322, 0.40119048953056335, -0.4823063313961029, -0.3155580759048462, -0.08292949199676514, 0.3100825250148773, 0.012901837937533855, 0.35222795605659485, -0.2062988430261612, 1.0081031322479248, -0.21925756335258484, 0.4102705717086792, -0.2525709271430969, 0.3065745234489441, 0.19122564792633057, -0.27315253019332886, 0.23365990817546844, -0.34124594926834106, 0.13298112154006958, 0.5236326456069946, -0.49085354804992676, -0.26048749685287476, -0.26505082845687866, -0.3957477807998657, 0.13156044483184814, 0.6605018973350525, 0.3331901729106903, 0.22355219721794128, -0.2507180869579315, 0.05119701847434044, 0.5163640379905701, -0.013465878553688526, -0.28477856516838074, 0.1839432418346405, 0.7436270117759705, -0.275638610124588, -0.2608329653739929, 0.8079513907432556, -0.29418155550956726, -0.5300875306129456, -1.1759262084960938, -0.6161487102508545, 0.6147995591163635, -0.5339019894599915, -0.7055878639221191, 0.6176546812057495, -0.08407943695783615, 0.0794031023979187, 0.3088630139827728, -0.4208042621612549, -0.24624592065811157, 0.6708881258964539, -1.0522892475128174, -1.081929326057434, 0.6628158688545227, -0.42063575983047485, -0.648792564868927, -0.05124402418732643, 1.123656988143921, 0.15293267369270325, -0.5440066456794739, 0.5093111395835876, -0.2095794975757599, 0.07348451018333435, -0.2660350203514099, -0.5606915354728699, 0.8918920755386353, 0.6982244253158569, -0.10318820923566818, 0.947734534740448, 0.26566052436828613, -0.02818259224295616, -0.4880470037460327, -0.006289225537329912, 1.2602741718292236, -0.9599534869194031, -0.392315536737442, -0.9430570602416992, -0.49261900782585144, 0.4215059280395508, 0.4572455883026123, -0.19350691139698029, 0.42514124512672424, 0.2604946792125702, -0.2946139872074127, -0.1845397800207138, -0.893092930316925, 0.08106669783592224, 0.6200305223464966, -0.988792359828949, -0.3602744936943054, -0.4430420696735382, -0.22579410672187805, -1.0603704452514648, -0.2157052904367447, 0.061845265328884125, 0.21053604781627655, -0.6547403931617737, 0.8966909646987915, -0.6001386046409607, 0.19145388901233673, 1.0227407217025757, -0.07870176434516907, -0.5558966994285583, -0.33511465787887573, -1.0077087879180908, -0.2596428394317627, 0.43839433789253235, 0.4536646902561188, -0.44226083159446716, 0.18873673677444458, 1.2934339046478271, 0.6196322441101074, -0.3041802942752838, -0.8050978183746338, 0.09161531180143356, 0.11516746133565903, -0.3796212673187256, 0.681432843208313, -0.20239664614200592, -0.29586735367774963, 0.0208953358232975, -0.14345364272594452, 0.646521806716919, 0.07596392184495926, -0.41068583726882935, 0.3134494125843048, -0.43765443563461304, 0.13214153051376343, -0.6269447803497314, -0.5438653826713562, -0.7443965673446655, 0.1079777330160141, -0.6995890140533447, 0.01515888050198555, -0.7158411741256714, -0.15669332444667816, -0.13822974264621735, -0.1570717990398407, 0.15677711367607117, 0.19857636094093323, 0.18893969058990479, -0.4372136890888214, -0.5678319931030273, -0.43311023712158203, 0.8071506023406982, 0.4681323766708374, -0.8418033719062805, 0.520828127861023, 0.16245511174201965, -0.1786157637834549, 0.3814953863620758, 0.37853842973709106, -0.7629728317260742, -0.730789303779602, -1.023592233657837, 0.16771666705608368, -0.4231772720813751, 0.13074991106987, -0.44399651885032654, 0.22640308737754822, 0.3004576563835144, -0.11294444650411606, 0.3177931010723114, 0.11664773523807526, -0.5489776134490967, -0.05863817408680916, 0.5562130808830261, -0.5235846042633057, -0.3576309084892273, 0.09379268437623978, -1.2666442394256592, -0.05065518245100975, 0.7126295566558838, -0.019810518249869347, -1.0230392217636108, -0.38251349329948425, 0.562648594379425, -0.6171000599861145, 0.6640712022781372, -0.5608009696006775, -0.10658903419971466, -0.8935244083404541, -0.2292352169752121, 0.08224604278802872, 0.23810315132141113, -0.27890825271606445, 0.8666592836380005, -0.03880862146615982, -0.8774861097335815, 0.12939906120300293, 0.572247326374054, -0.4538092613220215, -0.4135848879814148, 0.1650712490081787, 0.45999807119369507, -0.33107489347457886, 0.5699540376663208, 0.4807296395301819, 0.648156464099884, -1.2057465314865112, -0.36743319034576416, 0.7266345620155334, -0.5647085309028625, 0.19857007265090942, 1.2963695526123047, -0.27086400985717773, -1.3349511623382568, 0.006999464705586433, -0.9992197155952454, -0.32972636818885803, -0.4310161769390106, 0.575429379940033, 0.590733528137207, 0.00011227247887291014, -0.5992350578308105, -0.7822737693786621, -0.16392549872398376, 0.41114410758018494, -0.4071309268474579, 0.1335812509059906, -0.3604690432548523, -0.7538214325904846, 0.47243353724479675, 0.41148677468299866, 0.12105356901884079, -0.25766128301620483, -0.12307355552911758, -0.28338176012039185, -0.24322497844696045, 0.22999703884124756, -0.3502088189125061, -0.5373930335044861, 0.6433104276657104, 0.4025264382362366, 0.5264176726341248, -0.10340408235788345, -0.21851849555969238, 0.1699690818786621, 0.10046401619911194, 0.7288148403167725, -0.3213837742805481, -1.3752729892730713, 1.1921130418777466, 0.7214063405990601, -0.8127999901771545, 0.5129687786102295, -0.2268889993429184, -0.7105070352554321, 0.5579837560653687, 0.3814733922481537, 0.40666067600250244, 0.5595580339431763, -0.27517008781433105, 0.26898810267448425, 0.0286178570240736, -0.5326334238052368, -0.4501098692417145, 0.8627746105194092, 0.8296817541122437, 1.032293677330017, 0.2622590661048889, -0.2749079465866089, 1.2195287942886353, -0.15431943535804749, 0.1807098686695099, 0.11618267744779587, 0.47603774070739746, -0.00776352034881711, -0.24326656758785248, -0.14421963691711426, 0.7971169352531433, -1.0032236576080322, -0.6889062523841858, -0.39800742268562317, 0.6393529772758484, 0.49729979038238525, 0.5640074610710144, 0.470973402261734, 0.18684808909893036, 0.20216694474220276, 0.43041500449180603, 0.4948870539665222, -0.5323342680931091, -0.2521321475505829, -0.2402922660112381, -0.31157663464546204, 0.01777435466647148, -0.20116588473320007, -0.30122968554496765, -0.2229091227054596, -0.5892716646194458, 0.3084489703178406, 0.37714684009552, -0.3676784634590149, 1.0416244268417358, 0.7335212230682373, -0.00872102566063404, -0.4433293044567108, -0.15905389189720154, -0.3334403336048126, -0.7936137914657593, -0.28655949234962463, -0.6532551050186157, -0.143374502658844, -0.2450045645236969, -0.187505841255188, -0.82974773645401]}, "authors": [{"authorId": "2077390471", "name": "A. Zayed"}, {"authorId": "2267925411", "name": "Gon\u00e7alo Mordido"}, {"authorId": "3197429", "name": "S. Shabanian"}, {"authorId": "2276424514", "name": "Ioana Baldini"}, {"authorId": "123607932", "name": "Sarath Chandar"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "6927a5b0152433a199ab4974ad85e787454d6a30", "title": "Should We Attend More or Less? Modulating Attention for Fairness"}, {"paperId": "6edd112383ad494f5f2eba72b6f4ffae122ce61f", "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"}, {"paperId": "7ef43bacd43393ff116e6fcda6a52a6902e016d7", "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "0a81729daba7a482a3a988320559e4cb631f985f", "title": "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders"}, {"paperId": "d48d1e80b6ea9708fa3a09d1556a7ced3b147da2", "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation"}, {"paperId": "d0b7670f0739beb65f45b3e85f5675364f94bd30", "title": "Layer-wise Model Pruning based on Mutual Information"}, {"paperId": "57ed901be5d1b4d853d4f8998dadc1b60e2151f9", "title": "On Attention Redundancy: A Comprehensive Study"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "ce3b364b7e6358940ce97d8d5887a65e5024ca21", "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"}, {"paperId": "13f67bd8700d5cd869c07b4aa9cb92b9e79bb17e", "title": "Model Compression for Domain Adaptation through Causal Effect Estimation"}, {"paperId": "3d864a8bc5a55ccab9993aa66203d8e70b88148c", "title": "Measuring and Reducing Gendered Correlations in Pre-trained Models"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae", "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "84286c4a614c198593d0e19623cdce318416f212", "title": "Named Entity Recognition as Dependency Parsing"}, {"paperId": "91ac65431b2dc46919e1673fde67671c29446812", "title": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "5487dadb5b4b8b240ab4ae28705acc0b9f138db0", "title": "Dice Loss for Data-imbalanced NLP Tasks"}, {"paperId": "73a5605ce482bd639078ebbb19baac7b903017e2", "title": "A Unified MRC Framework for Named Entity Recognition"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd", "title": "Measuring Bias in Contextualized Word Representations"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "44fc8d79fb8e0f8c6c6f680179b5803a789c6227", "title": "Measuring and Mitigating Unintended Bias in Text Classification"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "title": "Gender Bias in Coreference Resolution"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "1cc2f313bcb3b106af081f7031b924c9ad2662bd", "title": "Exploring Sparsity in Recurrent Neural Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "5966d7c7f60898d610812e24c64d4d57855ad86a", "title": "Semantics derived automatically from language corpora contain human-like biases"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "48bf1d216339ee13ed8e518eb67bf5fef9b001df", "title": "Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation"}, {"paperId": "8a5975c1d2ed3018fe847bdff12766255eb9bd2d", "title": "Pruning Neural Machine Translation for Speed Using Group Lasso"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "We quantify the effect of removing attention heads on bias in language models, and use it as a proxy for their contribution to the model\u2019s overall bias"}, {"paperId": null, "title": "We propose a novel structured pruning method that considers both fairness and performance"}, {"paperId": null, "title": "2022. Training compute-optimal large language models"}, {"paperId": null, "title": "2023b. Deep Learning on Healthy Data Diet: Finding Important Examples for Fair-ness"}, {"paperId": null, "title": "2022. An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models"}]}