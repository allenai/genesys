{"paperId": "b204c257d06eb7148ef7e1fc0ca72e3e9eae2064", "abstract": "Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 20-33% fewer FLOPs compared to standard training, and is competitive or better than other efficient training methods. Furthermore, RaPTr shows better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5% compared to standard training and stacking. Finally, we provide a theoretical basis for RaPTr to justify (a) the increasing complexity of subnetworks in stages, and (b) the stability in loss across stage transitions due to residual connections and layer norm.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step, and achieves better pre-training loss for BERT and UL2 language models while requiring 20-33% fewer FLOPs compared to standard training, and is competitive or better than other efficient training methods."}, "embedding": {"model": "specter_v2", "vector": [0.20259155333042145, 0.9496666193008423, -0.12095257639884949, -0.05657277628779411, -0.22718824446201324, -0.00805790163576603, 0.3967376947402954, -0.24812883138656616, -0.2720041871070862, -0.16447336971759796, 0.5222643613815308, -0.2777605652809143, 0.2969878613948822, 0.021577153354883194, -0.1690153330564499, 0.10128758102655411, -1.2674617767333984, 0.32073378562927246, 0.00227217935025692, -0.03320692479610443, -0.40432894229888916, -0.7491942048072815, -0.8102005124092102, 0.22364382445812225, 0.3795190453529358, 0.40618377923965454, 0.05926647037267685, 1.0627624988555908, -0.1368144005537033, 0.5406240224838257, 0.3148333430290222, -0.2568386197090149, 0.37255167961120605, -0.4270975589752197, -0.549811065196991, -0.028391407802700996, -0.01123205665498972, -0.45673978328704834, -0.24948647618293762, 0.7146432995796204, 0.12967118620872498, 0.3623526990413666, 0.15808233618736267, -0.2868020534515381, 0.061148036271333694, 1.0899778604507446, 0.5408318638801575, 0.6586756706237793, -0.37085047364234924, -0.8460168242454529, 1.300161600112915, -1.3616337776184082, 0.20678624510765076, 1.6161810159683228, 0.7503843903541565, 0.4103706181049347, -0.3853146731853485, -0.31654760241508484, 0.7753403782844543, 0.1599176675081253, -0.8635094165802002, -0.11120612919330597, -0.1423386186361313, 0.08214206993579865, 1.4983386993408203, -0.45450615882873535, 0.002048620954155922, 0.21415968239307404, -0.2002665251493454, 1.6185060739517212, 0.08200027048587799, -0.49079659581184387, -0.35932713747024536, 0.2907245457172394, 0.45689812302589417, 0.7473957538604736, -0.584551215171814, 0.39076268672943115, -0.6746101975440979, 0.1134970560669899, 0.10884392261505127, -0.2700106203556061, -0.042316362261772156, -0.26733407378196716, 0.027777550742030144, 0.3929424285888672, 0.4707392156124115, 0.7038288712501526, -0.30946680903434753, 0.7244587540626526, 0.5348237156867981, 0.6100276112556458, 0.33071979880332947, 0.27916058897972107, -0.302958607673645, 0.6342592239379883, -0.9869162440299988, -0.003119239117950201, -0.35067492723464966, 0.6265022158622742, -0.15152256190776825, 0.2834664285182953, -0.7556787729263306, 0.10532306879758835, 1.6386035680770874, -0.3261381685733795, 0.2942904233932495, -0.710250735282898, 0.2104295790195465, -0.568701446056366, -0.2029273957014084, -0.07991985231637955, -0.39591658115386963, -0.6898326873779297, -0.8440707325935364, -1.0531517267227173, -0.7591519355773926, -0.30415451526641846, -0.8089781403541565, 0.7937574982643127, -0.12202154844999313, 0.49093183875083923, 0.11353728920221329, 0.41433751583099365, 0.2286263108253479, 0.7108156085014343, 0.41532281041145325, 0.21395617723464966, 0.7274345755577087, -0.9532653093338013, -0.5053038001060486, -0.6657425165176392, 0.8231648802757263, -0.28434112668037415, 0.041327103972435, 0.16398243606090546, -1.4622702598571777, -1.2163128852844238, -0.7984581589698792, 0.1133372038602829, -0.19931136071681976, 0.13917860388755798, 1.044029712677002, 0.47466254234313965, -1.2807413339614868, 1.0728076696395874, -0.36839166283607483, 0.0363268218934536, 0.5066379904747009, 0.3203951418399811, 0.41218140721321106, -0.44193035364151, -1.239672064781189, 0.29581528902053833, 0.39731305837631226, -0.06968371570110321, -0.2890966236591339, -1.0951719284057617, -0.8778622150421143, 0.07793204486370087, 0.05125139653682709, -0.6452982425689697, 1.4392949342727661, -0.18423455953598022, -1.4017632007598877, 0.2975062131881714, -0.12867189943790436, -0.0717615857720375, 0.6287493109703064, -0.4371796250343323, -0.1617879718542099, -0.08564357459545135, -0.40591153502464294, 0.7126704454421997, 0.14013667404651642, -0.19222426414489746, -0.1872548609972, 0.3535173535346985, -0.2681885063648224, 0.04023711010813713, -0.07909263670444489, 0.901594877243042, -0.41037213802337646, 0.0240703746676445, 0.39310601353645325, 0.8173366785049438, -0.5234425067901611, -0.415791779756546, -0.4755420386791229, -1.1215567588806152, 0.46622005105018616, -0.4284750521183014, 0.8018724918365479, -0.781391441822052, -0.5626776814460754, -0.06272569298744202, -0.04072720929980278, -0.10150530189275742, -0.6974403858184814, 0.21670179069042206, -0.5406010746955872, 0.4078599810600281, -0.29775935411453247, -1.5689154863357544, 0.07525560259819031, -0.4362137019634247, -0.22758953273296356, -0.39590001106262207, 0.33573293685913086, 1.1311264038085938, -0.9007694125175476, -0.05846866965293884, -0.26917943358421326, 0.33833935856819153, -1.2384343147277832, 1.2010830640792847, -0.38383936882019043, 0.37417009472846985, 0.08800799399614334, -0.3620198965072632, 0.4498785138130188, -0.22906310856342316, 0.5138697028160095, -0.20156623423099518, 0.19736282527446747, 0.8984944224357605, -0.5311598181724548, 1.237524151802063, -0.1547849327325821, 0.40709424018859863, -0.007741252891719341, -0.9325243234634399, -0.016814496368169785, 0.4400692284107208, -0.07579471915960312, -0.3120771646499634, 0.4473479390144348, 0.2516520321369171, -0.3317849338054657, 0.6121070384979248, 0.7357308864593506, 0.42017367482185364, -0.28653591871261597, 0.3839688003063202, 0.8234378099441528, 0.12413503229618073, 0.4256839156150818, 0.6407892107963562, 0.45632949471473694, 0.4882863461971283, 0.2394775003194809, -0.035360418260097504, 0.20398691296577454, -1.0440806150436401, 0.12053491175174713, 0.42427778244018555, 0.6343973278999329, 0.8364083170890808, 0.664543867111206, -0.45204973220825195, -0.6576048135757446, -0.24611306190490723, 0.5836458802223206, 1.1504510641098022, -0.35523441433906555, 0.12257418781518936, -0.6919971108436584, -0.5001112818717957, -0.43492591381073, 0.20859713852405548, -0.3679839074611664, -0.2775450348854065, -0.8527448177337646, -1.1520096063613892, 0.6596991419792175, 0.33856841921806335, 1.3127623796463013, -0.4642923176288605, 0.09126438945531845, -0.3658372461795807, 0.4652727544307709, -0.6842644810676575, -0.2731019854545593, 0.25766465067863464, -0.8108462691307068, -0.13471484184265137, -0.2735794484615326, -0.2600935697555542, 0.41857388615608215, -0.6453429460525513, 1.0337735414505005, -0.2984580099582672, -0.07567960768938065, 0.008355834521353245, 0.36797213554382324, -0.1515962779521942, -0.797228217124939, 0.06038448587059975, 0.3055606782436371, 0.03339318186044693, -0.08586938679218292, 0.45170533657073975, -0.11745918542146683, -0.047644611448049545, -0.055923085659742355, 0.38973695039749146, 0.2044227570295334, 0.10516469180583954, 0.5616968274116516, 0.19628596305847168, -0.12697330117225647, -1.8569904565811157, 0.6587313413619995, 0.042894940823316574, -0.3901650309562683, 0.2416391521692276, -0.5419538617134094, -0.4864838123321533, 0.6221931576728821, -0.37332913279533386, -0.40899747610092163, -1.3769689798355103, -0.02224504016339779, -0.33522874116897583, -0.14903509616851807, 0.14921079576015472, 0.30882084369659424, 0.2750433385372162, 0.08104075491428375, 0.1793868988752365, 0.12502983212471008, -0.41115331649780273, 0.5990676879882812, -1.0560182332992554, 0.1761191487312317, 0.3002053201198578, 0.430385947227478, -0.3448548913002014, -0.285673588514328, -0.5618925094604492, -0.5715404748916626, -0.30603426694869995, -0.28007474541664124, -0.19562743604183197, 0.22403362393379211, -0.7257493734359741, -0.6772282719612122, 0.13443711400032043, -0.8821191191673279, -0.8036710023880005, 0.11441852897405624, -0.18488559126853943, 0.04425220564007759, -0.8621894121170044, -1.4281374216079712, -0.5048579573631287, -0.8252006769180298, -0.6427425146102905, 0.09210215508937836, 0.14923131465911865, -0.42888009548187256, -0.6324114203453064, -0.1832709163427353, -0.31393662095069885, 0.8058581948280334, -0.8847845196723938, 0.6864985823631287, 0.07679226994514465, -0.43760064244270325, -0.16891521215438843, 0.17647123336791992, 1.007209300994873, -0.5827285647392273, 0.019444359466433525, -0.8100062608718872, 0.053764037787914276, -0.40055227279663086, -0.32415884733200073, 0.2180272936820984, 0.32640504837036133, 0.5577089786529541, 0.1048460528254509, -0.18504668772220612, 0.5280039310455322, 1.0900615453720093, -0.9769383668899536, 0.33498287200927734, 0.1102936789393425, 1.0425009727478027, 0.3851803243160248, -0.30501189827919006, 0.07444708794355392, 0.2471698671579361, 0.16084235906600952, -0.12425699830055237, -0.2712263762950897, -0.3377610445022583, -0.463456392288208, 0.7979422211647034, 1.614621639251709, 0.5019294619560242, 0.07557466626167297, -0.6641460061073303, 0.6846790909767151, -0.8284863233566284, -0.6548826694488525, 0.6292724013328552, 0.6597075462341309, 0.08967775106430054, -0.2104325145483017, -0.2570301592350006, 0.051637403666973114, 0.384864866733551, 0.08878078311681747, -0.11594213545322418, -0.751300573348999, 0.14081883430480957, 0.6454792022705078, 0.20245906710624695, 0.8278254866600037, -0.13631568849086761, 0.7440392971038818, 15.153536796569824, 0.3916550576686859, 0.26608219742774963, 0.7662122249603271, 0.9963242411613464, 0.09672028571367264, -0.49775704741477966, -0.1254439651966095, -1.1800521612167358, -0.095707468688488, 1.2897143363952637, 0.43556466698646545, 0.4770372807979584, -0.020717978477478027, 0.2173023521900177, 0.22906769812107086, -0.4901176989078522, 0.11216990649700165, 0.16122470796108246, -1.1422778367996216, 0.36287540197372437, -0.049475912004709244, 0.671127438545227, 0.744206964969635, 0.650182843208313, 1.068463683128357, 0.6218944191932678, -0.17407231032848358, 0.43657201528549194, 0.19172286987304688, 0.7174484133720398, -0.24821077287197113, 0.5278962254524231, 0.42136654257774353, -0.695889949798584, -0.057922303676605225, -0.47611573338508606, -1.0175189971923828, 0.6165403723716736, 0.21643203496932983, -0.7797264456748962, -0.2596418261528015, -0.1291983276605606, 0.7169480919837952, 0.1682199388742447, 0.6030451059341431, -0.5450131297111511, 1.0545365810394287, -0.406197726726532, 0.3733143210411072, 0.20834402740001678, 0.3734007179737091, 0.06352803856134415, -0.04055381566286087, 0.0034788076300174, 0.0840430036187172, 0.06052374839782715, 0.5603467226028442, -0.6208726167678833, -0.25934115052223206, 0.08650922775268555, -0.06606430560350418, -0.255181223154068, 0.7010058164596558, 0.36498165130615234, 0.07043454796075821, -0.33554908633232117, 0.4379279315471649, 0.5553117990493774, 0.16623952984809875, -0.08949607610702515, -0.06353336572647095, 0.5249283313751221, -0.5968829393386841, -0.04023890942335129, 0.41850653290748596, -0.3206092417240143, -0.33762460947036743, -0.9355198740959167, -0.2254946231842041, 0.2637854218482971, -0.9642041325569153, -0.3560560643672943, 0.7553738355636597, -0.3584015369415283, -0.5565718412399292, 0.09779288619756699, -0.669907808303833, -0.2839006781578064, 0.6480883955955505, -1.7758337259292603, -0.4027790129184723, 0.49547961354255676, -0.33369165658950806, -0.5437280535697937, -0.06381429731845856, 1.0711395740509033, 0.5045176148414612, -0.3947429955005646, 0.44131970405578613, -0.17274174094200134, 0.23804612457752228, -0.5633359551429749, -0.8143810033798218, 0.9604787230491638, 0.6384178400039673, -0.1993786096572876, 0.26068052649497986, -0.057143911719322205, 0.3262597620487213, -0.2848801612854004, -0.20235024392604828, 0.8620265126228333, -0.6759946942329407, -0.2707355320453644, -0.7497738003730774, -0.709143877029419, 0.4221417009830475, 0.39209285378456116, -0.22726833820343018, 0.4865341782569885, 0.2863316833972931, -0.2357497662305832, 0.0247073732316494, -0.5709691047668457, 0.06576947867870331, 0.5948635339736938, -0.7460134625434875, -0.0847332626581192, 0.10108302533626556, 0.7707160711288452, -0.9475146532058716, -0.5582988262176514, -0.1613478809595108, -0.26762640476226807, -0.1963696926832199, 1.053863763809204, -0.4077852666378021, 0.584639310836792, 0.7131786346435547, 0.249015212059021, -0.6597685813903809, -0.05958116054534912, -1.1067349910736084, -0.017631858587265015, 0.26920852065086365, 0.6371055841445923, -0.407195508480072, 0.5567352175712585, 0.40078726410865784, -0.056818556040525436, -0.5301032662391663, -0.4357961118221283, -0.06190112233161926, 0.0884135290980339, -0.7204188108444214, 0.10095648467540741, -0.2652202546596527, -0.4055001735687256, 0.0509943850338459, 0.3427909016609192, 0.9342918992042542, -0.028821805492043495, -0.7247122526168823, 0.232510045170784, 0.1383131891489029, -0.37482231855392456, -0.5934484601020813, -0.5940619111061096, -1.6153931617736816, -0.16749583184719086, -1.190537452697754, 0.09863906353712082, -1.1039897203445435, -0.43918168544769287, -0.35051193833351135, -0.5457890033721924, -0.18653206527233124, 0.022936755791306496, 0.009422407485544682, -0.24857541918754578, -0.5121142268180847, -0.8823812007904053, 0.9132287502288818, 1.023471474647522, -0.5742231011390686, -0.0683666467666626, 0.2480597347021103, 0.0111765181645751, 0.2898389399051666, 0.5036619305610657, -0.43034863471984863, -0.8121851086616516, -1.4603646993637085, 0.572167158126831, -0.29882052540779114, -0.11897553503513336, -0.7297235727310181, 0.4101247787475586, 1.021240234375, -0.16773520410060883, -0.22093655169010162, 0.1614479422569275, -0.6145659685134888, -0.7043641209602356, 0.2237083911895752, -0.8016911745071411, 0.03358716890215874, 0.1622009575366974, -0.6216319799423218, -0.22394470870494843, 0.6220769882202148, 0.04399224370718002, -1.0601502656936646, -0.7582339644432068, 0.3818393647670746, -0.5377410650253296, -0.005392151884734631, -0.4823002517223358, -0.16753195226192474, -0.6519152522087097, -0.12456615269184113, -0.3959116041660309, 0.660713255405426, -0.6858974099159241, 0.7094281911849976, 0.22569158673286438, -1.0304216146469116, -0.1999000608921051, 0.38354775309562683, -0.6418818235397339, 0.48444047570228577, 0.8413636684417725, 0.5991389155387878, -0.5747897028923035, 0.42277294397354126, 0.5638036131858826, 0.3432970941066742, -0.8423056602478027, -0.23973295092582703, 1.0469690561294556, -0.7722585797309875, -0.41024935245513916, 1.7135777473449707, -0.38071408867836, -1.5942460298538208, 0.1396007388830185, -0.8743019700050354, -0.6938846707344055, -0.2315952181816101, 0.6210142970085144, 0.1104060560464859, -0.2776336669921875, 0.09127070009708405, -0.3451145589351654, 0.11933798342943192, -0.05867892876267433, -0.4497801661491394, 0.6848142743110657, -0.2896755337715149, -0.5654997825622559, 0.6197953224182129, 0.9520642161369324, -0.7790863513946533, -0.8895522952079773, -0.7824464440345764, -0.30848225951194763, 0.31425824761390686, 0.3372390568256378, -0.36756402254104614, -0.6352263689041138, 1.1667436361312866, 0.49681514501571655, 0.07955863326787949, 0.37306225299835205, -0.19784042239189148, 0.2835251986980438, 0.3792949616909027, 0.3447689712047577, -0.5796545743942261, -0.6549069285392761, 1.1740862131118774, 0.732032299041748, -0.6365598440170288, 0.41340401768684387, -0.19632631540298462, -0.6622439622879028, 0.5932493209838867, 0.02091330476105213, -0.3755545914173126, 0.9406271576881409, -0.3594716489315033, 0.1497485190629959, 0.45659780502319336, -0.888588011264801, -0.3883202075958252, 0.8981115221977234, 0.7194321751594543, 0.7003991007804871, 0.24567700922489166, 0.22310124337673187, 0.9866102933883667, -0.1691443920135498, -0.14116045832633972, 0.25913509726524353, 0.16289599239826202, -0.009842856787145138, -0.24017579853534698, 0.2708948254585266, 0.3089105486869812, -0.624318540096283, -0.6922673583030701, 0.3326393663883209, 0.2929479777812958, 0.2751525640487671, 0.5287756323814392, 0.9714770913124084, -0.016244560480117798, 0.3537617623806, 0.11722928285598755, 0.5453676581382751, -0.6200182437896729, -0.3933568298816681, -0.006462716963142157, -0.3683571219444275, -0.26718640327453613, -0.08060485869646072, -0.6305745840072632, -0.5318141579627991, -0.5056214332580566, 0.36779022216796875, 0.1730501502752304, 0.10356955230236053, 1.3705074787139893, 0.5864927172660828, 1.0936411619186401, 0.06351134181022644, -0.7158212065696716, -0.3212105929851532, -1.0311344861984253, 0.074384905397892, -0.5681766271591187, -0.1591796875, 0.09203670918941498, -0.07602338492870331, -0.6319735646247864]}, "authors": [{"authorId": "2283306360", "name": "Abhishek Panigrahi"}, {"authorId": "10769461", "name": "Nikunj Saunshi"}, {"authorId": "41049476", "name": "Kaifeng Lyu"}, {"authorId": "138875199", "name": "Sobhan Miryoosefi"}, {"authorId": "1981186", "name": "Sashank J. Reddi"}, {"authorId": "2283307878", "name": "Satyen Kale"}, {"authorId": "2283420123", "name": "Sanjiv Kumar"}], "references": [{"paperId": "599f618862756da109d59356665ee96424cbcf22", "title": "Revisiting Vision Transformer from the View of Path Ensemble"}, {"paperId": "d55d9cc276f283a080ed003b097eb93a8202419c", "title": "Composable Function-preserving Expansions for Transformer Architectures"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "881883842c2661b41bbfc999d56c763b1ceef0bd", "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models"}, {"paperId": "ce9435c82dc9b576f2037aa2f4357a520be9b2aa", "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"}, {"paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d", "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"}, {"paperId": "148644bf4ccef7e022b965304e8b3178be8af0fa", "title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference"}, {"paperId": "a7a40b35b6f37c554f1c5c2038892ed70c693a64", "title": "Learning to Grow Pretrained Models for Efficient Transformer Training"}, {"paperId": "3f75c86479f982130e58d8c769724ca07121bd53", "title": "Dropout Reduces Underfitting"}, {"paperId": "59e3b8ae1e119e2f48c8e64ecb32229e45ffcc01", "title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics"}, {"paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458", "title": "Symbolic Discovery of Optimization Algorithms"}, {"paperId": "8a4e2828777c9b3703e8e2b68ac27d9af496261a", "title": "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "e64914f9d4d2eabf3fa1352e2d132364a5539aaa", "title": "Understanding Contrastive Learning Requires Incorporating Inductive Biases"}, {"paperId": "42f5d2724aa7d8d9ee12a7e0d3e31948d8b6098f", "title": "The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks"}, {"paperId": "e7b0b36ffbc583cd169c690c8d9a4da32bcd50cf", "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training"}, {"paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe", "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "e52051204cb1179584f3b008c9d38848b52c1f28", "title": "ReZero is All You Need: Fast Convergence at Large Depth"}, {"paperId": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "b0c5e8d8c06d8b7d87d87377d360b06c18e3c992", "title": "Towards Understanding the Spectral Bias of Deep Learning"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "fb12f691678e20f6a4a3af48ae79c9241a38d52e", "title": "Random Path Selection for Incremental Learning"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "f2bb7e2f5a1afad5370159c15760c44df93c0438", "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition"}, {"paperId": "dda6fb309f62e2557a071522354d8c2c897a2805", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "96c82727dd5a80fef93007f888bb8569feb6bd85", "title": "Fixup Initialization: Residual Learning Without Normalization"}, {"paperId": "d34b3bb6d5b611e6117e7e25f0b5419d3a99fdf1", "title": "Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks"}, {"paperId": "42ec3db12a2e4628885451b13035c2e975220a25", "title": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "39e734da43eb8c72e9549b42e96760545036f8e5", "title": "QuAC: Question Answering in Context"}, {"paperId": "715a73290f260cf2196307e59fe0b6776841f170", "title": "On the Spectral Bias of Neural Networks"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "0679950558d72791f16031dd08c39367d8dd47b8", "title": "Shampoo: Preconditioned Stochastic Tensor Optimization"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "97394554eb5a74c3160c6bd743fcd3e4bd6cbe28", "title": "LSDSem 2017 Shared Task: The Story Cloze Test"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "2a24b68ef180c0c8742bd494a55fb6f68864efed", "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "ebfa6690ce8aa2c8bb110fff840555e60ef4d24e", "title": "Analysis of Boolean Functions"}, {"paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"paperId": "4328bd97ce29e5a5def69eeb1e172d1aacf3676a", "title": "Efficient Training of Language Models using Few-Shot Learning"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "(a) With residual connection & layernorm, \u03a8 \u2113 ( x ) = O ( (cid:112) L/\u2113 ) & \u2225 F ( x ) \u2225 2 = \u2126( \u221a L ) . Then the gap in losses between stages is O (1 / \u221a L )"}, {"paperId": null, "title": "c) Without layernorm, we have \u03a8 \u2113 ( x ) = \u2126(2 ( L \u2212 1) / 2 ) and \u2225 F ( x ) \u2225 2 = O (2 L/ 2 )"}, {"paperId": null, "title": "b) Without residual connection, we have both \u03a8 \u2113 ( x ) = O ( \u03b4 \u2212 1 e \u2212 \u2113 ) and \u2225 F ( x ) \u2225 = O (1)"}]}