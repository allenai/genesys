{"paperId": "a7a40b35b6f37c554f1c5c2038892ed70c693a64", "abstract": "Scaling transformers has led to significant breakthroughs in many domains, leading to a paradigm in which larger versions of existing models are trained and released on a periodic basis. New instances of such models are typically trained completely from scratch, despite the fact that they are often just scaled-up versions of their smaller counterparts. How can we use the implicit knowledge in the parameters of smaller, extant models to enable faster training of newer, larger models? This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where we learn to linearly map the parameters of the smaller model to initialize the larger model. For tractable learning, we factorize the linear transformation as a composition of (linear) width- and depth-growth operators, and further employ a Kronecker factorization of these growth operators to encode architectural knowledge. Extensive experiments across both language and vision transformers demonstrate that our learned Linear Growth Operator (LiGO) can save up to 50% computational cost of training from scratch, while also consistently outperforming strong baselines that also reuse smaller pretrained models to initialize larger models.", "venue": "International Conference on Learning Representations", "year": 2023, "citationCount": 32, "influentialCitationCount": 8, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.00980", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where it is learned to linearly map the parameters of the smaller model to initialize the larger model."}, "embedding": {"model": "specter_v2", "vector": [0.16359882056713104, 0.7254729270935059, -0.19222936034202576, 0.2308151125907898, -0.008822028525173664, -0.1333242952823639, 0.7714397311210632, -0.6797803044319153, -0.2559342682361603, -0.8255518078804016, 0.23822863399982452, -0.2644882798194885, 0.4084944725036621, 0.03321044519543648, -0.32869890332221985, 0.15785600244998932, -0.8190379738807678, 0.34765636920928955, 0.11071073263883591, -0.588904082775116, -0.45501232147216797, -0.047978151589632034, -1.1949102878570557, -0.1693953573703766, 0.2842758893966675, 1.0324786901474, -0.0057360222563147545, 0.746097981929779, -0.21785897016525269, 0.39929383993148804, 0.6498970985412598, -0.20463989675045013, 0.6976742744445801, 0.4231093227863312, -0.1082967147231102, 0.33430513739585876, 0.7020629644393921, -0.6453599333763123, -0.6333261728286743, 0.73003089427948, -0.42558351159095764, 0.11171866953372955, 0.19107206165790558, -0.9502277970314026, -0.46538153290748596, 0.41528940200805664, 0.584868848323822, 0.7207897305488586, -0.7782111763954163, -0.39968955516815186, 1.1230924129486084, -1.3027095794677734, 0.09316695481538773, 1.1128126382827759, 0.8986966609954834, 0.28082606196403503, -0.2854927182197571, -0.6766371130943298, 0.534774661064148, -0.019217394292354584, -0.63570636510849, -0.44073426723480225, 0.007238336373120546, -0.1058170422911644, 1.9104353189468384, -0.994317352771759, 0.3190562427043915, 0.21281594038009644, -0.024526774883270264, 1.1420561075210571, -0.041471704840660095, -0.815691351890564, -0.43384402990341187, 0.1670917570590973, -0.1680866777896881, 1.3357244729995728, -0.3491472899913788, 0.48413294553756714, -0.9502894878387451, 0.23399938642978668, 0.8202378749847412, -0.0558696947991848, 0.33924534916877747, -0.4707198739051819, -0.04555921256542206, 0.5140915513038635, 0.8156039118766785, 0.9507547616958618, -0.23087115585803986, 1.0614393949508667, 0.38557490706443787, 0.479244202375412, -0.051678404211997986, 0.5207614302635193, -0.18771390616893768, 0.7455840706825256, -0.7268022894859314, -0.29928576946258545, -0.3252367377281189, 1.0200985670089722, 0.2493460476398468, 0.3534066677093506, -0.3333361744880676, -0.10339522361755371, 1.1429656744003296, 0.007551551330834627, 0.17859259247779846, -0.709808886051178, 0.48595544695854187, -0.5745714902877808, -0.15179507434368134, -0.1071171760559082, -0.1349010169506073, -0.5889936089515686, -0.799484372138977, -0.9910662770271301, -0.6448264122009277, 0.4091033935546875, -1.1043651103973389, 0.7567721009254456, -0.4149457514286041, 0.42397788166999817, -0.0988079234957695, 0.07324902713298798, 0.10577171295881271, 0.6067690849304199, 0.3936368227005005, 0.21880866587162018, 0.6920563578605652, -1.2219535112380981, 0.030218474566936493, -1.0868183374404907, 0.5563589930534363, -0.2908848226070404, 0.5225659608840942, -0.05188126116991043, -1.1529834270477295, -1.239529013633728, -0.8245543241500854, 0.15467707812786102, -0.5239097476005554, 0.39056408405303955, 1.5163661241531372, 0.384420782327652, -1.354913592338562, 0.5298510193824768, -0.3855498135089874, -0.11669525504112244, 0.7091878056526184, 0.4397415518760681, 0.03458644822239876, -0.28376516699790955, -0.9033559560775757, 0.16705027222633362, 0.1287446767091751, -0.406647652387619, -0.6320115923881531, -0.7419147491455078, -1.1463737487792969, 0.12804055213928223, 0.15832707285881042, -0.7970089912414551, 1.2574807405471802, 0.03578443452715874, -1.04518723487854, 0.7139425873756409, 0.009258958511054516, -0.03882376477122307, 0.13325919210910797, 0.07897790521383286, -0.33000513911247253, -0.540590763092041, -0.4284776747226715, 0.7168540358543396, 0.9227274060249329, -0.2091130167245865, -0.2322620004415512, 0.32602259516716003, 0.25455793738365173, -0.20004990696907043, -0.6056250929832458, 0.9464901089668274, -0.4400884211063385, 0.07329647988080978, 0.5849626660346985, 0.48255619406700134, -0.2593061029911041, -0.027402091771364212, -0.1047135591506958, -0.8556011915206909, 0.8696914911270142, -0.133470818400383, 0.7364071011543274, -1.015438199043274, -0.5538960099220276, -0.2182977795600891, 0.3641328513622284, -0.26248660683631897, -0.8614032864570618, 0.38002169132232666, -0.39412999153137207, 0.17946213483810425, 0.3851763904094696, -1.2252451181411743, 0.12166807055473328, -0.3925701081752777, -0.701928973197937, -0.15146605670452118, 0.05088777095079422, 1.2155382633209229, -0.6472833156585693, 0.27178955078125, 0.07606220990419388, 0.5847121477127075, -1.0078165531158447, 1.2294584512710571, -0.259078711271286, 0.18113097548484802, 0.06627120822668076, 0.15231600403785706, -0.1011894941329956, -0.6256542801856995, 0.1664673238992691, -0.8588919043540955, 0.07966341823339462, 0.3966468572616577, -0.12883363664150238, 1.5178016424179077, -0.9218387007713318, 0.4662318229675293, -0.01011969055980444, -0.7671119570732117, 0.24857743084430695, 0.21943794190883636, -0.06704732030630112, -0.3123937249183655, 0.4557405114173889, 0.48403146862983704, -0.5362127423286438, 0.6717385053634644, 0.4988190531730652, 0.493345707654953, -0.3311643898487091, -0.19605979323387146, 0.8856903910636902, -0.41893264651298523, 0.5088858008384705, 0.09167985618114471, 0.8051789402961731, 0.42703938484191895, 0.20240597426891327, 0.13618594408035278, 0.3566582202911377, -1.1404271125793457, -0.15327559411525726, 0.36784595251083374, 0.43769341707229614, 0.5292300581932068, 0.011061170138418674, -0.9921262860298157, -0.7415856122970581, -0.3101825714111328, 0.7903375029563904, 1.6707096099853516, -0.3664912283420563, -0.3293062150478363, -0.6804268956184387, -0.3006364703178406, -0.6213845014572144, -0.26581111550331116, -0.3771691620349884, -0.22504204511642456, -0.7025396823883057, -0.8706928491592407, 0.7126143574714661, 0.4676002264022827, 1.431477665901184, -0.2897370159626007, -0.20902186632156372, -0.3662876784801483, 0.4857272505760193, -1.0470982789993286, -0.5260217189788818, 0.37283840775489807, -0.6008508801460266, -0.011639278382062912, 0.2171855866909027, -0.4787962734699249, 0.33890417218208313, -0.6261762380599976, 0.7433233261108398, -0.2284589409828186, -0.45869478583335876, 0.26657184958457947, 0.837192952632904, -0.5042183995246887, -0.8810312747955322, 0.3953471779823303, 0.29559487104415894, -0.25779813528060913, -0.06220593303442001, 0.010146894492208958, 0.15900252759456635, 0.12939707934856415, -0.45748117566108704, 0.4463307857513428, 0.2428344041109085, -0.08934952318668365, 0.6828873753547668, -0.06651091575622559, -0.25777187943458557, -1.0795005559921265, 0.7097936272621155, 0.6635391712188721, -0.8537646532058716, 0.5654478669166565, -0.9337722063064575, -0.2679876983165741, 0.8711976408958435, -0.7485745549201965, 0.04521558806300163, -0.7464508414268494, 0.5954481959342957, -0.8364716172218323, -0.1600179672241211, -0.21476975083351135, 0.17791761457920074, -0.3616527318954468, 0.42739424109458923, 0.23342087864875793, 0.03938852995634079, 0.018556416034698486, 0.5877571702003479, -1.316184639930725, 0.6828344464302063, 0.08547786623239517, 1.0569497346878052, -0.07471206784248352, 0.13856567442417145, -0.3442074656486511, -0.4959869980812073, -0.24458302557468414, -0.45691362023353577, -0.43302208185195923, 0.05388906970620155, -0.6584562063217163, -0.38765546679496765, 0.14000925421714783, -0.6380845904350281, -0.355059951543808, -0.025969110429286957, -0.5708592534065247, -0.27375710010528564, -1.31201171875, -1.23268461227417, -0.3275662064552307, -0.7312842607498169, -1.3703607320785522, 0.504658579826355, 0.2509041726589203, 0.2228148728609085, -0.5569711327552795, -0.41072994470596313, -0.20327091217041016, 1.2131739854812622, -0.6736763715744019, 0.8780916929244995, 0.11942362040281296, -0.7905418276786804, -0.01278360653668642, 0.09246566146612167, 0.7852895259857178, -0.1912803053855896, 0.32213541865348816, -1.1807501316070557, -0.016423586755990982, -0.45400652289390564, -0.35526877641677856, 0.0736667662858963, 0.12219896912574768, 0.6416929364204407, 0.09925944358110428, -0.23097053170204163, 0.9041525721549988, 1.2902793884277344, -0.8023361563682556, 0.1216205582022667, 0.5722793936729431, 1.1772305965423584, 0.21433423459529877, -0.5972341895103455, 0.25915637612342834, 0.25211188197135925, 0.11454860121011734, 0.17650246620178223, -0.15810377895832062, -0.5738286375999451, -0.9170261025428772, 0.5550552606582642, 1.5954056978225708, 0.17353203892707825, 0.15034520626068115, -1.147623062133789, 0.44828784465789795, -0.9523854851722717, -0.2822854518890381, 1.3213087320327759, 0.4580448269844055, -0.00362121663056314, -0.1011451855301857, -0.3629366159439087, -0.22582106292247772, 0.41741061210632324, 0.4908265173435211, -0.43398386240005493, -0.5111964344978333, 0.030612047761678696, 0.6848459839820862, 0.3268149793148041, 0.42887747287750244, -0.35598284006118774, 0.9205054044723511, 14.689432144165039, 0.8033286333084106, -0.2965067923069, 0.8127864599227905, 0.6819686889648438, 0.4646799564361572, -0.48622822761535645, 0.14064595103263855, -1.1443456411361694, -0.5731790065765381, 1.0557775497436523, 0.2916294038295746, 1.3290493488311768, 0.0960538312792778, -0.31095632910728455, 0.4865427315235138, -0.498151034116745, 0.7511436939239502, 0.3642495572566986, -1.5223572254180908, 0.4204827547073364, 0.33961984515190125, 0.502336859703064, 0.5602031350135803, 1.151078224182129, 1.0262054204940796, 0.3054819107055664, -0.5101208686828613, 0.484397828578949, 0.08999285846948624, 1.1320135593414307, 0.3453088104724884, 0.05557514354586601, 0.44403764605522156, -1.3000986576080322, -0.2726341485977173, -0.37532734870910645, -1.008915901184082, 0.19538404047489166, -0.1484619677066803, -0.8403295874595642, -0.5740014910697937, 0.10610800236463547, 0.908368706703186, -0.40341559052467346, 0.20391321182250977, -0.47065192461013794, 0.6586257219314575, -0.5853371620178223, 0.7322742938995361, 0.2802640497684479, 0.4067619740962982, -0.2012048214673996, -0.6833154559135437, 0.1661802977323532, -0.5157231688499451, 0.2541279196739197, 0.4044475257396698, -0.8003076314926147, -0.17229531705379486, -0.30224207043647766, -0.4621068835258484, -0.052389178425073624, 1.1320081949234009, 0.12468517571687698, 0.49509865045547485, -0.5999301671981812, 0.06530514359474182, 0.6632826328277588, 0.3455440104007721, -0.5440804958343506, 0.4701918363571167, 0.03596751019358635, -0.23445703089237213, -0.11681576073169708, 0.5534948110580444, -0.20018541812896729, -0.7502941489219666, -0.6961585879325867, -0.460234135389328, 0.3509939908981323, -1.0730558633804321, -0.32955947518348694, 0.9887721538543701, -0.1429968774318695, -0.43804147839546204, 0.20727375149726868, -0.8948150277137756, -0.10665620118379593, 0.3873636722564697, -1.465745210647583, -1.5801440477371216, 0.11869891732931137, -0.4819585084915161, -0.407755047082901, -0.6452160477638245, 1.0603817701339722, 0.262694776058197, -0.2694684565067291, 0.21031510829925537, -0.31376615166664124, -0.10325590521097183, -0.29437732696533203, -0.536507248878479, 1.1879470348358154, 0.5309011936187744, 0.4528752863407135, 0.058426789939403534, 0.15513384342193604, 0.4279312193393707, -0.6411929130554199, 0.04341990128159523, 0.3611332178115845, -0.8287484645843506, 0.0687725767493248, -0.8244678378105164, -0.5590118765830994, 0.6912981867790222, 0.36592063307762146, 0.02024209313094616, 0.12538285553455353, 0.10866560786962509, -1.0479505062103271, -0.35792458057403564, -0.894788920879364, -0.057527706027030945, 0.8228123784065247, -1.120500922203064, -0.15641790628433228, -0.12242306023836136, 0.26488789916038513, -1.097395896911621, -0.6366519927978516, 0.14436087012290955, 0.051350872963666916, -0.26096054911613464, 1.120035171508789, -0.254543274641037, 0.5049341320991516, 0.756432294845581, -0.13817095756530762, -0.6739323735237122, -0.12947574257850647, -1.087875247001648, -0.13122180104255676, 0.02036439999938011, 0.5083228349685669, -0.5130242705345154, 0.4020209312438965, 0.5215628743171692, 0.23362873494625092, -0.4817124009132385, -0.47573167085647583, -0.05193100869655609, -0.061638396233320236, -0.7348628640174866, 0.5668312907218933, -0.4264422357082367, -0.5519355535507202, 0.2520434856414795, 0.490394651889801, 0.818431556224823, -0.17766979336738586, -0.7247112393379211, 0.3471328020095825, -0.06256737560033798, -0.441505491733551, -0.6106286644935608, -0.6829578280448914, -1.1959476470947266, 0.16996552050113678, -1.210216999053955, -0.1684652864933014, -1.0658326148986816, -0.485017865896225, -0.1825733780860901, -0.31729790568351746, 0.36215969920158386, 0.7104398012161255, -0.08479941636323929, -0.15706534683704376, -0.18088890612125397, -0.5085993409156799, 0.6518065929412842, 0.8424330353736877, -0.7968540787696838, 0.22659660875797272, -0.184258371591568, 0.057903267443180084, 0.5234163403511047, 0.21063125133514404, -0.052721817046403885, -1.256014108657837, -1.3301057815551758, 0.5408312678337097, -0.48643428087234497, -0.05498015135526657, -0.8729087710380554, 0.7452556490898132, 0.4944448173046112, -0.2366245985031128, 0.2914468050003052, 0.36931729316711426, -0.9408603310585022, -0.4888748228549957, 0.17089569568634033, -0.42581936717033386, 0.45020318031311035, 0.5347796678543091, -0.7945829629898071, -0.16218487918376923, 0.6429014801979065, 0.046794794499874115, -0.7757546305656433, -1.1456751823425293, 0.609519362449646, -0.14870648086071014, -0.12406179308891296, -0.3203601837158203, 0.012062564492225647, -1.1181139945983887, -0.34999504685401917, -0.1017688438296318, 0.31633785367012024, -0.16888010501861572, 0.8769446015357971, 0.35333287715911865, -1.1531635522842407, 0.30321770906448364, 0.5943437814712524, -0.08553614467382431, 0.07159499824047089, 0.22111296653747559, 0.4171834886074066, -0.6047886610031128, 0.4146184027194977, 0.1381293088197708, 0.48311424255371094, -0.5754806399345398, 0.018035786226391792, 1.0877729654312134, -0.9395173192024231, -0.1919296234846115, 1.4089319705963135, 0.1118369847536087, -1.1378947496414185, 0.18203690648078918, -1.2225096225738525, -0.41445598006248474, -0.5918808579444885, 0.3224996030330658, -0.2660734951496124, 0.04823267459869385, -0.016735730692744255, -0.3285025656223297, 0.4251689314842224, -0.08625718206167221, -0.4073505103588104, 0.8652980327606201, -0.06771818548440933, -0.4229183495044708, 0.43599390983581543, 0.5240442752838135, -0.7746450901031494, -1.080527663230896, -0.8438047170639038, -0.48784783482551575, -0.02103991061449051, 0.6238846778869629, -0.3255005478858948, -0.9935035705566406, 0.8641644716262817, 0.590543806552887, 0.080905020236969, 0.6071292161941528, 0.16706889867782593, 0.24688543379306793, 0.8094125986099243, 0.5145951509475708, -0.5425597429275513, -0.3336230516433716, 1.0482012033462524, 0.9161666631698608, -0.5589281320571899, 0.33847129344940186, -0.48414528369903564, -0.2973865568637848, 0.9046532511711121, 0.4846995770931244, -0.15267857909202576, 0.878875732421875, 0.09379272162914276, -0.026224195957183838, 0.216339111328125, -0.7841373085975647, -0.22505153715610504, 1.0058130025863647, 1.1833072900772095, 0.5419625639915466, 0.055297113955020905, 0.681280791759491, 0.6419314742088318, -0.17579717934131622, -0.019481854513287544, 0.10516710579395294, 0.3462698459625244, 0.0611712671816349, -0.2050829380750656, -0.02384762093424797, 0.5311100482940674, -0.25642094016075134, -0.580124020576477, 0.3421272039413452, 0.7671647667884827, 0.6694718599319458, 0.1808973252773285, 0.6925082802772522, -0.03735217824578285, 0.8007009625434875, 0.12253780663013458, 1.1800071001052856, -0.5399877429008484, -0.22892813384532928, 0.0027253502048552036, -0.6461294889450073, -0.024902889505028725, -0.14637970924377441, -0.35506460070610046, -0.41703978180885315, -0.23694166541099548, 0.16791099309921265, -0.4071686267852783, 0.4059942960739136, 1.078156590461731, 0.20333251357078552, 0.6959399580955505, -0.25914427638053894, -0.6696064472198486, -0.4522421956062317, -0.7051522731781006, 0.3116125762462616, -0.5740756392478943, -0.33092421293258667, -0.44076427817344666, -0.14938028156757355, -0.18697711825370789]}, "authors": [{"authorId": "2118952622", "name": "Peihao Wang"}, {"authorId": "1819152", "name": "Rameswar Panda"}, {"authorId": "1753629012", "name": "Lucas Torroba Hennigen"}, {"authorId": "73770180", "name": "P. Greengard"}, {"authorId": "2142741421", "name": "Leonid Karlinsky"}, {"authorId": "1723233", "name": "R. Feris"}, {"authorId": "2161310364", "name": "David D. Cox"}, {"authorId": "2969311", "name": "Zhangyang Wang"}, {"authorId": "152847918", "name": "Yoon Kim"}], "references": [{"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "159be298e25b7210ae577d7962cceb5e73aee687", "title": "Automated Progressive Learning for Efficient Training of Vision Transformers"}, {"paperId": "434f4ecbfdea4496bbcd763427fc605bf11abddc", "title": "Token Dropping for Efficient BERT Pretraining"}, {"paperId": "adb272fbdea3631059cf88ab764bb6c2ce29f965", "title": "Visual Prompt Tuning"}, {"paperId": "1098ca3dbda5778c2bf6c9e8cbb9bc7a02249e10", "title": "Staged Training for Transformer Language Models"}, {"paperId": "eeb29f35b1a4e2cbe2c76e6c63df858194ed5d5d", "title": "GradMax: Growing Neural Networks using Gradient Information"}, {"paperId": "34db717eac67fdc929f2f7006d26a32873f6063a", "title": "Low-Rank Constraints for Fast Inference in Structured Models"}, {"paperId": "7a49beff86a855f237f96ae3f0aefc9780cb31be", "title": "bert2BERT: Towards Reusable Pretrained Language Models"}, {"paperId": "981995fd64611f475179b280f4e9c241051ac185", "title": "Knowledge Inheritance for Pre-trained Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "3e1b060ebacfc7a966ec735c940e2ee48f2a7a99", "title": "Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks"}, {"paperId": "51f46cb42668cfe3745ecf029d032bf30253574f", "title": "GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training"}, {"paperId": "c16835c8e535ebd9c10a550ca9455fe384a14449", "title": "High-Performance Large-Scale Image Recognition Without Normalization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "0b98f8ec299de3358c5dfc0d842529b5aee0e97c", "title": "Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "dedcdc1fb3a6def9772dce674d89150923dd75b9", "title": "Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision"}, {"paperId": "e00484961fb2f30d2d48a5f9853fa3ebab140cac", "title": "Improving Transformer Optimization Through Better Initialization"}, {"paperId": "ff79c74c54c56a8b85c88e5b720e246765b04b16", "title": "Towards Adaptive Residual Network Training: A Neural-ODE Perspective"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "a68c3412e60560290400d2707596f82a914b7c00", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "48b7c2a2431f8fa8b67d693d0f21b741d7d23fe5", "title": "ChebNet: Efficient and Stable Constructions of Deep Neural Networks with Rectified Power Units using Chebyshev Approximations"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a29c3bb07d478a354fd5bc5635f98560ede8f8bb", "title": "Splitting Steepest Descent for Growing Neural Architectures"}, {"paperId": "d28c18a3c2a0afdc0a8634d18345af8d36e1f948", "title": "A Constructive Prediction of the Generalization Error Across Scales"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "96c82727dd5a80fef93007f888bb8569feb6bd85", "title": "Fixup Initialization: Residual Learning Without Normalization"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2c74a6477788e2017d3042fee8a1950ba2b47be2", "title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications"}, {"paperId": "449310e3538b08b43227d660227dfd2875c3c3c1", "title": "Neural Ordinary Differential Equations"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "cc2bc56b30e283bc6e7193dd42e66e56658b4875", "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm"}, {"paperId": "cd958525291ee1ab856d23aa93cb95c86d87ccbe", "title": "Multi-level Residual Networks from Dynamical Systems View"}, {"paperId": "84e65a5bdb735d62eef4f72c2f01af354b2285ba", "title": "Efficient Architecture Search by Network Transformation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "58b6bd06ea58c367c64286126ba14128b45041b8", "title": "ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "02f1c79f1a09d9501cd39fc7b527cd9e13dff7ad", "title": "Network Morphism"}, {"paperId": "2d33cbf5e62d0cbff5079b0ea0678892c4cc982e", "title": "Fast Orthogonal Projection Based on Kronecker Product"}, {"paperId": "97dc8df45972e4ed7423fc992a5092ba25b33411", "title": "All you need is a good init"}, {"paperId": "16cb6876666f3a7b56a636c1d85ad00bd0d98bf3", "title": "Net2Net: Accelerating Learning via Knowledge Transfer"}, {"paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa", "title": "Structured Transforms for Small-Footprint Deep Learning"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "a83cec6a91701bd8500f8c43ad731d4353c71d55", "title": "3D Object Representations for Fine-Grained Categorization"}, {"paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649", "title": "Understanding the difficulty of training deep feedforward neural networks"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "14cfd41e28c28c21e06a1583ef85d6fac321a557", "title": "Knowledge Transfer in Deep convolutional Neural Nets"}, {"paperId": "9e8cf03655d224b0994d0f9d4f5aa80bca07021a", "title": "The Recurrent Cascade-Correlation Architecture"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "11d36cf5fc2fc35c52bd5a735cdd29c685bae95e", "title": "MetaInit: Initializing learning by learning to initialize"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "598365b0c5024b362123dd8eba1a8bf941b89d49", "title": "The Kronecker Product"}, {"paperId": "995a3b11cc8a4751d8e167abc4aa937abc934df0", "title": "The Cascade-Correlation Learning Architecture"}]}