{"paperId": "187b574fbceea37c5b10f8af855b77a7df825117", "abstract": null, "venue": "", "year": 2021, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": null, "embedding": null, "authors": [{"authorId": "40759039", "name": "K. K. Kaushal"}], "references": [{"paperId": "e99a259299d4d555ee4c354f2095ab4401369c82", "title": "SciREX: A Challenge Dataset for Document-Level Information Extraction"}, {"paperId": "a82eb7d7e99b3d5c460c4db6fc9429ab0fa02738", "title": "Collection Management of Electronic Theses and Dissertations (CME) CS5604 Fall 2019"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8", "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1", "title": "A general framework for information extraction using dynamic span graphs"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "16c0ef924da1f6b510c9c783ac764156f5a3d631", "title": "A Survey on Deep Learning for Named Entity Recognition"}, {"paperId": "d393943a873ead524069d0f7f55acef05cc9ba45", "title": "Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "d257b1803b36aa602dc0dcaecaa6a7efd2a2a830", "title": "Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "ac17cfa150d802750b46220084d850cfdb64d1c1", "title": "Semi-supervised Multitask Learning for Sequence Labeling"}, {"paperId": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "title": "Semi-supervised sequence tagging with bidirectional language models"}, {"paperId": "d95f72fd2c84a4c412eb519b660e402db51d154a", "title": "CharNER: Character-Level Named Entity Recognition"}, {"paperId": "b616c6a2c50100d49184a8c5676965f8fd9dc4eb", "title": "Domain Specific Named Entity Recognition Referring to the Real World by Deep Neural Networks"}, {"paperId": "e2dba792360873aef125572812f3673b1a85d850", "title": "Enriching Word Vectors with Subword Information"}, {"paperId": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"}, {"paperId": "af88ce6116c2cd2927a4198745e99e5465173783", "title": "Bidirectional LSTM-CRF Models for Sequence Tagging"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "f8cdf754fb7c08caf6e2f82b176819230910be5b", "title": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes"}, {"paperId": "1f4c3833a229cc793b5ebe5e581b984bcd00d8cb", "title": "ChemSpot: a hybrid system for chemical named entity recognition"}, {"paperId": "b52fe0b796e4c899624ed3e9d9ea566453156844", "title": "Recognizing Named Entities in Tweets"}, {"paperId": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch"}, {"paperId": "e15167ab802b79f312f125b274a27547e887a90d", "title": "Hidden Markov Model"}, {"paperId": "0902676e007afc95aca48502a3b227535a7c9de6", "title": "Annotation of Chemical Named Entities"}, {"paperId": "a2e65df02ae308402a6376dce0a2970bfb4980c3", "title": "An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition"}, {"paperId": "d6eb31985086ab66bd91b2997f28d2e95f1b77cd", "title": "Unsupervised Named-Entity Recognition: Generating Gazetteers and Resolving Ambiguity"}, {"paperId": "e54d8b07ef659f9ee2671441c4355e414e408836", "title": "OntoNotes: The 90% Solution"}, {"paperId": "421151fa75e40dd86414215abf29d9f2c052a2e1", "title": "Unsupervised named-entity extraction from the Web: An experimental study"}, {"paperId": "889402d83c68927688f9fb441b55a68a9c919f6a", "title": "GENETAG: a tagged corpus for gene/protein named entity recognition"}, {"paperId": "4019e0732690521c69e60aad4e2cea7511454140", "title": "Enhancing HMM-based biomedical named entity recognition by studying special phenomena"}, {"paperId": "c432268dd885d9ae02190d8662321f3a3e325a4a", "title": "Biomedical Named Entity Recognition using Conditional Random Fields and Rich Feature Sets"}, {"paperId": "33779271302dff0495d728d192df8ac879ba2f11", "title": "Named Entity Recognition using Hundreds of Thousands of Features"}, {"paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}, {"paperId": "8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "title": "Early results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons"}, {"paperId": "ac93f3872d4bb20213731cd13188fb7ea42ee6d2", "title": "Entity Extraction without Language-Specific Resources"}, {"paperId": "f4ba954b0412773d047dc41231c733de0c1f4926", "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"}, {"paperId": "927abef52678ed23edf6c508ef7a26569440a329", "title": "An Algorithm that Learns What's in a Name"}, {"paperId": "e9292ba3230f01b9f6990362fdf06783b9347bf6", "title": "Nymble: a High-Performance Learning Name-finder"}, {"paperId": "6723dda58e5e09089ec78ba42827b65859f030e2", "title": "Message Understanding Conference- 6: A Brief History"}, {"paperId": "1db5c86848063970cd68e738057a5dd6244fc297", "title": "Maximum-entropy models in science and engineering"}, {"paperId": "f16841e022038e94a59f7e0a82002102b78d79a4", "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "8955aa0de8cf822104db246a4c39017125265364", "title": "Named Entity Recognition Over Electronic Health Records Through a Combined Dictionary-based Approach"}, {"paperId": "6ccb34bf2122304af5cbecf54402ee3d970e43f2", "title": "Induction of decision trees"}, {"paperId": "2a2d908d2306dd25f5ac6fff20bc83059c824bb1", "title": "University of Sheffield: Description of the LaSIE-II System as Used for MUC-7"}, {"paperId": "497355583d46f1bdd3487f4a59133fa5d246b798", "title": "NYU: Description of the MENE Named Entity System as Used in MUC-7"}, {"paperId": null, "title": "Support vector machines. IEEE Intelligent Systems and their applications"}, {"paperId": null, "title": "Scholkopf. Support vector machines"}, {"paperId": null, "title": "ner_trainer' location, run 'bash ./scripts/train_allennlp_local.sh out-put_dir"}, {"paperId": null, "title": "* output_dir is the directory where trained model files would be stored (can create an empty directory and point to it)"}, {"paperId": null, "title": "Model and point the BERT_VOCAB and BERT_WEIGHTS variable in \"ner_trainer/scripts/train_allennlp_local.sh\" to the Scibert folder present inside \"training_pipeline/ner_trainer/scibert"}, {"paperId": null, "title": "dataset_size\" variable in \"ner_trainer/scripts/train_allennlp_local.sh\" to the length of training data"}, {"paperId": null, "title": "* Put all the library positive class sentence embeddings in a text file named \"positive_library_sent_embeddings.txt\" inside \"TRAINING_DATA_BASE_PATH"}, {"paperId": null, "title": "Run the script located at \"training_pipeline/clustering_trainer.py\". -For training NER Model: * Data present in \"BILUO\" format should be placed in \"ner_trainer/data/cset"}]}