{"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "abstract": "We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass \\texttt{gpt-3.5-turbo-16k}\u2018s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama\u2019s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths \u2013 ablation results suggest that having abundant long texts in the pretrain dataset is \\textit{not} the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2023, "citationCount": 93, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.16039", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "An effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens is presented and ablation results suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance."}, "embedding": {"model": "specter_v2", "vector": [0.36066824197769165, 0.2337409257888794, -0.633458137512207, -0.2814592123031616, -0.2809538245201111, -0.26666897535324097, 0.7165424823760986, -0.10647954791784286, -0.4992079734802246, -0.18984225392341614, 0.762934148311615, -0.46560102701187134, 0.6707881093025208, 0.11228393018245697, -0.36130061745643616, 0.0641138106584549, -1.1609270572662354, -0.06354278326034546, -0.01990913599729538, -0.4700329005718231, -0.06601730734109879, -0.8215083479881287, -0.8850874304771423, 0.24222005903720856, 0.4698913097381592, 0.11483068764209747, 0.2766163647174835, 1.2151449918746948, -0.7836464047431946, 0.11337986588478088, 0.4506620764732361, -0.003631518455222249, 0.17799745500087738, 0.07449282705783844, -0.23010776937007904, -0.24573194980621338, 0.40188923478126526, -0.5006408095359802, -0.38593655824661255, 0.4262937009334564, -0.1061239242553711, 0.35682836174964905, -0.022133952006697655, -0.30914077162742615, -0.3615938723087311, 1.025823712348938, 0.45122030377388, 1.0441261529922485, -0.269948273897171, -0.5792213082313538, 1.4283071756362915, -1.4796772003173828, 0.10854995995759964, 1.1353142261505127, 0.766806423664093, 0.6005378365516663, -0.28158265352249146, -0.18591347336769104, 1.0012986660003662, 0.015935610979795456, -0.4721546769142151, -0.34442365169525146, -0.03740876168012619, 0.2605590224266052, 2.014439821243286, -0.3694649338722229, 0.25899991393089294, 0.9249633550643921, -0.12535622715950012, 1.5368258953094482, -0.15660271048545837, -1.2891759872436523, -0.27774837613105774, -0.19005107879638672, 0.4875686466693878, 0.3619747459888458, -0.28710880875587463, 0.4911935329437256, -0.8894939422607422, -0.14215782284736633, 0.38697582483291626, -0.15957942605018616, 0.3309582769870758, 0.0346977524459362, -0.4057072103023529, 0.48716792464256287, 0.010619751177728176, 1.085358738899231, 0.026191655546426773, 0.8913977742195129, 0.8180906772613525, 0.25300168991088867, 0.02096056379377842, 0.03071766532957554, -0.3667113482952118, 0.18376722931861877, -1.120668888092041, 0.11956645548343658, -0.04307844862341881, 0.8027535080909729, -0.1425493061542511, 0.27954110503196716, -0.5787402987480164, 0.17951692640781403, 1.1519862413406372, 0.2512558102607727, 0.6318241953849792, -0.41777709126472473, 0.37391990423202515, -0.6851038336753845, 0.055485982447862625, -0.11795452237129211, -0.30803683400154114, -0.5099408030509949, -0.3234955966472626, -1.263655424118042, -0.36185187101364136, -0.17506326735019684, -0.5148923993110657, 1.0107614994049072, 0.20811870694160461, 0.5604572892189026, 0.3762674927711487, 0.4106576144695282, 0.33362311124801636, 0.7651715874671936, 0.4670884609222412, 0.029834220185875893, 0.6851113438606262, -0.9641514420509338, -0.5616554617881775, -1.2993494272232056, 0.8595276474952698, -0.47502371668815613, 0.8789983987808228, -0.13782620429992676, -1.1147404909133911, -1.1075937747955322, -1.1059396266937256, -0.18132062256336212, -0.6170921921730042, 0.3260672390460968, 1.1863269805908203, 0.6137259006500244, -0.8327536582946777, 0.7389061450958252, -0.4973802864551544, 0.18988770246505737, 0.10086188465356827, 0.04332255944609642, 0.4114023745059967, -0.464194118976593, -1.6356858015060425, 0.05611754208803177, 0.42150643467903137, -0.4955148696899414, -0.012355062179267406, -0.5949425101280212, -1.0701977014541626, -0.2904677987098694, 0.46567434072494507, -0.16092446446418762, 1.4213111400604248, -0.1189664825797081, -1.0941270589828491, 0.34441348910331726, -0.2697458863258362, 0.17408980429172516, 0.0672835037112236, -0.43080785870552063, -0.8437470197677612, -0.39951711893081665, -0.35283443331718445, 0.4824581444263458, 0.11741587519645691, -0.10949527472257614, -0.4215603172779083, 0.11026261746883392, -0.25930336117744446, 0.12549161911010742, -0.2909701466560364, 0.548967719078064, -0.26254239678382874, -0.24965937435626984, 0.2731998860836029, 0.6360421180725098, -0.15048612654209137, -0.5471876263618469, -0.28135156631469727, -0.9886185526847839, 0.983420193195343, 0.03722952678799629, 1.4242409467697144, -0.9828549027442932, -1.0702412128448486, -0.13197994232177734, -0.6475064754486084, 0.06351190060377121, -0.8125839233398438, 0.48616981506347656, -0.2933415472507477, 0.298376202583313, -0.19802723824977875, -1.272301435470581, 0.1104503870010376, -0.030041802674531937, -0.9649860858917236, -0.43519434332847595, 0.10631804168224335, 0.9773299694061279, -0.9761470556259155, 0.0007629066240042448, -0.10859666019678116, 0.5403080582618713, -1.2372874021530151, 0.8185287117958069, -0.6537204384803772, 0.05163407325744629, -0.056307703256607056, -0.21183469891548157, -0.0639839768409729, -0.22322872281074524, 0.7682921290397644, -0.13164332509040833, -0.17808523774147034, 0.6858144998550415, -0.5142950415611267, 1.5055195093154907, -0.6706739068031311, 0.5586886405944824, -0.050893157720565796, -0.42478108406066895, -0.19813913106918335, 0.2327152043581009, -0.2326548546552658, -0.15795381367206573, 0.037194568663835526, 0.7649139165878296, -0.5398790836334229, 0.19250304996967316, 1.3728930950164795, 0.5739825963973999, -0.5462914109230042, 0.14232346415519714, 0.45272907614707947, -0.18823054432868958, 0.5895039439201355, 0.39483919739723206, 0.39916375279426575, 1.0798121690750122, 0.18993300199508667, -0.26549476385116577, 0.36356350779533386, -0.7787609100341797, -0.1840936839580536, 0.20976290106773376, 0.6105261445045471, 0.4428721070289612, 0.3938296139240265, -0.6398008465766907, -0.7885777354240417, 0.26094064116477966, 0.39352425932884216, 1.9737476110458374, -0.31814369559288025, -0.061750300228595734, -0.8998734951019287, -0.3313320577144623, -0.34189993143081665, 0.4488013684749603, -0.48738840222358704, 0.06108579784631729, -1.037440538406372, -0.9580706357955933, 0.7928103804588318, 0.4476776123046875, 0.8960402607917786, -0.47158265113830566, -0.3600701093673706, -0.09561954438686371, 0.1737796813249588, -0.9129579663276672, -0.8034258484840393, 0.608032763004303, -0.6705833077430725, -0.09731952846050262, 0.3717668354511261, -0.3010038733482361, -0.07409121841192245, -0.6495765447616577, 1.0160677433013916, 0.03324978053569794, -0.2901339828968048, 0.20666857063770294, 0.36108314990997314, -0.2535844147205353, -0.8372228145599365, 0.4223209619522095, 0.2898859977722168, -0.45114850997924805, 0.6137695908546448, 0.45598143339157104, 0.05834678187966347, 0.043334346264600754, -0.5636721253395081, 0.1855030208826065, 0.16663679480552673, 0.28972551226615906, 0.9321618676185608, -0.3470330238342285, 0.33136501908302307, -1.3551440238952637, 0.6222493052482605, 0.29580816626548767, -0.21141840517520905, 0.2892915606498718, -0.5936880707740784, -0.39731064438819885, 0.6346122622489929, -0.8640533685684204, -0.40602171421051025, -1.043466567993164, 0.17921887338161469, -0.11817137897014618, -0.28779393434524536, 0.1502757966518402, 0.2318524569272995, 0.3204233944416046, 0.465063214302063, 0.3489735722541809, -0.029375363141298294, -0.05107715725898743, 0.6093873381614685, -0.713752031326294, 0.5785735845565796, -0.28395676612854004, -0.11328946053981781, -0.18673762679100037, -0.017598185688257217, -0.8259445428848267, -0.6290084719657898, -0.5262402296066284, -0.6397579908370972, -0.18706712126731873, 0.18858885765075684, -0.6107732057571411, -0.5340562462806702, -0.30744725465774536, -0.6282573938369751, -0.9040737152099609, 0.027816521003842354, -0.32893282175064087, -0.24902068078517914, -0.7212440967559814, -1.1934518814086914, -0.5954672694206238, -0.5484035015106201, -0.8921101093292236, 0.559850811958313, 0.044673625379800797, -0.17798975110054016, -0.9887000322341919, 0.0002508586330804974, -0.5534748435020447, 0.9387584328651428, -0.4108509123325348, 0.6778913736343384, 0.07050886750221252, -0.16680322587490082, -0.04257186874747276, 0.6180458068847656, 0.5782458186149597, -0.6201443076133728, 0.6383590698242188, -1.0356190204620361, 0.22782982885837555, -0.41896846890449524, -0.39460521936416626, 0.2583736777305603, 0.1767876148223877, 0.3034501075744629, 0.21713806688785553, -0.45281529426574707, 0.3161422908306122, 1.2682874202728271, -0.9509555697441101, 0.07768545299768448, 0.19494037330150604, 0.9037238955497742, 0.1463315337896347, -0.09684636443853378, 0.2891811728477478, 0.1838759332895279, -0.05216728895902634, -0.06904047727584839, 0.14591550827026367, -0.08627041429281235, -0.6599327921867371, 0.8778730630874634, 1.6924278736114502, 0.36585599184036255, 0.2727399170398712, -1.1307827234268188, 0.7151110172271729, -1.0381417274475098, -0.7277060747146606, 0.5769148468971252, 0.834234893321991, 0.7197942733764648, -0.20171710848808289, -0.2359502762556076, -0.023089488968253136, 0.45035630464553833, 0.23244868218898773, -0.4416241943836212, -0.7830517888069153, -0.1266562044620514, 0.16252921521663666, -0.18590125441551208, 0.7041804790496826, -0.492983877658844, 0.6925666928291321, 14.874693870544434, 0.8435970544815063, 0.054465677589178085, 0.5957989692687988, 0.8773879408836365, -0.010158191435039043, -0.6876328587532043, 0.014950728975236416, -1.381201148033142, -0.17338250577449799, 1.5537025928497314, 0.5285893082618713, 0.7198001146316528, 0.22081680595874786, -0.03898791968822479, 0.03223180025815964, -0.6788327693939209, 0.1523684412240982, 0.45129185914993286, -1.1641427278518677, 0.11927621066570282, 0.023103028535842896, 0.7527126669883728, 0.6513341069221497, 0.851622462272644, 1.2351312637329102, 0.3006352484226227, 0.06250793486833572, 0.7174304127693176, -0.10745321959257126, 1.2612875699996948, -0.22893932461738586, 0.2504257261753082, 0.4243902862071991, -0.6871662735939026, -0.409576416015625, -0.6561556458473206, -1.1360054016113281, 0.21234995126724243, 0.1890346109867096, -0.6194427609443665, -0.5052274465560913, -0.5450863242149353, 0.3602077066898346, 0.12403639405965805, 0.05773124471306801, -0.37473031878471375, 0.9931737780570984, -0.2161148190498352, -0.011208425275981426, 0.5944153666496277, 0.5120539665222168, 0.16996221244335175, 0.2800830900669098, 0.10758325457572937, 0.12193848937749863, 0.42457690834999084, 0.1281488835811615, -0.4282539486885071, 0.10253139585256577, -0.05242102965712547, -0.337504118680954, -0.12685656547546387, 0.8379056453704834, 0.4376216232776642, 0.08038398623466492, -0.5294302105903625, 0.055511992424726486, 0.47146373987197876, 0.2265000194311142, -0.33360928297042847, 0.03510640561580658, 0.17969398200511932, -0.5906569361686707, -0.02935343235731125, 0.26393982768058777, -0.37402716279029846, -0.7381384968757629, -0.6868601441383362, -0.12228590250015259, 0.26703229546546936, -0.6087608337402344, -0.5475576519966125, 0.5766686201095581, -0.10391931980848312, -0.38087472319602966, 0.05884421989321709, -0.945826530456543, -0.43226486444473267, 0.33717095851898193, -1.6875115633010864, -0.7853264212608337, 0.47622376680374146, -0.4029982089996338, -0.36999985575675964, 0.05389045178890228, 1.2897917032241821, 0.039455704391002655, -0.37176477909088135, 0.18917624652385712, 0.14863015711307526, 0.011986068449914455, -0.1838981807231903, -1.022667407989502, 1.1930906772613525, 0.5021867156028748, -0.08538270741701126, 0.630404531955719, 0.1283048838376999, 0.013671031221747398, -0.7652009725570679, 0.03354775533080101, 0.7107634544372559, -1.0269756317138672, -0.2003181129693985, -0.8292878866195679, -0.8466441631317139, 0.3917115330696106, 0.6272639632225037, 0.004636510740965605, 0.776222825050354, 0.4034247398376465, -0.5260028839111328, 0.035826291888952255, -0.7204657793045044, 0.26935362815856934, 0.5336094498634338, -0.8041607141494751, -0.5426696538925171, -0.2260880321264267, 0.718697190284729, -1.2688558101654053, -0.5616199970245361, -0.36877018213272095, 0.15062014758586884, -0.1276949942111969, 0.8117591738700867, -0.16196702420711517, 0.9090569615364075, 0.9119032621383667, -0.489287406206131, -0.5834596157073975, 0.38837894797325134, -1.1049182415008545, -0.16020743548870087, 0.1938880831003189, 0.7241357564926147, -0.12339552491903305, 0.13870979845523834, 0.6028317213058472, 0.011190389283001423, -0.7808091044425964, -0.7448310852050781, -0.21819831430912018, 0.42599403858184814, -0.7459459900856018, 0.6235657334327698, 0.1436348855495453, 0.15041252970695496, 0.13823941349983215, 0.6492902636528015, 0.518737256526947, -0.41646987199783325, -0.5512922406196594, 0.35145896673202515, 0.4837995767593384, -0.3230886161327362, -0.40639984607696533, -0.22458240389823914, -1.4106135368347168, -0.10406754165887833, -1.0728235244750977, 0.01010370347648859, -0.5354134440422058, -0.6528241038322449, -0.36851537227630615, -0.34235143661499023, 0.029404813423752785, -0.07840953022241592, -0.4224148690700531, -0.5711420774459839, -0.9263526797294617, -0.4475136697292328, 0.8008609414100647, 0.9085189700126648, -0.24132882058620453, 0.044376518577337265, -0.04319659620523453, 0.29229453206062317, 0.23556290566921234, 0.4454224705696106, 0.1350046992301941, -0.734491229057312, -1.4538365602493286, 0.6511262655258179, -0.2813725173473358, -0.42910292744636536, -0.6203370094299316, 0.17589235305786133, 0.16464534401893616, 0.06359179317951202, 0.019333308562636375, 0.10030487179756165, -0.8220968842506409, -0.8811048269271851, -0.0012919945875182748, -0.7637504935264587, 0.13734044134616852, 0.556141197681427, -0.34820133447647095, -0.012648044154047966, 0.45302408933639526, -0.3298358619213104, -0.8564848303794861, -0.951285183429718, 0.06517384946346283, -0.32612499594688416, 0.21266505122184753, -0.5240065455436707, -0.027706824243068695, -1.1282365322113037, 0.045395106077194214, -0.055530399084091187, 0.37763693928718567, -0.16321729123592377, 0.968237578868866, -0.0011354659218341112, -1.172813057899475, 0.21659231185913086, 0.35614466667175293, -0.35087209939956665, 0.4186934530735016, 0.7028043866157532, 0.2957708239555359, -0.2815103828907013, 0.5018015503883362, 0.5494613647460938, 0.013273285701870918, -0.7054523229598999, 0.0010950255673378706, 0.5465673208236694, -0.5404834151268005, -0.2540295422077179, 1.0432775020599365, -0.47611820697784424, -1.1519030332565308, 0.008999312296509743, -1.5354697704315186, -0.49232742190361023, -0.6161972284317017, 0.9027233719825745, 0.03650383651256561, -0.19021150469779968, -0.1068330705165863, -0.27474135160446167, 0.1373547613620758, 0.04943626746535301, -0.5403047204017639, 0.6541035175323486, -0.3474659025669098, -0.15778954327106476, 0.9214428663253784, 0.550845742225647, -0.21139463782310486, -1.0273040533065796, -0.4934923052787781, -0.3352898955345154, 0.017970148473978043, 0.4296712875366211, -0.6891078352928162, -0.02559080347418785, 1.0464836359024048, 0.28615421056747437, 0.30798131227493286, 0.1176573857665062, -0.5372616648674011, 0.3759786784648895, 0.8214672803878784, 0.2834349572658539, -0.989995002746582, -0.4750698208808899, 1.4137510061264038, 0.9998615384101868, -0.8531972169876099, 0.41147851943969727, 0.0251412745565176, -0.558172881603241, 0.9584769606590271, 0.48298144340515137, 0.03681626543402672, 0.6789084076881409, -0.19879250228405, 0.018452264368534088, 0.5178525447845459, -1.146696925163269, -0.03292660415172577, 0.49216169118881226, 0.7076987624168396, 0.8315820693969727, 0.4081322252750397, 0.21121849119663239, 1.0263829231262207, 0.04251691699028015, -0.02866518311202526, 0.6870918273925781, 0.6697046160697937, -0.35275113582611084, -0.29240208864212036, -0.05647577345371246, 0.3639589846134186, -1.0118783712387085, -0.7354301810264587, 0.5634052753448486, 0.607673704624176, 0.403603196144104, 0.43164095282554626, 0.8197920322418213, 0.30620867013931274, 0.29394960403442383, 0.554739773273468, 0.6771208643913269, -0.5492274761199951, -0.444200336933136, 0.1965285837650299, -0.5019054412841797, 0.024863174185156822, -0.11130250990390778, -0.6165259480476379, -0.7997514009475708, -0.13412930071353912, -0.039227552711963654, -0.032365575432777405, 0.613669216632843, 1.2817442417144775, 0.4480084478855133, 0.6157472133636475, -0.3259255290031433, -0.686797022819519, -0.5794993042945862, -0.9256619811058044, 0.14922960102558136, -0.5815789699554443, -0.052247583866119385, 0.12446296215057373, 0.22577591240406036, -0.07712619006633759]}, "authors": [{"authorId": "22253126", "name": "Wenhan Xiong"}, {"authorId": "2301500865", "name": "Jingyu Liu"}, {"authorId": "66839644", "name": "Igor Molybog"}, {"authorId": "2248727777", "name": "Hejia Zhang"}, {"authorId": "51229603", "name": "Prajjwal Bhargava"}, {"authorId": "2266467782", "name": "Rui Hou"}, {"authorId": "2249724552", "name": "Louis Martin"}, {"authorId": "150282885", "name": "Rashi Rungta"}, {"authorId": "2178963", "name": "Karthik Abinav Sankararaman"}, {"authorId": "9185192", "name": "Barlas O\u011fuz"}, {"authorId": "2072010", "name": "Madian Khabsa"}, {"authorId": "2263641898", "name": "Han Fang"}, {"authorId": "2121361882", "name": "Yashar Mehdad"}, {"authorId": "46617804", "name": "Sharan Narang"}, {"authorId": "2248052258", "name": "Kshitiz Malik"}, {"authorId": "2247818297", "name": "Angela Fan"}, {"authorId": "2116473", "name": "Shruti Bhosale"}, {"authorId": "2068070", "name": "Sergey Edunov"}, {"authorId": "2247796743", "name": "Mike Lewis"}, {"authorId": "2096387", "name": "Sinong Wang"}, {"authorId": "2248016651", "name": "Hao Ma"}], "references": [{"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "b0db25e317cf856f1ec1ca3df0e573d850ed4696", "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "eb511ae6b9f04e4936891d26787f274b48b99d57", "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "705e49afd92130f2bc1e0d4d0b1f6cb14e88803f", "title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "382ba0c4452aab6ecdaf8a62d567bb3c4684e4f0", "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "3def68bd0f856886d34272840a7f81588f2bc082", "title": "Survey of Hallucination in Natural Language Generation"}, {"paperId": "3c209e0703ffff26231b1145268c935df494631a", "title": "QuALITY: Question Answering with Long Input Texts, Yes!"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "aa28873534c24e4a8c5deb7bff723cd5fc69a6f0", "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "ce3b364b7e6358940ce97d8d5887a65e5024ca21", "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "bcdc102c04fb0e7d4652e8bcc7edd2983bb9576d", "title": "VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text"}, {"paperId": "7b36c5602930abf08efd2867f92cdb48a1be757a", "title": "Together"}, {"paperId": null, "title": "Introducing 100K Context Windows"}, {"paperId": null, "title": "Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length"}, {"paperId": null, "title": "Llama-2-7b-32k-instruct -and fine-tuning for llama-2 models with together api"}, {"paperId": null, "title": "Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance"}, {"paperId": null, "title": "Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "Socialiqa: Com-monsense reasoning about social interactions"}, {"paperId": null, "title": "2022. A length-extrapolatable transformer"}, {"paperId": null, "title": "MosaicML"}, {"paperId": null, "title": "OpenAI. 2023."}, {"paperId": null, "title": "Introducing mpt-30b: Raising the bar for open-source foundation models"}, {"paperId": null, "title": "Introducing the world\u2019s first truly open instruction-tuned"}, {"paperId": null, "title": "NTK-Aware Scaled RoPE allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "the question and answer using XML tags (<question> and </question>, <answer> and </answer>). Again, the answer needs to be short"}, {"paperId": null, "title": "2022b. Hungry hungry hippos: Towards language modeling with state space models"}]}