{"paperId": "23b09ed66024fdd04d6713b9ba621b866f033d20", "abstract": "The advent of Large Language Models (LLMs) represents a notable breakthrough in Natural Language Processing (NLP), contributing to substantial progress in both text comprehension and generation. However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation. Understanding and extending the context length for LLMs is crucial in enhancing their performance across various NLP applications. In this survey paper, we delve into the multifaceted aspects of exploring why it is essential, and the potential transformations that superior techniques could bring to NLP applications. We study the inherent challenges associated with extending context length and present an organized overview of the existing strategies employed by researchers. Additionally, we discuss the intricacies of evaluating context extension techniques and highlight the open challenges that researchers face in this domain. Furthermore, we explore whether there is a consensus within the research community regarding evaluation standards and identify areas where further agreement is needed. This comprehensive survey aims to serve as a valuable resource for researchers, guiding them through the nuances of context length extension techniques and fostering discussions on future advancements in this evolving field.", "venue": "arXiv.org", "year": 2024, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This comprehensive survey aims to serve as a valuable resource for researchers, guiding them through the nuances of context length extension techniques and fostering discussions on future advancements in this evolving field."}, "embedding": {"model": "specter_v2", "vector": [0.5357239842414856, 0.43417856097221375, -0.49044597148895264, -0.1987173855304718, -0.571751594543457, -0.6879555583000183, 0.4114523231983185, -0.03710953891277313, -0.5941451787948608, 0.27020958065986633, 0.7637025713920593, -0.5045650601387024, 0.08527771383523941, 0.19880661368370056, -0.11373930424451828, 0.0712091401219368, -0.8083410263061523, 0.07522037625312805, -0.3846765458583832, -0.18043071031570435, -0.2806347906589508, -0.4066493809223175, -0.5031997561454773, 0.22843970358371735, 0.6639107465744019, -0.22673200070858002, 0.40214627981185913, 1.1113591194152832, -0.4900658130645752, 0.1152837947010994, 0.435730516910553, -0.14438439905643463, 0.21319691836833954, -0.07034885883331299, 0.20619478821754456, -0.4601558446884155, 0.49433162808418274, -0.5589056015014648, -0.2934335768222809, 0.5666898488998413, -0.18766044080257416, 0.31360921263694763, -0.018247170373797417, -0.7889083623886108, -0.3042515516281128, 1.4830069541931152, 0.5909953117370605, 0.7254169583320618, 0.4125051200389862, -0.535636842250824, 1.4896514415740967, -1.3180668354034424, 0.7877199053764343, 1.5908607244491577, 0.7552346587181091, 0.3842000365257263, -0.41223227977752686, -0.6233463287353516, 0.5950923562049866, -0.25710952281951904, -0.8953393697738647, -0.48548656702041626, 0.09664349257946014, -0.08152822405099869, 2.00333833694458, -0.008348291739821434, -0.3080432415008545, 0.6220929026603699, -0.06977322697639465, 1.239516258239746, -0.23401738703250885, -1.4716880321502686, -0.24002113938331604, 0.036602683365345, 0.07003974169492722, 0.32888662815093994, -0.537122368812561, 0.34993115067481995, -0.5409552454948425, -0.6731957197189331, 0.042892590165138245, -0.5017332434654236, -0.25095391273498535, 0.26438504457473755, -0.436557799577713, 0.8612071871757507, -0.040396545082330704, 0.87958824634552, 0.025599589571356773, 0.4780158996582031, 0.4000728726387024, 0.48512908816337585, 0.18335609138011932, 0.3700287640094757, -0.30329081416130066, 0.567145824432373, -1.2532892227172852, 0.6277608275413513, 0.2337070107460022, 1.151829719543457, -0.282650887966156, -0.1261828988790512, -0.896196722984314, 0.3651634752750397, 1.0368459224700928, -0.07469985634088516, 0.33294421434402466, -0.9056831002235413, 0.8885228633880615, -0.39803463220596313, 0.6790120601654053, -0.2502453625202179, -0.40563681721687317, -0.2539522349834442, -0.29043474793434143, -1.1418918371200562, 0.10064109414815903, -0.5521398186683655, -0.15676243603229523, 1.1204712390899658, -0.018128059804439545, -0.009609773755073547, 0.45957618951797485, 0.12102241069078445, -0.005159413442015648, 0.788608729839325, 0.057617634534835815, -0.5094181895256042, 0.7960520386695862, -0.43211114406585693, -0.728139340877533, -1.4212459325790405, 1.2014307975769043, -0.4357964098453522, 0.47177693247795105, -0.6186486482620239, -1.3113574981689453, -0.6140362024307251, -0.9041381478309631, -0.22617311775684357, -0.4386114180088043, 0.5307405591011047, 0.7471993565559387, 0.09586862474679947, -1.0731940269470215, 0.6037861704826355, 0.13925139605998993, -0.2863461375236511, -0.42960354685783386, -0.1905473917722702, 0.22555652260780334, -0.5820222496986389, -1.552479863166809, 0.23849166929721832, 0.3932678699493408, -0.8274171352386475, 0.24134086072444916, -0.3648492991924286, -1.3412011861801147, -0.06824386864900589, 0.35323747992515564, -0.37947097420692444, 1.2333015203475952, 0.11412996053695679, -0.8171544075012207, 0.15495681762695312, -0.5361395478248596, 0.03967789560556412, 0.3229713439941406, -0.14576701819896698, -0.7448685169219971, -0.6683323383331299, -0.048315804451704025, 0.5645715594291687, -0.10530602186918259, -0.10832459479570389, -0.536457359790802, 0.2384503334760666, -0.03546401485800743, 0.21290017664432526, -0.14189693331718445, 1.3608214855194092, -0.3074127435684204, -0.3219905495643616, 0.38607388734817505, 0.2928718030452728, -0.4917517900466919, -0.07029297947883606, -0.21323880553245544, -0.7987865805625916, 0.9626847505569458, -0.30045029520988464, 1.533264398574829, -0.46517419815063477, -0.9572442770004272, -0.388523668050766, -0.4908022880554199, -0.3369526267051697, -0.9382922053337097, 1.329447865486145, -0.18630751967430115, 0.4825192987918854, -0.5016219019889832, -0.7827850580215454, -0.1026240736246109, -0.21062196791172028, -0.7852602005004883, -0.37982046604156494, -0.32747170329093933, 0.8119213581085205, -0.9194725751876831, 0.08016901463270187, -0.015594311989843845, 0.11988215148448944, -0.6177403926849365, 0.8664427399635315, -0.8514292240142822, 0.2834150493144989, 0.17537492513656616, -0.1926080286502838, 0.1501334309577942, 0.006491811014711857, 0.5827106833457947, 0.023113857954740524, -0.6279300451278687, 0.28197136521339417, -0.3414376974105835, 1.6253788471221924, -0.34985262155532837, 0.531753420829773, 0.22969885170459747, 0.09207233786582947, -0.3790779411792755, 0.7207340598106384, -0.3846145570278168, 0.3725643455982208, -0.04047485440969467, 0.3787851631641388, -0.37348225712776184, 0.28174760937690735, 0.8948307633399963, 1.0377053022384644, -0.26110750436782837, 0.40334630012512207, 0.14409039914608002, -0.23021957278251648, 1.125840663909912, 0.3719390332698822, 0.4983339011669159, 0.44477275013923645, 0.5377549529075623, 0.021612999960780144, 0.6243504881858826, -0.6500104665756226, -0.048860032111406326, 0.4984336793422699, 0.6452861428260803, 0.43188801407814026, 0.3169158101081848, -0.41095176339149475, -0.4232923686504364, 0.4874984920024872, 0.6738168597221375, 1.740088939666748, -0.6422457098960876, -0.33427542448043823, -1.1144781112670898, -0.5202969312667847, -0.49211397767066956, 0.7779702544212341, -0.2804831564426422, 0.4901271164417267, -0.7808631658554077, -0.6780427098274231, 1.147870659828186, 0.39215943217277527, 0.09124726057052612, -0.6358265280723572, -0.19983747601509094, 0.22733038663864136, -0.05665973201394081, -1.0862877368927002, -0.7534769177436829, 0.09718726575374603, -0.9158716201782227, -0.32403892278671265, -0.0260773953050375, -0.1258324384689331, -0.03172318637371063, -0.7982721328735352, 0.6713077425956726, -0.39775750041007996, -0.053864456713199615, -0.13121803104877472, 0.41453447937965393, -0.7668502330780029, -1.334060788154602, 0.014282133430242538, 0.06742691248655319, -0.8124368190765381, 0.5383785963058472, 0.594208300113678, 0.26678889989852905, -0.13764822483062744, -0.6245945692062378, 0.07894822955131531, 0.05916399881243706, -0.13904300332069397, 0.7987727522850037, -0.4149675965309143, 0.15878461301326752, -1.336966872215271, 1.4505341053009033, 0.396563857793808, -0.7843130230903625, 0.7191088795661926, -0.8821556568145752, -0.3479209244251251, 0.6956915259361267, -0.6519590020179749, -0.5004686117172241, -0.6869816780090332, 0.2531515657901764, 0.2903554141521454, -0.5375440716743469, 0.611420750617981, 0.1431833952665329, 0.2693241536617279, 0.45202115178108215, 0.3692878782749176, 0.20487482845783234, -0.31215837597846985, 0.9253106713294983, -0.3908769190311432, 0.1824473738670349, 0.5802361965179443, -0.1873597502708435, -0.2531130313873291, -0.10716084390878677, -0.6387614607810974, -0.028005922213196754, -0.20041769742965698, -0.3578374683856964, 0.036305516958236694, -0.29409563541412354, -0.44257527589797974, 0.29357707500457764, -0.3145408630371094, -1.1835054159164429, 0.0026711977552622557, 0.291317880153656, -0.038085803389549255, 0.09148744493722916, -0.5523508787155151, -1.3919813632965088, -0.5619949102401733, -0.7369846701622009, -1.1179133653640747, 0.44853055477142334, -0.2028624415397644, -0.6495745182037354, -0.6577996015548706, 0.2721220850944519, -0.3097940683364868, 0.6929725408554077, -0.5173055529594421, 1.282243013381958, 0.018827524036169052, 0.3074660301208496, -0.44657227396965027, 0.34457865357398987, -0.1090894415974617, -0.21904565393924713, 0.34641608595848083, -0.6932886242866516, 0.11095038801431656, -0.18555569648742676, 0.04515727609395981, -0.36467668414115906, 0.7128992676734924, 0.11982300132513046, -0.21832013130187988, -0.7714909911155701, 0.24880771338939667, 1.2633721828460693, -0.6052265763282776, -0.34630322456359863, -0.06585800647735596, 0.6693375110626221, 0.5253551006317139, -0.10964378714561462, 0.574249267578125, -0.10287211090326309, 0.28584399819374084, 0.07397710531949997, -0.055607493966817856, 0.07730843871831894, -0.5427535176277161, 0.49220505356788635, 1.2985647916793823, 0.35945528745651245, -0.49979662895202637, -1.0036189556121826, 0.3741275370121002, -1.3667104244232178, -0.5238766074180603, 0.5526044964790344, 0.6895630955696106, 0.6217746734619141, -0.5379753112792969, -0.5195116400718689, -0.1560078114271164, 0.6086451411247253, 0.41473209857940674, -0.3532394468784332, -0.5104654431343079, -0.30463728308677673, -0.24727612733840942, 0.12166068702936172, 0.8694519996643066, -0.4380325376987457, 0.9353068470954895, 14.65365219116211, 0.8261733651161194, 0.29125452041625977, 0.49452832341194153, 0.19964034855365753, 0.1940978765487671, -0.48225176334381104, 0.14109177887439728, -1.485650658607483, -0.08626490831375122, 1.2078629732131958, -0.0029403166845440865, 0.8022323250770569, 0.20594409108161926, 0.7750986218452454, -0.221486896276474, -0.5412569046020508, 0.5291609764099121, 0.4246065318584442, -1.1988742351531982, 0.529596745967865, 0.23019540309906006, 0.4964923858642578, 0.7388541102409363, 0.19543221592903137, 0.6323796510696411, 0.1631230264902115, -0.34252700209617615, 0.3321266174316406, 0.007162776775658131, 0.8198308348655701, -0.19506330788135529, 0.6684698462486267, 1.1424192190170288, -1.0369235277175903, -0.6002426147460938, -0.9191895723342896, -1.3155783414840698, 0.4519408047199249, 0.3785651922225952, -0.7609357833862305, -0.4930971562862396, -0.6539430022239685, 0.38310277462005615, -0.14360713958740234, 0.2486572265625, -0.27599790692329407, 0.8620128631591797, -0.32698699831962585, 0.17620742321014404, 0.3503102958202362, 0.02069029025733471, 0.6063854694366455, 0.16499356925487518, 0.45431333780288696, -0.15245720744132996, 0.24392375349998474, 0.27004727721214294, -0.6052869558334351, 0.3242887854576111, -0.8613234758377075, -0.21395760774612427, 0.36643800139427185, 0.547752320766449, 0.3238467872142792, 0.235490620136261, -0.40639132261276245, 0.18902434408664703, 0.6834086179733276, 0.5408456921577454, 0.09729544818401337, -0.35180437564849854, 0.2639561891555786, -0.4147636890411377, -0.4958295226097107, 0.5214795470237732, -0.4049101173877716, -0.4154629409313202, -0.4683649241924286, -0.36290332674980164, 0.24731765687465668, -0.8471431732177734, -0.5304073095321655, 0.8414819836616516, 0.15448662638664246, -0.5644660592079163, -0.27123743295669556, -0.6112253665924072, -0.25545310974121094, 0.7528831362724304, -1.077931523323059, -1.1891344785690308, 0.7905535101890564, -0.4834141731262207, -0.06590869277715683, 0.10659017413854599, 1.376163125038147, -0.3796233832836151, -0.4573103189468384, 0.24453651905059814, 0.8803150653839111, 0.10801401734352112, -0.16617955267429352, -0.6426210403442383, 0.8678004145622253, 0.7914650440216064, -0.062163714319467545, 0.6182824969291687, 0.12312589585781097, -0.2766133248806, -0.6070720553398132, -0.23343926668167114, 1.2681316137313843, -1.0092346668243408, -0.45386022329330444, -0.8751298785209656, -0.9167293310165405, 0.388952374458313, 0.6019287109375, -0.5810239911079407, 0.5457990169525146, 0.13020938634872437, -0.10447947680950165, 0.2822709083557129, -1.0872594118118286, 0.33951765298843384, 0.7619186639785767, -0.792512834072113, -0.36479154229164124, 0.162419393658638, 0.6445992588996887, -1.1239655017852783, -0.5871911644935608, -0.12108515202999115, 0.09115488082170486, 0.28709015250205994, 0.6190311312675476, -0.4100305140018463, 0.31796324253082275, 0.687506377696991, -0.45493125915527344, -0.6421636343002319, -0.03489687293767929, -0.9554795026779175, -0.13778018951416016, 0.20045050978660583, 1.2203388214111328, -0.2426706850528717, 0.25776925683021545, 1.1442406177520752, 0.34534257650375366, -0.37838077545166016, -0.7512034773826599, -0.22216980159282684, 0.5065155625343323, -0.751058042049408, 0.7496905326843262, -0.3340380787849426, 0.16779345273971558, 0.137573704123497, 0.4320427477359772, 1.108638048171997, -0.29423168301582336, -0.44186875224113464, 0.32547998428344727, 0.08238586038351059, 0.17027926445007324, -0.43144717812538147, -0.09717659652233124, -1.4302277565002441, 0.21668373048305511, -0.7718423008918762, -0.10824170708656311, -1.2533254623413086, -0.6667335033416748, 0.05155731365084648, 0.23287634551525116, -0.20982292294502258, 0.4866190254688263, -0.736233651638031, -0.6797373294830322, -0.49770110845565796, -0.009935816749930382, 0.32519522309303284, 0.6106049418449402, -0.475632905960083, -0.1835329234600067, 0.07735070586204529, 0.4488571584224701, 0.4067855477333069, 0.43725594878196716, -0.36691853404045105, -1.2309640645980835, -1.6100060939788818, 0.615841269493103, 0.14431020617485046, -0.45790883898735046, -0.35423094034194946, 0.5132456421852112, -0.11808884888887405, -0.0518835075199604, -0.24934038519859314, 0.30923813581466675, -0.8969422578811646, -0.3520393967628479, 0.06422480195760727, -0.98362135887146, 0.26180770993232727, 0.16011430323123932, -0.4234369397163391, -0.2301557958126068, 0.02356928400695324, -0.4844065010547638, -1.0951581001281738, -0.36740952730178833, 0.07057030498981476, -0.7442294359207153, -0.04239056631922722, -0.12065073102712631, -0.25736579298973083, -0.6638491153717041, -0.31555482745170593, -0.1867576241493225, 0.4083472490310669, 0.023624375462532043, 0.8226773142814636, 0.2920325696468353, -0.6030126810073853, -0.14452409744262695, 0.31712809205055237, 0.03904271870851517, -0.22174683213233948, 0.3575490415096283, 0.07426929473876953, -0.3205815255641937, 0.8049914836883545, 0.8872507810592651, 0.3906797766685486, -1.3418556451797485, -0.04848490282893181, 0.2338419258594513, -0.4299999177455902, -0.2239736020565033, 1.1431732177734375, -0.5236217975616455, -1.3192261457443237, 0.27467095851898193, -1.633323311805725, -0.5242496728897095, -0.5579257011413574, 1.0761431455612183, 0.32904091477394104, 0.047372110188007355, -0.15338146686553955, -0.2058853805065155, 0.08673407137393951, 0.18569275736808777, -0.9357611536979675, 0.2836758494377136, -0.732540488243103, -0.3184325098991394, 0.2864361107349396, 0.20204512774944305, -0.12386435270309448, -0.7792339324951172, -0.3367103636264801, 0.1343337595462799, -0.17377370595932007, 0.3498707711696625, -0.7455991506576538, 0.17524930834770203, 0.5619096159934998, 0.4189175069332123, 0.1434425264596939, 0.12261202186346054, -0.17237485945224762, 0.42091473937034607, 0.7644440531730652, 0.21555766463279724, -0.8694126605987549, -1.5306485891342163, 1.2273945808410645, 1.541982889175415, -1.0117740631103516, 0.44632256031036377, 0.0358099564909935, -0.5734580755233765, 0.7661339044570923, 0.23821395635604858, 0.6197047829627991, 0.8275848031044006, -0.032013900578022, 0.4410926401615143, 0.5693854689598083, -1.0320053100585938, 0.34173253178596497, 0.5525502562522888, 1.0025156736373901, 1.1316297054290771, 0.3106342852115631, -0.45425155758857727, 1.0887130498886108, 0.09731419384479523, 0.4945162236690521, 0.7023704051971436, 0.8871663808822632, -0.31841060519218445, -0.5288353562355042, -0.32144051790237427, 0.5971070528030396, -0.7528693675994873, -1.013802170753479, -0.3414400815963745, 0.7348865866661072, 0.44843244552612305, 1.106813669204712, 0.2935440242290497, 0.3335302174091339, 0.2462940663099289, 0.5755560994148254, 0.30976346135139465, -0.6151127219200134, -0.35095590353012085, 0.040079765021800995, -0.2673082947731018, 0.17607025802135468, 0.2366996556520462, -0.5627781748771667, -0.18312831223011017, -0.009957458823919296, -0.14845390617847443, 0.23622450232505798, 0.5731837749481201, 0.8804910182952881, 0.6915670037269592, -0.16416332125663757, -0.6116801500320435, -0.13469985127449036, -0.5477766394615173, -1.4838945865631104, 0.04961865395307541, -0.7739859223365784, -0.46080631017684937, 0.43698737025260925, -0.13259990513324738, -0.08218805491924286]}, "authors": [{"authorId": "2279546596", "name": "Saurav Pawar"}, {"authorId": "2146190509", "name": "S. Tonmoy"}, {"authorId": "2277446934", "name": "S. M. M. Zaman"}, {"authorId": "2212131028", "name": "Vinija Jain"}, {"authorId": "40016108", "name": "Aman Chadha"}, {"authorId": "2258322706", "name": "Amitava Das"}], "references": [{"paperId": "2c0312c604f9f7638bb4533b39e0ae81e7f6ab12", "title": "The Falcon Series of Open Language Models"}, {"paperId": "908dad62c0e43d80e3e3cb3c0402f7c71c70499c", "title": "MemGPT: Towards LLMs as Operating Systems"}, {"paperId": "4c0428917aeee6aa7bd434f337d039f35996b736", "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"}, {"paperId": "368fb35a07076eba01c2e4700499323cd4524513", "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning"}, {"paperId": "c4d3b9a87295db0b9f6466b7f3ce2f175c6d3157", "title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "73290ecbec2f38d1d647ddef1ada69cee41725b3", "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "2dfb9171e180dcb0af23d305e024d43d311708ab", "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "eb511ae6b9f04e4936891d26787f274b48b99d57", "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding"}, {"paperId": "d9964ab436eefd21f923a4bc833c6b66692c7f00", "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text"}, {"paperId": "c8dd0fe00f9a71ea68b6856b36590b8daa316139", "title": "A Frustratingly Easy Improvement for Position Embeddings via Random Padding"}, {"paperId": "dbc368bc8b49347dd27679894524fa62f88492c9", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}, {"paperId": "f711aae062ae30c0888910b2bdcc5be6c1d1c340", "title": "Enhancing Large Language Model with Self-Controlled Memory Framework"}, {"paperId": "0a5af6c39fe47901e8a69ec538d6ebb95a30a23a", "title": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "5278a8eb2ba2429d4029745caf4e661080073c81", "title": "Generative Agents: Interactive Simulacra of Human Behavior"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "68adb03744692247fb834406798894db9fe77010", "title": "A Survey on Long Text Modeling with Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e", "title": "Full Stack Optimization of Transformer Inference: a Survey"}, {"paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e", "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context"}, {"paperId": "465471bb5bf1a945549d6291c2d23367966b4957", "title": "In-Context Retrieval-Augmented Language Models"}, {"paperId": "3d5922d71a370f32b7f232a596def914f67eebd1", "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering"}, {"paperId": "87126a964ed14d0d2207747fc732b197e2fc9493", "title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "b8ad6ba5a367dc44aeebc85baded2af54d28255d", "title": "ContraCLM: Contrastive Learning For Causal Language Model"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1", "title": "Training Language Models with Memory Augmentation"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "361a4637d7ce1f51ef45b1b72e9701d5309042b5", "title": "AdapLeR: Speeding up Inference by Adaptive Length Reduction"}, {"paperId": "081edae651e709e448bdd8a1f1b5760c7c7e1f53", "title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "2d82ee05b132d4681c3bd517afc17d608fe6e525", "title": "Simple Local Attentions Remain Competitive for Long-Context Tasks"}, {"paperId": "590432f953b6ce1b4b36bf66a2ac65eeee567515", "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction"}, {"paperId": "231e768f0cd280faa0f725bb353262cb4fed08d1", "title": "Hierarchical Transformers Are More Efficient Language Models"}, {"paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156", "title": "Recursively Summarizing Books with Human Feedback"}, {"paperId": "64522a5b3476e9f201f6a5b3e312ef0005c562f1", "title": "SHAPE: Shifted Absolute Position Embedding for Transformers"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "88064de690af282dbdf222774f03ff070b9df22b", "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f", "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"}, {"paperId": "d8e7bad2681ce70277c900c77a22181d4b03d705", "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "46c585ee9abf76779ea4b863d2da4358efd0d1d3", "title": "Adaptive Semiparametric Language Models"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "8b28d9e3ca408b8a41d32f8bd4da7fbbd4f12a4b", "title": "Towards Zero-Shot Knowledge Distillation for Natural Language Processing"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "67ee20536c30a225b86902af2f091e28e5e19b40", "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling"}, {"paperId": "a50d31c082521817a1e74cae584963a63163ca70", "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "84476fdf6ead3553f4493dff8e02308439d6222b", "title": "Improve Transformer Models with Better Relative Position Embeddings"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "030d7d7ae48a9f81700b2c1f7cf835235777b8e7", "title": "Relevance-guided Supervision for OpenQA with ColBERT"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "4287533d12143cdbc4948b60ecece28b6c750f17", "title": "Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks"}, {"paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d", "title": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "b70e7049d1e0a6cf4e98ed0addd3faa7b484689e", "title": "Dialogue Intent Classification with Long Short-Term Memory Networks"}, {"paperId": "9ef902f3c427d697f3579cd79844b44de99bc93c", "title": "UdL at SemEval-2017 Task 1: Semantic Textual Similarity Estimation of English Sentence Pairs Using Regression Model over Pairwise Features"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "29092f0deaac3898e43b3f094bf15d82b6a99afd", "title": "Learning to Remember Rare Events"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "c214c715b90a0fff8d8c5fd2115a9234a9bdd49e", "title": "Polysemy: Current perspectives and approaches"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "8264257f573696fc0a1ef7531c825041832197f8", "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"}, {"paperId": "147af99d852c516e16d90b128504a43c82ceffb8", "title": "\"Low-Resource\" Text Classification: A Parameter-Free Classification Method with Compressors"}, {"paperId": null, "title": ". Dynamically scaled rope further increases performance of long context llama with zero fine-tuning"}, {"paperId": null, "title": ". Introducing mpt-30b: Raising the bar for open-source foundation models"}, {"paperId": null, "title": "Gptq: Accurate post-training compression for generative pretrained transformers"}, {"paperId": null, "title": "Wikipedia corpus foundation"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "8ff46c88964a36985f2b45933a3d47b81bd87bd0", "title": "Quora Question Pairs"}, {"paperId": null, "title": "Movies: Towards story-like visual explanations by watching movies and reading books\u2014yukun zhu"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": null, "title": "2023. Focused transformer: Con-trastive training for context scaling"}, {"paperId": null, "title": "bloc97. 2023b. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "2023b. Randomized positional encodings boost length generalization of trans-formers"}, {"paperId": null, "title": "2022. A length-extrapolatable transformer"}, {"paperId": null, "title": "bloc97. 2023a. Add ntk-aware interpolation \"by parts\" correction"}, {"paperId": null, "title": "2023. Longqlora: Efficient and effective method to extend context length of large language models"}, {"paperId": null, "title": "2022. Proof-pile"}, {"paperId": null, "title": "2023. Memorybank: Enhancing large language models with long-term memory"}, {"paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0", "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"}, {"paperId": null, "title": "2023b. Qwen-vl"}, {"paperId": null, "title": "2022. Accelerating trans-former networks through recomposing softmax layers"}, {"paperId": null, "title": "2022. Induced natural language rationales and interleaved markup tokens enable extrapolation in large language models"}, {"paperId": null, "title": "2023. Things i\u2019m learning while training superhot"}, {"paperId": null, "title": "2023a. Longnet: Scaling transformers to 1,000,000,000 tokens 2023"}, {"paperId": null, "title": "2022. Improving language models by retrieving from tril-lions of tokens"}, {"paperId": null, "title": "2023a. Retentive network: A successor to trans-former for large language models"}, {"paperId": null, "title": "2022c. Task-oriented dialogue system as natural language generation"}, {"paperId": null, "title": "Together Computer"}, {"paperId": null, "title": "Bartosz Piotrowski Zhangir Azerbayev, Edward Ayers"}, {"paperId": null, "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2023a. Lm-infinite: Simple on-the-fly length generalization for large language models"}, {"paperId": null, "title": "2023a. Qwen technical report"}, {"paperId": null, "title": "2022a. Foundation transformers"}]}