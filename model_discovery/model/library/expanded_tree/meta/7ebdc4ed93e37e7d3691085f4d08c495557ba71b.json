{"paperId": "7ebdc4ed93e37e7d3691085f4d08c495557ba71b", "abstract": "Multimodal learning has been a field of increasing interest, aiming to combine various modalities in a single joint representation. Especially in the area of visiolinguistic (VL) learning multiple models and techniques have been developed, targeting a variety of tasks that involve images and text. VL models have reached unprecedented performances by extending the idea of Transformers, so that both modalities can learn from each other. Massive pre-training procedures enable VL models to acquire a certain level of real-world understanding, although many gaps can be identified: the limited comprehension of commonsense, factual, temporal and other everyday knowledge aspects questions the extendability of VL tasks. Knowledge graphs and other knowledge sources can fill those gaps by explicitly providing missing information, unlocking novel capabilities of VL models. In the same time, knowledge graphs enhance explainability, fairness and validity of decision making, issues of outermost importance for such complex implementations. The current survey aims to unify the fields of VL representation learning and knowledge graphs, and provides a taxonomy and analysis of knowledge-enhanced VL models.", "venue": "arXiv.org", "year": 2022, "citationCount": 6, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2211.12328", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "The current survey aims to unify the fields of VL representation learning and knowledge graphs, and provides a taxonomy and analysis of knowledge-enhanced VL models."}, "embedding": {"model": "specter_v2", "vector": [0.1212756484746933, 0.5961934328079224, -0.2633471190929413, 0.09635011106729507, -0.30014997720718384, -0.15279197692871094, 0.4468390643596649, 0.2452189326286316, -0.4818662703037262, 0.004949578549712896, 0.24176016449928284, -0.09491221606731415, -0.17817185819149017, -0.16340124607086182, -0.2741386294364929, 0.018603798002004623, -0.9853243827819824, 0.32229262590408325, 0.12660472095012665, -0.34108206629753113, -0.07136831432580948, -0.4279118478298187, -1.3099994659423828, 0.6503887176513672, 0.2529662847518921, 0.32802560925483704, 0.5814946293830872, 1.0871700048446655, -0.3836587965488434, 0.9420334696769714, 0.49022287130355835, -0.5732278823852539, -0.06858981400728226, 0.22637392580509186, -0.6463470458984375, -0.04007091745734215, 0.6791447997093201, -0.7033141255378723, -1.1469662189483643, 0.36413896083831787, -0.2053612470626831, 0.36895737051963806, 0.5941162109375, -1.0136747360229492, -0.38529324531555176, 0.9046961069107056, 0.8585909605026245, 0.6041027307510376, 0.28597554564476013, -0.6690885424613953, 1.589604377746582, -1.1566649675369263, 0.6230326294898987, 1.4710392951965332, 0.35040298104286194, 0.6011090278625488, -0.1823464334011078, -0.3950749337673187, 0.7132816314697266, 0.6827966570854187, -0.4438796639442444, -0.028788002207875252, -0.12449582666158676, -0.2422785758972168, 1.587571620941162, -0.507597029209137, 0.06796511262655258, 0.6162721514701843, -0.2765164077281952, 1.8952150344848633, 0.2364148050546646, -1.1687654256820679, -0.3989376723766327, 0.27698326110839844, 0.4489017724990845, 1.2698862552642822, -0.5931525230407715, 0.5430046916007996, -1.4605145454406738, 0.1384444385766983, 0.8801055550575256, -0.2670750617980957, -0.7783197164535522, -0.5666457414627075, -0.8503419160842896, 0.810544490814209, 0.5615609288215637, 0.547339677810669, -0.26759105920791626, 0.2915297746658325, 0.2889288663864136, 0.9816856980323792, -0.670647382736206, 0.28600895404815674, -0.1306399405002594, 0.6164253354072571, -0.530192494392395, 0.18171802163124084, 0.048551131039857864, 0.6375292539596558, 0.2922709882259369, 0.24112500250339508, -0.6546565890312195, 0.222056046128273, 1.639646053314209, -0.27803295850753784, 0.42608487606048584, -0.6497014164924622, 0.6498324275016785, -0.3831036388874054, 0.27183204889297485, -0.6908087134361267, -0.34754055738449097, -0.16681469976902008, -0.03924207016825676, -0.9474605917930603, 0.1966370940208435, 0.13433827459812164, -0.8551953434944153, 0.6437484622001648, -0.4698796272277832, -0.07896396517753601, 0.45712730288505554, 0.602272629737854, 0.9362682700157166, 0.4392150044441223, 0.5391583442687988, 0.10351065546274185, 0.7670480012893677, -0.32513323426246643, -0.7260405421257019, -0.8466953635215759, 0.7597595453262329, 0.2860487401485443, 0.22638697922229767, -0.20969103276729584, -1.0626667737960815, -1.033170461654663, -0.9647562503814697, -0.02025798335671425, -0.7444809079170227, 0.2611367404460907, 1.328986406326294, 0.41516149044036865, -1.199578881263733, 0.7471733689308167, 0.09629466384649277, -0.4275997579097748, 0.1741172969341278, 0.4465058147907257, -0.05257200077176094, -0.7087894678115845, -1.377199411392212, 0.5819328427314758, 0.7335034608840942, -0.43550416827201843, -0.771988570690155, 0.031571678817272186, -1.767367959022522, -0.2997415363788605, 0.39117667078971863, -0.8398212790489197, 0.9992015957832336, -0.27235206961631775, -1.0283045768737793, 0.6805135011672974, -0.3287869691848755, -0.061747919768095016, 0.540575385093689, -0.056681908667087555, -0.7868216037750244, 0.18491768836975098, -0.28955912590026855, 0.7554823756217957, 0.26326876878738403, -0.9752610325813293, -0.36952924728393555, -0.007031087763607502, 0.18236662447452545, 0.2155432403087616, -0.10888420045375824, 0.5733522176742554, -0.1176106408238411, -0.23277434706687927, 0.4734385311603546, 0.9875931143760681, 0.22200269997119904, -0.08507759124040604, -0.10450799018144608, -0.7426428198814392, 0.5814879536628723, -0.18795989453792572, 0.8442557454109192, -0.8169271945953369, -0.2563585340976715, 0.023614276200532913, 0.1001858189702034, -0.4034762680530548, -1.2547223567962646, 0.8162313103675842, -0.2658737003803253, -0.03718462586402893, -0.2763233482837677, -0.8529372811317444, 0.06808637827634811, 0.06436619162559509, -0.42817988991737366, -0.36810269951820374, -0.11890766024589539, 1.261299729347229, -0.9625160098075867, -0.24911613762378693, 0.5796337127685547, 0.044724103063344955, -0.3019729256629944, 1.1674407720565796, -0.5399805307388306, -0.034421298652887344, 0.08779346197843552, -0.2148052304983139, 0.06741254031658173, -0.13878963887691498, 0.5374090671539307, -0.5359745025634766, 0.23436452448368073, -0.02530461736023426, -0.9378673434257507, 1.7679051160812378, -0.22777299582958221, 1.155290961265564, -0.5356411337852478, -0.4087059199810028, -0.05815156549215317, 0.8454645872116089, -0.4736849069595337, -0.3901345133781433, 0.34264516830444336, -0.006791839376091957, -0.4159660339355469, -0.2688433825969696, 0.48011884093284607, 0.6217399835586548, 0.1887855976819992, 0.2089432179927826, 0.8496147990226746, -0.27261975407600403, 0.5197820067405701, 0.5794220566749573, 0.2939903140068054, 0.48449718952178955, 0.2887049615383148, 0.3899061381816864, 0.332935631275177, -0.6832449436187744, -0.5869206190109253, 0.5265370011329651, 0.8711507320404053, 0.5078659057617188, 0.08726467937231064, -0.4427850842475891, 0.31758126616477966, -0.305599570274353, 0.7595629692077637, 1.715918779373169, 0.18519264459609985, -0.45938417315483093, -0.5722277760505676, -0.44948187470436096, -0.35727083683013916, 0.6829190850257874, -0.735686719417572, -0.10313046723604202, -0.2163582742214203, -0.5752615332603455, 0.4663177728652954, 0.9154465794563293, 1.1986095905303955, -0.954594612121582, -0.5342391133308411, -0.29797276854515076, 0.16696126759052277, -0.4753188192844391, -0.19993631541728973, 0.11339996755123138, -0.8210965394973755, -0.8746240735054016, -0.07116128504276276, -0.5126524567604065, 0.32556647062301636, -0.4597759544849396, 0.8204662799835205, -0.38738754391670227, 0.044521138072013855, 0.7983867526054382, 0.6189455389976501, -0.6231186985969543, -0.47837764024734497, -0.4607684016227722, -0.5061760544776917, -0.11647835373878479, 0.12333378195762634, 0.7880426049232483, 0.39041557908058167, 0.2706480920314789, -0.524208664894104, 0.31098490953445435, 0.38931506872177124, 0.12026815116405487, 0.6632605195045471, -0.17580367624759674, 0.3467307984828949, -0.9626359343528748, 0.7927301526069641, 0.27779287099838257, -0.3653406798839569, 0.518481969833374, -0.33854833245277405, -0.4834875464439392, -0.03263235092163086, -0.6174501180648804, -0.648693323135376, -0.5382964015007019, 0.7325364351272583, -0.06274586170911789, -0.6597077250480652, 0.9345589876174927, 0.06550206989049911, -0.4669803977012634, 0.36752980947494507, 0.6255802512168884, 0.36702045798301697, 0.23005789518356323, 0.937890350818634, -0.9045151472091675, 0.4002947509288788, 0.2684633135795593, 0.17761652171611786, -0.38879892230033875, -0.11427042633295059, -0.6680031418800354, -0.3978486955165863, -0.21583767235279083, -0.5125957727432251, -0.6504408121109009, 0.05689288675785065, -0.47706690430641174, -1.085375428199768, -0.021641617640852928, -0.8143102526664734, -0.5983696579933167, 0.18465952575206757, 0.15657350420951843, -0.5076144933700562, -0.79694002866745, -0.8408793210983276, -0.49725234508514404, 0.3240349590778351, -0.7416964173316956, -0.27664974331855774, 0.08064120262861252, -0.36275458335876465, -0.8810803890228271, -0.1330772340297699, -0.0226795244961977, 0.441203773021698, -0.2780391573905945, 1.3724597692489624, 0.08555005490779877, -0.39866018295288086, -0.08444525301456451, 0.11746694147586823, 0.3825214207172394, -0.18909677863121033, -0.26321858167648315, -0.986162006855011, 0.2223253697156906, -0.04057800769805908, -0.7730901837348938, 0.3990028500556946, 0.35658934712409973, 0.6199977397918701, 0.3592633306980133, -0.38441693782806396, -0.16078530251979828, 1.502646565437317, -0.30964305996894836, 0.0032156838569790125, 0.2909095287322998, 1.139092206954956, 0.9039214849472046, -0.2501235902309418, 0.17713412642478943, 0.8449553847312927, 0.20305515825748444, 0.3499755859375, 0.07221848517656326, -0.41444945335388184, -0.4658886194229126, 0.4814751148223877, 0.9437942504882812, -0.3673611581325531, -0.13576878607273102, -0.9881739616394043, 0.6724113821983337, -1.3846741914749146, -0.7294520139694214, 0.5563161969184875, 0.874329149723053, 0.24654485285282135, -0.6345179677009583, -0.16484855115413666, 0.2269280105829239, 0.5854110717773438, 0.24134711921215057, -0.261197954416275, 0.11017496883869171, -0.25577980279922485, 0.3842674493789673, -0.20260103046894073, 0.9656276106834412, -0.36371004581451416, 0.34493210911750793, 14.504677772521973, 0.5526822805404663, 0.46935591101646423, 0.21928630769252777, 0.012181892991065979, 0.7138332724571228, -0.5226143598556519, -0.23774558305740356, -0.6011810898780823, -0.7945550084114075, 0.7014626860618591, 0.24338513612747192, 0.5038977265357971, -0.1016741469502449, -0.43889594078063965, -0.3515942096710205, -0.8503891825675964, 0.8926355242729187, 0.7799769043922424, -1.2237964868545532, 0.942668080329895, 0.05500777065753937, 0.011373625136911869, -0.02433059737086296, 0.8487504720687866, 0.8916731476783752, 0.10206551849842072, -0.7413153052330017, 0.5888221859931946, 0.28276968002319336, 0.7622119784355164, -0.4999808669090271, 0.5083310008049011, 0.628006100654602, -1.2208411693572998, -0.4036840498447418, -0.44471073150634766, -0.7813839316368103, 0.03417034447193146, -0.8292114734649658, -0.4220541715621948, -0.01049235463142395, -0.6671173572540283, 0.8371451497077942, -0.23929376900196075, 0.46689385175704956, -0.7408981919288635, 0.30976149439811707, 0.45728954672813416, 0.06966428458690643, -0.029077934101223946, 0.7460178732872009, 0.15602977573871613, -0.17870034277439117, 0.1370871365070343, 0.10060253739356995, 0.061959024518728256, 0.12328042089939117, -0.38020187616348267, 0.2165655940771103, -0.6744994521141052, -0.1599123477935791, -0.19145908951759338, 0.29741647839546204, 0.4414438307285309, 0.5828685760498047, -0.5249347686767578, 0.10408605635166168, 0.5487316250801086, 0.3630073070526123, -0.014894927851855755, -0.2629217505455017, -0.1777127981185913, -0.43077313899993896, 0.17854247987270355, 0.4664321541786194, -0.10849577933549881, -0.49076250195503235, -0.9180331230163574, -0.19229388236999512, 1.0763628482818604, -1.1931692361831665, -0.9146318435668945, 0.9336327314376831, -0.10363706201314926, -0.5254797339439392, 0.2738443613052368, -1.2661916017532349, -0.14725014567375183, 0.13601277768611908, -1.3502675294876099, -1.090758204460144, -0.4480045735836029, 0.6257330775260925, -0.3160340487957001, -0.45281708240509033, 1.48409104347229, -0.4129638373851776, -0.09232606738805771, -0.08152614533901215, -0.4377084970474243, -0.17173057794570923, -0.18752461671829224, -1.0301436185836792, -0.08107344061136246, -0.02345598116517067, 0.4111443758010864, 0.05789348855614662, -0.3246217966079712, -0.16004034876823425, -0.683530330657959, 0.24876803159713745, 0.5255600810050964, -0.8964831829071045, -0.5108311176300049, -0.4692643880844116, -1.2171804904937744, 0.6651960611343384, 0.7530363202095032, -0.6547770500183105, 0.5701426267623901, -0.061484433710575104, -0.4447168707847595, 0.15043775737285614, -1.0064187049865723, 0.3523900806903839, 0.5166975259780884, -0.6896201968193054, -0.9902070760726929, -0.002696022391319275, 0.0489601194858551, -0.6455006003379822, -0.32538673281669617, -0.42675158381462097, -0.007583836559206247, -0.21618996560573578, 0.9084916114807129, -0.6978975534439087, 0.1793636679649353, 0.49248814582824707, -0.3024131953716278, -1.0486897230148315, 2.091788155667018e-05, -0.6811137795448303, -0.25520265102386475, -0.14783062040805817, 0.8631454706192017, -0.45251429080963135, 0.11577325314283371, 0.5779454708099365, 0.26425737142562866, 0.00422653229907155, -0.35244253277778625, 0.05682815611362457, -0.3554524779319763, -0.7067606449127197, -0.036930572241544724, 0.02471127361059189, -0.23715342581272125, 0.11740053445100784, 0.6498619318008423, 0.9538664221763611, -0.04720091447234154, -0.552454948425293, 0.35218849778175354, -0.39720869064331055, -0.010792574845254421, -0.4730391800403595, -0.4184420704841614, -1.1311827898025513, 0.3376358449459076, -1.5636967420578003, -0.06250899285078049, -1.8167197704315186, -0.6250974535942078, 0.7556747794151306, -0.348407119512558, 0.3689793348312378, 0.39656656980514526, -0.6516714096069336, -0.16453932225704193, -0.6576129198074341, -0.442476361989975, 0.5497546792030334, 1.2369521856307983, -0.5148597359657288, 0.05053214728832245, -0.1837390512228012, 0.040979914367198944, 0.2687973380088806, 0.505258321762085, -0.39618387818336487, -1.384286880493164, -1.1882681846618652, 0.3054938316345215, 0.23423422873020172, 0.1828598976135254, -0.7518325448036194, 0.8751819729804993, 0.4683379828929901, 0.19686153531074524, -0.23159652948379517, 0.73061603307724, -0.9960047006607056, -0.4804878532886505, 0.24356858432292938, -0.8304573893547058, -0.022048350423574448, 0.6762059330940247, -0.19877557456493378, -0.6339906454086304, 0.38444972038269043, -0.12432191520929337, -0.9492563009262085, -1.3814700841903687, 0.2351299524307251, -0.765265941619873, -0.11005750298500061, 0.32562771439552307, -0.5792299509048462, -1.1632885932922363, -0.3717372715473175, -0.5997120141983032, 0.5711552500724792, -0.6109793186187744, 0.48499730229377747, 1.2785520553588867, -1.046404242515564, -0.13560372591018677, 0.2523755133152008, 0.3500405251979828, -0.1445615440607071, 0.8060096502304077, 0.34085896611213684, -0.12969814240932465, 0.2055603563785553, -0.3513597548007965, 0.5976195335388184, -0.9435759782791138, -0.2402218133211136, 0.8907785415649414, -0.023707086220383644, -0.07266126573085785, 1.0204226970672607, -0.2515958249568939, -0.9737970232963562, 0.6072371602058411, -0.577634334564209, -1.1974477767944336, 0.1378510743379593, 0.7645828723907471, 0.3230656683444977, -0.5421350598335266, -0.15625561773777008, -0.10090423375368118, 0.6976149082183838, -0.14246900379657745, -0.7070848345756531, 0.47292423248291016, -0.2111770659685135, -0.08692051470279694, 0.8862765431404114, 0.3471890985965729, -0.8390445709228516, -0.9456971287727356, -0.4897010326385498, -0.23443901538848877, -0.34844309091567993, 0.14173641800880432, -0.9665859937667847, -0.5969858169555664, 0.9102013111114502, 0.4726431369781494, 0.20224159955978394, 0.01766805164515972, 0.17091740667819977, -0.25835278630256653, 0.9884005188941956, -0.036870524287223816, -0.4377533793449402, -0.0502474345266819, 0.8873468041419983, 1.642973780632019, -1.0351266860961914, 0.06759674847126007, -0.40250980854034424, -1.0161031484603882, 1.5296275615692139, 0.43047165870666504, 0.21058514714241028, 0.5184360146522522, -0.8347422480583191, 0.2966160476207733, -0.06721258163452148, -0.631700873374939, -0.4626447856426239, 1.1412988901138306, 1.7926180362701416, 0.4985538125038147, 0.26124343276023865, 0.5928202867507935, 0.8216353058815002, 0.024358920753002167, 0.3996010720729828, 0.6754510998725891, 0.5960056781768799, -0.415852427482605, 0.18469670414924622, 0.3758644759654999, 0.4254337549209595, 0.04934580996632576, -0.3933182954788208, -0.03782346844673157, 1.176162600517273, 0.14584079384803772, 0.8542719483375549, 0.28483128547668457, 0.23634980618953705, 0.7733201384544373, 0.35180187225341797, 1.1958816051483154, -0.6688982844352722, -0.12198393046855927, -0.5299255847930908, -0.2999873757362366, -0.0557585172355175, -1.0242269039154053, -0.33929765224456787, -0.7537190914154053, 0.11696017533540726, 0.46412017941474915, -0.2385113686323166, 0.24615557491779327, 1.164186954498291, 0.20908516645431519, 0.33723026514053345, -0.934106707572937, 0.01226520910859108, -0.005475230515003204, -0.944103479385376, 0.031145840883255005, -0.7397192120552063, -0.1554347425699234, -0.8359700441360474, -0.39254030585289, -0.11000826209783554]}, "authors": [{"authorId": "2184294391", "name": "Maria Lymperaiou"}, {"authorId": "1719165", "name": "G. Stamou"}], "references": [{"paperId": "04302343c8944e2c82226a306bc17e492ab6ce38", "title": "An Impartial Transformer for Story Visualization"}, {"paperId": "9f7914c2577626868e6a69f2c9605b38f941055b", "title": "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation"}, {"paperId": "5b19bf6c3f4b25cac96362c98b930cf4b37f6744", "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"}, {"paperId": "c57293882b2561e1ba03017902df9fc2f289dea2", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"}, {"paperId": "d2938c9edc63d3b19a16861c5c75a96d638714e7", "title": "Reasoning with Multi-Structure Commonsense Knowledge in Visual Dialog"}, {"paperId": "04248a087a834af24bfe001c9fc9ea28dab63c26", "title": "A Survey of Vision-Language Pre-Trained Models"}, {"paperId": "6296aa7cab06eaf058f7291040b320b5a83c0091", "title": "Generative Adversarial Networks"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "43250eafc113dd90fd24d1371b43e2fe1e53b868", "title": "Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection"}, {"paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02", "title": "FLAVA: A Foundational Language And Vision Alignment Model"}, {"paperId": "98598d0c472694f3bdaf84997400c98d9ec7ffd6", "title": "FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization"}, {"paperId": "21ec90872abd986c12afe39bebe807732ffa70c9", "title": "Florence: A New Foundation Model for Computer Vision"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "ee1d0e141530190a2b0d834737096c5bfda6b304", "title": "Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization"}, {"paperId": "4e0709e83f62f7011ec98dd9182aa3cde81aa66d", "title": "Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering"}, {"paperId": "7af90c66039e5a4a93501090f12b3a7324f83c91", "title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation"}, {"paperId": "2672777d25562c9df6fc13b653181db62d39bece", "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"}, {"paperId": "57baa478fd06ed18f3d271fefe9de5e833f47961", "title": "KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs"}, {"paperId": "4e92fec0a61972ae076707d0630d1333affccdfc", "title": "Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "d0b59b3e34a79c8c79a31bf3944ded8ab7a803ae", "title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration"}, {"paperId": "8cee6b1e61b71b6af6086e442a6620c6bb8534bc", "title": "Knowledge is Power: Hierarchical-Knowledge Embedded Meta-Learning for Visual Reasoning in Artistic Domains"}, {"paperId": "7c5e6720fa4c3cd73fd915bc71dea5e78184b262", "title": "Boosting Entity-Aware Image Captioning With Multi-Modal Knowledge Graph"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "1c83f3f9789df43bf937ae2618721e2da83dcc06", "title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning"}, {"paperId": "9cdd420edb0c56eb4c003903fffb23b104454c42", "title": "Zero-shot Visual Question Answering using Knowledge Graph"}, {"paperId": "11f606fd65df3de5d99c0034d8dd4ec5205090f1", "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training"}, {"paperId": "f46f77630b35a43e8c247916da5d809d6e5b4210", "title": "Interpretable visual reasoning: A survey"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb", "title": "ERNIE-NLI: Analyzing the Impact of Domain-Specific External Knowledge on Enhanced Representations for NLI"}, {"paperId": "87a0279c1d640b486dc5c9f1e0d3705ed87754cf", "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency"}, {"paperId": "e560bdec8479c1ca4570a4233ec66bf759a0c6ad", "title": "Imagine, Reason and Write: Visual Storytelling with Graph Knowledge and Relational Reasoning"}, {"paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794", "title": "Diffusion Models Beat GANs on Image Synthesis"}, {"paperId": "660899e577a4e0c257582ff4bbdd8acbb7e81210", "title": "Vision\u2013Language\u2013Knowledge Co-Embedding for Visual Commonsense Reasoning"}, {"paperId": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"}, {"paperId": "2fa4938001b18f464c62aa38a5a469bb92569d57", "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning"}, {"paperId": "8dce342a435034fa0521b24b61393397df95c095", "title": "Multi-Modal Answer Validation for Knowledge-Based VQA"}, {"paperId": "cf8dd25f61803ff658d2572d2f5f9ba8c8d46fa1", "title": "Context-Aware Layout to Image Generation with Enhanced Object Appearance"}, {"paperId": "0b6f13177a90a02d44a41c62659988561f56c168", "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training"}, {"paperId": "b76054d7820249fd4ebd31ed8db949a8ad8b6cbc", "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering"}, {"paperId": "87a5e46d9fa5f8546bfdb5d603e0ad0bdcd8adbc", "title": "Multi-Level Knowledge Injecting for Visual Commonsense Reasoning"}, {"paperId": "8c78a46373241dde9daea12433fd5e0ee23d1b86", "title": "Image captioning with transformer and knowledge graph"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "ed4449f065fa225dd7c08929c64e6356ff1264f6", "title": "OntoZSL: Ontology-enhanced Zero-shot Learning"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "4cfe344e88d2ae5dbd18507025527bd9a27991ba", "title": "Commonsense Knowledge Aware Concept Selection For Diverse and Informative Visual Storytelling"}, {"paperId": "cb596bffc5c5042c254058b62317a57fa156fea4", "title": "Unifying Vision-and-Language Tasks via Text Generation"}, {"paperId": "48847adb786cb8a193818aca8519a887680c2d83", "title": "Reasoning over Vision and Language: Exploring the Benefits of Supplemental Knowledge"}, {"paperId": "9f620ad41b4e506e777c0665681b839c89cd682a", "title": "Dimensions of Commonsense Knowledge"}, {"paperId": "fe6e9bc5040a69e310d88677a1045a2fef640f48", "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation"}, {"paperId": "25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b", "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning"}, {"paperId": "1a9015e511ec3da873f6114eeb542905a92d7d62", "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "7912b8bb86a6d32ed355651d05ff0cbf37e9504e", "title": "KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning"}, {"paperId": "019227992f2e3d0a93332e4107765d8efd2976be", "title": "Towards Knowledge-Augmented Visual Question Answering"}, {"paperId": "45376c0dbf03efa4184ee3c2639c526cecd9f913", "title": "A Decade Survey of Content Based Image Retrieval Using Deep Learning"}, {"paperId": "9958887e8dd5f84595818c50fb734b566996541a", "title": "ConceptBert: Concept-Aware Representation for Visual Question Answering"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "32a9b74561ee7c7b2d9663248b515168676d9321", "title": "Improved-StoryGAN for sequential images visualization"}, {"paperId": "b9caa821f6996c7021e54a6e71a7d8f2a01aedbe", "title": "Image Captioning with Internal and External Knowledge"}, {"paperId": "c9940a17504a3b83bd1e9d613b095ddb204d2ad0", "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs"}, {"paperId": "f8a22859230e0ccafefc020dccc66b5a646fe0ac", "title": "COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs"}, {"paperId": "0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22", "title": "Boosting Visual Question Answering with Context-aware Knowledge Aggregation"}, {"paperId": "c845494445f3bfa01d8245a4759b144e27aa3788", "title": "A Survey of Knowledge-enhanced Text Generation"}, {"paperId": "eedf2748a9a1ba2779cde95fd8bad9c2260d5317", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention"}, {"paperId": "7eda139d737eea10fc1d95364327a41ec0cee4a4", "title": "CoLAKE: Contextualized Language and Knowledge Embedding"}, {"paperId": "e5fb7a72807af36a7d39049346b3feb422a50c3c", "title": "X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers"}, {"paperId": "fb106fdc2d02b077c100bd0a653395bd6cffcded", "title": "A Dataset and Baselines for Visual Question Answering on Art"}, {"paperId": "e526624783b3b5687da54b8cd4a7190a26a0b5e8", "title": "Cross-modal Knowledge Reasoning for Knowledge-based Visual Question Answering"}, {"paperId": "0fbe9dd4a8ab1a68739fa321be33d5ab8d611f4e", "title": "Injecting Prior Knowledge into Image Caption Generation"}, {"paperId": "bc996a4dbf9d4234eacdd0b930a94de1d158e256", "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"}, {"paperId": "7323239fc698f3aa176b5dd39276326c945556a5", "title": "Zero-Shot Learning with Common Sense Knowledge Graphs"}, {"paperId": "6b13065b4050800e30bb74e010b8aaba3355525d", "title": "Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering"}, {"paperId": "a7d483134515bc4cb64bbd19259e80b6ab94f0d0", "title": "Do Dogs have Whiskers? A New Knowledge Base of hasPart Relations"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3b0d38302115af1165639d6fb34c4c19187b2a6f", "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models"}, {"paperId": "7b12198a77972508689a904c544010b2a406ed20", "title": "YAGO 4: A Reason-able Knowledge Base"}, {"paperId": "f0d74a6a345cde53df29ec39d5aa9db294c660fd", "title": "VisualCOMET: Reasoning About the Dynamic Context of a Still Image"}, {"paperId": "5290d7921f0266c8b50b79fc8a0b7d22868f4f60", "title": "The Cost of Training NLP Models: A Concise Overview"}, {"paperId": "43d77f0547f4a1bf4faf29e5e7548564b70e758a", "title": "Are we pretraining it right? Digging deeper into visio-linguistic pretraining"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "4ae52766028e69186052ea8f33a137fbbbdb986a", "title": "BLEURT: Learning Robust Metrics for Text Generation"}, {"paperId": "319983fdf503e8484d5409c75993c96d279a811f", "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations"}, {"paperId": "9ca30b944df178c7ae8ba7ca7602abf9c2c69281", "title": "Generative Adversarial Zero-shot Learning via Knowledge Graphs"}, {"paperId": "4df184d6a74f1ffd84b644735c9afb5060552770", "title": "Joint Commonsense and Relation Reasoning for Image and Video Captioning"}, {"paperId": "598a2ee223e2949c3b28389e922c1892b4717d2a", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"}, {"paperId": "b9779ddeb6a8a9de0f7e104d8742728aa14578d6", "title": "InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining"}, {"paperId": "fc261c0efb5f9ce82581932d1440630b861fb85f", "title": "Grounded Situation Recognition"}, {"paperId": "1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0", "title": "XGPT: Cross-modal Generative Pre-Training for Image Captioning"}, {"paperId": "0ca7d8c3250d43d14fdde46bf6fc299654d861ef", "title": "Heterogeneous Graph Transformer"}, {"paperId": "845b4941d8c016aa5f8967da2f86d38ef6c18fa3", "title": "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications"}, {"paperId": "2d06caf8353882a62ba7993c66839c3ceb27ffd4", "title": "PororoGAN: An Improved Story Visualization Model on Pororo-SV Dataset"}, {"paperId": "9915315f5cae822e98c94382ce3b0a6f9a7f8e5e", "title": "12-in-1: Multi-Task Vision and Language Representation Learning"}, {"paperId": "f4cf4246f3882aa6337e9c05d5675a3b8463a32e", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"}, {"paperId": "e69e4bb78f812ad252771b9600e1cfeb88d36722", "title": "Knowledge-Enriched Visual Storytelling"}, {"paperId": "415efb7b4d9d1e5b64dbaf3fe4229ad462acce71", "title": "Multimodal Intelligence: Representation Learning, Information Fusion, and Applications"}, {"paperId": "2bd5b4aed18400bf1a1cc866d9b8d931aa047290", "title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"}, {"paperId": "aa63ac11aa9dcaa9edd4c88db18bec87e0834328", "title": "Graph Transformer Networks"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "628e570a6e992d68f141a2533cb9b3f6d994c7ad", "title": "Commonsense Knowledge Base Completion with Structural and Semantic Context"}, {"paperId": "ee356bc5c03bf822bbdd019fe1236c42595b4d6f", "title": "From Strings to Things: Knowledge-Enabled VQA Model That Can Read and Reason"}, {"paperId": "129e2359e072ba20029e48545f4acefa618d748a", "title": "Semantics-Enhanced Adversarial Nets for Text-to-Image Synthesis"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "4cede1c63336de84344922876e6ee23617e2afb3", "title": "Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network"}, {"paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "59fa415c5052a2d98712d6b8f5ec5a1b61758fed", "title": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "6b4a48cfeef3587e850a4a8faab6792c49d906da", "title": "Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling"}, {"paperId": "b82153bf85d5d1edd3f170aace830e5328ca9ed0", "title": "Fusion of Detected Objects in Text for Visual Question Answering"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f8a48678094adbe421d61d0045361bfc635a2900", "title": "Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods"}, {"paperId": "d0818dac77eee5b970736e57a478bcedfb1b15fe", "title": "KVQA: Knowledge-Aware Visual Question Answering"}, {"paperId": "1b9ce27801c077433245ad0f9e43e3c38441cecd", "title": "Vision-and-Dialog Navigation"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "96901acc92d68350443774596fa2b38bc522a0ce", "title": "Barack\u2019s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling"}, {"paperId": "f48ae425e2567be2d993efcaaf74c2274fc9d7c5", "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"}, {"paperId": "097981245eb3c66cc10a3164275d0bd52f5ae22a", "title": "Relational Reasoning using Prior Knowledge for Visual Captioning"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "b8e113808112dde9bc8fb72219230d4949ac960f", "title": "KG-GAN: Knowledge-Guided Generative Adversarial Networks"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "c192c7d1d94e7a64de7e18e2f2fdffbf2909fcff", "title": "Deep Multimodal Representation Learning: A Survey"}, {"paperId": "b158b038fd34bc1cddd2711d6cd719a22b244b2e", "title": "PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph"}, {"paperId": "e3bb5773205477ae4711524a9d4ae739bee40349", "title": "Challenges and Prospects in Vision and Language Research"}, {"paperId": "ab995f96722969a0dfc6dc9139eef4c9b13c0524", "title": "DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-To-Image Synthesis"}, {"paperId": "00b7efbf14a54cced4b9f19e663b70ffbd01324b", "title": "Heterogeneous Graph Attention Network"}, {"paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466", "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"}, {"paperId": "fb751f1d2ac984670a57663aec80ee8b4ac189d6", "title": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "3c54b796cc10cb530f77caa4d18e1c80ac863822", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}, {"paperId": "9695676deace8c05d4e95274b92f20ed1e97470c", "title": "CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions"}, {"paperId": "f4c45108cb41051010d8a5175b8da23eb246c967", "title": "Improving Image Captioning by Leveraging Knowledge Graphs"}, {"paperId": "3b87e795f1f501843f7f99e83e38f125f6af8600", "title": "StoryGAN: A Sequential Conditional GAN for Story Visualization"}, {"paperId": "f6feb1af1809dfd872d868dfcc13021cc42f496c", "title": "Auto-Encoding Scene Graphs for Image Captioning"}, {"paperId": "ca1a2b86d39495be5524a0e39b663f7c423a0397", "title": "Explainable and Explicit Visual Reasoning Over Scene Graphs"}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "0955252cd57db8503a2ed9e56f195fa44b1bc0d4", "title": "Visual Entailment Task for Visually-Grounded Language Learning"}, {"paperId": "19f894ab29ed7895b9a0cc7435866da69e9a63c6", "title": "Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction"}, {"paperId": "cf336d272a30d6ad6141db67faa64deb8791cd61", "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs"}, {"paperId": "ad08da5951437c117551a63c2f8b943bee2029ce", "title": "Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering"}, {"paperId": "662ef1ff78e4f4a61c3d1a84fd85a1affdbd6016", "title": "How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval"}, {"paperId": "d88eb94d7054d2668b1a8dfa311721f37ae1f059", "title": "Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering"}, {"paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"}, {"paperId": "26eff1aa014ed0f19fbda0ac6b554f8ba9881f25", "title": "Deepdiary: Lifelogging image captioning and summarization"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "f986968735459e789890f24b6b277b0920a9725d", "title": "Places: A 10 Million Image Database for Scene Recognition"}, {"paperId": "46b5d408d950287637dd21ce04772d9b2bacfd14", "title": "Image Generation from Scene Graphs"}, {"paperId": "a76706d350b8c483a3aff73e61b91d15b5687335", "title": "Universal Sentence Encoder"}, {"paperId": "ff65e3bf34e892ef75d91c5e3d7294e0b64d867d", "title": "Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs"}, {"paperId": "06ba3492e3a9a2e98df2c81b91ec94787e3f97fb", "title": "VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "c468bbde6a22d961829e1970e6ad5795e05418d1", "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"}, {"paperId": "8b35c00edfa4edfd7a99d816e671023d2c000d55", "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks"}, {"paperId": "c37c23b12e00168833eccff8025a830ce27c5abc", "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"}, {"paperId": "3b1d8eb163ffff598c2faa0d9d7cf933857a359f", "title": "Neural Natural Language Inference Models Enhanced with External Knowledge"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "4ace72f12491a7c06967a6011c4bef004192d767", "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks"}, {"paperId": "ecf6c42d84351f34e1625a6a2e4cc6526da45c74", "title": "Representation Learning on Graphs: Methods and Applications"}, {"paperId": "ee909ad489244016cf301bb7d7d8eeea423dbf35", "title": "Localizing Moments in Video with Natural Language"}, {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors"}, {"paperId": "8b9db19d0d3e2a7d740be811810a043a04d6226a", "title": "An Attention-based Regression Model for Grounding Textual Phrases in Images"}, {"paperId": "0c31441de0e50b3a76a2e7908835d42a815a7e7f", "title": "graph2vec: Learning Distributed Representations of Graphs"}, {"paperId": "7e6cc717311c9c3dcf7279bc44e0c25b29650c15", "title": "DeepStory: Video Story QA by Deep Embedded Memory Networks"}, {"paperId": "a9e28863c7fb963b40a379c5a4e0da00eb031933", "title": "A Corpus of Natural Language for Visual Reasoning"}, {"paperId": "231af7dc01a166cac3b5b01ca05778238f796e41", "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy"}, {"paperId": "cd8a9914d50b0ac63315872530274d158d6aff09", "title": "Modeling Relational Data with Graph Convolutional Networks"}, {"paperId": "03eb382e04cca8cca743f7799070869954f1402a", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"}, {"paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"}, {"paperId": "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921", "title": "StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "bed7834ae7d371171977a590872f60d137c2f951", "title": "GuessWhat?! Visual Object Discovery through Multi-modal Dialogue"}, {"paperId": "cad4ac0d2389a89cf1955dd4788278c1e8ac1af9", "title": "Learning What and Where to Draw"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f", "title": "SPICE: Semantic Propositional Image Caption Evaluation"}, {"paperId": "e2dba792360873aef125572812f3673b1a85d850", "title": "Enriching Word Vectors with Subword Information"}, {"paperId": "36ee2c8bd605afd48035d15fdc6b8c8842363376", "title": "node2vec: Scalable Feature Learning for Networks"}, {"paperId": "b60630911d7746fba06de7c34abe98c9a61c6bcc", "title": "FVQA: Fact-Based Visual Question Answering"}, {"paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea", "title": "Improved Techniques for Training GANs"}, {"paperId": "6c7f040a150abf21dbcefe1f22e0f98fa184f41a", "title": "Generative Adversarial Text to Image Synthesis"}, {"paperId": "90368e1751b34f22492ed18cc3b1ab19ae546afa", "title": "Learning Deep Representations of Fine-Grained Visual Descriptions"}, {"paperId": "bc9db6117d0026bc5b11eeba2303d2bddc96c306", "title": "Multi30K: Multilingual English-German Image Descriptions"}, {"paperId": "927987a48c2a519bbc097d8b6c925b64a85b7d8e", "title": "Visual Storytelling"}, {"paperId": "e32dcf5aa3c28e6fbf4da381e03f1bea73e4f1b0", "title": "Generating Visual Explanations"}, {"paperId": "55a7286f014cc6b51a3f50b1e6bc8acc8166f231", "title": "Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "1c72ab3484bea5aa8abbd041d31f6b17c17513de", "title": "Expressing an Image Stream with a Sequence of Natural Sentences"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "20dbdf02497aa84510970d0f5e8b599073bca1bc", "title": "Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge from External Sources"}, {"paperId": "def584565d05d6a8ba94de6621adab9e301d375d", "title": "Visual7W: Grounded Question Answering in Images"}, {"paperId": "0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a", "title": "Explicit Knowledge-based Reasoning for Visual Question Answering"}, {"paperId": "651e5bcc14f14605a879303e97572a27ea8c7956", "title": "A Diversity-Promoting Objective Function for Neural Conversation Models"}, {"paperId": "131125a5aadb48ec3eceb404cedbff713c401feb", "title": "Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883", "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning"}, {"paperId": "62a956d7600b10ca455076cd56e604dfd106072a", "title": "Exploring Models and Data for Image Question Answering"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "0834e74304b547c9354b6d7da6fa78ef47a48fa8", "title": "LINE: Large-scale Information Network Embedding"}, {"paperId": "9667f8264745b626c6173b1310e2ff0298b09cfc", "title": "Learning Deep Features for Scene Recognition using Places Database"}, {"paperId": "258986132bf17755fe8263e42429fe73218c1534", "title": "CIDEr: Consensus-based image description evaluation"}, {"paperId": "353ecf7b66b3e9ff5e9f41145a147e899a2eea5c", "title": "Conditional Generative Adversarial Nets"}, {"paperId": "3d29e1c4f1c2b079cf6b5dd458fa6cee246955f9", "title": "Towards a Visual Turing Challenge"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "dab7e605237ad4f4fe56dcba2861b8f0a57112be", "title": "Wikidata"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "19b505d471c88708de949dffd788a3529b66d7c8", "title": "Acquiring Comparative Commonsense Knowledge from the Web"}, {"paperId": "f3de86aeb442216a8391befcacb49e58b478f512", "title": "Distributed Representations of Sentences and Documents"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "fff114cbba4f3ba900f33da574283e3de7f26c83", "title": "DeepWalk: online learning of social representations"}, {"paperId": "44040913380206991b1991daf1192942e038fe31", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "908091b4a8757c3b2f7d9cfa2c4f616ee12c5157", "title": "SUN database: Large-scale scene recognition from abbey to zoo"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "47ced790a563344efae66588b5fb7fe6cca29ed3", "title": "The Probabilistic Relevance Framework: BM25 and Beyond"}, {"paperId": "927d6e5f78d7d4c18a4be20de0edf277f1353024", "title": "Judging a Book by Its Cover"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2", "title": "DBpedia: A Nucleus for a Web of Open Data"}, {"paperId": "7533d30329cfdbf04ee8ee82bfef792d08015ee5", "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "title": "Bidirectional recurrent neural networks"}, {"paperId": "ac91d2df6a7e01b5f632822de09fa1aef40d7d05", "title": "Design of the MUC-6 Evaluation"}, {"paperId": "0e3e3c3d8ae5cb7c4636870d69967c197484d3bb", "title": "Verb Semantics and Lexical Selection"}, {"paperId": "24bd5b7455a07631cbc4023c07eb2f0eef85ea85", "title": "Multimodal research in vision and language: A review of current and emerging trends"}, {"paperId": null, "title": "Photorealistic text-to-image diffu"}, {"paperId": null, "title": "Seyed Kamyar Seyed Ghasemipour"}, {"paperId": "2af476f7c2a7c040fc9ab7750bf41a84f66aa947", "title": "Knowledge Based Multilingual Language Model"}, {"paperId": "e5d143ae82ede67726aa1a9aeac3de4bf53d8920", "title": "KB-VLP: Knowledge Based Vision and Language Pretraining"}, {"paperId": "f5840646d4fc9224e8d041b7572e88066b5e959b", "title": "External Knowledge enabled Text Visual Question Answering"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Generic attentionmodel explainability for interpreting bi-modal and encoderdecoder transformers"}, {"paperId": "eccfd6174709c285d62b11fb3da9b68d485ca883", "title": "Integrating Rule-based Entity Masking into Image Captioning"}, {"paperId": "4c58950f88e6c6237bbfe410a0efd50af13aa4ef", "title": "Leveraging Concept-Enhanced Pre-Training Model and Masked-Entity Language Model for Named Entity Disambiguation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df", "title": "A Comprehensive Survey on Graph Neural Networks"}, {"paperId": "a7191c0cf7665b4486ac0f0d167518e35a05c1ee", "title": "WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation"}, {"paperId": null, "title": "2016c. Google\u2019s neural machine translation system: Bridging the gap between human and machine"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": "ea67efe9866b245ea2b0bbb526239fbd7070f635", "title": "An Introduction to Information Retrieval"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "d87ceda3042f781c341ac17109d1e94a717f5f60", "title": "Book Reviews: WordNet: An Electronic Lexical Database"}, {"paperId": null, "title": "Long shortterm memory"}, {"paperId": null, "title": "Graph representation learning. Synthesis Lectures on"}]}