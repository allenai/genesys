{"paperId": "42a7015e48a1e00b70ebb442a82afb4b10017c0b", "abstract": "Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and +11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \\url{https://github.com/google-research/vision_transformer}.", "venue": "International Conference on Learning Representations", "year": 2021, "citationCount": 269, "influentialCitationCount": 32, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper substantially improves the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning, and shows that the improved smoothness attributes to sparser active neurons in the first few layers."}, "embedding": {"model": "specter_v2", "vector": [0.5503082871437073, 0.7670973539352417, -0.6653468012809753, 0.4241068363189697, -0.12900355458259583, -0.09460041671991348, 1.092536449432373, -0.3492298722267151, -0.43351370096206665, -0.5661812424659729, 0.22795414924621582, 0.12274383753538132, 0.5169671177864075, 0.0773087665438652, -0.3318566381931305, -0.1042313203215599, -0.6677216291427612, 0.19410882890224457, 0.005904411431401968, -0.6484532952308655, 0.11069355905056, -0.4999891519546509, -0.7276623249053955, -0.3328937590122223, 0.21186059713363647, 1.3244075775146484, -0.010551166720688343, 1.1222829818725586, -0.1983030140399933, 0.6922385692596436, 0.62684565782547, -0.5326547622680664, 0.7358274459838867, 0.10746285319328308, -0.27906396985054016, 0.12780854105949402, 0.9411697387695312, -0.46132439374923706, -0.5270248651504517, 0.962061882019043, -0.24284717440605164, 0.05099662393331528, 0.5410762429237366, -1.046470046043396, -0.6214163899421692, 0.13163277506828308, 0.3388928472995758, 0.8474087715148926, -0.7561858892440796, 0.07359575480222702, 1.1622331142425537, -0.9176871180534363, -0.02225548028945923, 1.2521512508392334, 0.8740388751029968, 0.35170555114746094, -0.362411767244339, -0.5151755213737488, 0.9513673782348633, -0.06073398515582085, -0.2351723313331604, -0.2862061560153961, 0.4804862439632416, -0.3362720310688019, 1.299169659614563, -0.7264611124992371, 0.17200873792171478, 0.8827335834503174, 0.2626506984233856, 1.1744948625564575, 0.1998668909072876, -0.5900837182998657, -0.14144398272037506, 0.2978026866912842, 0.3105374872684479, 0.8564317226409912, -0.4509159028530121, 0.6436837315559387, -0.665531575679779, 0.2694326639175415, 0.5798304080963135, 0.2027771919965744, 0.17924313247203827, -0.1687428057193756, -0.11730010062456131, 0.8100627064704895, 1.1069707870483398, 0.4294246435165405, -0.48388442397117615, 1.0174651145935059, 0.4404670298099518, 0.39413684606552124, -0.07559125870466232, 0.3524290919303894, 0.038149841129779816, 0.9151768088340759, -0.46221724152565, -0.22360457479953766, -0.43850553035736084, 0.7123163342475891, -0.015862414613366127, 0.4717196524143219, -0.3710644841194153, -0.020345082506537437, 1.2625758647918701, 0.08107675611972809, 0.40860214829444885, -0.556930422782898, 0.09201975911855698, -0.5122515559196472, -0.3080572485923767, -0.9851352572441101, 0.047530658543109894, -0.6178770065307617, -1.270524024963379, -0.37608686089515686, -0.3620581030845642, 0.4953370690345764, -1.4010659456253052, 0.6728676557540894, -0.43713971972465515, 0.3467566967010498, 0.16367481648921967, 0.4777820408344269, 0.44927895069122314, 0.32094284892082214, 0.14821277558803558, 0.4781036674976349, 0.8646402955055237, -0.9071701765060425, -0.3280903995037079, -1.193611741065979, -0.20066732168197632, -0.17757046222686768, 0.45966196060180664, -0.01967673748731613, -1.2372430562973022, -1.3892260789871216, -1.0623772144317627, 0.1844618022441864, -0.5357877016067505, -0.017601504921913147, 1.1017405986785889, 0.33911484479904175, -1.2350306510925293, 0.6627352237701416, -0.4119790494441986, -0.12407449632883072, 0.879763126373291, 0.32194527983665466, 0.3275473713874817, -0.05956409126520157, -0.5234587788581848, 0.418186753988266, 0.14261269569396973, -0.32152700424194336, -0.89421147108078, -0.7900828719139099, -0.8105298280715942, -0.06488512456417084, 0.12432104349136353, -0.7202679514884949, 1.0087395906448364, -0.6436843872070312, -0.991195559501648, 0.725591778755188, 0.05627157911658287, -0.14323192834854126, 0.5375315546989441, -0.04315292462706566, 0.02076818235218525, -0.08031967282295227, -0.3918392062187195, 0.8956072926521301, 1.1959455013275146, -0.5370799899101257, -0.010573988780379295, 0.17335788905620575, -0.06938768178224564, -0.3454471528530121, -0.6328670382499695, 0.5116260647773743, -0.261162668466568, -0.24143600463867188, 0.22138118743896484, 0.7020456790924072, -0.3048361837863922, -0.05508226528763771, -0.35877934098243713, -0.681965172290802, 0.7574528455734253, 0.3090757429599762, -0.05224110558629036, -0.7405873537063599, -0.9194915890693665, 0.1666870266199112, 0.48379939794540405, -0.21893756091594696, -0.9098879098892212, 0.24505126476287842, -0.5021753311157227, 0.41510331630706787, 0.13936832547187805, -1.270537257194519, -0.031722888350486755, -0.17515148222446442, -0.9184383153915405, 0.019831029698252678, 0.3838522434234619, 1.4354078769683838, -0.650351345539093, 0.08196812868118286, 0.28851518034935, 0.436042845249176, -1.0619829893112183, 1.0323843955993652, -0.41228535771369934, 0.4121953248977661, 0.05073213577270508, 0.09941188246011734, 0.11877737939357758, -0.6893608570098877, 0.18027937412261963, -0.6727927327156067, 0.17652627825737, 0.4644636809825897, -0.2232058048248291, 1.0250084400177002, -0.12904387712478638, 0.6457352042198181, -0.052767083048820496, -1.1590217351913452, 0.3335552215576172, 0.012375622056424618, -0.07992991805076599, -0.7411784529685974, 0.45102453231811523, -0.03836332634091377, -0.9906328320503235, 0.6732954382896423, 0.725650429725647, 0.5534324645996094, -0.004709084518253803, -0.4515727758407593, 1.0113475322723389, -0.39897769689559937, -0.10278888046741486, 0.4898878335952759, 0.4844876825809479, 0.2541597783565521, -0.09220197051763535, -0.005237715318799019, -0.10886593908071518, -0.8124033212661743, -0.035517435520887375, 0.6598950028419495, 0.6234163045883179, 0.9835699796676636, 0.5769726634025574, -0.822288453578949, -0.4325762093067169, -0.3567923605442047, 0.6834449768066406, 1.092477798461914, 0.030784927308559418, 0.013991041108965874, -0.43665653467178345, -0.29738226532936096, -0.5902069211006165, -0.6792051792144775, -0.681822657585144, -0.32662180066108704, -0.18230237066745758, -1.1314579248428345, 0.779023289680481, 0.2627761662006378, 1.42996084690094, -0.4605051279067993, -0.14982523024082184, -0.5131737589836121, 0.40933504700660706, -0.9282407164573669, -0.4169257879257202, 0.3940943777561188, -0.642098069190979, -0.3825936019420624, 0.004355251789093018, -0.300265371799469, 0.29936590790748596, -0.8351327180862427, 0.7891918420791626, -0.14105656743049622, -0.3193814754486084, 0.4674258232116699, 0.8367186784744263, -0.7854708433151245, -0.43187034130096436, 0.16022373735904694, 0.015791749581694603, 0.1021508127450943, -0.3960264325141907, -0.00826540868729353, -0.21902193129062653, 0.06116650626063347, -0.38430917263031006, -0.253141850233078, 0.03522301837801933, 0.284979909658432, 0.9750799536705017, -0.4803878366947174, 0.11694914847612381, -0.7411484122276306, 0.739349901676178, 0.1949365884065628, -0.8018733859062195, 0.06505031138658524, -0.9904300570487976, -0.11013849079608917, 0.5172086358070374, -0.7418256998062134, 0.014417996630072594, -0.8936143517494202, 0.25864875316619873, -1.0327032804489136, -0.2644261419773102, -0.21696622669696808, 0.5585302114486694, -0.3947543501853943, 0.65807044506073, 0.19020934402942657, 0.22562992572784424, 0.10359372943639755, 0.44412338733673096, -1.3444111347198486, 1.035266399383545, 0.3639012277126312, 0.46847110986709595, 0.038897790014743805, 0.26710376143455505, -0.6820966601371765, -0.5831161737442017, -0.29558107256889343, -0.03186657279729843, -0.5594757795333862, 0.4225877523422241, -0.5580955147743225, -1.051328420639038, 0.3220674693584442, -0.34627601504325867, -0.4693293571472168, -0.4129061996936798, -0.24089357256889343, -0.3739314377307892, -1.177611231803894, -0.9067682027816772, -0.6405408978462219, -0.7806858420372009, -1.163055419921875, 0.10497899353504181, 0.5993896722793579, 0.16593743860721588, -0.8310758471488953, -0.14399805665016174, -0.19055919349193573, 1.048123836517334, -0.044010065495967865, 0.3975158631801605, 0.15406905114650726, -0.41422325372695923, 0.006012594792991877, 0.1405114233493805, 0.9211595058441162, -0.48680341243743896, 0.40601813793182373, -1.4806114435195923, 0.1383790522813797, -0.5276867151260376, -0.9373058080673218, 0.6265501379966736, 0.5734779834747314, 0.3868793845176697, -0.17892205715179443, -0.01516285166144371, 0.843989372253418, 1.54599928855896, -0.8997399806976318, 0.5342764258384705, 0.1598174124956131, 1.076672911643982, -0.1344257891178131, -0.5138384699821472, 0.23040203750133514, 0.28687581419944763, -0.17753729224205017, 0.8596697449684143, -0.7437207102775574, -0.5505909323692322, -0.9627854228019714, 0.3215925693511963, 0.9717811346054077, 0.1424071192741394, 0.10772379487752914, -0.796004593372345, 0.747164249420166, -0.7502840757369995, -0.7409726977348328, 0.7630631327629089, 0.5604924559593201, 0.2217099666595459, -0.0002995891554746777, -0.2711905241012573, -0.034542813897132874, 0.34169432520866394, 0.6741629242897034, -0.3801182508468628, -0.36061421036720276, -0.004329916555434465, 0.8301697969436646, 0.6160222291946411, 0.7228691577911377, -0.2606436014175415, 0.6420747637748718, 14.777494430541992, 0.5864948034286499, -0.49377894401550293, 0.4638078808784485, 0.8877958059310913, 0.5219131112098694, -0.3829403221607208, 0.15268534421920776, -0.7812221646308899, -0.2924310863018036, 0.6068769693374634, 1.0561038255691528, 0.821140468120575, 0.19923661649227142, -0.41692855954170227, 0.32712143659591675, -0.29894763231277466, 0.922775149345398, 0.4447373151779175, -1.6685203313827515, 0.06720983982086182, 0.2873384952545166, 0.7246336340904236, 1.0389938354492188, 1.2353705167770386, 0.5750529170036316, 0.515869140625, -0.3835122585296631, 0.632685661315918, 0.4665001630783081, 1.224072813987732, 0.2801832854747772, 0.08464933931827545, 0.18474234640598297, -0.8807552456855774, -0.07949767261743546, -0.4498942792415619, -1.0036462545394897, -0.1493740826845169, -0.09306700527667999, -0.5235723853111267, -0.40775763988494873, 0.15004213154315948, 0.9406160116195679, -0.13329418003559113, 0.10794690996408463, -0.274287611246109, 0.7283389568328857, -0.48190438747406006, 0.42384639382362366, 0.43954336643218994, 0.35780811309814453, 0.00480864429846406, -0.37012168765068054, -0.4333649277687073, -0.4421519935131073, 0.5060930252075195, 0.6708974242210388, -1.005474328994751, -0.51319819688797, -0.1582210212945938, -0.22258390486240387, -0.5220645666122437, 1.1349133253097534, 0.12532398104667664, 0.18873684108257294, 0.1201353445649147, 0.05155618116259575, 0.36607155203819275, 0.34127378463745117, -0.08802378922700882, 0.20405888557434082, 0.45036521553993225, -0.6563531756401062, -0.135626420378685, 0.5123832821846008, -0.42352089285850525, -0.6532058715820312, -0.6304572224617004, -0.10449203103780746, 0.382782906293869, -1.1490058898925781, -0.7042979001998901, 0.6762535572052002, -0.4161636531352997, -0.0424451120197773, 0.5104151964187622, -0.7922945618629456, -0.719677746295929, 0.32821395993232727, -2.034621477127075, -1.0459920167922974, -0.384164035320282, 0.12997490167617798, -0.5276083946228027, -0.3252946436405182, 0.7398415803909302, -0.030101357027888298, -0.14565879106521606, 0.2799855172634125, -0.4757802486419678, 0.13073354959487915, -0.39796629548072815, -0.8496403694152832, 1.0360779762268066, 0.4747082591056824, 0.18223826587200165, -0.1433648318052292, -0.024958109483122826, 0.584633469581604, -0.5018441677093506, 0.21097064018249512, 0.04714111611247063, -0.8759826421737671, -0.2056351900100708, -0.4861185550689697, -0.4380054473876953, 0.5884154438972473, 0.681989312171936, 0.23042339086532593, -0.2024909257888794, 0.05809643119573593, -0.7642999291419983, -0.36463791131973267, -0.962844967842102, -0.09693081676959991, 0.1733023226261139, -0.8026105165481567, -0.5043914914131165, -0.19877389073371887, 0.09764193743467331, -0.8267751336097717, -0.41055670380592346, -0.052009809762239456, 0.1639019399881363, -0.33045902848243713, 1.4037060737609863, -0.5269256234169006, 0.4694308638572693, 0.961272656917572, -0.22795715928077698, -0.7049697041511536, 0.20739519596099854, -0.980630099773407, 0.2786211669445038, 0.2124052196741104, 0.23879170417785645, -0.9094883799552917, 0.4759560823440552, 0.33161434531211853, 0.11860007047653198, -0.149668350815773, -0.3679312765598297, -0.14274510741233826, -0.08962846547365189, -0.6796151399612427, 0.17946118116378784, 0.13162648677825928, -0.5100460648536682, 0.1439930647611618, 0.3279089629650116, 0.49026167392730713, 0.21012049913406372, -0.8353855609893799, 0.45261844992637634, -0.25410524010658264, -0.2304655760526657, -0.43954867124557495, -0.9798940420150757, -1.5489144325256348, -0.33638688921928406, -1.3171380758285522, -0.33779841661453247, -0.8770409226417542, -0.7315558791160583, -0.21842536330223083, -0.5824084281921387, 0.4623223543167114, 0.3991696536540985, 0.21918368339538574, 0.0850219801068306, -0.37869271636009216, -0.23905372619628906, 0.9739221930503845, 0.8946794867515564, -1.0709282159805298, -0.12569652497768402, 0.01804666966199875, -0.3561535179615021, 0.5519417524337769, 0.47847017645835876, -0.28046685457229614, -1.009880781173706, -0.8774628043174744, 0.4923624098300934, -0.47279873490333557, 0.5332947969436646, -1.1473538875579834, 0.6706060171127319, 0.3978382349014282, 0.21821780502796173, 0.23587347567081451, 0.6875796914100647, -0.9555820226669312, -0.6067478060722351, 0.25385329127311707, -0.6825062036514282, -0.04076904430985451, 0.33885687589645386, -0.41629156470298767, -0.08866777271032333, 0.8215207457542419, 0.46317920088768005, -0.7580623626708984, -0.7764289975166321, 0.6217644214630127, -0.15708458423614502, 0.4748111069202423, -0.5163819193840027, -0.3506651818752289, -1.4641464948654175, -0.24113193154335022, -0.10950827598571777, 0.3950451910495758, -0.4422019124031067, 0.7111845016479492, 0.6051969528198242, -1.037551999092102, 0.33697640895843506, 0.5796346664428711, -0.17991554737091064, 0.33786216378211975, 0.21018029749393463, 0.7374692559242249, -0.2229892611503601, -0.25884124636650085, -0.17325647175312042, -0.1410662978887558, -0.41132432222366333, 0.20402944087982178, 1.1400468349456787, -0.5067139863967896, -0.22827230393886566, 1.056447148323059, 0.057454146444797516, -1.0598176717758179, 0.42093557119369507, -1.423363208770752, -0.5585294961929321, -0.09356614202260971, 0.31701868772506714, -0.12009146809577942, -0.0029711348470300436, 0.23098011314868927, -0.29065728187561035, 0.49177947640419006, 0.0998949259519577, -0.3376476764678955, 0.4124191403388977, -0.03089130111038685, 0.1807888001203537, 0.24744434654712677, 0.7209532260894775, -0.8415446877479553, -1.4801170825958252, -0.8602558374404907, -0.7831301689147949, -0.08554434031248093, 0.4751601815223694, -0.38329529762268066, -1.322144865989685, 0.9473462104797363, 1.035557508468628, 0.21326808631420135, 0.6231855750083923, 0.07774978131055832, -0.0564144030213356, 0.6910120248794556, -0.21475131809711456, -0.5699042677879333, 0.010466394014656544, 1.0457340478897095, 1.0573920011520386, -0.9106667041778564, 0.21586602926254272, 0.05946773290634155, -0.6915293335914612, 0.4558955729007721, 0.30802470445632935, -0.46975448727607727, 0.8065117001533508, -0.3819376230239868, 0.13992847502231598, -0.04852139577269554, -0.5452332496643066, -0.5094642043113708, 0.8246743083000183, 1.3719456195831299, -0.12551096081733704, -0.16753393411636353, 0.9535644054412842, 0.4661785662174225, -0.0025864620693027973, -0.08421492576599121, 0.37659627199172974, 0.3826298415660858, -0.012695890851318836, 0.40126702189445496, -0.23100094497203827, 0.6172038316726685, -0.8849529027938843, -0.4865295886993408, -0.028369156643748283, 0.6552821397781372, 0.3109041452407837, 0.5089920163154602, 0.811800479888916, -0.18297548592090607, 0.7148995995521545, -0.2664068043231964, 0.6476148962974548, 0.17413727939128876, -0.5085220336914062, -0.032087430357933044, -1.144730806350708, -0.2575455605983734, -0.3745492696762085, -0.2003820538520813, -0.001142541877925396, -0.36216750741004944, 0.23574966192245483, -0.5538784861564636, 0.3698491156101227, 0.8391859531402588, 0.07349149137735367, 0.7881082892417908, 0.0672932043671608, -0.8534117937088013, -0.3327081501483917, -0.7486308813095093, 0.41003307700157166, -0.4306616187095642, 0.45949456095695496, -0.35909855365753174, -0.3285878598690033, 0.04741108790040016]}, "authors": [{"authorId": "2143737082", "name": "Xiangning Chen"}, {"authorId": "1793529", "name": "Cho-Jui Hsieh"}, {"authorId": "40206014", "name": "Boqing Gong"}], "references": [{"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "0768aba7d87ddda3482fd7892b189f84711ede47", "title": "Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae", "title": "An Empirical Study of Training Self-Supervised Vision Transformers"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "a844cf0584ad80ba4ee497b2dfc4672648c6eb8c", "title": "Robust and Accurate Object Detection via Adversarial Learning"}, {"paperId": "9efe8dbde586d6248ecfc69f08b918012e2ac478", "title": "Revisiting ResNets: Improved Training and Scaling Strategies"}, {"paperId": "8d84c38f5fce1bd1b4ae1d55400c8fb7fa5d19c8", "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective"}, {"paperId": "fa08b41ccdfc5d8771adfbc34c176fa237d4646c", "title": "Is Space-Time Attention All You Need for Video Understanding?"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4", "title": "Why are Adaptive Methods Good for Attention Models?"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "a2cd073b57be744533152202989228cb4122270a", "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"}, {"paperId": "022622e024890d6e044ac50e2da6b44c59bdf418", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"}, {"paperId": "f5c8464032a936451b222be1984cabf42d6adfa8", "title": "Are we done with ImageNet?"}, {"paperId": "38643c2926b10f6f74f122a7037e2cd20d77c0f1", "title": "Supervised Contrastive Learning"}, {"paperId": "9739f7030feb8cdc9ab479ffcf742ab1dd24eaa5", "title": "Adversarial Weight Perturbation Helps Robust Generalization"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "38b64492ac1b5d6c8166c7952073e760bfb8f46a", "title": "Stabilizing Differentiable Architecture Search via Perturbation-based Regularization"}, {"paperId": "6d4a87759917132913319960389f17fa1fe8b630", "title": "Fast is better than free: Revisiting adversarial training"}, {"paperId": "37a1d26a5e7ee404fdf4f6f9aeede91045671dcd", "title": "Disentangling Trainability and Generalization in Deep Neural Networks"}, {"paperId": "bc51622358d8eea83248ef29402fe10640d07ba6", "title": "Big Transfer (BiT): General Visual Representation Learning"}, {"paperId": "948839277bface5780896e8e8791906818aa41ac", "title": "Adversarial Examples Improve Image Recognition"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "3242bf8767179c13c7322ccfdbe18c66c1e25a99", "title": "Understanding and Robustifying Differentiable Architecture Search"}, {"paperId": "27511da76d4caa12be7d0b9c1eff2b3bfe76921f", "title": "TRAINABILITY OF ReLU NETWORKS AND DATA-DEPENDENT INITIALIZATION"}, {"paperId": "21de3a36cb51adc205fad8a1d3d69118891dc3dd", "title": "AutoAugment: Learning Augmentation Strategies From Data"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "c92be891c5f8f0f60b6de206364f9a744612d1e8", "title": "Adversarial Training for Free!"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "49b64383fe36268410c430352637ed23b16820c5", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"}, {"paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"paperId": "5f9e2f6d4a844189b2e34da8fd0ba282f3f36c6f", "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length"}, {"paperId": "7bea9bdbe363f91f66d8abc8b3457c14d0684ae5", "title": "Initialization of ReLUs for Dynamical Isometry"}, {"paperId": "51c618e9bb59b0636e52e1dbb801f5b3695a9d96", "title": "An Alternative View: When Does SGD Escape Local Minima?"}, {"paperId": "6baca6351dc55baac44f0416e74a7e0ba2bfd03e", "title": "Visualizing the Loss Landscape of Neural Nets"}, {"paperId": "751201109e644e1422d025fe8433f29570997b7d", "title": "Mean Field Residual Networks: On the Edge of Chaos"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "ae4b0b63ff26e52792be7f60bda3ed5db83c1577", "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "aaed2a884af95852580fdedda4ea768f2effeb46", "title": "Practical Gauss-Newton Optimisation for Deep Learning"}, {"paperId": "540c226fdf7047ac602c7cb05a18db19ee595df0", "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data"}, {"paperId": "b6583fe9c9dc52bb129aff4cefc60519349f3b4c", "title": "Entropy-SGD: biasing gradient descent into wide valleys"}, {"paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "84b50ebe85f7a1721800125e7882fce8c45b5c5a", "title": "Cats and dogs"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "For the 350-epoch pretraining stage, the contrastive loss temperature is set as 0.1, and we use the LAMB optimizer (You et al., 2020) with learning rate 0.001\u00d7 batch size"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "ImageNet accuracy (Left) and improvement (Right) brought by SAM. defending contrived attack (Madry et al., 2018"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "8d3a318b62d2e970122da35b2a2e70a5d12cc16f", "title": "A method for solving the convex programming problem with convergence rate O(1/k^2)"}]}