{"paperId": "47beae741f6a4470dbd286d4f3f05ba2031c52d2", "abstract": "Recent advancements in large language models (LLMs) boasting billions of parameters have generated a significant demand for efficient deployment in inference workloads. While hardware accelerators for Transformer-based models have been extensively studied, the majority of existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead. This paper investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on FPGAs. Our approach involves the specialization of distinct hardware units for specific operators or layers, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA. This model can be extended to multi-FPGA settings for distributed inference. Through our analysis, we can identify the most effective parallelization and buffering schemes for the accelerator and, crucially, determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart. To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as open-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented BERT and GPT2 on an AMD Xilinx Alveo U280 FPGA device. Experimental results demonstrate our approach can achieve up to 13.4 \u00d7 speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2 \u00d7 speedup compared to DFX, an FPGA overlay, in the prefill stage, while achieving a 1.9 \u00d7 speedup and a 5.7 \u00d7 improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.", "venue": "ACM Transactions on Reconfigurable Technology and Systems", "year": 2023, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3656177", "status": "BRONZE"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper introduces a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA, and provides a library of high-level synthesis kernels that are composable and reusable."}, "embedding": {"model": "specter_v2", "vector": [0.20387624204158783, 0.08481206744909286, -0.46641772985458374, 0.020486723631620407, -0.4996653199195862, 0.01563318632543087, 0.5161318778991699, 0.024999164044857025, -0.20781920850276947, -0.5157515406608582, 0.49232181906700134, -0.46037691831588745, 0.2767276465892792, -0.27782902121543884, -0.15845677256584167, 0.2709702253341675, -0.8177313804626465, 0.24925430119037628, 0.08657925575971603, -0.014755569398403168, -0.19309677183628082, 0.1277393102645874, -1.5935133695602417, 0.47069835662841797, -0.04446671903133392, 0.8101187944412231, -0.1680794358253479, 0.9692894816398621, -0.19548489153385162, 0.351921409368515, 0.4336443841457367, 0.16122373938560486, 0.10376441478729248, 0.2606348693370819, 0.17406609654426575, -0.35153859853744507, -0.1856205016374588, -0.7770711183547974, -0.6199678778648376, 0.6247262358665466, -0.14182840287685394, 0.16661958396434784, 0.1494581550359726, -0.8725458383560181, 0.1850249469280243, 0.8688019514083862, 0.20523561537265778, 1.1555254459381104, -0.3975162208080292, -0.5452486872673035, 0.9420092105865479, -1.208440899848938, -0.09123282879590988, 1.4260661602020264, 0.33574604988098145, 0.11631304770708084, -0.08529826998710632, -0.42039188742637634, 0.10676006227731705, -0.5286270976066589, -1.047456979751587, -0.4716250002384186, -0.07588661462068558, -0.29031917452812195, 1.7533587217330933, 0.012953478842973709, 0.2518533766269684, 0.34367528557777405, 0.0758896917104721, 1.3479512929916382, 0.04450592026114464, -1.12788724899292, 0.22938238084316254, -0.16465097665786743, 0.44565874338150024, 0.9057466387748718, -0.0446874164044857, 0.5121740102767944, -1.154425024986267, -0.4014569818973541, 0.6102825999259949, -0.15070860087871552, 0.8916988968849182, 0.1473003774881363, -0.003704298520460725, 0.6676244735717773, 0.059283751994371414, 0.5392609238624573, -0.0012654936872422695, 1.1700718402862549, 0.917445957660675, -0.22878606617450714, 0.13663089275360107, 0.05523206666111946, 0.24380406737327576, 0.22688443958759308, -1.3528966903686523, 0.23513494431972504, -0.17182055115699768, 0.8766739368438721, -0.351060152053833, 0.7318496108055115, -0.797725260257721, -0.43938061594963074, 1.143431544303894, 0.29985156655311584, 0.22282597422599792, -0.23521167039871216, -0.07830644398927689, -0.9467010498046875, 0.02069561742246151, -0.27234649658203125, 0.08057237416505814, -0.43863725662231445, -0.6765976548194885, -0.949705183506012, -0.7014912962913513, 0.22128205001354218, -0.8477057814598083, 0.26531943678855896, -0.5210652351379395, 0.3013415038585663, 0.26640111207962036, 0.2653517425060272, 0.5179862976074219, 0.7664376497268677, 0.20904241502285004, 0.19518126547336578, 1.152084469795227, -1.0524808168411255, -0.4070523679256439, -1.0902520418167114, 0.3979458212852478, -0.27628442645072937, -0.237409308552742, -0.434918612241745, -1.600965142250061, -0.9711554646492004, -0.9073017835617065, 0.06609764695167542, -0.3228839337825775, 0.508533775806427, 1.511421799659729, 0.38974329829216003, -1.3455586433410645, 0.4752318859100342, -0.8360111117362976, 0.15568244457244873, 0.25579044222831726, 0.5735888481140137, 0.9401877522468567, 0.059081293642520905, -1.3099666833877563, -0.2545505464076996, 0.1472131460905075, -0.4712013602256775, -0.4018854796886444, -0.7020208835601807, -1.1258217096328735, 0.23579828441143036, -0.2620982825756073, -0.6233359575271606, 1.3502421379089355, 0.051005035638809204, -1.1350791454315186, 0.4450029730796814, -0.28742626309394836, -0.4436071217060089, 0.026458559557795525, 0.3505716919898987, -0.6017698049545288, -0.33484432101249695, -0.06041973829269409, 0.5351410508155823, 0.6742256283760071, 0.17767171561717987, -0.41215455532073975, 0.32875239849090576, -0.05549037829041481, -0.19021159410476685, -0.17799809575080872, 1.3473902940750122, -0.556917130947113, -0.12943428754806519, 0.0016378781292587519, 0.7247181534767151, -0.5649552941322327, -0.277373731136322, -0.25330162048339844, -0.5025593042373657, 0.7530553936958313, -0.1651586890220642, 1.3464970588684082, -1.0741130113601685, -1.1893723011016846, 0.18464307487010956, -0.04533844068646431, -0.1355053186416626, -0.06984041631221771, 0.6585434675216675, -0.3062877058982849, 0.09024506062269211, 0.2061331570148468, -0.5886552929878235, -0.12922053039073944, -0.7156931757926941, -0.8879909515380859, -0.41715019941329956, -0.07703837007284164, 0.979555606842041, -0.7976345419883728, 0.1772376447916031, -0.08607549220323563, 0.5751846432685852, -1.1059421300888062, 1.0607233047485352, -0.3371749520301819, 0.002362700877711177, -0.333759069442749, 0.10745064169168472, 0.11814938485622406, -0.615530252456665, 0.8010484576225281, -0.6394083499908447, -0.544485330581665, 0.3992811143398285, -0.1512107253074646, 1.2839468717575073, -0.43839356303215027, 0.5975952744483948, 0.012263170443475246, -0.3945084810256958, 0.33523795008659363, 0.328291654586792, -0.2910860776901245, -0.5802892446517944, 0.572945773601532, 0.6391666531562805, -0.31998616456985474, 0.6548940539360046, 0.9817160964012146, 0.9141656160354614, -0.7341592907905579, 0.33314844965934753, 0.16959747672080994, -0.18891729414463043, 0.6941333413124084, 0.24043314158916473, 0.7684892416000366, 0.05864938721060753, 0.33978670835494995, -0.535468578338623, 0.4492100179195404, -1.0137766599655151, -0.32800883054733276, 0.5033883452415466, 0.8247817754745483, 0.3728940188884735, 0.41810891032218933, -0.8011995553970337, -0.3655535578727722, 0.14592313766479492, 0.5890377163887024, 1.2598958015441895, -0.19299063086509705, -0.3655308485031128, -0.8239993453025818, -0.19855305552482605, -0.13820914924144745, -0.10791409760713577, 0.06470104306936264, -0.13055019080638885, -0.6753141283988953, -1.201381802558899, 0.9572932720184326, 0.8151530027389526, 0.657737135887146, -0.536311149597168, -0.9472717642784119, -0.3523806929588318, 0.7219900488853455, -1.0256236791610718, -0.38789141178131104, 0.47588828206062317, -0.47399085760116577, 0.5850043892860413, 0.46620967984199524, -0.01889229193329811, 0.3809422552585602, -0.8394859433174133, 0.682567298412323, -0.34989914298057556, -0.7358210682868958, -0.1968296319246292, 0.8741915225982666, -0.4312998354434967, -0.9734038710594177, 0.09335324168205261, -0.22064928710460663, -0.3078136444091797, 0.35333511233329773, 0.29062777757644653, 0.2545371353626251, -0.2404569387435913, -0.22975857555866241, 0.39371612668037415, 0.03341807425022125, -0.019961843267083168, 0.4934619963169098, 0.12711642682552338, -0.3168736398220062, -0.7930245399475098, 0.7306284308433533, 0.3049682676792145, -0.9207330346107483, -0.0751020610332489, -0.3692220151424408, -0.1245017722249031, 0.7837201952934265, -0.525093138217926, -0.3718768358230591, -0.8794866800308228, 0.26252710819244385, -0.5247376561164856, -0.11628083139657974, 0.15535853803157806, 0.3610534369945526, -0.16867700219154358, 0.015808576717972755, 0.4658856689929962, 0.3473895788192749, 0.18138152360916138, 0.214753195643425, -0.8509160280227661, 0.0863277018070221, -0.1840101033449173, -0.006401232443749905, 0.08183463662862778, -0.027270643040537834, -0.7297953963279724, -0.34501397609710693, -0.14834246039390564, -0.06297483295202255, -0.3532772362232208, -0.10986211150884628, -0.6801618933677673, -0.8390222787857056, -0.2557764947414398, -1.212797999382019, -0.23279531300067902, 0.3567921817302704, -0.5081173181533813, -0.1487831324338913, -1.2305619716644287, -1.4994953870773315, -0.7781305313110352, -0.914658784866333, -1.5310691595077515, 0.9188743233680725, 0.14539691805839539, -0.3599293529987335, -0.6619779467582703, -0.5625749826431274, -0.4433898329734802, 0.9295735359191895, -0.35689130425453186, 0.9967586994171143, -0.019570747390389442, -0.7471084594726562, -0.15744398534297943, 0.09187441319227219, 0.04189002513885498, -0.6948885917663574, 0.7577012181282043, -0.8452978134155273, 0.1714053452014923, -0.5766656398773193, -0.03040912002325058, -0.07045236974954605, 0.5364067554473877, 1.2395585775375366, 0.42313987016677856, -0.9324226975440979, 0.26364630460739136, 1.422318935394287, -0.29421374201774597, -0.10610568523406982, -0.14692595601081848, 0.9647693037986755, -0.3854772746562958, 0.15307435393333435, 0.9809605479240417, -0.3261565566062927, 0.818048894405365, -0.07544528692960739, -0.5361139178276062, -0.023944644257426262, -0.16396279633045197, 0.2981846034526825, 1.6123236417770386, 0.536561906337738, -0.02561473660171032, -0.9077807068824768, 0.5301697254180908, -1.3034807443618774, -0.34471818804740906, 0.9440932273864746, 0.7088441252708435, 0.11529342085123062, 0.008304114453494549, -0.17234140634536743, 0.38904914259910583, 0.767005205154419, 0.5441389083862305, -0.020652947947382927, -1.229239821434021, 0.4736173152923584, 0.9372071027755737, 0.33084726333618164, 0.8286491632461548, -0.24029523134231567, 0.6663116812705994, 14.713098526000977, 1.3423035144805908, -0.2349345088005066, 0.3253699541091919, 0.8368374109268188, 0.49870240688323975, -0.4204113185405731, -0.10004398971796036, -1.868167757987976, 0.042867474257946014, 1.713865876197815, -0.2384130209684372, 0.3777325749397278, 0.3117920458316803, -0.019473377615213394, 0.21176384389400482, -0.31650248169898987, 0.2971798777580261, 0.38439270853996277, -1.8592621088027954, 0.397503525018692, 0.1068795695900917, 0.059121862053871155, -0.013407310470938683, 0.7413502931594849, 0.45340096950531006, 0.233637735247612, -0.3807002305984497, 0.6700953841209412, -0.1199859082698822, 1.280890703201294, -0.6233564615249634, 0.38476553559303284, 0.4037824273109436, -1.2921608686447144, 0.061415478587150574, -0.15346203744411469, -1.3366799354553223, 0.10636822134256363, 0.4082927107810974, -1.0710716247558594, -0.5419087409973145, -0.4857912063598633, 0.8043522238731384, 0.4187012016773224, 0.23121103644371033, -0.11573830991983414, 0.3987773656845093, -0.08821093291044235, -0.14867088198661804, 0.07897233963012695, 0.5527268052101135, -0.17319291830062866, -0.01871747337281704, 0.07022439688444138, -0.2761043310165405, 0.25782454013824463, 0.4936603903770447, -0.04475526139140129, -0.46643680334091187, -0.2866133749485016, -0.6136115193367004, 0.12369590252637863, 0.8506484627723694, -0.25194627046585083, 0.4408794939517975, -0.5006883144378662, 0.1395529806613922, 0.5067421793937683, -0.02300753816962242, -0.8165968060493469, 0.08028359711170197, 0.5963857173919678, -0.3587150573730469, 0.070597805082798, 0.5575436353683472, -0.20472922921180725, -0.49920278787612915, -0.9909331798553467, -0.5846121907234192, 0.07373760640621185, -0.2297886312007904, -0.4141998291015625, 0.881310224533081, 0.0325150191783905, -0.08118848502635956, 0.13952481746673584, -0.8861299157142639, -0.018499774858355522, 0.31520894169807434, -1.1098474264144897, -0.8378946781158447, 0.6841809749603271, -0.5365119576454163, -0.3391491174697876, 0.08627484738826752, 1.2661185264587402, 0.40809017419815063, -0.31255629658699036, 0.03600899502635002, -0.1044941321015358, -0.13886785507202148, -0.6277381777763367, -0.31941190361976624, 1.2996220588684082, 0.6295613646507263, -0.13261990249156952, 0.3321676254272461, 0.06564059108495712, -0.12456724792718887, -0.9094645977020264, -0.23391421139240265, 0.8060296773910522, -0.3479335308074951, 0.0238204188644886, -1.433154821395874, -0.12385807931423187, 0.45248687267303467, 0.017278628423810005, 0.06553465873003006, 0.1528768241405487, -0.1471026986837387, -0.14719074964523315, -0.2614150941371918, -0.511867344379425, 0.0017955719958990812, 0.33365651965141296, -0.9416733384132385, 0.5859923362731934, 0.007434985600411892, 0.32480475306510925, -1.659845232963562, -0.5948066711425781, 0.016599589958786964, 0.08857107907533646, -0.33166858553886414, 1.3208904266357422, 0.09275992214679718, 0.9794070720672607, 0.7398926019668579, -0.15561985969543457, 0.026786696165800095, 0.16340942680835724, -1.1288232803344727, -0.3065311908721924, -0.08899842202663422, 0.551183819770813, -0.17043007910251617, 0.6380767822265625, 0.6011096239089966, 0.29355698823928833, -0.47789740562438965, -0.2516387701034546, -0.032194267958402634, -0.47290581464767456, -0.5492396354675293, 0.6278260350227356, -0.4923507273197174, -0.24441909790039062, -0.1129956841468811, 0.2738865315914154, 1.1650855541229248, -0.4014647901058197, -0.038517147302627563, 0.07638347148895264, 0.2414252758026123, -0.5313200354576111, -0.4958277940750122, -0.7361443042755127, -0.9871987104415894, 0.28277450799942017, -1.0742769241333008, 0.07874727994203568, -0.36671945452690125, -0.3489226698875427, -0.024449273943901062, -0.04580242931842804, 0.017591143026947975, 0.6165366172790527, -0.14037448167800903, -0.93673175573349, -0.8034613132476807, -0.21739308536052704, 0.5477007031440735, 0.4300752282142639, -0.4838222861289978, 0.21282990276813507, 0.032693006098270416, 0.5014479160308838, 0.3677835166454315, 0.08326370269060135, -0.30567365884780884, -1.0845355987548828, -1.2225513458251953, 0.30174383521080017, 0.47761106491088867, -0.10167261213064194, -0.8847507238388062, 0.8111696243286133, 0.4584316313266754, -0.2664955258369446, 0.215890571475029, 0.41029661893844604, -0.6137810945510864, -0.26484912633895874, 0.5443260073661804, -0.5751652121543884, 0.40408599376678467, 1.041831374168396, -0.8595778346061707, -0.34543943405151367, 0.7438284158706665, -0.21792355179786682, -0.5694035291671753, -0.985385537147522, 0.4930826425552368, -0.8637192845344543, -0.0367024801671505, -0.1433204561471939, 0.0755644217133522, -0.7939345240592957, 0.07158886641263962, 0.023258116096258163, -0.3223988711833954, -0.28584665060043335, 0.8362138867378235, 0.4892656207084656, -0.9645612835884094, 0.40608149766921997, 0.7070128917694092, -0.5299591422080994, -0.27712252736091614, 0.40471434593200684, 0.17111486196517944, -0.6832486987113953, 0.7097514271736145, 0.10382150113582611, 0.07631590962409973, -1.2159568071365356, -0.08329636603593826, 0.4229619801044464, -0.613987922668457, -0.06019776314496994, 1.2777968645095825, -0.11764023452997208, -0.6518220901489258, -0.39715608954429626, -1.1811978816986084, -0.15215373039245605, -0.7583543062210083, 0.6547070145606995, -0.017998361960053444, 0.09003300219774246, 0.41571345925331116, -0.8029504418373108, 0.04503035545349121, -0.15584814548492432, -0.7149638533592224, 0.007286738138645887, 0.26520100235939026, -0.6603944897651672, 0.34582075476646423, 0.33004230260849, -0.3014785051345825, -0.46479547023773193, -0.19831837713718414, -0.5103067755699158, -0.19471876323223114, 0.7459704875946045, -0.002447132021188736, -0.6794146299362183, 0.9518592357635498, -0.12989665567874908, 0.20735295116901398, 0.20483100414276123, -0.29183897376060486, 0.3053847849369049, -0.18701086938381195, 0.6104390025138855, -0.009045458398759365, -0.8270063996315002, 0.993150532245636, 0.5329210162162781, -0.11025568097829819, 0.4328324794769287, -0.868488609790802, -0.32806992530822754, 1.1532210111618042, 0.3406486213207245, -0.09318792074918747, 0.9376275539398193, 1.0129611492156982, -0.39491006731987, 0.2800108790397644, -0.9304596185684204, 0.009088506922125816, 0.7488954663276672, 0.7134513258934021, 0.9305524230003357, 0.41376227140426636, 0.09320617467164993, 0.8558496832847595, 0.24744351208209991, 0.3762050271034241, 0.5100682377815247, 0.7655925750732422, 0.2160683423280716, -0.3712042272090912, 0.07853742688894272, 0.5907586216926575, -0.4812741279602051, -1.070050597190857, 0.6486658453941345, 0.813662052154541, -0.16046586632728577, 0.28818342089653015, 1.2421270608901978, 0.1390196532011032, -0.051656171679496765, -0.38479194045066833, 0.9220926761627197, -0.6843487024307251, -0.45814943313598633, 0.22857904434204102, -0.17775914072990417, -0.08012405037879944, 0.11701387166976929, -0.250057190656662, -0.8027163147926331, -0.3719724714756012, 0.63149493932724, 0.08096230030059814, 0.41172540187835693, 0.9279658198356628, 0.760299563407898, 0.9348412752151489, -0.4956263601779938, -0.6146658658981323, -0.030862070620059967, -0.12271812558174133, -0.06924039125442505, -1.151222586631775, -0.3167129158973694, 0.0014011148596182466, 0.1592930406332016, -0.44159412384033203]}, "authors": [{"authorId": "2108844339", "name": "Hongzheng Chen"}, {"authorId": "2276449308", "name": "Jiahao Zhang"}, {"authorId": "2152976607", "name": "Yixiao Du"}, {"authorId": "31032322", "name": "Shaojie Xiang"}, {"authorId": "2229467567", "name": "Zichao Yue"}, {"authorId": "2242109097", "name": "Niansong Zhang"}, {"authorId": "2276454314", "name": "Yaohui Cai"}, {"authorId": "2242087633", "name": "Zhiru Zhang"}], "references": [{"paperId": "9529e50807f36acf3d2e4af994b5803c47e4746a", "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"}, {"paperId": "d8aebd1d14e39f3078ac3885e24810c7144123d1", "title": "AIM: Accelerating Arbitrary-Precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "56b828717f32251a5e0f0be9c0113077f23c8429", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "title": "SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "456f471ee207a451b87741f94fdc6fc58d39bec2", "title": "PASTA: Programming and Automation Support for Scalable Task-Parallel HLS Programs on Modern Multi-Die FPGAs"}, {"paperId": "2a44c6b7f291f625314a82ba3131e605009fd533", "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e", "title": "Full Stack Optimization of Transformer Inference: a Survey"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "6c148b4273eeab70ca4499400d5fcf34d813b957", "title": "Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training"}, {"paperId": "101958920c32d67b6509cd849e6644b28d7e473f", "title": "Binarized Neural Machine Translation"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "b3c1eaa20b27cc089e26d8833948ba01c2d945cb", "title": "FlexCNN: An End-to-end Framework for Composing CNN Accelerators on FPGA"}, {"paperId": "3692f4df9d11af68f9b9c9a526667db3f99e552c", "title": "Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632", "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"}, {"paperId": "276a00173085d75232dee025104cc27ad23eaf75", "title": "A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks"}, {"paperId": "86891d00499eebe86d3f1e39143d412addf2652b", "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "9cb04902d4f0734b3dfab7120d7dfdea3ea050ed", "title": "TRAC: Compilation-Based Design of Transformer Accelerators for FPGAs"}, {"paperId": "0f1358b84f319947e20304edd42ec6fdd2d5de2c", "title": "A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining"}, {"paperId": "e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28", "title": "FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization: late breaking results"}, {"paperId": "48fd8a608024000c1f1fc6184bbcd45520328874", "title": "Accelerator design with decoupled hardware customizations: benefits and challenges: invited"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "b8be066b41179472c0bff1388b2fc92e338117d0", "title": "High-Performance Sparse Linear Algebra on HBM-Equipped FPGAs Using HLS: A Case Study on SpMV"}, {"paperId": "27e200d3e46a54d2ba16d9173ba6f711eac18adc", "title": "HeteroFlow: An Accelerator Programming Model with Decoupled Data Placement for Software-Defined FPGAs"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "e930b3dc75f9cc36aa78aa640eee639b708a92bd", "title": "Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning"}, {"paperId": "8bb197ec64709c305c007cfff4260e2a782c8577", "title": "Accelerating Framework of Transformer by Hardware Design and Model Compression Co-Optimization"}, {"paperId": "62764bb2728d95805c93daef5f7a3a9debcd6417", "title": "Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "a6ceb0e50c052b1985465f759612b121108c1465", "title": "ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "b62d4f3e06fdda6c3c09c0225be420d1e8f7c8ba", "title": "Efficient Methods for Mapping Neural Machine Translator on FPGAs"}, {"paperId": "ce9a7d4652192a4f32cd10cff211b47b2a2d9817", "title": "Accommodating Transformer onto FPGA: Coupling the Balanced Model Compression and FPGA-Implementation Optimization"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "d6eeb0ca9e2f34f2427866aa864d364ec78e6049", "title": "Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning"}, {"paperId": "0204707f16dabd53d43a3a8874d64914152fd864", "title": "hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices"}, {"paperId": "9f840be023309cc957dc741dce85dfc6b1a3b486", "title": "NPE: An FPGA-based Overlay Processor for Natural Language Processing"}, {"paperId": "43d1465ec9b1ad716a8892a5b21f9cf689d60e27", "title": "Stratix 10 NX Architecture and Applications"}, {"paperId": "d42324618875384183bb01d02965934cbe802405", "title": "PyLog: An Algorithm-Centric Python-Based FPGA Programming and Synthesis Flow"}, {"paperId": "b0cf4438fdf1d54d4acc427f9bb949645de40c80", "title": "AutoSA: A Polyhedral Compiler for High-Performance Systolic Arrays on FPGA"}, {"paperId": "7a320541d7772bedc9b7f537f6bd459675675bb0", "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing"}, {"paperId": "d58eaf36cf77fb369ddc613d0054c15adb3991ec", "title": "AutoBridge: Coupling Coarse-Grained Floorplanning and Pipelining for High-Frequency HLS Design on Multi-Die FPGAs"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "f7e83bf07e88bb586b72ba9f5d8d0ffe90c116b7", "title": "FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations"}, {"paperId": "4533aea0bf9227ef8f566d69e246f3447d427576", "title": "Memory-Efficient Dataflow Inference for Deep CNNs on FPGA"}, {"paperId": "6d434957fa022768fbf0f5dcc57e9810b941e31d", "title": "DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of FPGA-based DNN Accelerator"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "1340978c92e7cbcf9abe87888150c60984e2964b", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "33715e08c111625427ddea46f3da52bba090e6da", "title": "HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing"}, {"paperId": "a821a00c54749a69a9d9fe0a7411de590d247715", "title": "DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs"}, {"paperId": "8daa392b62968206885f3e84c3d96cf04d48a5b4", "title": "FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks"}, {"paperId": "18495282a9a69efb08c13007288c932cb19a7172", "title": "FINN-R"}, {"paperId": "57f1e3397004ea0ce2877e07f0cbbfbb3045de41", "title": "Serving DNNs in Real Time at Datacenter Scale with Project Brainwave"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3b2491ddeeaa7beae4d311b217c292a9e16112cf", "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference"}, {"paperId": "8d67bac352dcd43c9c08f917ba8c4bebb444b55d", "title": "A cloud-scale acceleration architecture"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "TensorFlow: A system for large-scale machine learning"}, {"paperId": "3468a27c2e3019e1216ee9fe8bbf1ed3a0155ff4", "title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks"}, {"paperId": "f4f9ff1fb8fc987e0bbcbf1ae60546a09a74788d", "title": "AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving"}, {"paperId": null, "title": "AI Engines and Their Applications. White paper"}, {"paperId": "da37fb0a9071d1ceda0c7e359a049c6c7e627a01", "title": "Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"}, {"paperId": null, "title": "Xilinx. Alveo u280 data center accelerator card"}, {"paperId": null, "title": "Fully Sharded Data Parallel: faster AI training with fewer GPUs"}, {"paperId": null, "title": "PyTorch Distributed: Experiences on Accelerating Data Parallel Training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2023. Full Stack Optimization of 22"}, {"paperId": null, "title": "AMD Xilinx. 2023"}, {"paperId": null, "title": "Intel"}, {"paperId": null, "title": "2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"}, {"paperId": null, "title": "Vitis"}, {"paperId": null, "title": "ELS-RD"}, {"paperId": null, "title": "Vicuna: An open-source chatbot"}, {"paperId": null, "title": "AMD Xilinx. 2022. VCK5000 Versal Development Card"}, {"paperId": null, "title": "2022. xFormers: A modular and hackable Transformer modelling library"}, {"paperId": null, "title": "GPTQ: Accurate post-training compression for generative pretrained transformers"}, {"paperId": null, "title": "2022. Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning"}]}