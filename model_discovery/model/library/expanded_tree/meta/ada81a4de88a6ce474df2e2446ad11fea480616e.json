{"paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e", "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 444, "influentialCitationCount": 36, "openAccessPdf": {"url": "https://arxiv.org/pdf/2204.00598", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people."}, "embedding": {"model": "specter_v2", "vector": [0.08611743152141571, 0.7510083317756653, -0.30204352736473083, -0.2565785050392151, -0.3988153338432312, -0.09318304806947708, 0.5179197192192078, -0.2739839255809784, -0.5216272473335266, -0.25403547286987305, 0.7695580124855042, 0.2580782175064087, 0.280714213848114, 0.18226248025894165, -0.20228110253810883, 0.23983898758888245, -0.63631272315979, 0.519614040851593, 0.18753014504909515, -0.900406002998352, -0.31526854634284973, -0.818248987197876, -0.8641485571861267, 0.43589454889297485, 0.09790690988302231, 0.1849243938922882, 0.48066794872283936, 1.0879219770431519, 0.06400112062692642, 0.8670899868011475, 0.5291122794151306, -0.14928282797336578, -0.2573866546154022, 0.20969006419181824, -0.47061091661453247, 0.010314109735190868, 0.08829605579376221, -0.8185026049613953, -0.9259820580482483, 0.5991248488426208, -0.2458600252866745, 0.22134946286678314, 0.6700109839439392, -0.649823784828186, -0.6206141114234924, 0.47687727212905884, 1.2383270263671875, 0.28408050537109375, 0.4869635999202728, -0.2580910325050354, 1.5617884397506714, -1.3451569080352783, 0.5682569146156311, 1.9066811800003052, 0.18139788508415222, 0.8848137259483337, 0.057309459894895554, -0.19275806844234467, 1.0529801845550537, 0.21956491470336914, -0.6622571349143982, -0.15281163156032562, -0.1709517240524292, -0.007392545230686665, 1.13257896900177, -0.9160327911376953, 0.1369822472333908, 1.0576050281524658, 0.18140405416488647, 1.8063956499099731, -0.36314359307289124, -1.0609632730484009, -7.875201845308766e-05, 0.38494184613227844, 0.03169446438550949, 0.870040237903595, -0.628507137298584, 0.12437747418880463, -0.8789636492729187, 0.4091283679008484, 0.31212475895881653, 0.06681564450263977, -0.45986008644104004, -0.42668023705482483, -0.5663171410560608, 0.8225197792053223, 0.7159032225608826, 0.5625975728034973, -0.016502637416124344, 0.5115538835525513, 0.44351649284362793, 0.39158347249031067, -0.4792216420173645, 0.32234644889831543, -0.10449764877557755, 0.5586904883384705, -0.4676370918750763, 0.39295288920402527, 0.4580327272415161, 1.1701089143753052, -0.2850591838359833, -0.16923367977142334, -0.6754395365715027, 0.0009877394186332822, 1.4389386177062988, -0.008519179187715054, 0.6823840141296387, -0.6786671876907349, 0.24263140559196472, -0.7272754311561584, 0.9516046047210693, -1.0072786808013916, -0.42218783497810364, 0.13317687809467316, -0.17009660601615906, -1.3277995586395264, -0.3312474489212036, 0.2170734703540802, -0.9368607997894287, 0.8825221061706543, -0.30228614807128906, -0.2873370945453644, 0.33438989520072937, 0.6639593243598938, 0.8192526698112488, 0.7193852663040161, 0.3560689687728882, 0.6514851450920105, 0.8947114944458008, -0.8932700753211975, -0.7139447331428528, -1.3088188171386719, 0.4355820417404175, 0.3693840503692627, 0.5522106289863586, -0.33301979303359985, -0.7923851609230042, -1.0319689512252808, -1.0585347414016724, -0.46139392256736755, -0.9753934741020203, 0.6537638306617737, 0.8419486284255981, 0.316628634929657, -1.0923194885253906, 0.350830078125, -0.0819874182343483, -0.570554792881012, 0.21004383265972137, 0.32102930545806885, 0.09637013077735901, -0.9503841400146484, -1.2503793239593506, 0.6439835429191589, 0.23841345310211182, -0.4062100648880005, -0.7116914987564087, 0.15888746082782745, -1.8492660522460938, -0.41825753450393677, 0.618241548538208, -0.6663079857826233, 1.3373081684112549, -0.46708133816719055, -1.2905867099761963, 0.5980129241943359, -0.5951045155525208, 0.28653645515441895, 0.2834623456001282, -0.5133724808692932, -0.6225680112838745, 0.40690872073173523, -0.08117197453975677, 1.2833856344223022, 0.5033615827560425, -0.3818376660346985, 0.24288570880889893, 0.10718284547328949, 0.2725401818752289, -0.026548733934760094, -0.09591185301542282, 0.9125257134437561, -0.5508996248245239, -0.015209152363240719, 0.17500965297222137, 0.5256845951080322, -0.076057069003582, 0.12305088341236115, -0.13556267321109772, -1.0014584064483643, 0.6672508120536804, 0.1270104944705963, 0.31052279472351074, -0.9822292923927307, -0.31279951333999634, -0.6064693927764893, -0.21545344591140747, -0.5785952806472778, -1.3537532091140747, 0.6058844923973083, -0.14244088530540466, 0.3630838692188263, -0.05634545534849167, -1.1280124187469482, 0.20787431299686432, -0.008679688908159733, -0.4205034077167511, 0.01623738370835781, 0.45246562361717224, 1.4520502090454102, -1.2307318449020386, -0.386908620595932, 0.19649842381477356, 0.24080179631710052, -1.004732370376587, 1.116612434387207, -0.7129941582679749, -0.032381702214479446, -0.3576749265193939, 0.004287436604499817, -0.5023475885391235, -0.5284061431884766, 0.45150408148765564, -0.7135998010635376, 0.30647507309913635, 0.0832008644938469, -0.16795264184474945, 1.751511573791504, -0.11261402815580368, 0.7786768674850464, -0.264353483915329, -0.398855984210968, -0.23297445476055145, 0.6508020162582397, -0.3335205912590027, -0.23113465309143066, 0.5190114378929138, -0.02435814030468464, -0.8654822111129761, -0.5847740769386292, 0.639861524105072, 0.6088549494743347, -0.5053189396858215, -0.16033494472503662, 0.5814570784568787, -0.6730799078941345, 0.41398653388023376, 0.8199551105499268, 0.5174016952514648, 0.6888596415519714, 0.2067403495311737, 0.19110141694545746, 0.39241844415664673, -0.596047043800354, -0.5505424737930298, 0.5877610445022583, 0.5518252849578857, 1.1463689804077148, 0.244124174118042, -0.8479263186454773, -0.36080217361450195, 0.12364514172077179, 0.8767610192298889, 1.4044934511184692, 0.48428237438201904, -0.13847850263118744, -0.750993013381958, -0.11951304972171783, -0.7237091064453125, 0.1840645670890808, -0.7219556570053101, -0.08058752119541168, -0.09847971796989441, -0.5573828220367432, 0.32013610005378723, 0.6249014735221863, 0.9475804567337036, -0.7492110133171082, -0.2893524765968323, -0.21595923602581024, -0.132711723446846, -1.0822765827178955, -0.6098207235336304, -0.12415114045143127, -0.10183601826429367, -0.2676559388637543, -0.17462888360023499, -0.3284790515899658, 0.42992761731147766, -0.47407105565071106, 0.7984111905097961, -0.813102126121521, -0.5300014019012451, 1.0118968486785889, 0.34654363989830017, -0.4407709538936615, -0.5854257345199585, -0.24748428165912628, 0.07564008980989456, -0.12036759406328201, 0.19756948947906494, 0.6685346364974976, 0.007870526053011417, 0.3144981265068054, -0.3512710630893707, 0.2764764428138733, 0.18508131802082062, -0.020215198397636414, 0.5975655317306519, -0.6164111495018005, 0.22898800671100616, -0.5359252095222473, 0.5688364505767822, -0.05836663767695427, -0.11155882477760315, 0.34619757533073425, -0.0427803173661232, -0.5815277695655823, 0.48186108469963074, -1.061408281326294, -0.1995212882757187, -0.381217896938324, 0.723097562789917, -0.43649810552597046, -0.5407994985580444, 0.31939801573753357, -0.03043009527027607, 0.4099048674106598, 0.46741196513175964, 0.45880454778671265, 0.3769131898880005, 0.20787715911865234, 1.0358260869979858, -1.072205901145935, 0.8598204255104065, 0.23532915115356445, 0.15853050351142883, 0.26325592398643494, -0.2610551416873932, -0.6637442708015442, -0.7552146315574646, -0.5202815532684326, -0.4313269853591919, -0.7980762124061584, 0.8482920527458191, -0.8993321061134338, -0.8610594272613525, -0.2778666615486145, -1.206025242805481, -0.4438398778438568, 0.28735649585723877, -0.4213906526565552, -0.5084818005561829, -0.7800625562667847, -0.8149597644805908, -0.5286945104598999, 0.029445305466651917, -0.6155533194541931, 0.5253975987434387, 0.20947086811065674, -0.5773957967758179, -0.44414418935775757, -0.011864430271089077, -0.13338935375213623, 0.621283233165741, -0.45649078488349915, 0.6383872032165527, 0.17878447473049164, -0.24871201813220978, -0.6583667397499084, 0.13496200740337372, 0.3792925179004669, -0.28110355138778687, -0.0830109566450119, -0.7009568214416504, 0.19289301335811615, -0.5217582583427429, -0.9045650959014893, -0.034374214708805084, 0.017523586750030518, 0.37327051162719727, 0.14484663307666779, -0.3087908625602722, -0.19814187288284302, 1.4091577529907227, -0.6328369379043579, 0.1282479614019394, -0.08737016469240189, 0.7619884610176086, 0.5391120314598083, -0.08284527063369751, 0.43344807624816895, 0.8737658858299255, 0.30408185720443726, 0.22445908188819885, 0.18275338411331177, -0.027274899184703827, -0.9298200607299805, 1.0293323993682861, 0.8301678895950317, 0.2891501784324646, 0.03337855637073517, -1.022511601448059, 0.8305055499076843, -1.3743078708648682, -0.731859564781189, 0.8919767737388611, 0.4018801152706146, 0.03157602995634079, -0.7658074498176575, -0.24714767932891846, -0.7159227132797241, 0.5218304395675659, 0.11332400143146515, -0.5745441913604736, -0.07834593951702118, -0.09102237969636917, -0.08731314539909363, -0.2953949272632599, 0.5072001218795776, -0.8554575443267822, 0.5180779695510864, 14.444130897521973, 0.36871472001075745, 0.11189203709363937, 0.6422715187072754, 0.5118810534477234, 0.3494965136051178, -0.2831999957561493, -0.29855823516845703, -0.9687509536743164, -0.6907647252082825, 1.2206426858901978, 0.3959660232067108, 0.5262920260429382, 0.4917450547218323, 0.14594489336013794, 0.009645070880651474, -0.8589635491371155, 0.7088876962661743, 1.1017053127288818, -1.0216352939605713, 0.28144165873527527, -0.1985403150320053, 0.13561223447322845, 0.5980505347251892, 0.7682390809059143, 1.070639967918396, 0.4134214520454407, -0.4399128556251526, 0.8864917159080505, 0.40370607376098633, 0.9321831464767456, 0.1636861264705658, 0.304557740688324, 0.8149253129959106, -0.8586137294769287, -0.7259091138839722, -0.46819007396698, -0.7093254327774048, 0.32493457198143005, -0.8423336148262024, -0.3456246256828308, -0.38954195380210876, -0.19966091215610504, 0.7655140161514282, -0.03273404389619827, 0.014383705332875252, -0.25985196232795715, 0.011918785981833935, -0.08246205002069473, -0.41661369800567627, 0.3914932906627655, 0.6026124954223633, 0.16280800104141235, -0.23508745431900024, -0.08223634958267212, 0.47798627614974976, 0.22134432196617126, 0.9064183831214905, -0.27040791511535645, -0.10874486714601517, -0.6162499189376831, -0.3642211854457855, -0.560211718082428, 0.3871532380580902, 0.34072890877723694, 0.21676863729953766, -0.562157392501831, 0.48925086855888367, 0.20904439687728882, 0.508787989616394, -0.3213096559047699, -0.13657452166080475, -0.2543316185474396, -0.5635929107666016, 0.09084682911634445, 0.27421578764915466, 0.26008015871047974, -0.7913509607315063, -0.6138114929199219, -0.22118693590164185, 0.29641011357307434, -1.4550731182098389, -0.8468030691146851, 0.8204198479652405, -0.11509493738412857, -0.7282692790031433, 0.11105687916278839, -1.1346074342727661, -0.2573399245738983, -0.23573358356952667, -1.2968316078186035, -0.9495317935943604, -0.30905184149742126, 0.032094549387693405, 0.15542955696582794, 0.13773903250694275, 1.2040311098098755, -0.12773984670639038, -0.1654239147901535, -0.2286926507949829, -0.24182674288749695, 0.06533292680978775, -0.2232426553964615, -0.6012299656867981, 0.16676853597164154, -0.3214004933834076, -0.31109556555747986, 0.19490483403205872, 0.07879239320755005, 0.06791530549526215, -1.0848822593688965, 0.28254735469818115, 0.5107502341270447, -1.1574318408966064, -0.4360963702201843, -0.6883512139320374, -0.42546966671943665, 0.06494580954313278, 0.4195464849472046, -0.1546238213777542, 0.27046701312065125, -0.17993178963661194, -0.7789085507392883, 0.0524047389626503, -0.9615226984024048, 0.24594756960868835, 0.10429616272449493, -0.9701430797576904, -0.6456557512283325, 0.3133997619152069, 0.45250359177589417, -0.6938626170158386, 0.07131092995405197, -0.27955731749534607, 0.40970510244369507, 0.12262703478336334, 1.0520050525665283, -0.7510379552841187, 0.9133365154266357, 0.5878716111183167, -0.319569855928421, -0.3236857056617737, 0.21455559134483337, -0.6485819220542908, 0.17566271126270294, -0.45561230182647705, 0.7040579915046692, -0.1652207225561142, -0.0027710089925676584, 1.2802900075912476, 0.5102209448814392, -0.3702845573425293, -0.3184635639190674, -0.1472555249929428, -0.07819702476263046, -0.5419530868530273, -0.22765406966209412, -0.3582783639431, 0.26381030678749084, 0.2993772327899933, 0.31495359539985657, 0.8468850255012512, -0.39710813760757446, -0.9470967650413513, 0.6779682040214539, 0.21641695499420166, -0.08732277154922485, -0.0027131950482726097, -0.45623528957366943, -1.7866157293319702, -0.02840280532836914, -1.0265491008758545, 0.5537337064743042, -1.2843236923217773, -0.3966192901134491, 0.7691761255264282, -0.24834318459033966, 0.18183544278144836, 0.69761723279953, -0.3977321982383728, 0.05992581695318222, -0.5955426692962646, -1.095704197883606, 0.9300236701965332, 1.1489124298095703, -0.9866888523101807, 0.11075899004936218, -0.2545355260372162, -0.017208455130457878, 0.6661795377731323, 0.2795449495315552, 0.10704442113637924, -0.9520458579063416, -1.0523390769958496, 0.5844756364822388, 0.08852899074554443, 0.3128794729709625, -1.0555250644683838, 0.9695881009101868, 0.4513658881187439, -0.21068385243415833, -0.22547629475593567, 1.0084583759307861, -1.0674854516983032, -0.8397052884101868, 0.13431724905967712, -0.9575393199920654, -0.07789399474859238, 0.20497460663318634, 0.02162216790020466, -0.4790472388267517, 0.7036078572273254, -0.019831368699669838, -1.2676044702529907, -1.0192092657089233, 0.1307632029056549, -0.7501519322395325, -0.100777268409729, -0.19017836451530457, 0.07976724207401276, -1.0319805145263672, -1.016998291015625, -0.4486415386199951, 0.6192784309387207, -0.5590168833732605, 1.0521641969680786, 1.095086693763733, -0.975864827632904, 0.046787768602371216, -0.04757539927959442, 0.37373045086860657, 0.3017852306365967, 0.8146594762802124, 0.38811007142066956, -0.13748891651630402, 0.5316491723060608, 0.44290804862976074, 0.45639705657958984, -0.6130207777023315, 0.3116385340690613, 1.068211317062378, -0.09365980327129364, -0.3187998831272125, 0.9862752556800842, 0.1089160218834877, -1.1286052465438843, 0.41396966576576233, -0.7717470526695251, -0.7634786367416382, -0.2257600873708725, 0.6536219716072083, -0.2947385311126709, -0.8254144787788391, -0.27328839898109436, -0.4248829185962677, 0.7915806770324707, -0.004947960376739502, -0.7563246488571167, 0.35947051644325256, -0.3915661871433258, -0.20667652785778046, 0.7950786352157593, 0.9874444603919983, -1.0400099754333496, -0.6847760677337646, -0.4250198006629944, -0.690926730632782, 0.23004455864429474, 0.030404899269342422, -0.719290554523468, -0.09689769148826599, 0.7958743572235107, 0.5445448756217957, -0.17999665439128876, -0.00914266798645258, 0.4815000891685486, -0.09628232568502426, 1.3275089263916016, 0.055435020476579666, -0.6828386187553406, 0.35804641246795654, 1.0610953569412231, 1.6767802238464355, -1.132286548614502, 0.2438507080078125, -0.4382054805755615, -0.7071422934532166, 1.1391934156417847, 0.38844358921051025, -0.17966362833976746, 0.8929490447044373, -0.6017614006996155, -0.014511795714497566, 0.21159234642982483, -1.021345853805542, -0.15735769271850586, 0.8208043575286865, 1.2081177234649658, 0.08118370920419693, 0.42028364539146423, 0.41151878237724304, 0.8827945590019226, 0.21215897798538208, 0.4810086786746979, 0.7581670880317688, 0.33564624190330505, -0.48426052927970886, 0.34811675548553467, 0.25110283493995667, 0.21685612201690674, -0.10190963000059128, -0.2955338656902313, 0.06323797255754471, 0.5512010455131531, 0.10547339171171188, 0.780170202255249, 0.5673102140426636, 0.23027156293392181, 0.38371118903160095, 0.23270124197006226, 0.5686901807785034, -0.7509103417396545, 0.4839151203632355, -0.21579866111278534, -0.5936341285705566, -0.13618876039981842, -0.5138838291168213, -0.6362650990486145, -0.5873982906341553, 0.4439429044723511, 0.49854254722595215, -0.5253217816352844, 0.37022092938423157, 1.365665078163147, 0.40564653277397156, 0.2640761435031891, -0.5555132627487183, -0.7362335920333862, -0.34358376264572144, -0.6555783152580261, 0.6441674828529358, -0.6368466019630432, -0.047682926058769226, -0.5104967355728149, -0.5712279677391052, -0.15919022262096405]}, "authors": [{"authorId": "38591293", "name": "Andy Zeng"}, {"authorId": "153873821", "name": "Adrian S. Wong"}, {"authorId": "69426588", "name": "Stefan Welker"}, {"authorId": "1805203", "name": "K. Choromanski"}, {"authorId": "50516802", "name": "F. Tombari"}, {"authorId": "3187602", "name": "Aveek Purohit"}, {"authorId": "1766489", "name": "M. Ryoo"}, {"authorId": "1808676", "name": "Vikas Sindhwani"}, {"authorId": "2108488231", "name": "Johnny Lee"}, {"authorId": "2657155", "name": "Vincent Vanhoucke"}, {"authorId": "47686265", "name": "Peter R. Florence"}], "references": [{"paperId": "5406129d9d7d00dc310671c43597101b0ee93629", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "05bcf9999525656cfaa59bc71f8572d771ff3776", "title": "Language Models Can See: Plugging Visual Controls in Text Generation"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "cb898931e7b866e026696200292469d3a29c7fc0", "title": "Disentangled Representation Learning for Text-Video Retrieval"}, {"paperId": "18bd22b1b6091bec3c4b8f51ef97c7f11d7f110e", "title": "CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "d7d1bbade9453f0348fac8a5c60d131528b87fcf", "title": "Block-NeRF: Scalable Large Scene Neural View Synthesis"}, {"paperId": "1ce67b5555c123ae1efb710965567f51bf764423", "title": "mSLAM: Massively multilingual joint pre-training for speech and text"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "cc9826c222ac1e81b4b374dd9e0df130f298b1e8", "title": "Language-driven Semantic Segmentation"}, {"paperId": "400d619cbabeb669115bb7281a889ab869829ef5", "title": "MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"}, {"paperId": "4c70bdfe7bbdf4899a50e25f6fdaff0d7ee97ac1", "title": "Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer"}, {"paperId": "a2502d2cd7144c5e2bc1d0d7ec37d2c84b37d381", "title": "ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic"}, {"paperId": "a7aa150b55d64d339b1c154d6d88455fc3cbc44f", "title": "ClipCap: CLIP Prefix for Image Captioning"}, {"paperId": "197d5867a45a2988f4dd159063cdfbfe90164962", "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "7dfcc5c22859cee318b404f1dadb207c312c7c92", "title": "Wav2CLIP: Learning Robust Audio Representations from Clip"}, {"paperId": "848eb8367785910c2fe31372605954ad8f9dfe6c", "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video"}, {"paperId": "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0", "title": "Hybrid Random Features"}, {"paperId": "821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"}, {"paperId": "69ee9b3a915951cc84b74599a3a2699a66d4004f", "title": "CLIPort: What and Where Pathways for Robotic Manipulation"}, {"paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156", "title": "Recursively Summarizing Books with Human Feedback"}, {"paperId": "2672777d25562c9df6fc13b653181db62d39bece", "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"}, {"paperId": "fe0969ea1875c4c76bcdb205d818daaee7a84bfc", "title": "MURAL: Multimodal, Multitask Retrieval Across Languages"}, {"paperId": "0f9d994d21f16d35038f5317fccdd5df8c3398d6", "title": "Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss"}, {"paperId": "9289826beb6206eeaf500105f7329d6d5a495d8a", "title": "Robust fine-tuning of zero-shot models"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d", "title": "Multimodal Few-Shot Learning with Frozen Language Models"}, {"paperId": "c401e01c9ee32fab7d02670d1c754f44fc1ff99e", "title": "CLIP2Video: Mastering Video-Text Retrieval via Image CLIP"}, {"paperId": "61ff6872fa07788acdbe85b1319554a5fb9ce0db", "title": "XIRL: Cross-embodiment Inverse Reinforcement Learning"}, {"paperId": "439cea98c9ad49b419509181325c83e8bb9748bf", "title": "Audio Retrieval with Natural Language Queries"}, {"paperId": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14", "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "281ad83e06d731d5d686acf07cd701576f1188c4", "title": "CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval"}, {"paperId": "54423c91d0db22193439f21d424302eab2d83b24", "title": "Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos"}, {"paperId": "bac87bdb1cabc35fafb8176a234d332ebcc02864", "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "4989ccd377a50581ad4a38d57de0ef978cb9901b", "title": "A Straightforward Framework For Video Retrieval Using CLIP"}, {"paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "40ffdf58932db8284a33a56b6ce8bded2f5a829b", "title": "Video Summarization Using Deep Neural Networks: A Survey"}, {"paperId": "1bbf0ca3d492e52e57a83891e9afc91451a9be64", "title": "Recent Advances in Video Question Answering: A Review of Datasets and Methods"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "3e6a384a13e9e679759c30ab2e0f22fb0bdf7da2", "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "fb034caef3720744402363162e8f9b8758f0b82c", "title": "A Short Note on the Kinetics-700-2020 Human Action Dataset"}, {"paperId": "9f2a7a912624d850cf9e0587263b4fcd88d44f18", "title": "What Is More Likely to Happen Next? Video-and-Language Future Event Prediction"}, {"paperId": "78bc767ebd02c0cc690fdb334c37bf64cfaf0115", "title": "Support-set bottlenecks for video-text representation learning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "8725c167e385aeae93f90aceefb69819f84238c4", "title": "Survey of Machine Learning Accelerators"}, {"paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5", "title": "Hopfield Networks is All You Need"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66831f683141c11ed7e20b0f2e8b40700740c164", "title": "Vggsound: A Large-Scale Audio-Visual Dataset"}, {"paperId": "b430a5384c82beb6102106fbea0a134425a08c23", "title": "Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b", "title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"}, {"paperId": "9de403a58395a1b56bfceee6e009788c43db6d08", "title": "End-to-End Learning of Visual Representations From Uncurated Instructional Videos"}, {"paperId": "470dbb7572323a192c5b630d4b51eb59469e9f02", "title": "Self-Supervised Correspondence in Visuomotor Policy Learning"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "f3fbcfbb396f6c0391674c8637a373b729e5e531", "title": "Compositionality Decomposed: How do Neural Networks Generalise?"}, {"paperId": "7c9de67cc76aeecddcd07e8898acea3ef4eba738", "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "b16eeb1e975e8e6ea9450c78fd12da05cfd1375f", "title": "Use What You Have: Video retrieval using representations from collaborative experts"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "2a687609ac1cecb9b20ba52d4f5d72ba14e0eaf2", "title": "Sampled Softmax with Random Fourier Features"}, {"paperId": "3245b2436d9f39b9ecadd275a815d09c2e130497", "title": "Unsupervised Learning of Object Keypoints for Perception and Control"}, {"paperId": "792829f263a523eedf1a8748ec23d25cf664c2b4", "title": "What Would You Expect? Anticipating Egocentric Actions With Rolling-Unrolling LSTMs and Modality Attention"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "e525ba29497bab9b530ea7b056dd0128be22c48a", "title": "Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning"}, {"paperId": "5ac18d505ed6d10e8692cbb7d33f6852e6782692", "title": "The Open Images Dataset V4"}, {"paperId": "6a976c123037b138946f6767e1aa0de84b1682d4", "title": "Dual Encoding for Zero-Example Video Retrieval"}, {"paperId": "fa1723b216b1f41b085b62b450b7b0bd9f2fd281", "title": "In the Eye of Beholder: Joint Learning of Gaze and Actions in First Person Video"}, {"paperId": "8befcd91c24038e5c26df0238d26e2311b21719a", "title": "A Joint Sequence Fusion Model for Video Question Answering and Retrieval"}, {"paperId": "26eff1aa014ed0f19fbda0ac6b554f8ba9881f25", "title": "Deepdiary: Lifelogging image captioning and summarization"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "9dbca9da6a72ba3739813288b677888a6cf76272", "title": "Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval"}, {"paperId": "554626c58303f9fc6fc0c64595b1ab041147371f", "title": "Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos"}, {"paperId": "5c7e5248d9eb7f373f10277410bf8506160907ea", "title": "All-optical machine learning using diffractive deep neural networks"}, {"paperId": "fc50c9392fd23b6c88915177c6ae904a498aacea", "title": "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "031af500679ae544d0fc614f938de45a07c87c82", "title": "Predicting Visual Features From Text for Image and Video Caption Retrieval"}, {"paperId": "0b2790c2c1e23e5535421696e1ec763bbc651bbd", "title": "To Index or Not to Index: Optimizing Exact Maximum Inner Product Search"}, {"paperId": "e85327e43f8b7e052a52ff9ee6b845cc0bee990d", "title": "First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "1be1df8f3412961eaf4b7a9ca0023b06ca2c7344", "title": "Summarization of Egocentric Videos: A Comprehensive Survey"}, {"paperId": "3f3610711c9000106b56b8d1d5941c49ee3a5f54", "title": "First-Person Activity Forecasting with Online Inverse Reinforcement Learning"}, {"paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc", "title": "Unsupervised Pretraining for Sequence to Sequence Learning"}, {"paperId": "d8abf01fce0d44665949e7a73716fff7731fa6da", "title": "Places: An Image Database for Deep Scene Understanding"}, {"paperId": "444eba373d46c9f7d58cad74989ec9109b0d5219", "title": "Going Deeper into First-Person Activity Recognition"}, {"paperId": "f742fa8960cd04e21ba13200ff7fa46acc8e3817", "title": "A Review on Automatic Speech Recognition Architecture and Approaches"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "5e84fd5c73dfeea9d51e1cf59ea6f8ecf2097603", "title": "Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "932ac3707e1ed84ab67526692a1ef8f064f24ab5", "title": "Anticipating Visual Representations from Unlabeled Video"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "ec3d9f304f173362007f5c2e3c6c616d24b85c9f", "title": "Predicting Important Objects for Egocentric Video Summarization"}, {"paperId": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "title": "Pooled motion features for first-person videos"}, {"paperId": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "title": "Deep visual-semantic alignments for generating image descriptions"}, {"paperId": "258986132bf17755fe8263e42429fe73218c1534", "title": "CIDEr: Consensus-based image description evaluation"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "47990fefbad009186adc7da2f4218815f9d74a02", "title": "Mixture of experts: a literature survey"}, {"paperId": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da", "title": "The Stanford CoreNLP Natural Language Processing Toolkit"}, {"paperId": "6dbffa57b3c6c5645cf701b9b444984a4b61bb57", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "1109b663453e78a59e4f66446d71720ac58cec25", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"}, {"paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238", "title": "Visualizing and Understanding Convolutional Networks"}, {"paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "b8de958fead0d8a9619b55c7299df3257c624a96", "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"}, {"paperId": "aa4dd3eec733045c7d5f43535f201a12f71ef5f5", "title": "First-Person Activity Recognition: What Are They Doing to Me?"}, {"paperId": "5a90a0d0d4f349c2c330d9d137baf5076ee3f717", "title": "Pixel-Level Hand Detection in Ego-centric Videos"}, {"paperId": "616a23ebf79e35033c84797993943013c5dde5a0", "title": "Discovering important people and objects for egocentric video summarization"}, {"paperId": "9e81caf9dd31b893ebbee3970c312619b7eac7bf", "title": "Detecting activities of daily living in first-person camera views"}, {"paperId": "b72756c4d4237a857e1a764c876e82a82edd128c", "title": "Max-Margin Early Event Detectors"}, {"paperId": "bd5fc28c7356915ec71abafbe86b7596c60720aa", "title": "The conference paper"}, {"paperId": "bfc71c97f21b354ddfe69d17115e52db4fd4dce8", "title": "Understanding egocentric activities"}, {"paperId": "76be79c45335db6d08efcde7843ae298765d4d63", "title": "Human activity prediction: Early recognition of ongoing activities from streaming videos"}, {"paperId": "f4093c2270284b4ec9b154c7d06791bf99c7b942", "title": "KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera"}, {"paperId": "681f6e3defcbdf931de975d720e185b2193ff343", "title": "Unsupervised and Transfer Learning Challenge: a Deep Learning Approach"}, {"paperId": "a78273144520d57e150744cf75206e881e11cc5b", "title": "Multimodal Deep Learning"}, {"paperId": "8848d1abd31873594fc372e0022789f153112174", "title": "Fast unsupervised ego-action learning for first-person sports videos"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "1aaf8e1e8164e8968f693a100fee4343580b1d61", "title": "Temporal segmentation and activity classification from first-person sensing"}, {"paperId": "843959ffdccf31c6694d135fad07425924f785b1", "title": "Extracting and composing robust features with denoising autoencoders"}, {"paperId": "b3852f0113fcf8a3913c55ae92393ae6ccde347e", "title": "Self-taught learning: transfer learning from unlabeled data"}, {"paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd", "title": "Greedy Layer-Wise Training of Deep Networks"}, {"paperId": "8978cf7574ceb35f4c3096be768c7547b28a35d0", "title": "A Fast Learning Algorithm for Deep Belief Nets"}, {"paperId": "04e52944c3ad1cf49a5d9f65958d98939cde557d", "title": "Video summarization: methods and landscape"}, {"paperId": "dde691805cfa7d6f1bb88c7411c1c3377b6cdc67", "title": "Lifelong Learning Algorithms"}, {"paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning"}, {"paperId": "f6d8a7fc2e2d53923832f9404376512068ca2a57", "title": "Hierarchical Mixtures of Experts and the EM Algorithm"}, {"paperId": "07cd498aacfb4d39fa2e0e8d8a9c8ad881257300", "title": "Prompt Engineering for Text-Based Generative Art"}, {"paperId": null, "title": "Zeng et al., 2020) (inspired by CLIPort (Shridhar et al., 2022) for open vocabulary pick-and-place). The full prompt used as context to the LLM for multi-step planning"}, {"paperId": null, "title": "Large pretrained models on multimodal sentiment analysis. In Artificial Intelligence in China"}, {"paperId": "dae408a803836d8a783de20d6b6b8438625a3fa3", "title": "CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval"}, {"paperId": "179f1486db594a9ed9d53dc1894f3b718dd483e4", "title": "Concadia: Tackling image accessibility with context"}, {"paperId": "ede4ed9442c3688783c0411370dd9e19e32d6c80", "title": "Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer"}, {"paperId": null, "title": "2022b)). We observe that sampling with temperature performs comparably (slightly better for most metrics) versus both alternatives"}, {"paperId": "ef36d6c3579d37d3cd3e1ad52a002245d1a3e89c", "title": "Use What You Have: Video Retrieval Using Representations From Collaborative Experts"}, {"paperId": null, "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "0500e4208067e4f9e82d29159cd47ac0ca5be85a", "title": "Learning Visual Affordances for Robotic Manipulation"}, {"paperId": null, "title": "2020), which aligns with our observations. Relatively less research has been carried out specifically on egocentric image captioning (Fan et al., 2018)"}, {"paperId": "3a9d0876fb73085cc7dd394a485b64424432b1fb", "title": "Unsupervised Testing Strategies for ASR"}, {"paperId": "5c92bd27d4e5d95bceb3adbfcfc28358d99f6b3f", "title": "Research Showcase @ CMU"}, {"paperId": null, "title": "brown bowl\") objects = [\"orange bowl"}, {"paperId": null, "title": "Step 2. robot.pick and place(\"blue block"}, {"paperId": null, "title": "Step 1. robot.pick_and_place('red block', 'top left corner') Step 2. robot.pick_and_place('red block', 'top right corner') Step 3. robot.pick_and_place('red block"}, {"paperId": null, "title": "Speech-to-text: Automatic speech recognition | google cloud"}, {"paperId": null, "title": "AM: went to grocery store to buy orange juice, chocolate, and bread. 8:15 AM: I went to gas station to fill up the vehicle tank. 8:30 AM: drove back home and left the groceries in the kitchen"}, {"paperId": null, "title": "# ok now sort the remaining blocks in the same way. Step 1. robot.pick and place(\"blue block"}]}