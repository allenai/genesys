{"paperId": "2d629fa3d687cfc453c6b61909c46983ebea0323", "abstract": "Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.07661", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self,Noncausal cross, and causal cross attentions, and sheds light on the fundamental problems of efficient attentions."}, "embedding": {"model": "specter_v2", "vector": [0.26850220561027527, 0.1380559206008911, 0.19233405590057373, -0.2035108059644699, -0.2333855926990509, 0.005192609038203955, 0.5635865926742554, -0.3007398247718811, -0.47252896428108215, 0.07701190561056137, 0.7082646489143372, 0.10504414886236191, 0.35254690051078796, 0.10866578668355942, -0.17404228448867798, -0.1119316816329956, -0.9512974619865417, 0.10044297575950623, 0.1237802803516388, -0.4465658366680145, 0.301556795835495, -0.8121241331100464, -1.0093472003936768, 0.039441052824258804, 0.39430347084999084, 0.5985892415046692, 0.5636329650878906, 1.1214083433151245, -0.33326175808906555, 0.48848533630371094, 0.22956089675426483, -0.43582072854042053, 0.05592384934425354, -0.13465197384357452, -0.536760687828064, -0.37158510088920593, 0.44170889258384705, -0.30889788269996643, -0.557369589805603, 0.6826372742652893, 0.15650956332683563, 0.2949748933315277, 0.41250738501548767, -0.5875415205955505, -0.3693835735321045, 0.8476426005363464, 0.43714094161987305, 0.9616742730140686, -0.16367940604686737, -0.21509476006031036, 1.7824163436889648, -1.12570321559906, 0.08434196561574936, 1.3584851026535034, 0.4610898494720459, 0.443670392036438, -0.24348120391368866, -0.9493367075920105, 0.692533016204834, 0.3206367492675781, -0.5665711760520935, -0.25291410088539124, 0.4297822415828705, -0.038713034242391586, 1.6782547235488892, -0.2960668206214905, 0.38395941257476807, 1.0870248079299927, 0.3728044033050537, 1.1329056024551392, -0.23720857501029968, -0.6318636536598206, -0.044507939368486404, -0.5419732928276062, 0.7199071645736694, 0.6155918836593628, -0.5811824202537537, 0.4888874590396881, -0.8478819131851196, -0.04612681642174721, 0.4404347240924835, -0.2438967376947403, -0.07002060115337372, 0.06751490384340286, -0.568813145160675, 0.6831488013267517, 0.4323119819164276, 0.7816398739814758, -0.320710152387619, 0.8370250463485718, 0.7244482040405273, 0.21966999769210815, -0.24973896145820618, 0.3102405071258545, 0.35328373312950134, 0.4170278012752533, -0.6209337711334229, 0.14745430648326874, -0.2561156451702118, 1.1918264627456665, -0.04354420676827431, 0.7575031518936157, -0.521004319190979, 0.2748870253562927, 1.341772437095642, -0.059775397181510925, 0.41054248809814453, -0.9559782147407532, 0.03645629808306694, -0.6078002452850342, 0.026798125356435776, -0.8144164681434631, 0.03342759236693382, -0.37414035201072693, -0.8827695250511169, -0.7747031450271606, -0.22362618148326874, 0.38244545459747314, -0.432754248380661, 0.7805271744728088, -0.5975557565689087, 0.48008453845977783, -0.01803421787917614, 0.28785690665245056, 0.5759634375572205, 0.6534405946731567, 0.37465229630470276, 0.015172861516475677, 0.8451416492462158, -0.959474503993988, -0.41802680492401123, -1.118894338607788, 0.28739041090011597, -0.41737106442451477, 0.5043176412582397, -0.18630972504615784, -1.192247986793518, -1.1963611841201782, -0.9677823781967163, -0.0034443074837327003, -0.2446700781583786, -0.33948513865470886, 0.8691056966781616, -0.10870309174060822, -0.9529716968536377, 0.5785834193229675, -0.1234792098402977, -0.18084761500358582, 0.35996875166893005, -0.11239343136548996, 0.19307933747768402, -0.18475966155529022, -1.3283382654190063, 0.27845874428749084, 0.04838065430521965, -0.36855348944664, -0.5615250468254089, -0.5669589042663574, -1.2537227869033813, -0.11361249536275864, 0.17270918190479279, -0.30169251561164856, 1.1434807777404785, -0.22708451747894287, -0.8160815238952637, 0.6039011478424072, -0.5167961716651917, -0.10335512459278107, 0.0996306762099266, -0.23548199236392975, -0.6692361235618591, -0.5719679594039917, 0.20177458226680756, 0.7189452052116394, 0.19344662129878998, -0.2577785551548004, -0.09415064752101898, 0.10440446436405182, -0.33681681752204895, -0.38190847635269165, -0.11571433395147324, 0.9895061254501343, -0.19618262350559235, -0.44031792879104614, -0.292514443397522, 0.6328678131103516, -0.012596473097801208, -0.30987629294395447, -0.4893122613430023, -1.0405571460723877, 0.8174921274185181, 0.020037630572915077, 1.0255389213562012, -0.7172582745552063, -0.9724122881889343, -0.6271551251411438, -0.3445741832256317, -0.30951106548309326, -0.9797725677490234, 0.5604705214500427, -0.5274279713630676, 0.20887547731399536, -0.11005835980176926, -0.917532205581665, -0.23907944560050964, -0.1403481513261795, -0.9109835028648376, -0.2107321172952652, 0.11170574277639389, 1.1968094110488892, -1.074226975440979, -0.37805116176605225, -0.03356939181685448, 0.16550397872924805, -0.9655409455299377, 1.1798850297927856, -0.36333179473876953, 0.2551797926425934, 0.01301071047782898, -0.3552592694759369, -0.2434236854314804, -0.6667081117630005, 0.01484835334122181, -0.5364339351654053, -0.19097526371479034, 0.5702189803123474, -0.041550327092409134, 1.2845717668533325, -0.23012930154800415, 1.0714595317840576, -0.3455100953578949, -0.5487926602363586, 0.33963868021965027, 0.24635320901870728, -0.26443809270858765, -0.773222804069519, 0.3885250985622406, 0.044888053089380264, -0.3040034770965576, 0.21367666125297546, 0.5636075139045715, 1.14276123046875, -0.2997497618198395, 0.21128493547439575, 0.5201553702354431, -0.054589804261922836, 0.6582816243171692, 0.20983737707138062, 0.6498809456825256, 0.5235374569892883, 0.7270079851150513, -0.45600396394729614, 0.40439796447753906, -0.6354944109916687, -0.04901072010397911, 0.3514487147331238, 0.40433797240257263, 0.8847573399543762, 0.19252143800258636, -0.8416275382041931, -0.7341175079345703, 0.054071031510829926, 0.7658984661102295, 1.4527883529663086, 0.12951049208641052, -0.06306722015142441, -0.8206623792648315, -0.04915398731827736, -0.5748489499092102, 0.4213215410709381, -0.5815593004226685, -0.13048739731311798, -0.7740266919136047, -0.6488016247749329, 0.9515264630317688, 0.5047126412391663, 1.1380589008331299, -1.1043179035186768, -0.6743362545967102, -0.04213443771004677, 0.11999660730361938, -0.8061354160308838, -1.0258746147155762, -0.010692508891224861, -0.2912887930870056, -0.4057798385620117, -0.09237819910049438, -0.03360011428594589, -0.5061150193214417, -0.43279585242271423, 1.023167610168457, -0.5584912896156311, -0.3675885796546936, 0.3122856020927429, 0.8123326897621155, -0.6647951006889343, -0.7470925450325012, 0.13923293352127075, -0.1296505182981491, -0.10641692578792572, 0.22986982762813568, 0.226302370429039, -0.15600177645683289, 0.31126585602760315, -0.10057815909385681, -0.007497482933104038, 0.021177863702178, 0.2838602364063263, 0.5498479604721069, -0.5388316512107849, 0.10157217085361481, -0.49437636137008667, 1.0239766836166382, 0.3153012990951538, -0.5737369656562805, 0.37568220496177673, -0.6931473016738892, -0.4396713078022003, 0.14612020552158356, -0.6674873232841492, -0.1918564736843109, -1.239692211151123, 0.4327929615974426, -0.45160624384880066, -0.34034857153892517, -0.08226951211690903, 0.1831323504447937, 0.40501242876052856, 0.3479878008365631, 0.6182603240013123, 0.17243070900440216, 0.19044926762580872, 0.23751772940158844, -0.9019777178764343, 0.5836015939712524, 0.28190669417381287, -0.3559347689151764, 0.036314282566308975, -0.07792463898658752, -1.0401861667633057, -0.5809550881385803, -0.27640441060066223, -0.24373486638069153, -0.3608306348323822, 0.6781530976295471, -0.27831020951271057, -1.2238850593566895, 0.12684574723243713, -1.515407919883728, -0.4647502899169922, 0.19280336797237396, -0.2853509187698364, -0.5029757022857666, -0.9978805184364319, -0.8818715214729309, -0.6516645550727844, -0.5814460515975952, -1.0629019737243652, 0.31679630279541016, 0.025610730051994324, -0.6635729074478149, -0.5870535969734192, 0.1792834997177124, -0.4723411500453949, 0.9845228791236877, -0.38775742053985596, 0.6930021047592163, -0.2122488021850586, -0.49518173933029175, 0.1437138468027115, 0.0964038148522377, 0.33906951546669006, -0.08504835516214371, 0.2213204801082611, -1.0617315769195557, 0.28901106119155884, -0.1465526968240738, -0.09980795532464981, 0.0918642207980156, 0.4288887679576874, 0.7069913744926453, 0.15074634552001953, -0.39104124903678894, -0.0029892707243561745, 1.0876351594924927, 0.14414317905902863, 0.2748945951461792, 0.31327393651008606, 0.9067705869674683, 0.36531978845596313, -0.2522836923599243, 0.28797125816345215, 0.4381944239139557, 0.2915339469909668, 0.47183218598365784, 0.13057267665863037, -0.1278958022594452, -0.42466726899147034, 0.4183565676212311, 1.7021379470825195, -0.07890273630619049, 0.0460984893143177, -1.1956902742385864, 0.93906170129776, -1.0186140537261963, -1.2193535566329956, 0.6457229256629944, 0.5754376649856567, 0.20108474791049957, -0.48088812828063965, -0.4665316343307495, 0.004448023624718189, 0.8033725023269653, 0.5813884735107422, -0.4710700809955597, -0.6431334614753723, -0.30239009857177734, 0.109652079641819, -0.3747733533382416, 0.6531433463096619, -0.331951379776001, 0.43679583072662354, 15.158137321472168, 0.6181293725967407, -0.07881657779216766, 0.7210968732833862, 1.0097850561141968, 0.36696311831474304, -0.23083555698394775, -0.13573400676250458, -1.083572506904602, 0.0429416261613369, 1.102867603302002, 0.13941562175750732, 0.7014464735984802, 0.032840900123119354, -0.08326175063848495, 0.46716225147247314, -0.6197048425674438, 0.6195145845413208, 0.5015566349029541, -1.3071389198303223, 0.4726763069629669, 0.20066675543785095, 0.30921047925949097, 0.6269335150718689, 0.5934120416641235, 0.5213991403579712, 0.6063985228538513, -0.27844715118408203, 0.5073940753936768, 0.28823480010032654, 0.8844420909881592, -0.057568907737731934, 0.11706136167049408, 0.25530168414115906, -1.0363757610321045, -0.1950630098581314, -0.5949827432632446, -1.139883279800415, 0.2783604562282562, -0.15464454889297485, -0.0730946809053421, -0.5187825560569763, 0.024952344596385956, 0.7757045030593872, 0.22848601639270782, 0.33484387397766113, -0.24474969506263733, 0.49237266182899475, 0.44703182578086853, -0.17440572381019592, 0.49912354350090027, 0.39345666766166687, 0.5644649267196655, -0.0693095475435257, 0.16326725482940674, -0.069743312895298, -0.12938298285007477, 0.618317186832428, -0.17554521560668945, -0.07589508593082428, -0.5398642420768738, -0.2828724980354309, -0.0743180438876152, 0.347116082906723, 0.5764521360397339, 0.4544844329357147, -0.2951723635196686, 0.1558249294757843, 0.4717662036418915, 0.3724020719528198, -0.6533157825469971, -0.18916019797325134, 0.4001966416835785, -0.7085225582122803, -0.050492964684963226, 0.4907410442829132, -0.1741202026605606, -0.18852823972702026, -0.9773519039154053, -0.3549639582633972, 0.3876703083515167, -1.3772952556610107, -0.944104790687561, 1.1985374689102173, -0.19038759171962738, -0.13584257662296295, 0.2515535354614258, -0.6768800616264343, -0.745559811592102, 0.278653085231781, -1.187439203262329, -0.7104538679122925, -0.32114309072494507, -0.005272385664284229, 0.1293748915195465, -0.35735294222831726, 0.9812337160110474, 0.14564989507198334, -0.19879843294620514, -0.058623701333999634, -0.3894297182559967, 0.08160775154829025, -0.20218224823474884, -0.7257519960403442, 0.8504196405410767, 0.22913357615470886, -0.0336264930665493, 0.319986492395401, 0.08079766482114792, 0.214411661028862, -0.36303386092185974, -0.07603752613067627, 0.6162331104278564, -0.8029466271400452, -0.02203337289392948, -0.8431381583213806, -0.9693875312805176, 0.5067762732505798, 1.0694377422332764, -0.26297301054000854, 0.08633299916982651, -0.10210511088371277, -0.6619532704353333, -0.3037826418876648, -0.2655447721481323, 0.4068790078163147, 0.7471473813056946, -0.8048613667488098, -0.4402904808521271, -0.652826726436615, 0.8272594809532166, -0.6110692024230957, -0.21378383040428162, -0.11161155998706818, 0.33325523138046265, -0.543383002281189, 0.8140875697135925, -0.4846438467502594, 0.8270328640937805, 0.8515908122062683, -0.31827405095100403, -0.5804804563522339, -0.038617171347141266, -1.209996223449707, 0.4296516180038452, 0.4468923807144165, 0.45117419958114624, -0.6545159220695496, 0.0027121154125779867, 0.35114413499832153, 0.1163882240653038, -0.4059467613697052, -0.9600189924240112, -0.3505480885505676, -0.1255464255809784, -0.7257574796676636, 0.5810426473617554, -0.1439024657011032, -0.3160981833934784, 0.5272191762924194, 0.1885695904493332, 0.22862756252288818, -0.17796939611434937, -0.4241044819355011, 0.2831731140613556, -0.030231349170207977, 0.0896974429488182, -0.8171463012695312, -0.4321167767047882, -1.333182454109192, 0.17273981869220734, -0.8798620104789734, -0.05449880659580231, -0.797291100025177, -0.31786948442459106, 0.20411893725395203, -0.3531267046928406, -0.09383148699998856, 0.4835105836391449, -0.3853972256183624, -0.4696062505245209, -0.6153614521026611, -0.6466315388679504, 0.964272677898407, 0.987752377986908, -1.101527452468872, 0.20311608910560608, -0.24668888747692108, 0.1318468302488327, 0.45481324195861816, 0.3408922255039215, -0.512713611125946, -0.6632413268089294, -1.1661231517791748, 0.28734105825424194, -0.16397760808467865, 0.1703926920890808, -0.9868331551551819, 0.7179306149482727, 0.2581370174884796, -0.2503785789012909, -0.10285896062850952, 0.43053922057151794, -0.8577801585197449, -0.39714714884757996, 0.42059269547462463, -0.8449941277503967, 0.7216272950172424, 0.20599165558815002, -0.36988165974617004, -0.3934437036514282, 1.0016589164733887, 0.17271606624126434, -0.9483597874641418, -0.6785557270050049, 0.5350138545036316, -0.43473300337791443, 0.5128486156463623, 0.012580054812133312, -0.3000207543373108, -1.166727900505066, -0.42845308780670166, 0.002174868481233716, 0.3894025981426239, -0.671654462814331, 1.0493147373199463, 0.5089977979660034, -1.1262789964675903, 0.09244699776172638, 0.3500676453113556, -0.02673916518688202, -0.17941002547740936, 0.5513557195663452, 0.3240584433078766, -0.011496154591441154, 0.5543939471244812, 0.031105265021324158, 0.029688309878110886, -0.9415724277496338, 0.19608724117279053, 0.9932169914245605, -0.44073212146759033, -0.09046497195959091, 0.8556727170944214, 0.30358538031578064, -0.879749059677124, 0.09590993076562881, -0.8827239871025085, -0.8039259314537048, -0.2755277752876282, 0.6082050800323486, 0.5910251140594482, -0.681864857673645, -0.22360526025295258, -0.42710286378860474, 0.5850617289543152, 0.11736086755990982, -0.5415322184562683, 0.41686898469924927, -0.20378237962722778, -0.3245871663093567, 0.8780026435852051, 0.7270422577857971, -0.6171981692314148, -0.8547552824020386, -0.3846091330051422, -0.294177383184433, 0.17294463515281677, -0.10185962915420532, -0.18010304868221283, -0.5821255445480347, 1.2781691551208496, 0.3543662428855896, 0.4660179615020752, 0.07868818938732147, -0.22362050414085388, 0.010032295249402523, 0.6540985107421875, 0.05060059577226639, -0.3176960349082947, -0.06311550736427307, 1.3804707527160645, 1.8441340923309326, -0.7190715670585632, -0.15534639358520508, -0.050007980316877365, -0.7441806793212891, 0.6894320845603943, 0.6148577928543091, -0.265743613243103, 0.7366437911987305, -0.18255949020385742, 0.15009582042694092, 0.1260581761598587, -1.2590917348861694, -0.0791255459189415, 0.6684801578521729, 1.009521484375, 0.45214399695396423, -0.006625456269830465, 0.37069785594940186, 0.6296958327293396, 0.1354217827320099, -0.07565615326166153, 0.42662590742111206, -0.018754472956061363, -0.07117152214050293, 0.2139555960893631, 0.19493040442466736, 0.17702527344226837, -0.7325406670570374, -0.776045024394989, 0.3836080729961395, 0.8349905610084534, -0.10066622495651245, 0.7055560350418091, 1.0266977548599243, 0.23626188933849335, 0.512403666973114, 0.16234958171844482, 0.22522275149822235, -0.106107696890831, -0.1253308802843094, 0.10055583715438843, -0.5775713920593262, -0.35450583696365356, -0.41469621658325195, -0.6839175820350647, -0.10115646570920944, 0.03628590330481529, 0.08547502011060715, 0.0015782710397616029, 0.13084189593791962, 1.1119892597198486, 0.6317666172981262, 0.7791025638580322, -0.4862011671066284, -0.5502681136131287, -0.1344117820262909, -0.9372795224189758, 0.31809839606285095, -0.6826084852218628, 0.15718358755111694, -0.21643880009651184, -0.08305072784423828, -0.09166044741868973]}, "authors": [{"authorId": "27672597", "name": "Jinchao Zhang"}, {"authorId": "2119327053", "name": "Shuyang Jiang"}, {"authorId": "2093485", "name": "Jiangtao Feng"}, {"authorId": "1633166807", "name": "Lin Zheng"}, {"authorId": "47648549", "name": "Lingpeng Kong"}], "references": [{"paperId": "e6402720d224ebc0d533a6c10f3b8596dd9715ab", "title": "PARAGEN : A Parallel Generation Toolkit"}, {"paperId": "732e3faec4e5be4d144256f2c379b9dc49f0b227", "title": "Efficient Long-Text Understanding with Short-Text Models"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "2d82ee05b132d4681c3bd517afc17d608fe6e525", "title": "Simple Local Attentions Remain Competitive for Long-Context Tasks"}, {"paperId": "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4", "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "4b0541eccd8f98852d6807a14fbac17f775c7b40", "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m Method"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "title": "ABC: Attention with Bounded-memory Control"}, {"paperId": "80da612e1831b8c11539180871843cff6dfaac90", "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "c07651110d3b98b63607557b57808d15d99013dd", "title": "ProteinBERT: a universal deep-learning model of protein sequence and function"}, {"paperId": "e32a12b14e212506115cc6804667b3d8297917e1", "title": "Poolingformer: Long Document Modeling with Pooling Attention"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "8a1ea7b6e7e834d146ad782be5d63f57f806a9cc", "title": "Image Super-Resolution via Iterative Refinement"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "df6e4e70af7b3f0a7eb630ed8f36538e6258bc4b", "title": "LazyFormer: Self Attention with Lazy Update"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "baa8f524c82735f174b8d1ab512ac5750146d67e", "title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "c828f4bf1a752700dd2c4a96fdd08ba938cda43d", "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "a9d0fb74e6f23b7e02b0c0a0f5271a7db0a6eb13", "title": "Switching Poisson Gamma Dynamical Systems"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "1623d6ffb6efd94d21537db2b96b91a196842aef", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "4b7e85c3d036b23f63c3db628e97948377e44f75", "title": "Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "9c885cecb375bf5080e2c10d8a22748ba59f056b", "title": "SemSUM: Semantic Dependency Guided Neural Abstractive Summarization"}, {"paperId": "016a3ba7adcae71f5a23ed2663d8062ae1da63e6", "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "0b9476177e70d281d4a52aa60809b6a15d2a7523", "title": "PF-Net: Point Fractal Network for 3D Point Cloud Completion"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "82c11bded236d53e572a4c9a5b420075b5b6ad64", "title": "Pearson Correlation Coefficient"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "2a02c967dd9848064bca0aa69ea6c75b3765d0ee", "title": "Low-Rank and Locality Constrained Self-Attention for Sequence Modeling"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "b491cf2acfc34b075978b1c31e74ac8a8f372ab8", "title": "Informer: irregular traffic detection for containerized microservices RPC in the real world"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "8a0d25866c1ec9950f6a03cd56792ff1d34b1826", "title": "A review on deep learning techniques for 3D sensed data classification"}, {"paperId": "cc27ec53160d88c25fc5096c0df65536eb780de4", "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"}, {"paperId": "b05fb9a7d5e3b596325dec6795ce49ec3ac14907", "title": "What Do Single-View 3D Reconstruction Networks Learn?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "fedbcfe03e44f4f9610e2b2164c5673516543f38", "title": "Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "ceb2ebef0b41e31c1a21b28c2734123900c005e2", "title": "A Style-Based Generator Architecture for Generative Adversarial Networks"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "a364a2b6bb97fcb4207468e6500cf3042bbf8d07", "title": "Speech Recognition"}, {"paperId": "9f2dd5cc190fc713f1339fca838a5537931744f8", "title": "Neural Speech Synthesis with Transformer Network"}, {"paperId": "697e110df76fe33e232f019d7e44097af3572abd", "title": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms"}, {"paperId": "6e45251b16cd423f3c025f004959c6d2b26efab0", "title": "Accelerating Neural Transformer via an Average Attention Network"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454", "title": "Non-Autoregressive Neural Machine Translation"}, {"paperId": "744fe47157477235032f7bb3777800f9f2f45e52", "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "c20196183fd7d1de3171432edf1d1a987e671678", "title": "A New Iterative Method for Finding Approximate Inverses of Complex Matrices"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "eae2e0fa72e898c289365c0af16daf57a7a6cf40", "title": "Image quality assessment: from error visibility to structural similarity"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "2021) Efficient Attention"}, {"paperId": null, "title": "Linear unified nested attention"}, {"paperId": null, "title": "Linear Transformer (Katharopoulos et al., 2020a) Big Bird"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Startransformer. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": null, "title": "The lj speech dataset. https://keithito"}, {"paperId": null, "title": "Then we use python scipy library to fit the curve to these data points and obtain a, b, c and e, f for each attention"}, {"paperId": null, "title": "we compute the intersection points of y = ax 2 + bx + c and y = ex + f . This may produce two intersection points"}, {"paperId": null, "title": "67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference"}, {"paperId": null, "title": "Comprehensive Attention Benchmarking on"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}