{"paperId": "319b52f0984600119cf0fe1004814bf5c7eb39fe", "abstract": "With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning, domain-adaptive fine-tuning, domain-adaptive fine-tuning, few-shot learning, knowledge distillation, multi-task learning, parameter-efficient fine-tuning, and dynamic fine-tuning."}, "embedding": {"model": "specter_v2", "vector": [-0.20958521962165833, 0.5732603073120117, -0.44723939895629883, 0.25030142068862915, -0.4126494824886322, -0.275816947221756, 0.7599080204963684, -0.4203745126724243, -0.5594419240951538, -0.006920058745890856, 0.21350586414337158, 0.12233562767505646, 0.28907299041748047, 0.3860234320163727, 0.11153680086135864, 0.282840758562088, -0.5967234373092651, 0.9737961292266846, 0.32035836577415466, -0.7192617654800415, -0.5289726257324219, -1.195668339729309, -0.6733301281929016, -0.01624065637588501, 0.4988599121570587, 0.37479788064956665, 0.48687127232551575, 0.8034480810165405, -0.5843476057052612, 0.006097664590924978, 0.4194270074367523, -0.5230613350868225, 0.2185310572385788, 0.026451298967003822, -0.36708158254623413, -0.2660806477069855, 0.15120640397071838, -0.3762786388397217, -0.2716420888900757, 0.7334254384040833, 0.09990610182285309, 0.35102975368499756, 0.5427328944206238, -0.4456692337989807, -0.4554693400859833, 0.7355414628982544, 0.6522293090820312, 0.680335521697998, -0.3945581316947937, -0.47391557693481445, 1.1927739381790161, -1.33146071434021, 0.1447152942419052, 1.8172085285186768, 0.4570208489894867, 0.7365133762359619, -0.517137885093689, -0.8590666055679321, 0.6509086489677429, 0.22728005051612854, -0.9092704057693481, -0.27098608016967773, -0.1762976199388504, -0.1579737663269043, 1.974401831626892, -0.4132499098777771, -0.18461275100708008, 0.7908689975738525, -0.0014262570766732097, 1.1737213134765625, -0.2924305498600006, -0.9248623251914978, -0.25044459104537964, 0.1956890970468521, 0.07046878337860107, 0.8280255198478699, -0.8622543811798096, 0.36490175127983093, -0.7552410960197449, -0.3863508403301239, 0.39151719212532043, -0.5260189175605774, 0.016685938462615013, -0.12249716371297836, -0.3444134294986725, 1.0751062631607056, -0.09962344914674759, 0.7367702126502991, 0.14611534774303436, 0.5539377927780151, 0.7080174684524536, 0.5869095325469971, 0.2713056206703186, 0.5749738216400146, -0.36518988013267517, 0.033452264964580536, -0.664063036441803, 0.3545670211315155, 0.4369456171989441, 0.9113332629203796, -0.2774689197540283, -0.040155861526727676, -1.231592059135437, 0.9294700026512146, 1.256736159324646, -0.18107227981090546, 0.6722826957702637, -1.126924753189087, 0.49590131640434265, -0.8234556317329407, 0.3547320067882538, -0.25406646728515625, -0.1458083689212799, 0.03443313017487526, -0.683413028717041, -1.2610732316970825, -0.23144102096557617, -0.12513482570648193, -0.5382658243179321, 0.8342450857162476, -0.09489084780216217, 0.09401138871908188, -0.20397807657718658, 0.48516225814819336, 0.48708972334861755, 0.6617938876152039, 0.6107727289199829, -0.11247788369655609, 1.0690171718597412, -1.1588506698608398, -0.7884059548377991, -1.0446792840957642, 0.8403233885765076, -0.29068121314048767, 0.5858472585678101, -0.1576477736234665, -1.1418408155441284, -0.8287433981895447, -0.5577307939529419, -0.16430263221263885, -0.29730916023254395, 0.36315372586250305, 0.8077082633972168, 0.41511136293411255, -0.7855405211448669, 0.3195686340332031, 0.02858627401292324, -0.38036060333251953, -0.016043351963162422, -0.0020538801327347755, 0.27463963627815247, -0.39018577337265015, -1.8665295839309692, 0.5537837743759155, 0.43690893054008484, -0.8564631342887878, -0.6126271486282349, -0.6134549975395203, -0.7361509799957275, -0.12410583347082138, 0.6609659194946289, -0.7366704940795898, 1.6019314527511597, -0.23755526542663574, -1.7493144273757935, 0.6096017956733704, -0.1827450543642044, 0.1568067967891693, 0.772685706615448, 0.026985254138708115, -0.68989098072052, -0.5559300780296326, -0.32095208764076233, 0.7725343704223633, 0.7910407781600952, 0.14602401852607727, -0.2316778004169464, 0.39437031745910645, 0.20663891732692719, 0.2309689223766327, -0.2940475642681122, 0.9651064872741699, -0.819060742855072, -0.20902343094348907, -0.027452392503619194, 0.6448073387145996, -0.13857638835906982, -0.1313336193561554, -0.2519845962524414, -1.1850885152816772, 0.6917251348495483, 0.03674617037177086, 1.3146865367889404, -0.7692509293556213, -0.5062747597694397, -0.27665388584136963, 0.033895619213581085, 0.2392570823431015, -1.1971417665481567, 0.7869848608970642, 0.2040819525718689, 0.4013844430446625, 0.08679454028606415, -1.402040719985962, 0.37019026279449463, -0.15196405351161957, -0.29767054319381714, -0.48369061946868896, -0.12175324559211731, 1.1238293647766113, -0.9435240626335144, -0.246377632021904, -0.042357731610536575, 0.4189906716346741, -1.1851187944412231, 1.157323956489563, -0.8982927799224854, 0.31607717275619507, -0.11456980556249619, -0.396859735250473, -0.11357380449771881, -0.35427361726760864, 0.27178171277046204, -0.2640250623226166, 0.13960951566696167, 0.2113604098558426, -0.5342580676078796, 1.6294231414794922, -0.5181379914283752, 0.39694106578826904, 0.10120923817157745, -0.13299843668937683, -0.0020340837072581053, 0.7606324553489685, -0.5011972188949585, -0.5689963698387146, 0.19737350940704346, 0.5305423736572266, -0.6995846629142761, 0.11502502113580704, 0.8685818910598755, 0.5217394828796387, -0.14834588766098022, 0.18558835983276367, 0.6027799248695374, -0.44928452372550964, 0.7161810994148254, 0.35530489683151245, 0.44794753193855286, 0.10931313037872314, 0.48415374755859375, -0.08498045057058334, 0.5212140679359436, -0.836968183517456, -0.3255216181278229, 0.6687641739845276, 0.5019545555114746, 0.5885104537010193, 0.30327385663986206, -0.3497166931629181, -0.15393345057964325, 0.02469831332564354, 0.76023930311203, 2.244206428527832, -0.07278043776750565, -0.33785101771354675, -0.8634014129638672, -0.3266170620918274, -0.25725772976875305, 0.21315725147724152, -0.66729736328125, -0.08191650360822678, -0.42767491936683655, -0.8394196033477783, 0.7833176255226135, 0.0002484933065716177, 0.7288122773170471, -0.46671026945114136, 0.008761405944824219, 0.1695629060268402, 0.07020151615142822, -0.5642473697662354, -1.0059895515441895, 0.20945243537425995, -0.6055737137794495, -0.18554425239562988, -0.1714925765991211, -0.07849765568971634, -0.32763126492500305, -0.5194287896156311, 1.0354938507080078, -0.7329497933387756, 0.10376619547605515, -0.002512372797355056, 0.3550274670124054, -0.6116734147071838, -1.1677755117416382, 0.5851466655731201, 0.1565006822347641, -0.24101465940475464, 0.27340906858444214, 0.6019335985183716, 0.47466692328453064, 0.6100983023643494, -0.1776217669248581, 0.15673662722110748, -0.07507038116455078, 0.12423507869243622, 0.8656807541847229, -0.4474729001522064, 0.3200007677078247, -1.3500478267669678, 1.4294596910476685, 0.035540077835321426, -0.6779550313949585, 0.6010420918464661, -0.45179274678230286, -0.4794999957084656, 0.49438413977622986, -1.101552963256836, -0.7517746686935425, -1.0229028463363647, 0.2305298000574112, -0.4779740571975708, 0.012938305735588074, 0.30494749546051025, 0.23045606911182404, 0.12217990309000015, 0.6255512833595276, 0.43456748127937317, 0.3554011583328247, -0.49310725927352905, 0.9895612597465515, -0.6268289089202881, 0.1711955964565277, 0.0433589406311512, -0.02152547612786293, -0.3772413730621338, -0.7214705944061279, -0.6115012764930725, -0.36742737889289856, -0.49857449531555176, -0.32499608397483826, -0.09767649322748184, 0.2986351251602173, -0.6678994297981262, -0.7908941507339478, -0.1523362249135971, -1.0247082710266113, -0.37781238555908203, 0.2557554543018341, -0.2918385863304138, -0.3212839663028717, -0.9424387216567993, -1.330010175704956, -0.31017976999282837, -0.6926393508911133, -0.8754937052726746, 0.24098461866378784, -0.12693174183368683, -0.5666894316673279, -0.8885557055473328, 0.315750390291214, -0.06052856892347336, 1.253284215927124, -1.325810432434082, 1.194823145866394, -0.28215280175209045, -0.06051172688603401, -0.1221788302063942, 0.18636906147003174, 0.5781167149543762, -0.1461176574230194, 0.03621716424822807, -1.0420950651168823, 0.005095718894153833, -0.3912247121334076, -0.43618500232696533, 0.030249757692217827, 0.24762989580631256, 0.6487215757369995, 0.007407630793750286, -0.6171942353248596, 0.2599729299545288, 1.0239936113357544, -0.6688238978385925, -0.38987505435943604, 0.08842816203832626, 0.9657480716705322, 0.08550292253494263, -0.15461115539073944, 0.6359050273895264, 0.46205267310142517, 0.7137085795402527, -0.0252495389431715, 0.3152809739112854, -0.02837817184627056, -0.5654182434082031, 0.4590934216976166, 1.6922475099563599, 0.299512654542923, -0.01831604726612568, -0.8406906127929688, 0.5204383134841919, -1.434250831604004, -0.6269481182098389, 0.725150465965271, 0.7304518818855286, 0.7090420126914978, -0.33566978573799133, -0.15165939927101135, -0.6847872138023376, 0.3320271670818329, 0.3473344147205353, -0.3852941393852234, -0.8392646908760071, 0.2806902229785919, 0.036686819046735764, -0.25167161226272583, 0.6641807556152344, -0.4123317003250122, 0.8284701108932495, 14.344677925109863, 1.6135071516036987, 0.09928972274065018, 1.128144383430481, 0.3904121220111847, -0.031897157430648804, -0.5276848673820496, -0.49824389815330505, -1.2834523916244507, -0.17751353979110718, 1.146533489227295, -0.10627815872430801, 1.0879045724868774, 0.17418381571769714, 0.4124803841114044, 0.24253767728805542, -0.6358298063278198, 0.6351844668388367, 0.16849105060100555, -1.3122502565383911, 0.5265440344810486, -0.04728325456380844, 0.547508180141449, 0.7791576385498047, 0.5492991209030151, 1.3972206115722656, 0.5663220882415771, -0.2622169554233551, 0.6886619925498962, 0.0727633386850357, 0.7322409749031067, -0.11668318510055542, 0.1316225826740265, 1.0202635526657104, -0.6876590847969055, -0.4218364953994751, -0.6488133072853088, -0.87601238489151, 0.34476006031036377, -0.22073672711849213, -0.672771692276001, -0.7176862955093384, -0.30681082606315613, 0.4277743995189667, 0.15475279092788696, 0.32794564962387085, -0.03873739391565323, 0.7530031204223633, -0.12777471542358398, 0.08667552471160889, 0.446153849363327, 0.23895524442195892, 0.4025680720806122, 0.025956718251109123, 0.008760870434343815, 0.02750553749501705, 0.07569027692079544, 0.5367493629455566, -0.5363912582397461, 0.1701924055814743, -0.05059347674250603, -0.17298850417137146, 0.22107583284378052, 0.7178455591201782, 0.995742678642273, 0.29473695158958435, -0.45978227257728577, 0.33325648307800293, 0.8048866391181946, 0.2745802402496338, -0.3176443874835968, 0.339633971452713, 0.4168795645236969, -0.6018944382667542, -0.03607235476374626, 0.782137393951416, 0.09703622758388519, -0.4617384672164917, -0.8899210691452026, -0.5879403948783875, 0.3708384335041046, -0.7981448173522949, -1.1532399654388428, 0.7561241984367371, -0.06604623794555664, -0.4649996757507324, -0.06784165650606155, -0.3684217035770416, -0.20428209006786346, 0.7756620645523071, -1.1753374338150024, -0.8362084031105042, 0.6824407577514648, -0.2552858591079712, -0.049455802887678146, -0.12510934472084045, 1.5142604112625122, 0.18914853036403656, -0.39269086718559265, 0.2831801176071167, 0.6684788465499878, -0.26751601696014404, -0.019419819116592407, -0.37179267406463623, 0.9294854998588562, -0.3095707297325134, -0.2308366298675537, 0.04276121035218239, -0.07516507804393768, 0.30546998977661133, -0.3213721513748169, -0.24364662170410156, 0.7776839733123779, -0.9059661030769348, -0.3246039152145386, -0.7986559271812439, -1.0924526453018188, 0.25932466983795166, 0.62353515625, -0.29041576385498047, 0.2778531014919281, 0.15869536995887756, -0.703106701374054, 0.11836715787649155, -0.8023204803466797, 0.08980053663253784, 0.31243520975112915, -0.8419964909553528, -0.3417917788028717, 0.02651665359735489, 0.5479528903961182, -1.0412774085998535, -0.5181937217712402, -0.6012964844703674, 0.05243716016411781, 0.31881314516067505, 1.0053499937057495, -0.596735954284668, 0.12583819031715393, 1.04677414894104, -0.1849418580532074, -1.174988031387329, -0.21175378561019897, -0.7996896505355835, -0.18740087747573853, 0.07286103069782257, 0.8929017186164856, -0.3241651654243469, 0.023790888488292694, 0.9073085784912109, 0.493297815322876, -0.46699440479278564, -0.7889654040336609, -0.5921362042427063, 0.3854994773864746, -0.47180020809173584, 0.28471338748931885, -0.060905009508132935, -0.1264907866716385, 0.31124863028526306, 0.2637147307395935, 0.38963738083839417, -0.2931792140007019, -0.9809597134590149, 0.31999754905700684, -0.115260049700737, -0.28353774547576904, -0.5181376338005066, 0.4549245536327362, -1.368228793144226, 0.03877860680222511, -1.1137069463729858, -0.1619398295879364, -0.8785400390625, -0.3724660277366638, -0.056839410215616226, -0.20411518216133118, -0.1924383044242859, 0.3988693952560425, -0.47505709528923035, -0.15037675201892853, -0.2482554167509079, -0.8336681723594666, 0.9113492965698242, 1.3273364305496216, -0.9213065505027771, -0.1979493349790573, 0.15470056235790253, 0.29457947611808777, 0.36888521909713745, 0.6860640645027161, -0.5696387887001038, -0.9486847519874573, -1.3835150003433228, 0.2655811011791229, 0.08259080350399017, -0.40633708238601685, -0.5166813135147095, 0.53182452917099, -0.012592175044119358, -0.5090300440788269, 0.4406789243221283, 0.7033208608627319, -0.8775840401649475, -0.32400044798851013, 0.10402148962020874, -0.9196941256523132, -0.11861013621091843, 0.18834611773490906, -0.3864032030105591, -0.4188826382160187, 0.5386580228805542, 0.19412757456302643, -1.2752162218093872, -0.9203301072120667, 0.4723259210586548, -0.6397615671157837, 0.6351485252380371, -0.19082583487033844, -0.09529893845319748, -0.8602509498596191, -0.5659221410751343, -0.25120750069618225, 0.5703772306442261, -0.5238026976585388, 1.1209359169006348, 0.13371525704860687, -1.3140537738800049, -0.07599572092294693, 0.5272746682167053, 0.37156346440315247, -0.23747564852237701, 0.7033100724220276, 0.6267056465148926, 0.013867175206542015, 0.7124249935150146, 0.30935049057006836, 0.41178029775619507, -1.008541226387024, -0.19897426664829254, 1.1544734239578247, -0.7110247015953064, 0.07456893473863602, 1.367311716079712, -0.1502397060394287, -1.3776497840881348, 0.18980839848518372, -0.9740861654281616, -0.5739500522613525, -0.19373954832553864, 0.7230465412139893, 0.32399359345436096, -0.017084434628486633, -0.18657128512859344, -0.39707544445991516, 0.06313305348157883, -0.26048848032951355, -0.5711466073989868, 0.31893405318260193, -0.6572637557983398, -0.3217003047466278, 0.7858266830444336, 0.9837976694107056, -0.82883620262146, -1.0299599170684814, -0.7091919183731079, -0.37717366218566895, 0.0711708664894104, 0.12648354470729828, -0.8548951745033264, -0.39049726724624634, 0.8070520162582397, 0.5367807745933533, 0.15602591633796692, -0.22208982706069946, -0.046260423958301544, 0.08103685081005096, 0.9660692811012268, 0.2630791664123535, -1.0510292053222656, -0.4712679982185364, 1.598419427871704, 1.3247699737548828, -1.2161545753479004, -0.4572497010231018, -0.04135364666581154, -0.9202679395675659, 0.6971411108970642, 0.37380972504615784, 0.398879736661911, 0.9992617964744568, -0.7298569679260254, 0.3706575334072113, 0.36338305473327637, -1.4805943965911865, -0.10922021418809891, 1.0092304944992065, 0.909084677696228, 0.590480625629425, 0.41368597745895386, 0.2783955931663513, 0.9013967514038086, 0.2053402215242386, 0.4072740077972412, 0.3156537711620331, -0.009512468241155148, -0.4573962688446045, -0.08459749072790146, 0.23535151779651642, 0.5839554667472839, -0.5757066607475281, -0.6664484739303589, 0.15675482153892517, 0.6624669432640076, 0.27714407444000244, 0.6530841588973999, 0.32442739605903625, 0.05075033754110336, 0.5315355658531189, 0.40065160393714905, 0.44793084263801575, -0.766006350517273, -0.2899806797504425, -0.09972917288541794, -0.4434657096862793, -0.09823919087648392, 0.05751187726855278, -0.044388409703969955, 0.040942542254924774, -0.2201514095067978, 0.10720212757587433, 0.02227374166250229, 0.24300545454025269, 1.2614734172821045, 0.5740378499031067, 0.07437489926815033, -0.6866548657417297, -0.34777501225471497, -0.7204198241233826, -1.3404489755630493, -0.12096966058015823, -0.7655805349349976, -0.3856428861618042, -0.12683223187923431, -0.032492417842149734, -0.4603305757045746]}, "authors": [{"authorId": "2296715370", "name": "Benjue Weng"}], "references": [{"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "38c8111873cb40d28c8bbc8aa6836a172234b5fa", "title": "Nash Learning from Human Feedback"}, {"paperId": "833f5f0d9d271b9b04e742c11a1f3e33bbada3b9", "title": "TaskWeaver: A Code-First Agent Framework"}, {"paperId": "6fa0677731184444df0e1fc8070938419cd6da47", "title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"}, {"paperId": "9d10dc85d2b4e9d468e6170ce389a12b7e5e9234", "title": "LCM-LoRA: A Universal Stable-Diffusion Acceleration Module"}, {"paperId": "b3dbfcb91f8657d99942433daeccc6e17e8a40bd", "title": "Prompt Engineering a Prompt Engineer"}, {"paperId": "46fe9ce789408b8a50fb4259e6bf0cc5855f4ed5", "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs"}, {"paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"}, {"paperId": "3f413dca2607d68301143770e599b59d461a569e", "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "78a48c296267ae59b4be60ebbbf96b5d7f6c693b", "title": "Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations"}, {"paperId": "32b15b02bd2640346678a773079c5d42190bbac9", "title": "TimeGPT-1"}, {"paperId": "0078851695589a1dc1450240733add22f57f88ce", "title": "Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion"}, {"paperId": "368fb35a07076eba01c2e4700499323cd4524513", "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning"}, {"paperId": "d3ca116177369bf6fbe27de64506a2f401aca996", "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency"}, {"paperId": "9ea0757c750ab1222a7442d3485a74d1c526b04c", "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"}, {"paperId": "38939304bb760473141c2aca0305e44fbe04e6e8", "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "3b8871e4c25d3aaca2bee6606c07bc870337253c", "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb", "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "title": "Towards Expert-Level Medical Question Answering with Large Language Models"}, {"paperId": "7fa85f9c0fe44f1bf9e58a55f0f009296578c2f0", "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning"}, {"paperId": "886e0962479ec6dac563666399ca4c96a468fcaa", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"}, {"paperId": "f5a0c57f90c6abe31482e9f320ccac5ee789b135", "title": "Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"}, {"paperId": "bbb2fc6e95d24fb58ab6c25b216b14ac49a32fbe", "title": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction"}, {"paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891", "title": "DINOv2: Learning Robust Visual Features without Supervision"}, {"paperId": "5278a8eb2ba2429d4029745caf4e661080073c81", "title": "Generative Agents: Interactive Simulacra of Human Behavior"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "6007263dd3d14373be5f84fb6ccb0be3f7fce903", "title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "08b85bce712168998004ee80ce4e475390413c74", "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT"}, {"paperId": "efbe97d20c4ffe356e8826c01dc550bacc405add", "title": "Adding Conditional Control to Text-to-Image Diffusion Models"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "07b14c24833400b79978b0a5f084803337e30a15", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models"}, {"paperId": "77638cb15b7c2cbb031eb103081ce881297904b3", "title": "Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model"}, {"paperId": "ad07d3499faade81e6c33069902c45b13ba90c44", "title": "Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632", "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "99832586d55f540f603637e458a292406a0ed75d", "title": "ReAct: Synergizing Reasoning and Acting in Language Models"}, {"paperId": "6fcdad7b8d6b60b23bc51859e736c29f913b249a", "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "599be9043ef3571f65758cf36e184c9dc1781baf", "title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "75bb9eda70751c63fc54dbe63377c673b7dbdb15", "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "c57293882b2561e1ba03017902df9fc2f289dea2", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "0f17d7619e5de7bf41079d65783d4fb135825377", "title": "CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues"}, {"paperId": "5382c28b9f2dd30f15c836bea92382091b1d886f", "title": "Unified Structure Generation for Universal Information Extraction"}, {"paperId": "0f733817e82026f7c29909a51cb4df7d2685f0e7", "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ad471be93216ddbf8544721d50ee5aed14f07cae", "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "f4d68b5b81aa12cb5b6db0b48fc2a4709abcdbb8", "title": "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "title": "CogView: Mastering Text-to-Image Generation via Transformers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "cba0f1661d6206ce584def83884755e5b095ef25", "title": "Continual Learning for Natural Language Generation in Task-oriented Dialog Systems"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a", "title": "Denoising Diffusion Probabilistic Models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"}, {"paperId": "6f444b8a665ec8045e61775dd7ab6e9930e3bdf4", "title": "Few-Shot Text Classification with Induction Network"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0", "title": "An Overview of Multi-Task Learning in Deep Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "846aedd869a00c09b40f1f1f35673cb22bc87490", "title": "Mastering the game of Go with deep neural networks and tree search"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "title": "Recurrent Neural Network Regularization"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "title": "Backpropagation Applied to Handwritten Zip Code Recognition"}, {"paperId": null, "title": "\u201cPractical Tips for Finetun-ing LLMs Using LoRA (Low-Rank Adaptation)\u201d"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": null, "title": "\u201cFine-tune Llama 2 with DPO\u201d"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "b76e98a0a023d37c6534aa2ead09c8ff595f0bae", "title": "A Robustly Optimized BERT Pre-training Approach with Post-training"}, {"paperId": "2845c94a0cbc85dedb7b6f7b8f0167ee74fa3946", "title": "and generation"}, {"paperId": null, "title": "\u201cKnowledge Distillation: A Sur-vey\u201d"}, {"paperId": null, "title": "\u201cThe Illustrated Transformer\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cKullback\u2013Leibler Divergence: A Quantile Ap-proach\u201d"}, {"paperId": null, "title": "\u201cRoboCat: A Self-Improving Foundation Agent for Robotic Manipula-tion\u201d"}, {"paperId": null, "title": "\u201cLINGO-1: Exploring Natural Language for Autonomous Driving\u201d"}, {"paperId": "67be4a0c75c2b2da3e834eb12b8305050a64a525", "title": "AlphaCode 2 Technical Report"}, {"paperId": null, "title": "Autonomy: The ability to operate without continuous user prompts"}, {"paperId": null, "title": "\u201cGemini: A Family of Highly Capable Multimodal Models\u201d"}, {"paperId": null, "title": "\u201cFinetuning Falcon LLMs More Efficiently With LoRA and Adapters\u201d"}, {"paperId": null, "title": "When an input query is passed to the LLM,the most relevant information is retrieved from the external database using metrics such as cosine similarity and combined with the LLM as additional context"}, {"paperId": null, "title": "\u201cRAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance\u201d"}, {"paperId": null, "title": "User-Friendly Interface: While it requires some technical knowledge,its level of independence allows it to operate without in-depth programming expertise"}, {"paperId": null, "title": "Analysis of the rate of loss reduction under different parameter settings"}, {"paperId": null, "title": "Goal-Oriented Functionality: Capable of rewriting its own code,autonomously searching the internet,and performing tasks such as saving files to a computer"}, {"paperId": null, "title": "Fine-tuning for a small number of epochs can achieve good performance(Fig. 58, 59),which is more evident as the model size increases(Fig. 59)"}, {"paperId": null, "title": "Exploration of various fine-tuning methods,including Prefix-Tuning,Prompt-Tuning,RHLF,and Agent-Tuning"}, {"paperId": null, "title": "\u201cAn Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels\u201d"}, {"paperId": null, "title": "As the size of the model increases"}, {"paperId": null, "title": "Developing a benchmark called IE INSTRUC-TIONS,consisting of 32 different information extraction datasets,allowing for consistent and standardized evaluation of various information extraction tasks"}, {"paperId": null, "title": "\u201cA Survey on In-context Learn-ing\u201d"}, {"paperId": null, "title": ",the performance of the base model can even surpass that of the large model in some cases"}, {"paperId": null, "title": "Versatility: Able to generate text,answer questions,translate languages,summarize text,and provide recommendations"}, {"paperId": null, "title": "\u201cThe Falcon has landed in the Hugging Face ecosystem\u201d"}, {"paperId": null, "title": "Trials with different tasks,such as text classification,named entity recognition,and conversational question answering"}, {"paperId": null, "title": "\u201cIntroducing Gem-ini: our largest and most capable AI model\u201d"}]}