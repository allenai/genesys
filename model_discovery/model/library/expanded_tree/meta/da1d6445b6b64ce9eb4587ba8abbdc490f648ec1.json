{"paperId": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1", "abstract": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 107, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://arxiv.org/pdf/2205.12674", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents TRIME, a novel yet simple training approach designed for training LMs with memory augmentation that adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs."}, "embedding": {"model": "specter_v2", "vector": [0.20192977786064148, 0.6055683493614197, -0.4390159249305725, -0.017420455813407898, -0.08949906378984451, -0.005849962588399649, 0.905874490737915, -0.5803449749946594, -0.8506413698196411, -0.0996944010257721, 0.717430591583252, -0.48759809136390686, 0.7792952656745911, 0.15495796501636505, -0.24452799558639526, 0.26430103182792664, -0.7433693408966064, 0.29577744007110596, -0.45033687353134155, -0.2789653539657593, -0.3821367025375366, -0.7965393662452698, -0.4634217917919159, -0.05764874443411827, 0.22841255366802216, 0.19741632044315338, 0.25966349244117737, 0.8147228360176086, -0.7642908692359924, 0.348624587059021, 0.35050758719444275, -0.11902075260877609, 0.008045176975429058, -0.09974724799394608, -0.11284186691045761, 0.34422731399536133, 0.28097298741340637, -0.5569518208503723, -0.5083650946617126, 0.5376142859458923, -0.4269888699054718, 0.5217862129211426, 0.16672563552856445, -0.2428683042526245, -0.4284115731716156, 0.9469135999679565, 0.4868656396865845, 0.657514750957489, -0.665238618850708, -0.4341285824775696, 0.535240888595581, -1.955586552619934, 0.4719945192337036, 1.4244855642318726, 0.5661818981170654, 0.7365571856498718, -0.07995382696390152, -0.5405764579772949, 0.789695143699646, -0.3538452684879303, -1.0865983963012695, -0.7375935316085815, -0.04002102464437485, 0.09135261178016663, 2.1022872924804688, -0.2743392586708069, -0.21985043585300446, 0.33957740664482117, -0.49245429039001465, 1.2768052816390991, -0.1263221800327301, -0.7084867358207703, -0.6872982382774353, 0.6114187240600586, 0.11891650408506393, 1.0868356227874756, -0.6350376009941101, 0.1605643630027771, -0.7113535404205322, 0.1379878669977188, 0.4056341350078583, -0.2064666450023651, -0.1412157267332077, -0.07553499937057495, -0.41568800806999207, 0.7346760034561157, 0.17638926208019257, 0.7649699449539185, 0.0627499520778656, 0.5216411352157593, 0.43392086029052734, 0.43431225419044495, 0.5292239189147949, 0.41963282227516174, -0.3405302166938782, 0.8160406351089478, -1.091945767402649, -0.17886844277381897, -0.05499039217829704, 0.8647016286849976, -0.24436423182487488, 0.5233120322227478, -0.7010364532470703, 0.4288603365421295, 1.2195881605148315, 0.16563206911087036, 0.7207435369491577, -0.513016939163208, 0.6346538066864014, -0.7408620119094849, 0.14753586053848267, -0.3498518764972687, -0.5273329615592957, -0.701293408870697, -0.8466029763221741, -1.428636074066162, -0.6395300626754761, -0.0018457358237355947, -0.8166310787200928, 1.1059225797653198, -0.3073294460773468, 0.2020636647939682, 0.34393343329429626, 0.09527935832738876, 0.678459882736206, 1.143333077430725, 0.34193697571754456, -0.5045850276947021, 0.587293803691864, -1.5291410684585571, -0.8848185539245605, -1.2309759855270386, 1.0866650342941284, -0.20202571153640747, 0.3696959614753723, -0.39540302753448486, -1.1862163543701172, -0.9318673014640808, -0.8132396340370178, -0.15421688556671143, -0.5602397322654724, 0.8380718231201172, 0.8315981030464172, 0.6087957620620728, -1.2017093896865845, 0.5608852505683899, -0.020666426047682762, 0.07663818448781967, 0.2654770612716675, 0.39107808470726013, 0.182210773229599, -0.4665781259536743, -0.931708037853241, 0.46415790915489197, 0.3169599175453186, -0.5499140024185181, -0.10385379940271378, -0.6546015739440918, -1.0926159620285034, -0.3325869143009186, 0.28520500659942627, -0.5153235793113708, 1.1810308694839478, -0.2998504638671875, -1.5967726707458496, 0.5248604416847229, -0.45618048310279846, 0.006351332645863295, 0.2911655306816101, -0.27559608221054077, -0.8189534544944763, -0.886391818523407, -0.621970534324646, 0.9031919240951538, 0.46272075176239014, -0.08063651621341705, 0.1024676263332367, 0.331757515668869, -0.3707817792892456, 0.08315472304821014, -0.7875958681106567, 0.9225084185600281, -0.6614679098129272, -0.39032846689224243, 0.2933591902256012, 0.5308416485786438, -0.2367820143699646, -0.4039390981197357, -0.5816837549209595, -0.6698389053344727, 0.9766589999198914, -0.1733187437057495, 1.3034679889678955, -0.9688783884048462, -0.5848174691200256, -0.275560587644577, -0.4070627689361572, 0.22331072390079498, -1.0337915420532227, 0.4758743941783905, -0.18814708292484283, 0.5272933840751648, -0.0715503916144371, -1.8324270248413086, -0.0136870127171278, -0.09902866929769516, -0.774692714214325, -0.41281306743621826, 0.4158167541027069, 1.010202407836914, -0.6196015477180481, 0.10964269936084747, -0.22271835803985596, 0.6159346699714661, -0.9292521476745605, 1.2204786539077759, -0.5248178243637085, 0.7090167999267578, 0.32155805826187134, -0.40055274963378906, 0.12959834933280945, -0.38911762833595276, 0.6472188234329224, -0.20953452587127686, -0.34393730759620667, 0.6922809481620789, -0.4148927330970764, 1.0458362102508545, -0.6917796730995178, 0.4024558663368225, -0.11635114252567291, -0.5346893668174744, 0.16280968487262726, 0.45700886845588684, -0.4387384057044983, -0.3901532292366028, 0.36566805839538574, 0.6370026469230652, -0.7357049584388733, 0.5604451298713684, 1.1102757453918457, 0.5575850605964661, 0.041190072894096375, 0.06289065629243851, 0.8441537022590637, -0.11253247410058975, 0.6319463849067688, 0.14985370635986328, 0.49184444546699524, 0.3607516586780548, -0.00010559136717347428, 0.24272793531417847, 0.6275293827056885, -0.7705315947532654, -0.2275472730398178, 0.23590756952762604, 1.234907865524292, 0.6267877221107483, -0.03625756874680519, -0.5680012106895447, -0.5672836899757385, 0.1055978387594223, 0.7138835191726685, 1.5958242416381836, -0.5584756731987, 0.03175234794616699, -0.7003965377807617, -0.1202002465724945, -0.5823692083358765, 0.34547463059425354, -0.26534029841423035, -0.14875267446041107, -1.0432753562927246, -1.0646998882293701, 0.6597655415534973, 0.25483909249305725, 1.1260480880737305, -0.10172698646783829, 0.03188582882285118, -0.3625791072845459, 0.1964946985244751, -0.8416649103164673, -0.933251678943634, 0.306291401386261, -1.2236336469650269, 0.3726746141910553, -0.28659066557884216, -0.4632449150085449, 0.5071948170661926, -0.47884035110473633, 1.1443201303482056, -0.13973236083984375, 0.24307583272457123, -0.11250083148479462, 0.7827454209327698, -0.4632570147514343, -1.044869065284729, 0.293163925409317, 0.41438090801239014, -0.16205242276191711, 0.03838106244802475, 0.5995311141014099, 0.1923850178718567, 0.014998745173215866, -0.4430984854698181, 0.6246843934059143, 0.017250362783670425, 0.17292936146259308, 0.8501534461975098, -0.5306406617164612, 0.1397537738084793, -1.5442874431610107, 0.6853733062744141, 0.21811355650424957, -0.49143505096435547, 0.3795855939388275, -0.5734302997589111, -0.457456111907959, 0.4297240376472473, -0.9833341836929321, -0.473896861076355, -1.0545839071273804, 0.1118876188993454, -0.11307653039693832, 0.29757991433143616, 0.42352738976478577, 0.3444264829158783, 0.24388884007930756, -0.030393116176128387, 0.45078879594802856, 0.3980843126773834, -0.5610166192054749, 0.7153997421264648, -1.0516122579574585, 0.5546919703483582, 0.7211720943450928, 0.566093921661377, -0.2940485179424286, -0.3392929136753082, -0.6911289095878601, -0.33433234691619873, -0.29559609293937683, -0.34645500779151917, -0.23538510501384735, -0.05881712958216667, -0.637810468673706, -0.18514972925186157, -0.13235455751419067, -1.10603666305542, -0.18502479791641235, 0.12882478535175323, -0.18122760951519012, -0.01261171605437994, -1.1334134340286255, -1.3395097255706787, -0.7311938405036926, -0.6775650978088379, -0.9193519353866577, 0.16043928265571594, 0.05560239776968956, -0.24429090321063995, -0.7440968751907349, -0.025920938700437546, -0.3138434588909149, 1.4028732776641846, -1.0627866983413696, 0.905014157295227, 0.12418030202388763, -0.2484542429447174, -0.22040997445583344, 0.32990866899490356, 0.7652497291564941, -0.7353214621543884, 0.006124775391072035, -1.0075223445892334, 0.06995166838169098, -0.38655295968055725, -0.19061730802059174, 0.3148927390575409, 0.2274075746536255, 0.5155525207519531, -0.2771205008029938, -0.47386011481285095, 0.5816506147384644, 1.1370329856872559, -0.7439126372337341, 0.07487055659294128, 0.20102886855602264, 0.7994992733001709, -0.06371225416660309, -0.547065019607544, 0.4121421277523041, 0.10314685851335526, 0.47931107878685, 0.02632424421608448, -0.3044557571411133, -0.2268957942724228, -0.7356938719749451, 0.647596001625061, 2.077176094055176, 0.4174240529537201, -0.339131236076355, -0.8364846110343933, 0.4039555490016937, -0.7569133043289185, -0.3235985338687897, 0.6638811230659485, 0.8134835362434387, 0.6562319397926331, -0.6584465503692627, -0.3707919120788574, -0.5255001187324524, 0.3633165955543518, 0.3678540587425232, -0.30966269969940186, -0.7118237614631653, -0.27507415413856506, 0.4250485301017761, 0.2539689838886261, 0.6741309762001038, -0.3651721775531769, 0.6552233695983887, 14.424118995666504, 0.5058809518814087, -0.15451684594154358, 1.0986870527267456, 0.8361761569976807, -0.10884130001068115, -0.2384783923625946, -0.31816408038139343, -1.2521418333053589, -0.33741286396980286, 1.2345411777496338, 0.4776094853878021, 0.6886469125747681, 0.03404318913817406, 0.034119438380002975, 0.2949881851673126, -0.26074934005737305, 0.6879013776779175, 0.5804497003555298, -1.6406089067459106, 0.4480295181274414, 0.147971972823143, 0.8085780739784241, 0.6168729066848755, 1.1603742837905884, 1.172206997871399, -0.08728481829166412, -0.40891265869140625, 0.34359341859817505, 0.30854466557502747, 0.8678210377693176, 0.008901894092559814, 0.5380998849868774, 0.5775821208953857, -0.46049249172210693, -0.22101227939128876, -0.48654675483703613, -1.0099817514419556, 0.609413206577301, 0.04540012404322624, -0.5253077149391174, -0.6413617134094238, -0.5971453785896301, 0.26465603709220886, -0.1475721299648285, 0.21942134201526642, 0.1349836140871048, 0.9918727278709412, -0.21395987272262573, 0.143533855676651, 0.40943270921707153, 0.08461786061525345, 0.09102758765220642, 0.13564839959144592, 0.2695958912372589, -0.3875308632850647, -0.06678029894828796, 0.5525175333023071, -0.8761434555053711, 0.14764192700386047, -0.30853161215782166, -0.4323175549507141, -0.15663278102874756, 0.6755235195159912, 0.5580952763557434, -0.060153573751449585, -0.7556606531143188, 0.46967199444770813, 0.6144606471061707, 0.2085626721382141, -0.32596555352211, 0.3849447965621948, 0.06274145841598511, -0.18543501198291779, -0.07533836364746094, 0.028953248634934425, 0.004966148175299168, -0.4871395528316498, -0.7467514276504517, -0.2053820788860321, 0.03925754874944687, -0.6225544810295105, -0.18163742125034332, 0.9855045676231384, -0.4795564115047455, -0.4992770552635193, 0.28160297870635986, -0.9101861119270325, -0.18403470516204834, 0.6850376725196838, -1.4631004333496094, -0.5977911949157715, 0.5302026271820068, -0.7443253397941589, -0.4225391149520874, -0.3473484218120575, 1.4657304286956787, 0.5811281204223633, -0.6556223630905151, 0.23830874264240265, 0.3691462278366089, -0.08797145634889603, -0.3786327540874481, -0.31253352761268616, 0.8238675594329834, 0.10114216804504395, -0.08076892048120499, 0.5592117309570312, 0.10490868985652924, 0.2397027462720871, -1.086342692375183, -0.4581320881843567, 1.1340203285217285, -0.6974090337753296, -0.4675045609474182, -0.5855255722999573, -0.8740791082382202, 0.36188921332359314, 0.5832168459892273, -0.425677090883255, 0.6852412223815918, 0.3224277198314667, -0.4844701588153839, 0.22642099857330322, -0.5503025650978088, 0.14842469990253448, 0.562964916229248, -0.49323025345802307, -0.07069453597068787, 0.390941321849823, 0.5311382412910461, -1.1463485956192017, -0.42373380064964294, -0.15716490149497986, -0.527932345867157, 0.4237492084503174, 0.9027536511421204, -0.45549216866493225, 0.5072888731956482, 0.691331684589386, -0.08480248600244522, -0.8087141513824463, 0.3318255543708801, -0.7807964086532593, -0.11596822738647461, 0.2933821678161621, 0.9685333967208862, -0.1223461776971817, 0.14347971975803375, 0.6433844566345215, 0.09951094537973404, -0.3430885672569275, -0.5303879380226135, -0.6239954233169556, 0.18983201682567596, -0.3330446481704712, 0.11191888898611069, -0.046463098376989365, -0.21102365851402283, 0.5706762671470642, 0.1496036946773529, 0.31257790327072144, -0.5399990081787109, -1.0411516427993774, 0.573604941368103, 0.24451932311058044, -0.19251982867717743, -0.4727522134780884, -0.09148091077804565, -1.7144731283187866, 0.1778065711259842, -1.191447138786316, -0.18282340466976166, -0.7556239366531372, 0.015080524608492851, 0.13508641719818115, 0.24057777225971222, -0.030435170978307724, 0.408589243888855, 0.12628276646137238, -0.4351470470428467, -0.5435608625411987, -0.5469982028007507, 0.9418225288391113, 0.7980294227600098, -0.5925182700157166, -0.13834215700626373, -0.2064366489648819, 0.45477616786956787, 0.37038666009902954, 0.5165651440620422, -0.12038680911064148, -0.979580819606781, -1.7766512632369995, 0.4667765200138092, -0.20363099873065948, -0.3163950741291046, -0.48658210039138794, 0.5635204911231995, 0.6497414708137512, -0.21697553992271423, 0.01644124835729599, 0.5618247389793396, -0.5042108297348022, -0.19550172984600067, 0.14575810730457306, -0.6542771458625793, 0.5347957015037537, 0.49806270003318787, -0.8171889781951904, -0.30090993642807007, 0.4975939691066742, -0.0773768499493599, -0.9596893191337585, -0.7879395484924316, 0.3271409273147583, -0.7922595143318176, -0.015490245074033737, -0.7514494061470032, -0.0003163300862070173, -1.0730772018432617, -0.44418612122535706, -0.057982493191957474, 0.29904159903526306, -0.3348930776119232, 0.8209757208824158, 0.28632625937461853, -1.114271879196167, 0.01688762567937374, 0.5518559813499451, -0.3475782871246338, -0.23425564169883728, 0.5712829828262329, 0.849033534526825, -0.2817627191543579, 0.7488425970077515, 0.5397068858146667, 0.5753363966941833, -0.8968529105186462, 0.0033820862881839275, 0.8261277675628662, -0.5551187992095947, -0.09223371744155884, 1.3288710117340088, -0.40681004524230957, -1.654788613319397, 0.18642589449882507, -1.1090147495269775, -0.4550364315509796, -0.5681787133216858, 0.8557966947555542, 0.09959597885608673, 0.14290714263916016, -0.1443423330783844, -0.2493392825126648, 0.07161826640367508, 0.2024739384651184, -0.6764221787452698, 0.5732815265655518, -0.4212430417537689, -0.6390870809555054, 0.8203572630882263, 1.0224560499191284, -0.643327534198761, -0.4619594216346741, -1.0720020532608032, -0.3097502887248993, 0.07223937660455704, 0.5540987253189087, -0.46725863218307495, -0.6404033303260803, 0.786198079586029, 0.542716383934021, -0.29744860529899597, 0.21726253628730774, -0.40383991599082947, 0.21542643010616302, 0.7203775644302368, 0.15403929352760315, -0.886889636516571, -0.6263583302497864, 1.8298441171646118, 1.0785243511199951, -1.2151575088500977, 0.22602561116218567, -0.136094868183136, -0.6541253328323364, 0.776757538318634, 0.16683001816272736, 0.42772626876831055, 0.9764592051506042, -0.33723151683807373, 0.16274133324623108, 0.40167513489723206, -1.03880774974823, -0.05269280821084976, 0.9692181348800659, 0.7834948897361755, 1.2227792739868164, 0.3628697693347931, 0.01285974495112896, 0.8023364543914795, 0.18766507506370544, 0.08229491114616394, 0.3108437955379486, 0.4636455476284027, -0.39308008551597595, -0.28381261229515076, -0.029523465782403946, 0.568509042263031, -0.46460142731666565, -0.9260112643241882, 0.137786403298378, 0.7129683494567871, 0.3375912010669708, 0.6707940101623535, 1.0237191915512085, -0.09456780552864075, 0.5094708204269409, 0.591241180896759, 0.48123329877853394, -0.7562286853790283, -0.22588200867176056, -0.15001098811626434, -0.31675559282302856, 0.24332484602928162, -0.007856020703911781, -0.374935507774353, -0.2296391874551773, -0.39177918434143066, -0.006904050707817078, -0.20561295747756958, 0.6355891823768616, 1.0031570196151733, 0.37613341212272644, 0.5337002277374268, -0.19948385655879974, -0.6013908386230469, -0.3558366000652313, -1.3150949478149414, 0.08439596742391586, -0.6131486892700195, -0.4624984562397003, -0.08633118122816086, 0.12875141203403473, -0.2578476369380951]}, "authors": [{"authorId": "49164966", "name": "Zexuan Zhong"}, {"authorId": "49986267", "name": "Tao Lei"}, {"authorId": "50536468", "name": "Danqi Chen"}], "references": [{"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3", "title": "LaMemo: Language Modeling with Look-Ahead Memory"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "7d1e859fefee1eaac430c38d01cd35003604288b", "title": "GNN-LM: Language Modeling based on Global Contexts via GNN"}, {"paperId": "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d", "title": "Efficient Nearest Neighbor Language Models"}, {"paperId": "6d4a9f1c41b078846901362ba0dce8295dd6a2a8", "title": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering"}, {"paperId": "6551ba18a52809f057518f3ebc627c77689bfc93", "title": "Adaptive Nearest Neighbor Machine Translation"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5", "title": "ReadTwice: Reading Very Large Documents with Memories"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d", "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"}, {"paperId": "46c585ee9abf76779ea4b863d2da4358efd0d1d3", "title": "Adaptive Semiparametric Language Models"}, {"paperId": "20d51f8e449b59c7e140f7a7eec9ab4d4d6f80ea", "title": "Nearest Neighbor Machine Translation"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "0b09448f7543453cc066416f547292dc1e4471f6", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "ea8c46e193d5121e440daf96edfd15a47151c293", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "8905f3dcd215fbc3d56839b6f52a43d77ac59fe8", "title": "Augmenting Transformers with KNN-Based Composite Memory for Dialog"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "94e43de5dca9355a60a225565aa26bd1cc065c3e", "title": "Episodic Memory in Lifelong Language Learning"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "title": "A Call for Clarity in Reporting BLEU Scores"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "2ed4ebe1878fd8e421f24e1aac76fbdc89e9d381", "title": "Unbounded cache model for online language modeling with open vocabulary"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "2cbb8de53759e75411bc528518947a3094fbce3a", "title": "Billion-Scale Similarity Search with GPUs"}, {"paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "452059171226626718eb677358836328f884298e", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "47ced790a563344efae66588b5fb7fe6cca29ed3", "title": "The Probabilistic Relevance Framework: BM25 and Beyond"}, {"paperId": "46f30e94dd3d5902141c5fbe58d0bc9189545c76", "title": "Dimensionality Reduction by Learning an Invariant Mapping"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "e74e340f4b3546f898ec2b3f732410419e280293", "title": "Memories"}, {"paperId": "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7", "title": "MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION"}, {"paperId": null, "title": "Applying TRIMELMlong to SRU++ We apply our approach to SRU++ (Lei"}, {"paperId": null, "title": "2021), we build an external memory by taking all the translation contexts and the corresponding target token ((x, y<t), yt) on the training set. We use the output representation as f((x"}, {"paperId": null, "title": "order to better compare with previous work, we evaluate on two model configurations\u2014one uses a 247M Transformer model and a segment length L = 3, 072 following Baevski and Auli"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "More details are provided in Appendix D. ENWIK8 (Mahoney, 2009) is a character-level language modeling dataset that contains a total of 100M characters"}, {"paperId": null, "title": "To apply our approach to SRU++, we follow their data-batching method as it is required due to the recurrence of the model architecture. We construct the training memory using all the contexts"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": null, "title": "International Conference on Learning Representations"}, {"paperId": null, "title": "2022. Augmenting"}]}