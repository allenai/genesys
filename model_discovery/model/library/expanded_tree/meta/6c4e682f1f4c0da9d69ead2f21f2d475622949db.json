{"paperId": "6c4e682f1f4c0da9d69ead2f21f2d475622949db", "abstract": "Attention for transformers is a critical workload that has recently received significant\"attention\"as a target for custom acceleration. Yet, while prior work succeeds in reducing attention's memory-bandwidth requirements, it creates load imbalance between attention operators (resulting in severe compute under-utilization) and requires on-chip memory that scales with sequence length (which is expected to grow over time). This paper ameliorates these issues, enabling attention with nearly 100% compute utilization, no off-chip memory traffic bottlenecks, and on-chip buffer size requirements that are independent of sequence length. The main conceptual contribution is to use a recently proposed abstraction -- the cascade of Einsums -- to describe, formalize and taxonomize the space of attention algorithms that appear in the literature. In particular, we show how Einsum cascades can be used to infer non-trivial lower bounds on the number of passes a kernel must take through its input data, which has implications for either required on-chip buffer capacity or memory traffic. We show how this notion can be used to meaningfully divide the space of attention algorithms into several categories and use these categories to inform our design process. Based on the above characterization, we propose FuseMax -- a novel mapping of attention onto a spatial array-style architecture. On attention, in an iso-area comparison, FuseMax achieves an average $6.7\\times$ speedup over the prior state-of-the-art FLAT while using $79\\%$ of the energy. Similarly, on the full end-to-end transformer inference, FuseMax achieves an average $5.3\\times$ speedup over FLAT using $83\\%$ of the energy.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes FuseMax -- a novel mapping of attention onto a spatial array-style architecture and shows how Einsum cascades can be used to infer non-trivial lower bounds on the number of passes a kernel must take through its input data, which has implications for either required on-chip buffer capacity or memory traffic."}, "embedding": {"model": "specter_v2", "vector": [0.7403833270072937, 0.18872566521167755, -0.7027468085289001, 0.030428148806095123, -0.15735134482383728, 0.09193692356348038, 0.41965582966804504, 0.10108524560928345, -0.2843732535839081, -0.6882824301719666, 0.4272642135620117, -0.4106110632419586, 0.5718004703521729, -0.10985404253005981, -0.18650254607200623, 0.23892927169799805, -0.6844446063041687, -0.1754000037908554, 0.4107125997543335, -0.15297073125839233, 0.5037587881088257, -0.06892386078834534, -1.8592830896377563, 0.5232014060020447, 0.009894785471260548, 1.2521491050720215, -0.04925675690174103, 1.2443913221359253, -0.17276009917259216, 0.2831447720527649, 0.3390238285064697, -0.0922357365489006, 0.017402980476617813, 0.3434581756591797, -0.3106309771537781, -0.21082492172718048, 0.4781377613544464, -0.4333159327507019, -0.5735419392585754, 0.6097638607025146, 0.016607366502285004, 0.12436945736408234, 0.28552675247192383, -0.9464839696884155, -0.020177947357296944, 0.4656825661659241, -0.009392822161316872, 1.1780427694320679, -0.7730321884155273, -0.17412874102592468, 1.1422730684280396, -1.431479573249817, -0.21231737732887268, 1.0367330312728882, 0.31343477964401245, -0.229877769947052, -0.3232377767562866, 0.12154705822467804, 0.5245799422264099, 0.10864104330539703, -0.37783804535865784, -0.8750670552253723, -0.23236224055290222, 0.11292055994272232, 1.8618849515914917, -0.04965229332447052, 0.060807522386312485, -0.01312789786607027, 0.21538867056369781, 1.422448754310608, -0.053959209471940994, -1.0321999788284302, 0.24969807267189026, -0.34156206250190735, 1.0542395114898682, 0.4019347131252289, -0.06038738414645195, 0.08512218296527863, -1.0181411504745483, -0.31316184997558594, 0.3444911241531372, 0.3137824535369873, 0.40594682097435, -0.4758903682231903, -0.14843390882015228, 0.23964554071426392, 0.5839951634407043, 0.6268954873085022, -0.2793312668800354, 1.0713794231414795, 0.886054277420044, 0.0017692794790491462, 0.030825722962617874, 0.21405631303787231, 0.2974243760108948, 0.15605506300926208, -1.2478430271148682, -0.19190338253974915, -0.04650357738137245, 1.0313961505889893, -0.2474675178527832, 0.6221998333930969, -1.02720046043396, -0.5116102695465088, 0.608026385307312, 0.12888842821121216, 0.05966481193900108, -0.24192357063293457, -0.08553168177604675, -0.36105144023895264, 0.037701912224292755, -0.4754510521888733, 0.19907452166080475, -0.3546530306339264, -1.0581409931182861, -0.6238365769386292, -0.85707688331604, 0.7141574025154114, -0.6786348223686218, 0.06463859230279922, -0.27336978912353516, 0.4980968236923218, -0.4018268883228302, 0.46031415462493896, 0.46621814370155334, 0.42982217669487, 0.2936761975288391, 0.3701056241989136, 1.3803281784057617, -1.1352494955062866, -0.2108226865530014, -1.0551356077194214, 0.30908310413360596, -0.5911418795585632, 0.29176267981529236, -0.29808473587036133, -1.5520617961883545, -1.299694538116455, -1.0057470798492432, 0.09438134729862213, -0.5257753133773804, 0.34850013256073, 1.1170541048049927, 0.3230057656764984, -1.5459634065628052, 0.4108341336250305, -0.8255330324172974, 0.10602165758609772, 0.4659028649330139, 0.4069066345691681, 0.993446409702301, 0.047748155891895294, -0.49855732917785645, -0.07144341617822647, -0.19366568326950073, -0.9712318181991577, -0.3265746533870697, -1.1010794639587402, -0.7183542847633362, 0.6669678092002869, 0.12981341779232025, -0.7513630390167236, 1.3100656270980835, -0.1966942995786667, -0.5560721755027771, 0.6915115714073181, -0.642234742641449, 0.09742949903011322, -0.363312691450119, -0.29808467626571655, -0.3202107548713684, -0.12337800115346909, -0.2509547770023346, 0.3344839811325073, 0.9984368085861206, -0.04330527037382126, -0.045316532254219055, 0.27944815158843994, -0.2715499699115753, -0.20130737125873566, -0.631515383720398, 1.087867259979248, -0.3718857765197754, -0.11301123350858688, -0.04203655943274498, 0.5584559440612793, -0.46694353222846985, -0.21614453196525574, -0.30368632078170776, -0.6467720866203308, 0.9204539060592651, 0.7791668772697449, 1.4463521242141724, -1.0357615947723389, -0.8320454955101013, -0.14126937091350555, 0.14336538314819336, 0.20384736359119415, -0.09746354818344116, 0.2732636034488678, -0.7880618572235107, -0.11253201216459274, 0.1103673055768013, -0.5579301714897156, -0.04219132289290428, -0.9940430521965027, -1.1686229705810547, -0.3475376069545746, 0.0023999337572604418, 1.0588302612304688, -0.6007232069969177, 0.2963576912879944, -0.1802573949098587, 0.30718350410461426, -1.0034769773483276, 1.0999083518981934, 0.03434326499700546, -0.2107478380203247, -0.262277215719223, 0.05876738950610161, 0.020527178421616554, -1.2269501686096191, 0.31775912642478943, -1.0232278108596802, -0.09632880240678787, 0.14830316603183746, 0.2375360131263733, 1.0033818483352661, -0.32822251319885254, 0.3468509912490845, 0.14434553682804108, -0.7780494689941406, 0.22097019851207733, -0.04056583717465401, -0.04535915330052376, -0.5441982746124268, 0.7588644027709961, 0.1541820615530014, -0.5313835144042969, 0.631746768951416, 1.5335073471069336, 1.5136677026748657, -0.7004237771034241, 0.3737628757953644, 0.0981263518333435, -0.12418807297945023, 0.119308702647686, 0.4278791546821594, 1.3258402347564697, 0.16507558524608612, 0.402554988861084, -0.6646220684051514, 0.5259305834770203, -0.8336466550827026, -0.6028620600700378, 0.5813318490982056, 0.42148008942604065, 0.21030089259147644, 0.404779851436615, -1.036048173904419, -0.8332609534263611, -0.15420891344547272, 0.526368260383606, 1.761789083480835, 0.05998283624649048, 0.1008729562163353, -0.7766492962837219, -0.283307820558548, -0.3068638741970062, 0.04455846548080444, 0.11597588658332825, -0.47862327098846436, -0.6175223588943481, -1.1504017114639282, 0.6793282628059387, 0.8475679159164429, 0.6518468260765076, -0.612509548664093, -1.4539412260055542, -0.5504714846611023, 0.8838121294975281, -0.7828453779220581, -0.7945262789726257, 0.5057370066642761, -0.4747617244720459, 0.167221799492836, 0.3890233337879181, -0.06893178075551987, 0.6302816271781921, 0.08381398022174835, 1.2537665367126465, -0.10958898067474365, -0.9880242943763733, 0.27963683009147644, 0.6827242970466614, -0.2857040464878082, -0.28336942195892334, 0.5110018849372864, -0.25009387731552124, -0.22917971014976501, 0.5170045495033264, -0.1321580559015274, -0.22803282737731934, 0.08285562694072723, -0.5902591347694397, 0.013728044927120209, 0.24285732209682465, 0.1655322015285492, 0.5670733451843262, -0.7619679570198059, -0.027228331193327904, -0.7657411694526672, 0.5630931258201599, 0.10690343379974365, -0.372610867023468, -0.05613170564174652, -0.5227134823799133, -0.1812836080789566, 0.4622059464454651, -0.8608211278915405, -0.08832544088363647, -0.8520954251289368, 0.3967406153678894, -0.6746747493743896, -0.16143885254859924, -0.6108418107032776, 0.3209304213523865, -0.2246903032064438, 0.3397274315357208, 0.49416711926460266, 0.21153725683689117, 0.5276098847389221, 0.4411202669143677, -0.8392137289047241, 0.6248044967651367, -0.4063486158847809, -0.17116563022136688, 0.06313646584749222, 0.18408192694187164, -0.7264074087142944, -0.35342133045196533, -0.08509477972984314, 0.0369018092751503, -0.5110771059989929, 0.06394807994365692, -0.44568219780921936, -1.3260830640792847, -0.3057097792625427, -1.336169719696045, -0.33829864859580994, 0.2961532771587372, -0.7166728377342224, -0.3075070381164551, -1.183716893196106, -1.3216053247451782, -0.4035860598087311, -1.452507734298706, -1.6294939517974854, 0.8674600720405579, 0.2915858030319214, -1.0427098274230957, -0.15952442586421967, -0.575105607509613, -0.7985569834709167, 1.500157117843628, -0.4213889241218567, 0.6357960104942322, -0.03942951187491417, -0.9805126190185547, 0.12026422470808029, -0.34870898723602295, -0.33217987418174744, -0.9362854957580566, 0.3297809660434723, -0.801017165184021, 0.052163995802402496, -0.5648812651634216, -0.27345341444015503, 0.1572583019733429, 0.26428380608558655, 1.4030137062072754, 0.03598574548959732, -0.9290107488632202, 0.2417958825826645, 1.3994569778442383, -0.5704526305198669, 0.25814107060432434, -0.06087611988186836, 0.7388449907302856, -0.4852646589279175, -0.03808661922812462, 0.9756772518157959, -0.1101340800523758, 0.5783716440200806, 0.3373224139213562, -0.41990378499031067, -0.15402880311012268, 0.29321208596229553, 0.4447420835494995, 1.3800451755523682, 0.7556444406509399, 0.07765953242778778, -0.6857132315635681, 0.7452937960624695, -1.1636162996292114, -0.3071761429309845, 0.5093070268630981, 0.9710190892219543, -0.0004968981957063079, -0.15261198580265045, -0.08755680918693542, -0.05689823254942894, 0.7523455619812012, 0.4611729085445404, -0.6158515810966492, -1.265665054321289, 0.3857758641242981, 1.2552435398101807, 0.7059445977210999, 0.5563210248947144, -0.02527960017323494, 0.5264619588851929, 14.51030445098877, 0.9654990434646606, -0.005227863322943449, 0.6604183316230774, 0.9939053058624268, 0.1578574925661087, -0.4232167601585388, 0.22242231667041779, -1.5144933462142944, 0.22191256284713745, 1.5317623615264893, -0.08753466606140137, 0.024788077920675278, 0.48729026317596436, -0.4600493311882019, -0.06737318634986877, -0.7064898610115051, 0.4372973144054413, 0.64146888256073, -1.5006484985351562, 0.18810595571994781, 0.31253135204315186, 0.1737440675497055, 0.06368869543075562, 1.0602445602416992, 0.4751068949699402, 0.5983436107635498, -0.32055655121803284, 0.391602486371994, 0.23316523432731628, 1.2429413795471191, -0.5031778812408447, 0.15860767662525177, -0.02453770488500595, -1.280227541923523, 0.24531340599060059, -0.1032201424241066, -1.2739166021347046, 0.36617499589920044, 0.5307289958000183, -0.6119359135627747, -0.6717127561569214, -0.06022927910089493, 0.15464970469474792, 0.27053913474082947, 0.4076624810695648, 0.1435387283563614, 0.28054559230804443, -0.032381657510995865, -0.14052753150463104, -0.1787928193807602, 0.9983508586883545, -0.3382900655269623, 0.28985947370529175, 0.22119441628456116, -0.20139838755130768, 0.43380022048950195, 0.4063096046447754, -0.2394362986087799, -0.54705810546875, -0.26222315430641174, 0.15020862221717834, 0.2772098183631897, 1.1075267791748047, -0.07547427713871002, 0.054214395582675934, -0.5062950253486633, 0.3016762137413025, 0.2960836589336395, -0.10508301854133606, -0.724780797958374, -0.3907497823238373, 0.5724159479141235, -0.4732894003391266, 0.04776139184832573, 0.6986570358276367, -0.8460679054260254, -0.27463391423225403, -0.9676289558410645, -0.591133713722229, 0.18288874626159668, -0.6663821339607239, -0.3982425630092621, 0.7054550647735596, -0.22349606454372406, 0.019718779250979424, 0.6524530053138733, -0.880567729473114, -0.6036931872367859, 0.40759360790252686, -0.9184499979019165, -0.40749913454055786, -0.06978421658277512, -0.6958404779434204, -0.32867974042892456, 0.1880178302526474, 0.9645144939422607, -0.02879912406206131, -0.06257322430610657, 0.30944669246673584, -0.6245094537734985, -0.51437908411026, -0.1485716849565506, -0.3953600525856018, 1.4747395515441895, 0.49647238850593567, -0.8610158562660217, 0.23554252088069916, 0.25629496574401855, 0.0573757141828537, -1.294286847114563, 0.038850437849760056, 0.26305413246154785, -0.8371454477310181, 0.18049579858779907, -0.49860987067222595, -0.7326069474220276, 0.431918203830719, 0.5632972717285156, 0.41294974088668823, 0.30471697449684143, -0.1106833666563034, -0.34642720222473145, -0.32118937373161316, -0.34121009707450867, 0.19515405595302582, 0.7153444290161133, -0.7835391163825989, 0.046320635825395584, -0.5363430380821228, 0.4392774999141693, -1.2361860275268555, -0.7573212385177612, 0.033592768013477325, 0.06809723377227783, -0.31253916025161743, 1.3495314121246338, -0.036935724318027496, 1.0128507614135742, 0.7684025168418884, -0.0548604317009449, -0.25615331530570984, -0.24039366841316223, -0.8455826640129089, -0.5045329332351685, 0.026021169498562813, 0.0061715939082205296, -0.14622493088245392, 0.779845118522644, 0.3481226861476898, -0.147038072347641, -0.6118559241294861, -0.16246983408927917, 0.3434365689754486, -0.440603643655777, -0.4281691312789917, 0.4771852493286133, 0.004524868912994862, 0.14942532777786255, 0.2867526412010193, 0.5156155824661255, 0.7904182076454163, 0.0900442972779274, -0.2851162552833557, 0.6749844551086426, 0.04214286804199219, -0.36290040612220764, -0.923335075378418, -0.7835702300071716, -1.454560399055481, -0.018410371616482735, -1.1031743288040161, -0.01783595234155655, -0.03681220859289169, -0.3302035927772522, 0.06900874525308609, -0.13140514492988586, 0.2076212465763092, -0.08087487518787384, -0.19415739178657532, -0.7123042941093445, -0.8264989256858826, -0.8190189599990845, 0.668134331703186, 0.4119991660118103, -0.19730690121650696, 0.4002039134502411, -0.28000470995903015, -0.268175333738327, 0.3320235013961792, 0.2899143099784851, -0.10376611351966858, -0.5899636149406433, -1.0444660186767578, 0.1704581081867218, 0.14164863526821136, 0.12703754007816315, -0.9952166676521301, 1.079640507698059, 0.39263466000556946, -0.07519541680812836, -0.263445645570755, 0.2779502272605896, -0.7264407873153687, -0.40839582681655884, 0.6074461340904236, -0.4534195065498352, 0.6837854981422424, 0.9034363031387329, -0.7275882959365845, 0.25821375846862793, 0.9676198363304138, -0.19036106765270233, -0.5567845106124878, -1.0087412595748901, 0.43329522013664246, -0.5185574293136597, 0.08853206783533096, 0.0920908972620964, -0.14716516435146332, -1.415687918663025, -0.020740875974297523, 0.009553546085953712, 0.025191202759742737, 0.1372571885585785, 0.649182915687561, 0.36042967438697815, -1.1213244199752808, 0.1486993432044983, 0.5347957611083984, -0.18270547688007355, -0.11686794459819794, 0.662030041217804, 0.6600209474563599, -0.23592917621135712, 0.32990941405296326, -0.16487178206443787, 0.0005779417115263641, -1.1478747129440308, 0.3706871569156647, -0.07023333758115768, -0.5268673896789551, 0.14759179949760437, 0.9913855791091919, 0.04659469425678253, -0.3456551432609558, -0.3555540442466736, -1.0449482202529907, 0.10129382461309433, -0.4809928834438324, 0.8372637629508972, 0.3570564091205597, 0.22473376989364624, 0.2690412104129791, -0.8757503628730774, 0.15192222595214844, -0.14026108384132385, -0.1296151727437973, 0.13011424243450165, 0.14403928816318512, -0.706725001335144, 0.057412438094615936, 0.47447168827056885, -0.5025815367698669, -0.35772910714149475, -0.7795775532722473, -0.2528858482837677, -0.08053487539291382, 0.5931591987609863, 0.2458474487066269, -0.5505285263061523, 0.9289563298225403, 0.2753097116947174, 0.5203354954719543, 0.6883021593093872, -0.21089954674243927, 0.2933765947818756, -0.16848517954349518, 0.3964973986148834, -0.12662874162197113, -0.35239318013191223, 1.1974276304244995, 0.6385465264320374, -0.10734530538320541, 0.6180580854415894, -0.6054365038871765, -0.11483383178710938, 0.8209044337272644, 0.8112925291061401, -0.14437958598136902, 0.6539450287818909, 1.0698397159576416, -0.7644878029823303, 0.2045297771692276, -1.0792142152786255, -0.21153347194194794, 0.8121755719184875, 0.7693209648132324, 0.6522603631019592, 0.3893015384674072, 0.12673263251781464, 0.5441235303878784, 0.34284090995788574, 0.40222588181495667, 0.37318864464759827, 0.715169370174408, -0.306579053401947, 0.015614948235452175, -0.3644668161869049, 0.643866777420044, -0.6212050318717957, -0.9656960368156433, 0.5088413953781128, 0.5798287987709045, -0.2680596709251404, 0.07042890042066574, 1.4855064153671265, -0.00020976619271095842, 0.46259015798568726, -0.234524205327034, 0.6632724404335022, -0.48409217596054077, -0.39865633845329285, -0.2100447714328766, -0.6553160548210144, -0.17934168875217438, 0.13685691356658936, -0.14877356588840485, -0.15704886615276337, -0.6661979556083679, 0.551408588886261, -0.20707309246063232, 0.12870019674301147, 0.6232865452766418, 1.2660027742385864, 1.5680779218673706, -0.28785014152526855, -1.2777975797653198, -0.31301963329315186, -0.3923487067222595, 0.660372793674469, -0.770080029964447, -0.18036037683486938, -0.08379761874675751, 0.09670794755220413, -0.17104533314704895]}, "authors": [{"authorId": "152883643", "name": "Nandeeka Nayak"}, {"authorId": "2307828135", "name": "Xinrui Wu"}, {"authorId": "1577826085", "name": "Toluwanimi O. Odemuyiwa"}, {"authorId": "1790200", "name": "Michael Pellauer"}, {"authorId": "1775477", "name": "J. Emer"}, {"authorId": "2307010096", "name": "Christopher W. Fletcher"}], "references": [{"paperId": "288ba5795dc3f89e28a0f4c61827d6a3cd6ca4f0", "title": "The EDGE Language: Extended General Einsums for Graph Algorithms"}, {"paperId": "c999a3ef8397ed157f6e3f5d1e41ac45ea1149ec", "title": "TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis"}, {"paperId": "1b3aecc7dc11a9203fcb7be28e4ed512970dd1ea", "title": "DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators"}, {"paperId": "e90d9a96fa40d6c64bf298988222d1eb5ca4a6c3", "title": "Symphony: Orchestrating Sparse and Dense Tensors with Hierarchical Heterogeneous Processing"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "d4bfbc2874baddf3f562603665e936707db7d340", "title": "HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity"}, {"paperId": "f6c15f15b98afeb8a23e321e098b7fd9f3bce85a", "title": "TeAAL: A Declarative Framework for Modeling Sparse Tensor Accelerators"}, {"paperId": "e92a5332390f0ba94615935541da4da9bed56512", "title": "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization"}, {"paperId": "4ce86e7bb5a598ff0b5f07f040e1910bc50c8c49", "title": "Accelerating Sparse Data Orchestration via Dynamic Reflexive Tiling"}, {"paperId": "17a8bd6a5763f6607863348ce1757ac2ad3417fd", "title": "Accelerating Transformer Networks through Recomposing Softmax Layers"}, {"paperId": "7906159afac47695c5d87bc38baf8c75d3512c24", "title": "The Sparse Abstract Machine"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "732fa35f283143e05a6c4224931c22f79ad95b2a", "title": "Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"}, {"paperId": "4fffa5245d3972077c83614c2a08a47cb578631e", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"}, {"paperId": "f2cab59108f85157e180c7384f57d99ea2f7fae9", "title": "Low-Latency Bit-Accurate Architecture for Configurable Precision Floating-Point Division"}, {"paperId": "ea76c776b3cd3ad448572999ad3b4323abe47453", "title": "Gamma: leveraging Gustavson\u2019s algorithm to accelerate sparse matrix multiplication"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "97bac618fc866ae7656660f3965e9aae37993232", "title": "Efficient Processing of Deep Neural Networks"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "501e0b0bc563870a034b6761fefb80b3ecd13064", "title": "A Survey of Accelerator Architectures for Deep Neural Networks"}, {"paperId": "069e0d896da7c79faeee4cf057548d5da7ce885e", "title": "FlauBERT: Unsupervised Language Model Pre-training for French"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "8051e77dd37ba276b10a53da5b130d6b755c68c3", "title": "Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "9ded246119861dd325e7004b2050bba310e08797", "title": "Timeloop: A Systematic Approach to DNN Accelerator Evaluation"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "92f07228b76f68be566c32c7a26d8a7ac143135a", "title": "Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks"}, {"paperId": "1f0bbcbcea15b60b39012e9aedf4dac42dff9411", "title": "Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach"}, {"paperId": "1b990c161ae40ff85031802f8808f7fd1c47a57f", "title": "Tensor Network Contractions"}, {"paperId": "1fe1033a508caa43dea180f4faa135c57d931752", "title": "Plasticine: A reconfigurable architecture for parallel patterns"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "5ec594e9f5ca4b629be28625cd78c882514ea3be", "title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks"}, {"paperId": "4157ed3db4c656854e69931cb6089b64b08784b9", "title": "DaDianNao: A Machine-Learning Supercomputer"}, {"paperId": "199784f05c34bf4f9669cf79617d12dd40e36001", "title": "Hardware implementation of the exponential function using Taylor series"}, {"paperId": "579d73897ff190e6353ae4eea5ce22afee9fadf0", "title": "Efficient Spatial Processing Element Control via Triggered Instructions"}, {"paperId": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd", "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"}, {"paperId": "2a0b4deff8539840001131cb9e34b60028ffc667", "title": "An overview of the sparse basic linear algebra subprograms: The new standard from the BLAS technical forum"}, {"paperId": "ef62e60b81317a24dbeb8ded6dc4a8ed89b776a8", "title": "Basic Linear Algebra Subprograms for Fortran Usage"}, {"paperId": "5d69b3f00e029c8a4a25447539e4a87c7833fc1f", "title": "Unified Convolution Framework: A compiler-based approach to support sparse convolutions"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cTensorFlow: Large-scale machine learning on heterogeneous systems,\u201d"}, {"paperId": "1f462943c8d0af69c12a09058251848324135e5a", "title": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"}, {"paperId": null, "title": "\u201cGPTQ: Accurate post-training compression for generative pretrained transformers,\u201d"}, {"paperId": null, "title": "\u201cOur next-generation model: Gemini 1.5,\u201d"}]}