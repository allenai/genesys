{"paperId": "0ea8ceb4f6a759b07a5b871fd25cb5b7bf33acd8", "abstract": "Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while preserving their performance, thereby opening avenues for significantly more efficient use of LLMs.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Surprisingly, it is observed that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks."}, "embedding": {"model": "specter_v2", "vector": [0.3058904707431793, 0.8246085047721863, -0.23709671199321747, 0.04226101189851761, -0.3251115381717682, -0.08774228394031525, 0.6094443202018738, -0.21875643730163574, -0.5490577816963196, -0.1669689118862152, 0.5015555024147034, -0.40092718601226807, 0.18394502997398376, 0.5310536623001099, 0.14258059859275818, 0.2831193804740906, -0.7739923000335693, 0.47129568457603455, -0.052965570241212845, -0.22961658239364624, -0.3092157244682312, -0.5381877422332764, -0.9627085328102112, 0.1119813472032547, 0.5857729315757751, 0.7272708415985107, 0.15457886457443237, 0.9234686493873596, -0.5199865102767944, 0.45864441990852356, 0.5299908518791199, -0.48940950632095337, 0.19108065962791443, 0.17697712779045105, -0.007857396267354488, -0.15928710997104645, 0.6116561889648438, -0.44936349987983704, -0.32236549258232117, 0.7734317183494568, -0.3173640966415405, 0.16273079812526703, 0.24160055816173553, -0.8808155059814453, 0.050742968916893005, 1.0725946426391602, 0.4224960207939148, 0.7995880246162415, -0.4394005835056305, -0.5086560249328613, 1.093643069267273, -1.2948153018951416, 0.07313330471515656, 1.6921207904815674, 0.8263447880744934, 0.3851326107978821, -0.3452119827270508, -1.0132524967193604, 1.0095183849334717, -0.06893191486597061, -0.8707754015922546, -0.5945639610290527, -0.2016115039587021, -0.15243062376976013, 2.1310925483703613, -0.2788098454475403, -0.38675737380981445, 0.3714049160480499, -0.4569598436355591, 1.6354992389678955, -0.05204606056213379, -0.6611742973327637, -0.3828851878643036, 0.25077587366104126, 0.4802454113960266, 0.9751577377319336, -0.21857763826847076, 0.14791269600391388, -1.012384057044983, -0.6522508859634399, -0.07582923024892807, -0.2915673553943634, 0.11944268643856049, 0.10485628247261047, -0.02654307521879673, 0.8382588624954224, 0.36019429564476013, 0.4724162220954895, -0.2429606318473816, 0.6882757544517517, 0.41310790181159973, 0.3993573486804962, 0.356658935546875, 0.541054904460907, -0.6294716000556946, 0.7055926322937012, -0.9234821200370789, 0.14398348331451416, 0.2851087152957916, 0.802102267742157, -0.49796363711357117, 0.2267446666955948, -0.604153573513031, 0.338703989982605, 1.1176261901855469, -0.10381191968917847, 0.3146488070487976, -0.4940906763076782, 0.74114990234375, -0.4638892114162445, 0.027497325092554092, -0.3262057602405548, -0.46537119150161743, -0.3166896104812622, -0.9354032278060913, -1.457502841949463, -0.3406927287578583, -0.4095136225223541, -0.5765413641929626, 0.8702053427696228, -0.24749483168125153, 0.20360834896564484, -0.04141451045870781, 0.001215789350681007, 0.2618381679058075, 0.8329306244850159, 0.6332570910453796, 0.07265881448984146, 1.1160814762115479, -0.7567827701568604, -0.4998098611831665, -1.3961769342422485, 0.950322687625885, -0.15138347446918488, 0.2557049095630646, 0.003214232623577118, -1.137400507926941, -0.561493456363678, -0.6899278163909912, -0.07759249955415726, -0.5249125361442566, 0.6725319027900696, 1.1072156429290771, 0.1527550220489502, -0.6919886469841003, 0.7854427695274353, -0.09182687848806381, -0.05863000825047493, 0.548088014125824, 0.1949477642774582, 0.2938482165336609, -0.4393475353717804, -1.1826562881469727, 0.5254483819007874, 0.3792210519313812, -0.25892752408981323, 0.1264171451330185, -0.7355039715766907, -0.9819076657295227, 0.16036732494831085, 0.3210489749908447, -0.6391127705574036, 1.424109697341919, 0.22534099221229553, -1.0679612159729004, 0.4242208003997803, -0.38692519068717957, 0.1030583307147026, 0.32683396339416504, -0.292251318693161, -0.5989930629730225, -0.5039530992507935, -0.6452276110649109, 0.5366678237915039, 0.5296596884727478, 0.1619572788476944, -0.48880213499069214, 0.305509090423584, -0.22699454426765442, -0.24159857630729675, -0.43324920535087585, 1.0992584228515625, -0.3246082365512848, -0.14535439014434814, 0.6021662950515747, 0.5918097496032715, -0.3402732014656067, -0.1602160483598709, -0.6627581119537354, -1.2330727577209473, 0.6333886384963989, -0.5016820430755615, 1.1711163520812988, -0.7127276659011841, -0.7336640357971191, -0.28591421246528625, 0.12895560264587402, 0.1782589703798294, -0.8430539965629578, 0.6788163185119629, -0.14184033870697021, 0.7448058128356934, -0.20672869682312012, -1.3968836069107056, 0.21503083407878876, -0.38139089941978455, -0.6680264472961426, -0.30379748344421387, 0.2811163663864136, 0.7833216190338135, -0.5353971719741821, -0.024768223986029625, -0.19295068085193634, 0.41271793842315674, -1.1857924461364746, 0.9344332814216614, -0.6441957354545593, -0.015554076060652733, -0.1186346784234047, -0.2341192662715912, 0.31531208753585815, -0.20873130857944489, 0.4183727204799652, -0.3532126843929291, -0.3650955557823181, 0.7939173579216003, -0.434823215007782, 1.005562424659729, -0.5139883756637573, 0.42823001742362976, 0.2143799513578415, -0.37226954102516174, -0.37938985228538513, 0.4162241220474243, -0.644625723361969, -0.1548268049955368, 0.14706838130950928, 0.577757716178894, -0.446144700050354, 0.46946337819099426, 0.6840158104896545, 0.7646200656890869, -0.31821876764297485, 0.246235191822052, 0.5661783814430237, -0.16492721438407898, 0.6949030756950378, 0.6747918128967285, 0.49934643507003784, 0.1649484783411026, 0.22057677805423737, 0.040032099932432175, 0.39180630445480347, -0.7947760820388794, 0.1057470440864563, 0.3513542711734772, 0.8364762663841248, 0.4757947623729706, 0.7916853427886963, -0.6447083950042725, -0.3360351026058197, 0.3697810173034668, 0.5273716449737549, 1.5757683515548706, -0.8103283047676086, -0.029913144186139107, -0.48387399315834045, -0.4013022184371948, 0.06445897370576859, 0.1749119609594345, -0.14173315465450287, 0.22200427949428558, -0.6721628904342651, -1.3195440769195557, 1.008948802947998, -0.046264976263046265, 0.6444321274757385, -0.3214234709739685, 0.10160636901855469, -0.316899836063385, 0.3645528554916382, -1.0957685708999634, -0.15917105972766876, 0.5035684108734131, -0.9394113421440125, -0.3133503794670105, 0.02031879313290119, 0.041071683168411255, 0.29505008459091187, -0.7224932312965393, 0.8512134552001953, -0.48409274220466614, -0.17305871844291687, -0.2813993990421295, 0.6750863790512085, -0.7975450158119202, -0.7258950471878052, 0.4607960283756256, 0.2738284468650818, -0.3174377977848053, 0.2532494068145752, 0.763695478439331, 0.3907179832458496, -0.27922776341438293, -0.05896089971065521, 0.4785143733024597, 0.2783794105052948, -0.2424757033586502, 0.5227309465408325, -0.2988039255142212, 0.3530186116695404, -1.6959823369979858, 1.0603845119476318, -0.00863820593804121, -0.7516617178916931, 0.5050384402275085, -0.6104762554168701, -0.27726244926452637, 0.6964084506034851, -0.6775860786437988, -0.44106096029281616, -1.0358775854110718, 0.13933424651622772, -0.23225688934326172, -0.23715530335903168, 0.3053969442844391, 0.4486481845378876, 0.28987711668014526, 0.05299529433250427, 0.1833595335483551, 0.309538334608078, -0.8758183121681213, 0.38449838757514954, -0.6895111203193665, 0.18460498750209808, 0.6210741400718689, 0.2826920449733734, -0.5229129195213318, -0.3203257620334625, -0.832726240158081, -0.6955329775810242, -0.2691671848297119, -0.13829626142978668, 0.42453882098197937, -0.1683710813522339, -0.6641407012939453, -0.46129530668258667, -0.17756985127925873, -1.0342848300933838, -0.2961433529853821, 0.3270982801914215, 0.0012206799583509564, 0.2545841932296753, -1.2270188331604004, -1.6448994874954224, -0.6082359552383423, -0.6340137124061584, -0.792384922504425, 0.5245529413223267, -0.02395552769303322, -0.39054131507873535, -0.49302512407302856, -0.2021668702363968, -0.303859680891037, 1.0193232297897339, -0.932784378528595, 1.1848137378692627, -0.28613004088401794, 0.02912995032966137, -0.636391282081604, -0.0765732154250145, 0.5131980180740356, -0.6795833706855774, 0.1853957325220108, -1.1701252460479736, 0.1821884661912918, -0.4801565408706665, -0.1394309103488922, 0.2572008967399597, 0.30517667531967163, 0.6824594140052795, -0.22259558737277985, -0.7329720258712769, 0.6281305551528931, 1.2722256183624268, -1.152985692024231, -0.2791762351989746, -0.30657264590263367, 1.0544829368591309, 0.19592733681201935, -0.516619086265564, 0.4151563048362732, 0.11821313202381134, 0.033142171800136566, 0.08032691478729248, -0.26506248116493225, -0.25749441981315613, -0.6461656093597412, 0.3267093002796173, 1.7845913171768188, 0.544346272945404, -0.2373768389225006, -1.0610089302062988, 0.21230895817279816, -1.004112720489502, -0.2850518226623535, 0.31437528133392334, 0.6671513319015503, 0.4350039064884186, -0.3979456424713135, -0.5872467756271362, -0.19305898249149323, 0.3412668704986572, 0.3604488968849182, -0.49783632159233093, -0.753434419631958, 0.23017577826976776, 0.12789833545684814, 0.4849981963634491, 0.5890285968780518, -0.4213405251502991, 0.8655588626861572, 14.649596214294434, 0.7970288991928101, 0.006983554922044277, 0.7087588310241699, 0.6372939348220825, -0.010898725129663944, 0.05632719025015831, -0.26126012206077576, -1.7858597040176392, -0.10888765007257462, 1.4071907997131348, 0.31615278124809265, 0.7173036336898804, 0.08003908395767212, 0.212893545627594, 0.024871135130524635, -0.5083751678466797, 0.6559720039367676, 0.28342145681381226, -1.2226392030715942, 0.5642504692077637, 0.24583271145820618, 0.38504186272621155, 0.9141099452972412, 0.5172215104103088, 1.1320161819458008, 0.6340157389640808, -0.48975372314453125, 0.3533717095851898, 0.0790926069021225, 0.634909987449646, 0.034599535167217255, 0.4221540093421936, 1.203029990196228, -0.6652137041091919, -0.4660254716873169, -0.7235543131828308, -1.3621245622634888, 0.1746612787246704, 0.42487141489982605, -0.609290599822998, -0.6110498905181885, -0.49216318130493164, 0.6147013902664185, -0.31607866287231445, 0.5432921648025513, -0.12230021506547928, 0.771848201751709, -0.5599098801612854, 0.16418181359767914, 0.230936661362648, 0.6130467653274536, 0.20862938463687897, 0.13416826725006104, -0.0005545622552745044, -0.2896730601787567, 0.22858913242816925, 0.3179187476634979, -0.7435767650604248, -0.22330646216869354, -0.32902052998542786, -0.387371301651001, 0.15691091120243073, 0.7985528111457825, 0.6301165223121643, -0.05353320762515068, -0.3718317151069641, 0.033281031996011734, 0.8755266070365906, 0.2688431739807129, -0.00861147977411747, 0.23932217061519623, 0.47907131910324097, -0.5914834141731262, -0.18775589764118195, 0.5557454824447632, -0.2335061877965927, -0.730079710483551, -0.6598620414733887, -0.357520192861557, 0.4133104085922241, -0.5889050960540771, -0.7877702116966248, 0.6222833395004272, -0.23395588994026184, -0.10311415791511536, 0.13667446374893188, -0.7481811046600342, 0.05379343405365944, 0.7709373235702515, -1.7132798433303833, -0.7736803293228149, 0.6143473982810974, -0.5536825656890869, -0.0894000381231308, -0.05236402899026871, 1.4427822828292847, 0.3335862457752228, -0.9972814321517944, 0.14654000103473663, 0.08933977782726288, 0.046983782202005386, -0.7271178364753723, -0.38012683391571045, 1.0569010972976685, 0.2648327052593231, -0.1843373030424118, 0.2409762442111969, -0.19170628488063812, 0.21046249568462372, -0.7885031700134277, -0.1058582067489624, 1.3199536800384521, -0.7930592894554138, -0.2857368290424347, -0.7554499506950378, -0.7171846032142639, 0.36701449751853943, 0.11766046285629272, -0.1690729707479477, 0.3409241735935211, 0.43095049262046814, -0.8288149237632751, 0.17345187067985535, -0.6333468556404114, 0.08214198797941208, 0.3483726382255554, -0.9086419343948364, -0.11066547781229019, 0.3197071850299835, 0.2989526093006134, -0.6429372429847717, -0.35718169808387756, -0.05161873623728752, -0.02524007111787796, 0.2424008995294571, 0.8674885630607605, -0.6184408664703369, 0.5316065549850464, 0.9981026649475098, -0.02704641781747341, -0.836210310459137, 0.04580831155180931, -0.7695907354354858, -0.3061457574367523, 0.04299949109554291, 1.013218879699707, -0.6123087406158447, 0.0967526063323021, 1.0608491897583008, 0.15221287310123444, -0.41242626309394836, -0.4951731264591217, -0.27644285559654236, 0.10780827701091766, -0.4918782413005829, 0.2800903618335724, 0.06570343673229218, -0.16886167228221893, 0.16035468876361847, 0.4796367883682251, 0.844109058380127, -0.0792788714170456, -1.0074797868728638, 0.25983089208602905, -0.5734438896179199, 0.07702832669019699, -0.6196480393409729, 0.048474397510290146, -1.269555687904358, 0.10578235983848572, -1.1181508302688599, -0.08710266649723053, -1.1953208446502686, -0.5767343640327454, -0.2830686569213867, -0.10682277381420135, 0.21739907562732697, 0.32287493348121643, -0.42996177077293396, -0.3667523264884949, -0.369396448135376, -0.30763909220695496, 0.4850304424762726, 0.733474850654602, -0.4645947217941284, 0.11102388054132462, 0.14905168116092682, 0.02607470564544201, 0.8617202043533325, 0.5256684422492981, -0.4754980504512787, -0.762345552444458, -1.7245484590530396, 0.7423840165138245, 0.05057815462350845, -0.20875564217567444, -0.4698696434497833, 0.9011929035186768, 0.3203273117542267, -0.32437169551849365, 0.04647402837872505, 0.43068867921829224, -0.9509974122047424, -0.8442074656486511, 0.2891668379306793, -0.8067213296890259, 0.08076748996973038, 0.38933664560317993, -0.5635634660720825, -0.29761531949043274, 0.47310394048690796, -0.1990903913974762, -1.0336161851882935, -0.6735288500785828, 0.31618624925613403, -0.38390591740608215, -0.02267357148230076, -0.509355366230011, 0.09613484144210815, -1.0526422262191772, 0.20732353627681732, 0.16915877163410187, 0.5282328128814697, -0.6797617077827454, 0.8105229735374451, -0.042902857065200806, -0.5601523518562317, -0.20015770196914673, 0.4713367819786072, -0.2883470058441162, 0.1083543449640274, 0.4929981529712677, 0.5723500847816467, -0.41038331389427185, 0.8144290447235107, 0.7941283583641052, 0.5839540362358093, -0.9559944868087769, -0.23509858548641205, 0.723993718624115, -0.7124952077865601, -0.5587899684906006, 1.3235058784484863, -0.21675808727741241, -1.3489210605621338, 0.5332804918289185, -1.5437308549880981, -0.4375143349170685, -0.016727928072214127, 0.7893569469451904, 0.2554074227809906, 0.2878134548664093, 0.1646622121334076, -0.48505738377571106, -0.13217133283615112, 0.07553097605705261, -0.33770012855529785, 0.6892178654670715, -0.2379833310842514, -0.346809059381485, 0.5557101964950562, 0.8891889452934265, -0.7179017663002014, -0.9327225685119629, -0.7474233508110046, -0.03511688485741615, 0.03452213108539581, 0.6888800263404846, -0.7107883095741272, -0.4845511019229889, 0.767518162727356, 0.038231801241636276, 0.2676657438278198, -0.14597487449645996, -0.49519914388656616, 0.19701829552650452, 0.8090587258338928, 0.12070564180612564, -0.7372494339942932, -1.2509756088256836, 1.5084598064422607, 1.1833202838897705, -1.0586520433425903, 0.3968411087989807, -0.3621019721031189, -0.7651765942573547, 0.3425663411617279, 0.09110310673713684, -0.0066187409684062, 1.0738862752914429, -0.06819208711385727, -0.22389467060565948, 0.09961646050214767, -1.3155583143234253, -0.2110704928636551, 0.8836541771888733, 0.5020372867584229, 0.9133800864219666, 0.18816088140010834, 0.16370388865470886, 1.1022357940673828, -0.04491667449474335, 0.2310343086719513, 0.2846778929233551, 0.48145774006843567, -0.23894883692264557, 0.2611159086227417, 0.05003810301423073, 0.8508903384208679, -0.7500186562538147, -0.972517192363739, -0.11235938221216202, 0.7303314805030823, 0.2675345838069916, 0.523973286151886, 0.7768377065658569, 0.1598932147026062, 0.5335914492607117, 0.6688835620880127, 0.25016671419143677, -0.655834436416626, -0.41949182748794556, -0.3230227828025818, -0.27379992604255676, 0.23203158378601074, 0.3120095133781433, -0.36439236998558044, -0.07334551215171814, -0.389864057302475, 0.20130415260791779, -0.17791134119033813, 0.6583350896835327, 1.0808511972427368, 0.6012502908706665, 0.5865710973739624, -0.5467548370361328, -0.6257883310317993, -0.7280101776123047, -1.1734447479248047, -0.2041403204202652, -0.8141511678695679, -0.11671793460845947, 0.08724748343229294, -0.4262505769729614, -0.5425621271133423]}, "authors": [{"authorId": "2201687496", "name": "Shuzhou Yuan"}, {"authorId": "2197254657", "name": "Ercong Nie"}, {"authorId": "2188764477", "name": "Bolei Ma"}, {"authorId": "2281825175", "name": "Michael Farber"}], "references": [{"paperId": "7754ac3e8ff1286f17593159781487543cdddaba", "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"}, {"paperId": "ca53c1d1ba1a1386f860fa13d7729160571e1643", "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "7426dab0a96e9dfbb3fee77b286231fb0ab79daf", "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "40235eded15f44c8c4a7f48468adcc7df4e171fb", "title": "Rethinking Network Pruning \u2013 under the Pre-train and Fine-tune Paradigm"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "4a4646a5ce6b57e369403e4efea1a2e4559fe9f1", "title": "What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "675bb798f0cf542c0e10687c39482a8ff7e3318a", "title": "SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text"}, {"paperId": "f6fbb6809374ca57205bd2cf1421d4f4fa04f975", "title": "Linguistic Knowledge and Transferability of Contextual Representations"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers"}, {"paperId": "75895ce98904e8afaaa248f081a1da501bd2dbe2", "title": "Toward Semantics-Based Answer Pinpointing"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "89654f548e541a8ef233be6585316ce6cc201535", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "2022. Greedy-layer pruning: Speeding up transformer models for natural language processing"}, {"paperId": null, "title": "2023. How to train your (compressed) large language model"}]}