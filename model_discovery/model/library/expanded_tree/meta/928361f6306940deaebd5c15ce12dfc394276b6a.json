{"paperId": "928361f6306940deaebd5c15ce12dfc394276b6a", "abstract": "Recently, efficient Vision Transformers have shown great performance with low latency on resource-constrained devices. Conventionally, they use 4x4 patch embeddings and a 4-stage structure at the macro level, while utilizing sophisticated attention with multi-head configuration at the micro level. This paper aims to address computational redundancy at all design levels in a memory-efficient manner. We discover that using larger-stride patchify stem not only reduces memory access costs but also achieves competitive performance by leveraging token representations with reduced spatial redundancy from the early stages. Furthermore, our preliminary analyses suggest that attention layers in the early stages can be substituted with convolutions, and several attention heads in the latter stages are computationally redundant. To handle this, we introduce a single-head attention module that inherently prevents head redundancy and simultaneously boosts accuracy by parallelly combining global and local information. Building upon our solutions, we introduce SHViT, a Single-Head Vision Transformer that obtains the state-of-the-art speed-accuracy tradeoff. For example, on ImageNet-1k, our SHViT-S4 is 3.3x, 8.1x, and 2.4x faster than MobileViTv2 x1.0 on GPU, CPU, and iPhone12 mobile device, respectively, while being 1.3% more accurate. For object detection and instance segmentation on MS COCO using Mask-RCNN head, our model achieves performance comparable to FastViT-SA12 while exhibiting 3.8x and 2.0x lower backbone latency on GPU and mobile device, respectively.", "venue": "arXiv.org", "year": 2024, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper introduces SHViT, a Single-Head Vision Transformer that obtains the state-of-the-art speed-accuracy tradeoff and introduces a single-head attention module that inherently prevents head redundancy and simultaneously boosts accuracy by parallelly combining global and local information."}, "embedding": {"model": "specter_v2", "vector": [0.4401082396507263, 0.40175947546958923, -0.719246506690979, 0.429927796125412, -0.13512201607227325, 0.35561931133270264, 0.5900977253913879, -0.2017282098531723, -0.3664001226425171, -0.8574219942092896, 0.1722268909215927, 0.510844886302948, 0.8444310426712036, 0.040296025574207306, -0.01610628142952919, 0.4024982750415802, -0.5907105803489685, -0.17487730085849762, 0.6211323738098145, 0.07826392352581024, 0.4176206886768341, -0.5831807851791382, -1.4027656316757202, 0.1109592393040657, 0.09075217694044113, 1.3031312227249146, 0.5256564617156982, 0.9841753840446472, -0.15403199195861816, 0.5658044219017029, 0.2578132152557373, -0.3240162134170532, 0.22405657172203064, 0.20745883882045746, -0.3362955152988434, -0.08767955750226974, 0.9581659436225891, -0.1607891470193863, -0.22898828983306885, 0.8336350917816162, 0.19900169968605042, 0.09324029088020325, 0.29188865423202515, -0.5913691520690918, -0.20258986949920654, 0.2938787043094635, 0.27812787890434265, 0.6405752897262573, -0.8714003562927246, -0.41369253396987915, 1.381410837173462, -1.7229241132736206, 0.2262272834777832, 1.4548150300979614, 0.4354153573513031, 0.27417266368865967, -0.051158178597688675, -0.26868337392807007, 0.7947444319725037, 0.7365587949752808, -0.6562409996986389, -0.6340030431747437, -0.0900222584605217, 0.04836557060480118, 2.057840347290039, -0.4804322421550751, 0.15751871466636658, 0.30281051993370056, 0.28907841444015503, 1.515285849571228, -0.07333605736494064, -0.5602723956108093, 0.10315506905317307, -0.2901931703090668, 0.5256151556968689, 0.7760187983512878, 0.19917279481887817, -0.02763541042804718, -0.8189173340797424, 0.308875173330307, 0.6526209712028503, 0.3218574523925781, 0.8069604635238647, -0.07974358648061752, -0.24158117175102234, 0.5684487223625183, 0.9420294761657715, 0.6166284084320068, -0.5173539519309998, 0.832853376865387, 0.8704741597175598, -0.20851147174835205, -0.18224994838237762, 0.15015488862991333, 0.03181493654847145, 0.7518119215965271, -0.8066782355308533, 0.090878427028656, -0.2743018865585327, 1.07832670211792, -0.1603192538022995, 0.47519880533218384, -0.7467424869537354, 0.007778562605381012, 1.039974331855774, 0.34738123416900635, 0.26965072751045227, -0.5026798248291016, -0.017501091584563255, -0.857761561870575, -0.23246635496616364, -0.8322740197181702, -0.002963600680232048, -0.3656541109085083, -0.9836475849151611, -0.9435114860534668, -0.5462621450424194, 0.8165320754051208, -1.2456276416778564, 0.12293671816587448, -0.16053546965122223, 0.7140372395515442, -0.29694682359695435, 0.39894479513168335, 0.821669340133667, 0.13423925638198853, 0.5612677931785583, 0.4784298539161682, 1.545682668685913, -1.7607738971710205, -0.22874274849891663, -1.116418480873108, 0.0052000172436237335, -0.37700849771499634, 0.1391446590423584, -0.3447459638118744, -1.1894850730895996, -1.5957049131393433, -0.7342788577079773, -0.21506555378437042, -0.9254311919212341, 0.0821925476193428, 0.8400368690490723, 0.35445892810821533, -1.2585097551345825, 0.4850168824195862, -0.34014013409614563, -0.2550753951072693, 0.539201557636261, 0.22026002407073975, 0.5852710604667664, -0.413234144449234, -0.8752592206001282, 0.6050174832344055, -0.27200108766555786, -0.3233604431152344, -0.41193243861198425, -0.5525635480880737, -1.0287423133850098, 0.011464699171483517, 0.3634980618953705, -0.5873711109161377, 1.150815725326538, -0.428225576877594, -0.727202832698822, 0.6305316090583801, -0.4188896715641022, 0.018006376922130585, -0.11749698966741562, -0.1529250293970108, -0.3334425389766693, -0.1297166794538498, 0.05225328356027603, 0.7271671891212463, 1.363747000694275, 0.10715681314468384, -0.3085916340351105, -0.037784770131111145, -0.3860217332839966, -0.11089009046554565, -0.05998016148805618, 1.1099673509597778, -0.7060017585754395, -0.31851327419281006, 0.2411794662475586, 0.7327181100845337, -0.260854035615921, -0.29900744557380676, -0.28109684586524963, -0.9235519170761108, 1.111327886581421, 0.44808530807495117, 0.7143188118934631, -0.837510347366333, -1.0438450574874878, -0.006166444625705481, 0.17121994495391846, 0.2684204578399658, -0.9049248099327087, 0.064801424741745, -0.20251582562923431, 0.13559429347515106, 0.5000832080841064, -1.2276962995529175, 0.13616877794265747, -0.5123170018196106, -0.49349573254585266, -0.012469940818846226, 0.24039185047149658, 1.2837607860565186, -0.3997561037540436, -0.3475257456302643, -0.006057078018784523, 0.512921929359436, -1.0917270183563232, 0.8092644214630127, -0.25994613766670227, -0.4440632164478302, -0.10793957114219666, 0.34433606266975403, 0.02377210184931755, -0.4791531562805176, 0.49504223465919495, -0.9770207405090332, -0.251838743686676, 0.563974142074585, -0.438478946685791, 1.0895124673843384, -0.1763903796672821, 0.3555225729942322, 0.21153663098812103, -0.7829201221466064, 0.1972144991159439, 0.14862187206745148, -0.5366543531417847, -0.8701779842376709, 0.37887057662010193, 0.30526095628738403, -0.7527300119400024, 0.39393821358680725, 1.3333648443222046, 1.5062421560287476, -0.33103930950164795, -0.2210591584444046, 0.3639856278896332, 0.005350030958652496, 0.07801611721515656, 0.25563758611679077, 0.6684852242469788, -0.03560923412442207, 0.15315788984298706, -0.19606724381446838, 0.11233942210674286, -0.867371141910553, 0.11749523133039474, 0.9396778345108032, 0.03897829353809357, 1.0518913269042969, 0.41343989968299866, -0.8964271545410156, -0.559975266456604, 0.18956033885478973, 0.6389554142951965, 1.4990781545639038, 0.14843803644180298, 0.01284116879105568, -0.9322371482849121, -0.3854823708534241, -0.5731386542320251, -0.8776968121528625, -0.40526145696640015, 0.12622924149036407, -0.5611017346382141, -0.8789730668067932, 0.7706818580627441, 0.6634699106216431, 1.4487589597702026, -1.223055362701416, -0.9819701313972473, -0.29769468307495117, 0.43965044617652893, -1.0162793397903442, -0.85846346616745, 0.22631299495697021, -0.2389647215604782, -0.18862324953079224, 0.08022047579288483, -0.6973490715026855, 0.24754640460014343, -0.011415914632380009, 1.0260846614837646, -0.1752675175666809, -0.9432868957519531, 0.33480262756347656, 0.22532914578914642, -0.38384100794792175, 0.20778857171535492, 0.37392371892929077, -0.058877792209386826, -0.23058806359767914, 0.3088597357273102, 0.47584229707717896, -0.562751829624176, 0.049054887145757675, -0.19219434261322021, -0.1761861890554428, 0.39695844054222107, 0.08814382553100586, 1.0688484907150269, -0.5308910012245178, 0.22062446177005768, -0.929989218711853, 0.21467837691307068, 0.351702481508255, -0.031034300103783607, 0.00806543417274952, -0.6663945317268372, -0.7378351092338562, 0.35078269243240356, -0.5451176762580872, 0.09079919010400772, -0.6403855681419373, 0.41145023703575134, -0.7239772081375122, -0.4283238351345062, -0.25780758261680603, 0.12775559723377228, -0.08943531662225723, 0.14751990139484406, 0.5524998307228088, 0.2050657719373703, 0.004831982776522636, 0.5784326791763306, -0.9689968228340149, 0.8783901333808899, 0.17735597491264343, -0.15078462660312653, 0.1568814218044281, -0.07111689448356628, -1.0044535398483276, -0.18269553780555725, -0.6183199286460876, -0.4301109313964844, -0.18936923146247864, 0.5452960729598999, -0.779697060585022, -0.7262930274009705, 0.2741347551345825, -1.3195842504501343, -0.27115175127983093, 0.18223144114017487, -0.47417235374450684, -0.15056122839450836, -1.173207402229309, -1.0227373838424683, -0.18254268169403076, -1.1303592920303345, -1.222440242767334, 0.43123859167099, 0.3380117118358612, -0.26929616928100586, -0.27505597472190857, -0.38813287019729614, -0.5863634943962097, 1.3018840551376343, -0.300769567489624, 0.3170166313648224, -0.13597747683525085, -0.5719814300537109, -0.2193416953086853, -0.547933042049408, 0.2755773067474365, -0.3870496153831482, 0.2373301237821579, -1.1678417921066284, 0.17372947931289673, -0.4215351939201355, -0.11306048184633255, 0.7330563068389893, 0.5964325666427612, 0.6534491777420044, -0.01533864438533783, -0.3580112159252167, 0.3218453526496887, 1.4721508026123047, -0.8144025802612305, 0.5552512407302856, -0.00044103810796514153, 1.1929736137390137, -0.05729970335960388, -0.26363497972488403, 0.5108799338340759, 0.6628304719924927, 0.22152751684188843, 0.5913353562355042, -0.5188694000244141, -0.7596787810325623, -0.3802809417247772, 0.5431945323944092, 1.129767656326294, 0.22169023752212524, 0.4113803803920746, -0.5756208896636963, 1.0045325756072998, -1.18606698513031, -0.9782220721244812, 0.6122406125068665, 0.5324928760528564, -0.16200312972068787, -0.24278105795383453, 0.2075016051530838, -0.3094575107097626, 1.0316067934036255, 0.7266018986701965, -0.5220072269439697, -0.7833696007728577, 0.08344296365976334, 0.9072719812393188, 0.46126773953437805, 0.33102425932884216, -0.5786458253860474, 0.9230825304985046, 14.581387519836426, 0.9467407464981079, -0.4009479582309723, 0.5521999001502991, 0.796096920967102, 0.4665822386741638, 0.03438439965248108, 0.023123394697904587, -1.450365662574768, -0.1967029571533203, 0.8420828580856323, 0.47179311513900757, 0.11166583001613617, 0.6120262145996094, -0.06751125305891037, 0.28520479798316956, -0.43412986397743225, 0.638151228427887, 0.7758765816688538, -1.5050158500671387, 0.1069307029247284, 0.07232984155416489, 0.2834581732749939, 0.5761246085166931, 1.0689314603805542, 0.6584162712097168, 0.08273715525865555, -0.41366860270500183, 0.616603434085846, -0.19388650357723236, 1.1394727230072021, -0.045095428824424744, 0.06669250130653381, -0.14131295680999756, -1.582046389579773, 0.055994462221860886, -0.7577235102653503, -1.2605057954788208, -0.2601378262042999, 0.051563944667577744, -0.4243844449520111, -0.7015994787216187, 0.0751514658331871, 1.0195541381835938, -0.22072763741016388, 0.7321770191192627, -0.15972080826759338, 0.05429597198963165, 0.13687869906425476, -0.1024942547082901, 0.17716141045093536, 0.9713168144226074, 0.016863031312823296, 0.4102281928062439, -0.3783414363861084, -0.04433010518550873, 0.41486573219299316, 0.3112620413303375, -0.4222402572631836, -0.38166218996047974, -0.28305450081825256, 0.2366538941860199, 0.10356348752975464, 1.231951355934143, -0.19012747704982758, -0.03341585397720337, -0.5093461871147156, 0.20908194780349731, 0.19959552586078644, -0.09422949701547623, -0.5325527191162109, -0.36076462268829346, 0.47543227672576904, -0.5048714876174927, 0.7175899744033813, 0.47092244029045105, -0.5309882760047913, -0.5192880034446716, -0.7255719900131226, -0.20550929009914398, 0.3880349397659302, -0.46042901277542114, -0.36419445276260376, 0.95621258020401, -0.5043292045593262, -0.07365550100803375, 0.5280439257621765, -0.9410685896873474, -0.8351569175720215, 0.10741022974252701, -1.6172791719436646, -1.0740196704864502, 0.08940339088439941, -0.02153077907860279, -0.14170372486114502, 0.20885422825813293, 1.04518461227417, 0.27883008122444153, -0.4603354334831238, 0.3748951256275177, -0.5579843521118164, 0.09190109372138977, -0.16728708148002625, -0.4209856688976288, 1.1899139881134033, 0.5621427893638611, -0.3474809229373932, -0.17464230954647064, -0.11957067251205444, 0.2499239593744278, -0.7959842085838318, -0.4714886248111725, 0.6693902015686035, -0.43207603693008423, -0.47622424364089966, -0.49896496534347534, -0.41529300808906555, 0.18246644735336304, 0.6646386384963989, 0.44644078612327576, -0.11004894971847534, -0.03112541139125824, -1.0019426345825195, -0.3438185453414917, -0.8598344922065735, -0.05886152759194374, 0.217559352517128, -0.976276695728302, -0.18977698683738708, -0.3363839089870453, 0.3626539409160614, -0.8709157705307007, -0.4617387652397156, -0.4514550566673279, 0.3956625759601593, -0.16479742527008057, 1.2976534366607666, -0.1055929958820343, 0.7034123539924622, 0.7325194478034973, -0.15697264671325684, -0.5055868625640869, -0.036041777580976486, -0.6998431086540222, 0.09012769907712936, 0.04765693098306656, 0.2833092510700226, -0.33333274722099304, 0.3678611218929291, 0.7949840426445007, 0.22893843054771423, -0.4471028745174408, -0.48392146825790405, -0.2403164654970169, -0.26103973388671875, -0.4886219799518585, 0.21653123199939728, -0.1582646667957306, 0.14215783774852753, 0.026825977489352226, 0.5341166257858276, 0.31349894404411316, 0.3720068633556366, -0.5246285796165466, 0.31969717144966125, 0.0014351900899782777, -0.02519366703927517, -0.6133757829666138, -0.8775040507316589, -1.2195112705230713, -0.1163545697927475, -0.8135890960693359, 0.1054672822356224, -0.5428224802017212, -0.47876331210136414, 0.009907362051308155, -0.36584752798080444, 0.35967355966567993, 0.08453541994094849, 0.45331820845603943, -0.12733502686023712, -0.52513188123703, -1.0071642398834229, 0.4267056882381439, 0.9752461910247803, -0.713312566280365, 0.08819358050823212, -0.34717607498168945, -0.18392488360404968, 0.5020014047622681, 0.2969275414943695, -0.1411389708518982, -0.4549916088581085, -1.130786418914795, 0.08582490682601929, -0.19227729737758636, 0.28162556886672974, -0.9850691556930542, 1.1812996864318848, 0.5815566778182983, 0.007491103373467922, -0.058551810681819916, 0.06824791431427002, -0.5890153050422668, -0.9079350829124451, 0.5421974658966064, -0.4877166748046875, 0.4007229506969452, 0.3020714223384857, -0.6300601959228516, -0.26040852069854736, 1.0943814516067505, 0.331849604845047, -1.0444425344467163, -1.4535748958587646, 0.4364564120769501, -0.3845308721065521, 0.19995896518230438, -0.36163130402565, -0.10674426704645157, -1.6678860187530518, 0.08634667098522186, -0.042773179709911346, 0.08695031702518463, -0.3840545415878296, 0.7541840076446533, 0.8294721245765686, -0.910342812538147, 0.19320616126060486, 0.5600436925888062, -0.41122788190841675, 0.18261198699474335, 0.5846911072731018, 0.7517129778862, -0.321034699678421, 0.39146971702575684, -0.18500567972660065, 0.3205537497997284, -0.6654064655303955, 0.3205742835998535, 0.521994948387146, -0.37421125173568726, -0.20031926035881042, 0.9070327877998352, -0.31754177808761597, -0.32720789313316345, 0.21441558003425598, -1.238559603691101, -0.3784869611263275, -0.038450468331575394, 0.8882027268409729, 0.4201945662498474, 0.22593367099761963, -0.20706452429294586, -0.8394122123718262, 0.3713565766811371, -0.47836634516716003, -0.24677853286266327, 0.2472488433122635, 0.027663646265864372, -0.37587684392929077, -0.2116391956806183, 0.8844085931777954, -1.0777696371078491, -0.7471197247505188, -0.7544206976890564, -0.5196359157562256, 0.08330359309911728, 0.2916114628314972, -0.03932759538292885, -0.8998085856437683, 0.7196780443191528, 0.5472180843353271, 0.48359453678131104, 0.17535774409770966, -0.3452460467815399, 0.38887596130371094, 0.47773945331573486, 0.024987565353512764, -0.7322304844856262, -0.4106806218624115, 1.3612428903579712, 0.8142985105514526, -0.6681151390075684, 0.1234397366642952, -0.10955772548913956, -0.6314706206321716, 0.44995400309562683, 0.5727027058601379, -0.7378226518630981, 1.0934776067733765, 0.06443490833044052, -0.20145639777183533, 0.12515497207641602, -0.9862194061279297, -0.6662581562995911, 0.6721674799919128, 1.2633287906646729, 0.3260767459869385, 0.3054870069026947, 0.31978410482406616, 0.5172601938247681, 0.5995872020721436, -0.12872621417045593, 0.4649297893047333, 0.3759232759475708, -0.34632572531700134, 0.13137900829315186, -0.20946365594863892, 0.48858654499053955, -0.8357098698616028, -0.8307851552963257, 0.28506726026535034, 0.45503973960876465, 0.2196185290813446, 0.553644597530365, 1.2350587844848633, -0.08108613640069962, 0.7030428051948547, -0.12307536602020264, 0.5283585786819458, -0.6228168606758118, -0.3969041407108307, 0.08855913579463959, -0.994359016418457, -0.3999493420124054, -0.3179388642311096, -0.6250738501548767, -0.2435283660888672, -0.3486442267894745, 0.2955544888973236, -0.3867974877357483, 0.39460989832878113, 0.8533471822738647, 0.796968936920166, 0.6984153389930725, -0.2532184422016144, -1.31251060962677, -0.10317016392946243, -0.5718936324119568, 0.10669233649969101, -0.614538311958313, 0.0065468973480165005, -0.14523835480213165, 0.17032292485237122, 0.0637659877538681]}, "authors": [{"authorId": "2214504333", "name": "Seokju Yun"}, {"authorId": "2280736495", "name": "Youngmin Ro"}], "references": [{"paperId": "9a83aeadc8db65fb6da39ec977360541cddaff5c", "title": "EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention"}, {"paperId": "3375b5693331c37b899d78115d91215b8b5716e9", "title": "SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications"}, {"paperId": "52cc542b60c99e8866a7fc515c10f443089005f2", "title": "FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization"}, {"paperId": "a3aa1323a7f08c40207eaa359041e5bd72b25b27", "title": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks"}, {"paperId": "493cf3728f49af4e47c2c2f928510ade1e31cf00", "title": "Rethinking Mobile Block for Efficient Attention-based Models"}, {"paperId": "f35016b3180808fa97d59acbdecf47d6e2ed2819", "title": "Rethinking Vision Transformers for MobileNet Size and Speed"}, {"paperId": "05b7bd47fa5cbe10497c49004b57eb5ab4fdd0b4", "title": "EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications"}, {"paperId": "d83105503f3b7c6403e812cc1d354b270ab4c048", "title": "MobileOne: An Improved One millisecond Mobile Backbone"}, {"paperId": "066c143b427571fb5568f2c581ea9066478d2e55", "title": "Separable Self-attention for Mobile Vision Transformers"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bf6ce546c589fa8054b3972b266532664914bd21", "title": "Fast Vision Transformers with HiLo Attention"}, {"paperId": "dbf6e95cb618f207f029276a6df11f4a9a6313d4", "title": "Inception Transformer"}, {"paperId": "b4da9f3505e22d3e766ba21890285b822dc71599", "title": "EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers"}, {"paperId": "fa717a2e31f0cef4e26921f3b147a98644d2e64c", "title": "Focal Modulation Networks"}, {"paperId": "3a5b7838b5348315572a8c1aa8c33deea16f159d", "title": "The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy"}, {"paperId": "f9480350e1986957919d49f346ba20dcab8f5b71", "title": "CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction"}, {"paperId": "430bab3890e1e52c4c1f74900b0e408e47a1cb8f", "title": "How Do Vision Transformers Work?"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "b3ccbba3b77728cffc323d8d4bc3ada615e8e273", "title": "Multi-Dimensional Model Compression of Vision Transformer"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "c051ee2ad7ac203a26fa8f50eb6312424c729b27", "title": "Global Vision Transformer Pruning with Hessian-Aware Saliency"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "1cd6b0f41d62aca38ba5a69db10e79c05e618c21", "title": "Conditional DETR for Fast Training Convergence"}, {"paperId": "a66686e60a3eda0c606e036403cf0a07a5962595", "title": "Mobile-Former: Bridging MobileNet and Transformer"}, {"paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65", "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"}, {"paperId": "260ad39a1dac4b451019e2bf17925f4df8e3b69a", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "bf459cca52f336de2afe61e6e6348571c73943c1", "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "f0993e68c76d940763ef7f8106db86cdc97b3a49", "title": "Multi-head or Single-head? An Empirical Comparison for Transformer Training"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "576c462dbc1f3d732b919ef1daac37a817123e52", "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "2b8088253e2378fce001a090fe923b81e8dedf25", "title": "RepVGG: Making VGG-style ConvNets Great Again"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "c5e475417ad4bb22175e170f52f8ebf1e042a42d", "title": "Dynamic ReLU"}, {"paperId": "a4cc0701170331a1fd0e58bad962bd7f39f5efc9", "title": "GhostNet: More Features From Cheap Operations"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "21de3a36cb51adc205fad8a1d3d69118891dc3dd", "title": "AutoAugment: Learning Augmentation Strategies From Data"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e", "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89", "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "a40f5b9acd81fad048a97562336b46d04cde4023", "title": "Budgeted Training for Vision Transformer"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Searching for mo-bilenetv3"}, {"paperId": null, "title": "Onnx: Open standard for machine learning interoperability"}, {"paperId": null, "title": "Core ml tools"}]}