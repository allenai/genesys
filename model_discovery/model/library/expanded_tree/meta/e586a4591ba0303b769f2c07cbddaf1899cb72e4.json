{"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "abstract": "Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 51, "influentialCitationCount": 11, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.14048", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A novel approach for implementing the KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens is introduced and a theoretical guarantee for the novel eviction algorithm is proved."}, "embedding": {"model": "specter_v2", "vector": [0.1787508726119995, 0.7859935164451599, -0.2515775263309479, 0.07810840755701065, -0.5857337713241577, -0.3285292685031891, 0.9342179894447327, -0.36938801407814026, -0.18195508420467377, -0.2709117829799652, 0.4968053698539734, -0.14571797847747803, 0.42495498061180115, -0.09147138148546219, -0.38618266582489014, -0.034940388053655624, -0.748551070690155, 0.7853445410728455, -0.27092549204826355, -0.14758989214897156, -0.05580328404903412, -0.788230299949646, -1.3482351303100586, -0.03736422583460808, 0.4297088384628296, 0.3827548921108246, 0.12640953063964844, 1.2714459896087646, -0.37602949142456055, 0.41992858052253723, 0.5014220476150513, -0.24731434881687164, 0.08780214935541153, -0.3586741089820862, -0.4678157866001129, -0.050085123628377914, -0.1149652823805809, -0.44883081316947937, -0.43468549847602844, 0.6005257368087769, 0.12930794060230255, 0.13927657902240753, 0.3575977087020874, -0.9228476881980896, -0.06330657005310059, 0.7941518425941467, 0.3710958659648895, 0.5587099194526672, -0.05873701721429825, -0.16520407795906067, 1.4004154205322266, -1.4751205444335938, 0.36038944125175476, 1.615187644958496, 0.11405918002128601, 0.13342630863189697, -0.2853708565235138, -0.16069935262203217, 0.7418192625045776, 0.038169246166944504, -1.1214646100997925, -0.6236823797225952, -0.013700077310204506, 0.11159691959619522, 1.99207603931427, -0.04719294235110283, 0.19804412126541138, 0.6947460770606995, -0.3690781593322754, 1.6031091213226318, -0.3994145095348358, -0.9709153771400452, -0.0631365180015564, 0.09819668531417847, 0.3385787606239319, 1.000164270401001, -0.33898457884788513, 0.004137490410357714, -0.950931191444397, -0.5038473606109619, 0.4068046510219574, -0.41635259985923767, 0.3735034167766571, 0.024581599980592728, -0.04850323870778084, 0.9273850917816162, -0.30606815218925476, 0.4762823283672333, -0.022694803774356842, 0.7977609634399414, 0.3302643895149231, -0.16358478367328644, 0.2037985771894455, 0.1909455507993698, 0.2828976511955261, -0.21754369139671326, -0.7895631790161133, 0.5739802122116089, 0.40000852942466736, 0.9370003342628479, -0.44990450143814087, 0.11414965987205505, -1.0484243631362915, 0.17867426574230194, 1.2871829271316528, 0.0438866913318634, 0.4828625023365021, -0.7582547068595886, 0.2274903506040573, -0.9451512694358826, 0.1721109002828598, -0.33031147718429565, -0.03154054284095764, -0.07198122143745422, -0.7021252512931824, -1.1955705881118774, -0.4782922565937042, 0.05149194225668907, -0.5194530487060547, 0.6491231322288513, 0.03860513120889664, 0.30585977435112, -0.06673455983400345, 0.6007551550865173, 0.5653524398803711, 0.9261206984519958, -0.01960630528628826, 0.16644015908241272, 0.8598962426185608, -1.2178056240081787, -0.4943631887435913, -1.4720475673675537, 0.7521740198135376, -0.22354243695735931, 0.30070409178733826, 0.007105560507625341, -1.7447460889816284, -0.7100457549095154, -0.6975625157356262, -0.10953042656183243, -0.4327920079231262, 0.2932784855365753, 1.1716254949569702, 0.05468514934182167, -0.8517113327980042, 0.45258238911628723, -0.4556262195110321, -0.003360313130542636, 0.2611410915851593, 0.010770199820399284, 0.4683501720428467, -0.47017860412597656, -1.1534042358398438, 0.07605218887329102, 0.05454839766025543, -0.3349938988685608, -0.16969873011112213, -0.39268025755882263, -0.9466199278831482, 0.055629562586545944, 0.5888975858688354, -0.5287999510765076, 1.381891131401062, 0.3725062906742096, -1.3138984441757202, 0.7240152359008789, -0.4470340311527252, 0.25540459156036377, 0.6194556355476379, -0.2133096307516098, -0.15339896082878113, -0.7269452214241028, -0.2332543432712555, 0.4642699956893921, 0.3916834592819214, -0.02319178730249405, -0.1651724874973297, 0.16371595859527588, -0.35360321402549744, 0.048529479652643204, 0.07077232748270035, 0.606116533279419, -0.9338752627372742, 0.32999372482299805, 0.23212756216526031, 0.45280396938323975, -0.2942562997341156, -0.35899385809898376, -0.8046749830245972, -1.4186208248138428, 0.3606806993484497, 0.31071218848228455, 0.981599748134613, -0.6636475324630737, -0.7761651873588562, -0.27450069785118103, 0.13169798254966736, -0.33365243673324585, -0.4898797869682312, 0.6461872458457947, -0.06863805651664734, 0.25205209851264954, -0.0840681791305542, -1.3768361806869507, 0.4497002959251404, -0.10174010694026947, -0.754736602306366, -0.21373499929904938, 0.19938237965106964, 1.1893199682235718, -0.9840565323829651, -0.168144091963768, -0.21975092589855194, -0.012946988455951214, -1.108546495437622, 1.5308581590652466, -0.725446343421936, -0.15372155606746674, -0.2581944167613983, -0.2358587235212326, -0.016357077285647392, -0.5663866400718689, 0.7410064935684204, 0.005941287614405155, -0.29342523217201233, 0.5442638993263245, -0.45526692271232605, 1.2845335006713867, -0.09815431386232376, 0.2180929332971573, -0.45452624559402466, -0.521754264831543, 0.0783788189291954, 0.21225567162036896, -0.6454272866249084, -0.3397659957408905, 0.25758713483810425, 0.38017767667770386, -0.6117106676101685, 0.049768298864364624, 0.9423196315765381, 1.0512028932571411, -0.5970802903175354, 0.3504112660884857, 0.4863765835762024, -0.20171917974948883, 0.5145650506019592, 0.8445729613304138, 0.49174240231513977, 0.5786553025245667, 0.2435876429080963, 0.17195647954940796, 0.00795732345432043, -0.6781134605407715, -0.38116124272346497, 0.9216221570968628, 1.0740915536880493, 1.0067840814590454, 0.7651563286781311, -0.8727951049804688, -0.6840057373046875, 0.40477946400642395, 0.738711953163147, 1.3762506246566772, -0.06546027958393097, -0.4730701744556427, -0.8183464407920837, 0.06928395479917526, -0.3916904926300049, 0.3326061964035034, -0.36294975876808167, -0.19889973104000092, -0.5128464698791504, -1.2364716529846191, 0.9612067341804504, 0.22465766966342926, 0.7989910840988159, -0.5978221893310547, -0.33536139130592346, -0.2403055876493454, -0.016808850690722466, -0.9242430925369263, -0.4356395900249481, 0.12773695588111877, -0.0458524115383625, 0.3264242708683014, 0.14769531786441803, 0.03298710286617279, 0.009276367723941803, -0.732277512550354, 1.063766360282898, -0.25930067896842957, -0.5716134905815125, 0.10496542602777481, 0.7853884100914001, -0.4009053111076355, -0.8306258320808411, 0.3523249924182892, 0.018513554707169533, -0.23293974995613098, 0.10967998951673508, 0.45122814178466797, 0.25141745805740356, -0.3059147894382477, -0.15602806210517883, 0.8943380117416382, -0.07632410526275635, 0.04986243322491646, 0.4116513729095459, -0.6263359785079956, -0.32330018281936646, -1.14090895652771, 0.6045424938201904, -0.27396905422210693, -0.6963262557983398, 0.2291523963212967, -0.7200977206230164, -0.3245988190174103, 0.30461546778678894, -0.7763532400131226, -0.2256915122270584, -1.0137230157852173, 0.08383985608816147, -0.4102936089038849, -0.19492658972740173, -0.013456011191010475, 0.22658570110797882, 0.7450796961784363, 0.055952150374650955, 0.7555795311927795, 0.09959962964057922, -0.3323391079902649, 0.7500496506690979, -1.021201491355896, 0.7709342241287231, -0.010664829052984715, -0.06732920557260513, -0.12413772940635681, -0.08497107028961182, -0.6721307039260864, -0.5095481276512146, -0.31283047795295715, -0.2854496240615845, -0.32396888732910156, 0.2665354013442993, -0.35913485288619995, -1.0062261819839478, 0.15312102437019348, -1.498752236366272, -0.4392721354961395, 0.4843432307243347, -0.49585437774658203, -0.26587891578674316, -0.913252592086792, -1.1174638271331787, -0.8937843441963196, -0.5555959343910217, -0.9229123592376709, 0.6059709787368774, -0.05470475181937218, -0.484803169965744, -0.5255478620529175, 0.2747364640235901, -0.2764206528663635, 0.6354967951774597, -0.5685445666313171, 0.8527185916900635, 0.11693215370178223, -0.4524350166320801, -0.27722978591918945, 0.4685000777244568, -0.22802655398845673, -0.6622522473335266, 0.27391257882118225, -0.9651404619216919, 0.29242947697639465, -0.8284779191017151, -0.30080246925354004, 0.09708338975906372, 0.3525782823562622, 0.5859254598617554, -0.3908016085624695, -0.7318841814994812, 0.3436431884765625, 1.2291914224624634, -0.8185819387435913, 0.2991115152835846, 0.009289336390793324, 1.2594773769378662, 0.1100870668888092, -0.09108226746320724, 1.1422295570373535, 0.3146149218082428, 0.5824552178382874, 0.2319720834493637, -0.23065735399723053, 0.13410747051239014, -0.7561758160591125, 0.6893212795257568, 1.4804649353027344, 0.2552620768547058, -0.5969233512878418, -0.7094682455062866, 0.713881254196167, -1.6902377605438232, -0.7172707319259644, 0.213583841919899, 0.6786910891532898, 0.22984416782855988, -0.4850204885005951, -0.055809322744607925, -0.5520649552345276, 0.30264997482299805, 0.30487561225891113, -0.5489429831504822, -0.7710267305374146, -0.054949961602687836, 0.16843275725841522, -0.08887963742017746, 0.551243782043457, -0.007525123190134764, 0.8312591314315796, 14.81860637664795, 1.1762688159942627, 0.2995491623878479, 0.6549158096313477, 1.0922327041625977, -0.1664530336856842, -0.3846867084503174, -0.143635094165802, -1.360757827758789, -0.16073495149612427, 1.4672168493270874, 0.04170776903629303, 0.565561830997467, 0.2915576696395874, 0.14815068244934082, 0.09282634407281876, -0.4991224706172943, 0.7532389163970947, 0.5592451691627502, -1.3802837133407593, 0.35559603571891785, 0.36533257365226746, 0.3029493987560272, 0.633496105670929, 0.9288208484649658, 0.7403977513313293, 0.9338880777359009, -0.6520602107048035, 0.8547154664993286, 0.40292665362358093, 0.9766102433204651, -0.32180508971214294, 0.23691481351852417, 0.6830182075500488, -0.6089938282966614, 0.2048460692167282, -0.6203916072845459, -1.289306879043579, 0.4419291913509369, 0.1580161601305008, -0.9314960241317749, -0.532804012298584, -0.3092559278011322, 0.3869800865650177, 0.2502494156360626, -0.025120887905359268, -0.19445830583572388, 0.5753197073936462, -0.2378251850605011, 0.001404343987815082, 0.6063235402107239, 0.3210030794143677, 0.22044724225997925, -0.16272325813770294, 0.1266971379518509, 0.11469674855470657, 0.23349203169345856, 0.9929461479187012, -0.4073002338409424, -0.2709777057170868, -0.4422623813152313, -0.28571367263793945, 0.023584280163049698, 0.969765305519104, 0.4892338812351227, -0.1055261641740799, -0.342542827129364, 0.6796289682388306, 0.8835989832878113, -0.2705591917037964, -0.4246789216995239, 0.4437682032585144, 0.2693232595920563, -0.5309106707572937, 0.1094924584031105, 0.711181104183197, 0.05448758974671364, -0.4855230450630188, -0.7879335880279541, -0.38085106015205383, 0.3230925500392914, -0.8806465268135071, -0.5075384974479675, 0.8212560415267944, -0.1148783341050148, 0.021342037245631218, -0.573910117149353, -0.3057066798210144, -0.4170389771461487, 0.4299390912055969, -0.9994561672210693, -0.7538864016532898, 0.3535366356372833, -0.456184446811676, 0.028012823313474655, 0.40796998143196106, 1.3745036125183105, -0.15888625383377075, -0.5932408571243286, 0.30761444568634033, 0.2141207754611969, -0.16818614304065704, -0.5215323567390442, -0.7486281394958496, 1.511296272277832, 0.3035239577293396, 0.10340530425310135, 0.06494640558958054, 0.18941937386989594, 0.24055829644203186, -1.151654839515686, 0.11783649772405624, 0.8203679323196411, -1.1593797206878662, -0.6031587719917297, -1.0951422452926636, -0.5797283053398132, 0.2809855341911316, 0.5313445329666138, -0.20754849910736084, 0.17787879705429077, 0.14855161309242249, -0.3128306567668915, 0.3916817903518677, -0.3513524532318115, 0.15135125815868378, 0.6620413064956665, -0.48841288685798645, 0.21490319073200226, -0.0009472529054619372, 0.2808173894882202, -1.3118587732315063, -0.3408976197242737, -0.7222838997840881, 0.31797799468040466, -0.12330568581819534, 0.7905300259590149, -0.2906233072280884, 0.9897611737251282, 1.0667011737823486, 0.042070671916007996, -0.5269656777381897, -0.15141157805919647, -1.2425060272216797, -0.35299399495124817, 0.3136712610721588, 0.7164120078086853, -0.19422367215156555, 0.2159983217716217, 1.256172776222229, 0.4268609881401062, -0.48738807439804077, -0.46761077642440796, -0.16912683844566345, -0.0010919388150796294, -0.5687843561172485, 0.5088204145431519, -0.09972585737705231, -0.007816561497747898, 0.36559078097343445, -0.12682181596755981, 0.7376666069030762, 0.012195128947496414, -0.4541059732437134, 0.7891300320625305, 0.07078845798969269, -0.39189058542251587, -0.30381226539611816, -0.11485546082258224, -1.482002854347229, -0.2816104292869568, -1.0252403020858765, 0.13998720049858093, -0.8415961861610413, -0.1057971641421318, -0.004717926029115915, 0.2086334377527237, 0.11917691677808762, 0.30427083373069763, -0.09604768455028534, -0.555547833442688, -0.6908441781997681, -0.975692629814148, 0.7706029415130615, 0.701659619808197, -0.5937093496322632, 0.1871655434370041, -0.2427705079317093, 0.12549427151679993, 0.28639206290245056, 0.2975613474845886, -0.36861178278923035, -0.5814356803894043, -1.232214331626892, 0.5872089266777039, 0.14039932191371918, 0.15566810965538025, -0.45188653469085693, 0.5377770662307739, 0.4285713732242584, -0.28975823521614075, -0.1270122081041336, 0.3487487733364105, -0.16948935389518738, -0.13436780869960785, 0.17635896801948547, -0.8179870247840881, 0.22107747197151184, 0.05036759376525879, -0.8824055790901184, 0.24647046625614166, 0.5140114426612854, -0.4297369718551636, -1.2760485410690308, -0.5909090638160706, 0.39127203822135925, -0.5543071031570435, 0.25766339898109436, -0.7081720232963562, -0.0001341777970083058, -0.932367205619812, -0.5426144003868103, 0.10666896402835846, 0.09171364456415176, -0.3286399841308594, 1.0057162046432495, 0.21846191585063934, -1.0780500173568726, -0.19742804765701294, 0.26893350481987, -0.15651057660579681, 0.413460373878479, 0.7213171720504761, 0.3111227750778198, -0.013921143487095833, 0.44105538725852966, 0.7127092480659485, 0.18687038123607635, -0.922110378742218, 0.05826793611049652, 0.5442875623703003, -0.8061511516571045, -0.4699249863624573, 1.1281415224075317, -0.08200037479400635, -0.6631495356559753, -0.33543604612350464, -0.7757735252380371, -0.2846263647079468, -0.6549252271652222, 0.7430340051651001, 0.397743821144104, -0.08634982258081436, -0.06298453360795975, -0.5990723967552185, -0.1480504870414734, -0.1973828375339508, -0.728948175907135, 0.5310754179954529, -0.15257179737091064, -0.41996610164642334, 0.6699173450469971, 0.9120072722434998, -0.6905777454376221, -0.7170127034187317, -0.5731772780418396, -0.6707839965820312, 0.04274894669651985, 0.5610186457633972, -0.40538132190704346, -0.18619605898857117, 0.7406890988349915, 0.21677733957767487, 0.40678274631500244, -0.49646174907684326, 0.32629626989364624, 0.35494381189346313, 0.4362069070339203, 0.15863379836082458, -0.5715133547782898, -0.7017354965209961, 1.140860915184021, 1.054034948348999, -0.604070782661438, 0.11799278110265732, 0.03729631379246712, -0.877775251865387, 0.5225310921669006, 0.3793913424015045, 0.06910630315542221, 0.2856232821941376, 0.04522307217121124, -0.06822530925273895, -0.10730654746294022, -1.591158151626587, -0.16195058822631836, 0.7727013826370239, 0.6787675023078918, 0.3524777293205261, 0.2755669951438904, 0.26729223132133484, 1.0354729890823364, 0.10002341866493225, -0.07062089443206787, 0.3966103494167328, 0.2841513752937317, 0.15254080295562744, -0.003374415449798107, 0.0472959466278553, 0.6473960280418396, -0.6020909547805786, -0.8802890777587891, 0.11661076545715332, 0.33131492137908936, 0.07164128869771957, 0.6448565125465393, 1.087693452835083, 0.5669398307800293, 0.14453136920928955, 0.15670131146907806, 0.6080461740493774, -0.37183913588523865, 0.01390355359762907, 0.09854142367839813, -0.4193260967731476, -0.12427583336830139, 0.47078341245651245, -0.4963594377040863, -0.4458424150943756, -0.6052274107933044, 0.10729868710041046, 0.05653103440999985, 0.04494527354836464, 1.2266480922698975, 0.75981605052948, 0.4109514653682709, -0.12386319041252136, -0.6923530101776123, -0.25249794125556946, -0.866989016532898, -0.10015853494405746, -0.613359272480011, -0.5595518350601196, -0.05420428887009621, -0.11880640685558319, -0.3906991481781006]}, "authors": [{"authorId": "2109338656", "name": "Zhenyu (Allen) Zhang"}, {"authorId": "2209360681", "name": "Ying Sheng"}, {"authorId": "2190694474", "name": "Tianyi Zhou"}, {"authorId": "2034263179", "name": "Tianlong Chen"}, {"authorId": "2149970173", "name": "Lianmin Zheng"}, {"authorId": "2209882676", "name": "Ruisi Cai"}, {"authorId": "2214956470", "name": "Zhao Song"}, {"authorId": "1932187449", "name": "Yuandong Tian"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}, {"authorId": "2052981589", "name": "Clark W. Barrett"}, {"authorId": "2108404505", "name": "Zhangyang Wang"}, {"authorId": "4319427", "name": "Beidi Chen"}], "references": [{"paperId": "6cee47349bf526bd63ee62da15b3c88701f97f15", "title": "On the Optimization and Generalization of Multi-head Attention"}, {"paperId": "69285a2ece3779cfa13a480a04cf51cdc83e36dc", "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights"}, {"paperId": "72c02e167b0d43b6700d9bad9f116585fa17116e", "title": "Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention"}, {"paperId": "93e58491830abe1eb965ab37ec64fa97263f6048", "title": "HyperAttention: Long-context Attention in Near-Linear Time"}, {"paperId": "faab24bc6cd4a4dea6e82420d145f08445c05fc7", "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"}, {"paperId": "504b333ea6e13f92d76b6835c18e48a9d822d246", "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation"}, {"paperId": "0a9030dd6cc2d438509f7568c1081d58c2523d5c", "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning"}, {"paperId": "ffd97533ac66b7d02ca58c1d5951d5427da0ffd6", "title": "Improving Length-Generalization in Transformers via Task Hinting"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "68ed29e5d398e6030cddcc575f1977973c8b0791", "title": "A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"}, {"paperId": "d2b123cfb3cd8bde35955f0f80dc4c57ecd6acef", "title": "Streaming Semidefinite Programs: O(\u221an) Passes, Small Space and Fast Runtime"}, {"paperId": "761af20e7966e8a7d899975a332ac7eee3f92116", "title": "How to Protect Copyright Data in Optimization of Large Language Models?"}, {"paperId": "b169cbff7d5a11afac18f929d5c69ea0933a4da1", "title": "GradientCoin: A Peer-to-Peer Decentralized Large Language Models"}, {"paperId": "9264c721e66c15fc0ecc1ecacdfe1fbf702b6ea8", "title": "Convergence of Two-Layer Regression with Nonlinear Units"}, {"paperId": "13c1a09ed284602d7c21e76c1c63460bb77a4568", "title": "Zero-th Order Algorithm for Softmax Attention Optimization"}, {"paperId": "afa9f128435501d0f2c3ae524a0a7698d0bc3d21", "title": "Fast Quantum Algorithm for Attention Computation"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "acf1dfa02eefcab63124869f152fd36d2aad172a", "title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding"}, {"paperId": "42cf52baff90952944da0409ec52ff7611ed55dc", "title": "Representational Strengths and Limitations of Transformers"}, {"paperId": "e8f7cc40208f4b6f9eca59715666f320d66d386e", "title": "A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "5e11fec80f7ea81f1e456157dddfe803f5ce4c82", "title": "Fast Submodular Function Maximization"}, {"paperId": "30a60f2e382129bece174536aa3410d45c1bbf1a", "title": "Differentially Private Attention Computation"}, {"paperId": "d8b2497f1da0dbbd79679f1df9449f0b01c9c4f0", "title": "An Iterative Algorithm for Rescaled Hyperbolic Functions Regression"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3", "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression"}, {"paperId": "a5125c0eab822af20f55c969696aeaff6f4fdcf9", "title": "Attention Scheme Inspired Softmax Regression"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "f13c766f995917f855ccb7e0b567a7b95146c4db", "title": "Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension"}, {"paperId": "2dd27bed0b030941c25a4ed119b6fe6362d4186b", "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models"}, {"paperId": "8625290c1208297b56fbd723cff12b4bb0c7538c", "title": "Solving Regularized Exp, Cosh and Sinh Regression Problems"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "15288293edeae26dad6e37218cc1c0fc96316635", "title": "Do Transformers Parse while Predicting the Masked Word?"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "14c44df22b0d5361b61181fbf2ec0977ad189abf", "title": "KDEformer: Accelerating Transformers via Kernel Density Estimation"}, {"paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "title": "Benchmarking Large Language Models for News Summarization"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "f95c56a3287110f1524d01a1253f7014e81aa4eb", "title": "A Faster Small Treewidth SDP Solver"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "8d7958c939dd7261bf9e1efd2c5d28c72b3000e2", "title": "Unified Normalization for Accelerating and Stabilizing Transformers"}, {"paperId": "84f6ab620eb7b112e5b2ca64b305970894e679c1", "title": "Adaptive Gradient Methods at the Edge of Stability"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1b72a2842c535502f639e6082562723717582046", "title": "A very preliminary analysis of DALL-E 2"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20", "title": "Wordcraft: Story Writing With Large Language Models"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "cad3387a105d6030ece9c81607fee2af52b823c2", "title": "RadBERT-CL: Factually-Aware Contrastive Learning For Radiology Report Classification"}, {"paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737", "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"}, {"paperId": "64a0a4f357be12aaf30cc6e4964d1c3a9d927aac", "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models"}, {"paperId": "a548e76576500844d7236ff1e9246698b26940f6", "title": "Limitations."}, {"paperId": "1779dea2902f380ac95eb29af41e344552225e88", "title": "A faster algorithm for solving general LPs"}, {"paperId": "148011adfae37b821407aae84fcbbf7fb4619eb6", "title": "On the Expressive Power of Self-Attention Matrices"}, {"paperId": "6a6ad8dd1b1ac7bb0544912825e85ff733c580d8", "title": "MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture"}, {"paperId": "51f46cb42668cfe3745ecf029d032bf30253574f", "title": "GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training"}, {"paperId": "9d6acac70b2d1fdb861a08b00766ef263109cd7f", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks"}, {"paperId": "9ccaf20c70a9ff0613a127d8b071a990e6e1b920", "title": "Solving SDP Faster: A Robust IPM Framework and Efficient Implementation"}, {"paperId": "cd02e0a094953077217e2e62f3557b36a365acff", "title": "Optimizing Deeper Transformers on Small Datasets"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "e8f698b01939860cb835538554d32e389e90ae62", "title": "A Faster Interior Point Method for Semidefinite Programming"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "e67852824e57ebf9931e83f9c5ff3427248babbf", "title": "Space-Efficient Interior Point Method, with applications to Linear Programming and Maximum Weight Bipartite Matching"}, {"paperId": "b5271f4522fd72e335535c5f65d3afc01d1cb2bd", "title": "Very Deep Transformers for Neural Machine Translation"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "a702375365cbe24c43226b9460508da0e885cb3c", "title": "Resurrecting Submodularity for Neural Text Generation"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "8de7f044a673d1f5e3b454d0663811f91aa9811a", "title": "On the Efficacy of Knowledge Distillation"}, {"paperId": "b16ff8329e6b10914fd7908f22bda355130e9ba8", "title": "Why ADAM Beats SGD for Attention Models"}, {"paperId": "46ac7211ad22d15319064b1a20e88860c16a845f", "title": "(Nearly) Sample-Optimal Sparse Fourier Transform in Any Dimension; RIPless and Filterless"}, {"paperId": "2bf7c350a8280e7c593d46a60127f99b21517121", "title": "On the Variance of the Adaptive Learning Rate and Beyond"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "d77123b54dcc8014949584ab624e97298617bcad", "title": "Data-Free Quantization Through Weight Equalization and Bias Correction"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "5728919676a85553b3c3063626c220fe7a5634e4", "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale"}, {"paperId": "c7a0fdeaeb28eff10dbc2c80c6bfbfbae0315d64", "title": "Solving Empirical Risk Minimization in the Current Matrix Multiplication Time"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "67f7ff756e80f5f4711fab956c2c02da285ac9ee", "title": "Stronger L2/L2 compressed sensing; without iterating"}, {"paperId": "2fe7dba5a58aee5156594b4d78634ecd6c7dcabd", "title": "End-to-End Open-Domain Question Answering with BERTserini"}, {"paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235", "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"}, {"paperId": "bb5bc0acea8d452a7999c512127b4f7b3acf8a6d", "title": "Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration"}, {"paperId": "dd68bb556b2f70a31dd2f8b089d270506e0cd3c5", "title": "Solving linear programs in the current matrix multiplication time"}, {"paperId": "4a1004ecd34118116344633c7cdcc34493c423ee", "title": "Rethinking the Value of Network Pruning"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "86be5c90c4128ec59b1c320a16996bb5de68624e", "title": "Texygen: A Benchmarking Platform for Text Generation Models"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3db8730c203f88d7f08a6a99e8c02a077dc9b011", "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference"}, {"paperId": "daae2cdf397ccdc6b4c86b31f61720a956ce23a4", "title": "Heavy Hitters via Cluster-Preserving Clustering"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "ec5841cac594eea8bf0d6b2a75b3a1a630f5e053", "title": "LRFU: A Spectrum of Policies that Subsumes the Least Recently Used and Least Frequently Used Policies"}, {"paperId": "4b4ee1ee9bbfd9527fba0bbd761bd61a59f96a48", "title": "The LRU-K page replacement algorithm for database disk buffering"}, {"paperId": "b9e43395663f74c581982e9ca97a0d7057a0008c", "title": "An analysis of approximations for maximizing submodular set functions\u2014I"}, {"paperId": "d7e89fc53355892081fa03f17e80c58bb4dae36d", "title": "An anomaly in space-time characteristics of certain programs running in a paging machine"}, {"paperId": "7779c10dfa1f84953016b6292844815c5faf84f5", "title": "A Study of Replacement Algorithms for Virtual-Storage Computer"}, {"paperId": "4b56eef2862f7f553686f1dd190c56017122a6a0", "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"}, {"paperId": "bd754fe83021ef0b521729444b827f7c1559e7f6", "title": "An Online and Unified Algorithm for Projection Matrix Vector Multiplication with Application to Empirical Risk Minimization"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "afe910067c96f3a3b8e3db612e6757957ee58fb4", "title": "Oblivious Sketching-based Central Path Method for Linear Programming"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Submodularity in machine learning applications"}, {"paperId": null, "title": "Beyond convexity: Submodularity in machine learning"}, {"paperId": null, "title": "Combinatorial optimization: polyhedra and e\ufb03ciency , volume 24"}, {"paperId": null, "title": "HuggingFace"}, {"paperId": null, "title": "Early-Bird Property"}, {"paperId": null, "title": "Here \u03c3 min (A) denotes the smallest singular"}, {"paperId": null, "title": "Convex minimization with integer minima in (cid:101) O ( n 4 ) time"}, {"paperId": null, "title": "Alpacaeval: An automatic evaluator of instruction-following models"}, {"paperId": null, "title": "Suppose that b \u2265 0 n and b 1 \u2264 1. Here 0 n denotes a length-n vector where all the entries are zeros. (Here b \u2265 0 n denotes b i \u2265 0 for all i"}, {"paperId": null, "title": "Trainable trans-former in transformer"}, {"paperId": null, "title": "A nearly-linear time algorithm for structured support vector machines"}, {"paperId": null, "title": "Assume that w 2 i \u2265 200 \u00b7 exp(R 2 ) + l/\u03c3 min (A) 2 for all i"}, {"paperId": null, "title": "The proof framework follows from"}, {"paperId": null, "title": "Here A denotes the spectral norm of matrix A"}, {"paperId": null, "title": "remark that \u03c9 denotes the exponent of matrix multiplication (i.e., n \u03c9 is the time of multiplying \u00d7 n matrix with another n \u00d7 n matrix)"}, {"paperId": null, "title": "Following from Lemma D.47, we know the Hessian of the loss function is positive definite. Following from Lemma D.48, we know the Hessian of L is Lipschitz"}]}