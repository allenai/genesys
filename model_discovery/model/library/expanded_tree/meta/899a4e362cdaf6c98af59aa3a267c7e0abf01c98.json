{"paperId": "899a4e362cdaf6c98af59aa3a267c7e0abf01c98", "abstract": "We initiate the first empirical study on the use of MLP architectures for vision-and-language (VL) fusion. Through extensive experiments on 5 VL tasks and 5 robust VQA benchmarks, we find that: (i) Without pre-training, using MLPs for multimodal fusion has a noticeable performance gap compared to transformers; (ii) However, VL pre-training can help close the performance gap; (iii) Instead of heavy multi-head attention, adding tiny one-head attention to MLPs is sufficient to achieve comparable performance to transformers. Moreover, we also find that the performance gap between MLPs and transformers is not widened when being evaluated on the harder robust VQA benchmarks, suggesting using MLPs for VL fusion can generalize roughly to a similar degree as using transformers. These results hint that MLPs can effectively learn to align vision and text features extracted from lower-level encoders without heavy reliance on self-attention. Based on this, we ask an even bolder question: can we have an all-MLP architecture for VL modeling, where both VL fusion and the vision encoder are replaced with MLPs? Our result shows that an all-MLP VL model is sub-optimal compared to state-of-the-art full-featured VL models when both of them get pre-trained. However, pre-training an all-MLP can surprisingly achieve a better average score than full-featured transformer models without pre-training. This indicates the potential of large-scale pre-training of MLP-like architectures for VL modeling and inspires the future research direction on simplifying well-established VL modeling with less inductive design bias. Our code is publicly available at: https://github.com/easonnie/mlp-vil", "venue": "arXiv.org", "year": 2021, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is suggested that MLPs can effectively learn to align vision and text features extracted from lower-level encoders without heavy reliance on self-attention, and an all-MLP VL model is sub-optimal compared to state-of-the-art full-featured VL models when both of them get pre-trained."}, "embedding": {"model": "specter_v2", "vector": [0.353190541267395, 0.3634589612483978, -0.3124803304672241, -0.1109323799610138, -0.5189015865325928, -0.1560429185628891, 0.6241168975830078, 0.019693788141012192, -0.4664000868797302, -0.53035968542099, 0.9253224730491638, 0.19484801590442657, 0.6218768358230591, 0.4978523254394531, -0.16585330665111542, 0.2223515808582306, -0.45625925064086914, 0.009127692319452763, -0.4264596104621887, -0.7076616287231445, 0.060230549424886703, -0.6128256916999817, -0.8933675289154053, 0.2754458487033844, 0.29691317677497864, 0.6734541654586792, 0.20902469754219055, 1.4658960103988647, -0.3796643018722534, 0.7122515439987183, 0.4361923038959503, -0.6021353006362915, 0.11295384913682938, 0.22917227447032928, -0.407058984041214, 0.177036851644516, 0.5755606889724731, -0.49702009558677673, -0.37414535880088806, 0.6049085259437561, 0.23025022447109222, 0.005516527686268091, 0.6000693440437317, -0.6341761350631714, -0.8763121366500854, 0.3917458653450012, 0.28141725063323975, 0.252870112657547, -0.12756839394569397, 0.1738474816083908, 1.5110725164413452, -1.293894648551941, -0.1343291848897934, 1.592786192893982, 0.2292812168598175, 0.6317506432533264, 0.05167979747056961, -0.24533222615718842, 0.3601303994655609, -0.3261714279651642, -0.43524983525276184, -0.6455991268157959, 0.045749131590127945, -0.30803772807121277, 1.7383537292480469, -0.39599937200546265, -0.4885562062263489, 0.3555273413658142, 0.3229166567325592, 1.5722639560699463, -0.19195076823234558, -0.9512175917625427, -0.3677009046077728, -0.06230461969971657, 0.27621445059776306, 0.9084975719451904, -0.7377362251281738, 0.1941705197095871, -0.8857249617576599, -0.12967103719711304, 0.22979529201984406, 0.06950606405735016, -0.015667792409658432, -0.03362144157290459, -0.4041154086589813, 0.7934188842773438, 0.43816763162612915, 0.7515683770179749, -0.16450658440589905, 0.7107543349266052, 0.7418680787086487, 0.4610762894153595, -0.3805021643638611, 0.5750953555107117, 0.08421493321657181, 0.585888147354126, -0.7346643209457397, -0.020679056644439697, -0.4248638153076172, 0.901391327381134, -0.17822818458080292, 0.22580093145370483, -1.463247537612915, 0.20510564744472504, 1.6560662984848022, 0.4522233009338379, 0.0727175623178482, -0.8580562472343445, 0.5269911289215088, -0.6147041916847229, 0.1266661137342453, -0.493196040391922, -0.2828388214111328, -0.20683029294013977, -0.5749399065971375, -0.9840207695960999, -0.7052284479141235, 0.14672034978866577, -0.9599398374557495, 0.5555227398872375, -0.8245794177055359, -0.5520145297050476, 0.6463530659675598, 0.40462470054626465, 1.0742623805999756, 0.5927456617355347, 0.42086532711982727, 0.23071545362472534, 1.1668447256088257, -1.0091367959976196, -0.7785799503326416, -1.0282028913497925, 0.6005639433860779, -0.8268658518791199, 0.753049373626709, -0.44981008768081665, -1.1113686561584473, -1.3095581531524658, -0.9250193238258362, -0.24749645590782166, 0.019870596006512642, 0.7443957924842834, 0.7741529941558838, 0.35423555970191956, -1.4288517236709595, 0.4703853726387024, 0.3368847072124481, -0.328130304813385, 0.2978336215019226, 0.33673110604286194, 0.23825733363628387, -0.460705429315567, -0.9432405829429626, 0.4515295922756195, -0.1354866772890091, -0.3455422818660736, -0.24483129382133484, -0.13489095866680145, -1.4795740842819214, -0.27880483865737915, -0.058502137660980225, -0.9319570660591125, 1.398903489112854, -0.27428093552589417, -1.2313919067382812, 0.5516286492347717, -0.6429928541183472, 0.41789278388023376, -0.0048917196691036224, -0.21833939850330353, -0.6687842011451721, -0.3717278242111206, -0.3212631940841675, 1.1028721332550049, 0.740717351436615, -0.377309113740921, -0.1699717789888382, 0.3950823247432709, 0.011802288703620434, 0.28789451718330383, -0.25181493163108826, 1.2435091733932495, -0.6296799778938293, 0.06563670933246613, -0.20806917548179626, 0.9351456761360168, 0.09535630792379379, -0.42456141114234924, -0.7133248448371887, -1.0585535764694214, 0.6632034778594971, -0.025261754170060158, 0.5808542370796204, -1.1074037551879883, -0.6909671425819397, -0.5649821758270264, -0.0405878983438015, 0.07499832659959793, -1.1544182300567627, 0.8066095113754272, 0.11703097820281982, -0.030581969767808914, -0.1027594655752182, -1.8331857919692993, 0.25565409660339355, -0.16621871292591095, -0.7503077387809753, -0.36837172508239746, 0.4895556569099426, 1.432976484298706, -0.6635108590126038, -0.2546841502189636, 0.028980206698179245, 0.5027167201042175, -0.535006582736969, 1.3417576551437378, -0.36756670475006104, 0.46768149733543396, 0.15651893615722656, 0.09246357530355453, 0.06012110784649849, -0.3768584132194519, -0.19772614538669586, -0.6710452437400818, 0.1059158593416214, -0.10366141051054001, 0.17911337316036224, 1.6842557191848755, -0.23179501295089722, 0.8276820182800293, -0.12211879342794418, -0.23962880671024323, 0.35044941306114197, 0.5140162706375122, -0.421934574842453, -0.5467686653137207, 0.5017250776290894, 0.19604898989200592, -0.695626437664032, 0.046608634293079376, 0.9869251847267151, 0.740493893623352, -0.38428568840026855, -0.15059134364128113, 0.8446145057678223, -0.5118758678436279, 0.22698719799518585, 0.3133968710899353, 0.7260867357254028, 0.027710627764463425, 0.08574160188436508, 0.24623213708400726, 0.5181798338890076, -0.8848731517791748, -0.2713499367237091, 0.6994147300720215, 0.4629298746585846, 1.1120717525482178, 0.3716713786125183, -0.43051519989967346, -0.05055004358291626, -0.23101681470870972, 1.125210165977478, 1.5945630073547363, -0.09362329542636871, 0.0420011505484581, -0.611411452293396, -0.29612642526626587, -0.32149404287338257, 0.07050661742687225, -0.3607507348060608, -0.15265294909477234, -0.11863845586776733, -0.702940046787262, 0.8957284092903137, 0.259121835231781, 0.6875253915786743, -0.925839364528656, -0.11559474468231201, -0.5096381306648254, 0.2529975175857544, -0.8438050746917725, -0.851754903793335, -0.11992663890123367, -0.3460690677165985, 0.01643642783164978, -0.4070967435836792, -0.35168004035949707, 0.017916284501552582, -0.6678950190544128, 0.8495555520057678, -0.5684406757354736, -0.16527768969535828, 0.7959058880805969, 0.5530233979225159, -0.7781540155410767, -1.014366865158081, -0.36547335982322693, -0.04889675974845886, 0.13607071340084076, 0.2788742482662201, 0.9296838641166687, -0.030882421880960464, -0.37264570593833923, -0.3779672384262085, 0.12452609091997147, 0.6576629281044006, -0.12852631509304047, 0.5380082130432129, -0.6877117156982422, -0.013102425262331963, -0.6231266856193542, 1.3819135427474976, 0.5206129550933838, -0.22900572419166565, 0.5285598039627075, -0.46312057971954346, -0.24492834508419037, 0.02825547754764557, -0.33447471261024475, -0.23090200126171112, -0.8022986054420471, 0.10428125411272049, -0.6148476600646973, -0.34237581491470337, 0.016488445922732353, 0.18124637007713318, 0.07913721352815628, 0.08498251438140869, 0.49327605962753296, 0.5078455209732056, 0.06186157837510109, 0.9258859157562256, -0.25456297397613525, 0.8006759285926819, 0.43941259384155273, 0.053303465247154236, -0.008430331945419312, -0.25735586881637573, -0.8741521239280701, -0.38789257407188416, -0.393947571516037, -0.0662919282913208, -0.38055381178855896, 0.7646514773368835, -0.7115036249160767, -0.7814405560493469, -0.10250812768936157, -1.3702194690704346, 0.3032597005367279, -0.16217723488807678, -0.3458709120750427, -0.2583602964878082, -0.7204325795173645, -1.0230213403701782, -0.45339661836624146, -0.862660825252533, -1.3696448802947998, 0.528704822063446, 0.49591201543807983, -0.28241831064224243, -0.3009594976902008, 0.13283324241638184, -0.17117758095264435, 0.9936733841896057, -0.8138543963432312, 0.6704873442649841, -0.03964649885892868, 0.19857698678970337, -0.24992915987968445, 0.024318017065525055, 1.090666651725769, 0.14511413872241974, 0.353188693523407, -1.1762847900390625, 0.13439713418483734, -0.09600543230772018, -0.2613544464111328, 0.32606133818626404, 0.20451149344444275, 0.11389553546905518, 0.4307191073894501, -0.12681645154953003, -0.11942319571971893, 1.391158103942871, -0.7106481790542603, -0.14160439372062683, -0.24292874336242676, 1.2882146835327148, 0.4693027436733246, -0.5568484663963318, 0.38960951566696167, 0.6312362551689148, 0.21025653183460236, 0.28072288632392883, -0.3632740080356598, -0.5336451530456543, -0.34850960969924927, 0.68742835521698, 1.6431108713150024, 0.2600599527359009, -0.1711605042219162, -1.011818766593933, 0.6120728850364685, -1.1398539543151855, -0.1757887452840805, 0.5681487321853638, 0.31201738119125366, 0.05531642958521843, -0.6104496121406555, -0.44378966093063354, -0.28503069281578064, 0.42335614562034607, 0.5320497751235962, -0.0781790167093277, -1.1385829448699951, -0.20376358926296234, 0.34058576822280884, -0.019332949072122574, 0.6192545890808105, -0.5557375550270081, 0.25414907932281494, 14.537054061889648, 0.3515723645687103, -0.25498712062835693, 0.6246474385261536, 0.5303236842155457, 0.35576626658439636, -0.5288477540016174, -0.013958867639303207, -1.0043705701828003, -0.2736557424068451, 0.7762620449066162, 0.7980737090110779, 0.38941964507102966, -0.2170635163784027, -0.2441319227218628, 0.3224748969078064, -0.6911028623580933, 0.8592652678489685, 0.7211735248565674, -1.2212613821029663, 0.4463990032672882, 0.04452956095337868, 0.4461613893508911, 0.7604641318321228, 1.015777587890625, 0.9234759211540222, 0.2417239099740982, -0.38193392753601074, 0.3230418562889099, 0.7143890857696533, 0.8914155960083008, 8.058562525548041e-05, 0.5515601634979248, 0.22951805591583252, -0.6124133467674255, -0.42148131132125854, -0.406003475189209, -1.0280441045761108, 0.32862886786460876, -0.23511752486228943, -0.35638678073883057, -0.19135014712810516, -0.44210341572761536, 0.3968745172023773, -0.07965958118438721, 0.3910428285598755, -0.3133266866207123, 0.6208128929138184, 0.26268893480300903, -0.06651365756988525, 0.3798530399799347, 0.8782243728637695, 0.27352142333984375, 0.26740163564682007, -0.29405349493026733, -0.42687955498695374, 0.13408403098583221, 0.25076839327812195, -0.7717850208282471, 0.19268612563610077, -0.2520202100276947, -0.269563227891922, -0.5810577273368835, 0.7411953210830688, 0.6689726114273071, 0.529939591884613, -0.9304600954055786, 0.30571606755256653, 0.35339564085006714, 0.37117302417755127, -0.11275286227464676, -0.1034306213259697, 0.27503159642219543, -0.8406358361244202, 0.14421731233596802, 0.6457033157348633, -0.090266153216362, -0.44715553522109985, -0.6213893890380859, -0.19281941652297974, 0.2373359352350235, -1.0569802522659302, -1.246860146522522, 0.799827516078949, -0.602902352809906, -0.44927069544792175, 0.275112509727478, -0.7207710146903992, -0.6149069666862488, 0.6706506013870239, -1.6886202096939087, -1.0383292436599731, 0.44445183873176575, -0.3638085722923279, -0.40368348360061646, -0.5872689485549927, 1.0603193044662476, -0.031692784279584885, -0.056962836533784866, 0.0563858337700367, -0.1057838723063469, 0.24415846168994904, 0.04695797711610794, -0.4156925082206726, 0.6530259251594543, 0.34889891743659973, 0.047323185950517654, -0.1635138839483261, 0.2014668732881546, 0.25159716606140137, -0.9948371052742004, -0.006777632981538773, 0.7950165271759033, -0.8555608987808228, -0.5109753608703613, -0.5387097001075745, -0.6465033292770386, -0.06078416109085083, 1.2932921648025513, -0.01873128116130829, 0.26141563057899475, 0.04696841165423393, -0.5219607353210449, -0.1288927048444748, -0.940500020980835, 0.295964777469635, 0.04244980588555336, -1.1262346506118774, -0.614469587802887, 0.0016765923937782645, 0.6573482751846313, -0.7311751246452332, -0.3967582583427429, 0.11036590486764908, 0.03289736807346344, -0.02629680000245571, 0.9002112746238708, -0.2817658483982086, 0.646077036857605, 0.5770653486251831, -0.48958954215049744, -0.5735213160514832, 0.21131020784378052, -0.9119246006011963, 0.07247061282396317, 0.4237593710422516, 0.6757855415344238, -0.35767650604248047, -0.08239272981882095, 0.8045189380645752, 0.2841019928455353, -0.6382803320884705, -0.5493353605270386, 0.05931513011455536, -0.12479134649038315, -0.5732913017272949, 0.23008236289024353, -0.27461937069892883, -0.2758556306362152, 0.3405245244503021, 0.31692051887512207, 0.7278502583503723, -0.2771015167236328, -0.5876106023788452, 0.36407867074012756, -0.1601588875055313, -0.30292853713035583, -0.7256980538368225, -0.679038941860199, -1.3311854600906372, 0.00955134816467762, -1.093721628189087, 0.12833474576473236, -1.4505672454833984, -0.306509405374527, 0.46100154519081116, -0.4885031282901764, 0.44841474294662476, 0.2073632776737213, -0.2862619459629059, 0.21238376200199127, -0.22016675770282745, -0.7110384702682495, 0.9626848101615906, 1.2112438678741455, -1.121293544769287, 0.002536151558160782, -0.1946619749069214, -0.5567184090614319, 0.14495506882667542, 0.2708505392074585, -0.10175561904907227, -0.8719330430030823, -1.4494296312332153, 0.26846277713775635, 0.039826035499572754, -0.19300851225852966, -0.7452294230461121, 0.7649467587471008, 0.6666780114173889, -0.02053973451256752, -0.004165589809417725, 0.5811966061592102, -0.44482988119125366, -1.0163971185684204, 0.2611812949180603, -0.8464647531509399, 0.0737946555018425, 0.44747596979141235, -0.7991644740104675, -0.5213345289230347, 0.7327274680137634, -0.16663818061351776, -0.7064989805221558, -0.9115167856216431, 0.41648438572883606, -0.25105801224708557, -0.020070664584636688, -0.38609352707862854, 0.12155912071466446, -1.240958571434021, -0.5294215083122253, 0.07792609930038452, 0.4970869719982147, -0.5447511076927185, 1.2197171449661255, 1.0073920488357544, -0.8146408796310425, -0.36691972613334656, 0.41367852687835693, 0.21050140261650085, -0.36749014258384705, 0.6447377800941467, 0.44534388184547424, -0.15371930599212646, 0.31496506929397583, 0.13033118844032288, -0.16622915863990784, -0.9367436170578003, -0.07720620930194855, 0.6388249397277832, -0.4269961416721344, 0.18339425325393677, 1.3413317203521729, -0.17605948448181152, -1.2743768692016602, 0.19214744865894318, -1.0648514032363892, -0.9248510599136353, -0.32702401280403137, 0.5142969489097595, -0.32862749695777893, -0.05775180086493492, -0.5934688448905945, -0.5707609057426453, 0.5839487314224243, 0.051227882504463196, -0.7428015470504761, 0.22040396928787231, -0.16876384615898132, -0.38537946343421936, 0.5156142115592957, 0.7719886302947998, -0.3579935133457184, -0.8596985936164856, -0.7070252895355225, -0.4905022978782654, 0.26180604100227356, 0.11182942241430283, -0.46242862939834595, -0.647447943687439, 1.0986251831054688, 0.7201213240623474, 0.06499937176704407, 0.5160512328147888, 0.2596001923084259, 0.12722741067409515, 0.610305905342102, -0.2939978241920471, -0.42030951380729675, -0.5085699558258057, 1.3048969507217407, 1.3375440835952759, -1.1705987453460693, 0.10842113941907883, 0.0015219987835735083, -1.1550936698913574, 0.8664669394493103, 0.10248075425624847, 0.1518988162279129, 0.7250117659568787, -0.41254767775535583, 0.5691800117492676, 0.09885630756616592, -0.8296170830726624, -0.4190295338630676, 1.491848111152649, 1.0516945123672485, 0.7944129109382629, 0.36997750401496887, 0.3832443654537201, 0.5315366983413696, 0.26645970344543457, 0.13553275167942047, 0.5473555326461792, 0.5280501246452332, -0.06185971945524216, -0.10509414970874786, 0.1512780636548996, 0.6846179366111755, -0.5069650411605835, -0.4988578259944916, 0.24128831923007965, 0.2456667423248291, 0.3202733099460602, 0.9766018986701965, 0.749236524105072, -0.23831607401371002, 0.8862878680229187, 0.07858689874410629, 0.7984201908111572, -0.6412895917892456, -0.49116867780685425, -0.43137508630752563, -0.822907030582428, 0.023626063019037247, -0.5988351702690125, -0.5100284814834595, -0.5191403031349182, 0.09992770850658417, 0.5706256628036499, -0.3480788469314575, 0.6766634583473206, 1.336125373840332, 0.7489644885063171, 0.4569207727909088, -0.41210755705833435, -0.3992057740688324, -0.22607004642486572, -1.1434359550476074, 0.2114923745393753, -0.5964328646659851, 0.28178128600120544, -0.09635081887245178, 0.07365906983613968, -0.5024706721305847]}, "authors": [{"authorId": "20832663", "name": "Yi-Liang Nie"}, {"authorId": "2107923860", "name": "Linjie Li"}, {"authorId": "144702900", "name": "Zhe Gan"}, {"authorId": "2992833", "name": "Shuohang Wang"}, {"authorId": "8652308", "name": "Chenguang Zhu"}, {"authorId": "48262024", "name": "Michael Zeng"}, {"authorId": "2145253136", "name": "Zicheng Liu"}, {"authorId": "143977268", "name": "Mohit Bansal"}, {"authorId": "29957038", "name": "Lijuan Wang"}], "references": [{"paperId": "f8a287be26c30b4362f9504ba01e4e7269790c60", "title": "Are we ready for a new paradigm shift? A survey on visual deep MLP"}, {"paperId": "58970a426b687bb080b7fed3b4b78ab1ebaa56f4", "title": "Hire-MLP: Vision MLP via Hierarchical Rearrangement"}, {"paperId": "f75cddf2d42ed01b34686704eb3504becef67442", "title": "CycleMLP: A MLP-like Architecture for Dense Prediction"}, {"paperId": "71363797140647ebb3f540584de0a8758d2f7aa2", "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "8f167ec1149921fac63b1ea855443de109bb013a", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?"}, {"paperId": "11f606fd65df3de5d99c0034d8dd4ec5205090f1", "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training"}, {"paperId": "2435ffb8ed3212156d6b6f19f633a861399cf30e", "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition"}, {"paperId": "41274cf92e2ccd323deae48c5dcf09c8ab17ad0a", "title": "MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis"}, {"paperId": "60707f6d2bffeab09e8f1d073fce4fc06ab89ec1", "title": "S2-MLP: Spatial-Shift MLP Architecture for Vision"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "70b4c724a0f22198f9a04f504b1b298299e4cc37", "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation"}, {"paperId": "a00b5ef1dd2d2144d8372b72ca75a5b52f7b12b7", "title": "Human-Adversarial Visual Question Answering"}, {"paperId": "c1a48692bc40ad56edef126484860fcaacffd3af", "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "0768aba7d87ddda3482fd7892b189f84711ede47", "title": "Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "77a096d80eb4dd4ccd103d1660c5a5498f7d026b", "title": "Dynabench: Rethinking Benchmarking in NLP"}, {"paperId": "2fa4938001b18f464c62aa38a5a469bb92569d57", "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "ba4a4d31d2af23eefadbf19e5efd5a7d4fd89143", "title": "Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "81002fbb777f860f9aac2bbc24467a62345af279", "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers"}, {"paperId": "be0014c1fbc3e664686610d2c85f75038a4f6e4f", "title": "VinVL: Making Visual Representations Matter in Vision-Language Models"}, {"paperId": "2c340d7bc21aa6a9f44b466b7f74ac9150dfcb41", "title": "A Closer Look at the Robustness of Vision-and-Language Pre-trained Models"}, {"paperId": "cb9f472fba21e7c79b5042e7afcf3efd93b5a077", "title": "Roses are Red, Violets are Blue\u2026 But Should VQA expect Them To?"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3b0d38302115af1165639d6fb34c4c19187b2a6f", "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "5546e6073f3b82967b12c87d6b90ba722c4b85c6", "title": "Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "598a2ee223e2949c3b28389e922c1892b4717d2a", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"}, {"paperId": "20dc158a6abd1f92a4534ae064d527821a91685d", "title": "VQA-LOL: Visual Question Answering under the Lens of Logic"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "320464aa0231bc728c7d9ab7e71e552c12a7486b", "title": "Meta Module Network for Compositional Visual Reasoning"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029", "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "735a63b58349e07b84c2e31927ce1b1cfaf09980", "title": "Cycle-Consistency for Robust Visual Question Answering"}, {"paperId": "3c54b796cc10cb530f77caa4d18e1c80ac863822", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}, {"paperId": "0955252cd57db8503a2ed9e56f195fa44b1bc0d4", "title": "Visual Entailment Task for Visually-Grounded Language Learning"}, {"paperId": "cf336d272a30d6ad6141db67faa64deb8791cd61", "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs"}, {"paperId": "2fa693d586f09edba19512f3e11ed999a1bdef35", "title": "A Fast Proximal Point Method for Computing Exact Wasserstein Distance"}, {"paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "def584565d05d6a8ba94de6621adab9e301d375d", "title": "Visual7W: Grounded Question Answering in Images"}, {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "title": "Deep visual-semantic alignments for generating image descriptions"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "f22f6972e66bdd2e769fa64b0df0a13063c0c101", "title": "Multilayer feedforward networks are universal approximators"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "0c53dd35f92534faabcf4f1689cc12195d8a46ff", "title": "RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "attention to MLP-V I L is also able to substantially close performance gap for zero-shot evaluation ( \u223c 4 on R@ or even enable stronger performance than Transformer"}, {"paperId": null, "title": "Flickr30K after"}, {"paperId": null, "title": "(1) zero-shot (Table 8) and (2) \ufb01netuning (Table 9) We summarize our \ufb01nding below"}]}