{"paperId": "aa62931579f0a03f9a05fc16506a22b166ef0875", "abstract": "Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.", "venue": "arXiv.org", "year": 2023, "citationCount": 9, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2307.01189", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models), and introduces innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulateand fine-Tune a 125 million parameter transformer model within a single forward pass."}, "embedding": {"model": "specter_v2", "vector": [0.18509596586227417, 0.3697220981121063, -0.2228335291147232, -0.06114830821752548, -0.5238784551620483, -0.3252074122428894, 0.46638575196266174, -0.35642582178115845, -0.4415489137172699, -0.24802787601947784, 0.5253903269767761, -0.5102313756942749, 0.3360000550746918, 0.3093773126602173, -0.4128176271915436, 0.08870045095682144, -0.9313680529594421, 0.422586053609848, -0.1323804259300232, -0.1747826337814331, -0.6004303097724915, -0.5791428089141846, -0.9303550124168396, 0.312173992395401, 0.4939127266407013, 0.5088467597961426, 0.33093371987342834, 0.77800053358078, -0.48436421155929565, 0.5671359896659851, 0.7048725485801697, -0.4598897397518158, 0.18227022886276245, 0.20551079511642456, -0.13941308856010437, 0.02937290444970131, 0.2807456851005554, -0.10515463352203369, -0.3631207048892975, 0.5445783734321594, -0.22974573075771332, 0.17135678231716156, 0.22672660648822784, -0.6001353859901428, 0.10219874233007431, 1.1858272552490234, 0.9318199753761292, 0.8376283049583435, -0.6532396674156189, -0.5308517217636108, 1.1245008707046509, -1.525260329246521, -0.5497272610664368, 1.4792593717575073, 0.7860026359558105, 0.4762572944164276, -0.625953733921051, -0.733767569065094, 0.6687129139900208, -0.29139530658721924, -0.7305087447166443, -0.5517418384552002, -0.020491769537329674, 0.018197447061538696, 1.8862104415893555, -0.25426822900772095, 0.36603593826293945, 0.5476601719856262, -0.24221661686897278, 1.4239379167556763, -0.22999230027198792, -0.7636115550994873, -0.36836177110671997, 0.33171963691711426, 0.1184161901473999, 0.9397420287132263, -0.6033273339271545, 0.4607315957546234, -0.915915310382843, -0.16819249093532562, 0.31178563833236694, 0.040794648230075836, 0.43492335081100464, 0.1084245964884758, -0.28127196431159973, 0.8269502520561218, 0.44883447885513306, 1.1899009943008423, -0.09759959578514099, 0.9385464191436768, 0.5600083470344543, 0.364167720079422, -0.05128587409853935, 0.4325086772441864, -0.3155750334262848, 0.19013218581676483, -0.9671785831451416, -0.08515272289514542, -0.013516532257199287, 0.8446753621101379, -0.0680263340473175, 0.5013996362686157, -0.741325318813324, 0.3918486535549164, 1.2696248292922974, 0.29501253366470337, 0.2651525139808655, -0.6983970403671265, 0.5659250617027283, -0.7064667344093323, -0.2394590675830841, -0.014519227668642998, -0.2527045011520386, -0.7110631465911865, -0.5768101811408997, -1.3294428586959839, -0.4735890030860901, 0.015111454762518406, -0.8214170336723328, 0.9013837575912476, -0.282694011926651, 0.4337412416934967, 0.2682705521583557, -0.08991163969039917, 0.19414448738098145, 0.48457688093185425, 0.2018028199672699, -0.04163604974746704, 1.0622085332870483, -1.1943410634994507, -0.41309428215026855, -1.1563048362731934, 0.6421599984169006, -0.23467645049095154, 0.2894366681575775, -0.3635677993297577, -0.9806334972381592, -0.7433637380599976, -0.7114664912223816, 0.07252144068479538, -0.554884672164917, -0.11383530497550964, 1.1306135654449463, 0.7298068404197693, -1.3246698379516602, 0.6009812951087952, -0.23950038850307465, 0.05692270025610924, 0.2774444818496704, 0.23047979176044464, 0.4433020055294037, -0.23780101537704468, -1.4786994457244873, 0.18656262755393982, 0.38939839601516724, -0.3055885434150696, -0.7065348029136658, -0.9007328748703003, -0.944513201713562, -0.08829056471586227, 0.15434630215168, -0.5304990410804749, 1.6222999095916748, -0.3385668396949768, -1.3470406532287598, 0.6371892094612122, -0.47846320271492004, 0.10706472396850586, 0.24283075332641602, -0.05942322313785553, -0.5298641920089722, -0.7648299336433411, -0.24045242369174957, 0.4032191336154938, 0.578829288482666, -0.2762295603752136, -0.2765999436378479, -0.013259445317089558, 0.007891508750617504, 0.06845059245824814, -0.23160143196582794, 1.0174731016159058, -0.7151575684547424, -0.23706692457199097, 0.34195592999458313, 0.6951881647109985, -0.3813096284866333, -0.23013003170490265, -0.46647417545318604, -0.6673357486724854, 0.9439618587493896, 0.08171986788511276, 1.0709412097930908, -0.8962632417678833, -1.2720067501068115, 0.13639666140079498, -0.23772871494293213, 0.06003141403198242, -0.6616874933242798, 0.5903661847114563, -0.3576164245605469, 0.14422763884067535, -0.04221442714333534, -1.1045866012573242, 0.161489799618721, -0.12226182222366333, -0.7103027701377869, -0.6186645030975342, 0.10346540808677673, 0.9542510509490967, -1.0847505331039429, 0.03910975530743599, 0.016359368339180946, 0.6295580863952637, -1.1419603824615479, 1.1935491561889648, -0.3456727862358093, 0.40127402544021606, -0.14753837883472443, -0.01645219884812832, -0.09837307780981064, -0.22721977531909943, 0.3621886074542999, -0.3102474808692932, -0.23570148646831512, 0.9058572053909302, -0.2587398290634155, 1.236464023590088, -1.0298134088516235, 0.32251930236816406, 0.1465025693178177, -0.6820060610771179, -0.2870381474494934, 0.7046830058097839, -0.29783695936203003, -0.3147645890712738, 0.3173787593841553, 0.39454004168510437, -0.4158814251422882, 0.48995718359947205, 0.6181065440177917, 0.7025743126869202, -0.1519547402858734, 0.2336399108171463, 0.6142486929893494, -0.17852263152599335, 0.35218581557273865, 0.24117708206176758, 0.47133710980415344, 0.2726568877696991, 0.010366895236074924, -0.09454196691513062, 0.2727580964565277, -1.329578161239624, -0.07105289399623871, 0.7714067697525024, 0.6225040555000305, 0.6170194149017334, 0.43880513310432434, -0.4945201575756073, 0.011211197823286057, -0.19248813390731812, 0.36165758967399597, 1.6907330751419067, -0.7301396727561951, -0.055418532341718674, -1.0530579090118408, -0.4457559883594513, -0.3233945667743683, 0.19907301664352417, -0.4636743366718292, 0.11880392581224442, -0.4149416387081146, -0.9512930512428284, 1.100829839706421, 0.2679743766784668, 1.0390623807907104, -0.45485568046569824, 0.07777130603790283, 0.036669034510850906, 0.3391724228858948, -0.942473828792572, -0.7339356541633606, 0.8722958564758301, -0.9862740635871887, 0.00832388922572136, 0.318636953830719, -0.3516296148300171, 0.14208494126796722, -0.7589356303215027, 1.0229015350341797, -0.39211639761924744, -0.07585912197828293, 0.024146435782313347, 0.8343763947486877, -0.3196831941604614, -1.004395604133606, 0.35236719250679016, 0.18836107850074768, -0.039957378059625626, 0.08338535577058792, 0.04508833587169647, 0.34382086992263794, -0.07758276909589767, -0.20136046409606934, 0.12596668303012848, -0.08473360538482666, -0.09655364602804184, 0.5959451794624329, -0.18634502589702606, -0.17540350556373596, -1.489357352256775, 0.8481215834617615, 0.12451544404029846, -0.5897384881973267, 0.39305999875068665, -0.6816087365150452, 0.08981084823608398, 0.6674509644508362, -0.41974759101867676, -0.2297031283378601, -0.9955543279647827, -0.062420472502708435, -0.3165986239910126, 0.13392353057861328, 0.2393968403339386, 0.0025009214878082275, 0.1785058230161667, 0.005590848624706268, 0.5151807069778442, 0.031663645058870316, -0.20693904161453247, 0.5361724495887756, -0.9525671005249023, 0.035111647099256516, 0.0706971064209938, 0.4329190254211426, -0.5186871290206909, -0.5349209308624268, -0.49366495013237, -0.6182665228843689, -0.28016307950019836, 0.15052881836891174, -0.23002488911151886, -0.2625506520271301, -0.8236264586448669, -0.8142310976982117, 0.010880072601139545, -0.8574737310409546, -0.568720281124115, 0.32927191257476807, -0.3629505932331085, 0.05572860687971115, -1.2747734785079956, -1.2414594888687134, -0.6340632438659668, -0.7312861084938049, -1.4267537593841553, 0.46770766377449036, 0.061792075634002686, -0.03355906531214714, -1.2469831705093384, -0.4552007019519806, -0.44842416048049927, 1.3322237730026245, -0.9211251735687256, 0.706983745098114, 0.020340532064437866, -0.24526578187942505, 0.3020647168159485, 0.10019285976886749, 0.4530112147331238, -0.25566861033439636, 0.2916259765625, -1.4462438821792603, 0.26200297474861145, -0.3136100471019745, -0.07728146016597748, 0.19344766438007355, 0.14610600471496582, 0.8146706223487854, -0.21183843910694122, -0.8287515044212341, 0.680390477180481, 1.3034275770187378, -0.4119672179222107, 0.04372017830610275, -0.032202694565057755, 0.8570553660392761, -0.3311699628829956, -0.4646115303039551, 0.31146273016929626, 0.05562756955623627, 0.3639330267906189, 0.004386203363537788, -0.017219291999936104, 0.13186316192150116, -0.7193290591239929, 0.6422587633132935, 1.6369911432266235, 0.5595208406448364, 0.13333073258399963, -0.958497166633606, 0.694246768951416, -0.9940876364707947, -0.537132203578949, 0.9580808281898499, 0.7719038128852844, 0.8540105819702148, -0.2374388575553894, -0.4376949965953827, 0.11083338409662247, 0.21609406173229218, 0.30153828859329224, -0.2848135232925415, -0.9415475726127625, 0.3207213878631592, 0.4944097697734833, 0.11367842555046082, 1.078058123588562, -0.28152820467948914, 0.6695939898490906, 14.579035758972168, 0.9323692321777344, -0.08995402604341507, 0.7359870076179504, 0.5271911025047302, 0.47595563530921936, -0.595801055431366, -0.05827002972364426, -1.4073199033737183, -0.0655677318572998, 1.3023804426193237, 0.5026942491531372, 1.1711516380310059, 0.1866123378276825, 0.011164436116814613, 0.29415184259414673, -0.513474702835083, 0.4837969243526459, 0.3331722021102905, -1.1216247081756592, 0.4504404664039612, 0.06886287778615952, 0.057897359132766724, 0.7199257016181946, 0.7948183417320251, 1.032160997390747, 0.7029591202735901, -0.35922932624816895, 0.40851861238479614, 0.0006187962717376649, 0.9841822385787964, -0.37792131304740906, 0.16761773824691772, 0.5824358463287354, -1.05324387550354, -0.2815555930137634, -0.12505817413330078, -1.078303575515747, -0.09505663067102432, 0.5234896540641785, -0.9251607060432434, -0.7410266399383545, -0.05120911821722984, 0.6705267429351807, 0.1283804476261139, 0.3471350371837616, -0.4077969193458557, 0.9915333390235901, -0.38816386461257935, 0.1531309187412262, 0.41389912366867065, 0.3003448247909546, 0.1105479896068573, 0.21548742055892944, -0.08864671736955643, -0.22314293682575226, 0.07172012329101562, 0.35263368487358093, -0.5298045873641968, -0.3985899090766907, -0.4075528681278229, -0.1843756139278412, 0.038186654448509216, 0.837637722492218, 0.6278223395347595, 0.48174768686294556, -0.42572665214538574, 0.07686243206262589, 0.8327627778053284, 0.5280320048332214, -0.6202629208564758, 0.22635048627853394, 0.5944216251373291, -0.5665099024772644, 0.11731623858213425, 0.7123669385910034, 0.036277804523706436, -0.6658593416213989, -0.7240300178527832, -0.39063480496406555, 0.18952104449272156, -0.6776454448699951, -0.5569502115249634, 1.0607445240020752, -0.3139845132827759, -0.34518763422966003, 0.30069342255592346, -0.5033212304115295, -0.2896217405796051, 0.4047410488128662, -1.5006706714630127, -0.7250087857246399, 0.5817175507545471, -0.1646307408809662, -0.6108916401863098, 0.08132948726415634, 1.3247380256652832, 0.5552011132240295, -0.7224590182304382, 0.4699639678001404, 0.5792977809906006, 0.3061590790748596, -0.6392375230789185, -0.8079506158828735, 1.0356554985046387, 0.37689152359962463, 0.08662521094083786, 0.6126900315284729, -0.04067127779126167, 0.6961906552314758, -0.6872035264968872, -0.5263124108314514, 0.9117871522903442, -0.7680716514587402, -0.11709026992321014, -1.1441808938980103, -0.49739542603492737, 0.8129520416259766, 0.5342621207237244, -0.18125241994857788, 0.5284693241119385, 0.4312438368797302, -0.7644455432891846, -0.5420634150505066, -0.6893686056137085, 0.3021009862422943, 0.6481085419654846, -0.9413051009178162, -0.036280710250139236, -0.14512918889522552, 0.5969949960708618, -1.1032130718231201, -0.6405388116836548, -0.40200096368789673, 0.03319329395890236, -0.12616674602031708, 1.074171543121338, -0.24996253848075867, 0.2590591609477997, 0.8332788348197937, -0.25369635224342346, -0.8931907415390015, 0.171002596616745, -1.0517802238464355, -0.042488861829042435, 0.2517547309398651, 0.9820288419723511, -0.8501021862030029, 0.03498375043272972, 0.764387845993042, 0.18042638897895813, -0.5544513463973999, -0.6497065424919128, -0.1545444279909134, -0.14129862189292908, -0.9523575305938721, 0.4872189462184906, -0.10442809015512466, -0.21163146197795868, 0.34001466631889343, 0.8032660484313965, 0.5767682790756226, 0.08467403799295425, -0.8229863047599792, 0.14091859757900238, 0.046045828610658646, -0.31878387928009033, -0.3077203631401062, -0.33245545625686646, -1.0906102657318115, 0.012786922976374626, -1.3255109786987305, 0.11508442461490631, -0.6030184626579285, -0.26012304425239563, -0.5001202821731567, -0.33849117159843445, 0.28812962770462036, 0.1330111175775528, -0.4823521673679352, -0.46612125635147095, -0.8632408976554871, -0.49859359860420227, 0.681472659111023, 0.6812072992324829, -0.6299103498458862, 0.4184506833553314, 0.22180776298046112, -0.05073697119951248, 0.41261303424835205, 0.405448853969574, -0.6035229563713074, -1.0726414918899536, -1.4126508235931396, 0.17167094349861145, -0.2999809980392456, -0.24585506319999695, -0.821627676486969, 0.5380277037620544, 0.4751991629600525, -0.1297350972890854, 0.27071720361709595, 0.20391115546226501, -1.0734224319458008, -0.4114247262477875, 0.31986677646636963, -0.6510406136512756, 0.17218761146068573, 0.5685919523239136, -0.7436973452568054, -0.37923964858055115, 0.5949144959449768, 0.05007123574614525, -0.9982066750526428, -0.6611061096191406, 0.5421164035797119, -0.8348536491394043, 0.5423126220703125, -0.7083133459091187, -0.01746443659067154, -0.8402106165885925, -0.26055198907852173, 0.17094841599464417, 0.4941670000553131, -0.6012259721755981, 0.6461087465286255, 0.25212785601615906, -1.0556378364562988, 0.3209930956363678, 0.6495034694671631, -0.22032691538333893, -0.175155907869339, 0.2368089258670807, 0.5339012145996094, -0.4317624866962433, 0.4016379117965698, 0.5536360740661621, 0.18413938581943512, -0.8100390434265137, -0.10764318704605103, 1.1654958724975586, -0.8596805334091187, -0.0021674963645637035, 1.1927895545959473, -0.15500028431415558, -1.4649463891983032, 0.46724414825439453, -1.4609719514846802, -0.44240978360176086, -0.5365700125694275, 0.7388768792152405, -0.29626619815826416, 0.10732556134462357, 0.34146928787231445, -0.4850029945373535, 0.06260480731725693, 0.07431375235319138, -0.5358754992485046, 0.2851276099681854, -0.11777936667203903, -0.44858041405677795, 0.7800954580307007, 0.939842164516449, -0.5349105000495911, -0.8688923120498657, -0.8228896856307983, -0.375373512506485, 0.07020486891269684, 0.41281840205192566, -0.46115610003471375, -0.9461696743965149, 0.8245999813079834, 0.3280844986438751, 0.25188693404197693, 0.205796018242836, -0.514865517616272, 0.23603136837482452, 0.8848688006401062, 0.44517943263053894, -0.538318395614624, -0.7701940536499023, 1.6344541311264038, 1.0235446691513062, -0.8754197359085083, -0.038139719516038895, -0.17736414074897766, -0.6656548380851746, 0.5836215615272522, 0.2860810160636902, 0.07253085821866989, 1.3837482929229736, 0.266760915517807, 0.30846160650253296, 0.41523849964141846, -1.1314181089401245, -0.3908841013908386, 0.45240044593811035, 0.9407204389572144, 1.0100336074829102, 0.37766504287719727, 0.8827704191207886, 0.9213355779647827, 0.18137076497077942, -0.08930279314517975, 0.12696824967861176, 0.41557395458221436, -0.23476260900497437, -0.09939263761043549, -0.09785258024930954, 0.5396245718002319, -0.7370059490203857, -1.1680740118026733, 0.38120171427726746, 0.7498432993888855, 0.42763325572013855, 0.7025307416915894, 0.6464506387710571, -0.027994608506560326, 0.5297184586524963, 0.36507782340049744, 0.5415912866592407, -0.5348629355430603, -0.7261239886283875, -0.31762099266052246, -0.46073323488235474, -0.07215403765439987, 0.05191982537508011, -0.22774669528007507, -0.3767048716545105, -0.10947148501873016, 0.47099825739860535, 0.2752406895160675, 0.33258017897605896, 1.083886742591858, 0.3770284652709961, 0.6322604417800903, -0.1872919201850891, -0.41978245973587036, -0.3378947377204895, -0.8928285837173462, 0.039383456110954285, -0.8676267266273499, -0.009911843575537205, 0.06935618072748184, -0.1542672961950302, -0.5383983850479126]}, "authors": [{"authorId": "6191376", "name": "A. Panigrahi"}, {"authorId": "49288855", "name": "Sadhika Malladi"}, {"authorId": "67284811", "name": "Mengzhou Xia"}, {"authorId": "145563465", "name": "Sanjeev Arora"}], "references": [{"paperId": "7723b4da2f4565e81c5e7543cfdccc18311375b8", "title": "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context"}, {"paperId": "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b", "title": "What Algorithms can Transformers Learn? A Study in Length Generalization"}, {"paperId": "9bb3deca32af8d632e0d916c587cca6c185a6576", "title": "Uncovering mesa-optimization algorithms in Transformers"}, {"paperId": "cbec8bf16a459b0ae38856f604a6a14cd1343477", "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention"}, {"paperId": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8", "title": "Trained Transformers Learn Linear Models In-Context"}, {"paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3", "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"}, {"paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2", "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"}, {"paperId": "4487bdcf1eb42bdec83709ba0df5b32dcf388976", "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"}, {"paperId": "ad4b365630f1c13d74d78f0f5d8cee87ef356d41", "title": "Fine-Tuning Language Models with Just Forward Passes"}, {"paperId": "eefbd8b384a58f464827b19e30a6920ba976def9", "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability"}, {"paperId": "70ece7b4ba8f3b67f5a797daed544fb6a0b627bf", "title": "A Latent Space Theory for Emergent Abilities in Large Language Models"}, {"paperId": "b63e97330154acece935ffa6901e3f36518e5703", "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study"}, {"paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f", "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction"}, {"paperId": "da3aca9d7b50da823f669c983edeb60445720fe0", "title": "The Learnability of In-Context Learning"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "5969eff0e72e4a5bc0c7392c700be74a01ac2822", "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations"}, {"paperId": "f680d47a51a0e470fcb228bf0110c026535ead1b", "title": "Progress measures for grokking via mechanistic interpretability"}, {"paperId": "a5cc5edcabba4c9c62cfbc3379daa140084a2a24", "title": "Tracr: Compiled Transformers as a Laboratory for Interpretability"}, {"paperId": "69c85405cc1986a41f6387d869aa1648a5668d6f", "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "37ba9c33025fb31f25436010e12c65a0bafc0e1f", "title": "Meta-Learning Fast Weight Language Models"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "6aec7f40412ceaeb811d71345d9888fca7a24f82", "title": "How to Fine-Tune Vision Models with SGD"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "6edd112383ad494f5f2eba72b6f4ffae122ce61f", "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"}, {"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "title": "Transformers Learn Shortcuts to Automata"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737", "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "0e9ac2cfc5a3ecb66eeace720901390f7809ba0a", "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers"}, {"paperId": "0735fb79bf34698c1df4461a05ed51c232c412e4", "title": "Thinking Like Transformers"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "77366bef01df1ab277149b330336a0ef9c5041c4", "title": "Transformer"}, {"paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea", "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9d81bc8bebf1beb936427c224afb219b54a64f1e", "title": "Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right"}, {"paperId": "01400290c7db96c4d665d1c29519c42ba47401e0", "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks"}, {"paperId": "10c86505de83647c7b4157595ab10f64e97c94ef", "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"}, {"paperId": "ea8c46e193d5121e440daf96edfd15a47151c293", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "cd63025532a62fa245a02ec05e32ac4d23089631", "title": "Dynamic Evaluation of Transformer Language Models"}, {"paperId": "3694381e74445a8b9f8cb8d373e39626e47191b5", "title": "On the Turing Completeness of Modern Neural Network Architectures"}, {"paperId": "c6f913e4baa7f2c85363c0625c87003ad3b3a14c", "title": "Scalable agent alignment via reward modeling: a research direction"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "e86f71ca2948d17b003a5f068db1ecb2b77827f7", "title": "Concrete Problems in AI Safety"}, {"paperId": "1f61e15e0076a4439c98232ab679680dea0d1372", "title": "Equilibrium Propagation: Bridging the Gap between Energy-Based Models and Backpropagation"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"}, {"paperId": "cdcf7cb29f37ac0546961ea8a076075b9cc1f992", "title": "Mining and summarizing customer reviews"}, {"paperId": "167e1359943b96b9e92ee73db1df69a1f65d731d", "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts"}, {"paperId": null, "title": "Looped transformers as programmable computers, 2023"}, {"paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725", "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "62877098e34d5783960ac02ac8b76dbe729ea174", "title": "In-Context Learning of Large Language Models Explained as Kernel Regression"}, {"paperId": "4328bd97ce29e5a5def69eeb1e172d1aacf3676a", "title": "Efficient Training of Language Models using Few-Shot Learning"}, {"paperId": "ab4467bd55ddfe7b575aad37df11720ec93965d6", "title": "Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning"}, {"paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922", "title": "Attention is Turing-Complete"}, {"paperId": "2ff74d426e712522030057624510c03713fa77ba", "title": "Linear Transformers Are Secretly Fast Weight Memory Systems"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "1cff7cc15555c38607016aaba24059e76b160adb", "title": "Annotating Expressions of Opinions and Emotions in Language"}, {"paperId": null, "title": "Section 4 describes how T IN T uses first-order approximations and stop gradients to compute the simulated gradient of the auxiliary model"}, {"paperId": null, "title": "Backward Pass : Backpropagation to compute the gradient of the auxiliary model \u03b8 aux ( f ( \u03be ; \u03b8 aux ))"}, {"paperId": null, "title": "First order gradients ( 4 \u00d7 )"}, {"paperId": null, "title": "Linear attention : We use linear attention modules to perform the forward, backward, and gradient operations for an auxiliary model linear layer."}, {"paperId": null, "title": "The final linear backward operation can be performed by using a T IN T Linear back-propagation module (Appendix C) with the embeddings e (1) t and the prefix embeddings"}, {"paperId": null, "title": "The forward-forward algorithm: Some prelimi-nary"}]}