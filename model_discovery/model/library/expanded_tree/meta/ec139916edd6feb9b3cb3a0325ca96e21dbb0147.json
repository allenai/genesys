{"paperId": "ec139916edd6feb9b3cb3a0325ca96e21dbb0147", "abstract": "While transformers have begun to dominate many tasks in vision, applying them to large images is still computationally difficult. A large reason for this is that self-attention scales quadratically with the number of tokens, which in turn, scales quadratically with the image size. On larger images (e.g., 1080p), over 60% of the total computation in the network is spent solely on creating and applying attention matrices. We take a step toward solving this issue by introducing Hydra Attention, an extremely efficient attention operation for Vision Transformers (ViTs). Paradoxically, this efficiency comes from taking multi-head attention to its extreme: by using as many attention heads as there are features, Hydra Attention is computationally linear in both tokens and features with no hidden constants, making it significantly faster than standard self-attention in an off-the-shelf ViT-B/16 by a factor of the token count. Moreover, Hydra Attention retains high accuracy on ImageNet and, in some cases, actually improves it.", "venue": "ECCV Workshops", "year": 2022, "citationCount": 47, "influentialCitationCount": 4, "openAccessPdf": {"url": "http://arxiv.org/pdf/2209.07484", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces Hydra Attention, an extremely efficient attention operation for Vision Transformers (ViTs), which is computationally linear in both tokens and features with no hidden constants, making it significantly faster than standard self-attention in an off-the-shelf ViT-B/16 by a factor of the token count."}, "embedding": {"model": "specter_v2", "vector": [0.33281442523002625, 0.9754633903503418, -0.01480959914624691, 0.22740380465984344, -0.11020104587078094, 0.3297940492630005, 0.85548335313797, -0.2857418358325958, -0.40265318751335144, -0.6141753792762756, 0.5392298698425293, 0.5998387932777405, 0.6063706874847412, -0.1461213380098343, -0.3977578580379486, -0.22296762466430664, -1.0983806848526, 0.02318485826253891, 0.4356826841831207, -0.2628398537635803, 0.24864912033081055, -0.8469532132148743, -1.5497974157333374, 0.03467104211449623, 0.15900598466396332, 1.06049644947052, 0.5209502577781677, 1.0347511768341064, -0.12042231112718582, 0.9363921284675598, 0.17747223377227783, -0.22309361398220062, 0.3405381739139557, 0.02085873857140541, -0.7340812087059021, -0.08497396111488342, 0.9229008555412292, -0.3173103332519531, -0.4126048982143402, 1.0065945386886597, 0.058951426297426224, 0.05407002568244934, 0.26008373498916626, -0.7394102215766907, -0.3371162414550781, 0.6776645183563232, 0.23454304039478302, 0.7433133125305176, -0.4474736154079437, -0.23821303248405457, 1.427237629890442, -1.1842947006225586, 0.3427261412143707, 1.6686655282974243, 0.2733217775821686, 0.17738473415374756, 0.18839889764785767, -0.3931967616081238, 0.7646167874336243, 0.8895213603973389, -0.6780939698219299, -0.4650832414627075, 0.06098612770438194, 0.12360372394323349, 1.905070424079895, -0.6751779317855835, 0.2846502363681793, 0.3414122462272644, 0.25621873140335083, 1.6341465711593628, 0.14293083548545837, -0.8027504086494446, 0.038842182606458664, -0.01739167980849743, 0.11115782707929611, 0.8087846040725708, -0.535515546798706, -0.05814754217863083, -1.0832008123397827, 0.26366549730300903, 0.39892521500587463, -0.11013130098581314, 0.3703393042087555, -0.48609301447868347, -0.47771549224853516, 0.990051805973053, 0.7532035112380981, 0.5610620975494385, -0.5402207970619202, 0.9347430467605591, 0.5265476703643799, 0.05136159434914589, -0.32253772020339966, 0.3014075756072998, 0.27823561429977417, 0.7338306903839111, -0.6306446194648743, 0.20414263010025024, -0.10064796358346939, 1.1319853067398071, 0.17576897144317627, 0.4902762770652771, -0.5426596403121948, -0.15202230215072632, 1.159924864768982, 0.3552118241786957, 0.4504109025001526, -0.6822458505630493, -0.09377683699131012, -0.8981959819793701, -0.258102685213089, -1.0001063346862793, 0.1930864006280899, -0.27523136138916016, -0.9733471870422363, -0.3754570186138153, -0.19550195336341858, 0.43922969698905945, -0.8970851898193359, 0.433199018239975, 0.05907529219985008, 0.2614819407463074, -0.20407278835773468, 0.5739526152610779, 0.1806504726409912, 0.40698152780532837, 0.3651067614555359, 0.4727177619934082, 1.563852071762085, -0.9328275918960571, -0.6426020860671997, -1.0616812705993652, -0.3062296509742737, -0.2433694452047348, 0.1390000879764557, 0.03026876226067543, -1.304425597190857, -1.1189217567443848, -0.6911839842796326, -0.22926375269889832, -0.5639925599098206, -0.0549720823764801, 1.180403470993042, -0.06350927799940109, -1.2840840816497803, 0.5664025545120239, -0.04845571890473366, -0.20769725739955902, 0.6129259467124939, 0.39255207777023315, 0.28011709451675415, -0.051552146673202515, -1.1201555728912354, 0.4876060485839844, -0.0877193734049797, -0.248726025223732, -0.3561546206474304, -0.34514495730400085, -1.3146799802780151, 0.20462758839130402, 0.2987710237503052, -0.5923517942428589, 1.17698073387146, -0.20137618482112885, -0.9485350847244263, 0.7011319994926453, -0.5830535888671875, -0.17944969236850739, 0.08333855122327805, -0.27939966320991516, 0.007711721584200859, -0.1060754656791687, -0.14341528713703156, 0.9473908543586731, 1.2302217483520508, 0.1163749024271965, -0.38464221358299255, -0.24629899859428406, -0.48274505138397217, -0.0842154249548912, -0.283244788646698, 0.9789474606513977, -0.6683704257011414, 0.010209259577095509, 0.21734334528446198, 0.47753557562828064, 0.030549604445695877, -0.3183091878890991, -0.44644996523857117, -1.1753648519515991, 0.9114058017730713, 0.4304186701774597, 0.18335071206092834, -0.8133730888366699, -0.6596858501434326, -0.16816715896129608, 0.02432570792734623, -0.21547728776931763, -0.7422705888748169, 0.1967838853597641, -0.25862225890159607, 0.0065590813755989075, 0.16227176785469055, -0.9512165784835815, 0.3055087924003601, -0.26276782155036926, -0.6072080135345459, 0.11091629415750504, 0.28746163845062256, 0.9236123561859131, -0.6430907845497131, -0.6337730884552002, 0.2661176025867462, 0.41154876351356506, -1.095055103302002, 1.1385397911071777, -0.12025861442089081, -0.15178366005420685, -0.022460302338004112, -0.10074939578771591, 0.06643150746822357, -0.3524917960166931, 0.34304648637771606, -0.5053317546844482, -0.15611352026462555, 0.3328688144683838, -0.4427846670150757, 1.0509010553359985, -0.18983004987239838, 0.804474413394928, 0.14611248672008514, -0.5037172436714172, -0.05147460848093033, 0.11373462527990341, -0.41142362356185913, -0.6238065361976624, 0.4341994524002075, -0.3094019591808319, -0.6158163547515869, 0.07193976640701294, 0.6718443036079407, 1.4123990535736084, -0.20884810388088226, -0.2637633681297302, 0.8791612982749939, -0.4839502274990082, -0.08996149897575378, 0.5592712163925171, 0.5174079537391663, 0.28782007098197937, 0.48946264386177063, -0.08245716989040375, -0.005684335250407457, -0.8143380284309387, 0.010336101055145264, 0.8055624961853027, 0.3070019483566284, 0.9608659148216248, 0.3173066973686218, -1.037400245666504, -0.43892037868499756, 0.2715397775173187, 0.5728840827941895, 1.5238741636276245, -0.18788988888263702, 0.14028549194335938, -0.9044678211212158, 0.03575295954942703, -0.32934147119522095, -0.4746173620223999, -0.6991081833839417, 0.027424747124314308, -0.3286283314228058, -0.7328842878341675, 0.642726719379425, 0.34497466683387756, 1.3567638397216797, -1.1499862670898438, -0.9763602018356323, -0.39840465784072876, 0.5623956918716431, -1.0145010948181152, -0.7086008191108704, 0.2904784381389618, -0.13932326436042786, -0.07263027876615524, -0.11159197241067886, -0.5324772000312805, 0.09417974203824997, -0.4760136604309082, 0.7279594540596008, -0.5089282393455505, -0.6708796620368958, -0.03451038897037506, 0.7086970210075378, -1.1385961771011353, -0.07660435140132904, 0.13967376947402954, 0.13069400191307068, 0.16728150844573975, 0.03594633564352989, 0.31278330087661743, -0.32385024428367615, -0.2251676768064499, -0.14569123089313507, -0.3165886700153351, 0.4265526235103607, -0.1419951468706131, 0.673348069190979, -0.45828551054000854, -0.1248944103717804, -1.0832539796829224, 0.22840707004070282, 0.29794153571128845, -0.4866519868373871, -0.26766833662986755, -0.7211329936981201, -0.322759747505188, 0.19954955577850342, -0.3553124666213989, -0.00954206008464098, -0.07543622702360153, 0.32296305894851685, -0.5308154821395874, -0.5754291415214539, -0.4506624639034271, 0.2314041554927826, -0.2799034118652344, 0.24076618254184723, 0.48832616209983826, 0.3285498321056366, 0.36717137694358826, 0.6254290342330933, -1.0458627939224243, 1.1072255373001099, 0.49609023332595825, 0.2431941032409668, 0.17786367237567902, 0.094925157725811, -1.0489755868911743, -0.4046615660190582, -0.5899001955986023, -0.24581226706504822, -0.4621284306049347, 0.6650659441947937, -0.6443922519683838, -1.276479721069336, 0.4688040018081665, -1.1394436359405518, -0.34174808859825134, 0.011885818094015121, -0.09440204501152039, -0.5687541961669922, -0.824651837348938, -0.8787072896957397, -0.5505813360214233, -0.8259130716323853, -0.9911730289459229, 0.36368611454963684, 0.41130951046943665, -0.3942648768424988, -0.29717203974723816, -0.21662816405296326, -0.5538100004196167, 1.2617782354354858, -0.4773489832878113, 0.16916826367378235, 0.20787759125232697, -0.555054247379303, -0.15834614634513855, -0.008124792017042637, -0.017892232164740562, -0.31354257464408875, 0.03860345110297203, -1.3110812902450562, 0.47578224539756775, -0.4164309501647949, -0.5453203916549683, 0.5219890475273132, 0.5193530321121216, 0.646423876285553, -0.2436441332101822, -0.019924327731132507, 0.39454740285873413, 1.3454440832138062, -0.9108996391296387, 0.558904230594635, 0.39328891038894653, 0.932971715927124, 0.2308322787284851, -0.44662296772003174, 0.8273158669471741, 0.9063987135887146, 0.20971043407917023, 1.046688199043274, -0.5394507050514221, -0.4041788876056671, -0.3867151141166687, 0.3847883939743042, 0.9841805696487427, -0.02093436010181904, 0.21235302090644836, -0.8062427043914795, 1.0933289527893066, -0.8959183096885681, -1.1550577878952026, 0.5725294351577759, 0.5122638940811157, 0.08097686618566513, -0.5288031697273254, -0.2726984918117523, -0.3504118025302887, 0.7484617233276367, 0.5225163698196411, -0.6461009979248047, -0.6954501271247864, -0.23851516842842102, 0.7029624581336975, 0.41338634490966797, 0.6748126149177551, -0.5564793348312378, 0.716759443283081, 14.957645416259766, 0.30706918239593506, -0.2075653374195099, 0.3424817621707916, 0.34782248735427856, 0.5524892807006836, -0.07125107198953629, 0.07153096050024033, -0.9637410640716553, -0.3823249638080597, 0.6526966094970703, 0.5948050022125244, 0.5063822269439697, 0.47427377104759216, -0.09369434416294098, 0.08262014389038086, -0.27882665395736694, 0.6667158007621765, 0.8676245212554932, -1.3248556852340698, 0.07143813371658325, 0.31656235456466675, 0.22427405416965485, 0.6816083788871765, 0.8482457399368286, 0.6599686741828918, 0.8047586679458618, -0.05697932466864586, 0.6589660048484802, 0.1961982697248459, 0.8254432082176208, 0.0883186012506485, 0.22631287574768066, 0.10250580310821533, -0.9578048586845398, -0.1368585079908371, -0.6732478737831116, -1.1635812520980835, -0.0493154413998127, 0.015548452734947205, -0.5715650916099548, -0.7018018364906311, 0.4934834837913513, 0.3888207674026489, -0.22142337262630463, 0.45422273874282837, -0.27294212579727173, 0.23989492654800415, 0.022077960893511772, -0.17323023080825806, 0.44061315059661865, 0.9596250653266907, -0.11446186155080795, 0.09917471557855606, -0.1845267117023468, 0.08927582949399948, 0.007279693614691496, 0.5327285528182983, -0.2828979194164276, -0.36923912167549133, -0.28770679235458374, 0.30027949810028076, -0.004533643368631601, 1.1681857109069824, 0.4302808344364166, 0.062451932579278946, -0.040987566113471985, 0.3719711899757385, 0.4198615849018097, -0.15410958230495453, -0.17397919297218323, -0.6050924062728882, 0.37642595171928406, -0.5846351385116577, 0.5760875344276428, 0.6828324794769287, -0.14246545732021332, -0.48087337613105774, -0.907795786857605, -0.26734548807144165, 0.5078160762786865, -1.0185667276382446, -0.4059493839740753, 0.80588299036026, -0.5054731965065002, -0.016310477629303932, 0.2295212298631668, -0.5882171392440796, -0.8360053896903992, 0.3235083520412445, -1.7039130926132202, -0.9123945832252502, -0.13379724323749542, 0.17757894098758698, -0.09924650937318802, 0.21379627287387848, 0.9651101231575012, -0.22428099811077118, -0.07313019782304764, 0.031875044107437134, -0.7273126840591431, -0.033929359167814255, 0.05346561223268509, -1.0971099138259888, 1.0718554258346558, 0.4663753807544708, -0.367560476064682, 0.08437086641788483, 0.27526867389678955, 0.6454454064369202, -1.0969736576080322, 0.058263279497623444, 0.7299229502677917, -0.6721287965774536, -0.3795623779296875, -0.5350268483161926, -0.8123824000358582, 0.3519245386123657, 1.248708724975586, 0.2253170907497406, -0.21960042417049408, 0.0708221048116684, -0.6655266284942627, -0.21777693927288055, -0.6745314598083496, -0.06294751167297363, -0.07462941855192184, -0.7714958786964417, -0.3998985290527344, -0.3184153139591217, 0.04104076698422432, -0.8660963773727417, -0.29816174507141113, -0.591816782951355, 0.3384670615196228, -0.32425105571746826, 0.9885774254798889, -0.1891690045595169, 0.31416812539100647, 0.6551160216331482, 0.07537281513214111, -0.5650292038917542, -0.33554279804229736, -1.0171879529953003, 0.04730646312236786, 0.23031149804592133, 0.27382761240005493, -0.4570169448852539, 0.27438727021217346, 0.5554948449134827, 0.2749382257461548, -0.017754200845956802, -0.5406120419502258, -0.14494073390960693, -0.2774249017238617, -0.3979320824146271, 0.29951030015945435, 0.18842265009880066, 0.16813324391841888, 0.13513590395450592, 0.3220853805541992, 0.726410448551178, 0.48745712637901306, -0.6694024205207825, 0.5928375124931335, -0.06618262082338333, -0.07438939809799194, -0.45579835772514343, -0.8819410800933838, -0.9917428493499756, -0.11488926410675049, -0.8926004767417908, 0.23131099343299866, -1.0384979248046875, -0.39378446340560913, 0.3363613188266754, -0.10704872757196426, 0.4957711696624756, 0.12269233167171478, -0.06268478184938431, 0.08569512516260147, -0.5973882675170898, -0.7318500280380249, 0.5664383769035339, 0.8281536102294922, -0.8424872756004333, 0.3232927620410919, -0.3631531298160553, -0.2204996645450592, 0.6265997886657715, 0.3780441880226135, -0.2602793276309967, -0.29812267422676086, -1.1347575187683105, 0.18224623799324036, -0.24234235286712646, 0.18080540001392365, -1.1519060134887695, 1.1549038887023926, 0.5716131925582886, 0.2174651175737381, -0.10123708099126816, 0.3773837387561798, -0.5932657718658447, -0.9252262711524963, 0.4340370297431946, -0.6862742304801941, 0.14564724266529083, -0.006052522454410791, -0.43015825748443604, -0.25237634778022766, 1.065717339515686, 0.18247342109680176, -1.4172002077102661, -1.3541066646575928, 0.3682270348072052, -0.6594367623329163, 0.28142067790031433, -0.34894031286239624, -0.33139118552207947, -1.3165701627731323, -0.4603058695793152, 0.05543029308319092, 0.29372328519821167, -0.5899344682693481, 0.8090209364891052, 0.5642735958099365, -0.6256823539733887, 0.09091456979513168, 0.6355558037757874, -0.17882642149925232, 0.22422392666339874, 0.7265064716339111, 0.4141189754009247, -0.2045208364725113, 0.27818363904953003, -0.2841741144657135, -0.04627768695354462, -0.524268627166748, 0.28009355068206787, 0.4818134903907776, -0.08639313280582428, -0.2807219326496124, 0.9624541401863098, -0.1370692253112793, -0.7126141786575317, 0.2265976369380951, -1.2232298851013184, -0.739222526550293, 0.06454357504844666, 0.7986523509025574, 0.08999863266944885, -0.09864795207977295, -0.3123915195465088, -0.6542848348617554, 0.6265770792961121, -0.0753694698214531, -0.44370442628860474, 0.3672654628753662, -0.14591838419437408, -0.3025064766407013, 0.1251341700553894, 0.6390907764434814, -0.6153319478034973, -0.911925196647644, -0.8237100839614868, -0.6362254619598389, 0.09263487160205841, 0.38675224781036377, -0.01195529941469431, -0.7316203713417053, 1.0987781286239624, 0.6159696578979492, 0.39989274740219116, 0.17758838832378387, 0.08597535640001297, 0.1349768489599228, 0.7356438040733337, -0.0736660286784172, -0.5409626364707947, -0.27218711376190186, 1.3122272491455078, 1.062546968460083, -0.6755754351615906, 0.15059387683868408, -0.10116972774267197, -1.006012201309204, 0.4397130012512207, 0.47811055183410645, -0.42046552896499634, 0.5120493769645691, -0.21353715658187866, 0.20586863160133362, -0.011982439085841179, -0.9707215428352356, -0.6852210760116577, 0.6036711931228638, 1.3322911262512207, 0.17022626101970673, 0.07436545938253403, 0.7582194805145264, 0.14225348830223083, -0.15046977996826172, -0.10062606632709503, 0.6211509108543396, 0.19113901257514954, -0.4209069013595581, 0.5744608044624329, -0.3029141128063202, 0.22112153470516205, -0.866921067237854, -0.4405497610569, 0.1525414139032364, 0.6765252947807312, 0.21109457314014435, 0.4944659471511841, 0.9466001987457275, 0.2851227819919586, 0.6418343186378479, -0.12585662305355072, 0.5398473739624023, -0.28453949093818665, -0.18168196082115173, -0.04379252716898918, -1.0443300008773804, -0.3854963183403015, -0.41591694951057434, -0.9234995245933533, -0.3465648293495178, -0.1682654470205307, 0.10627834498882294, -0.3522794842720032, 0.49742379784584045, 0.8046291470527649, 0.6877421140670776, 0.7144160866737366, -0.14228472113609314, -1.0320695638656616, -0.229341521859169, -0.8631422519683838, 0.1504180133342743, -0.5862547755241394, 0.19089245796203613, -0.11186905205249786, -0.3055804371833801, 0.04718759283423424]}, "authors": [{"authorId": "93728539", "name": "Daniel Bolya"}, {"authorId": "2084646762", "name": "Cheng-Yang Fu"}, {"authorId": "4527324", "name": "Xiaoliang Dai"}, {"authorId": "2918780", "name": "Peizhao Zhang"}, {"authorId": "50196944", "name": "Judy Hoffman"}], "references": [{"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "004b97aea43f9f62cc49dec20f449abfbae28811", "title": "Masked Autoencoders As Spatiotemporal Learners"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a09cbcaac305884f043810afc4fa4053099b5970", "title": "Exploring Plain Vision Transformer Backbones for Object Detection"}, {"paperId": "4990f7542f0600e0501a7e7a931b32eb7cb804d5", "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"}, {"paperId": "f6b770ed3ed2d41f37668318b6ca4a04432c5b62", "title": "ViT-P: Rethinking Data-efficient Vision Transformers from Locality"}, {"paperId": "72e81bc41ffae1d414836169107910025aaacb75", "title": "Lite Vision Transformer with Enhanced Self-Attention"}, {"paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748", "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "3bda4938e11faf19e63d8755888bdae4561a2144", "title": "Poly-NL: Linear Complexity Non-local Layers With 3rd Order Polynomials"}, {"paperId": "649b706ba282de4eb5a161137f80eb49ed84a0a8", "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax"}, {"paperId": "a66686e60a3eda0c606e036403cf0a07a5962595", "title": "Mobile-Former: Bridging MobileNet and Transformer"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "76a9f336481b39515d6cea2920696f11fb686451", "title": "Quantifying Attention Flow in Transformers"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "5582bebed97947a41e3ddd9bd1f284b73f1648c2", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"}, {"paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "0d8be19e00af83388523baf86f8cdf682302a0d1", "title": "SPViT: Enabling Faster Vision Transformers via Soft Token Pruning"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}]}