{"paperId": "2f125c28e9769e3074b453cc0371edec7ef65b5c", "abstract": "State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter count and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks. We consider three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from the training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We first demonstrate they can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called \\textit{self-guided training}, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Experiments on the large RefinedWeb dataset show that our methods are both efficient and effective for training and inference. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Further applying self-guided training to the structured matrices with 32\\% FFN parameters and 2.5$\\times$ speed-up enables only a 0.4 perplexity increase under the same training FLOPs. Finally, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance. Our code is available at \\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks and considers three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices."}, "embedding": {"model": "specter_v2", "vector": [0.38394850492477417, 1.0118924379348755, -0.5979657173156738, -0.46899452805519104, -0.2782154083251953, -0.05772670730948448, 0.7568821907043457, -0.45690199732780457, -0.4720180332660675, -0.37113797664642334, 0.808358371257782, -0.2782962918281555, 0.2729269862174988, 0.3802827298641205, -0.14460831880569458, 0.20693042874336243, -1.0211377143859863, 0.3231232166290283, -0.34231430292129517, -0.2544703781604767, -0.41712936758995056, -0.4382766783237457, -0.5634289383888245, 0.14202643930912018, 0.30393826961517334, 0.6023977398872375, 0.508751392364502, 0.7727198600769043, -0.5049784779548645, 0.5667908787727356, 0.6225981116294861, -0.6122450828552246, 0.3596833050251007, -0.0795542523264885, -0.2739042639732361, 0.0008822388481348753, 0.46613186597824097, -0.6334652304649353, -0.6856667995452881, 0.900008499622345, -0.31415897607803345, 0.15709997713565826, 0.1695067286491394, -0.5449398756027222, -0.2818887531757355, 0.9569045901298523, 0.7402453422546387, 0.611690104007721, -0.5143240690231323, -0.4007761478424072, 1.4911046028137207, -1.669822096824646, 0.08415587991476059, 1.827364206314087, 0.6101323962211609, 0.23558472096920013, -0.22619986534118652, -0.8124396800994873, 0.6542162895202637, -0.17189155519008636, -0.7186668515205383, -0.7128844857215881, -0.26042309403419495, 0.06130904704332352, 1.8123828172683716, -0.6848155856132507, 0.05233731120824814, 0.704325795173645, -0.36067819595336914, 1.4919521808624268, 0.006776412017643452, -0.6702503561973572, -0.5742879509925842, -0.0042030601762235165, 0.23544557392597198, 1.0038071870803833, -0.33077868819236755, 0.23154139518737793, -0.8592767119407654, -0.05486655235290527, 0.4700826406478882, 0.015043608844280243, -0.07714151591062546, -0.10637829452753067, -0.20925939083099365, 0.9756767153739929, 0.5951542854309082, 0.8473842144012451, -0.10214962065219879, 0.7029825448989868, 0.48729372024536133, 0.44433674216270447, 0.2882556617259979, 0.14506708085536957, -0.3822017312049866, 0.531093180179596, -1.2379388809204102, -0.10445723682641983, -0.025602661073207855, 0.7962420582771301, -0.05691871792078018, 0.3578891456127167, -0.42690393328666687, 0.1833513230085373, 1.5446226596832275, 0.22893470525741577, 0.258228063583374, -0.6095210909843445, 0.48773133754730225, -0.7792729735374451, -0.20603908598423004, -0.6191169619560242, -0.47627341747283936, -0.6895345449447632, -1.1577303409576416, -1.4629343748092651, -0.5168184638023376, 0.290366530418396, -0.6770135760307312, 0.7281565070152283, -0.4685123860836029, 0.20941728353500366, -0.0012212847359478474, 0.39848488569259644, 0.5110993385314941, 0.6844437718391418, 0.20290791988372803, 0.23520493507385254, 0.6839139461517334, -1.1713634729385376, -0.853776752948761, -1.3627285957336426, 0.6783709526062012, -0.17444364726543427, 0.4598062038421631, -0.09768880903720856, -1.0204581022262573, -0.9469183683395386, -0.7645132541656494, -0.08675030618906021, -0.2422901839017868, 0.7681305408477783, 1.131920576095581, 0.42439723014831543, -1.1567424535751343, 0.8976125121116638, -0.20558515191078186, -0.1726546436548233, 0.6416731476783752, 0.36615243554115295, 0.09550707042217255, -0.49900272488594055, -1.142242431640625, 0.38704895973205566, 0.24216729402542114, -0.11411061137914658, -0.16002872586250305, -0.45999833941459656, -0.8854448795318604, 0.17841660976409912, 0.10815010219812393, -0.4725629985332489, 1.0940862894058228, -0.31463858485221863, -1.2210485935211182, 0.6861510872840881, -0.29105132818222046, -0.36450451612472534, 0.022705990821123123, -0.44982415437698364, -0.3565581142902374, -0.14197394251823425, -0.38075074553489685, 0.586487889289856, 0.5828876495361328, 0.055252838879823685, -0.03655436262488365, 0.17585395276546478, -0.5329886674880981, -0.16129279136657715, -0.28154993057250977, 1.0880099534988403, -0.6461315155029297, -0.27219158411026, 0.21096983551979065, 0.6121622323989868, -0.1900133639574051, -0.19067005813121796, -0.49549606442451477, -0.9870665669441223, 0.3816487491130829, -0.290283203125, 1.0437803268432617, -0.8624452352523804, -0.750294029712677, 0.0605425201356411, 0.3275650441646576, -0.003115020925179124, -0.9161933660507202, 0.6296287775039673, -0.33962348103523254, 0.27467435598373413, 0.1533220261335373, -1.3749818801879883, 0.2385842353105545, -0.2857092022895813, -0.810150146484375, 0.06343227624893188, 0.12548710405826569, 1.0678237676620483, -0.7917147278785706, 0.09001153707504272, -0.1280178725719452, 0.4097979962825775, -1.1007016897201538, 1.3271483182907104, -0.10455573350191116, -0.014072408899664879, 0.12038185447454453, -0.5063286423683167, 0.09675145894289017, -0.776530385017395, 0.6495440006256104, -0.5542649030685425, 0.08114772289991379, 0.6511184573173523, -0.2022465318441391, 1.3317015171051025, -0.2922855615615845, 0.927974283695221, 0.15815424919128418, -0.6361911296844482, 0.09113597869873047, 0.19842374324798584, -0.39225175976753235, -0.3849238455295563, 0.33725419640541077, 0.3356676399707794, -0.5035714507102966, 0.6430032253265381, 0.3284594714641571, 0.8083738684654236, -0.07025287300348282, 0.19216012954711914, 0.8676781058311462, 0.0336446538567543, 0.417935311794281, 0.756733238697052, 0.6636192798614502, 0.1569744199514389, 0.3030966520309448, -0.28097936511039734, 0.20494137704372406, -1.1452618837356567, -0.36684858798980713, 0.3813673257827759, 0.848973274230957, 0.691592276096344, 0.4777599573135376, -0.730455756187439, -0.5574809908866882, -0.10302766412496567, 0.5884508490562439, 1.3386796712875366, -0.7199029326438904, -0.34739983081817627, -0.6470609903335571, 0.054218921810388565, -0.31695830821990967, -0.14051631093025208, -0.22956044971942902, 0.07030002772808075, -0.4557334780693054, -0.9878888130187988, 0.8844330906867981, 0.07868023216724396, 0.9883303642272949, -0.050793811678886414, -0.03532872721552849, -0.4379465579986572, 0.23012775182724, -1.118045687675476, -0.6516245603561401, 0.40610161423683167, -0.7558997869491577, -0.10002429038286209, -0.05450708046555519, -0.03319055214524269, 0.21181201934814453, -0.7168336510658264, 0.9396639466285706, -0.5674543380737305, 0.061027176678180695, -0.010092257522046566, 0.6884609460830688, -0.32251518964767456, -0.6213789582252502, 0.2894652783870697, 0.25721076130867004, 0.04000759869813919, 0.08092484623193741, 0.5072789788246155, -0.06080296263098717, -0.16586332023143768, -0.15098746120929718, 0.4667518138885498, 0.30474185943603516, 0.09865517914295197, 0.33473148941993713, -0.254220575094223, 0.0021085646003484726, -1.4042997360229492, 0.7651678323745728, 0.27592039108276367, -0.43926307559013367, 0.16268153488636017, -0.6272233128547668, -0.1461794674396515, 0.5052536725997925, -0.5129082202911377, -0.23490606248378754, -0.7958239912986755, 0.07778101414442062, -0.6133556365966797, -0.02512052282691002, 0.022817295044660568, 0.24273689091205597, 0.2109849601984024, 0.25053897500038147, 0.3461977541446686, 0.27135249972343445, -0.40628325939178467, 0.601784884929657, -0.8095605969429016, 0.5892536044120789, 0.48382043838500977, 0.5046313405036926, -0.15568077564239502, -0.24368110299110413, -0.585803747177124, -0.8167324662208557, -0.4427706003189087, -0.2566492259502411, -0.2141665816307068, 0.03610100597143173, -0.9453533291816711, -0.6213515996932983, -0.08802458643913269, -0.8910125494003296, -0.3397982716560364, 0.34437817335128784, 0.025351092219352722, -0.09168315678834915, -1.1286052465438843, -1.6672371625900269, -0.5640211701393127, -0.8326553106307983, -0.9338356256484985, 0.32479995489120483, -0.033400628715753555, -0.15988722443580627, -0.7291077375411987, -0.26387864351272583, -0.3838125467300415, 1.2252522706985474, -0.8112457990646362, 0.9093005061149597, -0.18792825937271118, -0.2468225061893463, -0.027934987097978592, -0.2706312835216522, 0.6761342287063599, -0.5926085710525513, 0.14435496926307678, -0.9304490089416504, 0.10522504895925522, -0.5810897946357727, -0.2758477032184601, 0.04772944748401642, 0.4208128750324249, 0.29393288493156433, -0.20038887858390808, -0.45862624049186707, 0.8241013884544373, 1.3146498203277588, -1.0424050092697144, 0.04650284722447395, 0.06467387825250626, 1.0470731258392334, 0.1300303339958191, -0.5409919619560242, 0.5779677033424377, 0.39775484800338745, 0.040766436606645584, 0.1513909548521042, -0.31936272978782654, -0.28908681869506836, -0.8718432188034058, 0.4767610728740692, 1.8805334568023682, 0.43689534068107605, -0.004598348867148161, -1.0637153387069702, 0.3203670382499695, -0.877799928188324, -0.6310979723930359, 0.5082736611366272, 0.6104161739349365, 0.3575283885002136, -0.25855955481529236, -0.4812968969345093, -0.24714255332946777, 0.31480467319488525, 0.4928146302700043, -0.278441458940506, -0.4777328372001648, -0.17664189636707306, 0.5492209196090698, 0.39446234703063965, 0.5217426419258118, -0.30867594480514526, 0.802995502948761, 14.872940063476562, 0.8604392409324646, 0.10153283178806305, 0.7489652037620544, 0.9618395566940308, 0.03326556459069252, -0.34731966257095337, -0.11992684006690979, -1.4110215902328491, -0.2457241714000702, 1.432722806930542, 0.37630531191825867, 1.0141898393630981, -0.05830218270421028, 0.14131803810596466, 0.4554694592952728, -0.4704299569129944, 0.947180986404419, 0.37665316462516785, -1.3596069812774658, 0.4229913055896759, 0.1791144162416458, 0.21263693273067474, 0.8356345891952515, 0.609790563583374, 0.8422034382820129, 0.6074221730232239, -0.597549319267273, 0.44751185178756714, 0.2953152060508728, 1.1535322666168213, 0.012578600086271763, 0.2539314031600952, 0.5837684273719788, -0.9254553914070129, -0.24584531784057617, -0.7092776298522949, -1.1151447296142578, 0.07880263775587082, 0.45490995049476624, -0.37714746594429016, -0.5645182728767395, -0.03067546896636486, 1.042771339416504, -0.08430912345647812, 0.25838902592658997, -0.16652841866016388, 0.8857765197753906, -0.4269729256629944, 0.24049177765846252, 0.595018208026886, 0.29230108857154846, -0.09733755141496658, 0.17288219928741455, 0.087227001786232, -0.3151988983154297, 0.011608465574681759, 0.48583731055259705, -0.7343723773956299, -0.009316769428551197, -0.1346011906862259, -0.5507329702377319, 0.01242063008248806, 0.8361298441886902, 0.3628329336643219, 0.29797789454460144, -0.4970730245113373, 0.2945343554019928, 0.9703803658485413, 0.3170557916164398, -0.07898557186126709, 0.036778803914785385, 0.3270238935947418, -0.5750974416732788, 0.062002312391996384, 0.37956467270851135, -0.010449465364217758, -0.9322745203971863, -0.8913902640342712, -0.5143876671791077, 0.12186266481876373, -0.8912394046783447, -0.596591055393219, 1.0685073137283325, -0.27080589532852173, -0.24128787219524384, 0.2375739961862564, -0.8668897747993469, 0.12533356249332428, 0.4813258945941925, -1.4719775915145874, -0.6759116053581238, 0.48729971051216125, -0.013515200465917587, -0.33241239190101624, -0.14715830981731415, 0.9018464684486389, 0.4613465368747711, -0.39281412959098816, 0.23791846632957458, 0.16589851677417755, 0.17333504557609558, -0.36369526386260986, -0.4724997282028198, 1.0612415075302124, 0.3252424895763397, 0.09876588732004166, 0.520022451877594, -0.10826793313026428, 0.5000333786010742, -0.9167336821556091, 0.12455231696367264, 0.9388864040374756, -0.581829309463501, 0.027932289987802505, -1.0750136375427246, -0.3023281991481781, 0.4451843500137329, 0.5740697979927063, -0.2312351018190384, 0.18068726360797882, 0.12783284485340118, -0.7044202089309692, -0.08378656953573227, -0.46399444341659546, -0.07231820374727249, 0.6971104145050049, -0.8770701885223389, 0.004625769332051277, 0.14581720530986786, 0.223483145236969, -0.6461796164512634, -0.43202000856399536, -0.22968672215938568, -0.06247464567422867, -0.30821987986564636, 0.9264030456542969, -0.2787992060184479, 0.5223519206047058, 1.0993337631225586, -0.22666609287261963, -0.8694347739219666, -0.01898256689310074, -1.0725315809249878, -0.0859193354845047, 0.11300191283226013, 0.3816152811050415, -0.5397341251373291, 0.34008461236953735, 0.4803289771080017, 0.3099939525127411, -0.6194664835929871, -0.5837395191192627, -0.2566521465778351, -0.23465551435947418, -0.5721760392189026, 0.06852471828460693, -0.19694189727306366, -0.32109594345092773, 0.4261724650859833, 0.34175771474838257, 0.39965230226516724, 0.04264943301677704, -0.8760274052619934, 0.23254336416721344, -0.16789889335632324, -0.18338871002197266, -0.4451064467430115, -0.3401735723018646, -1.8877904415130615, 0.1808784455060959, -1.2182821035385132, -0.2285047024488449, -1.1174497604370117, -0.3478647470474243, -0.05768115073442459, 0.08286333084106445, 0.08373752981424332, 0.4943094849586487, -0.21520310640335083, -0.04544566571712494, -0.2991844415664673, -0.23040904104709625, 0.844905436038971, 0.8974851965904236, -0.5724983215332031, 0.5741657018661499, 0.03418431803584099, 0.3414614200592041, 0.49839410185813904, 0.20666851103305817, -0.23316800594329834, -0.9710008502006531, -1.4904528856277466, 0.6611661911010742, -0.143281951546669, -0.2214772254228592, -0.6986755132675171, 0.6222407221794128, 0.7056928873062134, -0.20285822451114655, 0.11487776041030884, 0.2786433696746826, -0.9591735601425171, -0.5326073169708252, 0.3078153133392334, -0.7000580430030823, 0.29025766253471375, 0.0959564745426178, -0.5539574027061462, -0.33197474479675293, 0.6411253213882446, 0.0919102281332016, -0.9547603130340576, -0.48190397024154663, 0.48648324608802795, -0.46066027879714966, 0.21401186287403107, -0.7108525037765503, 0.0053540486842393875, -0.9170863628387451, -0.5877149105072021, -0.00039771816227585077, 0.16954901814460754, -0.5128539204597473, 0.7311369180679321, 0.3738551139831543, -1.083487629890442, 0.05634990707039833, 0.4774986505508423, -0.3822595179080963, -0.18192021548748016, 0.1916482001543045, 0.3483918607234955, -0.4095696806907654, 0.7832741141319275, 0.3735797703266144, 0.32772859930992126, -0.5754767060279846, -0.015477875247597694, 0.8707719445228577, -0.7923438549041748, -0.2445441037416458, 1.2067538499832153, -0.14718587696552277, -1.4217500686645508, 0.43820032477378845, -1.2393403053283691, -0.23258152604103088, -0.33221492171287537, 0.3597533106803894, -0.06187499314546585, -0.11016231775283813, -0.11850125342607498, -0.30654773116111755, 0.12868152558803558, 0.18666154146194458, -0.29745933413505554, 0.6695706844329834, -0.38018283247947693, -0.3768295645713806, 0.49415940046310425, 1.3403295278549194, -0.6418207883834839, -0.7346407175064087, -1.0239827632904053, -0.3576032519340515, 0.045675404369831085, 0.5785664319992065, -0.22355639934539795, -0.8194781541824341, 0.9898271560668945, 0.4031064510345459, 0.08142068237066269, 0.32261359691619873, -0.20845037698745728, 0.15967483818531036, 0.7804993391036987, 0.2551623582839966, -0.41645264625549316, -0.5663979649543762, 1.4776859283447266, 0.9573169350624084, -0.7095975875854492, 0.3085695207118988, -0.23779623210430145, -0.49151310324668884, 0.9104434847831726, -0.040060315281152725, -0.17756770551204681, 1.0416820049285889, 0.15084393322467804, -0.14154677093029022, 0.12330461293458939, -1.1369622945785522, -0.35831549763679504, 0.9797438979148865, 0.5444343090057373, 0.7744318246841431, -0.056562040001153946, 0.24391070008277893, 0.865444540977478, -0.0043886746279895306, 0.0494106262922287, 0.003368375590071082, 0.14622993767261505, -0.23056213557720184, 0.2664279043674469, 0.12913334369659424, 0.7954766154289246, -0.6661222577095032, -1.1220920085906982, 0.33082151412963867, 0.4970771074295044, -0.04646780714392662, 0.509556770324707, 0.8580019474029541, 0.09353573620319366, 0.47871294617652893, 0.07193221151828766, 0.5932332277297974, -0.3409886658191681, -0.4185125529766083, -0.03590342029929161, -0.49344971776008606, -0.22995033860206604, -0.20338723063468933, -0.34361913800239563, -0.3494139313697815, -0.2961261570453644, 0.32202649116516113, -0.3048081696033478, 0.32692182064056396, 1.0911825895309448, 0.4166831374168396, 0.2879278361797333, -0.2260323017835617, -0.7171198129653931, -0.58193039894104, -1.014045000076294, -0.0334283672273159, -0.5216456651687622, -0.13296383619308472, 0.13571909070014954, -0.29226410388946533, -0.28586167097091675]}, "authors": [{"authorId": "2308483270", "name": "Xiuying Wei"}, {"authorId": "2277120112", "name": "Skander Moalla"}, {"authorId": "1996134", "name": "Razvan Pascanu"}, {"authorId": "2288101023", "name": "Caglar Gulcehre"}], "references": [{"paperId": "c1fa6255cc9fc3128f74befc7855e255bc7a2c6e", "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"}, {"paperId": "d53fe76bd2795a19ddf52d012917782f6f6f2c1e", "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models"}, {"paperId": "5c7a21e9262b62f0a27fefdc8b1270dfdcbd3912", "title": "The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "4d76206515d6b33903937474273885476fc2771e", "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "d603cf09499a953db33f072aecfd35c9f373a03f", "title": "Fast Feedforward Networks"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d", "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "f93f2476972228de142fde13913bccbec76859b8", "title": "Initialization and Regularization of Factorized Neural Layers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "71afb6daa77d488f2b2e123ba9c56b9fe5cfe8c5", "title": "TRP: Trained Rank Pruning for Efficient Deep Neural Networks"}, {"paperId": "8237d315a7453ed1d3dcc8d377f67dd23c191623", "title": "Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "217a85f667778567d7ffc8b56060783caf5803b0", "title": "Implicit Regularization in Deep Matrix Factorization"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89", "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "d5b4721c8188269b120d3d06149a04435753e755", "title": "Convolutional neural networks with low-rank regularization"}, {"paperId": "54c3e878bf0ff2fdde16e439b5579ee99ee0d0d8", "title": "ACDC: A Structured Efficient Linear Layer"}, {"paperId": "5934400081d9541339da0f16d2613263f1a4c2a2", "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "99c970348b8f70ce23d6641e201904ea49266b6e", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"}, {"paperId": "eff61216e0136886e1158625b1e5a88ed1a7cbce", "title": "Predicting Parameters in Deep Learning"}, {"paperId": "092217c2267f6e0673590aa151d811e579ff7760", "title": "Roofline: an insightful visual performance model for multicore architectures"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "9552ac39a57daacf3d75865a268935b5a0df9bbb", "title": "Neural networks and principal component analysis: Learning from examples without local minima"}, {"paperId": null, "title": ": Open models based on gemini research and technology"}]}