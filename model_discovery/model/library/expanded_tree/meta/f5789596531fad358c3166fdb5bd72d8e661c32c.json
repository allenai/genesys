{"paperId": "f5789596531fad358c3166fdb5bd72d8e661c32c", "abstract": "Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to which other known optimizer and model interventions influence the sensitivity of the final loss to changes in the learning rate. To this end, we study methods such as warm-up, weight decay, and the $\\mu$Param (Yang et al., 2022), and combine techniques to train small models that achieve similar losses across orders of magnitude of learning rate variation. Finally, to conclude our exploration we study two cases where instabilities can be predicted before they emerge by examining the scaling behavior of model activation and gradient norms.", "venue": "arXiv.org", "year": 2023, "citationCount": 31, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.14322", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work investigates the extent to which other known optimizer and model interventions influence the sensitivity of the final loss to changes in the learning rate, and studies two cases where instabilities can be predicted before they emerge by examining the scaling behavior of model activation and gradient norms."}, "embedding": {"model": "specter_v2", "vector": [-0.1991046816110611, 1.1616158485412598, -0.15482912957668304, 0.41694706678390503, -0.3967059254646301, -0.19371825456619263, 0.7969317436218262, -0.2698224186897278, 0.2197112739086151, -0.30168914794921875, 0.10358528792858124, -0.1625380963087082, 0.4292120933532715, -0.04431127384305, -0.3900565207004547, -0.34173843264579773, -1.079707145690918, 0.41053274273872375, 0.0070240069180727005, -0.014348689466714859, -0.5281476974487305, -0.2591902017593384, -0.8751125931739807, 0.01987379975616932, 0.5428641438484192, 1.3437201976776123, -0.2773926854133606, 0.4451422393321991, -0.3216967284679413, 0.45448410511016846, 0.4847968518733978, -0.23451940715312958, 0.37200888991355896, -0.04642144590616226, 0.040786243975162506, 0.3650694191455841, 0.7301429510116577, 0.036302417516708374, -0.263562947511673, 0.6093653440475464, -0.19485166668891907, -0.07639389485120773, 0.08780074119567871, -0.8695960640907288, -0.32304057478904724, 0.622575044631958, 0.2715299725532532, 0.7458997964859009, -0.6293351054191589, -0.116836316883564, 0.6794252991676331, -0.8354581594467163, -0.46459123492240906, 1.5504366159439087, 0.973163366317749, -0.09074877947568893, -0.5286106467247009, -0.9188043475151062, 0.6195505857467651, -0.03418790549039841, -0.8981035351753235, -0.04160270839929581, -0.08651575446128845, -0.4640985429286957, 1.4010004997253418, -0.741892397403717, 0.06292669475078583, 0.5754746198654175, 0.0564546138048172, 0.9836152195930481, 0.40448448061943054, -0.6137390732765198, -0.20044897496700287, 0.8655885457992554, 0.1284254640340805, 0.9360125660896301, -0.49872586131095886, 0.5416145324707031, -0.9012076258659363, -0.18015754222869873, 0.42757564783096313, -0.3805442452430725, 0.19027042388916016, -0.19459693133831024, 0.42487138509750366, 0.7029097676277161, 0.6772266626358032, 0.6561646461486816, -0.6625208854675293, 0.9282858371734619, 0.22743704915046692, 0.5542006492614746, 0.3150082528591156, -0.22013522684574127, -0.13812695443630219, 0.4296033978462219, -0.6475223898887634, -0.34327173233032227, 0.005184494890272617, 0.7183893918991089, 0.16547973453998566, 0.7600502967834473, -0.29650405049324036, 0.15724444389343262, 1.1188898086547852, -0.10153138637542725, 0.8948622345924377, -0.4930446445941925, 0.4510788023471832, -0.34620535373687744, -0.03404214233160019, -0.43845388293266296, -0.40707746148109436, -0.5639610290527344, -1.004094123840332, -0.774237871170044, -0.3581271469593048, 0.21298636496067047, -0.6569439768791199, 0.8194159865379333, -0.31588461995124817, 0.06495649367570877, -0.27513960003852844, 0.4282907247543335, 0.014715109020471573, 0.47746750712394714, 0.390421599149704, 0.3670802116394043, 0.4744069278240204, -0.7802192568778992, -0.23325695097446442, -1.0599945783615112, 0.3077780306339264, -0.04263889417052269, 0.6494288444519043, -0.19739529490470886, -1.3725043535232544, -0.8511225581169128, -0.6414109468460083, 0.968336820602417, -0.4252445101737976, -0.1536005586385727, 1.0934395790100098, 0.528362512588501, -0.9896266460418701, 0.6619637608528137, -0.42030611634254456, -0.4436997175216675, 0.6712412238121033, 0.47485455870628357, 0.48703843355178833, 0.09952542930841446, -1.0680502653121948, 0.7931766510009766, 0.0190106388181448, -0.17311523854732513, -0.16508543491363525, -0.8269418478012085, -0.6925702691078186, 0.230679452419281, 0.028162961825728416, -0.3881973922252655, 1.1781553030014038, -0.720582902431488, -1.1210815906524658, 0.24836499989032745, 0.36335310339927673, 0.39408206939697266, 0.7644005417823792, 0.5603655576705933, -0.43917304277420044, -0.4450448453426361, -0.23448620736598969, 0.2368428260087967, 0.519432783126831, -0.16465771198272705, 0.019809575751423836, 0.13492099940776825, -0.4291909635066986, -0.507405161857605, -0.469028115272522, 0.6966145038604736, 0.06767150014638901, 0.05476782098412514, 0.13876000046730042, 0.3602014482021332, -0.0951235294342041, -0.2998656630516052, -0.11434324085712433, -1.0330250263214111, 0.3622676432132721, -0.07310394197702408, 0.977152407169342, -0.7546634674072266, -0.8621472716331482, 0.2784687280654907, -0.23036959767341614, -0.4844858944416046, -0.44069814682006836, 0.14398011565208435, -0.8870336413383484, 0.6632673740386963, -0.024010421708226204, -0.8925980925559998, 0.25262370705604553, -0.2445395439863205, -0.6199579238891602, -0.0252764243632555, 0.23543918132781982, 1.114960789680481, -0.5569737553596497, 0.2079477459192276, -0.025165287777781487, 0.3055596351623535, -0.9509535431861877, 1.1183886528015137, 0.0721697062253952, 0.05457649007439613, 0.16894960403442383, 0.2931973934173584, 0.14499281346797943, -0.8277745246887207, 0.4440756142139435, -0.5810583829879761, 0.10252538323402405, 0.21353723108768463, -0.429984450340271, 0.7444793581962585, -0.5529674291610718, 0.410986065864563, 0.17160850763320923, -1.0775856971740723, -0.3283282220363617, 0.3762078285217285, 0.06155247613787651, -0.406223326921463, 0.43262195587158203, 0.5718733072280884, -0.8092445135116577, 0.5514053106307983, 0.2871914803981781, 0.36537081003189087, -0.43272605538368225, 0.06910006701946259, 1.0566104650497437, -0.2829480767250061, 0.04977329820394516, 0.12885817885398865, 0.5220770835876465, 0.1368855983018875, -0.04983608424663544, -0.19605883955955505, -0.23046362400054932, -1.3323465585708618, -0.058317702263593674, 0.32785969972610474, 0.4456392526626587, 0.6782481670379639, 0.8307332396507263, -0.5922921895980835, -0.3648206889629364, -0.6131018996238708, 0.4731957018375397, 1.1754183769226074, -0.662057101726532, 0.05156411975622177, -0.38583263754844666, 0.07743574678897858, -0.20963788032531738, -0.2551915943622589, -0.5819237232208252, -0.33182740211486816, -0.597105085849762, -1.424206256866455, 0.7758392095565796, 0.18554900586605072, 0.792945921421051, -0.5294594168663025, 0.031263552606105804, -0.12442836165428162, 0.5629615783691406, -0.7737663984298706, -0.1972559243440628, 0.5688926577568054, -0.7762410640716553, -0.1888723522424698, 0.03829152137041092, -0.21685302257537842, 0.20911812782287598, -1.275291085243225, 1.0637487173080444, -0.6481617093086243, -0.21809956431388855, 0.41180533170700073, 1.007401943206787, -0.5669542551040649, -0.6197375655174255, 0.11045241355895996, 0.4585117995738983, -0.2019176036119461, -0.20112042129039764, -0.1714126020669937, 0.07520677894353867, 0.06987509876489639, -0.4288184940814972, -0.403160959482193, 0.2871050238609314, 0.07886771112680435, 0.46115192770957947, -0.09592889249324799, 0.39724674820899963, -0.7515504956245422, 1.1269229650497437, -0.2643098533153534, -0.9422057867050171, 0.554541289806366, -1.195905089378357, -0.21623878180980682, 0.7440971732139587, -0.7445201277732849, 0.15175877511501312, -1.123895287513733, 0.5850483179092407, -0.8884211182594299, -0.02363697998225689, -0.05256686732172966, 0.5258236527442932, -0.4526651203632355, 0.30734243988990784, -0.030514998361468315, -0.137037456035614, -0.23063434660434723, 0.3337993025779724, -0.8435378670692444, 0.22812123596668243, 0.1280178427696228, 0.5163580179214478, -0.3538872301578522, -0.090979665517807, -0.44748857617378235, -0.5160645246505737, 0.23890969157218933, 0.0055906339548528194, -0.3040432929992676, -0.11606965214014053, -0.8557872176170349, -0.8334843516349792, 0.11999262124300003, -0.20557446777820587, -0.8385632634162903, -0.31588616967201233, -0.49665358662605286, -0.23078875243663788, -1.3613526821136475, -1.5216331481933594, -0.5082470774650574, -0.671907901763916, -0.9932066798210144, 0.20444945991039276, 0.5215983986854553, -0.30864420533180237, -0.6755471229553223, -0.2929423451423645, -0.5619288086891174, 1.2495925426483154, -0.39169350266456604, 0.5759833455085754, 0.3528773784637451, -0.5812786817550659, 0.10051476210355759, 0.35061293840408325, 0.5486842393875122, -0.1254439800977707, 0.14580991864204407, -1.1408677101135254, 0.1941891610622406, -0.3680536448955536, -0.671071469783783, -0.22453700006008148, 0.3042737543582916, 0.6686309576034546, -0.3235519826412201, 0.19059930741786957, 0.5988854169845581, 1.138907790184021, -1.0053753852844238, -0.11949928849935532, 0.4856018126010895, 0.96484375, 0.37284010648727417, -0.7965393662452698, 0.5238938331604004, 0.1030183658003807, 0.38340169191360474, -0.0022228688467293978, -0.2293425053358078, -0.08556365966796875, -0.8783281445503235, 0.6109392642974854, 1.6398507356643677, 0.5912948250770569, 0.44550636410713196, -0.9222696423530579, 0.36266517639160156, -0.892677366733551, -0.5474880337715149, 1.0225380659103394, 0.6464654207229614, 0.2436496466398239, -0.10538952797651291, -0.2183523327112198, -0.23543588817119598, 0.20264220237731934, 0.14060206711292267, -0.48943081498146057, -0.6051299571990967, -0.05604791268706322, 0.6687170267105103, 0.6626546382904053, 0.47000816464424133, -0.09535599499940872, 0.530637264251709, 15.212023735046387, 0.6721259951591492, -0.030010104179382324, 0.7455161809921265, 0.9074019193649292, 0.3128456771373749, -0.2858985960483551, -0.39629778265953064, -0.6507982015609741, 0.16345849633216858, 1.6712839603424072, 0.3761848211288452, 1.0623263120651245, 0.03335907310247421, 0.04802139103412628, 0.26277440786361694, -0.2374342978000641, 0.8574195504188538, 0.2871476411819458, -1.0776573419570923, 0.2112494856119156, -0.12664993107318878, 0.7995009422302246, 0.9899693131446838, 0.9535407423973083, 0.8582342267036438, 0.676321804523468, -0.11703123897314072, 0.7832381725311279, 0.13285696506500244, 0.8868387341499329, -0.328117698431015, 0.2052721381187439, 0.16105568408966064, -1.109737515449524, -0.6302316188812256, -0.07615087181329727, -0.7386141419410706, 0.3952489495277405, 0.007664738688617945, -0.4645976126194, -0.617096483707428, 0.3769879937171936, 0.17401616275310516, -0.6484140157699585, 0.3882546126842499, -0.5864711999893188, 0.9060690999031067, -0.7420627474784851, 0.7767868638038635, 0.2651057243347168, 0.18661555647850037, -0.19819502532482147, -0.4565311074256897, -0.0023660236038267612, -0.4817117154598236, 0.2777737081050873, 0.7518264651298523, -0.8193135857582092, -0.40370529890060425, -0.0011503577698022127, -0.4015681743621826, -0.14834272861480713, 0.8623849153518677, 0.22036905586719513, 0.1456928551197052, -0.216571643948555, 0.2214115858078003, 0.8078721165657043, 0.6048315763473511, -0.3409992754459381, 0.19411465525627136, 0.47539591789245605, -0.23996180295944214, -0.28963664174079895, 0.7520421147346497, -0.16969312727451324, -0.815824568271637, -0.5165097713470459, -0.4862903356552124, 0.17710120975971222, -1.0958343744277954, -0.9205921292304993, 0.7008014917373657, -0.5174344182014465, -0.06851249188184738, 0.25593721866607666, -0.4499324858188629, -0.36965465545654297, 0.5399205088615417, -1.4529799222946167, -0.9809831976890564, 0.12937238812446594, -0.1686047464609146, -0.42044198513031006, -0.31583088636398315, 0.8528765439987183, 0.396516889333725, -0.7045262455940247, 0.4397103786468506, 0.08992829918861389, -0.3319814205169678, -0.09098108112812042, -0.7703297138214111, 0.7812013626098633, 0.17691902816295624, -0.1403937190771103, -0.159232497215271, 0.1924905925989151, 0.6884254813194275, -0.32463422417640686, 0.1915218085050583, 0.031184403225779533, -0.8598757982254028, 0.5006183385848999, -0.7417935132980347, -0.7313095331192017, 0.5251994132995605, 0.12992773950099945, -0.030655549839138985, 0.06061156094074249, 0.04700003191828728, -0.41801682114601135, -0.4810531437397003, -0.6959443688392639, -0.27805095911026, 0.06443267315626144, -1.402862787246704, -0.1338663399219513, 0.13719400763511658, 0.2001868486404419, -0.6857836842536926, -0.7530413866043091, -0.07889540493488312, -0.10041738301515579, -0.2698265314102173, 0.8621514439582825, -0.3112797737121582, 0.015933191403746605, 0.8089579939842224, 0.04010458663105965, -0.8694491982460022, -0.05606621503829956, -0.8965686559677124, -0.1702580749988556, 0.3908305764198303, 0.3462158739566803, -0.7892168760299683, 0.4886527955532074, 1.024727702140808, 0.05684705451130867, -0.2374282330274582, -0.7157666683197021, -0.1980389505624771, 0.09471294283866882, -0.34693071246147156, 0.16363763809204102, -0.008320238441228867, 0.029309161007404327, -0.02786012925207615, 0.6611462235450745, 0.8283980488777161, 0.2203812599182129, -0.6832413673400879, -0.27123549580574036, -0.157799631357193, -0.14642223715782166, -0.7207196354866028, -0.6197712421417236, -0.829302191734314, 0.11683376878499985, -1.0918880701065063, -0.08090958744287491, -0.579571008682251, -0.7777470350265503, -0.3557308614253998, -0.5452910661697388, 0.05681544542312622, 0.4969191551208496, -0.20607195794582367, -0.259309858083725, -0.0575895681977272, -0.46462705731391907, 0.9042866826057434, 0.17654423415660858, -0.7730115056037903, -0.12881425023078918, 0.010320757515728474, 0.032593611627817154, 0.6034404635429382, 0.4612225592136383, -0.4366113245487213, -0.5195165276527405, -1.035040259361267, 0.3442825973033905, -0.6059761047363281, -0.34191712737083435, -0.939487099647522, 0.5074878931045532, 0.49562472105026245, 0.11371119320392609, 0.5332630276679993, 0.0068106818944215775, -1.0864251852035522, -0.20970751345157623, 0.03349001705646515, -0.26769721508026123, 0.5490936040878296, 0.5211182832717896, -0.5455056428909302, -0.15242093801498413, 1.122469425201416, 0.3516957759857178, -0.6567845344543457, -0.38671085238456726, 0.46322548389434814, -0.8383737206459045, 0.13314130902290344, -0.37582769989967346, 0.22466759383678436, -0.7595272660255432, -0.09695342183113098, -0.4084857702255249, 0.3695167005062103, -0.18269909918308258, 1.0281682014465332, -0.35498902201652527, -1.4533803462982178, 0.5588477849960327, 0.5101847648620605, -0.23657935857772827, 0.02081216312944889, 0.04540247470140457, 0.5650844573974609, -0.5462157726287842, 0.1936463564634323, 0.19608387351036072, 0.26809585094451904, -0.28078728914260864, -0.1683524250984192, 1.0806726217269897, -0.5016015768051147, 0.11230114847421646, 1.1957494020462036, 0.04524902626872063, -1.2988314628601074, 0.510615885257721, -1.5370427370071411, -0.3031711280345917, -0.0008006105781532824, 0.18936927616596222, 0.02300269342958927, -0.07759638130664825, 0.21060101687908173, -0.22862473130226135, 0.14584459364414215, 0.6564075350761414, -0.009167998097836971, 0.7203746438026428, -0.5403709411621094, -0.017904292792081833, 0.9887489676475525, 0.6591721773147583, -1.04379141330719, -0.8341595530509949, -0.6053355932235718, 0.013589832000434399, 0.11188653111457825, 0.31823205947875977, -0.34875279664993286, -1.2237112522125244, 0.7963102459907532, 1.3841590881347656, 0.12373863905668259, 0.387593537569046, 0.2890409231185913, -0.44802525639533997, 0.7860813736915588, 0.4098016023635864, -0.911357045173645, -0.4045066833496094, 0.7288983464241028, 1.098273754119873, -1.104013204574585, 0.32298487424850464, 0.14661307632923126, -0.3285442888736725, 0.7657970786094666, 0.6235700845718384, 0.19429345428943634, 1.0476317405700684, -0.20909026265144348, 0.04534865543246269, -0.04359199106693268, -0.994566023349762, -0.011075982823967934, 0.3747197985649109, 0.49336397647857666, 0.49447351694107056, 0.3138202726840973, 0.6200616955757141, 0.7759517431259155, -0.2034057080745697, 0.19363857805728912, 0.2691526412963867, 0.43965134024620056, -0.07467065006494522, -0.05044602230191231, -0.12068387866020203, 0.7535262703895569, -0.6122181415557861, -0.3816353678703308, 0.2125556766986847, 0.23301026225090027, 0.18836002051830292, 0.4463220238685608, 1.0285195112228394, 0.007481610402464867, 0.9128799438476562, 0.09306338429450989, 0.437491774559021, -0.06653904914855957, -0.6928328275680542, -0.05620168522000313, -0.6819586157798767, -0.620155394077301, -0.10651267319917679, -0.22303152084350586, -0.017227405682206154, -0.3960248529911041, -0.1013430655002594, 0.08754251152276993, 0.1115722805261612, 0.5848878026008606, 0.331555038690567, 0.7072606086730957, -0.0013225068105384707, -1.0412098169326782, -0.5762383341789246, -1.0682249069213867, -0.09481922537088394, -0.16507619619369507, -0.1588645577430725, -0.06819909811019897, -0.6877804398536682, -0.6032359004020691]}, "authors": [{"authorId": "2243002854", "name": "Mitchell Wortsman"}, {"authorId": "2258936205", "name": "Peter J. Liu"}, {"authorId": "50819275", "name": "Lechao Xiao"}, {"authorId": "2247694861", "name": "Katie Everett"}, {"authorId": "2247711955", "name": "A. Alemi"}, {"authorId": "1874006", "name": "Ben Adlam"}, {"authorId": "1388383230", "name": "John D. Co-Reyes"}, {"authorId": "3737312", "name": "Izzeddin Gur"}, {"authorId": "50333123", "name": "Abhishek Kumar"}, {"authorId": "39068839", "name": "Roman Novak"}, {"authorId": "143845796", "name": "Jeffrey Pennington"}, {"authorId": "1407546424", "name": "Jascha Narain Sohl-Dickstein"}, {"authorId": "36303818", "name": "Kelvin Xu"}, {"authorId": "49685832", "name": "Jaehoon Lee"}, {"authorId": "2243002880", "name": "Justin Gilmer"}, {"authorId": "2135550613", "name": "Simon Kornblith"}], "references": [{"paperId": "7368c3cdf7cbed194e96dc4da53ed61f185e3d82", "title": "Replacing softmax with ReLU in Vision Transformers"}, {"paperId": "b3c64a046d6cfc0f55a2aebc5176bbab69a7e021", "title": "Stable and low-precision training for large-scale vision-language models"}, {"paperId": "88c94b11bd18161d027a28dd758b58698063e029", "title": "A Theory on Adam Instability in Large-Scale Machine Learning"}, {"paperId": "fe1be27f0f3ad3399ae5aea1e5d3eb06251a64af", "title": "Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks"}, {"paperId": "5c326260baa5e91a6a39c4d87ebe6770ade86816", "title": "Effective Theory of Transformers at Initialization"}, {"paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba", "title": "Sigmoid Loss for Language Image Pre-Training"}, {"paperId": "385c363ea8e450f362d389f401beaeb5b42a0022", "title": "Stabilizing Transformer Training by Preventing Attention Entropy Collapse"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "aed643b9307f511a2928608b7967425c7eb3557e", "title": "DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule"}, {"paperId": "e750cf66d0e7ade8e21da38df3eb2a174b027f48", "title": "Learning-Rate-Free Learning by D-Adaptation"}, {"paperId": "b941c4bea1a71066f6f32275641aea1efc99b21b", "title": "Meta-Principled Family of Hyperparameter Scaling Strategies"}, {"paperId": "f21a88af78583bd7959b121b800eed5c1f7b7b99", "title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "84f6ab620eb7b112e5b2ca64b305970894e679c1", "title": "Adaptive Gradient Methods at the Edge of Stability"}, {"paperId": "ff53cb49cb18e71e622fce7d96692e813630e878", "title": "The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "0b0d7d87c58d41b92d907347b778032be5966f60", "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "7dbf3dfeb78a42389ce4df9d3b78d14986199778", "title": "Gradient Descent"}, {"paperId": "83474f6510e0b985595f936d233321e131966bae", "title": "A Loss Curvature Perspective on Training Instability in Deep Learning"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae", "title": "An Empirical Study of Training Self-Supervised Vision Transformers"}, {"paperId": "026bb8a1066f50ddc8797e1341353603149a8cb8", "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "7ef1210c6807c4dca773b561692a5f9a0c38d7e9", "title": "Neural Network"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "f10a04a77fd1cd719792de374a60f3fd03f6b944", "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "2bf7c350a8280e7c593d46a60127f99b21517121", "title": "On the Variance of the Adaptive Learning Rate and Beyond"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649", "title": "Understanding the difficulty of training deep feedforward neural networks"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for JAX, 2023"}, {"paperId": null, "title": "A random walk model of transformer parameter growth, 2023"}, {"paperId": null, "title": "Hyung Won Chung, Charles Sutton"}, {"paperId": "6c7384845f7d8347a6daf393ce1586e03dca971b", "title": "Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks"}, {"paperId": null, "title": "The DeepMind JAX Ecosystem, 2020. URL http://github.com/ deepmind"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "2db6b8e36bb5a3594b2200b2d4f931268d6279db", "title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "Grain - feeding jax models, 2023"}, {"paperId": null, "title": "contributed to the infrastructure used for experimentation, provided key insight related to parameterization, and contributed to the writing"}, {"paperId": null, "title": "Intriguing properties of transformer training instabilities"}]}