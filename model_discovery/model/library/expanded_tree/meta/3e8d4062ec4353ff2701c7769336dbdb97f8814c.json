{"paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c", "abstract": "Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy - $\\textbf{T}$oken $\\textbf{O}$mission $\\textbf{V}$ia $\\textbf{A}$ttention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only $\\frac{1}{8}$ of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs' most painful computational bottlenecks - the size of their key-value cache. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA", "venue": "arXiv.org", "year": 2024, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work demonstrates that decoder-only transformers can be conceptualized as unbounded multi-state RNNs - an RNN variant with unlimited hidden state size, and shows that transformers can be converted into $\\textit{bounded}$ multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache."}, "embedding": {"model": "specter_v2", "vector": [0.5863322615623474, 0.9197445511817932, -0.788103461265564, -0.04342208430171013, -0.3658204972743988, -0.35777896642684937, 0.6919205784797668, -0.10702525079250336, -0.5583817362785339, -0.04868767783045769, 0.8894098997116089, -0.6229380369186401, 0.5150736570358276, 0.14281228184700012, -0.22798693180084229, 0.2530742287635803, -0.5142071843147278, 0.16909189522266388, 0.10574576258659363, -0.5196886658668518, -0.2074665129184723, -0.5132046937942505, -0.8380632996559143, 0.09084922820329666, 0.31847918033599854, 0.608510434627533, 0.20196431875228882, 0.8659034967422485, -0.4600955545902252, 0.7302656173706055, 0.4998110830783844, -0.26826706528663635, 0.12531247735023499, 0.007071932312101126, -0.32425573468208313, -0.453102171421051, 0.014324546791613102, -0.29105690121650696, -0.5639804601669312, 0.9249623417854309, -0.23697218298912048, 0.19546525180339813, 0.15077029168605804, -0.6011849045753479, -0.40122395753860474, 0.9937568306922913, 0.8490864634513855, 0.6452215313911438, -0.12119564414024353, -0.6233236789703369, 1.5686954259872437, -0.9715091586112976, 0.10849257558584213, 1.3234888315200806, 0.6883512735366821, 0.5487530827522278, -0.18498624861240387, -0.604964017868042, 0.6616330146789551, 0.14252309501171112, -0.4922271966934204, -0.4933231472969055, -0.10768953710794449, 0.6598193049430847, 2.1514482498168945, -0.12594792246818542, 0.2377539873123169, 0.7786852121353149, -0.19350659847259521, 1.3158183097839355, -0.15501725673675537, -0.6903658509254456, -0.4107511341571808, -0.13069121539592743, -0.04900745302438736, 0.7250322699546814, -0.4855594336986542, 0.33276790380477905, -0.7609089612960815, -0.23508216440677643, 0.22484776377677917, 0.43212106823921204, -0.3129303455352783, -0.29585278034210205, -0.1301620751619339, 0.7835724353790283, 0.49358463287353516, 0.805220365524292, -0.2273213267326355, 0.6439406275749207, 0.6239331364631653, 0.37310823798179626, 0.19585520029067993, 0.2713247835636139, -0.4999028742313385, -0.09749138355255127, -1.0641282796859741, 0.10876549035310745, 0.0883375033736229, 0.6404235363006592, -0.4941500723361969, 0.16965511441230774, -0.6318829655647278, 0.13969261944293976, 1.3963971138000488, 0.18884851038455963, 0.6864792108535767, -0.7649171352386475, 0.33557769656181335, -0.7425270080566406, 0.27142786979675293, -0.25248485803604126, -0.19728459417819977, -0.3583274483680725, -0.6335346698760986, -1.4256353378295898, -0.6361260414123535, 0.1943693310022354, -0.7746103405952454, 0.8860225677490234, -0.45046448707580566, 0.29523903131484985, 0.3392212986946106, 0.3465913236141205, 0.5189968347549438, 0.9712035059928894, 0.004029577597975731, -0.30545857548713684, 0.8179413676261902, -0.9662237167358398, -0.917496383190155, -1.0813840627670288, 0.8343908190727234, 0.09147004783153534, 0.25114136934280396, -0.12745916843414307, -1.311196208000183, -0.5449409484863281, -0.7933546900749207, -0.1611950546503067, -0.487331360578537, 0.16564935445785522, 0.6402945518493652, 0.4474072754383087, -1.1664273738861084, 0.8811855316162109, -0.3841921091079712, -0.04131590947508812, 0.31241485476493835, 0.0037834220565855503, 0.21082426607608795, -0.38056695461273193, -1.4142305850982666, 0.49370673298835754, 0.3285379409790039, -0.25350233912467957, -0.1578068882226944, -0.6949618458747864, -1.1974972486495972, 0.4370806813240051, 0.3970092535018921, -0.324142724275589, 1.4964553117752075, 0.3440285921096802, -1.452586054801941, 0.30698323249816895, -0.6437078714370728, -0.24853672087192535, 0.17215849459171295, -0.4029582440853119, -0.4762263894081116, -0.1863141655921936, -0.2557332217693329, 0.3797968327999115, 0.5342547297477722, 0.004868162330240011, -0.20167309045791626, 0.2671380937099457, -0.3963913917541504, -0.14814256131649017, -0.13769739866256714, 0.8954651355743408, -0.3386344015598297, -0.05492670461535454, 0.28429463505744934, 0.6592124700546265, -0.07950696349143982, -0.16927297413349152, -0.7335397005081177, -1.1017636060714722, 0.7356138229370117, 0.09076220542192459, 1.0305484533309937, -0.8717169761657715, -0.6916245818138123, -0.3267103135585785, 0.10364443808794022, 0.08399650454521179, -0.8458802700042725, 0.46558040380477905, -0.6688115000724792, 0.4766184091567993, -0.07859299331903458, -1.2433749437332153, 0.02770065702497959, 0.010007154196500778, -0.9685918688774109, -0.25585445761680603, 0.17295615375041962, 1.1739368438720703, -0.9528015851974487, 0.039059773087501526, 0.13008259236812592, 0.5171330571174622, -0.9026057124137878, 1.149223804473877, -0.48302701115608215, -0.08859270066022873, -0.0849626362323761, -0.33759257197380066, -0.24552229046821594, -0.4950283467769623, 0.5509615540504456, -0.40251439809799194, -0.017653629183769226, 0.7597772479057312, -0.08554026484489441, 1.259948968887329, -0.4863758981227875, 0.4097880721092224, -0.1643555909395218, -0.6848539113998413, 0.0887225940823555, 0.44181010127067566, -0.17936280369758606, -0.4077213704586029, 0.2874754071235657, 0.32559895515441895, -0.6777764558792114, 0.6180248260498047, 0.8513937592506409, 0.7332156300544739, -0.2994781732559204, 0.20738014578819275, 0.47396838665008545, -0.34796762466430664, 0.21385034918785095, 0.6496591567993164, 0.9358710050582886, 0.3732813000679016, 0.46999087929725647, 0.2755676209926605, 0.1720358431339264, -1.0159755945205688, 0.0859764963388443, 0.4008786082267761, 0.549033522605896, 0.7108951210975647, 0.7391128540039062, -0.6644708514213562, -0.5644945502281189, 0.23202559351921082, 0.48321789503097534, 1.3460599184036255, -0.6423754692077637, -0.377794474363327, -0.5378643274307251, -0.40254855155944824, -0.44561436772346497, 0.3505955934524536, -0.34360232949256897, -0.1368676722049713, -0.4021470248699188, -0.9403075575828552, 1.1240772008895874, 0.21310144662857056, 0.7925445437431335, -0.7585286498069763, -0.3824341893196106, -0.4064898192882538, -0.052314046770334244, -1.0333019495010376, -0.5044286251068115, 0.6541153788566589, -0.9089462757110596, -0.3721613883972168, -0.042171623557806015, 0.1682438999414444, 0.07983165979385376, -0.9722000956535339, 0.7416163682937622, -0.5356610417366028, -0.016191046684980392, 0.23616419732570648, 0.5288342237472534, -0.3130863904953003, -0.8820459246635437, 0.37737569212913513, -0.051851581782102585, -0.41214606165885925, 0.18772445619106293, 0.42617154121398926, 0.13031800091266632, -0.2901698052883148, -0.471415251493454, 0.10710042715072632, 0.2547229826450348, -0.008869730867445469, 0.7066795825958252, -0.5165950059890747, -0.050162579864263535, -1.506668210029602, 1.0058623552322388, 0.2590866684913635, -0.4270174205303192, 0.35851624608039856, -0.9191163182258606, 0.06073284149169922, 0.654471218585968, -0.6095802187919617, -0.30866482853889465, -0.7310884594917297, -0.014961665496230125, -0.5903896689414978, -0.1534494161605835, 0.2233632653951645, 0.48353180289268494, 0.7223340272903442, 0.13902811706066132, 0.8829711675643921, 0.4610150456428528, -0.13842982053756714, 0.8802483081817627, -0.7277801036834717, 0.7033719420433044, 0.5755578279495239, 0.09769050776958466, -0.25820064544677734, 0.15310055017471313, -0.6300616264343262, -0.5669159889221191, -0.49698659777641296, -0.01756124198436737, 0.005545916501432657, 0.1252828687429428, -0.5030428767204285, -0.9405363202095032, -0.30354607105255127, -1.155672311782837, -0.3262961208820343, -0.03526260703802109, -0.49692028760910034, -0.25504907965660095, -0.8418416976928711, -1.5075329542160034, -0.731326699256897, -0.8467450737953186, -0.9584062099456787, 0.42396271228790283, 0.021266508847475052, -0.10542502254247665, -0.6804100275039673, 0.05078549683094025, -0.3511577248573303, 1.1322354078292847, -0.7349985241889954, 0.884159505367279, -0.2046412080526352, 0.07488049566745758, -0.2123427540063858, 0.057688627392053604, 0.3975968658924103, -0.47415468096733093, 0.020745528861880302, -0.7168313264846802, 0.26518669724464417, -0.3470735549926758, -0.5167672038078308, 0.3300116956233978, 0.4987342655658722, 0.5419818758964539, -0.1998574584722519, -0.5071011781692505, 0.47102388739585876, 1.3011223077774048, -0.8066388964653015, 0.33424630761146545, 0.055211376398801804, 0.623496949672699, -0.3573882281780243, -0.43317005038261414, 0.5359998345375061, 0.044056858867406845, 0.23541387915611267, 0.5804880261421204, 0.40292081236839294, 0.12815327942371368, -0.785873293876648, 0.7172542810440063, 1.394266128540039, 0.3789489269256592, -0.2260187715291977, -0.7383079528808594, 0.4140666723251343, -1.1415936946868896, -0.8019846081733704, 0.7220032215118408, 0.6584072709083557, 0.6860131621360779, -0.22739768028259277, -0.3437161445617676, -0.021790852770209312, 0.30137932300567627, 0.7850245237350464, -0.5459198355674744, -0.8061451315879822, 0.05705074965953827, 0.4660889506340027, 0.5120534300804138, 0.6702393889427185, 0.019617896527051926, 0.618931233882904, 14.787520408630371, 0.762580931186676, 0.09348542988300323, 0.6757572889328003, 0.494221031665802, -0.21213556826114655, -0.5235090851783752, -0.07663728296756744, -1.4029556512832642, 0.007871994748711586, 1.6170536279678345, 0.5513986349105835, 0.606898307800293, -0.011819020844995975, 0.07701051235198975, 0.3417704105377197, -0.2925789952278137, 0.5251569151878357, 0.46795353293418884, -1.3062782287597656, 0.32925301790237427, 0.04758754000067711, -0.09761898219585419, 0.9478123784065247, 0.8572692275047302, 0.9776156544685364, 0.8946000933647156, -0.5600635409355164, 0.4668613374233246, 0.3976398706436157, 1.481022834777832, 0.05590478330850601, 0.3586294651031494, 0.6578574180603027, -0.8987737894058228, -0.5248134136199951, -0.5901867747306824, -1.007377028465271, 0.4020107090473175, 0.12726250290870667, -0.22499911487102509, -0.6432271599769592, -0.011020444333553314, 0.9376673102378845, 0.21537117660045624, -0.01374276727437973, -0.512790858745575, 0.7640501856803894, -0.5553773045539856, -0.06143347918987274, 0.4693525731563568, 0.4472804069519043, 0.14971444010734558, -0.14090508222579956, 0.09434208273887634, -0.008865569718182087, -0.23645906150341034, 0.6366748809814453, -0.6616270542144775, -0.5012694001197815, -0.3836876451969147, -0.3881829082965851, 0.13433262705802917, 0.6684861183166504, 0.7117486596107483, 0.42007943987846375, -0.25075623393058777, 0.45599573850631714, 0.7092623710632324, 0.07073100656270981, -0.3555026352405548, -0.20075108110904694, 0.1456185281276703, -0.47719281911849976, 0.3227428197860718, 0.8227386474609375, 0.21987931430339813, -0.6112658977508545, -0.7320506572723389, -0.3061967194080353, 0.3213382959365845, -1.185870885848999, -0.18093271553516388, 0.8379999399185181, -0.4094957113265991, -0.15851782262325287, 0.17889337241649628, -0.6572539210319519, -0.11344882100820541, 0.26180675625801086, -1.8222516775131226, -0.7475548982620239, 0.7807103395462036, -0.02025226317346096, -0.38800615072250366, 0.13776998221874237, 1.2481348514556885, 0.014537272043526173, -0.35656264424324036, 0.04613995924592018, 0.2600778043270111, 0.1633460819721222, -0.5931772589683533, -0.7607864141464233, 0.9731492400169373, 0.19861170649528503, 0.08773552626371384, 0.228566512465477, 0.022899074479937553, 0.14486834406852722, -0.724236011505127, -0.016986332833766937, 0.7774373888969421, -0.9355669021606445, -0.24519219994544983, -0.6031314134597778, -0.6862162947654724, 0.7162675261497498, 0.4330842196941376, -0.05954304337501526, 0.11006317287683487, 0.07655911147594452, -0.7787669897079468, 0.23928602039813995, -0.6979701519012451, 0.10429935157299042, 0.6258203387260437, -0.8282216191291809, -0.2910475730895996, -0.1820725053548813, 0.8392478227615356, -1.067147135734558, -0.49613553285598755, -0.25245529413223267, 0.23722688853740692, -0.09635324031114578, 0.8752967715263367, -0.4601665437221527, 0.649876594543457, 1.1391074657440186, -0.09035506099462509, -0.708049476146698, 0.10432566702365875, -0.879831075668335, -0.3407456576824188, -0.046953991055488586, 0.7649266123771667, -0.6789873242378235, 0.534153938293457, 0.671024739742279, 0.20757298171520233, -0.6508234143257141, -1.0982803106307983, -0.26850074529647827, 0.00286107393912971, -0.636047899723053, 0.34460198879241943, -0.31702741980552673, 0.012945078313350677, 0.12338145822286606, 0.3808284401893616, 0.523601770401001, -0.39549219608306885, -1.0319883823394775, -0.24222736060619354, -0.2513619363307953, -0.004702626261860132, -0.34566381573677063, -0.26176217198371887, -1.70542311668396, 0.07927896082401276, -1.1752957105636597, -0.1462784707546234, -1.0493557453155518, -0.45255047082901, 0.04636584594845772, -0.19364534318447113, 0.3584074378013611, 0.7427470684051514, -0.5586709976196289, -0.20717957615852356, -0.4483315050601959, -0.6513754725456238, 0.9080176949501038, 0.7242518067359924, -0.6509280800819397, 0.259295254945755, 0.1981748789548874, 0.1304057538509369, 0.11938048154115677, 0.456864595413208, -0.6242321133613586, -0.7846187949180603, -1.0927786827087402, 0.7833274602890015, 0.2622057795524597, -0.1035347580909729, -0.4686603546142578, 0.6827494502067566, 0.6201992630958557, -0.5828289985656738, -0.11379414796829224, 0.49084895849227905, -1.269067406654358, -0.6148333549499512, 0.6215561032295227, -0.9381069540977478, 0.3749058246612549, 0.2265266329050064, -0.17160359025001526, -0.16170519590377808, 0.5452437996864319, -0.12823911011219025, -0.9586334824562073, -0.3158329427242279, 0.29908403754234314, -0.6928622722625732, 0.04926229640841484, -0.4228397309780121, -0.11053145676851273, -1.2271976470947266, -0.6764534115791321, -0.15579600632190704, 0.12138046324253082, -0.38914260268211365, 0.8365141749382019, 0.5182729363441467, -0.8600537180900574, 0.25080782175064087, 0.39630621671676636, -0.2053782045841217, 0.28856217861175537, 0.2706588804721832, 0.3087727129459381, -0.36945047974586487, 0.761523425579071, 0.5528146624565125, 0.3121508061885834, -0.8193631172180176, 0.0018973934929817915, 0.9640149474143982, -0.8011844754219055, -0.4642468988895416, 0.8270741701126099, -0.9125492572784424, -1.3223049640655518, 0.2667531967163086, -1.7535594701766968, -0.7065636515617371, -0.413181871175766, 0.387779176235199, 0.06512398272752762, -0.19071075320243835, 0.012486784718930721, -0.31164515018463135, 0.12687906622886658, 0.03603152558207512, -0.47766241431236267, 0.6820225715637207, -0.037398889660835266, -0.22620230913162231, 0.4953852891921997, 0.8500147461891174, -0.6928339600563049, -0.709743857383728, -1.0464706420898438, -0.4340994954109192, 0.2365787774324417, 0.6730680465698242, -0.5432915687561035, -0.6608616709709167, 0.7003514170646667, 0.5141581296920776, 0.4302056133747101, 0.24814094603061676, 0.010445152409374714, 0.41493481397628784, 0.8562119603157043, -0.12711462378501892, -0.31130361557006836, -0.6140909194946289, 1.3499345779418945, 1.004894733428955, -0.7832478284835815, 0.5046367049217224, 0.015984538942575455, -0.5642710328102112, 0.7466511130332947, -0.021313084289431572, 0.2307271808385849, 0.891183078289032, 0.03301043435931206, -0.026469752192497253, 0.12040095031261444, -1.3269407749176025, -0.18430311977863312, 0.47969383001327515, 0.9257755279541016, 0.4512234926223755, -0.021459540352225304, 0.3634986877441406, 0.8726804852485657, 0.2491440773010254, 0.31357619166374207, 0.48042264580726624, 0.4093971848487854, -0.27739882469177246, -0.028436994180083275, 0.172843798995018, 1.014702320098877, -1.0001845359802246, -1.0325833559036255, 0.3336927890777588, 0.37011560797691345, -0.11764508485794067, 0.6999108791351318, 0.8704378008842468, 0.1275937706232071, 0.5211250185966492, 0.18602591753005981, 0.20713533461093903, -0.5036472678184509, -0.3065538704395294, -0.3604578971862793, -0.7100056409835815, -0.2589187026023865, -0.013730502687394619, -0.34047701954841614, -0.3308198153972626, -0.3361281156539917, 0.12858512997627258, 0.13286980986595154, 0.2584216296672821, 1.0854475498199463, 0.66885906457901, 0.5028391480445862, -0.4164537191390991, -0.4173177182674408, -0.8125845193862915, -0.9850409030914307, -0.1607040911912918, -0.6389475464820862, -0.004670121241360903, 0.12584111094474792, -0.3527013063430786, -0.6321760416030884]}, "authors": [{"authorId": "2279022034", "name": "Matanel Oren"}, {"authorId": "2141516697", "name": "Michael Hassid"}, {"authorId": "2279021708", "name": "Yossi Adi"}, {"authorId": "2279023325", "name": "Roy Schwartz"}], "references": [{"paperId": "d955c898f968df520218f88c1e14db154caf514d", "title": "Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification"}, {"paperId": "2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2", "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"}, {"paperId": "8b8a7f1ac390a2394802234d3c539da86c56de66", "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"}, {"paperId": "1784c987e681d60c634765fe64c8d9c26f73d5ff", "title": "SnapKV: LLM Knows What You are Looking for Before Generation"}, {"paperId": "7a54aad06171f59149aca5380863c62729c70b41", "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"}, {"paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba", "title": "Yi: Open Foundation Models by 01.AI"}, {"paperId": "f395f022548d1d1f11e231f42d4852dbff5e9376", "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference"}, {"paperId": "f9f311472ed8c4b15cfd74e1539f0c4815e574e9", "title": "SubGen: Token Generation in Sublinear Time and Memory"}, {"paperId": "189fde3f4dfa105bb51472a8945618f395919560", "title": "Repeat After Me: Transformers are Better than State Space Models at Copying"}, {"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "c0aac09fe67c39fbb377e45e03e38cd0d9b24ab4", "title": "Optimizing Retrieval-augmented Reader Models via Token Elimination"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "38d64919ba526868a850a0e5f6239d4c474b7e7e", "title": "Large Language Models are not Fair Evaluators"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "eb511ae6b9f04e4936891d26787f274b48b99d57", "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "2537af99905a27d9b84ba9968715f4287f1d3359", "title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "title": "ABC: Attention with Bounded-memory Control"}, {"paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2", "title": "Primer: Searching for Efficient Transformers for Language Modeling"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "7a27cc0cc37931e85315ed41333f01cb6de18c02", "title": "Differentiable Subset Pruning of Transformer Heads"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "b9ed2fd3237539b0ad539dad8bdee15efbe0a26e", "title": "Single Headed Attention RNN: Stop Thinking With Your Head"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "0cf535110808d33fdf4db3ffa1621dea16e29c0d", "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1fe1c52181e1e70c42f9207340168c23a386311e", "title": "Better"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e9c771197a6564762754e48c1daafb066f449f2e", "title": "Unitary Evolution Recurrent Neural Networks"}, {"paperId": "cfdd423c8672a7b178ea85d56079328df4eea647", "title": "Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time"}, {"paperId": "5c5751d45e298cea054f32b392c12c61027d2fe7", "title": "S2ORC: The Semantic Scholar Open Research Corpus"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Re-nard"}, {"paperId": null, "title": "2022. SQuAL-ITY: Building a long-document summarization dataset the hard way"}, {"paperId": null, "title": "2023. LM-Infinite: Simple on-the-fly length generalization for large language models"}, {"paperId": null, "title": "2022. Di-agonal state spaces are as effective as structured state spaces"}, {"paperId": null, "title": "2022. Efficiently scaling transformer inference"}, {"paperId": null, "title": "2022. Block-recurrent transformers"}, {"paperId": null, "title": "2023. Vicuna: An open-source chatbot impressing GPT-4 with 90%* Chat-GPT quality"}, {"paperId": null, "title": "combine TOVA with additionally fixing the i tokens using i \u2208 { 1 , 4 }"}, {"paperId": null, "title": "C Prompts The prompts used for the different through this work"}, {"paperId": null, "title": "2023. Supervised fine-tuning and direct preference optimization on Intel Gaudi2"}, {"paperId": null, "title": "the crucial tokens. B Formal Description of Method Algorithm 1 provides a torch-like of the TOVA cache compression policy"}, {"paperId": null, "title": "To evaluate the generated texts"}]}