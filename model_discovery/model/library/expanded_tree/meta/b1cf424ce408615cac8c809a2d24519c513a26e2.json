{"paperId": "b1cf424ce408615cac8c809a2d24519c513a26e2", "abstract": "Fine-tuning large-scale pre-trained models is inherently a resource-intensive task. While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes. To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget. Our code is available at https://github.com/MIkumikumi0116/DoRA", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The Dynamic Low-Rank Adaptation (DoRA) method decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget."}, "embedding": {"model": "specter_v2", "vector": [-0.06386768817901611, 0.20473672449588776, -0.39540132880210876, 0.09415597468614578, -0.014598790556192398, 0.0356426015496254, 0.7643671035766602, -0.6990087032318115, -0.5449495911598206, -0.4122472405433655, 0.4003503918647766, 0.29875287413597107, -0.2388933300971985, 0.3233199119567871, -0.15346978604793549, -0.2247282862663269, -1.145495057106018, 0.8100267648696899, -0.009844344109296799, -0.5842060446739197, -0.36892634630203247, -0.2417943775653839, -0.702806293964386, 0.010616972111165524, 0.03767027333378792, 0.7510260939598083, 0.4105181097984314, 0.7451545000076294, -0.08508496731519699, -0.31100916862487793, 0.6284914612770081, -0.05053688958287239, 0.7403865456581116, 0.011678257957100868, -0.04963957890868187, -0.011520305648446083, 0.09025996178388596, -0.558919370174408, -0.3176136314868927, 0.6847065091133118, 0.0770278349518776, 0.34647729992866516, 0.7409693598747253, -0.29268211126327515, 0.05078382417559624, 0.20856060087680817, 0.15715332329273224, 0.8116567730903625, -0.7166576981544495, -0.573448657989502, 0.9808656573295593, -1.4929816722869873, -0.07443853467702866, 1.3146793842315674, 0.6336497068405151, 0.6380155086517334, -0.5450575351715088, -0.8990858793258667, 0.7263725996017456, -0.11308974027633667, -1.0868030786514282, -0.17919263243675232, -0.17562083899974823, -0.546500563621521, 1.523002028465271, -0.9023104906082153, -0.2373802214860916, 0.8195703029632568, 0.16915355622768402, 0.7613078355789185, -0.05609558895230293, -0.3231365978717804, -0.19618313014507294, 0.15800891816616058, -0.03901791572570801, 0.48209068179130554, -0.20263633131980896, 0.350046843290329, -1.0738606452941895, -0.23389019072055817, 0.5469809174537659, -0.31290093064308167, 0.21966950595378876, -0.2979523241519928, -0.290404349565506, 0.619205117225647, 0.31699517369270325, 0.2007213532924652, -0.3015010356903076, 0.7328268885612488, 0.5079203248023987, 0.6722292900085449, 0.3448793888092041, 0.7004501819610596, -0.4118499755859375, 0.6221274733543396, -0.6555191278457642, 0.019798798486590385, 0.04569200798869133, 0.5454616546630859, -0.0010183921549469233, 0.15359345078468323, -0.5281534790992737, 0.7189399600028992, 1.217431902885437, -0.13584543764591217, 0.6872310638427734, -0.4123789370059967, 0.43863916397094727, -0.7547451257705688, 0.13842487335205078, -0.7863947153091431, -0.5491251945495605, -0.6528410315513611, -1.192246913909912, -0.9379726648330688, -0.6386264562606812, 0.14373743534088135, -0.5631934404373169, 0.6966550946235657, -0.1002650260925293, -0.1114489734172821, -0.25350624322891235, 0.944527804851532, 0.061049483716487885, 0.6660434603691101, 0.2580210566520691, 0.2601042091846466, 0.8375471234321594, -0.9316713213920593, -0.7075499892234802, -1.1536918878555298, 0.06326556950807571, -0.4222617745399475, 0.7151552438735962, 0.1788974106311798, -0.9716400504112244, -0.7827506065368652, -1.0895761251449585, 0.2215626835823059, -0.2874417304992676, 0.8490927815437317, 1.2177234888076782, 0.0026370706036686897, -0.35145819187164307, 0.8383404612541199, -0.2899087071418762, -0.058323536068201065, 0.278980553150177, 0.6968276500701904, 0.20077931880950928, -0.12409616261720657, -1.4303561449050903, 0.3854351043701172, 0.5388463139533997, -0.20902474224567413, -0.7208991646766663, -0.6790533065795898, -0.19892293214797974, 0.060727208852767944, 0.28760436177253723, -1.0942810773849487, 1.0434565544128418, -0.11313183605670929, -1.527173638343811, 0.4115062952041626, 0.3418026268482208, 0.12840303778648376, 0.6240255236625671, -0.2602199614048004, -0.6100019216537476, -0.3740234673023224, -0.8371352553367615, 0.5024706125259399, 0.8859402537345886, 0.040314655750989914, -0.007792408112436533, 0.10343887656927109, -0.4118426740169525, 0.36335885524749756, -0.6035744547843933, 0.5586841702461243, -0.964400053024292, -0.41107988357543945, 0.5198136568069458, 0.5144442915916443, 0.04056747257709503, -0.17051216959953308, -0.1700376719236374, -0.5535070896148682, 0.4298994839191437, 0.09579994529485703, 1.0913801193237305, -0.9048988819122314, -0.3369230031967163, 0.26927027106285095, 0.0592319518327713, -0.2197476029396057, -0.9572933316230774, 0.5045703649520874, -0.4465111494064331, 0.17061837017536163, 0.30269482731819153, -1.1491410732269287, 0.019380850717425346, -0.40983957052230835, -0.5951257944107056, 0.16980943083763123, 0.2277728170156479, 0.6241609454154968, -0.6153697967529297, 0.6358966827392578, -0.2599888741970062, 0.6841849088668823, -1.3419952392578125, 1.0402021408081055, -0.16622130572795868, 0.3925120234489441, -0.24715816974639893, -0.2988106310367584, 0.012358232401311398, -0.6881847381591797, 0.5249619483947754, -0.8320780992507935, 0.5811017155647278, 0.38490086793899536, -1.035900354385376, 1.454901933670044, -0.7070336937904358, 0.4504932463169098, 0.3424851894378662, -0.29101207852363586, 0.029998525977134705, 0.19878843426704407, 0.1938771903514862, -0.5176113247871399, 0.36285045742988586, 0.5654643774032593, -0.6076928973197937, 0.6616458296775818, 0.5454099774360657, 0.8537601828575134, -0.38355743885040283, -0.05269837751984596, 0.6310470104217529, -0.15877874195575714, 0.4508925974369049, 0.21964004635810852, 0.3217371106147766, 0.3594007194042206, 0.5088186264038086, -0.24681419134140015, 0.2847660183906555, -1.0437440872192383, -0.14134150743484497, 0.4432930648326874, 0.8797505497932434, 0.5938766598701477, 0.3988967835903168, -0.791856586933136, -0.7252125144004822, -0.3716824948787689, 0.46403300762176514, 1.7257276773452759, -0.2576240599155426, 0.11649800091981888, -0.512164831161499, 0.044267438352108, -0.03784145787358284, -0.7532849907875061, -0.4341963231563568, -0.21935893595218658, -0.5158994197845459, -1.5673842430114746, 0.40431708097457886, -0.309272438287735, 0.8644633889198303, -0.09282274544239044, 0.2664373815059662, -0.12026206403970718, 0.4882412254810333, -0.6271171569824219, -0.8186087012290955, 0.5125586986541748, -0.6554521918296814, -0.21083053946495056, -0.04033290222287178, 0.11157193779945374, 0.14132444560527802, -0.6155478954315186, 1.0114490985870361, -0.8206244111061096, -0.12392716854810715, 0.036225829273462296, 0.6230264902114868, -0.4303348958492279, -0.3099554181098938, 0.831481397151947, 0.6949639916419983, 0.2696521580219269, -0.08077963441610336, 0.005746190901845694, -0.26281851530075073, 0.6221175193786621, -0.4645193815231323, 0.21400384604930878, 0.5173748731613159, 0.21876487135887146, 1.057724118232727, -0.2560156583786011, 0.4753606915473938, -1.6817615032196045, 1.4574387073516846, -0.16160206496715546, -0.7286784648895264, -0.23534263670444489, -1.0032538175582886, -0.11402641236782074, 0.5457438826560974, -1.1330534219741821, -0.3901972472667694, -0.6926328539848328, 0.21062204241752625, -0.6326354146003723, 0.23746849596500397, 0.07973681390285492, 0.4164937734603882, -0.1488114893436432, 0.9670720100402832, -0.13875538110733032, 0.2249828577041626, -0.5179933905601501, 0.5282358527183533, -1.0103641748428345, 0.6656861901283264, 0.1632867008447647, 0.665589451789856, -0.07458264380693436, -0.03934244439005852, -0.24565176665782928, -0.7549537420272827, -0.6431455612182617, -0.1570681631565094, -0.0979791134595871, -0.21174660325050354, -1.2763144969940186, -0.6699709296226501, -0.20148037374019623, -0.13939419388771057, -0.5253036022186279, 0.3162989020347595, 0.13052476942539215, -0.242395281791687, -1.3237226009368896, -1.4753913879394531, -0.011065596714615822, -0.944446325302124, -1.0824178457260132, 0.02216627448797226, -0.10276271402835846, -0.19155903160572052, -0.37882235646247864, -0.272092342376709, -0.6386905312538147, 1.1309127807617188, -0.8406049609184265, 0.7592268586158752, 0.07051847130060196, -0.072614885866642, -0.2675321698188782, 0.00776198273524642, 0.5416726469993591, -0.5259464383125305, -0.2839953601360321, -0.9531995058059692, -0.08671101927757263, -0.5960007905960083, -0.19304053485393524, 0.24612140655517578, 0.37195998430252075, 0.7465320825576782, 0.04306900128722191, -0.3145171105861664, 1.191152811050415, 1.3653863668441772, -1.1937719583511353, -0.13080589473247528, 0.5710068345069885, 0.9964454770088196, 0.1922137290239334, -0.13704761862754822, 0.9587223529815674, -0.04104170575737953, 0.5092796683311462, 0.24256516993045807, -0.10990316420793533, -0.5790729522705078, -0.618226945400238, 0.3932413160800934, 1.6393773555755615, 0.3995935320854187, 0.19474227726459503, -0.5738733410835266, 0.29189765453338623, -0.9532470703125, -0.27883657813072205, 0.704185962677002, 1.0487111806869507, 0.43233034014701843, -0.3249233067035675, -0.1648620367050171, -0.43778547644615173, 0.18443648517131805, 0.3840082287788391, -0.5342836976051331, -0.47111064195632935, 0.282715380191803, -0.2245604544878006, 0.7293574213981628, 0.5082975625991821, -0.3425567150115967, 0.5216866731643677, 14.696944236755371, 0.9677056670188904, 0.23026402294635773, 0.8180734515190125, 0.5222346782684326, -0.2635839581489563, -0.13965456187725067, -0.6529557704925537, -1.3461540937423706, 0.07723221927881241, 1.0864830017089844, 0.4435364007949829, 1.533844232559204, 0.19613702595233917, 0.1978546530008316, 0.33170947432518005, -0.39433106780052185, 0.9238762855529785, 0.2872634530067444, -1.1412689685821533, 0.2082539200782776, 0.11065308004617691, 0.7843939065933228, 0.8112793564796448, 0.8974120020866394, 1.0348697900772095, 0.3650181293487549, -0.11523013561964035, 0.10926670581102371, 0.27992504835128784, 1.1310007572174072, -0.36675316095352173, 0.4159919023513794, 0.4047831594944, -0.9084861278533936, -0.6450185179710388, -0.8959717750549316, -0.861045241355896, 0.07483610510826111, 0.48215940594673157, -0.5319586396217346, -0.6691210269927979, 0.3189963400363922, 0.6307861804962158, -0.12726126611232758, 0.6295547485351562, -0.16156069934368134, 0.42986613512039185, -0.18004797399044037, 0.6393149495124817, 0.039565425366163254, 0.15752603113651276, 0.1294156163930893, -0.08463520556688309, -0.03991732373833656, -0.13271880149841309, 0.059404630213975906, 0.2644737660884857, -0.7781829833984375, -0.21060459315776825, 0.23950070142745972, -0.002756706206128001, 0.17946757376194, 1.0524483919143677, 0.8760195374488831, 0.5655571222305298, -0.18781284987926483, 0.24680794775485992, 0.6076481938362122, 0.5216193199157715, -0.33749592304229736, 0.025841478258371353, 0.7879325747489929, -0.21419073641300201, -0.43179911375045776, 0.28026723861694336, -0.2193901389837265, -0.5547947883605957, -0.7708140015602112, -0.6273095607757568, 0.2290288805961609, -0.6924039125442505, -1.155348777770996, 0.6330252289772034, -0.09472455829381943, -0.6936392188072205, 0.36149293184280396, -0.6912897825241089, 0.20205183327198029, 0.7394077777862549, -1.5253968238830566, -0.220799520611763, 0.37032651901245117, -0.3330455720424652, -0.38494905829429626, -0.5866869688034058, 0.9367724061012268, 0.7826295495033264, -0.7212603092193604, 0.3292877972126007, 0.4876278340816498, -0.5850509405136108, 0.5867120623588562, -0.13891571760177612, 0.6803501844406128, 0.06041426956653595, -0.5801365971565247, 0.5463850498199463, 0.013927829451858997, 0.5373262763023376, -0.8852062821388245, -0.016228219494223595, -0.004449100233614445, -0.51338791847229, 0.15922871232032776, -0.8328433632850647, -0.8789870738983154, 0.10607413202524185, -0.03115409053862095, -0.030211906880140305, 0.20129141211509705, 0.48939862847328186, -0.8288026452064514, -0.4421181380748749, -0.716550350189209, -0.2118883728981018, 0.283623069524765, -0.8368661403656006, -0.043728094547986984, 0.01865382306277752, 0.043032340705394745, -1.3375811576843262, -0.8551260232925415, -0.13952426612377167, 0.17510986328125, 0.3188035786151886, 1.656965732574463, -0.03660264611244202, 0.22798597812652588, 0.4912942945957184, 0.0117707010358572, -0.9514182209968567, -0.13825802505016327, -0.5450775623321533, -0.16120900213718414, -0.22289419174194336, 0.5135452151298523, -0.44054797291755676, 0.2699331045150757, 0.2965545058250427, 0.1941257268190384, -0.38987573981285095, -0.23073546588420868, -0.27357059717178345, -0.25861695408821106, -0.24559982120990753, 0.11108584702014923, 0.03334222361445427, -0.7290149331092834, 0.2679058015346527, 0.04699274152517319, 1.0471676588058472, -0.1632859706878662, -0.937075674533844, 0.38578200340270996, -0.16734455525875092, -0.6186158061027527, -0.5069946646690369, 0.01498065609484911, -1.5431082248687744, -0.34954482316970825, -1.1018110513687134, -0.14914652705192566, -0.7333105206489563, -0.4837966561317444, 0.05545966327190399, -0.4708298146724701, -0.18070726096630096, 0.4414162039756775, -0.07373596727848053, -0.3317243158817291, 0.23787729442119598, -0.6164082884788513, 0.9330480098724365, 1.086746335029602, -0.7236070036888123, -0.32181334495544434, 0.17849963903427124, 0.31357359886169434, 0.5110108256340027, 0.5508629083633423, -0.5991203784942627, -0.9009491205215454, -1.4166795015335083, 1.0260515213012695, -0.413716584444046, -0.3114330470561981, -0.9199541807174683, 0.6767869591712952, 0.19165651500225067, 0.42090293765068054, 0.3027603030204773, 0.4258337914943695, -1.1763389110565186, -0.24785606563091278, 0.07621748000383377, -0.6476053595542908, 0.22876697778701782, 0.3045618236064911, -0.04962053894996643, -0.3010983467102051, 0.7763127088546753, 0.19256803393363953, -0.6636388897895813, -0.932208240032196, 0.5074320435523987, -0.5633009076118469, -0.0917305126786232, -0.7197721600532532, 0.1403348296880722, -1.0291378498077393, -0.17519907653331757, -0.14819255471229553, 0.5619690418243408, -0.59601891040802, 0.7276716232299805, 0.07868439704179764, -1.2876935005187988, 0.2398805022239685, 0.43532794713974, 0.05595546215772629, -0.07058072090148926, 0.7066308856010437, 0.7240138053894043, -0.3135088086128235, 0.41182786226272583, 0.004161444492638111, 0.5789564847946167, -0.15890534222126007, -0.21834099292755127, 1.0682895183563232, -0.6655288338661194, -0.031775008887052536, 1.1777095794677734, 0.08126544952392578, -1.425065040588379, 0.35794803500175476, -0.550564706325531, -0.6309104561805725, 0.05310120806097984, 0.7454866766929626, 0.12332760542631149, 0.10999999195337296, 0.23963387310504913, -0.5292152762413025, 0.3056156039237976, -0.078797347843647, -0.2486691027879715, 0.6553491353988647, -0.6615883111953735, -0.07504250109195709, 0.5647456049919128, 0.7988625168800354, -0.6861021518707275, -1.1951463222503662, -0.9947943091392517, -0.4710812270641327, -0.1803196668624878, 0.5359443426132202, -0.7158423066139221, -0.9476903080940247, 0.4402938783168793, 0.9296324849128723, -0.44653499126434326, 0.6239559650421143, -0.39394399523735046, -0.2645705044269562, 1.1463773250579834, 0.10067862272262573, -1.0725196599960327, -0.22160910069942474, 1.1829725503921509, 1.402574896812439, -0.8547954559326172, 0.7362009286880493, -0.019674699753522873, -0.7286121845245361, 0.8307511210441589, 0.29439011216163635, -0.21802617609500885, 0.7882129549980164, -0.4951821565628052, -0.22802433371543884, 0.13619373738765717, -1.0821117162704468, -0.23402689397335052, 1.3443987369537354, 0.7937068939208984, 0.28400638699531555, 0.14282022416591644, 0.061605554074048996, 0.8396062254905701, 0.10318130254745483, -0.2582831382751465, 0.5527369976043701, -0.3794528841972351, -0.1672694832086563, -0.05622472986578941, -0.04181196168065071, 0.8654395937919617, -0.7443428635597229, -0.23589511215686798, 0.3712005019187927, 0.7539920210838318, -0.11733534187078476, 0.30729785561561584, 0.5961503386497498, -0.27255046367645264, 0.8049360513687134, -0.3618803918361664, 0.12926796078681946, -0.6451203227043152, -0.3738311529159546, 0.4556683301925659, -0.475037544965744, -0.5588383078575134, 0.12538613379001617, -0.20360387861728668, -0.23297493159770966, -0.2816391587257385, 0.45809197425842285, -0.2159910798072815, 0.8012558221817017, 0.6484612226486206, 0.48947855830192566, 0.8343676924705505, -0.09071484208106995, -1.1254597902297974, -0.635055422782898, -0.9167178273200989, -0.09698476642370224, -0.5598511099815369, -0.24207031726837158, -0.3381834626197815, -0.3772282004356384, -0.5075924396514893]}, "authors": [{"authorId": "2302701835", "name": "Yulong Mao"}, {"authorId": "2281961173", "name": "Kaiyu Huang"}, {"authorId": "2273675023", "name": "Changhao Guan"}, {"authorId": "2303406435", "name": "Ganglin Bao"}, {"authorId": "2007643794", "name": "Fengran Mo"}, {"authorId": "2273905521", "name": "Jinan Xu"}], "references": [{"paperId": "8e5e99f5d8c641acf479fa7883dd40919a38a3f9", "title": "Mixture of Experts Using Tensor Products"}, {"paperId": "da053e2a4ba1b244940c8f2cad5dcdf0d730f85f", "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "0d2adcddccd72de47c263b6e4e0aab3dd0582a52", "title": "ReLoRA: High-Rank Training Through Low-Rank Updates"}, {"paperId": "5c61ff8c92025f105aefdb53361a2e5403aee57c", "title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning"}, {"paperId": "7d5dfe1a97d7c3b071518d518dd73a8168a4556f", "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection"}, {"paperId": "85e959eef45114974c8f8643e88af23936fff3d1", "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"}, {"paperId": "d4bf2cb0dba48f54614f79595587f06bdbfb2e4c", "title": "Efficient Fine-Tuning of BERT Models on the Edge"}, {"paperId": "2ac19d63e1adba20473a6d1122c598f81efc3c58", "title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning"}, {"paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"}, {"paperId": "511e2fe08d8ce170736b59d386a3301a6d6e57b0", "title": "NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM"}, {"paperId": "c28b7dfe341f1e13a5a98efbce7946ef795cf9b8", "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer"}, {"paperId": "5548d8a116dff78827d658ef0bbf3fa0dcaec8f1", "title": "MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "6f0aba8102d63938ce0b48ec23ff5ddd8110f2e8", "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "af5d212bbfbeaa76d884de140f5248d666d13312", "title": "Counter-Interference Adapter for Multilingual Machine Translation"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d89ee98810039d2061ed42ee8026da49c503d16b", "title": "Learning multiple visual domains with residual adapters"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Trans-formers: State-of-the-art natural language processing"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "db8885a0037fe47d973ade79d696586453710233", "title": "The Sixth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": null, "title": "2022. Delta tuning A comprehensive study of parameter efficient meth-ods for pre-trained language models"}, {"paperId": null, "title": "2022. Krona: Parameter efficient tuning with kro-necker adapter"}, {"paperId": null, "title": "Sample a mini-batch d from D and compute the true label loss"}, {"paperId": null, "title": "Compute the importance score s as Equation 7, update"}, {"paperId": null, "title": "Perform backpropagation to compute the gradients of L combined , and update the parameters with the learning rate \u03b3"}, {"paperId": null, "title": "Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1 (Long Papers)"}, {"paperId": null, "title": "and the 11th International Joint Conference on Natural"}, {"paperId": null, "title": "2022. From word embeddings to pre-trained language models: A state-of-the-art walk-through"}, {"paperId": null, "title": "Combine losses by adding true label loss and regularization loss L combined = L true + \u03b7 L reg"}, {"paperId": null, "title": "2023. A survey of large language models"}, {"paperId": null, "title": "2023a. Sparse low-rank adaptation of pre-trained language models"}, {"paperId": null, "title": "Compute the current parameter budget b ( t ) as Equation 9"}, {"paperId": null, "title": "Linguistics (Volume 2: Short Papers)"}]}