{"paperId": "434d751d355d7a7c20efa570e785c76286245e77", "abstract": "Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 39, "influentialCitationCount": 4, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers."}, "embedding": {"model": "specter_v2", "vector": [0.18844719231128693, 0.1022803783416748, 0.002664094790816307, -0.3324797749519348, 0.15959414839744568, 0.09046999365091324, 0.701908528804779, -0.4401929974555969, -0.5192670822143555, 0.12119685113430023, 0.7980666756629944, -0.10409966111183167, 0.8428913950920105, -0.05328068882226944, -0.020082276314496994, -0.03578691929578781, -1.209577202796936, -0.052313245832920074, 0.09631616622209549, -0.5419976115226746, 0.14152662456035614, -0.23764535784721375, -1.2272958755493164, -0.02608620934188366, -0.39237821102142334, 0.4504591226577759, 0.3050157129764557, 1.0833194255828857, -0.2622257173061371, 1.1063008308410645, 0.5880764126777649, -0.06611227989196777, 0.12183139473199844, -0.525393009185791, -0.4345484972000122, -0.19122061133384705, 0.030470332130789757, -0.3744421899318695, -0.7621550559997559, 0.36344560980796814, -0.17185698449611664, 0.54389488697052, 0.22361595928668976, -0.40501272678375244, 0.03014533221721649, 0.7522669434547424, 0.6411888599395752, 0.7201648950576782, -0.4490068256855011, -0.876778244972229, 1.3943672180175781, -1.2876241207122803, 0.10374079644680023, 1.205275297164917, 0.4791770279407501, 0.7438881993293762, 0.1561993509531021, -1.052618384361267, 1.0202256441116333, 0.015670284628868103, -0.6170246005058289, -0.0789421945810318, 0.049003615975379944, -0.22842015326023102, 1.6342304944992065, -0.33521902561187744, 0.2255050241947174, 0.7486434578895569, 0.2765845060348511, 0.8155530691146851, -0.07762700319290161, -0.5430013537406921, -0.3445129096508026, -0.5604994297027588, 0.5370659828186035, 0.992893397808075, -0.30857160687446594, 0.6933873891830444, -1.0390396118164062, 0.11764845252037048, 0.7884198427200317, 0.5847010016441345, 0.19509506225585938, 0.17125703394412994, 0.02786952629685402, 0.8757149577140808, 0.625774621963501, 1.0152230262756348, -0.4034402072429657, 0.8146260380744934, 0.5899606943130493, 0.0971767008304596, -0.20881964266300201, 0.15571509301662445, 0.12115123867988586, 0.4244897961616516, -0.5426390767097473, 0.06646338850259781, -0.2995234727859497, 1.0287970304489136, -0.034322064369916916, 0.6634652018547058, -0.4282105565071106, -0.08422626554965973, 1.3301570415496826, -0.49718478322029114, 0.4459976553916931, -0.49299582839012146, -0.2843136787414551, -0.662558376789093, -0.04245578497648239, -0.5602952837944031, -0.26514366269111633, -0.49759024381637573, -1.0720915794372559, -1.1917409896850586, -0.7749225497245789, 0.4859100580215454, -1.0509716272354126, 0.7333542108535767, -0.8175203800201416, 0.30479106307029724, -0.09866355359554291, -0.01950436271727085, 0.4316805303096771, 0.6056568026542664, 0.5878486633300781, -0.17233140766620636, 0.7689787745475769, -1.039119005203247, -0.8072242736816406, -0.8714457154273987, 0.4487329423427582, 0.093744657933712, -0.024809708818793297, -0.5113086700439453, -0.9616255760192871, -1.1068462133407593, -0.8493565320968628, 0.06473347544670105, -0.4229480028152466, -0.23937158286571503, 1.04487943649292, 0.2022830694913864, -1.1681569814682007, 0.8520624041557312, -0.5768085718154907, -0.2421557903289795, 0.27042055130004883, 0.07986236363649368, 0.5692572593688965, -0.1396820992231369, -1.4278403520584106, 0.23854021728038788, 0.018938912078738213, -0.15537452697753906, -0.5418805480003357, -0.3165651857852936, -1.0330690145492554, -0.17621110379695892, -0.2739681005477905, -0.32652735710144043, 1.3030284643173218, 0.23504233360290527, -1.183151125907898, 0.5471960306167603, -0.4603288471698761, -0.16604506969451904, 0.13014395534992218, -0.18350349366664886, -0.2871658205986023, -0.6611119508743286, -0.1817985326051712, 0.21468284726142883, 0.09183434396982193, -0.23847050964832306, -0.26022130250930786, -0.11895740777254105, -0.5490477681159973, -0.1852307766675949, -0.146651029586792, 1.0518721342086792, -0.2667374908924103, -0.21905966103076935, 0.16935411095619202, 0.5595734119415283, -0.4849032759666443, -0.6851221919059753, -0.12933550775051117, -0.8027619123458862, 0.7241863012313843, -0.24494782090187073, 1.1676661968231201, -0.6217712163925171, -1.0389024019241333, -0.18908272683620453, -0.20254401862621307, -0.22665637731552124, -0.8383870124816895, 0.676589846611023, -0.6554372906684875, 0.4665582478046417, 0.2335040122270584, -0.9380884170532227, -0.5314307808876038, -0.04877680167555809, -1.0182347297668457, -0.2795666754245758, -0.022647345438599586, 1.138922095298767, -1.0599474906921387, 0.22291111946105957, -0.11156472563743591, 0.4340963363647461, -0.6517677903175354, 1.6832231283187866, -0.13801945745944977, 0.383361279964447, -0.12210914492607117, -0.33773234486579895, -0.35982659459114075, -0.5090508460998535, 0.517485499382019, -0.36005017161369324, -0.20816278457641602, 0.5254097580909729, 0.08692973107099533, 0.9725201725959778, -0.34094274044036865, 0.6265036463737488, -0.4736461043357849, -0.6282493472099304, 0.6509338021278381, 0.24695928394794464, -0.12271776050329208, -0.5364243388175964, 0.4252921938896179, 0.013962137512862682, -0.8078611493110657, 0.6205068826675415, 0.7166979908943176, 0.5946765542030334, -0.48337700963020325, 0.11879139393568039, 0.5489100217819214, 0.06968534737825394, 0.38862693309783936, 0.37813130021095276, 0.9115447998046875, 0.3956145644187927, 0.5133796334266663, 0.21112403273582458, 0.3160601258277893, -1.158351182937622, 0.15524597465991974, 0.7756518125534058, 0.5966236591339111, 0.9720197319984436, 0.14903433620929718, -0.8577455282211304, -0.5396802425384521, 0.3367824852466583, 1.0495108366012573, 1.6179505586624146, -0.2784025967121124, -0.1505163609981537, -0.4435552954673767, 0.12185601145029068, -0.9344053864479065, -0.00010030354314949363, -0.39932528138160706, -0.10363104194402695, -0.8472116589546204, -0.9018833041191101, 0.791103184223175, 0.5920009613037109, 1.1600533723831177, -0.9363955855369568, -0.3734327256679535, -0.11817193031311035, 0.3141913115978241, -0.8179535269737244, -0.589587390422821, 0.5942541360855103, -0.7012155055999756, 0.000709591549821198, 0.12497497349977493, -0.11594768613576889, -0.06330288201570511, -0.5038125514984131, 1.0455424785614014, -0.10654838383197784, -0.3236386775970459, 0.14030387997627258, 0.5050972104072571, -0.6107354164123535, -0.6803705096244812, 0.18536905944347382, 0.15734079480171204, -0.2219589650630951, 0.08033164590597153, 0.48271313309669495, -0.08040040731430054, 0.10490646213293076, -0.28434473276138306, -0.0467308945953846, 0.04136863723397255, 0.2306366115808487, 0.45607221126556396, -0.25940611958503723, 0.1802283227443695, -1.0234127044677734, 0.6648895144462585, 0.4129428565502167, -0.4712655246257782, 0.2817510962486267, -0.5634316802024841, -0.2879565954208374, 0.3187481164932251, -0.31570813059806824, -0.21431989967823029, -0.941910982131958, 0.41751059889793396, -0.45168957114219666, -0.037429437041282654, 0.20639200508594513, 0.3920293152332306, 0.3624078035354614, -0.12197799235582352, 0.6287793517112732, 0.13049061596393585, 0.13199058175086975, 0.06537240743637085, -0.8785222172737122, 0.3772464990615845, 0.6696373224258423, 0.15932826697826385, -0.20748808979988098, 0.07501079142093658, -0.9507646560668945, -0.725860595703125, -0.5103232264518738, -0.2597980499267578, -0.4348437488079071, 0.15976057946681976, -0.7164671421051025, -1.0062087774276733, 0.1501142680644989, -1.0902658700942993, -0.3563074767589569, 0.3847213387489319, -0.21071334183216095, -0.23523865640163422, -1.1237378120422363, -1.3083351850509644, -1.058206558227539, -0.5208910703659058, -0.6158325672149658, -0.02550267055630684, 0.3189370930194855, -0.3556029796600342, -0.5952810049057007, -0.06391707807779312, -0.5489852428436279, 0.9676767587661743, -0.3543262779712677, 0.7687645554542542, -0.1302143633365631, -0.41944175958633423, -0.23044200241565704, 0.3359403908252716, 0.6836634278297424, 0.194213405251503, 0.14055486023426056, -0.7077979445457458, 0.04006253555417061, 0.047184769064188004, -0.033928852528333664, 0.3037656843662262, 0.12694986164569855, 0.9125927686691284, 0.3302467465400696, -0.4050965905189514, 0.05185944586992264, 1.272769570350647, -0.07679510116577148, 0.12595924735069275, 0.002695368602871895, 1.1300392150878906, -0.047253578901290894, -0.3584464192390442, 0.4192230701446533, 0.09744603931903839, 0.12886124849319458, 0.2724297046661377, -0.06686855852603912, -0.05809660628437996, -0.59079509973526, 0.5422030091285706, 1.7918161153793335, -0.12803222239017487, -0.0007379755261354148, -0.9155868291854858, 0.7123107314109802, -1.0110808610916138, -0.995278537273407, 0.7678206562995911, 0.5655539631843567, 0.31174522638320923, -0.3772415816783905, -0.31643688678741455, -0.21510350704193115, 0.7872666716575623, 0.6775755286216736, -0.3487602472305298, -0.6371769905090332, 0.010624902322888374, 0.3306249678134918, 0.20598728954792023, 0.6565101146697998, -0.1509605348110199, 0.40582799911499023, 14.940979957580566, 0.20668195188045502, -0.3391033113002777, 0.3233978748321533, 0.8093849420547485, 0.35450077056884766, -0.0008514849469065666, -0.2326788604259491, -1.6221874952316284, -0.37815573811531067, 1.454755425453186, 0.07921285182237625, 0.6990090012550354, -0.04174333065748215, 0.15053950250148773, 0.6074877381324768, -0.3946975767612457, 0.6551600098609924, 0.43117690086364746, -1.389503002166748, 0.6358278393745422, 0.15873806178569794, 0.25286880135536194, 0.6919248104095459, 0.6582958102226257, 0.9156022071838379, 0.7855084538459778, -0.5879738926887512, 0.08520100265741348, 0.45453527569770813, 0.6400178670883179, -0.0666591003537178, 0.5361335873603821, 0.3870517611503601, -1.1459238529205322, -0.166300430893898, -0.4211215078830719, -1.485358476638794, 0.12814785540103912, -0.10041064769029617, -0.3194640278816223, -0.259622186422348, -0.11894211918115616, 1.1231474876403809, 0.39482441544532776, 0.20069798827171326, -0.2438151240348816, 0.7558853626251221, 0.4426927864551544, 0.01528299879282713, 0.4884733259677887, 0.3908075988292694, -0.1436488777399063, -0.09889617562294006, 0.013989134691655636, 0.4553140699863434, 0.07602984458208084, -0.10742636770009995, -0.6639094352722168, -0.25872373580932617, -0.6074919104576111, -0.5041858553886414, -0.22451527416706085, 0.6159198880195618, 0.33305197954177856, 0.3907606303691864, -0.3421924412250519, 0.057684559375047684, 0.4115806221961975, 0.3453935980796814, -0.5295839309692383, 0.06446409970521927, 0.4128003418445587, -0.3208037316799164, 0.24822945892810822, 0.15856224298477173, -0.26841920614242554, -0.4995390772819519, -1.1816396713256836, -0.08572541177272797, 0.3515128493309021, -0.9080098867416382, -0.8207319974899292, 1.5716696977615356, -0.5012328028678894, -0.2631016969680786, 0.03445998579263687, -0.7036179900169373, -0.34527984261512756, 0.5573036074638367, -1.2992501258850098, -0.4921545088291168, -0.2021474838256836, -0.16913847625255585, -0.3206724226474762, 0.061478182673454285, 1.1848949193954468, 0.09802568703889847, -0.4864751398563385, -0.3209172785282135, -0.12343529611825943, 0.31454378366470337, -0.5732851624488831, -0.48822876811027527, 0.728373110294342, 0.31458744406700134, -0.009110952727496624, 0.5249654054641724, 0.212596133351326, 0.07951688766479492, -0.33414891362190247, -0.5995848178863525, 0.8285953402519226, -0.7291540503501892, -0.22277489304542542, -1.100213646888733, -0.9009235501289368, 0.8167254328727722, 0.5569177865982056, -0.10285311937332153, 0.25118690729141235, 0.07618088275194168, -0.5301427841186523, -0.14141042530536652, -0.3203203082084656, 0.18887865543365479, 0.8060985803604126, -0.9410986304283142, -0.1567409485578537, -0.361503005027771, 0.5386765003204346, -0.6351627111434937, -0.2544689178466797, -0.06144895404577255, 0.04928954318165779, -0.4261637032032013, 0.7047023773193359, -0.5876803398132324, 0.5306947827339172, 0.8504594564437866, -0.26982030272483826, -0.46111762523651123, -0.45358502864837646, -0.8836581110954285, 0.25509223341941833, 0.262858122587204, 0.12271394580602646, -0.3591276705265045, 0.46714991331100464, 0.5144318342208862, 0.3794980049133301, -0.6471588015556335, -0.454593300819397, -0.25162675976753235, -0.4012603759765625, -0.42431640625, 0.23969660699367523, -0.31404909491539, -0.7394596338272095, 0.3526064455509186, -0.04156982898712158, 0.5334387421607971, -0.33498790860176086, -0.6541338562965393, -0.036906611174345016, -0.20308414101600647, 0.3428496718406677, -0.4021042287349701, -0.7527931332588196, -1.2732300758361816, 0.12731337547302246, -0.8633168339729309, 0.0149626350030303, -0.9100282192230225, -0.2647475600242615, 0.404028981924057, -0.6623883247375488, 0.17979125678539276, 0.5135746598243713, -0.15154528617858887, -0.33921125531196594, -0.5222036242485046, -0.5584169626235962, 0.9796756505966187, 0.6619186997413635, -0.3292866349220276, 0.15336179733276367, -0.09851430356502533, 0.05279209837317467, 0.25394684076309204, 0.44309312105178833, -0.5065010786056519, -0.673160970211029, -0.7824637293815613, 0.2060021311044693, -0.043660618364810944, -0.11754032969474792, -0.8625960350036621, 0.6962457895278931, 0.3940151333808899, 0.06423868238925934, -0.33446165919303894, 0.22040073573589325, -0.6353803873062134, -0.23332545161247253, 0.6201801896095276, -0.9082632064819336, 0.4803199768066406, 0.11929931491613388, -0.35070452094078064, -0.5305899977684021, 1.0167458057403564, 0.047195881605148315, -1.0783886909484863, -0.6361925005912781, 0.7983687520027161, -0.8303964734077454, -0.14823922514915466, -0.3067815601825714, -0.10948223620653152, -0.6960641741752625, -0.6368751525878906, -0.16934573650360107, 0.33898329734802246, -0.6258633136749268, 1.0829441547393799, 0.7551923990249634, -1.233534336090088, 0.10108252614736557, 0.3311771750450134, -0.3407098352909088, -0.3387838304042816, 0.2826731503009796, 0.04235820099711418, -0.0175783671438694, 0.7581925392150879, 0.15710768103599548, 0.530756413936615, -1.2012755870819092, -0.06913909316062927, 1.076667070388794, -0.8255009055137634, -0.05204973742365837, 0.9924584627151489, -0.07057547569274902, -1.009554147720337, 0.20384614169597626, -1.1183013916015625, -1.0377569198608398, -0.12811768054962158, 0.8103103637695312, 0.06790391355752945, -0.1964661031961441, -0.26669126749038696, -0.531091034412384, 0.3208734691143036, -0.3719467222690582, -0.36133208870887756, 0.7978186011314392, -0.39399003982543945, -0.504108726978302, 1.4409427642822266, 0.7750694751739502, -0.992804765701294, -0.8356887102127075, -0.6898641586303711, -0.2018924355506897, 0.10892436653375626, 0.13995778560638428, -0.047184694558382034, -0.6445237994194031, 0.717254638671875, 0.5810580849647522, 0.5007748007774353, 0.16998037695884705, -0.48882654309272766, 0.02056449092924595, 0.5858961939811707, 0.3729681968688965, -0.0644720196723938, -0.09639019519090652, 1.9717230796813965, 1.5874418020248413, -0.4374556541442871, 0.1928122341632843, -0.12536561489105225, -0.6096774339675903, 0.9345464706420898, 0.3033811151981354, -0.4124521017074585, 1.092397689819336, -0.33327990770339966, 0.30782952904701233, 0.14782091975212097, -1.3133089542388916, -0.044954024255275726, 0.07373996078968048, 0.742946982383728, 1.026031494140625, 0.038894377648830414, 0.28497859835624695, 0.7689707279205322, 0.2766484320163727, 0.3721771240234375, 0.5479823350906372, 0.7181261777877808, 0.03582910820841789, 0.03744146227836609, 0.39836418628692627, 0.43718770146369934, -0.7219987511634827, -0.7181214690208435, 0.4734683036804199, 0.5338602066040039, -0.24080222845077515, 0.5371508598327637, 0.889332115650177, -0.2775128185749054, 0.6258434057235718, 0.20323708653450012, 0.34981414675712585, -0.5304369926452637, -0.5265516042709351, -0.19421494007110596, -0.7347006797790527, -0.3333769738674164, -0.48984503746032715, -0.5151640176773071, -0.5554212927818298, -0.002061312086880207, 0.5617834329605103, 0.14895828068256378, 0.24382637441158295, 1.0558826923370361, 0.4008420407772064, 0.6634385585784912, 0.11507273465394974, -0.5771387219429016, -0.308378130197525, -0.7649974226951599, -0.07327159494161606, -0.697702169418335, 0.1495860517024994, -0.20141933858394623, -0.04821281135082245, -0.24667280912399292]}, "authors": [{"authorId": "2171650015", "name": "Zhen Qin"}, {"authorId": "50591392", "name": "Songlin Yang"}, {"authorId": "2266275708", "name": "Yiran Zhong"}], "references": [{"paperId": "0664d52b1040e048fff7e7d1d13a310964207768", "title": "Accelerating Toeplitz Neural Network with Constant-time Inference Complexity"}, {"paperId": "2a38daf98d506477f8180806f503409d5036eaf4", "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer"}, {"paperId": "0a304c773678824f7ab82e9f1acc8ed86dc668af", "title": "Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues"}, {"paperId": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8", "title": "Linearized Relative Positional Encoding"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425", "title": "Toeplitz Neural Network for Sequence Modeling"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "54155c2977a977bf129849455dcae3a2b79b3f41", "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"}, {"paperId": "f640a2635f38cbb3dbb83775088c2e27b790ad77", "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc", "title": "Pretraining Without Attention"}, {"paperId": "24574078195f46dce0130bdae88ee3dfea912646", "title": "Simplifying and Understanding State Space Models with Diagonal Linear RNNs"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "240300b1da360f22bf0b82c6817eacebba6deed4", "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"}, {"paperId": "ee1e1afe75eb0e20d57bf316f5ab1ca2c369d100", "title": "Linear Video Transformer with Feature Fixation"}, {"paperId": "f6d8beb02771791d628f7e0773d8906261ce707c", "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights"}, {"paperId": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc", "title": "Liquid Structural State-Space Models"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "86c8d930b492a4f9cadc6c60aecdaaded49acc86", "title": "Neural Architecture Search on Efficient Transformers and Beyond"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "6be32b4321f95b79bb2e37feeab0c3c7f902195e", "title": "Vicinity Vision Transformer"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "96e8498509bddf69fb9a7752fce5d368b2f5365c", "title": "Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "0e2d8b8d81092037f9866c1ceddcebb87318e38b", "title": "AST: Audio Spectrogram Transformer"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "47d79963ac69111d8dc82a228d26e6a746a4d087", "title": "Transformers"}, {"paperId": "15d6f3d815d0ff176fafb14a3f46e5723ebac723", "title": "Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks"}, {"paperId": "10428cdda9387933c64f518cefe6ca0a685f579e", "title": "The unreasonable effectiveness of the forget gate"}, {"paperId": "0e9822e6c78b752f13ccf2943a8e41f9997f4a22", "title": "Can recurrent neural networks warp time?"}, {"paperId": "2dad7e558a1e2982d0d42042021f4cde4af04abf", "title": "Dilated Recurrent Neural Networks"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "title": "Simple Recurrent Units for Highly Parallelizable Recurrence"}, {"paperId": "64ca5f3e4fa2d33ae860bebaea81420fae0759af", "title": "Stock price prediction using LSTM, RNN and CNN-sliding window model"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "65eee67dee969fdf8b44c87c560d66ad4d78e233", "title": "Hierarchical Multiscale Recurrent Neural Networks"}, {"paperId": "7e45b68037b5f86c4bce305b2725f4871c6b091e", "title": "Strongly-Typed Recurrent Neural Networks"}, {"paperId": "e9c771197a6564762754e48c1daafb066f449f2e", "title": "Unitary Evolution Recurrent Neural Networks"}, {"paperId": "9a69aad72a69bbbf36190e53cbd8bd7698b50fd1", "title": "Weather forecasting using deep learning techniques"}, {"paperId": "97acdfb3d247f8250d865ef8a9169f06e40f138b", "title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding"}, {"paperId": "d46b81707786d18499f911b4ab72bb10c65406ba", "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units"}, {"paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "title": "LSTM: A Search Space Odyssey"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "5522764282c85aea422f1c4dc92ff7e0ca6987bc", "title": "A Clockwork RNN"}, {"paperId": "c582b1e0189d6dd6682c5450d47fd8d3ff4c096b", "title": "The Exponentially Weighted Moving Average"}, {"paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4", "title": "Learning to Forget: Continual Prediction with LSTM"}, {"paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922", "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"}, {"paperId": "ccf84c100fa78c599d0e901a3754ff044aa6bd9e", "title": "Encoding Recurrence into Transformers"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "30becc9209c02cd4f3f6aed86ea9bd37de50be9d", "title": "Improving the Gating Mechanism of Recurrent Neural Networks"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "2e5f2b57f4c476dd69dc22ccdf547e48f40a994c", "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"}, {"paperId": null, "title": "tasks. 6.4 Configurations We list detailed hyper-parameters of our experiments here"}]}