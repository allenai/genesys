{"paperId": "d613e92a30f19029d842d8ef7d73f8ad4dbbd3ae", "abstract": "In this work, we optimize speculative sampling for parallel hardware accelerators to improve sampling speed. We notice that substantial portions of the intermediate matrices necessary for speculative sampling can be computed concurrently. This allows us to distribute the workload across multiple GPU threads, enabling simultaneous operations on matrix segments within thread blocks. Additionally, we use fast on-chip memory to store intermediate results, thereby minimizing the frequency of slow read and write operations across different types of memory. This results in profiling time improvements ranging from 6% to 13% relative to the baseline implementation, without compromising accuracy. To further accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid. This approximation approach results in significantly greater relative improvements in profiling time, ranging from 37% to 94%, with a slight decline in accuracy. We conduct extensive experiments on both automatic speech recognition and summarization tasks to validate the effectiveness of our optimization methods.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "To accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid, and this approximation approach results in significantly greater relative improvements in profiling time, with a slight decline in accuracy."}, "embedding": {"model": "specter_v2", "vector": [0.5678327679634094, 0.7849869728088379, -0.5675573348999023, -0.01845826394855976, -0.6114923357963562, 0.611166775226593, 0.6431715488433838, 0.00996218528598547, -0.3594265878200531, -0.5765056014060974, 1.0316779613494873, -0.01300217118114233, 0.7753212451934814, -0.1726357787847519, -0.07097546756267548, 0.04796598479151726, -0.9060295224189758, -0.20488469302654266, 0.4301128387451172, 0.08711939305067062, -0.036346301436424255, -0.614482045173645, -1.5517468452453613, 0.5924251675605774, 0.2783086895942688, 0.6368938684463501, -0.19413383305072784, 1.0340251922607422, -0.4265824556350708, 0.18927626311779022, -0.15349404513835907, -0.02080630138516426, 0.10761410742998123, -0.2575911283493042, -0.19194021821022034, 0.06452366709709167, -0.013668053783476353, -0.4042714834213257, -0.6005465984344482, 0.7704412341117859, -0.15878358483314514, 0.41934072971343994, 0.23500531911849976, -0.3688552975654602, 0.1539250612258911, 0.8700684905052185, -0.03690202161669731, 1.060897946357727, 0.062130171805620193, -0.7181199789047241, 1.0647172927856445, -1.5902868509292603, -0.04391523823142052, 1.4240453243255615, 0.38946792483329773, -0.14411720633506775, -0.13487398624420166, 0.041691359132528305, 0.31181463599205017, 0.16163934767246246, -0.7293965220451355, -0.39783355593681335, -0.007858199998736382, -0.00999245885759592, 1.7991539239883423, 0.1911677122116089, 0.41265884041786194, 0.27899500727653503, 0.1061178669333458, 1.8336234092712402, -0.2225854992866516, -0.7161290645599365, 0.31425467133522034, -0.013141482137143612, 0.6817115545272827, 0.6079452037811279, -0.46805036067962646, 0.3526236414909363, -1.6746422052383423, -0.7485862374305725, -0.19219845533370972, -0.004041851032525301, 0.299057275056839, 0.022207681089639664, -0.2521567642688751, 0.8204610347747803, -0.07493975758552551, 0.0780184417963028, -0.4541996419429779, 0.4500170946121216, 0.9824881553649902, -0.17720551788806915, 0.47743216156959534, -0.14564836025238037, 0.4137992262840271, -0.016290070489048958, -1.4864063262939453, -0.13081668317317963, 0.2875448763370514, 0.9390053153038025, -0.5569730401039124, 0.7002843022346497, -1.0560460090637207, 0.2496946156024933, 1.0446735620498657, 0.43445611000061035, 0.42174282670021057, -0.2317536175251007, 0.0007911745924502611, -0.946855366230011, -0.032907675951719284, -0.12794272601604462, -0.10360370576381683, -0.4800361394882202, -0.7876822352409363, -0.7734097838401794, -0.6578512787818909, 8.693249583302531e-06, -0.6883673071861267, -0.038847580552101135, -0.11729579418897629, 0.33745869994163513, 0.1846621036529541, 0.1852858066558838, 0.26865673065185547, 0.8955696821212769, 0.009851258248090744, -0.15248200297355652, 1.3136285543441772, -1.2962063550949097, -0.4374765455722809, -0.8760424852371216, 0.08272046595811844, -0.37326252460479736, 0.3467150330543518, -0.018076464533805847, -1.505037546157837, -1.0514569282531738, -0.9669198989868164, 0.005320493597537279, -0.05241192504763603, 0.45110249519348145, 1.0797514915466309, 0.701422393321991, -0.8960305452346802, 0.7943977117538452, -0.7044888138771057, -0.09175225347280502, 0.5529409646987915, 0.1712777465581894, 1.001692295074463, 0.08735699951648712, -0.7933283448219299, -0.36585378646850586, -0.18294595181941986, -0.7789459228515625, 0.13059602677822113, -0.4946691691875458, -0.8299815654754639, 0.3348959982395172, -0.018804002553224564, -0.4060743451118469, 1.24830961227417, -0.017515365034341812, -1.767595887184143, 0.1933402717113495, -0.573024570941925, -0.32752886414527893, 0.3069785535335541, -0.07148578017950058, -0.343976765871048, -0.16542960703372955, -0.5040916204452515, 0.25146639347076416, 0.706129789352417, 0.6394204497337341, -0.3350570797920227, 0.2537791430950165, -0.6548522710800171, -0.05656026303768158, -0.4162668287754059, 0.887227475643158, -0.5019068121910095, -0.28596585988998413, 0.31497305631637573, 0.4965609014034271, -0.35706520080566406, -0.438810795545578, -0.6140323281288147, -0.8549844622612, 0.35438525676727295, 0.03854510560631752, 1.0395528078079224, -0.9871845245361328, -1.1616917848587036, 0.14141018688678741, -0.18062201142311096, 0.14052093029022217, -0.2732967734336853, 0.6173812747001648, -0.11035214364528656, -0.26517754793167114, 0.0054093217477202415, -0.8447673320770264, -0.011738347820937634, -0.6112712025642395, -0.8969223499298096, -0.013590759597718716, 0.21979382634162903, 0.8659183382987976, -0.8731417059898376, 0.15459558367729187, -0.14654240012168884, 0.3002755343914032, -1.1348844766616821, 1.0392963886260986, -0.24734798073768616, -0.15897664427757263, 0.0015700894873589277, -0.09989865869283676, 0.476829469203949, -0.4350137412548065, 0.9193999767303467, -0.5807735323905945, -0.25446027517318726, 0.7280500531196594, -0.4167599678039551, 0.7802755236625671, -0.24217958748340607, 0.4466419517993927, -0.1350840926170349, -0.5222468972206116, 0.2954583764076233, 0.12464533001184464, -0.26723796129226685, -0.41863080859184265, 0.5022903084754944, 0.1806505173444748, -0.787445604801178, 0.3764476776123047, 0.9105884432792664, 1.2675788402557373, -0.2478710263967514, 0.33491963148117065, 0.37462741136550903, -0.20355644822120667, 0.48424702882766724, 0.2966478168964386, 0.6079619526863098, 0.25205039978027344, 0.7616397142410278, -0.40405067801475525, 0.2775939404964447, -0.9410924911499023, -0.12198136746883392, 0.6180441975593567, 0.5044628381729126, 0.6674997806549072, 0.6029919981956482, -0.4255649447441101, -0.395851194858551, -0.2882029116153717, 0.5003869533538818, 1.6213033199310303, -0.3700377345085144, -0.08561687171459198, -0.83990478515625, -0.4575836956501007, -0.29028528928756714, 0.06342923641204834, -0.22852231562137604, 0.01411997526884079, -0.4569207429885864, -1.5312905311584473, 0.5565364360809326, 0.4954758286476135, 0.528704822063446, -0.5389500856399536, -0.47018149495124817, -0.6058521866798401, 0.5924347043037415, -0.5640838742256165, -0.5935844779014587, 0.44682925939559937, -0.6500056385993958, 0.3011522591114044, 0.09403271973133087, 0.060853566974401474, 0.1667812615633011, -0.2175721675157547, 1.266128659248352, -0.4422505497932434, -0.12077499181032181, -0.3084537386894226, 0.31512805819511414, -0.9300435185432434, -0.7424498200416565, 0.3653191030025482, -0.1338842660188675, -0.09878970682621002, 0.4550871253013611, 0.19850553572177887, -0.0002374875475652516, -0.2205352485179901, -0.18649506568908691, -0.012620038352906704, 0.23623979091644287, 0.2574350833892822, 0.5911340117454529, -0.4712655544281006, 0.00226891809143126, -1.0142252445220947, 0.9173876643180847, -0.12873269617557526, -0.3063047230243683, -0.0303998701274395, -0.2779224216938019, -0.18192698061466217, 0.48356857895851135, -0.841976523399353, -0.5235907435417175, -0.668932318687439, -0.2985191345214844, 0.1179606020450592, 0.21372662484645844, 0.13428913056850433, 0.6421535611152649, 0.2007819414138794, 0.5014461874961853, 0.6582193374633789, 0.573554515838623, 0.12716618180274963, 0.17777180671691895, -0.2834452688694, 0.2408042848110199, 0.35366809368133545, -0.33246704936027527, 0.38109156489372253, -0.2434503436088562, -1.4356234073638916, -0.2260976880788803, -0.2161053717136383, 0.0755857452750206, -0.3120208978652954, -0.23461590707302094, -0.4131838083267212, -1.062339425086975, -0.3617241084575653, -0.713222861289978, -0.08000476658344269, -0.25554195046424866, -0.03342418745160103, -0.23732832074165344, -0.6542578339576721, -1.2688567638397217, -0.7889404892921448, -1.0623083114624023, -1.123869776725769, 0.6975390315055847, 0.26891621947288513, -0.5665093660354614, -0.12645350396633148, -0.25196513533592224, -0.536663830280304, 0.5559607744216919, -0.8060553073883057, 0.3962579071521759, -0.3684307634830475, -0.2546555995941162, -0.5213297009468079, 0.2958444356918335, 0.4638572931289673, -0.6333820819854736, 0.4296317398548126, -0.8693901896476746, 0.2935985326766968, -0.07755246758460999, -0.03346005827188492, 0.34885746240615845, 0.5718665719032288, 0.898123562335968, 0.23682290315628052, -0.628297746181488, 0.12315786629915237, 0.7204565405845642, -0.7137418389320374, -0.03742213174700737, -0.4288521409034729, 0.5475180149078369, 0.04899565130472183, 0.1772046536207199, 1.1575113534927368, -0.25387296080589294, 0.6419613361358643, -0.2676190435886383, -0.13863994181156158, -0.2659838795661926, -0.12031345069408417, 0.6934103965759277, 1.9014058113098145, 0.9442043304443359, -0.08139905333518982, -0.6272429823875427, 0.6644325852394104, -1.2034025192260742, -0.541525661945343, 0.26653534173965454, 0.797186553478241, 0.10710244625806808, -0.0026195915415883064, 0.16235625743865967, 0.06413839757442474, 0.451880544424057, 0.7907900810241699, -0.4063912034034729, -0.9606857299804688, 0.39317870140075684, 0.6849707365036011, 0.22419016063213348, 0.568458616733551, -0.21519027650356293, 0.12727107107639313, 15.079704284667969, 1.0788357257843018, 0.005267698783427477, 0.38602152466773987, 0.7825817465782166, -0.06708911806344986, -0.3370632827281952, -0.29907578229904175, -1.3710887432098389, 0.16220469772815704, 1.5191158056259155, 0.04189446195960045, 0.38372236490249634, 0.5541330575942993, 0.032066721469163895, -0.05783519148826599, -0.13588038086891174, 0.2552415132522583, 0.2855956256389618, -1.6655179262161255, -0.11658693850040436, 0.05586768314242363, 0.27513447403907776, 0.930829644203186, 0.7176377773284912, 0.5700460076332092, 0.2973656952381134, 0.2695954442024231, 0.4294135570526123, 0.07486356794834137, 0.6011644005775452, -0.34417724609375, 0.3706606924533844, 0.3976118564605713, -0.997768759727478, 0.1452946662902832, -0.4696131646633148, -1.105672001838684, 0.38961607217788696, 0.6965051293373108, -1.2398065328598022, -0.6020638346672058, -0.5440250635147095, 0.357440710067749, 0.3421092629432678, 0.23369048535823822, 0.1310432106256485, 0.846241295337677, -0.28565725684165955, -0.8916411399841309, -0.06658029556274414, 0.33932092785835266, 0.1723947674036026, 0.3903038799762726, -0.11650732904672623, -0.1968275010585785, 0.2678428590297699, 0.09005078673362732, -0.12638022005558014, -0.19675417244434357, 0.07688784599304199, -0.16754509508609772, 0.03470885381102562, 0.7119267582893372, 0.3097423017024994, -0.11104918271303177, -0.8012769818305969, 0.2725682556629181, 0.4977729618549347, -0.1686774343252182, -0.6555269360542297, -0.24264536798000336, 0.5770353674888611, -0.589484453201294, 0.2426314800977707, 0.48475998640060425, -0.6035844087600708, -0.5182977318763733, -0.8158224821090698, -0.33470067381858826, 0.268744558095932, -0.18474727869033813, -0.5598624348640442, 0.7752296328544617, -0.30192407965660095, -0.4338546693325043, 0.08284224569797516, -0.383137583732605, -0.36223307251930237, 0.4982188940048218, -0.8744608759880066, -0.2301863431930542, 0.47891801595687866, -0.8031010031700134, 0.1384904384613037, 0.06820850819349289, 1.0722047090530396, -0.19379283487796783, -0.3142257034778595, 0.19624143838882446, 0.04804179072380066, -0.5332666635513306, -0.537896454334259, -0.3969945013523102, 1.668394923210144, 0.5906082391738892, -0.008546754717826843, 0.14449000358581543, 0.19871675968170166, -0.042307909578084946, -0.9184730648994446, -0.06244475021958351, 0.502715528011322, -0.04694882035255432, -0.2436184138059616, -0.902752161026001, -0.8449773192405701, -0.18820910155773163, 0.43762683868408203, 0.09390899538993835, 0.6963434815406799, 0.22127953171730042, -0.0041631171479821205, -0.001212043222039938, -0.5397855639457703, 0.11739775538444519, 0.38583874702453613, -0.7856904864311218, 0.17597706615924835, 0.27076417207717896, 0.25176772475242615, -1.0831663608551025, -0.5788200497627258, -0.32518523931503296, 0.24909718334674835, -0.5192323327064514, 1.0475358963012695, 0.0047810617834329605, 0.333601176738739, 0.6990209817886353, -0.05185648798942566, -0.5442531108856201, 0.09155295789241791, -0.9534528255462646, -0.3914993405342102, -0.2616153955459595, 0.5940653085708618, -0.265567809343338, 1.2087161540985107, 0.42819955945014954, 0.1438339650630951, -0.6200946569442749, -0.45890042185783386, 0.020428387448191643, -0.48420149087905884, -0.36083412170410156, 0.34408918023109436, -0.7091405391693115, 0.3657327890396118, 0.025007056072354317, 0.5806286334991455, 0.8372438549995422, -0.15624675154685974, -0.3373965322971344, 0.5154190063476562, 0.2750462591648102, -0.424611359834671, -0.5894553065299988, -0.3369937241077423, -1.5559111833572388, -0.06383369117975235, -0.9007137417793274, 0.11637937277555466, -0.25672879815101624, -0.5076512098312378, 0.48648321628570557, 0.08922702819108963, -0.49909666180610657, 0.012738009914755821, -0.24960297346115112, -0.7676641345024109, -0.5737352967262268, -0.3618291914463043, 0.7979134917259216, 0.7153139114379883, -0.5339672565460205, -0.22534418106079102, 0.47524142265319824, 0.315073698759079, 0.5351121425628662, 0.39842283725738525, -0.3442496955394745, -0.7007848620414734, -0.7230914831161499, 0.034997645765542984, 0.3308089077472687, -0.3953392803668976, -0.9270827174186707, 0.6340863704681396, 0.20499040186405182, -0.06066965311765671, -0.3925062119960785, 0.4581889808177948, -0.014688408002257347, -0.499332994222641, 0.5444217324256897, -0.7377563714981079, -0.08299288153648376, 0.08466408401727676, -0.4850326180458069, -0.17334119975566864, 0.4580534100532532, 0.21280035376548767, -0.5057048797607422, -0.465712308883667, 0.34607216715812683, -0.7241812944412231, 0.29980385303497314, -0.3963357210159302, -0.11156126856803894, -0.8260498642921448, -0.22904790937900543, 0.02764113061130047, -0.2343185842037201, -0.4555986821651459, 0.5487852692604065, 0.26687106490135193, -0.8461006283760071, 0.28449660539627075, 0.7295078039169312, -0.38003700971603394, -0.2946428954601288, 0.6332240700721741, 0.6412736773490906, -0.41700923442840576, 0.20012100040912628, 0.4086008369922638, 7.242845458677039e-05, -1.1497845649719238, -0.17991960048675537, 0.09396514296531677, -0.4569721817970276, -0.33928877115249634, 1.2463847398757935, -0.5474774241447449, -0.5827933549880981, -0.1316780298948288, -1.3312509059906006, -0.19859306514263153, -0.028482094407081604, 0.8538798093795776, 0.803303599357605, 0.40434592962265015, 0.45210716128349304, -0.8016884922981262, -0.5057010054588318, -0.08019983768463135, -0.6586149334907532, 0.26123207807540894, -0.012126760557293892, -0.16225369274616241, 0.44875672459602356, 0.8110941052436829, 0.03356322646141052, -0.8235636353492737, -0.7282195091247559, -0.32289960980415344, -0.4573329985141754, 0.5307977199554443, 0.11225306987762451, -0.5788562297821045, 0.6616822481155396, 0.19000475108623505, 0.8246780037879944, 0.05800064653158188, -0.3721325993537903, 0.5987577438354492, 0.2803061902523041, 0.07372258603572845, -0.709185779094696, -0.7360966205596924, 1.048538327217102, 0.8470907211303711, -0.46671998500823975, 0.8833327889442444, -0.7371119260787964, -0.9802423119544983, 0.9318888187408447, -0.2509482502937317, -0.5576606392860413, 1.0829260349273682, 0.23429927229881287, -0.16740551590919495, 0.48417186737060547, -0.780951201915741, -0.32806363701820374, 0.8595568537712097, 0.9452213644981384, 0.8462717533111572, 0.5990375280380249, -0.026056615635752678, 0.6651373505592346, 0.13737592101097107, 0.231565460562706, 0.6353987455368042, 0.6206921935081482, -0.1116056963801384, -0.11791554093360901, -0.017694562673568726, 1.0896238088607788, -0.6276230812072754, -0.7735021114349365, 0.6529324054718018, 0.2275761067867279, -0.39490675926208496, 0.7066934704780579, 1.0270916223526, 0.011621570214629173, 0.5247586965560913, -0.22736132144927979, 0.4594017267227173, -0.5900498032569885, -0.44951799511909485, 0.17566019296646118, -0.08498223125934601, -0.2019275724887848, 0.27508607506752014, -1.1238292455673218, -0.5164960026741028, -0.3394145965576172, 0.3218405842781067, 0.009851791895925999, 0.5136619210243225, 0.8623232245445251, 1.0431004762649536, 1.0037329196929932, -0.1820923537015915, -0.9904845952987671, -0.765700101852417, -0.8077391386032104, -0.11798540502786636, -0.7826592922210693, -0.15646208822727203, 0.09859967976808548, -0.003738459199666977, -0.5279163718223572]}, "authors": [{"authorId": "2150704442", "name": "Dominik Wagner"}, {"authorId": "2307032304", "name": "Seanie Lee"}, {"authorId": "2143242504", "name": "Ilja Baumann"}, {"authorId": "2141960555", "name": "Philipp Seeberger"}, {"authorId": "1767795", "name": "K. Riedhammer"}, {"authorId": "1795810", "name": "T. Bocklet"}], "references": [{"paperId": "b842b83a7ff5dff8e3b83915d8c15423b6085728", "title": "Gemma: Open Models Based on Gemini Research and Technology"}, {"paperId": "20025b2ef0cececcbc646a66093468da0c43769b", "title": "GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "0cee098244c9978032702862a43a09f468f691a4", "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding"}, {"paperId": "1c0a0ec50a639efe9569d0c57f73c8e1b47acbcd", "title": "Multi-Candidate Speculative Decoding"}, {"paperId": "d334af8f71c6172bbd22c8b365cba3932e997264", "title": "Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling"}, {"paperId": "56767c18bb5aaa2b6377624168bed1b6dcc4b94d", "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "7368c3cdf7cbed194e96dc4da53ed61f185e3d82", "title": "Replacing softmax with ReLU in Vision Transformers"}, {"paperId": "8df524e0c50903d0b2c4be338081906d13ea42af", "title": "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding"}, {"paperId": "43e624ddeed82df944a6cae0dedec3372438e243", "title": "Accelerating LLM Inference with Staged Speculative Decoding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "d193675b92fbfbf22ed82fda35cd2e73587e33bd", "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing"}, {"paperId": "f0c31511134abdd23f990310e8a2f2eb3a629b62", "title": "SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "52045d4d4ae305aebb9e92fbbcf23104242c4d31", "title": "A Study on ReLU and Softmax in Transformer"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "218c5c69f3cf0c158e9b6af239a2cc62a688c6de", "title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation"}, {"paperId": "00754af9cd0eaf613a99b13fb19e422b09beeee4", "title": "Robust Training of Neural Networks using Scale Invariant Architectures"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "8a0a7170977cf5c94d9079b351562077b78df87a", "title": "A White Paper on Neural Network Quantization"}, {"paperId": "4fffa5245d3972077c83614c2a08a47cb578631e", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510", "title": "Infinite attention: NNGP and NTK for deep attention networks"}, {"paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "title": "Training with Quantization Noise for Extreme Model Compression"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "63a71de0dafc90910e37a2b07169ff486d9b5fe5", "title": "Common Voice: A Massively-Multilingual Speech Corpus"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "7145a7f391c71efc6f6c0fc82197cf93adcbf63a", "title": "Hardware-Aware Softmax Approximation for Deep Neural Networks"}, {"paperId": "5e04881e91bff952d102d967c4ffb498ec30d4af", "title": "Blockwise Parallel Decoding for Deep Autoregressive Models"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "0e5d529befc3ca2e3e3371a0c39dc05731c1d5b7", "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking"}, {"paperId": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "title": "Efficient softmax approximation for GPUs"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "34038d9424ce602d7ac917a4e582d977725d4393", "title": "Librispeech: An ASR corpus based on public domain audio books"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "d7011869ba77b81f2b2504494ced6fd068a43e9b", "title": "Professional CUDA C Programming"}, {"paperId": "1e0b8416b9d2afb9b1ef87557958ef964cb4472b", "title": "TED-LIUM: an Automatic Speech Recognition dedicated corpus"}, {"paperId": "41bff2e236e73c5c9b21f8660856f58bba46aaf5", "title": "Program optimization space pruning for a multithreaded gpu"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "318def48f4414636555d44f52da8c0bab16a46bc", "title": "The cache performance and optimizations of blocked algorithms"}, {"paperId": "3607b92705cd1fba04eaf295f57be4b085791063", "title": "Dialogue Distillery: Crafting Interpolable, Interpretable, and Introspectable Dialogue from LLMs"}, {"paperId": "6ab59a2e4c10f6797e1a3f111644e32a8e092567", "title": "Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": null, "title": "Trans-formers: State-of-the-art natural language processing"}, {"paperId": null, "title": "NVIDIA A100 tensor core GPU architecture. Technical report,"}, {"paperId": "c55dd503164c8aaf67a976d1eaad7507ec80ecea", "title": "SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks"}, {"paperId": null, "title": "Attention is all"}, {"paperId": "6a07515741593b34b9a60db857703b6255b76122", "title": "What will you need"}, {"paperId": null, "title": "Optimizing parallel reduction in cuda"}, {"paperId": "830ccb44084d9d6cdcb70d623df5012ae4835142", "title": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"}, {"paperId": null, "title": "2023a. Qwen technical report"}, {"paperId": null, "title": "2023. The framework tax: Disparities between inference efficiency in NLP research and deployment"}, {"paperId": null, "title": "Distilling BERT for natural language understanding"}, {"paperId": null, "title": "2023. PaSS: Parallel speculative sampling"}, {"paperId": null, "title": "2024. The un-reasonable ineffectiveness of the deeper layers"}, {"paperId": null, "title": "2024. Recurrent drafter for fast speculative decoding in large language models"}, {"paperId": null, "title": "2023b. Transformers as statisticians: Provable in-context learning with in-context algo-rithm selection"}, {"paperId": null, "title": "2023. Efficiently scaling transformer inference"}, {"paperId": null, "title": "2022. Transformer quality in linear time"}, {"paperId": null, "title": "2023b. Approximate softmax functions for energy-efficient deep neural networks"}]}