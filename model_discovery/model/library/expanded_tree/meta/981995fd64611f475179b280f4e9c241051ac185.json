{"paperId": "981995fd64611f475179b280f4e9c241051ac185", "abstract": "Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named \u201cknowledge inheritance\u201d (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs\u2019 pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2021, "citationCount": 40, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://aclanthology.org/2022.naacl-main.288.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "A pre-training framework named \u201cknowledge inheritance\u201d (KI) is introduced and how could knowledge distillation serve as auxiliary supervision during pre- training to efficiently learn larger PLMs is explored, demonstrating the superiority of KI in training efficiency."}, "embedding": {"model": "specter_v2", "vector": [0.0428202748298645, 0.7015677094459534, -0.355396568775177, -0.08600625395774841, 0.009288039989769459, -0.13006314635276794, 0.7103222608566284, -0.45057734847068787, -0.7112419009208679, 0.17629772424697876, 0.4746020436286926, -0.2202823907136917, 0.20471379160881042, 0.1465574949979782, -0.3273839056491852, 0.04689725488424301, -0.8871589303016663, 0.5271801352500916, -0.06639990210533142, -0.6166156530380249, -0.7766829133033752, -0.827036440372467, -0.6738113164901733, -0.09788425266742706, 0.7788718342781067, 0.22922253608703613, 0.6200803518295288, 0.7626779079437256, -0.4242611527442932, 0.35625913739204407, 0.3818126618862152, -0.5536208152770996, 0.25160980224609375, -0.15746450424194336, -0.4781969487667084, -0.0013074168236926198, 0.16918815672397614, -0.5904012322425842, -0.5679632425308228, 0.7916812300682068, -0.21650268137454987, 0.34596702456474304, 0.3806234300136566, -0.6768375039100647, -0.6040464639663696, 0.881019115447998, 0.8925796151161194, 0.5020414590835571, -0.5467877388000488, -0.5566193461418152, 1.0000078678131104, -1.2039844989776611, 0.26729580760002136, 1.4364286661148071, 0.39778512716293335, 0.7387187480926514, -0.35138410329818726, -0.9165213108062744, 0.7747260332107544, -0.022434143349528313, -1.0555297136306763, -0.2946062684059143, 0.004851509351283312, -0.0851326584815979, 2.082988977432251, -0.6547607183456421, -0.12224479764699936, 0.3840954005718231, -0.32109174132347107, 1.4144684076309204, -0.34989821910858154, -0.8626020550727844, -0.6229804754257202, 0.5843786001205444, -0.15128760039806366, 1.0010900497436523, -0.45213061571121216, 0.24564944207668304, -0.8503469228744507, 0.04972660914063454, 0.37084829807281494, -0.4093174934387207, -0.17051275074481964, -0.04280756041407585, -0.343437522649765, 0.8237824440002441, 0.35577574372291565, 0.7856786847114563, -0.029034949839115143, 0.34004098176956177, 0.36538755893707275, 0.7524633407592773, -0.10370205342769623, 0.5684524178504944, -0.6591977477073669, 0.5328896045684814, -0.7504362463951111, 0.1122933104634285, 0.13477930426597595, 0.7389588952064514, 0.06386924535036087, 0.3996340036392212, -0.6434086561203003, 0.5099448561668396, 1.335578203201294, 0.14802344143390656, 0.7589623332023621, -0.8545365333557129, 0.4382936656475067, -0.5503427386283875, 0.07907789945602417, -0.5162504315376282, -0.17168661952018738, -0.38474783301353455, -0.8209084868431091, -1.4858087301254272, -0.5471996665000916, -0.22769780457019806, -0.7246835231781006, 1.3071495294570923, -0.07274473458528519, 0.37406104803085327, 0.4106616973876953, 0.29413560032844543, 0.7596254348754883, 0.833286702632904, 0.2152268886566162, -0.18849124014377594, 0.8449963331222534, -1.0527291297912598, -0.6431267857551575, -1.209690809249878, 0.5936790108680725, 0.038214776664972305, 0.06506449729204178, -0.23980668187141418, -1.046066164970398, -0.9565855264663696, -0.9423168301582336, -0.050342973321676254, -0.7564679384231567, 0.43883761763572693, 1.0455830097198486, 0.34944280982017517, -0.9589784741401672, 0.5518897175788879, 0.08271926641464233, -0.05165809020400047, 0.20087437331676483, 0.3610667288303375, 0.23956774175167084, -0.5098641514778137, -1.8265297412872314, 0.6092509627342224, 0.9128656387329102, -0.44907647371292114, -0.47491732239723206, -0.5243732333183289, -1.0318825244903564, -0.19126112759113312, 0.44035395979881287, -0.8337279558181763, 1.409712791442871, -0.49894633889198303, -1.8049352169036865, 0.6164419651031494, -0.223062202334404, -0.04695012420415878, 0.21200759708881378, -0.09182526916265488, -0.9270291924476624, -0.3945431411266327, -0.47987163066864014, 0.788763701915741, 0.49791839718818665, -0.1069083958864212, -0.2574349045753479, 0.40474843978881836, 0.06515765935182571, 0.27289554476737976, -0.5264791250228882, 0.7419254183769226, -0.6399115920066833, -0.24333862960338593, 0.15766388177871704, 0.5420689582824707, -0.07464221119880676, -0.4470522105693817, -0.20574012398719788, -1.065321922302246, 0.7155378460884094, -0.18284040689468384, 1.0400091409683228, -0.9424217939376831, -0.2985334098339081, -0.09860765933990479, -0.2832554578781128, 0.23781739175319672, -1.0495589971542358, 0.6205994486808777, 0.0227663591504097, 0.07275582104921341, -0.1795588582754135, -0.947100043296814, 0.1779920905828476, -0.024710780009627342, -0.6188964247703552, -0.5633478164672852, 0.30423954129219055, 1.1620360612869263, -1.1087570190429688, 0.09151572734117508, -0.19990874826908112, 0.654136598110199, -1.2239290475845337, 1.2959469556808472, -0.7751468420028687, 0.4638254642486572, 0.10555876791477203, -0.24433620274066925, -0.04578808695077896, -0.33089956641197205, 0.46661245822906494, -0.3073403835296631, -0.05166741460561752, 0.4368550181388855, -0.6673033833503723, 1.5954033136367798, -0.6448493599891663, 0.6182640194892883, 0.00634609954431653, -0.805276095867157, 0.13469736278057098, 0.6827996969223022, -0.13628919422626495, -0.14611081779003143, 0.2045212835073471, 0.6313005685806274, -0.35328784584999084, 0.3352709710597992, 0.751572847366333, 0.38929522037506104, -0.06809201091527939, 0.04011692479252815, 0.8277209401130676, -0.22619062662124634, 0.6127269864082336, 0.2681492567062378, 0.319701224565506, 0.27077344059944153, 0.3086850941181183, 0.06783358007669449, 0.8143061399459839, -0.751429557800293, -0.022351238876581192, 0.32581302523612976, 0.8459369540214539, 0.31823116540908813, -0.17503923177719116, -0.3239487111568451, 0.1651700884103775, -0.05485402047634125, 0.5130181312561035, 1.9873255491256714, -0.22813713550567627, -0.11101724952459335, -0.6143724322319031, -0.2458479106426239, -0.15436144173145294, 0.36524060368537903, -0.22946715354919434, -0.2027350217103958, -0.6281949877738953, -1.1229718923568726, 0.6823527216911316, 0.2018676996231079, 1.3286162614822388, -0.3320176601409912, 0.06530625373125076, 0.03535546734929085, 0.08340538293123245, -0.6878750324249268, -0.5664896368980408, 0.37671464681625366, -0.5540819764137268, -0.142196923494339, -0.22746841609477997, -0.42899399995803833, 0.3697851300239563, -0.7689597010612488, 0.976923406124115, -0.5914350748062134, -0.06420959532260895, 0.16146928071975708, 0.7829534411430359, -0.544880747795105, -0.9731340408325195, 0.16476884484291077, 0.41789695620536804, -0.11151000112295151, 0.30548909306526184, 0.4344552755355835, 0.20411038398742676, 0.2871909737586975, -0.3241039514541626, 0.27860069274902344, 0.20259322226047516, 0.022314786911010742, 0.8194618225097656, 0.19743461906909943, 0.44221389293670654, -1.4109998941421509, 0.7349235415458679, 0.1554940789937973, -0.4229261875152588, 0.4336026906967163, -0.4452711343765259, -0.3135947585105896, 0.6010254621505737, -0.7368769645690918, -0.7964728474617004, -1.0585834980010986, 0.2347462773323059, -0.07142122834920883, -0.11976900696754456, 0.5190802812576294, -0.003630421357229352, 0.22178232669830322, 0.14374250173568726, 0.6213609576225281, 0.20284777879714966, -0.4424838423728943, 0.9665584564208984, -0.7679687738418579, 0.5271172523498535, 0.23434078693389893, 0.4223102927207947, -0.437847763299942, -0.6119096279144287, -0.7564194202423096, -0.48436659574508667, -0.2612146735191345, -0.40456634759902954, 0.09395850449800491, 0.08029589802026749, -0.8473671078681946, -0.25214022397994995, 0.037836525589227676, -0.9801084995269775, -0.3934001922607422, 0.21991956233978271, -0.17651359736919403, 0.010477393865585327, -1.2463817596435547, -1.076116919517517, -0.2139965295791626, -0.4164867103099823, -0.9840784072875977, 0.033906176686286926, 0.07152876257896423, -0.1410805583000183, -1.0092616081237793, 0.15210093557834625, -0.16626963019371033, 1.3708947896957397, -1.2119890451431274, 1.0932775735855103, -0.04900633916258812, -0.18938331305980682, -0.020445019006729126, 0.34626907110214233, 1.008215308189392, -0.13777904212474823, 0.10288634896278381, -0.9548784494400024, -0.08627111464738846, -0.5703127384185791, -0.4939185380935669, 0.12043143063783646, 0.16658234596252441, 0.6087453365325928, -0.05493929609656334, -0.5391526222229004, 0.48461970686912537, 1.3020142316818237, -0.6143558025360107, -0.0530952550470829, 0.28975310921669006, 0.9583650231361389, 0.11590273678302765, -0.5749605894088745, 0.06563157588243484, 0.6433386206626892, 0.45396688580513, -0.2681041657924652, -0.16974660754203796, -0.23683953285217285, -0.6204400062561035, 0.716999351978302, 2.183455228805542, 0.01750819757580757, 0.011351748369634151, -0.9843760132789612, 0.5481714010238647, -0.9532111883163452, -0.5628041625022888, 1.0087687969207764, 0.6982088088989258, 1.1301509141921997, -0.6887480616569519, -0.6236403584480286, -0.5474384427070618, 0.32530808448791504, 0.1454392373561859, -0.6033485531806946, -0.4986204504966736, -0.02659546583890915, 0.11982806771993637, -0.21136614680290222, 0.8744876384735107, -0.3395797312259674, 0.7655039429664612, 14.346001625061035, 0.8457385897636414, 0.1328010857105255, 0.9100703001022339, 0.25804319977760315, 0.44885092973709106, -0.36858296394348145, -0.29153120517730713, -1.4327905178070068, -0.2854452133178711, 1.15681791305542, 0.1049085333943367, 0.8394719362258911, 0.045404188334941864, -0.2769069969654083, 0.2561066448688507, -0.4178187847137451, 0.5023531913757324, 0.5072614550590515, -1.159286379814148, 0.8136593103408813, 0.03405924141407013, 0.8596901893615723, 0.6832072138786316, 0.6009637713432312, 1.5112476348876953, 0.38089048862457275, -0.2787754237651825, 0.05139327421784401, 0.13354040682315826, 0.6997299790382385, -0.036205440759658813, 0.44775390625, 0.8750054836273193, -0.6691703796386719, -0.25284144282341003, -0.6017377376556396, -1.0060662031173706, 0.314612478017807, 0.057926326990127563, -0.5487646460533142, -0.5065901875495911, -0.5248161554336548, 0.7568325996398926, -0.1752966195344925, 0.13554777204990387, -0.5483092069625854, 0.7778633236885071, -0.04992027208209038, 0.31374210119247437, 0.19796735048294067, 0.2800455391407013, 0.21804781258106232, 0.08428700268268585, 0.01938805729150772, -0.21150830388069153, 0.1465609073638916, 0.2377244234085083, -0.6904338002204895, -0.1864314079284668, -0.11446560174226761, -0.2949032187461853, 0.010097221471369267, 0.4144081771373749, 1.0036242008209229, 0.1450957953929901, -0.7399928569793701, 0.32930248975753784, 0.6161473393440247, 0.444928914308548, -0.24727047979831696, 0.02989988960325718, 0.32896530628204346, -0.5499234795570374, -0.006188861094415188, 0.5933355093002319, 0.23934942483901978, -0.7408039569854736, -0.9310844540596008, -0.5359005928039551, 0.2911970019340515, -0.7665966749191284, -0.7867100834846497, 0.9096764326095581, -0.11456025391817093, -0.2373596876859665, 0.0400543212890625, -0.7475218176841736, 0.006877392530441284, 0.5098437666893005, -1.7181023359298706, -0.7172079682350159, 0.49600428342819214, -0.059404078871011734, -0.6526419520378113, -0.5281692147254944, 1.5289981365203857, 0.2710915207862854, -0.6801357269287109, 0.14706668257713318, 0.6297137141227722, 0.12924006581306458, -0.21266832947731018, -0.7003915309906006, 0.4106750190258026, 0.08052299916744232, 0.06252823024988174, 0.23292087018489838, -0.2253047078847885, 0.17147661745548248, -0.8622062802314758, -0.32020822167396545, 1.0099701881408691, -0.8927184343338013, -0.36821720004081726, -0.5947521924972534, -1.0946637392044067, 0.4828510284423828, 0.9432094097137451, -0.7003838419914246, 0.6494971513748169, 0.42533156275749207, -0.5146259665489197, -0.211009681224823, -0.7855699062347412, 0.09174752980470657, 0.4598599076271057, -0.700297474861145, -0.55642169713974, 0.14283013343811035, 0.3790648281574249, -0.9543417692184448, -0.6115083694458008, -0.03366595879197121, -0.3420381546020508, 0.3305979073047638, 1.2401840686798096, -0.4621529281139374, 0.19566044211387634, 0.7333675622940063, -0.05744084715843201, -1.0942223072052002, -0.1256825476884842, -0.8316471576690674, -0.11403142660856247, 0.21723203361034393, 1.0246870517730713, -0.5826886296272278, -0.2674483060836792, 0.731009840965271, 0.35449570417404175, -0.3234410583972931, -0.5864846110343933, -0.514500617980957, 0.08769568055868149, -0.38032296299934387, 0.27298763394355774, 0.2572591304779053, -0.10059336572885513, 0.5081323981285095, 0.5903641581535339, 0.7998552322387695, -0.4176134765148163, -0.843320906162262, 0.5650408864021301, 0.009571495465934277, -0.5066290497779846, -0.5665816068649292, -0.1481146663427353, -1.666980504989624, 0.29813849925994873, -1.6644301414489746, 0.1370793879032135, -1.1516057252883911, -0.2960849702358246, -0.013699840754270554, -0.39823418855667114, 0.21323734521865845, 0.1230577602982521, -0.3609297275543213, -0.5265275239944458, -0.50730961561203, -0.4739719033241272, 1.1287853717803955, 1.0874276161193848, -0.5686691999435425, -0.1276518553495407, -0.22941860556602478, 0.17075100541114807, 0.5220795273780823, 0.5844787955284119, -0.23693929612636566, -1.2937226295471191, -1.7920048236846924, 0.39741626381874084, -0.5776353478431702, -0.29095882177352905, -0.4556379020214081, 0.6716525554656982, 0.2187613695859909, -0.4007858335971832, 0.33047062158584595, 0.31394246220588684, -0.8641883134841919, -0.33949363231658936, 0.2368827909231186, -0.4114841818809509, -0.15548038482666016, 0.48072439432144165, -0.6307146549224854, -0.5632333159446716, 0.5524076223373413, 0.005968424957245588, -1.06378972530365, -1.1520005464553833, 0.3630625903606415, -0.5622047781944275, 0.4722082316875458, -0.4003036320209503, 0.192765012383461, -1.298944354057312, -0.26731523871421814, 0.113014355301857, 0.6223973631858826, -0.7601123452186584, 0.7702613472938538, 0.3222675919532776, -1.2375130653381348, -0.25098878145217896, 0.6500786542892456, -0.11892370879650116, 0.07249666750431061, 0.5590812563896179, 0.46936139464378357, -0.24379141628742218, 0.7487061023712158, 0.3778182864189148, 0.40834662318229675, -0.5767562389373779, -0.32720863819122314, 1.0359725952148438, -0.3842886686325073, -0.18923236429691315, 1.549778699874878, -0.2467801868915558, -1.55977201461792, 0.39341169595718384, -0.9737208485603333, -0.48210814595222473, -0.4380764961242676, 0.6552574634552002, -0.05939064174890518, -0.3303358256816864, -0.22964218258857727, -0.4459867477416992, 0.25028082728385925, -0.06915063410997391, -0.5010151863098145, 0.491510272026062, -0.3919965624809265, -0.40280038118362427, 0.6922685503959656, 0.827448308467865, -0.7577776312828064, -0.7436940670013428, -0.7420201301574707, -0.41635850071907043, -0.009479925036430359, 0.5825925469398499, -0.8875887393951416, -0.6507529020309448, 0.7390478253364563, 0.3873274624347687, 0.052658382803201675, 0.2937126159667969, -0.04391228407621384, 0.17822091281414032, 1.0811179876327515, 0.04099578410387039, -0.8857324123382568, -0.5036324858665466, 1.5289822816848755, 1.385842204093933, -1.2364274263381958, 0.041656848043203354, -0.17396166920661926, -0.8260371088981628, 0.6366248726844788, 0.4911865293979645, 0.44224274158477783, 0.9801260828971863, -0.5557782053947449, 0.2724381685256958, 0.31573837995529175, -1.1370972394943237, -0.28073152899742126, 1.0923867225646973, 0.8871015310287476, 0.9961442351341248, 0.3580096364021301, 0.28758347034454346, 0.9872035980224609, -0.10671263188123703, 0.2807360887527466, 0.17755548655986786, 0.3769869804382324, -0.5576316714286804, -0.058295778930187225, 0.24308674037456512, 0.3666740357875824, -0.217986598610878, -0.649917483329773, 0.07449907809495926, 0.7803956866264343, 0.6585378050804138, 0.3481421172618866, 0.36443275213241577, 0.2382916957139969, 0.6316147446632385, 0.5957171320915222, 0.7595477104187012, -0.7264544367790222, -0.14491528272628784, -0.39766719937324524, -0.4865414798259735, 0.12646803259849548, -0.5093323588371277, -0.04210228472948074, -0.3201710879802704, 0.07011774927377701, 0.29128792881965637, 0.0666695386171341, 0.3966498076915741, 1.2429828643798828, 0.49138343334198, 0.5138666033744812, -0.44144681096076965, -0.10611409693956375, -0.3869182765483856, -1.1027934551239014, 0.009289361536502838, -0.6997930407524109, -0.3198091387748718, -0.24887579679489136, -0.11247043311595917, 0.025222528725862503]}, "authors": [{"authorId": "50625437", "name": "Yujia Qin"}, {"authorId": "2149202150", "name": "Yankai Lin"}, {"authorId": "2106388389", "name": "Jing Yi"}, {"authorId": "2107983722", "name": "Jiajie Zhang"}, {"authorId": "145760425", "name": "Xu Han"}, {"authorId": "2148904862", "name": "Zhengyan Zhang"}, {"authorId": "48576745", "name": "Yusheng Su"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "144326610", "name": "Peng Li"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "48128428", "name": "Jie Zhou"}], "references": [{"paperId": "bc7984bfcfae537dbe633eeeb8d69c42a994c724", "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data"}, {"paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d", "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"}, {"paperId": "7a49beff86a855f237f96ae3f0aefc9780cb31be", "title": "bert2BERT: Towards Reusable Pretrained Language Models"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "2365410a710b421b2295cdca0074946cb50bb1d4", "title": "Are Pretrained Convolutions Better than Pretrained Transformers?"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "1c38fb9ee7a86755c184a37bcb654786150038c5", "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b", "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "0abb08c4ec5feab4cdd82c471866dd4395c573ce", "title": "Contrastive Distillation on Intermediate Representations for Language Model Compression"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "7ea2a78a8d8a6327bd13aa4f2d9ace9231bd9662", "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "8771679aac0e90371340bd8c657317f5be113e81", "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"}, {"paperId": "75352cc69a29bd5fc411e0e79737cb96b6309161", "title": "Distilling Knowledge Learned in BERT for Text Generation"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "ac713aebdcc06f15f8ea61e1140bb360341fdf27", "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "6885dc17f1607f1947721ef4c430f1eda22f1228", "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints"}, {"paperId": "145b8b5d99a2beba6029418ca043585b90138d12", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "3c6dca9041f54583aeab60587c9e6e9272104dc1", "title": "Reducing BERT Pre-Training Time from 3 Days to 76 Minutes"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "1b24b7b4ac2427d20ab60c8451563eb8d99caf9c", "title": "Multilingual Neural Machine Translation with Knowledge Distillation"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "16be95fd3f9b635e9ede5812cc223deebf0142bc", "title": "Measuring the Evolution of a Scientific Field through Citation Frames"}, {"paperId": "e2c72b79c2f3ca6b980c540b821323467456ad4a", "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students"}, {"paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1", "title": "Born Again Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "575b0b9e3996097962510191e3da186d7a32d56d", "title": "ChemProt-3.0: a global chemical biology diseases mapping"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "56010a55d49ac1f42355538f494427fd22402be1", "title": "Exploring the Limits"}, {"paperId": "eee62b0ac67fb8d9a0b4911acc0c68eeb5f47989", "title": "The transformer."}, {"paperId": "5c5751d45e298cea054f32b392c12c61027d2fe7", "title": "S2ORC: The Semantic Scholar Open Research Corpus"}, {"paperId": "93e1af1fcf7d028cefcc3d3953a3ca8ed2e1f3f5", "title": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2019. BERT: Pre-training"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "8eb9931452cac79ed4a723500c1fa86c03599848", "title": "Training Deep Neural Networks"}, {"paperId": null, "title": "of the 36th International Conference on Machine Learning,"}, {"paperId": null, "title": "Electra : Pre - training text encoders as discriminators rather than generators"}]}