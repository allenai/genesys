{"paperId": "4bf921e7d4b11e8a546129188c8c5358d3d30caf", "abstract": "Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is\"prompting\"- or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, namely for Kinyarwanda, Hausa, and Luganda. We consider three methods: few-shot prompting (prompt), language-adaptive fine-tuning (LAFT), and neural machine translation (translate), and evaluate on abstractive summarization, multi-class topic classification, and named-entity recognition. Although LAFT carries the greatest compute cost and intuitively should lead to the best results, our experiments exhibit that LAFT is only occasionally the optimal choice for adapting PLMs for prompting. Rather, the translate and prompt settings are a compute-efficient and cost-effective method of few-shot prompting for the selected low-resource languages. We find that the results are task and language dependent but find that the prompting method is the best on average across all tasks and languages. Results show that the prompt setting performs better than both translating and LAFT with statistical significance for all shots when aggregated across all tasks and languages.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, namely for Kinyarwanda, Hausa, and Luganda are considered, and three methods are considered: few-shot prompting, language-adaptive fine-tuning (LAFT), and neural machine translation (translate)."}, "embedding": {"model": "specter_v2", "vector": [0.1287180632352829, 0.3586464822292328, -0.14664746820926666, -0.5512599945068359, -0.8345393538475037, -0.44634392857551575, 1.0914140939712524, -0.11902637034654617, -0.5317903161048889, 0.20538650453090668, 0.8524297475814819, 0.02042733132839203, 0.2654763460159302, 0.3704117238521576, -0.21632517874240875, 0.247419074177742, -0.7570961713790894, 0.3970581293106079, -0.19100984930992126, -0.45135238766670227, -0.43868324160575867, -0.7608394622802734, -0.3646257221698761, 0.16838476061820984, 0.38112035393714905, -0.1169138103723526, 0.09981000423431396, 0.8056720495223999, -0.43903130292892456, 0.24570176005363464, 0.22829264402389526, -0.28397276997566223, 0.392390638589859, -0.3609793186187744, -0.5787683129310608, -0.08778564631938934, 0.6044418811798096, -0.8540135025978088, -0.22993671894073486, 0.2719971239566803, -0.1807120144367218, 0.34609586000442505, 0.5165175199508667, -0.49939054250717163, -0.46889859437942505, 0.8356382846832275, 0.9402746558189392, 0.7915472984313965, 0.09325841814279556, -0.31957679986953735, 1.1607797145843506, -1.2596453428268433, 0.49986782670021057, 1.4467461109161377, 0.3727458417415619, 0.7375515103340149, -0.10437801480293274, -0.6479010581970215, 0.8229079246520996, -0.05134307220578194, -0.2925023138523102, -0.7690973877906799, -0.11346305906772614, -0.15572629868984222, 1.878216028213501, -0.4311794936656952, -0.3983282744884491, 0.5929596424102783, -0.12439754605293274, 1.266667127609253, -0.1360664963722229, -0.6774824261665344, -0.341456800699234, 0.05170745030045509, 0.5590701103210449, 0.6295257210731506, -0.6531459093093872, 0.26076486706733704, -0.7632867097854614, -0.12287931889295578, 0.23264865577220917, -0.04330271854996681, -0.14089335501194, 0.18774756789207458, -0.7102609276771545, 0.62115877866745, 0.46267348527908325, 0.8415237665176392, -0.3691596984863281, 0.32761088013648987, 0.7911612391471863, 0.5884138345718384, 0.3678515553474426, 0.7233704328536987, -0.4857591390609741, 0.3492392599582672, -0.5354201197624207, 0.17779438197612762, 0.19971856474876404, 0.8570587038993835, -0.11066553741693497, -0.06795012950897217, -1.0370205640792847, 0.30885323882102966, 1.0271226167678833, -0.1089983880519867, 0.19952774047851562, -0.9379685521125793, 0.23925477266311646, -0.7373105883598328, 0.4011557996273041, -0.5771016478538513, -0.38317519426345825, 0.1316390484571457, -0.672359049320221, -1.6307852268218994, -0.10873354226350784, -0.2546057105064392, -0.4936937689781189, 0.9132975339889526, -0.12355449050664902, 0.17891977727413177, 0.40971487760543823, 0.5604035258293152, 0.8084856271743774, 1.0936936140060425, 0.6565135717391968, -0.5041214227676392, 0.6873273849487305, -0.8490861058235168, -0.84378582239151, -1.114113688468933, 1.2293367385864258, -0.23338083922863007, 0.33156976103782654, -0.5502462387084961, -0.8359046578407288, -0.5228725671768188, -0.7294368147850037, -0.142433300614357, -0.601634681224823, 0.34656891226768494, 0.9865695238113403, 0.1730789691209793, -0.6873344779014587, 0.4084705114364624, -0.024567553773522377, -0.866420567035675, 0.10147860646247864, -0.1975993663072586, 0.025322144851088524, -0.7812936902046204, -1.2640483379364014, 0.6448453068733215, 0.04621003568172455, -0.4934285283088684, -0.2362557202577591, -0.699203610420227, -1.0877307653427124, -0.2070043385028839, 0.670522153377533, -0.6603567004203796, 1.6670278310775757, -0.17429448664188385, -1.5286675691604614, 0.5527096390724182, -0.2340162992477417, 0.27047351002693176, 0.14178729057312012, -0.34824663400650024, -0.6032539010047913, -0.46582069993019104, 0.08419565856456757, 0.6162256598472595, -0.020025398582220078, 0.07023236155509949, -0.1275811791419983, 0.24957598745822906, -0.027446044608950615, 0.1715417504310608, -0.1611940860748291, 0.9478676319122314, 0.04951837658882141, 0.18270061910152435, -0.03875989094376564, 1.0730317831039429, -0.022561078891158104, -0.4306214451789856, -0.3324531614780426, -1.487594723701477, 0.7607569694519043, -0.10130870342254639, 0.73809415102005, -0.8171399831771851, -0.6867719292640686, -0.9312136173248291, -0.47389090061187744, 0.0741833969950676, -0.9141120910644531, 0.8463759422302246, 0.004576366860419512, 0.30731144547462463, -0.20713230967521667, -1.0612845420837402, -0.09076307713985443, -0.19543474912643433, -0.6375473141670227, -0.15674252808094025, 0.4454752206802368, 1.2472751140594482, -0.977277398109436, 0.24179963767528534, -0.09302514791488647, -0.11597425490617752, -0.8520352840423584, 1.4481748342514038, -0.4151707887649536, 0.2788832187652588, -0.3672715127468109, -0.3133900463581085, -0.04616841301321983, -0.17053037881851196, 0.2300853431224823, -0.43780311942100525, -0.32923343777656555, 0.500414252281189, -0.2785029709339142, 1.5287708044052124, -0.23207099735736847, 0.3645791709423065, -0.0698164626955986, -0.8884062767028809, 0.20024226605892181, 0.7009601593017578, -0.41938072443008423, -0.5742784142494202, 0.18461409211158752, 0.4797765612602234, -0.7902796268463135, 0.07527552545070648, 0.8232131004333496, 0.5735597610473633, -0.8501511812210083, 0.3082558810710907, 0.8532755970954895, -0.3057536482810974, 0.41114065051078796, 0.7665404677391052, 0.4991225004196167, 0.3019915819168091, 0.6474959254264832, -0.1778862029314041, 0.30482006072998047, -0.32218995690345764, 0.007922236807644367, 0.3197599947452545, 0.5065385103225708, 0.5866097211837769, 0.26232466101646423, -0.9065526127815247, -0.4806964695453644, 0.24718642234802246, 0.688724160194397, 1.7945526838302612, -0.10614924132823944, -0.20710846781730652, -0.9626836180686951, -0.6052197813987732, -0.6621618866920471, 0.39931562542915344, -0.23570361733436584, -0.17089715600013733, -0.7312970161437988, -0.5625948309898376, 0.5446334481239319, 0.320129930973053, 1.0726909637451172, -0.3182055950164795, -0.2830323874950409, 0.09993988275527954, 0.005668924655765295, -0.795557975769043, -0.9776021242141724, 0.27662450075149536, -0.4569079279899597, -0.09751677513122559, -0.3347958028316498, -0.0661671832203865, -0.0006737594958394766, -0.41065943241119385, 0.9371486902236938, -0.7758411169052124, 0.05640813708305359, 0.6204184293746948, 0.7168344855308533, -0.4791146218776703, -0.8957040905952454, 0.24718374013900757, 0.40578073263168335, -0.18087641894817352, 0.4549403488636017, 0.9981372952461243, 0.4822708070278168, 0.32194623351097107, -0.07640814036130905, 0.44643980264663696, -0.03164316713809967, 0.09592458605766296, 0.49015292525291443, -0.8572319149971008, 0.6469833850860596, -1.374371886253357, 1.203855037689209, -0.060914888978004456, -0.350335955619812, 0.6207815408706665, -0.30250391364097595, -0.6767356991767883, 0.6154884099960327, -0.6794816255569458, -0.6426873803138733, -0.9514538645744324, 0.6068601608276367, 0.20911981165409088, -0.7979429960250854, 0.4304497241973877, 0.14851976931095123, 0.6663913726806641, 0.5254168510437012, 0.21594908833503723, 0.15431219339370728, -0.41160786151885986, 0.862019956111908, -0.4493615925312042, 0.6746813654899597, 0.2339969426393509, 0.0706292986869812, -0.4021422564983368, -0.36679989099502563, -0.631334125995636, -0.5480956435203552, -0.5700415968894958, -0.5511502027511597, -0.38340458273887634, 0.19120383262634277, -0.5491952896118164, -0.27052003145217896, 0.22564609348773956, -1.5089706182479858, -0.48709824681282043, 0.05147264897823334, -0.2334425300359726, -0.22086992859840393, -1.2397373914718628, -1.0262398719787598, -0.1302974969148636, -0.5589679479598999, -0.5367007851600647, 0.40173202753067017, 0.3692079484462738, -0.4375711679458618, -0.5480005145072937, 0.4560222625732422, -0.5024020671844482, 1.1003730297088623, -0.6918087005615234, 0.8199218511581421, -0.39491400122642517, -0.10366935282945633, -0.25728505849838257, 0.2844700217247009, 0.3604814112186432, 0.08156385272741318, -0.1309674084186554, -0.7994532585144043, 0.2289123684167862, -0.30549025535583496, -0.6203333139419556, -0.2766743302345276, 0.412032812833786, -0.18945758044719696, -0.03499675542116165, -0.47588303685188293, -0.034489791840314865, 1.005629062652588, -0.517332136631012, -0.32504305243492126, -0.004396368749439716, 0.8849192261695862, 0.6618207693099976, 0.18469126522541046, 0.03915997967123985, 0.5777337551116943, 0.5028517246246338, -0.43782469630241394, -0.17501555383205414, -0.02639211155474186, -0.516775369644165, 0.8703925609588623, 1.7372347116470337, 0.2999427020549774, 0.08986715227365494, -1.1688681840896606, 0.5721281170845032, -1.4582046270370483, -0.4410168528556824, 0.5719106793403625, 0.4117276668548584, 0.5024532079696655, -0.9784475564956665, -0.45039433240890503, -0.9929115772247314, 0.490793377161026, 0.24881120026111603, -0.3730655610561371, -0.8260178565979004, 0.09931039065122604, -0.2306288480758667, -0.3290162980556488, 0.6489587426185608, -0.35629767179489136, 0.903883695602417, 14.64033031463623, 0.7367334365844727, -0.024984057992696762, 0.9341636896133423, 0.7017399668693542, 0.2719876170158386, -0.12815743684768677, -0.28542622923851013, -1.1677460670471191, -0.6040992140769958, 1.493653655052185, -0.32046225666999817, 0.5349540710449219, 0.022787179797887802, 0.16743087768554688, 0.058650411665439606, -0.9527507424354553, 0.5822834968566895, 0.609642744064331, -1.2976998090744019, 0.39242348074913025, 0.059512507170438766, 0.6216835379600525, 0.5453771948814392, 0.2703898549079895, 1.0225828886032104, 0.3162824809551239, -0.15601111948490143, 0.1407833993434906, 0.2617799937725067, 0.7562740445137024, 0.011624650098383427, 0.43428415060043335, 0.708845853805542, -0.455500990152359, -0.28505948185920715, -0.6162866353988647, -1.0099995136260986, 0.6345296502113342, -0.26260048151016235, -0.5600473880767822, -0.24951785802841187, -0.4297725260257721, 0.747444748878479, 0.33970892429351807, 0.4923982620239258, -0.1414693146944046, 0.5744233131408691, -0.046145662665367126, 0.03800368309020996, 0.5022489428520203, 0.40499117970466614, 0.49752354621887207, 0.268641859292984, -0.16720162332057953, 0.5578153133392334, 0.32164138555526733, 0.336479127407074, -0.7692903280258179, 0.3960229456424713, -0.8726425766944885, -0.01502621453255415, 0.015131247229874134, 0.6491478085517883, 0.6099620461463928, -0.17963893711566925, -0.46482494473457336, 0.19822047650814056, 0.6913741230964661, 0.303768515586853, 0.17998841404914856, -0.06428749859333038, 0.11263693124055862, -0.5333835482597351, -0.4331454038619995, 0.3898441195487976, -0.14551764726638794, -0.6291084885597229, -0.9348326325416565, -0.17502765357494354, 0.42195406556129456, -0.8952996134757996, -1.0042424201965332, 0.8934682607650757, 0.21169179677963257, -0.3928278386592865, -0.1838601976633072, -0.41567739844322205, -0.5240116119384766, 0.3777071535587311, -1.0903655290603638, -0.74395352602005, 0.2653847932815552, -0.22899237275123596, 0.11689125001430511, -0.11733115464448929, 1.460942029953003, 0.2296268194913864, -0.6725491881370544, -0.15626098215579987, 0.17797157168388367, -0.21144983172416687, 0.345108300447464, -0.5393947958946228, 0.699738085269928, 0.5164290070533752, -0.3296106159687042, 0.10851427167654037, -0.06713026762008667, 0.15870262682437897, -0.6924017071723938, -0.11339526623487473, 1.1362031698226929, -0.9986023306846619, -0.7699236273765564, -0.831873893737793, -1.302805781364441, 0.3422975242137909, 1.0875734090805054, -0.2579062581062317, 0.35098469257354736, 0.3262489438056946, -0.3966345191001892, -0.2330690622329712, -0.9423785209655762, 0.0072699254378676414, 0.31075629591941833, -0.717400848865509, -0.7105315923690796, 0.2282739281654358, 0.8254328966140747, -0.8864233493804932, -0.5060701370239258, -0.5256884694099426, -0.3240765333175659, 0.48705732822418213, 0.5100650191307068, -0.8368499875068665, 0.27338820695877075, 0.9413646459579468, -0.12481766194105148, -1.2511425018310547, -0.07498051226139069, -0.7853904366493225, 0.2781660854816437, 0.7726922035217285, 0.7316111922264099, -0.4683033525943756, -0.5317000150680542, 1.383141040802002, 0.14835435152053833, -0.1866573989391327, -0.720580518245697, -0.28266414999961853, 0.5090737342834473, -0.25694164633750916, 0.4906182289123535, 0.3549859821796417, -0.05638929829001427, 0.5943240523338318, 0.39963117241859436, 0.6287245154380798, -0.3392016887664795, -0.8935396075248718, 0.6528027653694153, -0.14091376960277557, 0.2702605724334717, -0.25128865242004395, -0.07971798628568649, -1.5390467643737793, 0.09115979075431824, -0.9551555514335632, 0.40757814049720764, -1.1839100122451782, -0.1917889416217804, 0.2870175540447235, 0.05383223667740822, 0.32360610365867615, 0.4821569323539734, -0.6144599914550781, -0.28779199719429016, -0.3967788815498352, -0.6800598502159119, 0.8722813129425049, 0.9036333560943604, -0.6927407383918762, -0.16148512065410614, -0.289106547832489, -0.05555609613656998, 0.13276199996471405, 0.38963019847869873, -0.5804373025894165, -0.4251726269721985, -1.5261566638946533, -0.11983297765254974, 0.08674420416355133, 0.003928148653358221, -0.2831309139728546, 0.8632630705833435, 0.3250541687011719, -0.48632752895355225, 0.040456123650074005, 0.0319400317966938, -0.700772762298584, -0.571114182472229, -0.08315812051296234, -0.9091579914093018, -0.045858755707740784, 0.46981823444366455, -0.6363502740859985, -0.21093837916851044, 0.8565311431884766, -0.10231111943721771, -0.9163806438446045, -0.9632316827774048, 0.3239923119544983, -0.6657312512397766, 0.21988441050052643, -0.19974492490291595, 0.2484901249408722, -1.382668375968933, -0.31663161516189575, 0.2129765748977661, 0.4905683398246765, -0.5707405805587769, 0.8830369114875793, 0.05529482662677765, -1.320744276046753, -0.3832424581050873, -0.014089186675846577, 0.07904147356748581, 0.22653035819530487, 0.7417463660240173, 0.42193543910980225, 0.07027055323123932, 0.8051804304122925, 0.1730911135673523, 0.12212763726711273, -0.5711057782173157, -0.11420327425003052, 0.8904438018798828, -0.3900231122970581, -0.3483608663082123, 0.8854559659957886, -0.16894297301769257, -1.5001245737075806, 0.04749713093042374, -0.9310079216957092, -0.6203159093856812, -0.19638101756572723, 0.8000715374946594, 0.12655282020568848, -0.0859910175204277, -0.29125532507896423, -0.34751972556114197, 0.17425748705863953, -0.3119387626647949, -0.3586523234844208, 0.8078949451446533, -0.49797630310058594, -0.1899469792842865, 0.9027702808380127, 0.7957491278648376, -1.007684588432312, -0.8467114567756653, -0.9386537075042725, -0.2630690634250641, 0.1398434042930603, 0.313277930021286, -0.9558394551277161, -0.1793629378080368, 0.7439367771148682, 0.41072985529899597, 0.1675674021244049, 0.047361988574266434, 0.2437712550163269, 0.09951599687337875, 0.9720613360404968, 0.08897283673286438, -0.5541796088218689, -0.6636525392532349, 1.3113631010055542, 1.4553648233413696, -1.0540236234664917, 0.03370329365134239, 0.020420115441083908, -0.6684300899505615, 0.8726277947425842, 0.6149576902389526, 0.282822847366333, 0.5992411971092224, -0.5967863202095032, 0.5908001661300659, 0.024713369086384773, -1.207095980644226, 0.2621452212333679, 0.7796570062637329, 1.0264413356781006, 0.5381511449813843, 0.5548234581947327, -0.10480570793151855, 1.0387561321258545, 0.3737974762916565, 0.28606095910072327, 0.8333580493927002, 0.23593299090862274, -0.3679030239582062, -0.016360938549041748, 0.10324877500534058, 0.5156914591789246, -0.4435337781906128, -0.35663753747940063, -0.1176721602678299, 0.6225017309188843, -0.28574439883232117, 0.5592868328094482, 0.6599233746528625, 0.5439131259918213, 0.9181499481201172, 0.44376763701438904, 0.16245323419570923, -0.849838376045227, -0.28022047877311707, -0.20348860323429108, -0.4052915871143341, -0.13979986310005188, -0.2836124897003174, -0.7875080108642578, -0.08809035271406174, -0.12737208604812622, 0.2281254231929779, -0.08899053186178207, 0.25242534279823303, 1.541901707649231, 0.7698753476142883, 0.06537551432847977, -0.3884871006011963, -0.66591876745224, -0.4062587320804596, -1.201050877571106, 0.15714608132839203, -0.6624276638031006, -0.19235749542713165, -0.12638293206691742, 0.008329612202942371, -0.19487839937210083]}, "authors": [{"authorId": "2290805626", "name": "Christopher Toukmaji"}], "references": [{"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "title": "Language Models are Multilingual Chain-of-Thought Reasoners"}, {"paperId": "c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6", "title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers"}, {"paperId": "2ac19d63e1adba20473a6d1122c598f81efc3c58", "title": "Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning"}, {"paperId": "42fc019b2668c9d9d984154d4c57f6c6d5a91619", "title": "Language Models are Few-shot Multilingual Learners"}, {"paperId": "769f8b6d73fcb5b5013ae2c1c48b94c801c88ba3", "title": "Discrete and Soft Prompting for Multilingual Models"}, {"paperId": "ecf5618b513aa5c4d5bf62ca251923a188251117", "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9", "title": "How many data points is a prompt worth?"}, {"paperId": "56446cb1da48cbe6e19e5051ed80c3861021e5ba", "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages"}, {"paperId": "1109d62ebd2b29a7dc148bc30dd6cfc803a63dec", "title": "IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP"}, {"paperId": "c360814d32fe45b7db2fcbaebb5d01402aff9b4f", "title": "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "03f22e693a0c00bae8a66a64a2fecb0f11a4b034", "title": "IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"}, {"paperId": "4de3595439ed8e433e19979b49ea9171c01dc846", "title": "Low-resource Languages: A Review of Past Work and Future Challenges"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "495da6f19baa09c6db3697d839e10432cdc25934", "title": "Multilingual Denoising Pre-training for Neural Machine Translation"}, {"paperId": "63a71de0dafc90910e37a2b07169ff486d9b5fe5", "title": "Common Voice: A Massively-Multilingual Speech Corpus"}, {"paperId": "4e96e6d2abc54bc55d5489a9e5a666f273b69410", "title": "Massive vs. Curated Embeddings for Low-Resourced Languages: the Case of Yor\u00f9b\u00e1 and Twi"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "39c40a00248c53cdfa4298cdf9c18a42d4a058f6", "title": "AlBERTo: Modeling Italian Social Media Language with BERT"}, {"paperId": "b61c6405f4de381758e8b52a20313554d68a9d85", "title": "CamemBERT: a Tasty French Language Model"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "1f3e14c58d86b44fcc931cd0bdc0a8dfc7500701", "title": "Analyzing the Forgetting Problem in the Pretrain-Finetuning of Dialogue Response Models"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1", "title": "Large-scale deep unsupervised learning using graphics processors"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "940fc621079ea349109202c7d705461b50d541d8", "title": "Cross-lingual Few-Shot Learning on Unseen Languages"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1", "title": "Few-shot Learning with Multilingual Language Models"}, {"paperId": "b2474a00d7de3373bab934c09acef1994fa82207", "title": "Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "8eb74f94e98006dfce35b8a5de5b6b3cd629efc2", "title": "iNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "OpenAI. 2023."}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "2023. Chain-of-thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2023. A survey on vision transformer"}]}