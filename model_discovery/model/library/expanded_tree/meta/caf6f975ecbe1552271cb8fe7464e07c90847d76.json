{"paperId": "caf6f975ecbe1552271cb8fe7464e07c90847d76", "abstract": "Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduction on average (up to 68%) with negligible loss in inference accuracy.", "venue": "arXiv.org", "year": 2023, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.09262", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A novel embedding technique to find semantically similar inputs to identify computation similarity and a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead are introduced."}, "embedding": {"model": "specter_v2", "vector": [-0.008662349544465542, 0.30865249037742615, -0.8061686754226685, 0.15505482256412506, 0.02849334105849266, 0.04628731310367584, 0.6124373078346252, 0.0971221849322319, -0.42785581946372986, -0.44767826795578003, 0.7399296164512634, -0.1302015483379364, 0.5972207188606262, -0.4183272123336792, -0.27140167355537415, 0.34044981002807617, -0.6770283579826355, 0.3400552570819855, 0.5338283181190491, -0.15613293647766113, 0.37139320373535156, -0.26448675990104675, -1.7488725185394287, 0.44964104890823364, 0.29765447974205017, 1.1754677295684814, -0.3146413266658783, 0.8509485721588135, -0.297188937664032, 0.6045329570770264, 0.5840969681739807, -0.11110822856426239, 0.0032339892350137234, 0.5933388471603394, -0.4893020987510681, -0.6693131327629089, 0.4412091374397278, -0.7949819564819336, -0.4144378900527954, 0.7307326197624207, -0.24734088778495789, 0.2028014212846756, 0.2793422043323517, -1.1236485242843628, -0.12581558525562286, 0.6488944888114929, 0.6768719553947449, 1.0006225109100342, -0.5612340569496155, -0.8192352056503296, 1.238535761833191, -1.6320220232009888, 0.12595432996749878, 1.1211925745010376, 0.10641895234584808, 0.23267649114131927, -0.18935774266719818, -0.44469305872917175, 0.4776754379272461, 0.3070894777774811, -0.8272900581359863, -0.8982858061790466, -0.021786635741591454, 0.16544818878173828, 1.9737071990966797, 0.19327059388160706, -0.11292202770709991, -0.05577041581273079, 0.34800806641578674, 1.2104638814926147, -0.1730709820985794, -0.7436767816543579, 0.24888499081134796, 0.1967843770980835, 0.3842863142490387, 0.964288592338562, -0.26105472445487976, -0.03967180848121643, -1.3366899490356445, -0.3319619596004486, 0.310871958732605, 0.4152797758579254, 0.45537295937538147, -0.3245711624622345, -0.11067704111337662, 0.47564592957496643, 0.259075403213501, 0.5989953875541687, -0.15971238911151886, 1.0102359056472778, 0.6700182557106018, 0.30362412333488464, 0.024015044793486595, 0.18534433841705322, 0.37073731422424316, 0.119630366563797, -0.9838880300521851, 0.03783934935927391, 0.12185734510421753, 0.85399329662323, -0.005057847127318382, 0.4119657576084137, -0.7519416213035583, -0.18824262917041779, 0.8705183267593384, 0.2544400095939636, 0.3501141667366028, -0.606182873249054, 0.17145106196403503, -0.3525562286376953, 0.12682679295539856, -0.6219998598098755, -0.05135437473654747, -0.23777227103710175, -0.7048488855361938, -0.8069359064102173, -0.8246902227401733, 0.5091853737831116, -0.5834434032440186, 0.16933241486549377, -0.35977476835250854, 0.20680420100688934, -0.28359255194664, 0.13153012096881866, 0.6736388802528381, 0.1465190201997757, -0.1126776784658432, 0.33947670459747314, 1.0411769151687622, -1.1898536682128906, -0.40964460372924805, -1.0511326789855957, 0.4763557016849518, -0.23015110194683075, 0.017995154485106468, -0.4620732069015503, -1.4411842823028564, -0.9918114542961121, -0.9421589970588684, 0.12878327071666718, -0.7634787559509277, -0.42570725083351135, 1.1068050861358643, 0.15539932250976562, -1.5813405513763428, 0.6492619514465332, -0.778019905090332, -0.16035649180412292, 0.3576420247554779, 0.5375310778617859, 0.6530383825302124, -0.07966994494199753, -1.024472951889038, 0.15907639265060425, 0.22394651174545288, -0.7226290106773376, -0.47510164976119995, -0.8348547220230103, -1.0265223979949951, 0.39672741293907166, 0.4776359796524048, -0.3605360984802246, 1.0526150465011597, 0.05584462732076645, -0.6499784588813782, 0.5992597341537476, -0.6905636191368103, 0.015298896469175816, 0.05140504986047745, -0.2364533543586731, -0.6860671043395996, -0.17210793495178223, -0.21096478402614594, 0.4389812648296356, 0.8600643277168274, -0.31297412514686584, -0.3803218901157379, -0.1837988942861557, -0.08815392851829529, 0.10462654381990433, -0.8865396976470947, 1.197357177734375, -0.5580631494522095, 0.15293297171592712, 0.32549625635147095, 0.8823838829994202, 0.05242185667157173, 0.18646709620952606, -0.34982332587242126, -0.8089132905006409, 0.815528154373169, 0.3752259612083435, 1.462740421295166, -0.9014830589294434, -0.7663394212722778, 0.1239306777715683, 0.4435502886772156, -0.2243708223104477, -0.08503256738185883, 0.5561504364013672, -0.36552685499191284, 0.3398653566837311, 0.20741762220859528, -0.4435194432735443, 0.09776937961578369, -0.4506996273994446, -0.9781036972999573, -0.6230913400650024, -0.23386046290397644, 1.3100249767303467, -0.9751314520835876, -0.24081426858901978, -0.06495138257741928, 0.15735945105552673, -0.7854702472686768, 1.5598653554916382, -0.37106648087501526, -0.3321360945701599, -0.36493369936943054, 0.1871553659439087, 0.20886436104774475, -0.8578577041625977, 0.6729725003242493, -0.8009040355682373, -0.2133757621049881, 0.6359153389930725, -0.19095979630947113, 0.6762431263923645, -0.48712795972824097, 0.25037962198257446, -0.14564187824726105, -0.6016199588775635, 0.10181152820587158, 0.24059510231018066, 0.13057632744312286, -0.8241110444068909, 0.26689842343330383, 0.23903676867485046, -0.6793687343597412, 0.3153892457485199, 1.1212058067321777, 1.1737943887710571, -0.29363375902175903, 0.4412117898464203, 0.15598489344120026, -0.016750527545809746, 0.12816232442855835, 0.4696993827819824, 1.0708919763565063, 0.29100000858306885, 0.20767223834991455, -0.2723574638366699, 0.3281669020652771, -0.6428439021110535, -0.06054961308836937, 0.6105098724365234, 0.8418228030204773, 0.3148764669895172, 0.32389575242996216, -0.70262211561203, -0.4859311878681183, 0.006767815910279751, 0.41959649324417114, 1.655106782913208, 0.15134690701961517, -0.42885151505470276, -0.730880856513977, -0.300423264503479, -0.17101317644119263, 0.22841592133045197, -0.3003712594509125, -0.44289952516555786, -0.5735489726066589, -0.8893668055534363, 0.7508184313774109, 1.0283831357955933, 1.410794973373413, -0.518126904964447, -0.5592027902603149, -0.5017911791801453, 0.3435731530189514, -0.7893421649932861, -0.3772110939025879, 0.4648658037185669, -0.7820040583610535, 0.2500476837158203, 0.5706015229225159, -0.3898240029811859, 0.3733057975769043, -0.5338079929351807, 1.366542100906372, 0.12578023970127106, -0.04779709130525589, 0.10766139626502991, 0.7385615110397339, -0.32232949137687683, -0.5970085859298706, 0.23450763523578644, 0.015186045318841934, -0.37713202834129333, 0.8140168190002441, 0.1907946765422821, -0.14033567905426025, 0.46587517857551575, -0.4451068639755249, 0.23991204798221588, 0.06858433783054352, -0.1987052857875824, 0.5398046970367432, -0.31849148869514465, -0.1982094943523407, -1.0794081687927246, 0.4029473662376404, -0.028131045401096344, -0.4834569990634918, 0.21823090314865112, -1.0305277109146118, -0.08886229246854782, 0.6505117416381836, -0.7052626013755798, -0.18663544952869415, -1.3586558103561401, 0.42898526787757874, -0.5941169261932373, 0.18355292081832886, -0.1562013030052185, 0.19731183350086212, -0.27159520983695984, 0.0022616235073655844, 0.6706419587135315, 0.1794225126504898, -0.021518895402550697, 0.2712884843349457, -0.8026885390281677, 0.658352255821228, -0.11962728947401047, 0.14633555710315704, 0.19372576475143433, -0.004578235559165478, -0.45847851037979126, -0.18124760687351227, -0.06759584695100784, -0.07891557365655899, 0.05428950861096382, 0.17142421007156372, -0.36269038915634155, -1.1405587196350098, -0.17445744574069977, -0.9147855639457703, -0.5198802947998047, 0.5087923407554626, -0.6787023544311523, 0.1338120698928833, -1.0701544284820557, -1.3189531564712524, -0.4202740788459778, -1.0889350175857544, -1.2135676145553589, 0.2882242500782013, -0.0656983032822609, -0.3187870979309082, -0.6083083748817444, -0.46211281418800354, -0.5584286451339722, 1.4324625730514526, -0.7854082584381104, 0.8997173309326172, 0.02814452163875103, -0.8867256045341492, 0.08537103235721588, -0.20437194406986237, 0.14320895075798035, -0.2436543107032776, -0.17473077774047852, -0.8606389760971069, 0.39699310064315796, -0.2195858508348465, 0.012533430010080338, -0.030478928238153458, -0.14571334421634674, 1.287924885749817, 0.024084627628326416, -0.9018034934997559, 0.11965827643871307, 1.55765962600708, -0.5736467242240906, 0.4097793698310852, 0.08481626212596893, 1.0564630031585693, -0.34814584255218506, 0.010847270488739014, 0.40356549620628357, 0.3053368926048279, 0.6108431220054626, 0.4484323561191559, -0.2535684108734131, -0.214043527841568, -0.11198583245277405, 0.467011034488678, 1.3451801538467407, 0.353764146566391, 0.09911959618330002, -0.9612529873847961, 0.9263810515403748, -1.2729887962341309, -0.4407467246055603, 0.9529691934585571, 1.1502118110656738, 0.3558140993118286, -0.167179673910141, -0.44090014696121216, -0.19146846234798431, 0.5247631669044495, 0.2265755534172058, -0.638771653175354, -0.9256755113601685, 0.4971657991409302, 0.9333317279815674, 0.6038901805877686, 0.5126449465751648, 0.035616207867860794, 0.5057578682899475, 14.792573928833008, 0.8222119808197021, -0.11709121614694595, 0.6350570321083069, 0.474174439907074, 0.44883468747138977, -0.5768914818763733, -0.010119800455868244, -1.4267568588256836, 0.06484129279851913, 1.557841420173645, -0.08765535801649094, 0.35200515389442444, 0.05284733325242996, -0.4075530171394348, -0.004075041506439447, -0.7625032067298889, 0.35119885206222534, 0.6611390709877014, -1.4225002527236938, 0.38355618715286255, 0.28032049536705017, -0.0758562758564949, 0.4921491742134094, 0.7218504548072815, 0.52751624584198, 0.6816749572753906, -0.3563397526741028, 0.08449466526508331, 0.22664228081703186, 1.0193727016448975, -0.3119634985923767, 0.441350519657135, 0.05223678797483444, -1.0975245237350464, -0.020135723054409027, -0.4877249598503113, -1.1642405986785889, -0.07519808411598206, 0.13110487163066864, -0.818500280380249, -0.716289758682251, -0.01925536058843136, 0.328595906496048, 0.11896243691444397, 0.182896226644516, -0.2233273833990097, 0.5565996170043945, -0.16599564254283905, -0.07895766198635101, -0.3024024963378906, 0.7280371189117432, -0.2609085142612457, -0.4368591010570526, -0.19539882242679596, -0.34809410572052, 0.15120939910411835, 0.5561429262161255, -0.43919962644577026, -0.39390575885772705, -0.33700791001319885, -0.13836900889873505, 0.38261929154396057, 0.8549580574035645, 0.46067526936531067, -0.048207394778728485, -0.711983323097229, 0.37968412041664124, 0.5187822580337524, -0.03531360253691673, -0.710911750793457, -0.07784038037061691, 0.48180389404296875, 0.006303855217993259, -0.05160200595855713, 0.9583394527435303, -0.2947101593017578, -0.5516251921653748, -0.7747211456298828, -0.1832951307296753, 0.5153586864471436, -0.7100162506103516, -0.20538219809532166, 0.9170064926147461, 0.04331164062023163, -0.4701971411705017, 0.016482945531606674, -0.5791117548942566, -0.2886767089366913, 0.19361434876918793, -1.1448622941970825, -0.5885235071182251, 0.08770425617694855, -0.37228158116340637, -0.6569889187812805, 0.36394062638282776, 1.4916428327560425, 0.4006587862968445, -0.21312740445137024, 0.19490066170692444, -0.23237667977809906, -0.09156740456819534, -0.028479760512709618, -0.6696164608001709, 1.0646288394927979, 0.305166095495224, -0.19018171727657318, 0.5566346049308777, -0.028915809467434883, 0.03114711493253708, -1.1497694253921509, -0.17471212148666382, 0.5980013012886047, -1.1211152076721191, -0.1315498650074005, -0.8770846724510193, -0.9478710889816284, 0.6742416620254517, 0.6469004154205322, 0.06306669116020203, 0.025971338152885437, 0.07372131943702698, -0.615734875202179, -0.4530918002128601, -0.8641991019248962, 0.3661665916442871, 0.7235139608383179, -0.9388972520828247, -0.13894657790660858, -0.31938835978507996, 0.4433448910713196, -0.9208894968032837, -1.231791615486145, -0.05454017221927643, 0.21918347477912903, -0.1530761867761612, 1.227525234222412, -0.19206774234771729, 1.1206979751586914, 0.9585538506507874, -0.23564299941062927, -0.17725056409835815, 0.14071311056613922, -0.6493587493896484, -0.7270827889442444, 0.2215205878019333, 0.8104642629623413, -0.6067865490913391, 0.6893603205680847, 1.188187837600708, 0.6332446932792664, -0.4645123779773712, -0.04645119607448578, -0.005833678878843784, -0.25135481357574463, -0.3734032213687897, 0.5029754042625427, 0.029431354254484177, 0.10303943604230881, 0.1672850102186203, 0.519993007183075, 0.6779465079307556, 0.21165943145751953, -0.2873929738998413, 0.532828688621521, -0.11606820672750473, -0.2100469321012497, -0.288127064704895, -0.6240929961204529, -1.1126106977462769, -0.12920744717121124, -1.1328320503234863, -0.008298329077661037, -0.6552704572677612, -0.04866378754377365, 0.07815464586019516, -0.38982295989990234, 0.21670842170715332, 0.16049639880657196, 0.1402312070131302, -0.705912709236145, -0.5477674603462219, -0.8364055156707764, 0.4147329032421112, 0.34339088201522827, -0.33578070998191833, 0.3475082516670227, -0.3466583788394928, -0.16000351309776306, 0.2192062884569168, 0.49494385719299316, -0.4360535442829132, -0.7425519227981567, -1.2371056079864502, 0.41541895270347595, -0.20189231634140015, -0.09461067616939545, -0.7910133600234985, 1.000901460647583, 0.4438631236553192, -0.1906752586364746, -0.18161724507808685, 0.6260225176811218, -0.7457968592643738, -0.49874699115753174, 0.42011478543281555, -0.5963431596755981, 0.5947700142860413, 0.6207238435745239, -0.8470908403396606, 0.057901471853256226, 0.9417502880096436, -0.22856004536151886, -0.851288914680481, -1.1903754472732544, 0.2470165491104126, -0.6773058176040649, 0.21728047728538513, -0.6722602248191833, -0.1734338104724884, -1.4399034976959229, -0.35743507742881775, 0.18719317018985748, 0.3163626492023468, -0.05876874178647995, 0.9175499677658081, 0.6399106383323669, -1.1989234685897827, 0.27607661485671997, 0.51141756772995, -0.11650045216083527, -0.12365515530109406, 0.16305437684059143, 0.6899713277816772, -0.1306254118680954, 0.24972571432590485, -0.02176906168460846, 0.3669186234474182, -1.0956052541732788, 0.4365500211715698, 0.376127153635025, -0.4442630410194397, 0.032058343291282654, 0.7057763934135437, -0.25037306547164917, -0.663699209690094, -0.1713050901889801, -1.0260120630264282, -0.36656758189201355, -0.6357817053794861, 0.836375892162323, 0.24084162712097168, 0.20005416870117188, 0.3341309428215027, -0.9002095460891724, -0.058711569756269455, -0.3103695213794708, -0.4357832372188568, 0.4560643136501312, -0.1064998134970665, -1.047734022140503, 0.3955926299095154, 0.7472314238548279, -0.485801637172699, -0.005944626871496439, -0.7991918325424194, -0.3350910246372223, -0.24176445603370667, 0.5830581188201904, -0.07301843166351318, -0.7159284949302673, 0.7815794348716736, 0.37490400671958923, 0.2960354685783386, 0.19309070706367493, -0.137640580534935, 0.5421800017356873, 0.5848312377929688, 0.3412088453769684, -0.6202410459518433, -0.5155593156814575, 0.8170656561851501, 0.8918374180793762, -0.5665574669837952, 0.45083704590797424, -0.45036882162094116, -0.25005391240119934, 0.6173320412635803, 0.8989858627319336, 0.08418616652488708, 0.8829589486122131, 0.6510757207870483, 0.039394378662109375, 0.04783903434872627, -1.4068241119384766, -0.23198141157627106, 0.5812017917633057, 0.9039008617401123, 0.6275322437286377, 0.03769237920641899, 0.27671322226524353, 0.39434555172920227, 0.5234830975532532, 0.4146566092967987, 0.3297167718410492, 0.6092737913131714, -0.4089164435863495, -0.12907163798809052, -0.0756460651755333, 0.9316036701202393, -0.5410954356193542, -1.0246647596359253, 0.1822374165058136, 0.6198396682739258, -0.34404027462005615, 0.4826645851135254, 1.3297330141067505, -0.36785888671875, 0.26379865407943726, -0.03293459489941597, 0.48604312539100647, -0.40912267565727234, -0.4523748457431793, -0.36044079065322876, -0.4467603862285614, -0.2979709804058075, 0.16987881064414978, -0.03784424066543579, -0.7165447473526001, -0.7033699154853821, 0.24930289387702942, 0.06645150482654572, 0.18600891530513763, 0.6909393668174744, 0.9296272993087769, 0.7347531318664551, -0.3466823101043701, -0.4824226200580597, -0.41687822341918945, -0.714942455291748, 0.364082008600235, -0.6165969967842102, -0.6470497250556946, -0.27289560437202454, -0.06486646831035614, -0.6869618892669678]}, "authors": [{"authorId": "2161638485", "name": "Yuan Feng"}, {"authorId": "33191825", "name": "Hyeran Jeon"}, {"authorId": "1745378", "name": "F. Blagojevic"}, {"authorId": "152491351", "name": "Cyril Guyot"}, {"authorId": "2117897204", "name": "Qing Li"}, {"authorId": "2179703418", "name": "Dong Li"}], "references": [{"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "6fd1a073a1094def703d004825d7fe8112da2d0b", "title": "DREW: Efficient Winograd CNN Inference with Deep Reuse"}, {"paperId": "111e39b157b19ce5afb4abec031e8d0224d0ecd0", "title": "Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic Sequence Length"}, {"paperId": "9202a718ce05395b6e17d5301e3a2e8b1021f31b", "title": "Prune Once for All: Sparse Pre-Trained Language Models"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "fb486b63058925d762317992efa65e3cd6f188de", "title": "Leveraging redundancy in attention with Reuse Transformers"}, {"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "5d1e3071932d12722595de9a4fec1d2309c8a717", "title": "Bliss: auto-tuning complex applications using a pool of diverse lightweight learning models"}, {"paperId": "28a147b0b938f4f9dae394e4a69253782d7c2e91", "title": "MD-HM: memoization-based molecular dynamics simulations on big memory system"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "b1948bbdd4535450f6193a913663bbaeadfa0fbf", "title": "REDUCT: Keep it Close, Keep it Cool! : Efficient Scaling of DNN Inference on Multi-core CPUs with Near-Cache Compute"}, {"paperId": "57ed901be5d1b4d853d4f8998dadc1b60e2151f9", "title": "On Attention Redundancy: A Comprehensive Study"}, {"paperId": "6da4b231148bf26677233d1f778d08a5d26f4313", "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference"}, {"paperId": "dd618e8ece2d2a248487522c987df685258c047b", "title": "Linear-Time Self Attention with Codeword Histogram for Efficient Recommendation"}, {"paperId": "40235eded15f44c8c4a7f48468adcc7df4e171fb", "title": "Rethinking Network Pruning \u2013 under the Pre-train and Fine-tune Paradigm"}, {"paperId": "ec4050b78945674cd170000eb024ed16bb140af4", "title": "GPTune: multitask learning for autotuning exascale applications"}, {"paperId": "4a9587ad4ed8f755e74524499452ea12bd6c7f3b", "title": "Optimizing Inference Performance of Transformers on CPUs"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"}, {"paperId": "96d654dbd6d77c8b3d4fe13ee4111feee4e4fa85", "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"}, {"paperId": "b20ddcbd239f3fa9acc603736ac2e4416302d074", "title": "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "851de6751aef4128d7feb7c6ca36b180a0e0835e", "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering"}, {"paperId": "9846725432d38cc5c1088945c8d5287ce6354b1b", "title": "Auto-tuning Parameter Choices in HPC Applications using Bayesian Optimization"}, {"paperId": "8951924ef9108c6440b6473351ed288388868dd8", "title": "\u2205sim: Preparing System Software for a World with Terabyte-scale Memories"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "a3ef6ee560e93e6f58be2b28f27aed0eb86dc463", "title": "Fine-tune BERT with Sparse Self-Attention Mechanism"}, {"paperId": "242d2ba526f9461e2b18bf602d05657b36d7e19a", "title": "Neuron-Level Fuzzy Memoization in RNNs"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "31865cfde9f198a3c9da47cd729a57582d3fa0a1", "title": "Deep reuse: streamline CNN inference on the fly via coarse-grained computation reuse"}, {"paperId": "ba753e521f03a352a45dafb43df84db3fc2b9a8e", "title": "The Architectural Implications of Facebook's DNN-Based Personalized Recommendation"}, {"paperId": "a39d5919531a56de0e36f6b76142041b5d508213", "title": "Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval"}, {"paperId": "88edf1a7010e85d8568c69d591d2bdeedb0e6c9a", "title": "INFaaS: Managed & Model-less Inference Serving"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "4fd0030812d083c81155625f8ef60c218d9a7315", "title": "Adaptive Deep Reuse: Accelerating CNN Training on the Fly"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2cbb8de53759e75411bc528518947a3094fbce3a", "title": "Billion-Scale Similarity Search with GPUs"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "a7506e8931e3f7ba5446643d6f74245976a3fdae", "title": "Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval"}, {"paperId": "699a2e3b653c69aff5cf7a9923793b974f8ca164", "title": "Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "title": "Learning representations by back-propagating errors"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "f216444d4f2959b4520c61d20003fa30a199670a", "title": "Siamese Neural Networks for One-Shot Image Recognition"}, {"paperId": null, "title": "Considerations in the design of a computer with high logic-to-memory speed ratio"}, {"paperId": null, "title": "Revolutionizing Memory and Storage"}, {"paperId": null, "title": "Compute Express Link Industry Members"}, {"paperId": null, "title": "Llama/example.py at main \u00b7 lambdalabsml/llama"}, {"paperId": null, "title": "Oracle Cloud Price list"}, {"paperId": null, "title": "Amazon EC2 High Memory Instances with 6, 9, and 12 TB of Memory,"}, {"paperId": null, "title": "Flashat-tention"}, {"paperId": null, "title": "Huggingface. Efficient inference on cpu"}, {"paperId": null, "title": "Intel"}, {"paperId": null, "title": "LambdaLabsML"}]}