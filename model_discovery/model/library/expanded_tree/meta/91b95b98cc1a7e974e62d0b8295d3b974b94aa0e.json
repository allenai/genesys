{"paperId": "91b95b98cc1a7e974e62d0b8295d3b974b94aa0e", "abstract": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student.Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 9, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.02031", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work focuses on Knowledge Distillation techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student, and derives a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD."}, "embedding": {"model": "specter_v2", "vector": [0.16874399781227112, 0.9248591065406799, -0.4819363057613373, 0.2835805416107178, -0.6819553971290588, -0.5223767161369324, 0.8497770428657532, 0.07238352298736572, -0.10376131534576416, 0.046185217797756195, 0.4258214831352234, -0.35123711824417114, 0.13558779656887054, -0.11092919856309891, -0.5661439895629883, -0.031924620270729065, -0.5913049578666687, 0.5195529460906982, -0.4199552536010742, -0.527658224105835, -0.2821890115737915, -0.9749847054481506, -0.771304726600647, -0.07355602085590363, 0.9524049162864685, 0.4065234065055847, -0.09568636864423752, 0.971182107925415, -0.506476104259491, -0.09152580052614212, 0.27161848545074463, -0.43321821093559265, 0.8944104909896851, -0.48180091381073, -0.1499084234237671, -0.05847559869289398, 0.2745470106601715, -0.38421568274497986, -0.651857316493988, 0.5411465167999268, 0.12092923372983932, 0.370949923992157, 0.5571475028991699, -0.4886535108089447, -0.9779402613639832, 0.892573893070221, 0.38577815890312195, 0.6666549444198608, -0.06754457950592041, -0.3650043308734894, 1.118886113166809, -1.3751145601272583, 0.3420439660549164, 1.3882746696472168, 0.3234463334083557, 0.7320870757102966, -0.4714680314064026, -0.4775982201099396, 0.3806946277618408, -0.11966708302497864, -0.8717355728149414, 0.2135789394378662, -0.20212475955486298, -0.009875288233160973, 1.1124800443649292, -0.4351419508457184, 0.10364074259996414, 0.7536362409591675, -0.7244206070899963, 1.6071776151657104, -0.06878341734409332, -0.9480651617050171, -0.3996688723564148, 0.21468785405158997, -0.0376596599817276, 0.9759042263031006, -0.16826772689819336, 0.6891874074935913, -0.947231650352478, -0.05487615242600441, 0.41738948225975037, -0.5602236390113831, -0.6262394785881042, -0.15414628386497498, 0.09020967781543732, 0.8515971302986145, 0.17533676326274872, 0.6690772175788879, 0.06630640476942062, 0.9266737103462219, 0.3067067861557007, 0.38516050577163696, 0.38020649552345276, 0.36386704444885254, -0.14778335392475128, 0.040684521198272705, -1.0041953325271606, 0.563586950302124, 0.1155976802110672, 0.8151881098747253, -0.2230604588985443, 0.4385385811328888, -1.177720069885254, 0.22933052480220795, 1.1187689304351807, -0.22494257986545563, 1.1143954992294312, -0.8390414118766785, 0.4805583357810974, -0.8454857468605042, 0.032328344881534576, -0.5228703618049622, 0.3774193525314331, -0.37508413195610046, -0.6401495933532715, -1.368828296661377, -0.2781677544116974, -0.15058504045009613, -1.3526933193206787, 0.9859382510185242, -0.1904752254486084, 0.37164506316185, 0.15979169309139252, 0.6023929715156555, 0.4315279722213745, 0.7795591354370117, 0.06571638584136963, -0.01724623516201973, 0.516551673412323, -0.6434659361839294, -0.5267984867095947, -0.7152630686759949, 0.9833198189735413, -0.019509093835949898, 0.3269534111022949, -0.1802172213792801, -1.6452184915542603, -1.1196174621582031, -0.7772728204727173, -0.1456887274980545, -0.5156420469284058, 0.24011872708797455, 0.936631977558136, 0.8221885561943054, -0.8905330896377563, 0.8179393410682678, -0.20660196244716644, 0.16414378583431244, 0.6114296317100525, -0.027523797005414963, 0.21467413008213043, -0.7680376172065735, -1.2733782529830933, 0.10086491703987122, 0.5605224967002869, -0.8772609233856201, -0.3829597234725952, -1.0764559507369995, -1.1209478378295898, -0.1906174123287201, 0.4744807183742523, -0.5106162428855896, 1.325475811958313, 0.23262222111225128, -1.0398427248001099, 0.5985122919082642, -0.030752109363675117, -0.15025624632835388, 0.6694469451904297, -0.316579133272171, -0.1656118482351303, -0.5473414659500122, -0.11866487562656403, 0.9785377383232117, 0.48901060223579407, -0.5415728092193604, 0.18151406943798065, 0.5622441172599792, -0.36361855268478394, 0.03561082482337952, -0.2331261783838272, -0.045041657984256744, -0.41508227586746216, -0.6312848329544067, 0.5208845138549805, 0.6202300786972046, -0.03179115429520607, -0.20092979073524475, -0.7653446197509766, -0.8352741003036499, 0.8789648413658142, -0.17040365934371948, 0.5611769556999207, -0.8488388657569885, -0.8107107877731323, -0.06832120567560196, -0.0520094633102417, 0.06746690720319748, -0.9236530065536499, 0.5913533568382263, -0.5901640057563782, 0.7751782536506653, -0.40465521812438965, -1.174933671951294, 0.17742274701595306, -0.0741407573223114, -0.7506584525108337, -0.5586931705474854, -0.08644049614667892, 1.3171063661575317, -0.9336130023002625, 0.2311234027147293, -0.09439218789339066, -0.011075826361775398, -1.132075309753418, 1.0269722938537598, -0.8361164927482605, 0.664196789264679, -0.2328345626592636, -0.3670486509799957, 0.017331944778561592, -0.12069495022296906, 0.0697280615568161, -0.28439459204673767, 0.3938216269016266, 0.6283349990844727, -0.3711771070957184, 1.6224218606948853, 0.08729236572980881, 0.285028338432312, -0.47701284289360046, -1.0657087564468384, 0.48706740140914917, 0.7134613394737244, -0.19547858834266663, -0.02910296805202961, 0.15328341722488403, 0.9399551749229431, -0.30797410011291504, 0.14102056622505188, 0.9601169228553772, 0.17594584822654724, -0.16536930203437805, 0.2949484586715698, 0.648444652557373, -0.5703100562095642, 0.7830571532249451, 0.5483392477035522, 0.5663659572601318, 0.42826515436172485, -0.10636751353740692, 0.17632941901683807, 0.09648793935775757, -0.614827573299408, -0.2698655426502228, 0.5559738874435425, 0.9192689061164856, 0.968116819858551, 0.15507496893405914, -0.6957449913024902, -0.5217766165733337, 0.0064981295727193356, 0.9241915345191956, 0.9227185249328613, -0.3497075140476227, -0.321062296628952, -0.6763519644737244, -0.7534826993942261, -0.08245661109685898, 0.35476765036582947, -0.4459803104400635, -0.4882833957672119, -0.688498318195343, -1.1697373390197754, 0.8799158930778503, 0.2770735025405884, 1.277071237564087, -0.09706403315067291, 0.20456454157829285, -0.372759073972702, 0.02603921853005886, -0.7654522657394409, -0.8162708282470703, 0.2754918336868286, -0.3353945314884186, -0.10820794105529785, -0.5852412581443787, -0.2127165049314499, 0.3502824902534485, -0.768601655960083, 0.9168720245361328, -0.161969855427742, -0.4724450409412384, 0.32058414816856384, 0.19500742852687836, -0.4712205231189728, -0.959771454334259, 0.3244888186454773, 0.2815525233745575, -0.5789785385131836, 0.1508444994688034, 0.41429316997528076, -0.09339282661676407, -0.3049078583717346, -0.42508965730667114, 0.10892115533351898, -0.20758964121341705, 0.1152307465672493, 0.7431941628456116, -0.15082883834838867, 0.0048790122382342815, -1.2219030857086182, 1.2752580642700195, 0.20603297650814056, -0.3859187364578247, 0.08651464432477951, -0.7030487656593323, -0.3747476637363434, 0.411353200674057, -0.801661491394043, -0.1273287683725357, -1.1516615152359009, 0.16484762728214264, -0.44955283403396606, -0.14157606661319733, 0.3622794449329376, 0.7528365850448608, -0.01732904277741909, 0.5504185557365417, 0.6882414817810059, 0.43779805302619934, -0.052915818989276886, 1.3470757007598877, -1.3313333988189697, 0.5201570987701416, -0.2795330882072449, 0.716423511505127, -0.3786902129650116, -0.014270998537540436, -0.243160679936409, -0.6355298757553101, 0.356175035238266, -0.10963834822177887, 0.05718269944190979, 0.2638501226902008, -0.6874933838844299, -0.5775415897369385, -0.23869849741458893, -0.9220155477523804, -0.47769495844841003, -0.6774590015411377, -0.33394408226013184, -0.5068946480751038, -0.8714591860771179, -1.3301750421524048, -0.46670040488243103, -0.38735872507095337, -0.9123282432556152, 0.44194266200065613, 0.006860373076051474, -0.23207765817642212, -0.5536039471626282, 0.17839452624320984, -0.4437367618083954, 0.7121751308441162, -0.5346492528915405, 1.0635802745819092, -0.18494713306427002, -0.6662794947624207, -0.22392021119594574, 0.39580801129341125, 0.5010854601860046, -0.12064038962125778, 0.10995802283287048, -0.6217666864395142, -0.018814969807863235, -0.3481939733028412, -0.8003489971160889, 0.22948551177978516, 0.6219558119773865, 0.711013674736023, -0.17869514226913452, -0.5579408407211304, 0.6126508116722107, 1.3365641832351685, -0.5459954142570496, -0.09291859716176987, -0.1124679297208786, 0.9904258847236633, 0.3635999262332916, -0.329450398683548, 0.23867736756801605, 0.2535403072834015, 0.19467660784721375, -0.10428853332996368, 0.06163851544260979, -0.2776159644126892, -0.9996033310890198, 0.13989225029945374, 1.50086510181427, 0.6121225357055664, -0.3878518044948578, -1.1109862327575684, 0.8473806381225586, -1.0213583707809448, -0.15450720489025116, 0.4878479540348053, 0.7205469608306885, 0.5497892498970032, -0.5863568186759949, -0.30099645256996155, 0.16103830933570862, 0.4236535429954529, 0.2827369272708893, -0.24635149538516998, -0.3963848948478699, 0.16773299872875214, 0.6607224941253662, 0.09022753685712814, 0.5683220624923706, 0.0004766004567500204, 0.5864866971969604, 14.688794136047363, 1.2701061964035034, 0.3204568028450012, 0.8083924651145935, 1.0263962745666504, 0.10300477594137192, -0.5743553042411804, -0.4039223790168762, -1.133748173713684, 0.03798787295818329, 1.0150351524353027, 0.08288316428661346, 0.48074713349342346, -0.058207351714372635, -0.2695901691913605, 0.15616030991077423, -0.2773161828517914, 0.6894179582595825, 0.44764167070388794, -1.5350773334503174, 0.6969936490058899, 0.3367641270160675, 0.8512694835662842, 0.2416263073682785, 1.0661592483520508, 0.9554190039634705, 0.5749058723449707, -0.4284481108188629, 0.4023391604423523, -0.16211768984794617, 1.1952686309814453, 0.048464443534612656, 0.0241512730717659, 0.7570832371711731, -0.6489648222923279, -0.1661931872367859, -0.428406685590744, -0.8200141191482544, 0.6410524249076843, 0.25804266333580017, -0.4024814963340759, -0.27615055441856384, -0.4480111300945282, 0.5250414609909058, 0.32762041687965393, 0.19127538800239563, -0.4302515685558319, 0.6647941470146179, -0.2844235599040985, 0.4849330186843872, 0.4068545997142792, 0.38453271985054016, 0.26823946833610535, -0.44750675559043884, 0.8131467700004578, 0.2527362108230591, 0.22410151362419128, 0.8517450094223022, -0.7547445893287659, -0.20509621500968933, -0.6011199951171875, -0.41864800453186035, -0.052604835480451584, 0.5124471187591553, 0.5992025136947632, 0.21205967664718628, -0.07300886511802673, 0.6258656978607178, 0.5108158588409424, 0.24678868055343628, 0.056772876530885696, 0.09557677805423737, 0.08422290533781052, -0.18948028981685638, 0.027652844786643982, 0.32179582118988037, -0.3702474534511566, -0.19406752288341522, -0.7641569375991821, -0.4180101156234741, 0.11525887995958328, -0.7324613332748413, -0.8966237306594849, 0.8206067681312561, -0.1445038914680481, -0.5550388097763062, -0.07658883929252625, -0.5580564737319946, -0.1467975527048111, 0.7099949717521667, -1.4231164455413818, -0.4719036817550659, 0.26201674342155457, -0.4892992079257965, -0.5138898491859436, -0.2872548997402191, 1.217781662940979, 0.0693412497639656, -0.06662371009588242, 0.35705941915512085, 0.14404447376728058, -0.21463820338249207, -0.3080621063709259, -1.1321395635604858, 1.3164633512496948, 0.13398078083992004, 0.13729001581668854, -0.017091581597924232, 0.02188747376203537, -0.20977672934532166, -0.5918725728988647, -0.2597121000289917, 0.49882563948631287, -1.064716100692749, -0.8567262291908264, -0.67867511510849, -0.8469706177711487, 0.23121505975723267, 0.34722715616226196, -0.1845329999923706, 0.5585863590240479, -0.09071216732263565, -0.4873224198818207, 0.29408758878707886, -1.15010666847229, 0.1736379861831665, 0.33642545342445374, -0.5396760106086731, -0.15773601830005646, 0.3372950851917267, 0.8216050863265991, -1.608913540840149, -0.47395816445350647, -0.13278162479400635, -0.4440730810165405, 0.22666224837303162, 1.0203880071640015, -0.13284307718276978, 0.9317985773086548, 0.8605502247810364, 0.00040867525967769325, -1.0232316255569458, 0.04963911324739456, -1.1300859451293945, -0.3040383458137512, 0.4335941672325134, 0.6050819158554077, -0.11234420537948608, 1.15243399143219, 0.9585973620414734, 0.13610555231571198, -0.49484461545944214, -0.5090811848640442, -0.6080871820449829, 0.3590010702610016, -0.5431987643241882, 0.28689175844192505, 0.1192910373210907, -0.020810576155781746, 0.08631286770105362, 0.301045686006546, 0.23931549489498138, -0.32032182812690735, -0.9998443126678467, 0.5341289639472961, 0.38083967566490173, -0.17263594269752502, -0.20002877712249756, -0.11435385793447495, -1.676468014717102, -0.23814992606639862, -1.5127975940704346, -0.017321711406111717, -0.7924001216888428, -0.6817862391471863, -0.15704283118247986, 0.38565441966056824, -0.17670880258083344, 0.1416795700788498, -0.15781177580356598, -0.2578127682209015, -0.7877864241600037, 0.37579941749572754, 1.2002158164978027, 0.9157678484916687, -0.4009089171886444, -0.08479950577020645, -0.45729586482048035, -0.14098700881004333, 0.16534267365932465, 0.3486076891422272, -0.4651143252849579, -1.2292556762695312, -1.2644447088241577, 0.1349107176065445, -0.07582024484872818, -0.012835700064897537, -0.7773175835609436, 0.26823690533638, 0.7696950435638428, -0.2563997805118561, -0.015116879716515541, 0.6393047571182251, -0.7371673583984375, -0.6936184763908386, 0.37395328283309937, -0.7670460343360901, 0.09540121257305145, 0.14066468179225922, -0.17911671102046967, -0.31195273995399475, 0.32589924335479736, -0.12362793833017349, -1.3780040740966797, -0.09913676977157593, 0.5394399762153625, -0.2706242799758911, 0.3928278386592865, -0.3077135682106018, -0.23755916953086853, -1.3620173931121826, -0.6412068605422974, -0.01752191036939621, 0.361469566822052, -0.6427450180053711, 0.6571835875511169, 0.4031992554664612, -1.3950470685958862, 0.023126449435949326, 0.5020666718482971, -0.3412097990512848, 0.3841599225997925, 0.6322789788246155, 0.18617932498455048, -0.29197072982788086, 0.512141227722168, 0.7255235314369202, 0.6116531491279602, -0.32629573345184326, 0.11797063052654266, 0.7759292125701904, -0.7045103311538696, -0.10909086465835571, 1.5813820362091064, -0.5854291915893555, -1.3295375108718872, 0.08937529474496841, -1.1030217409133911, -0.19117996096611023, -0.44372332096099854, 0.4084106683731079, 0.1180024966597557, -0.3706566095352173, 0.3119162917137146, 0.259454607963562, 0.2846897542476654, -0.2745891809463501, -0.9492660164833069, 0.3708893656730652, -0.12255843728780746, -0.10862996429204941, 0.18936210870742798, 0.7173123359680176, -0.838986873626709, -0.45996055006980896, -0.649225652217865, -0.5285655856132507, -0.4494340419769287, 0.08356139063835144, -0.6164248585700989, -0.7473276257514954, 0.8711704015731812, 0.49066686630249023, 0.34337887167930603, 0.12165289372205734, -0.2480727881193161, 0.7551915049552917, 0.4848135709762573, -0.025567714124917984, -0.26485157012939453, -0.15500514209270477, 1.0924642086029053, 0.9053342342376709, -0.7514718770980835, -0.04376666620373726, -0.560621440410614, -0.5975426435470581, 0.5183561444282532, 0.6892080307006836, 0.06902803480625153, 0.714788019657135, -0.6921966075897217, -0.18277086317539215, -0.1659356951713562, -0.8464325070381165, -0.21815234422683716, 0.9024154543876648, 1.3284896612167358, 0.7235308885574341, 0.007849574089050293, -0.1350361555814743, 0.7619292140007019, -0.1815192848443985, 0.22059635818004608, 0.6302965879440308, 0.3965000510215759, -0.20551007986068726, -0.7872994542121887, 0.07706454396247864, 0.8662711381912231, -0.8096630573272705, -0.8600179553031921, 0.270387202501297, 0.6155974268913269, 0.6149531602859497, 0.4749082326889038, 0.5146667957305908, -0.02026163414120674, 0.4046458899974823, 0.40467897057533264, 0.6939277648925781, -0.6197090744972229, -0.089650459587574, -0.26961666345596313, -0.6340925097465515, 0.2639014720916748, 0.0022726033348590136, -0.4463656544685364, -0.1764262169599533, -0.19105809926986694, 0.29289647936820984, 0.2999282479286194, 0.14635834097862244, 1.058103084564209, 0.4971228837966919, 0.5177275538444519, -0.45160746574401855, -0.19909150898456573, -0.31168681383132935, -0.7502045631408691, -0.07466645538806915, -0.5295071601867676, -0.3284556567668915, -0.07381328195333481, 0.023801028728485107, 0.019581088796257973]}, "authors": [{"authorId": "2135910736", "name": "Nitay Calderon"}, {"authorId": "2153292652", "name": "Subhabrata Mukherjee"}, {"authorId": "1762757", "name": "Roi Reichart"}, {"authorId": "1703796", "name": "Amir Kantor"}], "references": [{"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "fbd49b25bdab98c171af49962a41139c73dacbde", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning"}, {"paperId": "fd7e88a2313e176315d99fc299277e752d7703b7", "title": "Efficient Methods for Natural Language Processing: A Survey"}, {"paperId": "9ed047bb4ef6ddc296d473bf4f3b55488aeba350", "title": "GEMv2: Multilingual NLG Benchmarking in a Single Line of Code"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"}, {"paperId": "fc14091bbd7d1eb2c9f23ff9c75a4222f2604143", "title": "Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4", "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation"}, {"paperId": "0bffff14430c6fa212c3924cb68190734bd61e2d", "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization"}, {"paperId": "5060cae1ff3c07877950d5d44158762eb20b455f", "title": "DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation"}, {"paperId": "09beb3dfc544b0ceeb4311e2d3458716a02eb474", "title": "AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "a7e42bc1f09a5bf23258739a8d26d50b557986dd", "title": "Relating Neural Text Degeneration to Exposure Bias"}, {"paperId": "a3db0e653a4181847880d18113f023d21b31107d", "title": "Fixing exposure bias with imitation learning needs powerful oracles"}, {"paperId": "a5cc803d709442a24cf996d6769b41efa3f632f0", "title": "Scheduled Sampling Based on Decoding Steps for Neural Machine Translation"}, {"paperId": "2b615180ac839026ad7ffa5420014eb9c3aa8f94", "title": "Decoding Methods in Neural Language Generation: A Survey"}, {"paperId": "486b699576b4219168c9be0841556eaabc620b1a", "title": "Attention Temperature Matters in Abstractive Summarization Distillation"}, {"paperId": "926eb6dbb08791dad76e4a0468731b02a85a5bba", "title": "Selective Knowledge Distillation for Neural Machine Translation"}, {"paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"}, {"paperId": "13f67bd8700d5cd869c07b4aa9cb92b9e79bb17e", "title": "Model Compression for Domain Adaptation through Causal Effect Estimation"}, {"paperId": "b5b006dc558cb7fbd532d67e989173b536e8ac80", "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers"}, {"paperId": "59653e5cfa854a17c2ffcb86f2a454f27e12c716", "title": "Decoding and Diversity in Machine Translation"}, {"paperId": "42b8944b51be279ec12d06408e646a8d52d54b3d", "title": "Deep Learning for Text Style Transfer: A Survey"}, {"paperId": "6044e943a7f7e8741431028fdbdaf63754cd8d5f", "title": "Pre-trained Summarization Distillation"}, {"paperId": "92d879ed4be5438c2498d5269e356959da2030bd", "title": "Noisy Self-Knowledge Distillation for Text Summarization"}, {"paperId": "9baab08fbe37369856688b2abe5b3c90cce1682c", "title": "Compression of Deep Learning Models for Text: A Survey"}, {"paperId": "8c5a394654822a5de53ac2e4a355c1c6ead4750c", "title": "Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "0b15c6acfc9f7f92c52aa6a185a829f88975c743", "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation"}, {"paperId": "627327dcedf58c78ad8c0a1ce02f4a70ebf61c45", "title": "Evaluating the Evaluation of Diversity in Natural Language Generation"}, {"paperId": "9c5a239b75bade55c830b164e2fadc424e879137", "title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "ba08784bb30de51f72f88d5d64a64310d030db10", "title": "From Research to Production and Back: Ludicrously Fast Neural Machine Translation"}, {"paperId": "55e206fad7199312def88e92ef77395b030bec15", "title": "Characterizing the Deployment of Deep Neural Networks on Commercial Edge Devices"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "91b5d03050e416df04872361d8d9f35ad7091246", "title": "Generalization in Generation: A closer look at Exposure Bias"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "a550f576ff20b8cce98f3ddad0043d3783fbc9b4", "title": "Abductive Commonsense Reasoning"}, {"paperId": "f8de25118af2abc4c48afb947d6ec298e05ef1e5", "title": "When Does Label Smoothing Help?"}, {"paperId": "0feea94f89d395436bf41bd10c797447eecbc128", "title": "Unsupervised Data Augmentation for Consistency Training"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "295065d942abca0711300b2b4c39829551060578", "title": "BERTScore: Evaluating Text Generation with BERT"}, {"paperId": "ce1fc190b1424bf7ba658548be554b62733e96a1", "title": "Differentiable Sampling with Flexible Reference Word Order for Neural Machine Translation"}, {"paperId": "bc6dfc6bda2d929fec91042dce1831fd07999b39", "title": "Improved Knowledge Distillation via Teacher Assistant"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e", "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "2b1a3d7e6045dc6b544a548b372c1f8492b85967", "title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms"}, {"paperId": "15dff25a59e0953ae732917c25ba99866eddaf5e", "title": "Style Transfer in Text: Exploration and Evaluation"}, {"paperId": "9d0fbb8ddc10e473bb878af1493af7bb7d0c5f66", "title": "Shakespearizing Modern Language Using Copy-Enriched Sequence to Sequence Models"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d", "title": "Sequence Level Training with Recurrent Neural Networks"}, {"paperId": "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "e8679859bf0ad6ca4253603d05462838957733fb", "title": "A Systematic Exploration of Diversity in Machine Translation"}, {"paperId": "56010a55d49ac1f42355538f494427fd22402be1", "title": "Exploring the Limits"}, {"paperId": "146b3649b693ae591c8953c0aae5264512c26ea3", "title": "Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines"}, {"paperId": "c70f7b2f88709755fa9c84443979cca6ea08a15b", "title": "State of the Art of Natural Language Processing"}, {"paperId": "459b34447952feebacc2f12778c539618e8a299f", "title": "A Survey on Model Compression for Natural Language Processing"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": null, "title": "2020) and Decoder-only (DO) (Radford et al., 2019"}, {"paperId": null, "title": "marization, and NMT tasks, DO models excel on open-text generation and zero/few-shot setups Raffel et al"}, {"paperId": null, "title": "in-house model for a specific NLG task. Our findings also raise the question of why huge language models, such as GPT-3 and PaLM (Brown et al., 2020"}, {"paperId": "acf2dd4e2853f90832c01c556a2e716e7c720bc2", "title": "G ENIE A Leaderboard for Human-in-the-Loop Evaluation of Text Generation"}, {"paperId": null, "title": "2021a) and \u00a7B"}, {"paperId": null, "title": "processing the input in a single encoder layer (for ED) is reduced from"}, {"paperId": null, "title": "The correct way to measure inference time of deep neural networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2022), which in contrast to us, trained from scratch LMs. For the DO architecture, we use the GPT2-family models (Radford et al., 2019): GPT2, GPT2-M, and GPT2-L"}, {"paperId": "b88a0434c07188b21f36b0a3bc58ad81dbcdc8b4", "title": "Paraphrasing for Style"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": "967f32841955b72f358190436baa5510839d9ab3", "title": "A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins"}, {"paperId": null, "title": "The explanation is related to the observations but does not explain them"}, {"paperId": null, "title": "corresponding name in the results Table 3. More implementation details including hyperparameters"}, {"paperId": null, "title": "Conditional Language Modeling (fine-tuning Stages 1 and 2"}]}