{"paperId": "67aa2bbc59fc6ec26083f263775a72f5cee4a66d", "abstract": "Recent advances in large language models have brought immense value to the world, with their superior capabilities stemming from the massive number of parameters they utilize. However, even the GPUs with the highest memory capacities, currently peaking at 80GB, are far from sufficient to accommodate these vast parameters and their associated optimizer states when conducting stochastic gradient descent-based optimization. One approach to hosting such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most academic researchers, who always have a limited budget for many high-end GPU servers. In this paper, we focus on huge model fine-tuning on a single, even low-end, GPU in a commodity server, which is accessible to most AI researchers. In such a scenario, the state-of-the-art work ZeRO-Infinity suffers from two severe issues when running in a commodity server: 1) low GPU utilization due to inefficient swapping, and 2) limited trainable model size due to CPU memory capacity. The underlying reason is that ZeRO-Infinity is optimized for running on high-end GPU servers. To this end, we present Fuyou, a low-cost training framework that enables efficient 100B huge model fine-tuning on a low-end server with a low-end GPU and limited CPU memory capacity. The key idea is to add the SSD-CPU communication as an optimization dimension and thus carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization. The experimental results show that 1) Fuyou is able to fine-tune 175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while ZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model, Fuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves 45 TFLOPS.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Fuyou, a low-cost training framework that enables efficient 100B huge model fine-tuning on a low-end server with a low-end GPU and limited CPU memory capacity, to add the SSD-CPU communication as an optimization dimension and carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization."}, "embedding": {"model": "specter_v2", "vector": [-0.057977739721536636, -0.04148189723491669, -0.08584139496088028, 0.031679194420576096, 0.04270222783088684, -0.026969196274876595, 0.478095144033432, -0.6095487475395203, -0.8604350090026855, -0.39083966612815857, -0.18861782550811768, -0.17212142050266266, 0.6135848760604858, 0.1454070508480072, -0.2512845993041992, -0.016490546986460686, -0.5544902682304382, 0.9235864281654358, 0.15052510797977448, -0.7041194438934326, -0.1728343814611435, -0.3957769274711609, -1.4818627834320068, -0.002325658220797777, 0.4124704599380493, 0.7525818347930908, 0.3055022656917572, 1.024526596069336, -0.6820746660232544, -0.10084221512079239, 0.5232051610946655, -0.11228347569704056, 0.3414096236228943, 0.12840986251831055, -0.26540669798851013, -0.14577358961105347, 0.29669836163520813, -0.33284270763397217, -0.10894639790058136, 0.689588725566864, 0.14201530814170837, 0.3622375726699829, 0.3879759907722473, -0.9712483286857605, -0.16459262371063232, 0.2955031394958496, 0.18507976830005646, 0.8891919851303101, -0.8060757517814636, -0.2808803617954254, 0.815147876739502, -1.3092806339263916, 0.0955096110701561, 1.1413145065307617, 0.34983140230178833, 0.4663352370262146, -0.28875184059143066, -0.6489654183387756, 0.40200158953666687, -0.369425505399704, -0.980678141117096, -0.11673179268836975, -0.21860332787036896, -0.12715871632099152, 2.308253049850464, -0.1280556321144104, -0.02119964174926281, 0.5506831407546997, 0.12002582848072052, 1.2450863122940063, -0.11256615817546844, -0.7960988283157349, -0.21764999628067017, 0.2172221541404724, 0.5714283585548401, 1.0658971071243286, -0.6801016926765442, 0.6029050350189209, -0.9993861317634583, -0.41238507628440857, 0.6590973734855652, -0.5855931043624878, 0.4961572587490082, -0.4358801543712616, -0.10638555884361267, 0.831635057926178, 0.26612281799316406, 0.46784189343452454, 0.03506419062614441, 0.6357075572013855, 0.49693891406059265, 0.31302517652511597, 0.20970530807971954, 0.43023881316185, -0.1465546041727066, -0.004284482449293137, -0.8102009892463684, 0.18656674027442932, 0.19040794670581818, 0.8172099590301514, -0.33138126134872437, 0.3685119152069092, -0.8680800795555115, 0.45165345072746277, 1.1351290941238403, 0.10452256351709366, 0.7699769735336304, -0.388306200504303, 0.6768052577972412, -0.7433608770370483, 0.02284061349928379, -0.3307807147502899, -0.389860063791275, -0.3044383227825165, -0.6000734567642212, -0.7845919132232666, -0.6063311100006104, -0.2244546115398407, -1.0505485534667969, 0.45094630122184753, 0.2864324152469635, 0.1547025442123413, -0.14340204000473022, 1.0195729732513428, 0.89313805103302, 0.6974918842315674, 0.366422563791275, 0.5383014678955078, 0.8978531956672668, -1.331815481185913, -0.2408209592103958, -1.2513052225112915, 0.7677962183952332, -0.08447002619504929, 0.6806126236915588, -0.22457312047481537, -1.396274447441101, -0.8692628741264343, -0.642413318157196, -0.15343615412712097, -0.5912268161773682, 0.3498973548412323, 1.3577872514724731, 0.34198224544525146, -0.9223995208740234, 0.8014406561851501, -0.25866612792015076, -0.13681943714618683, 0.2578788101673126, 0.5520625114440918, 0.3742273151874542, -0.1576465219259262, -1.3708596229553223, 0.06874243915081024, 0.6638309359550476, -0.3965608775615692, 0.08753516525030136, -0.6585979461669922, -0.6479372978210449, 0.20773786306381226, 0.41645321249961853, -0.6820451617240906, 1.005298376083374, 0.08649493008852005, -1.551900029182434, 1.036439299583435, -0.13011153042316437, 0.19676253199577332, 0.6170018911361694, 0.2523595690727234, -0.3508003056049347, -0.9493056535720825, -0.7490921020507812, 0.8454380631446838, 0.742617666721344, 0.23260262608528137, -0.12859255075454712, 0.09766049683094025, -0.17987480759620667, 0.29720547795295715, -0.58588045835495, 1.130031943321228, -0.913711428642273, 0.002563492627814412, 0.45500731468200684, 0.16780804097652435, -0.011753327213227749, -0.21542203426361084, -0.394317626953125, -0.9442035555839539, 0.5051103830337524, 0.43018272519111633, 1.0866646766662598, -0.6465117335319519, -0.5426258444786072, -0.07151413708925247, -0.1076703816652298, 0.20783227682113647, -0.7999409437179565, 0.2965671122074127, -0.3406665027141571, 0.3939107060432434, -0.19490554928779602, -0.9849776029586792, 0.40564775466918945, -0.2758345603942871, -0.35380253195762634, -0.20200301706790924, 0.11190364509820938, 1.1166242361068726, -0.6538001894950867, -0.17458970844745636, -0.26582086086273193, 0.45046746730804443, -1.1558136940002441, 0.9825968146324158, -0.45703932642936707, 0.03513718768954277, -0.25274062156677246, -0.3053567707538605, 0.17807504534721375, -0.642110288143158, 0.2765181362628937, -0.28118449449539185, -0.20710423588752747, 0.18066507577896118, -0.5628550052642822, 1.4532994031906128, -0.16854386031627655, 0.12220818549394608, -0.027765579521656036, -0.4005541503429413, 0.1962609887123108, 0.13421902060508728, -0.46056389808654785, -0.6815044283866882, 0.46569523215293884, 0.6069478392601013, -0.4238510727882385, 0.6187261343002319, 0.9773030877113342, 0.9742901921272278, -0.03766534850001335, 0.31333261728286743, 0.26404744386672974, -0.6279427409172058, 0.5906897783279419, 0.3263571560382843, 0.2917560040950775, 0.6136214733123779, -0.05902472138404846, -0.4049830436706543, 0.2916812002658844, -0.5831131935119629, -0.30871254205703735, 0.6439377665519714, 0.6315135955810547, 0.841555655002594, 0.2916235029697418, -1.050935983657837, -0.3960615396499634, 0.2946334779262543, 0.4419794976711273, 1.8914376497268677, -0.2912563681602478, 0.09391418099403381, -0.8761623501777649, -0.2258043885231018, -0.06256638467311859, 0.029950851574540138, -0.3029203414916992, -0.11402350664138794, -0.6457116007804871, -1.5394163131713867, 0.5621112585067749, -0.0339757539331913, 0.7629449367523193, -0.762305736541748, -0.6121127009391785, -0.4903322160243988, 0.5903202295303345, -0.6473466157913208, -0.8412057757377625, 0.6038161516189575, -0.9017460346221924, 0.35239189863204956, 0.006966474000364542, -0.1090940535068512, 0.16451609134674072, -0.25431182980537415, 1.1395196914672852, -0.5346042513847351, -0.45631077885627747, -0.19827775657176971, 0.8668919801712036, -0.5677343606948853, -0.5899667739868164, 0.6968495845794678, 0.2217186540365219, -0.31762099266052246, 0.07860337942838669, 0.44493454694747925, 0.45304957032203674, 0.0738750621676445, -0.27010729908943176, 0.37628424167633057, -0.13590559363365173, 0.07584260404109955, 0.8753572702407837, -0.4866948425769806, -0.2598245143890381, -1.3013554811477661, 1.3158377408981323, 0.19437123835086823, -0.8486695289611816, 0.5066415071487427, -0.7263851761817932, -0.05186895281076431, 0.5526365637779236, -0.9742338061332703, -0.4171464145183563, -0.8849208950996399, 0.1794193536043167, -0.550726592540741, 0.058347806334495544, -0.05765172466635704, 0.7203372716903687, -0.32542604207992554, 0.5831790566444397, 0.6310247778892517, 0.13712619245052338, -0.22323322296142578, 0.5230511426925659, -0.8929959535598755, 0.21516045928001404, -0.009440437890589237, -0.053113363683223724, -0.33555734157562256, -0.28415578603744507, -0.6196920871734619, -0.3284107744693756, -0.2425658404827118, -0.2678239941596985, -0.15101130306720734, 0.10221383720636368, -0.37103232741355896, -1.139874815940857, 0.14405721426010132, -0.8024454116821289, -0.48634225130081177, 0.43281620740890503, -0.24797391891479492, -0.15189577639102936, -1.2130483388900757, -1.3345856666564941, -0.17468853294849396, -1.003851294517517, -1.2346115112304688, 0.5991926193237305, 0.034605707973241806, -0.21068806946277618, -0.4430229663848877, -0.09876085817813873, -0.22544170916080475, 1.10478675365448, -0.8646692633628845, 0.7195082306861877, 0.26077067852020264, 0.1621214896440506, -0.3758564591407776, 0.09549842029809952, 0.11415117233991623, -0.6953452229499817, 0.2827110290527344, -0.7533863186836243, 0.04357472434639931, -0.35652437806129456, -0.7656536102294922, 0.1959313005208969, 0.26556846499443054, 0.7053406238555908, 0.3743591606616974, -0.6870622634887695, 0.92167729139328, 1.3240631818771362, -0.7293031215667725, -3.451317752478644e-05, 0.12588199973106384, 1.3775038719177246, -0.41276663541793823, -0.17655591666698456, 0.7006586790084839, -0.040197934955358505, 0.6080489158630371, 0.1889898180961609, -0.19951875507831573, 0.08878707885742188, -0.2284255474805832, 0.1453302502632141, 1.5306237936019897, 0.33679747581481934, -0.19408027827739716, -0.9510437846183777, 0.06703382730484009, -1.095297932624817, -0.3029322624206543, 0.30632397532463074, 0.8423800468444824, 0.510725200176239, -0.14371468126773834, -0.36305126547813416, -0.8208895921707153, 0.4148562252521515, 0.4628545939922333, -0.849999725818634, -1.1144003868103027, 0.053861312568187714, 0.21565952897071838, 0.21569733321666718, 0.47421398758888245, -0.11230514943599701, 1.114672303199768, 14.535547256469727, 1.307538390159607, -0.29622966051101685, 0.37388142943382263, 0.9274124503135681, -0.19763179123401642, -0.17235808074474335, -0.6342501044273376, -1.4596869945526123, -0.12158990651369095, 1.301069974899292, 0.28253281116485596, 0.9502029418945312, 0.5180191397666931, -0.12167809158563614, 0.15676800906658173, -0.3608473539352417, 1.0823801755905151, 0.49616941809654236, -1.116501808166504, 0.11797294020652771, 0.2667936682701111, 0.43828362226486206, 1.0390257835388184, 0.8315088748931885, 1.0436700582504272, 0.6390244364738464, -0.4405777156352997, 0.44083431363105774, 0.1058962345123291, 0.6401385068893433, -0.24150823056697845, -0.07920432835817337, 0.703693151473999, -0.5966466665267944, 0.13835106790065765, -0.7354995012283325, -0.9658571481704712, 0.08747074007987976, -0.11556212604045868, -0.5937789082527161, -0.7350573539733887, -0.03415438160300255, 0.25879040360450745, 0.06821141391992569, 0.14418785274028778, 0.17770233750343323, 0.21707746386528015, -0.5565770268440247, -0.07104701548814774, 0.5699478983879089, -0.22470919787883759, 0.1663774698972702, -0.16372722387313843, 0.16425427794456482, 0.1390596181154251, 0.013237424194812775, 0.7819379568099976, -0.41427117586135864, -0.1752103716135025, 0.20045533776283264, -0.32212334871292114, -0.2788250148296356, 1.1390178203582764, 0.2674710750579834, 0.4490884840488434, -0.19338278472423553, 0.695959746837616, 1.388970971107483, -0.021878579631447792, -0.7035625576972961, 0.5163721442222595, 0.3234809637069702, -0.866787314414978, 0.07735831290483475, 0.47532257437705994, -0.14008688926696777, -0.6609945893287659, -0.42909273505210876, -0.6745828986167908, 0.10951079428195953, -0.5300989747047424, -0.5197465419769287, 0.9782902598381042, -0.46904799342155457, -0.06738951057195663, 0.25718584656715393, -0.678565502166748, 0.0018650729907676578, 0.7874861359596252, -1.2676000595092773, -0.42299196124076843, 0.6329700946807861, -0.6875410676002502, -0.5033491849899292, 0.11724003404378891, 1.4935702085494995, 0.1461775153875351, -0.9004826545715332, 0.2657422125339508, 0.439900666475296, -0.374076783657074, -0.30161941051483154, -0.3672885000705719, 1.5757226943969727, -0.10594163835048676, 0.019860902801156044, 0.042386721819639206, -0.030070345848798752, 0.14196237921714783, -1.0606180429458618, 0.17142798006534576, 0.6049860715866089, -0.7867281436920166, -0.23123866319656372, -0.9779040217399597, -0.9193719029426575, 0.12592114508152008, 0.11768729239702225, 0.08592608571052551, 0.03733863681554794, 0.3772374093532562, -0.5620373487472534, -0.009737635962665081, -0.5648353695869446, 0.14569731056690216, 0.6829319000244141, -0.44554004073143005, -0.01879352144896984, 0.19463840126991272, 0.2664221227169037, -1.7119606733322144, -0.7974903583526611, -0.34240487217903137, 0.28247132897377014, -0.17950598895549774, 1.27569580078125, -0.8293298482894897, 0.5262849926948547, 1.2198493480682373, 0.0744258463382721, -0.7879158854484558, 0.025151021778583527, -0.42423030734062195, 0.09860826283693314, -0.16363969445228577, 0.745571494102478, -0.5645332336425781, 0.49667567014694214, 1.4932022094726562, 0.33138903975486755, -0.6940178275108337, -0.29265937209129333, -0.3361051678657532, 0.06564226001501083, -0.5469464063644409, 0.4908112585544586, -0.06333830952644348, -0.14794528484344482, 0.19874145090579987, 0.17883092164993286, 0.4299025535583496, -0.31342747807502747, -0.37912002205848694, 0.3995836079120636, -0.07149700820446014, -0.23422940075397491, -0.5942820310592651, 0.026023730635643005, -1.3853503465652466, -0.09726471453905106, -1.278145432472229, -0.0847751572728157, -0.5254713892936707, -0.5296083688735962, -0.1920706033706665, -0.20292766392230988, 0.05255305767059326, 0.2609237730503082, -0.350147545337677, -0.3196980059146881, -0.4283844828605652, -0.32617026567459106, 0.8207138180732727, 0.9363731741905212, -0.5719730854034424, -0.26443469524383545, 0.053136877715587616, 0.3291757106781006, 0.20460693538188934, 0.5930141806602478, -0.6238858103752136, -0.8533329963684082, -1.6206105947494507, 0.3808256983757019, 0.14588218927383423, -0.30684977769851685, -0.7208372950553894, 0.6799149513244629, 0.0979602262377739, -0.41613316535949707, 0.2420465052127838, 0.5363654494285583, -0.9661663770675659, -0.41631197929382324, 0.2495349943637848, -0.546978771686554, 0.34165769815444946, 0.765976071357727, -0.8610677123069763, -0.0936819389462471, 0.4876909554004669, -0.049126651138067245, -0.930518627166748, -1.0155097246170044, 0.6613752841949463, -0.23628273606300354, 0.3014925420284271, -0.6023749709129333, 0.005408742930740118, -0.9625740051269531, 0.11189906299114227, 0.12408576160669327, 0.5290815234184265, -0.4245204031467438, 1.0898617506027222, 0.25375112891197205, -1.0861369371414185, 0.11014475673437119, 0.5342201590538025, -0.1702778935432434, -0.10921895503997803, 0.7572588324546814, 0.394338458776474, -0.6870424747467041, 0.5474433302879333, 0.06289937347173691, 0.3494528830051422, -1.1141581535339355, 0.1501554250717163, 1.045473575592041, -0.7362295389175415, -0.3163701593875885, 1.4814939498901367, -0.17894485592842102, -1.4423563480377197, 0.003169181989505887, -0.8642496466636658, -0.3859878480434418, -0.6979402303695679, 0.5161653161048889, 0.2186325043439865, 0.17748278379440308, -0.13179422914981842, -0.6758323907852173, 0.2168613076210022, -0.23514121770858765, -0.5136011838912964, 0.4509914219379425, -0.12695513665676117, -0.37410813570022583, 0.44405555725097656, 1.1187708377838135, -1.0812454223632812, -0.8678827285766602, -0.6743059158325195, -0.5571287870407104, 0.10260192304849625, 0.5823714733123779, -0.4544681906700134, -0.9171794652938843, 0.7707496285438538, 0.5726699233055115, -0.07667210698127747, -0.11376690119504929, -0.6995314359664917, 0.1651255041360855, 1.018268346786499, 0.12493963539600372, -0.8868201375007629, -1.03651762008667, 1.4138137102127075, 1.1581685543060303, -1.2752801179885864, -0.06626590341329575, -0.1546715348958969, -1.0935875177383423, 0.7050604820251465, 0.6543428301811218, 0.1523066610097885, 0.7908271551132202, -0.2847664952278137, -0.09285643696784973, -0.12730024755001068, -0.9530957937240601, -0.21018999814987183, 1.183834433555603, 0.35453978180885315, 0.5639114379882812, 0.2767691910266876, 0.08930771797895432, 0.48155686259269714, -0.1481017768383026, 0.1975160390138626, 0.12189038097858429, 0.5461767315864563, 0.016595613211393356, 0.11403681337833405, -0.003317026887089014, 0.8577390909194946, -0.3107112944126129, -1.0337799787521362, 0.21449477970600128, 0.5879262089729309, -0.027416346594691277, 0.32248446345329285, 0.8314515948295593, 0.0675308033823967, 0.5429208278656006, 0.13836750388145447, 0.5859110951423645, -0.3009858727455139, -0.4090965688228607, -0.06711014360189438, -0.6627235412597656, -0.040605995804071426, 0.2730427086353302, -0.2615806758403778, -0.7300487160682678, -0.4180123209953308, 0.2678804099559784, 0.07540298998355865, 0.5235386490821838, 1.1001627445220947, 0.7645756006240845, 0.662956714630127, -0.5506060719490051, -0.5755230188369751, -0.5258592963218689, -0.7178589105606079, -0.24930942058563232, -0.6182041168212891, -0.792512834072113, -0.10393484681844711, 0.20172694325447083, -0.690218985080719]}, "authors": [{"authorId": "2290691283", "name": "Changyue Liao"}, {"authorId": "2182376343", "name": "Mo Sun"}, {"authorId": "2267639447", "name": "Zihan Yang"}, {"authorId": "2267730662", "name": "Kaiqi Chen"}, {"authorId": "2290743423", "name": "Binhang Yuan"}, {"authorId": "2187558910", "name": "Fei Wu"}, {"authorId": "2283130898", "name": "Zeke Wang"}], "references": [{"paperId": "07b5926554222159609ee2752ed158d4d5fe0cbc", "title": "G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations"}, {"paperId": "2a55a14eb403128f9be2b85329202f501eb19cd8", "title": "STR: Hybrid Tensor Re-Generation to Break Memory Wall for DNN Training"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "bab5e35001757719d0f8338f94dde2860dae784a", "title": "How Large Language Models Will Disrupt Data Management"}, {"paperId": "d7e00702bbb5a0cccc97033f0405b634ae9e2d3c", "title": "Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent"}, {"paperId": "43cefce076df7ee54505fd78a8a97129c0f6d36b", "title": "MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism"}, {"paperId": "58d6ec0dec4952e93be7cf72c1cebb0216eac9df", "title": "Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers"}, {"paperId": "be157d55b4afd5be9c81619d75aa4897f5e201e4", "title": "Elixir: Train a Large Language Model on a Small GPU Cluster"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632", "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"}, {"paperId": "1a6a7fe065e42365515ccf7d0b5d1225b8088464", "title": "STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training"}, {"paperId": "1913ead27485fe83b405529e8d294d4b595f1e3a", "title": "POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "ba377c911b1d601cbe6a7f10f89e5b6175adcd98", "title": "TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "0759c30c37c2a2b868d0f768a9823a768db1e306", "title": "HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework"}, {"paperId": "10c0a1d3519dcc7b876d21d614f49d82467c9dc3", "title": "Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "3d0853eb0429fff9a6ff04fd46ea221e4f84fbf2", "title": "The Case for NLP-Enhanced Database Tuning: Towards Tuning Tools that \"Read the Manual\""}, {"paperId": "adf4b72532b970b6d891e3a67e5cc7b3fb3839b3", "title": "Sentinel: Efficient Tensor Migration and Allocation on Heterogeneous Memory Systems for Deep Learning"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "d491b204c0f7b9dadce9b6f29cb8e407dd107dff", "title": "Optimal GPU-CPU Offloading Strategies for Deep Neural Network Training"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "3c55dd7b8da5c7b47e91b2e749c264f50d007cd4", "title": "Dynamic Tensor Rematerialization"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b11c87681ee46ab8c82bf96d97f958bd95b8f0a8", "title": "Capuchin: Tensor-based GPU Memory Management for Deep Learning"}, {"paperId": "7cb7b80e50fd5418b7f47326e60ce32c9a0f6b38", "title": "SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via Smart Swapping"}, {"paperId": "6a5c0fc737b6fbd6672fc4265b5e0ca38de17416", "title": "Training Large Neural Networks with Constant Memory using a New Execution Algorithm"}, {"paperId": "74d04cc70c0828b15af604efaf3c5b9e0be55a0e", "title": "Optimal memory-aware backpropagation of deep join networks"}, {"paperId": "c591ffe721883f0360c1611d3c8c43dfa6e96c60", "title": "Optimal checkpointing for heterogeneous chains: how to train deep neural networks with limited memory"}, {"paperId": "fd431005d26100f5453590080683cbae9dc1189f", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "81d2f80e258fe2f26f1418fae6b9fd424241ac44", "title": "A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation"}, {"paperId": "fc83a8348eee59a2087c5075673a8d37679ae913", "title": "Dynamic Memory Management for GPU-Based Training of Deep Neural Networks"}, {"paperId": "181321e61a8a068f930f55f213df141c5a080211", "title": "Efficient Memory Management for GPU-based Deep Learning Systems"}, {"paperId": "9888edfb6276887eb56a6da7fe561e508e72a517", "title": "Layer-Centric Memory Reuse and Data Migration for Extreme-Scale Deep Learning on Many-Core Architectures"}, {"paperId": "ed9dfd432361e428ee084fb43d6165d388e0d887", "title": "TFLMS: Large Model Support in TensorFlow by Graph Rewriting"}, {"paperId": "ca45e17cf41cf1fd0aa7c9536f0a27bc0f4d3b33", "title": "Superneurons: dynamic GPU memory management for training deep neural networks"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "b1ee2e7040c396e2002022f876abc6dec61aa501", "title": "vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design"}, {"paperId": null, "title": "Defines: Enabling fastexplorationofthedepth-firstschedulingspacefordnnaccelerators throughanalyticalmodeling"}, {"paperId": null, "title": "Colossal examples"}, {"paperId": "00fd969c88ca880654520d2184e30bf73a2f3992", "title": "Microsoft"}, {"paperId": "e19b81683e9c9cae0bf0935e96b10e8bbc09d1e1", "title": "Efficient Combination of Rematerialization and Offloading for Training DNNs"}, {"paperId": "69b318a004d98ace66d47a81d9f0dc92b309d6b1", "title": "FlashNeuron: SSD-Enabled Large-Batch Training of Very Deep Neural Networks"}, {"paperId": "d668f12be54174141e6197fad737006b7b0c0571", "title": "PyTorch"}, {"paperId": null, "title": "Optimalgradientcheckpointsearchforarbitrary computation graphs"}, {"paperId": null, "title": "Zero:Memoryoptimiza-tions toward training trillion parameter models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "NVIDIA"}, {"paperId": null, "title": "Memory-efficientbackpropagationthroughtime"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": "e73f87a22a17812391c55e87c292cffec29f5dbe", "title": "Automatic differentiation"}, {"paperId": null, "title": "etal.Llama:Openand efficientfoundationlanguagemodels"}, {"paperId": null, "title": "Alpa:Automatinginter-andIntra-Operatorparallelismfordistributeddeeplearning"}]}