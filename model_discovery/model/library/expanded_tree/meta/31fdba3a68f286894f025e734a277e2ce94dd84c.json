{"paperId": "31fdba3a68f286894f025e734a277e2ce94dd84c", "abstract": "Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks. The code is available at https://github.com/yuweihao/MambaOut", "venue": "arXiv.org", "year": 2024, "citationCount": 10, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper conceptually concludes that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics, and constructs a series of models named MambaOut, which surpasses all visual Mamba models on ImageNet image classification and demonstrates the potential of Mamba for long-sequence visual tasks."}, "embedding": {"model": "specter_v2", "vector": [0.14473381638526917, 0.8096888065338135, -0.23386500775814056, 0.2296101301908493, -0.2924451529979706, 0.12288782000541687, 0.9530082941055298, -0.2044716328382492, -0.35759401321411133, -0.664462149143219, 0.09349457919597626, 0.3565138578414917, 0.733363151550293, 0.39974257349967957, -0.13683995604515076, -0.14482681453227997, -0.7339314222335815, -0.22728084027767181, 0.2846938669681549, -0.211469367146492, 0.12934303283691406, -0.5958791375160217, -1.1259863376617432, 0.17783617973327637, -0.2609538435935974, 1.1915287971496582, 0.32540714740753174, 1.1596013307571411, -0.37023672461509705, 1.0589104890823364, 0.20975230634212494, -0.10871320962905884, 0.4335598647594452, -0.2963760495185852, -0.7161635756492615, 0.11426734179258347, 1.163995623588562, -0.372808575630188, -0.7996981739997864, 0.9224494099617004, -0.4654959738254547, 0.4477081298828125, 0.44402652978897095, -0.7909156084060669, 0.012521310709416866, 0.6675968766212463, 0.5714547038078308, 0.9131675362586975, -1.0324435234069824, 0.049991730600595474, 0.9970284104347229, -1.0008188486099243, -0.0737219899892807, 1.565482258796692, 0.3448975682258606, 0.7401118278503418, -0.14221565425395966, -0.7100005149841309, 1.3130520582199097, 0.4613281786441803, -0.5759929418563843, -0.557705283164978, 0.13740970194339752, -0.47007909417152405, 1.4516868591308594, -0.4774089753627777, 0.14056478440761566, 0.7540034055709839, 0.3794417977333069, 1.4906225204467773, 0.014981014654040337, -0.5912058353424072, 0.04623767361044884, -0.14950107038021088, 0.6386101841926575, 0.7000146508216858, -0.4098462760448456, 0.20590081810951233, -0.7563859820365906, 0.4725545048713684, 0.646653950214386, 0.12063281238079071, 0.6978718042373657, 0.044035036116838455, -0.5385596752166748, 0.5288228988647461, 0.8153736591339111, 0.5822503566741943, -0.5178507566452026, 1.390108585357666, 0.2944033145904541, 0.15409086644649506, -0.4866142272949219, 0.6166552901268005, 0.14364272356033325, 0.7994230389595032, -0.8572652339935303, -0.04855070635676384, -0.5983611345291138, 1.073794960975647, -0.39225074648857117, 0.7307032942771912, -0.8008401393890381, -0.16800282895565033, 1.511055588722229, -0.35241222381591797, 0.6948999762535095, -1.0238441228866577, -0.18781249225139618, -0.8369474411010742, 0.24686414003372192, -0.5760871767997742, -0.15604615211486816, -0.42917901277542114, -1.071412444114685, -0.5763614773750305, 0.18083631992340088, 0.7899817228317261, -1.5338480472564697, 0.7290493249893188, -0.604783833026886, 0.379729300737381, 0.07710781693458557, 0.33144325017929077, 0.53659987449646, 0.7645652890205383, 0.7836015224456787, 0.16944414377212524, 1.312098741531372, -1.2961723804473877, -0.7697885036468506, -1.1196856498718262, -0.17183881998062134, -0.0458136722445488, -0.03368310630321503, 0.07590045034885406, -1.2458690404891968, -1.5156265497207642, -1.0770044326782227, -0.05658324807882309, -0.6584958434104919, -0.03934710845351219, 0.947630763053894, 0.10679629445075989, -0.9006310105323792, 0.6249564290046692, -0.7025410532951355, -0.3420177698135376, 0.7703981399536133, 0.05577873811125755, 0.4302288889884949, 0.09247077256441116, -0.7339195013046265, 0.8573220372200012, 0.1576467752456665, 0.18915987014770508, -0.8935790061950684, -0.14265719056129456, -1.1340672969818115, 0.276471346616745, 0.09095757454633713, -0.6354985237121582, 1.5139693021774292, -0.5561717748641968, -0.8700881004333496, 0.501968502998352, -0.5002315640449524, -0.1750575304031372, 0.08725570142269135, -0.020703449845314026, -0.40972667932510376, -0.5807899832725525, -0.5881862044334412, 1.1761107444763184, 1.065068006515503, -0.1484578251838684, -0.3114522695541382, 0.07761150598526001, -0.20856182277202606, -0.7045662999153137, -0.35402265191078186, 0.7210960984230042, -0.20987997949123383, -0.18360719084739685, 0.7124330401420593, 0.5032238960266113, -0.35896724462509155, -0.16391132771968842, -0.36390408873558044, -0.9142767190933228, 0.439962238073349, 0.3178798258304596, 0.4785443842411041, -0.8063393235206604, -1.036781668663025, -0.5155133008956909, -0.0725109875202179, -0.09201294928789139, -1.0497753620147705, 0.11331871151924133, -0.25853437185287476, 0.21795763075351715, 0.09250261634588242, -0.89176344871521, -0.1667829155921936, -0.6828338503837585, -0.586859405040741, 0.0736779272556305, 0.6344137787818909, 1.1025676727294922, -0.9256663918495178, -0.18827758729457855, 0.10927969217300415, -0.18344372510910034, -0.7523458003997803, 0.95665442943573, -0.5376898050308228, 0.3622470200061798, 0.15162228047847748, 0.20932209491729736, 0.0695757195353508, -0.5037437677383423, 0.3528978228569031, -0.7239142060279846, -0.07817916572093964, 0.4389583468437195, -0.455574095249176, 1.6360273361206055, -0.08465570956468582, 1.1309309005737305, -0.21082529425621033, -0.7970380783081055, -0.009359808638691902, 0.42341727018356323, -0.3101169764995575, -0.4448719024658203, 0.3776867985725403, -0.15328702330589294, -0.4545627236366272, 0.24779029190540314, 0.6388240456581116, 1.1531974077224731, -0.37243008613586426, -0.3857392966747284, 0.8252317309379578, -0.18533283472061157, -0.08468775451183319, 0.3942434787750244, 0.5794669985771179, -0.007467405870556831, 0.4181841313838959, -0.1979960948228836, 0.18691469728946686, -0.6756649613380432, -0.08104004710912704, 1.123581886291504, 0.14162403345108032, 0.9714652895927429, 0.295964777469635, -1.064178228378296, -0.3258450925350189, -0.11198340356349945, 0.5415710806846619, 1.0483311414718628, 0.32046133279800415, 0.23934225738048553, -0.6305602788925171, -0.20486605167388916, -0.6109628677368164, -0.4104161262512207, -0.3760420083999634, -0.177375927567482, -0.4567050635814667, -0.8988893032073975, 0.6816534996032715, 0.27588650584220886, 1.0354902744293213, -1.1755051612854004, -0.4269794523715973, -0.13451281189918518, 0.27475327253341675, -0.9120748043060303, -0.48228690028190613, 0.13692043721675873, -0.6168080568313599, -0.27140262722969055, 0.14983071386814117, -0.397094190120697, 0.16917677223682404, -0.4081932604312897, 0.6846529245376587, -0.7609193921089172, -0.42032375931739807, -0.15847839415073395, 0.8770855665206909, -1.1149989366531372, -0.5438169240951538, 0.18142089247703552, -0.03854261338710785, 0.03323864936828613, 0.1996675580739975, 0.5135838985443115, -0.42310571670532227, -0.09829794615507126, -0.43380415439605713, -0.0341847687959671, 0.4188516438007355, 0.06958525627851486, 0.7074741721153259, -0.5923262238502502, 0.6412472128868103, -0.6407267451286316, 0.4553864002227783, -0.050580449402332306, -0.7487147450447083, -0.05956105887889862, -0.7126556634902954, -0.10742415487766266, 0.09387466311454773, -0.9585869908332825, 0.09942855685949326, -0.10340336710214615, 0.39140206575393677, -0.814013659954071, -0.4373168647289276, -0.15218372642993927, 0.5387079119682312, 0.1257958561182022, 0.2855352759361267, 0.3870636522769928, 0.34300845861434937, 0.08153679221868515, 0.5716446042060852, -1.0640995502471924, 1.008039116859436, 0.8619415760040283, 0.019267290830612183, 0.3369585871696472, 0.18310299515724182, -0.888299822807312, -0.6624427437782288, -0.5921846032142639, -0.048597726970911026, -0.5676525831222534, 0.4708467423915863, -0.9168086647987366, -1.0111274719238281, 0.38762566447257996, -1.3225715160369873, -0.16055665910243988, 0.06108071655035019, 0.006893905811011791, -0.28005877137184143, -1.3440965414047241, -0.9544543623924255, -0.7891029119491577, -0.3275837302207947, -0.7850982546806335, -0.04711553454399109, 0.480170875787735, -0.34678903222084045, 0.007402521092444658, -0.09882687032222748, -0.4502359926700592, 1.1709100008010864, -0.4399167001247406, 0.3242456912994385, -0.2461756467819214, -0.7284381985664368, -0.4489825963973999, 0.25366678833961487, 0.7448359131813049, -0.24466216564178467, 0.3224033713340759, -1.5119707584381104, 0.26580193638801575, -0.14611007273197174, -0.3072188198566437, 0.8698326349258423, 0.9289106130599976, 0.8149600028991699, -0.06276056915521622, -0.048949919641017914, 0.2887161076068878, 1.1663748025894165, -0.4926288425922394, 0.4405621886253357, 0.0980118066072464, 0.9826313257217407, 0.1908368021249771, -0.14522431790828705, 0.14795471727848053, 0.36333969235420227, 0.1563289761543274, 0.9242136478424072, -0.20954519510269165, -0.2607783377170563, -0.38529956340789795, 0.3779900372028351, 0.9049153923988342, 0.5519403219223022, -0.007243720814585686, -0.8250524997711182, 1.0686510801315308, -1.4020012617111206, -1.1081748008728027, 0.6442460417747498, 0.6836686730384827, -0.12761229276657104, -0.24278444051742554, -0.04738457128405571, -0.2513914108276367, 0.44905799627304077, 0.5128543972969055, -0.2194136381149292, -0.75794517993927, 0.11330126971006393, 0.2463158816099167, 0.13382501900196075, 0.5283815264701843, -0.836348831653595, 0.8014862537384033, 14.602677345275879, 0.550102174282074, -0.43247583508491516, 0.5091638565063477, 0.7974745035171509, 0.32518595457077026, 0.31683728098869324, 0.1357843428850174, -1.1056956052780151, -0.03047204576432705, 1.1660423278808594, 0.8299291729927063, 0.21508367359638214, 0.39142557978630066, -0.06295925378799438, 0.24668779969215393, -0.7473060488700867, 0.837380051612854, 0.2685639262199402, -1.4824837446212769, 0.23256917297840118, -0.09091474860906601, 0.33178314566612244, 0.44830891489982605, 0.8328201174736023, 1.0448553562164307, 0.3129573166370392, -0.4306131601333618, 0.716964602470398, 0.5160123705863953, 0.49839115142822266, 0.40920960903167725, 0.03693033754825592, 0.524756908416748, -0.9755182862281799, -0.30238133668899536, -0.6126481294631958, -1.0729789733886719, -0.03448295220732689, -0.21152229607105255, -0.2515878975391388, -0.8747217059135437, -0.2659243047237396, 0.23673680424690247, -0.1330367475748062, 0.8167572617530823, 0.032183997333049774, 0.4852583408355713, 0.2753527760505676, 0.030555829405784607, 0.6000393033027649, 0.8167528510093689, 0.3442215621471405, 0.13449478149414062, -0.2054234743118286, 0.05024133250117302, 0.2633500397205353, 0.43738701939582825, -0.593296468257904, -0.6219860315322876, 0.11709146201610565, -0.26550936698913574, -0.07644986361265182, 1.0033891201019287, 0.20769661664962769, -0.08518488705158234, -0.08205423504114151, 0.040656913071870804, 0.016740083694458008, 0.2730797827243805, -0.3910119831562042, -0.10328418761491776, 0.6655836701393127, -0.7035032510757446, 0.2970501780509949, 0.493277370929718, -0.25891804695129395, -0.5875669717788696, -0.7054352760314941, -0.13668681681156158, 0.3944370150566101, -0.705558180809021, -0.5416435599327087, 1.1605507135391235, -0.2659239172935486, -0.4602580666542053, 0.19349050521850586, -0.63286954164505, -0.5384435653686523, 0.4508536159992218, -1.5599839687347412, -0.9845471382141113, -0.44572511315345764, -0.26778513193130493, 0.42166897654533386, -0.07945242524147034, 0.8653538227081299, 0.03923489898443222, -0.4548094570636749, -0.12890078127384186, -0.4088385999202728, 0.05617411434650421, -0.3283441960811615, -0.5735613107681274, 1.1337088346481323, 0.5836567282676697, 0.1810857504606247, -0.153462216258049, 0.07086731493473053, 0.6401979327201843, -0.23184971511363983, -0.19369854032993317, 0.4434794783592224, -1.1189342737197876, -0.5919898152351379, -0.7288632988929749, -0.8273136019706726, 0.38858941197395325, 0.45916423201560974, 0.2618185877799988, -0.6092805862426758, 0.005204514134675264, -0.6467130780220032, -0.3472782075405121, -0.46574312448501587, -0.08195209503173828, 0.0017029328737407923, -0.6317559480667114, -0.4343337416648865, -0.24331103265285492, 0.21785390377044678, -0.6202473044395447, -0.020839082077145576, -0.3590587079524994, 0.5630103349685669, -0.019592661410570145, 1.1301177740097046, -1.0683319568634033, 0.5549608469009399, 0.6807262897491455, 0.014194534160196781, -0.649549663066864, -0.30491605401039124, -0.8279687762260437, -0.23500454425811768, 0.13767297565937042, 0.35937005281448364, -0.3412635326385498, 0.45797112584114075, 0.4767898917198181, 0.12849979102611542, -0.20298083126544952, -0.6804656386375427, -0.7131546139717102, -0.0769050195813179, -0.34993284940719604, 0.06540022045373917, 0.028220847249031067, -0.20176398754119873, 0.0074193403124809265, 0.29556700587272644, 0.5883351564407349, -0.09373363107442856, -0.6074542999267578, 0.3777157962322235, -0.19462227821350098, 0.17190715670585632, -0.7505064606666565, -0.9500212669372559, -1.3400254249572754, -0.23268458247184753, -0.86021888256073, 0.3928786814212799, -0.9212717413902283, -0.6692344546318054, 0.2278873473405838, -1.047606110572815, -0.08336275815963745, 0.23206664621829987, -0.3858356475830078, 0.2549656331539154, -0.188970148563385, -0.6732468008995056, 0.46664661169052124, 0.7690424919128418, -1.0856119394302368, -0.0786513164639473, 0.20677518844604492, -0.2289896309375763, 0.6168904900550842, 0.30869218707084656, -0.6453999876976013, -0.5521358251571655, -0.9329142570495605, -0.18042157590389252, 0.18201540410518646, 0.13100171089172363, -1.4518239498138428, 0.9915598630905151, 0.09729879349470139, 0.06797703355550766, 0.009450484067201614, 0.8324275016784668, -0.8007349967956543, -0.6194559335708618, 0.6195910573005676, -1.048660159111023, 0.07248736172914505, 0.21303777396678925, -0.2335449606180191, -0.5002808570861816, 0.8506318926811218, 0.5233635306358337, -1.187997817993164, -1.1375559568405151, 0.520226001739502, -0.7130690813064575, -0.15198850631713867, -0.07456782460212708, -0.17963972687721252, -1.2492012977600098, -0.17741835117340088, -0.25160709023475647, 0.36738643050193787, -0.8810523152351379, 0.9992928504943848, 0.7215742468833923, -0.8206779956817627, -0.018580034375190735, 0.2709699273109436, -0.017055612057447433, 0.20653901994228363, 0.5008545517921448, 0.541431188583374, -0.2225998193025589, 0.44720911979675293, -0.33133429288864136, 0.06938294321298599, -0.396639347076416, 0.41833606362342834, 0.8931761384010315, -0.26595091819763184, 0.16357040405273438, 1.141903042793274, -0.0023948627058416605, -0.4428043067455292, 0.45370686054229736, -1.3240071535110474, -0.612353503704071, 0.1699126809835434, 0.5739134550094604, 0.26271525025367737, -0.09278489649295807, 0.15195956826210022, -0.5771389603614807, -0.21142257750034332, -0.13623866438865662, -0.553369402885437, -0.0025489928666502237, -0.03713683411478996, 0.13490290939807892, 0.7612078785896301, 0.6657283902168274, -1.068434715270996, -0.9732145071029663, -0.5798783898353577, -0.1608259528875351, -0.13006490468978882, 0.13562290370464325, -0.011736581102013588, -0.6722857356071472, 0.9142763614654541, 0.9776735901832581, 0.3563825488090515, 0.012824132107198238, -0.12107779085636139, -0.0374944731593132, 0.8720012307167053, -0.3000166118144989, -0.22938278317451477, -0.27041321992874146, 1.1249686479568481, 1.1102352142333984, -1.1822543144226074, 0.016206510365009308, -0.010977881029248238, -0.43173617124557495, 0.6742175221443176, 0.9569857120513916, -0.8196560740470886, 0.7421976327896118, -0.9687482714653015, 0.21186992526054382, 0.18084490299224854, -1.1947474479675293, -0.408771812915802, 0.8343538045883179, 0.8854910135269165, 0.2123642861843109, -0.13276180624961853, 0.5152905583381653, 0.6407730579376221, 0.5072311162948608, -0.35152503848075867, 0.6642700433731079, 0.2460252046585083, -0.42524898052215576, 0.5101115107536316, -0.03847306966781616, 0.7360965609550476, -0.6371313333511353, -0.4065510928630829, 0.28913137316703796, 0.7290468215942383, 0.0867842361330986, 0.6669203639030457, 1.445931077003479, 0.15029573440551758, 0.4521208703517914, 0.3388384282588959, 0.5957708358764648, -0.37906569242477417, -0.3625050187110901, -0.35922157764434814, -1.001474380493164, -0.17619197070598602, -0.4042070806026459, -0.6989166140556335, 0.09904541820287704, 0.2765067517757416, 0.09314613044261932, -0.38447439670562744, 0.7209063768386841, 0.9635866284370422, 0.3108583986759186, 0.9240996241569519, -0.2761603891849518, -0.8206329345703125, -0.40119263529777527, -1.1924042701721191, 0.282039999961853, -0.5568229556083679, 0.5274476408958435, -0.29203730821609497, -0.012708661146461964, 0.21061989665031433]}, "authors": [{"authorId": "23476952", "name": "Weihao Yu"}, {"authorId": "48631088", "name": "Xinchao Wang"}], "references": [{"paperId": "47741d9e57f7a986a350f5cb20de287b1b1b8ff8", "title": "Visual Mamba: A Survey and New Outlooks"}, {"paperId": "f5cea4651c32d2ba95171fb264cc6680262c17d7", "title": "A Survey on Visual Mamba"}, {"paperId": "cbaf689fd9ea9bc939510019d90535d6249b3367", "title": "Jamba: A Hybrid Transformer-Mamba Language Model"}, {"paperId": "62ac3ef81e54e1d1930fb5980b236345ee2e4f32", "title": "PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition"}, {"paperId": "fea8a3096391a418cb9ef724e0ff9754e5a467fd", "title": "SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series"}, {"paperId": "9f1dc0ebf06841f988d7a1d12d1d2206c0707b53", "title": "EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba"}, {"paperId": "5867382590f9f0ff8caf15804d20bde10845b2d2", "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan"}, {"paperId": "906d0688e1c683d5fec70e88e71ea1291c666b78", "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data"}, {"paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97", "title": "VMamba: Visual State Space Model"}, {"paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444", "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "c34952e5e073cf645fd1f18bc4f3663c6dedcaf4", "title": "TransNeXt: Robust Foveal Visual Perception for Vision Transformers"}, {"paperId": "740512dfadd91cd52771f2a721509c371efb9b89", "title": "SG-Former: Self-guided Transformer with Evolving Token Reallocation"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "182e6c877155d7730378846380a3d1dc294fb54a", "title": "InceptionNeXt: When Inception Meets ConvNeXt"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "a3aa1323a7f08c40207eaa359041e5bd72b25b27", "title": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks"}, {"paperId": "493cf3728f49af4e47c2c2f928510ade1e31cf00", "title": "Rethinking Mobile Block for Efficient Attention-based Models"}, {"paperId": "3cb3052825d387a2a0eed29c92f07466d2f4a9ed", "title": "Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition"}, {"paperId": "26c80bd65baa90f5b18157de4951f4eb0b62ab69", "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"}, {"paperId": "975c1fa7bf9c0b9afaeb949e15764eeaf0ddfff0", "title": "MogaNet: Multi-order Gated Aggregation Network"}, {"paperId": "fc4cbc7a75f5a3bbca59db5513231555f078fe78", "title": "MetaFormer Baselines for Vision"}, {"paperId": "a8a2a8229f99c291bf71ec92b801a073854c52e2", "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"}, {"paperId": "161e729481cc78d75d906e6e9fefa080f7286d41", "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features"}, {"paperId": "3e448df5aa191f7a3945d0fd609c8bc5966a2333", "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"}, {"paperId": "d1869155960e4b1b882b39171dbecd25a7eda3cd", "title": "More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity"}, {"paperId": "dbf6e95cb618f207f029276a6df11f4a9a6313d4", "title": "Inception Transformer"}, {"paperId": "c431408780586268e8bcf2483b01a80728d10960", "title": "Vision Transformer Adapter for Dense Predictions"}, {"paperId": "ad7bcec33f5206d4f28687a6a5a950de67010651", "title": "Neighborhood Attention Transformer"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "fa717a2e31f0cef4e26921f3b147a98644d2e64c", "title": "Focal Modulation Networks"}, {"paperId": "ba637c4f1a170f1e2dadeadb71a63cf2b9a46de2", "title": "Visual attention network"}, {"paperId": "f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1", "title": "UniFormer: Unifying Convolution and Self-Attention for Visual Recognition"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "1fb10189c500e4902cd1b5afd406f57323d21be8", "title": "VOLO: Vision Outlooker for Visual Recognition"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "2a218786f4615b82389f78472e7ff22e6ce57490", "title": "ConvBERT: Improving BERT with Span-based Dynamic Convolution"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e", "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "4dbc68cf2e14155edb6da0def30661aca8c96c22", "title": "Simplifying ConvNets for Fast Learning"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85", "title": "Bidirectional recurrent neural networks"}, {"paperId": "30ed843e8e1f4893f5885bdc419e1df67b01dde3", "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements"}, {"paperId": "63f1f2dad0a2e84d37a97258008c5609195487f0", "title": "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"}, {"paperId": "5a9bc55f6332e38f62eb509b684147a1d4f10fd9", "title": "Focal Attention for Long-Range Interactions in Vision Transformers"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "MMSegmentation Contributors"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": null, "title": "The MambaOut model configurations"}]}