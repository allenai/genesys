{"paperId": "97833e2aa0da5240e62436373b58af988a4ab6ab", "abstract": "Transformer language models encode the notion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that matters, but relative position, and the extent to which APEs can capture this type of information has not been investigated. In this work, we observe that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information. Specifically, when models are subjected to sentences starting from a non-zero position (excluding the effect of priming), they exhibit noticeably degraded performance on zero to full-shot tasks, across a range of model families and model sizes. Our findings raise questions about the efficacy of APEs to model the relativity of position information, and invite further introspection on the sentence and word order processing strategies employed by these models.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 9, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.12574", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is observed that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information, which raises questions about the efficacy of APEs to model the relativity of position information."}, "embedding": {"model": "specter_v2", "vector": [-0.052382178604602814, 0.7649359107017517, -0.30061832070350647, -0.4683585464954376, 0.1625175178050995, -0.4226236045360565, 0.8588104248046875, -0.09710989892482758, -0.970523476600647, -0.602752149105072, 0.8593222498893738, -0.24381692707538605, -0.09725412726402283, -0.15676848590373993, 0.06484060734510422, -0.2652710974216461, -0.6847626566886902, 0.39277413487434387, 0.26847973465919495, -0.9884818196296692, 0.04535545036196709, -0.7763808965682983, -0.5556814074516296, 0.04961752891540527, -0.14213800430297852, 0.11469187587499619, 0.3006959855556488, 0.5193492770195007, -0.3747391104698181, 0.4302815794944763, 0.584755003452301, -0.6246752142906189, 0.5406963229179382, 0.3032775819301605, 0.04257742315530777, -0.38485318422317505, 0.6201260089874268, -1.174302339553833, -0.7048807144165039, 0.7937432527542114, -0.15128299593925476, 0.10260321199893951, 0.1699075549840927, -0.7383273839950562, -0.5699198246002197, 1.3905035257339478, 0.8380762338638306, 1.1901293992996216, -0.2940516471862793, -0.6443973183631897, 1.9830944538116455, -1.249383568763733, 0.4866720139980316, 1.1567456722259521, 0.7181979417800903, 0.5309673547744751, 0.08582650125026703, -0.17692559957504272, 0.6535266637802124, 0.13280265033245087, -0.46684637665748596, -0.5904583930969238, 0.3202332854270935, 0.11192809045314789, 1.2912169694900513, -0.5238432884216309, -0.051353711634874344, 0.7487511038780212, 0.13026532530784607, 1.3294258117675781, 0.5407641530036926, -1.0656890869140625, -0.07984715700149536, -0.14706869423389435, 1.1385246515274048, 0.9090273976325989, -0.3582443594932556, 0.9056820869445801, -1.169816255569458, 0.2951469421386719, 0.5460384488105774, -0.14047648012638092, 0.0600423701107502, -0.2495216429233551, -0.4932340383529663, 0.25268006324768066, 0.6314210891723633, 1.0386106967926025, -0.440165251493454, 0.6794459223747253, 0.3122580647468567, 0.3036534786224365, -0.21826937794685364, 0.28304243087768555, -0.17540942132472992, 0.5818896889686584, -0.7810125350952148, -0.12836460769176483, -0.00025067865499295294, 0.1211521252989769, -0.23110099136829376, 0.22237953543663025, -0.8329104781150818, -0.3629423975944519, 1.6934839487075806, -0.5230333209037781, 0.7480818033218384, -0.8148841857910156, 0.10554701089859009, -0.877503514289856, 0.49611592292785645, -1.1829986572265625, -0.17450986802577972, -0.20084162056446075, -0.11558762192726135, -1.4317878484725952, 0.19620631635189056, 0.7619706988334656, -0.8393061757087708, 1.3419979810714722, -0.10926539450883865, 0.14174029231071472, 0.13160103559494019, 0.4957558512687683, 0.5335817933082581, 0.900934100151062, 0.506645917892456, 0.269846647977829, 1.0490145683288574, -0.2653757631778717, -0.9738460183143616, -1.2963001728057861, 1.0183022022247314, -0.05507032573223114, 0.6989086270332336, -0.5400597453117371, -0.988906741142273, -0.9926142692565918, -1.4188743829727173, 0.18237383663654327, -0.7393107414245605, 0.3761623799800873, 0.7656042575836182, 0.6373858451843262, -1.6211650371551514, 1.19970703125, -0.4103672504425049, -0.1049787700176239, 0.2573912739753723, 0.12744997441768646, 0.08974852412939072, -0.43746045231819153, -1.6910561323165894, 0.6938591003417969, 0.5969403982162476, 0.06752125173807144, 0.2918390929698944, -0.9317881464958191, -1.5497221946716309, -0.15277007222175598, 0.38263651728630066, -0.27186471223831177, 1.1597806215286255, 0.425082266330719, -1.2363359928131104, 1.0445654392242432, -0.42697590589523315, 0.24247972667217255, -0.16502168774604797, -0.14276693761348724, -0.31737470626831055, -0.4095395803451538, 0.22660429775714874, 0.6390925645828247, 0.09813572466373444, -0.4065065383911133, 0.11283475160598755, -0.47754374146461487, -0.10450796037912369, -0.0032664581667631865, -0.48255330324172974, 0.9765070080757141, 0.6105889081954956, -0.1869848519563675, 1.0345969200134277, 0.9577325582504272, 0.15231221914291382, -0.27025771141052246, -0.09030874818563461, -1.5546066761016846, -0.009512248449027538, -0.19092628359794617, 1.0361162424087524, -0.874426543712616, -0.8254776000976562, -0.4775014817714691, 0.08990101516246796, -0.2412370890378952, -0.8596489429473877, 0.6100745797157288, -0.48465052247047424, 0.5380281805992126, 0.10598978400230408, -1.0091564655303955, 0.5146444439888, 0.14602160453796387, -0.707694947719574, -0.13770791888237, -0.3550760746002197, 1.188269019126892, -1.2055442333221436, 0.07885359972715378, 0.2341308742761612, 0.025635305792093277, -0.5566139817237854, 0.7603762149810791, 0.20495325326919556, -0.45194023847579956, 0.575270414352417, -0.6752097010612488, -0.06678005307912827, -0.23200078308582306, 0.38036400079727173, -0.7824357748031616, -0.3918015658855438, 1.0437849760055542, -0.5051213502883911, 1.1053763628005981, 0.054917506873607635, 0.3253718614578247, -0.2847915291786194, -0.35724005103111267, -0.12985281646251678, 0.23755168914794922, -0.2759797275066376, -0.6242260932922363, 0.6099283695220947, 0.4510952830314636, 0.046262722462415695, 0.5920807123184204, 1.0104345083236694, 0.3009949028491974, -0.455131471157074, 0.21979296207427979, 0.394906610250473, 0.2046063095331192, 0.46613603830337524, 0.7192271947860718, 0.2880218029022217, 0.28226298093795776, 0.4171198308467865, -0.652087926864624, 0.005441166460514069, -0.8782016038894653, -0.03780747205018997, 0.12343879044055939, 0.3314630687236786, 0.5167632102966309, 0.4881117641925812, -0.9096463918685913, -0.5618308186531067, -0.5787620544433594, 0.3388344943523407, 1.7275469303131104, 0.0891861543059349, -0.6374287605285645, -0.3859313428401947, 0.0971330776810646, -0.7705500721931458, 0.5442607998847961, -0.5449879765510559, -0.4805944263935089, -1.0178264379501343, -0.659320592880249, 0.47490575909614563, 0.5547677874565125, 0.7763828039169312, -0.5204418897628784, -0.6200343370437622, -0.1381692737340927, 0.34386295080184937, -0.7685356736183167, -0.5063807368278503, 0.24618025124073029, -0.49403107166290283, -0.08120039105415344, 0.20401525497436523, -0.18299053609371185, 0.32094237208366394, -0.6724710464477539, 0.9634360074996948, -0.6959071755409241, -0.012133385054767132, 0.3621490001678467, 0.5657395124435425, -0.33619827032089233, -0.6597939133644104, -0.08685941994190216, 0.4753122925758362, -0.2703470289707184, 0.5101724863052368, 0.8777616620063782, -0.3074689507484436, 0.17080166935920715, -0.5245625376701355, -0.16102711856365204, 0.09036093205213547, 0.02101658657193184, 0.44936221837997437, -0.3701886236667633, -0.023916075006127357, -0.8437872529029846, 0.7787347435951233, 0.4000236988067627, -0.46469730138778687, 0.3150479793548584, -0.6001056432723999, -0.31129416823387146, 0.2802296280860901, -0.06955916434526443, 0.08451361954212189, -1.1989461183547974, 0.4098811149597168, 0.04385081306099892, 0.018989862874150276, 0.20952408015727997, 0.13882271945476532, 0.7986862659454346, 0.24409052729606628, 0.3270088732242584, 0.6451059579849243, 0.24791958928108215, 0.6239940524101257, -1.1303417682647705, 0.3928486704826355, 0.4446496367454529, 0.0064962394535541534, -0.30546268820762634, -0.03911986202001572, -0.7264853119850159, -0.847495436668396, -0.3724996745586395, -0.5787186622619629, -0.17312470078468323, 0.41513678431510925, -0.3108336627483368, -0.8269151449203491, 0.2568342685699463, -1.1632754802703857, -0.23716726899147034, 0.017731722444295883, -0.4150964915752411, 0.02440422773361206, -0.7995297908782959, -1.4216700792312622, -0.16988947987556458, -0.27892354130744934, -0.5892153382301331, 0.29175758361816406, -0.09279366582632065, -0.7378814816474915, -0.501099705696106, 0.09854134172201157, -0.5125383734703064, 0.86887526512146, -0.9549988508224487, 1.0091496706008911, -0.38703927397727966, -0.3560752868652344, -0.4971081614494324, 0.6854116320610046, 0.5713335871696472, 0.07874872535467148, 0.31158581376075745, -0.24860943853855133, 0.15474331378936768, 0.018634295091032982, 0.01995161361992359, -0.01472118217498064, 0.44792526960372925, 0.18171429634094238, -0.3273499608039856, -0.2988324761390686, -0.3259318768978119, 1.3980923891067505, -0.5786237120628357, 0.47827601432800293, 0.28118714690208435, 0.8024113178253174, 0.6651996970176697, 0.01649438962340355, 0.08558109402656555, 0.6705082654953003, 0.3086934983730316, 0.25281667709350586, 0.31887087225914, -0.21844233572483063, -0.7381024360656738, 0.6521574258804321, 1.96160888671875, 0.3826824128627777, 0.1943366825580597, -1.2710920572280884, 0.7391870617866516, -0.7770783305168152, -0.983377993106842, 0.6238022446632385, 0.9384174942970276, 0.4337588846683502, -0.5211610794067383, -0.4334908723831177, 0.2625759243965149, 0.7064612507820129, 1.0124971866607666, 0.39970770478248596, -0.40792423486709595, 0.12979531288146973, 0.30972135066986084, 0.13841624557971954, 0.9694315195083618, -0.37613385915756226, 1.0680304765701294, 14.481307029724121, 0.12176631391048431, 0.05061737447977066, 0.3065212070941925, 0.5474908947944641, 0.6030483245849609, -0.6222091317176819, -0.07059101015329361, -1.4503470659255981, 0.28141215443611145, 1.2769911289215088, 0.20114628970623016, 0.16946527361869812, 0.04533475637435913, -0.23304826021194458, 0.2531701922416687, -0.8094900250434875, 0.47283077239990234, 0.6300323009490967, -1.1074599027633667, 0.21157720685005188, 0.1896728128194809, -0.26081719994544983, 0.1775103062391281, 1.011519193649292, 0.4301929473876953, 0.3579268157482147, -0.2544344365596771, 0.6455428600311279, -0.2936558425426483, 0.2134099304676056, 0.004897798411548138, 0.325357586145401, 0.45866355299949646, -0.4682021141052246, -0.04234560206532478, -0.9555318355560303, -1.3393161296844482, 0.4076813757419586, -0.016732564195990562, -0.7266227006912231, -0.6504754424095154, -0.3752378523349762, 0.6147291660308838, 0.10842201858758926, 0.25146961212158203, -0.4332832098007202, 0.5318860411643982, -0.033857110887765884, -0.14406651258468628, 0.573135256767273, 0.30789726972579956, 0.48351994156837463, -0.8947548866271973, 0.3081609904766083, 0.34551629424095154, -0.05448225513100624, 0.41407957673072815, -0.19773389399051666, -0.06915317475795746, -0.5690106153488159, -0.41452354192733765, 0.18745356798171997, 0.7732717394828796, 0.14279231429100037, 0.1968311369419098, 0.22969873249530792, 0.2866979241371155, 0.4629283547401428, -0.30773821473121643, -0.1024496778845787, 0.1371399462223053, 0.458103746175766, -0.10100054740905762, -0.035090163350105286, 0.079018235206604, -0.043440081179142, -0.15456344187259674, -0.6497181057929993, 0.2072458416223526, 0.16854752600193024, -0.8357079029083252, -0.6665962338447571, 0.9118310213088989, -0.2867757976055145, -0.3049425482749939, 0.15359917283058167, -1.5640114545822144, -0.17844948172569275, -0.14245840907096863, -1.1767239570617676, -0.43295979499816895, 0.1922057718038559, -0.588357150554657, -0.4430564343929291, 0.2547711133956909, 1.0565446615219116, 0.12271670252084732, -0.4110437035560608, 0.008612007834017277, -0.2543754577636719, 0.11283552646636963, -0.15390315651893616, -1.2041823863983154, 0.7852513790130615, 0.29628676176071167, 0.4028734266757965, 0.7672508358955383, 0.38961559534072876, 0.3196573257446289, -0.44601261615753174, 0.6453892588615417, 1.295257329940796, -1.5784833431243896, -0.16566766798496246, -0.4924274682998657, -0.7597851157188416, 0.37530380487442017, 0.8058066368103027, -0.15925440192222595, 0.4535074234008789, 0.08303692191839218, -0.5398354530334473, -0.3007233142852783, -0.2978340685367584, 0.33667805790901184, 0.8322762250900269, -0.7754863500595093, -1.025900959968567, -0.28856348991394043, 0.22624613344669342, -0.9766871929168701, -0.2666521668434143, 0.011121154762804508, -0.16569288074970245, -0.2840590178966522, 1.132641315460205, -1.0760425329208374, 0.9407727122306824, 0.6491179466247559, -0.34706437587738037, -1.1433976888656616, -0.5642492175102234, -0.5278434157371521, 0.3192000091075897, 0.22827757894992828, 0.5821906328201294, -0.5471010208129883, 0.3031291961669922, 0.9283985495567322, -0.18803131580352783, -0.323764830827713, -0.798514187335968, -0.42365962266921997, 0.14365831017494202, -0.03592721372842789, 0.20831352472305298, 0.0663008764386177, -0.3777070939540863, 0.4628210663795471, 0.5662493705749512, 0.8684386610984802, 0.01990552619099617, -0.9653913974761963, -0.4750293493270874, -0.24232138693332672, 0.28154808282852173, -0.4203760325908661, -0.7496684789657593, -1.3408704996109009, 0.08827615529298782, -1.2603750228881836, 0.32611820101737976, -1.5956182479858398, -0.9134619235992432, 0.16920256614685059, -0.7485765218734741, 0.12139610201120377, 0.16519922018051147, -0.41995829343795776, -0.5458516478538513, -0.48116013407707214, -0.3679482936859131, 0.780587375164032, 0.4148695766925812, -0.35469797253608704, 0.32182109355926514, -0.06482411175966263, -0.10401946306228638, 0.332071989774704, 0.5688708424568176, -0.41306859254837036, -0.9144348502159119, -1.7339731454849243, 0.8257962465286255, -0.5099287033081055, 0.14099253714084625, -0.33892592787742615, 0.7804554104804993, 0.814382016658783, -0.2914147675037384, -0.3745943605899811, 0.5406835675239563, -1.1687273979187012, -0.7414306998252869, 0.5060756802558899, -0.6124173402786255, 0.6756343841552734, -0.17855213582515717, -0.486217200756073, -0.17952898144721985, 0.5901396870613098, -0.2584860622882843, -1.124190330505371, -1.2157866954803467, 0.48535576462745667, -0.4860277473926544, -0.13617201149463654, 0.08337851613759995, -0.70894855260849, -1.3266022205352783, 0.12967485189437866, 0.014589778147637844, 0.12673160433769226, -0.3377837836742401, 0.9298377633094788, 0.550990879535675, -1.4578183889389038, 0.23120497167110443, 0.08444869518280029, -0.06503056734800339, -0.25150802731513977, 0.26706796884536743, 0.1892777681350708, -0.3266822099685669, 0.5096129775047302, -0.28926903009414673, 0.2957157492637634, -1.0613642930984497, -0.15771237015724182, 0.4623797833919525, -0.10910668969154358, -0.03427862003445625, 1.259889006614685, -0.5006738901138306, -1.2663054466247559, -0.033445145934820175, -1.1381795406341553, -0.5247559547424316, 0.13100677728652954, 0.5263785719871521, 0.5582161545753479, -0.28006455302238464, -0.3696325421333313, -0.5917162895202637, 0.3386746346950531, -0.2223307490348816, -0.5988919138908386, 0.4953264594078064, 0.183400958776474, -0.8389884829521179, 0.9563490152359009, 0.6768836975097656, -0.5530720353126526, -0.6764860153198242, -0.5258976817131042, -0.20119233429431915, -0.4274245798587799, 0.21817873418331146, -0.12148430943489075, -0.13226793706417084, 0.8902039527893066, 0.23323413729667664, 0.37400904297828674, -0.6311993598937988, 0.051225412636995316, 0.04458016902208328, 0.38811272382736206, 0.2234799861907959, -0.4780178666114807, -0.007471510209143162, 1.3688066005706787, 1.1511324644088745, -0.6090534329414368, 0.07894666492938995, -0.45331960916519165, -0.3889804482460022, 0.5483837723731995, 0.6065618991851807, -0.004579499363899231, 0.8115311861038208, -0.33320650458335876, 0.4972359240055084, 0.024124817922711372, -0.5142351388931274, 0.13051314651966095, 0.6548182964324951, 0.7661455869674683, 0.7361974120140076, 0.17777489125728607, -0.16157382726669312, 1.097691535949707, -0.5071254372596741, -0.3812728524208069, 0.472863107919693, 0.5583817362785339, -0.6355111598968506, -0.1004955917596817, 0.4095900356769562, 0.5601698756217957, -0.36918574571609497, -0.9592764377593994, -0.09682448208332062, 0.7630622386932373, -0.37656915187835693, 0.048272669315338135, 1.0873363018035889, 0.168809175491333, 0.4252364933490753, 0.6826032400131226, 0.7198278903961182, -0.4425848424434662, 0.05120891332626343, -0.8362070918083191, -0.6386680603027344, 0.05675948038697243, -0.16187085211277008, -0.3802041709423065, -0.6235261559486389, -0.1013646125793457, 0.33853060007095337, -0.329873651266098, 0.2451607882976532, 1.1099276542663574, 0.4747574031352997, 0.5181682705879211, -0.4227159917354584, -0.2958081066608429, -0.9592928886413574, -0.6736517548561096, 0.23534847795963287, -0.5841017365455627, -0.3810523450374603, -0.3570522665977478, -0.25248369574546814, -0.6566116213798523]}, "authors": [{"authorId": "40910779", "name": "Koustuv Sinha"}, {"authorId": "1754452702", "name": "Amirhossein Kazemnejad"}, {"authorId": "145732771", "name": "Siva Reddy"}, {"authorId": "145134884", "name": "J. Pineau"}, {"authorId": "3449411", "name": "Dieuwke Hupkes"}, {"authorId": "81840293", "name": "Adina Williams"}], "references": [{"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "2fc6cda64da8badd1bd2efcbfef2766e68aa5592", "title": "minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "64522a5b3476e9f201f6a5b3e312ef0005c562f1", "title": "SHAPE: Shifted Absolute Position Embedding for Transformers"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "ed535e93d5b5a8b689e861e9c6083a806d1535c2", "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"}, {"paperId": "39b492db00faead70bc3f4fb4b0364d94398ffdb", "title": "Do Vision Transformers See Like Convolutional Neural Networks?"}, {"paperId": "49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2", "title": "Making Transformers Solve Compositional Tasks"}, {"paperId": "4e00843bc5f60d2b9116abc4320af6d184422291", "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "829580d6fc73fa601c4982e2b1b6832f2796270b", "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings"}, {"paperId": "4889ba5a8ae8b2169dd44d1d3a605bf9820bae8d", "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding"}, {"paperId": "06439c29150efb6441bfc928dfa5c0e0967edd9b", "title": "Exploring BERT\u2019s sensitivity to lexical cues using tests from semantic priming"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "88af48cda82b35bbeac263bad9203bf4efb1dcbb", "title": "Compositionality Decomposed: How do Neural Networks Generalise? (Extended Abstract)"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "5a2263092f49540fd0e049050a96882ff29b00c3", "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "68c1bf884f0fc0e86641466a1f1fa67e79f16a17", "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"}, {"paperId": "4099c4d272c12081b562392606e6d567e4ae7031", "title": "Masked Language Model Scoring"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba", "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "155345976aa505a10a45e9119f2853df4d7999d7", "title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme."}, {"paperId": null, "title": "2022) 2022 Rotary OPT"}, {"paperId": null, "title": "2022) take the experiments beyond synthetic datasets and show that APE\u2019s struggle in generalization to longer sequence of natural language. All of these amount to the evidence that points to APE\u2019s"}, {"paperId": null, "title": "A framework for few-shot language model"}, {"paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf", "title": "On Position Embeddings in BERT"}, {"paperId": "35c9a7e146097ed66b75068019285f231a4c2e5d", "title": "Multilingual ELMo and the Effects of Corpus Sampling"}, {"paperId": null, "title": "2021) explore different positional encodings and establish monotonicity, translation and symmetry properties of different methods, including APEs"}, {"paperId": null, "title": "2021) find relative positional encoding does not improve over APE"}, {"paperId": null, "title": "2021) proposes a novel technique to de-correlate the position encodings and token embeddings"}, {"paperId": null, "title": "The idea is to discover whether this attention summary metric is drastically different under different phase shift conditions"}, {"paperId": null, "title": "Following the experimental protocol"}, {"paperId": null, "title": "2021) train a Transformer model from scratch using shifted positional embeddings for machine translation, and observe improved performance in extrapolation and intrapolation setup"}, {"paperId": null, "title": "Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"}, {"paperId": null, "title": "2020) is a physical commonsense benchmark dataset, challenging language models\u2019 idea of the physical world. Given a physical goal, a model must choose the most plausible solution between two"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2019) is a linguistic acceptability dataset, where each example is an English sentence annotated with a binary label showing whether it is a grammatical sentence"}, {"paperId": null, "title": "Learned Absolute RoBERTa"}, {"paperId": null, "title": "2020) is a commonsense reasoning benchmark based on the Winograd Schema Challenge (WSC) (Levesque et al., 2011) with increased hardness and scale"}, {"paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "title": "The Third PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": null, "title": "Rotary Table 6: Positional encoding of commonly used pretrained language models"}, {"paperId": null, "title": "Name Release Year Positional Encoding Type"}]}