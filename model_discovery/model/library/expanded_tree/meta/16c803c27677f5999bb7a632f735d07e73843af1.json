{"paperId": "16c803c27677f5999bb7a632f735d07e73843af1", "abstract": "It is a common practice in natural language processing to pre-train a single model on a general domain and then fine-tune it for downstream tasks. However, when it comes to Large Language Models, fine-tuning the entire model can be computationally expensive, resulting in very intensive energy consumption. As a result, several Parameter Efficient Fine-Tuning (PEFT) approaches were recently proposed. One of the most popular approaches is low-rank adaptation (LoRA), where the key insight is decomposing the update weights of the pre-trained model into two low-rank matrices. However, the proposed approaches either use the same rank value across all different weight matrices, which has been shown to be a sub-optimal choice, or do not use any quantization technique, one of the most important factors when it comes to a model's energy consumption. In this work, we propose Bayesian-LoRA which approaches low-rank adaptation and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values. As a result, B-LoRA is able to fine-tune a pre-trained model on a specific downstream task, finding the optimal rank values and quantization levels for every low-rank matrix. We validate the proposed model by fine-tuning a pre-trained DeBERTaV3 on the GLUE benchmark. Moreover, we compare it to relevant baselines and present both qualitative and quantitative results, showing how the proposed approach is able to learn optimal-rank quantized matrices. B-LoRA performs on par with or better than the baselines while reducing the total number of bit operations by roughly 70% compared to the baseline methods.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Bayesian-LoRA is proposed which approaches low-rank adaptation and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values, and is able to fine-tune a pre-trained model on a specific downstream task, finding the optimal rank values and quantization levels for every low-rank matrix."}, "embedding": {"model": "specter_v2", "vector": [-0.01663782261312008, 0.054120682179927826, -0.21355009078979492, -0.43329596519470215, -0.1996760219335556, -0.1776920109987259, 0.7453262209892273, -0.15642641484737396, -0.6332517266273499, -0.5372840762138367, 0.7077450156211853, 0.2726452648639679, 0.22424377501010895, 0.01964716613292694, -0.29401758313179016, 0.20069396495819092, -1.0376940965652466, 0.5203884243965149, -0.041428517550230026, -0.851675271987915, -0.17249701917171478, -0.5154977440834045, -0.6513683199882507, 0.22053302824497223, -0.0901111513376236, 0.4227637052536011, 0.18173685669898987, 1.0032261610031128, -0.3277512788772583, -0.1799411177635193, 0.6795485615730286, -0.4634655714035034, 0.29852166771888733, 0.18558940291404724, -0.07496341317892075, -0.28781619668006897, 0.11369895935058594, -0.8665968179702759, -0.23671473562717438, 0.9930516481399536, -0.22000275552272797, 0.44090092182159424, 0.7601926326751709, 0.0638463944196701, -0.2661113739013672, 0.8121737241744995, 0.41438552737236023, 0.6749958395957947, -0.43685856461524963, -0.5483081340789795, 1.316868543624878, -1.469502568244934, 0.019174927845597267, 1.5251356363296509, 0.34055039286613464, 0.5001155138015747, -0.5609184503555298, -0.5793202519416809, 0.5413485765457153, 0.022876009345054626, -1.0864720344543457, -0.6882317662239075, -0.14832109212875366, -0.2122316211462021, 1.7305949926376343, -0.612446129322052, -0.14808206260204315, 0.48380568623542786, 0.21846634149551392, 0.8656346201896667, -0.0277530699968338, -0.5967363119125366, -0.12294795364141464, -0.041547924280166626, 0.15186987817287445, 0.7816528081893921, -0.5445651412010193, 0.32747167348861694, -1.3219808340072632, -0.23200583457946777, 0.42598193883895874, -0.6546819806098938, 0.1688557118177414, -0.24592608213424683, -0.05287174507975578, 0.8838585019111633, 0.1640303134918213, 0.4498025178909302, 0.1062408834695816, 0.9331769347190857, 0.7517784237861633, 0.6386902928352356, 0.0747559517621994, 0.2679274082183838, -0.3910004496574402, 0.1453680843114853, -0.7614060640335083, -0.010839003138244152, 0.09788962453603745, 0.5298592448234558, -0.24545371532440186, 0.23463813960552216, -0.8889614343643188, 0.6396759748458862, 1.586897373199463, -0.19281931221485138, 0.34375229477882385, -0.4458630681037903, 0.28391942381858826, -1.120801568031311, 0.06269820034503937, -0.521520733833313, -0.15267321467399597, -0.17245924472808838, -1.0928915739059448, -1.3163963556289673, -0.29368752241134644, 0.3311777412891388, -0.7575368285179138, 0.8698849081993103, -0.4342168867588043, -0.14086420834064484, 0.1577933430671692, 0.4989863336086273, 0.47595712542533875, 1.073760747909546, 0.20678576827049255, 0.16222608089447021, 1.2073891162872314, -0.562034010887146, -0.935117781162262, -1.1228368282318115, 0.6382523775100708, -0.13999296724796295, 0.8175597786903381, 0.2810071110725403, -1.0497196912765503, -0.738284707069397, -0.9461365938186646, -0.10683094710111618, -0.5543661713600159, 0.7015058398246765, 0.833541750907898, 0.6360856890678406, -1.013628363609314, 0.5463137626647949, -0.26596319675445557, -0.04827554151415825, 0.23150482773780823, 0.5157414078712463, 0.2590251863002777, -0.25003787875175476, -1.5518624782562256, 0.31843847036361694, 0.5183886289596558, -0.21331281960010529, -0.16694609820842743, -0.22136341035366058, -0.833069384098053, -0.02748446725308895, 0.15342363715171814, -0.5866419076919556, 1.105647325515747, 0.5566564798355103, -1.984738826751709, 0.46871891617774963, -0.1521417200565338, 0.21661479771137238, -0.06401580572128296, -0.302592396736145, -0.4760364890098572, -0.7709882259368896, -0.7803952097892761, 0.5550088286399841, 0.7514817118644714, 0.015827100723981857, 0.1324499547481537, 0.11423482745885849, -0.19604595005512238, 0.1982288956642151, -0.4793146848678589, 0.7618827223777771, -1.106387734413147, -0.41322043538093567, 0.530828058719635, 0.7924644351005554, -0.048870302736759186, -0.3526268005371094, -0.2896018326282501, -0.8310496807098389, 0.3627777099609375, -0.03667459636926651, 1.2054470777511597, -1.0452760457992554, -0.32203322649002075, 0.30260545015335083, -0.45561492443084717, -0.4381340444087982, -0.9493332505226135, 0.4294252395629883, -0.15955103933811188, 0.35214343667030334, 0.1429721713066101, -0.9999995231628418, 0.056129518896341324, -0.28505435585975647, -0.7594608664512634, 0.2123018205165863, 0.12386395037174225, 0.9152063131332397, -0.7535723447799683, 0.4699123799800873, -0.19716480374336243, 0.6588332056999207, -1.4234734773635864, 1.1627531051635742, -0.2913255989551544, 0.3175554573535919, 0.14789821207523346, -0.3096736967563629, -0.0040092747658491135, -0.26599276065826416, 0.38305437564849854, -0.7793914079666138, 0.5703789591789246, 0.5123366117477417, -0.8086844086647034, 1.556127667427063, -0.493127703666687, 0.3632468581199646, 0.059107858687639236, -0.2738265097141266, 0.17202909290790558, 0.11091258376836777, -0.45860227942466736, -0.1905619502067566, 0.5132077932357788, 0.39452093839645386, -0.6039169430732727, 0.6404051184654236, 0.8360127806663513, 0.8195686936378479, -0.5962092280387878, -0.07749711722135544, 0.6733691692352295, -0.7904853224754333, 0.7340651750564575, 0.361250638961792, 0.4604797959327698, 0.3122023642063141, 0.6952752470970154, 0.054085664451122284, 0.47216227650642395, -1.152899980545044, -0.37742289900779724, 0.3501773476600647, 1.0891166925430298, 0.5924022197723389, 0.30688875913619995, -0.5792813301086426, -0.22086097300052643, -0.44722893834114075, 0.38475510478019714, 1.7897616624832153, -0.3977826237678528, -0.31444251537323, -0.511719286441803, 0.10914292186498642, -0.6354783773422241, -0.10311666131019592, -0.13583941757678986, -0.06974752247333527, -0.6302635669708252, -1.4193525314331055, 0.5076355338096619, -0.11687961220741272, 0.7827011942863464, 0.0551818422973156, 0.07392001152038574, -0.03693527355790138, 0.503529965877533, -0.7378246784210205, -0.9102128148078918, 0.6276397109031677, -0.8993908762931824, 0.21347828209400177, 0.2150794416666031, 0.001924695447087288, 0.22969874739646912, -1.3595517873764038, 0.8415948152542114, -0.820556640625, 0.17035943269729614, -0.31044629216194153, 0.3042297661304474, -0.21581192314624786, -0.9367960095405579, 0.33692508935928345, 0.3502381145954132, 0.08836458623409271, 0.16775090992450714, 0.4710228741168976, 0.3163253962993622, -0.04759138450026512, -0.5006092190742493, 0.0031364483293145895, 0.0873006135225296, 0.2781461179256439, 0.8567612767219543, -0.18063832819461823, -0.09132447838783264, -1.4775428771972656, 1.4274325370788574, 0.3525954484939575, -0.8450378179550171, -0.10774771869182587, -0.6301460266113281, 0.12331689149141312, 0.47746384143829346, -0.5797699689865112, -0.1294168382883072, -0.5296716094017029, 0.055097561329603195, 0.044059015810489655, 0.1635732650756836, 0.3927192687988281, 0.19579774141311646, 0.2617168724536896, 0.46814465522766113, 0.09690218418836594, -0.004491179250180721, -0.21715456247329712, 0.8831081986427307, -0.5105608701705933, 0.7288669943809509, 0.11256295442581177, 0.6006219387054443, -0.2162286341190338, -0.43807753920555115, -0.4325101375579834, -0.2319020926952362, -0.5088325142860413, -0.2340192049741745, -0.17912571132183075, 0.049176815897226334, -0.8794193267822266, -0.6041567325592041, -0.05166889727115631, -1.0340992212295532, -0.11517608165740967, 0.20641155540943146, -0.21675075590610504, -0.24791009724140167, -1.042478322982788, -1.7206001281738281, -0.25757408142089844, -0.7334921360015869, -1.241039514541626, 0.32233014702796936, -0.27247533202171326, -0.33502838015556335, -0.14994998276233673, -0.08222055435180664, -0.4853721559047699, 1.0356497764587402, -1.0170780420303345, 1.1917248964309692, 0.04086397588253021, -0.07932565361261368, -0.049439143389463425, 0.17462138831615448, 0.6347010731697083, -0.3080131411552429, 0.005720939487218857, -1.0239812135696411, -0.14377343654632568, -0.25048592686653137, -0.21130496263504028, 0.14915615320205688, 0.24127043783664703, 0.7002493739128113, -0.09325673431158066, -0.07048241049051285, 0.7360156178474426, 1.3854485750198364, -0.6986871957778931, 0.030266597867012024, -0.015950294211506844, 0.8538392782211304, 0.1253327876329422, -0.36335432529449463, 0.8514263033866882, 0.2834740877151489, 0.7433499693870544, 0.06810598820447922, 0.31892305612564087, -0.09811282902956009, -0.3249490261077881, 0.9077327251434326, 2.196056604385376, 0.4157203435897827, -0.08371268212795258, -0.7192347645759583, 0.45706018805503845, -0.8402649760246277, -0.2597106099128723, 0.6991986036300659, 0.7673004269599915, 0.8139906525611877, -0.3323676884174347, -0.6053597927093506, -0.22709238529205322, 0.08773859590291977, 0.5638641119003296, -0.30405929684638977, -1.1636842489242554, 0.024902215227484703, 0.2183290421962738, 0.34976696968078613, 0.6913199424743652, -0.4803473949432373, 0.648829460144043, 14.710256576538086, 0.9879310131072998, 0.30633825063705444, 0.7022428512573242, 0.7368828654289246, 0.08994804322719574, -0.31890538334846497, -0.4308740496635437, -1.3857365846633911, -0.26368507742881775, 1.2325893640518188, 0.570256233215332, 1.321280837059021, -0.06232396140694618, 0.20888495445251465, 0.23473650217056274, -0.7794161438941956, 0.9751832485198975, 0.46256887912750244, -1.2831792831420898, 0.4464627802371979, 0.00197624322026968, 0.5098789930343628, 0.5600727796554565, 0.6977973580360413, 0.7868340611457825, 0.5355185270309448, -0.40390220284461975, 0.49142181873321533, 0.2843790054321289, 0.9638928771018982, -0.16099028289318085, 0.34492227435112, 0.29856279492378235, -0.9232600927352905, -0.5131271481513977, -0.5738520622253418, -0.6865491271018982, 0.27092909812927246, 0.3811948597431183, -0.4025150239467621, -0.8489934206008911, 0.2485985904932022, 0.6840419769287109, -0.030653418973088264, 0.28381553292274475, -0.2162223607301712, 0.4754609763622284, -0.20367607474327087, 0.2791614830493927, 0.11791156232357025, 0.034594256430864334, 0.19846346974372864, 0.02540362812578678, 0.11525147408246994, -0.09409374743700027, -0.20049826800823212, 0.2889229655265808, -0.8053661584854126, 0.2893386483192444, 0.05982845649123192, -0.17983342707157135, -0.04928809776902199, 0.6157755255699158, 0.6420519948005676, 0.46650177240371704, -0.2642430067062378, 0.40288764238357544, 0.7877970933914185, 0.25176382064819336, 0.03242511302232742, 0.12994351983070374, 0.2606440484523773, -0.39797306060791016, 0.06483373790979385, 0.28020864725112915, -0.15376415848731995, -0.6403690576553345, -0.7283239364624023, -0.707231342792511, 0.32290974259376526, -0.707342267036438, -0.9044186472892761, 0.5933393836021423, -0.03487449511885643, -0.3446386754512787, 0.04787971451878548, -0.8909014463424683, 0.2432328164577484, 0.5099424719810486, -1.3643927574157715, -0.13055406510829926, 0.7700388431549072, -0.3466731309890747, -0.3449452817440033, -0.37875208258628845, 1.2319505214691162, 0.20746324956417084, -0.2674974203109741, 0.15101316571235657, 0.6647573113441467, -0.09350241720676422, 0.21850427985191345, -0.4498983323574066, 0.7767369151115417, 0.463742733001709, -0.22909367084503174, 0.6493880152702332, 0.012080165557563305, 0.26316967606544495, -1.0450961589813232, -0.16349519789218903, 0.4715898633003235, -0.6328315734863281, -0.17827342450618744, -0.9201563596725464, -0.6550541520118713, 0.052004873752593994, 0.2519015371799469, -0.3645481467247009, 0.2147359400987625, 0.12510189414024353, -0.8656573295593262, -0.5514757037162781, -0.7760521173477173, 0.1186010017991066, 0.2070503830909729, -1.0672756433486938, -0.023792732506990433, -0.16614823043346405, 0.0242107342928648, -1.4601576328277588, -0.9066823124885559, -0.06813978403806686, -0.09272216260433197, 0.22428558766841888, 1.201064109802246, 0.00919027253985405, 0.4667273163795471, 0.3542770445346832, -0.45607566833496094, -0.9323597550392151, 0.14208067953586578, -0.7485061287879944, -0.11564904451370239, -0.2309989333152771, 0.691764235496521, -0.07977505773305893, 0.1991485208272934, 0.6471554636955261, 0.3841308057308197, -0.28492045402526855, -0.8828082084655762, -0.21455729007720947, -0.005989501718431711, -0.628620982170105, 0.2559720575809479, -0.44267916679382324, -0.28801634907722473, 0.30042609572410583, 0.11841064691543579, 0.7710157632827759, -0.4567928612232208, -1.0024375915527344, 0.3133445680141449, 0.041541144251823425, -0.5249912738800049, -0.40659022331237793, -0.21949544548988342, -1.756964087486267, -0.42262208461761475, -1.1439670324325562, 0.30060094594955444, -0.7567705512046814, -0.4004670977592468, 0.28715065121650696, -0.09058154374361038, 0.07389506697654724, 0.5932329297065735, 0.019358471035957336, -0.4286635220050812, -0.38716721534729004, -0.11441320925951004, 0.8819500803947449, 0.706794023513794, -1.077702522277832, -0.36180323362350464, -0.21563974022865295, 0.22214657068252563, 0.3499862551689148, 0.4177207052707672, -0.484633207321167, -0.893913209438324, -1.5218576192855835, 1.0308072566986084, -0.24481533467769623, -0.36989089846611023, -0.4798544645309448, 0.8316382765769958, 0.5222535133361816, -0.19739359617233276, 0.18419130146503448, 0.6966879367828369, -0.9010332822799683, -0.33337631821632385, 0.41609254479408264, -0.6634095907211304, 0.20806263387203217, 0.30152374505996704, -0.29912158846855164, -0.3656454086303711, 0.3894335925579071, 0.15899112820625305, -0.6315691471099854, -0.828773021697998, 0.2523558735847473, -0.6219189167022705, -0.014846374280750751, -0.7644687294960022, -0.10195772349834442, -0.8455110192298889, -0.4734772741794586, -0.0018065625336021185, 0.11812224239110947, -0.519871175289154, 0.7300501465797424, 0.5158783197402954, -0.9918109774589539, -0.006195778027176857, 0.5863462686538696, 0.1630844920873642, -0.07533971220254898, 0.19054511189460754, 0.21093295514583588, -0.4129311442375183, 0.3670458197593689, 0.38248178362846375, 0.22454550862312317, -0.6899081468582153, -0.6519249677658081, 0.7510559558868408, -0.7304317951202393, 0.06039780005812645, 1.2138957977294922, -0.42431309819221497, -1.2778348922729492, 0.0695769414305687, -1.2699611186981201, -0.7108779549598694, -0.15084724128246307, 0.4384395182132721, -0.08740855753421783, 0.345368891954422, -0.181351900100708, -0.45121896266937256, 0.4537290334701538, -0.3460288643836975, -0.6332674026489258, 0.639337420463562, -0.3056274950504303, -0.6137597560882568, 0.4738844037055969, 0.6935158371925354, -0.45091938972473145, -0.5712986588478088, -0.7609407305717468, -0.4122539162635803, -0.0442960150539875, 0.7424842715263367, -0.44084569811820984, -0.7952984571456909, 0.6385560631752014, 0.71131831407547, -0.3138130009174347, 0.38441702723503113, -0.304985374212265, -0.15772011876106262, 1.1822172403335571, 0.14874199032783508, -0.8592782616615295, -0.3923060894012451, 1.6345895528793335, 1.4018126726150513, -0.6026189923286438, 0.4793134033679962, -0.08484438061714172, -0.48538270592689514, 0.8552326560020447, 0.08815111964941025, 0.09883185476064682, 1.2554595470428467, -0.04776174947619438, -0.10319342464208603, 0.24674953520298004, -0.8919178247451782, -0.06689286231994629, 1.167824387550354, 0.7609449625015259, 0.700978696346283, 0.06145177036523819, 0.08266130834817886, 0.8466875553131104, -0.12170645594596863, -0.12583456933498383, 0.8224180340766907, -0.08327608555555344, -0.058705396950244904, -0.42656415700912476, -0.09380615502595901, 1.121084213256836, -0.6846904754638672, -0.8368619084358215, 0.49405795335769653, 0.3436945676803589, -0.10693316906690598, 0.5789501667022705, 0.6904771327972412, -0.15535525977611542, 0.49605509638786316, -0.025563262403011322, 0.45110952854156494, -0.8505432605743408, -0.4332379400730133, 0.21060198545455933, -0.46560606360435486, -0.23890253901481628, 0.047567810863256454, -0.3950473368167877, -0.38995492458343506, -0.0956970602273941, 0.3026544749736786, -0.3222651779651642, 0.5781208872795105, 0.8569034337997437, 0.2734609842300415, 0.2526254653930664, -0.5165978074073792, -0.43695634603500366, -0.683114767074585, -0.9386456608772278, -0.10322128236293793, -0.7072693705558777, -0.3799794614315033, 0.1726992428302765, 0.08832711726427078, 0.07591459900140762]}, "authors": [{"authorId": "2052095589", "name": "Cristian Meo"}, {"authorId": "2307470505", "name": "Ksenia Sycheva"}, {"authorId": "1996705", "name": "Anirudh Goyal"}, {"authorId": "2265787178", "name": "Justin Dauwels"}], "references": [{"paperId": "27001cc6c9176a0fdc5a076089ee7813b4e5c827", "title": "Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "85e959eef45114974c8f8643e88af23936fff3d1", "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "946a43eb327c5ee2087187720f15c46b2985cb1c", "title": "Bayesian Bits: Unifying Quantization and Pruning"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9d0a35aebe296fe24575b331ecd364c30d7c3119", "title": "Taxonomy and Evaluation of Structured Compression of Convolutional Neural Networks"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "3d8b62c060f8444907e7c975c6ae590373b51ed4", "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "2fd67bc7d239b26d9e2453fecd29e88cc7a8627d", "title": "A method to estimate the energy consumption of deep neural networks"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d89ee98810039d2061ed42ee8026da49c503d16b", "title": "Learning multiple visual domains with residual adapters"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "7a6fd5573d2679506765d461ec4892fd4017b745", "title": "Learning Ordered Representations with Nested Dropout"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "1e66fbcc886788dd54046c7221b69da1c82b6229", "title": ": Exploring"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": null, "title": "Chatgpt: Optimizing language models for dialogue"}]}