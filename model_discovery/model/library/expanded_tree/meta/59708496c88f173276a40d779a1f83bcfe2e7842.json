{"paperId": "59708496c88f173276a40d779a1f83bcfe2e7842", "abstract": "Retentive Network first emerged in the domain of NLP and immediately gained widespread attention due to its remarkable performance. A significant portion of its impressive capabilities stems from its explicit decay mechanism, which incorporates valuable prior knowledge. However, this explicit decay is unidirectional and one-dimensional, making it unsuitable for the bidirectional, two-dimensional modeling required in image-based tasks. To solve this, we propose a bidirectional, two-dimensional form of explicit decay specifically designed for vision models to introduce distance-related prior knowledge. Besides, unlike language models, the vision backbones use the same parallel form during training and inference. If this parallel form is replaced with recurrent or chunk-wise recurrent form, the parallelism of the model will be significantly disrupted, resulting in extremely slow inference speed. So we discard the two additional inference modes present in the original RetNet, retaining only the parallel form. Specifically, we incorporate bidirectional, two-dimensional explicit decay into the Self-Attention to form \\textbf{Re}tentive \\textbf{S}elf-\\textbf{A}ttention (ReSA). Furthermore, to reduce the complexity of global modeling, we decompose ReSA along the two axes of the image. Building upon ReSA, we construct RMT, a strong vision backbone. Abundant experiments have demonstrated that our RMT exhibits exceptional performance across various computer vision tasks. For example, RMT achieves \\textbf{84.1\\%} Top1-acc on ImageNet-1k using merely \\textbf{4.5G} FLOPs. To the best of our knowledge, among all models, RMT achieves the highest Top1-acc when models are of similar size and trained with the same strategy. Moreover, RMT significantly outperforms existing vision backbones in downstream tasks. Code will be released at https://github.com/qhfan/RMT.", "venue": "arXiv.org", "year": 2023, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.11523", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "A bidirectional, two-dimensional form of explicit decay specifically designed for vision models to introduce distance-related prior knowledge is proposed, and is incorporated into the Self-Attention to form ReSA, a strong vision backbone."}, "embedding": {"model": "specter_v2", "vector": [0.1001097708940506, 0.7008553743362427, -0.6517569422721863, -0.07079613208770752, 0.2059355527162552, 0.10941839218139648, 0.8287380337715149, -0.30747297406196594, -1.0691502094268799, -0.18944066762924194, 0.41489389538764954, -0.10812031477689743, 0.4990902543067932, 0.20154263079166412, -0.2703001797199249, 0.1924467831850052, -0.44407156109809875, 0.17362073063850403, 0.3836616277694702, -0.4587894082069397, 0.11542919278144836, -0.4468812942504883, -0.9812226295471191, -0.1285586953163147, 0.11275205761194229, 0.49230268597602844, 0.5093824863433838, 0.9692192673683167, -0.15200521051883698, 0.6242023706436157, 0.3577951192855835, -0.5002701282501221, 0.3066839277744293, 0.37902551889419556, -0.543832004070282, -0.23212411999702454, 0.4419431984424591, -0.6465427875518799, -0.9028682112693787, 1.0510271787643433, -0.10600082576274872, 0.48902013897895813, 0.6193995475769043, -0.2891979217529297, -0.7057613134384155, 0.409601092338562, 0.27085375785827637, 1.2060149908065796, -0.4433126151561737, -0.5268264412879944, 1.2675858736038208, -1.4493234157562256, 0.39966943860054016, 1.3670744895935059, 0.3766007423400879, 0.5255573391914368, -0.0936867967247963, -0.4383929669857025, 1.431580901145935, 0.3469715416431427, -0.7200494408607483, -0.03043983317911625, -0.04308697208762169, -0.1811438500881195, 1.630234956741333, -0.8207657337188721, 0.20535889267921448, 0.840614914894104, 0.06770146638154984, 1.4932433366775513, -0.11412236839532852, -0.8553311824798584, -0.6547646522521973, -0.028816310688853264, 0.49634677171707153, 0.9648629426956177, -0.32459917664527893, 0.5859159231185913, -0.6700858473777771, 0.28604909777641296, 0.7060064673423767, 0.2511253356933594, -0.07418175786733627, 0.02564789168536663, 0.012246527709066868, 0.7871185541152954, 0.9493248462677002, 0.6107372045516968, -0.5166839361190796, 0.5677499175071716, 0.33580800890922546, 0.11015494167804718, -0.003963623661547899, 0.12659718096256256, 0.11788083612918854, 0.9596724510192871, -0.633997917175293, 0.007546855602413416, -0.31820183992385864, 0.7930521965026855, 0.18552789092063904, 0.02663482166826725, -0.6262300610542297, 0.010143110528588295, 1.567359447479248, -0.23480427265167236, 0.49736684560775757, -0.7216902375221252, -0.002947307890281081, -0.7515787482261658, -0.10781846195459366, -1.156468152999878, 0.03333919867873192, -0.3780083656311035, -0.9029693603515625, -1.0236384868621826, -0.28189072012901306, 0.4129657745361328, -0.8298116326332092, 0.7400121688842773, -0.324959933757782, 0.11643431335687637, -0.0674416571855545, 0.7714332938194275, 0.4882326126098633, 0.4172986149787903, 0.5991575717926025, 0.3421177864074707, 0.8475034236907959, -1.3089443445205688, -0.4557388722896576, -1.016394019126892, 0.5698637962341309, -0.1310281753540039, -0.020541734993457794, -0.3154957592487335, -1.0008527040481567, -1.2694740295410156, -0.9590731859207153, -0.06171495094895363, -0.6741204261779785, 0.2564392387866974, 0.7760872840881348, 0.003824778599664569, -1.1889208555221558, 0.9369856715202332, 0.12521995604038239, -0.26293298602104187, 0.5409518480300903, 0.10122554004192352, 0.28357332944869995, -0.7612556219100952, -1.153466820716858, 0.6974567174911499, 0.3356597125530243, 0.15806658565998077, -0.7146511077880859, -0.32892757654190063, -1.0736831426620483, -0.3741680979728699, 0.3368167281150818, -0.9611668586730957, 1.3697010278701782, -0.45088985562324524, -0.8559715151786804, 0.9765834808349609, -0.49200695753097534, -0.1749303936958313, 0.38152986764907837, -0.43786054849624634, -0.41226598620414734, -0.3418707847595215, -0.11783557385206223, 0.9631721377372742, 0.9456153512001038, -0.2595943510532379, -0.2498553991317749, -0.11682490259408951, -0.05463872477412224, -0.11152257770299911, -0.3791404068470001, 0.832920491695404, -0.4252282679080963, -0.1548740416765213, 0.13319382071495056, 0.7294458150863647, 0.000409568048780784, 0.10676773637533188, -0.060983024537563324, -1.4131220579147339, 0.8105772137641907, 0.3925483524799347, 0.411642849445343, -0.8082780241966248, -0.725374162197113, -0.35566192865371704, 0.5157341957092285, 0.15357151627540588, -1.1229486465454102, 0.16029569506645203, -0.21447452902793884, 0.41883739829063416, 0.23837290704250336, -0.9277033805847168, -0.24410982429981232, 0.0214148610830307, -0.6848937273025513, 0.0958404466509819, 0.354316383600235, 1.1782379150390625, -1.2347371578216553, -0.21275615692138672, -0.1680593490600586, 0.2578660845756531, -0.7348275184631348, 1.0661903619766235, -0.5431503653526306, 0.10892561823129654, -0.06994787603616714, -0.441182941198349, 0.08174332231283188, -0.4362325668334961, 0.08077868819236755, -0.47933274507522583, -0.14828389883041382, 0.478079617023468, 0.021292785182595253, 1.3868144750595093, -0.1933477818965912, 0.7819978594779968, 0.12016063183546066, -0.6543328166007996, 0.4906315505504608, 0.019685253500938416, -0.4934433102607727, -0.46644118428230286, 0.36894646286964417, 0.013401438482105732, -0.6553474068641663, 0.17810837924480438, 1.0293844938278198, 0.9570682644844055, 0.20093822479248047, 0.03415541350841522, 0.15664884448051453, 0.040051039308309555, 0.1312197893857956, 0.7451530694961548, 0.5317321419715881, 0.5254917144775391, 0.14321017265319824, -0.13181518018245697, 0.37998536229133606, -0.9363351464271545, -0.030941026285290718, 0.6739072203636169, 0.329455703496933, 0.551764965057373, 0.14076833426952362, -0.9015867710113525, -0.2711070775985718, 0.3268069624900818, 0.5694188475608826, 1.5433626174926758, 0.05120190605521202, -0.03501581773161888, -0.16584211587905884, -0.44220107793807983, -0.4023149907588959, -0.30961453914642334, -0.5174062848091125, -0.17003187537193298, -0.36496397852897644, -1.0969500541687012, 0.8066929578781128, 0.3692995011806488, 1.345182180404663, -0.6374229788780212, -0.17678551375865936, -0.4152325391769409, 0.349699467420578, -0.792578399181366, -0.6677922010421753, 0.0911274179816246, -0.2839753031730652, -0.23118314146995544, -0.13991869986057281, -0.5055704712867737, 0.4661959409713745, -0.2995731234550476, 1.0541162490844727, 0.019477633759379387, -0.4141482412815094, 0.45319607853889465, 0.533969521522522, -0.5396069288253784, -0.23739320039749146, 0.3491261601448059, 0.15110576152801514, -0.22326050698757172, 0.42500361800193787, 0.7549141049385071, -0.5653111934661865, 0.3671490252017975, -0.45009124279022217, -0.07245659083127975, 0.12734124064445496, -0.02439495362341404, 0.7805922031402588, -0.25842538475990295, 0.5972399115562439, -0.918898344039917, 0.6445911526679993, 0.05879316106438637, -0.5273250937461853, -0.1032039150595665, -0.8212311863899231, -0.5246853828430176, 0.08795863389968872, -0.8521499633789062, -0.42065760493278503, -0.8679952025413513, 0.49097466468811035, -0.5062990188598633, -0.13000361621379852, 0.19033090770244598, 0.6843475103378296, 0.33447232842445374, 0.5599045157432556, 0.1091843992471695, 0.28964677453041077, 0.07784675061702728, 0.8221886157989502, -1.0871208906173706, 0.9182122349739075, 0.11770493537187576, -0.03477643430233002, -0.10503606498241425, 0.015062529593706131, -0.9787353277206421, -0.8685266375541687, -0.7992000579833984, -0.07936125248670578, -0.45276451110839844, 0.2688523530960083, -0.7687646150588989, -0.9169337153434753, 0.17272299528121948, -1.362226128578186, -0.17136839032173157, 0.006546895019710064, -0.17475999891757965, 0.14088362455368042, -0.8479772210121155, -1.1715179681777954, -0.5810784101486206, -0.34565380215644836, -0.6204495429992676, -0.060264188796281815, 0.26607492566108704, -0.021781332790851593, -0.7123773694038391, -0.00018938073480967432, -0.49874067306518555, 0.974195659160614, -0.6789548993110657, 0.6830857992172241, 0.42135047912597656, -0.2965758144855499, -0.2848087549209595, -0.44046565890312195, 0.7791732549667358, -0.42979007959365845, 0.17999717593193054, -1.0155487060546875, 0.3040045499801636, -0.3485232889652252, -0.6944028735160828, 0.8544312119483948, 0.5740095973014832, 0.7880847454071045, 0.1267574280500412, -0.6073273420333862, 0.03587132319808006, 1.1835254430770874, -1.1488062143325806, 0.5254889726638794, 0.1386195719242096, 0.8494618535041809, 0.273573100566864, -0.3453933894634247, 0.27116191387176514, 0.7626978754997253, -0.0894746482372284, 0.541511058807373, -0.23795585334300995, -0.37689658999443054, -0.8432462215423584, 0.36120346188545227, 1.3926023244857788, 0.15261921286582947, 0.051895834505558014, -0.7918816804885864, 0.7373687624931335, -1.508227825164795, -0.7313883304595947, 0.5464377999305725, 0.6261592507362366, 0.38730013370513916, -0.4076545834541321, -0.1322629302740097, -0.5939961671829224, 0.8773094415664673, 0.5067658424377441, -0.6363075971603394, -0.4465425908565521, -0.26695212721824646, -0.24475248157978058, 0.24342219531536102, 0.7765370607376099, -0.2798517942428589, 0.9525918960571289, 14.631003379821777, 0.5092183947563171, -0.0028340695425868034, 0.4420190453529358, 0.6486525535583496, 0.5919185876846313, -0.32904064655303955, -0.08489557355642319, -1.5685234069824219, -0.5180056095123291, 0.8579641580581665, 0.43084871768951416, 0.18370014429092407, 0.05893346294760704, -0.2732512950897217, 0.028546268120408058, -0.3369881808757782, 0.6996635794639587, 0.5798255205154419, -1.1987687349319458, 0.4563550353050232, 0.11015477776527405, 0.08079791069030762, 0.9231648445129395, 1.091152548789978, 0.8822786808013916, 0.6858230829238892, -0.32883164286613464, 0.37933340668678284, 0.4000276029109955, 0.7387455105781555, 0.4924071431159973, 0.3031524121761322, 0.736566424369812, -0.7280424237251282, -0.17846663296222687, -1.1325346231460571, -1.3404453992843628, 0.13620226085186005, 0.2442997395992279, -0.26349154114723206, -0.6313304305076599, -0.16069501638412476, 1.2474589347839355, -0.07188486307859421, 0.284655898809433, -0.5558726787567139, 0.23774363100528717, -0.014603646472096443, 0.07364534586668015, 0.35062769055366516, 0.48690319061279297, 0.21860133111476898, 0.09686276316642761, -0.20764854550361633, 0.14277686178684235, 0.3062404692173004, 0.6277067065238953, -0.9770981669425964, -0.15512414276599884, -0.3872776925563812, 0.0886569619178772, -0.20151756703853607, 0.9328916072845459, 0.6070708632469177, 0.034726835787296295, -0.1613723337650299, 0.1642223745584488, 0.739182710647583, 0.2346678376197815, -0.4711686670780182, 0.10186140984296799, -0.09599782526493073, -0.33897286653518677, -0.09273763746023178, 0.17675745487213135, 0.3181781470775604, -0.5656896829605103, -0.8326104283332825, 0.23404967784881592, 0.32889264822006226, -1.032855749130249, -0.9892357587814331, 1.2966943979263306, -0.47457003593444824, 0.18395397067070007, 0.4003293514251709, -0.8713101148605347, -0.15428298711776733, 0.47666260600090027, -2.149451494216919, -0.5285676121711731, 0.027063192799687386, 0.22043871879577637, -0.3387386202812195, 0.09002354741096497, 0.9816257953643799, 0.23550546169281006, -0.4898484945297241, -0.09425826370716095, -0.4651763439178467, 0.0648796334862709, -0.10194769501686096, -0.6124399900436401, 0.5253695249557495, 0.07384338229894638, -0.21754230558872223, 0.08568117022514343, -0.19410574436187744, 0.1864800602197647, -0.43186381459236145, -0.14182598888874054, 0.5843491554260254, -1.0216857194900513, -0.42615222930908203, -0.11567896604537964, -0.7148675322532654, 0.36576807498931885, 0.6645875573158264, 0.002254351507872343, -0.03928607329726219, 0.31520727276802063, -1.1324912309646606, 0.054082807153463364, -0.6134390234947205, 0.15264683961868286, 0.5582392811775208, -0.7639212012290955, -0.742907702922821, -0.19057267904281616, 0.4375617504119873, -0.4929436147212982, -0.2620079517364502, -0.2928529977798462, 0.07564488798379898, -0.14644180238246918, 1.1686292886734009, -0.4325517416000366, 0.6707027554512024, 0.7281748652458191, -0.19507759809494019, -0.5923035144805908, -0.1916230469942093, -0.9646306037902832, 0.01676815003156662, 0.11596674472093582, 0.6052712798118591, -0.6194434762001038, 0.2829819321632385, 0.8766577243804932, 0.39017343521118164, -0.3566462993621826, -0.33419376611709595, -0.418698787689209, 0.2920059561729431, -0.508003830909729, -0.010895220562815666, -0.08621833473443985, -0.16416877508163452, 0.5978772640228271, 0.48372483253479004, 0.3010038435459137, -0.200028195977211, -1.0809497833251953, 0.06421727687120438, -0.501408040523529, 0.08961974829435349, -0.4918020963668823, -0.4304857850074768, -2.0696723461151123, 0.1889098584651947, -1.370866298675537, -0.08318258821964264, -1.1354297399520874, -0.5845273733139038, -0.0517096072435379, -0.46975159645080566, -0.012686832807958126, 0.4879368245601654, -0.2759094536304474, -0.36521974205970764, -0.16480319201946259, -0.7182267308235168, 1.14429771900177, 1.1378989219665527, -0.7887601852416992, 0.03323814272880554, -0.10805122554302216, -0.14647221565246582, 0.6709421277046204, 0.4606478810310364, -0.5291925668716431, -0.7977307438850403, -1.6463887691497803, 0.7705448865890503, -0.5512218475341797, 0.3135155737400055, -0.5091118216514587, 1.1171759366989136, 0.8453240394592285, -0.04966574162244797, -0.12493142485618591, 0.5013947486877441, -0.9481212496757507, -1.131005048751831, 0.34477555751800537, -0.7506973743438721, 0.48252204060554504, 0.1372727006673813, 0.13759252429008484, -0.21890930831432343, 0.676254153251648, 0.1391456425189972, -1.063936710357666, -0.8041539788246155, 0.6291635036468506, -0.8474822640419006, 0.1685037463903427, -0.44589510560035706, -0.16123995184898376, -1.7630306482315063, -0.2925688624382019, -0.06516825407743454, 0.5006104707717896, -0.5943424105644226, 0.8323713541030884, 1.0249381065368652, -1.0304359197616577, -0.20368391275405884, 0.0741204023361206, -0.18817538022994995, 0.4387178421020508, 0.5087664127349854, 0.5139930844306946, 0.03863024339079857, 0.2505341172218323, 0.34839096665382385, 0.18435701727867126, -0.610389232635498, 0.5100600123405457, 0.8853920102119446, -0.3773777484893799, -0.6259536147117615, 0.9802387356758118, -0.0075208405032753944, -0.8357109427452087, 0.3999080955982208, -1.0044176578521729, -0.9488688707351685, -0.059969477355480194, 0.7273246645927429, 0.235311821103096, -0.3806832730770111, -0.16818474233150482, -0.4919877350330353, 0.345505028963089, -0.6264183521270752, -0.29442042112350464, 0.38754695653915405, -0.31873440742492676, -0.382110595703125, 0.21530397236347198, 0.8942141532897949, -0.9763069748878479, -1.0511292219161987, -1.1175013780593872, -0.21665921807289124, -0.07578656822443008, 0.4876460134983063, -0.25538894534111023, -0.6295341849327087, 0.8531838059425354, 0.3966033160686493, 0.22894179821014404, 0.005083214957267046, -0.12523820996284485, -0.014819147065281868, 0.9229143261909485, -0.3642216920852661, -0.5178902745246887, -0.1394762545824051, 1.5830893516540527, 1.4590497016906738, -0.6557360291481018, 0.35758277773857117, -0.2032693475484848, -0.4477761685848236, 0.6487537026405334, 0.5586249828338623, -0.5343976616859436, 0.9606031775474548, -0.6495285630226135, 0.014141110703349113, -0.01877906173467636, -0.7944243550300598, -0.39453908801078796, 0.6100092530250549, 1.2084981203079224, 0.3829838037490845, -0.19876252114772797, 0.23793233931064606, 0.46980300545692444, 0.46025675535202026, 0.11673013120889664, 0.5171351432800293, 0.07173032313585281, -0.2548663318157196, 0.21655474603176117, 0.2583578824996948, 0.3379925787448883, -0.508714497089386, -0.6047056317329407, 0.25362926721572876, 0.5308589935302734, 0.11043256521224976, 0.5817018747329712, 0.7250779867172241, 0.3409334421157837, 0.3806488811969757, -0.10415863990783691, 0.672819197177887, -0.2166198194026947, -0.2424364686012268, -0.22673152387142181, -0.930122971534729, 0.19807007908821106, -0.6875832676887512, -0.33075058460235596, -0.1153528243303299, -0.2666004002094269, -0.12977127730846405, -0.3786388635635376, 0.3547017574310303, 0.9045011401176453, 0.5741338729858398, 0.6768245697021484, -0.602368950843811, -0.4781409502029419, -0.303148090839386, -0.7813979387283325, 0.34623733162879944, -0.7684488296508789, 0.008332744240760803, -0.34824272990226746, -0.2519534230232239, -0.3729163706302643]}, "authors": [{"authorId": "2218598523", "name": "Qihang Fan"}, {"authorId": "32885778", "name": "Huaibo Huang"}, {"authorId": "2244559750", "name": "Mingrui Chen"}, {"authorId": "2243404770", "name": "Hongmin Liu"}, {"authorId": "2243338248", "name": "Ran He"}], "references": [{"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "3b8c717f6b0c0f0eac6306a95fea9fc6ea04a3e2", "title": "Scale-Aware Modulation Meet Transformer"}, {"paperId": "9c703f8d5ad3ba10564f25ffae62cf8a71f1f6cb", "title": "Lightweight Vision Transformer with Bidirectional Interaction"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "3a87875c67c005b31bcdef036d399015e2f97177", "title": "Rethinking Local Perception in Lightweight Vision Transformer"}, {"paperId": "182e6c877155d7730378846380a3d1dc294fb54a", "title": "InceptionNeXt: When Inception Meets ConvNeXt"}, {"paperId": "2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d", "title": "BiFormer: Vision Transformer with Bi-Level Routing Attention"}, {"paperId": "64caaab51d8339f1b99874d3bddb79debbe661ca", "title": "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"}, {"paperId": "3cb3052825d387a2a0eed29c92f07466d2f4a9ed", "title": "Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition"}, {"paperId": "8d6520112cf35d84cf680de38411ba84dfc4a4da", "title": "Vision Transformer with Super Token Sampling"}, {"paperId": "26c80bd65baa90f5b18157de4951f4eb0b62ab69", "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"}, {"paperId": "a8a2a8229f99c291bf71ec92b801a073854c52e2", "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"}, {"paperId": "3e448df5aa191f7a3945d0fd609c8bc5966a2333", "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"}, {"paperId": "0307caf20f0ea22d6ae979b65c7ab85f3abf2af1", "title": "Dual Vision Transformer"}, {"paperId": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4", "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning"}, {"paperId": "86609b3567c1f039aecd87cc87ef8b8a995215bc", "title": "Global Context Vision Transformers"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "bf6ce546c589fa8054b3972b266532664914bd21", "title": "Fast Vision Transformers with HiLo Attention"}, {"paperId": "dbf6e95cb618f207f029276a6df11f4a9a6313d4", "title": "Inception Transformer"}, {"paperId": "b4da9f3505e22d3e766ba21890285b822dc71599", "title": "EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers"}, {"paperId": "ba609e5c83ad9b32723e547b9d3c89d97373f755", "title": "VSA: Learning Varied-Size Window Attention in Vision Transformers"}, {"paperId": "ad7bcec33f5206d4f28687a6a5a950de67010651", "title": "Neighborhood Attention Transformer"}, {"paperId": "259c681c76335540e13081efad584efdf9101868", "title": "DaViT: Dual Attention Vision Transformers"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "fa717a2e31f0cef4e26921f3b147a98644d2e64c", "title": "Focal Modulation Networks"}, {"paperId": "01e2e629189b498482eb4e3ac6959addc4451b03", "title": "ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer"}, {"paperId": "ba637c4f1a170f1e2dadeadb71a63cf2b9a46de2", "title": "Visual attention network"}, {"paperId": "c8831d0629f0eaf7f723317d71bbd60b8eb3c39f", "title": "UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5", "title": "QuadTree Attention for Vision Transformers"}, {"paperId": "e5cb26148791b57bfd36aa26ce2401e231d01b57", "title": "Vision Transformer with Deformable Attention"}, {"paperId": "0a0c204919ec72c6c335296ebf639ebc379d3ac5", "title": "Learned Queries for Efficient Local Attention"}, {"paperId": "15b0e710a9b8069d898ae6a0963d627e0fb86bd8", "title": "MPViT: Multi-Path Vision Transformer for Dense Prediction"}, {"paperId": "72e81bc41ffae1d414836169107910025aaacb75", "title": "Lite Vision Transformer with Enhanced Self-Attention"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "e0e6ae2ef8ef9f02e0c65ada61eadd82b7ce8a9c", "title": "Shunted Self-Attention via Multi-Scale Token Aggregation"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede", "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"}, {"paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65", "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "1fb10189c500e4902cd1b5afd406f57323d21be8", "title": "VOLO: Vision Outlooker for Visual Recognition"}, {"paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "cc9f3a61ea4eaabf43cbb30cd1dd718074932679", "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1", "title": "Panoptic Feature Pyramid Networks"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "04957e40d47ca89d38653e97f728883c0ad26e5d", "title": "Cascade R-CNN: Delving Into High Quality Object Detection"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "1a857da1a8ce47b2aa185b91b5cb215ddef24de7", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "6dc61f37ecc552413606d8c89ffbc46ec98ed887", "title": "Acceleration of stochastic approximation by averaging"}, {"paperId": null, "title": "Self guided Transformer with Evolving Token Reallocation . Sucheng ren , xingyi yang , songhua liu , xinchao wang"}, {"paperId": "2eef0c474d9f070edfbb01e23f9a0c5ffb1ef786", "title": "Doubly-Fused ViT: Fuse Information from Vision Transformer Doubly with Local Representation"}, {"paperId": "7925339228d5abad97d3a644ed19054d7095a63e", "title": "Orthogonal Transformer: An Efficient Vision Transformer Backbone with Token Orthogonalization"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "MMSegmentation Contributors"}, {"paperId": null, "title": "Ming-Ming"}]}