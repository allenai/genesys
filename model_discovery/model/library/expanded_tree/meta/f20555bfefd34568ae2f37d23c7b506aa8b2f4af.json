{"paperId": "f20555bfefd34568ae2f37d23c7b506aa8b2f4af", "abstract": "Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent). Our method updates the token importance distribution in a recursive manner. For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions. Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token. Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require. We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes. The results show that our method consistently provides more faithful token importance distributions.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent), which can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require."}, "embedding": {"model": "specter_v2", "vector": [0.5624226331710815, 0.5393746495246887, 0.03739723935723305, -0.1064855232834816, -0.4921227693557739, -0.10741756856441498, 1.0035045146942139, -0.5095759034156799, -0.09462877362966537, -0.09300458431243896, 0.6307979226112366, -0.21936680376529694, 0.2859356999397278, -0.05385122075676918, -0.1459101289510727, -0.1754479706287384, -0.6729791760444641, 0.3828282058238983, -0.3480285406112671, -0.11900592595338821, -0.08455429971218109, -0.9826072454452515, -1.1350711584091187, -0.019963745027780533, 0.3009299039840698, 0.3256746828556061, 0.6572936773300171, 1.487809419631958, -0.4717331528663635, 0.2503144443035126, 0.3213752508163452, -0.38855084776878357, 0.15710753202438354, -0.6405671834945679, -0.5466181635856628, 0.054570265114307404, -0.07512051612138748, -0.3733791708946228, -0.3649790287017822, 0.5143066048622131, 0.046964745968580246, -0.010848545469343662, 1.0030769109725952, -0.9083625078201294, -0.4382852613925934, 1.2434844970703125, 0.6828240156173706, 0.9561201930046082, -0.267322301864624, -0.3332725167274475, 1.2683252096176147, -1.225602149963379, 0.21419447660446167, 1.5541743040084839, 0.09649088233709335, 0.4987792372703552, -0.2756016254425049, -0.45728328824043274, 0.9326340556144714, -0.10637351125478745, -0.8427088260650635, -0.3641582727432251, 0.20780465006828308, -0.2502003014087677, 1.6468400955200195, -0.3054924011230469, -0.09598864614963531, 0.7672870755195618, 0.033681195229291916, 1.476309895515442, -0.29333221912384033, -1.0395427942276, -0.5125572085380554, 0.18083874881267548, -0.08445166796445847, 0.7272452116012573, -0.526424765586853, 0.29764893651008606, -0.8543541431427002, 0.19743211567401886, 0.5828726291656494, -0.6097145080566406, 0.1676938384771347, 0.41222304105758667, -0.20738214254379272, 0.8350037932395935, 0.018033063039183617, 0.8029581904411316, -0.07377628982067108, 1.0098050832748413, 0.337506502866745, 0.03955841064453125, 0.1432955265045166, 0.06466899067163467, 0.142243430018425, 0.3778250813484192, -0.6868139505386353, 0.20103192329406738, 0.13268683850765228, 0.9753628373146057, -0.17473869025707245, 0.7858640551567078, -1.141376256942749, 0.18384546041488647, 1.7261260747909546, 0.23105041682720184, 0.9305596947669983, -0.488788902759552, 0.3274483382701874, -1.0467997789382935, -0.2587164044380188, -0.5076568126678467, 0.24524085223674774, -0.4203661382198334, -0.8105626106262207, -0.984652042388916, -0.4016645848751068, 0.08623643219470978, -0.9888818264007568, 1.1707252264022827, -0.22004781663417816, -0.0900595560669899, -0.09832528978586197, 0.3030601739883423, 0.3896217942237854, 0.5549972653388977, 0.41380760073661804, 0.5020285248756409, 0.5241824984550476, -0.6104162931442261, -0.557525098323822, -0.9261557459831238, 0.1565658152103424, -0.7225687503814697, 0.29611337184906006, -0.26574528217315674, -1.0154669284820557, -1.128659963607788, -0.6533733606338501, 0.1630997210741043, -0.2352556735277176, 0.5245176553726196, 1.196656584739685, 0.3371148109436035, -1.0638123750686646, 0.9337321519851685, 0.02647077664732933, 0.11922302097082138, 0.3493543565273285, 0.03711177036166191, 0.5082089304924011, -0.1247374564409256, -1.428523063659668, 0.06900785863399506, 0.14041917026042938, -0.6437294483184814, -0.5670773386955261, -0.4583006501197815, -0.9233004450798035, -0.09216611832380295, -0.07935087382793427, -1.0593762397766113, 1.689178228378296, -0.43343910574913025, -1.5554438829421997, 0.27936670184135437, -0.7702637910842896, 0.4178301990032196, 0.2958854138851166, -0.1000753715634346, -0.1252683401107788, -0.7853122353553772, 0.26035642623901367, 0.815421998500824, 0.9058420062065125, -0.023684926331043243, -0.4455626606941223, 0.5963732600212097, -0.4139866828918457, 0.09354189038276672, -0.3667956590652466, 0.6234840154647827, -0.2509673237800598, -0.3560955822467804, 0.2336837202310562, 0.8247677087783813, 0.06582160294055939, -0.6144222021102905, -0.7651383876800537, -1.4073569774627686, 0.6379484534263611, -0.13396821916103363, 0.7895961403846741, -0.5232974886894226, -0.6914641857147217, -0.13251151144504547, -0.12104077637195587, -0.5656768083572388, -0.543235182762146, 0.5741170644760132, -0.4142247438430786, 0.49290892481803894, 0.010576198808848858, -1.1212434768676758, 0.31479915976524353, -0.39454516768455505, -0.6912249326705933, -0.5112034678459167, 0.5838719606399536, 0.9232211112976074, -0.8654972910881042, -0.1418432742357254, -0.2042192667722702, 0.3830816149711609, -0.7083067297935486, 1.2603565454483032, -0.28502950072288513, 0.22646424174308777, -0.23208573460578918, -0.4334544241428375, -0.23717565834522247, -0.3033730685710907, 0.03800710290670395, 0.0742841362953186, 0.11114414781332016, 0.7077524065971375, -0.3330085277557373, 1.1242396831512451, -0.44856297969818115, 0.2079245150089264, -0.6817070245742798, -0.3255159854888916, 0.31787168979644775, 0.5816912651062012, -0.5189080834388733, -0.14779454469680786, 0.3650355339050293, 0.47390761971473694, -0.2596991956233978, 0.6033794283866882, 0.6827043294906616, 0.46630847454071045, -0.25353702902793884, 0.3851896822452545, 0.8803400993347168, -0.1951237916946411, 0.7620263695716858, 0.6847125291824341, 0.5675467252731323, 0.6465998888015747, 0.37309712171554565, 0.06482627987861633, 0.26688632369041443, -0.9910370111465454, -0.405050665140152, 0.5328568816184998, 0.9956796169281006, 1.2794262170791626, 0.2770449221134186, -0.7615821957588196, -0.2529342770576477, -0.2317340075969696, 0.8447505235671997, 1.4305051565170288, -0.5241872072219849, -0.2338806688785553, -0.6426092386245728, -0.17280206084251404, -0.15465357899665833, 0.46450144052505493, -0.5827286243438721, -0.5929804444313049, -0.22630631923675537, -1.1236414909362793, 0.8116393089294434, 0.3126799166202545, 0.5528889894485474, -0.6533478498458862, -0.0069267661310732365, -0.07576047629117966, -0.14006304740905762, -0.4118345379829407, -0.8165767788887024, 0.4902471601963043, -0.3244820833206177, 0.2205902338027954, -0.03668108955025673, 0.12147066742181778, 0.27387145161628723, -0.7824812531471252, 0.9898316860198975, -0.5086860060691833, -0.6647726893424988, -0.1656922549009323, 0.6797742247581482, -1.039847731590271, -1.256697654724121, 0.5028529763221741, 0.05959729850292206, -0.1261298805475235, 0.02106410637497902, 0.5320873856544495, 0.18127097189426422, -0.05772347375750542, -0.12903958559036255, 0.5374855399131775, -0.09287998080253601, 0.026356438174843788, 0.6586092710494995, -0.2871515154838562, -0.050759728997945786, -1.0413159132003784, 0.9838321805000305, -0.18796858191490173, -0.17192517220973969, -0.03966185450553894, -0.6955903172492981, -0.05246880650520325, 0.7114161252975464, -0.3963070213794708, -0.5365023016929626, -0.9675894975662231, 0.14514721930027008, -0.09988009929656982, -0.35760363936424255, 0.04927123337984085, 0.1997169852256775, 0.4967697560787201, 0.07810311764478683, 0.6819819808006287, 0.22751958668231964, -0.06335725635290146, 0.8031325936317444, -1.0277765989303589, 0.38920119404792786, 0.06306591629981995, 0.33153191208839417, -0.47674304246902466, -0.42236030101776123, -0.3725612759590149, -0.212483212351799, -0.303133100271225, -0.1707679182291031, -0.7410862445831299, -0.020408131182193756, -0.4179747700691223, -0.8311313390731812, 0.11016149818897247, -1.17815101146698, 0.007592330686748028, 0.020164474844932556, -0.38005220890045166, -0.15228639543056488, -1.1162376403808594, -0.8888532519340515, -0.5077136754989624, -0.6044790148735046, -0.9752311706542969, 0.334084153175354, 0.2970111668109894, -0.650846540927887, -0.8013413548469543, 0.14215661585330963, -0.37768441438674927, 1.287777066230774, -0.8305067420005798, 0.9022156000137329, 0.3019930124282837, -0.13589993119239807, -0.14425040781497955, 0.4566403925418854, 0.4373515844345093, 0.14874480664730072, 0.47660771012306213, -1.2659618854522705, 0.2950131297111511, -0.5459507703781128, -0.15700241923332214, 0.17102737724781036, 0.3886866569519043, 0.7429671883583069, -0.18927179276943207, -0.4176980257034302, 0.40778303146362305, 1.243909239768982, -0.7408642172813416, -0.04341127723455429, 0.06405927985906601, 1.0633848905563354, 0.11553649604320526, -0.5910468697547913, 0.8191028833389282, 0.6300056576728821, 0.515663743019104, 0.22853194177150726, -0.40148383378982544, -0.6289127469062805, -0.922328770160675, 0.7201296091079712, 1.5540214776992798, 0.026884129270911217, -0.2993667721748352, -0.8803963661193848, 0.5300698280334473, -1.0961076021194458, -0.6959188580513, 0.5434974431991577, 0.7484492659568787, 0.5376189947128296, -0.6294143199920654, -0.40373918414115906, -0.019345466047525406, 0.5363876223564148, -0.08352655172348022, -0.20971602201461792, -0.8441891670227051, 0.2786853313446045, 0.27390626072883606, -0.0675702840089798, 0.7280778288841248, -0.45858052372932434, 0.5702863335609436, 14.597164154052734, 0.803849458694458, -0.015086827799677849, 0.5166019201278687, 1.010606050491333, 0.16293828189373016, -0.7053079605102539, -0.13932377099990845, -1.7865339517593384, -0.04377739503979683, 1.2337875366210938, -0.1255791336297989, 0.350090891122818, 0.39323362708091736, 0.04129043594002724, 0.37932559847831726, -0.19622860848903656, 0.5013221502304077, 0.7255356311798096, -1.0255757570266724, 0.8587404489517212, 0.17058902978897095, 0.30276596546173096, 0.3626948297023773, 0.826321542263031, 0.6871591806411743, 0.6727533936500549, -0.5798134803771973, 0.94646817445755, 0.30155274271965027, 0.6802775859832764, -0.1215963140130043, 0.19882957637310028, 0.26637670397758484, -0.9350014925003052, -0.024607842788100243, -0.662014365196228, -1.0975817441940308, 0.36560437083244324, 0.41869592666625977, -1.382706642150879, -0.3405138850212097, -0.38336381316185, 0.42476314306259155, -0.03298905864357948, 0.23139390349388123, -0.5778300762176514, 1.25814688205719, 0.051899828016757965, 0.0027682045474648476, 0.27216216921806335, 0.710931122303009, 0.26664403080940247, 0.26871249079704285, 0.1412229686975479, -0.5131059288978577, 0.010731198824942112, 0.9132871627807617, -0.5366002321243286, -0.19964902102947235, -0.31344157457351685, -0.12853175401687622, -0.1338367760181427, 1.1313196420669556, 0.030110729858279228, 0.23669394850730896, 0.04427991434931755, 0.39692988991737366, 0.7567345499992371, 0.10847166925668716, -0.26493194699287415, -0.17988453805446625, 0.34719040989875793, -0.32173022627830505, 0.14553113281726837, 0.7636974453926086, -0.12475713342428207, -0.24341320991516113, -0.9229192137718201, -0.16956698894500732, 0.0982339084148407, -0.6502384543418884, -1.0470621585845947, 0.911702573299408, -0.17560866475105286, -0.11025583744049072, -0.34219011664390564, -0.5269045233726501, -0.7623369097709656, 0.9947360754013062, -1.578004240989685, -0.6835623979568481, 0.4517892897129059, -0.3399291932582855, -0.16607195138931274, -0.27363255620002747, 1.1865904331207275, -0.21723295748233795, -0.25191253423690796, 0.300322562456131, -0.06425841152667999, 0.0408828966319561, -0.19433896243572235, -1.1776258945465088, 1.2921745777130127, 0.5782890319824219, 0.0830148309469223, 0.24134674668312073, -0.014995330013334751, 0.1651083528995514, -0.5399065017700195, -0.3365131914615631, 1.1865381002426147, -0.9104282259941101, -0.769690990447998, -0.8024040460586548, -0.6054441332817078, -0.01177687756717205, 0.6446046829223633, -0.6440672874450684, 0.5410186648368835, 0.38851243257522583, -0.3623819947242737, -0.05588048696517944, -0.3447180986404419, -0.16914767026901245, 0.3434302508831024, -0.2853282690048218, -0.09447084367275238, 0.08495255559682846, 0.20489661395549774, -1.0115609169006348, -0.042080868035554886, -0.7231195569038391, -0.12104629725217819, 0.19949224591255188, 1.0199952125549316, -0.5638323426246643, 0.9132472276687622, 0.6702842116355896, 0.17619551718235016, -1.113455057144165, -0.8513545989990234, -0.9991262555122375, 0.49154025316238403, 0.6678354740142822, 1.0051870346069336, -0.3000979423522949, -0.07989499717950821, 0.6346570253372192, 0.096757672727108, -0.08904775232076645, -0.5407953262329102, -0.1691311001777649, 0.2575679123401642, -0.8054570555686951, 0.44850975275039673, -0.28919342160224915, -0.21496480703353882, 0.39630183577537537, 0.4219760596752167, 0.4146425724029541, -0.14927595853805542, -0.7083402872085571, 0.3778030574321747, 0.06998569518327713, -0.359437495470047, -0.8031018972396851, -0.4263368546962738, -0.8878710865974426, -0.0669940635561943, -1.6366722583770752, 0.29062220454216003, -1.0303089618682861, -0.16243046522140503, -0.11663000285625458, -0.24627342820167542, 0.4061930179595947, -0.2650250494480133, -0.29160991311073303, -0.2537957429885864, -0.5317003130912781, -0.3587547540664673, 0.8913339376449585, 0.800197958946228, -0.5427601933479309, 0.38779228925704956, 0.010157076641917229, -0.42917707562446594, 0.2568470239639282, 0.608751654624939, -0.6325882077217102, -0.8615606427192688, -1.5414294004440308, 0.31107044219970703, -0.5750504732131958, 0.08787630498409271, -0.6569361686706543, 0.43086525797843933, 0.5879178047180176, 0.3324507176876068, 0.350292831659317, -0.021030984818935394, -0.1311495304107666, -0.3419579565525055, 0.2073569893836975, -0.7586297392845154, 0.20440925657749176, 0.027705643326044083, -0.5888744592666626, -0.018318289890885353, 0.3994694650173187, -0.2834053933620453, -1.2126221656799316, -0.5662981271743774, 0.5488761067390442, -0.8090215921401978, 0.2940458655357361, -0.6523599624633789, -0.14211831986904144, -0.7955808043479919, -0.49975237250328064, 0.3987729847431183, -0.12525783479213715, -0.6745129823684692, 1.2364015579223633, 0.780669629573822, -0.985620379447937, -0.38760054111480713, 0.568167507648468, -0.22976666688919067, -0.12261341512203217, 0.4235466718673706, 0.23319293558597565, 0.07016648352146149, 0.5011051893234253, 0.4817202389240265, 0.23796553909778595, -0.8488937616348267, -0.6200161576271057, 0.6132099628448486, -0.5508145689964294, -0.23806379735469818, 1.47561776638031, -0.2304139882326126, -1.1491423845291138, -0.28750312328338623, -1.0160791873931885, -0.5584419369697571, -0.5205128788948059, 0.7342618107795715, -0.011133593507111073, -0.07864882051944733, -0.12881021201610565, -0.34690144658088684, -0.055477894842624664, -0.29672759771347046, -0.7079940438270569, 0.07355047762393951, -0.19749972224235535, -0.14227557182312012, 0.5153128504753113, 0.4627741575241089, -0.9831139445304871, -0.6733909845352173, -0.20890240371227264, -0.732185959815979, -0.249529629945755, 0.2920759916305542, -0.5863915085792542, -0.5877953767776489, 0.9112510681152344, 0.2595532536506653, 0.007790246047079563, -0.16858172416687012, -0.07158242911100388, -0.08953895419836044, -0.022449592128396034, 0.1412981152534485, -0.5371645092964172, -0.5747202634811401, 1.50174880027771, 1.345502495765686, -0.6218129396438599, -0.048175934702157974, -0.2066941261291504, -1.0173888206481934, 1.1502692699432373, 0.4211942255496979, 0.3838837444782257, 0.5573158264160156, -0.042672768235206604, 0.2729824185371399, 0.46005257964134216, -1.3419326543807983, -0.22891393303871155, 0.8209709525108337, 1.3240216970443726, 1.2694909572601318, 0.3986574411392212, 0.11344233900308609, 0.960521936416626, -0.311040997505188, 0.050344426184892654, 0.5176666975021362, 0.3347994387149811, -0.03787720575928688, -0.4028245210647583, -0.06819387525320053, 0.8731771111488342, -0.5755007266998291, -0.7353394031524658, 0.1673639863729477, 0.33868423104286194, 0.2118123322725296, 0.602076530456543, 0.5234492421150208, 0.17782677710056305, 0.16398142278194427, 0.36600542068481445, 0.5245172381401062, -0.547716498374939, -0.2637167274951935, -0.15036936104297638, -0.7906159162521362, 0.031879715621471405, 0.14540575444698334, -0.6097034215927124, -0.6253882050514221, -0.006401967257261276, 0.1278812140226364, 0.003703596070408821, 0.6418795585632324, 1.188724160194397, 0.6168692111968994, 0.8882063031196594, 0.04515109583735466, -0.7528694272041321, -0.2732382118701935, -1.1789406538009644, 0.029384836554527283, -0.43966564536094666, -0.57524573802948, -0.4519329071044922, -0.27138054370880127, 0.41682037711143494]}, "authors": [{"authorId": "2300129468", "name": "Zhixue Zhao"}, {"authorId": "2282137786", "name": "Boxuan Shan"}], "references": [{"paperId": "c169f97d1d7ca40b4cfcc00b5bf5ae199c33c23f", "title": "Incorporating Attribution Importance for Improving Faithfulness Metrics"}, {"paperId": "932b9fd1e2aaf3c56841304b7a49e30c804f6234", "title": "Inseq: An Interpretability Toolkit for Sequence Generation Models"}, {"paperId": "64fc00dd379c1cd176c001794308459b92bb2490", "title": "ferret: a Framework for Benchmarking Explainers on Transformers"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1c709eef701d933af1383c790c13209f06806b60", "title": "Rationales for Sequential Predictions"}, {"paperId": "3eeedb6651a629a105c1185ada862e2cad7a0522", "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives"}, {"paperId": "88fc348155ed6612e2832a811148167ef998923d", "title": "Investigating sanity checks for saliency maps with image and text classification"}, {"paperId": "e8a44417703e9facebf5a630069b7484664d6087", "title": "The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations"}, {"paperId": "d5de10333fb28b3987df399273cb6aa47ce02a7b", "title": "Attention vs non-attention for a Shapley-based explanation method"}, {"paperId": "a88eb2104efba1a421700501eb56a995b3bb2b1c", "title": "Flexible Instance-Specific Rationalization of NLP Models"}, {"paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"}, {"paperId": "36000367bcb2b51a2567bb27a9147e9f9293ff5e", "title": "RANCC: Rationalizing Neural Networks via Concept Clustering"}, {"paperId": "508884a136a461869be128027950d2aa1778518c", "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?"}, {"paperId": "ae06bc1e8e67c27b89329ebcfe61b71625d853f6", "title": "A Diagnostic Study of Explainability Techniques for Text Classification"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "922e6e3bafe38a712597c05d3a907bd10763b427", "title": "Learning to Faithfully Rationalize by Construction"}, {"paperId": "579476d19566efc842929ea6bdd18ab760c8cfa2", "title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?"}, {"paperId": "087dd95e13efd47aef2a6582e6801b39fc0f83d8", "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "64ffb253d20ee12114a8d15d01404bd17ae99220", "title": "Interpretable Neural Predictions with Differentiable Binary Variables"}, {"paperId": "3785f9083dcb46d2bea7ee771c2a513bb43917a6", "title": "Comparing Automatic and Human Evaluation of Local Explanations for Text Classification"}, {"paperId": "de01f74a899ecc7e3228cddbc743aaf6faf5e55f", "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks"}, {"paperId": "62e39c6dbcead1fe4d29017914591d929aa6ac4c", "title": "The (Un)reliability of saliency methods"}, {"paperId": "442e10a3c6640ded9408622005e3c2a8906ce4c2", "title": "A Unified Approach to Interpreting Model Predictions"}, {"paperId": "f302e136c41db5de1d624412f68c9174cf7ae8be", "title": "Axiomatic Attribution for Deep Networks"}, {"paperId": "e81317c5166b7c53654ea2ffa91484a08bc772ff", "title": "Investigating the influence of noise and distractors on the interpretation of neural networks"}, {"paperId": "5582bebed97947a41e3ddd9bd1f284b73f1648c2", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"}, {"paperId": "467d5d8fc766e73bfd3e9415f75479823f92c2f7", "title": "Rationalizing Neural Predictions"}, {"paperId": "c0883f5930a232a9c1ad601c978caede29155979", "title": "\u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier"}, {"paperId": "17a273bbd4448083b01b5a9389b3c37f5425aac0", "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation"}, {"paperId": "fafa541419b3756968fe5b3156c6f0257cb29c23", "title": "Visualizing and Understanding Neural Models in NLP"}, {"paperId": "c52acb4e4143ace520166691a29faaeaf892ac47", "title": "Extraction of Salient Sentences from Labelled Documents"}, {"paperId": "b8501c56646f0fb861e1ed237fc467a578c984c3", "title": "A Markovian approach to distributional semantics with application to semantic compositionality"}, {"paperId": "e89f679710507e239775a1e9c81988c3f928cbed", "title": "Word Embeddings through Hellinger PCA"}, {"paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238", "title": "Visualizing and Understanding Convolutional Networks"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "a468716866ffe032ff2d72a8978c5f372ce21242", "title": "Semantic Classification with Distributional Kernels"}, {"paperId": "527eb9c939801f1edcedace66eff7bbc02f74e80", "title": "Characterising Measures of Lexical Distributional Similarity"}, {"paperId": null, "title": "Evaluating Saliency Meth-ods for Neural Language Models"}, {"paperId": null, "title": "Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Is Attention Inter-pretable?"}, {"paperId": "bc8d82309e6d91819881b9bee7f6d8373987d18b", "title": "Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics"}, {"paperId": null, "title": "61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"}, {"paperId": null, "title": "2023. Self-checkgpt: Zero-resource black-box hallucination detection for generative large language models"}]}