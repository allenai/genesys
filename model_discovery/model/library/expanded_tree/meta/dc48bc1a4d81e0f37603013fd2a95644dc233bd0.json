{"paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0", "abstract": "Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5\u2019s RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.", "venue": "arXiv.org", "year": 2023, "citationCount": 21, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is theoretically prove that this can represent some of the popular relative position encodings, such as T5\u2019s RPE, Alibi, and Kerple, and empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks."}, "embedding": {"model": "specter_v2", "vector": [0.4816349744796753, 0.2180655300617218, -0.5446552634239197, -0.19095376133918762, -0.36063632369041443, -0.2818538248538971, 0.899918794631958, -0.2908550202846527, -0.6253892779350281, -0.3473157286643982, 0.6216111779212952, -0.3003031611442566, 0.17047607898712158, 0.19128085672855377, -0.11503001302480698, 0.11924628913402557, -0.7208378314971924, 0.1744515746831894, 0.20861224830150604, -0.7307950854301453, -0.1353786587715149, -0.9848909974098206, -1.100274682044983, 0.3109090030193329, 0.3228045403957367, 0.3242935240268707, 0.23763824999332428, 0.8835606575012207, -0.41297754645347595, 0.5877634882926941, 0.30110418796539307, -0.5227502584457397, 0.19946034252643585, -0.20530283451080322, -0.1733524352312088, -0.3350226581096649, 0.19941139221191406, -0.4843311011791229, -0.5020015239715576, 0.5605504512786865, -0.24360749125480652, 0.3471517264842987, 0.36046743392944336, -0.8038644194602966, -0.6381824612617493, 1.2015423774719238, 0.695620059967041, 0.7904420495033264, -0.11682651937007904, -0.742693305015564, 1.3892344236373901, -1.6069222688674927, 0.17493362724781036, 1.2787387371063232, 0.8668504953384399, 0.5526688694953918, -0.03154493123292923, -0.3274781405925751, 0.49645525217056274, 0.3293328881263733, -0.6714873909950256, -0.51837158203125, -0.08323118090629578, 0.056931495666503906, 1.6299139261245728, -0.3333096504211426, -0.06983786821365356, 0.8516516089439392, -0.2144777774810791, 1.1985888481140137, -0.2931623160839081, -1.0199710130691528, -0.4554358720779419, -0.08792819082736969, 0.43430426716804504, 0.6945421099662781, -0.878937304019928, 0.5441995859146118, -0.5711107850074768, 0.1312534064054489, 0.29631882905960083, 0.24886739253997803, -0.057172782719135284, 0.0343291237950325, -0.4093112349510193, 0.3538638949394226, 0.36131760478019714, 1.013692855834961, 0.08469673246145248, 0.8205379247665405, 0.4951382577419281, 0.4403565526008606, 0.18296772241592407, -0.08346494287252426, 0.2101307213306427, 0.20287422835826874, -0.8075148463249207, 0.016280604526400566, -0.10643211007118225, 0.8323122262954712, -0.4327736794948578, -0.00524748582392931, -0.9711547493934631, 0.3064306378364563, 1.0808197259902954, -0.3262399435043335, 0.2918100357055664, -1.0933445692062378, 0.049500253051519394, -0.8658427000045776, 0.3552110493183136, -0.5068087577819824, 0.41824883222579956, -0.2863721549510956, -0.5728314518928528, -1.3380603790283203, -0.13599680364131927, 0.4982917904853821, -0.7245485186576843, 0.8577117919921875, -0.4925191402435303, 0.5022405982017517, 0.014234243892133236, 0.25445622205734253, 0.5087245106697083, 0.8605924844741821, 0.3816848397254944, 0.006175863556563854, 0.6434550881385803, -0.6955041289329529, -0.7227796912193298, -1.0364081859588623, 0.8616414666175842, -0.2562253773212433, 0.6264102458953857, -0.19394555687904358, -1.1184799671173096, -0.9753379821777344, -1.1771214008331299, -0.23513488471508026, -0.47459062933921814, 0.29117003083229065, 0.5872006416320801, 0.37748846411705017, -1.090583086013794, 1.037136197090149, -0.26838648319244385, -0.13763722777366638, 0.2731819450855255, -0.3080189526081085, 0.378388911485672, -0.5795945525169373, -1.7549951076507568, 0.3748854994773865, 0.23108074069023132, -0.5319002270698547, -0.1233990341424942, -0.7524996995925903, -1.1989860534667969, -0.1954214870929718, 0.6218974590301514, -0.32661184668540955, 1.4990712404251099, -0.07878131419420242, -1.1298342943191528, 0.6358616352081299, -0.4658195674419403, -0.03343881294131279, 0.28681910037994385, -0.09202244132757187, -0.49552690982818604, -0.5511528253555298, -0.2766529619693756, 0.41467320919036865, 0.21743744611740112, -0.24227260053157806, -0.2861991226673126, 0.3375850319862366, 0.1045985221862793, -0.02997645176947117, -0.23365125060081482, 1.093641757965088, -0.290558785200119, -0.11659703403711319, 0.2819780707359314, 0.6938478946685791, 0.03776349872350693, -0.04455093294382095, 0.19944477081298828, -1.115748643875122, 0.9185123443603516, -0.11282400041818619, 1.1540230512619019, -1.0189480781555176, -0.7501246929168701, -0.46759307384490967, -0.3071664869785309, -0.17816421389579773, -0.6508137583732605, 0.4811459481716156, -0.3459508717060089, 0.6977599263191223, -0.04417870566248894, -1.1109964847564697, 0.09181800484657288, 0.0612470917403698, -0.5858887434005737, -0.22916936874389648, -0.30190446972846985, 1.314653754234314, -1.20801842212677, 0.17202211916446686, 0.2322084903717041, 0.3550562560558319, -0.6740355491638184, 0.8952442407608032, -0.4648151099681854, 0.0032027510460466146, 0.4010733664035797, -0.570794403553009, -0.3810390830039978, -0.3575964868068695, 0.2695249617099762, -0.4644465148448944, -0.14351576566696167, 0.7456979155540466, 0.004195487126708031, 1.4063185453414917, -0.40892496705055237, 0.655663788318634, -0.36268022656440735, -0.7830130457878113, -0.08324667066335678, 0.6811149716377258, 0.0035829979460686445, -0.49121180176734924, 0.36145052313804626, 0.2954534888267517, -0.5872448086738586, 0.37992796301841736, 0.8992328643798828, 0.5038256645202637, -0.460052490234375, 0.01707080379128456, 0.4677906632423401, 0.18212929368019104, 0.4005657434463501, 0.2829083502292633, 0.4501613676548004, 0.5494714379310608, 0.3544492721557617, -0.12174943089485168, 0.24967943131923676, -0.9826920032501221, -0.11482230573892593, 0.43932706117630005, 0.5803731083869934, 0.8118032813072205, 0.0663532167673111, -0.5708166360855103, -0.8437498807907104, -0.08901868015527725, 0.5272929668426514, 1.7111685276031494, 0.09761626273393631, -0.43268120288848877, -0.645422637462616, -0.12035315483808517, -0.7319595217704773, 0.500988245010376, -0.49945905804634094, -0.43806737661361694, -0.6816875338554382, -0.6069031357765198, 0.8682003021240234, 0.5310654044151306, 0.9408899545669556, -0.3359889090061188, -0.30197787284851074, -0.08252866566181183, -0.24417895078659058, -0.7050683498382568, -0.8417057991027832, 0.33396196365356445, -0.2992683947086334, -0.03338497504591942, 0.06161203980445862, -0.12820421159267426, -0.09237754344940186, -0.3866886496543884, 1.0842448472976685, -0.33205386996269226, -0.33526670932769775, 0.30925896763801575, 0.16507071256637573, -0.3600093126296997, -1.0232573747634888, 0.023691216483712196, -0.02375669591128826, -0.5171194672584534, 0.4300435483455658, 0.6751144528388977, -0.00408125901594758, 0.5010925531387329, -0.46275219321250916, 0.22980523109436035, -0.1582041084766388, 0.21487383544445038, 0.4792170226573944, -0.3719850182533264, -0.05094968527555466, -1.0447101593017578, 0.6791268587112427, 0.11348655819892883, -0.3049106001853943, 0.7772727608680725, -0.4977653920650482, -0.3326036334037781, 0.5117273330688477, -0.7524208426475525, -0.324510395526886, -1.4345331192016602, 0.21675345301628113, -0.4723358452320099, 0.09525914490222931, 0.1354527622461319, 0.27089616656303406, 0.598680853843689, 0.6032324433326721, 0.42401018738746643, 0.21158921718597412, 0.06690368056297302, 0.7428985238075256, -0.6743052005767822, 0.6984764337539673, 0.032187215983867645, -0.16555441915988922, -0.5360003709793091, -0.19576607644557953, -0.7796510457992554, -0.6089265942573547, -0.49544212222099304, -0.24362990260124207, -0.3699260950088501, 0.3588520288467407, -0.324336439371109, -0.7093483209609985, -0.18431790173053741, -1.0167490243911743, -0.2113894522190094, -0.0143863121047616, -0.7368248701095581, -0.16242648661136627, -0.9101075530052185, -1.401535987854004, -0.6034370064735413, -0.43689119815826416, -0.8823763728141785, 0.3953275680541992, -0.06089725345373154, -0.2840477228164673, -0.555762767791748, -0.19152073562145233, -0.3720466196537018, 0.9686253070831299, -0.5892618894577026, 0.7877353429794312, 0.1203213632106781, -0.39844030141830444, -0.1674129068851471, 0.5260604023933411, 0.6956027150154114, -0.03473404049873352, 0.4658140540122986, -0.7778121829032898, 0.13469070196151733, -0.10409519076347351, -0.13558286428451538, 0.16667313873767853, 0.09792860597372055, 0.910912811756134, -0.18122543394565582, -0.5153992772102356, 0.17741578817367554, 1.294722318649292, -0.2397235631942749, 0.432068794965744, 0.0070988028310239315, 0.8255670666694641, 0.16681396961212158, 0.0911463052034378, 0.35637417435646057, 0.1540573537349701, 0.3451175391674042, 0.029930872842669487, 0.25587916374206543, 0.07780233025550842, -0.6282063126564026, 0.7166727185249329, 1.5374271869659424, 0.07198523730039597, -0.2030114084482193, -1.1367526054382324, 0.6803069710731506, -1.135305404663086, -1.4629756212234497, 0.8462160229682922, 1.0383503437042236, 0.4907435476779938, -0.36034074425697327, -0.3443918824195862, 0.08292047679424286, 0.6189297437667847, 0.41636520624160767, -0.07132836431264877, -0.3788003623485565, 0.15051715075969696, 0.5054448246955872, 0.14441770315170288, 0.809380054473877, -0.3973633944988251, 0.6393015384674072, 15.010416030883789, 0.8886209726333618, -0.0916391909122467, 0.8404285311698914, 0.7995460033416748, 0.24950386583805084, -0.5846632719039917, 0.037141915410757065, -1.0532892942428589, 0.10115262866020203, 1.660292148590088, -0.1471291482448578, 0.6107470393180847, -0.058013033121824265, -0.10661663115024567, 0.37612664699554443, -0.8062099814414978, 0.6220427751541138, 0.40892675518989563, -1.4373894929885864, 0.2352614551782608, -0.16320408880710602, 0.27211177349090576, 0.1962762475013733, 0.8740875720977783, 0.8574878573417664, 0.4171762466430664, -0.5124903321266174, 0.6099156141281128, 0.19711080193519592, 1.2984932661056519, -0.3902624249458313, 0.15877395868301392, 0.6183024644851685, -0.95379239320755, -0.30994802713394165, -0.645957350730896, -0.9939324855804443, 0.08248471468687057, -0.07961387932300568, -0.7790988683700562, -0.30179810523986816, -0.3032326102256775, 0.9556462168693542, 0.43684402108192444, 0.15229663252830505, -0.26403024792671204, 0.746243417263031, 0.02385619282722473, 0.2375480979681015, 0.3242952227592468, 0.27515706419944763, 0.1401393711566925, -0.2267458587884903, 0.30419111251831055, 0.0077661736868321896, 0.30464276671409607, 0.3627873659133911, -0.3450086712837219, -0.3390312194824219, -0.5826968550682068, -0.203290194272995, 0.1606709361076355, 0.4382515549659729, 0.6063754558563232, 0.012419506907463074, -0.32247620820999146, 0.5613301396369934, 0.34282150864601135, 0.25085461139678955, -0.6431362628936768, 0.09794222563505173, 0.45396551489830017, -0.17449699342250824, -0.35486990213394165, 0.5999357104301453, 0.06056647375226021, -0.4535551369190216, -0.8788377046585083, -0.332256019115448, 0.2513626515865326, -0.8160796165466309, -0.5813127756118774, 0.8179779052734375, -0.055480021983385086, -0.7386319637298584, 0.018294261768460274, -0.7361350655555725, -0.15888801217079163, 0.345966100692749, -1.1571416854858398, -0.6262900829315186, 0.16191673278808594, -0.3269006311893463, -0.10796836018562317, 0.3476214110851288, 1.2017858028411865, 0.24439501762390137, -0.046260349452495575, 0.28354716300964355, 0.3239423334598541, 0.1688820719718933, -0.14211523532867432, -0.8396660089492798, 0.8735817074775696, 0.3718308210372925, 0.0801793560385704, 0.5799490213394165, 0.03294984623789787, 0.4192031919956207, -0.3574410676956177, -0.2272312343120575, 0.7989484071731567, -1.2954587936401367, -0.6677953600883484, -0.8260311484336853, -0.857835590839386, 0.727359414100647, 0.5651426315307617, -0.2886158227920532, 0.4686112105846405, -0.04155654087662697, -0.5223550200462341, -0.23964698612689972, -0.7189487218856812, 0.28521445393562317, 0.9916617274284363, -1.1145302057266235, -0.5835791230201721, -0.368274450302124, 0.44792866706848145, -1.037861943244934, -0.5371343493461609, 0.05123833939433098, 0.19471469521522522, -0.17573028802871704, 1.3176926374435425, -0.19585302472114563, 0.8855113387107849, 0.8057689666748047, -0.17719756066799164, -0.4948062002658844, -0.2264401614665985, -0.8226516842842102, 0.11131605505943298, 0.34523805975914, 0.39622509479522705, -0.2102770060300827, -0.007709616329520941, 0.6500348448753357, 0.14949676394462585, -0.3438407778739929, -0.6670348048210144, -0.4620044231414795, 0.3020774722099304, -0.6315211653709412, 0.14692674577236176, -0.023402120918035507, -0.07152833789587021, 0.21429191529750824, 0.14139583706855774, 0.3801787197589874, -0.12238332629203796, -0.8861932754516602, -0.1436617374420166, -0.004063889384269714, -0.02691383846104145, -0.4505559206008911, -0.2998654246330261, -1.6678190231323242, -0.25782403349876404, -1.1425648927688599, 0.36052989959716797, -0.9214872717857361, -0.5795629024505615, -0.05032186210155487, -0.5572205185890198, -0.03593818470835686, 0.2136295586824417, -0.3539491295814514, -0.5756508111953735, -0.5251151323318481, -0.5293758511543274, 0.7201669216156006, 0.6712673902511597, -0.6906272172927856, 0.3737718462944031, -0.05015032738447189, 0.11907636374235153, 0.0572260320186615, 0.4587344527244568, -0.2191583663225174, -1.2830778360366821, -1.267104148864746, 0.5038286447525024, 0.11870207637548447, -0.07710972428321838, -0.7204849720001221, 0.5497836470603943, 0.3197770118713379, -0.17019639909267426, 0.05938928946852684, 0.5000932812690735, -1.0164422988891602, -0.1888742744922638, 0.18747609853744507, -1.1119061708450317, 0.3041526973247528, -0.10086045414209366, -0.498531311750412, -0.0385209396481514, 0.5366237759590149, 0.201496422290802, -1.2602214813232422, -0.8036788105964661, 0.39966315031051636, -0.47674039006233215, 0.11663448810577393, -0.15082724392414093, -0.3617267608642578, -1.0637434720993042, -0.35185396671295166, -0.007930919528007507, 0.18689391016960144, 0.012908420525491238, 1.1837488412857056, 0.5720476508140564, -1.2310247421264648, 0.1324048936367035, 0.16132526099681854, 0.1011742353439331, -0.1628025621175766, 0.44770845770835876, 0.26655861735343933, -0.058528702706098557, 0.27750545740127563, 0.385241836309433, 0.10183483362197876, -1.1502002477645874, 0.043743714690208435, 0.9722636342048645, -0.2795468866825104, -0.23367354273796082, 1.2613084316253662, -0.4976363778114319, -0.9258785247802734, 0.11746349930763245, -1.3613451719284058, -0.5359230637550354, -0.4209067225456238, 0.7579198479652405, 0.1863112449645996, -0.3117474615573883, -0.10153431445360184, -0.5033146739006042, 0.11324214190244675, -0.17984482645988464, -0.7669845223426819, 0.40407466888427734, 0.030230047181248665, -0.3632073700428009, 1.0692042112350464, 0.7987074851989746, -0.6629157066345215, -0.8673465251922607, -0.6850484013557434, -0.266250342130661, -0.4928765892982483, 0.1311076432466507, -0.3689408004283905, -0.06121605634689331, 0.9633749127388, 0.4176720380783081, 0.10639141499996185, 0.08398541063070297, -0.16214407980442047, 0.1997389942407608, 0.4753980040550232, 0.3996032476425171, -0.43630605936050415, -0.28882232308387756, 1.4237531423568726, 1.2391996383666992, -0.5886456966400146, -0.08674809336662292, -0.02465098910033703, -0.557880163192749, 0.5977036356925964, 0.23473820090293884, 0.13988520205020905, 0.6377657055854797, 0.016965104267001152, 0.16117005050182343, 0.5997042059898376, -1.149641752243042, 0.08319570869207382, 0.7772325873374939, 0.9019659757614136, 0.9933772683143616, 0.02655147761106491, 0.18408066034317017, 0.7525050640106201, 0.18461394309997559, 0.2447790801525116, 0.3875960111618042, 0.5007437467575073, -0.3223944902420044, -0.3667994737625122, 0.49651849269866943, 0.3777253329753876, -0.7897354364395142, -0.7455713152885437, 0.18793895840644836, 0.23315559327602386, 0.3653925657272339, 0.6125118732452393, 0.7936084866523743, 0.027351194992661476, 0.32185229659080505, 0.589002251625061, 0.8143660426139832, -0.5182353854179382, -0.34186676144599915, -0.48120418190956116, -0.5662723183631897, 0.11537449061870575, -0.14880149066448212, -0.4437001943588257, -0.4059094488620758, -0.38191530108451843, 0.24899138510227203, 0.019666293635964394, -0.05521288514137268, 1.2746775150299072, 0.37494903802871704, 0.6027894020080566, -0.34263885021209717, -0.29552483558654785, -0.647637665271759, -0.8994594216346741, 0.5699729919433594, -0.12320990860462189, -0.14608687162399292, -0.1316273957490921, 0.04633145406842232, -0.11783569306135178]}, "authors": [{"authorId": "2257057803", "name": "Shanda Li"}, {"authorId": "2258548742", "name": "Chong You"}, {"authorId": "1947314", "name": "Guru Guruganesh"}, {"authorId": "1643737606", "name": "J. Ainslie"}, {"authorId": "2256997247", "name": "Santiago Ontanon"}, {"authorId": "1771307", "name": "M. Zaheer"}, {"authorId": "144074891", "name": "Sumit K. Sanghai"}, {"authorId": "2257099252", "name": "Yiming Yang"}, {"authorId": "2258094308", "name": "Sanjiv Kumar"}, {"authorId": "1798880", "name": "Srinadh Bhojanapalli"}], "references": [{"paperId": "8f490b938586d8e1b892304dd5209b2295c93ed7", "title": "Transformers Can Achieve Length Generalization But Not Robustly"}, {"paperId": "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b", "title": "What Algorithms can Transformers Learn? A Study in Length Generalization"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "d40dbe668d5b68419e934dfa4c5851ffa1c24aa2", "title": "Exposing Attention Glitches with Flip-Flop Language Modeling"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "08e5d8d1a9d481204b4caff58490755f59b0e88f", "title": "Monotonic Location Attention for Length Generalization"}, {"paperId": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e", "title": "Faith and Fate: Limits of Transformers on Compositionality"}, {"paperId": "af385c0fdd0eda2bbf429bea6fedffc327c8a180", "title": "Randomized Positional Encodings Boost Length Generalization of Transformers"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "e3ec55e9e6720194a0ed5d4033d93a941c8a4f99", "title": "Continual Pre-training of Language Models"}, {"paperId": "4ce987d4f8ae0f4680808c318980d42a82b9aa89", "title": "Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "108c25905be36b2a7a0fc7256ac314985ecd9699", "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a", "title": "Your Transformer May Not be as Powerful as You Expect"}, {"paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b", "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "dd3367e8c6c98d0630eda10e1d0c7ced99a90969", "title": "G-Mixup: Graph Data Augmentation for Graph Classification"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "63d70dba02c34e465f36fd8b123390efe7aa67e0", "title": "Can Vision Transformers Perform Convolution?"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "08ffdec40291a2ccb5f8a6cc048b01247fb34b96", "title": "Relative Positional Encoding for Transformers with Linear Complexity"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "db46b0de44c5113c47f0ec5392eb91d0726497bf", "title": "A Simple and Effective Positional Encoding for Transformers"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "9b539d413393047b28bb7be9b195f142aaf7a80e", "title": "Recipes for Building an Open-Domain Chatbot"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d", "title": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "d715b4a9282562b9d84fb66e04ee70e66b12e86d", "title": "Location Attention for Extrapolation to Longer Sequences"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "388e2fcdcefbe0834e153ab2a0be127092f9674d", "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "928f9dccb806a3278d20d82cc53781c5f44e2bb1", "title": "Constituency Parsing with a Self-Attentive Encoder"}, {"paperId": "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "915c4bb289b3642489e904c65a47fa56efb60658", "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"}, {"paperId": "8388f1be26329fa45e5807e968a641ce170ea078", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"paperId": "66e9dc728b5041271bff0cd6ac0d7eadcd88442f", "title": "Image Super-Resolution Using Deep Convolutional Networks"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": null, "title": "QMSum, SummScreenFD, and GovReport"}, {"paperId": null, "title": "Our work is based on a unified formulation of existing additive relative positional encoding approaches, and proposes new RPE variant aimed at improving length generalization"}, {"paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf", "title": "On Position Embeddings in BERT"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61", "title": "Do Transformers Really Perform Badly for Graph Representation?"}, {"paperId": null, "title": "2021), the representational power of attention modules and Transformers (Cordonnier"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "There are also many works that study positional encoding from different perspectives, including the disentanglement of positional and content information"}, {"paperId": null, "title": "32768} and use unigram overlap (F1) as the evaluation metric. Length generalization. Many existing works show the length generalization failure of standard Transformer models"}, {"paperId": null, "title": "2018) are the first to propose Relative Positional Encoding (RPE) for Transformers, and many follow-up"}, {"paperId": null, "title": "Positional encoding is a critical component of Transformers. Vaswani et al. (2017) propose sinusoidal Absolute Positional Encoding (APE) to encode positional information in the sequential input"}, {"paperId": null, "title": "Finetuning on long text benchmark"}, {"paperId": null, "title": "Complex reasoning in natural language"}]}