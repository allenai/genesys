{"paperId": "b6346f9fa093b8e85df712485a2b851b9f680dac", "abstract": "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset.", "venue": "arXiv.org", "year": 2023, "citationCount": 69, "influentialCitationCount": 13, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.12307", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "LongLoRA is presented, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost, and is compatible with most existing techniques, like Flash-Attention2."}, "embedding": {"model": "specter_v2", "vector": [0.22149591147899628, 0.134146049618721, -0.4589715003967285, -0.2337384968996048, -0.2396220564842224, -0.40666553378105164, 0.7379791736602783, -0.2355136126279831, -0.39150622487068176, -0.17999683320522308, 0.6249173879623413, -0.22793743014335632, 0.9194234013557434, 0.15513262152671814, -0.15838569402694702, 0.10633477568626404, -0.9750272631645203, -0.033201467245817184, 0.08029007911682129, -0.5062688589096069, -0.10327848792076111, -0.6296451091766357, -0.9088897705078125, 0.15339545905590057, 0.3394233286380768, 0.4641670882701874, 0.6015985608100891, 1.0024679899215698, -0.5106348991394043, 0.04155208542943001, 0.36663568019866943, -0.057756755501031876, -0.05478280037641525, 0.03546116128563881, -0.23655863106250763, -0.31630706787109375, 0.288705974817276, -0.4785231947898865, -0.22091414034366608, 0.46093228459358215, -0.08914715051651001, 0.2599342167377472, 0.11107172071933746, -0.5341397523880005, -0.3598939776420593, 0.6872059106826782, 0.48499181866645813, 0.9477996826171875, -0.8523291349411011, -0.38137200474739075, 1.129177212715149, -1.3392590284347534, -0.15023891627788544, 1.177638053894043, 0.31501486897468567, 0.4111710488796234, -0.3272099494934082, -0.7055557370185852, 0.8605983853340149, 0.049895841628313065, -0.7354618906974792, -0.20299944281578064, 0.020264148712158203, 0.06754029542207718, 2.2396419048309326, -0.301093190908432, -0.047009240835905075, 0.9289669394493103, 0.02950797788798809, 1.1784706115722656, -0.0056882137432694435, -0.8604632019996643, -0.33837658166885376, -0.055610448122024536, 0.5938412547111511, 0.3572179675102234, -0.14915843307971954, 0.24323074519634247, -0.5325837731361389, -0.4418034851551056, 0.27403488755226135, -0.17959706485271454, 0.5762660503387451, -0.07787646353244781, -0.2824925184249878, 0.7940587401390076, 0.17091624438762665, 1.1511321067810059, 0.16512401401996613, 0.985212504863739, 0.7755450010299683, 0.059709709137678146, 0.008367417380213737, 0.6550310850143433, -0.5977463126182556, 0.22583803534507751, -0.8532456159591675, 0.2087211161851883, -0.1889864206314087, 0.9818867444992065, -0.06795565038919449, 0.30512088537216187, -0.652230441570282, 0.11646527796983719, 0.888960063457489, 0.5186959505081177, 0.373664915561676, -0.7092392444610596, 0.32548266649246216, -0.7943709492683411, -0.12365250289440155, -0.5409797430038452, -0.27193549275398254, -0.5466979146003723, -0.7943376302719116, -1.4106411933898926, -0.5466154217720032, -0.08208827674388885, -0.14649084210395813, 0.9491336345672607, -0.09275288879871368, 0.3852849304676056, 0.13767774403095245, 0.10222523659467697, 0.4435141682624817, 0.7767865657806396, 0.2397068589925766, 0.08671281486749649, 1.1175405979156494, -1.4745675325393677, -0.5758954882621765, -1.3190884590148926, 1.1298236846923828, -0.29455745220184326, 0.706923246383667, -0.20724864304065704, -0.7401418685913086, -0.8113937973976135, -1.0912706851959229, -0.23374371230602264, -0.8823429942131042, 0.5027503967285156, 0.905368447303772, 0.49490055441856384, -0.8674449324607849, 0.39522379636764526, -0.371425062417984, -0.1638319045305252, 0.1286488026380539, 0.04301340505480766, 0.48058101534843445, -0.16333556175231934, -1.7015912532806396, 0.13196519017219543, 0.2996799945831299, -0.53818678855896, -0.1815897524356842, -0.486223429441452, -1.0086716413497925, -0.014125817455351353, 0.532988965511322, -0.20970147848129272, 1.293161153793335, -0.039142489433288574, -1.3030047416687012, 0.5497763752937317, -0.27015164494514465, 0.04788905382156372, -0.1416901797056198, -0.28209349513053894, -0.6389718055725098, -0.75201416015625, -0.5348732471466064, 0.47872835397720337, 0.5225058197975159, 0.14355036616325378, -0.15474151074886322, 0.2625381648540497, 0.08405054360628128, -0.10223843157291412, -0.2831965386867523, 0.9222217798233032, -0.6147144436836243, -0.34899434447288513, -0.1624188870191574, 0.2998344898223877, -0.13560545444488525, -0.53974848985672, -0.20012003183364868, -1.0783073902130127, 1.161250352859497, 0.22853372991085052, 1.507334589958191, -0.9953778982162476, -0.9145034551620483, -0.13220877945423126, -0.279577374458313, 0.2714659869670868, -1.0559802055358887, 0.34991246461868286, -0.240406796336174, 0.2636115849018097, -0.007696307264268398, -1.2633858919143677, 0.12136896699666977, -0.11260999739170074, -0.7690873742103577, -0.4634073078632355, 0.12219762802124023, 1.1150429248809814, -1.0234630107879639, 0.15735936164855957, -0.0946054607629776, 0.5345125198364258, -1.3627805709838867, 1.1604559421539307, -0.8337722420692444, 0.21007411181926727, -0.22068895399570465, -0.22841687500476837, -0.1576767861843109, -0.6180801391601562, 0.4835462272167206, -0.10337791591882706, 0.05323462560772896, 0.5724787712097168, -0.27812325954437256, 1.482861876487732, -0.920254111289978, 0.6009859442710876, 0.06334178149700165, -0.3164043724536896, -0.013379134237766266, -0.05683039873838425, -0.3614671230316162, -0.322903037071228, 0.25078389048576355, 0.42523106932640076, -0.9172890186309814, 0.12152377516031265, 1.0498191118240356, 1.2468949556350708, -0.549452006816864, 0.21660496294498444, 0.21682621538639069, -0.3527836799621582, 0.3330416679382324, 0.4986416697502136, 0.5252118706703186, 0.5856066942214966, 0.506645679473877, -0.2646257281303406, 0.44403529167175293, -1.0031441450119019, -0.3820763826370239, 0.4062515199184418, 0.520123302936554, 0.5395214557647705, 0.6497559547424316, -0.5052107572555542, -0.5759373307228088, 0.4273502826690674, 0.5715656280517578, 1.9595454931259155, -0.40435174107551575, -0.2396828830242157, -0.9771238565444946, -0.4041508436203003, -0.42641982436180115, 0.26247158646583557, -0.501142144203186, 0.18611234426498413, -0.916182279586792, -1.0886497497558594, 0.8404191136360168, 0.18701134622097015, 0.7675872445106506, -0.5350244641304016, -0.41650545597076416, -0.2714136838912964, 0.08909361809492111, -1.1675587892532349, -1.0916205644607544, 0.5533822774887085, -0.5864711403846741, 0.07228810340166092, 0.3385079801082611, -0.1562119722366333, -0.07436025887727737, -0.5749003291130066, 1.0731096267700195, -0.4255761504173279, -0.040499139577150345, 0.18380206823349, 0.4720706641674042, -0.3177874684333801, -0.6690609455108643, 0.7840610146522522, 0.299775630235672, -0.15710055828094482, 0.49256566166877747, 0.45300185680389404, 0.1983993649482727, -0.022476574406027794, -0.4126766622066498, 0.024907013401389122, 0.22737611830234528, 0.194444477558136, 1.016861081123352, -0.455378919839859, 0.2761448621749878, -1.5051803588867188, 0.5679436326026917, 0.07978000491857529, -0.6969505548477173, 0.18511171638965607, -0.7290089726448059, -0.15874865651130676, 0.6454983353614807, -0.8514580726623535, -0.2700453996658325, -0.8836826682090759, 0.24931031465530396, -0.2140895277261734, -0.22265489399433136, -0.020175281912088394, 0.03858766704797745, 0.18738707900047302, 0.2972981929779053, 0.4431147277355194, 0.0024546755012124777, -0.04163209721446037, 0.8682630658149719, -0.6462276577949524, 0.5060163736343384, 0.1264541894197464, 0.04708271846175194, -0.34610050916671753, -0.06884095817804337, -0.9030940532684326, -0.561750590801239, -0.5213319659233093, -0.5084099173545837, 0.21146553754806519, 0.35156601667404175, -0.7786685228347778, -0.5539272427558899, 0.018677227199077606, -0.9420960545539856, -0.6531374454498291, 0.3423483967781067, -0.37609007954597473, -0.15337014198303223, -0.8742020130157471, -1.0491594076156616, -0.22228018939495087, -0.902327835559845, -1.2469252347946167, 0.5485401749610901, 0.025718772783875465, -0.3651619553565979, -0.860226571559906, -0.0500737726688385, -0.49588343501091003, 1.2706509828567505, -0.5350999236106873, 0.7961129546165466, -0.06307362020015717, -0.31324610114097595, -0.1645718365907669, 0.11569225788116455, 0.5139015913009644, -0.3952949345111847, 0.2377082258462906, -1.2184735536575317, 0.028354573994874954, -0.3547562062740326, -0.3774901032447815, 0.08251382410526276, 0.19255517423152924, 0.8776480555534363, -0.035248152911663055, -0.5746563673019409, 0.6976531744003296, 1.3878722190856934, -0.844179093837738, 0.0259605310857296, 0.209431454539299, 1.0410640239715576, -0.14426186680793762, -0.15357746183872223, 0.37818366289138794, 0.2878986597061157, 0.2752220928668976, 0.2164953351020813, 0.28309354186058044, -0.06531284749507904, -0.3874284327030182, 0.5642573237419128, 1.6952940225601196, 0.7243956327438354, 0.1468040943145752, -0.8658118844032288, 0.7578137516975403, -0.6946520209312439, -0.4858933389186859, 0.672329306602478, 0.7532030344009399, 0.6671077609062195, -0.36730045080184937, -0.5039820671081543, -0.49959835410118103, 0.31122907996177673, 0.6763816475868225, -0.6003692150115967, -1.068457007408142, -0.19160117208957672, 0.08692482858896255, -0.11750245094299316, 0.7861412763595581, -0.6648489832878113, 0.7268597483634949, 14.570572853088379, 1.1466715335845947, -0.08292941004037857, 0.7241294980049133, 0.931397020816803, -0.14485123753547668, -0.4562883973121643, -0.16103090345859528, -1.5714958906173706, -0.0192249845713377, 1.4633252620697021, 0.6314982175827026, 0.7500284910202026, 0.3305026888847351, 0.0451023131608963, 0.11107829958200455, -0.5302720069885254, 0.6941644549369812, 0.5833299160003662, -1.333127498626709, 0.14524134993553162, 0.03534138202667236, 0.8222602009773254, 0.7435763478279114, 0.8985015153884888, 1.1905524730682373, 0.3379954695701599, -0.10166770964860916, 0.14689424633979797, 0.0158868208527565, 1.1259735822677612, -0.16610243916511536, 0.21914295852184296, 0.3846445083618164, -0.856636106967926, -0.32544440031051636, -0.5021892189979553, -1.1234040260314941, -0.12142841517925262, 0.10283020883798599, -0.31570184230804443, -0.8050186634063721, -0.23365598917007446, 0.5720205307006836, -0.3015430271625519, 0.03605533018708229, -0.07455741614103317, 0.717034637928009, -0.21544072031974792, -0.4136556386947632, 0.783050537109375, 0.3885175287723541, 0.3580622673034668, 0.2698498070240021, -0.02309442311525345, 0.3057366609573364, 0.06074562296271324, 0.5330759882926941, -0.3549632430076599, 0.022877585142850876, -0.28844043612480164, -0.25128594040870667, 0.17905552685260773, 0.9476987719535828, 0.7556638717651367, 0.17013044655323029, -0.5308784246444702, 0.22365711629390717, 0.6232036352157593, 0.22088605165481567, -0.48716798424720764, -0.055204447358846664, 0.3934561312198639, -0.5537266731262207, 0.03453816473484039, 0.3576210141181946, -0.2811461389064789, -0.6693428158760071, -0.649152934551239, -0.487689346075058, 0.1242406964302063, -1.021182894706726, -0.385617733001709, 0.9070589542388916, -0.19998767971992493, -0.2003215253353119, 0.06352680176496506, -0.729941189289093, -0.3070768713951111, 0.6076787710189819, -1.3457047939300537, -0.9492988586425781, 0.5847042202949524, -0.6268784999847412, -0.3564557135105133, -0.11106830090284348, 1.445175051689148, 0.05696865916252136, -0.5622822642326355, 0.36091431975364685, 0.5890400409698486, -0.1514347791671753, -0.027994602918624878, -0.6513449549674988, 1.1460498571395874, 0.47830045223236084, -0.36737388372421265, 0.2274569869041443, -0.001219509751535952, 0.27555906772613525, -0.7581874132156372, -0.44565442204475403, 0.7772489190101624, -1.2392081022262573, 0.07588513195514679, -1.044471263885498, -0.6718525290489197, 0.4991673231124878, 0.6979984641075134, -0.05857648700475693, 0.329192191362381, 0.22368794679641724, -0.8511959314346313, -0.23379483819007874, -0.4751461148262024, 0.12050598114728928, 0.4232291877269745, -0.9544146656990051, -0.13524232804775238, -0.23210392892360687, 0.7814047932624817, -1.1279985904693604, -0.5959914922714233, -0.32132789492607117, 0.14614275097846985, 0.03153098374605179, 0.9433632493019104, -0.20874780416488647, 0.7313641905784607, 1.0942047834396362, -0.5370631217956543, -0.6959775686264038, 0.09406743943691254, -1.0399770736694336, -0.41632843017578125, 0.240144744515419, 0.6193807125091553, -0.35329580307006836, 0.12338069826364517, 0.502325713634491, 0.43561914563179016, -0.9902423024177551, -0.5585796236991882, -0.3374292254447937, 0.5693319439888, -0.6465190649032593, 0.6476449370384216, 0.15676265954971313, -0.2521834671497345, 0.3813321888446808, 0.36910057067871094, 0.8445321321487427, -0.2612106502056122, -0.5380632877349854, 0.5150098204612732, 0.379749059677124, -0.4117043614387512, -0.5645117163658142, -0.3416435718536377, -1.5057346820831299, 0.024710360914468765, -0.9746400117874146, -0.32063713669776917, -0.1777334213256836, -0.2615585923194885, -0.19633959233760834, -0.15600411593914032, 0.05032733455300331, 0.048104967921972275, -0.35062599182128906, -0.4430125951766968, -0.8584184646606445, -0.6174948811531067, 0.7313339710235596, 0.6747952699661255, -0.5929847359657288, 0.07163060456514359, -0.028018131852149963, 0.19054178893566132, 0.25066593289375305, 0.512958288192749, -0.11155984550714493, -0.6543799638748169, -1.4963178634643555, 0.5458614230155945, -0.4510287344455719, -0.29182761907577515, -0.5835665464401245, 0.2624889016151428, 0.1305844932794571, -0.29125431180000305, -0.17935442924499512, 0.13908439874649048, -0.9240407943725586, -0.8011431097984314, 0.1751955896615982, -0.743965744972229, 0.3167376220226288, 0.651604413986206, -0.3975619077682495, 0.027598394080996513, 0.6664941310882568, -0.07773396372795105, -0.7360310554504395, -1.0523841381072998, 0.34223315119743347, -0.5579900741577148, 0.3709254264831543, -0.39854902029037476, 0.29868900775909424, -1.1853258609771729, -0.2757619023323059, -0.06745999306440353, 0.5422622561454773, -0.34620988368988037, 1.121002435684204, -0.13583189249038696, -1.1404156684875488, 0.16935981810092926, 0.49318021535873413, 0.06791727244853973, 0.18789179623126984, 0.5924215912818909, 0.40470394492149353, -0.3830121159553528, 0.958372950553894, 0.5175603628158569, 0.1608661562204361, -0.9330492615699768, 0.3749438226222992, 0.7487767338752747, -0.794697105884552, 0.12862351536750793, 1.1004782915115356, 0.002425879007205367, -1.3028793334960938, 0.1310938447713852, -1.6858488321304321, -0.6055389046669006, -0.48479947447776794, 0.939311683177948, -0.026773810386657715, -0.0882267951965332, -0.05872238799929619, -0.45913922786712646, 0.25992754101753235, 0.08314339816570282, -0.14509445428848267, 0.49476248025894165, -0.5636588931083679, -0.5603575110435486, 0.7099261283874512, 1.2852612733840942, -0.35180574655532837, -0.9430876970291138, -0.8763993382453918, -0.5055295825004578, 0.26805055141448975, 0.5013197064399719, -0.6282447576522827, -0.4108092784881592, 0.837740957736969, 0.5388038754463196, 0.17718958854675293, 0.25792285799980164, -0.310104638338089, 0.3222564160823822, 1.1761853694915771, 0.16570541262626648, -0.6384119987487793, -0.45587795972824097, 1.645871639251709, 1.1469138860702515, -1.0916612148284912, 0.154331773519516, 0.0513310432434082, -0.5147396922111511, 0.5399540662765503, 0.4852496385574341, 0.20367752015590668, 1.0249779224395752, -0.08013513684272766, 0.021960971876978874, 0.5107175707817078, -1.2950156927108765, 0.03417173773050308, 0.7225083708763123, 0.7054017782211304, 0.5496586561203003, 0.4238434433937073, 0.39950475096702576, 1.0479907989501953, 0.08711391687393188, -0.3923377990722656, 0.6851886510848999, 0.4118334650993347, -0.11887037009000778, 0.03723569214344025, -0.11581113189458847, 0.5230799913406372, -0.8606781363487244, -0.9574010372161865, 0.3378705680370331, 0.40700703859329224, 0.31319692730903625, 0.4715718626976013, 0.9300412535667419, 0.3010849356651306, 0.2355525642633438, 0.2854464650154114, 0.5586112141609192, -0.5473865270614624, -0.19580551981925964, -0.12477688491344452, -0.6979670524597168, -0.294960618019104, 0.17506209015846252, -0.4639940857887268, -0.24805063009262085, -0.16173461079597473, 0.18506935238838196, -0.20493142306804657, 0.46206793189048767, 1.1322704553604126, 0.666767954826355, 0.5978077054023743, -0.1432098001241684, -0.5459244251251221, -0.48280760645866394, -1.0110665559768677, 0.004095675423741341, -0.6975601315498352, -0.07035014033317566, 0.18939946591854095, 0.09809455275535583, -0.3174685537815094]}, "authors": [{"authorId": "2109297557", "name": "Yukang Chen"}, {"authorId": "152230789", "name": "Shengju Qian"}, {"authorId": "150127950", "name": "Haotian Tang"}, {"authorId": "2237802684", "name": "Xin Lai"}, {"authorId": "47781592", "name": "Zhijian Liu"}, {"authorId": "2243400730", "name": "Song Han"}, {"authorId": "2237811040", "name": "Jiaya Jia"}], "references": [{"paperId": "73290ecbec2f38d1d647ddef1ada69cee41725b3", "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "cff30a18e851f55030c9c39dcd87cabc1708e507", "title": "Neural Kaleidoscopic Space Sculpting"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7", "title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "d9cdf21e73519edc593bdf1a00fcd778764b13f6", "title": "Training Neural Networks with Fixed Sparse Masks"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "e13a762cf10bf251b80dbca1a97dd57cc59da5ee", "title": "GeoNet++: Iterative Geometric Neural Network with Edge-Aware Refinement for Joint Depth and Surface Normal Estimation"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "4fcdf1e95d91e5f369405a15002869c997c68e0b", "title": "RENAS: Reinforced Evolutionary Neural Architecture Search"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4ebf5a9c35f40d3343a2376d0b75d8d7a3126d8d", "title": "3D Graph Neural Networks for RGBD Semantic Segmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "7b36c5602930abf08efd2867f92cdb48a1be757a", "title": "Together"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": "2ae108f21571ad13bd7dc4a43ae263025dce4879", "title": "SafeConv: Explaining and Correcting Conversational Unsafe Behavior"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms"}, {"paperId": null, "title": "Redpajama: An open source recipe to reproduce llama training dataset"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Ntk-aware scaled rope"}, {"paperId": null, "title": "Proof-pile"}, {"paperId": null, "title": "Peft: Stateof-the-art parameter-efficient fine-tuning methods"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length?"}, {"paperId": null, "title": "Dumbledore is the headmaster of Hogwarts and a mentor figure to Harry. He trusts and believes in Harry even when others doubt him"}, {"paperId": null, "title": "They are also good friends. Hermione is often bossy and knowledgeable, but she also helps Harry and Ron out of difficult situations with her intelligence"}, {"paperId": null, "title": "They are close friends. Harry and Ron share a dorm room and have many adventures together at Hogwarts"}, {"paperId": null, "title": "Lockhart enjoys the fame and attention that comes from being connected to Harry. However, Harry finds Lockhart vain and insincere"}, {"paperId": null, "title": "Professor Quirrell holds the position when Harry arrives at Hogwarts. Snape's resentment toward not getting that job may cause him to take out his frustrations on Harry"}, {"paperId": null, "title": "They have an antagonistic relationship. Snape seems to dislike Harry from the start and favors the Slytherin students over Harry and his friends"}, {"paperId": null, "title": "They are enemies. Draco is part of the Slytherin house and is always trying to cause trouble for Harry and his friends"}]}