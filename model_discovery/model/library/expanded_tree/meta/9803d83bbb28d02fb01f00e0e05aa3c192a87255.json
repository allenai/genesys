{"paperId": "9803d83bbb28d02fb01f00e0e05aa3c192a87255", "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.", "venue": "", "year": 2024, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing, is introduced, which effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy."}, "embedding": {"model": "specter_v2", "vector": [0.34450361132621765, 0.5458726286888123, -0.628322958946228, -0.16673098504543304, -0.37067103385925293, -0.40964198112487793, 0.632503092288971, 0.11356939375400543, -0.5612685680389404, -0.23295162618160248, 0.44021400809288025, -0.3508703112602234, 0.7078014612197876, 0.3184465765953064, -0.18645645678043365, 0.21948778629302979, -0.7379081845283508, 0.2829032242298126, -0.3042309880256653, 0.005946512334048748, 0.22426436841487885, -0.8345243334770203, -1.0504902601242065, 0.2624228298664093, 0.4066033959388733, 0.5261406898498535, 0.47284531593322754, 0.9220231175422668, -0.8345787525177002, 0.5382904410362244, 0.4794536530971527, -0.23787632584571838, 0.213913232088089, 0.10365890711545944, -0.475646048784256, -0.38132527470588684, 0.38221946358680725, -0.4446953535079956, -0.36441388726234436, 0.6910335421562195, -0.18853650987148285, 0.5162537693977356, 0.26142653822898865, -0.5971991419792175, -0.12817108631134033, 0.978978157043457, 0.3312452733516693, 0.718116044998169, -0.12782703340053558, -0.3967149257659912, 1.397476077079773, -1.5932987928390503, 0.20375417172908783, 1.2360877990722656, 0.5684066414833069, 0.182932510972023, -0.016522832214832306, -0.43357735872268677, 0.9891176223754883, 0.30583301186561584, -0.7650142312049866, -0.7360708117485046, -0.30349841713905334, -0.07027334719896317, 2.403885841369629, -0.06518951803445816, 0.14159688353538513, 0.5557276606559753, -0.35460811853408813, 1.5313148498535156, -0.41011926531791687, -0.9530028700828552, -0.1475861519575119, -0.27752745151519775, 0.7077612280845642, 0.6306744813919067, -0.3820243775844574, 0.35165050625801086, -0.735339879989624, -0.3006160855293274, 0.38457465171813965, -0.04264582321047783, 0.5966547727584839, 0.29587841033935547, -0.25522249937057495, 0.6682436466217041, 0.009115345776081085, 0.8657494187355042, -0.18816930055618286, 0.6389012336730957, 0.6164759397506714, -0.12873810529708862, 0.05826491862535477, 0.32158005237579346, -0.11666516959667206, -0.29852646589279175, -1.2451422214508057, 0.23213833570480347, 0.1627884805202484, 0.9765890836715698, -0.3775360584259033, 0.26906952261924744, -0.9975558519363403, 0.27311939001083374, 1.187576413154602, 0.2057790458202362, 0.3942750096321106, -0.8074524402618408, 0.22417639195919037, -0.827961266040802, 0.057851988822221756, -0.5746256113052368, -0.1562722772359848, -0.13742859661579132, -0.5982905030250549, -1.3275150060653687, -0.316459059715271, 0.040479641407728195, -0.5011935234069824, 0.7400742173194885, -0.11811947077512741, 0.5562111139297485, 0.05984444543719292, 0.485900342464447, 0.8368596434593201, 0.9717810153961182, 0.2363446205854416, -0.03972024470567703, 1.0067862272262573, -1.3136810064315796, -0.3234435021877289, -1.3445674180984497, 0.8726780414581299, -0.47961777448654175, 0.17767390608787537, -0.27191293239593506, -1.3011723756790161, -1.0148260593414307, -0.6104244589805603, -0.2748057246208191, -0.5202658772468567, 0.30952033400535583, 1.012527585029602, 0.0490792877972126, -0.7607179880142212, 0.4481847882270813, -0.20280930399894714, -0.1489398330450058, 0.18767887353897095, -0.1579425036907196, 0.3728725016117096, -0.348918080329895, -1.5177221298217773, 0.2958186864852905, 0.15635085105895996, -0.33259469270706177, -0.06848647445440292, -0.6140801310539246, -1.340868592262268, 0.041038792580366135, 0.4048425257205963, -0.012893776409327984, 1.4479130506515503, 0.07482988387346268, -1.1408828496932983, 0.9024218916893005, -0.6218454241752625, 0.10224281251430511, 0.037555795162916183, -0.19480088353157043, -0.4860624670982361, -0.7179109454154968, -0.17805776000022888, 0.7244585752487183, 0.34012526273727417, 0.15648800134658813, -0.2937946915626526, 0.2742502689361572, -0.1895953118801117, 0.01725904643535614, 0.0643206238746643, 1.0220001935958862, -0.4669155776500702, -0.1019815132021904, 0.17596280574798584, 0.6358234882354736, -0.16155675053596497, -0.4092487692832947, -0.48475468158721924, -1.1694259643554688, 0.627065896987915, 0.18954183161258698, 1.2006428241729736, -0.8276399970054626, -1.0141417980194092, -0.17957249283790588, -0.24444933235645294, 0.29799214005470276, -0.9537699222564697, 0.5398404598236084, -0.3111591935157776, 0.16685751080513, -0.07098895311355591, -1.0688151121139526, 0.18405084311962128, -0.030815210193395615, -0.9030138850212097, -0.3526316285133362, -0.013137752190232277, 1.0481491088867188, -1.157494306564331, -0.11944467574357986, 0.05625137314200401, 0.16146588325500488, -1.2679985761642456, 1.2839149236679077, -0.6448368430137634, 0.0356513187289238, 0.04441416263580322, -0.11582466214895248, -0.019101202487945557, -0.43602660298347473, 0.44601404666900635, -0.25555962324142456, -0.39790719747543335, 0.5345662832260132, -0.4583961069583893, 1.5038268566131592, -0.4376794993877411, 0.23789429664611816, -0.04828460142016411, -0.5535425543785095, 0.08179303258657455, 0.16599805653095245, -0.42027774453163147, -0.4298098683357239, 0.018813634291291237, 0.6093600988388062, -0.562312662601471, 0.03237464278936386, 1.2373297214508057, 1.0925414562225342, -0.39453473687171936, 0.24509862065315247, 0.20442739129066467, 0.10458186268806458, 0.6046000719070435, 0.6266902089118958, 0.5424690246582031, 0.48196345567703247, 0.5545502305030823, 0.03171340376138687, 0.4054723381996155, -0.7330518364906311, -0.04894998297095299, 0.875209391117096, 0.9205737113952637, 0.7238072156906128, 0.3524187505245209, -0.9339182376861572, -0.4633931517601013, 0.7076396346092224, 0.6337018609046936, 1.6797723770141602, -0.035098567605018616, -0.14109569787979126, -0.6619554758071899, -0.3032119572162628, -0.017342807725071907, 0.22077207267284393, -0.003381825052201748, 0.12690334022045135, -0.6742731332778931, -0.8711758255958557, 0.8164491057395935, 0.23046737909317017, 0.5365172028541565, -1.0269525051116943, -0.3727359473705292, -0.09545449912548065, 0.031152067705988884, -1.1184139251708984, -0.7473094463348389, 0.542371392250061, -0.27704906463623047, -0.03604401275515556, 0.3548566997051239, -0.1731678992509842, 0.3150343596935272, -0.6986708641052246, 1.0364816188812256, -0.17695069313049316, -0.44356074929237366, -0.06028188392519951, 0.473273366689682, -0.10306224972009659, -0.6311567425727844, 0.49203068017959595, 0.15808814764022827, -0.36267393827438354, 0.6230185031890869, 0.473721444606781, 0.1248740553855896, -0.5571579337120056, -0.11971054971218109, 0.4758507311344147, 0.123465396463871, -0.03163537755608559, 0.8418880701065063, -0.7998648881912231, 0.09285719692707062, -1.1796748638153076, 0.5769160985946655, -0.16232138872146606, -0.36356085538864136, 0.10401111841201782, -0.6969913244247437, -0.38564735651016235, 0.609388530254364, -0.6556863784790039, -0.1987333744764328, -0.73734050989151, -0.17339617013931274, -0.28043797612190247, -0.4865482449531555, 0.1396234780550003, 0.33657798171043396, 0.4198496341705322, 0.04257480055093765, 0.6789060831069946, 0.19530652463436127, -0.40756669640541077, 0.8396751880645752, -0.43842756748199463, 0.7411971092224121, -0.08234401792287827, -0.19756096601486206, -0.37763604521751404, -0.31431812047958374, -0.8304950594902039, -0.7705502510070801, -0.7321067452430725, -0.2837761342525482, -0.05353928357362747, 0.07193257659673691, -0.3880508542060852, -0.955116868019104, 0.005619069095700979, -1.6776845455169678, -0.46560138463974, 0.07487861812114716, -0.36314260959625244, -0.14629876613616943, -0.7065871357917786, -1.3665753602981567, -0.8060427308082581, -0.8501610159873962, -0.8606375455856323, 0.4929649233818054, -0.017556624487042427, -0.7722052335739136, -0.5409032106399536, 0.18719696998596191, -0.5216601490974426, 0.9783375263214111, -0.8110113739967346, 0.6992590427398682, -0.24937213957309723, -0.1792357712984085, -0.05366688221693039, 0.009355315938591957, 0.1490776091814041, -0.33050352334976196, 0.20923322439193726, -0.8116635680198669, 0.3026770055294037, -0.5701798796653748, -0.3640812337398529, 0.09942692518234253, 0.2947680652141571, 0.7876662015914917, -0.3569815754890442, -0.6179438829421997, 0.5756887197494507, 1.2417546510696411, -0.4586225748062134, 0.18145276606082916, -0.10644938796758652, 1.023622989654541, -0.07958984375, 0.1535276174545288, 0.9157174229621887, 0.20021356642246246, 0.36739087104797363, 0.21799926459789276, -0.07237545400857925, 0.08548783510923386, -0.49079129099845886, 0.6279666423797607, 1.8600428104400635, 0.6053266525268555, 0.20666269958019257, -0.8383868336677551, 0.7028418779373169, -1.361422061920166, -0.8681212067604065, 0.42254552245140076, 0.6022588014602661, 0.7989204525947571, -0.5274860858917236, -0.2685527801513672, -0.4757544696331024, 0.32438042759895325, 0.6783163547515869, -0.4404115378856659, -1.0954012870788574, 0.08846809715032578, 0.1398433893918991, -0.282179594039917, 0.8898705244064331, -0.4236382842063904, 0.8746023774147034, 14.676751136779785, 1.2005844116210938, -0.01232712808996439, 0.5187883377075195, 0.9648972749710083, 0.19296568632125854, -0.3598567545413971, -0.13313281536102295, -1.721192717552185, -0.019818419590592384, 1.1482478380203247, 0.3031306266784668, 0.6405474543571472, 0.425132691860199, 0.06509242206811905, -0.0678967535495758, -0.6883940100669861, 0.9914031028747559, 0.4846935570240021, -1.3684314489364624, 0.20312224328517914, -0.13355080783367157, 0.31038999557495117, 0.5138936042785645, 0.7028746008872986, 0.9113894104957581, 0.4897519648075104, -0.5868400931358337, 0.7624428868293762, 0.27239325642585754, 1.2652713060379028, 0.04604044929146767, 0.3940295875072479, 0.6941357254981995, -0.9120994210243225, 0.04407137632369995, -0.7598749995231628, -1.4865802526474, 0.21697181463241577, 0.3385174572467804, -0.629249632358551, -0.6598128080368042, -0.6062256097793579, 0.7886610627174377, 0.35257846117019653, 0.3382609486579895, 0.035280998796224594, 0.5747160315513611, -0.13712377846240997, 0.025860633701086044, 0.6227381229400635, 0.34992876648902893, 0.2516118586063385, 0.3671710789203644, 0.061811305582523346, 0.19967174530029297, 0.3259461522102356, 0.6409578919410706, -0.27695125341415405, -0.002153558423742652, -0.29196658730506897, -0.2557111978530884, 0.2524862289428711, 1.1187893152236938, 0.49836426973342896, 0.010555922985076904, -0.4370824694633484, 0.23006930947303772, 0.7858254909515381, -0.07454358041286469, -0.3137866258621216, 0.1892808973789215, 0.4063483476638794, -0.7227296233177185, 0.07141316682100296, 0.6080989837646484, -0.2538357675075531, -0.5079234838485718, -0.8330181837081909, -0.7320951223373413, 0.4035135805606842, -0.5611873269081116, -0.708408534526825, 0.6968205571174622, -0.4072267711162567, -0.1466069519519806, -0.18759921193122864, -0.4987374246120453, -0.35215145349502563, 0.4197610020637512, -1.248878002166748, -0.6001454591751099, 0.45170021057128906, -0.6017109751701355, -0.10902529954910278, 0.21040695905685425, 1.4345099925994873, 0.24769531190395355, -0.38067615032196045, 0.11304773390293121, 0.1976868361234665, -0.08038704842329025, -0.4213445484638214, -0.5459199547767639, 1.2474154233932495, 0.4814184010028839, -0.014883521944284439, 0.36704927682876587, -0.16455397009849548, 0.022212892770767212, -0.7553024291992188, -0.2233673334121704, 1.038142442703247, -1.1007723808288574, -0.48014649748802185, -0.919111430644989, -0.8265832662582397, 0.37585535645484924, 0.4929099380970001, 0.0802285447716713, 0.309606671333313, 0.18914106488227844, -0.5686996579170227, -0.0633726641535759, -0.41354939341545105, -0.018311219289898872, 0.6781837940216064, -0.6862027049064636, -0.28635141253471375, -0.2538011372089386, 0.34446772933006287, -1.3460921049118042, -0.4045526683330536, -0.4379963278770447, 0.3648705780506134, 0.23203028738498688, 0.9024635553359985, -0.32035931944847107, 0.8271733522415161, 0.9757380485534668, -0.14075127243995667, -0.6214658617973328, 0.09718380868434906, -0.9402636885643005, -0.5125474333763123, 0.09085971117019653, 0.7298052310943604, -0.12200773507356644, 0.19863231480121613, 0.8402223587036133, 0.4032534658908844, -0.9650200009346008, -0.5395752191543579, -0.3593789339065552, 0.0989094004034996, -0.5537390112876892, 0.4950588345527649, 0.055640384554862976, 0.33468183875083923, 0.13013127446174622, 0.23228739202022552, 0.25369125604629517, -0.22775684297084808, -0.29055047035217285, 0.43569451570510864, 0.160283625125885, -0.16212444007396698, -0.4270387291908264, -0.14345699548721313, -1.724697232246399, -0.03164507821202278, -1.2701174020767212, 0.273959755897522, -0.6632772088050842, -0.15304072201251984, -0.4433198571205139, -0.21704378724098206, 0.4783557057380676, 0.11426835507154465, -0.49676018953323364, -0.4251561760902405, -0.7442893385887146, -0.9263170957565308, 0.6188535094261169, 0.53315669298172, -0.5001947283744812, 0.26105189323425293, -0.24736598134040833, 0.1048012226819992, 0.18163684010505676, 0.27623361349105835, -0.4318123161792755, -0.43916016817092896, -1.2840856313705444, 0.445446252822876, -0.017254043370485306, -0.27381327748298645, -0.41654476523399353, 0.7229068279266357, 0.15886461734771729, -0.33870309591293335, -0.15897415578365326, 0.00984079297631979, -0.6626683473587036, -0.5754209756851196, 0.3584265112876892, -0.8008282780647278, 0.13924704492092133, 0.2984040081501007, -0.9116452932357788, -0.07489204406738281, 0.319426953792572, -0.26548126339912415, -0.9777130484580994, -1.117997407913208, 0.47804492712020874, -0.46972566843032837, 0.5107505917549133, -0.5601193904876709, 0.10712076723575592, -1.1205384731292725, -0.22261527180671692, 0.13420554995536804, 0.28971168398857117, -0.3355097472667694, 1.0993096828460693, 0.2985134720802307, -0.9975203275680542, 0.10791268944740295, 0.3718792796134949, -0.21361500024795532, 0.21484607458114624, 0.5978203415870667, 0.40549391508102417, -0.20361435413360596, 0.5819787383079529, 0.44786536693573, 0.16113635897636414, -0.9918867349624634, 0.19804657995700836, 0.5594404935836792, -0.721979558467865, -0.2883310616016388, 1.050208568572998, -0.4376794397830963, -1.026832938194275, 0.17323404550552368, -1.358099102973938, -0.4565888047218323, -0.5071359276771545, 0.8839833736419678, 0.15956541895866394, 0.06038915365934372, -0.22004875540733337, -0.8188102841377258, 0.03516625985503197, -0.16748148202896118, -0.26919087767601013, 0.49954184889793396, -0.04786592349410057, -0.5641294121742249, 0.49420443177223206, 0.977969765663147, -0.36701107025146484, -0.7130764126777649, -0.47982916235923767, -0.31362223625183105, 0.025388456881046295, 0.1381644755601883, -0.4619855284690857, -0.42986252903938293, 0.7735899090766907, -0.04607364162802696, 0.24980419874191284, -0.3795429468154907, -0.21687301993370056, 0.5337982177734375, 0.8484912514686584, 0.038130179047584534, -0.7002555727958679, -0.7651762366294861, 1.6735997200012207, 1.1007144451141357, -1.102543592453003, -0.06998271495103836, -0.002773590385913849, -0.5340770483016968, 0.7045016288757324, 0.5177224278450012, 0.14782777428627014, 0.644580602645874, 0.12043178081512451, 0.04980931803584099, 0.21988323330879211, -1.1541144847869873, -0.1216316819190979, 0.6468453407287598, 0.6872482895851135, 0.8190651535987854, 0.24860413372516632, 0.21404755115509033, 1.1705862283706665, 0.24630743265151978, -0.02888163924217224, 0.19803845882415771, 0.5391971468925476, -0.16167956590652466, -0.09792765974998474, -0.05234382301568985, 0.8814805150032043, -0.7773136496543884, -1.1511191129684448, 0.48097220063209534, 0.17490214109420776, 0.3454812169075012, 0.4821353852748871, 1.1253266334533691, 0.46056702733039856, 0.15792612731456757, 0.3293136656284332, 0.5879178643226624, -0.7303534150123596, -0.2595866322517395, -0.4177984595298767, -0.6350856423377991, -0.3055698275566101, 0.13867656886577606, -0.4357113242149353, -0.6583148241043091, -0.22108888626098633, 0.4373292922973633, 0.1418362557888031, 0.09204699844121933, 1.2640571594238281, 0.7926042675971985, 0.25318124890327454, -0.5750910639762878, -0.5321036577224731, -0.4710487425327301, -0.9143871068954468, -0.004436156712472439, -0.5336735248565674, -0.2305891513824463, 0.13311178982257843, 0.03277822211384773, -0.2520003914833069]}, "authors": [{"authorId": "2181120463", "name": "Huiqiang Jiang"}, {"authorId": "1527099159", "name": "Yucheng Li"}, {"authorId": "2284970741", "name": "Chengruidong Zhang"}, {"authorId": "2108728536", "name": "Qianhui Wu"}, {"authorId": "13289447", "name": "Xufang Luo"}, {"authorId": "2309738728", "name": "Surin Ahn"}, {"authorId": "2281867465", "name": "Zhenhua Han"}, {"authorId": "2309244780", "name": "Amir H. Abdi"}, {"authorId": "2305587638", "name": "Dongsheng Li"}, {"authorId": "2257359863", "name": "Chin-Yew Lin"}, {"authorId": "2125051198", "name": "Yuqing Yang"}, {"authorId": "2160727304", "name": "Lili Qiu"}], "references": [{"paperId": "6c5e09cef64fe7fbeab9a6f3f062363bffba917d", "title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference"}, {"paperId": "c7f9706898bdfa3241601e075b1305649b174ff1", "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"}, {"paperId": "1c7db9fb18246787fbe3de6e0eaa370ae749e795", "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"}, {"paperId": "28eb18717cfa257f0fc49fb9512c48279cafa031", "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling"}, {"paperId": "0ac39ef1352641533b01f07c42da0dcb8cdc81a9", "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"}, {"paperId": "ca9f5b3bf0f54ad97513e6175b30497873670fed", "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality"}, {"paperId": "4f02df5e50a50d593a3336bf9a566c30e6fef00d", "title": "A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models"}, {"paperId": "b1f5087ab3e782f718a1393bed242b4b412e648b", "title": "Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis"}, {"paperId": "1d4c48335d841014d0145256c3c4e7f6c426b8fb", "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models"}, {"paperId": "53a803388e83ae89261624099d7be4287ace67cb", "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"}, {"paperId": "cf188f980d987d70358e414f44505e8427496d08", "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs"}, {"paperId": "1784c987e681d60c634765fe64c8d9c26f73d5ff", "title": "SnapKV: LLM Knows What You are Looking for Before Generation"}, {"paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3", "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"}, {"paperId": "d0deaec3e1f74701ac43600d9e64c5c969be7391", "title": "TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding"}, {"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "d8b51d518f2dd62943762ceaa8961d3b1bfbcc1a", "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?"}, {"paperId": "931c74198f596287245932232bfa95808b3dc1e6", "title": "Attention is Naturally Sparse with Gaussian Distributed Input"}, {"paperId": "2717e5c7384ec12cfd6cf9c34897c6adad3230ed", "title": "Long-context LLMs Struggle with Long In-context Learning"}, {"paperId": "cbaf689fd9ea9bc939510019d90535d6249b3367", "title": "Jamba: A Hybrid Transformer-Mamba Language Model"}, {"paperId": "3d45fc603e34934fc589b9547307815f7723de34", "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression"}, {"paperId": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6", "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference"}, {"paperId": "e9576198e9ee767ede4b1ac6a739267aa52a9832", "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"}, {"paperId": "41b47f33a24feefd6728bdc1339d0d4ff5fec7be", "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"}, {"paperId": "c0b454e0a6aa51ff3ba56778787d0c43932ef6ba", "title": "Yi: Open Foundation Models by 01.AI"}, {"paperId": "f05e84702562cb693dd68d3d1c88072519a7bd71", "title": "\u221eBench: Extending Long Context Evaluation Beyond 100K Tokens"}, {"paperId": "c9603ec967879c24973b5bd48861df2e5555932e", "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"}, {"paperId": "f288e2238ac8725baa7ca9874bbc3fed1e89a632", "title": "Data Engineering for Scaling Language Models to 128K Context"}, {"paperId": "ef1b02dc1b82f9955fc4760fcefd92c0fff9f227", "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference"}, {"paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c", "title": "Transformers are Multi-State RNNs"}, {"paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0", "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"}, {"paperId": "713806165610c237f551a7b68e6b09b3ded75502", "title": "SparQ Attention: Bandwidth-Efficient LLM Inference"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "95240dda409e28acccdc5cf619ad0c036cf4292d", "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"}, {"paperId": "94a5f96308729e31c1ffbc0f0618db87795092fe", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"}, {"paperId": "4c0428917aeee6aa7bd434f337d039f35996b736", "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"}, {"paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a", "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"}, {"paperId": "b12541867632737e826b7b01c7fbe1c4222d8655", "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80", "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9", "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"}, {"paperId": "f81a1b4510631d14b5b565c4701ee056f8d5c72f", "title": "CodePlan: Repository-level Coding using LLMs and Planning"}, {"paperId": "d9d84f8b9e6bf4839f43ab865158d79a3b9bb1e9", "title": "XGen-7B Technical Report"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "a7fc585cc4c2b6822646b2c410e0c427a20798f2", "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "32fa352ee110fd9f80c9d62282611b4a444f5300", "title": "On the Expressive Flexibility of Self-Attention Matrices"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "f3ca1504ab4cc14f491f07e5a8b38d93890551e1", "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "dbc368bc8b49347dd27679894524fa62f88492c9", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "5278a8eb2ba2429d4029745caf4e661080073c81", "title": "Generative Agents: Interactive Simulacra of Human Behavior"}, {"paperId": "32fbc1821ef4a6daee1f9e9f140056ff9cda238a", "title": "PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation"}, {"paperId": "690a37a2ba67b44b012bf9aa92e6a7f7670f487f", "title": "Dynamic Sparse Attention for Scalable Transformer Acceleration"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "148011adfae37b821407aae84fcbbf7fb4619eb6", "title": "On the Expressive Power of Self-Attention Matrices"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "b1d674e82f7579fdf2fb8de1e772ee8610bc2034", "title": "nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training"}, {"paperId": "5b94dd68c7484d828bb3ed4b065fc58ddbd3b202", "title": "Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention"}, {"paperId": null, "title": "Needle in a haystack - pressure testing"}, {"paperId": null, "title": "implementation of the flash attention v2 algorithm. Technical report"}, {"paperId": null, "title": "World model on million-length video and language with ringattention"}, {"paperId": null, "title": "Sequence can secretly tell you what to discard"}, {"paperId": null, "title": "Gradient"}, {"paperId": null, "title": "Llm-powered autonomous agents"}, {"paperId": null, "title": "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory"}]}