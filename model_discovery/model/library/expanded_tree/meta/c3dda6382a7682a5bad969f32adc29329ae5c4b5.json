{"paperId": "c3dda6382a7682a5bad969f32adc29329ae5c4b5", "abstract": "Self-attention-based transformers have outperformed recurrent and convolutional neural networks (RNN/ CNNs) in many applications. Despite the effectiveness, calculating self-attention is prohibitively costly due to quadratic computation and memory requirements. To solve this challenge, this article proposes a hybrid analog-ReRAM and digital-SRAM in-memory computing accelerator (HARDSEA), a computing-in-memory (CIM) accelerator supporting self-attention in transformer applications. To trade off between energy efficiency and algorithm accuracy, HARDSEA features an algorithm-architecture-circuit codesign. A product-quantization-based scheme dynamically facilitates self-attention sparsity by predicting lightweight token relevance. A hybrid in-memory computing architecture employs both high-efficiency analog ReRAM-CIM and high-precision digital SRAM-CIM to implement the proposed new scheme. The ReRAM-CIM, whose precision is sensitive to circuit nonidealities, takes charge of token relevance prediction where only computing monotonicity is demanded. The SRAM-CIM, utilized for exact sparse attention computing, is reorganized as an on-memory-boundary computing scheme, thus adapting to irregular sparsity patterns. In addition, we propose a time-domain winner-take-all (WTA) circuit to replace the expensive ADCs in ReRAM-CIM macros. Experimental results show that HARDSEA prunes BERT and GPT-2 models to 12%\u201333% sparsity without accuracy loss, achieving <inline-formula> <tex-math notation=\"LaTeX\">$13.5\\times $ </tex-math></inline-formula>\u2013<inline-formula> <tex-math notation=\"LaTeX\">$28.5\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$291.6\\times $ </tex-math></inline-formula>\u2013<inline-formula> <tex-math notation=\"LaTeX\">$1894.3\\times $ </tex-math></inline-formula> energy efficiency over GPU. Compared to state-of-the-art transformer accelerators, HARDSEA has <inline-formula> <tex-math notation=\"LaTeX\">$1.2\\times $ </tex-math></inline-formula>\u2013<inline-formula> <tex-math notation=\"LaTeX\">$14.9\\times $ </tex-math></inline-formula> better energy efficiency at the same level of throughput.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "year": 2024, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A hybrid analog-ReRAM and digital-SRAM in-memory computing accelerator (HARDSEA), a computing-in-memory (CIM) accelerator supporting self-attention in transformer applications, and a time-domain winner-take-all (WTA) circuit to replace the expensive ADCs in ReRAM-CIM macros are proposed."}, "embedding": {"model": "specter_v2", "vector": [0.6001524925231934, 0.7615382671356201, -0.6030892133712769, 0.3165404796600342, -0.39026790857315063, 0.30507802963256836, 0.11509741097688675, 0.4103115499019623, -0.2920280396938324, -0.40396663546562195, 0.40555667877197266, 0.2420409470796585, 0.4490174353122711, -0.1810108870267868, 0.09604426473379135, -0.11350817233324051, -0.7799410820007324, -0.14658135175704956, 0.3601076900959015, -0.050715044140815735, 0.4231220483779907, -0.6977735757827759, -1.3046183586120605, 0.24064667522907257, 0.4683915376663208, 1.2780572175979614, 0.22081217169761658, 0.5543569922447205, -0.32063931226730347, 0.6968558430671692, 0.613896906375885, -0.23745407164096832, 0.4295564293861389, 0.006431398913264275, -0.24258366227149963, -0.29203304648399353, 0.3646319508552551, 0.1409396529197693, -0.4942498207092285, 0.9581438302993774, 0.11990659683942795, 0.3091636896133423, 0.31722354888916016, -0.27852389216423035, -0.02295106276869774, 0.806693971157074, 0.4662938416004181, 0.9575740694999695, -0.7183132767677307, -0.6822719573974609, 1.3038753271102905, -1.3244541883468628, -0.2383667379617691, 0.7135674357414246, 0.44722938537597656, 0.440972238779068, 0.05658012628555298, -0.5079518556594849, 0.5286267995834351, 0.4551179111003876, -0.7222181558609009, -0.4853237569332123, 0.412880539894104, 0.048379868268966675, 1.849829912185669, -0.2871699929237366, 0.47515594959259033, 0.2148158848285675, 0.14028321206569672, 1.3539965152740479, -0.11193354427814484, -0.7848355174064636, 0.24479243159294128, -0.15489375591278076, 0.5594048500061035, 0.6707587838172913, -0.060015399008989334, -0.08800843358039856, -1.0462617874145508, 0.009074287489056587, 0.6244358420372009, 0.3247641623020172, 0.7550261616706848, -0.09463377296924591, 0.10570699721574783, 0.4330008327960968, 0.4619918465614319, 0.7941288352012634, -0.5486611127853394, 1.020880103111267, 0.721948504447937, -0.05385056883096695, -0.15342967212200165, 0.04473040997982025, 0.4308284521102905, 0.14726409316062927, -1.172974944114685, -0.017416831105947495, -0.42107081413269043, 1.018669843673706, -0.2514622211456299, 0.7846850156784058, -0.47353675961494446, 0.22606320679187775, 0.809251070022583, 0.03296443074941635, 0.8924463391304016, -0.3099924921989441, -0.0583852082490921, -0.4779263734817505, -0.1710740476846695, -0.5540614128112793, -0.1251184493303299, -0.7024836540222168, -1.4313925504684448, -0.8515886068344116, -0.883176863193512, 0.8512561917304993, -0.7460867762565613, 0.2142244130373001, -0.5260001420974731, 0.1627744883298874, -0.048908691853284836, 0.3691433072090149, 0.3064960241317749, 0.14664563536643982, 0.26769277453422546, -0.04290589317679405, 1.3761440515518188, -1.3068381547927856, -0.592808723449707, -0.665780782699585, -0.062060896307229996, -0.11795549839735031, 0.013437016867101192, -0.05730161443352699, -1.6655927896499634, -1.1548290252685547, -0.8954096436500549, -0.10973009467124939, -0.5837160348892212, -0.24061302840709686, 0.7786797285079956, 0.3642390966415405, -1.1983522176742554, 0.8794978857040405, -0.6816530823707581, -0.16856129467487335, 0.6392505168914795, 0.31407734751701355, 1.0210505723953247, 0.20561805367469788, -1.1430714130401611, 0.14613568782806396, -0.0743170827627182, -0.4892885684967041, -0.3616475760936737, -0.46784475445747375, -0.7493478059768677, 0.3429392874240875, -0.017456432804465294, -0.3370324671268463, 1.2585822343826294, -0.42679086327552795, -1.2272230386734009, 0.8195557594299316, -0.3922550082206726, -0.36087048053741455, 0.12949024140834808, 0.32455939054489136, -0.6098705530166626, 0.00913907028734684, -0.10208418220281601, 0.44148722290992737, 0.7270822525024414, -0.04759477451443672, -0.22720621526241302, -0.11128593981266022, -0.6789335012435913, 0.06269862502813339, -0.5067024230957031, 0.6410714983940125, -0.5622499585151672, -0.5245456695556641, 0.6225649118423462, 0.8026939630508423, -0.3006221652030945, -0.13897350430488586, -0.32574963569641113, -1.1016713380813599, 0.6090954542160034, 0.7981675267219543, 1.197108507156372, -1.01807701587677, -1.3993233442306519, 0.13168394565582275, 0.023919157683849335, 0.03178522735834122, -0.324484258890152, 0.274158775806427, -0.5038393139839172, 0.06695011258125305, 0.4005107879638672, -0.5750410556793213, -0.1118142306804657, -0.37175410985946655, -1.019405484199524, -0.34947314858436584, 0.20312613248825073, 1.0533981323242188, -1.0445491075515747, -0.046689651906490326, -0.18254828453063965, 0.24437157809734344, -0.8594225645065308, 0.766325056552887, -0.11533863097429276, -0.3536776602268219, -0.13641729950904846, 0.15864425897598267, 0.08447752147912979, -0.618772029876709, 0.5610071420669556, -0.8307024836540222, -0.16891661286354065, 0.735684871673584, -0.14372052252292633, 1.075071930885315, -0.2166055142879486, 0.40855672955513, -0.12644048035144806, -0.8585709929466248, 0.4684187173843384, 0.25729435682296753, 0.0890449658036232, -1.1094002723693848, 0.48924633860588074, 0.14457397162914276, -0.5745460987091064, 0.3945949375629425, 1.2962374687194824, 1.1698312759399414, -0.49555060267448425, -0.01740315742790699, 0.37493157386779785, -0.12630747258663177, 0.245985209941864, 0.3865598142147064, 0.656338095664978, 0.045553069561719894, 0.5272151231765747, -0.521084189414978, 0.0583212748169899, -1.1369566917419434, 0.16859348118305206, 0.83486407995224, 0.07095064222812653, 0.7593176364898682, 0.18322673439979553, -0.6685307621955872, -0.5648086667060852, 0.13096696138381958, 0.3398013114929199, 1.3579188585281372, -0.0015821016859263182, 0.17345665395259857, -0.7931535243988037, 0.05328192189335823, -0.5227130055427551, -0.3421342074871063, -0.1963285505771637, -0.3314998149871826, -0.06763672828674316, -1.0233277082443237, 0.965468168258667, 0.6533870697021484, 1.1958411931991577, -0.8692030310630798, -0.8561423420906067, 0.0954483225941658, 0.3399166166782379, -0.8222545385360718, -0.5204628109931946, 0.9113445281982422, -0.5447478294372559, 0.21331565082073212, 0.1688416451215744, -0.2546652853488922, 0.26499468088150024, -0.3029040992259979, 0.824577808380127, -0.43166378140449524, -0.5357950329780579, 0.041213955730199814, 0.4550843834877014, -0.48325932025909424, -0.12712469696998596, 0.419038325548172, -0.06458079814910889, -0.19103282690048218, 0.7770659923553467, -0.19592967629432678, -0.5086702704429626, -0.295457124710083, 0.017703862860798836, -0.2213793843984604, 0.20175684988498688, -0.028383469209074974, 0.8282947540283203, -0.21787652373313904, 0.31527262926101685, -0.973732054233551, 0.7103579640388489, -0.00764898955821991, -0.11674337834119797, -0.3072945177555084, -1.0200761556625366, -0.41327178478240967, 0.6283173561096191, -0.6820554733276367, 0.04312827065587044, -0.8179417848587036, 0.49230247735977173, -0.6337133049964905, 0.032946959137916565, -0.3106127381324768, 0.3294932544231415, -0.08027690649032593, 0.36215880513191223, 0.6932781934738159, -0.14843103289604187, 0.6724145412445068, 0.19268016517162323, -0.5997786521911621, 0.7354211211204529, -0.07142113894224167, -0.2496793270111084, 0.17432118952274323, -0.08764205127954483, -0.6932308673858643, -0.3732486963272095, -0.38485437631607056, -0.03587668761610985, -0.3783728778362274, 0.09361393749713898, -0.856760561466217, -1.3124158382415771, 0.029291927814483643, -0.641422688961029, -0.5324040651321411, -0.3317742943763733, -0.6671723127365112, -0.3394028842449188, -0.9283840656280518, -1.2543805837631226, -1.0046663284301758, -1.4815287590026855, -0.8355793356895447, 0.22341780364513397, 0.40769028663635254, -0.26557454466819763, -0.47935107350349426, -0.6576923131942749, -0.6190570592880249, 1.3591657876968384, -0.5732452273368835, 0.27326202392578125, -0.2167319506406784, -0.41980788111686707, 0.26853445172309875, -0.15834660828113556, 0.16405488550662994, -0.251189649105072, 0.06754744797945023, -1.009035348892212, 0.5435740351676941, -0.1902632713317871, -0.3109584152698517, 0.45524048805236816, 0.1798127144575119, 1.3786144256591797, 0.03208047151565552, -0.6253448724746704, 0.14940285682678223, 1.5658659934997559, -0.32982176542282104, 0.6252650618553162, -0.12741105258464813, 0.6950638294219971, -0.47805583477020264, -0.02815948612987995, 0.716247022151947, 0.44146427512168884, 0.4318944215774536, 0.2414887249469757, -0.5085691213607788, -0.1651444286108017, -0.16102834045886993, 0.25992700457572937, 1.7591092586517334, 0.4328759014606476, 0.24965238571166992, -0.5538051128387451, 1.2078508138656616, -1.2443434000015259, -0.8456906080245972, 0.482588529586792, 0.7756525278091431, 0.407001256942749, -0.03444483503699303, -0.12380742281675339, 0.280019074678421, 0.42865461111068726, 0.34366145730018616, -0.7192654609680176, -1.0983402729034424, -0.06269965320825577, 0.9136396050453186, 0.6821659803390503, 0.2408093363046646, -0.11984994262456894, 0.6904895901679993, 14.866179466247559, 1.0461807250976562, -0.23716644942760468, 0.6623935699462891, 0.9584634304046631, 0.42682430148124695, -0.28773096203804016, 0.07528314739465714, -1.2323890924453735, 0.19278541207313538, 1.1218394041061401, 0.374441921710968, 0.31403905153274536, 0.46066489815711975, -0.06321466714143753, 0.13423335552215576, -0.3183058798313141, 0.5023238062858582, 0.6792148947715759, -1.114834189414978, 0.199880450963974, 0.08024343848228455, 0.3287864625453949, 0.4826807677745819, 1.092687964439392, 0.5814622044563293, 0.6811248064041138, -0.08350573480129242, 0.42440468072891235, 0.29114672541618347, 1.3691463470458984, -0.044340264052152634, 0.06848898530006409, 0.06423624604940414, -1.2553377151489258, -0.1956157386302948, -0.738784909248352, -1.1413096189498901, -0.33782070875167847, 0.6280366778373718, -0.3311538100242615, -0.5164644122123718, 0.0017152881482616067, 0.5938568115234375, 0.39589887857437134, 0.17933110892772675, -0.15492203831672668, 0.41380253434181213, 0.03575943037867546, -0.2669079601764679, -0.0062592304311692715, 0.7452089786529541, -0.2684915065765381, 0.30963432788848877, 0.0330318920314312, 0.5380812883377075, -0.012131139636039734, 0.5403419137001038, -0.33146756887435913, -0.467946320772171, 0.1405932456254959, 0.0008906587027013302, 0.17273622751235962, 1.3831367492675781, 0.3265267014503479, 0.019296221435070038, -0.12977567315101624, 0.39613786339759827, 0.37267014384269714, -0.08553320914506912, -0.6437021493911743, -0.43320387601852417, 0.1444864273071289, -0.32971566915512085, 0.309942364692688, 0.5081363916397095, -0.6295681595802307, -0.46785977482795715, -0.7904689908027649, -0.2694600522518158, 0.24882423877716064, -0.9826900959014893, -0.6140125393867493, 1.0079301595687866, -0.939348578453064, -0.4401640295982361, 0.6748685836791992, -0.656885027885437, -0.6251285076141357, 0.1083066537976265, -1.4723151922225952, -0.36914968490600586, -0.09008711576461792, -0.2406848818063736, -0.2604926526546478, 0.08524865657091141, 0.7233260869979858, 0.24982480704784393, -0.3680076599121094, 0.6657480001449585, -0.17389748990535736, -0.2177920639514923, -0.36210134625434875, -0.556071400642395, 0.9200466871261597, 0.1950741559267044, -0.604785144329071, 0.03543245792388916, -0.11179351061582565, 0.4134904444217682, -0.8083835244178772, -0.4228794276714325, 0.4369773864746094, -0.10104172676801682, -0.04334824159741402, -0.9647822380065918, -0.8228617310523987, 0.07400897145271301, 0.5922598242759705, 0.1765696406364441, 0.1798432320356369, -0.2270696610212326, -0.3229278028011322, -0.44621577858924866, -0.6992281079292297, 0.14697784185409546, 0.6252022981643677, -0.836172342300415, -0.060878220945596695, -0.39220473170280457, 0.08999944478273392, -1.0583406686782837, -0.7867861390113831, -0.17590904235839844, 0.13772591948509216, -0.2913489043712616, 1.0629277229309082, 0.26781556010246277, 0.7894914150238037, 0.8009706139564514, 0.01708476059138775, -0.2533455789089203, -0.17118261754512787, -0.867423415184021, -0.0814020037651062, 0.3231443166732788, 0.15147069096565247, -0.4335271418094635, 0.8270488381385803, 0.7391737103462219, 0.10401293635368347, -0.883200466632843, -0.7193619608879089, 0.15341250598430634, -0.34864184260368347, -0.4305221736431122, 0.3396592140197754, 0.20215143263339996, 0.7852034568786621, -0.041616763919591904, 0.8336936235427856, 0.03349050506949425, 0.14020508527755737, -0.41158944368362427, -0.09396814554929733, -0.015915313735604286, 0.10277864336967468, -0.8124619126319885, -0.812195360660553, -1.3781827688217163, -0.3481523096561432, -1.2466604709625244, 0.034024398773908615, -0.5924098491668701, -0.1716628521680832, -0.31367361545562744, -0.695084810256958, 0.19350776076316833, 0.39433008432388306, -0.18973785638809204, -0.636915922164917, -0.4659162163734436, -0.9577876329421997, 0.7888986468315125, 0.7243690490722656, -0.7282963991165161, 0.21503111720085144, -0.26000097393989563, -0.2158578634262085, 0.30611950159072876, 0.5721179842948914, -0.4502592384815216, -0.42679643630981445, -0.7845218181610107, 0.06712105870246887, -0.14749601483345032, -0.11210142821073532, -1.2701489925384521, 1.4763442277908325, 0.4775245487689972, 0.02842310629785061, -0.39981696009635925, 0.08817513287067413, -0.8876580595970154, -0.6855908036231995, 0.5624057650566101, -0.8439997434616089, 0.20296147465705872, 0.2947848439216614, -0.7355472445487976, -0.1877840906381607, 0.48978784680366516, 0.06814150512218475, -0.9115170240402222, -0.807323694229126, 0.4026378393173218, -0.922543466091156, 0.269228994846344, -0.29910168051719666, -0.27503082156181335, -1.2587263584136963, -0.14103113114833832, -0.0764356404542923, 0.3232767879962921, -0.422475129365921, 0.7270163893699646, 0.37382498383522034, -1.307507872581482, 0.3318415880203247, 0.4011966288089752, -0.3162858784198761, 0.20176778733730316, 0.7115969657897949, 0.4653013050556183, -0.05459919199347496, 0.4296186864376068, 0.014955351129174232, 0.13060298562049866, -0.4422912895679474, 0.292742520570755, 0.8579927682876587, -0.7083196043968201, 0.07851742953062057, 0.4946291744709015, -0.4877716898918152, -0.5331183075904846, 0.049172695726156235, -1.1717710494995117, -0.4042719602584839, -0.35719403624534607, 0.4410257935523987, 0.38764527440071106, 0.07698532938957214, 0.10576032102108002, -0.47634539008140564, 0.22022804617881775, -0.29798638820648193, -0.29438674449920654, 0.13172324001789093, 0.09116391837596893, -0.14679068326950073, 0.3028574585914612, 0.5126392245292664, -0.7001111507415771, -0.7998446822166443, -0.9017068147659302, -0.5597624182701111, 0.23933404684066772, 0.41893094778060913, -0.007256310898810625, -1.0609129667282104, 0.7698917388916016, 0.903030514717102, 0.6581252217292786, 0.48877397179603577, -0.5126367807388306, 0.287110298871994, 0.40563806891441345, 0.15216337144374847, -1.1434881687164307, -0.23060224950313568, 1.1896265745162964, 0.6846107244491577, -0.5161345601081848, 0.3987925946712494, -0.3446834087371826, -0.46061182022094727, 0.8298510313034058, 0.5330962538719177, -0.39679694175720215, 0.8928990364074707, 0.3652515411376953, -0.23590190708637238, 0.03965228050947189, -1.211641788482666, -0.5200325846672058, 0.27837929129600525, 0.9565156698226929, 0.7037264108657837, 0.2224949598312378, 0.2391645312309265, 1.0720152854919434, 0.5838519930839539, 0.1073363646864891, 0.478719025850296, 0.010752229019999504, -0.32336628437042236, 0.1915847808122635, 0.09478414803743362, 0.6350158452987671, -0.7632229328155518, -0.8547605872154236, 0.6311706304550171, 0.4167896509170532, 0.1990164816379547, 0.5760867595672607, 1.4654358625411987, -0.10679163783788681, 0.3243398666381836, -0.38696423172950745, 0.4555796682834625, -0.23796749114990234, -0.5714379549026489, 0.01632927544414997, -1.0420013666152954, -0.10119485855102539, -0.1427445411682129, -0.5391161441802979, -0.717427670955658, 0.14621806144714355, -0.023275185376405716, -0.05207332968711853, 0.5692156553268433, 0.49995747208595276, 1.0502402782440186, 1.2011038064956665, -0.024243298918008804, -0.7422628402709961, -0.3245120346546173, -0.5552420616149902, 0.08747472614049911, -0.43971017003059387, -0.4786486029624939, 0.1322762370109558, -0.012491721659898758, -0.5352369546890259]}, "authors": [{"authorId": "2131166969", "name": "Shiwei Liu"}, {"authorId": "2149029623", "name": "Chen Mu"}, {"authorId": "2152631921", "name": "Hao Jiang"}, {"authorId": "2115940437", "name": "Yunzhengmao Wang"}, {"authorId": "2108141936", "name": "Jinshan Zhang"}, {"authorId": "2190562829", "name": "Feng Lin"}, {"authorId": "1491626804", "name": "Keji Zhou"}, {"authorId": "2144832015", "name": "Qi Liu"}, {"authorId": "2241935225", "name": "Chixiao Chen"}], "references": [{"paperId": "bd5c24ec7d3c91648fb0beec6508a5302823f97b", "title": "X-Former: In-Memory Acceleration of Transformers"}, {"paperId": "87bcf3e1e47b341d57ed6478e7ff6e2ff9955a80", "title": "16.2 A 28nm 53.8TOPS/W 8b Sparse Transformer Accelerator with In-Memory Butterfly Zero Skipper for Unstructured-Pruned NN and CIM-Based Local-Attention-Reusable Engine"}, {"paperId": "429a414f745150aa46ec0a7bd479faa09698b7a6", "title": "CTA: Hardware-Software Co-design for Compressed Token Attention Mechanism"}, {"paperId": "ec68079f15b620a8a7ad19b3477f895e2ecf193d", "title": "HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers"}, {"paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8", "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention"}, {"paperId": "4555465de28e6896378097e9801f3b1133a140c4", "title": "In-sensor reservoir computing system for latent fingerprint recognition with deep ultraviolet photo-synapses and memristor array"}, {"paperId": "200ef1cde362aafbf598a2b5a1c5f35504ca2289", "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design"}, {"paperId": "25f10e731f8a9787adaf8826d70289af529db44e", "title": "In-Memory Computation With Improved Linearity Using Adaptive Sparsity-Based Compact Thermometric Code"}, {"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"}, {"paperId": "f841f3d912be52a621aab1a979632e9daeab6599", "title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation"}, {"paperId": "86891d00499eebe86d3f1e39143d412addf2652b", "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"}, {"paperId": "a262a3d81f72e4b07558e636489e85ca88045156", "title": "An Algorithm\u2013Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers"}, {"paperId": "c0fa046006d5094512674335364135e2e2ca9ff4", "title": "STICKER-IM: A 65 nm Computing-in-Memory NN Processor Using Block-Wise Sparsity Optimization and Inter/Intra-Macro Data Reuse"}, {"paperId": "f5edad3a50ba8a6329d202f31677d988f1d0cf87", "title": "TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer"}, {"paperId": "626f0fcdb4c0938ac6e5a291e7730cbefdfa8656", "title": "Online Fault Detection in ReRAM-Based Computing Systems for Inferencing"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "7ca7fe437f3b8b0ae379c5533384d2fea0a4ac45", "title": "A 28nm 15.59\u00b5J/Token Full-Digital Bitline-Transpose CIM-Based Sparse Transformer Accelerator with Pipeline/Parallel Reconfigurable Modes"}, {"paperId": "1c1c186fa5bd75953b2874af4f74750c57c8d4a7", "title": "A 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing"}, {"paperId": "bca28d3aabf1238718a8db7d7519afbdf8681ed7", "title": "COMB-MCM: Computing-on-Memory-Boundary NN Processor with Bipolar Bitwise Sparsity Optimization for Scalable Multi-Chiplet-Module Edge Machine Learning"}, {"paperId": "6a8fdb7a773bf89262b7a9a7437617cf154592d9", "title": "Implementation of Discrete Fourier Transform using RRAM Arrays with Quasi-Analog Mapping for High-Fidelity Medical Image Reconstruction"}, {"paperId": "1b0d4caab34f0d532de892ea1be0fc6ee07b6cd1", "title": "MAT: Processing In-Memory Acceleration for Long-Sequence Attention"}, {"paperId": "78bb909c314784ff345fe2bea997e74ca08ee0e5", "title": "A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "dbbd6840a692763b7bebb28932d6efe180711588", "title": "A Highly Reliable RRAM Physically Unclonable Function Utilizing Post-Process Randomness Source"}, {"paperId": "704586af2e6ec65e1412b32333e4e64e2b20106c", "title": "Z-PIM: A Sparsity-Aware Processing-in-Memory Architecture With Fully Variable Weight Bit-Precision for Energy-Efficient Deep Neural Networks"}, {"paperId": "11e4f32f5d280b82369e54b32e71d3c0c8934a62", "title": "An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"}, {"paperId": "99026c277735284df33636455702e9eaf469583b", "title": "ReTransformer: ReRAM-based Processing-in-Memory Architecture for Transformer Acceleration"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "3ab69e432d2ed279bbd3a2837aaeeff0f596f594", "title": "An 8-Bit in Resistive Memory Computing Core With Regulated Passive Neuron and Bitline Weight Mapping"}, {"paperId": "614e222b921974f2a8b48c9fbbc8868c9362e99d", "title": "A Communication-Aware DNN Accelerator on ImageNet Using In-Memory Entry-Counting Based Algorithm-Circuit-Architecture Co-Design in 65-nm CMOS"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "f16f46456b3d30f0911c8344687417178bc29bd0", "title": "Fully hardware-implemented memristor convolutional neural network"}, {"paperId": "fb4462d05419804735720adab71ca7a993c9d3c6", "title": "DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a31d8d755192cd1cea18ebe87dc6ae277c93ff4c", "title": "Noise Injection Adaption: End-to-End ReRAM Crossbar Non-ideal Effect Adaption for Neural Network Mapping"}, {"paperId": "1c69b3bc6bbd9b1f0d6c58251d4be9f2193e9ddc", "title": "ROMANet: Fine-Grained Reuse-Driven Off-Chip Memory Access Management and Data Organization for Deep Neural Network Accelerators"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "edc0a758d95f62181c30ad30ab4ef2bef16e742d", "title": "Analogue signal and image processing with large memristor crossbars"}, {"paperId": "69fa579463dfbbd2316193dd3dc9438a527e09d1", "title": "Fine-Grained DRAM: Energy-Efficient DRAM for Extreme Bandwidth Systems"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2fdaa46ed54c593f8558dd90857cc5cadbabfa30", "title": "Dot-product engine for neuromorphic computing: Programming 1T1M crossbar to accelerate matrix-vector multiplication"}, {"paperId": "e15fdad9f7d160e11e9a313bd80ebe99952eff08", "title": "Quantization based Fast Inner Product Search"}, {"paperId": "676c29f8dffb018449168660f0477dddb47360fb", "title": "A 12-bit vernier ring time-to-digital converter in 0.13\u03bcm CMOS technology"}, {"paperId": "7e92d80c98140cb502781c7f012f830a965da29f", "title": "Incorporating Variability of Resistive RAM in Circuit Simulations Using the Stanford\u2013PKU Model"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "4748d22348e72e6e06c2476486afddbc76e5eca7", "title": "Product Quantization for Nearest Neighbor Search"}, {"paperId": null, "title": "\u201cLarge text compression benchmark,\u201d"}, {"paperId": null, "title": "HARDSEA: HYBRID ANALOG-ReRAM CLUSTERING AND DIGITAL-SRAM IN-MEMORY COMPUTING ACCELERATOR"}]}