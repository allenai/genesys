{"paperId": "4d42fc9c9dbdeadc8772ba3a00ac9231bfbbe2fb", "abstract": "Cross-modal encoders for vision-language (VL) tasks are often pretrained with carefully curated vision-language datasets. While these datasets reach an order of 10 million samples, the labor cost is prohibitive to scale further. Conversely, unimodal encoders are pretrained with simpler annotations that are less cost-prohibitive, achieving scales of hundreds of millions to billions. As a result, unimodal encoders have achieved state-of-art (SOTA) on many downstream tasks. However, challenges remain when applying to VL tasks. The pretraining data is not optimal for cross-modal architectures and requires heavy computational resources. In addition, unimodal architectures lack cross-modal interactions that have demonstrated significant benefits for VL tasks. Therefore, how to best leverage pretrained unimodal encoders for VL tasks is still an area of active research. In this work, we propose a method to leverage unimodal vision and text encoders for VL tasks that augment existing VL approaches while conserving computational complexity. Specifically, we propose Multimodal Adaptive Distillation (MAD), which adaptively distills useful knowledge from pretrained encoders to cross-modal VL encoders. Second, to better capture nuanced impacts on VL task performance, we introduce an evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of data constraints and conditions of domain shift. Experiments demonstrate that MAD leads to consistent gains in the low-shot, domain-shifted, and fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA performance on VCR compared to other single models pretrained with image-text data. Finally, MAD outperforms concurrent works utilizing pretrained vision encoder from CLIP. Code will be made available.", "venue": "arXiv.org", "year": 2022, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.10496", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Experiments demonstrate that MAD leads to consistent gains in the low-shot, domain-shifted, and fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA performance on V CR compared to other single models pretrained with image-text data."}, "embedding": {"model": "specter_v2", "vector": [0.21495385468006134, 0.5714285373687744, 0.09356369823217392, -0.18589088320732117, -0.5706826448440552, -0.38257697224617004, 0.930138349533081, -0.1625952124595642, -0.6636736392974854, -0.24845290184020996, 0.48165163397789, -0.24959707260131836, 0.4243442714214325, 0.1861151158809662, -0.14187079668045044, 0.28449127078056335, -0.43569982051849365, 0.34762245416641235, 0.060484614223241806, -0.9466845393180847, -0.2111768126487732, -0.8933764696121216, -0.7788776755332947, 0.27910274267196655, 0.03200037032365799, 0.5636966228485107, 0.3595893085002899, 1.3613026142120361, -0.2774433195590973, 0.7184959650039673, 0.42031416296958923, -0.5183299779891968, 0.04352641850709915, -0.24597448110580444, -0.6821402311325073, 0.20137394964694977, 0.5269185304641724, -1.0013463497161865, -0.6130949854850769, 0.6211774349212646, -0.06994515657424927, 0.3538675308227539, 0.9556465744972229, -0.6141784191131592, -1.307046890258789, 0.6528395414352417, 0.6417320966720581, 0.322868287563324, -0.0012017134577035904, -0.17643554508686066, 1.6711560487747192, -1.4224451780319214, 0.43794718384742737, 1.54567289352417, 0.004951108247041702, 1.0493122339248657, -0.23216591775417328, -0.3602309226989746, 0.5705105662345886, 0.28047603368759155, -0.6560912728309631, -0.25036323070526123, -0.01989947259426117, -0.3078819513320923, 1.593031644821167, -0.6417348384857178, -0.3245592713356018, 0.6842548847198486, -0.13944977521896362, 1.3292590379714966, -0.42574501037597656, -0.906365692615509, -0.34286290407180786, -0.02053816244006157, 0.23416215181350708, 0.987113356590271, -0.5141802430152893, 0.016575811430811882, -0.5651412606239319, 0.5376842617988586, 0.09462560713291168, -0.19704775512218475, -0.3445548415184021, -0.20345142483711243, -0.4610081911087036, 0.8693406581878662, 0.6469264626502991, 0.661875307559967, 0.05987197533249855, 0.47373440861701965, 0.6613054871559143, 0.5952032804489136, -0.6194276809692383, 0.35269874334335327, 0.23761333525180817, 0.820253312587738, -0.7408138513565063, 0.3623965382575989, -0.04965895786881447, 0.9799211621284485, -0.2566870450973511, 0.08420165628194809, -0.8864290714263916, -0.0104440962895751, 1.6224827766418457, 0.009572895243763924, 0.48185956478118896, -0.8054544925689697, 0.39414477348327637, -0.7465081214904785, -0.005176466889679432, -0.9312839508056641, -0.1018177717924118, 0.2545555830001831, -0.7207303643226624, -0.9394965171813965, -0.333217978477478, 0.3205472230911255, -1.1671233177185059, 0.995276689529419, -0.5174838900566101, -0.4983750879764557, 0.4486790597438812, 0.5267269611358643, 1.271809697151184, 0.5797146558761597, 0.4940617084503174, 0.5700414776802063, 1.160481333732605, -0.7667824029922485, -0.4849115014076233, -1.128020167350769, 0.3463486135005951, -0.3821523189544678, 0.5868759751319885, -0.36822250485420227, -1.0509570837020874, -1.1971858739852905, -1.154808521270752, -0.6802411675453186, -0.779962420463562, 0.455800324678421, 0.37384963035583496, 0.09049202501773834, -1.203269362449646, 0.10028791427612305, 0.24614110589027405, -0.4520193636417389, 0.31546422839164734, -0.15607094764709473, 0.09334450960159302, -0.9154483079910278, -1.1322070360183716, 0.5025285482406616, 0.3168521225452423, -0.5267660021781921, -0.6590672135353088, -0.015952417626976967, -1.9489169120788574, -0.4276089370250702, 0.05818869173526764, -0.855665922164917, 1.3493624925613403, -0.15876607596874237, -0.7713120579719543, 1.0700205564498901, -0.4445146918296814, 0.12825530767440796, 0.14207325875759125, -0.02918776497244835, -0.5409674048423767, 0.07169380784034729, 0.08280546218156815, 1.3563627004623413, 0.6116732954978943, -0.3449729382991791, 0.18860644102096558, 0.3581194281578064, 0.1459408849477768, -0.032302677631378174, -0.27364134788513184, 0.7244132161140442, -0.39892739057540894, -0.23663319647312164, 0.1823216825723648, 0.7786165475845337, 0.3200838565826416, 0.0639284998178482, -0.5966344475746155, -0.843991756439209, 0.9625107645988464, -0.04475958272814751, 0.1593228429555893, -1.110329270362854, -0.3705291450023651, -0.7604579925537109, 0.18919779360294342, -0.26355987787246704, -1.1673293113708496, 0.5537413954734802, -0.2122192084789276, 0.20207761228084564, -0.2204177975654602, -1.51893150806427, 0.4171505570411682, -0.10898606479167938, -0.8205530643463135, -0.38430726528167725, 0.4798033535480499, 1.5594854354858398, -0.5876173973083496, -0.40918317437171936, 0.21564188599586487, 0.19442901015281677, -0.8834452033042908, 0.994452953338623, -0.7478986382484436, 0.4875124990940094, 0.00916824210435152, 0.0008628825889900327, -0.22917500138282776, -0.6508252620697021, -0.09463087469339371, -0.8400977253913879, 0.3827769458293915, 0.14170591533184052, 0.2219466120004654, 1.7740963697433472, -0.20187973976135254, 1.209733486175537, -0.3543199598789215, -0.7443916201591492, 0.2602989971637726, 0.260885089635849, -0.5690104365348816, -0.4836371839046478, 0.4575299918651581, -0.011074190028011799, -0.611621618270874, -0.32316192984580994, 0.954194188117981, 0.4856574535369873, -0.4117261469364166, 0.035113535821437836, 0.5755152106285095, -0.6262876391410828, 0.531589150428772, 0.6752164959907532, 0.5524153709411621, 0.3038090169429779, 0.18938373029232025, 0.5203489065170288, 0.754078209400177, -0.4650014638900757, -0.7563062310218811, 0.4874039590358734, 0.4590320289134979, 1.1010266542434692, 0.18957959115505219, -0.5043836832046509, -0.4080114960670471, -0.21618901193141937, 0.7724853157997131, 1.501388430595398, 0.28102728724479675, 0.19741860032081604, -0.41485512256622314, -0.17431952059268951, -0.44546666741371155, 0.2180221676826477, -0.5048708915710449, -0.4043150544166565, -0.02716735377907753, -0.5061376690864563, 0.7184073328971863, 0.5044498443603516, 1.3919731378555298, -0.5803680419921875, -0.21736980974674225, -0.6753397583961487, -0.18098844587802887, -1.2802352905273438, -0.6054211854934692, -0.17816102504730225, 0.22073981165885925, -0.2633214592933655, -0.24999703466892242, -0.5701960921287537, 0.3159101903438568, -0.5710344910621643, 1.070885419845581, -1.1081559658050537, -0.6364693641662598, 0.7784900665283203, 0.35886529088020325, -0.17044833302497864, -0.9888321161270142, -0.39123719930648804, -0.40936756134033203, -0.1605212539434433, 0.6261062622070312, 0.9155778288841248, -0.0820993110537529, -0.009920777752995491, -0.6830723285675049, 0.18981291353702545, 0.19446803629398346, 0.04419936239719391, 0.7782202959060669, -0.5760135650634766, 0.18844963610172272, -0.4799357056617737, 0.8050665259361267, 0.02437266707420349, -0.1811644732952118, 0.30310124158859253, -0.27055925130844116, -0.4179883897304535, 0.32896795868873596, -0.6565436720848083, -0.4863641858100891, -0.8506584167480469, 0.3991597592830658, -0.6247756481170654, -0.5761540532112122, 0.118975929915905, 0.3166053891181946, 0.29931318759918213, 0.5422050356864929, 0.5081637501716614, 0.5003557801246643, 0.5180773735046387, 1.1394184827804565, -0.7244578003883362, 0.9441933631896973, 0.43123412132263184, 0.28402042388916016, 0.12361100316047668, 0.11544499546289444, -0.5785403251647949, -0.35603129863739014, -0.12634283304214478, -0.09325601905584335, -0.5486840009689331, 1.1656285524368286, -0.7468835711479187, -0.6787951588630676, -0.12364309281110764, -1.2458088397979736, -0.0956130400300026, -0.04946591332554817, -0.39768221974372864, -0.26956695318222046, -0.7625163793563843, -1.0678073167800903, -0.09476932138204575, -0.38495123386383057, -0.9935020804405212, 0.4874763786792755, 0.2902817726135254, -0.7357162833213806, -0.45465555787086487, -0.028580402955412865, 0.0031225450802594423, 0.6349159479141235, -0.46578770875930786, 0.9499041438102722, -0.058834969997406006, -0.21828019618988037, -0.5496218800544739, -0.1832018494606018, 0.8847954273223877, -0.04774792864918709, 0.21024924516677856, -0.9599780440330505, 0.14432038366794586, -0.5207516551017761, -0.9381264448165894, 0.3781546652317047, 0.23987697064876556, 0.2668343484401703, 0.4797530174255371, -0.045218780636787415, 0.03061763383448124, 1.4996893405914307, -0.6234135031700134, 0.03349010646343231, -0.017868412658572197, 0.9809458255767822, 0.4677514433860779, -0.2525039613246918, 0.42768558859825134, 1.0597764253616333, -0.013550433330237865, 0.38305729627609253, 0.006688040681183338, -0.5637246966362, -0.48039525747299194, 0.5550230145454407, 1.0238937139511108, 0.5771451592445374, 0.0685582384467125, -1.1957502365112305, 0.9396483302116394, -1.025617003440857, -0.5797741413116455, 0.7450059056282043, 0.20668762922286987, 0.28635644912719727, -0.7858419418334961, -0.601704478263855, -0.47919991612434387, 0.41674143075942993, 0.30385491251945496, -0.5549474358558655, -0.5626474022865295, -0.13574905693531036, 0.030353743582963943, -0.39169377088546753, 0.5406883358955383, -0.6922213435173035, 0.41018441319465637, 14.140166282653809, 0.37371379137039185, 0.1457918882369995, 0.5746660232543945, 0.6645556688308716, 0.3375098407268524, -0.5641062259674072, -0.4043940305709839, -1.1228160858154297, -0.649658739566803, 0.585715651512146, 0.7005817890167236, 0.3919265866279602, 0.017316875979304314, -0.4708014726638794, 0.17935319244861603, -0.9470330476760864, 0.9142078757286072, 1.1409975290298462, -1.056223750114441, 0.43308526277542114, -0.13122883439064026, 0.20944595336914062, 0.3581460416316986, 1.1976337432861328, 1.0815573930740356, 0.41457685828208923, -0.43420958518981934, 0.30492857098579407, 0.20457680523395538, 0.975660502910614, 0.23288308084011078, 0.05492967367172241, 0.3632985055446625, -0.9053068161010742, -0.3466055989265442, -0.39914655685424805, -0.6975825428962708, 0.16070523858070374, -0.5181228518486023, -0.28577879071235657, -0.4264284372329712, -0.10650452971458435, 0.7518442273139954, 0.07682494074106216, -0.11774136871099472, -0.2998327910900116, -0.040175314992666245, 0.45931196212768555, -0.18670707941055298, 0.26585888862609863, 0.7669363021850586, 0.4715608060359955, -0.20560327172279358, -0.16261506080627441, -0.13395725190639496, -0.05660613998770714, 1.0260510444641113, -0.4343172609806061, -0.34142330288887024, -0.7100200057029724, -0.3208608627319336, -0.37206795811653137, 0.6098011136054993, 0.5464759469032288, 0.45472612977027893, -0.735516369342804, 0.5613308548927307, 0.21214717626571655, 0.5633871555328369, -0.42976412177085876, -0.2519800364971161, 0.05167820677161217, -0.8380753993988037, 0.3610016703605652, 0.2995779812335968, 0.0858713909983635, -0.5068743228912354, -0.6768183708190918, -0.1971541792154312, 0.6413021683692932, -1.230988621711731, -1.1179181337356567, 0.9455242156982422, -0.24726104736328125, -0.7511821985244751, -0.23297713696956635, -1.2424061298370361, -0.1401137113571167, 0.47236931324005127, -1.5133496522903442, -0.9679904580116272, -0.1789274662733078, 0.059954918920993805, 0.16417774558067322, -0.5282523036003113, 1.0212669372558594, 0.11338362842798233, -0.13180068135261536, 0.011188055388629436, -0.48176199197769165, 0.3123279809951782, 0.16318942606449127, -0.6255934834480286, 0.3873477876186371, 0.3160972595214844, -0.18253029882907867, -0.24617840349674225, 0.0457894392311573, 0.22432927787303925, -0.8190233111381531, 0.06395590305328369, 0.7109605669975281, -0.9067511558532715, -0.3896934986114502, -0.45843398571014404, -0.4446544349193573, 0.39139890670776367, 1.0507994890213013, -0.25051799416542053, 0.41962093114852905, -0.18624135851860046, -1.0552303791046143, 0.06381194293498993, -0.978753924369812, 0.5103506445884705, -0.18334291875362396, -1.197914719581604, -0.6446527242660522, -0.04001309722661972, 0.5968731045722961, -0.7629457116127014, -0.2526441514492035, -0.19139507412910461, 0.1381542682647705, 0.14424747228622437, 1.24763023853302, -0.4908910393714905, 1.329548954963684, 0.6241994500160217, -0.7227774858474731, -0.6002198457717896, 0.11444888263940811, -0.3114383816719055, 0.03993815928697586, 0.32540395855903625, 0.5800215601921082, -0.2971649169921875, -0.45673784613609314, 1.082482933998108, 0.3757880628108978, -0.2299378514289856, -0.3148562014102936, -0.04385620355606079, 0.22355501353740692, -0.49152350425720215, -0.08478007465600967, -0.034376583993434906, 0.029343752190470695, 0.5709424614906311, 0.5927634239196777, 0.603086531162262, -0.3489629924297333, -0.5965181589126587, 0.36200353503227234, 0.20392395555973053, -0.4419690668582916, -0.6813669204711914, -0.6803686618804932, -1.4963711500167847, 0.19476255774497986, -1.236786961555481, 0.2370024472475052, -1.239052653312683, -0.33607301115989685, 0.923863410949707, 0.03174427151679993, 0.37704604864120483, 0.40543386340141296, 0.04494505003094673, -0.06368272751569748, -0.6217297315597534, -0.5547595024108887, 0.9382656812667847, 1.159859299659729, -1.2121309041976929, 0.42012831568717957, -0.6624100208282471, -0.34802713990211487, 0.23332267999649048, 0.08837641775608063, 0.3554602563381195, -1.181213140487671, -1.503091812133789, 0.47580885887145996, -0.18398785591125488, 0.4122650921344757, -0.796795129776001, 0.8136019110679626, 0.7540329694747925, 0.031263772398233414, -0.12192727625370026, 0.6935319900512695, -0.8150942921638489, -1.254424810409546, 0.20760203897953033, -0.9485064744949341, -0.0017372564179822803, 0.5548274517059326, -0.37687134742736816, -0.5913857221603394, 0.5661696791648865, 0.12362192571163177, -0.8939434885978699, -1.3611634969711304, 0.19961105287075043, -0.33147796988487244, 0.29638540744781494, 0.07825252413749695, -0.05476884916424751, -1.6119964122772217, -0.7787678241729736, -0.19134613871574402, 0.6128973364830017, -0.8898102641105652, 0.8304951190948486, 1.1978219747543335, -1.2120248079299927, -0.26893916726112366, 0.19313913583755493, 0.44847628474235535, 0.09782818704843521, 0.8148787021636963, 0.38458096981048584, -0.21926948428153992, 0.28802773356437683, 0.08626807481050491, 0.14633122086524963, -0.9279366135597229, 0.08456707000732422, 1.0294668674468994, -0.01172610279172659, 0.17208805680274963, 1.354925513267517, -0.0845613032579422, -1.090741753578186, 0.1765216886997223, -0.7643554210662842, -0.7383077144622803, -0.0810365304350853, 0.6424042582511902, -0.277727335691452, -0.3784119486808777, -0.3286048173904419, -0.34797602891921997, 0.6033441424369812, -0.1849641352891922, -1.0075178146362305, -0.11471738666296005, -0.443034827709198, -0.5277281999588013, 0.5864878296852112, 1.167752742767334, -1.0264465808868408, -0.44743412733078003, -0.6866405010223389, -0.829509437084198, 0.3863157033920288, 0.07221098989248276, -0.5559543967247009, -0.4914097785949707, 0.9056836366653442, 0.44885194301605225, 0.22884921729564667, 0.6099268198013306, 0.4140310287475586, 0.1532825082540512, 1.0299268960952759, -0.5765126943588257, -0.3227349519729614, 0.029759561643004417, 1.3234730958938599, 1.5038390159606934, -1.423036813735962, -0.023828519508242607, -0.16210320591926575, -0.8159455060958862, 0.7704975605010986, 0.5029076337814331, 0.07687169313430786, 0.8697355389595032, -0.565156102180481, 0.031150612980127335, 0.1928613930940628, -0.7313693165779114, -0.4504483938217163, 1.3833156824111938, 1.2792295217514038, 0.6571069955825806, 0.1677200198173523, 0.3293267488479614, 0.5952414870262146, 0.20267142355442047, -0.0510730966925621, 0.2701951265335083, 0.11505430191755295, -0.17897139489650726, 0.1349410116672516, 0.12272026389837265, 0.4309324026107788, -0.10260158032178879, -0.5720952153205872, 0.05211402848362923, 0.6250743269920349, 0.18999697268009186, 1.0028473138809204, 0.9262811541557312, 0.007382227573543787, 0.4919207990169525, 0.2863803803920746, 0.7691001892089844, -0.30878955125808716, 0.4433547854423523, -0.5527263879776001, -0.7653860449790955, 0.2102547585964203, -0.5159696936607361, -0.5121914744377136, -0.6692811846733093, -0.053304433822631836, 0.8375862240791321, -0.9538661241531372, 0.3894152045249939, 1.447563886642456, 0.3695472776889801, 0.7941102385520935, -0.8107144236564636, -0.10506662726402283, -0.2270199954509735, -0.756321132183075, 0.49307990074157715, -0.39789676666259766, 0.20326673984527588, -0.12019183486700058, -0.1940978467464447, 0.07193205505609512]}, "authors": [{"authorId": "2513111", "name": "Zhecan Wang"}, {"authorId": "40589056", "name": "N. Codella"}, {"authorId": "2378902", "name": "Yen-Chun Chen"}, {"authorId": "2116644664", "name": "Luowei Zhou"}, {"authorId": "3386593", "name": "Xiyang Dai"}, {"authorId": "2054421528", "name": "Bin Xiao"}, {"authorId": "145743311", "name": "Jianwei Yang"}, {"authorId": "30156979", "name": "Haoxuan You"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "2122374530", "name": "Shih-Fu Chang"}, {"authorId": "2150687325", "name": "Lu Yuan"}], "references": [{"paperId": "837173ef1f260adc0d50b76675915776e1cc8ade", "title": "RegionCLIP: Region-based Language-Image Pretraining"}, {"paperId": "c11ee33dea3f83cd77dcbc14684ee305b7a7e184", "title": "SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning"}, {"paperId": "d257547d681f3a153f4bbf85f5955b5a0189e500", "title": "Combined Scaling for Zero-shot Transfer Learning"}, {"paperId": "9c81394363178ce94b1eac111df4e3ad11bdb7c2", "title": "FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "8f167ec1149921fac63b1ea855443de109bb013a", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "90357a6dc817e2f7cec477a51156675fbf545cf1", "title": "MERLOT: Multimodal Neural Script Knowledge Models"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "7631808b179c2c03030d528ddd1b76356a6a71e9", "title": "A Case Study of the Shortcut Effects in Visual Commonsense Reasoning"}, {"paperId": "a97e8aa628ae2e3fbb03c25df2f2a3c1432a2f3b", "title": "Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation"}, {"paperId": "2fa4938001b18f464c62aa38a5a469bb92569d57", "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning"}, {"paperId": "8866ad5f6d12fa86c689da3802882b7383a6c2c6", "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering"}, {"paperId": "889c9c37634766b3543424ac6955811f83f260e0", "title": "Compressing Visual-linguistic Model via Knowledge Distillation"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "2ff7d8d79c1ab50c1826d965475a1eb32db0c133", "title": "SEED: Self-supervised Distillation For Visual Representation"}, {"paperId": "09f3c809cfaab3340baa92619f4bfdbb3669d00b", "title": "Wasserstein Contrastive Representation Distillation"}, {"paperId": "19477cbadc03fbbca4601a4588f21fab28edaf2e", "title": "Adaptive multi-teacher multi-level knowledge distillation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "0abb08c4ec5feab4cdd82c471866dd4395c573ce", "title": "Contrastive Distillation on Intermediate Representations for Language Model Compression"}, {"paperId": "bc996a4dbf9d4234eacdd0b930a94de1d158e256", "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"}, {"paperId": "2f5f81bc516a6d085d39479378af1fc27104f91e", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3b0d38302115af1165639d6fb34c4c19187b2a6f", "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models"}, {"paperId": "2528a82dd2266600d4ee2b54165556a984de94d4", "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "598a2ee223e2949c3b28389e922c1892b4717d2a", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"}, {"paperId": "122b75042daae44f93153dedda15b0fb11b3f279", "title": "What Do Models Learn from Question Answering Datasets?"}, {"paperId": "3913afb4d9747dcbf27e75e75dcf4c48e6da00b2", "title": "Knowledge distillation via adaptive instance normalization"}, {"paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"}, {"paperId": "9cb32bdd43f64b36cb447ba1307869c5d8bf675c", "title": "YAKE! Keyword extraction from single documents using multiple local features"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "97f4d09175705be4677d675fa27e55defac44800", "title": "Contrastive Multiview Coding"}, {"paperId": "fd4675526ee569196ad1698935b8f5a529b1f9ba", "title": "Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"}, {"paperId": "735a63b58349e07b84c2e31927ce1b1cfaf09980", "title": "Cycle-Consistency for Robust Visual Question Answering"}, {"paperId": "3c54b796cc10cb530f77caa4d18e1c80ac863822", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "45ec1446f42c0a7c7fe74319118335c76e0f7b19", "title": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization"}, {"paperId": "b80f128830114896df94999b4104cb75408e657e", "title": "Transfer Learning via Unsupervised Task Discovery for Visual Question Answering"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1", "title": "Born Again Neural Networks"}, {"paperId": "90873a97aa9a43775e5aeea01b03aea54b28bfbd", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "8a8224266b8ab1483f6548307ab96227147f34da", "title": "Zero-Shot Visual Question Answering"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "5fa973b8d284145bf0ced9acf2913a74674260f6", "title": "Yin and Yang: Balancing and Answering Binary Visual Questions"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "4cc7401df83ff6d280fdf472a4fba473d1104217", "title": "Zero-Shot Detection via Vision and Language Knowledge Distillation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "7adef3d0200207baec75e39bbb852cacfaf8268b", "title": "Learning to Specialize with Knowledge Distillation for Visual Question Answering"}, {"paperId": null, "title": "Title Suppressed Due to Excessive Length 17"}]}