{"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.", "venue": "arXiv.org", "year": 2023, "citationCount": 230, "influentialCitationCount": 44, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.17453", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more."}, "embedding": {"model": "specter_v2", "vector": [0.42381951212882996, 0.556252658367157, -0.6038306355476379, 0.027768881991505623, -0.7892288565635681, -0.43255850672721863, 0.6015783548355103, 0.1383572220802307, -0.45422378182411194, -0.3212187886238098, 0.5727067589759827, 0.13360154628753662, 0.7242481112480164, 0.17690221965312958, -0.22750940918922424, 0.45251229405403137, -1.2857933044433594, 0.21022041141986847, 0.18502171337604523, -0.25091180205345154, -0.22878430783748627, -0.9962087869644165, -0.8551307916641235, 0.32883554697036743, 0.6388558745384216, 0.14737209677696228, 0.024729955941438675, 1.2174557447433472, -0.39602166414260864, 0.5682926177978516, 0.22761580348014832, -0.1534111499786377, 0.14107179641723633, -0.19628891348838806, -0.4909902513027191, -0.4031130373477936, 0.12061074376106262, -0.4981212019920349, -0.4100310802459717, 0.4893459379673004, -0.27558839321136475, 0.291934072971344, -0.26951736211776733, -0.7337567806243896, -0.061181556433439255, 1.1191294193267822, 0.5713789463043213, 0.5248706340789795, 0.13309936225414276, -0.6235634088516235, 1.4402415752410889, -1.3909863233566284, 0.23253783583641052, 1.5462579727172852, 0.39461833238601685, 0.4851766526699066, -0.17365291714668274, -0.32686686515808105, 0.9961131811141968, 0.12606212496757507, -0.7129048705101013, -0.5813136696815491, -0.01603870466351509, 0.2383752018213272, 1.8196055889129639, 0.15263190865516663, 0.5490133762359619, 0.375786691904068, -0.6137313842773438, 1.9071035385131836, -0.5179831981658936, -0.70183265209198, -0.2313351184129715, 0.2608647048473358, 0.22617341578006744, 0.5603136420249939, -0.5638189315795898, -0.22138568758964539, -1.131052017211914, -0.3414481282234192, 0.18492475152015686, -0.20110563933849335, 0.17417298257350922, -0.14813576638698578, -0.29483985900878906, 0.7878876328468323, -0.09001175314188004, 0.5398932099342346, -0.2098396122455597, 0.5496021509170532, 0.8149614930152893, 0.260587602853775, 0.39983075857162476, 0.23706185817718506, -0.2197250872850418, -0.30458176136016846, -0.791296124458313, 0.47337454557418823, 0.11658013612031937, 0.9268526434898376, -0.47736212611198425, 0.1301356554031372, -0.6798216700553894, 0.4513484537601471, 1.1513527631759644, 0.06669790297746658, 0.35977986454963684, -0.8551557064056396, 0.5094714164733887, -0.7357624173164368, 0.26320716738700867, -0.2521057724952698, -0.15089824795722961, 0.10984348505735397, -0.31076374650001526, -1.5823620557785034, -0.3245582580566406, -0.247656911611557, -0.27470454573631287, 0.7159311771392822, 0.005722965579479933, 0.2229873389005661, 0.193537637591362, 0.17913125455379486, 0.6112280488014221, 0.9745341539382935, 0.15289922058582306, -0.3826863467693329, 0.7874041795730591, -0.9183369874954224, -0.7465581297874451, -1.3477396965026855, 1.0078952312469482, -0.08247791975736618, 0.39174678921699524, -0.002472993917763233, -1.6625536680221558, -0.6606135368347168, -0.6810343265533447, 0.17270006239414215, -0.35600125789642334, -0.04900728538632393, 1.1355546712875366, 0.4703523814678192, -0.7641140222549438, 0.8009553551673889, -0.5435547828674316, -0.31280866265296936, 0.4087997376918793, 0.27302953600883484, 0.32509151101112366, -0.25233426690101624, -1.2676503658294678, 0.194244384765625, -0.06576929986476898, -0.5583016872406006, 0.0005063859280198812, -0.7586587071418762, -1.2268962860107422, -0.38893407583236694, 0.258452832698822, 0.3184124827384949, 1.5111868381500244, -0.20960652828216553, -1.5283325910568237, 0.3238024115562439, -0.5264356732368469, -0.13950379192829132, 0.40213707089424133, -0.6955006122589111, -0.5116466879844666, -0.26958903670310974, -0.27344709634780884, 0.2544502913951874, 0.21216651797294617, 0.10216467082500458, -0.43892547488212585, 0.1939183920621872, -0.28699764609336853, -0.14742536842823029, -0.42532584071159363, 0.9368642568588257, -0.4463665783405304, 0.16184991598129272, 0.130764901638031, 0.7892470359802246, -0.14477424323558807, -0.4663088619709015, -0.2968093752861023, -1.193704605102539, 0.9250059127807617, -0.11837735772132874, 1.4115628004074097, -0.7107853889465332, -0.5095271468162537, -0.3355655074119568, -0.389764666557312, 0.34692251682281494, -0.7023800015449524, 1.1105018854141235, -0.10303962230682373, 0.2918255925178528, -0.04823075607419014, -1.2573812007904053, -0.01589844562113285, -0.25414374470710754, -0.679684579372406, -0.19437432289123535, 0.32522907853126526, 1.0629006624221802, -1.047541856765747, 0.0787176787853241, -0.0748390257358551, 0.2144591063261032, -1.2325513362884521, 1.5252892971038818, -0.582757294178009, 0.15528202056884766, -0.22578823566436768, -0.23118332028388977, 0.1873604655265808, -0.05788756534457207, 0.9032797813415527, -0.043695226311683655, -0.4721681475639343, 0.48135414719581604, -0.6223605871200562, 1.3667469024658203, -0.33443984389305115, 0.44393473863601685, -0.08962893486022949, -0.7498387694358826, 0.12556181848049164, 0.4926687180995941, -0.11432360112667084, -0.4372555613517761, -0.14766018092632294, 0.44866564869880676, -0.6871482729911804, -0.06730534881353378, 1.0206730365753174, 1.0268068313598633, -0.3540991544723511, 0.12377376854419708, 0.4998074173927307, -0.10001742839813232, 0.4766335189342499, 0.7312166094779968, 0.5829533934593201, 0.582012414932251, 0.2199101597070694, 0.2562296390533447, 0.3960997760295868, -0.7155961394309998, 0.1813962161540985, 0.42897000908851624, 0.7061834931373596, 0.7666012048721313, 0.6938063502311707, -0.7852802872657776, -0.44188693165779114, 0.4659157693386078, 0.7823336720466614, 1.7170435190200806, -0.15237239003181458, -0.3758064806461334, -1.0366967916488647, -0.26033636927604675, -0.18713697791099548, 0.2337632179260254, -0.3540283143520355, -0.04334411770105362, -0.948477566242218, -0.7627807259559631, 0.9558169841766357, 0.30324506759643555, 0.6024935245513916, -0.626395046710968, -0.3771738111972809, -0.05833476781845093, -0.003866896266117692, -0.8520659804344177, -0.7976529002189636, 0.0958760678768158, -0.504752516746521, 0.14514341950416565, -0.04431277886033058, -0.1705196648836136, -0.17835098505020142, -0.7080092430114746, 0.7760549783706665, -0.2696566879749298, -0.10156025737524033, 0.3483564257621765, 0.4236677289009094, -0.7007319927215576, -0.9456610679626465, 0.2541431486606598, 0.28869131207466125, -0.005353932268917561, -0.0013023995561525226, 0.7860860824584961, -0.014179429970681667, -0.35394877195358276, -0.3735654056072235, 0.3956519663333893, -0.2828349173069, -0.1167791411280632, 0.057027317583560944, -0.8787004351615906, 0.4167802035808563, -1.4276317358016968, 0.7701107263565063, -0.019749905914068222, -0.3870605528354645, 0.25622838735580444, -0.3453717529773712, -0.6728447675704956, 0.28803086280822754, -1.138676643371582, -0.2083742320537567, -1.1543536186218262, 0.18733933568000793, 0.05853315815329552, 0.22811871767044067, 0.27275723218917847, 0.18420517444610596, 0.6124337911605835, -0.21044310927391052, 0.5803871750831604, 0.26788076758384705, -0.34324127435684204, 0.48298579454421997, -0.6231060028076172, 0.25361281633377075, 0.1914617121219635, -0.07185496389865875, -0.1866440325975418, -0.5193641185760498, -1.0921342372894287, -0.3055819571018219, -0.41955140233039856, -0.3936569392681122, -0.28698262572288513, -0.23765142261981964, -0.5318806171417236, -1.0756689310073853, 0.09435035288333893, -1.4231741428375244, -0.6182317733764648, 0.06412171572446823, -0.259659081697464, -0.3628285527229309, -0.8958023190498352, -1.1733900308609009, -1.0075408220291138, -0.5399415493011475, -0.7292202115058899, 0.08367624133825302, 0.15083912014961243, -0.770014226436615, -0.6546714901924133, 0.3244986832141876, -0.4779186546802521, 0.9050853252410889, -0.759200394153595, 0.6221525073051453, -0.1838587373495102, -0.054841361939907074, -0.4406627416610718, 0.4663272500038147, 0.38161027431488037, -0.4340142607688904, 0.2480710744857788, -0.9644606709480286, 0.044328078627586365, -0.8472889065742493, -0.3969399034976959, 0.08115237951278687, 0.3685641884803772, 0.5332584381103516, -0.224970281124115, -0.5397626161575317, 0.09274188429117203, 0.958543598651886, -0.48984605073928833, -0.0949844941496849, -0.09326764196157455, 1.084151268005371, 0.39095017313957214, -0.01570923998951912, 0.7920514345169067, 0.5395194888114929, 0.4680536985397339, 0.12064976245164871, -0.26034194231033325, 0.23108357191085815, -0.7914368510246277, 0.9716362357139587, 1.7569580078125, 0.3086127042770386, -0.20888571441173553, -0.7011104822158813, 0.8630602359771729, -1.6397740840911865, -0.8691056966781616, 0.5178846716880798, 1.0031503438949585, 0.5476832389831543, -0.5521097183227539, 0.17717640101909637, -0.45658573508262634, 0.27660611271858215, 0.48097872734069824, -0.5270344018936157, -1.1858000755310059, 0.22638168931007385, 0.29573875665664673, -0.013516082428395748, 0.8235240578651428, -0.1473199725151062, 0.7507285475730896, 14.687203407287598, 0.791914701461792, 0.12841477990150452, 0.596973717212677, 0.39714378118515015, -0.03059116192162037, -0.41861265897750854, -0.010750600136816502, -1.092400312423706, -0.32953599095344543, 1.4620335102081299, -0.21669408679008484, 0.639050304889679, -0.28279224038124084, 0.41014325618743896, 0.10235567390918732, -0.5622051954269409, 0.13873958587646484, 0.46884483098983765, -1.2031993865966797, 0.29633283615112305, 0.022943008691072464, 0.2322106510400772, 0.6645989418029785, 0.7195847034454346, 0.9463728666305542, 0.4498513638973236, -0.302094042301178, 0.663475513458252, 0.11468714475631714, 1.128024697303772, -0.21882566809654236, 0.19908761978149414, 0.7058228254318237, -0.7728326320648193, -0.09818708896636963, -0.5179542899131775, -1.2781797647476196, 0.35497424006462097, -0.2917691767215729, -0.5578327775001526, -0.7843552231788635, -0.41689398884773254, 0.7286249399185181, 0.3220391869544983, 0.1904304325580597, 0.29005229473114014, 0.9822741150856018, -0.2282910943031311, -0.14979320764541626, 0.31774818897247314, 0.5630254745483398, 0.21156282722949982, 0.49785053730010986, -0.07440654188394547, 0.05970823019742966, 0.20663368701934814, 0.3637309968471527, -0.6611891984939575, 0.09926024824380875, -0.4137915074825287, -0.4973149299621582, 0.12289215624332428, 0.8093117475509644, 0.8545900583267212, -0.1908562183380127, -0.6781933307647705, 0.34306809306144714, 0.4707590639591217, 3.615915193222463e-05, -0.11738009750843048, 0.135701984167099, 0.41235023736953735, -0.20641334354877472, -0.06993956118822098, 0.5935667753219604, 0.21088697016239166, -0.5032355785369873, -0.8278493285179138, -0.40188243985176086, 0.3370441794395447, -0.2585242688655853, -0.2565017640590668, 0.6042255163192749, -0.09669427573680878, -0.3367977440357208, 0.011648404411971569, -0.19633127748966217, -0.651512086391449, 0.32016849517822266, -1.197261929512024, -0.9348267912864685, 0.7120877504348755, -0.24202996492385864, 0.08675466477870941, 0.45194727182388306, 1.5967745780944824, 0.004679288249462843, -0.5472975969314575, 0.1521187126636505, 0.2550097703933716, -0.016724592074751854, -0.258815199136734, -0.9711974263191223, 1.161460041999817, 0.12425848096609116, -0.06684248894453049, 0.1851363480091095, 0.03965529054403305, 0.20699113607406616, -1.2081564664840698, -0.10649757087230682, 0.8439568877220154, -0.7499904036521912, -0.36760595440864563, -0.9601008296012878, -1.109020709991455, 0.571316659450531, 0.7528664469718933, -0.026563121005892754, 0.43865063786506653, 0.3750637471675873, -0.30205243825912476, -0.016680201515555382, -0.6416088342666626, -0.0007731291116215289, 0.23972533643245697, -0.5018089413642883, 0.15829649567604065, -0.14475156366825104, 0.7193660140037537, -1.1981496810913086, -0.589895486831665, -0.5670925974845886, 0.40615931153297424, -0.011455722153186798, 0.6823383569717407, -0.5681065320968628, 0.5008524656295776, 1.308626413345337, 0.10691332817077637, -0.7523300647735596, 0.44686272740364075, -1.4397472143173218, -0.07110493630170822, 0.5413110256195068, 0.7179091572761536, -0.2985415458679199, 0.11591809242963791, 1.091200351715088, 0.517648458480835, -0.3971830904483795, -0.6296838521957397, 0.01848398894071579, 0.5015488266944885, -0.8035657405853271, 0.3591136038303375, 0.04585479572415352, 0.5636977553367615, 0.1035519540309906, 0.0771910548210144, 0.6335432529449463, -0.01801973022520542, -0.852636992931366, 0.655671238899231, -0.03453619033098221, 0.09009993076324463, -0.21455071866512299, 0.00988539308309555, -1.3859630823135376, -0.2538009285926819, -0.7804257273674011, 0.0063896989449858665, -0.7597264051437378, -0.40487140417099, 0.1017891988158226, -0.00845300406217575, -0.1503545343875885, 0.3243234157562256, -0.3789176642894745, -0.2458033710718155, -0.9254416227340698, -0.9620310068130493, 0.6475138068199158, 1.084283709526062, -0.5579890012741089, -0.1875917762517929, 0.25587189197540283, 0.24626989662647247, 0.4665938913822174, 0.5261713862419128, -0.3843274712562561, -0.5629714727401733, -1.2203757762908936, 0.500205934047699, 0.3621046543121338, -0.24899247288703918, -0.07167850434780121, 0.5145021677017212, 0.0806831493973732, -0.4115884006023407, -0.13610675930976868, 0.4295196831226349, -0.4320365786552429, -0.7691327929496765, 0.20583562552928925, -1.1047171354293823, 0.2549407482147217, 0.3181191682815552, -0.7446697354316711, 0.0019175974885001779, 0.7301878333091736, -0.13417015969753265, -1.0167934894561768, -0.7292705774307251, 0.47408708930015564, -0.8389471173286438, 0.6285402774810791, -0.4312696158885956, 0.0026721761096268892, -0.8301804065704346, -0.5469927191734314, -0.03637072443962097, 0.4292534589767456, -0.5407990217208862, 1.049442172050476, 0.3694416582584381, -0.8667342662811279, -0.3073752224445343, 0.6149313449859619, -0.035382483154535294, 0.24436749517917633, 0.7013529539108276, 0.31230464577674866, -0.09783030301332474, 0.3148263096809387, 0.6720487475395203, 0.14839673042297363, -1.1660345792770386, -0.18196438252925873, 0.45902198553085327, -0.3506937623023987, -0.35495129227638245, 1.1940478086471558, -0.5182824730873108, -1.0071303844451904, 0.001474929042160511, -1.2374532222747803, -0.5347356796264648, -0.30631783604621887, 0.8751240372657776, 0.37867894768714905, -0.21193403005599976, -0.08259756863117218, -0.5683543086051941, -0.06300847977399826, -0.15603281557559967, -0.5045240521430969, 0.6784862279891968, -0.2731342613697052, -0.2881636917591095, 1.1742750406265259, 0.7203588485717773, -0.23876957595348358, -0.6954619288444519, -0.838476836681366, -0.46431633830070496, -0.03117099404335022, 0.14738251268863678, -0.32928696274757385, -0.14015236496925354, 0.9802906513214111, 0.2722263038158417, 0.8556573390960693, -0.3812173306941986, -0.06458614021539688, 0.4294244647026062, 0.3238474130630493, 0.07052574306726456, -0.8003571033477783, -0.620477557182312, 1.235266089439392, 1.1969295740127563, -0.9036181569099426, 0.08656539022922516, 0.28238365054130554, -1.0586968660354614, 0.6640913486480713, 0.19857488572597504, -0.05309441685676575, 0.5977405905723572, -0.14614342153072357, 0.3036459684371948, 0.5936416983604431, -1.5055437088012695, -0.005463253706693649, 0.41452309489250183, 0.8973175883293152, 0.9277663826942444, 0.6627035140991211, 0.13639244437217712, 0.7993680238723755, 0.41531190276145935, 0.24044691026210785, 0.5826029777526855, 0.3415474593639374, -0.19556938111782074, -0.28235283493995667, 0.20606862008571625, 0.370061457157135, -0.7907700538635254, -0.5505058765411377, 0.09999924153089523, 0.271513432264328, 0.13321442902088165, 0.8055868744850159, 0.8993204236030579, 0.2899981439113617, 0.6246011853218079, 0.05701041594147682, 0.30236005783081055, -0.8289128541946411, -0.4007267653942108, -0.12881425023078918, -0.45198455452919006, -0.1168288066983223, -0.03779635950922966, -0.7860711216926575, -0.5646955966949463, -0.41268274188041687, 0.09363308548927307, 0.30755817890167236, -0.08584194630384445, 1.1101549863815308, 0.5621758699417114, 0.3310694992542267, -0.0749288946390152, -0.5079385042190552, -0.45431333780288696, -1.108677864074707, -0.25186312198638916, -0.5774394869804382, 0.13745567202568054, 0.4243299663066864, 0.18043722212314606, -0.5346490740776062]}, "authors": [{"authorId": "2046958974", "name": "Guangxuan Xiao"}, {"authorId": "2249538771", "name": "Yuandong Tian"}, {"authorId": "2249538643", "name": "Beidi Chen"}, {"authorId": "2249530374", "name": "Song Han"}, {"authorId": "2249533054", "name": "Mike Lewis"}], "references": [{"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "d193675b92fbfbf22ed82fda35cd2e73587e33bd", "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "6bef46eccb4c7f521e4f255a01595ebf9994ae22", "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "title": "Benchmarking Large Language Models for News Summarization"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "9001eb3c3d5a96ad3d804410c2437e6f60feade9", "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "da43a455e65f8d1fec2ac72932ac2dd6c6ddc20d", "title": "Evaluating Factuality in Generation with Dependency-level Entailment"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "cc27ec53160d88c25fc5096c0df65536eb780de4", "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "7b36c5602930abf08efd2867f92cdb48a1be757a", "title": "Together"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length"}, {"paperId": null, "title": "Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": null, "title": "Things I\u2019m learning while training superhot"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Attention is off by one, 2023"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "a9075f6332542e12b2bf3cdbdb3a6ed44733fb41", "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}, {"paperId": null, "title": "Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "Llama-2-7b-32k-instruct -and fine-tuning for llama-2 models with together api"}, {"paperId": null, "title": "Chatgpt: Optimizing language models for dialogue"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "A suite for analyzing large"}]}