{"paperId": "f7bc5be4cc305f70b36456a959d7b95669b48ac2", "abstract": "Large language models (LLMs) have recently transformed natural language processing, enabling machines to generate human-like text and engage in meaningful conversations. This development necessitates speed, efficiency, and accessibility in LLM inference as the computational and memory requirements of these systems grow exponentially. Meanwhile, advancements in computing and memory capabilities are lagging behind, exacerbated by the discontinuation of Moore's law. With LLMs exceeding the capacity of single GPUs, they require complex, expert-level configurations for parallel processing. Memory accesses become significantly more expensive than computation, posing a challenge for efficient scaling, known as the memory wall. Here, compute-in-memory (CIM) technologies offer a promising solution for accelerating AI inference by directly performing analog computations in memory, potentially reducing latency and power consumption. By closely integrating memory and compute elements, CIM eliminates the von Neumann bottleneck, reducing data movement and improving energy efficiency. This survey paper provides an overview and analysis of transformer-based models, reviewing various CIM architectures and exploring how they can address the imminent challenges of modern AI computing systems. We discuss transformer-related operators and their hardware acceleration schemes and highlight challenges, trends, and insights in corresponding CIM designs.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "An overview and analysis of transformer-based models of compute-in-memory (CIM) technologies is provided, reviewing various CIM architectures and exploring how they can address the imminent challenges of modern AI computing systems."}, "embedding": {"model": "specter_v2", "vector": [0.4636581242084503, 0.2184709757566452, -0.4145370423793793, 0.289009153842926, -0.14424888789653778, 0.041228361427783966, 0.5879254937171936, 0.013422357849776745, -0.8368744850158691, -0.3536601662635803, 0.1968790590763092, -0.11162685602903366, 0.18044745922088623, -0.36496326327323914, -0.19861985743045807, 0.02144652046263218, -0.7912935018539429, 0.5849108695983887, -0.05954042449593544, -0.00212275143712759, -0.18362516164779663, -0.14523635804653168, -1.4368854761123657, 0.32390856742858887, 0.28761622309684753, 0.9570162296295166, -0.05442606657743454, 1.1145247220993042, -0.3700750470161438, 0.5883328318595886, 0.6283966302871704, -0.08269978314638138, 0.243965283036232, 0.3042926490306854, -0.4643572270870209, -0.9017493724822998, 0.10259875655174255, -0.7369803786277771, -0.780975878238678, 0.8989406824111938, -0.37002238631248474, 0.2582188546657562, 0.36145350337028503, -0.927754282951355, 0.05639955773949623, 0.4881223738193512, 0.5198119878768921, 0.954978346824646, -0.2707025110721588, -0.5426762104034424, 1.38894522190094, -1.25746488571167, 0.11210986226797104, 1.4867125749588013, 0.3362389802932739, 0.09641650319099426, -0.09222573786973953, -0.39075446128845215, 0.21470414102077484, 0.1802762746810913, -0.974217414855957, -0.5682963132858276, -0.00969152245670557, 0.24587157368659973, 1.827467441558838, -0.1259710043668747, 0.2634780704975128, 0.1100987046957016, 0.24632646143436432, 1.5093551874160767, 0.08907205611467361, -1.314160704612732, 0.11711300909519196, -0.2287878394126892, 0.36365264654159546, 1.190555214881897, -0.15214285254478455, -0.03466358035802841, -1.2961770296096802, -0.5828971266746521, 0.5288941860198975, -0.2649765908718109, 0.23454757034778595, -0.15526182949543, -0.5377757549285889, 0.7709346413612366, 0.33964595198631287, 0.4317319989204407, -0.2195838987827301, 0.7692155838012695, 0.6147969961166382, 0.16786739230155945, -0.01381770521402359, 0.08117280155420303, 0.3069494962692261, 0.33967193961143494, -0.8548438549041748, 0.4047277271747589, 0.1837920844554901, 0.9968134164810181, -0.5766963362693787, 0.3046373724937439, -0.7376455664634705, -0.004367687273770571, 1.055477261543274, 0.325758159160614, 0.4451116919517517, -0.39406052231788635, 0.13149960339069366, -0.523309588432312, 0.3665127158164978, -0.21216246485710144, -0.382129430770874, -0.15334804356098175, -1.0039637088775635, -0.9421627521514893, -0.479739248752594, 0.3003123998641968, -0.8768635392189026, 0.2336752563714981, -0.411216139793396, 0.1604008674621582, 0.03671262040734291, 0.30077818036079407, 0.2401193529367447, 0.5654982328414917, 0.2964135706424713, 0.029161393642425537, 1.3948088884353638, -0.8565093874931335, -0.5201774835586548, -1.1633050441741943, 0.5194607377052307, -0.026965295895934105, 0.22671136260032654, -0.20159126818180084, -1.5424741506576538, -0.8487225770950317, -0.7934243679046631, -0.26419517397880554, -0.494566410779953, 0.41924989223480225, 1.237587809562683, -0.018834657967090607, -1.276229977607727, 0.29932448267936707, -0.4991680681705475, 0.18499228358268738, 0.26136791706085205, 0.44106554985046387, 0.5659447312355042, -0.3540550172328949, -1.05586576461792, 0.382797509431839, 0.18357282876968384, -0.6787262558937073, -0.21927833557128906, -0.2746342718601227, -0.8445482850074768, 0.22965219616889954, -0.08675935119390488, -0.7952020168304443, 1.287326455116272, 0.23732805252075195, -0.8886498808860779, 0.7474604249000549, -0.6322013139724731, -0.21873432397842407, -0.04346907511353493, -0.14819012582302094, -0.6590501070022583, 0.08354082703590393, -0.2270241528749466, 0.5394218564033508, 0.6934784054756165, -0.10397525131702423, -0.5207295417785645, 0.0934319868683815, -0.2169797569513321, 0.10891778767108917, -0.2709178924560547, 1.0331233739852905, -0.6242313981056213, -0.0677461326122284, 0.44689762592315674, 0.41423535346984863, -0.3167429268360138, 0.2265302538871765, 0.010944684967398643, -0.8162962198257446, 0.5778593420982361, 0.25977012515068054, 1.340117335319519, -0.9646939039230347, -0.6981894969940186, 0.11531135439872742, 0.16458050906658173, -0.028917182236909866, -0.2739536166191101, 0.3344968259334564, -0.1210676059126854, 0.09001200646162033, -0.06379029899835587, -0.4605799615383148, -0.08539623767137527, -0.40880611538887024, -1.005768895149231, -0.27393123507499695, -0.024198895320296288, 1.122165560722351, -0.6457204818725586, -0.09099642187356949, -0.24044464528560638, 0.06409431993961334, -1.0122079849243164, 1.185949444770813, -0.6208474636077881, -0.028194105252623558, -0.3263084590435028, -0.19726163148880005, 0.20031222701072693, -0.6676208972930908, 0.5865017175674438, -0.6258816123008728, -0.370539128780365, 0.4228990972042084, -0.05939901992678642, 1.3570775985717773, -0.16486382484436035, 0.5316362380981445, 0.01898329146206379, -0.543422520160675, -0.11303259432315826, 0.46488556265830994, -0.4432576596736908, -0.5251677632331848, 0.6206730008125305, 0.41808032989501953, -0.2361154407262802, 0.3841756582260132, 1.008619785308838, 0.9674241542816162, -0.5243690013885498, 0.44023603200912476, 0.10102269798517227, -0.42163169384002686, 0.573948323726654, 0.2993694841861725, 0.5699390769004822, 0.2307063192129135, 0.43621596693992615, -0.15965425968170166, 0.46068075299263, -0.6950839161872864, -0.5744494795799255, 0.7567588090896606, 0.5516434907913208, 0.19417062401771545, 0.1366286724805832, -1.0339035987854004, -0.44525328278541565, -0.011113066226243973, 0.3562048375606537, 1.3927725553512573, 0.017612077295780182, -0.20699654519557953, -0.8686910271644592, 0.18320132791996002, -0.4524570107460022, 0.2736388146877289, 0.3799574673175812, -0.1663084328174591, -0.602313220500946, -1.0172005891799927, 1.1889299154281616, 0.5817574858665466, 1.1384546756744385, -0.5917608141899109, -0.9068844318389893, -0.3963060975074768, 0.08146775513887405, -0.9625387787818909, -0.1876697689294815, 0.3519620895385742, -0.7536197900772095, 0.4157867431640625, 0.2701781094074249, -0.24242323637008667, 0.3276747763156891, -0.548176646232605, 0.9564160704612732, -0.2956084907054901, -0.5983924269676208, -0.010024353861808777, 1.0886932611465454, -0.7605467438697815, -0.6762929558753967, -0.023999467492103577, 0.3538838028907776, -0.22533801198005676, 0.7599227428436279, 0.19292955100536346, 0.09435254335403442, -0.48765695095062256, -0.3858872354030609, 0.4262259006500244, 0.012863713316619396, 0.06017589941620827, 0.5299959182739258, -0.19818763434886932, -0.3497311472892761, -0.563625156879425, 0.9438250660896301, 0.39833611249923706, -0.5252951383590698, 0.2670411765575409, -0.6202231049537659, 0.11626733094453812, 0.6786925792694092, -0.6279866099357605, -0.19052696228027344, -0.8450713157653809, 0.36334866285324097, -0.38632258772850037, -0.141374409198761, 0.12948234379291534, 0.46281254291534424, 0.025461850687861443, 0.058440666645765305, 0.6220767498016357, 0.24761971831321716, 0.14785660803318024, 0.40023377537727356, -0.4969627261161804, 0.5924134850502014, -0.1034456267952919, -0.07134522497653961, 0.2120126485824585, 0.03785688430070877, -0.5273767113685608, -0.18689391016960144, -0.06093817949295044, -0.17303466796875, -0.3924581706523895, 0.2080143839120865, -0.6508193016052246, -1.1874308586120605, 0.01957450993359089, -1.2115730047225952, -0.12300537526607513, 0.5156463980674744, -0.13315308094024658, -0.18341191112995148, -1.14663827419281, -1.5210779905319214, -0.7956951260566711, -1.1671037673950195, -1.0696686506271362, 0.4651739001274109, 0.21381956338882446, -0.4704491198062897, -0.3640728294849396, -0.46752575039863586, -0.2415839284658432, 0.9871020317077637, -0.854188859462738, 1.2892687320709229, 0.027690941467881203, -0.5269523859024048, -0.31333956122398376, -0.08680155873298645, -0.21628744900226593, -0.854520320892334, -0.023844586685299873, -0.9453249573707581, 0.3064451515674591, -0.3814948797225952, -0.2947017252445221, 0.06387770175933838, 0.09964904189109802, 0.8937370181083679, 0.23182904720306396, -0.8773150444030762, 0.04830488562583923, 1.2027511596679688, -0.5209817290306091, -0.02327883429825306, -0.16867373883724213, 0.8852918148040771, -0.5687208771705627, -0.21858134865760803, 0.7643882632255554, 0.060946568846702576, 0.6736069917678833, -0.11037769168615341, -0.16290780901908875, 0.2959800064563751, -0.19773277640342712, 0.3788429796695709, 1.417751431465149, 0.12326335161924362, -0.22183707356452942, -1.0311921834945679, 0.32128703594207764, -1.268419623374939, -0.44184327125549316, 0.5286893844604492, 0.8698256611824036, 0.1905893236398697, 0.3032083511352539, -0.3492298424243927, 0.10374767333269119, 0.28835999965667725, 0.12747961282730103, -0.3798328936100006, -1.0316940546035767, 0.09917115420103073, 0.6924110651016235, 0.42089954018592834, 0.45619747042655945, -0.4813683331012726, 0.7260642051696777, 15.039388656616211, 1.0602343082427979, 0.09815315902233124, 0.500670850276947, 0.5190135836601257, 0.20066019892692566, -0.4124394655227661, -0.14012928307056427, -1.0541937351226807, -0.17439736425876617, 1.4530317783355713, -0.28183284401893616, 0.713232696056366, 0.3429214358329773, -0.2867822051048279, -0.04859576001763344, -0.6610993146896362, 0.6284928917884827, 0.6071082949638367, -1.385008454322815, 0.692070484161377, 0.08984839916229248, -0.14439481496810913, 0.4588572382926941, 0.5451217293739319, 0.5107563734054565, 0.45111343264579773, -0.5890407562255859, 0.6145606637001038, 0.28270408511161804, 1.033609390258789, -0.3093199133872986, 0.15081658959388733, 0.8724382519721985, -1.3965415954589844, -0.07727246731519699, -0.5297114253044128, -1.4561214447021484, 0.22478272020816803, 0.12773340940475464, -0.5042775273323059, -0.48158785700798035, -0.3743746876716614, 0.16061809659004211, 0.5013735294342041, 0.0816706195473671, -0.07195548713207245, 0.40726742148399353, -0.5203033089637756, 0.024937007576227188, 0.09935270249843597, 0.2029341161251068, -0.061288196593523026, -0.27851709723472595, 0.03337151184678078, 0.03785485774278641, 0.23662635684013367, 0.597450852394104, -0.33736589550971985, -0.4194425642490387, -0.5921174883842468, -0.42114758491516113, 0.05223853886127472, 0.6859110593795776, 0.22290386259555817, 0.19850505888462067, -0.5165165662765503, 0.5474144220352173, 0.7405012845993042, 0.05717924237251282, -0.5304116606712341, 0.020196393132209778, 0.04243931919336319, -0.6251159906387329, -0.16276875138282776, 0.42371049523353577, -0.33517736196517944, -0.5174607634544373, -0.6755504012107849, -0.6111218929290771, 0.5089027285575867, -0.657494068145752, -0.44785791635513306, 1.0132168531417847, -0.11992570757865906, -0.46375638246536255, 0.3645341694355011, -1.1196517944335938, -0.17067112028598785, 0.44378533959388733, -0.9773381948471069, -0.7897563576698303, 0.4795604646205902, -0.29327306151390076, -0.3365749418735504, 0.0696045532822609, 1.7659834623336792, 0.2416245937347412, -0.15936048328876495, 0.053138092160224915, -0.030857020989060402, -0.1685858517885208, -0.7449955344200134, -0.21453261375427246, 1.052812933921814, 0.417915940284729, 0.06621013581752777, 0.3834533393383026, -0.14979185163974762, 0.16804535686969757, -1.0863018035888672, 0.37551191449165344, 0.8364277482032776, -0.8422092795372009, -0.24500508606433868, -0.8493615388870239, -0.5647274851799011, 0.4636087417602539, 0.15751312673091888, -0.299833208322525, 0.18481895327568054, -0.30295830965042114, -0.39646008610725403, -0.00549369165673852, -0.46648478507995605, 0.5028497576713562, 0.5911422967910767, -1.094525694847107, 0.030122261494398117, 0.005689552519470453, 0.17472532391548157, -1.2421393394470215, -0.6297569870948792, 0.0015104810008779168, 0.35057103633880615, 0.0479852519929409, 1.3238245248794556, -0.2461550384759903, 0.7248866558074951, 0.5320931077003479, 0.11211757361888885, -0.15350352227687836, -0.13222309947013855, -0.7160873413085938, -0.3582582175731659, -0.48951470851898193, 0.5071830749511719, -0.15343688428401947, 0.2236333191394806, 1.1556580066680908, 0.5071656107902527, -0.40609222650527954, -0.8218697309494019, 0.11061252653598785, -0.23830270767211914, -0.7072490453720093, 0.18630462884902954, -0.5063542723655701, 0.20896874368190765, 0.2072407752275467, 0.18025097250938416, 0.6493276357650757, -0.2734275460243225, -0.19414815306663513, 0.31428220868110657, -0.09580987691879272, 0.14117063581943512, -0.6083250045776367, -0.31562212109565735, -1.5579240322113037, 0.10605701059103012, -1.317082166671753, -0.06309614330530167, -0.7778183221817017, -0.33152472972869873, 0.038764018565416336, 0.10545818507671356, -0.1056743934750557, 0.3645718991756439, -0.25760558247566223, -0.6229166984558105, -0.2562336027622223, -0.43699225783348083, 0.5652413368225098, 0.6660513877868652, -0.6163809895515442, 0.29452407360076904, -0.1526106894016266, 0.6672787666320801, 0.5373240113258362, 0.37630346417427063, -0.1740068793296814, -0.9075426459312439, -1.1601245403289795, 0.3963295817375183, 0.053177524358034134, -0.03566623479127884, -1.0036581754684448, 1.0064384937286377, 0.35352572798728943, -0.18998681008815765, 0.046171922236680984, 0.6749460697174072, -0.9957452416419983, -0.41038164496421814, 0.68710857629776, -0.8525190353393555, 0.3799890875816345, 0.6019929647445679, -0.7110270261764526, -0.11137622594833374, 0.5895246267318726, -0.2684282958507538, -0.8707138895988464, -0.8314841389656067, 0.23457738757133484, -0.8879384994506836, 0.03271682187914848, -0.1379362791776657, 0.08811397850513458, -0.9767425656318665, -0.20534449815750122, 0.37191760540008545, -0.008047428913414478, -0.35507047176361084, 0.7994253635406494, 0.48514223098754883, -1.010409951210022, 0.16640998423099518, 0.729926586151123, -0.2261112630367279, -0.05380091443657875, 0.2684326469898224, 0.47259557247161865, -0.6655975580215454, 0.6572242379188538, 0.16314110159873962, 0.28204405307769775, -1.07200288772583, 0.28712019324302673, 0.26723113656044006, -0.6741347908973694, -0.30788061022758484, 0.977964460849762, -0.5062885284423828, -0.7098529934883118, -0.05553337559103966, -1.0991498231887817, -0.2789249122142792, -0.7507826089859009, 0.6423380374908447, 0.04821079596877098, -0.12109585851430893, 0.1517653614282608, -0.667547345161438, -0.010969297029078007, 0.04627922549843788, -0.861203134059906, -0.16249394416809082, 0.11687985062599182, -0.7788307666778564, 0.5478070974349976, 0.3802478611469269, -0.4123680889606476, -0.10748669505119324, -0.7254632711410522, -0.3035876452922821, 0.05091781169176102, 0.4692417085170746, 0.033717215061187744, -0.38466307520866394, 0.9025167226791382, 0.46258774399757385, 0.2136593610048294, 0.21213398873806, -0.30814382433891296, 0.42459216713905334, 0.6397141814231873, 0.5779160261154175, -0.6934937238693237, -0.958571195602417, 1.3448151350021362, 0.8820761442184448, -0.47798576951026917, 0.466097891330719, -0.4716378152370453, -0.49156445264816284, 1.0052921772003174, 0.27797791361808777, 0.18811169266700745, 0.9115749001502991, 0.613020658493042, -0.05580081418156624, 0.11852318793535233, -0.9438029527664185, -0.11344565451145172, 0.8768604397773743, 0.5106607675552368, 0.9198654890060425, 0.4918576180934906, -0.014958390034735203, 0.8916459083557129, 0.03954780846834183, 0.7266934514045715, 0.13736207783222198, 0.8170385956764221, -0.18394899368286133, 0.13293145596981049, -0.30606740713119507, 0.8572989702224731, -0.32756295800209045, -1.1021853685379028, 0.3683769106864929, 0.5479261875152588, -0.0372220054268837, 0.5331923365592957, 1.3781239986419678, 0.06811171025037766, 0.15312451124191284, 0.3237875998020172, 0.6174597144126892, -0.19997186958789825, -0.0677410215139389, -0.2264905869960785, -0.3364097774028778, 0.2383253574371338, -0.07587680220603943, 0.04720064997673035, -0.9675068259239197, -0.42021408677101135, 0.33094149827957153, -0.061017341911792755, 0.1452663540840149, 0.9716602563858032, 0.8855754137039185, 0.5635351538658142, -0.9003147482872009, -0.3726365864276886, -0.2829233407974243, -0.7434300184249878, 0.3511519432067871, -0.6558020710945129, -0.49234738945961, -0.04716164618730545, 0.04728934168815613, -0.31602704524993896]}, "authors": [{"authorId": "2199013702", "name": "Christopher Wolters"}, {"authorId": "2306067125", "name": "Xiaoxuan Yang"}, {"authorId": "2305816321", "name": "Ulf Schlichtmann"}, {"authorId": "2231831", "name": "T. Suzumura"}], "references": [{"paperId": "e8ad063c3ba3e9bd4aa75ee2bc89629a77532caa", "title": "An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference"}, {"paperId": "c3dda6382a7682a5bad969f32adc29329ae5c4b5", "title": "HARDSEA: Hybrid Analog-ReRAM Clustering and Digital-SRAM In-Memory Computing Accelerator for Dynamic Sparse Self-Attention in Transformer"}, {"paperId": "56d719999ecb20bfdf54407dae13cd1ecbe4ae8e", "title": "Zero-Space Cost Fault Tolerance for Transformer-based Language Models on ReRAM"}, {"paperId": "3695739b3a8b0e92b8ae90081124d098ae33b15c", "title": "FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs"}, {"paperId": "5473f76e92a553c25a95293fab5f861a055bbf60", "title": "MulTCIM: Digital Computing-in-Memory-Based Multimodal Transformer Accelerator With Attention-Token-Bit Hybrid Sparsity"}, {"paperId": "dfd4071ad86b31811b4431cdb21931e0ced99ed5", "title": "A Heterogeneous Chiplet Architecture for Accelerating End-to-End Transformer Models"}, {"paperId": "2463dedcd8e3fe806e561b88899ca593400d7019", "title": "Design of Analog-AI Hardware Accelerators for Transformer-based Language Models (Invited)"}, {"paperId": "523537c15fa4adaa884bbad40075b8ddbf003b9a", "title": "A Hardware Evaluation Framework for Large Language Model Inference"}, {"paperId": "c5c3140852779e61b4b5469b3e84a4bcff1433e8", "title": "RACE-IT: A Reconfigurable Analog CAM-Crossbar Engine for In-Memory Transformer Acceleration"}, {"paperId": "6f9f20df891e8b6246808dac19460de449b84019", "title": "MGen: A Framework for Energy-Efficient In-ReRAM Acceleration of Multi-Task BERT"}, {"paperId": "dc054c3a7fd783ff6024efdb05b95e1ffb73dfa6", "title": "Neural inference at the frontier of energy, space, and time"}, {"paperId": "11dae14829a470bd853cbeb0eedb628dace6ef11", "title": "\u2018Mind-blowing\u2019 IBM chip speeds up AI"}, {"paperId": "87c1cd5f11182a91b46e7cfd0f7ff9da6c4967a6", "title": "PIM-GPT: A Hybrid Process-in-Memory Accelerator for Autoregressive Transformers"}, {"paperId": "bc1a6759e62a1f62345fc2bb88c2661c3300e31e", "title": "Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators"}, {"paperId": "3e6a74a3e96133a3f488b50dde2d1f6d04d78a94", "title": "Large language models and brain-inspired general intelligence"}, {"paperId": "4ea8e22236681a09225ee3f8ff5fffd934ec9bae", "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference"}, {"paperId": "5b0fe50dc6df8f4eba13f8177dcd4bbe5a2b0e23", "title": "A Survey of Techniques for Optimizing Transformer Inference"}, {"paperId": "8d5ec8ee881d85fde646e522a14f75a34da1348f", "title": "Three Challenges in ReRAM-Based Process-In-Memory for Neural Network"}, {"paperId": "56b85672e02fb1800c2f7a529c756f8e4ce4e40a", "title": "Memory-Centric Computing"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "bd5c24ec7d3c91648fb0beec6508a5302823f97b", "title": "X-Former: In-Memory Acceleration of Transformers"}, {"paperId": "083e52983cd99b5726e3db8eb476aecc061bbf3b", "title": "Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators"}, {"paperId": "e74484896fbad57b91d493243aac6fd9658838b5", "title": "In-Memory Computing Accelerators for Emerging Learning Paradigms"}, {"paperId": "f005194130f4a7f651e6aeb3af61ae46c5ccc462", "title": "Biologically Plausible Learning on Neuromorphic Hardware Architectures"}, {"paperId": "e28c05440f28c2092a251cf413f2a9d3c90fd1bb", "title": "A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference"}, {"paperId": "b4d600d7d28f32b527aa2326f72eafc3c51256e0", "title": "Compute-in-Memory Technologies and Architectures for Deep Learning Workloads"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "cbbc4df4edb7f2c71334486580a2fb40a6fb5ccc", "title": "Analog-memory-based 14nm Hardware Accelerator for Dense Deep Neural Networks including Transformers"}, {"paperId": "7502cf8c1c79f33b8880d62d0fb40ae81db9c3e3", "title": "Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models"}, {"paperId": "47596705a00601eca040d82b8fe25457f2683bd1", "title": "Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator"}, {"paperId": "6f3ba20fcd66d26202d5cd690493828da8736f00", "title": "Full-Circuit Implementation of Transformer Network Based on Memristor"}, {"paperId": "f5edad3a50ba8a6329d202f31677d988f1d0cf87", "title": "TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer"}, {"paperId": "78bb909c314784ff345fe2bea997e74ca08ee0e5", "title": "A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture"}, {"paperId": "a82b020fb5358a69173201a276aa4ea18c9dd0cd", "title": "Toward Software-Equivalent Accuracy on Transformer-Based Deep Neural Networks With Analog Memory Devices"}, {"paperId": "25fd8b80c0808868b6834e2967c9de786dfb3fda", "title": "In-Memory Computing based Accelerator for Transformer Networks for Long Sequences"}, {"paperId": "99026c277735284df33636455702e9eaf469583b", "title": "ReTransformer: ReRAM-based Processing-in-Memory Architecture for Transformer Acceleration"}, {"paperId": "8e2e2618e48522d0e8f8b90eb6d389de11a8080c", "title": "ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks"}, {"paperId": "a1311f8ea761fe0d74c2de4160e73646e596d650", "title": "There\u2019s plenty of room at the Top: What will drive computer performance after Moore\u2019s law?"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0ff02cddd42e0434ff0e768ab1117ff326e3db6c", "title": "Memory devices and applications for in-memory computing"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4f9ba5e89a7d23675ca65473ae85e352a6d2c379", "title": "Aging-aware Lifetime Enhancement for Memristor-based Neuromorphic Computing"}, {"paperId": "d74bdbc0bce8ff90e815c74368cdac49b0eb4185", "title": "PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference"}, {"paperId": "3d8b62c060f8444907e7c975c6ae590373b51ed4", "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "921da70f380e9f9082e5594b128369cfd0fdf120", "title": "A Neural Network for Factoid Question Answering over Paragraphs"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "7bea855e19fd13461590e4f2d44bbf7b807ce3e3", "title": "A bitter lesson."}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": null, "title": "\u201cBring mem-ristive in-memory computing into general-purpose machine learning: A perspective,\u201d"}]}