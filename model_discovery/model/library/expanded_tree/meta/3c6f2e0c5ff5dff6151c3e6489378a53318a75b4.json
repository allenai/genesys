{"paperId": "3c6f2e0c5ff5dff6151c3e6489378a53318a75b4", "abstract": "As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.", "venue": "arXiv.org", "year": 2024, "citationCount": 25, "influentialCitationCount": 8, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality, and proposes a straightforward pruning approach: layer removal, in which the redundant layers in LLMs are deleted based on their BI scores."}, "embedding": {"model": "specter_v2", "vector": [0.024235233664512634, 0.7782279253005981, -0.4782849848270416, 0.0034660310484468937, -0.3399122357368469, 0.03425110504031181, 0.6457566022872925, -0.40447279810905457, -0.3275315463542938, -0.08476796746253967, 0.6296400427818298, 0.09254679083824158, 0.11513639986515045, 0.2637265920639038, -0.1438431292772293, 0.19489794969558716, -0.8871467113494873, 0.3330060541629791, 0.14585520327091217, -0.28180113434791565, -0.1079711765050888, -0.6322599053382874, -0.8337042331695557, 0.03671922907233238, 0.37972044944763184, 0.6136108040809631, -0.027721000835299492, 0.6353207230567932, -0.4992006719112396, 0.5384886264801025, 0.3089412748813629, -0.36592647433280945, 0.20423004031181335, -0.15942463278770447, 0.06952400505542755, -0.3181198239326477, 0.3190627098083496, -0.3528999388217926, -0.6477071642875671, 0.9459961652755737, -0.11460007727146149, 0.28874990344047546, 0.2980785667896271, -0.6874900460243225, 0.24738886952400208, 1.2497550249099731, 0.5608656406402588, 0.42892250418663025, -0.47960248589515686, -0.5123795866966248, 1.3641740083694458, -1.3309228420257568, 0.18032154440879822, 1.8616787195205688, 0.5506108403205872, 0.3167582154273987, -0.5559474229812622, -0.8806195855140686, 0.8482555747032166, -0.22497710585594177, -1.2096410989761353, -0.5802569389343262, -0.2443736046552658, 0.014902184717357159, 2.0773003101348877, -0.32408955693244934, -0.24542760848999023, 0.3502238094806671, -0.030731836333870888, 1.556449055671692, -0.20313408970832825, -0.48912569880485535, -0.25375208258628845, 0.06994545459747314, 0.2468206137418747, 0.7798560857772827, -0.04274361953139305, 0.24493087828159332, -0.8422569632530212, -0.5113437175750732, 0.10812681168317795, -0.3382992744445801, -0.0016705554444342852, -0.058361392468214035, 0.14930744469165802, 0.7547603845596313, 0.2745850682258606, 0.8322180509567261, 0.033475734293460846, 1.0209459066390991, 0.17512492835521698, 0.48995086550712585, 0.2069360911846161, 0.3074760437011719, -0.24012760818004608, 0.28420713543891907, -1.027212381362915, 0.2600761950016022, 0.24650384485721588, 0.7638412714004517, -0.14726169407367706, 0.29940083622932434, -0.5648978352546692, 0.5034857988357544, 1.606148600578308, -0.007077088113874197, 0.3779824376106262, -0.8936643600463867, 0.5370359420776367, -0.5102964043617249, -0.06271542608737946, -0.6897429823875427, -0.2358819842338562, -0.4268477261066437, -0.687163233757019, -1.0787556171417236, -0.5478857755661011, 0.07945030182600021, -0.5720707178115845, 1.0249888896942139, -0.5489417910575867, 0.41249513626098633, -0.18455399572849274, -0.009748452343046665, 0.4905537962913513, 0.6671425104141235, 0.31140437722206116, 0.030465498566627502, 0.617438018321991, -1.0007133483886719, -0.39225226640701294, -1.3376368284225464, 0.945813000202179, -0.37772586941719055, -0.22164644300937653, -0.09391691535711288, -1.201141595840454, -0.7940818071365356, -1.0029118061065674, 0.32406172156333923, -0.4234466850757599, 0.041782625019550323, 0.6603666543960571, 0.24129056930541992, -0.9121901988983154, 1.1781795024871826, -0.11011934280395508, 0.10749100893735886, 0.4716021418571472, 0.40816137194633484, 0.23846325278282166, -0.4069806933403015, -1.3414398431777954, 0.37328091263771057, 0.13636673986911774, -0.6314541697502136, -0.07330556958913803, -0.6389524340629578, -1.0554462671279907, 0.09114588797092438, 0.41504180431365967, -0.6244114637374878, 1.0987153053283691, 0.05700123310089111, -1.0403931140899658, 0.17505155503749847, -0.40161487460136414, 0.07995319366455078, 0.01977083645761013, -0.005451525561511517, -0.5698403120040894, -0.3030019700527191, -0.5447053909301758, 0.5444262027740479, 0.8203036189079285, -0.07962264865636826, -0.18803411722183228, 0.29616546630859375, -0.2012304961681366, -0.15673883259296417, -0.17405784130096436, 1.026818037033081, -0.7074106931686401, -0.3546030521392822, 0.4008558690547943, 0.4373742938041687, -0.21856749057769775, -0.15828734636306763, -0.2739346921443939, -1.1299700736999512, 0.5764980912208557, -0.22805450856685638, 1.4912022352218628, -0.6806346774101257, -0.5619854927062988, -0.018359536305069923, 0.09393986314535141, 0.16316168010234833, -0.8467334508895874, 0.6026437282562256, -0.29203668236732483, 0.9896806478500366, -0.18217088282108307, -1.2452939748764038, 0.2618658244609833, -0.32114535570144653, -0.4315456449985504, -0.3139491379261017, 0.08401986211538315, 0.8283478021621704, -0.6985162496566772, -0.14817486703395844, -0.1838754415512085, 0.2470695674419403, -0.9848994016647339, 0.7160760760307312, -0.4249485731124878, -0.1034332662820816, -0.23425330221652985, -0.15102748572826385, 0.281096488237381, -0.1718398779630661, 0.23262929916381836, -0.2704690992832184, -0.06405800580978394, 0.7721987962722778, -0.30750346183776855, 1.2472803592681885, -0.1440470665693283, 0.5508285760879517, 0.022261345759034157, -0.12582795321941376, -0.2305794358253479, 0.33805179595947266, -0.14433687925338745, -0.2091992050409317, 0.44654205441474915, 0.26162484288215637, -0.3341270685195923, 0.7294891476631165, 0.34801918268203735, 0.7918707728385925, -0.5335206985473633, 0.525189995765686, 0.588978111743927, -0.48377102613449097, 0.9016055464744568, 0.7003229856491089, 0.6783557534217834, 0.17639116942882538, 0.39271607995033264, 0.16525641083717346, 0.5664457678794861, -1.0102412700653076, 0.2864380478858948, 0.5714882016181946, 0.3718728721141815, 0.7292578220367432, 0.834072470664978, -0.4901583194732666, -0.27581197023391724, 0.225711390376091, 0.7918529510498047, 1.3299062252044678, -0.5540655851364136, -0.7396690249443054, -0.5428518056869507, -0.28494948148727417, -0.05204836279153824, 0.042417723685503006, -0.2507888376712799, -0.20963886380195618, -0.5485974550247192, -1.2580102682113647, 1.0375986099243164, -0.15455862879753113, 0.5822123289108276, -0.3617858290672302, 0.38634732365608215, -0.2779659330844879, 0.416727215051651, -0.7557702660560608, -0.35882318019866943, 0.3351929187774658, -0.6901580095291138, -0.12722675502300262, 0.16351363062858582, 0.22950518131256104, 0.01436492707580328, -0.6552634239196777, 1.0509028434753418, -0.44235527515411377, -0.04193517565727234, -0.4042723476886749, 0.6317134499549866, -0.558567225933075, -1.0078445672988892, 0.16651105880737305, 0.38142266869544983, -0.3457936942577362, 0.35571786761283875, 0.5820246934890747, -0.03481904789805412, 0.33034464716911316, -0.334784597158432, 0.29480302333831787, 0.34111514687538147, -0.34659621119499207, 0.6271218657493591, 0.05007540062069893, -0.02376854233443737, -1.0437617301940918, 1.053566575050354, 0.17646756768226624, -0.604792058467865, 0.3043893575668335, -0.655320942401886, -0.2813717722892761, 0.3134083151817322, -0.1823950558900833, -0.09939050674438477, -1.4432156085968018, 0.451612651348114, -0.29485002160072327, 0.1155228242278099, 0.22327059507369995, 0.3662692904472351, 0.0029072435572743416, -0.02613079734146595, 0.21998341381549835, 0.18621034920215607, -0.6149088740348816, 0.274772584438324, -0.6112434267997742, -0.11587031930685043, 0.5213028192520142, 0.1419827789068222, -0.19252850115299225, -0.27407312393188477, -0.8268524408340454, -0.2568014860153198, -0.3093959391117096, -0.028491709381341934, 0.11227070540189743, -0.17216020822525024, -0.7477338910102844, -0.18520765006542206, -0.0791129544377327, -0.8072498440742493, -0.0064417170360684395, 0.3683710992336273, -0.05913995951414108, 0.2278348058462143, -0.9404577016830444, -1.4880006313323975, -0.505311131477356, -0.6045481562614441, -1.0946786403656006, 0.3161007761955261, -0.13099732995033264, -0.6889524459838867, -0.24460938572883606, -0.20170237123966217, -0.4022676348686218, 0.7750896215438843, -0.8874361515045166, 0.9076424837112427, -0.1004185751080513, -0.24223560094833374, -0.49032989144325256, -0.05327339470386505, 0.929813027381897, -0.34218353033065796, 0.40884989500045776, -0.8415756821632385, -0.07709562033414841, -0.2807983458042145, 0.0072987135499715805, 0.4130239188671112, 0.5047470927238464, 0.642583966255188, 0.07965341955423355, -0.7126687169075012, 0.5031574964523315, 1.1048202514648438, -0.9519356489181519, -0.03215009346604347, -0.26201701164245605, 0.9488446116447449, 0.3566450774669647, -0.461722731590271, 0.36573272943496704, -0.19869744777679443, 0.3149031400680542, 0.046849995851516724, -0.03499547019600868, -0.24487972259521484, -0.7456858158111572, 0.6058744788169861, 2.0265235900878906, 0.4889795780181885, -0.1635858714580536, -0.812827467918396, 0.25510933995246887, -1.0894871950149536, -0.4522106349468231, 0.6431572437286377, 0.7963805198669434, 0.16208523511886597, -0.12846621870994568, -0.6605957746505737, -0.20561331510543823, 0.20922809839248657, 0.5811848044395447, -0.4154568016529083, -0.761763870716095, -0.2529617249965668, 0.26317787170410156, 0.41745278239250183, 0.5609961748123169, -0.34162577986717224, 0.8082599639892578, 15.022073745727539, 1.113634467124939, 0.2640412449836731, 0.38582396507263184, 0.8165374398231506, -0.044729460030794144, -0.37364253401756287, -0.0693485215306282, -1.5028706789016724, 0.11827421188354492, 1.3573592901229858, 0.00436798483133316, 0.588620126247406, -0.09469621628522873, 0.5357425212860107, 0.04540310800075531, -0.21729333698749542, 0.5585172772407532, 0.4240007698535919, -0.9013937711715698, 0.7083098888397217, 0.23794643580913544, 0.4952913522720337, 1.1102795600891113, 0.3672305941581726, 1.0203839540481567, 0.379126638174057, -0.5425558686256409, 0.2131197303533554, 0.23409712314605713, 0.5572981238365173, -0.15477842092514038, 0.25829631090164185, 0.7128980755805969, -0.646152913570404, -0.17693234980106354, -0.7052082419395447, -1.1978027820587158, 0.1508387327194214, 0.2569721043109894, -0.47167330980300903, -0.6777397990226746, -0.3036496937274933, 0.948116660118103, -0.27151915431022644, 0.42945095896720886, -0.3028680682182312, 0.7369425296783447, -0.6011874079704285, 0.09494711458683014, 0.3833043575286865, 0.27694934606552124, 0.5028439164161682, -0.14740526676177979, 0.1096247211098671, -0.5135260224342346, -0.14471527934074402, 0.3989693522453308, -0.7567451000213623, -0.02701784111559391, -0.4348987936973572, -0.5619956254959106, -0.14504094421863556, 0.6247227787971497, 0.47432956099510193, -0.08902142941951752, -0.5448070764541626, 0.049727246165275574, 0.30675745010375977, 0.3548150360584259, -0.2252742499113083, 0.12599648535251617, 0.7989460229873657, -0.5282004475593567, -0.016208168119192123, 0.6722359657287598, -0.1523124724626541, -0.5341692566871643, -0.7853440642356873, -0.3634362518787384, 0.2317032366991043, -0.8898233771324158, -0.4911603033542633, 0.8129336833953857, -0.07719866931438446, -0.2593613862991333, -0.0069675687700510025, -0.5850704312324524, 0.006829975638538599, 0.7860730886459351, -1.3430843353271484, -0.6512024402618408, 0.687973141670227, -0.5558991432189941, -0.3505750298500061, -0.30410608649253845, 1.2317124605178833, 0.3173069357872009, -0.7374818921089172, 0.023806221783161163, 0.31309807300567627, -0.1915489286184311, -0.27472609281539917, -0.40117257833480835, 0.9488745927810669, 0.423505961894989, 0.020225508138537407, 0.5430245995521545, -0.03523428365588188, 0.07802986353635788, -0.26367413997650146, -0.16227282583713531, 1.1002371311187744, -0.6972873210906982, -0.20283958315849304, -0.7054941654205322, -0.7363830208778381, 0.14756813645362854, 0.2124079316854477, -0.40544480085372925, 0.4255480170249939, 0.16720908880233765, -0.6183895468711853, -0.08789025247097015, -0.5122381448745728, 0.29029834270477295, 0.5672585964202881, -0.8123490214347839, -0.12815609574317932, -0.19656629860401154, 0.4160568416118622, -0.3401956558227539, -0.3808591365814209, 0.1682969331741333, 0.06194620579481125, -0.23629766702651978, 0.9429171085357666, -0.5799933671951294, 0.5930351614952087, 0.5715718269348145, -0.17984585464000702, -0.9005621075630188, -0.14149753749370575, -1.1814579963684082, -0.06976038217544556, -0.18280673027038574, 1.0536853075027466, -0.392191082239151, 0.3095247447490692, 0.8098515868186951, 0.33338209986686707, -0.4603393077850342, -0.6954878568649292, -0.15371519327163696, 0.14718608558177948, -0.7781893014907837, 0.4151882827281952, -0.374614417552948, -0.05078395828604698, 0.16283395886421204, 0.49206459522247314, 0.46101996302604675, -0.17372503876686096, -1.011750340461731, 0.2836001217365265, -0.26808100938796997, -0.011170640587806702, -1.0220551490783691, -0.3348087668418884, -1.3257819414138794, -0.013989767991006374, -0.9305993318557739, -0.3523038923740387, -1.063105821609497, -0.46276605129241943, 0.020001163706183434, -0.5297282338142395, -0.2550041675567627, 0.5206920504570007, -0.08268077671527863, -0.3098975419998169, -0.35684406757354736, -0.6805509328842163, 0.7232399582862854, 0.918666422367096, -0.8483559489250183, 0.246785968542099, 0.0038634382653981447, 0.07379722595214844, 0.8941511511802673, 0.5513031482696533, -0.5691652297973633, -0.8624271154403687, -1.4774359464645386, 0.7024992108345032, -0.38926756381988525, -0.5497364401817322, -0.6853548884391785, 0.660834014415741, 0.6320186257362366, -0.28493207693099976, 0.003882484044879675, 0.5926440954208374, -0.666553258895874, -0.49377554655075073, 0.4314262568950653, -0.519790768623352, -0.06799306720495224, 0.16973069310188293, -0.5691075325012207, -0.5216500759124756, 0.3861722946166992, -0.06771539151668549, -1.242565631866455, -0.55843186378479, 0.4375136196613312, -0.6540576219558716, 0.05674997344613075, -0.4226395785808563, -0.18460799753665924, -0.7669231295585632, -0.3685104250907898, -0.21161812543869019, 0.4724912643432617, -0.5594678521156311, 1.1091415882110596, 0.2847961485385895, -0.9656617045402527, -0.17795713245868683, 0.5761475563049316, -0.05375012755393982, -0.26289474964141846, 0.23505428433418274, 0.3899456858634949, -0.5259066820144653, 0.7904108762741089, 0.8579323887825012, 0.4835742115974426, -1.1718342304229736, -0.36250343918800354, 0.9027243256568909, -0.7284197211265564, -0.04621778801083565, 1.3731558322906494, -0.01044587604701519, -1.0164674520492554, 0.29191023111343384, -1.5574852228164673, -0.4113740026950836, -0.001991154393181205, 0.7727282047271729, 0.21307078003883362, 0.13207535445690155, -0.15541428327560425, -0.29847028851509094, -0.08487678319215775, 0.08914906531572342, -0.4441186785697937, 0.43105146288871765, -0.40285810828208923, -0.6832003593444824, 0.6516450643539429, 1.0513707399368286, -0.6466977596282959, -0.5291978120803833, -0.6927043199539185, -0.29234951734542847, 0.04426981881260872, 0.6244558691978455, -0.3527352213859558, -0.35674503445625305, 0.751446008682251, 0.2258043885231018, 0.2718793749809265, -0.2255890816450119, -0.480400025844574, 0.1524939090013504, 0.5295411944389343, 0.19685696065425873, -0.9269611239433289, -1.006542444229126, 1.501651644706726, 1.3344248533248901, -0.9826939105987549, 0.5873422622680664, -0.10497787594795227, -0.36510834097862244, 0.34522122144699097, 0.06220259144902229, -0.01274288073182106, 1.322965383529663, -0.09930848330259323, 0.06437811255455017, 0.20812541246414185, -1.3881213665008545, -0.301777720451355, 0.8751172423362732, 0.3904421627521515, 0.9157926440238953, 0.21353459358215332, -0.15360704064369202, 0.8666346073150635, 0.06548836082220078, 0.044699184596538544, 0.14720427989959717, 0.32635411620140076, -0.3347564935684204, -0.09405095130205154, 0.0751817524433136, 0.7865203022956848, -0.7383430600166321, -0.907943069934845, -0.2588886320590973, 0.5996749997138977, 0.3564838171005249, 0.634974479675293, 0.7308222055435181, -0.005155791994184256, 0.3066302537918091, 0.36785605549812317, 0.4760778844356537, -0.4924135208129883, -0.33316028118133545, 0.16581234335899353, -0.2353026568889618, -0.060809869319200516, 0.0835479274392128, -0.2710955739021301, -0.06708453595638275, -0.5135848522186279, 0.22580115497112274, 0.06936720013618469, 0.3925163149833679, 0.9009510278701782, 0.6952331066131592, 0.5591486692428589, -0.34919440746307373, -0.4034442603588104, -0.39900529384613037, -1.297483205795288, -0.23319345712661743, -0.5674164891242981, -0.5271340608596802, -0.02007438801229, -0.5274593830108643, -0.8808261752128601]}, "authors": [{"authorId": "2242124145", "name": "Xin Men"}, {"authorId": "2290224627", "name": "Mingyu Xu"}, {"authorId": "2290140556", "name": "Qingyu Zhang"}, {"authorId": "2242351985", "name": "Bingning Wang"}, {"authorId": "2116455765", "name": "Hongyu Lin"}, {"authorId": "1831434", "name": "Yaojie Lu"}, {"authorId": "2118233348", "name": "Xianpei Han"}, {"authorId": "2290136725", "name": "Weipeng Chen"}], "references": [{"paperId": "7d00e7337fbbccd0208361b4c204c75239a96521", "title": "LaCo: Large Language Model Pruning via Layer Collapse"}, {"paperId": "7754ac3e8ff1286f17593159781487543cdddaba", "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"}, {"paperId": "d6ee0b9ecd89be38b7a1d94ecdb08ce510469b5d", "title": "Weight subcloning: direct initialization of transformers using larger pretrained ones"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "af67be0fff8d087a0d8554b6e8998ab12409bbda", "title": "TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df", "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599", "title": "Explanations from Large Language Models Make Small Reasoners Better"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "c96470f0cc2791e0847a20a8309e19a5feab0a76", "title": "A New Measure of Model Redundancy for Compressed Convolutional Neural Networks"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "ecf5618b513aa5c4d5bf62ca251923a188251117", "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"}, {"paperId": "57ed901be5d1b4d853d4f8998dadc1b60e2151f9", "title": "On Attention Redundancy: A Comprehensive Study"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "e278e072774f23675266881750e20bca74804cb9", "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "1b07a24b81834116f6ad1d0232485ba81b9445f3", "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"}, {"paperId": "78b29eba4d6c836483c0aa67d637205e95223ae4", "title": "Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "8dd85e38445a5ddb5dd71cabc3c4246de30c014f", "title": "A Survey of Model Compression and Acceleration for Deep Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "15a8372fe06e8ab09e36a4c2db5c75a2b51a1207", "title": "Detecting parameter redundancy"}, {"paperId": "72c03b873e8c5cd86b15bf73186df341da4731c9", "title": "Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"}, {"paperId": "d7db793f9824a179518a3eb4ea3bd8bc620f6b6f", "title": "Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "2023. Massive language models can be accurately pruned in one-shot"}, {"paperId": null, "title": "2022. A survey of quantization methods for efficient neural network inference"}, {"paperId": null, "title": "2024. Llm-pruner: On the structural pruning of large language models"}]}