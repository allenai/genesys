{"paperId": "a56453c98c6244329f660d4fd74e2afd4fe7d349", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge developments in MoE research, we have established a resource repository accessible at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The structure of the MoE layer is introduced, followed by proposing a new taxonomy of MoE, and the core designs for various MoE models including both algorithmic and systemic aspects are overviewed, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations."}, "embedding": {"model": "specter_v2", "vector": [0.026156844571232796, 0.33286914229393005, -0.5181498527526855, -0.22757446765899658, -0.5727116465568542, -0.22690711915493011, 0.6390832662582397, -0.4165465533733368, -0.6618479490280151, -0.3350284695625305, 0.5592010021209717, 0.2978775203227997, 0.3981858491897583, 0.7551083564758301, -0.2719529867172241, 0.2401193529367447, -0.819684624671936, 0.3798238933086395, -0.1082242876291275, -0.41580575704574585, -0.5027228593826294, -0.7787469625473022, -0.959242045879364, 0.11514502018690109, 0.543585479259491, 0.4343491494655609, 0.40726667642593384, 0.9762970805168152, -0.47746846079826355, 0.10859714448451996, 0.694068193435669, -0.37764954566955566, 0.4076022207736969, 0.13974100351333618, 0.11831915378570557, 0.2964496612548828, 0.5909404158592224, -0.5459676384925842, -0.823010265827179, 0.7103173732757568, 0.006955117918550968, 0.3984321653842926, 0.5969400405883789, -0.6331414580345154, -0.22005799412727356, 1.1780813932418823, 0.6709631681442261, 0.7004891633987427, -0.4915751516819, -0.5224924683570862, 1.4052021503448486, -1.662995457649231, 0.07414823025465012, 1.766457438468933, 0.55776447057724, 0.29548969864845276, -0.6059285402297974, -0.8258925080299377, 1.0243092775344849, -0.18874701857566833, -0.7821515202522278, -0.5185991525650024, -0.01526205986738205, -0.08575449883937836, 1.9774222373962402, -0.5156510472297668, -0.025954516604542732, 0.7927238941192627, 0.02970896288752556, 1.6722770929336548, -0.06654444336891174, -0.8117929100990295, -0.22477179765701294, 0.32105880975723267, 0.303826242685318, 0.7450331449508667, -0.5774624347686768, 0.4374476373195648, -1.031830072402954, -0.3049914836883545, 0.126090869307518, -0.21056164801120758, -0.0047073508612811565, -0.21533618867397308, -0.22249221801757812, 0.9310153126716614, 0.28007858991622925, 0.8834188580513, -0.16536898910999298, 0.6150055527687073, 0.21986337006092072, 0.08718662708997726, 0.14249184727668762, 0.5466366410255432, -0.3813754916191101, 0.7256767153739929, -0.8993515372276306, 0.26574572920799255, 0.4027296006679535, 0.8955186009407043, -0.3658554255962372, 0.24090923368930817, -0.6358824968338013, 0.5208679437637329, 1.4845848083496094, -0.04513782635331154, 0.32363662123680115, -0.8372200131416321, 0.23664484918117523, -0.7373576164245605, 0.2296670824289322, -0.5752713680267334, -0.13260240852832794, -0.56448894739151, -0.7925800085067749, -1.1713855266571045, -0.6289240121841431, 0.3628461956977844, -0.7264247536659241, 0.9556249976158142, -0.4087372422218323, 0.25855207443237305, 0.12364737689495087, 0.6621620655059814, 0.39030396938323975, 0.6615331768989563, 0.5737683773040771, 0.20623308420181274, 0.7190060615539551, -1.0200361013412476, -0.6113417744636536, -1.4058908224105835, 0.5974138975143433, -0.39116549491882324, 0.027654843404889107, -0.08349153399467468, -0.9092472195625305, -0.9057617783546448, -0.8748498558998108, 0.04873673990368843, -0.36968502402305603, 0.9030327796936035, 0.9245793223381042, 0.3537500500679016, -1.0749729871749878, 0.44161394238471985, 0.0652596652507782, 0.1385708898305893, 0.22209887206554413, 0.39667606353759766, -0.10738825798034668, -0.6724445223808289, -1.3961522579193115, 0.38532504439353943, 0.7108950018882751, -0.3977881073951721, -0.5306409597396851, -0.3203417956829071, -0.9132639765739441, 0.00324018276296556, 0.19496367871761322, -0.6384132504463196, 1.179944634437561, -0.5344997644424438, -1.2363016605377197, 0.6810830235481262, -0.3702593743801117, 0.027256706729531288, 0.3780539929866791, -0.21012401580810547, -0.8290684223175049, -0.6449763178825378, -0.39558520913124084, 0.8748909831047058, 0.589505672454834, -0.3023492693901062, -0.1879837065935135, 0.2739010453224182, -0.17105193436145782, 0.03445086255669594, -0.3792381286621094, 0.8584733009338379, -0.5055775046348572, -0.4100559651851654, 0.6432616114616394, 0.47536882758140564, -0.41364365816116333, -0.2760719656944275, -0.3516375720500946, -1.0049896240234375, 0.6486496925354004, -0.09004853665828705, 0.9043550491333008, -0.7692209482192993, -0.6795768737792969, -0.11666339635848999, 0.13458578288555145, -0.010397029109299183, -1.1935404539108276, 0.7576650977134705, -0.3107607364654541, 0.1857030987739563, -0.3329900801181793, -1.5989712476730347, 0.17098893225193024, -0.2688179612159729, -0.6166514158248901, -0.01985830068588257, 0.3088679015636444, 0.8025436401367188, -0.9803707599639893, 0.2141760289669037, -0.06292863935232162, 0.40909549593925476, -1.0734286308288574, 0.9563291668891907, -0.6011033654212952, 0.15203848481178284, 0.0950770229101181, -0.274400919675827, 0.298084020614624, -0.30525025725364685, 0.6802060604095459, -0.3393521010875702, 0.20546850562095642, 0.3685606122016907, -0.8630843162536621, 1.4439656734466553, -0.34977662563323975, 0.6385082006454468, 0.07761317491531372, -0.44751599431037903, -0.31573212146759033, 0.7399823069572449, -0.1969730406999588, -0.02074652537703514, 0.5220611691474915, 0.6092195510864258, -0.6377254724502563, 0.5363658666610718, 0.805837869644165, 0.8458312749862671, -0.1398000419139862, 0.1669282764196396, 0.7059212923049927, -0.10635922104120255, 0.5251047611236572, 0.5196004509925842, 0.35279181599617004, 0.2025499939918518, 0.44415825605392456, -0.1476297229528427, 0.3555624783039093, -1.0304896831512451, -0.17519885301589966, 0.47748178243637085, 0.5935072898864746, 0.5441242456436157, -0.026881661266088486, -0.5958353877067566, -0.3670138120651245, 0.13301832973957062, 0.7915084958076477, 1.8030273914337158, -0.20583803951740265, -0.11586369574069977, -0.7182404398918152, -0.3108329772949219, -0.2525331676006317, 0.03788692504167557, -0.41412121057510376, 0.09813005477190018, -0.4554726481437683, -1.2389874458312988, 0.5917683839797974, 0.38966917991638184, 0.535258412361145, -0.24619299173355103, -0.18114343285560608, -0.19522863626480103, 0.15577895939350128, -1.0178824663162231, -1.0550270080566406, 0.28579622507095337, -0.6263769268989563, -0.3236749768257141, -0.05579684302210808, -0.10004560649394989, 0.2738158404827118, -0.6600785255432129, 1.1559083461761475, -0.9439047574996948, -0.04268517717719078, 0.14188571274280548, 0.5806545615196228, -0.5576525926589966, -0.9156834483146667, 0.048261307179927826, 0.48929363489151, 0.05876404047012329, 0.27397823333740234, 0.39379724860191345, 0.37637659907341003, 0.1416894644498825, -0.4238664209842682, 0.24601420760154724, 0.30230405926704407, 0.11506045609712601, 0.8947498798370361, -0.3003109395503998, 0.019319381564855576, -1.3485159873962402, 0.9710044860839844, -0.21168574690818787, -0.3942585587501526, 0.35721150040626526, -0.6455166935920715, -0.2715766429901123, 0.5873273015022278, -0.9880565404891968, -0.3815159201622009, -0.4229816496372223, 0.4810003638267517, -0.549486517906189, -0.1906680315732956, 0.08361805975437164, 0.1384028196334839, 0.29586827754974365, 0.3570936918258667, 0.3283132016658783, -0.015779202803969383, -0.24814794957637787, 0.816810131072998, -0.8098025918006897, 0.5460103750228882, 0.3397725522518158, -0.12440451979637146, -0.34187212586402893, -0.6277134418487549, -0.3866959512233734, -0.10957631468772888, -0.8845863342285156, -0.10644756257534027, -0.3303704857826233, -0.15424343943595886, -0.9471476078033447, -0.43621206283569336, 0.3110993802547455, -1.085044264793396, 0.17033827304840088, 0.7247529625892639, 0.05086284503340721, -0.050963807851076126, -1.14143967628479, -1.1355606317520142, -0.6501339673995972, -0.552094578742981, -1.0372565984725952, 0.4050295650959015, 0.14889052510261536, -0.28831323981285095, -0.6480622291564941, 0.11052507907152176, -0.12803258001804352, 0.9727964997291565, -0.7845232486724854, 0.7875446677207947, -0.3243936002254486, -0.21545632183551788, -0.6761905550956726, 0.2617431581020355, 0.5701781511306763, -0.43174418807029724, 0.2989220917224884, -1.2939212322235107, -0.0065443795174360275, -0.5191178917884827, -0.3069809377193451, 0.37958115339279175, 0.507564902305603, 0.3987989127635956, -0.04427682235836983, -0.6346276998519897, 0.36688727140426636, 1.361480951309204, -0.9230295419692993, -0.46720796823501587, -0.3063555359840393, 0.9208677411079407, 0.24062786996364594, -0.6214572191238403, 0.5172894597053528, 0.3538779020309448, 0.3400997519493103, 0.15684874355793, 0.20849277079105377, 0.04928597807884216, -0.46536916494369507, 0.5251086950302124, 1.4892469644546509, 0.42641788721084595, -0.26525673270225525, -0.9390985369682312, 0.6989161968231201, -1.3053529262542725, -0.6606394648551941, 0.7489435076713562, 0.7579105496406555, 0.29534077644348145, -0.46825653314590454, -0.1325034350156784, -0.4843441843986511, 0.5521231889724731, 0.5527610778808594, -0.5222069025039673, -0.22638258337974548, -0.3240928053855896, 0.08585257828235626, 0.06806168705224991, 0.7615962624549866, -0.7873294353485107, 0.7330801486968994, 14.70442008972168, 0.9784953594207764, 0.10509583353996277, 0.8869407773017883, 0.774931013584137, 0.08330951631069183, -0.6813891530036926, 0.013393317349255085, -1.3173283338546753, 0.020578423514962196, 1.133068323135376, 0.6409499645233154, 0.8107579946517944, 0.3633975386619568, -0.10259725898504257, 0.04505864903330803, -0.7090688943862915, 0.9413215517997742, 0.5585348010063171, -1.1162781715393066, 0.4479280710220337, 0.00841290783137083, 0.9014002084732056, 0.6667792797088623, 0.598450779914856, 1.0248653888702393, 0.5261852741241455, -0.6224427819252014, 0.6083777546882629, 0.4504315257072449, 0.5272063612937927, 0.09033460170030594, 0.3385067284107208, 1.0513783693313599, -1.1663748025894165, -0.4830953776836395, -0.6453774571418762, -0.986229419708252, 0.13622508943080902, 0.3601731061935425, -0.5815820693969727, -0.4509924650192261, -0.2146759033203125, 0.7462393045425415, 0.10266657173633575, 0.3553352355957031, 0.1988048404455185, 0.5902246832847595, -0.4949711263179779, 0.36122769117355347, 0.4125715494155884, 0.19644246995449066, 0.42275938391685486, 0.22434189915657043, -0.16173391044139862, 0.008881738409399986, 0.5145004987716675, 0.389522910118103, -0.6688697338104248, 0.0873517319560051, -0.14135310053825378, -0.5196673274040222, -0.004506698343902826, 0.6856353282928467, 0.5705634355545044, 0.20569118857383728, -0.2874678075313568, 0.3330962061882019, 0.7562005519866943, 0.5115953683853149, -0.15325790643692017, 0.42351382970809937, 0.0726424977183342, -0.7709956169128418, -0.1399218738079071, 0.9020254611968994, -0.0705278217792511, -0.5885657668113708, -0.5119816064834595, -0.5565341711044312, 0.26554030179977417, -0.5099886655807495, -1.062172532081604, 0.9062615633010864, -0.04078670218586922, -0.4191391170024872, 0.1565050482749939, -0.730322003364563, -0.18388889729976654, 0.8160992860794067, -1.0089073181152344, -0.8176459074020386, 0.39829373359680176, -0.6810325980186462, -0.24481020867824554, -0.5306075811386108, 1.1361411809921265, 0.21016260981559753, -0.4789746105670929, 0.18138469755649567, 0.35054394602775574, -0.06463529169559479, -0.1422179639339447, -0.48463377356529236, 0.8339257836341858, 0.3344346880912781, -0.047330982983112335, 0.28264039754867554, 0.02641753852367401, 0.2251000851392746, -0.3811142146587372, 0.12155602127313614, 0.9259616136550903, -0.8547693490982056, -0.6998113393783569, -0.6060888767242432, -0.6039233207702637, 0.21456167101860046, 0.39600399136543274, -0.09925590455532074, 0.43275219202041626, -0.03022727742791176, -0.9867479801177979, 0.07035332918167114, -0.5316155552864075, -0.15231452882289886, 0.4827735424041748, -0.9460741281509399, -0.21913504600524902, 0.09539950639009476, -0.06637132912874222, -0.9554948806762695, -0.20912383496761322, -0.1335589736700058, 0.2904585301876068, -0.06303612142801285, 1.1662362813949585, -0.8727249503135681, 0.35930827260017395, 0.785379946231842, -0.5393158197402954, -0.9399478435516357, -0.022779744118452072, -0.6968265175819397, -0.24685968458652496, -0.1593817174434662, 0.49142196774482727, -0.41088855266571045, 0.0049866121262311935, 0.7151610255241394, 0.3423115313053131, -0.6439999341964722, -0.6383516192436218, -0.3304518759250641, -0.08065970987081528, -0.6438954472541809, 0.15489225089550018, -0.05740030109882355, -0.5384191870689392, 0.16999176144599915, 0.16732189059257507, 0.7058292031288147, -0.15399028360843658, -0.8099896907806396, 0.45243367552757263, -0.23009546101093292, -0.4786926209926605, -0.7737399339675903, 0.007480022497475147, -1.4373565912246704, 0.4680810868740082, -1.2881053686141968, 0.25018247961997986, -0.9171442985534668, -0.2698933482170105, 0.002379524288699031, -0.28485801815986633, -0.28848329186439514, 0.06286875158548355, -0.5137031674385071, -0.3058321177959442, -0.24195027351379395, -0.4205661714076996, 0.8336423635482788, 0.630366325378418, -0.8878059387207031, 0.023343022912740707, 0.2447812706232071, 0.3985588550567627, 0.30767354369163513, 0.43108245730400085, -0.5411775708198547, -1.047486662864685, -1.3174222707748413, 0.3209958076477051, -0.0819307267665863, -0.06288307905197144, -0.847277045249939, 0.6802043914794922, 0.3228278160095215, -0.11850430816411972, 0.23548102378845215, 0.7227924466133118, -1.1919949054718018, -0.04040437191724777, 0.28329917788505554, -0.9420822858810425, 0.19726859033107758, 0.09688985347747803, -0.08996027708053589, -0.47925737500190735, 0.6163796186447144, -0.019219107925891876, -1.1189284324645996, -0.640658438205719, 0.6514306664466858, -0.46641531586647034, 0.10638199746608734, -0.09396746009588242, -0.11984816938638687, -0.8455307483673096, -0.45347100496292114, 0.0739307776093483, 0.5221949815750122, -0.3833570182323456, 0.6526706218719482, 0.41092589497566223, -1.0561659336090088, -0.2793097496032715, 0.439476877450943, 0.24914541840553284, -0.17544639110565186, 0.8087448477745056, 0.5008522868156433, -0.21807867288589478, 0.8612471222877502, 0.3510154187679291, 0.3845925033092499, -0.7742888927459717, -0.17471206188201904, 1.2140412330627441, -0.8024107217788696, 0.07715197652578354, 1.579533338546753, -0.04138874262571335, -1.4771523475646973, 0.32023748755455017, -0.7520132064819336, -0.608588695526123, -0.32219573855400085, 0.5255982875823975, 0.35205140709877014, -0.26770180463790894, -0.3764328956604004, -0.5759507417678833, 0.17229345440864563, 0.01188794057816267, -0.7537189722061157, 0.4264255464076996, -0.11572961509227753, -0.33779194951057434, 0.7083738446235657, 0.9246999621391296, -0.5317420959472656, -1.0583575963974, -0.896066427230835, -0.5288076996803284, -0.07063209265470505, 0.36098700761795044, -0.6024820804595947, -0.5222710371017456, 0.6448603272438049, 0.5761470794677734, -0.15844745934009552, 0.20991019904613495, -0.2597784399986267, 0.16271516680717468, 0.700860321521759, 0.015548445284366608, -0.8648620247840881, -0.773612380027771, 1.3455965518951416, 1.1906039714813232, -1.21483314037323, -0.04035229980945587, -0.24693948030471802, -0.7323025465011597, 0.7399673461914062, 0.37743350863456726, -0.1598501205444336, 1.0274463891983032, -0.46009212732315063, -0.027837062254548073, 0.4411035478115082, -1.136810541152954, -0.06226326525211334, 1.4298678636550903, 0.7057458162307739, 0.7340499758720398, 0.5178511142730713, 0.15079805254936218, 1.0043046474456787, 0.32131427526474, -0.08974664658308029, 0.19052951037883759, 0.3898666501045227, -0.18437248468399048, -0.0842580571770668, -0.01638818345963955, 0.5635413527488708, -0.43891626596450806, -0.6874002814292908, -0.01965789683163166, 0.21588823199272156, 0.5599414110183716, 0.6954206228256226, 0.45812034606933594, 0.034220390021800995, 0.17234547436237335, 0.35323020815849304, 0.4278269112110138, -0.5031996965408325, -0.18400415778160095, 0.007914203219115734, -0.6598360538482666, -0.10361003130674362, -0.4741905629634857, -0.047376472502946854, 0.1268092840909958, -0.031078603118658066, 0.3524312973022461, -0.24519461393356323, 0.5336322784423828, 1.281876802444458, 0.520100474357605, 0.4056091010570526, -0.4092370271682739, -0.3950420320034027, -0.6142259240150452, -1.2851765155792236, -0.1435280442237854, -0.576755940914154, -0.15314143896102905, -0.10750800371170044, -0.24701803922653198, -0.30572858452796936]}, "authors": [{"authorId": "2296001947", "name": "Weilin Cai"}, {"authorId": "2294682530", "name": "Juyong Jiang"}, {"authorId": "2304542351", "name": "Fan Wang"}, {"authorId": "2310483288", "name": "Jing Tang"}, {"authorId": "2257349580", "name": "Sunghun Kim"}, {"authorId": "2295676687", "name": "Jiayi Huang"}], "references": [{"paperId": "05830547cfd19b734777b8546f4d606fd79ebd2b", "title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training"}, {"paperId": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731", "title": "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models"}, {"paperId": "c8b18682965ff9dccc0130dab3d679f78cefa617", "title": "A Survey on Large Language Models for Code Generation"}, {"paperId": "773597e8acb8c9ee2a4603e0bd40bb74b7fba871", "title": "MPMoE: Memory Efficient MoE for Pre-Trained Models With Adaptive Pipeline Parallelism"}, {"paperId": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c", "title": "Yuan 2.0-M32: Mixture of Experts with Attention Router"}, {"paperId": "57ed7527c45f6b685eeefc65abe083dd350cd579", "title": "Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts"}, {"paperId": "53a803388e83ae89261624099d7be4287ace67cb", "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"}, {"paperId": "4b879f069d023e03bf537309a99bdaeb39916ea5", "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training"}, {"paperId": "146075b3b59aba98edf97a7e4c8303e0e6e560c6", "title": "Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping"}, {"paperId": "ebcf108f8bc42140721ff02b6727b0a291362957", "title": "MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts"}, {"paperId": "c67c4c81beed122d7f94580d8816a6dc68867ec4", "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"}, {"paperId": "3940c3071e343e00d0a1d8c129854eee9430e3fb", "title": "Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts"}, {"paperId": "cbaf689fd9ea9bc939510019d90535d6249b3367", "title": "Jamba: A Hybrid Transformer-Mamba Language Model"}, {"paperId": "916b4926cda574dc3f9486bb9994b6f2788dd800", "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey"}, {"paperId": "6675bcf6dc97c87da7afda223938ec7e51ecc3b2", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"}, {"paperId": "07894aeadab9158fdb97647c4792816ede1b60b9", "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"}, {"paperId": "94e531c3b139cdc50ab4b7be21a29dba8f87e3c0", "title": "Higher Layers Need More LoRA Experts"}, {"paperId": "37ac7683543f0e039197a56e71e752a9ebe5998e", "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models"}, {"paperId": "af9676f9beaeca214bfbf7f2897828820d48abdc", "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs"}, {"paperId": "dab65cdf3dcbe780ca5201d4576e8429e5fa1e07", "title": "Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference"}, {"paperId": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda", "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"}, {"paperId": "3a2bb5b4f1f724df9295de5e0e1235cc30769b82", "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts"}, {"paperId": "a88a6112ddd6f2cb171ab61310ffa773b149d773", "title": "HOMOE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts"}, {"paperId": "525cd5d5c7d823d0b2ad4eaf086622940d45ed6d", "title": "SiRA: Sparse Mixture of Low Rank Adaptation"}, {"paperId": "71c9d2b995c19d43c519eb5ca9504ac790490398", "title": "Skywork: A More Open Bilingual Foundation Model"}, {"paperId": "66129521108ddf3db1377f425cc846ee5aa36a1e", "title": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation"}, {"paperId": "bcd84a2b8f9ae40a908f375425f113c82f8dd739", "title": "Sparse Universal Transformer"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9", "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "5aae7d84f8eaa55f3386cee41d94769e7ab01e9d", "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"}, {"paperId": "3ed178316be914658a80e561bf00576577f34389", "title": "Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"}, {"paperId": "6f88133bc591cd964667a626a06debad17775757", "title": "Experts Weights Averaging: A New General Training Scheme for Vision Transformers"}, {"paperId": "2edccb8fa562ed52cd49ea6fc67ed32db6218247", "title": "From Sparse to Soft Mixtures of Experts"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "9618aa98729670f74418d2087f5e47ab137856b4", "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models\u2019 Memories"}, {"paperId": "7a816dd242c4c3f652a448dba54daa53f89a9e4f", "title": "Soft Merging of Experts with Adaptive Routing"}, {"paperId": "59f87a2464ab1d3c0376ca30d09c9204c89653dd", "title": "Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "e9b3e82b1c9eb4136df28e94f24cd823431be93b", "title": "Lifelong Language Pretraining with Distribution-Specialized Experts"}, {"paperId": "dbbc5003af690799fa4fe6330fb795311cde106f", "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "9d12916dd46df7a6446cbec0bc4d054f7dafcdab", "title": "Scaling Vision-Language Models with Sparse Mixture of Experts"}, {"paperId": "443c1bef6a7dc3db941375ae76451c884ceffb8a", "title": "A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training"}, {"paperId": "1462a0e5b7db47301bb0995db56426e1f4a0ac7d", "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"}, {"paperId": "a34384389f74b7b2c31c696b0db0bf813e8bb301", "title": "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training"}, {"paperId": "32fbc1821ef4a6daee1f9e9f140056ff9cda238a", "title": "PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation"}, {"paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e", "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"}, {"paperId": "43014fc85c4860487336579ec98f509fec1803f7", "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"}, {"paperId": "3820231d31540ecb05d94c74d959a2f61d3136ea", "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token"}, {"paperId": "ca086f4c09cf8de705830ac2b70951737fab93ca", "title": "A Review of Sparse Expert Models in Deep Learning"}, {"paperId": "8b3a67c7e5289eed160d2acfd04d71cfb552c67d", "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models"}, {"paperId": "33d97417aa0e5c6fc4a58716003b02c09736e782", "title": "Towards Understanding Mixture of Experts in Deep Learning"}, {"paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation"}, {"paperId": "bedf0d6e0623ab48349e3d2a493e7fbb79ca5ef5", "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs"}, {"paperId": "2e700ff36108119f5ed19a53bd2eaa22b42ec3d8", "title": "Tutel: Adaptive Mixture-of-Experts at Scale"}, {"paperId": "499d3bb3acbc10730dd6582bd9b8f646bf22ccd5", "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"}, {"paperId": "0fb8e89e34f00365a56c3e67f7e862fdcf935f2c", "title": "Task-Specific Expert Pruning for Sparse Mixture-of-Experts"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "1da214f8f265445b5997f5d677452819b334bdfb", "title": "Eliciting and Understanding Cross-task Skills with Task-level Mixture-of-Experts"}, {"paperId": "eb4d54651c4f610749caf2bf401af3ce28ddc439", "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"}, {"paperId": "ffabceeeab1e6298b3633ce5a0117fe4f4fd0270", "title": "SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System"}, {"paperId": "68309384f7a8e96d98ede4271e7c04425c23f3f2", "title": "AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "c26bb68806a992bf4fc85b5639e1657a445c4781", "title": "On the Representation Collapse of Sparse Mixture of Experts"}, {"paperId": "c9550f0d1940ee1adf1549c9a0d699ef896dbefd", "title": "StableMoE: Stable Routing Strategy for Mixture of Experts"}, {"paperId": "df434c1289f3c7243b585cb9982afac3c5bf0439", "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "408efcc963ab871558e5e145b54ad7ae2aeb11c7", "title": "BaGuaLu: targeting brain scale pretrained models with over 37 million cores"}, {"paperId": "54b8bc5be8bbffae333dd73f2cb9d93a492d438e", "title": "HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"}, {"paperId": "c2536182c010c41941e8a031071a1880c34cec60", "title": "Unified Scaling Laws for Routed Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "9caa83288602f6c3734c78d8d89bb358b263da24", "title": "One Student Knows All Experts Know: From Sparse to Dense"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "cf4f4b69b76dc58dc8c0d443ab88ceb461eec719", "title": "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "547284c6a9c074d7b5cee4049b59fb52ad93df41", "title": "Tricks for Training Sparse Translation Models"}, {"paperId": "ad471be93216ddbf8544721d50ee5aed14f07cae", "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning"}, {"paperId": "0e23cc159fd2fb34550600d60dd9148c93636183", "title": "Taming Sparsely Activated Transformer with Stochastic Experts"}, {"paperId": "561f9f5abb2c0960a886ab6221c821295f0461a1", "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"}, {"paperId": "8ae292cbd9144acbf4b42b7ead82b079faf33192", "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference"}, {"paperId": "d52977458284acce166bb1291a9d79143aa59070", "title": "Scalable and Efficient MoE Training for Multitask Multilingual Models"}, {"paperId": "917c63f2186119166b3379f5d2816bb1a2f39b09", "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "36ffa5b1f643f59ccf8396cff9865e5474c8dae7", "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "16e623059ffccab60f4c35be028a2d4f10933515", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b", "title": "FastMoE: A Fast Mixture-of-Expert Training System"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "687b13c44f849d23c2496996b5da83e706094db9", "title": "Beyond English-Centric Multilingual Machine Translation"}, {"paperId": "0182197c3996d11867ef650b66a2ddf1efa6f631", "title": "Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "7fe088b52bc11c74c70ac92b805c432e2bc2eb97", "title": "Infinite Mixtures of Gaussian Process Experts with Latent Variables and its Application to Terminal Location Estimation from Multiple-Sensor Values"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "be928f91385999fa90d1e2fe06058f9dbcfd7186", "title": "Routing Networks and the Challenges of Modular and Compositional Computation"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "af8a8dcb74561d52d904f7bc4afcc747e079b702", "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts"}, {"paperId": "6428fa299a76ccce775b17ea6f3401ef0cfccf04", "title": "Deep Mixture of Experts via Shallow Embedding"}, {"paperId": "fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2", "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy"}, {"paperId": "e9c9da57bbf9a968489cb90ec7252319bcab42fb", "title": "Hard Mixtures of Experts for Large Scale Weakly Supervised Vision"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "66b8d34477cf1736f91fd22b27e37ce0b703c86e", "title": "Expert Gate: Lifelong Learning with a Network of Experts"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "5a5d48986b855b83a7d9df5005bbd155024ce756", "title": "Dynamic Capacity Networks"}, {"paperId": "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "title": "Conditional Computation in Neural Networks for faster models"}, {"paperId": "f267934e9de60c5badfa9d3f28918e67ae7a2bf4", "title": "Generative Image Modeling Using Spatial LSTMs"}, {"paperId": "e2e81c568ac0aa067e32fbc9ca9396824fa04d66", "title": "Distributed Gaussian Processes"}, {"paperId": "44ddac48353ead135eef4096859956eaa31be2a5", "title": "Learning Factored Representations in a Deep Mixture of Experts"}, {"paperId": "cf3229e74f912ef365d67d1954441b32ce2573ee", "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "a78273144520d57e150744cf75206e881e11cc5b", "title": "Multimodal Deep Learning"}, {"paperId": "67107f78a84bdb2411053cb54e94fa226eea6d8e", "title": "Deep Sparse Rectifier Neural Networks"}, {"paperId": "81c9cb2c3c08070a80c7b26c288789084f70b43b", "title": "Nonlinear Models Using Dirichlet Process Mixtures"}, {"paperId": "16174aa9f7b2f72a078b1301244367f40a754502", "title": "A Parallel Mixture of SVMs for Very Large Scale Problems"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": null, "title": ". Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters"}, {"paperId": null, "title": "2022. Residual mixture of experts. arXiv preprint arXiv:2204.09636 (2022)"}, {"paperId": "0bd2602df71e89c8961562175c8759e625e99389", "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data"}, {"paperId": "5fffc5f56c67740603c68473dea50ce059cbb78f", "title": "LoRAMoE: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment"}, {"paperId": "1fb8f2d080e965c833c777f06fccf09dc9856b91", "title": "MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"}, {"paperId": "50f2a5b103884c2edb11506445eabff613a79b6e", "title": "SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts"}, {"paperId": "5c6a17850c9ad6bf6dc8992ec598cd932ce42208", "title": "SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization"}, {"paperId": null, "title": ". Chatgpt: Optimizing language models for dialogue"}, {"paperId": "91a7d2af2c3294a8e0356e4eeb043636bbb3ca04", "title": "M6-T: Exploring Sparse Expert Models and Beyond Anonymous ACL submission"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "HierarchicalmixturesofexpertsandtheEMalgorithm"}, {"paperId": null, "title": "2023. Mixtureofcluster-conditionalloraexpertsforvision-languageinstructiontuning"}, {"paperId": null, "title": "2024. JetMoE: Reaching Llama2 Performance with 0.1 M Dollars"}, {"paperId": null, "title": "2024.Mixtralofexperts"}, {"paperId": null, "title": "2024. Snowflake Arctic: The Best LLM for Enterprise AI \u2014 Efficiently Intelligent, Truly Open"}, {"paperId": null, "title": "2023. Patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks"}, {"paperId": null, "title": "2024. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models"}, {"paperId": null, "title": "2022. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models"}, {"paperId": "67f3725da6b721a0f31461c2a0bd18236efcec0f", "title": "A Case Study of Instruction Tuning with Mixture of Parameter-Efficient Experts"}, {"paperId": null, "title": "2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)"}, {"paperId": null, "title": "2022. Go wider instead of deeper"}, {"paperId": null, "title": "2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models"}, {"paperId": null, "title": "2023. Zero Bubble Pipeline Parallelism"}, {"paperId": null, "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2024. Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"}, {"paperId": null, "title": "2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models"}, {"paperId": null, "title": "2023. Mixture-of-experts meets instruction tuning: A winning combination for large language models"}, {"paperId": null, "title": "2022. Multimodal research in vision and language: A review of current and emerging trends"}, {"paperId": null, "title": "2022. Alpa: Automating inter-and { Intra-Operator } parallelism for distributed deep learning"}, {"paperId": null, "title": "2023. Fusing Models with Complementary Expertise"}, {"paperId": null, "title": "2023. Brainformers: Trading simplicity for efficiency"}, {"paperId": null, "title": "2023. Pangu-{\\ Sigma } : Towards trillion parameter language model with sparse heterogeneous computing"}, {"paperId": null, "title": "2022. A survey of recommender systems with multi-objective optimization"}]}