{"paperId": "90de947a4c324d1a8ff8e72843ccc465b72b49c4", "abstract": "Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications. We replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude. By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting model is further reduced. Compared with existing LLMs, our methods achieve a speedup of 1.3X and a model compression ratio of 2.64X for pretaining without accuracy drop. For finetuning, our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our models are smaller than 0.59 GB, allowing inference on a smartphone.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "High-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications are presented, which replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows the number of parameters to be reduced by several orders of magnitude."}, "embedding": {"model": "specter_v2", "vector": [0.04761066287755966, -0.0335845947265625, -0.2655402719974518, -0.15478362143039703, -0.27720174193382263, 0.009064692072570324, 0.7162200212478638, -0.24198274314403534, -0.6030442714691162, -0.3741169273853302, 0.3433203101158142, 0.03918672725558281, 0.4710629880428314, 0.09515252709388733, -0.11523840576410294, -0.04182544723153114, -0.898419201374054, 0.5125437378883362, -0.2550630271434784, -0.234284445643425, -0.10596749931573868, -0.10207545012235641, -1.2852345705032349, 0.23141810297966003, 0.4049522280693054, 0.8404858708381653, 0.27033835649490356, 0.975498616695404, -0.5746848583221436, 0.4530732333660126, 0.6510611176490784, -0.14943049848079681, 0.09210015088319778, 0.06622951477766037, -0.1625165194272995, -0.13362087309360504, 0.26231464743614197, -0.30440789461135864, -0.37044304609298706, 0.7175023555755615, -0.25546035170555115, 0.5909441113471985, 0.48160824179649353, -0.5258870124816895, -0.06998647004365921, 0.1843595951795578, 0.6277623176574707, 0.8224944472312927, -0.42192378640174866, -0.429072767496109, 1.0404398441314697, -1.4588350057601929, -0.12427464127540588, 1.3416959047317505, 0.7361395955085754, 0.12043438851833344, -0.09327495843172073, -0.5733853578567505, 0.42479223012924194, -0.26779091358184814, -0.8024822473526001, -0.20538312196731567, -0.3091376721858978, -0.4892364740371704, 2.123835325241089, -0.16529841721057892, 0.09047071635723114, 0.5332069993019104, 0.18366843461990356, 1.2291865348815918, 0.3464004695415497, -0.5341700911521912, -0.3465845584869385, 0.13328014314174652, 0.26543304324150085, 1.0053706169128418, -0.5053563714027405, 0.35056525468826294, -1.0824295282363892, -0.4405558109283447, 0.33289214968681335, -0.16271263360977173, 0.5959601998329163, 0.11947926133871078, -0.1901082694530487, 0.9877620935440063, 0.32276979088783264, 0.5870886445045471, -0.15307749807834625, 0.523140013217926, 0.5467895269393921, -0.057388897985219955, 0.20056907832622528, 0.34406736493110657, -0.18928168714046478, 0.2498944103717804, -1.2292007207870483, 0.22121396660804749, 0.08085041493177414, 0.4725860059261322, -0.43597543239593506, 0.3919718563556671, -0.5791704654693604, 0.23781074583530426, 1.2220526933670044, 0.2570023834705353, 0.7290664911270142, -0.5074514746665955, 0.38171446323394775, -0.8175157904624939, -0.1686106026172638, -0.8124418258666992, -0.4403958022594452, -0.4323377013206482, -0.9804180264472961, -1.0653862953186035, -0.7806981205940247, 0.020572133362293243, -0.4910537600517273, 0.31473231315612793, -0.23757565021514893, 0.5882614254951477, 0.20621243119239807, 0.5742613077163696, 0.5177150964736938, 0.9122631549835205, 0.27554064989089966, 0.23512911796569824, 1.075955867767334, -1.1344795227050781, -0.4154604971408844, -1.2166379690170288, 0.3857653737068176, -0.40567272901535034, 0.45285046100616455, -0.13719531893730164, -0.9904789924621582, -0.8534954786300659, -0.6750449538230896, -0.108306385576725, -0.4795198142528534, 0.3835059404373169, 1.3897565603256226, 0.23820729553699493, -0.768211305141449, 0.8684093356132507, -0.285738468170166, 0.04000029340386391, 0.2772469222545624, 0.20933227241039276, 0.5808535218238831, -0.1273227483034134, -1.5020549297332764, 0.23292043805122375, 0.1657579243183136, -0.4905276298522949, -0.03237391635775566, -0.29730212688446045, -0.7217642664909363, 0.14408540725708008, -0.06639973074197769, -0.3990102708339691, 1.1274590492248535, -0.05208681523799896, -1.3515639305114746, 0.7041256427764893, -0.23907119035720825, -0.09241610765457153, 0.38263553380966187, -0.04881618544459343, -0.4133235216140747, -0.45288318395614624, -0.3388111889362335, 0.32026243209838867, 0.7651466727256775, 0.2631971836090088, -0.04648229852318764, 0.3576647639274597, -0.47988569736480713, 0.21611258387565613, -0.411963552236557, 1.2245702743530273, -0.6386464238166809, -0.36029383540153503, 0.1859605610370636, 0.47745686769485474, -0.23927819728851318, -0.29060912132263184, -0.06360870599746704, -0.7086443901062012, 0.5864756107330322, -0.019384851679205894, 0.8612268567085266, -0.9242276549339294, -0.9192428588867188, -0.0699845403432846, -0.16232100129127502, 0.09444426745176315, -0.5838162302970886, 0.41450023651123047, -0.1927424967288971, 0.10695743560791016, -0.048272505402565, -1.2867052555084229, 0.09077472239732742, -0.024478094652295113, -0.8925449252128601, -0.18909792602062225, 0.12027160823345184, 0.982383668422699, -0.7938933968544006, -0.04759357497096062, -0.1947878897190094, 0.7595069408416748, -1.152426838874817, 1.3267037868499756, -0.38335999846458435, 0.18658959865570068, -0.27979016304016113, -0.1817736178636551, 0.07638432085514069, -0.8030174374580383, 0.4470248222351074, -0.1300136148929596, -0.11118998378515244, 0.5031170845031738, -0.4872398376464844, 0.9964926838874817, -0.41017621755599976, 0.33813151717185974, -0.1590781807899475, -0.6361430883407593, 0.18018485605716705, 0.09265214204788208, -0.03303327038884163, -0.5128793716430664, 0.371025949716568, 0.5374148488044739, -0.9481000900268555, 0.2194366157054901, 0.931923508644104, 0.5462101101875305, -0.3889717757701874, 0.2292618602514267, 0.3879867196083069, -0.45860767364501953, 0.4369196593761444, 0.1530928909778595, 0.046158887445926666, 0.44967395067214966, 0.14454558491706848, 0.17416448891162872, 0.22037969529628754, -0.8533822894096375, -0.20933221280574799, 0.5929819941520691, 0.7234309315681458, 0.7235089540481567, 0.4435187578201294, -0.6741764545440674, -0.2615838050842285, 0.2720002830028534, 0.5394302606582642, 1.594443440437317, -0.5701072812080383, 0.012046550400555134, -0.6386462450027466, -0.036951012909412384, 0.12346312403678894, -0.3933098018169403, 0.02136586792767048, 0.18448983132839203, -0.398416668176651, -1.6489580869674683, 0.8571138381958008, 0.09542662650346756, 0.7332624793052673, -0.6212626099586487, -0.33545419573783875, -0.43082883954048157, 0.4374224543571472, -1.0624996423721313, -0.7121569514274597, 0.5323079228401184, -0.7950308322906494, 0.1463545560836792, 0.06801330298185349, 0.008682305924594402, 0.0025302162393927574, -0.7558414936065674, 0.8377982974052429, -0.5892966985702515, -0.3330937922000885, -0.10765572637319565, 0.8374038338661194, -0.4657571017742157, -0.5940929651260376, 0.11573728919029236, 0.4508582651615143, 0.06557665020227432, 0.18657109141349792, 0.4162660837173462, 0.4355996549129486, -0.3254741132259369, -0.27347394824028015, 0.29530447721481323, 0.11711481958627701, -0.03541737049818039, 0.8442968130111694, -0.24727924168109894, -0.30661508440971375, -1.3393280506134033, 0.9832969307899475, 0.05676969140768051, -0.8705452084541321, -0.013968602754175663, -0.80683434009552, -0.18070973455905914, 0.67061847448349, -0.7991018295288086, -0.12181920558214188, -0.7086352109909058, 0.1893688142299652, -0.639883816242218, -0.015598858706653118, 0.5377944707870483, 0.5820908546447754, 0.051615506410598755, 0.15246008336544037, 0.32670387625694275, 0.14953728020191193, -0.3094196915626526, 0.272204726934433, -0.6455858945846558, 0.5958282947540283, 0.20736561715602875, 0.06937060505151749, -0.2560873329639435, -0.09827529639005661, -0.7678852081298828, -0.685867428779602, -0.4231046438217163, -0.3106479048728943, -0.11480493098497391, 0.11010117083787918, -0.7972111701965332, -0.8284181952476501, 0.06550075113773346, -0.7382133603096008, -0.4026550352573395, 0.3710291385650635, 0.020450659096240997, -0.02962564490735531, -1.0407607555389404, -1.6292277574539185, -0.7238430976867676, -0.9625061750411987, -1.0453002452850342, 0.5091871023178101, 0.0966208204627037, -0.09960924834012985, -0.5844400525093079, -0.30859994888305664, -0.27451637387275696, 1.0600043535232544, -0.38375842571258545, 0.6566504836082458, 0.05677799880504608, -0.07238974422216415, -0.1426965296268463, -0.067052461206913, 0.6206793785095215, -0.5349881649017334, 0.17899726331233978, -0.9714567065238953, 0.0652785375714302, -0.3336040675640106, -0.3620298206806183, 0.3405745029449463, 0.05876540392637253, 0.8120403289794922, -0.030093414708971977, -0.29901525378227234, 0.7859718203544617, 1.2549779415130615, -0.998083770275116, -0.42291003465652466, -0.06961563974618912, 1.079398274421692, -0.09448160976171494, -0.25935986638069153, 0.7273651361465454, 0.03183934837579727, 0.43075382709503174, -0.13902729749679565, -0.16544575989246368, 0.04531516507267952, -0.43405067920684814, 0.515741229057312, 2.0644819736480713, 0.41404610872268677, 0.1545257717370987, -1.103346586227417, 0.26183661818504333, -0.9143053889274597, -0.643153965473175, 0.4671434164047241, 0.620376467704773, 0.4607798159122467, -0.1734018474817276, -0.34980064630508423, -0.42778927087783813, 0.15472276508808136, 0.5949714779853821, -0.5686435103416443, -0.954423189163208, -0.06636863201856613, 0.45283862948417664, 0.24670587480068207, 0.6249208450317383, -0.632924497127533, 0.4694599211215973, 15.033283233642578, 0.7834569215774536, -0.4611618220806122, 0.7507650256156921, 1.0142401456832886, 0.10542847216129303, 0.039245977997779846, -0.6090010404586792, -1.471003770828247, 0.11747772991657257, 1.6876171827316284, 0.4397450089454651, 0.8154205083847046, 0.5046588182449341, 0.18748129904270172, 0.3294178545475006, -0.1993272304534912, 1.0082882642745972, 0.6058456301689148, -1.311715841293335, 0.067204050719738, 0.14346517622470856, 0.48536545038223267, 0.8417773842811584, 0.9219865798950195, 0.8539614081382751, 0.42175954580307007, -0.6577776074409485, 0.5341058373451233, 0.45687100291252136, 1.018980860710144, -0.18590039014816284, 0.4389251172542572, 0.7313571572303772, -0.8608295917510986, -0.13003447651863098, -0.6556698679924011, -1.3727964162826538, -0.05925591289997101, 0.39433324337005615, -0.5184541940689087, -0.48795178532600403, -0.038824811577796936, 0.8257365822792053, 0.06067119538784027, 0.102534219622612, 0.19669777154922485, 0.6113044023513794, -0.21291489899158478, -0.044174518436193466, 0.5603541135787964, -0.09745994955301285, -0.11203847825527191, 0.2621663510799408, 0.2162260115146637, -0.07738984376192093, 0.4262775778770447, 0.2623409330844879, -0.4626110792160034, -0.06956333667039871, -0.0478472001850605, -0.6450611352920532, -0.2737007737159729, 0.8580606579780579, 0.3642890453338623, 0.17499883472919464, -0.49546724557876587, 0.45394062995910645, 0.7135733962059021, -0.0306392852216959, -0.35507863759994507, 0.3034803867340088, 0.5771855115890503, -0.27059516310691833, 0.1318996250629425, 0.17915615439414978, -0.2913128137588501, -0.8153907060623169, -0.7442257404327393, -0.5038005113601685, 0.3561893403530121, -0.7795822620391846, -0.9839714169502258, 0.7885147333145142, -0.3472912907600403, -0.2544592618942261, 0.14266571402549744, -1.1027714014053345, 0.09190808981657028, 0.8745074272155762, -1.601353406906128, -0.31865379214286804, 0.503140926361084, -0.46674492955207825, -0.5996897220611572, -0.12968803942203522, 1.3850635290145874, 0.3341315984725952, -0.4398207366466522, -0.20383474230766296, 0.37623098492622375, -0.0029357292223721743, -0.47676169872283936, -0.29786744713783264, 0.9455311894416809, 0.260011225938797, 0.03644483536481857, 0.2351636290550232, -0.16139520704746246, 0.1555033028125763, -0.9395511746406555, -0.33035290241241455, 1.0700819492340088, -0.6509138941764832, 0.004708708729594946, -1.0307729244232178, -0.6851988434791565, 0.14669516682624817, 0.057610318064689636, 0.09708374738693237, 0.34629443287849426, 0.0521584115922451, -0.6390570998191833, 0.09136831760406494, -0.5820842385292053, -0.3679226040840149, 0.16501514613628387, -1.0462933778762817, -0.04794004559516907, 0.26051098108291626, 0.24812214076519012, -1.3547230958938599, -0.46513691544532776, -0.3660045564174652, 0.1280049979686737, -0.09148349612951279, 1.3030179738998413, -0.39070671796798706, 0.7891362309455872, 0.9424219131469727, -0.30248528718948364, -0.4181663691997528, 0.14314301311969757, -0.4470713138580322, -0.17574819922447205, -0.2763681411743164, 0.3683278262615204, -0.45544329285621643, 0.2735443413257599, 0.7049446105957031, 0.5022268891334534, -0.6645678877830505, -0.3810121417045593, -0.2354995459318161, -0.03412628546357155, -0.6147722601890564, 0.36269307136535645, 0.22037097811698914, -0.4082179367542267, 0.07869117707014084, 0.28496360778808594, 0.6897825598716736, -0.37016865611076355, -0.2909786105155945, 0.13105587661266327, -0.21397258341312408, -0.43517938256263733, -0.4567084014415741, -0.12691642343997955, -1.205195665359497, 0.1289442628622055, -1.346412181854248, -0.3269181251525879, -0.7086477875709534, -0.33608829975128174, -0.15937700867652893, -0.31686830520629883, 0.2234378457069397, 0.20018695294857025, -0.1126030832529068, -0.3940346837043762, -0.313268780708313, -0.4079837501049042, 0.7066785097122192, 0.8291425108909607, -0.5590515732765198, 0.056685060262680054, 0.00010663907596608624, 0.3745526969432831, 0.3419221043586731, 0.4088722765445709, -0.4406953752040863, -0.47362223267555237, -1.0580196380615234, 0.37534278631210327, -0.03733209893107414, -0.1282368302345276, -0.6393870711326599, 0.4640367031097412, 0.04845237731933594, -0.03649086132645607, 0.20671780407428741, 0.5258198976516724, -0.5883954167366028, -0.3944161534309387, 0.45560747385025024, -0.625155508518219, 0.3343695104122162, 0.3939915895462036, -0.7523924708366394, -0.383116751909256, 0.6595866084098816, 0.034463681280612946, -0.8386321067810059, -0.6555484533309937, 0.5820203423500061, -0.5664756894111633, 0.17410920560359955, -0.6583186984062195, 0.38633814454078674, -0.5924100279808044, -0.23945066332817078, 0.01410854421555996, 0.18483151495456696, -0.5489954948425293, 0.894777774810791, 0.29258373379707336, -1.1530357599258423, 0.1715988963842392, 0.5695223808288574, -0.44388535618782043, -0.24530304968357086, 0.4650990068912506, 0.3932705223560333, -0.4241982698440552, 0.7103651165962219, 0.3696160316467285, 0.4002390205860138, -0.9242838621139526, -0.1354214996099472, 0.9873989820480347, -0.8405511975288391, -0.18236804008483887, 1.1549537181854248, -0.4564451277256012, -1.2854644060134888, -0.05388753488659859, -1.1882988214492798, -0.6404690146446228, -0.18450042605400085, 0.6338696479797363, -0.18041978776454926, 0.4088594615459442, -0.26598232984542847, -0.5568059682846069, 0.05155274644494057, 0.3338947594165802, -0.35936304926872253, 0.6777656078338623, 0.02333650551736355, -0.5063807964324951, 0.9674910306930542, 1.134046196937561, -0.5675632953643799, -0.7852573394775391, -0.806887686252594, -0.4057563841342926, -0.10512682795524597, 0.531122088432312, -0.253185898065567, -0.7174651622772217, 0.7922053337097168, 0.562181293964386, 0.004126805812120438, 0.12150182574987411, -0.5228674411773682, 0.37464162707328796, 0.8288866281509399, 0.0959901288151741, -0.7554545998573303, -0.5674088001251221, 1.642940878868103, 0.923595666885376, -0.9329524636268616, 0.4366609752178192, -0.11456440389156342, -0.860139787197113, 0.766001284122467, -0.05411934480071068, -0.06605009734630585, 0.9969916939735413, 0.07405106723308563, 0.014420907013118267, 0.12545843422412872, -0.9193695187568665, -0.26910001039505005, 1.1780900955200195, 0.41844192147254944, 0.8049405217170715, 0.4532899260520935, 0.2572670876979828, 0.8169382810592651, -0.1843220293521881, 0.12572798132896423, 0.1956672966480255, 0.4497103989124298, 0.02390337362885475, 0.11691336333751678, -0.3114333152770996, 0.9280980229377747, -0.8520352840423584, -0.9076525568962097, 0.5132509469985962, 0.19597023725509644, 0.18806517124176025, 0.30698949098587036, 0.9379581212997437, 0.15705245733261108, 0.4947502017021179, 0.23234717547893524, 0.29965847730636597, -0.5273382067680359, -0.4227584898471832, -0.003920474089682102, -0.6775752305984497, -0.2568516433238983, -0.1397368460893631, -0.6260436773300171, -0.6235453486442566, -0.39776989817619324, 0.867953360080719, 0.015496795997023582, 0.48743492364883423, 1.0700801610946655, 0.5023282766342163, 0.6026401519775391, -0.423411101102829, -0.7810084223747253, -0.5565630793571472, -0.577113151550293, -0.2873547077178955, -0.5308158993721008, -0.24475233256816864, -0.024628417566418648, -0.06421832740306854, -0.34511300921440125]}, "authors": [{"authorId": "2284933439", "name": "Xiao-Yang Liu"}, {"authorId": "2284968911", "name": "Jie Zhang"}, {"authorId": "2284974721", "name": "Guoxuan Wang"}, {"authorId": "2269985973", "name": "Weiqin Tong"}, {"authorId": "2292421581", "name": "Anwar Elwalid"}], "references": [{"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "af67be0fff8d087a0d8554b6e8998ab12409bbda", "title": "TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"}, {"paperId": "776b6efafc2a0de434df5c9ddcd509dc8a6fc08d", "title": "High-Performance Tensor Learning Primitives Using GPU Tensor Cores"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "1b3f2d2696bcb19ed16ec47e014c0a148c84df19", "title": "High Performance Hierarchical Tucker Tensor Learning Using GPU Tensor Cores"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "4e05fcab867bbe8d4777531a777039e93bc8d777", "title": "High performance GPU primitives for graph-tensor learning operations"}, {"paperId": "4e11aee8ab3bdd8d0bff270e49fb3ae351b5857a", "title": "High Performance GPU Tensor Completion With Tubal-Sampling Pattern"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "76c7b2973c19b1fef3d8b4a3abcf821f0a9800a1", "title": "cuTensor-Tubal: Efficient Primitives for Tubal-Rank Tensor Learning Operations on GPUs"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "e3b41424637d716f63ade41390ff4b969853d049", "title": "Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks"}, {"paperId": "fc2511514112faaac4c6b3b09770c9cb519eaf7b", "title": "High-Performance Homomorphic Matrix Completion on GPUs"}, {"paperId": "fc0b44eb5229cc7389977bbc07f4c089795271fe", "title": "High-Performance Tensor Decoder on GPUs for Wireless Camera Networks in IoT"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "6977ce5793cb96f4762e3a6f436e058968eb00a6", "title": "Cutensor-tubal: Optimized GPU Library for Low-tubal-rank Tensors"}, {"paperId": "7191680b572ee7145f1a9d95ff11ab1ff44259f3", "title": "WWW'18 Open Challenge: Financial Opinion Mining and Question Answering"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e6f2f3a5cc7c7213835b9aede15715b5830520e1", "title": "Tensorizing Neural Networks"}, {"paperId": "5934400081d9541339da0f16d2613263f1a4c2a2", "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections"}, {"paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "title": "One weird trick for parallelizing convolutional neural networks"}, {"paperId": "4211bff1388da30a3b7dfd35d6aef2032900ca5c", "title": "Good debt or bad debt: Detecting semantic orientations in economic texts"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "44ee91d83d3b804780d8ec43ee5af0e41d3b0787", "title": "Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment"}, {"paperId": null, "title": "Falcon-40B: an open large language model with state-of-the-art performance (2023)"}, {"paperId": null, "title": "Twitter financial news senti-ment"}, {"paperId": null, "title": "Data-centric FinGPT: Democra-tizing internet-scale data for financial large language models"}]}