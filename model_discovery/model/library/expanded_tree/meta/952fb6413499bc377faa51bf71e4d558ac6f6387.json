{"paperId": "952fb6413499bc377faa51bf71e4d558ac6f6387", "abstract": "Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models."}, "embedding": {"model": "specter_v2", "vector": [0.3603019714355469, 0.08764564245939255, -0.3759298324584961, -0.03238570690155029, -0.4372158348560333, -0.04210742190480232, 0.4410054087638855, 0.004618256352841854, -0.4704257547855377, -0.5213241577148438, 0.7069007158279419, -0.0660894587635994, 0.34723514318466187, 0.4024055302143097, -0.01973331719636917, 0.4401722550392151, -0.7092315554618835, 0.34253978729248047, -0.20612280070781708, -0.46824321150779724, 0.2146989107131958, -0.7552947998046875, -1.286893367767334, 0.053709786385297775, 0.6418412923812866, 0.5372458696365356, 0.6733291745185852, 1.239508867263794, -0.45043495297431946, 0.0794631764292717, 0.6977872252464294, 0.015830181539058685, -0.22159206867218018, -0.14819681644439697, -0.3367079794406891, -0.33400774002075195, 0.19832423329353333, -0.2815028131008148, -0.22959095239639282, 0.6315525770187378, 0.0555388517677784, 0.2644622325897217, 0.41175588965415955, -0.23945878446102142, -0.4024292528629303, 0.7440873384475708, 0.3397831916809082, 0.6672226786613464, -0.08019199222326279, -0.6089048385620117, 1.252280831336975, -1.4260072708129883, 0.09275192767381668, 1.5675771236419678, 0.3630092442035675, 0.32733428478240967, -0.12613672018051147, -0.38498160243034363, 0.8762987852096558, 0.013590133748948574, -0.7714787721633911, -0.5161624550819397, -0.388704389333725, 0.10086701065301895, 2.0572757720947266, -0.05754264444112778, 0.14900106191635132, 0.4052108824253082, -0.001435913611203432, 1.5140966176986694, -0.5125100612640381, -0.9133232235908508, -0.212632417678833, -0.4698353111743927, 0.7247999906539917, 0.4850864112377167, -0.06781146675348282, -0.08129659295082092, -0.6831632256507874, -0.12583662569522858, 0.002384814666584134, 0.02905411645770073, 0.08643854409456253, -0.05799021199345589, -0.030368434265255928, 0.6389319896697998, 0.10537946224212646, 0.9043916463851929, 0.0009964179480448365, 0.7752849459648132, 0.5017971992492676, 0.07508694380521774, 0.31013187766075134, 0.4462432861328125, -0.07867737859487534, 0.0032233966048806906, -1.2768090963363647, 0.35196346044540405, 0.1608339101076126, 1.406463623046875, -0.4747803211212158, -0.0011792040895670652, -0.6271882057189941, 0.29576727747917175, 1.2986366748809814, 0.5006546378135681, 0.5627566576004028, -0.5088496804237366, 0.20065045356750488, -0.9107479453086853, -0.18482854962348938, -0.7262312769889832, -0.12139567732810974, -0.3594573438167572, -0.6071322560310364, -1.6023900508880615, -0.7225035429000854, 0.2094142585992813, -0.567048192024231, 0.6809571981430054, -0.48722270131111145, 0.34611156582832336, -0.05144801735877991, 0.3745829463005066, 0.661943793296814, 0.8169819116592407, 0.2364746481180191, 0.038536641746759415, 0.989336371421814, -1.00473952293396, -0.47929948568344116, -1.5228402614593506, 1.0972234010696411, -0.7096092700958252, 0.6288371682167053, -0.31229567527770996, -1.335968017578125, -0.7635574340820312, -0.7904825210571289, -0.562369704246521, -0.49954506754875183, 0.17802700400352478, 0.7930064797401428, 0.2510458827018738, -1.0725237131118774, 0.2648828327655792, -0.3111873269081116, -0.26235389709472656, 0.3241008520126343, -0.11939358711242676, 0.6780100464820862, -0.7860493063926697, -1.2290786504745483, 0.10113439708948135, -0.1635630875825882, -0.5064088106155396, -0.05964883789420128, -0.36017704010009766, -1.4098460674285889, 0.18010301887989044, 0.17739731073379517, -0.38634851574897766, 1.1384917497634888, 0.0418417751789093, -1.0082757472991943, 0.5739246010780334, -0.8208627700805664, 0.2530324459075928, -0.2527216076850891, -0.6859756708145142, -0.31468358635902405, -0.4901692271232605, -0.33847829699516296, 0.47078853845596313, 0.48766133189201355, 0.2635008692741394, 0.055823616683483124, 0.34764957427978516, -0.5462042689323425, 0.2193402647972107, -0.13936570286750793, 1.2308788299560547, -1.030949592590332, -0.15208503603935242, 0.4036675989627838, 0.4034351706504822, -0.05622900649905205, -0.6565738916397095, -0.2866939604282379, -0.7924411296844482, 1.2644543647766113, -0.18693803250789642, 1.3226255178451538, -0.965601921081543, -0.362983763217926, -0.21370549499988556, -0.09356783330440521, -0.08095736056566238, -0.9097025990486145, 0.3357856273651123, 0.061623234301805496, 0.4695977568626404, -0.17169280350208282, -1.3806452751159668, 0.09473204612731934, -0.3911034166812897, -0.8342402577400208, -0.5148838758468628, 0.09197865426540375, 1.083508849143982, -1.0345003604888916, -0.07943200320005417, -0.3188002109527588, 0.4340539574623108, -1.1727418899536133, 1.186091423034668, -0.8548798561096191, 0.17811807990074158, -0.2900477349758148, 0.21645782887935638, -0.07778779417276382, -0.6421818137168884, 0.4758002758026123, -0.1901548206806183, 0.22413168847560883, 0.4150952398777008, 0.12254694849252701, 1.3682231903076172, -0.6988276839256287, 0.5045084357261658, -0.14856716990470886, -0.1782725602388382, 0.20650047063827515, 0.21791799366474152, -0.32295122742652893, -0.43566107749938965, 0.42486876249313354, 0.6166635751724243, -0.5915530920028687, 0.2617020905017853, 1.165479063987732, 1.2897753715515137, -0.7324283123016357, 0.2619522213935852, 0.3759658932685852, -0.3110232949256897, 0.4806574881076813, 0.6918696761131287, 0.7228637337684631, 0.3846766948699951, 0.24806703627109528, -0.039062339812517166, 0.313677579164505, -0.8612632751464844, -0.39581406116485596, 0.7764994502067566, 0.7264899015426636, 1.0569690465927124, 0.3283053934574127, -0.5728342533111572, -0.5967395305633545, 0.7093262672424316, 0.9753063321113586, 1.7753323316574097, 0.1241769939661026, -0.19111481308937073, -0.7428407669067383, -0.15781807899475098, -0.39455485343933105, -0.052710920572280884, -0.041021693497896194, -0.08332101255655289, -0.7678045630455017, -0.9402981996536255, 0.6301013827323914, -0.016839737072587013, 0.6187711358070374, -0.7200013399124146, -0.39734339714050293, -0.37606117129325867, 0.016655322164297104, -1.1009323596954346, -0.8944591879844666, 0.1759420782327652, -0.6053624749183655, 0.11102476716041565, 0.018304161727428436, 0.08157571405172348, -0.06162194907665253, -0.267177015542984, 1.2721266746520996, -0.29309210181236267, -0.3411605954170227, 0.48731058835983276, 0.18675753474235535, -0.5191141366958618, -0.6615330576896667, 0.4702840745449066, 0.1258612722158432, -0.3792708218097687, 0.8010533452033997, 0.5924138426780701, -0.01633370667695999, -0.17064961791038513, -0.22884699702262878, 0.256303608417511, 0.3366038203239441, -0.17877896130084991, 0.9833633303642273, -0.8709191679954529, -0.1559683084487915, -1.1516810655593872, 0.804745078086853, -0.23398296535015106, -0.264522522687912, 0.3239403963088989, -0.7010257244110107, -0.4439947307109833, 0.3149971663951874, -0.7136015892028809, -0.21003535389900208, -0.9223780632019043, 0.4229085445404053, -0.5246124863624573, -0.24060876667499542, 0.47026440501213074, 0.3108733594417572, 0.32551220059394836, -0.06996539235115051, 0.7635529637336731, 0.3961220681667328, -0.5202279090881348, 0.5874130129814148, -0.7146377563476562, 0.31827512383461, 0.2031625360250473, -0.36924082040786743, -0.3786727488040924, -0.2368927299976349, -1.0130970478057861, -0.3252222537994385, -0.48740270733833313, -0.21859237551689148, -0.14796440303325653, 0.3139282464981079, -0.5698350667953491, -0.1771591156721115, -0.20806021988391876, -1.3210961818695068, -0.11507885903120041, 0.592549204826355, -0.27488258481025696, -0.14461582899093628, -0.9118188619613647, -1.3250645399093628, -0.3323759138584137, -1.0466150045394897, -1.4320056438446045, 0.8021433353424072, -0.046449627727270126, -0.7624362707138062, -0.4149852991104126, -0.06578703969717026, -0.44502368569374084, 1.2645050287246704, -0.7674517035484314, 0.7709382176399231, -0.1182604432106018, -0.33379051089286804, -0.5828726291656494, 0.012910157442092896, 0.1246723011136055, -0.36183851957321167, 0.24914026260375977, -0.8591498136520386, -0.0493575781583786, -0.16771826148033142, -0.2091871201992035, 0.30541563034057617, 0.503624677658081, 0.7102679014205933, 0.0023560412228107452, -0.6746936440467834, 0.5088164806365967, 1.2957907915115356, -0.7181561589241028, 0.06647417694330215, -0.11592941731214523, 1.2455902099609375, -0.007593053858727217, -0.31518790125846863, 0.5354981422424316, 0.10643386840820312, 0.3817451298236847, 0.18018564581871033, 0.2389257848262787, -0.3095529079437256, -0.27114397287368774, 0.689166247844696, 2.241006374359131, 0.5049624443054199, -0.2235122174024582, -1.0496467351913452, 0.6047273278236389, -1.0833486318588257, -0.5149054527282715, 0.5493453741073608, 0.7526058554649353, 0.3663621246814728, -0.6129686832427979, -0.4365350604057312, -0.7180249094963074, 0.354887455701828, 0.4733963906764984, -0.4375693202018738, -0.8724108934402466, 0.020408470183610916, 0.25420302152633667, -0.07836443185806274, 0.7385413646697998, -0.28662383556365967, 0.6756512522697449, 14.573801040649414, 0.9990559816360474, -0.24240706861019135, 0.8862366676330566, 0.9533973932266235, -0.3630017638206482, -0.22947558760643005, -0.2686287760734558, -1.5584309101104736, 0.07111489027738571, 1.5022239685058594, 0.12874425947666168, 0.4306395947933197, 0.2823444604873657, 0.06743331253528595, 0.37642940878868103, -0.6443843245506287, 0.75873202085495, 0.6085755228996277, -1.0984888076782227, 0.5504529476165771, 0.17297667264938354, 0.2284170687198639, 0.6651527881622314, 1.0074448585510254, 0.9504445791244507, 0.25751179456710815, -0.6249943971633911, 0.34036555886268616, 0.44454336166381836, 0.9824528694152832, -0.01640080288052559, 0.4557988941669464, 0.5086415410041809, -0.8978388905525208, -0.1825430691242218, -0.37189680337905884, -1.1397374868392944, 0.30551818013191223, 0.037948135286569595, -0.2547869384288788, -0.7400081157684326, -0.283610463142395, 0.8174028396606445, -0.18707169592380524, 0.26204714179039, 0.13652795553207397, 0.6698921918869019, -0.34149667620658875, -0.2622254192829132, 0.5579217076301575, 0.48126479983329773, 0.37132877111434937, 0.5309066772460938, 0.1809086948633194, 0.004609168507158756, 0.07272034138441086, 0.48786231875419617, -0.4288741946220398, 0.14210964739322662, -0.5950368046760559, -0.31565117835998535, 0.21353065967559814, 0.4204241931438446, 0.3081645369529724, 0.1076875850558281, -0.5783371329307556, 0.24086526036262512, 0.476748526096344, 0.19224081933498383, -0.06031888350844383, 0.1644412875175476, 0.26870712637901306, -0.6294774413108826, 0.08690489083528519, 0.4858333468437195, -0.31516727805137634, -0.721143901348114, -0.5845080614089966, -0.7252717614173889, 0.39611122012138367, -0.6959081888198853, -0.5318765044212341, 0.8398995995521545, -0.06024600565433502, -0.29408395290374756, -0.09947668761014938, -0.3340347111225128, -0.1411488652229309, 0.7130200862884521, -1.208203673362732, -0.8175299763679504, 0.4624468982219696, -0.8034195899963379, -0.0027823743876069784, 0.007825766690075397, 1.3539063930511475, 0.13750715553760529, -0.36021414399147034, 0.270302414894104, 0.3501027822494507, -0.29438480734825134, -0.259736567735672, -0.34378063678741455, 0.9331150054931641, 0.5014532804489136, -0.27454864978790283, 0.30709972977638245, 0.01602725125849247, -0.09544699639081955, -1.246131181716919, -0.2570890784263611, 0.9257673621177673, -0.8326156139373779, -0.5904261469841003, -0.7947818636894226, -0.9761132597923279, 0.3978325426578522, 0.3660697042942047, -0.1588965207338333, 0.4109056890010834, 0.3458901047706604, -0.6371220946311951, 0.08032629638910294, -0.5215224623680115, 0.12132008373737335, 0.3576565086841583, -0.8351919651031494, 0.05102258175611496, -0.16221238672733307, 0.898331344127655, -1.112278699874878, -0.2776523530483246, -0.26036450266838074, 0.3347441256046295, 0.25034910440444946, 0.9447847604751587, -0.18758226931095123, 1.0424869060516357, 1.1789696216583252, -0.3835993707180023, -0.38515403866767883, -0.025771794840693474, -0.8590533137321472, -0.6149837374687195, -0.13343971967697144, 0.6371748447418213, -0.09300307929515839, 0.016702909022569656, 0.8452312350273132, 0.4729596674442291, -0.7175896763801575, -0.5422626733779907, -0.19272808730602264, 0.23203237354755402, -0.635308563709259, 0.49350640177726746, -0.2754969000816345, 0.021051818504929543, 0.13253924250602722, 0.24890710413455963, 0.5677776336669922, -0.033182740211486816, -0.614949643611908, 0.48144757747650146, 0.09660883992910385, -0.2531522810459137, -0.8696732521057129, -0.40850770473480225, -1.67000150680542, -0.1914045810699463, -0.8710811734199524, -0.06269661337137222, -0.40229931473731995, -0.2454937845468521, -0.0216373298317194, -0.02016223780810833, -0.13662494719028473, 0.2580243945121765, -0.13699203729629517, -0.2595764100551605, -0.45587030053138733, -1.0190587043762207, 0.6485884189605713, 0.7495800256729126, -0.5239712595939636, -0.012565395794808865, -0.30788108706474304, 0.060137759894132614, 0.281457781791687, 0.277383029460907, -0.2501586675643921, -0.6903174519538879, -1.7094439268112183, 0.49062108993530273, -0.1453893780708313, -0.41080522537231445, -0.5489588379859924, 0.5797293782234192, 0.4679880142211914, -0.3952789902687073, -0.05844498798251152, 0.19102294743061066, -0.7575502991676331, -0.6795223951339722, 0.3317147195339203, -0.9318702220916748, 0.3506886661052704, 0.5948512554168701, -0.5304860472679138, -0.2231430858373642, 0.6895574331283569, -0.1282268911600113, -0.9383243918418884, -0.8171347379684448, 0.32098188996315, -0.3457515239715576, 0.15335702896118164, -0.6647578477859497, 0.269234299659729, -0.9469683766365051, -0.7206414937973022, -0.10981887578964233, 0.27760830521583557, -0.29468604922294617, 1.0922869443893433, 0.7556458115577698, -0.8862720131874084, -0.12153700739145279, 0.35116395354270935, -0.10173091292381287, 0.25883248448371887, 0.6472591757774353, 0.4881317615509033, -0.5213383436203003, 0.7414343953132629, 0.7557865977287292, -0.007071235217154026, -1.191414713859558, 0.11649724841117859, 0.558506190776825, -0.6242842078208923, -0.1141509935259819, 1.1634783744812012, -0.3588225841522217, -0.8048283457756042, -0.04795064404606819, -1.2038334608078003, -0.5699887871742249, -0.4926677346229553, 0.917172908782959, -0.03693049028515816, -0.04494592919945717, -0.24195314943790436, -0.580329954624176, 0.27146512269973755, -0.3915225565433502, -0.4725460410118103, 0.4638236463069916, -0.5177603960037231, -0.8031350374221802, 0.48724615573883057, 1.1852658987045288, -0.7113897800445557, -0.2699095904827118, -0.8621206283569336, -0.2187725305557251, 0.04545019194483757, 0.6168334484100342, -0.34786874055862427, -0.3406940996646881, 0.7184370756149292, 0.36751648783683777, 0.2731713056564331, 0.34532058238983154, -0.06046707183122635, 0.6702982783317566, 0.7126190066337585, -0.05146421119570732, -0.3432009220123291, -0.8290174603462219, 1.7422252893447876, 1.372839331626892, -0.9831401705741882, 0.3308553695678711, 1.645885822654236e-05, -0.34043240547180176, 0.5898017287254333, 0.21805846691131592, 0.07047246396541595, 0.9116090536117554, -0.01675904169678688, -0.08522346615791321, 0.13348355889320374, -1.1798123121261597, -0.013216382823884487, 1.1401216983795166, 0.609609067440033, 1.074089765548706, 0.2428099811077118, -0.08443371951580048, 0.6858989000320435, 0.25964924693107605, 0.06566084176301956, 0.27780142426490784, 0.6187902688980103, -0.2531624138355255, 0.14478516578674316, -0.0736624076962471, 0.75551438331604, -0.7753067016601562, -1.1171936988830566, 0.34893250465393066, 0.23763887584209442, -0.08973173052072525, 0.4651739299297333, 1.0730880498886108, 0.1965487003326416, 0.04258102551102638, 0.2558826506137848, 0.4132337272167206, -0.7847126126289368, -0.05314910039305687, 0.169295534491539, -0.8623172640800476, -0.18723812699317932, 0.14087355136871338, -0.6770868301391602, -0.12529227137565613, -0.36531341075897217, 0.5439194440841675, -0.21227601170539856, 0.19025276601314545, 1.1622258424758911, 0.9565035700798035, 0.36161336302757263, -0.5693586468696594, -0.7260551452636719, -0.16808737814426422, -1.2075306177139282, 0.12344591319561005, -0.5104894638061523, -0.1451098620891571, 0.16941377520561218, -0.005547925364226103, -0.16320431232452393]}, "authors": [{"authorId": "48737592", "name": "Tianyu Fu"}, {"authorId": "2308095892", "name": "Haofeng Huang"}, {"authorId": "6636914", "name": "Xuefei Ning"}, {"authorId": "2184278623", "name": "Genghan Zhang"}, {"authorId": "2307968175", "name": "Boju Chen"}, {"authorId": "2307981105", "name": "Tianqi Wu"}, {"authorId": "2278587594", "name": "Hongyi Wang"}, {"authorId": "2278457016", "name": "Zixiao Huang"}, {"authorId": "2242132627", "name": "Shiyao Li"}, {"authorId": "2283520504", "name": "Shengen Yan"}, {"authorId": "144290348", "name": "Guohao Dai"}, {"authorId": "2177314863", "name": "Huazhong Yang"}, {"authorId": "2288064224", "name": "Yu Wang"}], "references": [{"paperId": "edd705ebe3546272b7fe952e2ed6088200adad76", "title": "Retrieval Head Mechanistically Explains Long-Context Factuality"}, {"paperId": "5be7e6b04c5a240cff340034aae2b57c677e211f", "title": "A Survey on Efficient Inference for Large Language Models"}, {"paperId": "3d776d7a659795cbc719cf80019cae0dd6bfe69e", "title": "CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models"}, {"paperId": "3c6f2e0c5ff5dff6151c3e6489378a53318a75b4", "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect"}, {"paperId": "8b8ac20444ef31690f1ad1d80181d121fbde5bab", "title": "Evaluating Quantized Large Language Models"}, {"paperId": "f288e2238ac8725baa7ca9874bbc3fed1e89a632", "title": "Data Engineering for Scaling Language Models to 128K Context"}, {"paperId": "03edb4f89aeb59a8ce056ec6b75a3e259350593f", "title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K"}, {"paperId": "a1bcf68d6ed2fec1ecaf16b67f2d19bc20c00ee6", "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models"}, {"paperId": "5851121df5ce46be5faea265c868ec0beabfce96", "title": "Efficient Large Language Models: A Survey"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "d6a8e685b46f79056076a6b65803d49493a99dca", "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"}, {"paperId": "4d76206515d6b33903937474273885476fc2771e", "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "title": "SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "d203c764fb5dec2b053be667c8b06e516ea6ef10", "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "43d8ef2c8d361aaec01131104a287fcc45ccab55", "title": "The geometry of hidden representations of large transformer models"}, {"paperId": "b0c5c673c690c644a7d4af73adb783bd98486181", "title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences"}, {"paperId": "240300b1da360f22bf0b82c6817eacebba6deed4", "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "cc27ec53160d88c25fc5096c0df65536eb780de4", "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "d9ede71b944460c1293294c9f64bd88a8cc400de", "title": "A Flexible Framework for Multi-Objective Bayesian Optimization using Random Scalarizations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "edf9b7367660d7f8255633717bf050f000a7c8fd", "title": "Introducing the LCC Metaphor Datasets"}, {"paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers"}, {"paperId": "75895ce98904e8afaaa248f081a1da501bd2dbe2", "title": "Toward Semantics-Based Answer Pinpointing"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length?"}, {"paperId": "4b56eef2862f7f553686f1dd190c56017122a6a0", "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"}, {"paperId": null, "title": "Together Computer"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "a55b328b7ef4674879c74ee0a40acc493a801ef2", "title": "On a bicriterion formation of the problems of integrated system identification and system optimization"}, {"paperId": null, "title": "Acknowledgement This work was supported by National Natural Science Foundation of China (No. 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164), Tsinghua EE Xilinx"}, {"paperId": null, "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable"}, {"paperId": null, "title": "Llm-mq: Mixed-precision quantization for efficient llm deployment"}]}