{"paperId": "4d5cf66cc91ba4bbb8197d7e5711e11043002ad2", "abstract": "Many studies have been conducted to improve the efficiency of Transformer from quadric to linear. Among them, the low-rank-based methods aim to learn the projection matrices to compress the sequence length. However, the projection matrices are fixed once they have been learned, which compress sequence length with dedicated coefficients for tokens in the same position. Adopting such input-invariant projections ignores the fact that the most informative part of a sequence varies from sequence to sequence, thus failing to preserve the most useful information that lies in varied positions. In addition, previous efficient Transformers only focus on the influence of sequence length while neglecting the effect of hidden state dimension. To address the aforementioned problems, we present an efficient yet effective attention mechanism, namely the Dynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length by input-sensitive dynamic projection matrices and achieves linear time and space complexity by jointly optimizing the sequence length and hidden state dimension while maintaining state-of-the-art performance. Specifically, we first theoretically demonstrate that the sequence length can be compressed non-destructively from a novel perspective of information theory, with compression matrices dynamically determined by the input sequence. Furthermore, we show that the hidden state dimension can be approximated by extending the Johnson-Lindenstrauss lemma, optimizing the attention in bilinear form. Theoretical analysis shows that DBA is proficient in capturing high-order relations in cross-attention problems. Experiments over tasks with diverse sequence length conditions show that DBA achieves state-of-the-art performance compared with various strong baselines while maintaining less memory consumption with higher speed.", "venue": "arXiv.org", "year": 2022, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2211.16368", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "The Dynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length by input-sensitive dynamic projection matrices and achieves linear time and space complexity by jointly optimizing the sequencelength and hidden state dimension while maintaining state-of-the-art performance."}, "embedding": {"model": "specter_v2", "vector": [0.08227098733186722, 0.16252757608890533, -0.4725930392742157, 0.04575861990451813, -0.5569400191307068, -0.44902461767196655, 0.2780402600765228, -0.08454418182373047, -0.3543190658092499, -0.1699085831642151, 0.6723945140838623, 0.20362135767936707, -0.09507580101490021, 0.21418048441410065, -0.30278480052948, 0.029596954584121704, -0.8597059845924377, 0.3719901144504547, -0.14106883108615875, -0.5105307102203369, 0.44179633259773254, -0.60884028673172, -0.8190373778343201, 0.26809099316596985, 0.3738230764865875, 0.7110718488693237, 0.6419993042945862, 0.8691765666007996, -0.47246503829956055, 0.43978771567344666, 0.43753525614738464, -0.8560765385627747, 0.6330885291099548, -0.00836728885769844, -0.636742889881134, -0.6285194754600525, 0.414506733417511, -0.6496744155883789, -0.723179042339325, 0.7550665140151978, -0.5538241863250732, 0.3770623207092285, 0.37637126445770264, -0.5722286701202393, -0.5048447847366333, 0.8649597764015198, 0.5709145069122314, 0.9150921106338501, 0.2547982931137085, -0.76444011926651, 1.9773565530776978, -1.0893172025680542, 0.055598173290491104, 1.374871850013733, 0.5677629113197327, 0.2943485975265503, -0.23911336064338684, -0.3745698034763336, 0.5563022494316101, 0.7264363765716553, -0.6487435102462769, -0.025102578103542328, 0.12362714111804962, -0.13500836491584778, 1.4011807441711426, -0.10008052736520767, 0.1838926374912262, 0.4220484793186188, -0.146527960896492, 1.4216253757476807, 0.20025594532489777, -0.6419304013252258, -0.5789228081703186, -0.18009580671787262, 0.49813053011894226, 0.7417164444923401, -0.2554096579551697, 0.3626790940761566, -1.251333236694336, 0.16680221259593964, 0.4536263644695282, 0.12347621470689774, 0.13842789828777313, -0.1510011851787567, -0.034043505787849426, 0.5298762321472168, 0.3710825443267822, 0.5412496328353882, -0.13218171894550323, 0.6915163397789001, 0.8865324258804321, 0.3599485456943512, -0.4099448621273041, 0.42343875765800476, 0.26478487253189087, -0.3075558841228485, -0.5367966294288635, 0.05354873463511467, 0.12129706144332886, 0.6851760149002075, -0.11098827421665192, 0.0017712273402139544, -0.30266273021698, 0.2341984063386917, 1.0737370252609253, -0.45394837856292725, 0.32990387082099915, -0.6733114123344421, 0.43565139174461365, -0.7640262842178345, -0.2638809084892273, -1.2483839988708496, 0.21240277588367462, 0.08368474245071411, -0.8802499771118164, -1.29762864112854, -0.5789247155189514, 0.7271533012390137, -0.24108150601387024, 0.3713464140892029, -0.7601991295814514, 0.23295056819915771, 0.09296361356973648, 0.6759406924247742, 0.3631989061832428, 0.8393722176551819, -0.1435893326997757, 0.05546364188194275, 0.9213144779205322, -0.7455297708511353, -0.8176071643829346, -0.8708009719848633, 0.41725701093673706, -0.32996636629104614, 0.4518279433250427, 0.04433799535036087, -0.9789832234382629, -0.911981463432312, -0.8421751260757446, -0.22371704876422882, -0.32603809237480164, -0.011270727962255478, 0.8608357310295105, 0.004480344709008932, -0.7950847148895264, 1.00338876247406, -0.3747626841068268, 0.03672628104686737, 0.2758065462112427, 0.22045370936393738, 0.10970603674650192, -0.3392428159713745, -1.522802710533142, 0.42638397216796875, 0.36095988750457764, -0.2199002504348755, -0.07893384993076324, -0.7198213934898376, -1.2736883163452148, 0.07979615777730942, 0.5174594521522522, -0.22095675766468048, 0.9542892575263977, 0.4570556879043579, -1.2489748001098633, 0.4672374725341797, -0.14417773485183716, 0.24419265985488892, 0.30294883251190186, -0.5092406272888184, -0.09778135269880295, -0.28503283858299255, 0.28135931491851807, 0.12042759358882904, 0.05225403979420662, -0.42899569869041443, -0.1513359546661377, 0.1496104896068573, -0.3310543894767761, 0.000691403285600245, -0.13809187710285187, 0.701696515083313, -0.38815316557884216, -0.22177040576934814, 0.2697161138057709, 0.6893081068992615, 0.245708167552948, -0.31752070784568787, -0.5933075547218323, -0.9314174652099609, 0.7684153318405151, -0.1096978485584259, 1.0623291730880737, -0.767899215221405, -0.5762690901756287, -0.4293935000896454, -0.3305306136608124, -0.2538304924964905, -0.7091221213340759, 0.9565645456314087, -0.3203548192977905, 0.5437635183334351, -0.029798075556755066, -0.772999107837677, 0.1163402795791626, 0.13482511043548584, -0.9320390224456787, -0.1606321483850479, -0.08744215220212936, 0.9021326303482056, -0.9778502583503723, -0.015195392072200775, 0.2911304831504822, -0.14821849763393402, -0.8440065383911133, 1.389054298400879, 0.15100125968456268, -0.049912337213754654, -0.07032447308301926, -0.526321530342102, -0.14094197750091553, -0.30804774165153503, 0.22072696685791016, -0.5000746846199036, 0.10852720588445663, 0.9958192706108093, -0.541276752948761, 1.2519394159317017, -0.4440608620643616, 0.7843204140663147, -0.4989372193813324, -0.5612668395042419, 0.5401982069015503, 0.005173424258828163, 0.3256266117095947, -0.34425827860832214, 0.033119894564151764, 0.006179410964250565, -0.5466015934944153, -0.03112831339240074, 1.0342164039611816, 0.8582041263580322, -0.6930925846099854, 0.20318105816841125, 0.3861221671104431, -0.28452304005622864, 0.75527024269104, 0.5413052439689636, 0.4277963936328888, 0.4485546946525574, 0.7661098837852478, 0.24543091654777527, 0.2937472462654114, -0.7684000730514526, 0.08326646685600281, 0.3186870217323303, 0.7238929271697998, 0.7783414721488953, 0.5880153179168701, -0.7601844668388367, -0.7636720538139343, 0.100676529109478, 0.5191707611083984, 1.3719531297683716, 0.5055543780326843, -0.7138770818710327, -0.35625457763671875, -0.21888162195682526, -0.47952014207839966, -0.2113906443119049, -0.31339848041534424, -0.3621686100959778, -0.6602126359939575, -0.4741918742656708, 0.9427230358123779, 0.4717898368835449, 1.1569095849990845, -0.5292531251907349, -0.14021699130535126, -0.12835101783275604, -0.0728742703795433, -0.8697460889816284, -0.9396710395812988, 0.433015376329422, -0.06626899540424347, -0.2528972625732422, 0.4298992156982422, -0.020659402012825012, -0.35646340250968933, -0.8468559980392456, 0.8072115778923035, -0.7475342750549316, -0.3839356601238251, -0.0025510212872177362, 0.2971072793006897, -0.40455156564712524, -0.5575549006462097, 0.41243600845336914, 0.10940717160701752, -0.3687055706977844, 0.4560333788394928, 0.314047634601593, 0.3194708824157715, -0.013238671235740185, -0.38031747937202454, 0.28403589129447937, 0.25009116530418396, 0.40999171137809753, 0.48766690492630005, -0.51569664478302, -0.2928088307380676, -0.703370213508606, 0.7459763884544373, 0.3624548316001892, -0.5990171432495117, -0.015536531805992126, -0.5369983315467834, -0.3039851486682892, 0.5667836666107178, -0.6928082704544067, 0.1944250762462616, -0.9371567368507385, 0.21315282583236694, -0.6845993995666504, -0.23902270197868347, 0.2958960235118866, 0.21346358954906464, 0.426185667514801, 0.16292771697044373, 0.6604600548744202, 0.20926693081855774, -0.18120640516281128, 0.29910609126091003, -0.8312151432037354, 0.7375836968421936, 0.02687671408057213, 0.21447737514972687, -0.31675371527671814, 0.08020953088998795, -0.5447530746459961, -0.6920015811920166, -0.5253183245658875, -0.8494898080825806, -0.07049164921045303, 0.12705565989017487, -0.05708221718668938, -1.505091667175293, -0.34531447291374207, -1.1709963083267212, -0.006699573248624802, -0.12803131341934204, -0.1119823232293129, -0.539643406867981, -0.803412675857544, -1.6138296127319336, -0.6180543303489685, -0.5146973729133606, -1.0887428522109985, 0.0688260942697525, -0.13060897588729858, -0.2684727907180786, -0.1777600347995758, 0.027222992852330208, -0.31851693987846375, 0.3636094927787781, -0.5104613900184631, 0.7092090845108032, -0.15973855555057526, -0.5526522994041443, -0.031165793538093567, 0.18221236765384674, 0.5833198428153992, 0.24699033796787262, 0.1541658192873001, -0.38048678636550903, 0.27361905574798584, -0.4345168173313141, -0.24161933362483978, 0.06232525780797005, 0.002830610377714038, 0.7250070571899414, -0.6691460013389587, -0.2576224207878113, 0.39868268370628357, 1.6285759210586548, -0.322994202375412, 0.23317721486091614, 0.38749009370803833, 1.2176731824874878, 0.5503795146942139, -0.07846806198358536, 0.5963244438171387, 0.5166134834289551, 0.5440305471420288, 0.11829528957605362, 0.33524614572525024, 0.07292135804891586, -0.8461578488349915, 0.4926532804965973, 2.1509828567504883, 0.19421064853668213, 0.2835756540298462, -0.9421855807304382, 0.946153998374939, -1.1497118473052979, -1.1723893880844116, 0.6413053274154663, 0.40311017632484436, 0.3450256586074829, -0.6996663212776184, -0.18932577967643738, 0.18536396324634552, 0.4559565782546997, 0.4373079538345337, -0.24628858268260956, -0.83875572681427, 0.12605126202106476, -0.13592809438705444, 0.06610988825559616, 0.5267603993415833, -0.22029536962509155, 0.4922286570072174, 15.223688125610352, 0.7532967925071716, -0.002877769758924842, 0.4398929476737976, 0.48203331232070923, 0.23300765454769135, -0.3184824585914612, -0.1529993712902069, -1.3275872468948364, 0.09495559334754944, 1.0513299703598022, -0.14776356518268585, 0.5374774932861328, 0.034615177661180496, -0.15776263177394867, 0.40966689586639404, -0.7137523293495178, 0.9248208999633789, 0.6597557663917542, -1.1500518321990967, 0.21844744682312012, 0.35580888390541077, 0.2022092640399933, 0.03852854296565056, 0.8353081345558167, 0.42990049719810486, 0.6702594757080078, -0.446592777967453, 0.46091964840888977, 0.3078361451625824, 1.0772567987442017, -0.4049016237258911, 0.3272399604320526, 0.4215935170650482, -1.0561689138412476, -0.35638511180877686, -0.7878013253211975, -0.9843610525131226, 0.2747139036655426, 0.05670573189854622, -0.5882390141487122, 0.06422357261180878, 0.21702320873737335, 1.2092878818511963, 0.49405354261398315, 0.3116031289100647, -0.29095667600631714, 0.13331511616706848, 0.023665357381105423, 0.04998474195599556, 0.33042365312576294, 0.14970910549163818, 0.2966570258140564, -0.10522481799125671, -0.011327202431857586, 0.07653754204511642, 0.2541087567806244, 0.5647419095039368, -0.35898369550704956, -0.020900223404169083, -0.7192205190658569, -0.2792103588581085, -0.10608375072479248, 0.6315636038780212, 1.0570409297943115, 0.413627028465271, -0.19969744980335236, 0.2975521683692932, 0.7041282653808594, -0.1572847068309784, -0.10729920119047165, -0.22698979079723358, 0.5721860527992249, -0.34105175733566284, 0.0064587644301354885, 0.30718907713890076, -0.6165304183959961, -0.38562479615211487, -0.5281081795692444, -0.3555890917778015, 0.6107523441314697, -0.7693989276885986, -1.1340197324752808, 1.0129450559616089, 0.04574483633041382, -0.6418300271034241, -0.11705639213323593, -0.5042547583580017, 0.35720139741897583, 0.2563839256763458, -0.9652097225189209, -0.3763387203216553, 0.2517370879650116, -0.3209201991558075, -0.07211808115243912, 0.1908460259437561, 0.7865700125694275, 0.3439289629459381, -0.18632358312606812, 0.09136278182268143, 0.06069614365696907, 0.33556798100471497, -0.33060696721076965, -0.8162652850151062, 0.6507487893104553, 0.37026911973953247, 0.2010415643453598, 0.4118398129940033, -0.16959169507026672, 0.07782798260450363, -0.7444478273391724, 0.00325557729229331, 0.6267813444137573, -0.7721489667892456, -0.193324014544487, -0.9408872127532959, -1.142640233039856, 0.447008341550827, 0.3631010949611664, -0.3044015169143677, 0.31103259325027466, 0.025285353884100914, -0.6168316602706909, -0.4452245533466339, -0.37838441133499146, -0.008045977912843227, 0.4432329833507538, -0.9690333604812622, -0.7038595676422119, -0.5165076851844788, 0.3051396906375885, -1.14210045337677, -0.8182522654533386, -0.1162167564034462, 0.10670570284128189, -0.0718490332365036, 0.942427396774292, -0.39950209856033325, 1.3057270050048828, 0.8417527675628662, -0.11060880869626999, -0.6729997396469116, -0.2552194595336914, -0.8433017134666443, 0.04816444590687752, 0.21846145391464233, 0.16833467781543732, -0.022245610132813454, 0.3017182946205139, 0.5126872062683105, 0.16332414746284485, -0.48994767665863037, -0.43266889452934265, -0.41715988516807556, -0.2972491681575775, -0.4656820297241211, 0.2667500376701355, 0.027958814054727554, -0.07748495042324066, 0.09884601831436157, -0.03708134591579437, 0.46937838196754456, -0.3265629708766937, -0.4529012441635132, 0.1882777065038681, 0.035199765115976334, 0.10752552002668381, -0.4120762348175049, -0.7346458435058594, -1.7593005895614624, 0.12135815620422363, -1.214039921760559, 0.3194844126701355, -1.0252207517623901, -0.19828051328659058, 0.11130216717720032, -0.3292078971862793, 0.17083969712257385, 0.3502136170864105, -0.16450096666812897, -0.08885085582733154, -0.25217923521995544, -0.3872568607330322, 0.7679659724235535, 0.8981073498725891, -0.6181280016899109, 0.11573817580938339, -0.09104607254266739, -0.1279604136943817, -0.0440487265586853, 0.25785160064697266, -0.6804884672164917, -0.24747319519519806, -0.9785984754562378, 0.27799510955810547, 0.1699259728193283, -0.006203060504049063, -0.44171392917633057, 0.918332576751709, 0.12112216651439667, -0.3967733681201935, -0.5703442692756653, 0.6780536770820618, -1.1717336177825928, -0.44405266642570496, 0.4574384093284607, -1.0833450555801392, 0.466531366109848, -0.0006602490320801735, -0.5322444438934326, -0.5131207704544067, 0.6023579835891724, 0.10490772873163223, -0.8524858355522156, -0.657556414604187, 0.657171905040741, -0.46192893385887146, 0.17305153608322144, -0.19102656841278076, -0.2193092703819275, -1.052643895149231, -0.40449953079223633, 0.24035657942295074, 0.009085332974791527, -0.545444905757904, 1.0189908742904663, 0.486203670501709, -1.437094807624817, 0.10296065360307693, 0.3522266745567322, -0.03507133200764656, -0.032560791820287704, 0.258747398853302, 0.3141988515853882, -0.1805998980998993, 0.6285955309867859, -0.2201227992773056, 0.15730774402618408, -0.9782775044441223, 0.16149181127548218, 0.6274147033691406, -0.7710720300674438, -0.2543637156486511, 0.6997397541999817, -0.4106181859970093, -0.9332543611526489, 0.0361812561750412, -0.8093590140342712, -0.5137487053871155, -0.2878865897655487, 0.4948892891407013, 0.6323406100273132, -0.19882187247276306, -0.3141564130783081, -0.6576342582702637, 0.4274044334888458, -0.2047116905450821, -0.49591919779777527, 0.5502803921699524, -0.23707498610019684, -0.5471371412277222, 0.8810592293739319, 1.2552765607833862, -0.7099500894546509, -0.581758975982666, -0.6329227685928345, -0.315037339925766, -0.6307681202888489, -0.1871877759695053, -0.19464342296123505, -0.547527551651001, 0.8374377489089966, 0.24762655794620514, 0.6382754445075989, 0.07286770641803741, -0.11034327000379562, 0.3235726058483124, 0.6836011409759521, -0.0258158128708601, -0.3914365768432617, -0.12022774666547775, 1.3985868692398071, 1.571411371231079, -0.4699670672416687, 0.2989102900028229, -0.38886743783950806, -0.8862147927284241, 0.905160665512085, 0.4687349498271942, -0.2477295696735382, 0.3578019440174103, 0.20462150871753693, 0.0012945450143888593, 0.4342208504676819, -1.0344583988189697, -0.2741840183734894, 1.0440216064453125, 1.1886389255523682, 0.22099681198596954, 0.04157614707946777, 0.053097981959581375, 0.7079207301139832, -0.4469583332538605, -0.22786912322044373, 0.6607462167739868, 0.1514238715171814, 0.0052513922564685345, -0.27501872181892395, -0.09933629631996155, 0.7756910920143127, -0.9796910285949707, -0.6234082579612732, 0.1368238925933838, 0.6518004536628723, -0.24293628334999084, 0.575849175453186, 0.8148605823516846, -0.1922406256198883, 0.6539736390113831, -0.19635367393493652, 0.34996750950813293, -0.5156862735748291, -0.41457581520080566, -0.21964550018310547, -0.912634551525116, -0.2981696128845215, 0.06721161305904388, -0.673636257648468, -0.15727423131465912, -0.3796902596950531, 0.34574535489082336, 0.12927643954753876, 0.3479812741279602, 0.7370683550834656, 0.42678821086883545, 0.5076532363891602, -0.1033426895737648, -0.1741289496421814, -0.4677441418170929, -0.7500990033149719, -0.04964613541960716, -0.23763316869735718, 0.012481345795094967, 0.16096463799476624, 0.09084346145391464, -0.1160619705915451]}, "authors": [{"authorId": "1740906917", "name": "Bosheng Qin"}, {"authorId": "2108998895", "name": "Juncheng Li"}, {"authorId": "2118071462", "name": "Siliang Tang"}, {"authorId": "2125211", "name": "Yueting Zhuang"}], "references": [{"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06", "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4", "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "4b0541eccd8f98852d6807a14fbac17f775c7b40", "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m Method"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "title": "Improving Transformers with Probabilistic Attention Keys"}, {"paperId": "c88e2d70e44493d5508bfe517be978a9040be6a5", "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede", "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c", "title": "Rethinking Graph Transformers with Spectral Attention"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "e32a12b14e212506115cc6804667b3d8297917e1", "title": "Poolingformer: Long Document Modeling with Pooling Attention"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "60dd8f53b98d875428d31fa89ac6db8b161ba5b0", "title": "An Improved Attention for Visual Question Answering"}, {"paperId": "2051548f7681c96d603de932ee23406c525276f9", "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb", "title": "Array programming with NumPy"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "c3afcd7e57c3e04b03b0b5001a3854482fa39441", "title": "In Defense of Grid Features for Visual Question Answering"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8a1744da011375d711ed75fc2d160c6fdca2cf89", "title": "Deep Modular Co-Attention Networks for Visual Question Answering"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d8abb8206b913d185b4bd406880131c13759a6ff", "title": "The UEA multivariate time series classification archive, 2018"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "048d02623d94cc30dfbe3bda8ba75100735f33d8", "title": "An algorithmic theory of learning: Robust concepts and random projection"}, {"paperId": "b433b07dc47ede78dd75c965bce64c745a5bd4b2", "title": "A spelling device for the paralysed"}, {"paperId": "2e7a0645edc8f0de6ef439f8d7e183e039f5919b", "title": "Deep Residual Weight-Sharing Attention Network With Low-Rank Attention for Visual Question Answering"}, {"paperId": "01d08fa6c229bf3070600e49f8ab05449361817e", "title": "Long-range Sequence Modeling with Predictable Sparse Attention"}, {"paperId": "c49c292e1fb1d215c88828a52134b7ccfa52be44", "title": "Sparse Attention with Learning to Hash"}, {"paperId": "1f53e69f94a1020b48ea7d282221c576555c38a3", "title": "ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer"}, {"paperId": "5a9bc55f6332e38f62eb509b684147a1d4f10fd9", "title": "Focal Attention for Long-Range Interactions in Vision Transformers"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Bilinear Attention Networks, volume 31 of Advances in Neural Information"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "Extensions of lipschitz maps into a hilbert space"}, {"paperId": null, "title": "Performance of DBA on the LRA benchmark with different d p and d in"}]}