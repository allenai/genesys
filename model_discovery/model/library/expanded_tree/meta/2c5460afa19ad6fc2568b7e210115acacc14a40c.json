{"paperId": "2c5460afa19ad6fc2568b7e210115acacc14a40c", "abstract": "Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.", "venue": "APSIPA Transactions on Signal and Information Processing", "year": 2023, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications."}, "embedding": {"model": "specter_v2", "vector": [0.2680148184299469, 0.7527142763137817, -0.3742213547229767, -0.23031078279018402, -0.14713753759860992, 0.1158938929438591, 0.7791690230369568, -0.30612537264823914, -0.6645784378051758, 0.14071467518806458, 0.6299411654472351, -0.34248605370521545, 0.3041973114013672, -0.03354600444436073, -0.5038198232650757, 0.05614766478538513, -1.0506502389907837, 0.3846767544746399, -0.17012576758861542, 0.009734142571687698, -0.5589936375617981, -0.8154919743537903, -0.5204730033874512, 0.28786835074424744, 0.17328448593616486, -0.30273160338401794, 0.5894652605056763, 0.8753931522369385, -0.4387966990470886, 0.6933729648590088, 0.47474685311317444, -0.48130202293395996, -0.2295554280281067, -0.08328093588352203, -0.08354690670967102, -0.35900551080703735, 0.004878109786659479, -0.46683254837989807, -0.9397497177124023, 0.8136088252067566, -0.5696693062782288, 0.18077202141284943, 0.814997136592865, -0.5994553565979004, -0.13282924890518188, 1.5036849975585938, 0.6412944793701172, 0.93795245885849, -0.06933127343654633, -0.8210332989692688, 1.5170040130615234, -0.9903355240821838, 0.2979983687400818, 1.9356658458709717, 0.4185170531272888, 0.476593554019928, -0.7692873477935791, -0.594801664352417, 0.48132285475730896, -0.031030405312776566, -0.952206552028656, -0.23942524194717407, -0.2074393630027771, -0.2912789285182953, 1.993503451347351, -0.3724345862865448, -0.3434240520000458, 0.8576738834381104, 0.12029332667589188, 1.4526108503341675, -0.06148661673069, -1.2302885055541992, -0.3788439631462097, 0.4889720678329468, 0.06127168983221054, 0.8017722964286804, -0.32584062218666077, 0.601294994354248, -1.0728271007537842, -0.40141013264656067, 0.4364228844642639, -0.44848042726516724, -0.19418156147003174, 0.18770663440227509, -0.7609742283821106, 1.2453725337982178, 0.12755008041858673, 0.889071524143219, -0.16049516201019287, 0.57659912109375, 0.20488901436328888, 0.15395864844322205, -0.04637248069047928, -0.03051017038524151, -0.3345400393009186, 0.6726781129837036, -0.9635934829711914, 0.16178877651691437, 0.3142494857311249, 0.6510152816772461, -0.23069308698177338, 0.39730116724967957, -0.9983348846435547, 0.37299102544784546, 1.6859503984451294, 0.26413846015930176, 0.507208526134491, -0.5234244465827942, 0.3417760133743286, -0.7662703990936279, 0.1640341728925705, -0.629777729511261, -0.2966844141483307, -0.07561840862035751, -0.6001861691474915, -1.4009345769882202, -0.3349088430404663, -0.2479516565799713, -0.814936637878418, 0.9074855446815491, -0.5872178673744202, -0.139476016163826, 0.2243170142173767, 0.21414102613925934, -0.059300050139427185, 1.2689462900161743, 0.6519113183021545, -0.32077082991600037, 0.7407280802726746, -0.4685320258140564, -0.9374040365219116, -1.2575894594192505, 0.3827705681324005, -0.07320299744606018, 0.04531368985772133, -0.37391388416290283, -0.7475598454475403, -0.8956849575042725, -0.6239147186279297, 0.3180733323097229, 0.034796200692653656, 0.7627248764038086, 1.2399686574935913, 0.39298945665359497, -0.7493095993995667, 0.9125465154647827, -0.30368900299072266, -0.23016226291656494, 0.06735782325267792, 0.033577464520931244, 0.2688353657722473, -0.09674129635095596, -1.6378867626190186, 0.531324028968811, 0.6098089814186096, -0.773796558380127, -0.45523443818092346, -0.19650030136108398, -1.0918055772781372, -0.15944445133209229, -0.039888039231300354, -0.7024680376052856, 1.596045970916748, 0.05233622342348099, -1.2644520998001099, 0.25851601362228394, -0.80478835105896, -0.4427209794521332, 0.018909640610218048, -0.41240859031677246, -0.6620250344276428, -0.5284424424171448, 0.056208737194538116, 0.4112977087497711, 0.19857989251613617, -0.4320191740989685, -0.5725648999214172, 0.22529014945030212, -0.42169901728630066, -0.28561022877693176, -0.19993290305137634, 0.9844686388969421, -0.4299521744251251, -0.4428989887237549, 0.14691254496574402, 0.7453590035438538, -0.39040952920913696, -0.49411842226982117, -0.39514127373695374, -1.0051939487457275, 0.18717969954013824, -0.2855655550956726, 1.2244219779968262, -0.6817307472229004, -0.5273191332817078, -0.3057883381843567, -0.5880032777786255, -0.1789369434118271, -0.45446860790252686, 0.979246973991394, -0.4297785460948944, 0.3477492928504944, -0.4714864194393158, -1.1764850616455078, -0.16267815232276917, 0.07665082812309265, -0.8663520216941833, -0.25454002618789673, 0.21653085947036743, 0.792072057723999, -0.8957819938659668, -0.03474845364689827, 0.237966850399971, 0.13349589705467224, -0.7809576392173767, 1.4061493873596191, -0.42602384090423584, -0.0143008166924119, -0.04828309267759323, -0.7860172390937805, -0.08766679465770721, 0.1329134851694107, 0.5036720633506775, -0.08181539922952652, -0.4500176012516022, 0.5090864896774292, -0.4775460362434387, 1.2505396604537964, -0.20862264931201935, 0.49623632431030273, -0.23845602571964264, -0.5138219594955444, -0.16460900008678436, 0.8636425137519836, -0.2700251042842865, 0.07043913751840591, 0.20160011947155, 0.18882794678211212, -0.44950708746910095, 0.12797757983207703, 0.2494983673095703, 0.18420986831188202, -0.09710931032896042, 0.42266762256622314, 0.8690625429153442, -0.15352292358875275, 0.9477276802062988, 0.6645911931991577, 0.5019628405570984, 0.3096645176410675, 0.5969416499137878, 0.03310278430581093, 0.4769042432308197, -0.5881626009941101, -0.0014963290886953473, 0.5574227571487427, 0.9817021489143372, 0.6563233733177185, 0.528701901435852, -0.45332086086273193, -0.07825632393360138, -0.2327318787574768, 0.6309571266174316, 1.3258445262908936, -0.6361790895462036, -0.4270404875278473, -0.6225285530090332, -0.23024745285511017, -0.27083510160446167, 1.098821759223938, -0.34476473927497864, 0.22061917185783386, -0.6299408674240112, -1.4366060495376587, 0.7808421850204468, 0.3233321011066437, 0.37899699807167053, -0.5114121437072754, 0.2974662482738495, -0.04910550266504288, 0.3242933452129364, -0.7438511848449707, -0.48539549112319946, 0.2746924161911011, -0.72181236743927, -0.2802142798900604, 0.24168485403060913, -0.08524815738201141, 0.09716417640447617, -0.7856716513633728, 0.6504505276679993, -0.7559995055198669, -0.06432236731052399, -0.2400694638490677, 0.843594491481781, -1.1258584260940552, -1.16537606716156, -0.22259601950645447, 0.19521492719650269, 0.06662146747112274, 0.3763768970966339, 0.41958001255989075, 0.47457313537597656, -0.28276121616363525, -0.5151270031929016, 0.38669684529304504, 0.20208600163459778, 0.3561978340148926, 0.35445648431777954, 0.08443120867013931, -0.3232431411743164, -1.6011230945587158, 0.9239112138748169, 0.10931232571601868, -0.4959760904312134, 0.1371520310640335, -0.43408000469207764, -0.15938587486743927, 0.4448874592781067, -0.582464873790741, -0.9152865409851074, -0.5585287809371948, 0.2566147744655609, 0.19353020191192627, -0.43022680282592773, 0.5236474275588989, 0.3776111900806427, 0.7109902501106262, 0.12882107496261597, 0.5352467894554138, 0.5061339735984802, -0.3197057545185089, 0.5215469002723694, -0.5770227313041687, 0.14345812797546387, 0.44025418162345886, 0.4252791702747345, -0.06077159196138382, -0.7722717523574829, -0.7429960370063782, -0.38597628474235535, -0.36724862456321716, -0.11910386383533478, -0.18207606673240662, 0.0655057281255722, -0.6284841299057007, -0.586972177028656, 0.006861723959445953, -1.3091315031051636, -0.0844482034444809, 0.5103145837783813, 0.20753037929534912, 0.177663654088974, -0.7850782871246338, -1.3827182054519653, -0.8841084241867065, -0.33487188816070557, -0.432867169380188, 0.18573695421218872, 0.05329734832048416, -0.21960310637950897, -0.852080762386322, 0.3158714175224304, -0.2217244952917099, 0.8570127487182617, -0.9084070324897766, 1.3466112613677979, 0.14638638496398926, 0.19356654584407806, -0.30238986015319824, 0.4215983748435974, 0.5818853378295898, -0.2042911797761917, 0.2945937514305115, -0.775360643863678, 0.25427722930908203, -0.10486212372779846, 0.1405576467514038, 0.10591482371091843, 0.5882734656333923, 0.2808445394039154, 0.13718441128730774, -0.6076239943504333, -0.06555616110563278, 1.2334030866622925, -0.48648953437805176, -0.21466557681560516, -0.14615729451179504, 0.775292694568634, 0.4199179410934448, -0.5390772223472595, 0.35461047291755676, 0.47133761644363403, 0.4792143404483795, -0.14173252880573273, -0.1773943454027176, 0.037573959678411484, -0.7461685538291931, 0.8501515984535217, 1.563562035560608, 0.15950366854667664, -0.40848949551582336, -0.9991341233253479, 0.42029327154159546, -1.2675405740737915, -0.5588365793228149, 0.3046101927757263, 0.502945065498352, 0.4432119131088257, -0.4276078939437866, -0.5841612219810486, 0.1051778569817543, 0.6138061881065369, 0.24063792824745178, 0.008846014738082886, -0.3638828694820404, -0.21398504078388214, 0.24881435930728912, 0.13663750886917114, 0.8969483971595764, -0.744452714920044, 0.6436848044395447, 14.751677513122559, 0.6628957390785217, 0.12423820793628693, 0.5790311098098755, 0.3563650846481323, 0.4149172902107239, -0.24026691913604736, 0.20142604410648346, -1.2261295318603516, -0.20610232651233673, 1.2968121767044067, -0.17338374257087708, 0.7301579117774963, 0.0020483057014644146, 0.3541778028011322, -0.2221788465976715, -0.2878316044807434, 0.5076155662536621, 0.5940625071525574, -1.2377039194107056, 0.899549126625061, 0.20027509331703186, 0.30240291357040405, 0.7280600070953369, 0.17599764466285706, 0.8310351967811584, 0.543705940246582, -0.4411330223083496, 0.6315427422523499, 0.2879658341407776, 0.3941119909286499, 0.23083020746707916, 0.7304401397705078, 1.0629544258117676, -0.7987869381904602, -0.37767890095710754, -0.7677088379859924, -1.187738060951233, 0.4780428111553192, 0.31635624170303345, -0.7834349274635315, -0.20036718249320984, -0.39460599422454834, 0.5927371382713318, 0.237845778465271, 0.11813464015722275, -0.6149751543998718, 1.2741515636444092, -0.07311604917049408, -0.03892070800065994, 0.005013258662074804, 0.4782522916793823, 0.39018797874450684, 0.04741263389587402, 0.05577488988637924, 0.15405240654945374, 0.008159209974110126, 0.45645394921302795, -0.4395967721939087, 0.20956513285636902, -0.1752854287624359, -0.634465217590332, -0.3777490556240082, 0.6035128235816956, 0.6936709880828857, 0.3117358684539795, -0.5698538422584534, 0.02248978242278099, 0.5400975942611694, 0.1815604418516159, -0.2514464557170868, 0.0457131490111351, 0.22519731521606445, -0.3155151307582855, -0.26145854592323303, 0.4405413269996643, 0.03865192458033562, -0.6175774931907654, -1.0405259132385254, -0.026486027985811234, 0.5238609910011292, -0.7224823832511902, -1.0398398637771606, 1.0895066261291504, -0.06235196813941002, 0.10839111357927322, 0.28224900364875793, -1.229000449180603, -0.4056934714317322, 0.6396946310997009, -1.2246278524398804, -0.3803783357143402, 0.682966411113739, -0.2044169008731842, -0.3786821961402893, -0.4234471023082733, 1.5937507152557373, -0.07160291075706482, -0.7789746522903442, -0.4448535442352295, 0.12702688574790955, 0.2980385720729828, -0.4007318913936615, -1.0073914527893066, 0.7621251344680786, 0.544204592704773, 0.3536631166934967, 0.6477199196815491, 0.08285796642303467, -0.012788968160748482, -0.7113205790519714, -0.06459083408117294, 1.2738291025161743, -0.6697431206703186, -0.046109676361083984, -0.6225919127464294, -0.6138283610343933, 0.1889624297618866, 0.6324602365493774, -0.8773351311683655, 0.3065519332885742, 0.054835982620716095, -0.10665839910507202, 0.04981562867760658, -0.30686119198799133, 0.05431945621967316, 0.2926790416240692, -0.5380836725234985, -0.8005411028862, 0.3875722587108612, 0.21165403723716736, -0.425947368144989, -0.23802821338176727, -0.18951939046382904, -0.35855814814567566, 0.29768040776252747, 0.7450642585754395, -0.7341426610946655, 0.38844695687294006, 0.5177911520004272, -0.1290481686592102, -0.9089298248291016, -0.3111669719219208, -1.0271408557891846, 0.09386058151721954, 0.04314659535884857, 1.0722624063491821, -0.3027118742465973, -0.09529871493577957, 0.5430102348327637, 0.22515612840652466, -0.024364415556192398, -0.7858587503433228, -0.07858548313379288, -0.057789549231529236, -0.9350864887237549, 0.2172164022922516, -0.5050562620162964, -0.07350412011146545, 0.39899498224258423, 0.24632732570171356, 1.1386135816574097, -0.11572808772325516, -0.8027649521827698, 0.3190678656101227, -0.032563716173172, -0.16676180064678192, -0.39012786746025085, -0.18496006727218628, -1.2064769268035889, 0.42274758219718933, -1.1530919075012207, 0.1282465010881424, -1.2685080766677856, -0.15394459664821625, 0.15685439109802246, -0.0657220110297203, 0.05973103642463684, 0.1661698818206787, -0.5852954387664795, -0.6162272095680237, -0.34763869643211365, 0.2641114294528961, 0.7000225782394409, 0.6506300568580627, -0.6158561706542969, 0.054673004895448685, 0.23516465723514557, 0.39284655451774597, 0.34185802936553955, 0.45896679162979126, -0.7338730692863464, -0.760399341583252, -1.7058460712432861, 0.3164350092411041, 0.08029728382825851, 0.007618212606757879, -0.4446602463722229, 0.6581307649612427, 0.24520274996757507, -0.3668577969074249, 0.182920902967453, 0.362369567155838, -0.42899224162101746, -0.13964377343654633, 0.45018675923347473, -0.8386813998222351, -0.07526698708534241, -0.08597076684236526, -0.29664090275764465, -0.5786885023117065, 0.28625673055648804, -0.23841573297977448, -1.0157451629638672, -0.2400985062122345, 0.5275900959968567, -1.0535763502120972, -0.06825381517410278, -0.05545910447835922, 0.24959637224674225, -0.6203373670578003, -0.30624935030937195, -0.03575252741575241, 0.08738643676042557, -0.8133041858673096, 0.915524423122406, 0.32585155963897705, -0.6348401308059692, -0.2684362530708313, 0.515579342842102, -0.2707926630973816, -0.36517369747161865, 0.240617036819458, 0.30248311161994934, -0.3969939053058624, 1.1934483051300049, 0.7373765110969543, 0.34003573656082153, -0.7993198037147522, -0.39685165882110596, 0.6336787939071655, -0.2511909604072571, -0.1979895383119583, 1.2636314630508423, -0.3303074836730957, -1.0021580457687378, 0.2461031824350357, -0.9438146352767944, -0.8931555151939392, -0.4758058190345764, 0.7543651461601257, 0.11158537864685059, -0.39591270685195923, -0.44769495725631714, -0.30957740545272827, -0.02698352001607418, 0.32812976837158203, -0.6530172824859619, 0.8931673765182495, -0.0867152214050293, -0.5315924882888794, 1.1452969312667847, 0.3834375739097595, -0.6281678676605225, -0.5064526200294495, -0.5803683400154114, -0.05915607511997223, -0.059611059725284576, 0.5524945855140686, -0.3951149880886078, -0.46744781732559204, 0.9168522953987122, 0.23052774369716644, 0.24212779104709625, -0.3346072733402252, -0.21294938027858734, 0.14041167497634888, -0.014220123179256916, 0.4494043290615082, -0.6867170333862305, -1.126522421836853, 1.4526911973953247, 1.2665609121322632, -0.7316651940345764, 0.13566544651985168, -0.46245408058166504, -1.0362051725387573, 1.1550356149673462, -0.06605838239192963, 0.12101095169782639, 1.0855985879898071, -0.19559432566165924, 0.20471589267253876, 0.34105947613716125, -1.1074914932250977, -0.10871075838804245, 0.780348002910614, 0.8021261692047119, 1.1540127992630005, 0.6689090728759766, -0.1684863418340683, 1.1321200132369995, -0.06882889568805695, 0.12575258314609528, 0.6288416385650635, 0.4550841748714447, -0.10108012706041336, -0.47351932525634766, -0.053692225366830826, 0.6337528824806213, -0.613756537437439, -0.7069071531295776, 0.1611103117465973, 0.6896071434020996, 0.37712544202804565, 0.9183440208435059, 0.5054948329925537, 0.14913110435009003, 0.5150408148765564, 0.38235288858413696, 0.22421392798423767, -0.9205517768859863, -0.14698168635368347, -0.05078970640897751, -0.27260836958885193, 0.0926232859492302, -0.433017373085022, -0.6673569679260254, -0.3204093873500824, -0.02566329389810562, 0.07478886842727661, 0.20153479278087616, 0.5824667811393738, 1.1328166723251343, 0.2709370255470276, 0.22757059335708618, -0.14355342090129852, -0.19244785606861115, -0.5028459429740906, -1.6290960311889648, -0.17194804549217224, -0.8072159290313721, -0.4155343472957611, -0.14090034365653992, -0.4412410855293274, 0.017494402825832367]}, "authors": [{"authorId": "1780581845", "name": "Chengwei Wei"}, {"authorId": "6853732", "name": "Yun Cheng Wang"}, {"authorId": null, "name": "Bin Wang"}, {"authorId": "9363144", "name": "C.-C. Jay Kuo"}], "references": [{"paperId": "5a90a8f4ec612cef6b1bb9cf4eae897385d33c2d", "title": "An Overview on Generative AI at Scale With Edge\u2013Cloud Computing"}, {"paperId": "c76dd4a70361c3afd2e19d046343e2dedd16ecc3", "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search"}, {"paperId": "d94fc7f6c68e96e18d6751a05437e2e44d803e18", "title": "Relational Sentence Embedding for Flexible Semantic Matching"}, {"paperId": "8806ba65f2a477fe685c250c1536d2e8ef450958", "title": "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering"}, {"paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "title": "Large Language Models Are Human-Level Prompt Engineers"}, {"paperId": "764a616937a5923aaf22288b35f6b991ae41521d", "title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation"}, {"paperId": "3eda2fc2dbae1d6255c54667df3e63ff39621c2d", "title": "Analyzing and Evaluating Faithfulness in Dialogue Summarization"}, {"paperId": "ad3dfb2514cb0c899fcb9a14d229ff2a6018892f", "title": "Deep Bidirectional Language-Knowledge Graph Pretraining"}, {"paperId": "3510e82d17ef39e5f0aebc9129439643793e0adb", "title": "Green Learning: Introduction, Examples and Outlook"}, {"paperId": "44279244407a64431810f982be6d0c7da4429dd7", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"}, {"paperId": "9e33e71a2f8950e197b7f2d602f877d6e847b20b", "title": "A Focused Study on Sequence Length for Dialogue Summarization"}, {"paperId": "5456aef84577e61bc9118464bdc520e0d0983981", "title": "GreenKGC: A Lightweight Knowledge Graph Completion Method"}, {"paperId": "8af69a903a14657ec96b93c4b6e139771beec106", "title": "On the Explainability of Natural Language Processing Deep Models"}, {"paperId": "44ce738296c3148c6593324773706cdc228614d4", "title": "CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations"}, {"paperId": "4ba496a286afe4a1efd196fedd25495be772ea60", "title": "SynWMD: Syntax-aware Word Mover's Distance for Sentence Similarity Evaluation"}, {"paperId": "895396389a9fee1b607bf5141a6cc7925bb1e069", "title": "Knowledge Graph Contrastive Learning for Recommendation"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "3def68bd0f856886d34272840a7f81588f2bc082", "title": "Survey of Hallucination in Natural Language Generation"}, {"paperId": "4ab41d9780f1d1ac34d39fa7e527e73652507fcc", "title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "3159bd7af555c9612de2e59ea82d100d4140f667", "title": "Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions"}, {"paperId": "9ca3c5c87d9c7763e1eccbbadeddae6694d51512", "title": "Detecting computer-generated disinformation"}, {"paperId": "71ec44a46f38105e5f69256c4641b9bff72c75e9", "title": "KGBoost: A Classification-based Knowledge Base Completion Method with Negative Sampling"}, {"paperId": "17d7fd18123e12efbb9c255c8b986a5e84578b07", "title": "Time Waits for No One! Analysis and Challenges of Temporal Misalignment"}, {"paperId": "37187ceb6008d49e1758bab0d4f86bf39aa175cf", "title": "A Survey on Green Deep Learning"}, {"paperId": "2c6df83795cd5baf3b8c6e2639b85e2df0cee1d0", "title": "Sustainable AI: Environmental Implications, Challenges and Opportunities"}, {"paperId": "d72366a862c9963869d47d8274c6b283ee90a0ca", "title": "Task-Specific Dependency-based Word Embedding Methods"}, {"paperId": "ed8931af08ce757a92a01ed43a0619522e10e8ff", "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "704ac069de5a539ef42489ddb6cee0bd1650d54c", "title": "Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "c8559021289f08eaf8cf2294e406bc1c6b506d19", "title": "Recent advances in deep learning based dialogue systems: a systematic survey"}, {"paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "209f9bde2dee7cf1677801586562ffe56d435d38", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"}, {"paperId": "3950df97ea527009a32569cb7016bc3df1383dca", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering"}, {"paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "3e8f037d1b2c893f4df37deda1da3dcc577a8bac", "title": "Non-Autoregressive Text Generation with Pre-trained Language Models"}, {"paperId": "ba233d75aa403092bda0bffc026be7913673ad69", "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models"}, {"paperId": "4c2733d191e347753bb28afa46a1c55c65e085be", "title": "Persistent Anti-Muslim Bias in Large Language Models"}, {"paperId": "9b54941de1e21826ecc28b32730ac3f69991ede4", "title": "Robustness Gym: Unifying the NLP Evaluation Landscape"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "1841cf23c65ff2f27f21ba0d2268c3445f20332f", "title": "Few-Shot Text Generation with Natural Language Instructions"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "31392ad8722d9c66181b621936e2013199e02edc", "title": "When Do You Need Billions of Words of Pretraining Data?"}, {"paperId": "9438bc5626b2d9a771cecc7a41ecabf6639db53c", "title": "Automatic Detection of Machine Generated Text: A Critical Survey"}, {"paperId": "00b677e971ded11ac4a7da1b80ffda95b4f1ed78", "title": "Pre-Training Transformers as Energy-Based Cloze Models"}, {"paperId": "9da9cab545bd1534a8c953bb66e611243dcee24d", "title": "Authorship Attribution for Neural Text Generation"}, {"paperId": "07696855918fd575504b7072b42bf5c863082d2a", "title": "SLM: Learning a Discourse Language Representation with Sentence Unshuffling"}, {"paperId": "cbe87f3ff4ff30def2fa507ba4a511fee3e34188", "title": "Probing Task-Oriented Dialogue Representation from Language Models"}, {"paperId": "b360427d0991143013da6a208ccf28bcc8028fab", "title": "Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training"}, {"paperId": "01400290c7db96c4d665d1c29519c42ba47401e0", "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks"}, {"paperId": "194d44738fa1d105a2986f9eae1219bde1fc7e1c", "title": "Inductive Learning on Commonsense Knowledge Graph Completion"}, {"paperId": "54b804fd5511a431bbeb49accd21152296dcad67", "title": "E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce"}, {"paperId": "5d639fa3df4f3a7bacd24f2651daf0b97d2b75a3", "title": "Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems"}, {"paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"}, {"paperId": "2734bf6fd46357504fd1deef33ad69207b39fd57", "title": "TweepFake: About detecting deepfake tweets"}, {"paperId": "91cfd15b587c5ed604e7e49326db6d045276c2a5", "title": "The Computational Limits of Deep Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d47a682723f710395454687319bb55635e653105", "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP"}, {"paperId": "3736715a5b9f3383bd359c6e1ed79d3a4ea258f6", "title": "Toward Better Storylines with Sentence-Level Language Models"}, {"paperId": "fa83f4f369af53e5d4fbdc7cce1808520e8e50a2", "title": "Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"paperId": "5290d7921f0266c8b50b79fc8a0b7d22868f4f60", "title": "The Cost of Training NLP Models: A Concise Overview"}, {"paperId": "b0b0dddb8310e01b9407a21674c2d33a23a6e967", "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "8771679aac0e90371340bd8c657317f5be113e81", "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "d808e7468ef6265d39d3cd9c657c9f52e889cbc2", "title": "SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models"}, {"paperId": "c44120f765fc43994c5cfb4e12e4f62999efeae6", "title": "How Context Affects Language Models' Factual Predictions"}, {"paperId": "7cf8510d5905bd8a63f1e098e05ab591d689e0fd", "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "c7fc1cac162c0e2a934704184c7554fd6b6253f0", "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model"}, {"paperId": "0b5580e2ef3686f67db3beb7001e4481b2f07ce5", "title": "Deepfake Bot Submissions to Federal Public Comment Websites Cannot Be Distinguished from Human Submissions"}, {"paperId": "8d255b88fe5c88e193343b145db85121c15c8bd0", "title": "Models in the Wild: On Corruption Robustness of Neural NLP Systems"}, {"paperId": "5f46d8e18138fe572b6fae897475a2ad645a3e1a", "title": "A Density Ratio Approach to Language Model Fusion in End-to-End Automatic Speech Recognition"}, {"paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"}, {"paperId": "9146414fca384e73f11ccfd3db8ad6d2a1e8eda2", "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled"}, {"paperId": "4099c4d272c12081b562392606e6d567e4ae7031", "title": "Masked Language Model Scoring"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "14d48df8d474f8846f1b43525684ddadb1e92ba7", "title": "Word-level Textual Adversarial Attacking as Combinatorial Optimization"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "b9c5f9059e8b6e2e7f44f315e442aaba815f38d1", "title": "Effective Sentence Scoring Method Using BERT for Speech Recognition"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97", "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "eb606d9ce65139754232cee62f6ab77f3e0c665f", "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"}, {"paperId": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2", "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "2723f54b4f5ed2d3f3a47c1b6749bbf5d8c660fd", "title": "Are Red Roses Red? Evaluating Consistency of Question-Answering Models"}, {"paperId": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25", "title": "A Tensorized Transformer for Language Modeling"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "title": "Learning Deep Transformer Models for Machine Translation"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "455a8838cde44f288d456d01c76ede95b56dc675", "title": "A Structural Probe for Finding Syntax in Word Representations"}, {"paperId": "867db5097ad6aaef098c60b0845785b440eca49a", "title": "GLTR: Statistical Detection and Visualization of Generated Text"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee", "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "dc92db2149ae862a9c96743bd5ec5bf79a2afa1e", "title": "Component Fusion: Learning Replaceable Language Model Component for End-to-end Speech Recognition System"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "5e9c85235210b59a16bdd84b444a904ae271f7e7", "title": "On Measuring Social Biases in Sentence Encoders"}, {"paperId": "d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18", "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "08dffc357ef8a04f64cb13eab70f1c2e6a625437", "title": "Global context-dependent recurrent neural network language model with sparse feature learning"}, {"paperId": "7572aefcd241ec76341addcb2e2e417587cb2e4c", "title": "Knowledge Graph Embedding Based Question Answering"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "051091195626920ca8ce91ee72d68ec37110eeec", "title": "Importance of Search and Evaluation Strategies in Neural Dialogue Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "7fedd981f2769bd009f749a3dff7044d8378c9b4", "title": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information"}, {"paperId": "da4b7f47d0d9c4d46740cdd6fb712c6a87c10db3", "title": "A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition"}, {"paperId": "4b3ce4774cd91ebac8627f9766f6ddba111f80dc", "title": "Recurrent Stacking of Layers for Compact Neural Machine Translation Models"}, {"paperId": "13c7764ba6b0455ef5f77c3cdd5425c3110603b9", "title": "The Second Workshop on Knowledge Graphs and Semantics for Text Retrieval, Analysis, and Understanding (KG4IR)"}, {"paperId": "22616702da06431668022c649a017af9b333c530", "title": "Automated Fact Checking: Task Formulations, Methods and Future Directions"}, {"paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6", "title": "Hierarchical Neural Story Generation"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "1b1e3f7218f1c0f0db56bf2bd9475521454693a1", "title": "Diverse Beam Search for Improved Description of Complex Scenes"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "478d6102a2df86b0f4e69e398f96619312ecdc8c", "title": "An Analysis of Incorporating an External Language Model into a Sequence-to-Sequence Model"}, {"paperId": "ef9ddbc35676ce8ffc2a8067044473727839dbac", "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"}, {"paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454", "title": "Non-Autoregressive Neural Machine Translation"}, {"paperId": "f4a67539e254dcbf8d662f6ab7435ceb81a61990", "title": "Neural Response Generation via GAN with an Approximate Embedding Layer"}, {"paperId": "33125ec92a0b4b1687ccd153762d6275668e3d09", "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8e091d4e247f74ff30f0ff7fe7426574b8817474", "title": "Character-Word LSTM Language Models"}, {"paperId": "58dfeb0bc41429393bf27ff882b7b679031f106c", "title": "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders"}, {"paperId": "137347897a3f46fa2d67d925ec2bb3e71cde1f27", "title": "Trainable Greedy Decoding for Neural Machine Translation"}, {"paperId": "7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786", "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "e235bc8ccbe85de40f406d1a1201d50aec893b2d", "title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation"}, {"paperId": "f4819c08515a03f086ada34f5babbe3395b3ffe9", "title": "Character-level language modeling with hierarchical recurrent neural networks"}, {"paperId": "58001259d2f6442b07cc0d716ff99899abbb2bc7", "title": "Gated Word-Character Recurrent Language Model"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "title": "Character-Aware Neural Language Models"}, {"paperId": "3056add22b20e3361c38c0472d294a79d4031cb4", "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"}, {"paperId": "4b762c0344f14bb00d590f5666c27b3aac7b0a7d", "title": "Dependency Recurrent Neural Language Models for Sentence Completion"}, {"paperId": "dffe530167186edf2a8713f286732adc03907c17", "title": "Bidirectional recurrent neural network language models for automatic speech recognition"}, {"paperId": "5fcd41ca42659ff792fc8ee7d535156e8e69f987", "title": "On Using Monolingual Corpora in Neural Machine Translation"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0a975021d1ff05c747baba391736be98488275a1", "title": "Word-phrase-entity language models: getting more mileage out of n-grams"}, {"paperId": "e11274bb4549fbdb9a7ab64d09076976e6f71e75", "title": "Cache based recurrent neural network language model inference for first pass speech recognition"}, {"paperId": "72b4ff7387223cf0398c298c3cc62ee07d9c0043", "title": "Dependency Language Models for Sentence Completion"}, {"paperId": "0fbdddacd3e231d5b4d4c011eb7b7fe7732e631e", "title": "Converting Neural Network Language Models into Back-off Language Models for Efficient Decoding in Automatic Speech Recognition"}, {"paperId": "a17745f1d7045636577bcd5d513620df5860e9e5", "title": "Deep Neural Network Language Models"}, {"paperId": "ed6262b569c0a62c51d941228c54f34e563af022", "title": "Japanese and Korean voice search"}, {"paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11", "title": "Generating Text with Recurrent Neural Networks"}, {"paperId": "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "title": "Extensions of recurrent neural network language model"}, {"paperId": "2818bd090206ef33f9d7e1be03bc4f742c6762d1", "title": "Uyghur morpheme-based language models and ASR"}, {"paperId": "805ccb88e942346413b8f951177fbe3715866eb3", "title": "Morphology-based and sub-word language modeling for Turkish speech recognition"}, {"paperId": "b42abb25f97fa729a4bd22cb09d3c49aebe6dce3", "title": "Morph-based speech recognition and modeling of out-of-vocabulary words across languages"}, {"paperId": "0fcc184b3b90405ec3ceafd6a4007c749df7c363", "title": "Continuous space language models"}, {"paperId": "16e987de3d97c8fe93a10cee9bb4df3ce7b72696", "title": "Large vocabulary continuous speech recognition of an inflected language using stems and endings"}, {"paperId": "3e016c8fd8c988e80a8bf3a05ad77a2f20bdaad2", "title": "Joint Morphological-Lexical Language Modeling (JMLLM) for Arabic"}, {"paperId": "f83615a105d602fefb4076293a3c95d3659f26b6", "title": "Gaussian Mixture Language Models for Speech Recognition"}, {"paperId": "8b395470a57c48d174c4216ea21a7a58bc046917", "title": "Training Neural Network Language Models on Very Large Corpora"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "297b0d80575ae36e3e26772ba7e70fa6b570c68d", "title": "A Decoder for Syntax-based Statistical MT"}, {"paperId": "62864f78fa4cb5f1ab45ebbe5a420b546b62d7a6", "title": "An Efficient A* Search Algorithm for Statistical Machine Translation"}, {"paperId": "ca71b6b20e5c14fb7b8b20bdfdb6a47783fb7137", "title": "Data-driven approach to designing compound words for continuous speech recognition"}, {"paperId": "a1c3748820d6b5ab4e7334524815df9bb6d20aed", "title": "Structured language modeling"}, {"paperId": "2722b9e5ab8da95f03e578bb65879c452c105385", "title": "Catastrophic forgetting in connectionist networks"}, {"paperId": "4d8cb09f19c0afdc68d39cc55743104ec396d86e", "title": "Efficient sampling and feature selection in whole sentence maximum entropy language models"}, {"paperId": "0922ec1bf7a00391a53e8f3db93e72a51e8cc56c", "title": "Exploiting Syntactic Structure for Language Modeling"}, {"paperId": "e9fac1091d9a1646314b1b91e58f40dae3a750cd", "title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions"}, {"paperId": "f72084efcae8b2007e590b0c5a8f1decb61ef935", "title": "A whole sentence maximum entropy language model"}, {"paperId": "218395e3d1198b5ae2bf3bfd2a18b164fea2d79a", "title": "Class phrase models for language modeling"}, {"paperId": "e44a6eb19639aa00d46c97548ac68fcd446b0161", "title": "Bayesian estimation methods for n-gram language model adaptation"}, {"paperId": "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "title": "An Empirical Study of Smoothing Techniques for Language Modeling"}, {"paperId": "a208a5a99e2a4d8f48673e980e8cc0479df890f8", "title": "A variable-length category-based n-gram language model"}, {"paperId": "fb486e03369a64de2d5b0df86ec0a7b55d3907db", "title": "A Maximum Entropy Approach to Natural Language Processing"}, {"paperId": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "title": "Improved backing-off for M-gram language modeling"}, {"paperId": "9dd1e37157df40356c4c7cbcf73d4f736b0c5715", "title": "Towards better language models for spontaneous speech"}, {"paperId": "cf0d87dc3f228baa01468345099ccc4bd91af9ee", "title": "Ergodic hidden Markov models and polygrams for language modeling"}, {"paperId": "1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8", "title": "A new algorithm for data compression"}, {"paperId": "3de5d40b60742e3dfa86b19e7f660962298492af", "title": "Class-Based n-gram Models of Natural Language"}, {"paperId": "eadf7d20852caa92310d0cb582269b94226b1e58", "title": "Adaptive Language Modeling Using Minimum Discriminant Estimation"}, {"paperId": "a1066659ec1afee9dce586f6f49b7d44527827e1", "title": "A Statistical Approach to Machine Translation"}, {"paperId": "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"}, {"paperId": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "title": "A Maximum Likelihood Approach to Continuous Speech Recognition"}, {"paperId": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "title": "Continuous speech recognition by statistical methods"}, {"paperId": "22b6737a38179c01444d69443e327850c9956c15", "title": "Design of a linguistic statistical decoder for the recognition of continuous speech"}, {"paperId": "37c931cbaa9217b829596dd196520a838562a109", "title": "Generalized Iterative Scaling for Log-Linear Models"}, {"paperId": "25035fe0d23f9ffa62c6c1d2c9172e03aa5b4801", "title": "I.\u2014PROBABILITY: THE DEDUCTIVE AND INDUCTIVE PROBLEMS"}, {"paperId": "3893dcba5556556adf9b6fa35053303ba1572569", "title": "A Domain Knowledge Enhanced Pre-Trained Language Model for Vertical Search: Case Study on Medicinal Products"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "e7d4fd44400391888226b3f5d8acac0ab2106bac", "title": "KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships"}, {"paperId": null, "title": "Unified multilingual robustness evaluation toolkit for natural language processing"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": null, "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "f9a1b3850dfd837793743565a8af95973d395a4e", "title": "LSTM Neural Networks for Language Modeling"}, {"paperId": "5999fd9b9712fee3184989d043bff899935b4208", "title": "Mandarin Word-Character Hybrid-Input Neural Network Language Model"}, {"paperId": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"}, {"paperId": "b4fc91e543ec868658cde6170f1e59c33292e595", "title": "Recurrent Neural Network Based Language Modeling in Meeting Recognition"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "2d6a97f83bb8207ea9d88118618ed3ab52054a88", "title": "Unsupervised Morpheme Segmentation and Morphology Induction from Text Corpora Using Morfessor 1.0"}, {"paperId": "dae559d16c0795d1f76bf22a2690a30c7da08296", "title": "Whole-sentence exponential language models: a vehicle for linguistic-statistical integration"}, {"paperId": "bc5a3d71a8c2e80299f6b4b0bff80e44c50a76de", "title": "Variable n-grams and extensions for conversational speech language modeling"}, {"paperId": "aa6680249fe24b5196fb3d9d9c9ca056a7bb0273", "title": "Data-Driven Determination of Appropriate Dictionary Units for Korean LVCSR"}, {"paperId": "464097da8922ff8de5de0dcd0aa96066bc403f49", "title": "Exploiting Syntactic Structure for Language Modeling"}, {"paperId": "ef9190e7669ea5523c3ef61180b35385b0ea345f", "title": "A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams"}, {"paperId": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b", "title": "Interpolated estimation of Markov source parameters from sparse data"}, {"paperId": null, "title": "Note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities"}, {"paperId": "d36910319d11359b995ff5413696aa9e9995e163", "title": "\\self-organized Language Modeling for Speech Recognition\". In"}]}