{"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.", "venue": "arXiv.org", "year": 2020, "citationCount": 3066, "influentialCitationCount": 566, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Following prior work on long-sequence transformers, the Longformer is evaluated on character-level language modeling and achieves state-of-the-art results on text8 and enwik8 and pretrain Longformer and finetune it on a variety of downstream tasks."}, "embedding": {"model": "specter_v2", "vector": [0.38324472308158875, 0.5810707807540894, -0.1330222338438034, -0.16554223001003265, -0.41025930643081665, -0.5092007517814636, 1.0230765342712402, 0.004763444419950247, -0.2540006935596466, -0.17605620622634888, 1.0488533973693848, 0.12478966265916824, 0.2257661670446396, 0.027571752667427063, -0.28655490279197693, -0.06593752652406693, -0.7381138205528259, 0.4639306664466858, -0.006245112046599388, -0.056906670331954956, 0.235916867852211, -1.008202075958252, -0.6066888570785522, 0.20674200356006622, 0.6624887585639954, -0.024800727143883705, 0.42417722940444946, 1.1088619232177734, -0.4832683503627777, 0.21751603484153748, 0.18936192989349365, -0.5295925140380859, -0.03396889194846153, -0.38340824842453003, -0.5572856068611145, -0.010005821473896503, 0.4131578505039215, -0.2938985228538513, -0.33822667598724365, 0.5320718288421631, -0.005093055311590433, 0.0331779345870018, 0.7651586532592773, -0.20824438333511353, -0.27746838331222534, 1.545795202255249, 0.5610857605934143, 0.8549615740776062, -0.21276257932186127, -0.58915114402771, 1.646259069442749, -1.4517428874969482, 0.34381163120269775, 1.5476793050765991, 0.32312893867492676, 0.21145415306091309, -0.3446658253669739, -0.4203919470310211, 0.9639551043510437, 0.23592489957809448, -0.640273928642273, -0.3483268916606903, -0.008619298227131367, -0.2302137315273285, 1.9943046569824219, -0.41708904504776, 0.14026829600334167, 0.7068310379981995, 0.05063159763813019, 1.5874361991882324, -0.7670004367828369, -0.43208831548690796, -0.4446217119693756, -0.40070995688438416, 0.5923851728439331, 0.5893087387084961, -0.4995537996292114, 0.08573279529809952, -0.8802165985107422, 0.010511555708944798, 0.42742058634757996, -0.2216109037399292, 0.02062172256410122, 0.11228431016206741, -0.4881032109260559, 0.40969985723495483, 0.006660069804638624, 1.046269178390503, -0.295417845249176, 0.7429666519165039, 0.5231260061264038, 0.07232734560966492, 0.17704540491104126, 0.45934757590293884, 0.16324740648269653, -0.12608717381954193, -1.100547432899475, 0.47080618143081665, -0.22886623442173004, 0.8781818747520447, -0.314641535282135, 0.4454631209373474, -1.0990976095199585, 0.22365130484104156, 1.3444128036499023, 0.18361125886440277, 0.695597767829895, -0.8601927161216736, 0.23053526878356934, -0.5995761156082153, 0.052144814282655716, -0.6335664391517639, -0.15752515196800232, -0.6059873700141907, -0.7994553446769714, -1.6011707782745361, -0.4611509144306183, 0.35008880496025085, -0.5052608847618103, 0.807305634021759, -0.3716309666633606, 0.06221634894609451, -0.0347382053732872, 0.33734771609306335, 0.6150023341178894, 1.1538047790527344, 0.257402241230011, -0.12774930894374847, 0.6368183493614197, -1.079262137413025, -0.7804805636405945, -1.103492021560669, 0.5828585028648376, -0.1699671447277069, 0.13188451528549194, 0.09133259952068329, -1.5591318607330322, -0.9050245881080627, -0.6868590116500854, -0.055661123245954514, -0.1991414576768875, 0.22564597427845, 0.6466301083564758, 0.20677410066127777, -0.7294220924377441, 0.7995632290840149, -0.32223278284072876, -0.2654736340045929, 0.10962344706058502, -0.341776967048645, 0.25408509373664856, -0.24002014100551605, -1.3270257711410522, 0.1813136488199234, 0.01782948710024357, -0.44510915875434875, -0.35684192180633545, -0.9291706085205078, -0.9895846843719482, 0.17804336547851562, 0.21290096640586853, -0.5373530983924866, 1.5413846969604492, 0.14168395102024078, -1.3580644130706787, 0.2920403480529785, -0.6798706650733948, 0.07399977743625641, 0.20640747249126434, -0.5557697415351868, -0.15850457549095154, -0.5887227654457092, 0.030706556513905525, 0.1980404108762741, 0.39824050664901733, -0.1817389726638794, -0.21382856369018555, 0.030964862555265427, -0.555919885635376, -0.12981462478637695, -0.12594769895076752, 0.734302282333374, -0.608959972858429, -0.2645552456378937, -0.1481223702430725, 0.9867547750473022, 0.3856986165046692, -0.8361042141914368, -0.6466702222824097, -1.5611472129821777, 0.362326055765152, 0.13593575358390808, 1.3004134893417358, -0.7934477925300598, -0.4028739631175995, -0.5894309878349304, -0.23612633347511292, -0.0014306483790278435, -0.8056012392044067, 0.9482172131538391, -0.39918237924575806, 0.3880138397216797, -0.28234168887138367, -1.3275256156921387, 0.28939366340637207, -0.050708428025245667, -0.7282898426055908, -0.22367089986801147, 0.3728003203868866, 1.2789762020111084, -1.192142128944397, -0.1378195583820343, -0.18583060801029205, 0.10156597197055817, -0.9627000689506531, 1.1571085453033447, -0.28358739614486694, -0.05848819389939308, -0.13370849192142487, -0.6356600522994995, -0.11725643277168274, -0.2956050932407379, 0.24156805872917175, -0.290264755487442, -0.3179323673248291, 0.71676105260849, -0.2612908184528351, 1.4219449758529663, -0.07444389164447784, 0.25916045904159546, -0.45238086581230164, -0.6826440095901489, 0.14691807329654694, 0.208412304520607, -0.061815373599529266, -0.49045050144195557, -0.07991085946559906, 0.14303722977638245, -0.6966568827629089, 0.057985611259937286, 0.8274361491203308, 0.6815478801727295, -0.5269683003425598, 0.42413073778152466, 0.9021145701408386, -0.15157391130924225, 0.9096937775611877, 0.9399626851081848, 0.8892205953598022, 0.6598901152610779, 0.45382148027420044, -0.18935495615005493, 0.12777897715568542, -0.7414190173149109, 0.08858198672533035, 0.539625346660614, 0.9585782885551453, 1.0910577774047852, 0.5252500772476196, -0.41822269558906555, -0.6537796854972839, -0.2528937757015228, 0.8667107224464417, 1.4337328672409058, -0.3165367543697357, -0.5074663162231445, -0.7081920504570007, -0.053610917180776596, -0.3627322018146515, 0.5097842216491699, -0.48732876777648926, -0.20087286829948425, -0.7860894799232483, -0.8837730884552002, 0.9031027555465698, 0.3743996024131775, 0.6668213605880737, -0.5355933308601379, -0.25011205673217773, 0.11653144657611847, -0.060617562383413315, -0.5403798222541809, -0.95816570520401, 0.2188294231891632, -0.17392437160015106, 0.02803254872560501, -0.12429071962833405, 0.011961519718170166, -0.013397015631198883, -0.6306347250938416, 1.1978888511657715, -0.7354055643081665, -0.26253819465637207, 0.17917776107788086, 0.34848111867904663, -0.6839340925216675, -0.5999616980552673, 0.2335834801197052, -0.008396143093705177, 0.003606523619964719, 0.03046124428510666, 0.3526771366596222, 0.07165717333555222, -0.15840457379817963, -0.42194268107414246, 0.27368009090423584, 0.00894108135253191, 0.2952713370323181, 0.33375853300094604, -0.011831535957753658, -0.10597224533557892, -1.1683204174041748, 0.9004261493682861, 0.23675598204135895, -0.2609127163887024, 0.42634934186935425, -0.5008296966552734, -0.33825892210006714, 0.39318665862083435, -0.568167507648468, -0.5631591081619263, -1.1667475700378418, 0.44764894247055054, 0.13421235978603363, -0.022401949390769005, 0.30463048815727234, -0.13629043102264404, 1.1088770627975464, 0.03175308555364609, 0.726374089717865, 0.028447553515434265, -0.30112284421920776, 0.750058114528656, -0.8638883233070374, 0.8282330632209778, 0.21847335994243622, 0.3036375045776367, -0.2248641699552536, -0.2843494415283203, -0.8037237524986267, -0.6053740382194519, -0.8083980083465576, -0.2901488244533539, -0.4262109100818634, 0.4209105372428894, -0.2931334674358368, -0.8043451309204102, 0.22506724298000336, -1.6064023971557617, -0.28685227036476135, 0.018283139914274216, -0.7535292506217957, -0.013387980870902538, -0.7398437857627869, -1.0676857233047485, -0.5082059502601624, -0.7429215312004089, -0.49363261461257935, 0.6129066348075867, 0.08976343274116516, -0.5311641097068787, -0.7003008127212524, 0.5003662109375, -0.43974918127059937, 0.8617271184921265, -0.5933374166488647, 0.6946027874946594, -0.020281536504626274, -0.22873200476169586, -0.09513834118843079, 0.41472363471984863, 0.47361209988594055, -0.18460848927497864, 0.37794968485832214, -0.3296984136104584, 0.27723896503448486, -0.3607989549636841, -0.10070828348398209, 0.2663525342941284, 0.38638728857040405, 0.35444438457489014, -0.11511124670505524, -0.5115936994552612, 0.04858463257551193, 1.0712001323699951, -0.5909660458564758, 0.2569423019886017, 0.006953455042093992, 0.9263194799423218, 0.4258994460105896, -0.20276783406734467, 0.7411442995071411, 0.6380193829536438, 0.3032020330429077, 0.01603403128683567, -0.059650059789419174, -0.02053527534008026, -0.7929823398590088, 0.738560140132904, 1.8934240341186523, 0.13663189113140106, -0.6152779459953308, -1.0553905963897705, 1.0403075218200684, -1.4128141403198242, -1.1807470321655273, 0.25701481103897095, 0.3675352931022644, 0.365324467420578, -0.6918846368789673, -0.20632001757621765, -0.20257370173931122, 0.11927823722362518, 0.35699060559272766, -0.25086846947669983, -0.621543824672699, -0.22834289073944092, 0.15166325867176056, -0.08747780323028564, 0.733401358127594, -0.07367207854986191, 0.48577067255973816, 14.761926651000977, 0.6588373780250549, 0.3118375539779663, 0.6484260559082031, 0.5068691372871399, -0.008626149967312813, -0.6007338762283325, -0.2233538180589676, -1.2999666929244995, -0.08710543066263199, 1.0624024868011475, -0.22313271462917328, 0.3237383961677551, -0.2625427842140198, 0.5137790441513062, 0.10373678058385849, -0.6630566120147705, 0.4322315752506256, 0.7076236009597778, -1.198464035987854, 0.639135479927063, 0.2523571848869324, 0.252617746591568, 0.45045167207717896, 0.7731115221977234, 0.8184835910797119, 0.6231507062911987, -0.17592208087444305, 0.639018714427948, 0.7151418328285217, 0.5333042144775391, -0.03530102223157883, 0.38760432600975037, 0.6088844537734985, -0.6738067269325256, -0.1658419966697693, -0.7820234894752502, -0.8810555338859558, 0.7037733197212219, 0.38374432921409607, -0.443811297416687, -0.033133089542388916, -0.23579345643520355, 0.8723987340927124, 0.03791000321507454, 0.42446139454841614, -0.26246923208236694, 1.159400463104248, 0.30718138813972473, 0.01844620890915394, 0.5128351449966431, 0.3635927736759186, 0.513340413570404, 0.3458416759967804, 0.22446009516716003, 0.39304763078689575, -0.02718339115381241, 0.58342045545578, -0.3289109766483307, 0.027080992236733437, -0.11796891689300537, -0.4616330862045288, -0.1359540820121765, 0.7829589247703552, 0.7881618738174438, 0.2278885543346405, -0.1337112933397293, 0.40978461503982544, 0.5319109559059143, 0.07448352128267288, -0.28851062059402466, -0.17200671136379242, -0.02949431911110878, -0.15049394965171814, 0.13495472073554993, 0.3734489679336548, 0.0267910398542881, -0.2519316077232361, -0.8526631593704224, -0.15384997427463531, 0.20590899884700775, -0.8818780779838562, -0.7840002179145813, 0.6913136839866638, -0.1378941833972931, -0.2249109297990799, -0.20090024173259735, -0.42914721369743347, -0.6849934458732605, 0.43274497985839844, -1.1154769659042358, -0.8289952278137207, 0.23309457302093506, -0.5550765991210938, 0.06464225798845291, 0.04709186777472496, 0.9747291207313538, 0.08080580085515976, -0.3486470878124237, -0.01330783311277628, 0.36462873220443726, -0.15392665565013885, -0.17812372744083405, -1.1988519430160522, 0.8432625532150269, 0.4309731125831604, -0.21152065694332123, 0.4969112277030945, 0.3588600754737854, 0.027363356202840805, -0.8901711106300354, -0.12331099808216095, 1.179840087890625, -1.3215916156768799, -0.3220445215702057, -0.869266152381897, -1.003726601600647, 0.09550021588802338, 1.0233900547027588, -0.5517526865005493, 0.32314804196357727, 0.11091465502977371, -0.26031601428985596, -0.2549663484096527, -0.2973088324069977, -0.3873837888240814, 0.6598500609397888, -0.42000362277030945, -0.5063214898109436, -0.05739477276802063, 0.6080325245857239, -0.7946575880050659, -0.38168004155158997, -0.6202231049537659, 0.1007797122001648, 0.13445626199245453, 0.6318396329879761, -0.2586382329463959, 0.8373828530311584, 0.7452041506767273, 0.2518937587738037, -1.1177319288253784, -0.6513345241546631, -1.15831458568573, 0.17323066294193268, 0.7912277579307556, 0.7159231305122375, -0.018206849694252014, 0.07228687405586243, 0.5066436529159546, 0.13516345620155334, -0.42240452766418457, -0.6399515867233276, -0.5718054175376892, 0.32610249519348145, -0.2708604335784912, 0.250504732131958, -0.2212580442428589, 0.1076759621500969, 0.4497080445289612, 0.23845553398132324, 0.386726975440979, -0.07779989391565323, -0.7288672924041748, 0.5200432538986206, -0.043868113309144974, 0.0646604374051094, -0.6525522470474243, -0.2538723349571228, -1.417603611946106, -0.12954245507717133, -0.9957460761070251, 0.1577427238225937, -1.5589512586593628, 0.06948605179786682, 0.5635871291160583, -0.09742603451013565, -0.08611351251602173, 0.24280232191085815, -0.6417959928512573, -0.43577173352241516, -0.7440429329872131, -1.0098713636398315, 1.0393458604812622, 0.9978281259536743, -1.0583606958389282, 0.25462469458580017, -0.21502721309661865, -0.01870398223400116, -0.10492248833179474, 0.17751623690128326, -0.42495056986808777, -0.6424799561500549, -1.6130833625793457, 0.44206467270851135, -0.06973569840192795, -0.18861176073551178, -0.5040127038955688, 0.6397173404693604, 0.45499303936958313, -0.2395704686641693, -0.46721741557121277, 0.04863031208515167, -0.20783793926239014, -0.3048025369644165, 0.2219959795475006, -0.8933796286582947, 0.46824556589126587, -0.22394324839115143, -0.5677748322486877, -0.2520988881587982, 0.6467078328132629, -0.32165390253067017, -1.2134101390838623, -0.2190299928188324, 0.4039550721645355, -0.8703279495239258, 0.16371537744998932, -0.5084712505340576, -0.1477733701467514, -1.0018908977508545, -0.428530752658844, 0.36672085523605347, 0.6256862878799438, -0.4767139256000519, 1.2228165864944458, 0.2730754315853119, -0.9053755402565002, -0.276160329580307, 0.10223197937011719, -0.09058652073144913, 0.11003182828426361, 0.7051194310188293, 0.1404358446598053, 0.019708076491951942, 0.7332263588905334, 0.5058808326721191, 0.21267934143543243, -0.862259030342102, -0.22175799310207367, 0.6530875563621521, -0.8315305113792419, -0.19511891901493073, 1.0982199907302856, -0.24200870096683502, -1.0758930444717407, -0.08390610665082932, -0.947454035282135, -0.7266183495521545, -0.26701489090919495, 0.9240249395370483, 0.4473554193973541, -0.40037810802459717, -0.15046074986457825, -0.4795736074447632, 0.1493806093931198, -0.09977897256612778, -0.7077611684799194, 1.0678902864456177, -0.27813971042633057, -0.5123425126075745, 0.8231024742126465, 0.764562726020813, -0.56587815284729, -0.5437443256378174, -0.5888227820396423, -0.3895946741104126, 0.20777586102485657, 0.2634681463241577, -0.5640764236450195, -0.13980616629123688, 0.9887046217918396, 0.17036965489387512, 0.5992698073387146, -0.07758865505456924, -0.08331934362649918, 0.14672937989234924, 0.5111703276634216, -0.11739475280046463, -0.8173177242279053, -0.3875349164009094, 1.593153953552246, 1.511330485343933, -0.423695832490921, 0.20924407243728638, 0.18924778699874878, -0.9375778436660767, 0.8470993638038635, 0.2805415987968445, 0.021312996745109558, 0.5265575051307678, -0.16823282837867737, -0.11804704368114471, -0.01643368788063526, -1.3248800039291382, -0.038062479346990585, 0.8062717914581299, 1.0998070240020752, 0.8342776894569397, 0.19357599318027496, 0.031181247904896736, 1.0634647607803345, 0.15181787312030792, -0.21786630153656006, 0.5998092889785767, 0.03923391178250313, -0.16496165096759796, -0.3593381345272064, 0.22714227437973022, 0.47951850295066833, -0.6113030314445496, -0.4437432289123535, 0.015460411086678505, 0.11547895520925522, 0.14383994042873383, 0.7576883435249329, 0.8259192109107971, 0.5445500016212463, 0.4825843870639801, 0.2017488181591034, 0.5762704014778137, -0.662537157535553, -0.33630868792533875, -0.15988001227378845, -0.49091336131095886, -0.1710643619298935, -0.18207292258739471, -0.9604792594909668, -0.3221164345741272, 0.14265961945056915, 0.16763651371002197, 0.15979085862636566, 0.28507116436958313, 1.250844955444336, 0.571523904800415, 0.7188795208930969, -0.2206011414527893, -0.6516628265380859, -0.4643970727920532, -1.3925248384475708, -0.12673962116241455, -0.319274365901947, -0.20345743000507355, -0.1692889779806137, 0.042233068495988846, -0.045105837285518646]}, "authors": [{"authorId": "46181066", "name": "Iz Beltagy"}, {"authorId": "39139825", "name": "Matthew E. Peters"}, {"authorId": "2527954", "name": "Arman Cohan"}], "references": [{"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "63857190aaf5aab1d94b54bb257b7b03b8cb5a50", "title": "GMAT: Global Memory Augmentation for Transformers"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "c6c547e5ecbb8e26bae02fd6f37154145ae0053b", "title": "A Simple Yet Strong Pipeline for HotpotQA"}, {"paperId": "ac87ce22d3b8cd2668793d93ce7b361cda7193c0", "title": "A Divide-and-Conquer Approach to the Summarization of Academic Articles"}, {"paperId": "8f0e74dfc845318b84fe8a8166db0957c6e2c24e", "title": "Is Graph Structure Necessary for Multi-hop Reasoning?"}, {"paperId": "40c71c9ab559c3a39f1d20f221c8c02e6f394c4b", "title": "Graph Sequential Network for Reasoning over Sequences"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "2c88d7486f9871cb741ba3c7076b8adbb7fd5b68", "title": "Hierarchical Graph Network for Multi-hop Question Answering"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "5534d774a06039e13b72876c21d39949132b512b", "title": "Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "acf5a74ccb14b01430dab2d200d9aabc5ee9dc16", "title": "MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension"}, {"paperId": "42e66d52cfa212bafc96dcf4b2abe1c5c403e53c", "title": "Multi-hop Question Answering via Reasoning Chains"}, {"paperId": "6c8503803760c5c7790f72437d0f8b874334e6f0", "title": "Span Selection Pre-training for Question Answering"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "127ffe6d21b75bd41dd808e3313bc392b9428346", "title": "BERT for Coreference Resolution: Baselines and Analysis"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "fc089a09074c84979d1f34e89341318a5bc26d3d", "title": "SemEval-2019 Task 4: Hyperpartisan News Detection"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "0feea94f89d395436bf41bd10c797447eecbc128", "title": "Unsupervised Data Augmentation for Consistency Training"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "e6566ece21f6637c515fe9969f9d1ec6cca6d36c", "title": "Higher-Order Coreference Resolution with Coarse-to-Fine Inference"}, {"paperId": "853d4d94651c6d9f8ed4d114e1eb21f15f786daa", "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "3c78c6df5eb1695b6a399e346dde880af27d1016", "title": "Simple and Effective Multi-Paragraph Reading Comprehension"}, {"paperId": "7d5cf22c70484fe217936c66741fb73b2a278bde", "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "104715e1097b7ebee436058bfd9f45540f269845", "title": "Reading Wikipedia to Answer Open-Domain Questions"}, {"paperId": "df0402517a7338ae28bc54acaac400de6b456a46", "title": "WaveNet: A Generative Model for Raw Audio"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "f8cdf754fb7c08caf6e2f82b176819230910be5b", "title": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "5665805becad6c87b194b260f2270d86d560bd3f", "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}, {"paperId": "a07609c2ed39d049d3e59b61408fb600c6ab0950", "title": "GPU Kernels for Block-Sparse Weights"}, {"paperId": null, "title": "Semisupervised classification with graph convolutional networks"}, {"paperId": null, "title": "Large text compression benchmark"}]}