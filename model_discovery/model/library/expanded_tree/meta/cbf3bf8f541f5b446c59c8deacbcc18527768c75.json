{"paperId": "cbf3bf8f541f5b446c59c8deacbcc18527768c75", "abstract": "Industrial recommender systems have been growing increasingly complex, may involve \\emph{diverse domains} such as e-commerce products and user-generated contents, and can comprise \\emph{a myriad of tasks} such as retrieval, ranking, explanation generation, and even AI-assisted content production. The mainstream approach so far is to develop individual algorithms for each domain and each task. In this paper, we explore the possibility of developing a unified foundation model to support \\emph{open-ended domains and tasks} in an industrial recommender system, which may reduce the demand on downstream settings' data and can minimize the carbon footprint by avoiding training a separate model from scratch for every task. Deriving a unified foundation is challenging due to (i) the potentially unlimited set of downstream domains and tasks, and (ii) the real-world systems' emphasis on computational efficiency. We thus build our foundation upon M6, an existing large-scale industrial pretrained language model similar to GPT-3 and T5, and leverage M6's pretrained ability for sample-efficient downstream adaptation, by representing user behavior data as plain texts and converting the tasks to either language understanding or generation. To deal with a tight hardware budget, we propose an improved version of prompt tuning that outperforms fine-tuning with negligible 1\\% task-specific parameters, and employ techniques such as late interaction, early exiting, parameter sharing, and pruning to further reduce the inference time and the model size. We demonstrate the foundation model's versatility on a wide range of tasks such as retrieval, ranking, zero-shot recommendation, explanation generation, personalized content creation, and conversational recommendation, and manage to deploy it on both cloud servers and mobile devices.", "venue": "arXiv.org", "year": 2022, "citationCount": 133, "influentialCitationCount": 17, "openAccessPdf": {"url": "https://arxiv.org/pdf/2205.08084", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper builds their foundation upon M6, an existing large-scale industrial pretrained language model similar to GPT-3 and T5, and leverage M6's pretrained ability for sample-efficient downstream adaptation, by representing user behavior data as plain texts and converting the tasks to either language understanding or generation."}, "embedding": {"model": "specter_v2", "vector": [0.24081023037433624, 0.5432107448577881, -0.3076878786087036, -0.24769145250320435, -0.0702744573354721, -0.16623686254024506, 0.56339031457901, -0.5758079886436462, -0.24210213124752045, -0.330550879240036, 0.5615442991256714, 0.10258606821298599, -0.2746853232383728, 0.07028870284557343, -0.2132585644721985, 0.35812172293663025, -0.8559674620628357, 0.9077862501144409, -0.3174505829811096, -0.7329925894737244, -0.09350631386041641, -1.1888401508331299, -0.8922328948974609, 0.276233971118927, 0.5891382694244385, 0.19736348092556, 0.2598215341567993, 1.033428430557251, -0.11242558062076569, -0.11430802941322327, 0.7058360576629639, -0.7316184639930725, 0.3653736114501953, -0.19456233084201813, -0.06989365816116333, 0.23449508845806122, -0.3518007695674896, -0.6542878746986389, -0.1755913347005844, 0.45086798071861267, -0.04554792866110802, 0.5937209129333496, 0.7032856941223145, -0.4363698959350586, -0.9094021320343018, 0.9073898792266846, 1.2270538806915283, 0.233799010515213, 0.23418964445590973, -0.6748130917549133, 1.7733845710754395, -1.5965558290481567, 0.323516845703125, 1.5062384605407715, 0.37868449091911316, 0.7935128211975098, -0.38377073407173157, -0.40222927927970886, 0.7131943702697754, -0.35090065002441406, -0.823576807975769, -0.21704818308353424, -0.21918652951717377, -0.09523356705904007, 1.2935311794281006, -0.3519955575466156, -0.1614697277545929, 1.046345829963684, -0.07056260108947754, 1.2082499265670776, -0.4469756484031677, -0.49094295501708984, 0.04625462368130684, 0.49926260113716125, 0.22845818102359772, 0.6008760929107666, -0.26032981276512146, 0.3697764277458191, -0.800469160079956, 0.01660625822842121, 0.4286920428276062, 0.1563335806131363, 0.010150404646992683, -0.13493449985980988, 0.07603681832551956, 1.07180917263031, 0.12825146317481995, 0.7245281934738159, -0.140560120344162, 0.34941622614860535, 0.18651030957698822, 0.298078715801239, 0.16913709044456482, 0.46648019552230835, -0.26036953926086426, 0.3283657133579254, -0.45994412899017334, 0.564296543598175, 0.33876392245292664, 1.041254997253418, -0.2728973925113678, -0.19774626195430756, -1.0137007236480713, 0.49225255846977234, 1.6825857162475586, -0.17845505475997925, 0.8625695109367371, -0.874639630317688, 0.031187232583761215, -0.6647799015045166, 0.7542189955711365, -0.5570909380912781, -0.27642524242401123, -0.23121637105941772, -0.83929842710495, -1.5829792022705078, -0.46303367614746094, -0.16709266602993011, -0.6062040328979492, 0.5025588870048523, -0.5147770643234253, 0.22077496349811554, 0.14918076992034912, 0.814681887626648, 0.7549090385437012, 0.6878156661987305, -0.022261664271354675, 0.13833795487880707, 0.35729753971099854, -1.0394433736801147, -0.6779283881187439, -1.2641551494598389, 0.7084112763404846, 0.10507135838270187, 0.4637335240840912, -0.03274639695882797, -1.1438924074172974, -0.9133579730987549, -0.7386703491210938, -0.2153623104095459, -0.45278212428092957, 0.5273471474647522, 1.639620304107666, 0.6936309933662415, -0.5755724906921387, 0.7508842945098877, 0.07440516352653503, -0.2776070237159729, -0.1946246176958084, 0.40500059723854065, 0.5123152136802673, -0.7078126668930054, -1.5161985158920288, -0.08510375022888184, 0.16156068444252014, -0.5161067843437195, -0.36256760358810425, -0.3885141909122467, -0.7388967871665955, 0.09356879442930222, 0.7846104502677917, -0.9469649195671082, 1.6614389419555664, -0.25953948497772217, -1.7855461835861206, 0.2616710066795349, 0.13544951379299164, 0.05784916505217552, 0.29905033111572266, -0.41703829169273376, -0.8777945041656494, -0.8814776539802551, -0.3235456645488739, 0.1401984542608261, 0.3102582097053528, -0.15365110337734222, -0.22911949455738068, 0.41831958293914795, 0.03456040844321251, 0.25495919585227966, -0.18221095204353333, 0.696548342704773, -0.9380170106887817, 0.11890466511249542, -0.0031649109441787004, 0.6144787073135376, -0.44221144914627075, -0.3504329025745392, 0.17977996170520782, -1.1435883045196533, 0.2047746330499649, 0.26625242829322815, 1.0177998542785645, -0.7722451090812683, -0.7217205762863159, 0.14517982304096222, -0.3296601176261902, 0.02559802122414112, -0.8310278654098511, 0.6188294291496277, 0.19175320863723755, 0.3176153302192688, 0.1665080040693283, -1.0761525630950928, 0.46574312448501587, -0.028623826801776886, -0.260539174079895, -0.5813450813293457, -0.18586164712905884, 0.8796555399894714, -1.0293207168579102, 0.30874982476234436, -0.3053026795387268, 0.41941037774086, -1.7231276035308838, 1.1202647686004639, -0.9205811619758606, -0.02004147320985794, -0.8202553391456604, -0.5543763637542725, -0.19590644538402557, -0.43189114332199097, 0.5373433232307434, 0.05253556743264198, 0.18281559646129608, 0.4084852635860443, -0.886099100112915, 1.61214280128479, -0.342293381690979, -0.21117569506168365, -0.16345787048339844, -0.226351797580719, 0.2577345669269562, 0.5214399099349976, -0.1164390817284584, -0.33559390902519226, 0.1713341772556305, 0.5738077759742737, -0.63918536901474, 0.08418094366788864, 0.7153196334838867, 0.6707016825675964, -0.40697699785232544, 0.32825279235839844, 0.6645075082778931, -0.022998154163360596, 0.816321849822998, 0.3563882112503052, 0.4662690758705139, 0.5577318668365479, 0.10922681540250778, -0.026933882385492325, 0.4695310890674591, -1.1416209936141968, 0.03191302344202995, 0.9094125628471375, 1.015102505683899, 1.0529693365097046, -0.33594030141830444, -0.8034324049949646, -0.06412411481142044, 0.18285010755062103, 1.0017924308776855, 1.9262279272079468, 0.012169618159532547, -0.5661354660987854, -0.5006271600723267, -0.16647550463676453, -0.2706167995929718, 0.6739968061447144, -0.5316280722618103, -0.1723073571920395, -0.2524409294128418, -1.0921623706817627, 0.1798679679632187, 0.1721566617488861, 0.6617424488067627, -0.24154575169086456, 0.317523717880249, 0.01207690592855215, -0.02668681927025318, -0.35912269353866577, -1.229570984840393, 0.07393147051334381, -0.20741748809814453, 0.357991486787796, -0.19524209201335907, 0.07519327849149704, 0.015964800491929054, -0.4958529472351074, 0.9881001710891724, -0.10778937488794327, 0.07018113136291504, 0.2979625165462494, 0.16252224147319794, -0.20302988588809967, -0.5700554251670837, -0.14276528358459473, 0.40933024883270264, 0.08312931656837463, 0.20068374276161194, 0.3547966480255127, -0.19958944618701935, 0.5033159255981445, -0.04171459749341011, 0.21844951808452606, 0.10068558901548386, 0.12786361575126648, 0.4492983818054199, 0.002643309533596039, -0.28757643699645996, -1.668752908706665, 1.0046772956848145, -0.03249114751815796, -0.29268863797187805, 0.11734860390424728, -1.1376248598098755, -0.5610684156417847, 0.7109785079956055, -0.8904808759689331, -0.8119648694992065, -1.149936318397522, 0.20030415058135986, 0.17180588841438293, -0.2077978402376175, 0.49923279881477356, 0.014936882071197033, 0.49622416496276855, 0.42534518241882324, 0.5983294248580933, -0.15041334927082062, -0.5337100625038147, 0.908584475517273, -0.7738295793533325, 0.3859994411468506, -0.03121483325958252, 0.3050891160964966, -0.5795385241508484, -0.44498565793037415, -0.4909592270851135, -0.7687862515449524, -0.39767396450042725, -0.08136957138776779, -0.19960545003414154, 0.07857096940279007, -0.5735114812850952, -0.7771815657615662, -0.6820885539054871, -0.6431929469108582, -0.2787803113460541, 0.3288329541683197, -0.22082072496414185, -0.3119371831417084, -1.3178907632827759, -0.9720636606216431, -0.7465888857841492, -0.652233898639679, -0.5131970643997192, 0.2903303802013397, -0.008839037269353867, -0.06801308691501617, -0.6584669351577759, 0.305720716714859, 0.10923876613378525, 1.044156551361084, -0.9680866003036499, 0.5282084941864014, -0.19625097513198853, -0.12018634378910065, -0.34569284319877625, 0.42752113938331604, 0.17676319181919098, -0.433063805103302, 0.14507950842380524, -0.29055535793304443, 0.31333521008491516, -0.44520828127861023, -0.042960114777088165, -0.19937102496623993, 0.15821605920791626, 0.5460062026977539, -0.21352118253707886, -0.3377652168273926, 0.40955936908721924, 1.12458336353302, -0.5169770121574402, 0.028504377231001854, 0.05677546560764313, 0.8447189331054688, 0.11320342868566513, 0.0696190595626831, 0.7479172348976135, 0.5014516115188599, 0.6522151231765747, -0.30989140272140503, 0.1280042976140976, 0.21369266510009766, -0.9940431714057922, 0.8812210559844971, 1.2697042226791382, -0.14887134730815887, -0.6634921431541443, -0.663744330406189, 0.36343443393707275, -1.517547607421875, -0.9569753408432007, 0.3778747320175171, 0.7988229990005493, 0.15247249603271484, -0.3609350621700287, -0.02219233103096485, -0.5759888887405396, 0.21806541085243225, -0.3493025004863739, -0.32344263792037964, -0.6150071620941162, 0.2969089448451996, 0.037452392280101776, -0.1645275503396988, 0.5764347314834595, 0.036078546196222305, 0.5116240382194519, 14.499372482299805, 0.5849601030349731, 0.5381305813789368, 0.5640741586685181, 0.7479501366615295, 0.18295404314994812, -0.6089958548545837, -0.3212284445762634, -1.235639214515686, -0.12098630517721176, 1.6271259784698486, -0.4283381402492523, 0.9197760820388794, 0.3777311146259308, 0.43655070662498474, 0.6030889749526978, -0.7303102016448975, 0.39906492829322815, 0.5726321935653687, -1.0652797222137451, 0.6781526207923889, 0.5071531534194946, 0.3755844533443451, 0.43885406851768494, 1.151476263999939, 1.366698980331421, 0.768704354763031, -0.336523175239563, 0.3425261080265045, 0.5469431281089783, 0.673226535320282, -0.1941082626581192, 0.39058828353881836, 0.9291812777519226, -0.41307181119918823, -0.39248916506767273, -0.8687929511070251, -1.2212035655975342, 0.3411102592945099, -0.10176312923431396, -0.639036238193512, -0.20612689852714539, -0.37087011337280273, 0.7317240238189697, 0.3427662253379822, 0.09682466834783554, -0.0702664703130722, 0.6266022324562073, -0.10081344842910767, -0.18174269795417786, 0.15795789659023285, 0.006143700797110796, 0.091072216629982, -0.23416566848754883, 0.5153343081474304, 0.4962826669216156, 0.36119046807289124, 0.5928261280059814, -0.5143810510635376, 0.10023807734251022, -0.08687740564346313, -0.24631337821483612, -0.4462992250919342, 0.7581048011779785, 0.8824965953826904, 0.2081240713596344, -0.6054226756095886, 0.4906560182571411, 0.22158505022525787, 0.0386292040348053, -0.1621994525194168, 0.44909921288490295, 0.22649115324020386, -0.22426116466522217, -0.030441800132393837, 0.6962023973464966, 0.028980432078242302, -0.8934469223022461, -1.2489166259765625, -0.4486227035522461, 0.32908207178115845, -0.9137081503868103, -1.1492027044296265, 0.6734969615936279, -0.4912426471710205, -0.44458460807800293, -0.44786548614501953, -0.713927686214447, -0.08933616429567337, 0.36242470145225525, -0.9562625288963318, -0.67507404088974, 0.6244240403175354, -0.27732664346694946, -0.0984693393111229, 0.06572099030017853, 1.5215439796447754, 0.24327978491783142, -0.5043236613273621, 0.12470298260450363, 0.4916737675666809, -0.3492395579814911, -0.1473224014043808, -0.912444531917572, 0.7362032532691956, 0.12734396755695343, 0.04109221324324608, 0.7646956443786621, 0.3605906367301941, 0.33355194330215454, -1.1099013090133667, -0.44271156191825867, 0.6971306204795837, -1.00361168384552, -0.6760967969894409, -1.0993503332138062, -0.44004136323928833, 0.2829558551311493, 0.4481657147407532, -0.46377143263816833, 0.6625707149505615, 0.5806697010993958, -0.18336549401283264, -0.11371097713708878, -0.8042324185371399, 0.2267572432756424, 0.8261955976486206, -0.1654839664697647, -0.16321799159049988, 0.2955905497074127, 0.02017395943403244, -1.0054779052734375, -0.2717723250389099, -0.40513086318969727, 0.44015616178512573, -0.129150852560997, 1.0772552490234375, -0.14254429936408997, 0.5297479629516602, 0.9399436712265015, -0.015646591782569885, -0.7645255327224731, -0.8168143033981323, -1.1508185863494873, 0.12169977277517319, 0.17776033282279968, 1.0109251737594604, -0.0522976852953434, 0.2916339039802551, 1.3785662651062012, 0.3111816346645355, -0.050970468670129776, 0.18513444066047668, -0.20982839167118073, -0.06497140228748322, -0.34398365020751953, 0.28179019689559937, -0.3544192910194397, -0.09201402962207794, 0.45259612798690796, -0.1435343474149704, 0.8548281788825989, -0.21698997914791107, -0.789181113243103, 0.7453392744064331, -0.19032113254070282, -0.6511566042900085, -0.14561058580875397, 0.4741506278514862, -1.9314851760864258, -0.39275357127189636, -1.0645743608474731, 0.4426836669445038, -1.087111473083496, -0.2760046422481537, 0.2998202443122864, -0.4261494576931, -0.5070711374282837, 0.4915130138397217, -0.1317601054906845, -0.4684412181377411, -0.6925770044326782, -0.8905627131462097, 0.681820809841156, 1.081760287284851, -0.5849990248680115, -0.18131522834300995, 0.44345107674598694, -0.3990718424320221, 0.31391310691833496, 0.36952048540115356, -0.2658623158931732, -1.0708006620407104, -1.2616058588027954, 0.6302079558372498, 0.028865080326795578, -0.40891194343566895, -0.5057384967803955, 0.722556471824646, 0.12397442013025284, -0.14327478408813477, 0.18055720627307892, 0.42708200216293335, -0.8686913847923279, 0.0233388002961874, -0.19275225698947906, -1.0932235717773438, 0.08153806626796722, -0.5218400955200195, -0.36301788687705994, 0.3884977698326111, 0.6644222140312195, -0.5598024725914001, -1.1761678457260132, -0.5553763508796692, 0.7157650589942932, -0.6814221739768982, 0.026754610240459442, -0.7543511390686035, 0.0002338015619898215, -0.6033188104629517, -0.8304948210716248, 0.019890235736966133, 0.31840217113494873, -0.6475285887718201, 1.0978091955184937, 0.6105882525444031, -1.0648273229599, -0.3557544946670532, 0.3481979966163635, -0.010957451537251472, -0.3387983739376068, 0.6572127342224121, 0.04130725562572479, 0.17316551506519318, 0.5569842457771301, 0.5350078344345093, 0.6531612277030945, -0.5328496694564819, 0.19832058250904083, 0.7430526614189148, -0.6312379837036133, -0.4864746332168579, 1.122618556022644, -0.2893776297569275, -1.1150288581848145, -0.004527634475380182, -0.8749163746833801, -0.7991786003112793, -0.9115468263626099, 0.7155048847198486, 0.07836735993623734, -0.45217832922935486, -0.1894071251153946, -0.3421027958393097, 0.10725869238376617, -0.2740080952644348, -0.4779926538467407, 0.917295515537262, -0.648351788520813, 0.0771438479423523, 0.5458303093910217, 0.716659426689148, -0.5617538690567017, -0.5333448052406311, -0.389788419008255, -0.23421908915042877, -0.14024297893047333, 0.518966794013977, -1.1292215585708618, -0.23940782248973846, 0.20579981803894043, 0.4603469669818878, 0.19745416939258575, 0.32090091705322266, -0.22066108882427216, 0.1370212733745575, 0.8136186003684998, 0.268451452255249, -0.614310085773468, -0.47777891159057617, 1.0809422731399536, 1.243945837020874, -0.8932558298110962, 0.22038723528385162, -0.2404245138168335, -1.015417218208313, 0.7459068298339844, 0.2780626714229584, -0.30736029148101807, 0.8856638073921204, -0.574366569519043, -0.1261414885520935, 0.2113988995552063, -1.5392554998397827, -0.30988889932632446, 1.3315221071243286, 1.0471312999725342, 0.9021532535552979, 0.3929098844528198, -0.35062241554260254, 1.5341558456420898, 0.07723796367645264, 0.5161355137825012, 0.6955254077911377, -0.2987429201602936, -0.10521596670150757, -0.38781997561454773, 0.4502071142196655, 0.5533875226974487, -0.4238871932029724, -0.47310352325439453, 0.09400343149900436, 0.30241987109184265, 0.5201336145401001, 0.9134885668754578, 0.050269681960344315, 0.07187780737876892, 0.7086159586906433, -0.12410838901996613, 0.17573122680187225, -0.9818964600563049, -0.42882153391838074, 0.24999916553497314, -0.36297544836997986, -0.12998473644256592, 0.0852387323975563, -0.42396125197410583, -0.23211099207401276, -0.0216645710170269, 0.19090041518211365, 0.2748696804046631, -0.11503700911998749, 1.136474847793579, 0.771260142326355, 0.11955225467681885, -0.25688499212265015, -0.6970605850219727, -0.4889649748802185, -0.7758470773696899, -0.2989159822463989, -0.11261771619319916, -0.630423367023468, -0.38141152262687683, -0.3333873748779297, -0.2865048050880432]}, "authors": [{"authorId": "72723327", "name": "Zeyu Cui"}, {"authorId": "47793076", "name": "Jianxin Ma"}, {"authorId": "144161025", "name": "Chang Zhou"}, {"authorId": "1709595", "name": "Jingren Zhou"}, {"authorId": "38385080", "name": "Hongxia Yang"}], "references": [{"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "873bdaeb8110bcd660d0fa21a42b5d105767856c", "title": "Zero-Shot Recommendation as Language Modeling"}, {"paperId": "9202a718ce05395b6e17d5301e3a2e8b1021f31b", "title": "Prune Once for All: Sparse Pre-Trained Language Models"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "24e775b20adf21e9b5b95c6a9b7a5c164d055849", "title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "238bb889ec2c817646c438fac26ccb3fac86a5a0", "title": "M6: Multi-Modality-to-Multi-Modality Multitask Mega-transformer for Unified Pretraining"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "bc764c56ebb9096714070b91536997818a2ae442", "title": "Pre-trained Language Model for Web-scale Retrieval in Baidu Search"}, {"paperId": "f131e2f7bcf250a7ee25b79a0b9a442f12bd7df1", "title": "M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers"}, {"paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "title": "CogView: Mastering Text-to-Image Generation via Transformers"}, {"paperId": "0c9a2adda11ed49d091948211fcfd517113b5243", "title": "Personalized Transformer for Explainable Recommendation"}, {"paperId": "069c109507ee685dfe04534f0461b837c5cb224d", "title": "Pre-trained Language Model based Ranking in Baidu Search"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "120c8155652168cce8637c83b7ac95c7551ee43e", "title": "Practical Compositional Fairness: Understanding Fairness in Multi-Component Recommender Systems"}, {"paperId": "a11676f2864b2d923bb9facc9f6548c812f9e005", "title": "M6: A Chinese Multimodal Pretrainer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "20cfb6eaf5e3c06af379fb161a84297b88ab1b9c", "title": "Generate Neural Template Explanations for Recommendation"}, {"paperId": "5ef1cea9e0cdf5b30db6e0ac3f948be6ae0c34fe", "title": "PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision"}, {"paperId": "baf2843b8720906d8d405f05395ea36aa1930fff", "title": "One Person, One Model, One World: Learning Continual User Representation without Forgetting"}, {"paperId": "0cd4811f7387486dc4a967f964f7e662ab79e212", "title": "Knowledge Transfer via Pre-training for Recommendation: A Review and Prospect"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "2275558bfc6003fd7b146a86a70f3512a33f5874", "title": "Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "21082bd98071f6948097df05cc9e4770fcd87de6", "title": "Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems"}, {"paperId": "60b8ad6177230ad5402af409a6edb5af441baeb4", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "704a1a4ff7b6fed65b0c49ef87b6845d60755fa7", "title": "TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval"}, {"paperId": "2ca1abd015c72e7c04d63fa653b0a27d9592fc02", "title": "Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation"}, {"paperId": "41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76", "title": "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "42ffd3a1ab4a139c5ec33b50bd7e5759077f03c4", "title": "Towards Knowledge-Based Recommender Dialog System"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "288b40c498c56257bb6e3d53120c5a06bed1873f", "title": "Towards Knowledge-Based Personalized Product Description Generation in E-commerce"}, {"paperId": "303b260d5bef9b5c87c868110ec429fe5ea934ad", "title": "Explainable Recommendation: A Survey and New Perspectives"}, {"paperId": "76d8ccc5a35affb08271fee720a24418c8943b44", "title": "Visually-Aware Fashion Recommendation and Design with Generative Image Models"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "47527f35481e3b16c64e3a27d46499f5efeca096", "title": "Neural Rating Regression with Abstractive Tips Generation for Recommendation"}, {"paperId": "b8c6ccd5c1eb4f9837fc4877d27e55b7349781be", "title": "Deep Interest Network for Click-Through Rate Prediction"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f904a1037c4d31d441e5ecec479d08d9384426fd", "title": "Learning to Generate Product Reviews from Attributes"}, {"paperId": "125ccd810f43f1cba83c6681836d000f83d1886d", "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification"}, {"paperId": "896de8418884f4aab1ae4a60027500c9e8baffc3", "title": "BranchyNet: Fast inference via early exiting from deep neural networks"}, {"paperId": "5e383584ccbc8b920eaf3cfce3869da646ff5550", "title": "Deep Neural Networks for YouTube Recommendations"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "7422756d2416c62d7660bd217d817acc8ec35a09", "title": "On Transferability of Prompt Tuning for Natural Language Understanding"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Conference\u201917, July 2017, Washington, DC, USA"}]}