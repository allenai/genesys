{"paperId": "e10ee483325f590b1e139dfafdb03edeb2e1766a", "abstract": "Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work studies the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability and proposes a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention."}, "embedding": {"model": "specter_v2", "vector": [0.12556515634059906, 0.5995844602584839, -0.45637524127960205, -0.08731094002723694, -0.3689258098602295, -0.011280515231192112, 0.5932976007461548, -0.2581593096256256, -0.3598043918609619, -0.2497376650571823, 0.5513666272163391, 0.4889233410358429, 0.3119761049747467, -0.2515171468257904, -0.4280511140823364, 0.039077308028936386, -0.793143630027771, 0.4018124043941498, 0.18993762135505676, -0.2767189145088196, 0.18416595458984375, -0.8082363605499268, -1.1290607452392578, 0.19911374151706696, 0.5486929416656494, 0.5712460279464722, 0.5056746006011963, 0.9199320673942566, -0.7764186859130859, 0.47128239274024963, 0.541207492351532, -0.4177588224411011, 0.16856637597084045, -0.1981978416442871, -0.1395713835954666, -0.020066747441887856, 0.8595884442329407, -0.35661256313323975, -0.5864911079406738, 1.1086184978485107, -0.05712521821260452, 0.054079651832580566, 0.5600965619087219, -0.29391512274742126, -0.7889708876609802, 0.7405228018760681, 0.4013465344905853, 0.997828483581543, 0.026296541094779968, -0.6499713063240051, 1.563658356666565, -1.2462985515594482, 0.2980886995792389, 1.8408795595169067, 0.2844586670398712, 0.31610262393951416, 0.061037782579660416, -0.6941777467727661, 0.9022228717803955, 0.5163319706916809, -0.7693532705307007, -0.3357408344745636, 0.07564780861139297, -0.10738363862037659, 1.737532615661621, -0.49835434556007385, -0.09224382042884827, 0.42892009019851685, 0.22595439851284027, 1.5919915437698364, -0.07459219545125961, -1.074615478515625, -0.38171154260635376, 0.2172987163066864, 0.1912795901298523, 0.871361255645752, -0.3337869644165039, -0.1393643170595169, -0.9489830732345581, 0.055235039442777634, 0.5425800681114197, -0.3465519845485687, -0.11006837338209152, -0.3916164040565491, 0.06336862593889236, 0.48890867829322815, 0.3714470863342285, 0.990307629108429, -0.303495317697525, 0.6412973999977112, 0.3086492717266083, 0.3900904953479767, 0.26056328415870667, 0.32895398139953613, 0.36874714493751526, 0.2567903995513916, -1.001283049583435, 0.16080576181411743, -0.28906676173210144, 1.1589703559875488, 0.08569914102554321, 0.24419443309307098, -0.6741255521774292, 0.3219318687915802, 1.294442892074585, 0.40384572744369507, 0.9488316774368286, -0.3825131952762604, 0.03567387908697128, -0.4302907884120941, 0.116523876786232, -1.3169543743133545, 0.14956152439117432, -0.4494285583496094, -0.8424012064933777, -1.2069085836410522, -0.3946705758571625, 0.7741460800170898, -0.4414602816104889, 0.6999292373657227, -0.4769243896007538, -0.40507906675338745, -0.18675705790519714, 0.2336622178554535, 0.2438308149576187, 0.6211381554603577, 0.30139926075935364, 0.07475065439939499, 0.9939355850219727, -0.7869967222213745, -0.8151602745056152, -1.119475245475769, 0.35926735401153564, -0.2630985975265503, 0.6887130737304688, -0.21495485305786133, -1.1589727401733398, -0.9196150898933411, -0.6485309600830078, 0.11342424899339676, -0.4185216426849365, 0.30003735423088074, 0.5204697251319885, 0.1929679661989212, -1.2670483589172363, 0.5139763355255127, -0.0655626729130745, -0.6087020039558411, 0.6795311570167542, -0.1528630554676056, 0.16906093060970306, -0.5729113221168518, -1.3731141090393066, 0.4890434145927429, -0.06649747490882874, -0.6790472865104675, -0.06435680389404297, -0.515637993812561, -1.2460845708847046, 0.02297578938305378, 0.33255067467689514, -0.2926231622695923, 0.9992189407348633, -0.38572168350219727, -0.969912052154541, 0.8517192602157593, -0.7626021504402161, -0.22807346284389496, 0.44498470425605774, -0.4802553355693817, -0.28441211581230164, -0.29137247800827026, 0.22299234569072723, 0.5135318040847778, 0.5433675646781921, -0.3144146502017975, -0.3239290416240692, -0.0664961040019989, -0.4464043974876404, -0.0735335648059845, -0.6563002467155457, 0.7762405276298523, -0.8442649245262146, -0.3047189712524414, 0.3001551032066345, 0.7449063062667847, 0.39796122908592224, -0.4210667014122009, -0.3830919563770294, -1.24767005443573, 0.6631737947463989, 0.12882447242736816, 1.081181526184082, -0.775432825088501, -0.8539897203445435, -0.27205678820610046, -0.19462604820728302, -0.33780011534690857, -0.8708654642105103, 0.4977032244205475, -0.52071613073349, 0.3286048173904419, 0.04271669685840607, -0.9227287769317627, 0.029253972694277763, -0.5615236163139343, -0.645484447479248, -0.07788517326116562, 0.1103590801358223, 1.2513022422790527, -1.2331044673919678, -0.14441807568073273, -0.27871057391166687, 0.2922852635383606, -0.6917389631271362, 1.2941596508026123, -0.45403310656547546, -0.13482409715652466, 0.006219862494617701, -0.2547869384288788, 0.052750322967767715, -0.4910713732242584, 0.19189642369747162, -0.43992748856544495, -0.06650868058204651, 0.35887086391448975, 0.043895430862903595, 0.8717291355133057, -0.293925940990448, 0.9775243401527405, -0.4121658205986023, -0.5788346529006958, 0.15036797523498535, 0.038865845650434494, -0.06657642126083374, -0.7691574096679688, 0.30078545212745667, -0.24159285426139832, -0.7329325079917908, 0.12360944598913193, 0.642400860786438, 1.060159683227539, -0.09437556564807892, 0.23249949514865875, 0.7047893404960632, -0.003193202894181013, 0.7538222670555115, 0.3918132781982422, 0.7091014385223389, 0.3982107937335968, 0.5876145362854004, -0.25733092427253723, 0.42638352513313293, -0.7627887725830078, -0.37416502833366394, 0.6894088387489319, 0.7631166577339172, 0.6351340413093567, 0.402418315410614, -0.40213069319725037, -0.7519691586494446, 0.1150629073381424, 0.8215307593345642, 1.578531265258789, -0.4320776164531708, -0.3301503360271454, -0.8832553029060364, -0.03315198794007301, -0.6519727110862732, 0.11106518656015396, -0.6475986242294312, -0.24664725363254547, -0.370652437210083, -0.9603333473205566, 0.5798442959785461, 0.3281921446323395, 1.2337456941604614, -0.590509295463562, -0.468539834022522, -0.0639883354306221, 0.17668204009532928, -0.8214463591575623, -0.7611015439033508, -0.010702495463192463, -0.3904310464859009, -0.163986936211586, -0.365161269903183, -0.0014484103303402662, 0.20926077663898468, -0.6143431663513184, 1.0338505506515503, -0.7741833925247192, -0.25298529863357544, 0.7570497393608093, 0.4959408938884735, -0.9664499163627625, -0.4435858130455017, 0.21832077205181122, 0.3138444423675537, 0.11932650953531265, 0.7459499835968018, 0.4929247498512268, -0.11242307722568512, 0.2238362729549408, -0.33090224862098694, -0.3248366415500641, 0.2935947775840759, 0.13508929312229156, 0.6690921783447266, 0.19158565998077393, -0.1487119346857071, -0.9901333451271057, 1.0589585304260254, 0.39856305718421936, -0.5631770491600037, 0.2615821361541748, -0.8791578412055969, -0.5617166757583618, 0.1291877031326294, -0.8334779739379883, -0.35558998584747314, -0.7221021056175232, 0.8386719822883606, -0.42089203000068665, -0.1604042947292328, 0.15900301933288574, -0.1810881644487381, 0.17538012564182281, 0.28315216302871704, 0.7136375308036804, 0.3241944909095764, 0.20864184200763702, 0.5057350397109985, -0.8986647725105286, 0.8626170754432678, 0.27291589975357056, 0.10266668349504471, 0.09695550799369812, -0.45825499296188354, -1.0255509614944458, -0.5523059964179993, -0.5688201189041138, -0.2846301198005676, -0.3976080119609833, 0.17726509273052216, -0.5318456292152405, -0.7533637881278992, 0.12430693954229355, -0.9510446190834045, -0.04411521553993225, -0.00877733901143074, -0.3669432997703552, -0.34560704231262207, -0.78005450963974, -0.9589817523956299, -1.0081651210784912, -0.7974908351898193, -0.5843958258628845, 0.2817237377166748, 0.15629807114601135, -0.44513407349586487, -0.41731885075569153, -0.04328880086541176, -0.6272598505020142, 1.3729175329208374, -0.8369617462158203, 0.6511996388435364, -0.0315692201256752, -0.7337216138839722, -0.7003821134567261, 0.041755497455596924, 0.4481421113014221, 0.135393425822258, 0.030052660033106804, -0.635042667388916, 0.21448002755641937, 0.03177161142230034, -0.014105540700256824, 0.1255556046962738, 0.5813038349151611, 0.6421990990638733, -0.19705438613891602, -0.67446368932724, -0.12329959124326706, 1.4958738088607788, -0.5235066413879395, 0.19145245850086212, 0.6228305697441101, 0.8716955184936523, 0.4449615776538849, -0.3163069486618042, 0.30583682656288147, 0.6310995221138, 0.2235316038131714, 0.19993305206298828, -0.17986954748630524, -0.014212146401405334, -0.4884738028049469, 0.38617637753486633, 1.4796922206878662, 0.12678980827331543, -0.38773113489151, -1.0262424945831299, 0.994940459728241, -1.1767735481262207, -1.378157138824463, 0.9108093976974487, 0.5799059271812439, 0.09515559673309326, -0.36392173171043396, -0.32324254512786865, -0.2726384103298187, 0.6145393252372742, 0.20469911396503448, -0.45846378803253174, -0.10071875900030136, -0.44407403469085693, -0.02554505132138729, 0.3551625609397888, 0.5299546718597412, -0.49842900037765503, 0.80986487865448, 14.967791557312012, 0.5437511205673218, -0.14586079120635986, 0.5871280431747437, 0.703622579574585, 0.344482421875, -0.14189749956130981, -0.39123353362083435, -1.1587224006652832, -0.07790260016918182, 0.8129020929336548, -0.19160151481628418, 0.4300394654273987, 0.383378267288208, 0.15742208063602448, 0.1586824357509613, -0.4347165524959564, 0.6791511178016663, 1.0239580869674683, -0.9223851561546326, 0.776762068271637, 0.1328868418931961, 0.1645122766494751, 0.39603278040885925, 0.9151238799095154, 0.4250594973564148, 0.42839139699935913, -0.13553164899349213, 0.2925237715244293, 0.7444007992744446, 1.0318325757980347, 0.09555913507938385, 0.19135995209217072, 0.33897289633750916, -0.698647677898407, -0.3325742483139038, -0.8366605639457703, -0.7937667369842529, 0.15973861515522003, 0.34388822317123413, -0.2784021496772766, -0.6463001370429993, 0.4224238395690918, 0.6472788453102112, -0.165554478764534, 0.5093517899513245, -0.06827771663665771, 0.5197994112968445, 0.17508673667907715, -0.12721991539001465, 0.3565197288990021, 0.47464731335639954, 0.26965588331222534, 0.2535299062728882, 0.30318278074264526, 0.06268224120140076, -0.28262898325920105, 0.5340025424957275, -0.29164427518844604, 0.35536742210388184, -0.36362576484680176, -0.38636884093284607, -0.30571338534355164, 0.6668664813041687, 0.7095807790756226, 0.03203083202242851, -0.029581107199192047, 0.38108932971954346, 0.6217966079711914, 0.15334606170654297, -0.12772515416145325, -0.1456705778837204, 0.09751064330339432, 0.11641943454742432, 0.1287066638469696, 0.5878674387931824, -0.025111572816967964, -0.527757465839386, -0.8251298069953918, -0.3603515625, 0.1383819431066513, -1.1222213506698608, -1.0952092409133911, 1.2571316957473755, -0.16896075010299683, -0.250462144613266, 0.23032815754413605, -0.8525121212005615, -0.3151913583278656, 0.28571829199790955, -1.1220660209655762, -0.5221352577209473, -0.09559642523527145, -0.5233492851257324, -0.1896275132894516, -0.4541667103767395, 0.8190338015556335, 0.14398041367530823, -0.07652778178453445, 0.21733751893043518, 0.23144418001174927, -0.12486015260219574, 0.16812044382095337, -0.8394161462783813, 0.5053963661193848, 0.08962380886077881, -0.21117116510868073, 0.45768508315086365, 0.298414021730423, 0.12011031061410904, -0.5821136832237244, 0.10864043980836868, 0.8686617016792297, -0.7634131908416748, -0.20817869901657104, -0.8124328851699829, -1.1755962371826172, 0.12163613736629486, 0.718830406665802, -0.5182005167007446, 0.026989609003067017, -0.13515311479568481, -0.35359182953834534, -0.13523638248443604, -0.5822396874427795, 0.051637280732393265, 0.5006808042526245, -0.7692766785621643, -0.30528169870376587, 0.09772098809480667, 0.08485129475593567, -0.8828907608985901, 0.023080598562955856, -0.2860434651374817, -0.0006314261700026691, -0.2056482583284378, 0.8919200301170349, -0.32569146156311035, 0.7512665390968323, 0.7338032722473145, -0.03959544003009796, -0.7091947793960571, -0.6558698415756226, -0.5747589468955994, -0.015904204919934273, 0.6811519861221313, 0.4351470470428467, -0.3535710871219635, 0.44617679715156555, 0.6180733442306519, 0.6307881474494934, -0.49004802107810974, -0.6641921401023865, -0.3910404145717621, -0.09447717666625977, -0.428266704082489, 0.2317470908164978, -0.1179366260766983, -0.17459701001644135, 0.19086503982543945, 0.3336564898490906, 0.24574069678783417, 0.03612619265913963, -0.78785640001297, 0.4505752623081207, -0.1278400421142578, 0.5864975452423096, -0.7426075339317322, -0.5385967493057251, -1.5378541946411133, 0.2882682681083679, -0.8093745112419128, 0.057185783982276917, -1.38052499294281, -0.2912976145744324, 0.5107898116111755, -0.20711606740951538, -0.3514423072338104, 0.5870868563652039, -0.30653560161590576, -0.6249840259552002, -0.4040413796901703, -0.6915631890296936, 0.7846999764442444, 0.6410900950431824, -1.165972352027893, 0.24456211924552917, -0.08446653187274933, -0.12351473420858383, 0.40829530358314514, 0.38091129064559937, -0.34931623935699463, -0.5424533486366272, -1.139367938041687, 0.35003960132598877, -0.6774967908859253, -0.27612435817718506, -0.7504689693450928, 1.1090312004089355, 0.5544636845588684, 0.0919189602136612, -0.34165719151496887, 0.36269858479499817, -0.9376580715179443, -0.8382070064544678, 0.19859881699085236, -0.8681346774101257, 0.564428448677063, 0.008331049233675003, -0.2848355770111084, -0.4431925415992737, 0.848969578742981, 0.07800818234682083, -1.466166377067566, -0.4061146676540375, 0.49456578493118286, -0.8383548855781555, 0.34974154829978943, -0.29348623752593994, -0.22730600833892822, -0.9781373739242554, -0.8701626062393188, 0.06467652320861816, 0.33124056458473206, -0.3518722951412201, 1.176486849784851, 0.3596525192260742, -1.2992221117019653, 0.1864437460899353, 0.3311537504196167, 0.16621656715869904, -0.11991731077432632, 0.7236382961273193, 0.07711514830589294, -0.028697052970528603, 0.8270078897476196, 0.21156738698482513, 0.039811939001083374, -0.8361132144927979, 0.2429211139678955, 0.9719172120094299, -0.5129142999649048, -0.10601150989532471, 0.8848347663879395, 0.02450801245868206, -0.9352146983146667, 0.18286815285682678, -0.8612471222877502, -0.799142599105835, 0.11711952090263367, 0.8757150769233704, 0.31656691431999207, -0.45729756355285645, -0.5223388075828552, -0.4177406132221222, 0.5398263931274414, -0.20877546072006226, -0.5476539134979248, 0.5414692759513855, -0.4597100019454956, -0.6657825112342834, 0.7636623978614807, 0.571637749671936, -0.6612914204597473, -0.4059658944606781, -1.0180877447128296, -0.3263465464115143, -0.27189159393310547, 0.24315445125102997, -0.10728433728218079, -0.3980916738510132, 0.6840100884437561, 0.8321384787559509, 0.4900299608707428, 0.13627596199512482, -0.05958627909421921, -0.1965187042951584, 0.7125253081321716, -0.12872904539108276, -0.6919219493865967, -0.3835010528564453, 1.4054572582244873, 1.5396080017089844, -0.6696586012840271, 0.06819327175617218, -0.16657450795173645, -0.49589264392852783, 0.7218809723854065, 0.6572188138961792, -0.13876064121723175, 0.8019927740097046, -0.2206730842590332, 0.2928290367126465, 0.08374739438295364, -1.0708674192428589, -0.35692405700683594, 1.0822781324386597, 1.114364743232727, 0.8277310132980347, -0.10765016078948975, 0.1616528481245041, 0.45828866958618164, -0.02109367959201336, 0.21674397587776184, 0.4804632067680359, 0.013697566464543343, -0.49043136835098267, 0.3060535788536072, -0.05721985921263695, 0.5539032816886902, -0.703536331653595, -0.60295569896698, 0.134988471865654, 0.5284115672111511, -0.08225824683904648, 0.41474103927612305, 1.1798312664031982, 0.13730531930923462, 0.6534834504127502, 0.1998751014471054, 0.5178543925285339, -0.22598780691623688, 0.3473251461982727, 0.041618507355451584, -1.0691502094268799, -0.032496895641088486, -0.45043838024139404, -0.6497622728347778, 0.18238689005374908, 0.16238635778427124, 0.12217742949724197, -0.10604550689458847, 0.23726992309093475, 0.8110275864601135, 0.5677815079689026, 0.6076003909111023, -0.5998168587684631, -0.37060537934303284, -0.27762484550476074, -1.279910683631897, 0.2026778608560562, -0.6534498333930969, -0.21308891475200653, -0.23004522919654846, -0.12998048961162567, -0.2689017057418823]}, "authors": [{"authorId": "51498210", "name": "Yury Nahshan"}, {"authorId": "8176957", "name": "Dor-Joseph Kampeas"}, {"authorId": "6536444", "name": "E. Haleva"}], "references": [{"paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425", "title": "Toeplitz Neural Network for Sequence Modeling"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "4b0541eccd8f98852d6807a14fbac17f775c7b40", "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m Method"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "edbb8719af7e9bff523bf4f6f19dc0394a3c2358", "title": "Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines"}, {"paperId": "fb987ebe5ff5276fbbe6a5c5b16b6bfd759afa37", "title": "KVT: k-NN Attention for Boosting Vision Transformers"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "e3b75f2b044bc78664480c3127b3eecb1d1b4ccc", "title": "The Perron\u2013Frobenius Theorem"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "203b543bfa1e564bb80ff4229b43174d7c71b0c0", "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "41c987b8a7e916d56fed2ea7311397e0f2286f3b", "title": "ACIQ: Analytical Clipping for Integer Quantization of neural networks"}, {"paperId": "bb669de2fce407df2f5cb2f8c51dedee3f467e04", "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "title": "An Analysis of Neural Language Modeling at Multiple Scales"}, {"paperId": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0", "title": "Deep Neural Networks as Gaussian Processes"}, {"paperId": "b5799525379e505234d727b01f8005f69a4d252d", "title": "What does Attention in Neural Machine Translation Pay Attention to?"}, {"paperId": "2bba018cbdc4ceafb9c3326bd8d463f589baa8b8", "title": "The spectral gap of sparse random digraphs"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "e11b2403a0b8ad3b4a747ce6fc915a8aee04eb6b", "title": "Principal Component Analysis"}, {"paperId": "b57075c4f5c31152fc7002383dc6352659165693", "title": "Broad distribution effects in sums of lognormal random variables"}, {"paperId": "d6e6c3243e9e4e6dd8f0bc783d7612b7d3863d5f", "title": "Matrix Analysis and Applied Linear Algebra"}, {"paperId": "f9262396b2aaf7240ac328911e5ff1e46ebbf3da", "title": "On the Exact Variance of Products"}, {"paperId": "4eb86d25c674522aced50462382c8b735f40f188", "title": "The Sum of Log-Normal Probability Distributions in Scatter Transmission Systems"}, {"paperId": "b48694cb275eba60b48026f3159373c92c1b286c", "title": "Functions of Positive and Negative Type, and their Connection with the Theory of Integral Equations"}, {"paperId": "927d707786ac1e19cb421aed6f5b0603b9dadc88", "title": "Robustify Transformers with Robust Kernel Density Estimation"}, {"paperId": "5665805becad6c87b194b260f2270d86d560bd3f", "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models"}, {"paperId": null, "title": "Normal product distribution"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "107d1cc9216776d4083932e2c85ca9d75b3515c2", "title": "Markov chains and mixing times"}, {"paperId": "05175204318c3c01e3301fd864553071039605d2", "title": "On Estimating Regression"}, {"paperId": null, "title": "Neural machine translation by jointly learning to align and translate, January 2015"}, {"paperId": null, "title": "The devil in linear transformer, 2022b"}, {"paperId": null, "title": "cosformer: Rethinking softmax"}, {"paperId": null, "title": "Sparse sinkhorn attention, 2020b"}, {"paperId": null, "title": "Linear Log-Normal Attention with Unbiased Concentration A P REPRINT"}]}