{"paperId": "9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73", "abstract": "Attention based language models have become a critical component in state-of-the-art natural language processing systems. However, these models have significant computational requirements, due to long training times, dense operations and large parameter count. In this work we demonstrate a set of modifications to the structure of a Transformer layer, producing a more efficient architecture. First, we add a convolutional module to complement the self-attention module, decoupling the learning of local and global interactions. Secondly, we rely on grouped transformations to reduce the computational cost of dense feed-forward layers and convolutions, while preserving the expressivity of the model. We apply the resulting architecture to language representation learning and demonstrate its superior performance compared to BERT models of different scales. We further highlight its improved efficiency, both in terms of floating-point operations (FLOPs) and time-to-train.", "venue": "arXiv.org", "year": 2021, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work demonstrates a set of modifications to the structure of a Transformer layer, producing a more efficient architecture, and applies the resulting architecture to language representation learning and demonstrates its superior performance compared to BERT models of different scales."}, "embedding": {"model": "specter_v2", "vector": [0.22002409398555756, 0.7594987750053406, -0.16204677522182465, -0.31307414174079895, -0.18734286725521088, -0.12230858951807022, 0.8077305555343628, -0.17205806076526642, -0.6380254030227661, -0.4343072175979614, 0.7070603370666504, 0.2327760010957718, 0.31895679235458374, -0.029158715158700943, 0.00041422623326070607, 0.169583261013031, -0.8017904758453369, 0.36519506573677063, 0.04278501495718956, -0.6596716642379761, -0.36891067028045654, -0.5487551689147949, -0.6411845088005066, 0.18017707765102386, 0.07085778564214706, 0.4918191134929657, 0.4757108688354492, 0.712934136390686, -0.2715424597263336, 0.7821179628372192, 0.7511998414993286, -0.2263399213552475, 0.2057214081287384, -0.10481303930282593, -0.21790871024131775, -0.3095608949661255, 0.585964024066925, -0.19577237963676453, -0.4911454916000366, 0.8306944966316223, -0.3767105042934418, 0.7367827892303467, 0.2515612542629242, -0.42041516304016113, -0.2451491802930832, 1.1337857246398926, 0.8120163679122925, 0.7770131826400757, -0.5918388962745667, -0.6870585083961487, 1.4429231882095337, -1.6454757452011108, 0.13931842148303986, 1.6237870454788208, 0.37542444467544556, 0.37924379110336304, -0.20080113410949707, -0.7126681208610535, 0.7859537601470947, 0.056708358228206635, -0.9788174629211426, -0.43654292821884155, 0.1567768156528473, 0.05226411297917366, 2.0436532497406006, -0.49151313304901123, 0.038791146129369736, 0.1346561312675476, 0.016487376764416695, 0.9938825368881226, -0.08053872734308243, -0.8806811571121216, -0.4714915156364441, -0.058787137269973755, 0.4641554057598114, 1.076877236366272, -0.42632928490638733, 0.2809641361236572, -0.9005703330039978, -0.025595102459192276, 0.456952303647995, -0.06336627900600433, 0.1309448778629303, -0.26535019278526306, -0.28120502829551697, 0.8416607975959778, 0.6533421277999878, 1.1583842039108276, -0.33554187417030334, 1.0143555402755737, 0.3854270577430725, 0.5129794478416443, 0.14712585508823395, 0.4702867865562439, 0.19270583987236023, 0.817133367061615, -0.7023771405220032, 0.35594838857650757, -0.21440647542476654, 0.8546628355979919, 0.38869622349739075, 0.5541597604751587, -0.5524037480354309, 0.2736123502254486, 1.510005235671997, 0.18098610639572144, 0.27064546942710876, -0.3543793261051178, 0.08794138580560684, -0.7214164137840271, -0.3955884873867035, -0.7225804924964905, -0.0959654450416565, -0.4361058473587036, -0.667842447757721, -1.331032633781433, -0.492494136095047, 0.47711053490638733, -0.8093432188034058, 0.8964989185333252, -0.796837329864502, -0.10528957098722458, 0.044983524829149246, -0.10565799474716187, 0.5686309337615967, 0.41246044635772705, 0.8772037029266357, 0.5266003012657166, 1.1916959285736084, -0.8091632127761841, -0.7829928994178772, -1.1870307922363281, 0.4591498374938965, -0.11667212098836899, 0.11919307708740234, -0.56729656457901, -1.1685680150985718, -1.1135426759719849, -0.49306240677833557, -0.10405842959880829, -0.8062383532524109, 0.10043632239103317, 0.6995986700057983, 0.35579952597618103, -1.4616776704788208, 0.6373156309127808, -0.28273463249206543, -0.49772909283638, 0.41405782103538513, 0.5737377405166626, 0.1945071667432785, -0.07543759047985077, -1.2223819494247437, 0.5151410698890686, 0.3825526237487793, -0.5240503549575806, -0.09650830924510956, -0.6299231052398682, -1.3629908561706543, 0.08229895681142807, 0.09807076305150986, -0.587846040725708, 1.1744027137756348, -0.2153451144695282, -1.0323337316513062, 0.8693860173225403, -0.509208083152771, -0.32480013370513916, -0.2906734347343445, -0.2109280675649643, -0.4099803864955902, -0.46634364128112793, -0.28832295536994934, 0.6395482420921326, 0.480744868516922, 0.004219754599034786, 0.05238647013902664, -0.04797890782356262, -0.4904194474220276, -0.011636294424533844, -0.4706375002861023, 1.339471697807312, -0.297672301530838, -0.4296844005584717, 0.3243560791015625, 0.6458398103713989, -0.03911485895514488, -0.4868965446949005, -0.18205510079860687, -1.2716208696365356, 0.8447129130363464, 0.008964210748672485, 0.8227136731147766, -0.9337297677993774, -0.8642855882644653, -0.060699447989463806, 0.10968510061502457, -0.16715772449970245, -0.7173655033111572, 0.8205307722091675, -0.8277522921562195, 0.3488491475582123, 0.08249235153198242, -1.095025897026062, 0.05230957269668579, -0.12685473263263702, -0.752558171749115, -0.2336631417274475, 0.11620929092168808, 1.4105284214019775, -1.094285488128662, -0.03367684409022331, 0.09430503100156784, 0.17333438992500305, -1.118510365486145, 1.4805363416671753, -0.1091281995177269, 0.0067300437949597836, 0.02531971037387848, -0.5060827136039734, -0.07366806268692017, 0.07449852675199509, 0.2607414722442627, -0.6907476782798767, -0.5394514203071594, 0.6472785472869873, 0.3097594678401947, 1.1695209741592407, -0.46325284242630005, 0.4113210141658783, -0.03538103029131889, -0.5485213398933411, 0.49277493357658386, 0.37563586235046387, -0.27348262071609497, -0.5395848155021667, 0.5197278261184692, 0.3007482588291168, -0.3697512447834015, 0.4651861786842346, 0.6852931976318359, 0.8486032485961914, -0.38668936491012573, 0.10180753469467163, 0.8854275941848755, -0.3412119150161743, 0.41081148386001587, 0.5951678156852722, 0.6214112639427185, 0.2613649368286133, 0.4863622784614563, -0.5906126499176025, 0.3810705840587616, -0.8882696032524109, -0.38711705803871155, 0.4986785352230072, 0.7652578353881836, 0.8295989036560059, 0.18280015885829926, -0.6262153387069702, -0.29344865679740906, -0.06965335458517075, 1.1510049104690552, 1.6058629751205444, -0.4411132037639618, -0.10505359619855881, -0.5397359132766724, -0.03333953768014908, -0.7328975796699524, 0.05479239299893379, -0.33722954988479614, -0.33596789836883545, -0.8887502551078796, -0.6725919246673584, 0.8405395746231079, 0.2670842707157135, 1.1500113010406494, -0.7193038463592529, -0.11810602247714996, -0.2778121829032898, 0.5643962025642395, -0.8787348866462708, -0.9019889831542969, 0.6597713828086853, -0.351792573928833, -0.1642007976770401, 0.23036164045333862, -0.5216252207756042, 0.23017564415931702, -0.637867271900177, 1.0321787595748901, -0.7343763709068298, -0.11847102642059326, 0.15656331181526184, 0.585837185382843, -0.8305498361587524, -0.950677752494812, 0.24482348561286926, -0.01594080962240696, 0.04910608381032944, 0.4447464048862457, 0.5892089009284973, 0.2840381860733032, -0.15392298996448517, -0.23144640028476715, 0.12891243398189545, -0.09574781358242035, 0.1649053990840912, 0.35343149304389954, -0.49666163325309753, -0.1004967913031578, -1.2325667142868042, 0.8608211874961853, 0.455119788646698, -0.340831458568573, -0.07045486569404602, -0.12424711883068085, -0.25383126735687256, 0.520160973072052, -0.12628228962421417, -0.26873520016670227, -0.6182728409767151, 0.25361672043800354, -0.4252610206604004, -0.246403306722641, 0.1944493055343628, 0.052058134227991104, 0.09500669687986374, 0.16815084218978882, 0.40247786045074463, 0.6798936724662781, -0.05942261964082718, 0.40523838996887207, -1.0787553787231445, 0.608439564704895, 0.3179718554019928, 0.43790125846862793, -0.08157818764448166, -0.13718199729919434, -0.7750954031944275, -0.3191602826118469, -0.5887296795845032, -0.22810453176498413, -0.11180193722248077, 0.29625213146209717, -0.7804358601570129, -0.6551497578620911, 0.09356239438056946, -1.017802357673645, -0.28597018122673035, -0.0016268229810521007, -0.1531827300786972, -0.09068310260772705, -1.066402792930603, -1.0175360441207886, -0.44815993309020996, -0.8267006278038025, -0.4650656580924988, 0.11811012029647827, -0.09455136209726334, -0.326578289270401, -0.8978796005249023, -0.1200709268450737, -0.6729962825775146, 1.3466606140136719, -0.5313236117362976, 0.9914274215698242, -0.42117640376091003, -0.3527069687843323, -0.23921021819114685, 0.09642619639635086, 0.8133764863014221, 0.11844070255756378, -0.12031848728656769, -0.7674818634986877, 0.22819165885448456, -0.315580278635025, 0.1175333708524704, 0.30149057507514954, 0.14323483407497406, 0.5813366174697876, -0.07821828871965408, -0.24842225015163422, 0.36370256543159485, 1.2158088684082031, -0.3849109709262848, 0.2250729352235794, 0.1223783865571022, 1.025561809539795, 0.2585090696811676, -0.46500492095947266, 0.0204242542386055, 0.5862147212028503, 0.5982843637466431, 0.46711301803588867, -0.42814287543296814, -0.4779805541038513, -0.5529723763465881, 0.425106406211853, 1.5006123781204224, 0.10811280459165573, -0.17732873558998108, -1.0320154428482056, 0.7484291791915894, -0.993363082408905, -0.7502464652061462, 0.5640723705291748, 0.5729293823242188, 0.011213221587240696, -0.8170273303985596, -0.41361644864082336, -0.39167171716690063, 0.6471459269523621, 0.4862033426761627, 0.16062049567699432, -0.85252445936203, -0.13554219901561737, 0.7313745021820068, 0.317533016204834, 0.7856209874153137, -0.5647193193435669, 0.9696329236030579, 14.811052322387695, 0.3468652367591858, -0.0864291712641716, 0.5278584957122803, 0.44449660181999207, 0.4720933139324188, -0.40508630871772766, -0.1381610631942749, -1.0334001779556274, -0.4655294418334961, 0.8758520483970642, -0.3460157513618469, 0.8307960629463196, 0.013754518702626228, 0.3258991539478302, 0.5077295303344727, -0.44069984555244446, 0.6610618829727173, 0.7848319411277771, -0.9189015626907349, 0.4765858054161072, 0.1811305582523346, 0.015739355236291885, 0.6408531069755554, 0.5880776643753052, 0.8054947257041931, 0.5138939619064331, -0.5564206838607788, 0.33807143568992615, 0.2365628182888031, 0.48124223947525024, -0.030792489647865295, 0.2017572671175003, 0.3717850148677826, -1.18911874294281, -0.13731078803539276, -0.6930288076400757, -1.2142716646194458, -0.03891106694936752, -0.16180667281150818, -0.6940954327583313, -0.31931421160697937, -0.20763283967971802, 1.0738062858581543, -0.15336483716964722, 0.21387942135334015, -0.30701425671577454, 0.5673689842224121, -0.0932568907737732, -0.17756129801273346, 0.2957353889942169, 0.5119186043739319, 0.25566887855529785, 0.025415964424610138, 0.11717749387025833, 0.011640344746410847, 0.02194439247250557, 0.6147067546844482, -0.5323632955551147, 0.09220187366008759, -0.5474067330360413, -0.23262539505958557, -0.318346232175827, 0.9619155526161194, 0.5664163827896118, 0.2344779372215271, -0.4188734292984009, -0.01677241176366806, 0.6820567846298218, 0.2521423101425171, -0.19401246309280396, -0.07674791663885117, 0.05430152639746666, 0.009439431130886078, 0.09122510254383087, 0.2873716950416565, 0.04381750896573067, -0.32251816987991333, -1.2497566938400269, -0.06247596815228462, 0.44687432050704956, -0.8315489888191223, -0.5615842342376709, 1.386574387550354, 0.08205274492502213, -0.40850913524627686, 0.12149088829755783, -0.8630814552307129, -0.4206973910331726, 0.6935104727745056, -1.2412185668945312, -0.9440838098526001, -0.10247886180877686, -0.04702240601181984, -0.49281033873558044, -0.35698190331459045, 1.3513163328170776, 0.148372620344162, -0.10176275670528412, 0.08741174638271332, -0.5529638528823853, 0.12175506353378296, -0.3601614534854889, -0.6521756649017334, 0.8940225839614868, 0.2879221439361572, 0.053246866911649704, 0.507036566734314, -0.09948110580444336, 0.40043094754219055, -0.6378495097160339, -0.41151511669158936, 1.241411805152893, -0.6811636686325073, -0.021848278120160103, -1.0550191402435303, -0.769036591053009, 0.31606829166412354, 0.8372445106506348, -0.15687981247901917, 0.2291526347398758, 0.008223770186305046, -0.6544245481491089, -0.22252598404884338, -0.6883090734481812, -0.12171001732349396, 0.49716857075691223, -0.9783655405044556, -0.5336714386940002, -0.25872060656547546, 0.19049720466136932, -0.7747870683670044, -0.1658749133348465, -0.19775909185409546, -0.06623857468366623, 0.011716548353433609, 0.8749226331710815, -0.3483363687992096, 0.3191559612751007, 0.679194450378418, -0.16959944367408752, -0.904492974281311, -0.3926509916782379, -0.745795726776123, 0.18344956636428833, 0.07801678031682968, 0.3907011151313782, -0.6777303218841553, 0.2905881702899933, 0.6538415551185608, -0.2689008116722107, -0.25852683186531067, -0.47023478150367737, -0.3395134508609772, -0.1525188386440277, -0.3112295866012573, 0.236540287733078, -0.2114306390285492, -0.22687800228595734, 0.3470871150493622, 0.6247854232788086, 0.39867836236953735, -0.16401247680187225, -0.9416067600250244, 0.18679045140743256, -0.09124663472175598, 0.11515013873577118, -0.5899730920791626, -0.730535089969635, -1.1487611532211304, 0.18354083597660065, -1.3405693769454956, -0.05600643530488014, -1.107812762260437, -0.5402781963348389, 0.08337780833244324, -0.3765873908996582, 0.6984808444976807, 0.38657569885253906, -0.05315646529197693, -0.2113683819770813, -0.5294356942176819, -0.4460236430168152, 0.8926236629486084, 0.6599278450012207, -0.7026621699333191, 0.10227281600236893, -0.3466654419898987, 0.03838330879807472, 0.34338870644569397, 0.48909521102905273, -0.2923855781555176, -0.7835709452629089, -1.4264376163482666, 0.20162150263786316, -0.5499627590179443, 0.23926451802253723, -0.9061794877052307, 0.8535586595535278, 0.6833984851837158, -0.3995615243911743, -0.16763825714588165, 0.3460873067378998, -0.7646079063415527, -0.5309984683990479, 0.48095789551734924, -0.7143297791481018, 0.41618072986602783, 0.35342922806739807, -0.8474907279014587, -0.6557883620262146, 0.9916592240333557, 0.08555112034082413, -1.2421540021896362, -0.8720707297325134, 0.6509321331977844, -0.7242298126220703, 0.041019294410943985, -0.15680237114429474, -0.18692044913768768, -0.9478144645690918, -0.4134041666984558, -0.03240704908967018, 0.2692354619503021, -0.5271896719932556, 1.0801255702972412, 0.44760507345199585, -1.0038145780563354, 0.03359457850456238, 0.6201133131980896, -0.1012619212269783, -0.1873006820678711, 0.2487068474292755, 0.30609866976737976, -0.2993842363357544, 0.7322705984115601, 0.2409180998802185, 0.4105113446712494, -1.1489794254302979, -0.07495928555727005, 0.9571076035499573, -0.5427323579788208, -0.13885323703289032, 1.1871130466461182, 0.05477968230843544, -1.1656665802001953, -0.23458647727966309, -1.1727163791656494, -0.7137225270271301, -0.2092982679605484, 0.6088051199913025, -0.05093618482351303, 0.24030154943466187, -0.3755526840686798, -0.46866926550865173, 0.282291442155838, -0.14720919728279114, -0.34969601035118103, 0.5309728980064392, 0.03435687720775604, -0.8311477899551392, 0.49928200244903564, 0.8125200867652893, -0.6443595886230469, -0.2636457085609436, -0.7608193755149841, -0.5774784684181213, 0.2590607702732086, 0.39946040511131287, -0.048585448414087296, -0.7409189343452454, 0.6837788820266724, 0.38309141993522644, 0.4516621530056, 0.16987992823123932, -0.15594689548015594, -0.25690120458602905, 0.6728670001029968, 0.23720869421958923, -0.41876551508903503, -0.5617685317993164, 1.9397368431091309, 1.5177873373031616, -0.3038442134857178, 0.01024586707353592, -0.7138633131980896, -0.5487768054008484, 0.8851736187934875, 0.11848360300064087, -0.4486848711967468, 1.164845585823059, -0.12956739962100983, 0.3618357181549072, -0.022712819278240204, -1.1220157146453857, -0.4351048171520233, 0.7585550546646118, 1.068651795387268, 0.942660391330719, 0.1944779008626938, 0.12636946141719818, 0.7852287292480469, 0.032943446189165115, -0.15543262660503387, 0.19190502166748047, 0.40797847509384155, -0.07174551486968994, -0.16817116737365723, 0.02121109887957573, 0.46169304847717285, -0.385789692401886, -0.897908091545105, 0.009746013209223747, 0.8213387131690979, 0.09511734545230865, 0.5135126113891602, 0.9724876284599304, 0.15125077962875366, 0.8520880937576294, 0.2201736569404602, 0.4661730229854584, -1.0357722043991089, -0.5604960322380066, -0.3039074242115021, -0.5250386595726013, -0.01980600878596306, -0.1266293078660965, -0.39427369832992554, -0.6584276556968689, 0.1978200376033783, 0.37605494260787964, -0.011134604923427105, 0.40014350414276123, 0.8839126825332642, 0.38490450382232666, 0.8141726851463318, -0.17203304171562195, -0.2970508337020874, -0.10248725116252899, -1.1243104934692383, 0.03039049170911312, -0.5581883192062378, -0.24184487760066986, -0.12449966371059418, -0.1129051223397255, -0.24141792953014374]}, "authors": [{"authorId": "66190473", "name": "Ivan Chelombiev"}, {"authorId": "39145648", "name": "Daniel Justus"}, {"authorId": "145474032", "name": "Douglas Orr"}, {"authorId": "2057429281", "name": "A. Dietrich"}, {"authorId": "10682156", "name": "Frithjof Gressmann"}, {"authorId": "2097479", "name": "A. Koliousis"}, {"authorId": "49147045", "name": "C. Luschi"}], "references": [{"paperId": "07f0112a00059ba09864a044e1f9bffb22df3867", "title": "Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training"}, {"paperId": "eb5fe6a806c30a9eae3f5430c6704780b230bdb7", "title": "Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1", "title": "A Practical Survey on Faster and Lighter Transformers"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "2a218786f4615b82389f78472e7ff22e6ce57490", "title": "ConvBERT: Improving BERT with Span-based Dynamic Convolution"}, {"paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69", "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"}, {"paperId": "056935031bc5cf0aeeaa0946320de26e14a1817e", "title": "Revisiting Few-sample BERT Fine-tuning"}, {"paperId": "8b9d77d5e52a70af37451d3db3d32781b83ea054", "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "d808dbdd6a53259bea908266397b3983f246aabc", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking"}, {"paperId": "841d43cf4015042a4ee45745c5b6f2c59c184da5", "title": "DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling"}, {"paperId": "2e3002f131e1815bda7a10303eff97f79dea01ec", "title": "Rigging the Lottery: Making All Tickets Winners"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "title": "Learning Deep Transformer Models for Machine Translation"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "f552d03c45298e919d7b4fc1a5b72a9539499a88", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "b08c360ddf899923aebf25913706b4f03e54eccd", "title": "DeLighT: Deep and Light-weight Transformer"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "4e-4}. Each model required a sweep to identify the best candidate. The sweep for the baseline BERT models was performed according to range of hyperparamters"}, {"paperId": null, "title": "The WikiText long term dependency language modeling dataset"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Figure 7: BERT Base attention maps, for all layers L 0 \u2212L 11"}]}