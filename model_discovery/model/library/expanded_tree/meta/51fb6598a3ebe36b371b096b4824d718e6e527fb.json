{"paperId": "51fb6598a3ebe36b371b096b4824d718e6e527fb", "abstract": "The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.", "venue": "arXiv.org", "year": 2023, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This survey examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques."}, "embedding": {"model": "specter_v2", "vector": [0.11840181797742844, 0.3149154484272003, -0.5347583293914795, -0.2067396342754364, -0.5169996023178101, -0.046829983592033386, 0.2813373804092407, -0.002347922185435891, -0.8214392066001892, -0.289183109998703, 0.17361421883106232, -0.10023462027311325, 0.10448925942182541, 0.19731076061725616, -0.24700435996055603, 0.16850924491882324, -0.8798871040344238, 0.7592530846595764, -0.24686364829540253, -0.11868211627006531, -0.3049333393573761, -0.30395185947418213, -1.3242906332015991, -0.016708750277757645, 0.37304699420928955, 0.5411304235458374, 0.0870356336236, 1.1992741823196411, -0.5567060112953186, 0.24796509742736816, 0.5653021335601807, -0.39683419466018677, 0.16967594623565674, -0.03668434917926788, -0.14210999011993408, -0.1494460254907608, 0.2633620798587799, -0.4884515404701233, -0.8122870922088623, 0.6413885354995728, -0.06982720643281937, 0.4662248194217682, 0.33375105261802673, -0.5284384489059448, -0.1464328169822693, 0.6371412873268127, 0.49076345562934875, 0.8826722502708435, -0.5069465637207031, -0.5571943521499634, 1.187880516052246, -1.3647903203964233, 0.07677412033081055, 1.594303011894226, 0.6638376116752625, 0.2031114101409912, -0.47905683517456055, -0.6095677614212036, 0.37171414494514465, -0.3374328911304474, -1.1267755031585693, -0.7673287391662598, -0.4111923575401306, 0.11182517558336258, 2.1013858318328857, 0.02828368730843067, -0.20396780967712402, 0.1978665143251419, -0.13006894290447235, 1.4827033281326294, -0.11094395816326141, -0.9644258618354797, 0.17937587201595306, 0.017922788858413696, 0.23468080163002014, 1.0228902101516724, -0.26949915289878845, 0.4045434594154358, -1.1339895725250244, -0.4650072753429413, 0.0324583537876606, -0.48065558075904846, 0.25616854429244995, -0.225410595536232, 0.35286465287208557, 1.057801604270935, 0.01238048542290926, 0.5004090666770935, 0.04314814880490303, 0.7418380975723267, 0.5393986701965332, 0.40198391675949097, 0.5370609164237976, 0.2969972491264343, -0.2418355494737625, 0.016417192295193672, -1.22078537940979, 0.39810895919799805, 0.27185410261154175, 1.0569348335266113, -0.46660998463630676, 0.30222591757774353, -0.6389737129211426, 0.7552623748779297, 1.135015606880188, 0.2407478392124176, 0.7056500315666199, -0.6191587448120117, 0.41935843229293823, -0.681606650352478, -0.1159682422876358, -0.32792606949806213, -0.38421523571014404, -0.6877315044403076, -0.9815200567245483, -1.2471238374710083, -0.5890231728553772, 0.10419365018606186, -0.7075684666633606, 0.5855407118797302, -0.3552325665950775, 0.05941016972064972, 0.12390604615211487, 0.3061884045600891, 0.2749105393886566, 0.7291408777236938, 0.2792268395423889, -0.17298266291618347, 0.8965354561805725, -0.7847888469696045, -0.5144458413124084, -1.5535269975662231, 1.060355305671692, -0.1979324221611023, 0.13697703182697296, -0.44309934973716736, -1.5824848413467407, -0.7629013061523438, -0.5688616037368774, -0.18840058147907257, -0.3580797016620636, 0.6704031229019165, 1.3474352359771729, 0.3248600363731384, -1.0572024583816528, 0.4350723326206207, -0.29474443197250366, -0.0019559618085622787, 0.4689914882183075, 0.4025067985057831, 0.2971794605255127, -0.4743594527244568, -0.9142759442329407, 0.13266824185848236, 0.33380958437919617, -0.7820936441421509, 0.07618911564350128, -0.20655685663223267, -1.0764700174331665, 0.11455333977937698, 0.0526965968310833, -0.6431204080581665, 1.23337721824646, -0.028924068436026573, -1.147258996963501, 0.6466864347457886, -0.49106013774871826, -0.31279370188713074, 0.3518717885017395, 0.04173002019524574, -0.4668842554092407, -0.5462477803230286, -0.6153989434242249, 0.5716497302055359, 0.42655348777770996, -0.08566148579120636, -0.31753697991371155, 0.2218460738658905, -0.2161538004875183, 0.09291407465934753, -0.2761879563331604, 0.9487268924713135, -0.9432474970817566, -0.4585275948047638, 0.6636199951171875, 0.32027164101600647, -0.558125376701355, -0.07535409182310104, -0.18829025328159332, -0.6926959156990051, 0.33961382508277893, -0.2335907369852066, 1.404005765914917, -0.9303923845291138, -0.7019549012184143, 0.0400501973927021, -0.07495807856321335, -0.019664227962493896, -0.7954592704772949, 0.6511092185974121, -0.05586874857544899, 0.3245798349380493, -0.5921517014503479, -1.3774514198303223, 0.18637779355049133, -0.005251001566648483, -0.551949679851532, -0.21052764356136322, -0.050194017589092255, 0.7148835062980652, -0.6969515681266785, 0.05227268859744072, -0.1368471384048462, 0.3088744580745697, -1.3393884897232056, 1.2915009260177612, -0.6438292860984802, -0.29323917627334595, 0.2590997517108917, -0.31804096698760986, 0.39102500677108765, -0.4164803624153137, 0.7096997499465942, -0.00748496362939477, -0.11579592525959015, 0.29380568861961365, -0.44475749135017395, 1.4221720695495605, -0.44017699360847473, 0.3481232821941376, -0.14260420203208923, -0.2759930491447449, -0.17411814630031586, 0.32834744453430176, -0.4028981328010559, -0.3961670398712158, 0.5100670456886292, 0.6361628770828247, -0.4977172613143921, 0.49859750270843506, 0.9591388702392578, 0.7555631995201111, -0.21996188163757324, 0.4602447748184204, 0.6335018873214722, -0.25931957364082336, 1.0394881963729858, 0.4764474928379059, 0.48632529377937317, 0.07893062382936478, 0.4980317950248718, -0.0007961741066537797, 0.3749007284641266, -1.0295538902282715, -0.39834874868392944, 0.7426867485046387, 0.9894084930419922, 0.5496273040771484, -0.028687918558716774, -0.6520505547523499, -0.42251625657081604, 0.34882304072380066, 0.569616436958313, 1.7211999893188477, -0.3290354311466217, -0.3481675982475281, -0.9793505072593689, -0.1212313175201416, -0.16388756036758423, 0.17155753076076508, -0.13013304769992828, 0.04904999956488609, -0.6039311289787292, -1.337614893913269, 0.9281724691390991, 0.3303801119327545, 0.5675846934318542, -0.24433013796806335, -0.3525797128677368, -0.3797648549079895, 0.3191741704940796, -0.8258100748062134, -0.6214041113853455, 0.6962053179740906, -0.992199718952179, 0.04255146533250809, 0.2346397340297699, -0.032887812703847885, 0.3009621202945709, -0.5353304743766785, 0.9176161289215088, -0.13416171073913574, -0.19403058290481567, -0.2982253432273865, 0.7473528981208801, -0.6565005779266357, -1.1788084506988525, 0.3726765215396881, 0.26973140239715576, -0.4066476821899414, 0.3472262918949127, 0.5259901285171509, 0.5049359798431396, -0.2076248675584793, -0.5406175851821899, 0.31971466541290283, 0.30060362815856934, 0.10611575841903687, 0.9274711608886719, -0.19917453825473785, -0.26720553636550903, -1.179341435432434, 1.1358742713928223, 0.17269845306873322, -0.7043511271476746, 0.43436282873153687, -0.639876663684845, 0.1812116652727127, 0.7279565930366516, -0.6263211965560913, -0.1561182588338852, -0.5992771983146667, -0.041417643427848816, -0.2911212146282196, -0.2244272232055664, 0.13248082995414734, 0.6968688368797302, 0.0244445838034153, 0.2557613253593445, 0.7651623487472534, 0.5287290215492249, -0.5627959370613098, 0.5535498261451721, -0.5078323483467102, 0.37466228008270264, 0.2452298402786255, 0.10053667426109314, -0.36676648259162903, -0.33728888630867004, -0.5078319907188416, -0.294109970331192, -0.34615686535835266, -0.12762227654457092, -0.004364783875644207, -0.2541768550872803, -0.5397176742553711, -0.46270856261253357, -0.25479385256767273, -0.915450930595398, 0.008344028145074844, 0.39831259846687317, -0.10396495461463928, -0.23521560430526733, -0.9746145009994507, -1.6345196962356567, -0.9781230688095093, -0.8279144167900085, -1.1311157941818237, 0.547042965888977, -0.1503777951002121, -0.345743328332901, -0.47263815999031067, -0.16555263102054596, -0.2617754638195038, 1.0105959177017212, -1.0896726846694946, 1.1436378955841064, -0.16797910630702972, -0.021636679768562317, -0.4037318527698517, 0.25157544016838074, 0.1750056892633438, -0.7148788571357727, 0.2720392942428589, -1.2176899909973145, -0.2035960555076599, -0.527345597743988, -0.2772003412246704, 0.2229863554239273, 0.5000754594802856, 1.0209983587265015, -0.23698346316814423, -0.5663310289382935, 0.5982865691184998, 1.248827576637268, -0.6631573438644409, -0.3461480438709259, -0.21440178155899048, 0.5036250352859497, -0.1913450062274933, -0.5169935822486877, 0.8418421745300293, -0.1259264498949051, 0.6002823710441589, -0.057365089654922485, -0.1526147574186325, 0.03978370130062103, -0.45674723386764526, 0.22270503640174866, 2.027263879776001, 0.5278710126876831, -0.5006008744239807, -0.6973071098327637, 0.14503638446331024, -1.1644420623779297, -0.6015651226043701, 0.6718745827674866, 0.9014726281166077, 0.4346180856227875, -0.057526327669620514, -0.0641668438911438, 0.05848459154367447, 0.11746038496494293, 0.4035051465034485, -0.4167661964893341, -0.7852317094802856, -0.19689461588859558, 0.4789065718650818, 0.4550262689590454, 0.48062995076179504, -0.4663580060005188, 0.40597522258758545, 14.7566556930542, 1.3653435707092285, 0.23121440410614014, 0.6498141884803772, 0.9053840637207031, 0.11853504925966263, -0.3144608736038208, -0.5263576507568359, -1.135727047920227, -0.08810428529977798, 1.4972660541534424, -0.033875927329063416, 1.1270315647125244, 0.32318753004074097, -0.057034581899642944, 0.22894158959388733, -0.3834840953350067, 0.9048440456390381, 0.30710387229919434, -1.5259013175964355, 0.525291383266449, 0.04426082223653793, 0.21487335860729218, 0.67807537317276, 0.5243459343910217, 0.9501438140869141, 0.523772656917572, -0.5740017294883728, 0.7518008351325989, 0.08941882848739624, 1.0352343320846558, -0.20595042407512665, 0.4880639910697937, 0.9726654887199402, -1.0307780504226685, -0.2125125229358673, -0.6772236824035645, -1.164205551147461, 0.10239887982606888, 0.03816693276166916, -0.6372782588005066, -0.40231361985206604, -0.5811945796012878, 0.39877012372016907, -0.14773325622081757, 0.41709473729133606, 0.3239560127258301, 0.7687366008758545, -0.6221781969070435, 0.0992942526936531, 0.17259101569652557, 0.2200622260570526, -0.10281812399625778, 0.13270966708660126, 0.207641139626503, -0.06653344631195068, 0.07680604606866837, 0.19603540003299713, -0.6617373824119568, 0.21022821962833405, -0.3747326135635376, -0.47237303853034973, 0.2020847052335739, 0.38367605209350586, 0.3767482340335846, 0.27337685227394104, -0.3668115437030792, 0.24698886275291443, 1.038173794746399, 0.08449025452136993, -0.3135305345058441, 0.4337289333343506, 0.337942510843277, -0.7649103403091431, -0.1175088882446289, 0.7512850761413574, -0.25829195976257324, -0.463264137506485, -0.626678466796875, -0.7261808514595032, 0.6301752924919128, -0.42072775959968567, -0.767910361289978, 0.9243573546409607, -0.16674435138702393, -0.25210973620414734, 0.2587727904319763, -0.5463606715202332, 0.3084771931171417, 0.4373517334461212, -1.0152757167816162, -0.41863781213760376, 0.7381476759910583, -0.725223183631897, -0.04233226552605629, -0.15625575184822083, 1.7253016233444214, 0.31716790795326233, -0.5093303322792053, 0.5537772178649902, 0.5959835648536682, -0.303528755903244, -0.827900230884552, -0.22157998383045197, 1.0868268013000488, 0.5130437016487122, 0.1447487771511078, 0.5924519896507263, -0.0016618656227365136, 0.4471999704837799, -0.9372429251670837, -0.21672238409519196, 0.9318205714225769, -0.5330989360809326, -0.563642144203186, -0.8433098793029785, -0.7170202732086182, 0.17879566550254822, -0.02703673206269741, -0.4260134696960449, 0.47950586676597595, 0.036647673696279526, -0.5773908495903015, 0.10300856083631516, -0.7353855967521667, -0.03970472887158394, 0.9026702642440796, -0.9071622490882874, 0.17752040922641754, 0.13736416399478912, 0.12554453313350677, -1.2193968296051025, -0.29586419463157654, -0.20681652426719666, -0.09685489535331726, 0.16347618401050568, 0.9386122822761536, -0.549224853515625, 0.5578950643539429, 0.8643044829368591, -0.4118645489215851, -0.5584509372711182, 0.012097381055355072, -0.7028042078018188, -0.44463083148002625, -0.5564311146736145, 0.8987321853637695, -0.3288480043411255, 0.24186179041862488, 0.8641170859336853, 0.3403130769729614, -0.5239736437797546, -0.9321823120117188, -0.2343634068965912, -0.12004989385604858, -0.5515130758285522, 0.36975887417793274, -0.44251295924186707, -0.35910937190055847, 0.12283162772655487, 0.22630096971988678, 0.6124110817909241, -0.39676207304000854, -0.4506797790527344, 0.5250795483589172, 0.011680088005959988, -0.3318774104118347, -0.7130810618400574, -0.05150126665830612, -1.669351577758789, 0.11367271840572357, -1.340577244758606, -0.022604864090681076, -0.6530201435089111, -0.2144983857870102, -0.22203369438648224, 0.1333824098110199, -0.20489268004894257, 0.5334396958351135, -0.3236580789089203, -0.5574848055839539, -0.41809746623039246, -0.444109171628952, 0.685653567314148, 0.4036320447921753, -0.48779699206352234, 0.25864073634147644, -0.19315965473651886, 0.5997015237808228, 0.3289230167865753, 0.46132776141166687, -0.6752007007598877, -1.0216403007507324, -1.4780559539794922, 0.3316652178764343, 0.09871619194746017, -0.5195717811584473, -0.683817446231842, 0.6451932787895203, 0.10231873393058777, -0.3357909917831421, 0.27807047963142395, 0.5821158289909363, -0.8810085654258728, -0.15763027966022491, 0.5719128847122192, -1.092656135559082, 0.4898071587085724, 0.5210758447647095, -0.5480484366416931, -0.2961934506893158, 0.3934052586555481, 0.1093444898724556, -0.7614894509315491, -0.33246612548828125, 0.5619522333145142, -0.43275943398475647, 0.22687670588493347, -0.34967249631881714, -0.003475155681371689, -0.7659493088722229, -0.24099057912826538, 0.12042375653982162, 0.03677154332399368, -0.21398265659809113, 0.7945478558540344, 0.3819190561771393, -0.8108074069023132, 0.05713546276092529, 0.7499410510063171, 0.014273314736783504, -0.16560877859592438, 0.4431527853012085, 0.28034526109695435, -0.5956003665924072, 0.677392303943634, 0.7087520956993103, 0.5532076954841614, -1.2034753561019897, -0.11515188962221146, 0.9122317433357239, -0.619627058506012, -0.1308329850435257, 1.5322450399398804, -0.3685346841812134, -1.2608839273452759, 0.20125873386859894, -1.014857530593872, -0.26051703095436096, -0.6774061918258667, 0.4456010162830353, 0.0313715934753418, 0.21251018345355988, -0.14460241794586182, -0.5360587239265442, -0.10144772380590439, 0.14013038575649261, -0.44740262627601624, 0.4129221439361572, -0.06009873375296593, -0.3974790573120117, 0.4312155246734619, 1.039321780204773, -0.6134326457977295, -0.4904431402683258, -0.620630145072937, -0.18694132566452026, -0.37371429800987244, 0.4157143235206604, -0.2596110701560974, -0.717194676399231, 0.6803844571113586, 0.3565480709075928, 0.14518533647060394, 0.1724659502506256, -0.34816452860832214, 0.2151491492986679, 0.6878582239151001, 0.43534767627716064, -0.7894985675811768, -1.1632198095321655, 1.3225431442260742, 1.113835334777832, -0.7943409085273743, 0.39964520931243896, -0.2191389501094818, -0.4963163435459137, 0.6917511820793152, 0.02137618511915207, 0.2284078747034073, 0.85296231508255, 0.1560223400592804, -0.0777326375246048, 0.136529341340065, -1.2700893878936768, 0.00018992101831827313, 0.980943500995636, 0.47574174404144287, 0.9127183556556702, 0.45229971408843994, -0.11378247290849686, 0.7241223454475403, -0.23187069594860077, 0.2764100134372711, 0.10439567267894745, 0.5620487332344055, -0.17321547865867615, -0.040437549352645874, 0.2902030348777771, 0.942240297794342, -0.5436307191848755, -1.0237469673156738, 0.3416157364845276, 0.46952179074287415, 0.15289220213890076, 0.7217069864273071, 0.7229836583137512, 0.057348623871803284, 0.1816973090171814, 0.309506893157959, 0.5315302014350891, -0.5034087300300598, -0.23160162568092346, -0.1324508637189865, -0.21492761373519897, 0.2514127194881439, -0.08293411135673523, -0.12805764377117157, -0.31690463423728943, -0.5960649251937866, 0.15296143293380737, 0.13905438780784607, 0.4375919699668884, 0.9785761833190918, 0.7400190830230713, 0.2000930905342102, -0.3972316086292267, -0.04365311563014984, -0.6492948532104492, -1.0099223852157593, -0.09797464311122894, -0.5610551834106445, -0.42645835876464844, 0.13273052871227264, -0.13614727556705475, -0.3067396283149719]}, "authors": [{"authorId": "2257191230", "name": "Tianyu Ding"}, {"authorId": "2257252148", "name": "Tianyi Chen"}, {"authorId": "2268495701", "name": "Haidong Zhu"}, {"authorId": "2257278916", "name": "Jiachen Jiang"}, {"authorId": "2269696935", "name": "Yiqi Zhong"}, {"authorId": "2257235854", "name": "Jinxin Zhou"}, {"authorId": "2269161996", "name": "Guangzhi Wang"}, {"authorId": "2269697742", "name": "Zhihui Zhu"}, {"authorId": "15623770", "name": "Ilya Zharkov"}, {"authorId": "46225943", "name": "Luming Liang"}], "references": [{"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "4d76206515d6b33903937474273885476fc2771e", "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs"}, {"paperId": "ca53c1d1ba1a1386f860fa13d7729160571e1643", "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"}, {"paperId": "ef9079f32e806e4d297cee28f36b2321acee9eb3", "title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation"}, {"paperId": "3d181992f7b6b65c889346ee7ea99fdc1570d9b6", "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation"}, {"paperId": "c85268696fe1435605ae66a18653cfdcf8153753", "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, {"paperId": "4880ba8910bc320cb7c1aa943992a500f4c41f07", "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "faab24bc6cd4a4dea6e82420d145f08445c05fc7", "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"}, {"paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80", "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"}, {"paperId": "7f0cf97b771d2988fc4b5ba829aa9f4f29ce7bc0", "title": "Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation"}, {"paperId": "f29f8b8aa2b7e608199b65d3cf751969d4024132", "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction"}, {"paperId": "e26888285436bc7998e5c95102a9beb60144be5e", "title": "Textbooks Are All You Need II: phi-1.5 technical report"}, {"paperId": "a9caf21a845cb0b1b1d453c052188de118006093", "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "3bade55fb2ebe46fd1140c89e31f52b6a508463c", "title": "INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "7a6a298efb965ce9a351a3212f6f536e94dbbb03", "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "5aa26e0b2bb27162a4de07bf8c3d5e0e9d3b0853", "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions"}, {"paperId": "9d460930d9b5d12a65ff2b3efa23047ec75fbca1", "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter"}, {"paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df", "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "af385c0fdd0eda2bbf429bea6fedffc327c8a180", "title": "Randomized Positional Encodings Boost Length Generalization of Transformers"}, {"paperId": "d3f79210b54e168c76b8c311488f42d7d1048b81", "title": "PandaGPT: One Model To Instruction-Follow Them All"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "b2ec81b572fd5f0a5f5de843e3c62985b7d9c5a1", "title": "Lifting the Curse of Capacity Gap in Distilling Language Models"}, {"paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, {"paperId": "5c7aaee5651221893ea0e67c363cab4c4be53b83", "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning"}, {"paperId": "bda605928d6ebe4db906e69ab5d343df75918727", "title": "Large Language Model Guided Tree-of-Thought"}, {"paperId": "28085f480ce456a376ebace9b899e3bc93dbc048", "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "56fa65d8dc41708082f9b2ef7752c49cee9ebe01", "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation"}, {"paperId": "389ec3e8902a5dcfcde1adec735854e93f845937", "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "ebb93e2b3d4fdbd5689cca8ddc01da016738b57e", "title": "Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism"}, {"paperId": "13581a46d32822e44cbeb1acdba4a59cef2b2ec1", "title": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "12c6be503e4e5b7c9cb1810152d4364f26628a8d", "title": "Data-centric Artificial Intelligence: A Survey"}, {"paperId": "1462a0e5b7db47301bb0995db56426e1f4a0ac7d", "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "08b85bce712168998004ee80ce4e475390413c74", "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "ae3d869719c15099889c02c03b922516b3b60aa0", "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation"}, {"paperId": "306c0576750d8ac1298f70474560aa951490b2a1", "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization"}, {"paperId": "489ab1945feb21f17b3efbcf40726c8cbb52bb75", "title": "Q-Diffusion: Quantizing Diffusion Models"}, {"paperId": "fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0", "title": "A Survey on Efficient Training of Transformers"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "4b308ba40e67b0b4b25c6fde17195d5a456a2f41", "title": "Cramming: Training a Language Model on a Single GPU in One Day"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "7004ac6a8044fc06650aebbfc3ac07d7531fbc67", "title": "FSCNN: A Fast Sparse Convolution Neural Network Inference System"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "2537af99905a27d9b84ba9968715f4287f1d3359", "title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers"}, {"paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "title": "Large Language Models Are Human-Level Prompt Engineers"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "ed38c6b157c11476939c426ec6871c926f2f3524", "title": "Leveraging Large Language Models for Multiple Choice Question Answering"}, {"paperId": "33fd110d1e4ca5f91d1b7ca7ff24ce1e9335359e", "title": "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics"}, {"paperId": "fb9f9e98d35340875905730e1a80221fec818944", "title": "Revisiting Neural Scaling Laws in Language and Vision"}, {"paperId": "1daff0ad9be0828eec75f04db644a5d7de46c7ab", "title": "Sparsity-guided Network Design for Frame Interpolation"}, {"paperId": "86d0d3855f94105e25d81cab9f3d269c6062a9c4", "title": "Selective Annotation Makes Language Models Better Few-Shot Learners"}, {"paperId": "fd7e88a2313e176315d99fc299277e752d7703b7", "title": "Efficient Methods for Natural Language Processing: A Survey"}, {"paperId": "eb92cedb7e9182b8887a36d25f319196e11d9ca4", "title": "Prioritizing Samples in Reinforcement Learning with Reducible Loss"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "45122c8f76a4e2fd0163d1f0522db37e97ea4721", "title": "Beyond neural scaling laws: beating power law scaling via data pruning"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b", "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"}, {"paperId": "a221def07d3cd4f29c38234e271faf8a523e0f5a", "title": "Towards Unified Prompt Tuning for Few-shot Text Classification"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "2f211fc941a215ce2f2ddbd0444e06bb74f5e5c9", "title": "ALLSH: Active Learning Guided by Local Sensitivity and Hardness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "f17d331a7a337dcf878969c9dab2e2c1a9e3b4ef", "title": "Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "7d3ccd931aeed833073b33fe15af5c194b6401ea", "title": "On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features"}, {"paperId": "c546e5447f412bf4f274e490996718641b211aa6", "title": "A Survey on Model Compression and Acceleration for Pretrained Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "bb89ee94fe01d39efce914ca59a1eb13aaccbfcf", "title": "NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "b6f616e9305e59c9dc7ccf33c311ede47584caf6", "title": "Kronecker Decomposition for GPT Compression"}, {"paperId": "d291149a75d7ac194382bd61e515eb40ed0aa106", "title": "Predicting Attention Sparsity in Transformers"}, {"paperId": "38f683ec0b9fda2069c0b2cb7ee1c71035915723", "title": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation"}, {"paperId": "6b1462ee55b8a82bcb32d30c3bf0a4bc84f95c10", "title": "Active Learning by Acquiring Contrastive Examples"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "6d178f85c8ca9698b02d0f4f1f4a8f2fc4895411", "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models"}, {"paperId": "88ad1104e61d25c6e8919cb6d2af0aaedd6f0526", "title": "Only Train Once: A One-Shot Neural Network Training And Pruning Framework"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "d8e7bad2681ce70277c900c77a22181d4b03d705", "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "659b95b28a9f3b6cfa3bd06fa7bd004165db5663", "title": "Tesseract: Parallelize the Tensor Parallelism Efficiently"}, {"paperId": "d8df456f790381f4ddb388be24a546625bd75ee2", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "16e623059ffccab60f4c35be028a2d4f10933515", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}, {"paperId": "509b16378deec0fb6bbec1d7aeb32a4bdeedddb1", "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs"}, {"paperId": "24c8adb79ad4ee7070fc0d48807b7f6a42148a74", "title": "A Geometric Analysis of Neural Collapse with Unconstrained Features"}, {"paperId": "04a3f5f95c802cb0a55e3e5eca2875fafbc669dd", "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "017fb5cccc1bc47e070d0e26c7c3351792940c6c", "title": "CDFI: Compression-Driven Network Design for Frame Interpolation"}, {"paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35", "title": "Are NLP Models really able to Solve Simple Math Word Problems?"}, {"paperId": "21d0613c3e7fe2cb31f34441c1604edc9882fa45", "title": "NVIDIA A100 Tensor Core GPU: Performance and Innovation"}, {"paperId": "eb90f7a7f281ccbf64084e11adf822fae2d9bc3f", "title": "Sustainable AI: AI for sustainability and the sustainability of AI"}, {"paperId": "040ad14a2c97e51510889ae6a0c3c23b29da801d", "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models"}, {"paperId": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f", "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch"}, {"paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb", "title": "Scaling Laws for Transfer"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "d1870f667cbd309df45a244c170d1d4ba36bac03", "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "d509fce431fcab681f3c01159ef524da2d256ad3", "title": "Neural Network Compression Via Sparse Optimization"}, {"paperId": "327b12c21e7b8f2126bd7d64ebc2ede0db220c88", "title": "Do We Need to Create Big Datasets to Learn a Task?"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "e585f6e752fb2668b33f7d4b18c8af9bd5abc1a4", "title": "The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research"}, {"paperId": "2e06b7f72270900544284e0898aac2bb564ff58b", "title": "Cold-start Active Learning through Self-Supervised Language Modeling"}, {"paperId": "61c22521b90aa1969883d5ddf4958eac16f81db3", "title": "MESA: Boost Ensemble Imbalanced Learning with MEta-SAmpler"}, {"paperId": "47e1dd3da7ae1b8e7ba283e46d0ea5f0927c9c91", "title": "Self-Paced Learning for Neural Machine Translation"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "930cd22ceac7d82e3c0383d2d58b3d80fa203fe2", "title": "Half-Space Proximal Stochastic Gradient Method for Group-Sparsity Regularized Problem"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "063f8b1ecf2394ca776ac61869734de9c1953808", "title": "AdapterHub: A Framework for Adapting Transformers"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d", "title": "DAPPLE: a pipelined data parallel approach for training large models"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "26299d5fdc5137291dc6a091573b3d18aba1d1c2", "title": "MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "d352e582b8bbf364396e558365ad0777d69d2e1b", "title": "Orthant Based Proximal Stochastic Gradient Method for \ud835\udcc11-Regularized Optimization"}, {"paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"}, {"paperId": "cb674948d44850fd17c27cc0e5d1d432dee8860f", "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation Models"}, {"paperId": "408d0580d7e2befabf542119d7fc8318c684572b", "title": "Pipelined Backpropagation at Scale: Training Large Models without Batches"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "0acc25f3993bd9431418ae275aa12a536b740b77", "title": "ZeroQ: A Novel Zero Shot Quantization Framework"}, {"paperId": "4a4646a5ce6b57e369403e4efea1a2e4559fe9f1", "title": "What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "3f8df14de4320cca46c547a538b615b88fc6a083", "title": "Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach"}, {"paperId": "1340978c92e7cbcf9abe87888150c60984e2964b", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "9f73c3f86026c21d0e5e55c70462952c6ada1175", "title": "Accelerating Deep Learning by Focusing on the Biggest Losers"}, {"paperId": "d28c18a3c2a0afdc0a8634d18345af8d36e1f948", "title": "A Constructive Prediction of the Generalization Error Across Scales"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e900e21f2d1b49633394913ff8b1e585469806e6", "title": "Discriminative Active Learning"}, {"paperId": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25", "title": "A Tensorized Transformer for Language Modeling"}, {"paperId": "ad3a75fa2a26e6f69b7059466828f49492d31789", "title": "BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning"}, {"paperId": "572b6fe8d75032c159896c644319dacdf594e9c6", "title": "One Epoch Is All You Need"}, {"paperId": "cf5a21684aefb1b8db6e0490167636d245396095", "title": "Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds"}, {"paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec", "title": "A Study of BFLOAT16 for Deep Learning Training"}, {"paperId": "79586c6544b32f4d7ac32beae10a2a6c794f0915", "title": "AutoAssist: A Framework to Accelerate Training of Deep Neural Networks"}, {"paperId": "3ce5a172a96008cbdc5ffedf4572b783301fd468", "title": "The role of artificial intelligence in achieving the Sustainable Development Goals"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "730b4474474dd8f4ea983abeb12847f315bdf354", "title": "Diverse mini-batch Active Learning"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"}, {"paperId": "838fbfd9066dbbac6c10059c5b183046fb1cd9d1", "title": "Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study"}, {"paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8", "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "453f7610c6e66bfafcb989d7a9a08e889559f041", "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0", "title": "Deep Learning Scaling is Predictable, Empirically"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "c342c71cb23199f112d0bc644fcce56a7306bf94", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach"}, {"paperId": "48bdeb305a9e3fd34932e2382d246ba364dfa4d0", "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation"}, {"paperId": "0410659b6a311b281d10e0e44abce9b1c06be462", "title": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "7a530fcd15657d8b53caf261a40c7a20f4c4216d", "title": "Biased Importance Sampling for Deep Neural Network Training"}, {"paperId": "da5c65b0ac8b525c3d3d4889bf44d8a48d254a07", "title": "Deep Bayesian Active Learning with Image Data"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "f7b032a4df721d4ed2bab97f6acd33d62477b7a5", "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference"}, {"paperId": "ad811af8ae243838b61829f8467b28fc88ce2cd9", "title": "Using Random Undersampling to Alleviate Class Imbalance on Tweet Sentiment Data"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "8604f376633af8b347e31d84c6150a93b11e34c2", "title": "FitNets: Hints for Thin Deep Nets"}, {"paperId": "8215fb083cb4b0ed2b6858b81dcc30fbd0afb6e1", "title": "Mining of Massive Datasets"}, {"paperId": "50684b147b752a07c313cb73d864f7b21bd8b703", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": "17f70b9d1fcf3b31948ffa578ac89399751fe73d", "title": "Petuum: A New Platform for Distributed Machine Learning on Big Data"}, {"paperId": "d770060812fb646b3846a7d398a3066145b5e3c8", "title": "Do Deep Nets Really Need to be Deep?"}, {"paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68", "title": "One billion word benchmark for measuring progress in statistical language modeling"}, {"paperId": "5cea23330c76994cb626df20bed31cc2588033df", "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets"}, {"paperId": "be921e6d9b3824c4bad331415445a8a8a6829d9c", "title": "Active learning"}, {"paperId": "07ca885cb5cc4328895bfaec9ab752d5801b14cd", "title": "Extensions of recurrent neural network language model"}, {"paperId": "03c93452818c3a7649b5c980d99fe9a08a5ac188", "title": "Active Learning with Clustering"}, {"paperId": "a049555721f17ed79a97fd492c8fc9a3f8f8aa17", "title": "Self-Paced Learning for Latent Variable Models"}, {"paperId": "80115070418534dfa88ee00945168ab2a1309f7a", "title": "Off to a Good Start: Using Clustering to Select the Initial Training Set in Active Learning"}, {"paperId": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning"}, {"paperId": "abf8552208d52de45fdaa551b3b8fe69bea5cea8", "title": "Decompositions of a Higher-Order Tensor in Block Terms - Part II: Definitions and Uniqueness"}, {"paperId": "6bc5142acd2798eca4b0f8bd96d245f846609bba", "title": "Statistical Language Models for Information Retrieval: A Critical Review"}, {"paperId": "e1eef97a5e8a79542c3c93e059c585ed71598e71", "title": "Representative Sampling for Text Classification Using Support Vector Machines"}, {"paperId": "08e39912a54fc46f25f9e79bfa06ee44311b051a", "title": "Active Learning for Statistical Natural Language Parsing"}, {"paperId": "c6586e7c73cc1c9e9a251947425c54c5051be626", "title": "Two decades of statistical language modeling: where do we go from here?"}, {"paperId": "2ebb3dd597bbd7028d8c68bcf509e5bb09ea1e78", "title": "Improving retrieval performance by relevance feedback"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "ff409ef2f03be68527599f7aecb054f52ce106db", "title": "A sequential algorithm for training text classifiers: corrigendum and additional data"}, {"paperId": "d5ddb30bf421bdfdf728b636993dc48b1e879176", "title": "Learning and development in neural networks: the importance of starting small"}, {"paperId": "d05fa6815806727e77987abd58098dc82933d196", "title": "Scaling laws in learning of classification tasks."}, {"paperId": "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214", "title": "A method for disambiguating word senses in a large corpus"}, {"paperId": "a5319b1136149a2dd2b37eba90ef03d1f717a857", "title": "Four Types of Learning Curves"}, {"paperId": "2498a4e1755f047accc06a6e0fab0b0eb1b37ae0", "title": "Statistical mechanics of learning from examples."}, {"paperId": "af386a4e0f2615ed929fdc64a86df8e383bd6121", "title": "A tree-based statistical language model for natural language speech recognition"}, {"paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0", "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "8c1d8eb1dbb4899894ced674ce9c3a91bc978eea", "title": "Towards Automatic Neural Architecture Search within General Super-Networks"}, {"paperId": "d7db793f9824a179518a3eb4ea3bd8bc620f6b6f", "title": "Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"}, {"paperId": "520711a1e93e6c4221f2a7c97c27a508379e8e37", "title": "Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts"}, {"paperId": "c958af9a7b872a8b84844a24fce3c8e02ac21093", "title": "Blockwise Parallel Transformer for Long Context Large Models"}, {"paperId": "f20d99befd90863112a06caaa4e75339d9575a53", "title": "LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing"}, {"paperId": null, "title": "Decomposition enhances reasoning via self-evaluation guided decoding"}, {"paperId": "5350fa166ee3bd093bb031fc3b0aae2b97e18b72", "title": "Uncertainty-Based Active Learning for Reading Comprehension"}, {"paperId": "070ff40f38675cfa42a104a545a47584ad823e70", "title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers"}, {"paperId": "d9a67e3aa1fbd5f07f8f6f8554f193ba36c5abe7", "title": "Non-uniform Step Size Quantization for Accurate Post-training Quantization"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "1ca1a371f8cbfb714126350afd5225eafe7f65ce", "title": "Low-resource Interactive Active Labeling for Fine-tuning Language Models"}, {"paperId": "01d08fa6c229bf3070600e49f8ab05449361817e", "title": "Long-range Sequence Modeling with Predictable Sparse Attention"}, {"paperId": "c49c292e1fb1d215c88828a52134b7ccfa52be44", "title": "Sparse Attention with Learning to Hash"}, {"paperId": null, "title": "2022. Efficient Transformers: A Survey"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "6799e796fb26d081253027feb5ec33ae6664ee23", "title": "Edit Distance Based Curriculum Learning for Paraphrase Generation"}, {"paperId": "ecd12726118b95117bf339ad069c0a87a102cbcc", "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation"}, {"paperId": null, "title": "Curriculumlearning:Aregularizationmethodforefficientandstablebillion-scalegptmodelpre-training"}, {"paperId": "b60a16d2978b10ead102a6a6cd03dc940b1194cc", "title": "Compressing Pre-trained Language Models by Matrix Decomposition"}, {"paperId": null, "title": "PyTorch distributed: experiences on accelerating data parallel training"}, {"paperId": null, "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "From theories to queries: Active learning in practice"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "258a5968e2df9f2009306319b54dd838321a7717", "title": "Semi-Supervised Learning"}, {"paperId": null, "title": "Burr Settles"}, {"paperId": "231f6de83cfa4d641da1681e97a11b689a48e3aa", "title": "Statistical methods for speech recognition"}, {"paperId": null, "title": "2023. Palm 2 technical report"}, {"paperId": null, "title": "2022. Sustainable ai: Environmental implications, challenges and opportunities"}, {"paperId": null, "title": "2022. On the parameterization and initialization of diagonal state space models"}, {"paperId": null, "title": "2023. Extendingcontextwindowoflargelanguagemodelsviapositionalinterpolation"}, {"paperId": null, "title": "2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"}, {"paperId": null, "title": "2022. Designing effective sparse expert models"}, {"paperId": null, "title": "2023. SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": null, "title": "2022. Broken Neural Scaling Laws"}, {"paperId": null, "title": "2023. Depgraph:Towardsanystructuralpruning"}, {"paperId": null, "title": "2022. Mask-guided vision transformer (mg-vit) for few-shot learning"}, {"paperId": null, "title": "2023. Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"}, {"paperId": null, "title": "2022. Alpa: Automating inter-and { Intra-Operator } parallelism for distributed deep learning"}, {"paperId": null, "title": "2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo"}, {"paperId": null, "title": "2022. Hiddenstatevariabilityofpretrainedlanguagemodelscanguidecomputation reduction for transfer learning"}, {"paperId": null, "title": "2023. vllm: Easy, fast, and cheap llm serving with pagedattention"}, {"paperId": null, "title": "2023. OTOv2: Automatic, Generic, User-Friendly"}, {"paperId": null, "title": "2022. Multimodalcontrastivelearningwithlimoe:thelanguage-image mixture of experts"}, {"paperId": null, "title": "2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing"}, {"paperId": null, "title": "2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models"}, {"paperId": null, "title": "Planning for AGI and beyond"}, {"paperId": null, "title": "2022. A survey of quantization methods for efficient neural network inference"}, {"paperId": null, "title": "2023. Stanford Alpaca: An Instruction-following LLaMA model"}, {"paperId": null, "title": "2023. One-ShotSensitivity-AwareMixedSparsityPruningforLargeLanguageModels"}, {"paperId": null, "title": "2023. GPT understands, too"}, {"paperId": null, "title": "2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models"}, {"paperId": null, "title": "AMissing SECRET SAUCE"}, {"paperId": null, "title": "2023. Efficient memorymanagementforlargelanguagemodelservingwithpagedattention"}, {"paperId": null, "title": "2023. An efficient 2d method for training super-large deep learning models"}, {"paperId": null, "title": "2023. Visual instruction tuning"}, {"paperId": null, "title": "2023. Output Sensitivity-Aware DETR Quantization"}, {"paperId": null, "title": "2023. Introducing PyTorch Fully Sharded Data Parallel (FSDP) API"}, {"paperId": null, "title": "PathwaysLanguageModel(PaLM):Scalingto540BillionParametersforBreakthroughPerformance"}, {"paperId": null, "title": "Efficient LLM Algorithmic Survey, Nov, 2023, USA"}, {"paperId": null, "title": "Introducing Claude 2.1"}]}