{"paperId": "d53fe76bd2795a19ddf52d012917782f6f6f2c1e", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.", "venue": "arXiv.org", "year": 2024, "citationCount": 50, "influentialCitationCount": 9, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training."}, "embedding": {"model": "specter_v2", "vector": [0.048287343233823776, 0.28422120213508606, -0.6276234984397888, -0.3106038570404053, -0.21164022386074066, -0.20496748387813568, 0.6356116533279419, -0.08847484737634659, -0.36707431077957153, -0.22658272087574005, 0.6406253576278687, -0.36294251680374146, 0.8530166149139404, 0.029532410204410553, -0.24222852289676666, 0.3147576153278351, -1.1888136863708496, -0.25505200028419495, 0.1250630021095276, -0.3692004978656769, 0.17993229627609253, -0.20652340352535248, -1.31789231300354, 0.3018919825553894, -0.03851114585995674, 0.6683256030082703, 0.05079549551010132, 1.0787113904953003, -0.2524915635585785, 0.8947122097015381, 0.29183468222618103, 0.4474083483219147, 0.0045966398902237415, -0.08706800639629364, -0.5047651529312134, -0.46408402919769287, 0.42615580558776855, -0.47783762216567993, -0.25262805819511414, 0.30753424763679504, -0.163223996758461, 0.5551542639732361, -0.09409880638122559, -0.584407389163971, -0.06322991102933884, 1.0500707626342773, 0.490726500749588, 0.8525063991546631, -0.47703197598457336, -0.6228079199790955, 1.111102819442749, -1.1297231912612915, -0.036470893770456314, 1.1787753105163574, 0.5315243005752563, 0.47436121106147766, 0.1700013130903244, -0.6459857821464539, 1.0822420120239258, 0.16309422254562378, -0.6131121516227722, -0.7558284401893616, -0.24368317425251007, 0.30389729142189026, 2.0203943252563477, -0.06258030980825424, 0.15305085480213165, 0.39329832792282104, -0.3565436005592346, 1.251824140548706, 0.014663022942841053, -0.8340612053871155, -0.20693786442279816, -0.22668227553367615, 0.4657520055770874, 0.7715625166893005, -0.33853092789649963, 0.16096536815166473, -1.1035515069961548, -0.18651214241981506, 0.29409846663475037, 0.2951349914073944, 0.2269100844860077, 0.2639826238155365, -0.16341349482536316, 1.0404630899429321, 0.5093058347702026, 0.9593707919120789, -0.2936265468597412, 0.9864974021911621, 0.6486592888832092, 0.20905892550945282, 0.050055477768182755, 0.24131034314632416, -0.07656989246606827, 0.2106442153453827, -1.0731163024902344, 0.13759981095790863, -0.21540099382400513, 0.9287517070770264, -0.16964460909366608, 0.5327481031417847, -0.5590688586235046, -0.13632994890213013, 1.291911005973816, -0.09556172043085098, 0.4783679246902466, -0.6345161199569702, 0.0954887792468071, -0.8720840215682983, -0.2972860336303711, -0.2780649662017822, -0.20001079142093658, -0.16705766320228577, -0.921529233455658, -1.231635332107544, -0.8289456963539124, 0.06573613733053207, -0.9742058515548706, 0.6121916770935059, -0.551943302154541, 0.41079655289649963, 0.0708479955792427, 0.23372314870357513, 0.4620688557624817, 0.8035317063331604, 0.10955961793661118, 0.006838656030595303, 0.8849632143974304, -1.2304630279541016, -0.5778385400772095, -1.2076839208602905, 0.7125176191329956, -0.20317471027374268, -0.20738568902015686, -0.34127962589263916, -1.2144322395324707, -0.8420373797416687, -0.9439262747764587, -0.04638087376952171, -0.41197049617767334, -0.04614822939038277, 1.168504238128662, 0.39266037940979004, -1.2170071601867676, 0.7536689043045044, -0.8603957891464233, -0.24458393454551697, 0.32504963874816895, 0.13491767644882202, 0.6943453550338745, -0.001908942242152989, -1.2982513904571533, 0.023528603836894035, 0.0645093023777008, -0.19771268963813782, -0.012964164838194847, -0.47492772340774536, -1.020405650138855, -0.057703129947185516, -0.004334867931902409, -0.09036007523536682, 1.5128737688064575, 0.26825711131095886, -1.3205082416534424, 0.5850985050201416, -0.4848688542842865, -0.17172598838806152, 0.0026760532055050135, -0.17431503534317017, -0.3818603456020355, -0.4645356833934784, -0.2317224144935608, 0.32852882146835327, 0.47085288166999817, 0.17287033796310425, -0.022327175363898277, -0.04442110285162926, -0.5156804323196411, -0.5436643362045288, -0.15281902253627777, 0.9334118366241455, -0.30103403329849243, 0.04017788916826248, 0.5010570883750916, 0.6266454458236694, -0.3705487549304962, -0.40413519740104675, -0.47328588366508484, -0.8954448699951172, 1.0469729900360107, -0.08364679664373398, 1.2203338146209717, -0.8492699265480042, -1.0750212669372559, 0.10456844419240952, -0.38691040873527527, 0.05098777264356613, -0.4990371763706207, 0.4638335704803467, -0.349902480840683, 0.4968074560165405, 0.29131653904914856, -0.9817396402359009, -0.34807902574539185, -0.269121915102005, -1.0437225103378296, -0.27874529361724854, 0.11309073865413666, 1.0023541450500488, -0.8297049403190613, 0.13655829429626465, -0.1370004117488861, 0.30166441202163696, -1.0704952478408813, 1.5981260538101196, -0.4827701151371002, 0.21314360201358795, -0.33745279908180237, -0.2638837695121765, -0.1252831071615219, -0.5322226285934448, 0.7780857682228088, -0.13970451056957245, -0.29774901270866394, 0.8609937429428101, -0.3037536144256592, 1.1587496995925903, -0.7052807807922363, 0.5209004282951355, -0.09983357042074203, -0.8090088367462158, 0.4761168360710144, 0.26919254660606384, -0.5117391347885132, -0.42640289664268494, 0.37566524744033813, 0.39794495701789856, -0.8971151113510132, 0.4181632995605469, 0.9488101005554199, 1.271344542503357, -0.4721200466156006, 0.43365955352783203, 0.3564305305480957, -0.18692997097969055, 0.0011531146010383964, 0.5668696761131287, 0.7839502096176147, 0.40078288316726685, 0.32176852226257324, 0.10250654071569443, 0.006263711489737034, -0.9590604305267334, -0.03645880147814751, 0.8122251629829407, 0.7652615904808044, 0.7942109107971191, 0.39476141333580017, -0.843626856803894, -0.5184438228607178, 0.40186750888824463, 0.7255460023880005, 1.616379737854004, -0.5106445550918579, -0.11286408454179764, -0.5330550670623779, -0.15870681405067444, -0.7153903245925903, -0.07926996797323227, -0.012711976654827595, 0.08210098743438721, -0.9731035232543945, -0.7662960886955261, 0.8680364489555359, 0.5033011436462402, 1.1282099485397339, -0.928264856338501, -0.7017656564712524, -0.5648412704467773, 0.2964598834514618, -0.8051261305809021, -0.5241457223892212, 0.5714685916900635, -0.8229781985282898, 0.36777523159980774, 0.4425997734069824, -0.16057679057121277, -0.017574377357959747, -0.7211980819702148, 1.005292534828186, 0.12443727999925613, -0.2773045599460602, -0.3111088275909424, 0.4671747088432312, -0.3823314309120178, -0.7400023341178894, 0.6356732845306396, 0.10020183771848679, -0.4326832890510559, 0.0942373126745224, 0.48211604356765747, -0.06608015298843384, -0.26447632908821106, -0.1858542561531067, -0.1464274823665619, 0.06791995465755463, -0.06782427430152893, 0.31261467933654785, -0.68189537525177, 0.17170724272727966, -1.1371723413467407, 0.37091323733329773, 0.05752728134393692, -0.5935183763504028, -0.03057877905666828, -0.6986878514289856, 0.0805545225739479, 0.3940487205982208, -0.4481132924556732, 0.03381059691309929, -1.141348958015442, 0.08193094283342361, -0.40472835302352905, -0.04282455891370773, 0.012655028142035007, 0.5637086033821106, 0.4990714490413666, 0.0336238369345665, 0.33347204327583313, 0.24449169635772705, -0.007104774005711079, 0.5088075995445251, -0.47621166706085205, 0.4521721601486206, 0.38627535104751587, -0.1172100305557251, -0.28777390718460083, -0.10155750066041946, -0.8271452784538269, -0.5309088230133057, -0.30922308564186096, -0.11433649808168411, 0.02868962474167347, 0.005768605507910252, -0.6365867257118225, -1.057661771774292, -0.027915822342038155, -1.2146763801574707, -0.5258246064186096, 0.2818642854690552, -0.2682441771030426, -0.00976607482880354, -1.0298471450805664, -1.3591355085372925, -0.9064829349517822, -0.9018515348434448, -0.7694493532180786, 0.35937970876693726, 0.1549995094537735, -0.5096814632415771, -0.5010491609573364, -0.060312047600746155, -0.6491215825080872, 1.374611735343933, -0.5683043003082275, 0.6111345887184143, -0.033769641071558, -0.5474944114685059, -0.14869998395442963, 0.17451734840869904, 0.4489598870277405, -0.5640325546264648, 0.3259488344192505, -1.1653766632080078, 0.1286740005016327, -0.43707239627838135, -0.5728752017021179, 0.4687165319919586, 0.23686234652996063, 0.9084029793739319, -0.07957612723112106, -0.45316624641418457, 0.47642001509666443, 1.1209708452224731, -0.6992726922035217, 0.27261170744895935, 0.07360389083623886, 0.7424006462097168, -0.19629108905792236, -0.24301669001579285, 0.588066577911377, -0.021218791604042053, 0.3323765993118286, 0.32142162322998047, 0.20575259625911713, 0.1025453731417656, -0.32586759328842163, 0.5783299207687378, 1.5165433883666992, 0.31852710247039795, -0.029389459639787674, -0.5646630525588989, 0.7802825570106506, -1.1597458124160767, -0.7760474681854248, 0.7129409909248352, 0.8120430111885071, 0.31801196932792664, -0.44029465317726135, -0.3109886646270752, -0.15543842315673828, 0.5612587332725525, 0.8791842460632324, -0.42239663004875183, -1.381432056427002, 0.24031272530555725, 0.7542335987091064, 0.4023773670196533, 0.5493190884590149, -0.13868571817874908, 0.5063276290893555, 14.971034049987793, 0.5977597236633301, -0.40864112973213196, 0.6390551328659058, 0.6862984895706177, 0.037610866129398346, -0.18008990585803986, -0.07109929621219635, -1.5131670236587524, -0.2869657278060913, 1.762791633605957, 0.4739549160003662, 0.4583878219127655, 0.09261814504861832, -0.044513069093227386, 0.28730884194374084, -0.3040911555290222, 0.11421269923448563, 0.41335445642471313, -1.4857701063156128, 0.08356224000453949, -0.020417677238583565, 0.4278327226638794, 0.8604940176010132, 0.8654813170433044, 0.8553399443626404, 0.61605304479599, -0.5456228852272034, 0.29348695278167725, 0.31506431102752686, 0.9886674284934998, -0.22667695581912994, 0.4116601347923279, 0.3669300973415375, -0.850223183631897, -0.11895390599966049, -0.20346982777118683, -1.3562597036361694, -0.015788711607456207, -0.053464800119400024, -0.7356688976287842, -0.47873467206954956, -0.06165451183915138, 0.6514165997505188, 0.45881718397140503, 0.03992446884512901, -0.34972599148750305, 0.7629518508911133, -0.27779486775398254, -0.2123272866010666, 0.47747692465782166, 0.7266351580619812, -0.43123477697372437, -0.05059788376092911, 0.04371044784784317, -0.15784811973571777, 0.10584545135498047, 0.32025760412216187, -0.6440884470939636, -0.25888004899024963, -0.31556418538093567, -0.3589831590652466, 0.1503268927335739, 0.7997387647628784, 0.518359363079071, 0.3026195466518402, -0.3388042151927948, 0.37554603815078735, 0.464954137802124, -3.3798203276091954e-06, -0.21974432468414307, -0.20471526682376862, 0.25304946303367615, -0.32274553179740906, 0.21278029680252075, 0.498918354511261, -0.05439950153231621, -0.4444274604320526, -0.7604821920394897, -0.22794373333454132, 0.35491758584976196, -0.7325301766395569, -0.3153858184814453, 1.0537720918655396, -0.4052594006061554, -0.22115355730056763, -0.35275939106941223, -0.47063082456588745, -0.389984130859375, 0.75935959815979, -1.3275777101516724, -0.6961981058120728, 0.17980198562145233, -0.37503892183303833, -0.5902764797210693, 0.3298419713973999, 1.4931412935256958, 0.025121668353676796, -0.3400575518608093, 0.009655516594648361, -0.013496236875653267, 0.21216924488544464, -0.29624143242836, -0.5210870504379272, 1.264513611793518, 0.534126341342926, -0.12026761472225189, 0.21738876402378082, -0.08964074403047562, 0.34033575654029846, -0.4995919466018677, -0.4452771544456482, 0.868943452835083, -0.7225247025489807, -0.2852340042591095, -0.862049400806427, -0.7471470236778259, 0.9631280899047852, 0.4890020191669464, 0.18754765391349792, 0.0922137200832367, 0.19696581363677979, -0.7136287689208984, -0.11539440602064133, -0.5329815149307251, -0.05343645066022873, 0.4771786332130432, -0.7677413821220398, 0.027668271213769913, -0.21472790837287903, 0.6250649690628052, -0.9579384922981262, -0.39508798718452454, -0.2286079227924347, 0.22459034621715546, -0.1897605061531067, 0.670184314250946, -0.3611476719379425, 0.5883892774581909, 1.063597559928894, -0.041733283549547195, -0.4672878682613373, 0.25805261731147766, -0.9863176345825195, -0.27854597568511963, 0.15915879607200623, 0.542585551738739, -0.3888387680053711, 0.6331573724746704, 0.3238658905029297, 0.46078434586524963, -0.4726809859275818, -0.44256794452667236, -0.13210761547088623, -0.07230953872203827, -0.5488510727882385, 0.399790495634079, 0.02016342058777809, -0.15781068801879883, 0.2648621201515198, 0.24372054636478424, 0.5062134861946106, -0.3268137574195862, -0.4931367039680481, 0.22782647609710693, -0.10990554839372635, -0.0615600161254406, -0.4122019112110138, -0.6879220604896545, -1.3372782468795776, -0.024142052978277206, -0.8442445397377014, 0.08092070370912552, -0.4890178143978119, -0.6324790716171265, -0.16802658140659332, -0.22577786445617676, 0.4544258415699005, 0.3705943524837494, 0.06495607644319534, -0.4955349564552307, -0.6780988574028015, -0.7270106077194214, 0.5947641134262085, 0.3346301019191742, -0.3063589930534363, -0.11727047711610794, 0.06334342807531357, 0.137259379029274, 0.08904588222503662, 0.6036012172698975, -0.2675653398036957, -0.4172867238521576, -1.0979427099227905, 0.2302793264389038, 0.020869148895144463, -0.21036089956760406, -0.6590012311935425, 0.617754340171814, 0.502242386341095, -0.16445741057395935, -0.23819364607334137, 0.12605535984039307, -0.3890635073184967, -0.6552171111106873, 0.4508388042449951, -0.7229871153831482, 0.3613760769367218, 0.40829867124557495, -0.662690281867981, -0.04977836087346077, 0.77054762840271, 0.048745084553956985, -0.8867907524108887, -0.8376331329345703, 0.3334655165672302, -0.6923254728317261, 0.023440932855010033, -0.445909321308136, 0.08321478962898254, -1.2530640363693237, -0.495389461517334, -0.16532443463802338, 0.36531388759613037, -0.44874194264411926, 0.9710614681243896, 0.2290194034576416, -0.8276383280754089, 0.06298176944255829, 0.653266429901123, -0.3233935534954071, 0.16865673661231995, 0.40070000290870667, 0.3791106045246124, -0.5285806655883789, 0.6606807708740234, 0.41302865743637085, 0.13637731969356537, -0.9031080603599548, -0.017295721918344498, 0.6113294363021851, -0.5940962433815002, -0.02534060925245285, 1.1051054000854492, -0.859246551990509, -0.9742308855056763, 0.09076836705207825, -1.4780633449554443, -0.5631546974182129, -0.20773842930793762, 0.6809232831001282, 0.05923980101943016, 0.3364206552505493, 0.21106623113155365, -0.6217342019081116, 0.14296101033687592, -0.1525357961654663, -0.41623225808143616, 0.5684701204299927, -0.06025771424174309, -0.699998140335083, 0.93473881483078, 0.8043285608291626, -0.5209363698959351, -0.3547356128692627, -0.9195177555084229, -0.4086719751358032, 0.3620241582393646, 0.5273858904838562, -0.09719279408454895, -0.7371612191200256, 0.8264475464820862, 0.5150849223136902, 0.4122060537338257, 0.1483011543750763, -0.1002189889550209, 0.24786092340946198, 0.8110688328742981, 0.16650636494159698, -0.1633671671152115, -0.4847991466522217, 1.5199741125106812, 0.8795126676559448, -0.6207461357116699, 0.5350669026374817, 0.05541003495454788, -0.3455984890460968, 0.7789396047592163, 0.19980548322200775, -0.16412480175495148, 0.9200307726860046, 0.034160953015089035, 0.43671008944511414, 0.2826676070690155, -1.3198721408843994, -0.08123122155666351, 0.05828070640563965, 0.48297297954559326, 0.8949735760688782, 0.2377123236656189, 0.31639769673347473, 0.43631500005722046, 0.24242033064365387, 0.12985973060131073, 0.37407299876213074, 0.9124521017074585, -0.1167939156293869, 0.0006937546422705054, -0.059673011302948, 0.49972596764564514, -0.9972363114356995, -0.8402379751205444, 0.5379694700241089, 0.46518638730049133, -0.03922688961029053, 0.7669798731803894, 1.2745623588562012, 0.18165172636508942, 0.33403778076171875, 0.16377447545528412, 0.46650904417037964, -0.749983549118042, -0.5773501992225647, -0.3669512867927551, -0.6733880639076233, -0.3879689574241638, -0.04006896913051605, -0.3621949255466461, -0.6162279844284058, -0.7123270034790039, 0.2886348068714142, 0.27232620120048523, 0.4752863943576813, 0.9994997382164001, 0.8004271984100342, 0.6566864848136902, 0.07964958995580673, -0.6018530130386353, -0.4898200035095215, -1.1176416873931885, -0.06861855089664459, -0.9323844313621521, 0.23809495568275452, 0.36325138807296753, -0.30445078015327454, -0.45309996604919434]}, "authors": [{"authorId": "2289159449", "name": "Soham De"}, {"authorId": "2288823236", "name": "Samuel L Smith"}, {"authorId": "2211434392", "name": "Anushan Fernando"}, {"authorId": "3436640", "name": "Aleksandar Botev"}, {"authorId": "2288525579", "name": "George Cristian-Muraru"}, {"authorId": "2288528607", "name": "Albert Gu"}, {"authorId": "2288530861", "name": "Ruba Haroun"}, {"authorId": "48092709", "name": "Leonard Berrada"}, {"authorId": "2288753519", "name": "Yutian Chen"}, {"authorId": "2059763226", "name": "S. Srinivasan"}, {"authorId": "2755582", "name": "Guillaume Desjardins"}, {"authorId": "2288524127", "name": "Arnaud Doucet"}, {"authorId": "2508525", "name": "D. Budden"}, {"authorId": "2245908218", "name": "Y. W. Teh"}, {"authorId": "1996134", "name": "Razvan Pascanu"}, {"authorId": "1737568", "name": "Nando de Freitas"}, {"authorId": "2288101023", "name": "Caglar Gulcehre"}], "references": [{"paperId": "189fde3f4dfa105bb51472a8945618f395919560", "title": "Repeat After Me: Transformers are Better than State Space Models at Copying"}, {"paperId": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27", "title": "MambaByte: Token-free Selective State Space Model"}, {"paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444", "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797", "title": "GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "0a304c773678824f7ab82e9f1acc8ed86dc668af", "title": "Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "7c25adf2ddb35df05a61c697da97efb8583d77df", "title": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "b55ee75940d24934a54d7f1acfde06e9cb45ac44", "title": "It's Raw! Audio Generation with State-Space Models"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "60a5394c4a07a1d57dfa17ec5e92725fd341439a", "title": "The Design Process for Google's Training Chips: TPUv2 and TPUv3"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c904d42a6aeb81c385dbc94d95380734dee43fbc", "title": "NVIDIA Tensor Core Programmability, Performance & Precision"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "title": "Quasi-Recurrent Neural Networks"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "1a3d22599028a05669e884f3eaf19a342e190a87", "title": "Backpropagation Through Time: What It Does and How to Do It"}, {"paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time"}, {"paperId": null, "title": "etal. Tenlessonsfromthreegenerationsshapedgoogle\u2019stpuv4i: Industrialproduct"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed", "title": "Efficient BackProp"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "255a77422b1da74da05d1714b7875356187385bd", "title": "A New Approach to Linear Filtering and Prediction Problems"}, {"paperId": "7509b472cbe7b1fe71a8fccf60f34cc873d1ab63", "title": "Turing computability with neural nets"}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}, {"paperId": null, "title": "achieve comparable training efficiency to Transformers on TPU-v3. Since diagonal RNN layers are memory bound, we achieve this with a kernel for the RG-LRU layer"}, {"paperId": null, "title": "during training, and can also efficiently learn copying and retrieval tasks from training data (Section 6). However"}, {"paperId": null, "title": "During inference, both Hawk and Griffin achieve significantly higher throughput than MQA Trans-formers (Figure 1(b)), and they achieve lower latency when sampling"}, {"paperId": null, "title": "Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models"}]}