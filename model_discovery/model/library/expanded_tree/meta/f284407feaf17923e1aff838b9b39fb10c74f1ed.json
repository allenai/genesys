{"paperId": "f284407feaf17923e1aff838b9b39fb10c74f1ed", "abstract": "The pre-trained language models are continually fine-tuned to better support downstream applications. However, this operation may result in significant performance degeneration on general tasks beyond the targeted domain. To overcome this problem, we propose LM-Cocktail which enables the fine-tuned model to stay resilient in general perspectives. Our method is conducted in the form of model merging, where the fine-tuned language model is merged with the pre-trained base model or the peer models from other domains through weighted average. Despite simplicity, LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain. We conduct comprehensive experiments with LLama and BGE model on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the efficacy of our proposed method. The code and checkpoints are available at https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.", "venue": "arXiv.org", "year": 2023, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain."}, "embedding": {"model": "specter_v2", "vector": [-0.10022996366024017, 0.21571287512779236, -0.3973008990287781, 0.08029979467391968, -0.6368075609207153, -0.2750949561595917, 0.6226134896278381, -0.5839031934738159, -0.3580320477485657, -0.2640089690685272, 0.42895540595054626, -0.2163230925798416, 0.43123501539230347, 0.3427199721336365, -0.15073688328266144, 0.035063281655311584, -1.0545042753219604, 0.24991920590400696, -0.3173450529575348, -0.6825765371322632, -0.7611870169639587, -0.5975432991981506, -0.7079919576644897, 0.26869451999664307, 0.8282013535499573, 0.35411033034324646, 0.26863086223602295, 1.1640145778656006, -0.1921830177307129, -0.034187477082014084, 0.5004335045814514, -0.19850145280361176, 0.3650668263435364, -0.160507932305336, 0.07915834337472916, 0.4455002248287201, 0.36153915524482727, -0.4060002565383911, -0.3327971398830414, 0.9489424824714661, -0.05113760754466057, 0.2798314094543457, 0.15928144752979279, -0.630966067314148, -0.32527273893356323, 0.8447490334510803, 0.5645175576210022, 0.6644591093063354, -0.7311660647392273, -0.3661656081676483, 0.7884452939033508, -1.086391568183899, -0.0899273008108139, 1.475956678390503, 0.3473956882953644, 0.5837960839271545, -0.08948346972465515, -0.9104712605476379, 0.7652758955955505, -0.04241945594549179, -0.8132964968681335, -0.6027452945709229, 0.0261377040296793, -0.11265470087528229, 1.9100629091262817, -0.4112696349620819, -0.5558311343193054, 0.37184831500053406, -0.0617760606110096, 1.1457655429840088, 0.026445379480719566, -0.6922364234924316, -0.4596262574195862, 0.4216138422489166, 0.2368759661912918, 0.6385573148727417, -0.32185864448547363, 0.09519197046756744, -0.7547248005867004, -0.3584628701210022, -0.015145956538617611, -0.6788111329078674, -0.05229302495718002, 0.10531022399663925, -0.10806955397129059, 0.7675727605819702, 0.28518256545066833, 0.9732024669647217, -0.09733322262763977, 0.7044816613197327, 0.678835391998291, 0.5552079677581787, 0.5128093957901001, 0.6030269265174866, -0.41966745257377625, 0.5873534083366394, -1.073256015777588, 0.28335079550743103, -0.10652374476194382, 0.973021388053894, -0.31993889808654785, -0.022981777787208557, -0.5333872437477112, 0.15953944623470306, 1.3143967390060425, -0.07795548439025879, 0.7705514430999756, -0.5976521372795105, 0.6339520215988159, -0.8450897336006165, -0.15101481974124908, -0.358104944229126, -0.41113004088401794, -0.5804869532585144, -0.9156432747840881, -1.381064534187317, -0.6794723868370056, -0.007724239490926266, -0.9140917658805847, 1.1403721570968628, -0.369203120470047, 0.19267992675304413, 0.289989173412323, 0.17867779731750488, 0.8259300589561462, 0.614007294178009, 0.4155181348323822, -0.20598962903022766, 0.805799126625061, -1.3044469356536865, -0.6152669787406921, -1.0705666542053223, 0.8923249840736389, -0.6315910816192627, 0.23363548517227173, -0.08471307158470154, -1.3114569187164307, -0.9464659690856934, -1.124363899230957, 0.07745130360126495, -0.49553051590919495, 0.3536606729030609, 0.8095108866691589, 0.4010217785835266, -0.8571925759315491, 0.742408037185669, -0.09972508251667023, -0.257135272026062, 0.06263986229896545, 0.17665286362171173, 0.23249904811382294, -0.10800813883543015, -1.5722119808197021, 0.15658198297023773, 0.5985965728759766, -0.6199951767921448, -0.04350597411394119, -0.428759902715683, -0.6585714221000671, -0.2503683567047119, 0.2218252271413803, -0.6376834511756897, 1.3251116275787354, 0.04220077022910118, -1.1557600498199463, 0.5659551620483398, -0.032458506524562836, -0.009120762348175049, 0.6552953124046326, -0.1550678014755249, -0.7991657853126526, -0.7379171848297119, -0.3661845922470093, 0.6339198350906372, 0.32178375124931335, -0.18298004567623138, 0.09133772552013397, 0.37099599838256836, -0.3304712176322937, 0.05563953518867493, -0.5353515148162842, 0.7459469437599182, -0.7253851890563965, -0.4304813742637634, 0.5687994360923767, 0.5373837947845459, -0.1130918338894844, -0.7033608555793762, -0.35766929388046265, -0.8899404406547546, 0.9391816854476929, -0.36104562878608704, 1.4078258275985718, -0.8650543093681335, -0.5761927366256714, -0.24851439893245697, -0.36126983165740967, 0.09452448040246964, -0.9754531383514404, 0.6190413236618042, -0.07618383318185806, 0.5235471129417419, -0.11083158850669861, -1.5954391956329346, 0.10054826736450195, -0.5086631178855896, -0.6302402019500732, -0.39168840646743774, 0.33961227536201477, 1.0339624881744385, -0.7452307939529419, 0.21493397653102875, -0.3742150366306305, 0.27912116050720215, -1.2781643867492676, 1.1249130964279175, -0.8201579451560974, 0.5714112520217896, 0.07104285061359406, 0.13706450164318085, 0.35078635811805725, -0.4015352129936218, 0.5346773266792297, -0.29844146966934204, 0.0631616860628128, 0.43254536390304565, -0.629784107208252, 1.552049160003662, -0.4168212115764618, 0.24864554405212402, 0.15231481194496155, -0.3959145247936249, 0.11769962310791016, 0.4993492662906647, -0.3134901821613312, -0.37073200941085815, 0.5402930378913879, 0.6557791233062744, -0.43980738520622253, 0.3658148944377899, 0.9365125298500061, 0.8531078100204468, -0.5513662695884705, 0.05557853356003761, 0.7435528039932251, -0.3373367190361023, 0.5011125206947327, 0.31459465622901917, 0.39293375611305237, 0.37055033445358276, 0.42536863684654236, -0.062463413923978806, 0.5900487303733826, -0.9143001437187195, -0.2305174022912979, 0.5441975593566895, 0.34143340587615967, 0.5111174583435059, 0.06465098261833191, -0.43988868594169617, -0.22348418831825256, -0.11536875367164612, 0.41099822521209717, 1.7572025060653687, -0.3286023736000061, -0.11531999707221985, -0.8286328911781311, -0.1313243955373764, -0.3777630031108856, -0.17637471854686737, -0.23774373531341553, -0.2484622448682785, -0.7522538304328918, -1.2503846883773804, 0.5681700706481934, -0.3313177824020386, 1.240805745124817, -0.3589032292366028, 0.13662870228290558, -0.040620651096105576, -0.0009469863143749535, -0.8949546813964844, -0.8083973526954651, 0.24079003930091858, -0.7081196904182434, 0.26764917373657227, 0.09339068830013275, -0.11594399809837341, -0.099439337849617, -0.4779869616031647, 1.035158634185791, -0.46336913108825684, 0.07312911003828049, -0.04799585044384003, 0.5541360378265381, -0.4950203597545624, -1.3321059942245483, 0.37902799248695374, 0.4421047270298004, -0.0861288458108902, 0.11854948848485947, 0.4737739562988281, 0.21371804177761078, -0.058527130633592606, -0.38105887174606323, 0.1994633823633194, 0.0080471346154809, 0.008943192660808563, 0.8048082590103149, -0.08842723071575165, 0.3293234407901764, -1.3120357990264893, 1.3197063207626343, -0.266486793756485, -0.5328999757766724, 0.3875683844089508, -0.42768168449401855, -0.43347856402397156, 0.4370902478694916, -0.969176709651947, -0.2467075139284134, -1.3463702201843262, 0.4671004116535187, -0.34115344285964966, -0.001805684994906187, 0.1358186900615692, -0.023067479953169823, 0.3206801116466522, 0.24940644204616547, 0.27719345688819885, -0.1583152860403061, -0.5841022729873657, 0.8078259825706482, -0.5778799057006836, 0.7409268021583557, 0.4039406478404999, -0.11693470925092697, -0.3217690587043762, -0.4160551130771637, -0.6089621186256409, -0.2392440140247345, -0.34505847096443176, -0.3703645169734955, -0.13205759227275848, 0.029533620923757553, -1.006475806236267, -0.1262672245502472, 0.22140496969223022, -0.9268090128898621, -0.14498966932296753, 0.5388599634170532, -0.04505384340882301, -0.3046480417251587, -1.2065151929855347, -1.135591983795166, -0.42347562313079834, -0.8127379417419434, -1.2915894985198975, 0.5000635385513306, 0.16798004508018494, -0.47567203640937805, -0.575832188129425, 0.143392875790596, -0.20485536754131317, 1.1321543455123901, -0.5765373706817627, 0.6772518754005432, -0.08189919590950012, -0.012999601662158966, -0.38210806250572205, 0.09808549284934998, 0.8063787817955017, -0.2324768602848053, 0.15420815348625183, -1.4488787651062012, -0.1730344295501709, -0.017525792121887207, -0.1608278453350067, 0.5819814801216125, 0.263561487197876, 0.6785764694213867, 0.01475954707711935, -0.6212199926376343, 0.7488908767700195, 1.1466197967529297, -0.6917573809623718, -0.263367623090744, 0.2134431004524231, 0.763586163520813, 0.0745539739727974, -0.3294898271560669, 0.3062569200992584, 0.10445059835910797, 0.2061096578836441, 0.0075325858779251575, -0.2692641019821167, -0.13962243497371674, -0.375212699174881, 0.5814297795295715, 2.115330457687378, 0.704639732837677, -0.39288416504859924, -0.7598798871040344, 0.45626747608184814, -1.3184680938720703, -0.3699449598789215, 0.9082176685333252, 0.6466073393821716, 0.5268821716308594, -0.4072916805744171, -0.31577759981155396, -0.3941056728363037, 0.4852250814437866, 0.23257824778556824, -0.5347849726676941, -1.0459942817687988, -0.16484259068965912, -0.11274212598800659, 0.024651765823364258, 0.46168726682662964, -0.8062877058982849, 0.7117879986763, 14.726912498474121, 1.0157688856124878, 0.04535550996661186, 1.09442138671875, 0.7450821995735168, 0.09810295701026917, -0.12330368906259537, -0.3860894441604614, -1.2716076374053955, 0.17348675429821014, 1.3599812984466553, 0.20132163166999817, 0.7149990797042847, 0.04514613747596741, 0.044869329780340195, 0.3402840197086334, -0.1304464042186737, 0.3423597812652588, 0.3849376142024994, -1.1581255197525024, 0.3849083185195923, 0.22084635496139526, 1.1244337558746338, 1.1621531248092651, 0.7863982319831848, 0.9824305772781372, 0.4618893265724182, -0.2888922393321991, 0.334723562002182, 0.15471729636192322, 0.5908671021461487, -0.13773082196712494, 0.41754263639450073, 0.5631205439567566, -0.6738377213478088, -0.264110267162323, -0.5445021390914917, -1.10137140750885, 0.350604385137558, 0.12672384083271027, -0.4593859910964966, -0.6081234216690063, -0.11417625844478607, 0.2755049467086792, 0.058835022151470184, 0.3578227758407593, 0.2041167914867401, 0.4604716897010803, 0.03620491549372673, 0.4245271682739258, 0.4748014211654663, 0.43082839250564575, 0.4771137535572052, 0.29654815793037415, 0.13953803479671478, -0.4632148742675781, 0.2814374566078186, 0.3628326952457428, -0.9491877555847168, 0.017860060557723045, -0.25785762071609497, -0.2161845862865448, 0.0810207948088646, 0.930163562297821, 0.7003839015960693, -0.11648204177618027, -0.38600704073905945, 0.2263382077217102, 0.61186683177948, 0.09495451301336288, -0.6075848937034607, 0.356232225894928, 0.5529148578643799, -0.49385079741477966, -0.28037598729133606, 0.3170096278190613, 0.010712949559092522, -0.5307320356369019, -0.5855703949928284, -0.556624710559845, 0.3097582459449768, -0.5557830929756165, -0.6932690143585205, 1.1058237552642822, -0.17392049729824066, -0.23622314631938934, 0.12832653522491455, -0.3839840590953827, -0.31740882992744446, 0.5930632948875427, -1.5734455585479736, -0.8705024123191833, 0.507981538772583, -0.5106709599494934, -0.49105554819107056, -0.42028433084487915, 1.162795066833496, 0.3254258632659912, -0.541845977306366, 0.3879580497741699, 0.5021346807479858, -0.18051043152809143, 0.1746179163455963, -0.3473810851573944, 1.35114324092865, 0.6413213014602661, -0.27310910820961, 0.1211203932762146, 0.07680189609527588, 0.23403894901275635, -0.5863494873046875, -0.41377708315849304, 0.7196617722511292, -0.502774715423584, -0.07016286253929138, -0.7633273005485535, -0.9373763799667358, 0.1362389773130417, 0.7340684533119202, -0.2662124037742615, 0.2619452178478241, -0.08585264533758163, -0.6944681406021118, 0.19530591368675232, -0.8838295340538025, 0.08679290115833282, 0.40656477212905884, -0.6471461653709412, 0.0016827615909278393, 0.39758193492889404, 0.8459842801094055, -1.1397079229354858, -0.5236784219741821, 0.16622774302959442, -0.23988109827041626, -0.07206743955612183, 0.8418294191360474, -0.6014233827590942, 0.171906515955925, 0.9770761728286743, -0.5917922258377075, -0.9052279591560364, 0.062087297439575195, -1.0703656673431396, -0.20181545615196228, 0.46179789304733276, 0.5780287384986877, -0.36067086458206177, -0.4158695936203003, 0.6650447249412537, 0.1558712124824524, -0.46542832255363464, -0.4982461929321289, -0.26668381690979004, 0.5441632866859436, -0.636881411075592, 0.7432963252067566, -0.02718057669699192, -0.12323117256164551, -0.2111327052116394, 0.397610604763031, 0.7564915418624878, -0.19198806583881378, -0.79258793592453, 0.5259361267089844, 0.13213106989860535, -0.5703015327453613, -0.8610371947288513, -0.2672537565231323, -1.0740326642990112, -0.0031372110825031996, -1.2651442289352417, 0.0002294247824465856, -0.5993492603302002, -0.4351331293582916, 0.09981106966733932, -0.3511938750743866, -0.2859889566898346, 0.3157685101032257, 0.05149511620402336, -0.1274987757205963, -0.4974392056465149, -0.431916743516922, 0.6274405717849731, 1.0634770393371582, -0.7769293785095215, -0.35773587226867676, -0.04115256294608116, -0.1440853774547577, 0.6154413223266602, 0.656240701675415, -0.18895916640758514, -0.5810099244117737, -1.6367242336273193, 0.018916208297014236, -0.36841943860054016, -0.33863550424575806, -0.7152392268180847, 0.7985039949417114, 0.5563869476318359, -0.34486061334609985, 0.11098524928092957, 0.35526230931282043, -0.5541252493858337, -0.5314632654190063, 0.23810815811157227, -0.5044088959693909, 0.44732165336608887, 0.6180803775787354, -0.84194016456604, -0.3190457224845886, 0.8325641751289368, 0.15573939681053162, -0.8772042393684387, -0.8424543738365173, 0.27049869298934937, -0.36884620785713196, 0.283931165933609, -0.40005356073379517, 0.41935765743255615, -1.3085192441940308, -0.29393327236175537, -0.1471209079027176, 0.4717426598072052, -0.5554059743881226, 1.0639809370040894, -0.18707430362701416, -1.1564066410064697, -0.2911083996295929, 0.7599219083786011, -0.23312759399414062, -0.16185034811496735, 0.6070178151130676, 0.4830920398235321, -0.2139623612165451, 0.7462475299835205, 0.673933207988739, 0.1747908890247345, -0.8542296290397644, 0.0016664856811985373, 0.872724175453186, -0.7721154689788818, 0.05896584689617157, 1.4827685356140137, -0.01791030913591385, -1.4172613620758057, 0.14901207387447357, -1.0529706478118896, -0.17764219641685486, -0.26874789595603943, 0.7472987771034241, 0.37714385986328125, 0.26859283447265625, -0.2939056158065796, -0.6131476163864136, 0.14897651970386505, -0.05154547095298767, -0.35427340865135193, 0.3544086515903473, -0.673596978187561, -0.5444730520248413, 0.4925249516963959, 0.9854055643081665, -0.5537402629852295, -0.47097352147102356, -0.9371656179428101, -0.3509600758552551, 0.35030320286750793, 0.6510445475578308, -0.4842800498008728, -0.54226154088974, 0.42695915699005127, 0.38781896233558655, 0.15014146268367767, 0.49469634890556335, -0.2412240356206894, 0.20572300255298615, 0.5458500385284424, 0.003761883592233062, -0.4870759844779968, -0.9414955377578735, 1.2079092264175415, 1.160589575767517, -1.278601884841919, 0.2591184675693512, 0.17032118141651154, -0.4891486167907715, 0.5564508438110352, 0.41512975096702576, -0.14369279146194458, 1.1373076438903809, -0.150498628616333, 0.3541533946990967, 0.5623065829277039, -1.2325578927993774, -0.14306418597698212, 1.1177384853363037, 0.5000964999198914, 0.7444903254508972, 0.5072680115699768, -0.08749131858348846, 0.7752866744995117, 0.014186553657054901, -0.14869467914104462, 0.5145273208618164, 0.0683358907699585, -0.17793552577495575, -0.22445210814476013, -0.28611403703689575, 0.2695409059524536, -0.607856035232544, -0.4825451374053955, 0.025221141055226326, 0.7231866717338562, 0.5113487839698792, 0.8981211185455322, 0.8410230278968811, -0.16305062174797058, 0.3307380676269531, 0.19106167554855347, 0.3442356288433075, -0.38075438141822815, -0.26780441403388977, 0.46716827154159546, -0.9001489281654358, 0.0252204742282629, -0.09121625125408173, -0.2811574339866638, -0.13966575264930725, -0.5518560409545898, 0.16642244160175323, -0.19230933487415314, 0.5824427008628845, 1.0988271236419678, 0.5945870876312256, 0.7542257905006409, -0.2199772596359253, -0.6133110523223877, -0.4466867744922638, -1.095409870147705, 0.05560740455985069, -0.4389851689338684, -0.1916862577199936, 0.13411760330200195, -0.12554799020290375, -0.7424498200416565]}, "authors": [{"authorId": "2051175765", "name": "Shitao Xiao"}, {"authorId": "2240687341", "name": "Zheng Liu"}, {"authorId": "2153419738", "name": "Peitian Zhang"}, {"authorId": "2267725682", "name": "Xingrun Xing"}], "references": [{"paperId": "c0230760f644f6b7538d93e4296a5e9aa7028e45", "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "838cd69a0b6c9c244a6eebb0f4742c0625132de6", "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning"}, {"paperId": "3f459219d75de63b5b7a26a8c6447ec1e79a985c", "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "eb6e30d13acd4df1d94575f715d56d314e834d1a", "title": "\u03c0-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation"}, {"paperId": "f7212245d3787c66b8dc1e9fa4bc48349cef1155", "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "3ff08b5ca57e786d8af7b204ef94c9972bd9a61e", "title": "Dataless Knowledge Fusion by Merging Weights of Language Models"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "65f056d32dac701240a52a5daf8cedb611b04ceb", "title": "Patching open-vocabulary models by interpolating weights"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "06b20a1c6883464fcb2855adc146874fe7937c41", "title": "Merging Models with Fisher-Weighted Averaging"}, {"paperId": "c28b7dfe341f1e13a5a98efbce7946ef795cf9b8", "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer"}, {"paperId": "9289826beb6206eeaf500105f7329d6d5a495d8a", "title": "Robust fine-tuning of zero-shot models"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "7b99c51d562e33309a46601c846abbe72a65c6a4", "title": "What to Pre-Train on? Efficient Intermediate Task Selection"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "220d2282be4538678e388e1532f3c06ee49e217e", "title": "Versatile black-box optimization"}, {"paperId": "f2f3c83db919a2429c4fcad2d0a0ed4e5294354a", "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "261004890f902acd810ac4e9b1025ca5981ceedf", "title": "Model Fusion via Optimal Transport"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "1eb7f46b1a0a7df823194d86543e5554aa21021a", "title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift"}, {"paperId": "338c5f71f98b3a5ab5e1ce1b21ae232b9bfb0648", "title": "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation"}, {"paperId": "d9ff7a9344dd5d6653bd7a02bfd704422bb29951", "title": "Experience Replay for Continual Learning"}, {"paperId": "59a922212153d3407e658109f36c11a34ee7d283", "title": "Continual Learning with Deep Generative Replay"}, {"paperId": "a36c43156fda548b40539b691706fddb1b142a9c", "title": "Encoder Based Lifelong Learning"}, {"paperId": "802168a81571dde28f5ddb94d84677bc007afa7b", "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "1703631a938b397ba7e858161ce16448f6046d6f", "title": "iCaRL: Incremental Classifier and Representation Learning"}, {"paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a", "title": "Learning without Forgetting"}, {"paperId": "0407b605b8f55db72e2545586bfe8e946b691b70", "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks"}, {"paperId": "7fc70d4cc5118fdbc8e8807979eae8b61948ff91", "title": "The elements of statistical learning: data mining, inference and prediction"}, {"paperId": "2ff694e20f492a7acf7fd0646c5e1576f0b3c901", "title": "C-Pack: Packaged Resources To Advance General Chinese Embedding"}, {"paperId": "a884bc8bc5b04e5ad096649856df5b7931fd3d23", "title": "Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer"}, {"paperId": "2cc134293669b20dce3d55a67d08fea665745e7b", "title": "Combining Parameter-efficient Modules for Task-level Generalisation"}, {"paperId": "f473fa00987739cc14af7d23c75a74973d86d6e5", "title": "Model Fusion of Heterogeneous Neural Networks via Cross-Layer Alignment"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2022. Mteb: Massive text embedding benchmark"}, {"paperId": null, "title": "2023. Learning to retrieve in-context examples for large language models"}, {"paperId": null, "title": "2023. Multitask pre-training of modular prompt for chinese few-shot learning"}, {"paperId": null, "title": "Scialom"}, {"paperId": null, "title": "2023. Resolving interference when merging models"}, {"paperId": null, "title": "SQuAD 30,000 10,570 Exact Match"}, {"paperId": null, "title": "Winogrande 30,000 1,267 Accuracy CommenGen 30,000 4,018 ROUGE-L MRPC 3,668 408 Accuracy"}]}