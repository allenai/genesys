{"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "abstract": null, "venue": "Science China Technological Sciences", "year": 2020, "citationCount": 1239, "influentialCitationCount": 46, "openAccessPdf": {"url": "https://arxiv.org/pdf/2003.08271", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks."}, "embedding": {"model": "specter_v2", "vector": [0.32923126220703125, 0.8177388310432434, -0.39420992136001587, -0.2217072993516922, -0.46409666538238525, -0.31320053339004517, 0.6277937889099121, 0.022092871367931366, -1.013790488243103, -0.007626906502991915, 0.6121290326118469, -0.017628470435738564, 0.24982121586799622, 0.07661668211221695, -0.40047094225883484, 0.365158349275589, -0.6424179077148438, 0.44235506653785706, -0.3751560151576996, -0.27465006709098816, -0.43505555391311646, -0.5568517446517944, -0.6806082129478455, 0.26123151183128357, 0.26651862263679504, 0.026440709829330444, 0.25856468081474304, 0.5947132110595703, -0.4102645814418793, 0.4515255093574524, 0.6510049700737, -0.5588051080703735, 0.32807955145835876, 0.14585164189338684, -0.41542014479637146, -0.37309855222702026, 0.2683469355106354, -0.46638286113739014, -1.0017598867416382, 0.8923695087432861, -0.540101170539856, 0.5699569582939148, 0.7147926092147827, -0.3675552010536194, -0.5635712742805481, 1.3407212495803833, 0.9236618876457214, 0.6182408928871155, 0.08471085131168365, -0.4320128858089447, 1.5621638298034668, -1.4752546548843384, 0.5162876844406128, 1.8821399211883545, 0.5588014721870422, 0.48827365040779114, -0.4559713304042816, -0.9202946424484253, 0.5778120160102844, -0.08159779757261276, -0.8542525172233582, -0.6571338176727295, -0.30400803685188293, -0.022944685071706772, 2.0395562648773193, -0.5369488000869751, -0.33909136056900024, 0.28139132261276245, -0.1529473513364792, 1.4490429162979126, -0.0845140591263771, -1.0219197273254395, -0.7815876007080078, -0.005549394525587559, 0.3395150899887085, 0.9979443550109863, -0.44279712438583374, 0.44715461134910583, -0.7363778948783875, -0.1802474707365036, 0.6150040626525879, -0.2940932512283325, -0.5123997330665588, 0.09915587306022644, -0.48602280020713806, 0.99167799949646, 0.844475507736206, 0.9305649399757385, -0.4103088676929474, 0.48551759123802185, 0.25485408306121826, 0.42143452167510986, -0.06326628476381302, 0.8329514861106873, -0.33140328526496887, 0.5319488048553467, -0.7368540167808533, 0.12092459201812744, 0.13843658566474915, 0.7362930178642273, -0.018971620127558708, 0.1822866052389145, -1.2314351797103882, 0.4487687945365906, 1.289365291595459, 0.06311532109975815, 0.469215452671051, -0.7949028015136719, 0.3067967891693115, -0.5368328094482422, 0.34111809730529785, -0.7046359777450562, -0.27939391136169434, -0.05332169309258461, -0.6311479806900024, -1.6940995454788208, -0.10939669609069824, -0.015474645420908928, -1.1291807889938354, 1.1263498067855835, -0.7764104604721069, -0.15020085871219635, 0.4407557547092438, 0.3149440288543701, 0.43127885460853577, 1.2067698240280151, 0.7238298654556274, -0.27163735032081604, 1.0980387926101685, -0.538732647895813, -0.9304352402687073, -1.1653389930725098, 0.9610508680343628, 0.1014721468091011, 0.14693385362625122, -0.4734356105327606, -0.8762637972831726, -0.920091450214386, -0.4664651155471802, -0.37552502751350403, -0.38565593957901, 0.7830873727798462, 1.2132683992385864, 0.2090034782886505, -0.9796576499938965, 0.7087116837501526, 0.2051406055688858, -0.17198334634304047, 0.43262535333633423, 0.13321515917778015, -0.10220970213413239, -0.8244192004203796, -1.3816490173339844, 0.7402670979499817, 0.6457531452178955, -0.4180528223514557, -0.6287802457809448, -0.13046535849571228, -1.5632833242416382, 0.03557761013507843, 0.18079786002635956, -0.7399619221687317, 1.4500300884246826, -0.3490721881389618, -1.1637927293777466, 0.8195230960845947, -0.8547106385231018, -0.12243571877479553, -0.16859684884548187, -0.38715773820877075, -0.7633313536643982, -0.5819502472877502, 0.00016564290854148567, 0.6390929818153381, 0.2837101221084595, -0.42400968074798584, -0.12009093165397644, 0.24914038181304932, -0.1596456915140152, 0.032489847391843796, -0.47595906257629395, 1.0709919929504395, -0.34545615315437317, -0.28647860884666443, 0.533279538154602, 0.7470846176147461, -0.28811347484588623, -0.1320074051618576, -0.2458534836769104, -1.2402334213256836, 0.23362253606319427, -0.046648260205984116, 0.8256455063819885, -1.0390738248825073, -0.7436667680740356, 0.036585088819265366, -0.3423769474029541, 0.12986770272254944, -1.117221713066101, 0.8698630332946777, -0.49130797386169434, 0.520550549030304, -0.26654601097106934, -1.1199300289154053, -0.16381895542144775, -0.13313226401805878, -0.6682689189910889, -0.14671960473060608, 0.138017475605011, 1.0681734085083008, -1.2727338075637817, 0.09337744861841202, 0.007887245155870914, 0.14888997375965118, -0.7089096307754517, 1.1549032926559448, -0.3747524619102478, -0.034096572548151016, -0.1441488116979599, -0.4064054787158966, 0.256829172372818, -0.07279106974601746, 0.2926117181777954, -0.5182936787605286, -0.4248819351196289, 0.5815948843955994, -0.32087814807891846, 1.2193818092346191, -0.43224188685417175, 0.7160444855690002, -0.008729728870093822, -0.36207398772239685, -0.23628270626068115, 0.776533842086792, -0.39717668294906616, -0.0880652442574501, 0.350352019071579, 0.41210660338401794, -0.5731171369552612, 0.30165278911590576, 0.5796148180961609, 0.3269914388656616, -0.06913026422262192, 0.3283514380455017, 0.6019077897071838, -0.2377506047487259, 0.6530405879020691, 0.27011585235595703, 0.7644463181495667, 0.020221445709466934, 0.5700600743293762, 0.1775076687335968, 0.5558294653892517, -0.3264639377593994, 0.2118576616048813, 0.7209954261779785, 0.9232956171035767, 0.5087596774101257, -3.8803202187409624e-05, -0.5210630893707275, 0.18320968747138977, 0.11881761252880096, 0.663852334022522, 1.6828511953353882, -0.4043126106262207, -0.2924579977989197, -0.3050112724304199, -0.5109705328941345, -0.383940190076828, 0.5709754228591919, -0.3026578724384308, 0.10192327201366425, -0.6149280071258545, -0.7851393222808838, 1.0218766927719116, 0.40501508116722107, 0.7851501107215881, -0.8594309091567993, 0.35559287667274475, -0.10507356375455856, 0.24734152853488922, -0.6920759081840515, -0.6470299959182739, 0.48872679471969604, -0.704014778137207, -0.4680930972099304, 0.04234251752495766, -0.4906923174858093, 0.5696448683738708, -0.3636603057384491, 0.802838146686554, -0.5985472202301025, 0.20044264197349548, -0.030550943687558174, 0.6144225001335144, -0.8381286263465881, -0.6849257349967957, -0.22315743565559387, 0.2534211277961731, -0.2634727358818054, 0.49372392892837524, 0.9514151811599731, 0.5232062339782715, -0.09458407759666443, -0.47939929366111755, 0.04722510278224945, 0.4051170349121094, 0.2696376144886017, 0.9144052267074585, -0.011806076392531395, 0.04555490240454674, -1.6181480884552002, 1.0821177959442139, 0.24132949113845825, -0.21357432007789612, 0.40391969680786133, -0.3073824644088745, -0.15697863698005676, 0.6304686665534973, -0.5541597604751587, -0.72502601146698, -0.10743647068738937, 0.21192285418510437, 0.0015560095198452473, -0.42894554138183594, 0.9929512739181519, 0.27565354108810425, 0.22493009269237518, 0.4868621826171875, 0.5291146039962769, 0.43386200070381165, -0.26129212975502014, 0.8762184381484985, -0.5992881059646606, 0.4839807450771332, 0.3498144745826721, 0.5742408037185669, -0.40145280957221985, -0.7565044164657593, -0.7820208668708801, -0.5783204436302185, -0.5282588601112366, -0.012906701304018497, -0.16751635074615479, -0.13728399574756622, -0.690475583076477, -0.3521799147129059, -0.1647370606660843, -1.608956217765808, 0.07559756189584732, -0.0036270308773964643, 0.1517896205186844, 0.29935503005981445, -1.0023157596588135, -1.2228116989135742, -0.58642578125, -0.5005764961242676, -0.3804522454738617, 0.21230389177799225, 0.07878242433071136, -0.11944828182458878, -0.40619853138923645, 0.3254445493221283, -0.32164424657821655, 0.8820613622665405, -0.8415983319282532, 1.4308750629425049, -0.04128772392868996, 0.29929396510124207, -0.34507787227630615, -0.021927539259195328, 0.697218120098114, -0.16123183071613312, 0.2580071985721588, -1.0368340015411377, 0.13463427126407623, -0.16047707200050354, -0.39783573150634766, 0.4067919850349426, 0.5742995142936707, 0.5784624814987183, 0.027775665745139122, -0.6215232610702515, 0.2996208369731903, 1.388054609298706, -0.6657597422599792, -0.16744662821292877, -0.23669977486133575, 0.7694825530052185, 0.4894043207168579, -0.31347817182540894, 0.08078966289758682, 0.3195955753326416, 0.17331379652023315, -0.034989167004823685, -0.1016053706407547, -0.0648464560508728, -0.6987389922142029, 0.7183513045310974, 1.2599934339523315, 0.03853313624858856, -0.47976475954055786, -1.4121752977371216, 0.573582649230957, -1.1675159931182861, -0.49840670824050903, 0.2803744673728943, 0.5090458989143372, 0.507452130317688, -0.4692675769329071, -0.6163166761398315, -0.39205294847488403, 0.6613501906394958, 0.20759887993335724, -0.12779571115970612, -0.41086238622665405, -0.2004414200782776, 0.37549126148223877, 0.17785543203353882, 0.5780653357505798, -0.8827740550041199, 1.044938325881958, 14.201623916625977, 0.5540618896484375, 0.20695503056049347, 0.4851340353488922, 0.2547309696674347, 0.9109104871749878, -0.40078139305114746, -0.1337822824716568, -0.9738695025444031, -0.8303969502449036, 0.7518823742866516, -0.16201463341712952, 0.608832597732544, 0.20160292088985443, 0.15567895770072937, 0.1342034786939621, -0.8259317278862, 0.9742864966392517, 0.45346003770828247, -1.1356037855148315, 1.1484094858169556, 0.22800695896148682, 0.2926346957683563, 0.5950013995170593, 0.5110045075416565, 1.3067882061004639, 0.3927590847015381, -0.715914785861969, 0.10083471983671188, 0.38285762071609497, 0.6116774082183838, 0.3138471841812134, 0.7682632207870483, 0.9629047513008118, -0.9375618696212769, -0.7020033001899719, -0.7208871841430664, -1.168294072151184, 0.32805439829826355, -0.020212901756167412, -0.5165187120437622, -0.4590567648410797, -0.720305860042572, 0.7510748505592346, 0.00889462511986494, 0.5783988833427429, -0.6517891883850098, 0.7321431636810303, -0.4449537694454193, 0.14284566044807434, 0.19590157270431519, 0.6312388181686401, 0.418292373418808, 0.35122713446617126, 0.11890776455402374, 0.4204809367656708, 0.125503271818161, 0.2793430685997009, -1.1175415515899658, 0.41962265968322754, -0.7747684121131897, -0.5791139602661133, -0.10645779222249985, 0.5347553491592407, 0.7332912683486938, 0.3733014464378357, -0.16496501863002777, 0.06647700816392899, 0.6917849183082581, 0.3769446611404419, 0.08087105304002762, -0.08062811195850372, 0.015536575578153133, -0.2632278800010681, -0.23182745277881622, 0.5154821872711182, -0.1043238714337349, -0.5713015794754028, -0.9109492897987366, -0.08496182411909103, 0.46720513701438904, -0.5939444899559021, -1.305361032485962, 1.0753270387649536, -0.32899603247642517, -0.2744062542915344, 0.2098975032567978, -1.1812019348144531, -0.17009496688842773, 0.5614961981773376, -1.5172690153121948, -0.7791809439659119, 0.3509884178638458, -0.012660223990678787, -0.5752882361412048, -0.2748136818408966, 1.6997324228286743, -0.16939792037010193, -0.5951380133628845, -0.1861802041530609, 0.28523629903793335, 0.5539973974227905, -0.32399407029151917, -0.5911977887153625, 0.27220770716667175, 0.39730414748191833, 0.2886199951171875, 0.5948360562324524, -0.1691512167453766, -0.08487281203269958, -0.7542345523834229, -0.2334168702363968, 1.366766333580017, -0.7004972696304321, -0.7302860617637634, -0.4510238468647003, -0.9568992257118225, 0.37034159898757935, 0.45833858847618103, -0.3419436812400818, 0.5142635703086853, 0.3903062641620636, -0.5763574838638306, 0.10004778206348419, -0.9919464588165283, 0.051392845809459686, 0.5720080733299255, -0.9791560769081116, -0.7614104747772217, 0.32942911982536316, -0.03767354041337967, -0.5318211317062378, -0.18189558386802673, -0.24636010825634003, -0.3449254035949707, 0.8807554244995117, 0.7281789779663086, -0.8109506964683533, 0.2933600842952728, 0.3807334005832672, 0.025145359337329865, -1.0381745100021362, -0.5215773582458496, -0.5574154853820801, -0.03699253872036934, -0.13213805854320526, 1.1917688846588135, -0.23934251070022583, 0.049778446555137634, 1.038266897201538, -0.011948157101869583, -0.028510648757219315, -0.8044893145561218, -0.4799114465713501, -0.1376354694366455, -0.5532851219177246, -0.17195512354373932, 0.04066872224211693, -0.2852858603000641, 0.5049294233322144, 0.7012065052986145, 0.7700380682945251, -0.45293885469436646, -0.6876525282859802, 0.2844667136669159, -0.3431036174297333, 0.19046375155448914, -0.445853590965271, 0.014408024959266186, -1.7923798561096191, 0.4759746193885803, -1.6082706451416016, 0.3601874113082886, -1.6488300561904907, -0.4153825342655182, 0.11268413066864014, -0.13257375359535217, 0.6811136603355408, 0.41095447540283203, -0.49626514315605164, -0.3359377384185791, -0.31664222478866577, -0.15910734236240387, 0.6355065107345581, 0.936801552772522, -0.5858694911003113, -0.2368616908788681, -0.022600792348384857, 0.07355831563472748, 0.4231395125389099, 0.4398094415664673, -0.8230503797531128, -0.7895041108131409, -1.837391972541809, 0.24182799458503723, -0.17688189446926117, -0.2191452980041504, -0.42105937004089355, 1.018397569656372, 0.31177419424057007, -0.2646004259586334, 0.22681181132793427, 0.563617467880249, -0.9910333752632141, -0.5873537063598633, 0.31778597831726074, -0.8177538514137268, 0.08538871258497238, 0.16988438367843628, -0.20313730835914612, -0.8268996477127075, 0.38831427693367004, -0.15775461494922638, -1.0924537181854248, -0.5992958545684814, 0.39989471435546875, -0.9139803051948547, 0.042279452085494995, -0.0197340976446867, -0.11146192997694016, -0.8840625286102295, -0.4057960510253906, 0.013204248622059822, 0.40135565400123596, -0.7397832870483398, 0.5783588290214539, 0.8908296227455139, -0.8666806221008301, -0.3898094594478607, 0.4280090034008026, 0.10170412808656693, -0.17388887703418732, 0.25481992959976196, 0.08504293859004974, -0.21289131045341492, 0.9121648669242859, 0.4909043312072754, 0.6694552898406982, -0.7802066802978516, -0.287423312664032, 0.9157509803771973, -0.4665547013282776, -0.2027989625930786, 1.1441644430160522, -0.3691996932029724, -1.086683750152588, 0.38669905066490173, -1.184856653213501, -1.0334810018539429, -0.2116445004940033, 0.6140386462211609, -0.10996504873037338, -0.2885483503341675, -0.6120553016662598, -0.2925886809825897, -0.00646919896826148, -0.05851177126169205, -0.48969048261642456, 0.770100474357605, 0.20925596356391907, -0.47561296820640564, 0.5019317865371704, 0.6315448880195618, -0.7554944157600403, -0.2966998219490051, -0.7311462163925171, -0.005574479699134827, -0.305992066860199, 0.6366887092590332, -0.5260863900184631, -0.5259775519371033, 0.602880597114563, 0.1890508085489273, -0.08415286242961884, -0.12428785115480423, -0.4585164189338684, 0.11211475729942322, 1.0312210321426392, 0.05033019930124283, -0.6649706959724426, -0.7780146598815918, 2.0737192630767822, 1.5149312019348145, -1.0198016166687012, 0.020515916869044304, -0.6862207651138306, -0.6712908148765564, 1.2212646007537842, 0.18136636912822723, 0.01940157450735569, 1.3012678623199463, -0.8219251036643982, 0.049399133771657944, -0.01830371282994747, -0.5450351238250732, -0.31917431950569153, 0.9255235195159912, 0.9930757284164429, 1.0307711362838745, 0.34536293148994446, -0.0047309305518865585, 1.0947597026824951, 0.1194518655538559, 0.34546855092048645, 0.30303889513015747, 0.6118587255477905, -0.35248011350631714, -0.1979779601097107, 0.28896257281303406, 0.9372903108596802, -0.4401394724845886, -0.6943711638450623, -0.20313751697540283, 0.8145462274551392, 0.5440758466720581, 0.6805381774902344, 0.39563992619514465, 0.3226563632488251, 0.7581886649131775, 0.6091225147247314, 0.3846215605735779, -1.2240715026855469, -0.5747015476226807, -0.6183066964149475, -0.1494230479001999, -0.0086244260892272, -0.5383599400520325, -0.4640825688838959, -0.21259771287441254, 0.16834905743598938, 0.19090908765792847, 0.09985507279634476, 0.3736571669578552, 0.9821137189865112, 0.40267935395240784, -0.18145005404949188, -0.5029032826423645, -0.06779184192419052, -0.4359654486179352, -1.5392106771469116, 0.0324011966586113, -0.734947144985199, -0.31883445382118225, -0.18444553017616272, -0.3037683069705963, -0.282125324010849]}, "authors": [{"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "153345698", "name": "Tianxiang Sun"}, {"authorId": "26339093", "name": "Yige Xu"}, {"authorId": "95329799", "name": "Yunfan Shao"}, {"authorId": "145493218", "name": "Ning Dai"}, {"authorId": "1790227", "name": "Xuanjing Huang"}], "references": [{"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "d73d8edbc804b7848b4a5a1a11b6d33f9f42fa94", "title": "Elbert: Fast Albert with Confidence-Window Based Early Exit"}, {"paperId": "03662672662f49e6b06148e94b407b60b0bb72f3", "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models"}, {"paperId": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3", "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit"}, {"paperId": "9631f5bc3e6d345db42425824f1e7d21d35efa0c", "title": "Early Exiting with Ensemble Internal Classifiers"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "209f9bde2dee7cf1677801586562ffe56d435d38", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"}, {"paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"}, {"paperId": "a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9", "title": "How many data points is a prompt worth?"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e", "title": "WARP: Word-level Adversarial ReProgramming"}, {"paperId": "7eda139d737eea10fc1d95364327a41ec0cee4a4", "title": "CoLAKE: Contextualized Language and Knowledge Embedding"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "e34fdd75a78d08e1ff901ed466e6b1aaf4915750", "title": "FlauBERT : des mod\u00e8les de langue contextualis\u00e9s pr\u00e9-entra\u00een\u00e9s pour le fran\u00e7ais (FlauBERT : Unsupervised Language Model Pre-training for French)"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8747f028acccde9ee7c35c858da8091613d3e574", "title": "Faster Depth-Adaptive Transformers"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "2ffcf8352223c95ae8cef4daaec995525ecc926b", "title": "Adversarial Training for Large Neural Language Models"}, {"paperId": "929b4775b6896634e11a8feb0ca4ca64ef7b3e24", "title": "Extractive Summarization as Text Matching"}, {"paperId": "c5cc2340766d68ece08bb1520d357bcf8c03ad48", "title": "The Right Tool for the Job: Matching Model and Instance Complexities"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"}, {"paperId": "dc0ce66f5ab4c5173cdef951649044e4c4c05076", "title": "BERT-ATTACK: Adversarial Attack against BERT Using BERT"}, {"paperId": "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286", "title": "A Survey on Contextual Embeddings"}, {"paperId": "33496cb3a5623925267528fa6b726f015e4dcda2", "title": "Data Augmentation using Pre-trained Transformer Models"}, {"paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"}, {"paperId": "501a8b86428563539667e8117cd8409674ef97c3", "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"}, {"paperId": "738215a396f6eee1709c6b521a6199769f0ce674", "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"}, {"paperId": "efe638a32c6bd9ad24a233784008bfe5b33cfc83", "title": "Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "12aa5c0c9fa3077f41f153cdccfa2aacf8940be3", "title": "From static to dynamic word representations: a survey"}, {"paperId": "dc373d5e108a90a70f55285a852a32706adbeb45", "title": "Incorporating BERT into Neural Machine Translation"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "4243555758433880a67b15b50f752b1e2a8c4609", "title": "UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"}, {"paperId": "704a1a4ff7b6fed65b0c49ef87b6845d60755fa7", "title": "TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval"}, {"paperId": "01ff40e32d810c09535bcfd21f315f3bc9248784", "title": "Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference"}, {"paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"}, {"paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"}, {"paperId": "1359d2ef45f1550941e22bf046026c89f6edf315", "title": "AraBERT: Transformer-based Model for Arabic Language Understanding"}, {"paperId": "157cdb1634f3f2c797e1c75d9eda259826424982", "title": "Adversarial Training for Aspect-Based Sentiment Analysis with BERT"}, {"paperId": "7cf8510d5905bd8a63f1e098e05ab591d689e0fd", "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"}, {"paperId": "8d00049c345b9c8cc76ea2ea2565f8bb69f6b683", "title": "Retrospective Reader for Machine Reading Comprehension"}, {"paperId": "495da6f19baa09c6db3697d839e10432cdc25934", "title": "Multilingual Denoising Pre-training for Neural Machine Translation"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b", "title": "RobBERT: a Dutch RoBERTa-based Language Model"}, {"paperId": "c6a84615bc36486cd0170f8a3e1b7e5ec8f5344e", "title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation"}, {"paperId": "c7fc1cac162c0e2a934704184c7554fd6b6253f0", "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model"}, {"paperId": "a4d5e425cac0bf84c86c0c9f720b6339d6288ffa", "title": "BERTje: A Dutch BERT Model"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "3b2538f84812f434c740115c185be3e5e216c526", "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study"}, {"paperId": "477d66dcd2c08243dcc69822d6da7ec06393773a", "title": "Multilingual is not enough: BERT for Finnish"}, {"paperId": "f1957038e9ded19108d3c71340d7462152b70f25", "title": "Integrating Graph Contextualized Knowledge into Pre-trained Language Models"}, {"paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968", "title": "How Can We Know What Language Models Know?"}, {"paperId": "f67fcbb1aec92ae293998ddfd904f61a31bef334", "title": "Inducing Relational Knowledge from BERT"}, {"paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"}, {"paperId": "c12e6c65e1de5d3993c5b65d0e234ae1f60c85ae", "title": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection"}, {"paperId": "b61c6405f4de381758e8b52a20313554d68a9d85", "title": "CamemBERT: a Tasty French Language Model"}, {"paperId": "9df6cc3bf35b70613abe95ad269ac74f169c9080", "title": "BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA"}, {"paperId": "68c1bf884f0fc0e86641466a1f1fa67e79f16a17", "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"}, {"paperId": "849b74ecce69108d36bf87ea9b87219cbfa10562", "title": "Negated LAMA: Birds cannot fly"}, {"paperId": "348be8e64565a3df80d743f0580b63d3fbb49f35", "title": "SentiLARE: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis"}, {"paperId": "497b1a8711d5f5f0a27254fd4a1bba16ad0d9625", "title": "SentiLR: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6007bd2a34385132a7885b934d90b519a1f65bba", "title": "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"}, {"paperId": "5534d774a06039e13b72876c21d39949132b512b", "title": "Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents"}, {"paperId": "20b847537d3b7b9661733c1770c5faab3c0e2215", "title": "Biomedical Named Entity Recognition with Multilingual BERT"}, {"paperId": "80e949e0e58e06c6f4f75ac4a2d7216b0fa1cfb8", "title": "Recycling a Pre-trained BERT Encoder for Neural Machine Translation"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "2be2f27c079663d3e3a769bcb04b0d341e76a707", "title": "SpeechBERT: Cross-Modal Pre-trained Language Model for End-to-end Spoken Question Answering"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "530a059cb48477ad1e3d4f8f4b153274c8997332", "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"}, {"paperId": "4585611042d2be0d997ee135e3fe219d668db9ec", "title": "Depth-Adaptive Transformer"}, {"paperId": "b85d339e49399966d629973c889e8edfca56517c", "title": "A Mutual Information Maximization Perspective of Language Representation Learning"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "22b6e7e7063f596c213d7094ca991a29426be241", "title": "Progress Notes Classification and Keyword Extraction using Attention-based Deep Learning Models with BERT"}, {"paperId": "327d7e55d64cb34d55bd3a3fe58233c238a312cd", "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models"}, {"paperId": "a23975683b044a871e39afb7781662f5eccaa4ba", "title": "Exploiting BERT for End-to-End Aspect-based Sentiment Analysis"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "d01fa0311e8e15b8b874b376123530c815f52852", "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "740e4599b0e3113ad804cee4394c7fa7c0e96ca5", "title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections"}, {"paperId": "54416048772b921720f19869ed11c2a360589d03", "title": "UNITER: Learning UNiversal Image-TExt Representations"}, {"paperId": "cc3be86706e0aff342e67b6ab84293fddc98423d", "title": "MobileBERT: Task-Agnostic Compression of BERT by Progressive Knowledge Transfer"}, {"paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97", "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"paperId": "025a0dc4a2a98742f1b410b6318a46de2c854b22", "title": "Learning Video Representations using Contrastive Bidirectional Transformer"}, {"paperId": "663f4cc30a69aee07e291299d196806ead12d520", "title": "Technical report on Conversational Question Answering"}, {"paperId": "aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47", "title": "Cross-Lingual Natural Language Generation via Pre-Training"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "06a73ad09664435f8b3cd90293f4e05a047cf375", "title": "K-BERT: Enabling Language Representation with Knowledge Graph"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8", "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"}, {"paperId": "3f9df96b26c42dea6dd6cad64557a3b7d698ea90", "title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning"}, {"paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations"}, {"paperId": "0fc85e11928eb15d3c3a2fa737490ffc7b3986e2", "title": "Transformer to CNN: Label-scarce distillation for efficient text classification"}, {"paperId": "e14c93e69cbf9645abb70cef09391f21f644d6d8", "title": "Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity"}, {"paperId": "faaaf0a3c6e8283465bb719f1ee4999479a2624e", "title": "Informing Unsupervised Pretraining with External Linguistic Knowledge"}, {"paperId": "65f788fb964901e3f1149a0a53317535ca85ed7d", "title": "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"}, {"paperId": "f98e135986414cccf29aec593d547c0656e4d82c", "title": "Commonsense Knowledge Mining from Pretrained Models"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "4d6ca284c20a886919e517017a5f8dca97d1d50c", "title": "Specializing Word Embeddings (for Parsing) by Information Bottleneck"}, {"paperId": "c93b2d64fce8737506757bbce51e17b533f9285b", "title": "On the use of BERT for Neural Machine Translation"}, {"paperId": "2f9d4887d0022400fc40c774c4c78350c3bc5390", "title": "Small and Practical BERT Models for Sequence Labeling"}, {"paperId": "6c1beae31b92c70b42ebeb99e5598d73bff6eea5", "title": "NEZHA: Neural Contextualized Representation for Chinese Language Understanding"}, {"paperId": "4f2841cf0ed8edd5f9d2b7f76b95ec2a8674afb1", "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "93ad19fbc85360043988fa9ea7932b7fdf1fa948", "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "88051a6dce3b67541d8096647da2f6d31daa9e9a", "title": "Latent Relation Language Models"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "3caf34532597683c980134579b156cd0d7db2f40", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP"}, {"paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029", "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"}, {"paperId": "3012f85c312412a6ac665cb1cc5180ad194332c8", "title": "Multi-Task Self-Supervised Learning for Disfluency Detection"}, {"paperId": "772717eb2e369cd68c11b7da7aa779450dced9d0", "title": "SenseBERT: Driving Some Sense into BERT"}, {"paperId": "8492269d2bb474d57d6def97efcf86c42735554a", "title": "BERT-based Ranking for Biomedical Entity Normalization"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "d56c1fc337fb07ec004dc846f80582c327af717c", "title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"}, {"paperId": "b82153bf85d5d1edd3f170aace830e5328ca9ed0", "title": "Fusion of Detected Objects in Text for Visual Question Answering"}, {"paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb", "title": "Text Summarization with Pretrained Encoders"}, {"paperId": "fedea6d90747397cd144d1a419edf234110895d8", "title": "Mask and Infill: Applying Masked Language Model for Sentiment Transfer"}, {"paperId": "a0e49f65b6847437f262c59d0d399255101d0b75", "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"}, {"paperId": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"}, {"paperId": "335613303ebc5eac98de757ed02a56377d99e03a", "title": "What Does BERT Learn about the Structure of Language?"}, {"paperId": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2", "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "367c41f623f86e75d3154f6cab5b749cb7eb06b5", "title": "Searching for Effective Neural Extractive Summarization: What Works and What\u2019s Next"}, {"paperId": "e7046bf945ad6326537a1ac78a96fd2f45acc900", "title": "Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension"}, {"paperId": "2ff41a463a374b138bb5a012e5a32bc4beefec20", "title": "Pre-Training with Whole Word Masking for Chinese BERT"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "96901acc92d68350443774596fa2b38bc522a0ce", "title": "Barack\u2019s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling"}, {"paperId": "f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c", "title": "Contrastive Bidirectional Transformer for Temporal Representation Learning"}, {"paperId": "0de0a44b859a3719d11834479112314b4caba669", "title": "A Multiscale Visualization of Attention in the Transformer Model"}, {"paperId": "afd110eace912c2b273e64851c6b4df2658622eb", "title": "Visualizing and Measuring the Geometry of BERT"}, {"paperId": "809cc93921e4698bde891475254ad6dfba33d03b", "title": "How Multilingual is Multilingual BERT?"}, {"paperId": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad", "title": "Is Attention Interpretable?"}, {"paperId": "455a8838cde44f288d456d01c76ede95b56dc675", "title": "A Structural Probe for Finding Syntax in Word Representations"}, {"paperId": "82d40215de43fbc2aa3b0f8c6ebba73f35e64c9b", "title": "An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "cc94d15ba408c260c8fe4fa4f1cb6797a996dd21", "title": "Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "07c53193b50aa0b108b14b9edfbef64ea1e9119b", "title": "Story Ending Prediction by Transferable BERT"}, {"paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee", "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "a022bda79947d1f656a1164003c1b3ae9a843df9", "title": "How to Fine-Tune BERT for Text Classification?"}, {"paperId": "dca404a59d66f196f55789e28033eaec85da6a91", "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "145b8b5d99a2beba6029418ca043585b90138d12", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "203b543bfa1e564bb80ff4229b43174d7c71b0c0", "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "7ebed46b7f3ec913e508e6468304fcaea832eda1", "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding"}, {"paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3", "title": "ERNIE: Enhanced Representation through Knowledge Integration"}, {"paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"}, {"paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2", "title": "Publicly Available Clinical BERT Embeddings"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "a4bc4b98a917174ac2ab14bd5e66d64306079ab5", "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "b5aa927c906101b3f8854a29f374551e3ea64474", "title": "Pre-trained language model representations for language generation"}, {"paperId": "0de47f354468283efc7765ec0b3588b2ae483c77", "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence"}, {"paperId": "9f1c5777a193b2c3bb2b25e248a156348e5ba56d", "title": "Cloze-driven Pretraining of Self-attention Networks"}, {"paperId": "8659bf379ca8756755125a487c43cfe8611ce842", "title": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "f6fbb6809374ca57205bd2cf1421d4f4fa04f975", "title": "Linguistic Knowledge and Transferability of Contextual Representations"}, {"paperId": "578e050d6e007797d032a07e712142035f2666dc", "title": "An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models"}, {"paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "title": "Attention is not Explanation"}, {"paperId": "403227333329b36183004f04db72362b604adef3", "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "efeab0dcdb4c1cce5e537e57745d84774be99b9a", "title": "Assessing BERT's Syntactic Abilities"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "188024469a2443f262b3cbb5c5d4a96851949d68", "title": "Conditional BERT Contextual Augmentation"}, {"paperId": "b47381e04739ea3f392ba6c8faaf64105493c196", "title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"paperId": "a3143eaa68040d366848a9c324b29d3f56f97a5d", "title": "Shallow-Deep Networks: Understanding and Mitigating Network Overthinking"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "af3825437b627db1a99f946f7aa773ba8b03befd", "title": "Learning deep representations by mutual information estimation and maximization"}, {"paperId": "421fc2556836a6b441de806d7b393a35b6eaea58", "title": "Contextual String Embeddings for Sequence Labeling"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "0b43f66d99f43017db3e2975d38f1d86d1bde1ca", "title": "A Multi-task Approach to Learning Multilingual Representations"}, {"paperId": "6411da05a0e6f3e38bcac0ce57c28038ff08081c", "title": "Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d393943a873ead524069d0f7f55acef05cc9ba45", "title": "Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling"}, {"paperId": "93b4cc549a1bc4bc112189da36c318193d05d806", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "8dd85e38445a5ddb5dd71cabc3c4246de30c014f", "title": "A Survey of Model Compression and Acceleration for Deep Neural Networks"}, {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "263210f256603e3b62476ffb5b9bbbbc6403b646", "title": "What do Neural Machine Translation Models Learn about Morphology?"}, {"paperId": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "title": "Semi-supervised sequence tagging with bidirectional language models"}, {"paperId": "896de8418884f4aab1ae4a60027500c9e8baffc3", "title": "BranchyNet: Fast inference via early exiting from deep neural networks"}, {"paperId": "bd345877856dc83c2c10c125dbf0f41e2bde38b1", "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding"}, {"paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc", "title": "Unsupervised Pretraining for Sequence to Sequence Learning"}, {"paperId": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "e2dba792360873aef125572812f3673b1a85d850", "title": "Enriching Word Vectors with Subword Information"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b", "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "96acb1c882ad655c6b8459c2cd331803801446ca", "title": "Representation Learning of Knowledge Graphs with Entity Descriptions"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "318b558717ff9a4a996e45368b26a1233f03d1d7", "title": "Aligning Knowledge and Text Embeddings by Entity Descriptions"}, {"paperId": "adcfebbe2a1960e5a23243d1a5f3837832109ff1", "title": "Distributional vectors encode referential attributes"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "title": "Character-Aware Neural Language Models"}, {"paperId": "1f600f213dbbd70f06093438855f39022957b4bf", "title": "Long Short-Term Memory Over Recursive Structures"}, {"paperId": "311d7757c7820ff4b28958403ae077eb9458d91f", "title": "How Well Do Distributional Models Capture Different Types of Semantic Knowledge?"}, {"paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors"}, {"paperId": "c7188396767227c0d9ed087a7b077f22fccd7372", "title": "Bilingual Word Representations with Monolingual Quality in Mind"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "f0efb4f8e1e5957bb252d9d530202b1cef9b0494", "title": "Knowledge Graph and Text Jointly Embedding"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "f3de86aeb442216a8391befcacb49e58b478f512", "title": "Distributed Representations of Sentences and Documents"}, {"paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "title": "A Convolutional Neural Network for Modelling Sentences"}, {"paperId": "9d3aaa919c78c06f24588d97ed1028d51860b321", "title": "Improving Vector Space Word Representations Using Multilingual Correlation"}, {"paperId": "53ca064b9f1b92951c1997e90b776e95b0880e52", "title": "Learning word embeddings efficiently with noise-contrastive estimation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "title": "Linguistic Regularities in Continuous Space Word Representations"}, {"paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b", "title": "Representation Learning: A Review and New Perspectives"}, {"paperId": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch"}, {"paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972", "title": "A Survey on Transfer Learning"}, {"paperId": "268b8f10a45e71f63daab6403bb453da31ae28a7", "title": "Melting of Peridotite to 140 Gigapascals"}, {"paperId": "e3ce36b9deb47aa6bb2aa19c4bfa71283b505025", "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"}, {"paperId": "0d2336389dff3031910bd21dd1c44d1b4cd51725", "title": "Why Does Unsupervised Pre-training Help Deep Learning?"}, {"paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9", "title": "Model compression"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "3106e66537a0c8f53278e553bcb38f0b0992ec0e", "title": "Distributed Representations"}, {"paperId": "4bb08f30bb2c83334b21fbbc68c3f2622d4fb04b", "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"}, {"paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Pre-trained Models for Natural Language Advances"}, {"paperId": "08ee34a64247c0fe3c22b9f3c0848eb921041a8d", "title": "Supplementary Material: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": null, "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"}, {"paperId": null, "title": "Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are fewshot learners"}, {"paperId": null, "title": "Adversarial QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 31 training for large neural language models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "a3347bbd82938788ec085772813c095de17a0b37", "title": "Is BERT Really Robust? Natural Language Attack on Text Classi\ufb01cation and Entailment"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "\u00c9ric Villemonte de la Clergerie"}, {"paperId": null, "title": "PALs: Projected attention layers for efficient adaptation in multi-task learning"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c50dca78e97e335d362d6b991ae0e1448914e9a3", "title": "Reducing the Dimensionality of Data with Neural"}, {"paperId": null, "title": "BART: denoising sequence-to-24"}, {"paperId": null, "title": "Pre-training text encoders as discriminators rather than generators"}]}