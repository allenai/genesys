{"paperId": "3f8332a84034eed88f46c9720df7f1260ce0d721", "abstract": "Language models, characterized by their black-box nature, often hallucinate and display sensitivity to input perturbations, causing concerns about trust. To enhance trust, it is imperative to gain a comprehensive understanding of the model's failure modes and develop effective strategies to improve their performance. In this study, we introduce a methodology designed to examine how input perturbations affect language models across various scales, including pre-trained models and large language models (LLMs). Utilizing fine-tuning, we enhance the model's robustness to input perturbations. Additionally, we investigate whether exposure to one perturbation enhances or diminishes the model's performance with respect to other perturbations. To address robustness against multiple perturbations, we present three distinct fine-tuning strategies. Furthermore, we broaden the scope of our methodology to encompass large language models (LLMs) by leveraging a chain of thought (CoT) prompting approach augmented with exemplars. We employ the Tabular-NLI task to showcase how our proposed strategies adeptly train a robust model, enabling it to address diverse perturbations while maintaining accuracy on the original dataset.", "venue": "", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study introduces a methodology designed to examine how input perturbations affect language models across various scales, including pre-trained models and large language models (LLMs), and presents three distinct fine-tuning strategies to address robustness against multiple perturbations."}, "embedding": {"model": "specter_v2", "vector": [0.009029447101056576, 0.2299349009990692, -0.10747536271810532, 0.08883296698331833, -0.5020996928215027, -0.7104188799858093, 0.8401443362236023, -0.29561662673950195, -0.5609814524650574, -0.0009736270876601338, 0.2396915853023529, -0.4705617427825928, 0.3057284951210022, 0.46666401624679565, -0.5020051002502441, 0.5471306443214417, -0.7114928364753723, 0.38834235072135925, -0.3509508967399597, -0.4479128122329712, -0.32134148478507996, -0.5935739874839783, -0.5926576852798462, 0.004962649662047625, 0.5476076602935791, 0.242871955037117, -0.3339574933052063, 1.1015369892120361, 0.006201178301125765, 0.33259740471839905, 0.20875146985054016, -0.347590833902359, 0.26081210374832153, 0.2569316029548645, -0.038069453090429306, -0.10811939090490341, 0.3943271338939667, -0.5942056179046631, -0.3066858649253845, 0.8767997622489929, -0.32577863335609436, 0.2834199070930481, 0.5397328734397888, -0.5575236082077026, -0.8680408000946045, 0.6074261665344238, 0.8806225657463074, 0.8223657011985779, 0.10388388484716415, -0.30559876561164856, 1.14130437374115, -1.5273076295852661, 0.30247628688812256, 1.550115942955017, 0.7109081149101257, 0.48172470927238464, -0.6043873429298401, -0.871153712272644, 0.6432480812072754, -0.2946598529815674, -1.0196363925933838, -0.3897669017314911, -0.4717297852039337, -0.2704707980155945, 1.6696279048919678, -0.4755612015724182, -0.5983651876449585, 0.6406785249710083, 0.3313072919845581, 1.3062565326690674, 0.021990466862916946, -0.793175220489502, -0.19188295304775238, 0.45054706931114197, 0.020574433729052544, 0.43431997299194336, -0.32850000262260437, 0.45167112350463867, -0.8001556396484375, -0.45370909571647644, 0.10093152523040771, -0.6246280074119568, -0.2758268415927887, 0.1902814656496048, -0.471716970205307, 0.6453781723976135, 0.08619696646928787, 0.8547033071517944, 0.0888092964887619, 0.5300795435905457, 0.46892455220222473, 0.8103285431861877, 0.36597681045532227, 0.5708049535751343, -0.5855438709259033, 0.26541540026664734, -0.6484727263450623, 0.41428142786026, 0.3960871398448944, 0.6942499279975891, -0.5609205365180969, 0.16963328421115875, -0.7820158004760742, -0.015338927507400513, 1.4290403127670288, -0.025817010551691055, 0.3941747844219208, -0.29900267720222473, 0.2636522948741913, -0.8764176368713379, 0.7738614082336426, -0.7248596549034119, -0.3566369414329529, -0.31778478622436523, -0.4639115333557129, -1.2540932893753052, -0.07156345248222351, 0.19335967302322388, -0.8554827570915222, 0.9686075448989868, -0.34540319442749023, -0.24571554362773895, 0.19735175371170044, 0.4733750820159912, 0.6670485138893127, 0.5834808349609375, 0.2397712916135788, 0.015698181465268135, 0.5702269077301025, -0.30444008111953735, -0.6514695882797241, -1.0710406303405762, 1.0144319534301758, -0.7747151851654053, 0.4920578598976135, -0.18857231736183167, -1.1608940362930298, -0.8670878410339355, -0.8559379577636719, 0.057829517871141434, -0.22022444009780884, 0.3671626150608063, 0.4961979389190674, 0.2882869243621826, -1.0430095195770264, 0.5289337038993835, 0.056650348007678986, -0.35084542632102966, -0.12681075930595398, 0.22918567061424255, 0.10260419547557831, -0.6417055130004883, -1.4199817180633545, 0.5997593402862549, 0.3984948694705963, -0.41743677854537964, -0.3777492046356201, -0.22733375430107117, -1.0646822452545166, -0.3015238046646118, 0.3617006838321686, -0.33204251527786255, 1.553356409072876, -0.1507120132446289, -0.9193627834320068, 0.15001702308654785, -0.018253006041049957, 0.4497414529323578, 0.3410220444202423, -0.020178603008389473, -0.7742840647697449, -0.6033744215965271, -0.07180549949407578, 0.4403362572193146, 0.5091694593429565, -0.32409417629241943, 0.20064984261989594, 0.5625935196876526, -0.30611875653266907, -0.12075593322515488, -0.12135118246078491, 0.8824366331100464, -0.09235681593418121, -0.35378745198249817, 0.3931335508823395, 0.668079674243927, 0.03396147862076759, -0.20322570204734802, -0.517119288444519, -0.9158045649528503, 0.5698673129081726, -0.2923315763473511, 1.1683039665222168, -0.8949936628341675, -0.8060083985328674, 0.013142074458301067, -0.27043771743774414, -0.2793695032596588, -0.8496795296669006, 0.9879148006439209, -0.29713883996009827, 1.1477608680725098, -0.032158900052309036, -1.2868380546569824, 0.05581476539373398, -0.48491987586021423, -0.6867415308952332, -0.2431916743516922, 0.1466635912656784, 1.2095024585723877, -0.602241039276123, 0.2711033821105957, -0.25509113073349, 0.20899006724357605, -0.9956387281417847, 1.1404675245285034, -0.7374762296676636, 0.4892966151237488, 0.21565119922161102, 0.010406511835753918, 0.03867907449603081, -0.4564721882343292, 0.15175089240074158, -0.5109442472457886, -0.02449720911681652, 0.17604035139083862, -0.4481823742389679, 0.9810183644294739, -0.027431100606918335, 0.11047463864088058, -0.19465568661689758, -0.17170096933841705, -0.18374837934970856, 0.8213711380958557, -0.4818167984485626, -0.3050904870033264, 0.25473839044570923, 0.672208845615387, -0.4371967017650604, 0.33983951807022095, 0.8524686694145203, 0.40107274055480957, -0.5344228148460388, 0.4932769536972046, 0.3488711416721344, -0.4932433068752289, 0.519370973110199, 0.3457478880882263, 0.5294855833053589, 0.2908361852169037, 0.39607831835746765, -0.045827075839042664, 0.5064182877540588, -0.6787054538726807, -0.3130740225315094, 0.7604115009307861, 0.36086463928222656, 0.5507205128669739, 0.2334098368883133, -0.7707729339599609, -0.23549410700798035, -0.1472117304801941, 0.40444937348365784, 1.564011573791504, -0.3362870514392853, -0.11910845339298248, -0.5297064781188965, -0.30036044120788574, -0.08957371860742569, 0.2837732434272766, -0.6759803891181946, -0.1296941488981247, -0.2648918032646179, -1.0093679428100586, 0.8286533355712891, -0.04530828446149826, 0.43451783061027527, -0.5350626707077026, 0.0817793682217598, -0.01782844215631485, 0.2459534853696823, -0.7370699048042297, -0.9326629638671875, 0.18834088742733002, -0.34875598549842834, -0.17114557325839996, 0.1916372925043106, 0.02708646096289158, 0.12798628211021423, -0.5926888585090637, 0.777698814868927, -0.21974754333496094, 0.039735324680805206, 0.32453691959381104, 0.5646201372146606, -0.5720297694206238, -1.5177810192108154, -0.12422994524240494, 0.45469945669174194, -0.28566354513168335, -0.011044797487556934, 0.9138919711112976, 0.33494845032691956, 0.5386217832565308, -0.8162469267845154, 0.10182114690542221, -0.013506169430911541, 0.31756484508514404, 0.5173410177230835, -0.24773544073104858, 0.3390544652938843, -1.1187316179275513, 1.515213966369629, -0.07864180207252502, -0.5054889917373657, 0.6423186659812927, -0.7908552885055542, -0.4725010395050049, 0.8206788897514343, -0.9202718734741211, -0.2175813913345337, -1.4126213788986206, 0.6188526749610901, 0.26816534996032715, 0.04523150995373726, 0.687585711479187, 0.5589687824249268, 0.05476224049925804, 0.6456186771392822, 0.4701537787914276, 0.3783595860004425, -0.1861673891544342, 0.6452214121818542, -0.6949926614761353, 0.26162511110305786, 0.17784927785396576, 0.5291014909744263, -0.2849200665950775, -0.6770281195640564, -0.45077741146087646, -0.31169593334198, 0.13591483235359192, -0.05141417309641838, -0.43819940090179443, -0.25941991806030273, -0.6937889456748962, -0.624624490737915, 0.1032940074801445, -1.0738438367843628, -0.14313939213752747, 0.3709332346916199, -0.2713327407836914, 0.17003460228443146, -1.0966676473617554, -1.377320647239685, -0.582955002784729, -0.5560916066169739, -0.9434420466423035, 0.33770865201950073, 0.06476116180419922, -0.6893337368965149, -0.5717846751213074, 0.18075193464756012, -0.24790990352630615, 1.1334702968597412, -0.6654649972915649, 1.1054867506027222, -0.22916951775550842, -0.0763925090432167, -0.4866078495979309, 0.07861834764480591, 0.4487704932689667, -0.11756478250026703, 0.6489561200141907, -1.1385256052017212, -0.4157339036464691, 0.036765340715646744, -0.4499828517436981, -0.27704498171806335, 0.0729011669754982, 0.04857078939676285, -0.12473872303962708, -0.23933559656143188, 0.2433290034532547, 1.1090182065963745, -0.5832664966583252, -0.6248614192008972, 0.30962124466896057, 0.7497286796569824, 0.2867867648601532, -0.3273165225982666, 0.43582683801651, 0.4419320821762085, 0.18330663442611694, 0.01634318381547928, 0.17224512994289398, 0.4698142409324646, -0.7871127724647522, 0.7576614618301392, 1.5589854717254639, 0.670632004737854, -0.3437880575656891, -1.0973985195159912, 0.29627537727355957, -1.1021864414215088, -0.22868363559246063, 0.8547478318214417, 0.8761188983917236, 0.5133569240570068, -0.1943228542804718, -0.5481882095336914, -0.16901661455631256, 0.1044732928276062, 0.2277752012014389, -0.5053001642227173, -0.5306808352470398, 0.05189377814531326, -0.12005311995744705, -0.2531908452510834, 0.49569016695022583, -0.4300594627857208, 0.6128911375999451, 14.934728622436523, 0.656320333480835, 0.17766481637954712, 0.9395065903663635, 0.6770636439323425, 0.4320031702518463, -0.7372152805328369, -0.33116626739501953, -0.9510185718536377, -0.3379592299461365, 1.1316698789596558, -0.1197214275598526, 0.8542246222496033, -0.30347707867622375, 0.22858604788780212, -0.06795376539230347, -0.4204648733139038, 0.2027340531349182, 0.6544132828712463, -1.1352107524871826, 0.49636271595954895, -0.0385931096971035, 0.7564099431037903, 0.6767434477806091, 0.7150261402130127, 0.8153374195098877, 0.36707863211631775, -0.6130152344703674, 0.8205747008323669, -0.029503801837563515, 0.8903236389160156, -0.014421450905501842, 0.4795058071613312, 0.8922170996665955, -0.7778348922729492, -0.2230546772480011, -0.43850138783454895, -1.2523386478424072, 0.11020585894584656, -0.2594691514968872, -0.8689101338386536, -0.657168447971344, -0.23946650326251984, 0.3318155109882355, -0.2810553014278412, 0.12715639173984528, -0.03560071438550949, 0.7577102184295654, -0.1803358942270279, 0.15754269063472748, -0.0033224483486264944, 0.6934080123901367, 0.30599504709243774, -0.3220686614513397, -0.04746677353978157, -0.4449315369129181, 0.10207203030586243, 0.5826414227485657, -0.7482205033302307, 0.21293576061725616, -0.6112874150276184, -0.4000050723552704, 0.04750997573137283, 0.35683685541152954, 0.32956477999687195, 0.4002913236618042, -0.4370776414871216, 0.17055800557136536, 0.6771381497383118, 0.6534340977668762, 0.0998854711651802, 0.6750057935714722, 0.35350501537323, -0.5272525548934937, -0.5282774567604065, 0.32431575655937195, -0.0907202959060669, -0.6122554540634155, -0.40784597396850586, -0.7157881855964661, 0.08967044949531555, -0.8764715194702148, -1.0392625331878662, 0.8411023020744324, -0.14754873514175415, -0.4312632083892822, 0.11610429733991623, -0.5718039870262146, -0.13937528431415558, 0.5616243481636047, -1.4162678718566895, -1.0602142810821533, 0.6838594675064087, -0.5030165314674377, -0.5844197273254395, -0.2452179193496704, 1.5032751560211182, 0.03797582536935806, -0.6403363347053528, 0.26295584440231323, 0.5212445259094238, -0.08683138340711594, 0.2338971197605133, -0.6556184887886047, 1.2760009765625, 0.4025733768939972, -0.07759793102741241, 0.7678365111351013, 0.2229413539171219, -0.2528115212917328, -0.8472424745559692, -0.0433289110660553, 0.8776496052742004, -1.1229082345962524, -0.050832029432058334, -0.6297599077224731, -0.9721606969833374, 0.18229392170906067, 0.28810325264930725, -0.24485278129577637, 0.37077075242996216, -0.01356561854481697, -0.7330536246299744, 0.14908267557621002, -0.9720351696014404, 0.1439020186662674, 0.03382197394967079, -1.1555211544036865, -0.12529298663139343, 0.4025842845439911, 0.38055261969566345, -1.1709502935409546, -0.46392300724983215, -0.046682413667440414, 0.07149495929479599, 0.10879502445459366, 0.5114059448242188, -0.6341834664344788, 0.2697996199131012, 0.4969943165779114, -0.3955909013748169, -0.8584915399551392, 0.01740938238799572, -0.8796883821487427, 0.2030230313539505, -0.0647016242146492, 1.0497584342956543, -0.5325835943222046, -0.4262411892414093, 1.2361836433410645, 0.5667955875396729, -0.2522876560688019, -0.7856965065002441, -0.17094597220420837, 0.32271042466163635, -0.8282889127731323, 0.1439332515001297, -0.293060302734375, -0.1748809814453125, -0.03263137489557266, 0.2213633507490158, 0.9101690053939819, -0.43473172187805176, -0.9353861212730408, 0.45936331152915955, -0.09649160504341125, -0.1694970428943634, -0.2977364659309387, 0.08616817742586136, -1.0656366348266602, -0.1290244311094284, -0.9584906697273254, 0.05406896397471428, -0.9085819125175476, -0.49109241366386414, 0.2561418116092682, -0.14177332818508148, -0.09176827222108841, 0.46997687220573425, -0.12327264249324799, -0.05257698893547058, 0.0026266328059136868, -0.1602625548839569, 0.6686602234840393, 1.098753571510315, -0.6578822731971741, 0.045331891626119614, 0.04375951737165451, -0.14988309144973755, 0.19462040066719055, 0.3451269567012787, -0.54869145154953, -0.7386319041252136, -1.294800877571106, 0.5835651755332947, -0.30853188037872314, -0.12064981460571289, -0.43144381046295166, 0.7906932830810547, 0.35439345240592957, -0.08874109387397766, 0.31083476543426514, 0.24447539448738098, -0.9309986233711243, -0.23961851000785828, 0.27897587418556213, -0.620225191116333, 0.6476228833198547, 0.7729801535606384, -0.48130151629447937, -0.2710587680339813, 0.7304348945617676, -0.16768839955329895, -0.8714860677719116, -0.32872167229652405, 0.31083419919013977, -0.7958644032478333, 0.29426032304763794, -0.21575140953063965, 0.042553845793008804, -1.0371347665786743, -0.6362550854682922, 0.0608496218919754, 0.40126627683639526, -0.20487737655639648, 1.037282943725586, 0.5946877002716064, -0.8636418581008911, 0.0421222560107708, 0.7502588033676147, 0.02549980953335762, -0.1827917844057083, 0.31403765082359314, 0.2708631157875061, -0.34194087982177734, 0.5443724989891052, 0.6704223155975342, 0.2711924910545349, -0.746164858341217, -0.3658771514892578, 0.9190283417701721, -0.4632814824581146, 0.18220101296901703, 1.332513451576233, -0.1873856633901596, -1.58362877368927, 0.19196826219558716, -1.289336085319519, -0.3247399628162384, -0.38864412903785706, 0.44361886382102966, -0.22249920666217804, -0.050400469452142715, -0.13369916379451752, -0.37471866607666016, 0.07638315856456757, -0.30462566018104553, -0.8338497877120972, -0.11428719758987427, -0.4124922752380371, -0.19426517188549042, 0.853032648563385, 0.5279112458229065, -0.5480656623840332, -0.4785209596157074, -0.6887636184692383, -0.13688883185386658, 0.1469728797674179, 0.144148051738739, -0.8372932076454163, 0.11759645491838455, 0.25565510988235474, 0.33399125933647156, 0.2849518358707428, -0.13059566915035248, -0.3787824213504791, 0.07279109954833984, 0.6655110716819763, 0.27867749333381653, -0.43694007396698, -0.6167573928833008, 1.3630554676055908, 1.4730905294418335, -1.4501293897628784, 0.24547846615314484, 0.4672875702381134, -0.6350215673446655, 0.9673675298690796, 0.5220332741737366, 0.5111743807792664, 1.1722285747528076, -0.34125179052352905, 0.6842772364616394, 0.28592851758003235, -1.1502653360366821, 0.34067368507385254, 1.0552887916564941, 0.3281998336315155, 0.8729078769683838, 0.39721035957336426, 0.01258047390729189, 0.6746202707290649, 0.1736636459827423, 0.44862353801727295, 0.881711483001709, 0.5173486471176147, -0.012787860818207264, -0.11947927623987198, -0.2535565495491028, 0.5881737470626831, -0.5638716220855713, -0.7579865455627441, -0.09395908564329147, 0.6939824223518372, 0.2128550261259079, 0.751984715461731, 0.3240949511528015, -0.3478529751300812, 0.6740010976791382, 0.4985419511795044, 0.06648434698581696, -0.6622424721717834, -0.4312569499015808, -0.03437911346554756, -0.5370745658874512, 0.3823946714401245, -0.007434959523379803, -0.4466715157032013, -0.029400523751974106, -0.2752722203731537, 0.03576350957155228, -0.17727762460708618, 0.20481473207473755, 1.3218597173690796, 0.3337844908237457, -0.00748832244426012, -0.34106117486953735, -0.3660045564174652, -0.853187084197998, -1.3745834827423096, 0.1325286626815796, -0.5934966802597046, 0.004589006304740906, -0.03836507722735405, -0.5536259412765503, -0.5959482789039612]}, "authors": [{"authorId": "2266840906", "name": "Vatsal Gupta"}, {"authorId": "2266750092", "name": "Pranshu Pandya"}, {"authorId": "2074269068", "name": "Tushar Kataria"}, {"authorId": "2266840908", "name": "Vivek Gupta"}, {"authorId": "2266750435", "name": "Dan Roth"}], "references": [{"paperId": "a1c02351d6bc689150546bcb6829e0c219a305c9", "title": "Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness"}, {"paperId": "675e079cc3c11f9234f8f70bab9f763911b97955", "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis"}, {"paperId": "59e0e0c1aa06d51430792eb5d8308911a1b0110f", "title": "Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks"}, {"paperId": "c90632cf343ebe229d5104d040bda99ef68ee50c", "title": "Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations"}, {"paperId": "6988596f88276920a4e555cbe624e1431bc8a9f7", "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "be8a5395cd4ffdeb3ffd4c90c91b5978b1d69a98", "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation"}, {"paperId": "47b6249f281a2f3f6a788436acd8308640ff4363", "title": "Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization"}, {"paperId": "b3bcd6d3b5dcc7da100ab052780ef50f6bdbe65d", "title": "Enhancing Tabular Reasoning with Pattern Exploiting Training"}, {"paperId": "59e529bedb04265d3bdc600641289b1583ea17aa", "title": "Robust Fine-tuning via Perturbation and Interpolation from In-batch Instances"}, {"paperId": "f91dbd39d4c742ba675e447b04a0b0c70b33e836", "title": "Measure and Improve Robustness in NLP Models: A Survey"}, {"paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d", "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"}, {"paperId": "2505543a8458e6bcc0f3c3e715b419593c1b59f7", "title": "Sample Selection for Fair and Robust Training"}, {"paperId": "3b451fa663704f927e1ec602d7c0845a9826922d", "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations"}, {"paperId": "eb2adbbc77aed0983346f9613d615bc70512b8e6", "title": "TabPert : An Effective Platform for Tabular Perturbation"}, {"paperId": "294a3bbfaa5382c7b475ebd283aac7ad02e2d075", "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers"}, {"paperId": "46efbc5108e3176ebb6f4df74d38ba16f6d3ed0c", "title": "SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS)"}, {"paperId": "99be5048b4d7c6b018dd36c6c5940b98487e074e", "title": "Incorporating External Knowledge to Enhance Tabular Reasoning"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "42d376cdf2437769b9619aa38db64921772920ca", "title": "Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets"}, {"paperId": "91300db0f0a47bcd7530ef89d4b0c19a54fcc283", "title": "CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation"}, {"paperId": "65be695739d0fa35212e49ccccd129535e6d9e15", "title": "Understanding tables with intermediate pre-training"}, {"paperId": "8b2cbb2f101b025c16e12d0d7628f65e5378e10d", "title": "GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing"}, {"paperId": "ee5fff85d3ec62698eddba162f054b7e73670b2a", "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "07c1c2429b63fefdae41eb546c31b40de2a880f7", "title": "INFOTABS: Inference on Tables as Semi-structured Data"}, {"paperId": "33ec7eb2168e37e3007d1059aa96b9a63254b4da", "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"}, {"paperId": "dcb0249ba85140849a07e3cbae358ec3e9b89ac5", "title": "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering"}, {"paperId": "c9b56cb026a38e39bb0228faac57accd6f65e6f7", "title": "TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP"}, {"paperId": "f2f3c83db919a2429c4fcad2d0a0ed4e5294354a", "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting"}, {"paperId": "2ffcf8352223c95ae8cef4daaec995525ecc926b", "title": "Adversarial Training for Large Neural Language Models"}, {"paperId": "dc0ce66f5ab4c5173cdef951649044e4c4c05076", "title": "BERT-ATTACK: Adversarial Attack against BERT Using BERT"}, {"paperId": "2eda2921a8da4b325f9d05f556594a5884c398a7", "title": "Overfitting in adversarially robust deep learning"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f", "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"}, {"paperId": "78c372f8cdf975b3943611ee7e431b2911106ff7", "title": "Auto-completion for Data Cells in Relational Tables"}, {"paperId": "ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96", "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "1adfa30bf112de20cb959014e44626d760aa8e4e", "title": "Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency"}, {"paperId": "9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7", "title": "Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets"}, {"paperId": "42ed4a9994e6121a9f325f5b901c5b3d7ce104f5", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"}, {"paperId": "f91175950edf3804ff1573f570b03db9b108dece", "title": "TextBugger: Generating Adversarial Text Against Real-world Applications"}, {"paperId": "8e773b1840b894603c06b677a0f15ebcf0f26378", "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"}, {"paperId": "413a03a146e6f7b16c11e73243d83e6f1a6627a3", "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences"}, {"paperId": "be4a4f7f65d397a4e07dc83b95da6b414e0634e2", "title": "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "0407b605b8f55db72e2545586bfe8e946b691b70", "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks"}, {"paperId": "ec46bcbced500820521e9f65b0f9ffef5a83ae11", "title": "k-DPPs: Fixed-Size Determinantal Point Processes"}, {"paperId": "70bbed0c658df1f7f9e1516c3e94281e64b3ee56", "title": "Saama AI Research at SemEval-2023 Task 7: Exploring the Capabilities of Flan-T5 for Multi-evidence Natural Language Inference in Clinical Trial Data"}, {"paperId": "cba7fe63921f3f5ab7d3442ff3da1f814929bd17", "title": "Exploring Robust Overfitting for Pre-trained Language Models"}, {"paperId": "67f24ddc450926c5a46f53c8a621d0ee98133354", "title": "PARROT: Zero-Shot Narrative Reading Comprehension via Parallel Reading"}, {"paperId": "48151071d50240df76df400910dbcb5c65dabc07", "title": "The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task"}, {"paperId": "291a00d8433fecd2dd10f7f13b62dae8ce500043", "title": "Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models"}, {"paperId": "f01457a49d4e2c62c91ca29b239eec6b5e71798c", "title": "Robustness and Adversarial Examples in Natural Language Processing"}, {"paperId": null, "title": "On the robustness of vision trans-formers to adversarial examples"}, {"paperId": null, "title": "Grappa: Fine-tuning on the entire perturbations necessitates access perturbations, which is infeasible"}, {"paperId": null, "title": "Improving robustness of language models from an information theoretic perspective"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "ba74ca75aefa2b7bf8f0ed60ff0ebbe29f8854aa", "title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}, {"paperId": null, "title": "2022. Scaling instruction-finetuned language models"}, {"paperId": null, "title": "2023. Chain-of-thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "Scialom"}]}