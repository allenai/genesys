{"paperId": "9f721e41c2380a494bb4c8a78139638ef5310a38", "abstract": "As deep learning is pervasive in modern applications, many deep learning frameworks are presented for deep learning practitioners to develop and train DNN models rapidly. Meanwhile, as training large deep learning models becomes a trend in recent years, the training throughput and memory footprint are getting crucial. Accordingly, optimizing training workloads with compiler optimizations is inevitable and getting more and more attentions. However, existing deep learning compilers (DLCs) mainly target inference and do not incorporate holistic optimizations, such as automatic differentiation and automatic mixed precision, in training workloads. In this paper, we present RAF, a deep learning compiler for training. Unlike existing DLCs, RAF accepts a forward model and in-house generates a training graph. Accordingly, RAF is able to systematically consolidate graph optimizations for performance, memory and distributed training. In addition, to catch up to the state-of-the-art performance with hand-crafted kernel libraries as well as tensor compilers, RAF proposes an operator dialect mechanism to seamlessly integrate all possible kernel implementations. We demonstrate that by in-house training graph generation and operator dialect mechanism, we are able to perform holistic optimizations and achieve either better training throughput or larger batch size against PyTorch (eager and torchscript mode), XLA, and DeepSpeed for popular transformer models on GPUs.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.04759", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "RAF, a deep learning compiler for training is presented and it is demonstrated that by in-house training graph generation and operator dialect mechanism, it is able to perform holistic optimizations and achieve either better training throughput or larger batch size against PyTorch, XLA, and DeepSpeed for popular transformer models on GPUs."}, "embedding": {"model": "specter_v2", "vector": [0.014054544270038605, 0.1766878068447113, -0.7309185862541199, 0.031645290553569794, 0.19458109140396118, -0.26215043663978577, 0.5065672397613525, -0.36524322628974915, -0.5069193840026855, -0.4382764399051666, 0.10163084417581558, -0.8036242127418518, 0.7732134461402893, -0.250519335269928, -0.1942802369594574, 0.16395340859889984, -0.4861202538013458, 0.07082198560237885, 0.025550495833158493, -0.0517362579703331, 0.0075805517844855785, 0.1749468445777893, -1.6120749711990356, -0.028044499456882477, 0.409103125333786, 0.9076452255249023, -0.10593169182538986, 1.3470741510391235, -0.4496926963329315, 0.7195294499397278, 0.570254921913147, -0.4407823979854584, 0.4004517197608948, 0.36301788687705994, -0.5191656351089478, -0.19181545078754425, 0.5924837589263916, -0.7360860109329224, -0.3325210213661194, 0.8991814851760864, 0.12812209129333496, 0.11652569472789764, 0.010724809020757675, -1.1072880029678345, 0.2613944113254547, 0.2602475881576538, 0.3394215703010559, 0.7712973356246948, -1.060207724571228, -0.19571420550346375, 1.1820985078811646, -1.22848379611969, -0.08762369304895401, 0.8048486709594727, 0.579260528087616, 0.3928969204425812, -0.18553246557712555, -0.35782963037490845, 0.3095775842666626, -0.2912064790725708, -0.6742784976959229, -0.4734179973602295, -0.1441592425107956, -0.36399197578430176, 1.9813019037246704, -0.39822250604629517, -0.09973029047250748, 0.19532166421413422, 0.22824940085411072, 0.9974368214607239, -0.021888267248868942, -0.764946460723877, -0.17299091815948486, -0.120811328291893, 0.6882481575012207, 1.1334927082061768, 0.13647611439228058, 0.426105260848999, -0.7325927019119263, -0.1672670543193817, 0.8143532276153564, 0.23472997546195984, 0.6144697666168213, -0.30584508180618286, 0.0007050015847198665, 0.7783622145652771, 0.47226396203041077, 0.08079708367586136, -0.3348318040370941, 1.1355234384536743, 1.1392139196395874, 0.038029130548238754, -0.17084108293056488, 0.1363753229379654, -0.24114808440208435, 0.4732690155506134, -0.8036662340164185, 0.2766161561012268, 0.03583722561597824, 0.6880609393119812, 0.1401999294757843, 0.4564366042613983, -0.5812702178955078, -0.09863899648189545, 0.9991820454597473, -0.14705725014209747, 0.15744233131408691, -0.7934252023696899, 0.30758944153785706, -0.5054200291633606, -0.6158477067947388, -0.193711519241333, -0.13622456789016724, -0.6662357449531555, -0.9582740068435669, -0.40420201420783997, -0.7782863974571228, -0.37250468134880066, -1.021304965019226, 0.43296533823013306, -0.2569838762283325, 0.5263919830322266, 0.3550514280796051, 0.6501091122627258, 1.0107084512710571, 0.2937806248664856, 0.29351022839546204, 0.5318911671638489, 0.9416663646697998, -1.61117684841156, 0.15960144996643066, -1.160544753074646, 0.6882769465446472, -0.232614204287529, 0.013889841735363007, -0.09883684664964676, -1.051547884941101, -1.2566652297973633, -0.8083935379981995, -0.31443363428115845, -0.5107935070991516, 0.23772263526916504, 1.4647551774978638, 0.410866916179657, -1.3585658073425293, 0.8394920229911804, -0.1987558901309967, 0.03191595897078514, 0.23984530568122864, 0.42613765597343445, 0.5286590456962585, -0.2243136167526245, -0.9563990831375122, -0.31804218888282776, 0.46458709239959717, -0.6800659894943237, -0.5566733479499817, -0.802415132522583, -0.5503739714622498, 0.03789593651890755, 0.08150084316730499, -0.9276462197303772, 1.448541522026062, -0.31661370396614075, -1.0922296047210693, 1.0018043518066406, -0.07333692908287048, -0.11816715449094772, 0.2556329071521759, -0.1403193324804306, -0.3526672124862671, -0.3091694414615631, -0.3526502251625061, 0.5407882332801819, 0.826650083065033, -0.11808329075574875, 0.01031684409826994, 0.2598022222518921, 0.07215075939893723, 0.03540467470884323, -0.356593519449234, 1.0381168127059937, -0.5937609672546387, -0.160746768116951, -0.019478995352983475, 0.5812732577323914, -0.2831692695617676, -0.05462780222296715, -0.6342576146125793, -0.5479485392570496, 1.0612645149230957, 0.21669667959213257, 0.9381958246231079, -0.9437949061393738, -0.9559149146080017, 0.13543590903282166, 0.5173097848892212, 0.09078648686408997, -0.32397976517677307, 0.2464028149843216, -0.6535326242446899, 0.23607118427753448, 0.10535461455583572, -0.9311099052429199, 0.06751871854066849, -0.18463680148124695, -0.6858528852462769, -0.5371352434158325, -0.04334169626235962, 0.8627054691314697, -0.4794997274875641, -0.053133606910705566, -0.3624871075153351, 0.3511461615562439, -1.4093067646026611, 1.2762163877487183, -0.3943491280078888, -0.3201827108860016, 0.1596194952726364, -0.20171701908111572, 0.18168771266937256, -0.675649881362915, 0.6522223353385925, -0.5645832419395447, -0.17681746184825897, 0.6971996426582336, -0.26031532883644104, 1.277288556098938, -0.528389036655426, 0.16833792626857758, 0.1154462918639183, -0.5277295708656311, 0.4189682900905609, 0.06626956164836884, -0.28357651829719543, -0.4281357228755951, 0.2387058138847351, 0.6808478236198425, -0.554885745048523, 0.6648322939872742, 1.2242741584777832, 0.724845290184021, -0.007214801385998726, 0.3565015196800232, 0.6622180342674255, 0.014020370319485664, 0.3887476325035095, 0.6523727774620056, 0.7549146413803101, 0.38602715730667114, -0.1724845916032791, -0.10173682868480682, 0.4528685510158539, -0.5708755254745483, -0.2341432273387909, 0.33012011647224426, 0.5982536673545837, 0.5608289837837219, 0.8485649824142456, -0.7622025609016418, -0.5144206285476685, 0.07125075906515121, 0.6878190040588379, 1.796631932258606, -0.2768556475639343, -0.04331487417221069, -0.852485716342926, -0.850591778755188, -0.12576709687709808, -0.008919254876673222, -0.00143022695556283, -0.31776735186576843, -0.45663976669311523, -1.248673439025879, 0.9959980249404907, 0.5350563526153564, 1.3828701972961426, -0.3897775709629059, -0.6962960362434387, -0.8747067451477051, 0.800052285194397, -0.9307169914245605, -0.6619302034378052, 0.8026061058044434, -1.0177063941955566, -0.050114475190639496, 0.2841741442680359, -0.35107406973838806, 0.27198338508605957, -0.19876588881015778, 1.2610551118850708, -0.022988256067037582, -0.43376603722572327, -0.07409830391407013, 1.009434700012207, -0.14244244992733002, -0.5148320198059082, 0.4696638286113739, -0.10984991490840912, -0.4127371907234192, 0.2223995327949524, 0.23615363240242004, 0.09672325849533081, -0.003070945618674159, -0.25403353571891785, 0.347415566444397, 0.12228140234947205, 0.033522941172122955, 0.9042766094207764, -0.001757090794853866, -0.3292998671531677, -1.4970555305480957, 0.7730169296264648, 0.09677659720182419, -0.6907796263694763, -0.03479461371898651, -1.008966326713562, 0.1234508529305458, 0.8559553027153015, -0.36298125982284546, -0.3015649914741516, -0.999237060546875, 0.30651727318763733, -0.9299126267433167, -0.147235706448555, -0.030424488708376884, 0.4262951612472534, -0.37920936942100525, 0.5365234017372131, 0.24876825511455536, 0.5575911402702332, -0.12677542865276337, 0.4608176052570343, -1.3037960529327393, 0.6185469627380371, -0.22750988602638245, 0.2626363933086395, -0.564365029335022, 0.1627703160047531, -0.1987638920545578, -0.36530548334121704, -0.17496056854724884, -0.1286410689353943, -0.39428845047950745, 0.2634871006011963, -0.7346653938293457, -0.8610821962356567, -0.013631586916744709, -0.999690592288971, -0.6183004379272461, 0.4698241949081421, -0.5194358825683594, 0.01138969138264656, -1.305342197418213, -1.2001171112060547, -0.05049482733011246, -1.1639052629470825, -1.545762538909912, 0.40713217854499817, 0.26855775713920593, 0.25012731552124023, -0.8444638252258301, -0.10844087600708008, -0.5039153099060059, 1.0413947105407715, -0.1213829517364502, 0.854751706123352, 0.09834464639425278, -0.21505707502365112, 0.39629191160202026, -0.5487707257270813, 0.8305999040603638, -1.0373713970184326, 0.7849192023277283, -1.1009963750839233, 0.29874980449676514, -0.7049083709716797, -0.7020204067230225, 0.39360877871513367, 0.20381580293178558, 1.116617202758789, -0.03246665000915527, -0.2929028868675232, 1.1045914888381958, 2.0018560886383057, -1.1791131496429443, 0.28915640711784363, 0.03634000942111015, 1.435533881187439, -0.2531258165836334, -0.610590398311615, 0.45110079646110535, 0.053226079791784286, 0.262932151556015, 0.5777595043182373, -0.48182135820388794, -0.25665682554244995, -0.3592071235179901, 0.6123043894767761, 1.1893284320831299, 0.4421350657939911, 0.2815878689289093, -1.0473592281341553, 0.2041160762310028, -0.7982607483863831, -0.29346907138824463, 0.31554967164993286, 0.6138213276863098, 0.10229536890983582, -0.012785855680704117, 0.11388932913541794, -0.012188682332634926, 0.57029128074646, 0.5662708878517151, -0.705731213092804, -1.5426433086395264, 0.6464718580245972, 0.9104742407798767, 1.0112314224243164, 0.2692488431930542, -0.03603430837392807, 0.5109092593193054, 14.278663635253906, 0.9478899836540222, -0.5167516469955444, 0.38464391231536865, 0.838203489780426, 0.36183154582977295, -0.39070573449134827, -0.015687033534049988, -1.5692058801651, -0.42867550253868103, 1.46198570728302, 0.19909478724002838, 0.35575729608535767, 0.9378654956817627, -0.25404244661331177, 0.263151079416275, -0.042132019996643066, 0.6244776844978333, 0.2597515881061554, -1.570109486579895, 0.25523102283477783, 0.44262176752090454, 0.3119197487831116, 0.8431214690208435, 0.8740531802177429, 0.82978755235672, 0.6718631982803345, -0.39318525791168213, 0.5428987741470337, 0.01316713448613882, 1.367081880569458, -0.5255919098854065, 0.5296827554702759, 0.328058660030365, -0.988097071647644, 0.3000892400741577, -0.4311123788356781, -1.3064897060394287, -0.2413340061903, 0.36778905987739563, -1.1013765335083008, -0.12100449204444885, -0.3389202952384949, 1.264182448387146, -0.03688053786754608, 0.19553150236606598, -0.14113891124725342, 0.3711094558238983, 0.124910868704319, 0.32056665420532227, 0.18003632128238678, 0.4799632132053375, -0.4882124364376068, 0.001646404154598713, -0.2174222320318222, -0.33887094259262085, 0.3139752447605133, 0.36359938979148865, -0.7842342853546143, -0.4373914301395416, -0.06799207627773285, -0.45116400718688965, -0.18476007878780365, 1.2236759662628174, 0.19975893199443817, 0.3525187373161316, -0.5163955092430115, 0.28214168548583984, 0.6856001019477844, -0.07755972445011139, -0.7504517436027527, -0.15890231728553772, 0.8483389616012573, -0.404134601354599, -0.07882291823625565, 0.3431444466114044, -0.7135986089706421, -0.4848533570766449, -1.0534930229187012, -0.5008907318115234, 0.4265187084674835, -0.5036899447441101, -0.3305804133415222, 0.8096152544021606, -0.439072847366333, -0.08679741621017456, 0.37028899788856506, -1.3278422355651855, -0.5123212337493896, 0.6357384324073792, -1.8218108415603638, -0.2562563121318817, 0.08112593740224838, -0.1086132600903511, -0.8182120323181152, -0.10096638649702072, 1.0674444437026978, 0.4218176305294037, -0.6939319372177124, 0.20129573345184326, -0.3083938956260681, 0.18128620088100433, -0.4609537720680237, -0.8942081928253174, 1.4813556671142578, 0.505188524723053, -0.4229131042957306, 0.04670587182044983, -0.2272387444972992, 0.3403724431991577, -0.9313783645629883, -0.5381439328193665, 0.3936886787414551, -0.20268002152442932, 0.015055054798722267, -0.6454048752784729, -0.5064271092414856, 0.40153875946998596, 0.18392089009284973, 0.15917296707630157, 0.30777686834335327, 0.10300152003765106, -0.9291150569915771, -0.3622010350227356, -0.6931532621383667, -0.027055924758315086, 0.5494179725646973, -0.8260075449943542, 0.4817485213279724, 0.2819172441959381, 0.6072618961334229, -1.2499505281448364, -0.7025707364082336, -0.42734530568122864, -0.02912505716085434, -0.6971681118011475, 1.236588478088379, -0.0282514039427042, 1.2952419519424438, 0.996883749961853, 0.05930964648723602, -0.19703565537929535, 0.36950448155403137, -0.8054121732711792, 0.0449448898434639, -0.06128956750035286, 0.3910689949989319, -0.7984696626663208, 0.7322209477424622, 0.5606613159179688, -0.15823109447956085, -0.49963700771331787, 0.1961895078420639, -0.067154660820961, -0.07442446053028107, -0.6040461659431458, 0.5274823307991028, -0.007353650871664286, -0.5185068845748901, 0.21541281044483185, 0.5668535232543945, 0.40033605694770813, 0.22164851427078247, -0.4939289093017578, 0.5915452241897583, -0.10061129927635193, -0.5392526388168335, -0.7405340075492859, -0.5213786959648132, -1.2347469329833984, 0.20939381420612335, -1.471846580505371, -0.28415897488594055, -0.5564811825752258, -0.42378729581832886, -0.42002972960472107, -0.15060405433177948, 0.4541109800338745, 0.32642725110054016, -0.058869242668151855, -0.5566951632499695, -0.5810509920120239, -0.40357664227485657, 0.43151479959487915, 0.8515869975090027, 0.12598437070846558, 0.4109991490840912, -0.20617303252220154, 0.07529893517494202, 0.2999292016029358, 0.5998339056968689, -0.21749110519886017, -0.8307503461837769, -1.258194923400879, 0.24873021245002747, -0.08522218465805054, 0.30324873328208923, -1.0534586906433105, 0.5654454827308655, 0.468472421169281, -0.3319638967514038, 0.34202414751052856, -0.25714945793151855, -0.6065139770507812, -0.5134262442588806, 0.431180477142334, -0.27777355909347534, 0.66064453125, 0.5891066193580627, -0.8218576908111572, -0.09048600494861603, 0.6253365874290466, 0.14548437297344208, -0.661287784576416, -1.2572379112243652, 0.6268496513366699, 0.09023786336183548, 0.2089391052722931, -0.4362887740135193, -0.0226134043186903, -1.4346253871917725, 0.23208698630332947, 0.3165537416934967, 0.23573359847068787, 0.10456756502389908, 0.3819722831249237, 0.2503799796104431, -0.9705864787101746, 0.4265999495983124, 0.48259833455085754, -0.5523461103439331, 0.10614746063947678, 0.3876343071460724, 0.6933180689811707, -1.247206449508667, 0.19659322500228882, -0.17537283897399902, 0.3592163622379303, -0.7829011082649231, 0.18537449836730957, 0.9108514785766602, -0.8385114669799805, -0.3059327304363251, 1.2189170122146606, -0.189336359500885, -1.0863975286483765, -0.20160499215126038, -1.0684725046157837, -0.3392608165740967, -0.78045254945755, 0.4802190959453583, 0.13159912824630737, 0.4760846197605133, 0.19396421313285828, -0.6816978454589844, -0.00778269674628973, -0.09628032147884369, -0.2397150695323944, 0.1888922154903412, 0.35619452595710754, -0.6891593933105469, 0.09311666339635849, 1.0733190774917603, -0.8247110247612, -0.8036196827888489, -0.6545108556747437, -0.6547269225120544, 0.08438905328512192, 0.7895649075508118, -0.2958791255950928, -1.3548556566238403, 0.9948299527168274, 0.06936472654342651, 0.16444779932498932, 0.6677490472793579, -0.8207855820655823, 0.5263328552246094, 0.1546398103237152, 0.1563262641429901, -0.22747421264648438, -0.5968324542045593, 1.4260025024414062, 0.5066469311714172, -0.5649588108062744, 0.5118182897567749, -0.4604285955429077, -0.8931093811988831, 1.143627405166626, 0.4891038239002228, -0.3443675935268402, 0.7427372932434082, 0.0755695328116417, -0.320811003446579, 0.21832089126110077, -1.0104408264160156, -0.36086976528167725, 0.5598819255828857, 0.5716556906700134, 0.8035094738006592, 0.23470400273799896, 0.2480539083480835, 0.5028444528579712, 0.026632005348801613, 0.029733072966337204, 0.34152889251708984, 0.5719180703163147, -0.10197486728429794, -0.09648782014846802, -0.21286748349666595, 0.6355789303779602, -0.7558331489562988, -0.8666539192199707, 0.35173743963241577, 0.7967146635055542, 0.12970714271068573, 0.12103144079446793, 0.7539686560630798, -0.2774907946586609, 0.5451523065567017, 0.13174866139888763, 0.39251914620399475, -0.6414966583251953, -0.7005692720413208, -0.3343561887741089, -0.5212249159812927, -0.23738056421279907, 0.06603153049945831, -0.138175830245018, -0.908762514591217, -1.0058090686798096, 0.8531846404075623, -0.1157698854804039, 0.5921454429626465, 0.7936830520629883, 0.7635140419006348, 1.156661868095398, -0.14418600499629974, -0.763849675655365, -0.2457359880208969, -0.16552816331386566, -0.2397342175245285, -0.9008735418319702, -0.4142858684062958, -0.24814462661743164, -0.22094620764255524, -0.31606021523475647]}, "authors": [{"authorId": "17813719", "name": "Cody Hao Yu"}, {"authorId": "1431226311", "name": "Haozheng Fan"}, {"authorId": "2211028612", "name": "Guangtai Huang"}, {"authorId": "144094780", "name": "Zhen Jia"}, {"authorId": "2136341779", "name": "Yizhi Liu"}, {"authorId": "2146044942", "name": "Jie Wang"}, {"authorId": "2211050470", "name": "Zach Zheng"}, {"authorId": "2145108437", "name": "Yuan Zhou"}, {"authorId": "3050154", "name": "Haichen Shen"}, {"authorId": "1931349", "name": "Junru Shao"}, {"authorId": "1701799", "name": "Mu Li"}, {"authorId": "2108024054", "name": "Yida Wang"}], "references": [{"paperId": "cd733ce920e055415e9a9a7d90d3ec89f8750866", "title": "MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud"}, {"paperId": "ebe259796870ebccf26577044d0087884209b884", "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training"}, {"paperId": "2a8fa407e074bebeaf1e254be37fae7fc54610e3", "title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network"}, {"paperId": "bc4f195d2f0937dab16bf50a24bf385c10781dd7", "title": "MLIR: Scaling Compiler Infrastructure for Domain Specific Computation"}, {"paperId": "148d65a0f0e53a3e824b6dfdbbe9cc8bf623090f", "title": "LazyTensor: combining eager execution with domain-specific compilers"}, {"paperId": "9ff525d1ebd389c359ddbf06df3e99c433c2bf9e", "title": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition"}, {"paperId": "2b2056bf5763e32811a69769fa8c223160125f9e", "title": "DNNFusion: accelerating deep neural networks execution with advanced operator fusion"}, {"paperId": "682bc0138e7c6e555eb741844e94e740aef9917d", "title": "FusionStitching: Boosting Memory Intensive Computations for Deep Learning Workloads"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "3c55dd7b8da5c7b47e91b2e749c264f50d007cd4", "title": "Dynamic Tensor Rematerialization"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "fd431005d26100f5453590080683cbae9dc1189f", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "d8e73f9034e16a380903282353e762d7f9517983", "title": "MLPerf Training Benchmark"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f90a7bc396e205b204d5d6066a10162f84b128f9", "title": "Learning to optimize halide with tree search and random programs"}, {"paperId": "f12557da0cff05914ed4a28d11c8a561e08fbd18", "title": "Astra: Exploiting Predictability to Optimize Deep Learning"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "8c7310477fd027193cd040288f0aa9824c80b91f", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "f61e9fd5a4878e1493f7a6b03774a61c17b7e9a4", "title": "Memory-Efficient Backpropagation Through Time"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "TensorFlow: A system for large-scale machine learning"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "0b9aaee517e0cabb274f5d7cfb01d8f58e51d76e", "title": "PolyMage: Automatic Optimization for Image Processing Pipelines"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "31c36d445367ba204244bb74893c5654e31c3869", "title": "cuDNN: Efficient Primitives for Deep Learning"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "4d23db55e6671a82c95dacec33b2967a4b8b677d", "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "f0f4757aa2f923a349e8357e73850a78e9b80fee", "title": "A practical automatic polyhedral parallelizer and locality optimizer"}, {"paperId": "904ab6edd082c70e18a657cb4fadacafcd094ca3", "title": "Reverse-mode AD in a functional framework: Lambda the ultimate backpropagator"}, {"paperId": "a33a876dff9bd77eae1bbf0fa7f2b1be1448c2c4", "title": "Apollo: Automatic Partition-based Operator Fusion through Layer by Layer Optimization"}, {"paperId": "667776b1050569e3bb0585feb08a085574fc019f", "title": "Collage: Automated Integration of Deep Learning Backends"}, {"paperId": "d668f12be54174141e6197fad737006b7b0c0571", "title": "PyTorch"}, {"paperId": null, "title": "cuDNN Release 8"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c99321c13ab11d98bc8164eff1c753d7d3e7d80c", "title": "Efficient Rematerialization for Deep Networks"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}, {"paperId": null, "title": "JAX: Composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "CUTLASS: CUDA Templates for Linear Al-gebra Subroutines"}, {"paperId": null, "title": "XLA Team"}, {"paperId": "963a1e639971e7e3a4e6c871cf9c0b410e5532d0", "title": "Polyhedral parallel code generation for CUDA"}, {"paperId": "fbc6562814e08e416e28a268ce7beeaa3d0708c8", "title": "Large-Scale Machine Learning with Stochastic Gradient Descent"}, {"paperId": null, "title": "Xla -tensorflow, compiled"}, {"paperId": null, "title": "Nvidia"}, {"paperId": null, "title": "Intel"}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}]}