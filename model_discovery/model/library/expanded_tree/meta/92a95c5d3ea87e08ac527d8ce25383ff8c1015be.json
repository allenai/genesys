{"paperId": "92a95c5d3ea87e08ac527d8ce25383ff8c1015be", "abstract": "Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. However, both the attention mechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently efficient due to dense multiplications, leading to costly training and inference. To this end, we propose to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch. Specifically, all $\\texttt{MatMuls}$ among queries, keys, and values are reparameterized using additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized with shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameterization on attention maintains model accuracy, while inevitably leading to accuracy drops when being applied to MLPs. To marry the best of both worlds, we further propose a new mixture of experts (MoE) framework to reparameterize MLPs by taking multiplication or its primitives as experts, e.g., multiplication and shift, and designing a new latency-aware load-balancing loss. Such a loss helps to train a generic router for assigning a dynamic amount of input tokens to different experts according to their latency. Extensive experiments on various 2D/3D Transformer-based vision tasks consistently validate the effectiveness of our proposed ShiftAddViT, achieving up to $\\textbf{5.18$\\times$}$ latency reductions on GPUs and $\\textbf{42.9}$% energy savings, while maintaining a comparable accuracy as original or efficient ViTs.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.06446", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed ShiftAddViT, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch."}, "embedding": {"model": "specter_v2", "vector": [0.8118343949317932, 0.30725210905075073, -0.11096179485321045, 0.10803421586751938, -0.1384543925523758, 0.15421569347381592, 0.8452793955802917, -0.23250068724155426, -0.5207381844520569, -1.0997086763381958, 0.2408449649810791, 0.3052489161491394, 0.5097370743751526, -0.13999241590499878, -0.4461542069911957, 0.06701285392045975, -0.22536566853523254, 0.22968006134033203, 0.5761486887931824, -0.6336046457290649, 0.4424378275871277, -0.2648998498916626, -1.6926794052124023, 0.2174176275730133, 0.1452181339263916, 1.4195053577423096, 0.11060135066509247, 1.056186556816101, -0.39315491914749146, 0.5792281627655029, 0.568686306476593, -0.6237079501152039, 0.45335182547569275, 0.44907575845718384, -0.34985339641571045, -0.033049605786800385, 0.8041867017745972, -0.5229639410972595, -0.7488006949424744, 0.8899822235107422, 0.18048979341983795, 0.3152497708797455, 0.5716233849525452, -1.0718660354614258, -0.5384775996208191, 0.12899553775787354, 0.1315777450799942, 0.5246855616569519, -0.6097310781478882, -0.22972138226032257, 0.9131402373313904, -1.2901173830032349, -0.18730567395687103, 1.308678150177002, 0.5698184370994568, 0.06041473150253296, -0.14656925201416016, -0.35911014676094055, 0.4674472510814667, 0.4063040316104889, -0.530249834060669, -0.7134817838668823, -0.048856716603040695, -0.11599090695381165, 1.686767578125, -0.5567706227302551, -0.11278333514928818, 0.47987431287765503, 0.0688655897974968, 1.307713508605957, 0.1356704831123352, -0.7676188945770264, 0.020434927195310593, -0.09550383687019348, 0.12086613476276398, 0.8729863166809082, -0.19432426989078522, 0.05390184745192528, -1.227372169494629, 0.20082813501358032, 0.49147137999534607, 0.2549023926258087, 0.5911124348640442, -0.4385104775428772, -0.2846822440624237, 0.605771541595459, 0.8767611384391785, 0.2714783251285553, -0.2431630939245224, 1.2899466753005981, 0.6525646448135376, -0.06871344149112701, -0.2671513259410858, -0.05597909167408943, 0.15900596976280212, 0.6718275547027588, -0.7292991876602173, -0.08913160860538483, -0.377172589302063, 0.9893321990966797, 0.01325517799705267, 0.22685356438159943, -0.8421617150306702, 0.06455478072166443, 1.0943498611450195, 0.46133261919021606, 0.16942422091960907, -0.4743327796459198, 0.1264551430940628, -0.8580623865127563, -0.012636932544410229, -0.8514261245727539, 0.3160378634929657, -0.2616036534309387, -0.857926607131958, -0.5985388159751892, -0.7133563756942749, 0.6268295049667358, -1.404006004333496, 0.20120154321193695, -0.4400407373905182, 0.4323340356349945, -0.15334545075893402, 0.4182833731174469, 0.6865350604057312, 0.3319304287433624, 0.1291205734014511, 0.8629559278488159, 1.5380003452301025, -1.1791496276855469, -0.06444988399744034, -1.0741080045700073, -0.04729820787906647, -0.49616512656211853, 0.28264519572257996, -0.017839649692177773, -1.2970762252807617, -1.5122779607772827, -0.6961126327514648, -0.4876055121421814, -0.6804627180099487, 0.35701730847358704, 0.9478490352630615, 0.27616316080093384, -1.5107431411743164, 0.3723366856575012, -0.2625550627708435, -0.23361651599407196, 0.6819416284561157, 0.33688923716545105, 0.6155615448951721, -0.4657900929450989, -0.7969748973846436, 0.42077380418777466, -0.289241760969162, -0.41006627678871155, -0.20218466222286224, -0.3399241268634796, -1.0733964443206787, 0.315249502658844, 0.1460159569978714, -0.8588892817497253, 1.4642002582550049, -0.017453743144869804, -0.7709128260612488, 0.7865647673606873, -0.47568678855895996, 0.06444717198610306, -0.015842752531170845, 0.08449826389551163, 0.005409120116382837, -0.24099458754062653, -0.3749442994594574, 0.9531564116477966, 1.439582347869873, 0.011730075813829899, -0.4243415892124176, 0.32729867100715637, -0.1960148811340332, 0.03658800944685936, -0.3563190996646881, 0.8875536918640137, -0.7485711574554443, -0.25553247332572937, 0.5325430631637573, 0.5578234195709229, -0.11233507096767426, 0.07569858431816101, -0.03173057362437248, -0.6606492400169373, 0.7201620936393738, 0.5286638140678406, 0.2186608761548996, -0.8965726494789124, -0.9629223346710205, -0.10695360600948334, 0.30845320224761963, -0.06595294922590256, -0.7876535654067993, 0.5477067232131958, -0.21855765581130981, -0.2987424433231354, 0.182991161942482, -1.4206219911575317, 0.3648110330104828, -0.37955498695373535, -1.0788334608078003, -0.09143029898405075, 0.1842614710330963, 1.416105031967163, -0.5489622354507446, 0.04810324311256409, 0.2373770922422409, 0.5386595726013184, -0.8083158731460571, 0.7408609986305237, -0.48751717805862427, -0.09516887366771698, 0.03511222079396248, 0.07842501997947693, 0.35333460569381714, -0.6539559960365295, 0.1290987879037857, -0.977031409740448, 0.18755659461021423, 0.456178218126297, -0.19059763848781586, 1.4789961576461792, -0.004248043522238731, 0.5639384388923645, -0.2564525306224823, -0.9791794419288635, 0.39216405153274536, -0.14229430258274078, -0.14387528598308563, -0.6990072727203369, 0.5540118217468262, 0.16789217293262482, -0.8420913815498352, 0.33798131346702576, 0.9907053709030151, 1.3138574361801147, -0.3781664967536926, -0.17038223147392273, 0.4380611479282379, -0.35212546586990356, 0.018308505415916443, 0.3858543932437897, 0.613233208656311, 0.4170726537704468, -0.022024210542440414, 0.18132352828979492, 0.2239464670419693, -0.8060330152511597, -0.5094316601753235, 0.9126030802726746, 0.5596647262573242, 0.8632065653800964, 0.249394953250885, -1.0683039426803589, -0.22619789838790894, -0.09331288188695908, 0.6135018467903137, 1.7129799127578735, 0.27176788449287415, 0.17956574261188507, -0.5761237144470215, -0.14062592387199402, -0.2735930383205414, -0.5967442393302917, -0.2553251087665558, -0.218973770737648, -0.23941127955913544, -0.9800320267677307, 0.7814275026321411, 0.31567883491516113, 1.2485027313232422, -0.6138668060302734, -0.8604001998901367, -0.6382068395614624, 0.19534164667129517, -1.0848021507263184, -0.6814648509025574, 0.39425167441368103, -0.09365852177143097, 0.08554017543792725, -0.02855447679758072, -0.4838695824146271, 0.37945201992988586, -0.2910293936729431, 0.7514300346374512, -0.7591925859451294, -0.901901364326477, 0.35988807678222656, 0.40422385931015015, -0.6905133128166199, -0.1808934509754181, 0.013913525268435478, -0.14985957741737366, 0.19275645911693573, 0.36493057012557983, 0.14374029636383057, -0.45201992988586426, -0.09721389412879944, -0.44667282700538635, 0.07329997420310974, 0.36214175820350647, -0.3162910044193268, 0.9659281969070435, -0.622811496257782, -0.35321441292762756, -0.5418810248374939, 0.5947107672691345, 0.3361869752407074, -0.37214377522468567, 0.003543493337929249, -0.8033486008644104, -0.11787594854831696, 0.2253800630569458, -0.7047179341316223, 0.08893532305955887, -0.5003026723861694, 0.3676145672798157, -0.9860612154006958, -0.32042598724365234, -0.6157092452049255, 0.8171901106834412, -0.6193451285362244, 0.4823613464832306, 0.7659478783607483, 0.3280586004257202, 0.3929348886013031, 0.6776370406150818, -0.8203005194664001, 1.0306005477905273, 0.16115856170654297, 0.22434906661510468, 0.17814669013023376, 0.33407706022262573, -0.48018479347229004, -0.08748038113117218, -0.3825073838233948, -0.0074304682202637196, -0.40885090827941895, 0.3780180811882019, -0.5430464148521423, -1.2912122011184692, 0.35120776295661926, -1.0100362300872803, 0.034825798124074936, 0.0002314587909495458, -0.2765902578830719, -0.396614134311676, -0.7978981137275696, -1.0165878534317017, -0.32066455483436584, -0.973556399345398, -1.5272164344787598, 0.549884021282196, 0.3816176652908325, 0.00921436958014965, -0.09356853365898132, -0.3204090893268585, -0.3713189661502838, 1.289968490600586, -0.3587130308151245, 0.41122254729270935, 0.03573775291442871, -0.6932314038276672, -0.04664785414934158, -0.3406870365142822, 0.21669955551624298, -0.35182610154151917, 0.3119811713695526, -1.4101227521896362, 0.2322806566953659, -0.4404367208480835, -0.40330103039741516, 0.6772041320800781, 0.5307601690292358, 0.7016552090644836, 0.10282633453607559, -0.47491222620010376, 0.6253827810287476, 1.5413492918014526, -0.6058760285377502, 0.330459326505661, -0.12046793103218079, 1.0067989826202393, -0.026182379573583603, -0.34950441122055054, 0.7579848170280457, 0.5849677324295044, 0.2849045395851135, 0.594209611415863, -0.5538263320922852, -0.4830229878425598, -0.30470696091651917, 0.27979400753974915, 0.8108600974082947, 0.4794207811355591, -0.05096243694424629, -0.8780931830406189, 0.4789530634880066, -1.0644525289535522, -0.6236816048622131, 0.4959430992603302, 0.6172767877578735, -0.2281116545200348, -0.22145472466945648, -0.32846274971961975, -0.39118847250938416, 0.7505268454551697, 0.6597750186920166, -0.5197712779045105, -0.8615776896476746, 0.1361134648323059, 0.9088221788406372, 0.6120779514312744, 0.265096515417099, -0.22774669528007507, 0.6524536609649658, 14.654584884643555, 0.761623203754425, -0.5792455077171326, 0.295155793428421, 0.7819885611534119, 0.2503013014793396, -0.1149909496307373, -0.02017439343035221, -0.9811343550682068, -0.08923525363206863, 0.6339486241340637, 0.6208760738372803, 0.7813423871994019, 0.2856021523475647, -0.6443281173706055, 0.10008298605680466, -0.6450937390327454, 1.21404230594635, 0.888875424861908, -1.3288400173187256, 0.13006553053855896, 0.06884706765413284, 0.23739773035049438, 0.807611882686615, 1.339162826538086, 0.6766573786735535, 0.3626369535923004, -0.3063846826553345, 0.4214085042476654, 0.3991963565349579, 1.296256422996521, 0.2755841016769409, 0.017673959955573082, 0.04697320982813835, -1.2473434209823608, 0.07812225073575974, -0.6111055612564087, -1.0782626867294312, -0.1585196703672409, -0.07450582087039948, -0.6264771223068237, -0.40905535221099854, 0.19164255261421204, 0.6642531752586365, 0.029778389260172844, 0.46216097474098206, 0.16633401811122894, 0.02088664285838604, -0.2743090093135834, 0.05191575735807419, 0.27153506875038147, 0.5510790348052979, -0.3079349100589752, -0.11214737594127655, 0.028668195009231567, -0.3785783648490906, 0.035386692732572556, 0.46047183871269226, -0.5709203481674194, -0.528351902961731, -0.1318097561597824, 0.00732959620654583, -0.03908500447869301, 0.9398464560508728, -0.025958605110645294, 0.33969172835350037, -0.277218759059906, 0.6755121946334839, 0.30313077569007874, 0.18566104769706726, -0.6643723249435425, -0.15927602350711823, 0.42925527691841125, -0.5857213735580444, 0.6748866438865662, 0.8802928328514099, -0.43194323778152466, -0.35172852873802185, -0.61090487241745, -0.27648794651031494, 0.36604437232017517, -1.129805564880371, -0.4765208959579468, 0.7403552532196045, -0.3562212884426117, -0.23304861783981323, 0.42382970452308655, -1.096960186958313, -0.46695250272750854, 0.5099421739578247, -1.5962269306182861, -1.1200122833251953, -0.0929817482829094, -0.36676928400993347, -0.36705857515335083, -0.011406113393604755, 0.8721489906311035, -0.032576099038124084, 0.06425879150629044, 0.15370826423168182, -0.38374707102775574, -0.19709447026252747, 0.24854910373687744, -0.4124467372894287, 1.3638542890548706, 0.45419707894325256, -0.02859581634402275, -0.2003203183412552, 0.005818623583763838, 0.4812309443950653, -0.9643151164054871, 0.030665768310427666, 0.38682520389556885, -0.600520133972168, -0.6409153342247009, -0.5496736168861389, -0.38204821944236755, 0.29165124893188477, 0.5419273376464844, 0.17259621620178223, -0.18073371052742004, -0.21355924010276794, -0.9440756440162659, -0.48735511302948, -0.8213101625442505, 0.006963611580431461, 0.2522793710231781, -0.8567578792572021, -0.0217574555426836, -0.11714453250169754, 0.08515321463346481, -1.2073684930801392, -0.22671204805374146, 0.11143973469734192, 0.3803601861000061, -0.3736029267311096, 1.5814592838287354, 0.06907615810632706, 0.7949695587158203, 0.6888766288757324, -0.2338520586490631, -0.41252264380455017, -0.1509658843278885, -0.7384385466575623, -0.022168884053826332, -0.13384635746479034, -0.0010129937436431646, -0.7142313122749329, 0.2785830497741699, 0.6233080625534058, 0.24904164671897888, -0.6193526387214661, -0.32355016469955444, 0.32608267664909363, -0.48586857318878174, -0.58262038230896, 0.5438768267631531, -0.30394789576530457, -0.27850809693336487, 0.07589906454086304, 0.44286856055259705, 0.62459796667099, 0.10325030982494354, -0.43454378843307495, 0.4223466217517853, -0.22472240030765533, -0.37125423550605774, -0.6650367975234985, -0.958202600479126, -1.5884851217269897, -0.4383828938007355, -1.0136984586715698, 0.03750938922166824, -1.220370888710022, -0.676975429058075, 0.04308922961354256, -0.46010735630989075, 0.38288623094558716, 0.5075510740280151, 0.2145029455423355, -0.3877950608730316, -0.6239527463912964, -0.4509356617927551, 0.6027035713195801, 0.9150042533874512, -0.7020630240440369, 0.3804941475391388, -0.2686983048915863, -0.28726133704185486, 0.34087038040161133, 0.22131118178367615, -0.18094013631343842, -0.9499632716178894, -1.2459664344787598, 0.4510880708694458, -0.31181052327156067, 0.16902707517147064, -1.1565464735031128, 1.1884177923202515, 0.534397304058075, 0.00868418999016285, -0.18684184551239014, 0.7720243334770203, -0.7747267484664917, -0.7132316827774048, 0.5887630581855774, -0.5817590355873108, 0.012604919262230396, 0.43498554825782776, -0.6346612572669983, 0.2616844177246094, 0.9323429465293884, 0.024791816249489784, -0.6448746919631958, -1.227707028388977, 0.647738516330719, -0.21870285272598267, 0.024279218167066574, -0.15715068578720093, -0.39448702335357666, -1.54391348361969, -0.4456025958061218, 0.10948225110769272, 0.09111616760492325, -0.4657164514064789, 0.7389596700668335, 0.8815420866012573, -0.9081161618232727, 0.13726767897605896, 0.6105800271034241, 0.039315175265073776, 0.14324180781841278, 0.59736168384552, 0.7122872471809387, -0.5431265234947205, 0.2204255908727646, -0.25659096240997314, 0.10775122791528702, -0.5877826809883118, 0.4584137201309204, 0.616242527961731, -0.06450861692428589, -0.06388331204652786, 1.3038173913955688, -0.08066851645708084, -0.5686976909637451, 0.33513814210891724, -1.4663875102996826, -0.20668861269950867, -0.18991775810718536, 0.316144198179245, 0.1915263831615448, 0.2993781864643097, 0.07655549049377441, -0.6307263374328613, 0.4381703734397888, -0.24016761779785156, -0.6505776643753052, 0.02873358502984047, 0.06833653151988983, -0.38222169876098633, -0.1386396288871765, 0.6139695048332214, -0.7987056374549866, -0.9224533438682556, -1.2245463132858276, -0.9830319285392761, -0.20020747184753418, 0.3064984977245331, -0.1638544201850891, -0.7544087767601013, 0.9877010583877563, 0.6261699795722961, 0.13874982297420502, 0.620459794998169, -0.026033516973257065, 0.33441296219825745, 0.8842144012451172, -0.12826400995254517, -0.3593023121356964, -0.2972569763660431, 1.044791340827942, 0.9952972531318665, -0.7460212707519531, -0.06752801686525345, -0.3378252685070038, -0.6724919080734253, 0.5292797684669495, 0.4489804208278656, -0.23788325488567352, 0.7008515000343323, 0.09324253350496292, -0.20020273327827454, 0.17040765285491943, -0.6039455533027649, -0.6106628775596619, 1.0785690546035767, 1.2417100667953491, 0.10039139539003372, -0.022157950326800346, 0.7556154131889343, 0.375815749168396, 0.13919278979301453, 0.011058758944272995, 0.11770656704902649, 0.4897926151752472, -0.17518316209316254, 0.17385050654411316, -0.3885399103164673, 0.649889349937439, -0.3350394368171692, -0.8550263047218323, 0.3149331510066986, 0.1320328712463379, 0.4819464087486267, 0.5343271493911743, 1.1582722663879395, -0.2608422338962555, 0.8194304704666138, -0.12598900496959686, 0.8495303988456726, -0.21509107947349548, -0.092445969581604, -0.0872144103050232, -1.210842490196228, -0.1658368557691574, -0.1861545890569687, -0.2983720600605011, -0.20637032389640808, -0.586342990398407, 0.6843838095664978, -0.6620409488677979, 0.5071862936019897, 0.7554506063461304, 0.6906766295433044, 0.8991685509681702, -0.23236222565174103, -0.8102914690971375, -0.34706413745880127, -0.6054221987724304, 0.39565402269363403, -0.5087657570838928, -0.008619111962616444, -0.23027987778186798, 0.1584051102399826, 0.06479492038488388]}, "authors": [{"authorId": "47113848", "name": "Haoran You"}, {"authorId": "30984015", "name": "Huihong Shi"}, {"authorId": "102623966", "name": "Yipin Guo"}, {"authorId": "3138925", "name": "Yingyan Lin"}], "references": [{"paperId": "977351c92f156db27592e88b14dee2c22d4b312a", "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference"}, {"paperId": "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5", "title": "BiViT: Extremely Compressed Binary Vision Transformers"}, {"paperId": "200ef1cde362aafbf598a2b5a1c5f35504ca2289", "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"}, {"paperId": "2475b38a76a9c2dc67f74446e2e686815764b0f2", "title": "EcoFormer: Energy-Saving Attention with Linear Complexity"}, {"paperId": "ec139916edd6feb9b3cb3a0325ca96e21dbb0147", "title": "Hydra Attention: Efficient Attention with Many Heads"}, {"paperId": "b3bee2d9bec8f4b28eaaab963cc63eec6b0ee744", "title": "Is Attention All That NeRF Needs?"}, {"paperId": "c1edb783484e6a529e5a46a86fc70d0c175e6a3e", "title": "Searching for Energy-Efficient Hybrid Adder-Convolution Neural Networks"}, {"paperId": "85167901371c71519241f8681db13fd85ba961e2", "title": "BiT: Robustly Binarized Multi-distilled Transformer"}, {"paperId": "e574fbeee3e163c67693c60db0a68547029f234e", "title": "ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "0dab58e476f3f0e6f580a295f7c4756c86f1f198", "title": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "b52844a746dafd8a5051cef49abbbda64a312605", "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "ec887397ed6c66f3f5b22e0e175ec9ea46c7ca6f", "title": "An Empirical Study of Adder Neural Networks for Object Detection"}, {"paperId": "0a0c204919ec72c6c335296ebf639ebc379d3ac5", "title": "Learned Queries for Efficient Local Attention"}, {"paperId": "b6e9f1189fd46dabd6f1059c546cd2f4589fe65d", "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation"}, {"paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748", "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "c67b1a62b868a758791c88d5465c7b6d53510fc3", "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b", "title": "FastMoE: A Fast Mixture-of-Expert Training System"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "400891424d0fb0e7165c23054ccd771ed7249d64", "title": "AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "a124ef47b879d80f1379b07edfe597ba20cebdd5", "title": "ShiftAddNet: A Hardware-Inspired Deep Network"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "47202763df18ad69d1ef8c22d4c8cabceb11f149", "title": "Kernel Based Progressive Distillation for Adder Neural Networks"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "86b3e0c9cbdc74b4d0a34345d9f779b6cc38562f", "title": "Nimble: Efficiently Compiling Dynamic Neural Networks for Model Inference"}, {"paperId": "428b663772dba998f5dc6a24488fff1858a0899f", "title": "NeRF"}, {"paperId": "38bccac4f05585ec595c7bb7c0e747561dcad886", "title": "DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures"}, {"paperId": "70f04285f1af8b0dc41a9339f2512ae6dcc41266", "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "078ff90be5cff10a547d97ea533d69c5e5b56f61", "title": "DeepShift: Towards Multiplication-Less Neural Networks"}, {"paperId": "1f7723bfb6eaf8375b0cb523ff59690d853902cd", "title": "High-Performance FPGA-Based CNN Accelerator With Block-Floating-Point Arithmetic"}, {"paperId": "cf0671ec7da36af49699de81bee05e9549140db2", "title": "Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "c468bbde6a22d961829e1970e6ad5795e05418d1", "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "70c810ba62c5ee40d611e134b2ac2ca61c4de16b", "title": "Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "18b8871c2af635d1cdfe09642ec6581775cd3a10", "title": "Local Binary Convolutional Neural Networks"}, {"paperId": "ffdaa12ef011de9dbf43be46d45a3abcc8288965", "title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"}, {"paperId": "15fda82d76606152576403acb69f4f15dd92326b", "title": "A Low-Voltage Micropower Asynchronous Multiplier With Shift\u2013Add Multiplication Approach"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "eae2e0fa72e898c289365c0af16daf57a7a6cf40", "title": "Image quality assessment: from error visibility to structural similarity"}, {"paperId": "0eb70445e7244bc2d615be6d525716aa0745b093", "title": "Adaptive equalizer using finite-bit power-of-two quantizer"}, {"paperId": "7d2a78a1f713b71c3a337247d042c5c2f0b2da84", "title": "EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition"}, {"paperId": "17eaf8f5d53407baa3c8ce3bdf5cc1a3503182eb", "title": "Adder Attention for Vision Transformer"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "af39e137818ef8304ccea2ada546700de1dd2f8c", "title": "Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}]}