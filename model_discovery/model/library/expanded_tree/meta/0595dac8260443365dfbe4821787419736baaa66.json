{"paperId": "0595dac8260443365dfbe4821787419736baaa66", "abstract": "Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.", "venue": "arXiv.org", "year": 2024, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel extension to RoPE is introduced which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window and validate the superiority of the method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks."}, "embedding": {"model": "specter_v2", "vector": [0.35051631927490234, 0.6503432989120483, -0.8815969228744507, -0.22355100512504578, -0.5752342343330383, -0.09969733655452728, 0.6234112977981567, -0.14981983602046967, -0.5749318599700928, -0.21897442638874054, 0.7383416891098022, -0.34175360202789307, 0.5439532399177551, 0.4213446080684662, -0.021534422412514687, 0.11310926079750061, -1.0085504055023193, 0.013563865795731544, -0.02022767812013626, -0.332348108291626, -0.19485320150852203, -0.9521673321723938, -0.5567677617073059, 0.3607397973537445, 0.39766114950180054, 0.4563353955745697, 0.5817651152610779, 1.0268880128860474, -0.2736664414405823, 0.07015172392129898, 0.15790340304374695, 0.0004069295828230679, 0.053348518908023834, -0.14737777411937714, -0.25383687019348145, -0.6009622812271118, 0.1942870318889618, -0.44450730085372925, -0.23944726586341858, 0.5149291753768921, -0.27055078744888306, 0.4759076237678528, 0.17931601405143738, -0.6849851012229919, -0.6257389783859253, 0.9913007020950317, 0.6480077505111694, 0.596542477607727, -0.09441924095153809, -0.5769019722938538, 1.6189905405044556, -1.345893383026123, 0.14634644985198975, 1.3252568244934082, 0.6284217834472656, 0.5029317140579224, -0.30469226837158203, -0.3296484351158142, 0.8427908420562744, 0.2915119230747223, -0.38872891664505005, -0.24245785176753998, 0.1811995506286621, 0.2646714150905609, 1.681897521018982, -0.05317956954240799, 0.33123907446861267, 1.1475802659988403, -0.07765953987836838, 1.3760218620300293, 0.08755780011415482, -1.1544525623321533, -0.13019569218158722, 0.28492963314056396, 0.21769098937511444, 0.3975435793399811, -0.9027423858642578, 0.2550836503505707, -0.45875903964042664, -0.5261114239692688, 0.194644033908844, -0.09177061170339584, 0.16307491064071655, 0.16294679045677185, -0.5662899017333984, 0.6208158731460571, 0.5051945447921753, 0.9338871836662292, 0.14409391582012177, 0.5982003808021545, 0.28493666648864746, 0.27172619104385376, 0.49502480030059814, 0.06954440474510193, -0.2824358642101288, 0.26925843954086304, -1.004561185836792, 0.14819248020648956, -0.14062464237213135, 0.77044278383255, -0.133423313498497, -0.4869532883167267, -0.8824216723442078, 0.3535413146018982, 1.5730876922607422, -0.14173197746276855, 0.6760864853858948, -0.9016885757446289, 0.514285683631897, -0.6045799255371094, 0.16327762603759766, -0.449415385723114, 0.07990480959415436, -0.06544190645217896, -0.381096214056015, -1.2484300136566162, -0.24647264182567596, -0.08720069378614426, -0.22027087211608887, 1.077358365058899, -0.055406179279088974, 0.21616458892822266, 0.13419194519519806, 0.12952880561351776, 0.16356894373893738, 0.911315381526947, 0.19569480419158936, -0.03459101915359497, 0.8779284954071045, -1.0192582607269287, -0.8799334168434143, -1.3656514883041382, 0.9875152707099915, -0.3586011230945587, 0.939052402973175, -0.43158116936683655, -0.9587091207504272, -0.7939535975456238, -1.177718162536621, -0.28559502959251404, -0.6139218807220459, 0.47373613715171814, 0.7081037759780884, 0.21592840552330017, -0.8718780875205994, 1.0934419631958008, -0.5034778714179993, -0.07975548505783081, 0.24315103888511658, 0.07284742593765259, 0.1788126528263092, -0.21853342652320862, -1.5458879470825195, 0.351146936416626, 0.4708015024662018, -0.34410613775253296, 0.39214807748794556, -0.607382595539093, -0.9814310669898987, -0.09048081189393997, 0.3816578686237335, -0.19218647480010986, 1.3666070699691772, -0.155953049659729, -1.5677646398544312, 0.11607280373573303, -0.34970441460609436, 0.306135892868042, 0.49317002296447754, -0.37923821806907654, -0.5864512324333191, -0.5017979145050049, -0.38233891129493713, 0.62168288230896, 0.26595160365104675, 0.19321046769618988, -0.2897343933582306, 0.40405189990997314, -0.3575310707092285, 0.16330218315124512, -0.21519704163074493, 0.6993446350097656, -0.08052468299865723, -0.04378641024231911, -0.11119449138641357, 0.4692084789276123, -0.020648157224059105, -0.03299153596162796, -0.031007612124085426, -1.0867490768432617, 1.1152074337005615, -0.13409319519996643, 1.486791968345642, -0.829405665397644, -0.7149049043655396, -0.28878968954086304, -0.260295033454895, -0.14984683692455292, -0.8079439401626587, 0.7099061012268066, -0.031114952638745308, 0.418636679649353, -0.010931501165032387, -1.4819941520690918, 0.15785406529903412, -0.19536037743091583, -0.44248679280281067, -0.440980464220047, -0.15340156853199005, 1.1502811908721924, -1.1226701736450195, -0.1269473284482956, -0.161979541182518, 0.5507602691650391, -1.2464067935943604, 1.0309258699417114, -0.6949293613433838, 0.26423710584640503, 0.06787867844104767, -0.32141539454460144, -0.3530530035495758, -0.4452536106109619, 0.5422129034996033, -0.03524646535515785, -0.040982041507959366, 0.7467831373214722, -0.2939157485961914, 1.324408769607544, -0.6592968106269836, 0.9595934748649597, -0.025097444653511047, 0.12928909063339233, -0.11146161705255508, 0.441188782453537, -0.41158440709114075, 0.1823439747095108, 0.05350564047694206, 0.6204814910888672, -0.4957149922847748, 0.23977108299732208, 0.9183975458145142, 1.115576982498169, -0.31110879778862, 0.0930476188659668, 0.27579575777053833, 0.06582026183605194, -0.03363482654094696, 0.12311021983623505, 0.06927674263715744, 0.4100988805294037, 0.7004852294921875, -0.03909888491034508, 0.18245309591293335, -1.1759923696517944, -0.2044658213853836, 0.25954583287239075, 0.3446448743343353, 0.5729495286941528, 0.5135539770126343, -0.46347784996032715, -0.599769651889801, 0.1819954663515091, 0.6010329723358154, 1.7649637460708618, -0.4829500615596771, -0.2466934323310852, -0.781038761138916, -0.3124094009399414, -0.32037603855133057, 0.08466040343046188, -0.5868019461631775, 0.03232244402170181, -0.6701035499572754, -0.8287081122398376, 0.5186470150947571, 0.2526637017726898, 0.6753520369529724, -0.8341783285140991, -0.2965741455554962, -0.05046268180012703, -0.16399937868118286, -0.732623279094696, -0.9440781474113464, 0.13960511982440948, -0.7450860738754272, 0.019925976172089577, 0.07314114272594452, -0.08077291399240494, -0.11915211379528046, -0.9090325236320496, 0.47464093565940857, -0.46516838669776917, 0.1653503179550171, 0.09308812767267227, 0.1883680373430252, -0.5681372284889221, -0.909043550491333, 0.7386553287506104, 0.25373899936676025, -0.30659040808677673, 0.3275916576385498, 0.40469685196876526, -0.27128177881240845, -0.15118277072906494, -0.30412206053733826, 0.14715079963207245, 0.2777630388736725, -0.011215743608772755, 0.5440333485603333, -0.7460604906082153, 0.2483988255262375, -1.0426247119903564, 1.1647977828979492, -0.004056945443153381, -0.2512286901473999, 0.2629019021987915, -0.7840721607208252, -0.5302674174308777, 0.3068457841873169, -1.0929222106933594, -0.09964907169342041, -1.1852232217788696, 0.11720409989356995, -0.1048012375831604, -0.1386517882347107, 0.18767637014389038, 0.25596877932548523, 0.47539281845092773, 0.3226911127567291, 0.18621598184108734, 0.5258830785751343, -0.16449426114559174, 0.6573456525802612, -0.3712254762649536, 0.3246172368526459, 0.2863079607486725, -0.31887155771255493, -0.3517816364765167, -0.24093981087207794, -0.8990070223808289, -0.4695932865142822, -0.2738570272922516, -0.24308207631111145, -0.16516385972499847, 0.13372215628623962, -0.4050752520561218, -0.7925747632980347, -0.5545452237129211, -0.9197745323181152, -0.6283459067344666, 0.31633836030960083, -0.0281012374907732, -0.38968828320503235, -0.8762697577476501, -1.2149221897125244, -0.41705259680747986, -0.6465826034545898, -1.0890471935272217, 0.2837661802768707, -0.11406069993972778, -0.8401079773902893, -0.947533130645752, 0.16674883663654327, -0.40921273827552795, 1.0094695091247559, -0.4464411735534668, 0.7711225152015686, -0.08536287397146225, -0.05361147224903107, -0.5107730031013489, 0.5965672135353088, 0.6511425971984863, 0.07605196535587311, 0.366502046585083, -0.961736798286438, 0.13162963092327118, -0.22970061004161835, 0.052536170929670334, -0.06843969970941544, 0.36196330189704895, 0.5741835832595825, -0.6176649928092957, -0.5756116509437561, 0.21008352935314178, 1.0878732204437256, -0.8058282136917114, 0.08211373537778854, 0.282701313495636, 0.7536843419075012, 0.1330648809671402, -0.016687454655766487, 0.44739824533462524, 0.12444504350423813, 0.5928353071212769, 0.1316687911748886, 0.23652303218841553, 0.026001745834946632, -0.9075608849525452, 0.838297963142395, 1.7869460582733154, 0.2028903216123581, 0.19884870946407318, -0.8797419667243958, 0.3995535969734192, -1.1406705379486084, -0.8386885523796082, 1.0573726892471313, 0.7052513360977173, 0.8390628099441528, -0.5287911891937256, -0.15929006040096283, -0.2689996361732483, 0.3591172695159912, 0.7659649848937988, -0.0798870176076889, -0.7474213242530823, 0.4245510995388031, -0.14439643919467926, -0.2193388193845749, 1.233184576034546, -0.4726839065551758, 0.5434632897377014, 14.855511665344238, 0.8859665989875793, 0.05931984633207321, 0.643082320690155, 0.7564911246299744, -0.05224417522549629, -0.30305221676826477, -0.032584574073553085, -1.3266403675079346, 0.41438448429107666, 1.4925563335418701, 0.44720515608787537, 0.7098321318626404, 0.16972263157367706, 0.19144704937934875, 0.20850487053394318, -0.9643535614013672, 0.3473043739795685, 0.22683082520961761, -1.2544400691986084, 0.17551667988300323, 0.18432225286960602, 0.5076173543930054, 0.6091291904449463, 1.0149637460708618, 0.7180485129356384, 0.10581649839878082, -0.2425212413072586, 0.6362367272377014, 0.09005483239889145, 1.1462706327438354, -0.09863122552633286, 0.23726457357406616, 0.6805447936058044, -0.7658965587615967, -0.39772501587867737, -0.6664621829986572, -1.1766328811645508, 0.07188991457223892, -0.048824019730091095, -0.6922808289527893, -0.738582968711853, -0.2912357747554779, 0.6474220752716064, 0.03541193902492523, 0.0728951245546341, -0.17719990015029907, 0.46876081824302673, -0.025120792910456657, -0.3852556645870209, 0.6102283596992493, 0.1695137768983841, 0.39279118180274963, 0.1680406779050827, 0.2588363289833069, -0.20331904292106628, 0.008354696445167065, 0.5086205005645752, -0.4420328736305237, -0.040811605751514435, -0.29323703050613403, -0.12954780459403992, 0.3988223671913147, 0.6694889068603516, 0.9111899137496948, 0.14444410800933838, -0.24891436100006104, 0.3604350984096527, 0.6489136219024658, 0.48435890674591064, -0.027361877262592316, -0.14592435956001282, 0.8066614270210266, -0.4615747332572937, -0.26882562041282654, 0.4991931915283203, -0.026027699932456017, -0.3555668890476227, -0.4730595648288727, -0.26597610116004944, -0.13826647400856018, -0.6670721173286438, -0.46445271372795105, 0.7550318241119385, 0.026777099817991257, -0.5357241034507751, -0.10868019610643387, -0.6761800050735474, -0.4043736159801483, 0.5564061999320984, -1.4461398124694824, -0.7514832615852356, 0.6104350686073303, -0.512849748134613, -0.2204587459564209, 0.3189202547073364, 1.3171627521514893, 0.19145861268043518, -0.4156537652015686, 0.3291691243648529, 0.3273472487926483, -0.06113235652446747, 0.09462002664804459, -0.8981457948684692, 0.9056314826011658, -0.07413085550069809, -0.23087450861930847, 0.36602210998535156, -0.02657724730670452, 0.15973113477230072, -0.14171873033046722, -0.25841379165649414, 0.9239888787269592, -1.2919318675994873, -0.25037261843681335, -1.0032185316085815, -0.8839903473854065, 0.7447657585144043, 0.596094012260437, -0.11482415348291397, 0.4964628517627716, 0.2003733366727829, -0.36563295125961304, -0.06988038867712021, -0.5457023978233337, 0.47571778297424316, 0.23070700466632843, -1.0051345825195312, -0.47655656933784485, -0.11950447410345078, 0.7422096133232117, -1.2325536012649536, -0.7171875238418579, -0.3602921664714813, 0.33400392532348633, -0.08650714159011841, 0.9701669216156006, -0.34202030301094055, 0.7243428230285645, 1.0899511575698853, -0.3054409325122833, -0.7936352491378784, 0.10620617121458054, -1.0456846952438354, -0.3373066186904907, 0.17835234105587006, 0.9648159146308899, -0.48590460419654846, 0.32880404591560364, 0.6912704110145569, 0.34764233231544495, -0.9007664322853088, -0.7051091194152832, -0.26645395159721375, 0.30693185329437256, -0.6228693723678589, 0.5006491541862488, 0.015568339265882969, 0.04165443778038025, 0.07200302183628082, 0.4467765986919403, 0.6216468214988708, -0.18313007056713104, -0.9360847473144531, -0.16847319900989532, -0.05538702383637428, -0.2471320778131485, -0.6493951082229614, -0.09705851972103119, -1.3728083372116089, -0.16569171845912933, -1.1187474727630615, -0.22229072451591492, -0.6258206367492676, -0.5095567107200623, -0.08619070798158646, -0.3350317180156708, -0.14557616412639618, -0.03356873244047165, -0.5244291424751282, -0.32663893699645996, -0.43037518858909607, -0.36277487874031067, 0.8472925424575806, 0.9867036938667297, -0.49821242690086365, -0.1532726287841797, 0.38964083790779114, 0.2687246799468994, 0.17193688452243805, 0.6366851925849915, -0.3918677866458893, -0.9379485249519348, -1.2166049480438232, 0.6819379925727844, -0.3459157645702362, -0.034450892359018326, -0.4319226145744324, 0.3493826389312744, 0.2500784993171692, -0.11147269606590271, 0.013392371125519276, 0.5639719367027283, -0.9692608118057251, -0.5258879661560059, 0.13928119838237762, -1.1989200115203857, 0.34097936749458313, 0.0982593521475792, -0.31287702918052673, -0.09383872896432877, 0.5042653679847717, -0.23351885378360748, -0.7886047959327698, -0.4982275366783142, 0.42026495933532715, -0.384932279586792, 0.04839412122964859, -0.44966259598731995, -0.2497321367263794, -1.0909799337387085, -0.3479420244693756, -0.2502287030220032, 0.1029747799038887, -0.08057516813278198, 0.8704991340637207, 0.27914175391197205, -1.4744558334350586, 0.21629923582077026, 0.40896040201187134, -0.03249463811516762, 0.1331292986869812, 0.45432212948799133, 0.5138200521469116, -0.21563337743282318, 0.3084827959537506, 0.5250917673110962, 0.07629291713237762, -1.188928246498108, -0.1530156284570694, 0.4420846402645111, -0.38933902978897095, -0.25921985507011414, 1.113289713859558, -0.6223630309104919, -1.1916413307189941, 0.2392159253358841, -1.673630952835083, -0.4055210053920746, -0.25205299258232117, 0.7362987399101257, 0.2196001410484314, -0.41458603739738464, 0.19495917856693268, -0.4200575649738312, 0.11078424006700516, -0.039043180644512177, -0.5544014573097229, 0.385947048664093, -0.42914435267448425, -0.23404374718666077, 0.9923343062400818, 1.222778081893921, -0.47471317648887634, -1.3109380006790161, -0.7388604283332825, -0.19091956317424774, -0.0656052902340889, 0.13115203380584717, -0.3274085819721222, 0.13836337625980377, 0.8439405560493469, 0.39239731431007385, 0.2696011960506439, -0.045427534729242325, 0.16507917642593384, 0.20911504328250885, 0.7067772150039673, -0.14387929439544678, -0.8484993577003479, -0.4087745249271393, 1.2233449220657349, 1.2794572114944458, -1.103435754776001, 0.3930278718471527, 0.19757722318172455, -0.7105461955070496, 0.6447803974151611, 0.11886055022478104, 0.2671411335468292, 0.8857735395431519, -0.4951370656490326, 0.2580948770046234, 0.7407456040382385, -1.2937864065170288, 0.45338812470436096, 0.9363646507263184, 0.9414659738540649, 0.6540321707725525, 0.5449944734573364, 0.3571210205554962, 0.7224553823471069, -0.13121770322322845, -0.13388113677501678, 0.42066049575805664, 0.5439481139183044, -0.2798915505409241, -0.2784075438976288, 0.0646258071064949, 0.41193535923957825, -0.8339111804962158, -0.6391705870628357, 0.027823472395539284, 0.6329185366630554, -0.033881302922964096, 0.38958072662353516, 0.9226266145706177, 0.2266494184732437, 0.09605249762535095, 0.3810255527496338, 0.3156730830669403, -0.6271411776542664, -0.21813467144966125, -0.27901244163513184, -0.785586953163147, -0.1324511617422104, 0.06314632296562195, -0.5047463774681091, -0.07221410423517227, -0.12991219758987427, 0.31007471680641174, -0.04407262057065964, 0.19998769462108612, 0.9862338304519653, 0.7306318283081055, 0.37514612078666687, -0.20828507840633392, -0.6372032165527344, -0.49889105558395386, -1.153505802154541, 0.32254549860954285, -0.6750696897506714, 0.21596945822238922, 0.3526536524295807, -0.04925481230020523, -0.6076686978340149]}, "authors": [{"authorId": "2279654115", "name": "Yikai Zhang"}, {"authorId": "2278801242", "name": "Junlong Li"}, {"authorId": "2256991660", "name": "Pengfei Liu"}], "references": [{"paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54", "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "2dfb9171e180dcb0af23d305e024d43d311708ab", "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "f97413a497d47c739d41d237917e6566154647b4", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "594d8e1696619f3cebb7c6bffdad8e0a5592f006", "title": "Scaling Transformer to 1M tokens and beyond with RMT"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "d3dd80269f2542cc173afb3a1df24b582a1e4af2", "title": "Overcoming a Theoretical Limitation of Self-Attention"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": null, "title": "2023. Yarn: Efficient context window extension of large language models"}, {"paperId": null, "title": "bloc97. 2023a. Add NTK-Aware interpolation \"by parts\" correction"}, {"paperId": null, "title": "2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "2023. How long can open-source llms truly promise on context length?"}, {"paperId": null, "title": "2022. Memorizing transformers"}, {"paperId": null, "title": "2023. Focused transformer: Con-trastive training for context scaling"}, {"paperId": null, "title": "2023. Longnet: Scaling transformers to 1,000,000,000 to-kens"}, {"paperId": null, "title": "2023. Things I\u2019m learning while training superhot"}, {"paperId": null, "title": "bloc97. 2023b. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "2022. A length-extrapolatable transformer"}]}