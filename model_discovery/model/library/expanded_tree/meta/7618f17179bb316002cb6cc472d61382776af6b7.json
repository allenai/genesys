{"paperId": "7618f17179bb316002cb6cc472d61382776af6b7", "abstract": "We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms (<1%) BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61% faster. We also present a faster variant, prosaically named Fast Sparse Mixer, that marginally underperforms BERT on SuperGLUE, but trains and runs nearly twice as fast. We justify the design of these two models by carefully ablating through various mixing mechanisms, MoE configurations and hyperparameters. Sparse Mixer overcomes many of the latency and stability concerns of MoE models and offers the prospect of serving sparse student models, without resorting to distilling them to dense variants.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 10, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2205.12399", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "The Sparse Mixer encoder model overcomes many of the latency and stability concerns of MoE models and offers the prospect of serving sparse student models, without resorting to distilling them to dense variants."}, "embedding": {"model": "specter_v2", "vector": [-0.10470598936080933, 0.8699407577514648, -0.8463964462280273, 0.04990406334400177, -0.6585919260978699, -0.5334058403968811, 0.7491681575775146, -0.2047737091779709, 0.22280725836753845, -0.4701189398765564, 0.5638551115989685, 0.15447017550468445, 0.49507781863212585, 0.36127692461013794, -0.40794122219085693, 0.2620318830013275, -1.014872431755066, 0.11011755466461182, -0.04740490764379501, -0.18315929174423218, -0.4250369668006897, -0.7949351072311401, -1.0321695804595947, 0.4072953164577484, 0.5857039093971252, 0.782890796661377, -0.32070648670196533, 1.0258013010025024, -0.18256627023220062, 0.9315497875213623, 0.6552912592887878, -0.5266458988189697, 0.6142303347587585, -0.26773735880851746, 0.1307726353406906, -0.06767137348651886, 0.5061023235321045, -0.3417569398880005, -0.6248980760574341, 0.5104506611824036, -0.12196209281682968, 0.3454838991165161, 0.6161602735519409, -0.973736584186554, -0.11698457598686218, 0.9288355708122253, 0.6010949611663818, 0.7556865811347961, -0.28671398758888245, -0.5740299224853516, 1.1032649278640747, -1.2595875263214111, -0.05388663336634636, 1.446925163269043, 0.3333011567592621, 0.20194151997566223, -0.35254889726638794, -0.6131123304367065, 0.9289907217025757, -0.0018132698023691773, -0.48875927925109863, -0.7638593912124634, -0.0541946217417717, -0.23309959471225739, 1.1307693719863892, -0.4794655740261078, 0.06812910735607147, 1.1100211143493652, -0.3289961516857147, 1.4785821437835693, 0.19258266687393188, -0.5242713093757629, -0.007715664803981781, 0.7005111575126648, 0.12554048001766205, 1.0248538255691528, -0.35103440284729004, 0.37617769837379456, -1.3109616041183472, 0.03491388261318207, 0.45671194791793823, 0.008381403051316738, 0.19956889748573303, -0.5395745038986206, -0.1793542504310608, 0.9242632389068604, 0.3779917061328888, 0.8502954244613647, -0.2268098145723343, 0.9349068403244019, 0.2806535065174103, 0.1938389092683792, -0.10036022961139679, 0.20764248073101044, -0.16813458502292633, 0.13923752307891846, -1.1720211505889893, 0.04566328972578049, 0.5223311185836792, 0.9157583713531494, -0.5053427815437317, 0.3075847625732422, -0.5981466174125671, -0.2192731648683548, 1.8725210428237915, -0.15153875946998596, 0.4057529866695404, -0.9837836623191833, 0.30197688937187195, -1.035780668258667, -0.007758222054690123, -0.7630310654640198, 0.13299447298049927, -0.6898998022079468, -0.9978595972061157, -1.3254600763320923, -0.4091910421848297, 0.3217266798019409, -1.0015779733657837, 0.4925813376903534, -0.3378969430923462, 1.0441874265670776, 0.16752271354198456, 0.437795490026474, 0.9196599721908569, 0.8449614644050598, 0.3273712694644928, 0.34019240736961365, 0.8578982353210449, -1.1193000078201294, -0.5486381649971008, -1.3861578702926636, 0.35454532504081726, -0.2062404453754425, -0.33372965455055237, 0.23366981744766235, -1.3105854988098145, -1.0629594326019287, -0.947504997253418, 0.09552957117557526, -0.24841618537902832, 0.1670638918876648, 1.5694591999053955, 0.7910977005958557, -0.6617118716239929, 0.612083375453949, -0.8571144342422485, 0.09155155718326569, 0.516252338886261, 0.21499967575073242, 0.0760100930929184, -0.33525100350379944, -0.7928931713104248, 0.14005473256111145, 0.2419668287038803, -0.5249720811843872, -0.43153858184814453, -0.7449226975440979, -1.1033114194869995, 0.2818422317504883, 0.2714747488498688, -0.7605348229408264, 1.3666822910308838, -0.128274068236351, -1.4297153949737549, 0.4745274782180786, -0.5329700708389282, 0.1115024983882904, 0.4267362952232361, 0.01870816946029663, -0.6275138258934021, -0.13717493414878845, -0.2571616768836975, 0.4785327911376953, 0.6840235590934753, -0.4248790442943573, -0.10764110833406448, 0.18411992490291595, -0.44248315691947937, -0.5372797846794128, 0.08214341104030609, 0.6352167725563049, -0.18573109805583954, 0.02820543572306633, 0.49196410179138184, 0.9639070630073547, -0.3907449245452881, -0.4715406000614166, -0.15325796604156494, -0.6390003561973572, 0.6663063764572144, -0.011333093978464603, 0.7737334370613098, -1.0645148754119873, -0.7433117032051086, 0.3192298114299774, 0.20836910605430603, -0.2034691572189331, -0.7813460230827332, 0.1816115379333496, -0.5649905204772949, 0.1725524365901947, 0.005201658699661493, -1.1662096977233887, 0.18305604159832, -0.46242937445640564, -0.4888432025909424, -0.31923729181289673, 0.29645803570747375, 0.9680962562561035, -1.008909821510315, 0.06965316087007523, 0.0811198353767395, 0.46796050667762756, -1.686816930770874, 1.3053678274154663, -0.19566795229911804, 0.45930907130241394, -0.17179478704929352, 0.0331997387111187, -0.018792875111103058, -0.6062390804290771, 0.569040060043335, -0.3580488860607147, 0.49570798873901367, 0.40962138772010803, -1.3312677145004272, 1.4309614896774292, -0.5004483461380005, 0.437583327293396, 0.2538471817970276, -1.0895298719406128, 0.168669655919075, 0.7757755517959595, -0.04176051542162895, -0.2843112051486969, 0.5588145852088928, 0.9237229824066162, -0.6954546570777893, 0.15187011659145355, 0.4610559642314911, 0.6163143515586853, -0.09849759191274643, -0.33462297916412354, 1.2000620365142822, -0.3230743408203125, 0.38189297914505005, 0.6253343224525452, 0.5467950105667114, 0.4260328710079193, 0.2936626672744751, 0.25896501541137695, -0.17261779308319092, -1.2612371444702148, 0.0933699682354927, 0.6748024225234985, 0.5707507729530334, 0.7738329172134399, 0.3495240807533264, -0.3418281674385071, -0.1613810509443283, -0.23646116256713867, 0.9074917435646057, 1.2183418273925781, -0.0896318331360817, -0.39561477303504944, -0.3335571587085724, -0.47864383459091187, -0.2851655185222626, 0.05694396793842316, 0.0004729651554953307, -0.2773360311985016, -0.6169425845146179, -0.986655056476593, 0.30661821365356445, 0.3702680468559265, 0.9590151309967041, -0.06909868866205215, -0.07223539799451828, -0.2192252278327942, 0.05183770880103111, -0.7053884863853455, -0.43660810589790344, 0.5676955580711365, -0.7700883150100708, -0.1708540916442871, 0.07811862230300903, -0.1564243584871292, -0.031035855412483215, -0.8677964806556702, 1.1099354028701782, -0.7517721652984619, -0.13044647872447968, 0.4078631103038788, 0.5025827288627625, -0.21504051983356476, -0.4500584006309509, 0.15872956812381744, 0.07067292183637619, 0.1658424437046051, 0.1349700391292572, -0.1358308345079422, -0.0784793347120285, 0.0017539587570354342, -0.1831110119819641, -0.11391668766736984, 0.157308891415596, -0.27639099955558777, 0.3105146884918213, -0.34238898754119873, -0.03502540662884712, -1.1844136714935303, 0.2197674661874771, -0.5782697200775146, -0.3095436692237854, -0.18519896268844604, -0.5045086741447449, -0.20076695084571838, 0.31614696979522705, -0.8598991632461548, -0.19093486666679382, -0.6832314133644104, 0.267096608877182, -0.20913003385066986, -0.3704887926578522, 0.018059024587273598, 0.2751939594745636, 0.09417836368083954, 0.41614416241645813, 0.023129228502511978, -0.13448256254196167, -0.30819082260131836, 1.0227793455123901, -1.040773868560791, 0.7037515044212341, 0.11911047250032425, 0.34795644879341125, -0.2161407619714737, -0.2587414085865021, -0.31023305654525757, -0.5744300484657288, -0.5579233765602112, -0.08024182170629501, -0.12631136178970337, 0.17703302204608917, -1.1117780208587646, -1.1744705438613892, -0.3372255563735962, -0.8166466355323792, -0.14506669342517853, -0.039933156222105026, 0.310621976852417, -0.32596904039382935, -1.2816740274429321, -1.2051644325256348, -0.71686190366745, -0.019601143896579742, -1.1429797410964966, 0.4556918740272522, 0.5109208226203918, -0.4559161365032196, -0.541563868522644, 0.048338886350393295, -0.17945611476898193, 1.1388797760009766, -0.4460691213607788, 0.0549645721912384, -0.3617861568927765, -0.8540551662445068, -0.2653515040874481, 0.461233526468277, 0.3982214033603668, -0.6080489754676819, 0.4085494577884674, -1.0995985269546509, 0.2626018822193146, -0.4694150984287262, -0.4044129550457001, 0.40669819712638855, 0.627774715423584, 0.3944777548313141, -0.1289830058813095, -0.44674405455589294, 0.8039364814758301, 1.0981597900390625, -0.491525799036026, 0.04878157377243042, 0.014835064299404621, 0.9023735523223877, 0.1746739149093628, -0.6658830046653748, 0.4661587178707123, 0.3882243037223816, 0.1090279221534729, 0.15728984773159027, 0.19395753741264343, 0.03277404606342316, -0.8986209630966187, 0.9785947799682617, 1.339323878288269, 0.3478529751300812, -0.12794722616672516, -0.5167866945266724, 0.4855297803878784, -1.3268256187438965, -0.8880360126495361, 0.5492491722106934, 0.5782377123832703, -0.02151472121477127, -0.5911543369293213, -0.06214357540011406, -0.19997210800647736, 0.11900459975004196, 0.5501758456230164, 0.0630926713347435, -0.4763261675834656, 0.3431696891784668, 1.0556597709655762, 0.2229088842868805, 0.5913666486740112, -0.15894673764705658, 0.41851040720939636, 14.9814453125, 0.7111492156982422, 0.11865182965993881, 0.7323737144470215, 0.828122079372406, 0.0499124750494957, -0.3736121952533722, 0.13800381124019623, -1.0960558652877808, 0.17617212235927582, 1.533168077468872, 0.6879456639289856, 0.5595487952232361, 0.4739442765712738, -0.25734347105026245, 0.2520574629306793, -0.7544553279876709, 0.8711357116699219, 0.4077630937099457, -1.2398390769958496, -0.1829446703195572, -0.0543251559138298, 0.7594420313835144, 0.23779965937137604, 1.028603196144104, 1.1458858251571655, 0.8703029751777649, -0.4946005344390869, 0.5610455870628357, 0.3678750991821289, 0.4401039183139801, -0.1557067483663559, 0.2916609048843384, 0.30918988585472107, -0.9578058123588562, -0.34484514594078064, 0.09832814335823059, -0.8567109107971191, 0.2849583625793457, 0.30557897686958313, -0.1534169763326645, -0.3013884425163269, -0.07229354232549667, 0.7865049242973328, 0.21082505583763123, 0.6150336265563965, -0.31308937072753906, 1.0058428049087524, -0.2958535850048065, 0.7139041423797607, 0.31259721517562866, 0.3422064483165741, 0.03044828400015831, -0.21837148070335388, 0.40498608350753784, -0.04671640321612358, 0.32340219616889954, 0.506990909576416, -0.5229163765907288, -0.3836175203323364, -0.09033788740634918, -0.5733978748321533, -0.20200340449810028, 1.0044951438903809, 0.3507207930088043, -0.136124387383461, -0.4410315752029419, 0.5072427988052368, 0.18910157680511475, 0.26428139209747314, -0.29532405734062195, 0.1352357715368271, 0.14718161523342133, -0.6420379281044006, -0.017680546268820763, 0.6654067039489746, 0.01629328541457653, -0.9063940644264221, -0.9277088642120361, -0.8518105149269104, 0.6149457693099976, -0.53182452917099, -0.7259300351142883, 0.40611568093299866, -0.36738407611846924, -0.4378325343132019, 0.013713368214666843, -0.5698265433311462, -0.5303671360015869, 0.29523542523384094, -1.1085293292999268, -0.5858345627784729, 0.17007744312286377, -0.43059396743774414, -0.5084020495414734, -0.16748209297657013, 0.9130339622497559, 0.31377503275871277, -0.26782965660095215, -0.10280147194862366, -0.08531305938959122, -0.15654730796813965, 0.027661588042974472, -0.7885782122612, 0.7594690322875977, 0.5400688648223877, 0.4639311134815216, 0.10506854951381683, 0.07578408718109131, 0.3577057719230652, -0.41996484994888306, 0.3899550139904022, 0.6585900187492371, -1.2599718570709229, -0.3044770061969757, -0.6922091841697693, -0.3669987618923187, 0.8748127818107605, 0.4781270921230316, 0.23476137220859528, 0.503516674041748, 0.18286815285682678, -0.9661738276481628, -0.24180598556995392, -0.914530873298645, -0.19811192154884338, 0.1757732778787613, -0.6182504892349243, -0.3388121724128723, -0.2013384848833084, 0.10764391720294952, -1.084985375404358, -0.356682151556015, -0.3627350628376007, 0.2562853991985321, -0.4712055027484894, 1.1723942756652832, -0.3615167438983917, 0.6050944924354553, 0.9633073210716248, -0.17079675197601318, -1.0375171899795532, 0.11341004073619843, -1.1237256526947021, -0.6296072602272034, 0.3339376747608185, 0.26338836550712585, -0.435210645198822, 0.4862723648548126, 0.6398519277572632, 0.37957561016082764, 0.014948594383895397, -0.3286501169204712, -0.14417441189289093, -0.3940484821796417, -0.41766148805618286, 0.0056267124600708485, 0.10535469651222229, -0.05823110044002533, 0.06740254163742065, 0.2712641656398773, 0.7560036182403564, 0.07913585007190704, -0.9094839692115784, 0.5824248790740967, -0.2760164141654968, -0.7649685144424438, -0.5418252944946289, -0.5741264820098877, -1.4104071855545044, -0.23819883167743683, -1.249537467956543, -0.0643591657280922, -0.6097822189331055, -0.5459303855895996, 0.23184339702129364, -0.4124951660633087, -0.24559056758880615, 0.13965143263339996, 0.10164535045623779, -0.08912204951047897, -0.9994504451751709, -0.6118528246879578, 0.5417634844779968, 0.6873295307159424, -0.6892822980880737, -0.17809514701366425, 0.08659106492996216, -0.2729138135910034, 0.438662588596344, 0.1731555163860321, -0.6604650616645813, -0.40415069460868835, -0.7382170557975769, 0.37806278467178345, 0.13548405468463898, 0.256712943315506, -0.919858992099762, 0.4570000469684601, 0.596368670463562, 0.16325783729553223, 0.2940317392349243, 0.29802873730659485, -1.1562306880950928, -0.1009593978524208, 0.3001612424850464, -0.657211184501648, -0.1747189164161682, -0.055341966450214386, -0.39020633697509766, -0.28264978528022766, 0.7030439972877502, 0.09777259826660156, -1.1013020277023315, -0.5070185661315918, 0.542786180973053, -0.5025482177734375, 0.20722170174121857, -0.3727041780948639, 0.005758893676102161, -0.9271151423454285, -0.5072007179260254, -0.42214059829711914, 0.3550669252872467, -0.8158102631568909, 0.8574210405349731, -0.1720750778913498, -1.0459100008010864, 0.2110929787158966, 0.49980151653289795, -0.2741818130016327, 0.09717808663845062, 0.6116599440574646, 0.5681087374687195, -0.23944048583507538, 0.12858062982559204, 0.367063969373703, 0.4029437303543091, -0.35108888149261475, 0.11136885732412338, 0.6799080967903137, -0.6552881002426147, 0.08682036399841309, 1.3172067403793335, -0.5040053725242615, -1.0390652418136597, 0.44068676233291626, -1.2105633020401, -0.31010401248931885, 0.0016681115375831723, 0.49684977531433105, 0.5407196283340454, -0.40011298656463623, 0.1970231831073761, -0.679373562335968, 0.2023829221725464, 0.28089842200279236, -0.6078823208808899, 0.7482420206069946, 0.017649957910180092, -0.04520415514707565, 0.6605846285820007, 0.7683621644973755, -0.19205845892429352, -0.7432588934898376, -0.43069779872894287, -0.4907495081424713, -0.2173161655664444, 0.31320953369140625, -0.4770278334617615, -0.8702836036682129, 0.8835093975067139, 0.6716331243515015, 0.02091563306748867, -0.094542495906353, 0.09699521213769913, 0.06279201060533524, 0.4484404921531677, 0.04190422594547272, -0.30250486731529236, -0.20372286438941956, 0.6011633276939392, 0.31439781188964844, -1.1449403762817383, -0.053594376891851425, -0.37582898139953613, -0.4981744587421417, 0.7495365738868713, 0.2471097856760025, -0.2857438921928406, 0.6243615746498108, -0.33976197242736816, 0.06351825594902039, 0.2499094009399414, -0.8203288912773132, -0.30466195940971375, 1.0079944133758545, 0.8526638746261597, 0.7738118171691895, 0.33022627234458923, 0.47785401344299316, 0.983065664768219, 0.18380029499530792, 0.13870055973529816, 0.25656193494796753, 0.18024541437625885, -0.12053005397319794, -0.19754080474376678, -0.10757141560316086, 0.6878646016120911, -0.8649212121963501, -0.37599635124206543, 0.4911714196205139, 0.480796754360199, 0.6686023473739624, 0.8878666162490845, 0.6009155511856079, 0.3938446342945099, 0.412053644657135, 0.25726258754730225, 0.830750048160553, -0.5240218043327332, -0.37896960973739624, 0.04834383353590965, -0.7838744521141052, -0.6058176159858704, -0.23916828632354736, -0.023487402126193047, -0.006280765403062105, -0.3101135194301605, 0.5051959156990051, 0.140369713306427, 0.2248346358537674, 1.1747111082077026, 0.4473668336868286, 0.7685967683792114, -0.028331831097602844, -0.7549928426742554, -0.17099560797214508, -0.7302637696266174, -0.2480512261390686, -0.6925573945045471, -0.01702885329723358, -0.17856410145759583, -0.6913948059082031, -0.1899264007806778]}, "authors": [{"authorId": "1405626394", "name": "J. Lee-Thorp"}, {"authorId": "1643737606", "name": "J. Ainslie"}], "references": [{"paperId": "499d3bb3acbc10730dd6582bd9b8f646bf22ccd5", "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"}, {"paperId": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "title": "Designing Effective Sparse Expert Models"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "a170bb9e077b2efa9cc61643d17710cc570ff93a", "title": "PyGlove: Symbolic Programming for Automated Machine Learning"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "07a9f47885cae97efb7b4aa109392128532433da", "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "57f123c95ecf9d901be3a53291f53302740451e2", "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks"}, {"paperId": "a42ca00fc188beb5586ad4c7108b70aeb5317da0", "title": "Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms"}, {"paperId": "c6a537a31ae14aa440d95239c20d84b17cd5c027", "title": "On Circulant Matrices"}, {"paperId": "f8e9b050c93af6dea582563f61b6460b590bc3af", "title": "The Design and Implementation of FFTW3"}, {"paperId": "f6d8a7fc2e2d53923832f9404376512068ca2a57", "title": "Hierarchical Mixtures of Experts and the EM Algorithm"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "0e6beb95b5150ce99b108acdefabf70ccd3fee30", "title": "An algorithm for the machine calculation of complex Fourier series"}, {"paperId": null, "title": "2022) recommend using a smaller scaled weight initialization to provide stability to MoE encoder-decoder models, especially in larger configurations"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for JAX"}, {"paperId": null, "title": "ELECTRA: pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2019. BERT: Pre-training"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "Signature veri-\ufb01cation using a\" siamese\" time delay neural network"}, {"paperId": null, "title": "Optimizing fine-tuning learning protocols for MoE models. The \"Default LR"}, {"paperId": null, "title": "we fine-tune all but the MoE layers; and for \"Dense MLP\", we only fine-tune the dense MLP sublayers. The default model is repeated in several rows: \"0.2 ex dropout"}, {"paperId": null, "title": "of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages"}]}