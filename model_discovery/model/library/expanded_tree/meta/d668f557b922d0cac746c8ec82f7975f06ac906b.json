{"paperId": "d668f557b922d0cac746c8ec82f7975f06ac906b", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.", "venue": "arXiv.org", "year": 2024, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated."}, "embedding": {"model": "specter_v2", "vector": [0.1653679609298706, 0.49757662415504456, -0.45887160301208496, -0.3130345344543457, -0.4478099048137665, -0.09422428905963898, 0.9391272068023682, -0.16256064176559448, -0.5629141926765442, -0.171737939119339, 0.41337740421295166, -0.28623032569885254, 0.7726706266403198, 0.15782180428504944, -0.05865449458360672, 0.16081389784812927, -0.7977154850959778, 0.28611576557159424, 0.15714387595653534, -0.5160091519355774, 0.2563408613204956, -0.6072314977645874, -1.0768921375274658, 0.48621153831481934, 0.04814217612147331, 0.8556841015815735, 0.4352531433105469, 0.928104817867279, -0.811122715473175, 0.6387396454811096, 0.31216561794281006, 0.13142970204353333, -0.020533084869384766, -0.03669924661517143, -0.5398968458175659, -0.22005540132522583, 0.8939470648765564, -0.5504932999610901, -0.7626273036003113, 0.5533758401870728, -0.3551287353038788, 0.36586809158325195, 0.26682719588279724, -0.839171826839447, -0.029544200748205185, 0.8244299292564392, 0.927108883857727, 0.9416482448577881, -0.5616427659988403, -0.44402074813842773, 1.129690170288086, -1.0990949869155884, -0.11454534530639648, 1.4122905731201172, 0.23988936841487885, 0.9212824702262878, -0.27519553899765015, -0.7380622625350952, 1.0435881614685059, 0.4892725646495819, -0.7772253155708313, -0.6237102746963501, -0.001913893735036254, -0.05979141965508461, 2.027869939804077, -0.06957367062568665, 0.24120549857616425, 0.6900415420532227, -0.19564945995807648, 1.5798957347869873, 0.13131099939346313, -0.8160092234611511, -0.07580432295799255, 0.059414591640233994, 0.5297817587852478, 0.8632148504257202, -0.4784140884876251, -0.0039046539459377527, -1.09581458568573, 0.011011848226189613, 0.4130483567714691, -0.09244901686906815, 0.40281182527542114, 0.03258610889315605, -0.6551510691642761, 0.7608814835548401, 0.07819150388240814, 0.9225064516067505, -0.04602251201868057, 1.04720139503479, 0.45191842317581177, 0.19817766547203064, -0.1154324859380722, 0.5484036207199097, -0.1840965300798416, 0.1669866144657135, -0.9718385338783264, 0.2512311041355133, -0.19404612481594086, 0.7859377264976501, -0.4063473641872406, 0.6185839772224426, -0.7448036074638367, -0.031717266887426376, 1.540533185005188, 0.27840951085090637, 0.5193592309951782, -0.7370695471763611, 0.22519391775131226, -0.8352199196815491, 0.28052079677581787, -0.28835007548332214, -0.20866699516773224, 0.06886892020702362, -0.3124130666255951, -1.1112399101257324, -0.15479664504528046, 0.5570246577262878, -0.8316375017166138, 0.7879402041435242, -0.28310614824295044, 0.40943560004234314, 0.14395101368427277, 0.36622005701065063, 0.6897910237312317, 0.9955184459686279, 0.43901219964027405, 0.0869462862610817, 0.8972745537757874, -0.8086409568786621, -0.9456117153167725, -1.2134920358657837, 0.7523953914642334, 0.2487143576145172, 0.35413599014282227, -0.09963241219520569, -1.5450489521026611, -0.7811130285263062, -1.0582510232925415, 0.09732776880264282, -0.6372304558753967, 0.19733908772468567, 1.0965185165405273, 0.4867335855960846, -1.152449131011963, 0.385781466960907, -0.6068428754806519, -0.06500250846147537, 0.40772104263305664, 0.029745223000645638, 0.4012077748775482, -0.23005418479442596, -1.3781508207321167, 0.6985858082771301, 0.5119736790657043, -0.34713950753211975, -0.5228586196899414, -0.6322433948516846, -1.2901692390441895, 0.0018749109003692865, 0.3090026378631592, -0.14954592287540436, 1.4756642580032349, 0.10954786837100983, -1.4066566228866577, 0.5982136130332947, -0.4307005703449249, -0.12679798901081085, -0.0967264249920845, -0.024156121537089348, -0.6004983186721802, -0.46881285309791565, -0.802630603313446, 0.7752202749252319, 0.5069700479507446, -0.04653209447860718, -0.509048581123352, 0.2488277703523636, -0.21863430738449097, -0.07632242888212204, -0.3604678213596344, 0.9049066305160522, -0.18954582512378693, -0.10188354551792145, 0.45546743273735046, 0.6383759379386902, -0.046744849532842636, -0.06445253640413284, -0.3909284770488739, -0.8457319736480713, 0.6278090476989746, -0.28109419345855713, 1.2476816177368164, -0.9235110282897949, -0.7812193036079407, -0.2874510586261749, -0.3089262545108795, -0.0227254219353199, -0.7479063272476196, 0.2339426428079605, 0.10340577363967896, 0.37253084778785706, -0.10694798082113266, -1.2200695276260376, 0.11184273660182953, -0.35561713576316833, -0.7354562878608704, -0.18138347566127777, -0.0288286991417408, 0.8963200449943542, -0.8935810923576355, -0.23313704133033752, 0.37563765048980713, -0.03411609306931496, -0.8874510526657104, 1.043555498123169, -0.704264760017395, 0.2443397045135498, 0.007854833267629147, -0.21541041135787964, -0.10355085879564285, -0.3573080599308014, 0.7260768413543701, -0.2725194990634918, -0.06271566450595856, 0.9726638793945312, -0.5701276063919067, 1.798467755317688, -0.5690637230873108, 0.8725353479385376, -0.011499624699354172, -0.6399399638175964, -0.03681357577443123, 0.3597235381603241, -0.5469685196876526, -0.6947320699691772, 0.17906567454338074, 0.3524636924266815, -0.5716767907142639, 0.4886513352394104, 0.9150916337966919, 1.133798599243164, -0.4587657153606415, 0.08975926041603088, 0.37570032477378845, -0.3413003087043762, 0.13140380382537842, 0.6595665812492371, 0.5854556560516357, 0.08589209616184235, 0.17865605652332306, -0.3390066921710968, 0.2594278156757355, -0.6635373830795288, -0.28898897767066956, 0.6832759976387024, 0.4079119563102722, 0.7207522392272949, 0.1188201829791069, -0.6384848356246948, -0.3045113980770111, -0.22816349565982819, 0.5894349217414856, 1.5761806964874268, 0.18567988276481628, -0.14640772342681885, -0.694148063659668, -0.06534097343683243, -0.5708850026130676, 0.41951292753219604, -0.198629230260849, -0.08153215050697327, -0.9785195589065552, -0.7359968423843384, 0.7226940393447876, 0.538378119468689, 0.9998591542243958, -0.8146844506263733, -0.5800176858901978, -0.3718189001083374, 0.2515198886394501, -0.934037983417511, -0.42059075832366943, 0.11284239590167999, -0.8278082609176636, -0.02975703589618206, 0.5763417482376099, -0.2031729817390442, -0.003673071740195155, -0.637173593044281, 0.9066532850265503, -0.426437646150589, -0.287288099527359, 0.058750346302986145, 0.7519049644470215, -0.4504593312740326, -0.8698486685752869, 0.3042498528957367, 0.15270453691482544, -0.4255773723125458, 0.15584351122379303, 0.5503005981445312, 0.15599320828914642, -0.13419222831726074, -0.19775137305259705, 0.4649263918399811, 0.09957028925418854, -0.05560093745589256, 0.6335872411727905, -0.5991119146347046, 0.16377556324005127, -1.0284576416015625, 0.4884580075740814, 0.229069322347641, -0.7687089443206787, 0.43863120675086975, -0.5046209096908569, -0.21556885540485382, 0.04091168940067291, -0.804214596748352, -0.47151386737823486, -0.6301700472831726, 0.35397985577583313, -0.3427634835243225, -0.3641120195388794, 0.07595404237508774, 0.20185579359531403, 0.27652454376220703, 0.0255715511739254, 0.7517040967941284, 0.41168659925460815, -0.024102672934532166, 0.7073788642883301, -0.7945843935012817, 0.45781439542770386, 0.4805492162704468, -0.10270722210407257, -0.06125585362315178, -0.14671385288238525, -0.839668869972229, -0.4157021939754486, -0.22107645869255066, 0.12771424651145935, -0.07341879606246948, 0.0547107495367527, -0.5842728018760681, -0.9215470552444458, 0.38747185468673706, -1.0385687351226807, -0.6668599247932434, 0.3906034231185913, -0.23752461373806, -0.18568645417690277, -1.1151907444000244, -1.5270476341247559, -0.8060252666473389, -0.6048650145530701, -0.8936439156532288, 0.10607606172561646, 0.028610413894057274, -0.3826634883880615, -0.5716128945350647, -0.10749432444572449, -0.45618727803230286, 1.2584984302520752, -0.806935727596283, 0.9279722571372986, -0.1981453150510788, -0.40521475672721863, -0.1464819759130478, 0.6455321311950684, 0.27565330266952515, -0.3741775155067444, 0.27496370673179626, -1.0091344118118286, 0.4095664322376251, -0.35123759508132935, -0.353091835975647, 0.31824415922164917, 0.6766483187675476, 0.8132013082504272, 0.054113466292619705, -0.3917393684387207, 0.28657257556915283, 1.0968215465545654, -0.3306379020214081, 0.13685019314289093, -0.04243135452270508, 0.9545560479164124, 0.07398150116205215, -0.08265531808137894, 0.42056533694267273, 0.3695572018623352, 0.4582614004611969, 0.520503580570221, 0.3548744320869446, 0.1134781539440155, -0.1986457258462906, 0.41086071729660034, 1.320567011833191, 0.6907165050506592, -0.01635558530688286, -0.8052107691764832, 0.773052453994751, -1.1609282493591309, -0.7404203414916992, 0.7778475880622864, 0.8464367985725403, 0.6214240789413452, -0.4340994656085968, -0.17223654687404633, 0.010177084244787693, 0.44970786571502686, 0.5710416436195374, -0.11198586225509644, -0.7589290142059326, 0.03752906993031502, 0.361357718706131, -0.06720235198736191, 0.8271488547325134, -0.5793974995613098, 0.8324351906776428, 14.731247901916504, 1.0473949909210205, -0.2624247670173645, 0.5114730596542358, 0.6319507956504822, -0.2190673053264618, -0.38700008392333984, -0.35362666845321655, -1.3213355541229248, 0.10187990963459015, 1.5257298946380615, 0.687342643737793, 0.8033557534217834, 0.1741330921649933, -0.3085840344429016, 0.11527137458324432, -0.7147030234336853, 0.6765445470809937, 0.3593539297580719, -1.7733197212219238, 0.003061467781662941, -0.17628444731235504, 0.10509286820888519, 0.27780386805534363, 1.0368671417236328, 1.043972134590149, 0.26139113306999207, -0.47917410731315613, 0.8085054159164429, 0.21615712344646454, 1.0818631649017334, 0.03841550275683403, -0.09079670161008835, 0.607438862323761, -0.8465237021446228, -0.13650187849998474, -0.3648405373096466, -1.0325510501861572, 0.16246694326400757, -0.35813602805137634, -0.6355826258659363, -1.0233608484268188, -0.21506790816783905, 0.08146178722381592, 0.24218270182609558, 0.32629290223121643, -0.08917553722858429, 0.8418254256248474, -0.13277630507946014, -0.4065544307231903, 0.3059599995613098, 0.6729962229728699, 0.16081666946411133, 0.17883490025997162, -0.049878694117069244, 0.16966599225997925, -0.02938414178788662, 0.5663695335388184, -0.1626230925321579, -0.30860719084739685, -0.04162551835179329, -0.5952371954917908, 0.4427090287208557, 0.8212432861328125, 0.37994876503944397, 0.1024685949087143, -0.21072842180728912, 0.10158160328865051, 0.4524805545806885, 0.03222467750310898, -0.4150577783584595, 0.11410995572805405, 0.6937143206596375, -0.7030090689659119, 0.2689068019390106, 0.41207265853881836, -0.2622339427471161, -0.48897743225097656, -0.9225665330886841, -0.43955788016319275, 0.41342470049858093, -0.6992859840393066, -0.28547975420951843, 0.8599913716316223, 0.12210284918546677, -0.031772978603839874, -0.08391530811786652, -0.7622674107551575, -0.2584195137023926, 0.42250025272369385, -1.2504322528839111, -0.6564939618110657, 0.20675790309906006, -0.18134303390979767, -0.021228652447462082, 0.06292198598384857, 1.479795217514038, -0.06830783933401108, -0.38366153836250305, 0.1024622842669487, 0.06928764283657074, -0.17051544785499573, -0.497567355632782, -0.7853667140007019, 1.1223376989364624, 0.2507326304912567, 0.12512734532356262, 0.10195378959178925, -0.04840704798698425, 0.3548177182674408, -0.8509790897369385, 0.03221753239631653, 0.5681502223014832, -1.1777689456939697, -0.47354254126548767, -1.075714349746704, -0.8491079211235046, 0.5684704184532166, 0.44881314039230347, -0.04910559207201004, -0.029494239017367363, 0.027211328968405724, -0.6533883213996887, -0.07209067046642303, -0.7180012464523315, 0.03202188387513161, 0.5315166115760803, -0.8677800297737122, -0.48837557435035706, -0.43585464358329773, 0.36347445845603943, -1.1146233081817627, -0.3124077022075653, -0.3543531596660614, 0.20642134547233582, 0.03032311052083969, 1.0753095149993896, -0.6321320533752441, 0.5458601713180542, 1.1138756275177002, -0.5015616416931152, -0.7901444435119629, 0.08873603492975235, -0.5602841377258301, -0.657596230506897, 0.012398906983435154, 0.5997930765151978, -0.42163515090942383, 0.3545069098472595, 0.7406744360923767, 0.1168120950460434, -0.5911076664924622, -0.7397628426551819, -0.32940873503685, -0.0038552030455321074, -0.597869873046875, 0.4621663987636566, 0.18720316886901855, -0.07277274876832962, 0.06692485511302948, 0.5181263089179993, 0.6036984920501709, -0.48679453134536743, -0.6236959099769592, 0.02915121801197529, 0.15187212824821472, -0.17262347042560577, -0.805831253528595, -0.5742426514625549, -1.1793807744979858, 0.18281544744968414, -0.7857468128204346, 0.3068529963493347, -0.799159586429596, -0.518666684627533, -0.2239658683538437, -0.576532244682312, 0.06498579680919647, 0.04781731963157654, -0.5628436803817749, -0.27323612570762634, -0.7981725931167603, -0.6613700985908508, 0.4607276916503906, 0.42018967866897583, -0.7318757176399231, -0.027517253533005714, -0.13420382142066956, 0.41356295347213745, 0.08284853398799896, 0.5013567209243774, -0.16878122091293335, -1.0705690383911133, -1.2670583724975586, 0.16720840334892273, 0.06749915331602097, -0.14748427271842957, -0.8333848118782043, 0.6001100540161133, 0.01106281392276287, -0.4320438802242279, -0.06962529569864273, 0.9773658514022827, -0.5920189023017883, -0.6061143279075623, 0.5344323515892029, -1.1021147966384888, 0.051962871104478836, 0.7497451901435852, -0.6682128310203552, -0.10828352719545364, 0.5929631590843201, -0.17996647953987122, -0.9771110415458679, -1.1855955123901367, 0.22284585237503052, -0.9399069547653198, 0.06091221794486046, -0.35999608039855957, -0.11603080481290817, -1.2386173009872437, -0.43110203742980957, 0.07943394780158997, 0.2615371644496918, -0.44236916303634644, 0.6909953355789185, 0.6292373538017273, -0.9544446468353271, 0.10516631603240967, 0.2883414328098297, 0.0219159834086895, 0.12181778997182846, 0.5909689664840698, 0.4424538016319275, -0.3025206923484802, 0.6316362023353577, -0.041004784405231476, 0.17969241738319397, -0.8496534824371338, -0.013902014121413231, 0.8076934218406677, -0.06429166346788406, 0.008192221634089947, 1.339678406715393, -0.2974030077457428, -0.8485903739929199, 0.4854300916194916, -1.4875742197036743, -0.5151292085647583, -0.47573861479759216, 0.7624073028564453, 0.13855288922786713, -0.19231845438480377, 0.16074822843074799, -0.5601087808609009, -0.22004909813404083, -0.03317299112677574, -0.8469768166542053, 0.0359596386551857, 0.1553345024585724, -0.1066633015871048, 1.019581913948059, 0.6887702345848083, -0.6471689939498901, -0.7393243908882141, -0.6659033298492432, -0.42510557174682617, -0.05657624825835228, 0.48680296540260315, -0.4402187466621399, -0.5526121854782104, 1.0455067157745361, 0.5627731084823608, 0.2963649332523346, 0.0029515314381569624, -0.1974295973777771, 0.2841786742210388, 0.8548904061317444, 0.17363886535167694, -0.29260367155075073, -0.6020362377166748, 1.2607808113098145, 1.0051004886627197, -1.0868642330169678, -0.03448619693517685, -0.18850381672382355, -0.5742456912994385, 0.599646270275116, 0.6566895246505737, -0.08319364488124847, 0.7776005268096924, -0.42222580313682556, 0.17673444747924805, 0.22364884614944458, -1.3318462371826172, -0.04611729457974434, 0.7722466588020325, 0.8313625454902649, 0.38384321331977844, 0.43508294224739075, 0.4165956676006317, 0.87347811460495, 0.2810058891773224, -0.40759778022766113, 0.30159491300582886, 0.8988187909126282, -0.36290863156318665, 0.35051751136779785, 0.08163172006607056, 0.7430862188339233, -0.39509642124176025, -0.9059975743293762, 0.21016111969947815, 0.8871235847473145, -0.23277881741523743, 0.5243511199951172, 1.3905991315841675, 0.0642491802573204, 0.08710899204015732, 0.34373462200164795, 1.1159149408340454, -0.4576052129268646, -0.33904749155044556, -0.5440945029258728, -0.4078962504863739, -0.3089306354522705, 0.03216749429702759, -0.4880306124687195, -0.5791937708854675, -0.30840936303138733, 0.18997815251350403, -0.07712789624929428, 0.6575451493263245, 1.241500735282898, 0.42650893330574036, 0.3811703622341156, -0.6346411108970642, -0.394925594329834, -0.6692643761634827, -1.1258195638656616, 0.2009032666683197, -0.6851996779441833, -0.0689411610364914, -0.18741953372955322, 0.10807070136070251, -0.4852585792541504]}, "authors": [{"authorId": "103967638", "name": "R. Waleffe"}, {"authorId": "145965455", "name": "Wonmin Byeon"}, {"authorId": "2305812595", "name": "Duncan Riach"}, {"authorId": "2305804179", "name": "Brandon Norick"}, {"authorId": "3111334", "name": "V. Korthikanti"}, {"authorId": "2269146652", "name": "Tri Dao"}, {"authorId": "2269161650", "name": "Albert Gu"}, {"authorId": "31374559", "name": "Ali Hatamizadeh"}, {"authorId": "2306467514", "name": "Sudhakar Singh"}, {"authorId": "2305820716", "name": "Deepak Narayanan"}, {"authorId": "2305813696", "name": "Garvit Kulshreshtha"}, {"authorId": "2306162066", "name": "Vartika Singh"}, {"authorId": "2260131747", "name": "Jared Casper"}, {"authorId": "2248128248", "name": "Jan Kautz"}, {"authorId": "1911755", "name": "M. Shoeybi"}, {"authorId": "2264406909", "name": "Bryan Catanzaro"}], "references": [{"paperId": "ca9f5b3bf0f54ad97513e6175b30497873670fed", "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality"}, {"paperId": "3dd6bc488283f1e4cc967d98a6a6c3d7f1a6cf76", "title": "Zamba: A Compact 7B SSM Hybrid Model"}, {"paperId": "d8b51d518f2dd62943762ceaa8961d3b1bfbcc1a", "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?"}, {"paperId": "cbaf689fd9ea9bc939510019d90535d6249b3367", "title": "Jamba: A Hybrid Transformer-Mamba Language Model"}, {"paperId": "d53fe76bd2795a19ddf52d012917782f6f6f2c1e", "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models"}, {"paperId": "b54c3599a17db5a71349877a8567400117efbade", "title": "Nemotron-4 15B Technical Report"}, {"paperId": "9da427202cc48370fd66359f5d72ff5ff3bc8b57", "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks"}, {"paperId": "189fde3f4dfa105bb51472a8945618f395919560", "title": "Repeat After Me: Transformers are Better than State Space Models at Copying"}, {"paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60", "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "0a067fab18c67d4a386efa846c080f8afff5e8f3", "title": "Block-State Transformers"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "7d3771a8ab66e0c6ef6672cfc57ecce16a713554", "title": "Diagonal State Space Augmented Transformers for Speech Recognition"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ec307b17f193b14292206b65a1bcc95bfd8f02ed", "title": "\u266b MuSiQue: Multihop Questions via Single-hop Question Composition"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "9001eb3c3d5a96ad3d804410c2437e6f60feade9", "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "a81874b4a651a740fffbfc47ef96515e8c7f782f", "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers"}, {"paperId": null, "title": "A Framework for Few-shot Language Model Evaluation"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "NVIDIA H100 Tensor Core GPU"}, {"paperId": null, "title": "\u201cNTK-aware Scaled RoPE allows LLaMA models to have Extended (8k+) Context Size Without any Fine-tuning and Minimal Perplexity Degradation\u201d"}]}