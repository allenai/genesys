{"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "abstract": "Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. This is demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The resulting models, which we name LongLLaMA, exhibit advancements in tasks requiring a long context. We further illustrate that our LongLLaMA models adeptly manage a $256 k$ context length for passkey retrieval.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 83, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.03170", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "The Focused Transformer (FoT) is introduced, a technique that employs a training process inspired by contrastive learning that enhances the structure of the (key, value) space, enabling an extension of the context length."}, "embedding": {"model": "specter_v2", "vector": [-0.05455978959798813, -0.38004255294799805, -0.6597490310668945, 0.00962247233837843, -0.7445387840270996, -0.45270711183547974, 0.9778972864151001, 0.2688758671283722, -0.7856024503707886, -0.3015648424625397, 0.45857876539230347, -0.08977732807397842, -0.014279691502451897, 0.056203894317150116, -0.012210841290652752, 0.0968879908323288, -0.981513261795044, 0.5125434994697571, -0.07749880850315094, -0.7595053911209106, -0.003569387597963214, -0.5777994990348816, -0.9207683801651001, 0.413923442363739, 0.19008564949035645, 0.6212129592895508, 0.2150079756975174, 1.0816679000854492, -0.4897952377796173, 0.1253562867641449, 0.26944032311439514, -0.08754263818264008, 0.18938277661800385, 0.3083467483520508, -0.5102005004882812, -0.3823341429233551, 0.5887283682823181, -0.2686767280101776, -0.45508307218551636, 0.15981091558933258, -0.14025500416755676, 0.518561840057373, 0.11417137831449509, -0.6427827477455139, -0.4837309718132019, 0.7726471424102783, 0.7443543076515198, 1.1050488948822021, -0.25924402475357056, -0.8744358420372009, 1.5483453273773193, -1.6757816076278687, -0.044078998267650604, 1.1573729515075684, 0.5369550585746765, 0.3660328984260559, -0.13915474712848663, -0.20288485288619995, 1.2545380592346191, 0.23910236358642578, -0.7040174603462219, -0.24793756008148193, -0.20546434819698334, 0.2332032322883606, 1.949915885925293, -0.16630710661411285, 0.19175678491592407, 0.3599414825439453, -0.19826640188694, 1.6143110990524292, -0.021590668708086014, -1.2320266962051392, -0.29675009846687317, 0.07434188574552536, 0.4187774062156677, 0.4802512228488922, -0.5280534029006958, 0.20190320909023285, -0.8549430966377258, -0.36703190207481384, 0.19622312486171722, 0.22377045452594757, 0.05530402436852455, -0.11357666552066803, -0.0027009868063032627, 0.4998090863227844, 0.6001575589179993, 1.0991427898406982, -0.3186888098716736, 0.6578660607337952, 0.18374419212341309, 0.5642037391662598, 0.2604354918003082, 0.7509170770645142, -0.4712493419647217, 0.23456509411334991, -1.0735429525375366, 0.4005090296268463, 0.01099556963890791, 0.6194844245910645, -0.26579004526138306, -0.14071381092071533, -0.7762096524238586, 0.399747759103775, 0.8478147983551025, 0.11781857162714005, 0.32956984639167786, -0.37127479910850525, 0.3603230118751526, -0.5270538926124573, 0.10513152927160263, -0.3191699683666229, -0.09117886424064636, -0.24809324741363525, -0.025639159604907036, -1.437902808189392, -0.540436327457428, 0.38410767912864685, -0.7958719730377197, 0.4996674060821533, -0.45576807856559753, 0.05557222664356232, 0.11371555924415588, 0.27918240427970886, 0.3913526237010956, 0.6063516139984131, 0.5542377829551697, 0.12351811677217484, 0.8433667421340942, -0.9110305905342102, -0.26463770866394043, -1.4680997133255005, 0.9412269592285156, -0.5059947371482849, 0.7428653836250305, -0.30823051929473877, -1.2400189638137817, -0.8161123991012573, -0.8125081658363342, 0.258588582277298, -1.211525321006775, 0.09569407999515533, 0.8914353251457214, 0.2984589636325836, -1.0762200355529785, 0.30099737644195557, 0.018732186406850815, -0.30402252078056335, 0.39140576124191284, 0.17203395068645477, 0.2262812852859497, -0.850835382938385, -1.650143027305603, 0.408549964427948, -0.019873298704624176, -0.8180652856826782, -0.07335866242647171, -0.6136336922645569, -0.7839362621307373, 0.062359701842069626, 0.5812848210334778, -0.48474809527397156, 1.1831169128417969, -0.06944833695888519, -0.48568007349967957, 0.4705512225627899, -0.1666969358921051, 0.3519788086414337, 0.16634249687194824, -0.6268283724784851, -0.5357382297515869, -0.3681918680667877, -0.36870643496513367, 0.2795851528644562, 0.42992255091667175, -0.18678238987922668, -0.43046560883522034, 0.054683465510606766, 0.03886139765381813, 0.3041236996650696, -0.47199031710624695, 0.8810009360313416, -0.6095133423805237, -0.2939271330833435, 0.3300216794013977, 0.7412475943565369, 0.06471754610538483, -0.22104109823703766, 0.41456303000450134, -0.8490298986434937, 0.6159279346466064, -0.0635688528418541, 1.4749462604522705, -0.6411440968513489, -0.7069597840309143, -0.07089310139417648, -0.31194138526916504, 0.06725478172302246, -0.8992845416069031, 1.0384159088134766, -0.22453568875789642, 0.2640727460384369, 0.020576659590005875, -0.9805127382278442, -0.0354045070707798, -0.18890900909900665, -0.6160938143730164, -0.4637177288532257, -0.03517461195588112, 1.1533581018447876, -0.9624561667442322, -0.02300604246556759, -0.040911342948675156, 0.38166046142578125, -0.8332207798957825, 1.123091697692871, -0.5368372201919556, -0.21699756383895874, -0.3774651885032654, 0.1378394514322281, 0.16512183845043182, -0.35884177684783936, 0.3584176003932953, -0.2681680917739868, -0.05214680731296539, 0.4181450307369232, -0.11450131237506866, 1.2233147621154785, -0.6951716542243958, 0.3085423409938812, -0.03539222106337547, -0.593346357345581, -0.24160543084144592, 0.2901783883571625, 0.027103031054139137, -0.6397741436958313, 0.18905888497829437, 0.37853914499282837, -0.9265948534011841, 0.3638412058353424, 1.2504982948303223, 0.8507003784179688, -0.6637550592422485, -0.1390034407377243, 0.6226047873497009, -0.16089969873428345, 0.552708089351654, 0.668168306350708, 0.6053570508956909, 0.4587193727493286, 0.25255805253982544, -0.13414782285690308, 0.5050044059753418, -0.6484518051147461, -0.4923734962940216, 0.6834830641746521, 0.794305682182312, 0.49124670028686523, 0.2693525552749634, -0.634110152721405, -0.43574291467666626, 0.14285893738269806, 0.6456524133682251, 1.8786602020263672, -0.15925657749176025, -0.5118374824523926, -0.6409587264060974, -0.6413909196853638, -0.14019550383090973, -0.08320850878953934, -0.6191385984420776, -0.2908320426940918, -0.7447735071182251, -0.8110823035240173, 0.6572739481925964, 0.5765060782432556, 0.6521255970001221, -0.5024983286857605, -0.19853875041007996, -0.1608475148677826, 0.2319040596485138, -0.6944273114204407, -0.7905218601226807, 0.5844564437866211, -0.2694585621356964, -0.2295362651348114, 0.012797408737242222, -0.2506486177444458, -0.03210781514644623, -0.10955842584371567, 1.2859779596328735, -0.016074463725090027, -0.22616532444953918, 0.8396274447441101, 0.19596998393535614, -0.5678520798683167, -0.5806578993797302, 0.26117926836013794, 0.22398999333381653, -0.4264317452907562, 0.6868298649787903, 0.8938500881195068, -0.07259053736925125, 0.29425543546676636, -0.4848202168941498, -0.11383097618818283, 0.17905297875404358, 0.4426043629646301, 0.6917434334754944, -0.6307550668716431, 0.11684905737638474, -1.00355064868927, 1.0720727443695068, 0.1884213238954544, -0.4573715329170227, 0.6797112822532654, -0.7155855298042297, -0.6203487515449524, 0.39545661211013794, -0.5946950912475586, -0.06479837745428085, -1.3071593046188354, 0.23534533381462097, -0.2820006012916565, 0.16596484184265137, 0.4118483066558838, 0.32353952527046204, 0.1470280885696411, 0.23589487373828888, 0.6701167225837708, 0.1622060239315033, -0.30242422223091125, 0.6079725027084351, -0.6725586652755737, 0.3071933388710022, -0.06983763724565506, -0.1985553652048111, -0.2678147554397583, -0.1749684065580368, -0.5909385085105896, -0.23332196474075317, -0.7859265804290771, -0.30574843287467957, -0.047442056238651276, -0.22057297825813293, -0.6897410154342651, -0.565762460231781, -0.20310944318771362, -0.5171782374382019, -0.41094404458999634, 0.05858677253127098, -0.2684468626976013, -0.20533986389636993, -1.2109172344207764, -1.246085286140442, -0.40530455112457275, -0.6468935012817383, -0.7865465879440308, 0.21011638641357422, -0.06155569106340408, -0.40355220437049866, -0.747750997543335, 0.045990701764822006, -0.48803722858428955, 1.2357220649719238, -0.9726569652557373, 0.8359894156455994, -0.10261654853820801, -0.1984679251909256, -0.7687257528305054, 0.14178365468978882, 0.5291469693183899, -0.05303490161895752, 0.05072157457470894, -0.9043322801589966, 0.007787043694406748, -0.2591051161289215, -0.09007184207439423, 0.49409183859825134, 0.2676308751106262, 0.5235382914543152, 0.11358065903186798, -0.8006309866905212, -0.047029197216033936, 1.3544774055480957, -0.5553790926933289, -0.01750778779387474, 0.5362780094146729, 0.7302011251449585, 0.22540177404880524, 0.3910585641860962, 0.2226332724094391, 0.13445661962032318, 0.42487987875938416, -0.08013401925563812, -0.045955196022987366, -0.26854777336120605, -0.6267975568771362, 0.10678974539041519, 1.4048887491226196, 0.24219363927841187, -0.243352472782135, -1.0920262336730957, 0.6874048709869385, -1.0507159233093262, -0.4811672568321228, 1.2119754552841187, 0.9140065312385559, 0.2902073860168457, -0.4894142746925354, -0.3369300067424774, -0.29854437708854675, 0.14236949384212494, 0.4305693805217743, -0.041429318487644196, -0.5100085139274597, 0.022620853036642075, 0.5835224986076355, 0.17126241326332092, 0.7171666622161865, -0.44141915440559387, 0.8475479483604431, 14.975496292114258, 0.9430445432662964, 0.007663101423531771, 0.5928879976272583, 0.573762834072113, -0.04028799757361412, -0.311639666557312, -0.2443016767501831, -1.17548406124115, -0.13585899770259857, 1.2756483554840088, 0.1715824156999588, 0.4787464439868927, 0.12047728151082993, -0.28078967332839966, 0.031478337943553925, -0.43771231174468994, 0.5633379220962524, 0.690634548664093, -1.2792147397994995, 0.4540676474571228, 0.21576134860515594, 0.13152259588241577, 0.44078296422958374, 0.9630451798439026, 1.1313096284866333, 0.34751391410827637, -0.08756109327077866, 0.2853655517101288, 0.20699460804462433, 0.5659990906715393, -0.3040008246898651, 0.2950865626335144, 0.19071601331233978, -0.7505847811698914, -0.7512463927268982, -0.5987839102745056, -0.8024718761444092, 0.15099844336509705, -0.23673948645591736, -0.554358959197998, -0.5505109429359436, -0.0421445295214653, 0.2562776207923889, -0.16813215613365173, 0.40974125266075134, 0.17645838856697083, 0.31290188431739807, -0.11863639950752258, 0.0301217008382082, 0.6899400949478149, 0.6703718900680542, 0.3255334198474884, 0.1287296861410141, 0.08977341651916504, -0.16726893186569214, 0.1195894256234169, 0.023231975734233856, -0.6510748267173767, 0.17321926355361938, -0.39831283688545227, -0.1837962567806244, 0.24671012163162231, 0.6564292907714844, 0.50569748878479, 0.0420256145298481, -0.5257861614227295, 0.2513665556907654, 0.5401074886322021, 0.24147537350654602, -0.038016561418771744, 0.33748912811279297, 0.5435457229614258, -0.1428787112236023, 0.0971919521689415, 0.6298491954803467, 0.07180045545101166, -0.470020592212677, -0.5084003210067749, -0.3544013202190399, 0.6290850639343262, -0.6551690101623535, -0.4919642210006714, 1.145214319229126, 0.17877131700515747, -0.6124891638755798, -0.00417315261438489, -0.738077700138092, 0.016435356810688972, 0.618640124797821, -1.425546646118164, -1.090656042098999, 0.3703400194644928, -0.3357929587364197, -0.38088586926460266, -0.06368626654148102, 1.524871826171875, 0.3689209520816803, 0.11995566636323929, 0.20092326402664185, 0.2513498067855835, -0.19969293475151062, 0.2574768662452698, -0.7728928923606873, 0.9852290153503418, 0.21452085673809052, 0.15725767612457275, 0.5360000729560852, -0.1288115233182907, 0.1812126636505127, -0.7816125750541687, 0.19055919349193573, 0.7990885972976685, -1.4080798625946045, -0.5424062609672546, -0.7822950482368469, -0.7871668934822083, 0.23133613169193268, 0.4370817244052887, 0.16645246744155884, 0.3605584502220154, 0.4955877661705017, -0.7516500353813171, -0.30706629157066345, -0.956006646156311, 0.09484796971082687, 0.44696754217147827, -0.7804632186889648, -0.4297811985015869, -0.19189050793647766, 0.4156816303730011, -1.0762442350387573, -0.4242463707923889, -0.4400656521320343, 0.048087719827890396, 0.174091175198555, 1.27090322971344, -0.6828333139419556, 0.3731576204299927, 1.0223411321640015, -0.6266570091247559, -0.8926863670349121, -0.18057304620742798, -0.3346717059612274, -0.4065695106983185, 0.08743488788604736, 0.5864920616149902, -0.25317683815956116, 0.3841463625431061, 0.9889691472053528, 0.23633892834186554, -0.7673293948173523, -0.29472818970680237, -0.17585855722427368, 0.33707237243652344, -0.5379403829574585, 0.5451673269271851, 0.056864891201257706, -0.06195578724145889, -0.06067287176847458, 0.4914763867855072, 0.5721887350082397, -0.5188792943954468, -0.7152594923973083, 0.2936176657676697, -0.19769546389579773, 0.14262354373931885, -0.4623705744743347, -0.46577873826026917, -1.326088786125183, -0.45406684279441833, -0.6022028923034668, -0.20329195261001587, -0.8053802847862244, -0.7711051106452942, -0.27140331268310547, -0.7488385438919067, -0.07458017766475677, 0.016115030273795128, -0.3301849961280823, -0.24988198280334473, -0.725408673286438, -0.5587262511253357, 0.30282464623451233, 0.68077552318573, -0.5717065930366516, -0.10355893522500992, -0.007506475318223238, -0.17318150401115417, 0.16488012671470642, 0.29112905263900757, -0.07729070633649826, -0.6607705354690552, -1.2845144271850586, 0.6445704698562622, -0.3523946702480316, -0.24069203436374664, -0.6794165372848511, 0.715407133102417, 0.18479293584823608, 0.030940307304263115, -0.1316550076007843, 0.7989674806594849, -1.3027551174163818, -0.9723407030105591, 0.006609991192817688, -0.6861375570297241, 0.41637536883354187, 0.4853332042694092, -0.7069737315177917, -0.47411084175109863, 0.8994234204292297, -0.2392359972000122, -1.23097562789917, -1.0284385681152344, 0.3145027458667755, -0.3740791380405426, 0.1801518201828003, -0.38548538088798523, -0.00873682089149952, -1.0428962707519531, -0.30757927894592285, -0.05160147324204445, 0.6005165576934814, -0.4232317805290222, 1.2264963388442993, 0.37747734785079956, -1.2300931215286255, 0.08938993513584137, 0.4779001772403717, 0.24460072815418243, -0.09798097610473633, 0.542670726776123, 0.34325405955314636, -0.10119444876909256, 0.7755854725837708, 0.20187410712242126, 0.298410564661026, -1.124833345413208, 0.4072487950325012, 0.606040894985199, -0.6755165457725525, 0.10940928757190704, 1.2938474416732788, -0.4366437792778015, -0.9451441764831543, -0.011422153562307358, -1.1785540580749512, -0.6225089430809021, 0.027677355334162712, 1.013559341430664, 0.01750156655907631, 0.32715654373168945, -0.09218151867389679, -0.6241613030433655, 0.20230384171009064, -0.4948881268501282, -0.43791523575782776, 0.4690572917461395, -0.46030011773109436, -0.3769444525241852, 0.5736576914787292, 0.7298312187194824, -0.44928956031799316, -0.5638482570648193, -0.9695403575897217, -0.5000578761100769, -0.40327540040016174, 0.5438092350959778, -0.6755388379096985, -0.24311193823814392, 0.5712052583694458, 0.7295328974723816, 0.4371423125267029, 0.03207677602767944, -0.038644202053546906, 0.12965938448905945, 0.7521865963935852, 0.10870326310396194, -0.8144542574882507, -0.5776902437210083, 1.2575645446777344, 1.0724517107009888, -0.8640841841697693, 0.2625105381011963, 0.31422561407089233, -0.5189740061759949, 0.6176795363426208, 0.5505073666572571, 0.3109576106071472, 0.9342573881149292, -0.1830262988805771, 0.5541225671768188, 0.07141320407390594, -1.1604732275009155, -0.17941918969154358, 0.8932358622550964, 0.8591738343238831, 0.5837156176567078, 0.25770309567451477, 0.2064693123102188, 0.812852680683136, 0.2935633063316345, 0.1399005800485611, 0.1449020504951477, 0.9851081967353821, -0.20766820013523102, -0.37353894114494324, -0.1527700424194336, 0.567388653755188, -0.862528920173645, -1.0016398429870605, 0.04990324005484581, 0.8322664499282837, 0.05809779837727547, 0.3615614175796509, 0.8948189616203308, 0.12263254821300507, 0.7525004744529724, 0.5660552382469177, 0.7951139807701111, -0.45848098397254944, -0.48358798027038574, -0.32179996371269226, -0.5876380205154419, 0.26825734972953796, -0.03276636451482773, -0.3571231961250305, 0.0017146965255960822, -0.18867626786231995, 0.250000536441803, 0.3444187641143799, 0.30353760719299316, 0.9434725642204285, 0.4861413240432739, 0.4839668571949005, -0.4225403964519501, -0.39472126960754395, -0.5579995512962341, -0.8644633293151855, 0.15300297737121582, -0.4987330138683319, -0.10835468024015427, -0.2500949800014496, -0.012840336188673973, -0.739095151424408]}, "authors": [{"authorId": "2134879789", "name": "Szymon Tworkowski"}, {"authorId": "2190105786", "name": "Konrad Staniszewski"}, {"authorId": "2050138429", "name": "Mikolaj Pacek"}, {"authorId": "3374063", "name": "Yuhuai Wu"}, {"authorId": "47407464", "name": "H. Michalewski"}, {"authorId": "2219300363", "name": "Piotr Milo's"}], "references": [{"paperId": "881883842c2661b41bbfc999d56c763b1ceef0bd", "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "f661bdc053ba13df80eda479791306e1178db235", "title": "Magnushammer: A Transformer-based Approach to Premise Selection"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "eecb45aa040064cbc0b37fd100706c02e7dc880e", "title": "Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1", "title": "Training Language Models with Memory Augmentation"}, {"paperId": "c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c", "title": "Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "231e768f0cd280faa0f725bb353262cb4fed08d1", "title": "Hierarchical Transformers Are More Efficient Language Models"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "8d1369a218a39214d82ea77ff964570eca057c15", "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup"}, {"paperId": "806adbb35ed4a95f51518f5962fd59685ad4706b", "title": "Query-Key Normalization for Transformers"}, {"paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b", "title": "Generative Language Modeling for Automated Theorem Proving"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "55e960535f643637161b2e99a8c21a92c0d13757", "title": "Representation Degeneration Problem in Training Natural Language Generation Models"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2cbb8de53759e75411bc528518947a3094fbce3a", "title": "Billion-Scale Similarity Search with GPUs"}, {"paperId": "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers"}, {"paperId": "75895ce98904e8afaaa248f081a1da501bd2dbe2", "title": "Toward Semantics-Based Answer Pinpointing"}, {"paperId": null, "title": "Easylm: A simple and scalable training framework for large language models, 2023"}, {"paperId": null, "title": "Openllama: An open reproduction of llama, May 2023"}, {"paperId": null, "title": "Introducing mpt-30b: Raising the bar for open-source foundation models"}, {"paperId": "cc548e5b68ff5a958ce5959e91f0e758a6af6f80", "title": "Efficient Training of Retrieval Models using Negative Cache"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length?"}, {"paperId": null, "title": "Contrastive learning for causal language"}, {"paperId": null, "title": "Inference Both models use a very similar memory attention layer. The difference is how the retrieved ( key, value ) pairs are integrated"}, {"paperId": null, "title": "Things i \u00b4m learning while training superhot"}, {"paperId": null, "title": "cb_attn_weights = jnp.einsum(\"bqhd ,bckhd ->bhqck\", pquery , attention_keys , precision=precision) 10"}, {"paperId": null, "title": "An open source recipe to reproduce"}, {"paperId": null, "title": "A framework for few-shot language model evaluation,"}, {"paperId": null, "title": "Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length"}]}