{"paperId": "333212e246fb65f7c9d43862021e78f007c48449", "abstract": "Transformer, an attention-based encoder\u2013decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern convolution neural networks (CNNs). In this survey, we have reviewed over 100 of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, two promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2021, "citationCount": 193, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://arxiv.org/pdf/2111.06091", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey has reviewed over 100 of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios."}, "embedding": {"model": "specter_v2", "vector": [0.5849723219871521, 0.33122915029525757, -0.40420207381248474, -0.01655321940779686, -0.23294572532176971, -0.052498530596494675, 0.8134093880653381, -0.1237286850810051, -0.7360689043998718, -0.9562765955924988, 0.23304951190948486, 0.37829408049583435, 0.5940402150154114, -0.17708535492420197, -0.15820780396461487, 0.028567029163241386, -0.4224121868610382, 0.19544634222984314, 0.3560195863246918, -0.2744068205356598, 0.1708802431821823, -0.4894624352455139, -1.4098823070526123, 0.2506345212459564, 0.11494358628988266, 0.8997727632522583, 0.3344736099243164, 1.0114517211914062, -0.5713320970535278, 0.2938251495361328, 0.5223132967948914, -0.40215665102005005, 0.39443549513816833, 0.18380433320999146, -0.2958378493785858, -0.02178766205906868, 0.9687674045562744, -0.3341563940048218, -0.7802520394325256, 1.2054272890090942, -0.24207313358783722, 0.27417680621147156, 0.5669257044792175, -0.9428736567497253, -0.6511795520782471, 0.23697198927402496, 0.5590589046478271, 0.694152295589447, -0.3248063027858734, -0.20424218475818634, 1.541771650314331, -1.4863591194152832, 0.09649012982845306, 1.6621216535568237, 0.6689814329147339, 0.2946223318576813, -0.08150234073400497, -0.4360266327857971, 0.5584036707878113, 0.10768041759729385, -0.47991812229156494, -0.20303383469581604, 0.01281206775456667, -0.31632816791534424, 1.788588523864746, -0.5973562598228455, 0.3792417049407959, 0.9091458320617676, -0.110118068754673, 1.451691746711731, -0.1321975737810135, -0.7354891300201416, -0.09299296885728836, -0.2203150987625122, 0.10436876863241196, 1.0614548921585083, -0.2709255516529083, 0.45754939317703247, -0.9792057871818542, 0.21565097570419312, 0.5036051869392395, 0.26537951827049255, 0.0848449170589447, -0.4343562722206116, -0.31670621037483215, 0.8080915808677673, 1.0848325490951538, 0.340427428483963, -0.2142343521118164, 0.774598240852356, 0.5879867672920227, -0.19857946038246155, -0.1610492467880249, 0.32379695773124695, 0.3982137143611908, 0.7113683223724365, -0.8927741646766663, 0.22498078644275665, -0.3419126272201538, 1.1423035860061646, -0.3567323088645935, 0.36545294523239136, -0.5632687211036682, 0.2835705280303955, 1.4773305654525757, 0.5449536442756653, 0.08148539811372757, -0.6239284873008728, 0.2310582399368286, -0.5331509113311768, -0.06345885992050171, -0.7198426127433777, 0.3535866141319275, -0.2903846800327301, -0.7558754682540894, -0.7346147894859314, -0.3573787808418274, 0.7084656357765198, -1.4072051048278809, 0.28321683406829834, -0.8337210416793823, 0.13845543563365936, 0.13351131975650787, 0.6027711629867554, 0.8222800493240356, 0.7787712216377258, 0.5429398417472839, 0.5915513634681702, 1.2325142621994019, -1.0425862073898315, -0.4307675063610077, -1.1516238451004028, -0.01611975207924843, -0.2844924032688141, 0.4847164452075958, -0.034835655242204666, -1.027483582496643, -1.3140490055084229, -0.7475491166114807, -0.23292922973632812, -0.7862978577613831, 0.3529675304889679, 0.5364509224891663, -0.05550890788435936, -1.4200702905654907, 0.5214636921882629, -0.03133989870548248, -0.7144961357116699, 0.8064590096473694, -0.015592041425406933, 0.3786695897579193, -0.38555508852005005, -0.8679689168930054, 0.3833337128162384, -0.07287463545799255, -0.28259071707725525, -0.4984937906265259, -0.01861453615128994, -1.5112910270690918, 0.15070435404777527, 0.02294047549366951, -0.7564164400100708, 1.1589491367340088, -0.18167860805988312, -0.5370741486549377, 0.7571495771408081, -0.7944167256355286, -0.16598019003868103, 0.301373690366745, -0.3964926302433014, -0.007699522655457258, -0.13274015486240387, 0.21076101064682007, 0.8984535932540894, 0.9022560119628906, -0.41662001609802246, -0.4144039452075958, 0.3158247470855713, -0.3817586898803711, 0.07392605394124985, -0.4235605299472809, 0.8777120113372803, -0.6650354266166687, -0.20607228577136993, 0.38185596466064453, 0.9503233432769775, -0.11632442474365234, 0.1378099024295807, -0.4918808043003082, -0.9803151488304138, 0.9723993539810181, 0.4072995185852051, 0.33162328600883484, -0.9874438643455505, -0.8533158302307129, -0.4533750116825104, 0.08420409262180328, -0.005617992486804724, -0.8567651510238647, 0.540237545967102, -0.1838778853416443, 0.37719136476516724, -0.14983157813549042, -1.365524172782898, -0.11595971137285233, 0.0292102862149477, -0.8630310297012329, 0.08129963278770447, 0.5731330513954163, 1.3925554752349854, -1.0977447032928467, -0.17235234379768372, 0.41115710139274597, 0.1595817655324936, -0.5944105982780457, 1.149908185005188, -0.38658037781715393, -0.4783760607242584, -0.02664518728852272, 0.16193217039108276, 0.03767910599708557, -0.39505037665367126, 0.008742664009332657, -0.6902079582214355, -0.3223157525062561, 0.35626813769340515, -0.009541203267872334, 1.2670985460281372, 0.19330933690071106, 0.8549605011940002, -0.419679194688797, -0.8245115280151367, 0.5174424052238464, 0.29958444833755493, -0.478636234998703, -0.8020567893981934, 0.3554883897304535, 0.08519258350133896, -0.9735773801803589, 0.170399472117424, 0.8868544697761536, 0.828177273273468, -0.15958186984062195, -0.2153826355934143, 0.5210058093070984, -0.4694710671901703, 0.17737185955047607, 0.4574914872646332, 0.4219588041305542, 0.35215887427330017, 0.2610119879245758, -0.045146532356739044, 0.07808984071016312, -0.5117993950843811, -0.3362481892108917, 0.8730278015136719, 0.38111555576324463, 1.4893944263458252, 0.42852583527565, -0.889819860458374, -0.5927213430404663, -0.42174068093299866, 0.7950283885002136, 1.023705005645752, 0.19568219780921936, -0.26897960901260376, -0.39536723494529724, -0.405738890171051, -0.38453221321105957, -0.4884897768497467, -0.5700048804283142, 0.01016439963132143, -0.19956013560295105, -0.7996219396591187, 1.0040409564971924, 0.5731698870658875, 1.22940194606781, -0.9022893905639648, -0.6140126585960388, -0.7121252417564392, 0.23841744661331177, -1.0077797174453735, -0.37280339002609253, 0.434408038854599, -0.5586810111999512, -0.4125650227069855, -0.011759581044316292, -0.4107857942581177, 0.35209915041923523, -0.3429861068725586, 0.7009448409080505, -0.7163944244384766, -0.37671592831611633, 0.14350196719169617, 0.5280139446258545, -0.8658730983734131, -0.2253032773733139, 0.005598052404820919, -0.24971257150173187, -0.2326972335577011, 0.6663947105407715, 0.5890922546386719, -0.05861439183354378, -0.12767304480075836, -0.412915974855423, -0.18579082190990448, 0.0845421552658081, 0.027577834203839302, 1.1412367820739746, -0.6538016200065613, -0.26727861166000366, -0.7475722432136536, 0.9686631560325623, 0.3156503438949585, -0.3647080957889557, 0.16640254855155945, -0.540471613407135, -0.3989473581314087, 0.4098871052265167, -0.3909316658973694, -0.01875891163945198, -0.24054187536239624, 0.35227635502815247, -1.1892529726028442, -0.6316700577735901, -0.023962121456861496, 0.5501988530158997, 0.24572861194610596, 0.46431148052215576, 0.5719006061553955, 0.3601589500904083, 0.051698096096515656, 0.6533471345901489, -0.8156025409698486, 0.7578356266021729, 0.5475191473960876, 0.21455882489681244, 0.3004654049873352, -0.14568614959716797, -0.8709272742271423, -0.538937509059906, -0.8190212249755859, -0.34003764390945435, -0.5464629530906677, 0.64284747838974, -0.5749006867408752, -0.7332016229629517, 0.1904059499502182, -1.333284616470337, 0.15855436027050018, -0.12911202013492584, -0.5282107591629028, -0.2392701804637909, -0.9713439345359802, -0.8809674978256226, -0.4631209075450897, -0.6816439628601074, -0.9145011305809021, 0.5415432453155518, 0.4693681597709656, -0.00845049787312746, -0.529106855392456, -0.19678270816802979, -0.5796075463294983, 0.8430572748184204, -0.27180027961730957, 0.6982796788215637, -0.2065477818250656, -0.2600105106830597, -0.12318246066570282, -0.35396435856819153, 0.6130213141441345, -0.18948107957839966, 0.43180257081985474, -1.3977688550949097, 0.36751121282577515, -0.024375762790441513, -0.16260918974876404, 0.7136154174804688, 0.39569440484046936, 0.4977399706840515, 0.23131375014781952, -0.5869696736335754, 0.5127401351928711, 1.6699732542037964, -0.37210705876350403, 0.07589448243379593, 0.08877377212047577, 1.0432826280593872, -0.010617267340421677, -0.25563016533851624, 0.30100715160369873, 0.20016519725322723, 0.07529395818710327, 0.9278395175933838, -0.6232955455780029, -0.4328303933143616, -0.8399506211280823, 0.3265809416770935, 1.0511417388916016, 0.21671997010707855, -0.2213796228170395, -1.2141985893249512, 1.099839448928833, -1.036896824836731, -0.7418960928916931, 0.5122573375701904, 0.4953298270702362, -0.21878813207149506, -0.22873316705226898, -0.2604644298553467, -0.3336500823497772, 0.8460692167282104, 0.8923364281654358, -0.2416812628507614, -0.762705385684967, -0.12718090415000916, 0.5896597504615784, 0.22319400310516357, 0.3521187901496887, -0.8046214580535889, 0.4964808225631714, 14.552816390991211, 0.5162618160247803, -0.3722629249095917, 0.26579177379608154, 0.46750932931900024, 0.39562568068504333, -0.19739069044589996, -0.10855057835578918, -1.002783179283142, -0.38003644347190857, 0.5227895379066467, 0.4188671112060547, 0.15352202951908112, 0.4425235092639923, -0.23723572492599487, 0.33453068137168884, -0.6659966111183167, 1.1570266485214233, 0.6536454558372498, -1.3720099925994873, 0.5005355477333069, 0.07086148113012314, -0.027221284806728363, 0.7848517298698425, 1.0332999229431152, 0.49132463335990906, 0.3253171741962433, -0.7589167356491089, 0.8996616005897522, 0.012334846891462803, 1.1252706050872803, 0.42498287558555603, 0.2878434956073761, 0.326836496591568, -1.4903615713119507, -0.32374000549316406, -0.6157786250114441, -1.184867024421692, 0.2569226026535034, -0.25772807002067566, -0.6132640838623047, -0.27229776978492737, 0.07722043991088867, 1.4222983121871948, 0.07956314831972122, 0.6695807576179504, -0.369174599647522, 0.5214642286300659, -0.3020362854003906, -0.13062125444412231, 0.8272572755813599, 0.49487438797950745, 0.2882969081401825, 0.15627582371234894, 0.11017453670501709, 0.07260971516370773, 0.12511451542377472, 0.7919216752052307, -0.40127453207969666, -0.319232702255249, -0.7225968241691589, -0.29354625940322876, -0.2522211968898773, 0.6402294039726257, -0.020755669102072716, 0.48150432109832764, -0.3119255602359772, 0.2846163213253021, 0.35641607642173767, 0.3506901264190674, -0.322057843208313, -0.21662040054798126, 0.28969886898994446, -0.1983126997947693, 0.6569836735725403, 0.646954357624054, -0.26287397742271423, -0.41249528527259827, -0.4945230185985565, 0.17539168894290924, 0.49106961488723755, -0.8137337565422058, -0.8303183317184448, 1.181031584739685, -0.23302385210990906, -0.6728991866111755, 0.569957971572876, -0.9918621778488159, -0.4996218979358673, 0.21529021859169006, -1.5515823364257812, -0.8313522934913635, -0.24021781980991364, -0.1852761059999466, 0.15181255340576172, -0.13562054932117462, 0.790582001209259, -0.16725151240825653, 0.0860636755824089, -0.21592746675014496, -0.5754815936088562, 0.4408542513847351, -0.5084396600723267, -0.8739334344863892, 1.075416922569275, 0.5016306042671204, 0.23014819622039795, 0.04909050464630127, -0.07101762294769287, 0.1920938938856125, -0.36298540234565735, -0.2030969113111496, 0.6706946492195129, -0.4442034959793091, -0.732337474822998, -0.8223064541816711, -0.6327983736991882, 0.15816514194011688, 0.7212892174720764, -0.013093250803649426, -0.3605608642101288, -0.02071423828601837, -0.7214639186859131, 0.02469344064593315, -0.5484619140625, -0.042273711413145065, 0.5187450051307678, -0.8210180997848511, -0.5938274264335632, -0.16602632403373718, 0.2929694354534149, -0.8532966375350952, 0.13862016797065735, -0.11856549233198166, -0.005495558958500624, -0.32323122024536133, 1.4643967151641846, -0.45693469047546387, 1.0037206411361694, 0.6648051142692566, -0.3377946615219116, -0.5306147336959839, -0.02350633591413498, -0.4051394462585449, 0.28230059146881104, -0.029232973232865334, 0.3882768750190735, -0.4283057749271393, 0.4273909330368042, 0.27650970220565796, 0.19725948572158813, -0.6520935893058777, -0.503919780254364, -0.15303686261177063, -0.11756160855293274, -0.6029162406921387, 0.07067308574914932, -0.40414905548095703, -0.3591286242008209, 0.25538724660873413, 0.6553831100463867, 0.3648776710033417, -0.08597055822610855, -0.5203878879547119, 0.06144624948501587, -0.19188958406448364, 0.2356126308441162, -0.4387775957584381, -0.6195077300071716, -1.6771920919418335, -0.13378798961639404, -1.2093935012817383, 0.25325119495391846, -1.5734301805496216, -0.4997541904449463, 0.2216184437274933, -0.4076465666294098, 0.29885008931159973, 0.6652771830558777, -0.1438472718000412, 0.2095639407634735, -0.2931245267391205, -0.6172388792037964, 0.7654106020927429, 0.9903431534767151, -0.9512887001037598, 0.2781522572040558, 0.009251304902136326, 0.21387256681919098, 0.63257896900177, 0.16579248011112213, -0.5241832137107849, -0.9248356819152832, -1.296242594718933, 0.29689761996269226, -0.31417208909988403, 0.4052209258079529, -0.9533720016479492, 1.0517351627349854, 0.5618180632591248, -0.12364833056926727, -0.1614006906747818, 0.7800151109695435, -0.8083992600440979, -0.9319970011711121, 0.3777625858783722, -0.7117132544517517, 0.2828563153743744, 0.23296062648296356, -0.2779613137245178, -0.7933756113052368, 0.7757878303527832, 0.20766867697238922, -1.168319582939148, -1.1917335987091064, 0.4548487663269043, -0.3415318727493286, 0.3324117362499237, 0.007157038897275925, -0.46279582381248474, -1.245522141456604, -0.32827770709991455, 0.0168453361839056, 0.17275263369083405, -0.34172162413597107, 0.9750788807868958, 1.437458872795105, -0.9784930348396301, 0.22173719108104706, 0.5854670405387878, -0.08376637101173401, 0.4957030415534973, 0.1506246030330658, 0.3767233192920685, -0.4787057638168335, 0.48545801639556885, 0.1533486247062683, -0.17895442247390747, -0.9119520783424377, 0.011778387241065502, 1.074512243270874, -0.034205544739961624, -0.5126765370368958, 1.3559691905975342, 0.018020007759332657, -0.8401349186897278, 0.253286749124527, -1.124294400215149, -0.6147693395614624, -0.005929883569478989, 0.2258356511592865, -0.16641882061958313, -0.13458819687366486, -0.14880189299583435, -0.5757585763931274, 0.5124766826629639, -0.015408424660563469, -0.48976296186447144, 0.42544499039649963, 0.27739134430885315, -0.15421953797340393, 0.31267061829566956, 0.7686553597450256, -1.1237497329711914, -1.2177928686141968, -0.85862135887146, -0.5921437740325928, -0.4889715015888214, -0.15435665845870972, -0.16440631449222565, -0.885745644569397, 1.258567214012146, 0.4238542318344116, 0.5000231862068176, 0.3583430051803589, 0.19702328741550446, -0.0006076004938222468, 0.3954865336418152, -0.34947794675827026, -0.422615647315979, -0.29140740633010864, 1.4126527309417725, 1.0580337047576904, -0.7024747133255005, 0.19081555306911469, -0.5508657693862915, -0.6813027262687683, 0.8779966235160828, 0.2940548360347748, -0.5130236744880676, 0.7877009510993958, -0.05388796329498291, 0.04219401255249977, -0.21341946721076965, -0.7471293210983276, -0.48549702763557434, 0.8773093819618225, 1.4467670917510986, 0.23151549696922302, -0.03662969544529915, 0.47804391384124756, 0.3919706344604492, 0.20156638324260712, -0.022738849744200706, 0.3405919075012207, 0.26396456360816956, -0.16827939450740814, 0.1794874668121338, -0.0910923033952713, 0.723595917224884, -0.46803659200668335, -0.7618322372436523, -0.027633104473352432, 0.798055112361908, 0.21773028373718262, 0.6005047559738159, 1.1825469732284546, -0.0758419781923294, 0.783341109752655, 0.028204239904880524, 0.414867639541626, -0.7079786062240601, -0.2798350751399994, -0.1731530874967575, -1.0648114681243896, -0.1567622274160385, -0.5684281587600708, -0.8396929502487183, -0.24641089141368866, 0.1945313960313797, 0.44598388671875, -0.38675886392593384, 0.2538430094718933, 0.9902356863021851, 0.3096354007720947, 0.4160061180591583, -0.7948755621910095, -0.5050410628318787, -0.26054617762565613, -0.6402562856674194, 0.33016619086265564, -0.3018552362918854, 0.10597894340753555, -0.38662102818489075, -0.038319483399391174, 0.0761142149567604]}, "authors": [{"authorId": "2152797611", "name": "Yang Liu"}, {"authorId": "2118390623", "name": "Yao Zhang"}, {"authorId": "2108734687", "name": "Yixin Wang"}, {"authorId": "2027666137", "name": "Feng Hou"}, {"authorId": "2150063810", "name": "Jin Yuan"}, {"authorId": "1864630691", "name": "Jiang Tian"}, {"authorId": "9226966", "name": "Yang Zhang"}, {"authorId": "2558130", "name": "Zhongchao Shi"}, {"authorId": "2152732685", "name": "Jianping Fan"}, {"authorId": "46915878", "name": "Zhiqiang He"}], "references": [{"paperId": "774edded0de3f7093246b368597f637cdb1282d6", "title": "SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency"}, {"paperId": "b4ce19f3b3819accb160acffabffa849f18f4758", "title": "Anchor DETR: Query Design for Transformer-Based Detector"}, {"paperId": "f1a287bad3dbabea302fb5fbef215f9e6c981a13", "title": "mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation"}, {"paperId": "44d1b81911e35e2aa2c03a5347b88ae479602837", "title": "Multi-View Transformer for 3D Visual Grounding"}, {"paperId": "9d4eb3a74c3b3cd196834e7cb04b6a0871cdf13d", "title": "TubeDETR: Spatio-Temporal Video Grounding with Transformers"}, {"paperId": "b2a2a8eb62dd95f8bc8dbea5697bbc50af1fa3bc", "title": "MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection"}, {"paperId": "54fbb9300530d1deea596ad19807adedf1dc89dc", "title": "TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers"}, {"paperId": "c96c551ece333d6e7f95f77176cedef07b3b1b18", "title": "Masked Discrimination for Self-Supervised Learning on Point Clouds"}, {"paperId": "3d183fe445627003a3c5466aecd25de500b5aa8c", "title": "MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer"}, {"paperId": "de4c059dfacfa993a3e688af140f854303882d7c", "title": "FUTR3D: A Unified Sensor Fusion Framework for 3D Detection"}, {"paperId": "8a8eacd96dbf53a9bf54239815b752941ab967aa", "title": "Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds"}, {"paperId": "de20efa062cb54ce06beab24ac70be9501423f6a", "title": "Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding"}, {"paperId": "a601e71127d95bdae30fd818d2a0cc34b80b13f7", "title": "Masked Autoencoders for Point Cloud Self-supervised Learning"}, {"paperId": "9dc481ec44178e797466bbad968071917842156b", "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"}, {"paperId": "78d02f2909a582c624eca2d0f67c91ee91974180", "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising"}, {"paperId": "9e27b04748901ca7a0323e34bc7ac5b4a78d7209", "title": "3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification"}, {"paperId": "430bab3890e1e52c4c1f74900b0e408e47a1cb8f", "title": "How Do Vision Transformers Work?"}, {"paperId": "8f2bca9d684005675e294b33c26481e36f528cdb", "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"}, {"paperId": "004f1d2b1b7d7dcecafdd94daee9c1b0aa3e65cf", "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR"}, {"paperId": "babf62e12510a025dad4a602bd2e9f3a8331314d", "title": "Embracing Single Stride 3D Object Detector with Sparse Transformer"}, {"paperId": "c52e79e407fe44ab6c0b64c87ccf8b985ddace54", "title": "Fast Point Transformer"}, {"paperId": "dd2819016c6bf244c39b3e6707b60389bbdbcd21", "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"}, {"paperId": "e0d272e01929024f28f0f7cacf26177cd60b3ee7", "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity"}, {"paperId": "e939b55a6f78bffeb00065aed897950c49d21182", "title": "Searching the Search Space of Vision Transformer"}, {"paperId": "cc48dd21e92f5f154f6c86495a224e4d69e29ea2", "title": "Deep Point Cloud Reconstruction"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "02964e5acb950d9fc6f7faffb81cb4bcc71ad959", "title": "MVT: Multi-view Vision Transformer for 3D Object Recognition"}, {"paperId": "e15fdbde1d56a80743b7d7eafb3409a0a5870094", "title": "HRFormer: High-Resolution Transformer for Dense Prediction"}, {"paperId": "48e2d76d35b44edc21d09d460021103ce997c804", "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries"}, {"paperId": "f6647ef31fa746f1feb825500276421cfbc5bcb9", "title": "Dynamic DETR: End-to-End Object Detection with Dynamic Attention"}, {"paperId": "19b3b074d38b250d024920732ae51a8ffa0996dd", "title": "Pix2seq: A Language Modeling Framework for Object Detection"}, {"paperId": "03a2befad038a9f29859295fdfcdbfa52c564622", "title": "An End-to-End Transformer Model for 3D Object Detection"}, {"paperId": "f7e449d7695fbbf43081cc820a81fe0ccb11c3db", "title": "PnP-DETR: Towards Efficient Visual Analysis with Transformers"}, {"paperId": "813b03e123d448d53d93a087a2d34a04dfe70c5c", "title": "Voxel Transformer for 3D Object Detection"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "312160aa805e1c9936b26359c7c2e0acef81b936", "title": "Improving 3D Object Detection with Channel-wise Transformer"}, {"paperId": "80da612e1831b8c11539180871843cff6dfaac90", "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers"}, {"paperId": "1cd6b0f41d62aca38ba5a69db10e79c05e618c21", "title": "Conditional DETR for Fast Training Convergence"}, {"paperId": "6a57e9350aaba83e8651f4bd5715059cf120be27", "title": "Multiview Detection with Shadow Transformer (and View-Coherent Data Augmentation)"}, {"paperId": "dbc9e73b75c205d493e0a7242b038e5c6ac04efe", "title": "SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer"}, {"paperId": "0a4e7b3b98c1eea83bf253cb0bcb99e19e31659b", "title": "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding"}, {"paperId": "a9c214e846188adb645021cd7b1964b8ea1fef6f", "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"}, {"paperId": "260ad39a1dac4b451019e2bf17925f4df8e3b69a", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation"}, {"paperId": "cb56ea2d4de29481e25df6c318afc217eb7e4a7d", "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding"}, {"paperId": "64ac6c834bee275eb17f9a34d58ebf18e0f98e6a", "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "1fb10189c500e4902cd1b5afd406f57323d21be8", "title": "VOLO: Vision Outlooker for Visual Recognition"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "36b9d0f8610a82fd25854889d9327a04da4ff8fd", "title": "MST: Masked Self-Supervised Transformer for Visual Representation"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "8602fd5b0ac73bb422f238b265479f363c0ffe61", "title": "Refiner: Refining Self-attention for Vision Transformers"}, {"paperId": "9dcaf5ab101ba551ac334f3ede177a444e154643", "title": "Referring Transformer: A One-step Approach to Multi-task Visual Grounding"}, {"paperId": "3d4b1c580c4df032549a84ee1a5114a09863ce18", "title": "SOLQ: Segmenting Objects by Learning Queries"}, {"paperId": "9d1934ea1bd69d928d17e05d44495d42edf8601d", "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "14b97585f136671742f6ce4151081e487b1fc1fe", "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition"}, {"paperId": "e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60", "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"}, {"paperId": "adb4302eb7c420a46d770afe2448d4508c65fe58", "title": "ResT: An Efficient Transformer for Visual Recognition"}, {"paperId": "68f080e0ac836ea230cb5316fbed273c70422d75", "title": "Segmenter: Transformer for Semantic Segmentation"}, {"paperId": "5924ffeb05f8de7c6df37a47b3a74c63555caaf1", "title": "Visual Grounding with Transformers"}, {"paperId": "db33c408174eef1e40661e8279afbbbf6db2352c", "title": "Self-Supervised Learning with Swin Transformers"}, {"paperId": "f1324648ea19d775eef646b6e7078abe2de8eab8", "title": "Instances as Queries"}, {"paperId": "d089a60e8eb5406d70857fffaf7a4a447ae1fd20", "title": "Deep Relation Transformer for Diagnosing Glaucoma With Optical Coherence Tomography and Visual Field Function"}, {"paperId": "816b977342fd291fc4f200a9642fa6df9eb601e9", "title": "ISTR: End-to-End Instance Segmentation with Transformers"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "a1bb2e9134ca0ee67d2cbd8c54eb021625a8f17d", "title": "Vision Transformers with Patch Diversification"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "cc9f3a61ea4eaabf43cbb30cd1dd718074932679", "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers"}, {"paperId": "c0559fc7e7d6ea0f783ba791ddd5deaa74cf58a9", "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving"}, {"paperId": "ef89d5899eff8d5e62f85018b3f11889d920a1aa", "title": "TransVG: End-to-End Visual Grounding with Transformers"}, {"paperId": "14c52ffa7ea9c1971d5d82ea369c946c98d056a9", "title": "LocalViT: Bringing Locality to Vision Transformers"}, {"paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae", "title": "An Empirical Study of Training Self-Supervised Vision Transformers"}, {"paperId": "37b66aefc502b205da52a0a811fd9a3bafb399f3", "title": "Efficient DETR: Improving End-to-End Object Detector with Dense Prior"}, {"paperId": "c2cf5e5f21cca7045be719afce6bf66cc2934370", "title": "Group-Free 3D Object Detection via Transformers"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "7c3ce1b3ad598a282546e03e2dc8b52c338caed6", "title": "Transformer Tracking"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "1536febbc3e15d51642991797f6ce624838a468e", "title": "COTR: Correspondence Transformer for Matching Across Images"}, {"paperId": "b1efb8fd489a6eae614ea9c77836577603250adc", "title": "Multi-view 3D Reconstruction with Transformers"}, {"paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "2984ab83ade26639c3a82d29628d0d9e4abbebb0", "title": "Incorporating Convolution Designs into Visual Transformers"}, {"paperId": "f5b4d3131fd07681e4c039d409a450e18e30f36c", "title": "Multi-view analysis of unregistered medical images using cross-view transformers"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "a87bb9e70d1127b6a9c0e721e48c0b58c997c70e", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer"}, {"paperId": "6e8f35c6d54acb14109c9b792a62609eac8a7b5e", "title": "TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "24b8a0b02bcb7934967757fc59d273a71ba67e30", "title": "TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "7eb2e2c086c9b6e76854ee58c92f993bcd171029", "title": "Position, Padding and Predictions: A Deeper Look at Position Information in CNNs"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "69621df0df837d345d764525696899e0570194b6", "title": "Fast Convergence of DETR with Spatially Modulated Co-Attention"}, {"paperId": "73a73d8bfad0fdfd4afe7f34314ae0666a19cf14", "title": "Learning to Match Anchors for Visual Object Detection"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "d29430adccb805ab57b349afa8553954347b3197", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"}, {"paperId": "d40c77c010c8dbef6142903a02f2a73a85012d5d", "title": "A Survey on Vision Transformer"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "16ecba2d6e6907dce95560e5d3448d8bc7559b39", "title": "3D Object Detection with Pointformer"}, {"paperId": "ff50b46b4e1cc0fd9beb832fc3468785b635a824", "title": "PCT: Point cloud transformer"}, {"paperId": "6a9f9a9f1f7f0a6862c774661b8e99976e1184c9", "title": "What Makes for End-to-End Object Detection?"}, {"paperId": "787119e3c3f819244c82b7d97779473773e60696", "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"}, {"paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a", "title": "Pre-Trained Image Processing Transformer"}, {"paperId": "2ac7999cce9f415ee87643f56631b55ed26aa10e", "title": "End-to-End Video Instance Segmentation with Transformers"}, {"paperId": "1b945b488732aff7583319b8e962854c5c327926", "title": "Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks"}, {"paperId": "6d5f423164cd5ef9324281652987c8a65009e98e", "title": "Sparse R-CNN: End-to-End Object Detection with Learnable Proposals"}, {"paperId": "234763381de73a18f49430b0238310a6853d184e", "title": "Rethinking Transformer-based Set Prediction for Object Detection"}, {"paperId": "42aa775a29ff4b3a1cec178301708b8c54266ae7", "title": "Attention-Based Transformers for Instance Segmentation of Cells in Microstructures"}, {"paperId": "2e1db8cb373f4d4a51d44308b7a457886d855fbb", "title": "End-to-End Object Detection with Adaptive Clustering Transformer"}, {"paperId": "c13a8f9edb933e60c7a989244aee56283a54ce37", "title": "UP-DETR: Unsupervised Pre-training for Object Detection with Transformers"}, {"paperId": "706bb87043cb169518a94e69a2ee7228feb0ebbe", "title": "Human-Centric Spatio-Temporal Video Grounding With Visual Transformers"}, {"paperId": "e1d082562981a9f51649c60663aa484ee623dbb0", "title": "Point Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "53794499a3830c3ebb365ecc57f0e8c8a20a682d", "title": "ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes"}, {"paperId": "672aef6f84f240005a09cf4f31ab6880584c1491", "title": "Feature Pyramid Transformer"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af", "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments"}, {"paperId": "a0185d4f32dde88aa1749f3a8000ed4721787b65", "title": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision"}, {"paperId": "5335fe1bf347f7ad1dce1611ea4b60bd8391a090", "title": "Transferring Inductive Biases through Knowledge Distillation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33", "title": "Exploring Self-Attention for Image Recognition"}, {"paperId": "7b9b756ab509cb9f52dbac95e3e901d571f0784f", "title": "A Survey of the Usages of Deep Learning for Natural Language Processing"}, {"paperId": "fb93ca1e004cbdcb93c8ffc57357189fa4eb6770", "title": "ResNeSt: Split-Attention Networks"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "4f7712e6d5b55ff524e31de7788459cde4c7adb0", "title": "Global and Local Knowledge-Aware Attention Network for Action Recognition"}, {"paperId": "d73795d03114e3ff80c1b42e9f7a1bb95872bea9", "title": "SOLOv2: Dynamic and Fast Instance Segmentation"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "d14e56568dc5f57ccdae899d84f91e34ad847670", "title": "How Much Position Information Do Convolutional Neural Networks Encode?"}, {"paperId": "3b44345d670d816fb5a3e53262e2d35413a163ec", "title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences"}, {"paperId": "02acc835a3d5e115d86cb6885a28d9bab78c86d9", "title": "BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation"}, {"paperId": "bc51622358d8eea83248ef29402fe10640d07ba6", "title": "Big Transfer (BiT): General Visual Representation Learning"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "8cb34cbdcf65c23ef98430441b14a648c4e8d992", "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "55e960535f643637161b2e99a8c21a92c0d13757", "title": "Representation Degeneration Problem in Training Natural Language Generation Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "bc626a52664e948a0ffb2b95d0e1e6377a01171a", "title": "Cascade R-CNN: High Quality Object Detection and Instance Segmentation"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "title": "Local Relation Networks for Image Recognition"}, {"paperId": "66143960c0325c70329a3869cc8052f0416b87aa", "title": "GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond"}, {"paperId": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks"}, {"paperId": "dabc815780ab0aa0c8136b58c0cf1abec1336884", "title": "Pixel-Adaptive Convolutional Neural Networks"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "e2751a898867ce6687e08a5cc7bdb562e999b841", "title": "FCOS: Fully Convolutional One-Stage Object Detection"}, {"paperId": "6303bac53abd725c3b458190a6abe389a4a1e72d", "title": "Deep High-Resolution Representation Learning for Human Pose Estimation"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "ab456c1ed181c5c48a34adb61395d4806a0ba949", "title": "Attention in Natural Language Processing"}, {"paperId": "5132500b23d2da47129b3f4f68dd30947a29e502", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation"}, {"paperId": "4bbfd46721c145852e443ae4aad35148b814bf91", "title": "TSM: Temporal Shift Module for Efficient Video Understanding"}, {"paperId": "5ac18d505ed6d10e8692cbb7d33f6852e6782692", "title": "The Open Images Dataset V4"}, {"paperId": "b7339c1deeb617c894cc08c92ed8c2d4ab14b4b5", "title": "A2-Nets: Double Attention Networks"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2", "title": "CBAM: Convolutional Block Attention Module"}, {"paperId": "7998468d99ab07bb982294d1c9b53a3bf3934fa6", "title": "Object Detection With Deep Learning: A Review"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "3a58efcc4558727cc5c131c44923635da4524f33", "title": "Relational inductive biases, deep learning, and graph networks"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "dce916351ef589afa7a63452648dd8acba931e92", "title": "Panoptic Segmentation"}, {"paperId": "815aa52cfc02961d82415f080384594639a21984", "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification"}, {"paperId": "04957e40d47ca89d38653e97f728883c0ad26e5d", "title": "Cascade R-CNN: Delving Into High Quality Object Detection"}, {"paperId": "6a0aaefce8a27a8727d896fa444ba27558b2d381", "title": "Relation Networks for Object Detection"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8674494bd7a076286b905912d26d47f7501c4046", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "4a73a1840945e87583d89ca0216a2c449d50a4a3", "title": "Deformable Convolutional Networks"}, {"paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878", "title": "Feature Pyramid Networks for Object Detection"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "761b69164d8b34398f010931faa534c031bdaad6", "title": "Why Deep Learning Works: A Manifold Disentanglement Perspective"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "cab372bc3824780cce20d9dd1c22d4df39ed081a", "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"}, {"paperId": "c91baa9a5bd7e9ff685478a58edc5a1f41fd33c5", "title": "Convolution in Convolution for Network in Network"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd", "title": "You Only Look Once: Unified, Real-Time Object Detection"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "title": "LSTM: A Search Space Odyssey"}, {"paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "d4414fb2e91836a82799d94251e27ed12f46d879", "title": "Classification in the Presence of Label Noise: A Survey"}, {"paperId": "44040913380206991b1991daf1192942e038fe31", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "3f1e54ed3bd801766e1897d53a9fc962524dd3c2", "title": "Locality-sensitive hashing scheme based on p-stable distributions"}, {"paperId": "9af3bef464bde26096ba9270126126270c10db03", "title": "MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection"}, {"paperId": "64cb695fef260e36c1d3d8830b197923f1e865ea", "title": "FP-DETR: Detection Transformer Advanced by Fully Pre-training"}, {"paperId": "b50bd1f96e20952d7ad1bf774ea6199ce12f2fa6", "title": "Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length"}, {"paperId": "a4f09d7b8b44f1a1dbfba06e562210e7e982d2fb", "title": "QueryInst: Parallelly Supervised Mask Query for Instance Segmentation"}, {"paperId": "279696f90d13b1327ec7adb73e711e7d8f5db761", "title": "Token Labeling: Training a 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "03b537c948a9711cf2a7b324e02e030874df8f73", "title": "HRViT: Multi-Scale High-Resolution Vision Transformer"}, {"paperId": "2a23ffef0b4f5f8689ffffb4cd9f515cb28336bd", "title": "HRFormer: High-Resolution Vision Transformer for Dense Predict"}, {"paperId": "0a095687420af951092ffd3cda1fa3d0e790d79c", "title": "by Segmentation"}, {"paperId": "44b49bcd122eb9b336d6a2545665d2e80a78b014", "title": "Layer"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5925a25dfe107c49c636eccb8f9fd1aeef7b438c", "title": "Temporal Shift Module for Efficient Video Understanding"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "The improved bipartite matching strategy is bene\ufb01cial to avoid redundant prediction, add positive gradients, and perform end-to-end object detection"}, {"paperId": null, "title": "is with the School of Computer Science and Engineering, Southeast University, Nanjing 214135, China"}, {"paperId": null, "title": "Lenovo Research, Beijing 100000, China"}, {"paperId": null, "title": "The deep Transformer, such as Re\ufb01ned-ViT [37] and CaiT [42]"}, {"paperId": null, "title": "Intuitive Comparison"}, {"paperId": null, "title": "Multiscale features and iterative box re\ufb01nement are bene\ufb01t DETR for small object detection"}]}