{"paperId": "24e775b20adf21e9b5b95c6a9b7a5c164d055849", "abstract": "Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called\"Pseudo-to-Real\"for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.", "venue": "arXiv.org", "year": 2021, "citationCount": 35, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Pseudo-to-Real is a simple training strategy for high-memory-footprint-required large models that is compatible with large models with architecture of sequential layers and demonstrates a practice of pretraining unprecedented 10-trillion-parameter model on solely 512 GPUs within 10 days."}, "embedding": {"model": "specter_v2", "vector": [0.2517399191856384, 0.6745774149894714, -0.3337565064430237, 0.08094524592161179, 0.1109878420829773, -0.05593683198094368, 0.4908578395843506, -0.5326009392738342, -0.4179089367389679, -0.4220368266105652, -0.2749681770801544, 0.07442574203014374, 0.3140977919101715, 0.07118361443281174, -0.43731367588043213, 0.10306114703416824, -1.0318654775619507, 0.8610121607780457, 0.22042156755924225, -0.40280237793922424, 0.001511238981038332, -0.2089647650718689, -1.3655444383621216, 0.3905135691165924, 0.47307419776916504, 1.1778919696807861, -0.09933046996593475, 0.8766768574714661, -0.2549718916416168, -0.12455667555332184, 0.4814455807209015, -0.2762100100517273, 0.8960568904876709, 0.249341681599617, -0.15364675223827362, -0.07633861154317856, 0.4734032154083252, -0.1562073528766632, -0.30293720960617065, 0.7383664846420288, -0.03938165307044983, 0.25881242752075195, 0.24801576137542725, -0.9945427775382996, 0.06877164542675018, 0.40534698963165283, 0.28733935952186584, 0.6967540383338928, -0.6925599575042725, -0.14254528284072876, 1.0194730758666992, -1.186514973640442, -0.0519653856754303, 1.083763599395752, 0.7332956194877625, 0.5161969661712646, -0.2466878592967987, -0.8105546236038208, 0.3414384126663208, -0.07680684328079224, -0.6341074705123901, -0.08213351666927338, 0.16041405498981476, 0.0025758559349924326, 1.8232569694519043, -0.7086504697799683, 0.6243321895599365, 0.6917758584022522, -0.07090699672698975, 1.2466773986816406, 0.10358911752700806, -0.6835542321205139, 0.015948843210935593, 0.28155970573425293, 0.17738452553749084, 0.8290886282920837, 0.01433798111975193, 0.33260491490364075, -1.0699632167816162, -0.13381077349185944, 0.750461757183075, 0.08347180485725403, 0.5503937005996704, -0.33264395594596863, -0.40512004494667053, 0.7540830373764038, 0.6230112314224243, 0.4678492248058319, -0.4795331060886383, 1.109434723854065, 0.2522251307964325, 0.4075992703437805, -0.06022699922323227, 0.7295585870742798, -0.14923666417598724, 0.19512289762496948, -0.8188045620918274, 0.1384022831916809, -0.09159989655017853, 0.6811943650245667, -0.0706932470202446, 0.29184749722480774, -0.16130878031253815, 0.1396629810333252, 1.137426733970642, -0.177399143576622, 0.6148373484611511, -0.9450904130935669, 0.43048012256622314, -0.2634887397289276, 0.3975178599357605, -0.536137580871582, -0.24170979857444763, -0.6371781826019287, -0.8845394253730774, -0.7303043603897095, -0.7027925848960876, -0.017779838293790817, -0.9493595361709595, 0.3588440716266632, -0.008739463984966278, 0.633029043674469, -0.092509426176548, 0.6566588878631592, 0.29816392064094543, 0.7714927792549133, -0.028490278869867325, 0.4397582709789276, 0.7975203394889832, -1.1662943363189697, 0.1224827691912651, -1.148306131362915, 0.20800526440143585, 0.2183966040611267, 0.38420116901397705, 0.2352762371301651, -1.3839268684387207, -0.9728959798812866, -1.1242256164550781, 0.4583924412727356, -0.6344348788261414, 0.14659886062145233, 1.547018051147461, 0.17303195595741272, -0.7720130681991577, 1.0587702989578247, -0.47007766366004944, -0.08040565252304077, 0.395757794380188, 0.36419183015823364, 0.3213484287261963, 0.12141994386911392, -1.1663089990615845, 0.08108218014240265, 0.7355575561523438, -0.17049868404865265, -0.4741223156452179, -0.8793970346450806, -0.3059113323688507, 0.12722301483154297, 0.21141736209392548, -0.8781581521034241, 1.061280369758606, -0.2632005214691162, -1.3702343702316284, 1.140053629875183, 0.1428283005952835, -0.10751810669898987, 0.594664454460144, 0.060314636677503586, -0.6140804886817932, -0.1038007065653801, -0.877169132232666, 1.1192070245742798, 0.8092561364173889, -0.22315368056297302, 0.0574137382209301, 0.09855376183986664, -0.0392753928899765, 0.013651463203132153, -0.3931286036968231, 0.6030910611152649, -0.5617892146110535, 0.03724927827715874, 0.5930827260017395, 0.33221015334129333, -0.31369301676750183, -0.07492626458406448, -0.1998368799686432, -0.8644728064537048, 0.4667190611362457, 0.5796915888786316, 0.6315281987190247, -0.8121880292892456, -0.7250868678092957, 0.17265203595161438, 0.17679540812969208, -0.010923168621957302, -0.48268380761146545, 0.33217641711235046, -0.5334713459014893, 0.33010032773017883, -0.2227902114391327, -0.7936080694198608, 0.17241422832012177, -0.27050498127937317, -0.704778254032135, -0.12906138598918915, 0.19523943960666656, 1.03974449634552, -0.6386180520057678, 0.32867780327796936, -0.08401477336883545, 0.2166285365819931, -1.3364917039871216, 0.9024401307106018, -0.7100143432617188, 0.12377537786960602, -0.05891314148902893, -0.3211372196674347, 0.1188306137919426, -0.6138598918914795, 0.31952694058418274, -0.5355733633041382, 0.20007573068141937, 0.42086073756217957, -0.6934220790863037, 1.5350613594055176, -0.1941860467195511, 0.24526157975196838, -0.12236864864826202, -0.6469295024871826, 0.13283094763755798, 0.1870049387216568, -0.12148545682430267, -0.5163725018501282, 0.31341302394866943, 0.6407647132873535, -0.476359486579895, 0.41458019614219666, 0.8201258778572083, 0.9527041912078857, -0.299325555562973, -0.0010926952818408608, 0.425236314535141, -0.593670129776001, 0.3022473454475403, 0.2102021872997284, 0.004419359378516674, 0.4447193443775177, -0.026270976290106773, -0.23315279185771942, -0.07764352113008499, -0.7553218603134155, 0.08946247398853302, 0.41816073656082153, 0.5935290455818176, 0.647137463092804, 0.46761688590049744, -1.2758687734603882, -0.7156987190246582, 0.2190532237291336, 0.615263044834137, 1.3352364301681519, -0.1511942744255066, 0.047988589853048325, -0.7013348340988159, -0.29874327778816223, 0.07602521032094955, 0.012554172426462173, -0.29251542687416077, -0.2657301127910614, -0.7054587602615356, -1.5969122648239136, 0.8256194591522217, -0.06617850065231323, 1.1358882188796997, -0.6954194903373718, -0.8592217564582825, -0.47831860184669495, 0.7831222414970398, -0.75580894947052, -0.3191254138946533, 0.545722246170044, -0.7550398111343384, 0.05843822658061981, 0.09692098945379257, -0.058728232979774475, 0.33167943358421326, -0.5569007992744446, 0.8393987417221069, -0.3259687125682831, -0.4528142809867859, -0.20704098045825958, 1.069371223449707, -0.42159807682037354, -0.42816147208213806, 0.12208631634712219, 0.22905921936035156, -0.012967568822205067, -0.10519269853830338, -0.14829595386981964, -0.4299262762069702, 0.17568251490592957, -0.195040762424469, 0.39210620522499084, 0.03883393853902817, -0.19501851499080658, 0.8257436752319336, -0.3630795478820801, 0.037284351885318756, -1.3528845310211182, 0.7290062308311462, 0.09935585409402847, -0.6674185395240784, 0.26586928963661194, -0.949657142162323, -0.3188437521457672, 0.6650164723396301, -0.9935206770896912, -0.007490155752748251, -1.2542210817337036, 0.331745445728302, -0.6793227195739746, -0.15184563398361206, -0.18142305314540863, 0.5352749228477478, -0.3391355276107788, 0.7022563219070435, 0.26146233081817627, -0.2864989936351776, -0.05826401337981224, 0.571388304233551, -1.1643521785736084, 0.527579665184021, 0.07479391247034073, 0.18835802376270294, -0.1320173144340515, 0.06528569012880325, -0.2812294065952301, -0.6175131797790527, -0.18890751898288727, -0.28282684087753296, -0.23585887253284454, 0.08015500754117966, -0.5096558928489685, -0.9126531481742859, 0.2967988848686218, -0.7512316107749939, -0.6932394504547119, 0.05383537709712982, -0.1960831880569458, -0.2707711458206177, -1.332452654838562, -1.0592427253723145, -0.19929642975330353, -0.7719987630844116, -0.9420315027236938, 0.2936501204967499, 0.35392090678215027, 0.014548503793776035, -0.5778828859329224, -0.18860334157943726, -0.32218268513679504, 1.1095207929611206, -0.3678210973739624, 0.640622615814209, -0.01690440997481346, -0.1683458387851715, -0.13730324804782867, -0.0028397103305906057, 0.18488317728042603, -0.8984643220901489, 0.22992999851703644, -0.8526493310928345, 0.17039933800697327, -0.643337607383728, -0.805722713470459, 0.3614683449268341, -0.0728713870048523, 0.8413280844688416, 0.03791709989309311, -0.3800078332424164, 1.0975230932235718, 1.4328386783599854, -0.8246578574180603, 0.23516270518302917, 0.1979096382856369, 1.117577075958252, -0.23507259786128998, -0.2889832556247711, 0.6824965476989746, 0.14224697649478912, 0.09427454322576523, 0.39159587025642395, -0.3659684658050537, -0.1690852791070938, -0.6201550960540771, 0.07600364834070206, 1.2888630628585815, 0.520430326461792, -0.0008764405502006412, -0.9446613788604736, 0.41623780131340027, -0.881335437297821, -0.5713915228843689, 0.7702065110206604, 0.9451213479042053, 0.32744771242141724, 0.1934528946876526, -0.3421204388141632, -0.49316442012786865, 0.5638576149940491, 0.38148728013038635, -0.8993304967880249, -0.8229512572288513, -0.052875082939863205, 0.2339051514863968, 0.304510235786438, 0.13514651358127594, 0.127597838640213, 0.8594394326210022, 14.978768348693848, 0.9806028008460999, -0.27589115500450134, 0.6024863719940186, 0.9159741997718811, 0.15542277693748474, -0.43144914507865906, -0.11044687777757645, -1.3266831636428833, 0.1136188730597496, 1.6207634210586548, 0.4229329228401184, 0.9377990961074829, 0.21211284399032593, -0.30193227529525757, 0.10576704889535904, -0.3927990794181824, 1.1024218797683716, 0.40402790904045105, -1.34169602394104, 0.4768756628036499, 0.2008899748325348, 0.7506940364837646, 0.9551244974136353, 0.8122493028640747, 0.9020112752914429, 0.4867568016052246, -0.4475994110107422, 0.4279465079307556, 0.35626551508903503, 1.1212342977523804, -0.27816736698150635, -0.06208799034357071, 0.5799964666366577, -0.6196393966674805, 0.040695201605558395, -0.806776225566864, -0.9544511437416077, -0.07170533388853073, 0.14368650317192078, -0.3107018768787384, -0.5353906154632568, -0.11899196356534958, 0.4625599980354309, 0.15535461902618408, 0.5273427963256836, -0.2107749581336975, 0.3685978651046753, -0.7897246479988098, 0.34899356961250305, 0.336174875497818, 0.04764881357550621, 0.08064886182546616, -0.4469795525074005, 0.1693447083234787, -0.03904274106025696, 0.1853664666414261, 0.6197406053543091, -0.7973902821540833, -0.5335742831230164, -0.13354378938674927, -0.36259204149246216, 0.07125195115804672, 1.1966301202774048, 0.24353985488414764, 0.13634492456912994, 0.17611096799373627, 0.5165291428565979, 0.7799239754676819, 0.022171415388584137, -0.6802322864532471, 0.0026069344021379948, 0.07985886931419373, -0.751639187335968, -0.17947354912757874, 0.30977150797843933, -0.29809504747390747, -0.7251062989234924, -0.5803143382072449, -0.5918125510215759, 0.17879557609558105, -0.8273556232452393, -0.3307477533817291, 0.804062008857727, -0.33859342336654663, -0.32639917731285095, 0.3114801347255707, -0.7718304991722107, -0.43491995334625244, 0.19644476473331451, -1.401984691619873, -0.6628649234771729, 0.2803046703338623, -0.4210032820701599, -0.469927579164505, -0.143384650349617, 1.0070409774780273, 0.16164100170135498, -0.561125636100769, 0.2751045525074005, 0.06420070677995682, -0.597479522228241, -0.39850595593452454, -0.6048564314842224, 1.45112144947052, -0.11563863605260849, -0.04580051451921463, -0.027342965826392174, -0.13685721158981323, 0.4653787910938263, -0.9279757738113403, 0.07670002430677414, 0.14755599200725555, -0.7171773314476013, -0.09904749691486359, -0.943252444267273, -0.6653797030448914, 0.2707647979259491, 0.31898072361946106, 0.16485093533992767, 0.009787525981664658, -0.07384523004293442, -0.621170163154602, -0.20848163962364197, -0.9601052403450012, 0.12661762535572052, 0.7417659163475037, -0.6521226763725281, -0.31161752343177795, 0.03697925806045532, 0.2678726613521576, -1.0897406339645386, -0.7664934992790222, 0.004059407860040665, 0.3534500002861023, -0.44481658935546875, 1.2771644592285156, -0.3402400016784668, 0.2761496603488922, 0.9430383443832397, 0.09939083456993103, -0.8638408184051514, 0.2445012927055359, -1.0256555080413818, -0.014583184383809566, -0.2139175534248352, 0.6322628259658813, -0.8318586349487305, 0.6771528124809265, 0.7417191863059998, 0.3172720670700073, -0.7357162237167358, -0.28076913952827454, -0.16339655220508575, 0.002322019776329398, -0.5523878335952759, 0.5491880774497986, 0.15323974192142487, -0.2510347366333008, -0.046498168259859085, 0.307507187128067, 0.5727615356445312, -0.03967766463756561, -0.45965850353240967, 0.3382185399532318, -0.23462380468845367, -0.4198642075061798, -0.4225355088710785, -0.40421047806739807, -1.5248680114746094, -0.321613609790802, -1.3619258403778076, -0.2803962528705597, -0.6911876201629639, -0.6061023473739624, -0.4179254174232483, -0.34381046891212463, -0.12069510668516159, 0.5163424015045166, 0.047658659517765045, -0.41447415947914124, -0.22846047580242157, -0.6494956016540527, 0.8905808925628662, 0.7621598243713379, -0.3858153522014618, -0.12316536158323288, -0.08898716419935226, 0.17866647243499756, 0.6445284485816956, 0.6669180393218994, -0.4300917387008667, -0.7314286231994629, -1.3139437437057495, 0.8042495250701904, -0.19904524087905884, 0.003227018751204014, -1.2808080911636353, 0.7199052572250366, 0.2283291071653366, -0.11391756683588028, 0.022031521424651146, 0.6291244029998779, -1.06308114528656, -0.26096174120903015, 0.36365580558776855, -0.4172423183917999, 0.43901291489601135, 0.4437008500099182, -0.6388870477676392, 0.09080220758914948, 0.6086457967758179, 0.06215101107954979, -0.9211578369140625, -1.0676307678222656, 0.42327263951301575, -0.25487786531448364, 0.3244325518608093, -0.08257922530174255, -0.24674317240715027, -1.2875192165374756, 0.2570236623287201, -0.13463397324085236, 0.6574748754501343, -0.22392889857292175, 0.6496532559394836, 0.38916444778442383, -1.080599308013916, 0.2747061848640442, 0.6917025446891785, -0.32279759645462036, 0.13467256724834442, 0.7417199015617371, 0.46455150842666626, -0.5839133858680725, 0.3021795451641083, -0.20523151755332947, 0.301845520734787, -0.03892843425273895, 0.09030813723802567, 1.11447012424469, -0.6395767331123352, -0.34771493077278137, 1.2566808462142944, -0.30305129289627075, -1.3996024131774902, 0.26553526520729065, -0.861601710319519, -0.359906941652298, -0.3548053503036499, 0.4051845669746399, 0.18037262558937073, -0.039489179849624634, 0.42848047614097595, -0.3779803514480591, 0.46918076276779175, -0.0037771877832710743, -0.36174044013023376, 0.5615648031234741, 0.04872769117355347, -0.033191584050655365, 0.8015941977500916, 0.7471151947975159, -0.8265829086303711, -1.2837156057357788, -0.516635000705719, -0.509041428565979, 0.12457714229822159, 0.3218148648738861, -0.6092334985733032, -0.7874873876571655, 1.0595077276229858, 0.6849238872528076, 0.008591712452471256, 0.04667891561985016, -0.4258025884628296, 0.2826847732067108, 1.0307725667953491, 0.37181326746940613, -0.7572337985038757, -0.18192733824253082, 1.0497997999191284, 0.9918229579925537, -0.7826591730117798, 0.24396471679210663, -0.11007693409919739, -0.7923359870910645, 0.7380625009536743, 0.7373899221420288, -0.6010682582855225, 0.6034330129623413, -0.41420039534568787, -0.2967592477798462, -0.15774519741535187, -1.2521708011627197, -0.015974899753928185, 0.84661865234375, 0.3777957856655121, 0.05085952952504158, 0.2210981845855713, 0.4625183641910553, 0.6185728907585144, -0.034040793776512146, -0.08098871260881424, 0.36898988485336304, 0.3465684652328491, -0.27270936965942383, 0.3183876574039459, 0.10347581654787064, 0.622406005859375, -0.5336260199546814, -0.5947770476341248, 0.2616611421108246, 0.6556850671768188, 0.4355684220790863, 0.2367173731327057, 0.9203421473503113, 0.06554342806339264, 0.4972412586212158, 0.16110053658485413, 0.7601196765899658, -0.26337766647338867, -0.46067652106285095, -0.1648995280265808, -0.6939585208892822, -0.19851882755756378, 0.19620837271213531, -0.014307301491498947, -0.5752974152565002, -0.5017127990722656, 0.2556588649749756, 0.2616337835788727, 0.4953843355178833, 0.6703033447265625, 0.7922918796539307, 0.6035915017127991, -0.169669046998024, -1.1486612558364868, -0.4683626890182495, -0.6148809194564819, 0.0358409509062767, -0.5436206459999084, -0.7231854796409607, -0.2529827952384949, -0.1948663890361786, -0.8209880590438843]}, "authors": [{"authorId": "35996608", "name": "Junyang Lin"}, {"authorId": "143936592", "name": "An Yang"}, {"authorId": "41211611", "name": "Jinze Bai"}, {"authorId": "144161025", "name": "Chang Zhou"}, {"authorId": "1485086888", "name": "Le Jiang"}, {"authorId": "3440541", "name": "Xianyan Jia"}, {"authorId": "153294981", "name": "Ang Wang"}, {"authorId": "40539618", "name": "J. Zhang"}, {"authorId": "1962987077", "name": "Yong Li"}, {"authorId": "71666838", "name": "Wei Lin"}, {"authorId": "1709595", "name": "Jingren Zhou"}, {"authorId": "38385080", "name": "Hongxia Yang"}], "references": [{"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "f131e2f7bcf250a7ee25b79a0b9a442f12bd7df1", "title": "M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers"}, {"paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "title": "CogView: Mastering Text-to-Image Generation via Transformers"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "a11676f2864b2d923bb9facc9f6548c812f9e005", "title": "M6: A Chinese Multimodal Pretrainer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "cc50f846ed7222698d130cddbc58ed4d547914ed", "title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model"}, {"paperId": "629801eb8a8b94104ed7403dd6709a4b2f6f8cdf", "title": "Whale: A Unified Distributed Training Framework"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "cd410e078d091c567a1fb3438f705ac5d75009ec", "title": "EFLOPS: Algorithm and System Co-Design for a High Performance Distributed Training Platform"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "9a618cca0d2fc78db1be1aed70517401cb3f3859", "title": "Deep Equilibrium Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "91a7d2af2c3294a8e0356e4eeb043636bbb3ca04", "title": "M6-T: Exploring Sparse Expert Models and Beyond Anonymous ACL submission"}, {"paperId": "5861636a1c3c0e9ccf6f683fe70b3b5182ef1697", "title": "Whale: Scaling Deep Learning Model Training to the Trillions"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": null, "title": "Fairscale: A general purpose modular pytorch library for high performance and large scale training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "a9075f6332542e12b2bf3cdbdb3a6ed44733fb41", "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}]}