{"paperId": "f2e78a574925486d1f13440f55688bcffde80101", "abstract": "Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase in parameters via the inserted adapters. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our sparse models, dubbed Camelidae outperform all other opensource sparse models and exhibit superior general capabilities compared to GPT3.5.", "venue": "arXiv.org", "year": 2024, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture, facilitating model capacity expansion through a minimal increase in parameters via the inserted adapters."}, "embedding": {"model": "specter_v2", "vector": [0.2824493646621704, 0.4236851632595062, -0.5018519759178162, -0.00733557203784585, -0.17915557324886322, -0.15532442927360535, 0.532311737537384, -0.40248677134513855, -0.3836604058742523, -0.4290958344936371, 0.4640029966831207, -0.14652056992053986, 0.38923242688179016, 0.39386045932769775, 0.06040423363447189, 0.4101808965206146, -0.707216203212738, 0.6184401512145996, 0.03752744942903519, -0.5438287258148193, -0.11283271014690399, -0.6738881468772888, -1.3177133798599243, 0.14453236758708954, 0.8566466569900513, 0.5220797657966614, 0.38889551162719727, 0.7665640711784363, -0.581635057926178, -0.09503418207168579, 0.8087470531463623, -0.06386662274599075, 0.32588300108909607, 0.2414999157190323, 0.26936131715774536, -0.14252908527851105, 0.4819929301738739, -0.4435870051383972, -0.5278519988059998, 0.7735508680343628, -0.22824011743068695, 0.27578964829444885, 0.9256244897842407, -0.7122927308082581, -0.27021658420562744, 0.6397411227226257, 0.725524365901947, 0.8514052033424377, -0.3196446895599365, -0.15489766001701355, 1.1114814281463623, -1.5060580968856812, 0.23966394364833832, 1.3571258783340454, 0.5722174644470215, 0.4504232704639435, -0.22912299633026123, -0.7795067429542542, 0.6067598462104797, -0.19292527437210083, -0.7757806181907654, -0.6306201815605164, -0.5463070273399353, -0.23623105883598328, 2.367100715637207, -0.5936921238899231, -0.33126264810562134, 0.533899188041687, 0.11325901001691818, 1.2560087442398071, 0.031984709203243256, -0.483790785074234, -0.27863651514053345, -0.01522908266633749, 0.2800578474998474, 1.1437461376190186, -0.38762831687927246, 0.3786460757255554, -1.1433006525039673, -0.3353467881679535, 0.664950430393219, -0.4132302403450012, -0.031203025951981544, -0.06973344832658768, -0.37208402156829834, 0.977283775806427, 0.09372802823781967, 0.8954207897186279, -0.12955495715141296, 0.283285528421402, 0.6035097241401672, 0.300767183303833, -0.11860929429531097, 0.5473639369010925, -0.4509861171245575, 0.3932303190231323, -1.0283477306365967, 0.028549665585160255, 0.7695348858833313, 1.078786015510559, -0.3017154633998871, 0.37130671739578247, -1.0236971378326416, 0.23760701715946198, 1.3396321535110474, 0.09490533918142319, 0.6427037715911865, -0.5398476719856262, 0.507871687412262, -0.7586350440979004, 0.20155185461044312, -0.844538152217865, -0.4047127962112427, -0.4515209496021271, -0.9447770714759827, -1.4019230604171753, -0.3951660990715027, 0.13455592095851898, -0.8440108895301819, 0.9763213992118835, -0.2814772427082062, 0.28202664852142334, 0.020147420465946198, 0.37570229172706604, 0.8669675588607788, 0.8452455997467041, 0.5733895897865295, 0.26359236240386963, 1.1523412466049194, -0.8641235828399658, -0.5920093655586243, -1.4207614660263062, 0.7756989002227783, -0.3210797607898712, 0.3350074589252472, -0.04568987712264061, -1.0407472848892212, -0.9552364349365234, -0.5064004063606262, -0.27540484070777893, -0.26382842659950256, 0.8529597520828247, 1.1335312128067017, 0.475790411233902, -0.8637674450874329, 0.4183286130428314, -0.10681644827127457, -0.1566230207681656, 0.35609644651412964, 0.298891544342041, 0.10385322570800781, -0.5485984086990356, -1.536596417427063, 0.13836945593357086, 0.6929075121879578, -0.8385290503501892, -0.5256714224815369, -0.7583867311477661, -1.2649898529052734, 0.4563083350658417, 0.4893142282962799, -0.761652946472168, 1.1614030599594116, -0.06073390692472458, -1.2003743648529053, 0.7547444701194763, -0.5348871350288391, 0.1358005553483963, -0.06160394847393036, 0.1344359964132309, -0.6017115712165833, -0.28247568011283875, -0.13356997072696686, 0.8750877976417542, 0.8640416860580444, 0.08053123950958252, -0.14780546724796295, 0.5263229012489319, -0.2365730106830597, 0.07467707246541977, -0.3872109353542328, 0.7409626841545105, -0.5506511330604553, -0.27948197722435, 0.36236807703971863, 0.5396853089332581, -0.20081458985805511, -0.10759725421667099, -0.1517910361289978, -1.1107240915298462, 0.6029220819473267, -0.22739052772521973, 1.008394718170166, -1.0869004726409912, -0.5875296592712402, 0.06934557110071182, 0.10563302785158157, 0.060566291213035583, -1.0332344770431519, 0.4108828008174896, -0.2579675316810608, 0.25583523511886597, -0.4054809808731079, -1.1886911392211914, 0.2934359908103943, -0.15603961050510406, -0.4583955705165863, -0.47389715909957886, 0.25051170587539673, 1.0277844667434692, -0.9238995313644409, 0.06611497700214386, -0.16437658667564392, 0.4447144567966461, -1.420413851737976, 1.0381669998168945, -0.8007704019546509, 0.3737773895263672, -0.015259843319654465, -0.3224192261695862, -0.14539644122123718, -0.6069197058677673, 0.34133562445640564, -0.7924712896347046, 0.18135155737400055, 0.41467979550361633, -0.6548112630844116, 1.6096012592315674, -0.63126540184021, 0.12548698484897614, 0.20429763197898865, -0.5360534191131592, 0.2198006808757782, 0.6531468033790588, -0.4554601013660431, -0.45629191398620605, 0.39681780338287354, 0.9587071537971497, -0.42419159412384033, 0.1199064776301384, 0.6110873222351074, 0.8087831735610962, -0.2670954167842865, 0.3667563199996948, 0.7224934697151184, -0.5930860638618469, 0.9719684720039368, 0.48318901658058167, 0.5055515170097351, 0.1318909376859665, 0.7299178242683411, -0.3274482190608978, 0.5538250803947449, -0.7781885862350464, -0.3499572277069092, 0.4883621037006378, 0.8257251381874084, 0.5979606509208679, 0.35692328214645386, -0.6647703051567078, -0.30272620916366577, 0.14659510552883148, 0.7019391059875488, 1.8954788446426392, -0.578730046749115, 0.16454468667507172, -0.5383760333061218, -0.3717856705188751, -0.34537577629089355, 0.16377660632133484, -0.13630901277065277, -0.07864151149988174, -0.43184104561805725, -1.030871033668518, 0.3659728169441223, 0.16538362205028534, 0.9930827617645264, -0.48601847887039185, -0.23226608335971832, -0.31353628635406494, 0.2408437430858612, -1.011696696281433, -1.0550228357315063, 0.4339561462402344, -0.5983757376670837, -0.3081416189670563, 0.1694955974817276, -0.16960445046424866, 0.3385781943798065, -0.4354797899723053, 1.4013386964797974, -0.7388651371002197, -0.10550958663225174, 0.401806503534317, 0.5414146780967712, -0.35270941257476807, -0.9117759466171265, 0.6127065420150757, 0.26563289761543274, -0.18583877384662628, 0.44426238536834717, 0.43821683526039124, 0.2496330887079239, 0.28447774052619934, -0.6046684384346008, 0.14577068388462067, 0.3333772122859955, -0.008306113071739674, 0.901304304599762, -0.3637760281562805, 0.09650223702192307, -1.4829840660095215, 1.0607258081436157, -0.36334121227264404, -0.3218037188053131, 0.24207086861133575, -0.21208040416240692, -0.026251396164298058, 0.7532047629356384, -0.7550827860832214, -0.53333979845047, -0.5188450813293457, 0.21211668848991394, -0.1681034415960312, -0.4721759557723999, 0.3340323567390442, 0.33584073185920715, -0.27680593729019165, 0.6819260716438293, 0.4739607870578766, 0.33479955792427063, 0.13345040380954742, 0.6332502961158752, -0.8999202251434326, 0.5766622424125671, 0.006148126907646656, 0.07804392278194427, -0.37154415249824524, -0.5679865479469299, -0.6136717796325684, -0.3947361707687378, -0.47505906224250793, -0.09151212871074677, 0.23924681544303894, 0.033619511872529984, -0.8603909611701965, -0.6161434054374695, -0.25126928091049194, -0.9796624779701233, -0.1345728635787964, 0.346213698387146, -0.22058840095996857, -0.10454215109348297, -1.0429803133010864, -1.1797184944152832, -0.0041787633672356606, -0.769778847694397, -1.3059890270233154, 0.6068671345710754, 0.1945154219865799, -0.5967571139335632, -0.7998479008674622, -0.1630716323852539, -0.37445884943008423, 1.3327394723892212, -0.9697214365005493, 1.0965791940689087, -0.4561889171600342, 0.005224991589784622, -0.24911251664161682, -0.07765816897153854, 0.335429847240448, -0.3832339942455292, 0.04756365343928337, -0.891555905342102, 0.10642705857753754, -0.1302000731229782, -0.45159319043159485, 0.13840782642364502, 0.4351498782634735, 0.7241853475570679, 0.040189892053604126, -0.5246185660362244, 0.8820198774337769, 1.3121464252471924, -1.0087345838546753, -0.24675986170768738, -0.2880909740924835, 1.056246042251587, 0.1997266262769699, -0.6193376183509827, 0.6665361523628235, 0.43744635581970215, 0.2624257504940033, 0.0651823952794075, 0.333476722240448, -0.11672599613666534, -0.36200177669525146, 0.461927592754364, 1.7219797372817993, 0.4670807123184204, 0.00921250693500042, -1.1197881698608398, 0.2461361140012741, -0.9802974462509155, -0.15771779417991638, 0.5057872533798218, 0.6669893860816956, 0.5110321044921875, -0.4788263738155365, -0.5461229681968689, -0.5282834768295288, 0.15882523357868195, 0.36042073369026184, -0.696029782295227, -0.6659910678863525, 0.14135301113128662, 0.3895190954208374, 0.16704431176185608, 0.5042973756790161, -0.47268012166023254, 0.5915377736091614, 14.37601375579834, 1.3449712991714478, 0.24327312409877777, 0.7646490335464478, 0.6772267818450928, 0.4314521253108978, -0.5081380009651184, -0.5252915024757385, -1.6533302068710327, -0.23557010293006897, 1.231872320175171, 0.39888590574264526, 0.7975605130195618, 0.15283304452896118, 0.06410481780767441, 0.02344868890941143, -0.5216870307922363, 0.9067729115486145, 0.7028660178184509, -1.1844964027404785, 0.5577482581138611, -0.24384339153766632, 0.7580312490463257, 0.3891773819923401, 0.7964037656784058, 1.113209843635559, 0.4455346465110779, -0.6460236310958862, 0.07019834220409393, 0.27940833568573, 0.9415724277496338, 0.2659597098827362, 0.18667559325695038, 0.8522558212280273, -0.9337925910949707, -0.22607409954071045, -0.45972201228141785, -0.8295165300369263, 0.17240077257156372, 0.2873912751674652, -0.522264838218689, -0.6309115886688232, -0.4560419023036957, 0.8985069394111633, 0.18892765045166016, 0.37961143255233765, -0.24934637546539307, 0.4730963110923767, -0.39299890398979187, 0.45883575081825256, 0.349602073431015, 0.5079054832458496, 0.20086544752120972, 0.10480708628892899, 0.3145361840724945, -0.027811838313937187, 0.3759530484676361, 0.6857288479804993, -0.4813101589679718, -0.16400831937789917, -0.28822410106658936, -0.3154723048210144, -0.05422533303499222, 0.9913678765296936, 0.15356384217739105, 0.3893567621707916, -0.5756533741950989, 0.24469643831253052, 0.5885360240936279, 0.240857794880867, -0.061369139701128006, 0.37779489159584045, -0.053170375525951385, -0.6207266449928284, -0.07496156543493271, 0.232872873544693, -0.3320586383342743, -0.6485602855682373, -0.9421318769454956, -1.084471583366394, 0.28082239627838135, -0.45561012625694275, -0.9870567917823792, 0.44322970509529114, -0.06397371739149094, -0.2956703007221222, 0.2078220695257187, -0.9111291170120239, -0.3428785800933838, 0.7237111330032349, -1.2743905782699585, -0.682891309261322, 0.38180384039878845, -0.411043256521225, -0.08741701394319534, -0.6190654635429382, 1.436828851699829, -0.06605570018291473, -0.33506909012794495, 0.07563931494951248, 0.0007837137090973556, -0.41542676091194153, -0.1618783324956894, -0.3769175112247467, 0.91566401720047, 0.08248759061098099, -0.18416449427604675, 0.2379612922668457, 0.05615440383553505, -0.08977210521697998, -0.7727423310279846, -0.0022748333867639303, 0.8623270988464355, -0.6064028739929199, -0.2017679363489151, -0.6165947914123535, -1.0355778932571411, 0.24313139915466309, 0.27678143978118896, -0.19713550806045532, 0.6631496548652649, -0.017771631479263306, -0.9214861392974854, 0.4334520995616913, -0.8426356911659241, -0.1606174260377884, 0.46111902594566345, -0.7817647457122803, -0.26657429337501526, 0.3560923933982849, 0.2693507671356201, -1.3867735862731934, -0.6401246190071106, -0.36405959725379944, 0.06287454813718796, 0.030962757766246796, 1.1062026023864746, -0.2037399858236313, 0.5533084273338318, 1.0087213516235352, 0.014366650022566319, -0.9722713828086853, 0.0400875024497509, -0.6489032506942749, -0.2623768448829651, -0.5986599326133728, 0.7981122732162476, -0.5168928503990173, 0.072174571454525, 0.8346094489097595, 0.02134675532579422, -0.22501493990421295, -0.4789905548095703, -0.20882388949394226, -0.061834245920181274, -0.3272000551223755, 0.10913974046707153, -0.10782306641340256, -0.4289565682411194, 0.15216386318206787, 0.5905986428260803, 0.7378686666488647, -0.34200555086135864, -0.8380994200706482, 0.2283596396446228, -0.17751801013946533, -0.5228293538093567, -0.5905759930610657, 0.20261222124099731, -1.4181464910507202, 0.21769385039806366, -1.2375195026397705, 0.01150550041347742, -0.678309440612793, -0.4387897849082947, 0.006250996608287096, 0.06245524808764458, 0.2715636193752289, -0.01555420458316803, -0.4734388589859009, -0.6506565809249878, -0.17498479783535004, -0.5095604062080383, 0.6879172325134277, 1.0271941423416138, -0.630556046962738, -0.1285601407289505, -0.10976802557706833, 0.152858167886734, 0.38666778802871704, 0.7327854037284851, -0.2593281865119934, -0.9102784395217896, -1.4372048377990723, 0.7298896908760071, -0.17474280297756195, -0.29266390204429626, -0.8366971015930176, 0.9493297338485718, 0.4936860501766205, -0.507358193397522, 0.4422640800476074, 0.14717258512973785, -1.0691193342208862, -0.532638430595398, 0.5249032974243164, -0.5683935880661011, 0.17597325146198273, 0.7401866316795349, -0.47272220253944397, -0.368512362241745, 0.23904423415660858, -0.20512092113494873, -1.1889017820358276, -0.9812200665473938, 0.5947272181510925, -0.4366612136363983, 0.4099714756011963, -0.3136183023452759, 0.09257347881793976, -1.227752447128296, -0.3333483636379242, -0.05173246189951897, 0.6570113897323608, -0.5514029860496521, 0.5174664258956909, 0.4212166965007782, -0.9013769030570984, 0.04770217463374138, 0.7390657663345337, 0.048068027943372726, 0.010354029946029186, 0.8226675391197205, 0.575040340423584, -0.43909892439842224, 0.9102379083633423, 0.2121935337781906, 0.3637070655822754, -0.5728132724761963, -0.405985951423645, 0.868855357170105, -0.8870828151702881, 0.022160740569233894, 1.1402828693389893, -0.22659483551979065, -1.4639736413955688, 0.1273816078901291, -1.1936675310134888, -0.7789856791496277, -0.2433464676141739, 0.6467715501785278, -0.02631387673318386, 0.06072348356246948, -0.08752907812595367, -0.2378842681646347, 0.27738121151924133, -0.1605965793132782, -0.2223903387784958, 0.463286817073822, 0.05588006600737572, -0.37423187494277954, 0.33597245812416077, 0.6248440742492676, -0.8974831104278564, -0.7984156012535095, -0.7969808578491211, -0.569818377494812, 0.06891466677188873, 0.2993369996547699, -0.38978153467178345, -0.7975752949714661, 0.689386785030365, 0.2640777826309204, -0.022968122735619545, 0.052932146936655045, -0.4751361310482025, 0.1915542334318161, 0.808702290058136, 0.1266777217388153, -0.545100748538971, -0.5548834204673767, 1.7966707944869995, 1.2590992450714111, -1.4592119455337524, -0.1022147536277771, -0.5419319868087769, -0.7535046935081482, 0.8992579579353333, 0.4353693425655365, -0.04117845371365547, 0.7983726859092712, -0.14346368610858917, -0.5232521891593933, 0.06350903958082199, -0.8853384256362915, -0.18448078632354736, 1.2886608839035034, 1.0827834606170654, 0.9393364787101746, 0.25456997752189636, -0.061407171189785004, 0.9419121146202087, 0.011052907444536686, 0.4251265823841095, 0.11098728328943253, 0.22052480280399323, -0.594373881816864, 0.11076269298791885, -0.12034834176301956, 1.230994462966919, -0.41797924041748047, -1.0654046535491943, 0.6204534769058228, 0.38050541281700134, 0.3631504476070404, 0.3626905679702759, 0.6423376202583313, 0.4203417897224426, 0.4825933575630188, 0.3969469666481018, 0.3482246398925781, -1.0220634937286377, -0.3946956992149353, -0.29288387298583984, -0.6539258360862732, -0.2153128832578659, -0.16146256029605865, -0.08117759972810745, -0.3013805150985718, -0.021989833563566208, 0.7431160807609558, -0.19757048785686493, 0.37365803122520447, 1.2820003032684326, 0.742061972618103, 0.48414307832717896, -0.7220276594161987, -0.7626028060913086, -0.47082409262657166, -1.0397199392318726, 0.15177811682224274, -0.7799514532089233, -0.7372462749481201, -0.25292882323265076, -0.3848741054534912, -0.2382994145154953]}, "authors": [{"authorId": "2275801674", "name": "Haoyuan Wu"}, {"authorId": "67219756", "name": "Haisheng Zheng"}, {"authorId": "2278380755", "name": "Bei Yu"}], "references": [{"paperId": "411114f989a3d1083d90afd265103132fee94ebe", "title": "Mixtral of Experts"}, {"paperId": "2d4a853affeb0b164fc1134df612aea658f36459", "title": "Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning"}, {"paperId": "2edccb8fa562ed52cd49ea6fc67ed32db6218247", "title": "From Sparse to Soft Mixtures of Experts"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "dbfd154190667087ed1cd6c7f75a81858c2f397e", "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e", "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "c9550f0d1940ee1adf1549c9a0d699ef896dbefd", "title": "StableMoE: Stable Routing Strategy for Mixture of Experts"}, {"paperId": "8c62277dada489904a63de4dd87336c27c68fb5e", "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "54a14855b0417e5ce44c9842ae5790a260147caa", "title": "Speeding up Deep Model Training by Sharing Weights and Then Unsharing"}, {"paperId": "24e775b20adf21e9b5b95c6a9b7a5c164d055849", "title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "061f90dff7b2b42e6ba75ec3fd86e9ce7fd18e8d", "title": "Universal Approximation with Deep Narrow Networks"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "7ad66cba3b7e3abae7ef33122588512a146f7f77", "title": "A Survey on Multi-Task Learning"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "16cb6876666f3a7b56a636c1d85ad00bd0d98bf3", "title": "Net2Net: Accelerating Learning via Knowledge Transfer"}, {"paperId": "990504fc9e5a1f3619ace4fa7f5bf667069018b1", "title": "Original Contribution: Multilayer feedforward networks with a nonpolynomial activation function can approximate any function"}, {"paperId": "386cbc45ceb59a7abb844b5078e5c944f17723b4", "title": "On the approximate realization of continuous mappings by neural networks"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Metamath: Boot-strap your own mathematical questions for large language models"}, {"paperId": null, "title": "Magi-coder: Source code is all you need"}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "Suschat"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "mistral.ai/news/ announcing-mistral-7b"}, {"paperId": null, "title": "OpenCompass"}, {"paperId": null, "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks"}, {"paperId": null, "title": "Claude2"}, {"paperId": null, "title": "SUSTech-IDEA"}, {"paperId": null, "title": "\u201dTeknium\u201d"}]}