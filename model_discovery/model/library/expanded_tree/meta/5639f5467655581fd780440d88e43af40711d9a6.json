{"paperId": "5639f5467655581fd780440d88e43af40711d9a6", "abstract": "Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.01507", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work pushes optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments, and identifies a zero point problem of quantizing the second moment, and solves this problem with a linear quantizer that excludes the zero point."}, "embedding": {"model": "specter_v2", "vector": [0.01619507186114788, 0.48749396204948425, -1.0719044208526611, -0.25597983598709106, -0.3272320032119751, 0.2947721481323242, 0.22509463131427765, -0.1285145878791809, -0.6574040055274963, -0.6116302013397217, 0.43352314829826355, -0.22059978544712067, 0.7244716882705688, 0.11686517298221588, -0.2546881139278412, 0.20071613788604736, -0.9281460642814636, -0.12087228894233704, -0.1745215356349945, -0.15738706290721893, 0.3498256504535675, -0.23083573579788208, -1.1052329540252686, 0.4330030381679535, 0.49595797061920166, 1.142137050628662, -0.2825499475002289, 0.8126658201217651, -0.8266786336898804, 0.3953334093093872, 0.32161468267440796, -0.32556042075157166, 0.19139453768730164, 0.2876846492290497, -0.2584266662597656, -0.2144135683774948, 0.5032327771186829, -0.6590415835380554, -0.3495140075683594, 0.9793049693107605, -0.3131945729255676, 0.3405719995498657, 0.2807680666446686, -0.832874059677124, -0.17173492908477783, 0.590203583240509, 0.29749009013175964, 0.6852794289588928, -0.7933452725410461, -0.19856873154640198, 1.1645363569259644, -1.1315743923187256, -0.00906229019165039, 1.1839203834533691, 0.48147639632225037, 0.42424607276916504, -0.24788585305213928, -0.5137422680854797, 0.4165406823158264, 0.13480877876281738, -0.6883319616317749, -0.7790407538414001, -0.2842303216457367, -0.043310146778821945, 2.187727212905884, -0.3559129536151886, -0.14957427978515625, 0.18175555765628815, 0.7163702845573425, 1.2668355703353882, 0.1443149745464325, -0.7279196381568909, 0.09455204010009766, -0.14415699243545532, 0.5576961636543274, 1.0988248586654663, -0.25161421298980713, 0.29521501064300537, -1.398308277130127, -0.0305875726044178, 0.231954887509346, 0.12279698997735977, 0.14730162918567657, -0.36018112301826477, -0.16250379383563995, 0.626504123210907, 0.43020349740982056, 0.5105441212654114, -0.15999816358089447, 1.038669228553772, 0.511166512966156, 0.1808963268995285, 0.05902109667658806, 0.1620047241449356, -0.3268708288669586, -0.16905754804611206, -1.2891525030136108, -0.22990857064723969, 0.020009120926260948, 0.9362754225730896, 0.03150990232825279, 0.4661129415035248, -0.9614725112915039, 0.02832426317036152, 1.08354914188385, 0.09720640629529953, 0.5008684396743774, -0.5719391703605652, 0.4698457717895508, -0.6041504144668579, -0.025521762669086456, -0.4367668330669403, -0.15758417546749115, -0.5556046366691589, -0.7697705626487732, -1.1284242868423462, -0.6400591135025024, -0.18686887621879578, -1.1748605966567993, 0.51570063829422, -0.5467310547828674, 0.3733743727207184, -0.015075409784913063, 0.4128676950931549, 0.20012713968753815, 0.8640821576118469, 0.06189367175102234, 0.04551806300878525, 0.6315059661865234, -1.072991967201233, -0.596989095211029, -1.0000916719436646, 0.9169088006019592, -0.3606990873813629, 0.2115897536277771, -0.08302092552185059, -1.3230998516082764, -0.9967605471611023, -1.076185941696167, -0.14368866384029388, -0.5040420293807983, 0.19021911919116974, 1.1316744089126587, 0.6152094006538391, -1.0537306070327759, 1.1262009143829346, -0.7784224152565002, 0.2792498469352722, 0.4839150011539459, 0.5595672726631165, 0.4956640899181366, -0.11742260307073593, -0.9494537711143494, 0.1265900582075119, 0.2723348140716553, -0.6795005202293396, 0.04815471172332764, -0.8313693404197693, -1.0686935186386108, 0.5010851621627808, 0.12716557085514069, -0.2891225516796112, 1.084552526473999, 0.25076019763946533, -1.4970650672912598, 0.2390236258506775, -0.5543961524963379, -0.13846878707408905, -0.15978407859802246, -0.1456129252910614, -0.42110034823417664, -0.08349470049142838, -0.44747501611709595, 0.7147502899169922, 0.6577179431915283, -0.16722223162651062, 0.05303334817290306, 0.2438083291053772, -0.5324538946151733, -0.19558724761009216, -0.4043758809566498, 0.6753958463668823, -0.46887674927711487, -0.15588386356830597, 0.5491686463356018, 0.4975124001502991, -0.06910520792007446, -0.4860878884792328, -0.5589028000831604, -0.6736633777618408, 0.47738897800445557, 0.20694735646247864, 1.1497836112976074, -0.9435759782791138, -1.0134022235870361, 0.12756110727787018, -0.2265366017818451, -0.16890986263751984, -0.6933940052986145, 0.2465209811925888, -0.6756698489189148, 0.69259113073349, 0.0016685465816408396, -1.250763177871704, 0.34019938111305237, -0.28720176219940186, -1.0190200805664062, -0.22133910655975342, 0.3127494752407074, 1.1287875175476074, -0.3833097517490387, 0.06944205611944199, -0.23778507113456726, 0.29973867535591125, -1.3841664791107178, 0.881049394607544, -0.14320479333400726, -0.17466996610164642, 0.19580596685409546, -0.05077563226222992, 0.039589785039424896, -0.5747714638710022, 0.23072299361228943, -0.8721809983253479, -0.2222839891910553, 0.5097154974937439, -0.41531723737716675, 1.2008368968963623, -0.40360960364341736, 0.5595813393592834, -0.36971452832221985, -0.4092741310596466, 0.008845379576086998, 0.17949886620044708, 0.14571426808834076, -0.36544203758239746, 0.496620237827301, 0.23960664868354797, -0.5464790463447571, 0.49888238310813904, 1.0584852695465088, 0.7258265018463135, -0.33284246921539307, 0.3893202543258667, 0.7830219864845276, -0.5770577788352966, 0.40818947553634644, 0.24037516117095947, 0.5055845379829407, 0.0016436637379229069, 0.2750121057033539, 0.006110022310167551, -0.08064479380846024, -0.9399824738502502, 0.0339072160422802, 0.5306615829467773, 0.4037618041038513, 0.795100212097168, 0.7061886191368103, -0.6558179259300232, -0.5934789776802063, 0.10703083127737045, 0.5492960214614868, 1.4026131629943848, -0.4155503213405609, -0.34616580605506897, -0.9294384121894836, -0.18945391476154327, -0.5331543684005737, 0.2424153834581375, -0.13646934926509857, -0.38000187277793884, -0.42973682284355164, -1.0634703636169434, 0.8061261177062988, 0.2744569480419159, 1.2445148229599, -0.1441352367401123, -0.27776071429252625, -0.7030344605445862, 0.34296682476997375, -0.8797504901885986, -0.7352766990661621, 0.6898345947265625, -1.031383991241455, 0.2404673993587494, 0.35535386204719543, 0.15392369031906128, 0.18068593740463257, -0.6247568726539612, 0.8961396813392639, -0.08448722213506699, -0.014917410910129547, -0.015495277009904385, 0.6677034497261047, -0.5930011868476868, -0.57991623878479, 0.45974019169807434, 0.14333350956439972, -0.24755053222179413, 0.4322139620780945, 0.4665178060531616, 0.03391702473163605, -0.15286754071712494, -0.5090036392211914, 0.05394984409213066, 0.4055423438549042, -0.06005847081542015, 0.817433774471283, -0.28759828209877014, -0.19303728640079498, -0.8333202004432678, 0.8497909903526306, 0.28650417923927307, -0.3632523715496063, 0.32577258348464966, -0.8933982253074646, 0.2346416413784027, 0.5291579365730286, -0.43455472588539124, 0.039477162063121796, -0.8844310641288757, 0.14555205404758453, -0.5862885117530823, -0.09441614151000977, -0.03411836177110672, 0.5438143610954285, -0.1546524465084076, 0.2853899896144867, 0.33705857396125793, 0.15223079919815063, 0.15146175026893616, 0.48579737544059753, -0.6414933800697327, 0.3067188560962677, 0.07151275873184204, -0.09185948222875595, -0.10074729472398758, -0.028533460572361946, -0.4382553696632385, -0.4447116255760193, -0.38089874386787415, -0.24441951513290405, -0.08407296240329742, 0.3550514876842499, -0.337962806224823, -0.7660359144210815, -0.37577638030052185, -1.0888365507125854, -0.18103203177452087, 0.028213011100888252, -0.12350670993328094, -0.5335913896560669, -1.0943243503570557, -1.467759609222412, -0.3121441602706909, -0.750390350818634, -1.5879377126693726, 0.5871492028236389, 0.013451403006911278, -0.30049243569374084, -0.07096900790929794, -0.5734224915504456, -0.4809083342552185, 0.936132550239563, -0.6901416778564453, 1.0813246965408325, 0.08659835904836655, -0.47208404541015625, 0.006711638532578945, 0.10378290712833405, 0.5871407985687256, -0.45230206847190857, 0.42009371519088745, -0.8063771724700928, 0.2011415958404541, -0.08143004029989243, -0.2984895408153534, 0.33965250849723816, 0.2108524888753891, 0.7322803735733032, -0.27361831068992615, -0.284360408782959, 0.8028708100318909, 1.294368028640747, -0.7000214457511902, 0.2576698660850525, -0.029027337208390236, 0.8767656087875366, -0.15920673310756683, -0.48856672644615173, 0.5520830154418945, -0.22109085321426392, 0.14377743005752563, 0.10742415487766266, 0.28773000836372375, -0.09881173074245453, -0.24057208001613617, 0.6805784106254578, 1.723085641860962, 0.5813676118850708, 0.2746618986129761, -1.0205997228622437, 0.3982987701892853, -0.7395065426826477, -0.438069224357605, 0.6365460753440857, 0.7157700061798096, 0.4620967209339142, -0.04139986261725426, -0.6337692141532898, -0.1424066126346588, 0.28240498900413513, 0.7419813871383667, -0.511090874671936, -1.1974178552627563, 0.22680264711380005, 0.690376877784729, 1.062879204750061, 0.6062922477722168, -0.08147347718477249, 0.5686797499656677, 14.979056358337402, 0.9467836022377014, -0.5141093730926514, 0.3802017271518707, 0.9639982581138611, 0.0018277456983923912, -0.13446493446826935, -0.14566472172737122, -1.1945713758468628, 0.036397673189640045, 1.502058506011963, 0.6715127825737, 0.5778587460517883, 0.2630045413970947, 0.10284973680973053, 0.0703938752412796, -0.531417727470398, 0.953533947467804, 0.42503654956817627, -1.6343803405761719, 0.4448292553424835, 0.05998412147164345, 0.8654889464378357, 0.6383602619171143, 1.099931240081787, 0.6233395338058472, 0.11736870557069778, -0.5364474058151245, 0.4543793201446533, 0.48559823632240295, 1.113409161567688, -0.5629889369010925, 0.25265324115753174, 0.47749194502830505, -0.6004787087440491, -0.03741214796900749, -0.40492144227027893, -0.8571218252182007, -0.010053153149783611, 0.283676415681839, -0.5911414623260498, -0.5177860260009766, -0.24351000785827637, 0.33992865681648254, -0.058150604367256165, 0.29622212052345276, 0.0391073115170002, 0.7149865627288818, -0.189377561211586, -0.05131494253873825, 0.49135035276412964, 0.4046006202697754, -0.0007180030224844813, -0.029976729303598404, 0.17924004793167114, -0.12690956890583038, 0.11203918606042862, 0.3773294687271118, -0.6067848205566406, -0.21614204347133636, 0.08359316736459732, -0.344708651304245, -0.40500307083129883, 0.5514165163040161, 0.23416022956371307, 0.11810694634914398, -0.3083842992782593, 0.361233115196228, 0.6044522523880005, 0.17729875445365906, -0.3370119035243988, -0.08887282013893127, 0.3416934609413147, -0.6769182682037354, 0.045171841979026794, 0.04043528065085411, -0.7136666178703308, -0.7837139964103699, -0.7161260843276978, -0.6267986297607422, 0.13188610970973969, -0.836356520652771, -0.09158077090978622, 0.4689004719257355, -0.20804208517074585, -0.09172888845205307, 0.29413294792175293, -1.0168678760528564, -0.10423919558525085, 0.32461443543434143, -1.245674729347229, 0.01085646077990532, 0.533624529838562, -0.5835338234901428, -0.39681777358055115, 0.03268361836671829, 1.1933125257492065, 0.1393885463476181, -0.4992264211177826, 0.15346185863018036, 0.14258739352226257, 0.006966329179704189, -0.2496633231639862, -0.7360380291938782, 1.0460418462753296, 0.35464656352996826, 0.036976512521505356, 0.731861412525177, -0.11432774364948273, 0.20511163771152496, -0.9023419618606567, -0.020209617912769318, 0.4579763412475586, -0.5474516153335571, -0.358367919921875, -0.5241746306419373, -0.6236744523048401, 0.2936839163303375, 0.5338717103004456, -0.29161858558654785, 0.5079407691955566, 0.11781883984804153, -0.35530373454093933, -0.20090952515602112, -0.5664373636245728, 0.578125536441803, 0.3007708489894867, -0.832754373550415, 0.008974667638540268, -0.08214939385652542, 0.33493074774742126, -1.1455748081207275, -0.7082061767578125, 0.03030030056834221, -0.03028026781976223, -0.509745717048645, 0.6613368988037109, -0.6004331707954407, 0.999191403388977, 0.9337875843048096, -0.10888976603746414, -0.7780806422233582, 0.30045753717422485, -0.8819491863250732, -0.10906396806240082, -0.23383569717407227, 0.741416335105896, -0.32192322611808777, 0.7426669597625732, 1.1386184692382812, 0.07186353206634521, -0.7461420297622681, -0.6998867392539978, -0.05189794301986694, -0.11745772510766983, -0.7384458184242249, 0.5179790258407593, -0.2102295607328415, -0.14954891800880432, 0.04467855766415596, 0.5959662199020386, 0.5325426459312439, -0.4088408648967743, -0.5973338484764099, 0.15342582762241364, 0.4664642810821533, -0.2371153086423874, -0.7139185667037964, -0.6219159364700317, -1.7621275186538696, 0.04415376856923103, -1.1622421741485596, -0.034386299550533295, -0.6661235094070435, -0.5253051519393921, 0.14068688452243805, -0.3625774383544922, 0.14530959725379944, 0.4324968159198761, -0.1159224584698677, -0.4585002660751343, -0.3340511918067932, -0.644794762134552, 0.7655530571937561, 0.761544406414032, -0.7552877068519592, 0.14302057027816772, -0.25264373421669006, 0.20724473893642426, 0.253252774477005, 0.591508686542511, -0.31600767374038696, -0.7812697291374207, -1.3404295444488525, 0.350964218378067, -0.07820582389831543, -0.3042147755622864, -0.8781999945640564, 0.7849711775779724, 0.26675665378570557, -0.023748096078634262, -0.22119244933128357, 0.26150083541870117, -0.8395768404006958, -0.6023030281066895, 0.4396383464336395, -0.7996810078620911, 0.2494916021823883, 0.5488821864128113, -0.6721777319908142, -0.3073488771915436, 0.26029813289642334, 0.11606789380311966, -0.6411255598068237, -0.7021406292915344, 0.5157106518745422, -0.0904928520321846, 0.04061088711023331, -0.45775508880615234, -0.07938465476036072, -1.2071077823638916, -0.35348600149154663, -0.06252378970384598, 0.13246701657772064, -0.0668652281165123, 0.5451385378837585, 0.4124704599380493, -0.99913489818573, 0.6209628582000732, 0.5574712753295898, -0.36805450916290283, -0.07379300147294998, 0.3388691842556, 0.5184952616691589, -0.7513154745101929, 0.6072695851325989, 0.3054511249065399, 0.22114723920822144, -0.45757514238357544, -0.04540786147117615, 0.783713698387146, -0.6787979602813721, 0.182265043258667, 0.9715327620506287, -0.6298813223838806, -0.6868926286697388, -0.1881946325302124, -1.6887024641036987, -0.09218381345272064, -0.3613056242465973, 0.4052415192127228, 0.21384097635746002, 0.6299857497215271, 0.09967555850744247, -0.38659608364105225, 0.12729114294052124, -0.1002412736415863, -0.6376838088035583, 0.2338935136795044, 0.11353155970573425, -0.5117500424385071, 0.5873209238052368, 1.3711137771606445, -0.6002750992774963, -0.8130353093147278, -0.509715735912323, -0.2400500476360321, -0.03814741224050522, 0.4659229815006256, -0.013740090653300285, -0.9612827301025391, 1.041490912437439, 0.53519207239151, 0.34696802496910095, 0.21435467898845673, -0.6602410078048706, 0.44713619351387024, 0.817983090877533, 0.16306112706661224, -0.8133707642555237, -0.5706852674484253, 1.5110058784484863, 0.8450705409049988, -0.792797863483429, 0.6641049981117249, -0.1956983059644699, -0.6316068172454834, 0.8452450037002563, 0.3327462375164032, -0.07427600026130676, 1.021576166152954, 0.182712584733963, 0.046803172677755356, 0.13804179430007935, -1.0169079303741455, -0.04279877990484238, 1.073043704032898, 0.6387513875961304, 0.694786548614502, 0.095645010471344, 0.3011651635169983, 0.8635136485099792, -0.15498101711273193, -0.058791615068912506, 0.35379812121391296, 0.5622983574867249, -0.3406866788864136, 0.10818395018577576, -0.14891317486763, 0.957808256149292, -0.9966164827346802, -1.0619573593139648, 0.4347183108329773, 0.5943409204483032, 0.11469457298517227, 0.6001713871955872, 0.9307388663291931, -0.07928300648927689, 0.5548035502433777, 0.4465438425540924, 0.4233464002609253, -0.6997377276420593, -0.3421112895011902, -0.3480139374732971, -0.8094582557678223, -0.4536130428314209, 0.17847919464111328, -0.20423312485218048, -0.7207433581352234, -0.38237231969833374, 0.22889378666877747, 0.05513591691851616, 0.5344662666320801, 0.9708626866340637, 0.9613562226295471, 0.7228197455406189, -0.24145030975341797, -0.5363738536834717, -0.6622981429100037, -0.6378836631774902, 0.23665998876094818, -0.5214313864707947, -0.0752289891242981, 0.10819321125745773, -0.0652308538556099, -0.11601026356220245]}, "authors": [{"authorId": "2238090779", "name": "Bingrui Li"}, {"authorId": "2276707", "name": "Jianfei Chen"}, {"authorId": "145254043", "name": "Jun Zhu"}], "references": [{"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "3f6243097a58e386aea1215fed4f372dee07a100", "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8c62277dada489904a63de4dd87336c27c68fb5e", "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "04a3f5f95c802cb0a55e3e5eca2875fafbc669dd", "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "423c0246d016b092b131132dca8249c3faacd000", "title": "Extreme Tensoring for Low-Memory Preconditioning"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "e5b2843d3fbb3260a3f84872230ed1437552b131", "title": "Memory-Efficient Adaptive Optimization for Large-Scale Learning"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c9d64aaa2007b60ef7814acc895dd90f15578a20", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "510a6ec82a2af8cdee185418c3296cb8e2b9716c", "title": "8-Bit Approximations for Parallelism in Deep Learning"}, {"paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "d0b0c3e5a1e768490bc9b759685930541957508b", "title": "Introductory Lectures on Convex Optimization - A Basic Course"}, {"paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91", "title": "On the importance of initialization and momentum in deep learning"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": "ae9cb0984a176dba838c340ce97daa8a9b21a1e5", "title": "GACT: Activation Compressed Training for General Architectures"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "735d4220d5579cc6afe956d9f6ea501a96ae99e2", "title": "On the momentum term in gradient descent learning algorithms"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}]}