{"paperId": "d5cfc82ac7ad6c5e9c91ef18bba6d2979b632443", "abstract": "Foundation models are in the process of becoming the dominant deep learning technology. Pretraining a foundation model is always time-consuming due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the pretraining process is extremely memory- and communication-intensive. These challenges make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism, and tensor model parallelism, to achieve high training efficiency. However, current 3D parallelism frameworks still encounter two issues: i) they are not transparent to model developers, requiring manual model modification to parallelize training, and ii) their utilization of computation resources, GPU memory, and network bandwidth is insufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automatically deploys 3D parallelism with an automatic model partitioner, which includes a graph-sharding algorithm and proxy node-based model graph. Merak also offers a non-intrusive API to scale out foundation model training with minimal code modification. In addition, we design a high-performance 3D parallel runtime engine that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation. Experiments on 64 GPUs demonstrate Merak's capability to speed up training performance over state-of-the-art 3D parallelism frameworks of models with 1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42, 1.39, 1.43, and 1.61\u00d7, respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "year": 2022, "citationCount": 25, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2206.04959", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Merak is proposed, an automated 3D parallelism deep learning training framework with high resource utilization that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation."}, "embedding": {"model": "specter_v2", "vector": [-0.5152958631515503, 0.07653142511844635, -0.5187243819236755, 0.10776607692241669, 0.3712299168109894, 0.28565436601638794, 0.5465568900108337, -0.6110499501228333, -0.601599395275116, -0.3368687331676483, 0.2530425488948822, -0.28098490834236145, 0.5110403299331665, 0.1244843527674675, 0.01728893257677555, 0.04248021915555, -0.7921071648597717, 0.20949774980545044, 0.5002148747444153, -0.05022742226719856, -0.01876129023730755, 0.2899106442928314, -1.4803606271743774, 0.3621971309185028, 0.057815439999103546, 1.061540961265564, 0.08732996135950089, 1.0769726037979126, -0.4069032371044159, 0.3723871111869812, 0.4063933789730072, 0.02433714270591736, 0.47475719451904297, 0.2329619973897934, -0.4091993272304535, -0.03621016815304756, 0.4126816987991333, -0.8101617097854614, -0.4815926253795624, 0.715522050857544, 0.05103215202689171, 0.13319826126098633, 0.12358036637306213, -1.162764072418213, 0.44221413135528564, 0.033386778086423874, 0.37507158517837524, 1.0923923254013062, -1.1529885530471802, -0.4913938045501709, 0.9852472543716431, -1.4842875003814697, -0.1440614014863968, 1.1057696342468262, 0.7622396945953369, 0.5316222906112671, -0.4705009460449219, -0.4922490119934082, 0.29132696986198425, -0.11992229521274567, -0.4630715847015381, -0.2219763696193695, 0.08165086805820465, -0.6894371509552002, 1.9605178833007812, -0.691315233707428, 0.39801403880119324, 0.32280614972114563, 0.18059001863002777, 1.141912817955017, -0.07542438805103302, -0.5961617231369019, 0.023925835266709328, -0.48024359345436096, 0.5996962189674377, 0.8645229339599609, 0.17855867743492126, 0.6181737780570984, -1.2886204719543457, -0.17967788875102997, 0.9199745655059814, 0.25634369254112244, 0.7085397243499756, -0.14342540502548218, -0.18633253872394562, 0.7506360411643982, 0.5473132729530334, 0.2654874622821808, -0.6566753387451172, 1.0313589572906494, 0.76621413230896, -0.10942845791578293, 0.05353856459259987, -0.1480780690908432, -0.11744072288274765, 0.4656694233417511, -0.9707633256912231, 0.24909453094005585, 0.18448397517204285, 0.46632882952690125, 0.2027920037508011, 0.25452396273612976, -0.30286499857902527, -0.03265838325023651, 1.3754396438598633, -0.17065061628818512, 0.36204901337623596, -0.6616455316543579, 0.0999351218342781, -0.264340341091156, -0.3136831820011139, -0.3994712829589844, -0.44854944944381714, -0.4880898892879486, -1.074015736579895, -0.4293595850467682, -0.7888088822364807, -0.16200388967990875, -0.889359176158905, 0.025619223713874817, 0.0516001433134079, 0.7232953310012817, 0.09457314014434814, 0.6616939306259155, 0.7075251340866089, 0.3675443232059479, 0.40070727467536926, 0.8696838617324829, 0.9898914098739624, -1.6873936653137207, -0.0008612306555733085, -0.6901887059211731, 0.5367206335067749, -0.0437493734061718, -0.13745439052581787, -0.24878792464733124, -1.2896016836166382, -1.4018728733062744, -0.801855206489563, -0.035115331411361694, -0.2377743422985077, -0.10005241632461548, 1.6434299945831299, -0.006552372593432665, -1.145044207572937, 1.161171555519104, -0.5937240719795227, 0.16328701376914978, 0.375409871339798, 0.32091715931892395, 0.4586579501628876, -0.17180953919887543, -1.1094313859939575, -0.16336318850517273, 0.3699599504470825, -0.24775664508342743, -0.6484872102737427, -0.7960959076881409, -0.14879541099071503, -0.0043555921874940395, -0.3442734479904175, -1.1748032569885254, 1.357481598854065, -0.07248973101377487, -0.8653844594955444, 0.7908788919448853, 0.07865802198648453, 0.03601888567209244, 0.1273900866508484, -0.001847594860009849, -0.44890448451042175, 0.002599377417936921, -0.2873283624649048, 0.7532984018325806, 0.6250417232513428, 0.10652041435241699, -0.14657224714756012, -0.03603667393326759, -0.16728778183460236, 0.06560143083333969, -0.6083887219429016, 1.0276826620101929, -0.460318922996521, 0.028909027576446533, 0.5417680740356445, 0.6828774809837341, -0.4425480365753174, 0.019390298053622246, -0.45387372374534607, -0.8495495319366455, 0.7767741084098816, 0.2878241240978241, 0.606789231300354, -1.0212129354476929, -0.8866183757781982, -0.02561286650598049, 0.5018062591552734, -0.11978164315223694, -0.4793460965156555, 0.4866268038749695, -0.5226140022277832, 0.20328053832054138, 0.47886866331100464, -0.9582036733627319, -0.17583297193050385, -0.1965254247188568, -0.5626932382583618, -0.5446428060531616, -0.04946719855070114, 0.820825457572937, -0.6090661883354187, 0.11111890524625778, -0.6513030529022217, 0.5502837300300598, -1.2541723251342773, 1.1101725101470947, -0.5630826950073242, -0.01799253560602665, -0.04739765077829361, -0.29472485184669495, 0.06492079049348831, -0.6015163660049438, 0.3503417670726776, -0.5961384177207947, -0.07626670598983765, 0.6372388601303101, -0.8380681872367859, 1.4578334093093872, -0.1224915012717247, 0.07204069197177887, 0.134735107421875, -0.5244539380073547, 0.44024452567100525, 0.43038448691368103, 0.04398993402719498, -0.22380578517913818, 0.2255694717168808, 0.8041321635246277, -0.3823336958885193, 0.5480783581733704, 1.1541179418563843, 0.7663890719413757, -0.013611421920359135, 0.4450426995754242, 0.42386582493782043, 0.0976288840174675, 0.4357231855392456, 0.300419420003891, 0.5174956917762756, 0.2505469024181366, -0.39077654480934143, -0.467074990272522, 0.09258999675512314, -0.6435586810112, -0.04815046861767769, 0.3220865726470947, 0.40795180201530457, 0.37782520055770874, 0.8288447856903076, -0.951711893081665, -0.6836072206497192, 0.4020574688911438, 0.6622517704963684, 1.448361873626709, -0.2037837654352188, 0.1956998109817505, -0.5777840614318848, -0.6867644190788269, -0.007120923604816198, -0.6352603435516357, 0.09253149479627609, 0.1445782482624054, -0.2838141918182373, -1.24508535861969, 0.9535481333732605, 0.6086187958717346, 1.4894174337387085, -0.5426391959190369, -0.5751060247421265, -0.6290206909179688, 0.8359026908874512, -0.8022244572639465, -0.38114842772483826, 0.6035923957824707, -1.173715591430664, -0.22652943432331085, 0.18504175543785095, -0.17289185523986816, 0.37021857500076294, -0.03972617909312248, 1.2914990186691284, 0.007446094881743193, -0.35439246892929077, 0.029823461547493935, 0.8191804885864258, -0.44100120663642883, -0.3990371823310852, 0.3233073055744171, 0.22716355323791504, -0.46264874935150146, 0.33489811420440674, 0.15584394335746765, -0.4454299807548523, 0.21346339583396912, -0.4337160289287567, 0.5269491076469421, 0.029512809589505196, 0.009944370947778225, 1.0169583559036255, -0.0961533710360527, 0.031796373426914215, -1.0949971675872803, 0.8162555694580078, -0.004569349344819784, -0.6166031360626221, -0.23647883534431458, -0.6531400084495544, -0.13367721438407898, 0.3420744836330414, -0.49481943249702454, -0.07743673026561737, -1.0196422338485718, 0.3817373216152191, -0.9490207433700562, -0.3153587281703949, -0.03677622973918915, 0.6156390905380249, -0.17280547320842743, 0.6840899586677551, -0.010154617950320244, 0.41823694109916687, -0.3213347792625427, 0.3251167833805084, -1.0355439186096191, 0.2574257254600525, -0.269138902425766, -0.10419350862503052, -0.09653893113136292, 0.2969283163547516, -0.5667786598205566, -0.6730252504348755, -0.7197619080543518, -0.6544185280799866, -0.19699923694133759, 0.41301628947257996, -0.7688362002372742, -0.8363021016120911, 0.21563412249088287, -1.2261959314346313, -0.582813024520874, 0.24356557428836823, 0.024223290383815765, 0.202042818069458, -1.271293044090271, -1.5924550294876099, -0.03394573554396629, -1.2194470167160034, -1.2148321866989136, 0.3426482081413269, 0.35495585203170776, 0.19397473335266113, -0.9044590592384338, -0.09537255764007568, -0.6654479503631592, 0.8765150308609009, -0.2254394143819809, 0.8054224848747253, 0.025555532425642014, -0.0440533347427845, 0.10602718591690063, -0.5370253324508667, 0.9340060949325562, -0.9584477543830872, 0.4460037350654602, -0.6876705288887024, 0.5154463648796082, -0.5252846479415894, -0.7627691626548767, 0.5057036876678467, 0.2552846074104309, 0.9070551991462708, 0.5807247757911682, -0.27987492084503174, 0.885945737361908, 1.40390145778656, -1.1479134559631348, 0.0522429421544075, -0.03408289700746536, 1.173814296722412, -0.049538929015398026, -0.9061987996101379, 0.5179752707481384, -0.026997413486242294, 0.2264612466096878, 0.5024396181106567, -0.611431896686554, -0.8125155568122864, -0.3215126693248749, 0.11764290183782578, 1.525169014930725, 0.4427134394645691, 0.5170835256576538, -1.094336748123169, 0.20631043612957, -1.0452221632003784, -0.2920747399330139, 0.3174397945404053, 0.47145745158195496, 0.12279819697141647, -0.17403295636177063, 0.10239189118146896, -0.22594596445560455, 0.5310447812080383, 0.4477261006832123, -0.6883678436279297, -0.9022228717803955, 0.4873408079147339, 0.3585354685783386, 0.43316754698753357, 0.16594502329826355, -0.0055278693325817585, 0.4419921040534973, 14.437776565551758, 0.6002748608589172, -0.30063584446907043, 0.3279018998146057, 0.8940891623497009, 0.5199483036994934, -0.40320590138435364, -0.010900774970650673, -1.8689764738082886, -0.18012452125549316, 1.5125765800476074, 0.3541553318500519, 0.25252315402030945, 0.6595668196678162, -0.026094507426023483, 0.20869436860084534, -0.536415159702301, 0.6563745141029358, 0.23413506150245667, -1.6635526418685913, 0.2727648615837097, 0.3020142614841461, 0.5857504606246948, 1.050408124923706, 0.5226306319236755, 0.689721941947937, 0.39059486985206604, -0.2599673271179199, 0.16248002648353577, 0.13617511093616486, 1.1071408987045288, -0.052598629146814346, 0.43031832575798035, 0.5638092756271362, -0.7318406105041504, 0.3705110549926758, -0.7635583281517029, -1.4278546571731567, -0.13712908327579498, 0.5125650763511658, -0.8065930008888245, -0.38638973236083984, -0.1891034096479416, 0.93157559633255, 0.24111881852149963, 0.5090025663375854, -0.46151548624038696, 0.39470601081848145, -0.1808588206768036, 0.33776333928108215, 0.15353208780288696, 0.5634709000587463, -0.3874795138835907, -0.05125721916556358, -0.19416476786136627, -0.20051413774490356, 0.6059116721153259, 0.3538415729999542, -0.8802247047424316, -0.5528776049613953, 0.15476219356060028, -0.11275576800107956, -0.3892814517021179, 1.2753268480300903, 0.17331969738006592, 0.3937089145183563, -0.5566246509552002, 0.3411097228527069, 0.6407533884048462, -0.05374785512685776, -0.2857208251953125, 0.18783660233020782, 0.48786094784736633, -0.521926760673523, -0.1702132225036621, 0.017400236800312996, -0.6359404921531677, -0.5128445625305176, -1.0485055446624756, -0.16441690921783447, 0.3819258511066437, -0.6635490655899048, -0.8620032072067261, 1.0034993886947632, -0.2812502384185791, -0.13696794211864471, 0.2918493449687958, -1.1971640586853027, -0.59810471534729, 0.895800769329071, -1.7627376317977905, -0.44231608510017395, -0.009895576164126396, 0.2262231707572937, -0.8648638725280762, 0.015635646879673004, 1.4308838844299316, 0.35278984904289246, -0.6184383034706116, -0.3095340430736542, -0.32811856269836426, 0.21157218515872955, -0.5015188455581665, -0.6705513000488281, 1.340722680091858, 0.2678969204425812, -0.04652060568332672, -0.37839794158935547, -0.6360648274421692, 0.18773062527179718, -0.6191529631614685, -0.312966912984848, 0.38954541087150574, -0.24705322086811066, 0.09299453347921371, -0.5421488881111145, -0.65202397108078, 0.22769111394882202, 0.22039781510829926, 0.32124969363212585, 0.6417378783226013, 0.44713494181632996, -0.9613150358200073, -0.23965376615524292, -0.4990641474723816, 0.08444497734308243, 0.39248836040496826, -0.5550511479377747, 0.008606241084635258, 0.17407652735710144, 0.593610405921936, -1.247194766998291, -0.837920606136322, -0.37043076753616333, 0.04558543488383293, -0.4827478528022766, 1.0268406867980957, -0.24849776923656464, 0.9418097734451294, 0.9849341511726379, -0.06793529540300369, -0.4184287190437317, 0.24016514420509338, -0.6988937854766846, -0.11571497470140457, -0.31231990456581116, 0.3000172972679138, -0.315590500831604, 0.9332339763641357, 0.8543813824653625, -0.09494812041521072, -0.5227745175361633, -0.045868463814258575, -0.2329644113779068, -0.11070825904607773, -0.7994494438171387, 0.2824815809726715, -0.2944454252719879, -0.452745258808136, 0.3863414227962494, 0.6732760667800903, 0.540690004825592, 0.14599554240703583, -0.4124004542827606, 0.5514996647834778, -0.43731585144996643, -0.5633776783943176, -0.5555381774902344, -0.5927183628082275, -1.4979596138000488, -0.08451492339372635, -1.466996192932129, -0.47762855887413025, -0.9060631990432739, -0.3400926887989044, -0.34637951850891113, -0.4503050446510315, 0.28747740387916565, 0.4823712706565857, 0.09671434015035629, -0.6951004862785339, 0.002742861397564411, -0.3921424150466919, 0.6341969966888428, 1.2217222452163696, 0.13002197444438934, -0.12312471121549606, -0.25516313314437866, 0.23400627076625824, 0.6150869131088257, 0.5301464200019836, -0.16178233921527863, -0.6512937545776367, -1.4916086196899414, 0.21185031533241272, -0.051308393478393555, 0.3976849317550659, -0.94499272108078, 0.9602494239807129, 0.5578796863555908, -0.03305152803659439, -0.09645827114582062, -0.1009470596909523, -0.7116519212722778, -0.6825661659240723, 0.471378892660141, -0.21611692011356354, 0.395212322473526, 0.7403461337089539, -0.6142908334732056, -0.19595052301883698, 0.6453238129615784, -0.18044701218605042, -0.6861990094184875, -1.4091826677322388, 0.733740508556366, 0.04725661128759384, -0.011763541027903557, -0.4615118205547333, -0.017020108178257942, -1.186731219291687, 0.3374587297439575, -0.0628838762640953, 0.29539740085601807, -0.4701463580131531, 0.28108441829681396, 0.40657326579093933, -1.1692699193954468, 0.14901262521743774, 0.45883774757385254, -0.7715870141983032, 0.5799261927604675, 0.8169254064559937, 0.6756029725074768, -0.7584480047225952, 0.2530294358730316, -0.09733455628156662, 0.18278571963310242, -0.5066136121749878, 0.00856479350477457, 0.9964014291763306, -0.6812933683395386, -0.608296275138855, 1.1661502122879028, -0.37452951073646545, -0.9236363768577576, 0.17333194613456726, -0.9135950207710266, -0.46613577008247375, -0.48762497305870056, 0.7525755763053894, 0.29739707708358765, 0.39307093620300293, 0.41990235447883606, -0.48746755719184875, -0.07387279719114304, -0.24145260453224182, -0.28600722551345825, 0.3189403712749481, 0.4647117555141449, -0.5754180550575256, 0.5197935700416565, 0.7096279859542847, -0.9770980477333069, -1.2557588815689087, -0.7996751666069031, -0.18811486661434174, 0.12384465336799622, 0.6857599020004272, -0.27933627367019653, -0.9524078369140625, 0.9487899541854858, 0.22881218791007996, 0.28887274861335754, 0.3300424814224243, -0.5092561841011047, 0.7686904072761536, 0.3938901424407959, -0.046119581907987595, -0.34889882802963257, -0.04722891375422478, 1.3878437280654907, 0.9602227807044983, -0.5355292558670044, 0.4315309226512909, -0.10384104400873184, -0.6523216962814331, 1.278917670249939, 0.3969700038433075, -0.4691775441169739, 1.153193712234497, -0.09852959960699081, -0.1813383251428604, -0.271898090839386, -1.0251280069351196, -0.19813451170921326, 0.7877123951911926, 0.5808042287826538, 0.6627553701400757, 0.02456020750105381, 0.10899806022644043, 0.8414597511291504, 0.22435498237609863, -0.083550363779068, 0.15889275074005127, 0.30024319887161255, -0.09975884109735489, 0.0019432631088420749, 0.17278996109962463, 0.4138640761375427, -0.5584602355957031, -0.7594378590583801, 0.46371182799339294, 0.7650930285453796, 0.20378783345222473, 0.44379088282585144, 1.0542848110198975, -0.12397672981023788, 0.6482216119766235, 0.136554554104805, 0.3184134066104889, -0.7328623533248901, -0.46235543489456177, 0.06405727565288544, -0.682905375957489, -0.3990703821182251, -0.3054788112640381, -0.05539923533797264, -1.0229952335357666, -0.743115246295929, 0.8256744742393494, -0.024959521368145943, 0.8209977149963379, 0.6136403679847717, 1.046431303024292, 1.0358145236968994, 0.045498643070459366, -0.9136549830436707, -0.5766053795814514, -0.4820249080657959, -0.257897287607193, -0.6841913461685181, -0.6450878977775574, -0.10164535790681839, -0.3504384458065033, -0.511803388595581]}, "authors": [{"authorId": "3050140", "name": "Zhiquan Lai"}, {"authorId": "2153699246", "name": "Shengwei Li"}, {"authorId": "2109927338", "name": "Xudong Tang"}, {"authorId": "84577042", "name": "Ke-shi Ge"}, {"authorId": "46642223", "name": "Weijie Liu"}, {"authorId": "2151245013", "name": "Yabo Duan"}, {"authorId": "2570205", "name": "Linbo Qiao"}, {"authorId": "2135905059", "name": "Dongsheng Li"}], "references": [{"paperId": "114cc72d93c73b97ba03ed8c4e4a8d937b344607", "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models"}, {"paperId": "baa467a4dccf87bc7e2c5a4ea6fd5e401d962d39", "title": "AutoPipe: A Fast Pipeline Parallelism Approach with Balanced Partitioning and Micro-batch Slicing"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "5a38d67cd2f3881fa3b80f2099460ecf06ba7005", "title": "torch.fx: Practical Program Capture and Transformation for Deep Learning in Python"}, {"paperId": "355fe8111c250dee86a7d6a5cd818b6f61e41279", "title": "End-to-end Adaptive Distributed Training on PaddlePaddle"}, {"paperId": "ce8f2077fae890cc1967d1a9aa5c544127f1ca47", "title": "Amazon SageMaker Model Parallelism: A General and Flexible Framework for Large Model Training"}, {"paperId": "43332a71939ae7f3bd4756cba2c5ef0763b5cfac", "title": "Varuna: scalable, low-cost training of massive deep learning models"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "3186b9dd1331b647cf3304d185c248ea7ec9ad1b", "title": "OneFlow: Redesign the Distributed Deep Learning Framework from Scratch"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "20e9bee9e0a59afb5ba6543eb93c64c3db3076c0", "title": "Hippie: A Data-Paralleled Pipeline Approach to Improve Memory-Efficiency and Scalability for Large DNN Training"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "bf38bdd65710ed4e3831c750275f5843058250bc", "title": "BAGUA: Scaling up Distributed Learning with System Relaxations"}, {"paperId": "d8df456f790381f4ddb388be24a546625bd75ee2", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "ceecad72381aca9dae1ab24c46f014301dfe8b44", "title": "Whale: Efficient Giant Model Training over Heterogeneous GPUs"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d", "title": "DAPPLE: a pipelined data parallel approach for training large models"}, {"paperId": "c75390f8138e2a422956d0e1b00cb6a39579bc95", "title": "PyTorch distributed"}, {"paperId": "3c55dd7b8da5c7b47e91b2e749c264f50d007cd4", "title": "Dynamic Tensor Rematerialization"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "9d9dbb4487aca2b62ca3659446d7010ac65aa642", "title": "HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "408d0580d7e2befabf542119d7fc8318c684572b", "title": "Pipelined Backpropagation at Scale: Training Large Models without Batches"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "1340978c92e7cbcf9abe87888150c60984e2964b", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "fd431005d26100f5453590080683cbae9dc1189f", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "686366ca6fdbff6d97648daacfc7f1e5fea97f70", "title": "Pipe-Torch: Pipeline-Based Distributed Deep Learning in a GPU Cluster with Heterogeneous Networking"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "e2c8726d092aea573e69f5b0a2654225883cfacf", "title": "Horovod: fast and easy distributed deep learning in TensorFlow"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "6f4e48c2a5de9337d147ebbb7d0ff0e555adceca", "title": "Bandwidth optimal all-reduce algorithms for clusters of workstations"}, {"paperId": "5861636a1c3c0e9ccf6f683fe70b3b5182ef1697", "title": "Whale: Scaling Deep Learning Model Training to the Trillions"}, {"paperId": "d5f8fc9b0db83d0c11c9534525d0c56e609f1068", "title": "2.5-dimensional Distributed Model Training"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "33ac6c15883552e54d68d7cef87b51c97f91b0a2", "title": "the 2021 USENIX"}, {"paperId": "053c168f94acd75880acdacb846bc3f416e5c07c", "title": "PatrickStar: Parallel Training of Pre-trained Models via a Chunk-based Memory Management"}, {"paperId": "7039f7f662a55b49288a7db11e07a9cf4e465a8b", "title": "PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management"}, {"paperId": null, "title": "Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation"}, {"paperId": null, "title": "Deepspeed: Extreme-scale model training for everyone"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "NVIDIA collective communications library (NCCL)"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "Merak: A Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation"}, {"paperId": null, "title": "A Survey on Auto - Parallelism of Neural Networks Training , \u201d 4 2022 . [ Online ]"}]}