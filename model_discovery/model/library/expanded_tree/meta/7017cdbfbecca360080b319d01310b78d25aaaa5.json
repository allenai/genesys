{"paperId": "7017cdbfbecca360080b319d01310b78d25aaaa5", "abstract": "The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models, while powerful, require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers. This paper introduces a novel Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference. We empirically demonstrate that implementing SA across various LLMs results in minimal accuracy loss on standard benchmarks. Our findings suggest that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel Shared Attention mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers, which utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference."}, "embedding": {"model": "specter_v2", "vector": [0.16430573165416718, 0.6861829161643982, -0.5596160888671875, -0.2347724884748459, -0.6035303473472595, 0.3025762736797333, 0.3596491515636444, -0.055974606424570084, -0.8734434843063354, -0.36446380615234375, 0.4364527761936188, 0.36347147822380066, 0.2280619591474533, 0.43548583984375, -0.26000529527664185, 0.1879224330186844, -0.9768645167350769, 0.20667067170143127, 0.31577181816101074, 0.10638665407896042, -0.1982012838125229, -0.5679604411125183, -1.260009527206421, 0.11722970753908157, 0.4507509171962738, 0.8345543146133423, 0.7195432186126709, 1.0481475591659546, -0.11697232723236084, 0.40892404317855835, 0.6959794759750366, -0.2049429714679718, -0.0614616833627224, -0.142970472574234, -0.3522725999355316, -0.6026943922042847, 0.68724524974823, -0.49442926049232483, -0.30630576610565186, 0.8882227540016174, -0.2941643297672272, 0.6684855222702026, 0.428640753030777, -0.852765679359436, -0.20682118833065033, 0.9798524975776672, 0.4871932864189148, 0.910690188407898, -0.7207266092300415, -0.6757309436798096, 1.4763990640640259, -1.5559165477752686, 0.08132561296224594, 1.6296041011810303, 0.5909112095832825, 0.19827820360660553, -0.3014487624168396, -0.5768930912017822, 1.267657995223999, 0.11097284406423569, -1.1340978145599365, -0.5969181060791016, -0.14443466067314148, 0.27601924538612366, 2.2634429931640625, -0.181235209107399, 0.19448158144950867, 0.18716199696063995, 0.3706129491329193, 1.9024465084075928, -0.38137033581733704, -0.6657801270484924, -0.31345444917678833, 0.3459572196006775, 0.6246957778930664, 0.8595042824745178, -0.3905564844608307, -0.04154348745942116, -0.9706022143363953, -0.5165504813194275, 0.21451984345912933, -0.3267107903957367, 0.14052186906337738, -0.22590923309326172, -0.14725662767887115, 0.866100013256073, 0.657857358455658, 0.5190042853355408, -0.4849318563938141, 0.6831554174423218, 0.2491372972726822, 0.03598591685295105, 0.5253041982650757, 0.25318068265914917, -0.19254732131958008, 0.5198159217834473, -1.1524890661239624, 0.1747640073299408, -0.16005387902259827, 0.8137409090995789, -0.3147091269493103, 0.3365888297557831, -0.5228721499443054, 0.3535498082637787, 1.7361201047897339, 0.23495428264141083, 0.5061154365539551, -0.5288453698158264, 0.17948180437088013, -0.8495331406593323, -0.20326420664787292, -0.9424059391021729, -0.3122499883174896, -0.33948826789855957, -0.6937456130981445, -1.3904473781585693, -0.6444306373596191, 0.38165873289108276, -0.2751238942146301, 1.019635558128357, 0.01533442735671997, 0.4331037104129791, 0.06906099617481232, 0.27853628993034363, 0.3522464632987976, 0.6321528553962708, 0.7063047289848328, 0.214087575674057, 0.8271191120147705, -0.9370896220207214, -0.6292087435722351, -1.4431774616241455, 0.5676848888397217, -0.13508781790733337, 0.09882587939500809, -0.4417792558670044, -1.3400943279266357, -1.0419282913208008, -0.6576114892959595, -0.3025760352611542, -0.3903559148311615, 0.2719303369522095, 1.2881842851638794, -0.302019864320755, -1.0810385942459106, 0.8808110952377319, -0.3350180387496948, 0.13146832585334778, 0.6819137334823608, 0.48127293586730957, 0.4316147565841675, -0.12402839213609695, -1.1928592920303345, 0.3405584692955017, 0.3861533999443054, -0.18928085267543793, -0.04610324651002884, -0.6803041696548462, -1.2670809030532837, 0.2791554629802704, 0.27725714445114136, -0.37398630380630493, 1.3254327774047852, -0.41775500774383545, -1.1521748304367065, 0.46491819620132446, -0.4255661368370056, 0.2819032669067383, -0.10848427563905716, -0.4451301097869873, -0.291467547416687, -0.22084413468837738, -0.5214987993240356, 0.9737641215324402, 0.755029559135437, 0.049500517547130585, -0.3862949311733246, -0.03208189830183983, -0.5204004049301147, 0.0037354708183556795, -0.5623220801353455, 0.8863028883934021, -0.616696298122406, -0.19234247505664825, 0.45632368326187134, 0.40217703580856323, -0.03503529727458954, -0.013468941673636436, 0.01872069761157036, -1.2356120347976685, 0.44362109899520874, -0.2987050414085388, 0.9352448582649231, -0.7569838762283325, -0.504368007183075, 0.03868916630744934, -0.04374130815267563, -0.12323670834302902, -0.8256554007530212, 0.09124544262886047, -0.08591459691524506, 0.1628839671611786, 0.17853973805904388, -1.345918893814087, 0.1626780778169632, -0.30702900886535645, -0.42568346858024597, -0.17182625830173492, 0.13115045428276062, 1.2324804067611694, -1.0443488359451294, -0.24169224500656128, -0.34778088331222534, 0.3720715641975403, -1.2238956689834595, 1.3192983865737915, -0.3714802861213684, -0.13099946081638336, -0.22660543024539948, -0.5791253447532654, 0.0750640332698822, -0.5243825316429138, 0.838542103767395, -0.7595030069351196, -0.04718444496393204, 0.5480653643608093, -0.39059191942214966, 1.128883719444275, -0.45968300104141235, 0.6562155485153198, 0.045109156519174576, -0.2843961715698242, -0.26222506165504456, 0.2662668228149414, -0.5829749703407288, -0.544560968875885, 0.6314386129379272, 0.3190128207206726, -0.6058915853500366, 0.599814772605896, 0.8172590136528015, 1.2901431322097778, -0.34592166543006897, 0.5055621862411499, 0.6773406267166138, -0.1306307166814804, -0.0069502415135502815, 0.5390384197235107, 0.08112075924873352, 0.38709068298339844, 0.5643988847732544, -0.3399927020072937, 0.26653358340263367, -0.839366614818573, -0.12416470050811768, 0.6017627716064453, 0.6222102046012878, 0.5481380224227905, 0.30871817469596863, -0.5318048596382141, -0.34909090399742126, 0.3634561002254486, 0.5671054720878601, 1.68349027633667, -0.18042035400867462, 0.35707327723503113, -0.7312706112861633, -0.08275356888771057, -0.12177873402833939, 0.1250094771385193, -0.3408176898956299, -0.3336198627948761, -0.4653800427913666, -1.0186375379562378, 0.8292258977890015, 0.6022705435752869, 0.8615952134132385, -0.6655469536781311, -0.34910818934440613, -0.27101096510887146, 0.5033091306686401, -0.857101321220398, -0.6663199663162231, 0.44655126333236694, -0.7009603977203369, -0.17249678075313568, 0.19849900901317596, -0.037270814180374146, 0.2803826332092285, -0.4153033494949341, 0.9665377140045166, -0.37665480375289917, -0.22541916370391846, 0.19233950972557068, 0.9177594184875488, -0.7237327694892883, -0.47841742634773254, 0.21237264573574066, 0.4493086636066437, 0.08511670678853989, 0.13000375032424927, 0.9700820446014404, 0.01138608530163765, -0.20511731505393982, -0.014444125816226006, 0.5193008780479431, 0.18871714174747467, -0.18160700798034668, 1.0747835636138916, -0.6296995878219604, 0.26522207260131836, -1.199249267578125, 0.7143072485923767, -0.03835233673453331, -0.6468636989593506, 0.49424734711647034, -0.47763416171073914, -0.33859527111053467, 0.3968387544155121, -0.8274363279342651, -0.684055507183075, -0.945637583732605, 0.2169477790594101, -0.41463229060173035, -0.03796148672699928, 0.17108573019504547, 0.431131511926651, -0.12656952440738678, -0.1587001085281372, 0.3367360532283783, 0.38435351848602295, -0.4016847312450409, 0.28512680530548096, -0.7138510346412659, 0.6103124022483826, 0.47293996810913086, -0.2883950173854828, -0.48530977964401245, -0.39751696586608887, -0.9255722761154175, -0.5792223215103149, -0.47259852290153503, -0.23240859806537628, -0.16805493831634521, 0.14144682884216309, -0.7301447987556458, -0.7327809929847717, 0.04709339141845703, -0.9909248352050781, -0.5398657917976379, 0.5005305409431458, 0.4851500391960144, 0.06539100408554077, -1.3448972702026367, -1.4183807373046875, -0.5330125689506531, -0.9958578944206238, -1.0601919889450073, 0.3018810749053955, 0.2023724913597107, -0.7684363126754761, -0.6811217665672302, -0.17265941202640533, -0.3537285327911377, 1.3921185731887817, -1.0181626081466675, 1.034319281578064, -0.07947512716054916, -0.1379174292087555, -0.3441011607646942, 0.1645537167787552, 0.425754189491272, -0.5946214199066162, 0.09335534274578094, -1.1710742712020874, 0.07468817383050919, -0.5898177623748779, -0.4258541166782379, 0.2379196584224701, 0.5725353956222534, 0.9655085802078247, -0.04588942602276802, -0.5830549597740173, 0.4332774579524994, 1.39275324344635, -1.1327077150344849, -0.32596084475517273, -0.22039295732975006, 1.036919355392456, -0.06326007843017578, -0.29028668999671936, 0.29468461871147156, 0.5250411629676819, 0.5186317563056946, 0.27977463603019714, -0.23287266492843628, -0.2677111327648163, -0.40683090686798096, 0.2048557847738266, 2.0560688972473145, 0.3224840760231018, -0.03883315250277519, -0.6319707632064819, 0.39351287484169006, -1.2816286087036133, -0.6602355241775513, 0.7109206318855286, 0.9199883341789246, 0.27228638529777527, -0.6168341636657715, -0.5782924294471741, -0.7098413705825806, 0.35156065225601196, 0.40081149339675903, -0.5984082221984863, -0.7278926968574524, 0.29470929503440857, 0.34389886260032654, 0.1514700949192047, 0.7617518305778503, -0.39993521571159363, 0.8764415383338928, 14.415251731872559, 0.822623074054718, 0.1531578153371811, 0.9284428358078003, 0.7048643827438354, 0.09738951176404953, -0.3907638490200043, -0.6535329818725586, -1.617661952972412, -0.32348525524139404, 1.3577951192855835, 0.297085165977478, 0.7453315258026123, 0.4318937659263611, 0.01943129301071167, 0.010305637493729591, -0.4103405475616455, 0.5796469449996948, 0.6689004898071289, -1.3300683498382568, 0.47016945481300354, -0.054730452597141266, 0.4818173944950104, 0.9063454866409302, 0.5979067087173462, 0.8887496590614319, 0.4746077060699463, -0.35927703976631165, 0.4868761897087097, 0.4893726110458374, 0.44841262698173523, -0.2512447237968445, 0.43357399106025696, 0.6918107867240906, -1.1416738033294678, -0.020871685817837715, -0.7110010385513306, -0.88817298412323, 0.04223690181970596, -0.3839559257030487, -0.06985533982515335, -0.6562505960464478, -0.0751258134841919, 0.49792546033859253, -0.23017752170562744, 0.34105220437049866, -0.061419740319252014, 0.5194675326347351, -0.266693651676178, -0.014028471894562244, 0.3046896457672119, 0.6636407375335693, -0.20424991846084595, 0.050923287868499756, -0.17202353477478027, -0.11317691951990128, 0.17654095590114594, 0.5953168272972107, -0.5479336977005005, -0.06417042762041092, -0.16325798630714417, -0.23326519131660461, 0.12438595294952393, 0.6973440051078796, 0.37284722924232483, 0.043309133499860764, -0.5310582518577576, 0.43714550137519836, 1.0641517639160156, 0.1680458039045334, -0.3288991451263428, 0.322135329246521, 0.5799264311790466, -1.1938762664794922, 0.15165945887565613, 0.5844627618789673, 0.08337190002202988, -0.5553762316703796, -0.9536004662513733, -0.29646626114845276, 0.28141385316848755, -0.8860473036766052, -0.6986801624298096, 0.8381198644638062, 0.1439196765422821, 0.2549296021461487, 0.18074090778827667, -0.5021858811378479, -0.1383884847164154, 0.6348943114280701, -1.2380056381225586, -0.7108958959579468, 0.3697472810745239, -0.36578160524368286, -0.24909532070159912, 0.031504470854997635, 1.3831794261932373, 0.0391000434756279, -0.7274056077003479, 0.4275757670402527, -0.013785673305392265, -0.3182401657104492, -0.49853405356407166, -0.2780577540397644, 0.8295238018035889, 0.23391865193843842, -0.013456355780363083, 0.3724946081638336, -0.5273976922035217, 0.18688078224658966, -0.9784306287765503, -0.14294551312923431, 0.9452386498451233, -0.5286788940429688, -0.4891505539417267, -0.6759936213493347, -1.1585553884506226, 0.45100992918014526, 0.535559356212616, 0.03514035791158676, 0.4040880501270294, 0.34024831652641296, -0.4061684310436249, 0.03883049264550209, -0.5219908356666565, -0.058067116886377335, 0.2637583911418915, -0.8154024481773376, -0.16832219064235687, -0.04641369730234146, 0.27971333265304565, -1.0307601690292358, -0.6364981532096863, -0.39543697237968445, 0.20665490627288818, 0.2982689440250397, 1.110351324081421, -0.6807882189750671, -0.00709378719329834, 1.076719045639038, -0.197922945022583, -0.8600707054138184, -0.30946919322013855, -0.30259647965431213, -0.2968633770942688, -0.02857338637113571, 0.6982549428939819, -0.45505040884017944, -0.09194034337997437, 0.9894896745681763, 0.2813263237476349, -0.5755656957626343, -0.5159543752670288, 0.049686249345541, -0.4074379503726959, -0.8332396745681763, 0.19599565863609314, -0.21412718296051025, -0.35372063517570496, -0.1695016771554947, 0.48291251063346863, 0.9219391942024231, 0.15903884172439575, -0.6243069171905518, 0.1756962239742279, -0.157274067401886, -0.14669948816299438, -0.5606676936149597, -0.3001008927822113, -1.2586995363235474, 0.3655942976474762, -1.2191444635391235, 0.02481718361377716, -0.8011091351509094, -0.16536341607570648, -0.25222158432006836, -0.3802340030670166, 0.32649245858192444, -0.03916081041097641, 0.08435675501823425, -0.23462313413619995, -0.39083632826805115, -0.8946809768676758, 0.6680586934089661, 0.6331412196159363, -0.44372400641441345, 0.017542285844683647, -0.1128365620970726, 0.43419384956359863, 0.6221221685409546, 0.5028083920478821, -0.6335357427597046, -0.9445086717605591, -1.5041193962097168, 0.6337962746620178, -0.33464711904525757, 0.07673832029104233, -0.7516342997550964, 1.04402756690979, 0.27080440521240234, 0.08518336713314056, -0.060577213764190674, 0.44891998171806335, -0.9738752245903015, -0.3770159184932709, 0.3361698389053345, -0.9681903719902039, 0.3377944529056549, 0.6243349313735962, -0.4555678367614746, -0.2229802906513214, 0.5341027975082397, 0.12503288686275482, -1.0303871631622314, -1.263566255569458, 0.5293945074081421, -0.5830844640731812, 0.342246949672699, -0.3910857141017914, 0.0762094110250473, -0.8432930707931519, -0.09969369322061539, 0.09745842963457108, 0.4322803020477295, -0.6050434708595276, 0.9272035360336304, 0.7331267595291138, -1.0689067840576172, 0.08784076571464539, 0.6506010293960571, -0.1614813208580017, 0.035879384726285934, 0.8342462778091431, 0.627195417881012, -0.1771327257156372, 0.6085860133171082, 0.377956360578537, 0.16879485547542572, -1.238492488861084, -0.26976490020751953, 0.7591139674186707, -0.3159317374229431, -0.4427787959575653, 1.2627811431884766, -0.17705880105495453, -1.1806021928787231, 0.28363290429115295, -1.055845022201538, -0.543406069278717, -0.025141578167676926, 0.8775554895401001, -0.043080467730760574, -0.10680045187473297, -0.18767550587654114, -0.6257560849189758, 0.17996740341186523, -0.1198764443397522, -0.30764907598495483, 0.6663683652877808, -0.17784585058689117, -0.42748916149139404, 0.7443829774856567, 1.0743736028671265, -0.9198316931724548, -0.9087380170822144, -0.776961624622345, -0.5154469013214111, -0.0776093378663063, 0.9559486508369446, -0.11974004656076431, -0.8266644477844238, 0.5830304622650146, 0.44412603974342346, -0.2580334544181824, -0.1838216632604599, 0.002630011411383748, 0.041054584085941315, 0.590402364730835, 0.2207634150981903, -0.6809350252151489, -0.7193490862846375, 1.1981064081192017, 1.1069751977920532, -0.9218471050262451, 0.39601537585258484, -0.06592825800180435, -0.7936718463897705, 0.6701300740242004, 0.4453350305557251, -0.08212625235319138, 0.7516330480575562, -0.27241623401641846, -0.08186334371566772, 0.25048089027404785, -1.2141046524047852, -0.025648558512330055, 1.3927022218704224, 0.6843074560165405, 0.6880020499229431, 0.24617813527584076, 0.4158347249031067, 0.7201385498046875, 0.45181721448898315, 0.08795134723186493, -0.016010688617825508, 0.3896532952785492, -0.5528037548065186, 0.4844443202018738, 0.23941129446029663, 0.9326751232147217, -0.5996592044830322, -0.8295168876647949, 0.45962944626808167, 0.7344691753387451, 0.049595728516578674, 0.2358108013868332, 1.1362347602844238, 0.4000592529773712, 0.08633110672235489, 0.2820116877555847, 0.5449391603469849, -0.5636216402053833, -0.46294888854026794, -0.2630472481250763, -0.5754930377006531, -0.39146947860717773, -0.10868461430072784, -0.30619508028030396, -0.3517453372478485, -0.12029953300952911, 0.09276609867811203, -0.12336228787899017, 0.3216750919818878, 1.0708953142166138, 0.6963989734649658, 0.8735501170158386, -0.6925039291381836, -0.6683231592178345, -0.4981720745563507, -0.8433710932731628, -0.14988699555397034, -0.7261022925376892, -0.3253653645515442, -0.0286239106208086, -0.1601579636335373, -0.6634718775749207]}, "authors": [{"authorId": "1922666913", "name": "Bingli Liao"}, {"authorId": "2275626492", "name": "Danilo Vasconcellos Vargas"}], "references": [{"paperId": "8b8a7f1ac390a2394802234d3c539da86c56de66", "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"}, {"paperId": "b085968c4362fb286ad6c5ef71a5db9630da0498", "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": null, "title": "2023. Alpaca: A strong, replicable instruction-following model"}, {"paperId": null, "title": "We empirically validate the effectiveness of Shared Attention by implementing it across various benchmarks and demonstrate that it achieves comparable accuracy"}, {"paperId": null, "title": "2023. Llama 2: Open foundation and \ufb01ne-tuned chat models"}, {"paperId": null, "title": "2024b. A tuning-free asymmetric 2bit quantization for kv cache"}, {"paperId": null, "title": "2023. Ef\ufb01cient streaming language models with attention sinks"}, {"paperId": null, "title": "We propose a novel Shared Attention mechanism that reduces computational and memory overhead by directly sharing pre-computed attention weights across multiple layers in LLMs"}]}