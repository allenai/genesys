{"paperId": "337f421a364a6fa8ca423afd627bee2426f69395", "abstract": "The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.", "venue": "arXiv.org", "year": 2023, "citationCount": 12, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc."}, "embedding": {"model": "specter_v2", "vector": [-0.18553029000759125, 0.6659597158432007, -0.5266068577766418, 0.07491904497146606, -0.13093894720077515, 0.11807014793157578, 0.026111239567399025, -0.46749091148376465, -0.6155918836593628, 0.18928927183151245, 0.00911024771630764, 0.1427839994430542, -0.3227494955062866, 0.06588194519281387, -0.8887093663215637, -0.005959309171885252, -1.4863674640655518, 0.4951258599758148, -0.005495891906321049, -0.2832036316394806, -0.446585088968277, -0.12357404083013535, -1.0107213258743286, 0.01745006814599037, 0.35169196128845215, -0.0035682281013578176, 0.6672136783599854, 1.2089672088623047, 0.1027691587805748, 0.8798863887786865, 1.0436322689056396, 0.16769948601722717, 0.3554747998714447, 0.11257530003786087, -0.29819580912590027, -0.517865777015686, 0.34244251251220703, -0.857906699180603, -1.2559324502944946, 0.7102584838867188, -0.24763372540473938, 0.16370916366577148, 0.4418697953224182, -0.7352725267410278, 0.10764387995004654, 0.6447859406471252, 0.8803640604019165, 0.5745981931686401, 0.019539043307304382, -0.5008274912834167, 0.9794869422912598, -0.9051401615142822, 0.179488867521286, 1.7451696395874023, 0.5933464169502258, 0.7703419923782349, -0.19538463652133942, -1.339187741279602, 0.14590251445770264, -0.4562620222568512, -0.5890827178955078, -0.06797123700380325, -0.10352789610624313, -0.5773006677627563, 1.0978050231933594, -0.790695309638977, 0.13380159437656403, 0.5735159516334534, 0.5870448350906372, 1.4080806970596313, 0.09332742542028427, -0.9124594926834106, -0.07149608433246613, 0.2971511483192444, 0.3307718336582184, 1.392195224761963, 0.17204222083091736, 0.7944190502166748, -1.3571103811264038, 0.12563034892082214, 0.9685287475585938, -0.3343833088874817, -0.12414941936731339, -0.7246220111846924, -0.4366847574710846, 0.8268339037895203, 0.517495334148407, 0.19559207558631897, -0.22783027589321136, 0.7572717666625977, 0.4732622802257538, 0.5546124577522278, -0.8613828420639038, 0.143853560090065, 0.06675564497709274, 0.6765140891075134, 0.040274787694215775, 0.48399966955184937, 0.16152575612068176, 0.9834950566291809, 0.10601118952035904, 0.30778267979621887, -0.28522002696990967, 0.1704479455947876, 1.7946540117263794, 0.18425242602825165, 0.9675067067146301, -0.7932706475257874, 0.3963724374771118, -0.4297574758529663, 0.7925019860267639, -0.42727866768836975, -0.463901549577713, -0.41972145438194275, -0.1788494735956192, -0.6590746641159058, -0.426463782787323, 0.1623738408088684, -0.7921170592308044, 0.8264157772064209, -0.2530304789543152, -0.31978389620780945, 0.755534291267395, 0.4755149185657501, 0.377271443605423, 0.23142987489700317, 0.3342067301273346, 0.20439152419567108, 0.4256625771522522, -0.9848915934562683, -1.0215702056884766, -0.9374100565910339, 0.5907113552093506, 0.4247349202632904, 0.26835766434669495, -0.46751195192337036, -0.9613009691238403, -1.0867583751678467, -1.1154406070709229, 0.4379013776779175, -0.1982588768005371, 0.384019672870636, 1.968700647354126, 0.08194860816001892, -0.8175879716873169, 0.7366278767585754, -0.5615489482879639, -0.3234986662864685, -0.03209669888019562, 0.9534317255020142, -0.16225160658359528, -0.20032203197479248, -1.1565685272216797, 0.7351076602935791, 1.179840326309204, -0.4188438057899475, -1.1077907085418701, 0.5045004487037659, -1.524275779724121, -0.6184535026550293, 0.15886947512626648, -0.5071634650230408, 1.4553552865982056, -0.01716788299381733, -1.5202282667160034, 0.2799041271209717, 0.08581431955099106, -0.2535630762577057, 0.37913212180137634, -0.5138620138168335, -0.7382886409759521, -0.12168315052986145, -0.18954038619995117, 0.6630892157554626, -0.21262264251708984, -0.9532895684242249, -0.27999404072761536, 0.18501561880111694, 0.23092862963676453, -0.022314513102173805, -0.24408653378486633, 0.5705390572547913, -0.11968207359313965, 0.10663735121488571, 0.38218608498573303, 0.30715659260749817, -0.34093427658081055, 0.1814529001712799, 0.08495789766311646, -0.8036057949066162, 0.800909698009491, 0.06689291447401047, 0.8468285202980042, -0.8147124648094177, -0.6289676427841187, -0.06049801781773567, -0.2561362087726593, -0.3885784149169922, -0.8114832639694214, 0.606261134147644, -0.2900315821170807, -0.21089932322502136, -0.01824239268898964, -0.22207583487033844, -0.36709314584732056, 0.40586018562316895, -0.5611735582351685, 0.043457936495542526, -0.3923387825489044, 0.7507960796356201, -1.2872307300567627, 0.018963022157549858, 0.351136177778244, -0.17974428832530975, -0.9572954773902893, 1.4378055334091187, -0.055270660668611526, 0.20246343314647675, -0.2897271513938904, -0.4967940151691437, -0.4033655524253845, 0.017901267856359482, 0.3426717519760132, -0.247366264462471, 0.011037914082407951, 0.4735283851623535, -0.8116763234138489, 1.8666846752166748, -0.2768573462963104, 0.4724636375904083, 0.2670406699180603, -0.30350565910339355, 0.07513412088155746, 0.8179071545600891, -0.19358842074871063, -0.05295174941420555, 0.32004955410957336, 0.5492635369300842, 0.00418125418946147, -0.2499772012233734, 0.3131800889968872, 0.6023241877555847, -0.1027635931968689, 0.031296998262405396, 0.8572741150856018, -0.4855097234249115, 0.5706444382667542, 0.2346808910369873, 0.34978070855140686, 0.5793337821960449, 0.2962578535079956, -0.2514663636684418, 0.1949903666973114, -0.7760359644889832, -0.1672680675983429, 0.699781596660614, 0.5316717624664307, 0.25708770751953125, -0.6242761611938477, -0.9584075808525085, 0.22478285431861877, -0.415549099445343, 0.7994915246963501, 0.9350160360336304, 0.41608476638793945, -0.20054438710212708, -0.6784953474998474, -0.2689186632633209, -0.26027190685272217, 0.4092181324958801, -0.4649866223335266, -0.3781450688838959, -0.7935820817947388, -0.5416361689567566, 0.6987293362617493, 0.5159821510314941, 1.1984691619873047, -0.7028859853744507, -0.5506263375282288, -0.09821473807096481, 0.6429965496063232, -0.3872505724430084, -0.12887774407863617, 0.570763111114502, -0.9113289713859558, -0.5736268162727356, 0.3381726145744324, -0.5150825381278992, 0.12456725537776947, -0.34216365218162537, 0.258594810962677, -0.5928779244422913, -0.09398683160543442, 0.48579052090644836, 1.1883782148361206, -0.9591174721717834, -0.7263809442520142, -0.21981480717658997, 0.6492729187011719, -0.5915405750274658, -0.09701395034790039, 0.41340816020965576, 0.5182489156723022, -0.0812024399638176, -0.1780872941017151, 0.27393588423728943, 0.45534464716911316, 0.19575166702270508, 0.3002983331680298, -0.08498872816562653, 0.2886357307434082, -0.5563048720359802, 1.1642568111419678, 0.46395570039749146, -0.43173277378082275, 0.1379765123128891, -0.5690290331840515, -0.002431059954687953, 0.3250100612640381, -0.7388895750045776, -0.12302045524120331, -0.5656341314315796, 0.7583781480789185, -0.2019815444946289, -0.46168234944343567, 0.4231075048446655, 0.5313880443572998, -0.3999873101711273, 0.4723646640777588, 0.39981600642204285, 0.7247262001037598, 0.13055601716041565, 0.15314394235610962, -0.5434414744377136, 0.4557788372039795, 0.019855989143252373, 0.15956443548202515, -0.38813939690589905, -0.3461211621761322, -0.41070300340652466, -0.3308583199977875, -0.1130250096321106, -0.14357343316078186, -0.4770148992538452, -0.09271470457315445, -1.0423696041107178, -0.7631020545959473, -0.0857161283493042, -0.6453846096992493, -0.6114465594291687, 0.06827636808156967, 0.6730165481567383, -0.730911135673523, -1.058645486831665, -1.288257360458374, -1.08535897731781, -0.24345998466014862, -1.1652412414550781, -0.043532416224479675, 0.24784134328365326, -0.4703746438026428, -0.5062904953956604, 0.24501584470272064, -0.2530698776245117, 0.6473184823989868, -0.8163787126541138, 0.7584169507026672, -0.06384874880313873, -0.171681746840477, -0.2607087790966034, 0.5763545036315918, 0.23024636507034302, 0.11340384185314178, -0.17627990245819092, -0.6017873287200928, -0.11808932572603226, -0.2654169797897339, -0.8546531796455383, -0.3440374433994293, -0.0886770486831665, 0.45771726965904236, 0.005821583792567253, -0.4190010726451874, -0.13081909716129303, 1.0032930374145508, -0.019397273659706116, -0.09087223559617996, 0.4709954857826233, 0.9623354077339172, 0.38200831413269043, -0.2665461003780365, -0.025872167199850082, 0.5472888946533203, 0.5003269910812378, 0.28507328033447266, 0.2260153591632843, -0.24694760143756866, -0.7702253460884094, 0.9767086505889893, 1.30914306640625, 0.03356894850730896, 0.5437948703765869, -0.8920767307281494, 0.37842193245887756, -1.2904608249664307, -0.5869064927101135, 1.088462471961975, 0.8485586643218994, 0.5342208743095398, -0.08737009763717651, -0.1993049830198288, 0.0575813390314579, 0.766194224357605, -0.5055758357048035, -0.20650522410869598, -0.03501419350504875, 0.34154781699180603, -0.07772263139486313, 0.08442306518554688, 0.847221314907074, -0.40738967061042786, 0.2661806643009186, 14.74305534362793, 0.5495427846908569, -0.04834768921136856, 0.21364614367485046, -0.08855915069580078, 0.7276825904846191, -0.0949791744351387, -0.4808353781700134, -1.0353323221206665, -0.4068598449230194, 1.192697286605835, 0.1889951527118683, 0.9807846546173096, 0.7479779720306396, -0.08211318403482437, -0.10007089376449585, -0.6732302308082581, 0.6652362942695618, 0.3880622386932373, -0.7368243932723999, 0.991710364818573, -0.07957122474908829, 0.07337736338376999, 0.7028634548187256, 0.3538016974925995, 1.0451735258102417, 0.6609482169151306, -0.4803397059440613, 1.0479471683502197, 0.07021404057741165, 0.6332371234893799, -0.07927510887384415, 0.5474914908409119, 1.0854812860488892, -1.0284253358840942, -0.5669347047805786, -0.32111799716949463, -1.2210867404937744, 0.0790543258190155, -0.5067235231399536, -0.5241972208023071, -0.3615172207355499, -0.4310864806175232, 0.48312848806381226, 0.4925452768802643, 0.3401975631713867, -0.5385720133781433, 0.44694027304649353, -0.2687636911869049, 0.007697999477386475, -0.10051142424345016, 0.5094124674797058, 0.05715779587626457, -0.2753005921840668, -0.04922792688012123, -0.05977638438344002, 0.5931112170219421, 0.10432744771242142, -0.1811463087797165, -0.3156116306781769, -0.7199395298957825, -0.47024083137512207, -0.3346062898635864, 0.4939388632774353, 0.45487985014915466, 0.5685957074165344, -0.008071482181549072, -0.229498490691185, 0.8751468062400818, 0.2463388890028, -0.20234864950180054, -0.2690373659133911, 0.11575870215892792, -0.6498370170593262, -0.6179800033569336, -0.060782648622989655, -0.2246677726507187, -0.6321706771850586, -0.784844696521759, -0.24470873177051544, 0.07198597490787506, -0.8566282987594604, -0.4803057909011841, 0.9315757155418396, 0.28335508704185486, -0.4083806276321411, 0.19075314700603485, -1.143323540687561, -0.08314084261655807, -0.019234031438827515, -1.0537031888961792, -0.7915424108505249, 0.057506442070007324, 0.21054236590862274, -0.24579916894435883, -0.25084567070007324, 1.2940480709075928, -0.04800283536314964, -0.3581807017326355, -0.2696586847305298, -0.24005989730358124, 0.02331547997891903, -0.4877302348613739, -0.41543832421302795, -0.16470715403556824, -0.05332658439874649, -0.03188703581690788, 0.7300489544868469, 0.16518926620483398, -0.11220792680978775, -1.143296480178833, 0.17772598564624786, -0.04286681115627289, -0.8120059967041016, -0.1449950635433197, -0.6523247361183167, -0.7156835794448853, 0.6683932542800903, 0.20100237429141998, -0.27860158681869507, 0.12976354360580444, -0.3337687849998474, -0.0971270427107811, -0.24210543930530548, -0.9808244705200195, 0.21671153604984283, 0.5595758557319641, -0.9362722039222717, -0.9505813717842102, 0.1159147247672081, 0.23157088458538055, -1.1624841690063477, -0.09820843487977982, 0.16813600063323975, 0.40386995673179626, 0.24500234425067902, 0.9089040756225586, -0.6161013245582581, 0.08628696203231812, 0.41578209400177, -0.2657001316547394, -0.7119981050491333, 0.0033687097020447254, -0.7223761081695557, -0.09369255602359772, -0.44278523325920105, 0.7296499609947205, -0.6774758100509644, -0.024267466738820076, 0.8358588814735413, -0.06848025321960449, -0.22041980922222137, -0.6199048161506653, -0.34062322974205017, -0.1985468566417694, -0.4253785014152527, -0.09245497733354568, -0.6453224420547485, 0.10963179171085358, 0.17393741011619568, 0.42155155539512634, 0.7684565186500549, -0.13021808862686157, -0.6440858840942383, 0.6980665922164917, 0.051915448158979416, -0.21758298575878143, -0.08306367695331573, -0.31641072034835815, -1.3821254968643188, 0.1448664665222168, -1.2917594909667969, 0.721583366394043, -1.1484514474868774, -0.37423229217529297, -0.06178824603557587, -0.45970502495765686, -0.04278635233640671, 0.7653827667236328, -0.7661870121955872, -0.556000828742981, 0.005705204326659441, -0.6110300421714783, 0.952110230922699, 1.1290820837020874, -0.4825441241264343, -0.17550194263458252, -0.15079127252101898, 0.505308985710144, 0.6894279718399048, 0.5443196892738342, -0.5589652061462402, -1.1245532035827637, -1.0611902475357056, 0.08318785578012466, -0.10163983702659607, -0.3223971724510193, -1.368743896484375, 0.9305418133735657, 0.21056413650512695, 0.02137158066034317, 0.2857953608036041, 0.4877610504627228, -1.334397554397583, -0.013519496656954288, 0.9894172549247742, -1.0236217975616455, -0.06733696907758713, 0.4739239811897278, -0.16792605817317963, -0.3157065808773041, 0.33004456758499146, -0.0066391960717737675, -1.0211139917373657, -0.6708388924598694, 0.28231215476989746, -0.9374867081642151, -0.33452293276786804, 0.28526315093040466, 0.0657612606883049, -0.33356189727783203, -0.16179457306861877, -0.10368309170007706, 0.3759545683860779, -0.44665011763572693, 0.8038320541381836, 0.657448947429657, -1.2990176677703857, 0.5743715763092041, 0.5564802885055542, -0.23348398506641388, 0.06717993319034576, 0.014032120816409588, 0.08032237738370895, -0.4551037847995758, 0.5901810526847839, -0.03761924430727959, 0.45901593565940857, -0.334775447845459, -0.1514798104763031, 0.8393719792366028, -0.30560389161109924, -0.21928711235523224, 0.7662208080291748, -0.13774527609348297, -1.6035963296890259, 0.6586889028549194, -0.8786162734031677, -0.7778029441833496, -1.0614243745803833, 0.58644038438797, 0.20128999650478363, -0.8317619562149048, -0.291562020778656, -0.23543615639209747, 0.4614177346229553, -0.09100888669490814, -0.3984139561653137, 0.3843865990638733, -0.10305385291576385, -0.08745080232620239, 1.142883539199829, 0.10512731969356537, -0.6027907133102417, -0.9128785133361816, 0.1776014268398285, -0.297773540019989, 0.08772796392440796, -0.24456915259361267, -0.4578651487827301, -0.478651225566864, 0.5543534159660339, 0.925575852394104, -0.33858782052993774, -0.29651468992233276, -0.29671093821525574, -0.22159472107887268, 1.1910614967346191, 1.1205912828445435, -0.5626342296600342, -0.34788739681243896, 1.2979553937911987, 1.707170009613037, -0.9413930177688599, 0.2683429419994354, -0.6240252256393433, -0.34539172053337097, 1.3925983905792236, 0.9133581519126892, -0.36674121022224426, 0.6949266791343689, -0.2738243043422699, 0.22398090362548828, 0.11027949303388596, -0.46509411931037903, 0.15651735663414001, 0.3759598433971405, 1.095955729484558, 0.7508871555328369, 0.4929223358631134, 0.2055952399969101, 0.7829189300537109, -0.10001427680253983, 0.8425710201263428, 0.6876950860023499, 1.0065529346466064, -0.6454814076423645, 0.39438381791114807, 0.17004574835300446, 0.500616192817688, -0.24861468374729156, 0.25415167212486267, 0.4352806806564331, 1.0728830099105835, -0.1552414447069168, 0.44590264558792114, 0.09042834490537643, -0.3954333961009979, 0.520520031452179, -0.07072439044713974, 0.6814621090888977, -0.6580539345741272, 0.047747768461704254, -0.26069483160972595, -0.3507833778858185, -0.28661203384399414, -0.7120351195335388, -0.07924133539199829, -1.1772321462631226, 0.49906766414642334, -0.12024612724781036, 0.5260555148124695, 0.5575955510139465, 1.0146249532699585, 0.3453553020954132, 0.1140856072306633, -0.8102177977561951, -0.5767230987548828, -0.681959331035614, -1.096222162246704, 0.10114806890487671, -1.042023777961731, 0.0318395160138607, -1.126460075378418, -0.4219861328601837, -0.4208594262599945]}, "authors": [{"authorId": "2268665261", "name": "Xuan Xiao"}, {"authorId": "2224379550", "name": "Jiahang Liu"}, {"authorId": "2260614650", "name": "Zhipeng Wang"}, {"authorId": "144111479", "name": "Yanmin Zhou"}, {"authorId": "2268432799", "name": "Yong Qi"}, {"authorId": "2268399263", "name": "Qian Cheng"}, {"authorId": "2260639385", "name": "Bin He"}, {"authorId": "2224153707", "name": "Shuo Jiang"}], "references": [{"paperId": "8a9e443bf8426a881795dc53edf0ea740dcd76d3", "title": "Learning a Flexible Neural Energy Function With a Unique Minimum for Globally Stable and Accurate Demonstration Learning"}, {"paperId": "c62711f6b5d8620ba36bc2c378ec6ab53f6e197c", "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"}, {"paperId": "9a8819369283adcb40280dda3528ba0ff39d6247", "title": "Dual Stream Meta Learning for Road Surface Classification and Riding Event Detection on Shared Bikes"}, {"paperId": "42706e1e7d1ab5cd55cfcf2249f80eeba79fdec1", "title": "DEFT: Dexterous Fine-Tuning for Real-World Hand Policies"}, {"paperId": "54f107c7cdffc43490ba18e3e4313a8b4dff4950", "title": "MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations"}, {"paperId": "dd4dfee7ad7a2ed179f9b2e80b83685b37661dbf", "title": "Human-like systematic generalization through a meta-learning neural network"}, {"paperId": "45872b94798c3125abfb185b7926689c5e767763", "title": "GraphGPT: Graph Instruction Tuning for Large Language Models"}, {"paperId": "ad4beeecbdfc7dd238eb55e2b19113f62096d95b", "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots"}, {"paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc", "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"}, {"paperId": "974f0e1a85c1ece2555718342ff2abb6bcb6a825", "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models"}, {"paperId": "ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f", "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models"}, {"paperId": "c3d14e7a319ab764297a60112ce74af201762a73", "title": "Learning Interactive Real-World Simulators"}, {"paperId": "e7d09b6f2bc878cf2c993acf675f409d0b55f35a", "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"}, {"paperId": "54814744b42b06c855c97b23de1366e0bcbe775a", "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"}, {"paperId": "6ec855b6c80a258200e41b65f118c6116ab908a6", "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "334d6aa9b6e85d3ed3fb13daf5c2aeda89ee2b27", "title": "See to Touch: Learning Tactile Dexterity through Visual Incentives"}, {"paperId": "a281094d05e96b7cca044fdd87ff7c3c65649e20", "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "af3ab5da98e0807784b57e321ed887a3666a8ab6", "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants"}, {"paperId": "570b125788bb43384def312127a0af1b4b13f443", "title": "Grasp-Anything: Large-scale Grasp Dataset from Foundation Models"}, {"paperId": "0c72450890a54b68d63baa99376131fda8f06cf9", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey"}, {"paperId": "fa75a55760e6ea49b39b83cb85c99a22e1088254", "title": "NExT-GPT: Any-to-Any Multimodal LLM"}, {"paperId": "b65144033c6d29103879fb178d8efba610cfcd27", "title": "Large Language Model for Science: A Study on P vs. NP"}, {"paperId": "4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f", "title": "Hypothesis Search: Inductive Reasoning with Language Models"}, {"paperId": "24d52678c887331b9da0368e8a2f58bec07f7203", "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"}, {"paperId": "f8a2dca1e8fe56e698984c077f7ff58d8ca867e9", "title": "Large Language Models as Optimizers"}, {"paperId": "de7bdb2e79c5298244b6b3ff329ad5e4a77e2dd1", "title": "Deep Imitation Learning for Humanoid Loco-manipulation Through Human Teleoperation"}, {"paperId": "e4bb1b1f97711a7634bf4bff72c56891be2222e6", "title": "Cognitive Architectures for Language Agents"}, {"paperId": "316f980cfd2e217234386166a46eb080bf027cdd", "title": "Physically Grounded Vision-Language Models for Robotic Manipulation"}, {"paperId": "cc92496398fbb78646341f95d79e25080ff58b53", "title": "Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning"}, {"paperId": "4c2ed9907a3e966b3caadec5dddfde0e4c83f9da", "title": "CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection"}, {"paperId": "719f574226f34cecc57c72be6082353dd8c2f04d", "title": "Hybrid hierarchical learning for solving complex sequential tasks using the robotic manipulation network ROMAN"}, {"paperId": "71f1d46eb773a8d2c2c17a1fba4480c26c51f247", "title": "A principal odor map unifies diverse tasks in olfactory perception"}, {"paperId": "b083a81fcf36e72bc3918bf8efa3de0e6541fc1e", "title": "Socratis: Are large multimodal models emotionally aware?"}, {"paperId": "fad72b1e1e3c1a299038f74ce5773ff1152ff914", "title": "GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields"}, {"paperId": "83089d2fd243ce21d13a2430f484a431545f2065", "title": "Language-Conditioned Path Planning"}, {"paperId": "f1dc0b8b844332f08b7503d9728e4831e8bd3607", "title": "Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception"}, {"paperId": "1aee86a54287779617721c97ca9a3692e50cf9c9", "title": "Learning Vision-based Pursuit-Evasion Robot Policies"}, {"paperId": "28c6ac721f54544162865f41c5692e70d61bccab", "title": "A Survey on Large Language Model based Autonomous Agents"}, {"paperId": "c3925ef53f864e5c30189272f63801248ff1406f", "title": "March in Chat: Interactive Prompting for Remote Embodied Referring Expression"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "451a657dabf80ebc43f6a3be518250b2cd5dfe1a", "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models"}, {"paperId": "7b4a09463e1fec7d9a49f325035453a67b842773", "title": "Bayesian Flow Networks"}, {"paperId": "3c8444cc4e96bdbe6853b886caf032afd1ee1d20", "title": "CausalLM is not optimal for in-context learning"}, {"paperId": "9b5536c16978e4dbbb6370d75dc0f245db9ed08a", "title": "Task-Driven Reinforcement Learning With Action Primitives for Long-Horizon Manipulation Skills"}, {"paperId": "658cd67a91da86cf451e6f1b015f762b56015172", "title": "Detecting and Preventing Hallucinations in Large Vision Language Models"}, {"paperId": "a9a05fdbbc7d469bb4a308c3af39135225a3acba", "title": "Foundation Model is Efficient Multimodal Multitask Model Selector"}, {"paperId": "c60116a51bf66bc363d11b797d97eba84b13cfd7", "title": "LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking"}, {"paperId": "64a80a33018a0fdc182b06111e32b2e08e186f6a", "title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment"}, {"paperId": "507acddb0b7f36b83fd7c8bff2f121eb506ac8fb", "title": "Cumulative Reasoning with Large Language Models"}, {"paperId": "4fb3695d7a3cba3db438cda198c724225ab48a38", "title": "Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods"}, {"paperId": "5dbf93a68b7fda600521f046dea35ea8ba9e884f", "title": "AgentBench: Evaluating LLMs as Agents"}, {"paperId": "ebbffe5db352a10fde868843b8d5787b87843f09", "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"}, {"paperId": "863a4cff37f2bbda38bce76689ca3dea5ba04c5b", "title": "Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose Service Robots"}, {"paperId": "7124d130b5a84250c7349f387929f6e065729093", "title": "MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation"}, {"paperId": "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04", "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"}, {"paperId": "70a75b05a9410198fd71c3a0fe937a77d15d6bc8", "title": "Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text"}, {"paperId": "1e26b42669b060a3850e4766dea0db6e3c85cdec", "title": "Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey"}, {"paperId": "7fbc502441d66daf1f53765d5d86a8dfba9ab0ce", "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"}, {"paperId": "5e274869d8b9aaee4fb4af09c3228b711c845d1d", "title": "HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions"}, {"paperId": "7777ffb5f2f1d52b7080725332035b81af2994c9", "title": "AnyLoc: Towards Universal Visual Place Recognition"}, {"paperId": "e451bcd5b503d10677188a986f2bdfc3f4f4002d", "title": "The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions"}, {"paperId": "1e6102c981b9464c632ef0b00dbd11dfb0564e4e", "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"}, {"paperId": "ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7", "title": "LISA: Reasoning Segmentation via Large Language Model"}, {"paperId": "7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4", "title": "When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities"}, {"paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"}, {"paperId": "38939304bb760473141c2aca0305e44fbe04e6e8", "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"}, {"paperId": "af6d0ba799213cbbcbfceb1fb9b78d2858486308", "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"}, {"paperId": "584ca135b61482fd89247113da87d784f738dbfa", "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook"}, {"paperId": "e41482f4ee984f17382f6cdd900df094d928be06", "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents"}, {"paperId": "dd586f20551db661be3e1eeaf4dfc962b0fbdf93", "title": "A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI"}, {"paperId": "9f4017de7deded49c032a83d7844efcd9ea1aa21", "title": "Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"}, {"paperId": "83c48aa341850af478247e3b34ba1ee1db9f1236", "title": "Meta-Transformer: A Unified Framework for Multimodal Learning"}, {"paperId": "e01ab53663e5df5961a021506a9cb09f4efc3788", "title": "Challenges and Applications of Large Language Models"}, {"paperId": "18b75ea107ed166d7120c12c162b94f02e20b417", "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks"}, {"paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9", "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"}, {"paperId": "d64fbe0dabe47bae13bd7a3b0abf838078a375f4", "title": "Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations"}, {"paperId": "d89a1cd530f83f399a0d522215c751c2ca0f3ffd", "title": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning"}, {"paperId": "c5d18dbb92d0cd5393baa1e69de33d6922ac3e57", "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models"}, {"paperId": "3c8de7a0a37fcdea5cabe0f0848319f1bbcfa75a", "title": "SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks"}, {"paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf", "title": "A Survey on Evaluation of Large Language Models"}, {"paperId": "587352c3b95c90de6d37f061c8e117f42be0b575", "title": "Building Cooperative Embodied Agents Modularly with Large Language Models"}, {"paperId": "d293a75f529d7b1abb161480a95122c9a3ed6376", "title": "Causal Reinforcement Learning: A Survey"}, {"paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae", "title": "All in One: Multi-Task Prompting for Graph Neural Networks"}, {"paperId": "df710c46594c04fb59ef9a93d3b4e1cb387a1b2b", "title": "Embodied Task Planning with Large Language Models"}, {"paperId": "d1500f1dbd62e26ef0753f31e845078f58479968", "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"}, {"paperId": "7619a98ef077c8f75e0bfb98953457649209e07e", "title": "Review of Large Vision Models and Visual Prompt Engineering"}, {"paperId": "039c49d19493bcda3b0de86fab91f83fa2626c13", "title": "Real-time Vision-based Navigation for a Robot in an Indoor Environment"}, {"paperId": "42b920abd44e76d73708859bfe13034555f1f8cb", "title": "DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment"}, {"paperId": "942130a875ccfe55a4c60c27c636f693e25cb13d", "title": "Personality Traits in Large Language Models"}, {"paperId": "03251361c1d67c6b5badffc7059fdd7fbfea1fed", "title": "Statler: State-Maintaining Language Models for Embodied Reasoning"}, {"paperId": "d87a88548010dd157b20a11e388d5b4f19320cff", "title": "FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global Connectivity"}, {"paperId": "d9823ffa34f865fb1d0adef95d64a0c352ae125f", "title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"}, {"paperId": "2421734fb9b3f8dfd08c9e078da20b03c406d222", "title": "Explainable Multimodal Emotion Recognition"}, {"paperId": "f94c040b02bdd6cf1b85f374e3912630c66861c3", "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback"}, {"paperId": "c53a121d6c99f8c2add0eddca41262c7fc0bd795", "title": "RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "ebedc4d7a2356090904baba4104ef0832bc236df", "title": "A Survey on Multimodal Large Language Models"}, {"paperId": "3efb81de24eb88017d6dbcf22cb4215084223fd8", "title": "AudioPaLM: A Large Language Model That Can Speak and Listen"}, {"paperId": "02033e83ff310f35e4623bd339982c52d926f2d5", "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling"}, {"paperId": "3b0c02955e88f5862e61b560c7f70ba8cf235b1d", "title": "HomeRobot: Open-Vocabulary Mobile Manipulation"}, {"paperId": "2562fe379554d201aad312f786903f4c60b68acf", "title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation"}, {"paperId": "53aae0a4e001853245800c70bed04b31bc25835b", "title": "CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation"}, {"paperId": "2b806bc0a075f9088021f7362ffa5b8b86fd75ab", "title": "Robot Learning with Sensorimotor Pre-training"}, {"paperId": "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6", "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap"}, {"paperId": "94bcf0390d5acb1b92323bd15cc1dc311314122c", "title": "Language to Rewards for Robotic Skill Synthesis"}, {"paperId": "eebb4a3162c1251b51e50ccd83797babc5b776c0", "title": "Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures"}, {"paperId": "993df7df129f8d18816877d69923d7df7b347d85", "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"}, {"paperId": "3b8871e4c25d3aaca2bee6606c07bc870337253c", "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions"}, {"paperId": "b2cad5ae3b2c7a9d90c2050cf37402b01725446a", "title": "The feasibility of artificial consciousness through the lens of neuroscience"}, {"paperId": "b5cfea5990b0f2aa15854a7883266ca5e5e90194", "title": "Layout-based Causal Inference for Object Navigation"}, {"paperId": "b15d51d72711b350e5717f18b28d9e469bf056b4", "title": "How can LLMs transform the robotic design process?"}, {"paperId": "e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7", "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"}, {"paperId": "431b2c3f4c6b2fe1ec4fead396286a48ed5edbbf", "title": "Train Offline, Test Online: A Real Robot Learning Benchmark"}, {"paperId": "119d3beca449efd9096d58674cf01a99c793a9a7", "title": "Towards Open-World Interactive Disambiguation for Robotic Grasping"}, {"paperId": "2d2b49c9de7833f8a306a587229caddd20013d4f", "title": "On Human Grasping and Manipulation in Kitchens: Automated Annotation, Insights, and Metrics for Effective Data Collection"}, {"paperId": "8199c9d55dd998f69f703e0ad250ca0697e3ad27", "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"}, {"paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb", "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"}, {"paperId": "00cb69a9f280317d1c59ac5827551ee9b10642b8", "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"}, {"paperId": "0f416c637a5a78435e6b12ebf1ce891224de0edc", "title": "Scaling Speech Technology to 1, 000+ Languages"}, {"paperId": "35631fd55c2545615811fa8072015356ac8198e7", "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities"}, {"paperId": "afc5092a4116f27b4c64733c7815cd662bab78f7", "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, {"paperId": "dd7f28d93dc2ec3cbc1cc66c3443c1c17105f1b3", "title": "Small Models are Valuable Plug-ins for Large Language Models"}, {"paperId": "1856bebc4cb35e68368d9c83bd2ac2d26cd4bcfa", "title": "A Comprehensive Survey on Segment Anything Model for Vision and Beyond"}, {"paperId": "1fbdd5a3ba7d17ab9c24c8c0e6ea93d7ccb90c16", "title": "Perpetual Humanoid Control for Real-time Simulated Avatars"}, {"paperId": "9bdcf270bce9f680bad5385bc7920536d4fa0c53", "title": "DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects"}, {"paperId": "e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a", "title": "TidyBot: Personalized Robot Assistance with Large Language Models"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "da3b810e10638507be9901e1b5cb94db126c8166", "title": "Programmatically Grounded, Compositionally Generalizable Robotic Manipulation"}, {"paperId": "003ef1cd670d01af05afa0d3c72d72228f494432", "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891", "title": "DINOv2: Learning Robust Visual Features without Supervision"}, {"paperId": "ae6a4cd221684be6ca3082b6f526a7901281490b", "title": "Emergent autonomous scientific research capabilities of large language models"}, {"paperId": "f274ce903eca789be99fd6e721dc6e213f8debeb", "title": "Learning a Universal Human Prior for Dexterous Manipulation from Human Preference"}, {"paperId": "7a84a26647892daa9d10dbfc97c0382619ac2f4d", "title": "ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "311d53dcb0bfe50017959323393dd88ea2b451d8", "title": "Online augmentation of learned grasp sequence policies for more adaptable and data-efficient in-hand manipulation"}, {"paperId": "e1bd151a3f670fd0f77580702fe7a85dc78a41cb", "title": "Chain-of-Thought Predictive Control"}, {"paperId": "5d5024fae7223a0f8a8b86e1fdf5c5f4b7a9d9c6", "title": "UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "9ab6c3c3be48627aba40ad89e5bbc15d7140d873", "title": "MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "9fac3d0728a8c833a593446e3e176e90d856df04", "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking"}, {"paperId": "ac7771c332da42b29a913b116bd6ef622cbf89cf", "title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"}, {"paperId": "5d332ecf011cfd2338f625909591cb46ec6170d1", "title": "ARMBench: An Object-centric Benchmark Dataset for Robotic Manipulation"}, {"paperId": "87fa694400cd3d818e7014e36b08745afc64041f", "title": "Toward Human-Like Social Robot Navigation: A Large-Scale, Multi-Modal, Social Human Navigation Dataset"}, {"paperId": "17e4b3fb4d8e27f8a4f63b251179ee62ba1eeb51", "title": "CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis"}, {"paperId": "ecceb57a5fe8e771e35b7a00278e7ab0fad2165a", "title": "On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills"}, {"paperId": "dae9be0f0d815b53b46974377a0edf9169a99f3f", "title": "Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play"}, {"paperId": "8f2d4758e6d525509ae36bb30224dc9259027e6b", "title": "Text2Motion: from natural language instructions to feasible plans"}, {"paperId": "ba38c19d132ec29a40e64bd3734bf4d6b0059637", "title": "Rotating without Seeing: Towards In-hand Dexterity through Touch"}, {"paperId": "362cbfd0d05e139cd6cf049754098a6e1520b910", "title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "2ebd5df74980a37370b0bcdf16deff958289c041", "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities"}, {"paperId": "a8f842ed0dec8d81e3530ad26fc05512c8958198", "title": "Real-world humanoid locomotion with reinforcement learning"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "f02d56e630986997e0aea3d92bf53e0f363ce401", "title": "Prismer: A Vision-Language Model with Multi-Task Experts"}, {"paperId": "96a19cd83080d70e10f3e6fd5af327a1263f20d6", "title": "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations"}, {"paperId": "777317e5af8742b30408e98778fa067750e69f78", "title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "0ba581718f294db1d7b3dbc159cc3d3380f74606", "title": "ChatGPT for Robotics: Design Principles and Model Abilities"}, {"paperId": "634127f09be5119f4d29a175866f11b14b3821e0", "title": "ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills"}, {"paperId": "bf8491bef353df126e2306ad2fe4b898697b906a", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"}, {"paperId": "d77ec728e45f2a0feb80ddf1e2e6226170ac56e2", "title": "Recent Trends in Task and Motion Planning for Robotics: A Survey"}, {"paperId": "9348656b761f7b76fb65cfe6fac55386b04a3a8a", "title": "A Comprehensive Survey of Continual Learning: Theory, Method and Application"}, {"paperId": "64c1ba56a32ed9a42f2cac010de56002380c2408", "title": "Zero-shot causal learning"}, {"paperId": "b81f8c734331a6fef1de842f7ea5952e26151e22", "title": "Software variability in service robotics"}, {"paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "title": "Towards Reasoning in Large Language Models: A Survey"}, {"paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc", "title": "Reasoning with Language Model Prompting: A Survey"}, {"paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d", "title": "RT-1: Robotics Transformer for Real-World Control at Scale"}, {"paperId": "5666cf5dd2cb1eb55dfa88c1486a6cacdf94c840", "title": "See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "f403f84183be8660c1c7aa91c98cea74f39d3924", "title": "Navigating to objects in the real world"}, {"paperId": "55dff8d4cdeab9c86dccb1c8b739ac0518cdbed0", "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "ee5661886abd37718edd5efa38b8bb660e2a7204", "title": "Towards Versatile Embodied Navigation"}, {"paperId": "a451c2226c3cc4f4d0a902f5e9b4eb157ba3138d", "title": "Grasp Learning: Models, Methods, and Performance"}, {"paperId": "b287a2765e5bceb732de39dafdf70594dc9cd664", "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"}, {"paperId": "0e34addae55a571d7efd3a5e2543e86dd7d41a83", "title": "Interactive Language: Talking to Robots in Real Time"}, {"paperId": "c305ab1bdba79442bec72ec7f5c5ee7c49c2a566", "title": "Visual Language Maps for Robot Navigation"}, {"paperId": "bdf99df7839c02abf7253e33e67caafa21eda5fd", "title": "DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation"}, {"paperId": "99832586d55f540f603637e458a292406a0ed75d", "title": "ReAct: Synergizing Reasoning and Acting in Language Models"}, {"paperId": "25425e299101b13ec2872417a14f961f4f8aa18e", "title": "VIMA: General Robot Manipulation with Multimodal Prompts"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "5178858ff4ff5aab811e91cfab654c140f81dd46", "title": "A survey of Semantic Reasoning frameworks for robotic systems"}, {"paperId": "6902d6f9f4d4c1303f87f0d131ff595aca6a951e", "title": "Multiple Mobile Robot Task and Motion Planning: A Survey"}, {"paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"}, {"paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c", "title": "Code as Policies: Language Model Programs for Embodied Control"}, {"paperId": "344cba18285949fe409fc0b332831bc186cfeb17", "title": "Multi-skill Mobile Manipulation for Object Rearrangement"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "876eb375cb7b365475040046df669c039ad54202", "title": "CodeT: Code Generation with Generated Tests"}, {"paperId": "cdf54c147434c83a4a380916b6c1279b0ca19fc2", "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"}, {"paperId": "23525374cfd3af714f3ffb7a203b1ef3253333fe", "title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents"}, {"paperId": "25bc06b508b2c63b9faf77881e528530b147b988", "title": "DayDreamer: World Models for Physical Robot Learning"}, {"paperId": "32c9b3859086d15184989454eb878638659e64c6", "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "c2fbc8ad4e2403c795529c2606e4686f5a571215", "title": "From attribution maps to human-understandable explanations through Concept Relevance Propagation"}, {"paperId": "75b181c0e62b5261e3e0f3e3e96687c128208134", "title": "ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts"}, {"paperId": "5922f437512158970c417f4413bface021df5f78", "title": "A Generalist Agent"}, {"paperId": "9f5120b815fddaaef25c7042035ffe5680507a65", "title": "Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items"}, {"paperId": "a58b3f2ab75fdbda082e684d027ab4f552b0b5d3", "title": "Correcting Robot Plans with Natural Language Feedback"}, {"paperId": "1bf46fd55008c3fe2dd531c5cdb97dceafd6b217", "title": "Semantic Exploration from Language Abstractions and Pretrained Representations"}, {"paperId": "9b97b9649feeef07dda2c0db97f7c933c26f8488", "title": "ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "f1fa3e971b93e123ebb3e76e4832509199b3daa5", "title": "OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction"}, {"paperId": "521ca2049131f48c804859a5402d3e8ab4b7e2a9", "title": "Socially CompliAnt Navigation Dataset (SCAND): A Large-Scale Dataset Of Demonstrations For Social Navigation"}, {"paperId": "742b195fb4c2868a4e60012c8e0bf7db43bb5650", "title": "CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation"}, {"paperId": "18c302c9d51146ea91784638748e5d737da75e12", "title": "Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation"}, {"paperId": "2a0b8be3594e8163f9ea4988658223d7c46cfcb3", "title": "HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction"}, {"paperId": "3def68bd0f856886d34272840a7f81588f2bc082", "title": "Survey of Hallucination in Natural Language Generation"}, {"paperId": "e9c16da93e3f3f3fa61954d92afc4983a5eb6ac0", "title": "Robotic Grasping from Classical to Modern: A Survey"}, {"paperId": "5eaa7b7db558a4cbef6d73a4ef4be9130df48154", "title": "RFUniverse: A Multiphysics Simulation Platform for Embodied AI"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "9d270c56dd80a2f493e206781faf82a22351a6b2", "title": "Learning to Act with Affordance-Aware Multimodal Neural SLAM"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "34e82c55ae8920e893d1635c8156193cbcc78ce5", "title": "M2DGR: A Multi-Sensor and Multi-Scenario SLAM Dataset for Ground Robots"}, {"paperId": "4be02694125b71876552900a53c85c47a2a83614", "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks"}, {"paperId": "582b6ce5b3266a934f8be7cdcea5e09352d2c389", "title": "Teleoperation and Visualization Interfaces for Remote Intervention in Space"}, {"paperId": "826383e18568c9c37b5fc5dd7e2913352db22b47", "title": "Simple but Effective: CLIP Embeddings for Embodied AI"}, {"paperId": "4a02061f8623f68502991d8bdf7728ae50669091", "title": "A decade retrospective of medical robotics research from 2010 to 2020"}, {"paperId": "0ee9b633a0914b51f1eec3ad434752aa58e10149", "title": "A System for General In-Hand Object Re-Orientation"}, {"paperId": "e7eba2aa3c625beec289cb14914e7b5d36469d04", "title": "Robot Learning From Randomized Simulations: A Review"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424", "title": "TEACh: Task-driven Embodied Agents that Chat"}, {"paperId": "6bae20930eaa0d9d489317f3b3b1aaaf18205ef8", "title": "Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets"}, {"paperId": "69ee9b3a915951cc84b74599a3a2699a66d4004f", "title": "CLIPort: What and Where Pathways for Robotic Manipulation"}, {"paperId": "096618aba19bce1d917df488a04891ab239cffdd", "title": "Towards Natural Language Interfaces for Data Visualization: A Survey"}, {"paperId": "49142e3e381c0dc7fee0049ea41d2ef02c0340d7", "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning"}, {"paperId": "5c89bc8e91fb85f8af7761e8096d27dd740491d1", "title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation"}, {"paperId": "c0c9f77cb097f2ce53feb91802bcfbae57fcc42f", "title": "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments"}, {"paperId": "5aeaaa5a11697c8b7f6e52080c223ad8564e4855", "title": "Utilization of multilayer perceptron for determining the inverse kinematics of an industrial robotic manipulator"}, {"paperId": "4aa88c1406414cda3ce9cf76c8af0abaa8391760", "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat"}, {"paperId": "32feca141fce06c6588b4014d27953a3fc25f19b", "title": "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World"}, {"paperId": "f8677102449c803512c87b240b3989e4e9276cbc", "title": "Toward next-generation learned robot manipulation"}, {"paperId": "d05fdc686ebe5148196b25b5c7e3880654c3354e", "title": "Emerging Wearable Interfaces and Algorithms for Hand Gesture Recognition: A Survey"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "21ec9c0f869bdb33b06c7dbc8880169db0397d08", "title": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark"}, {"paperId": "f79bc5e08d5d00c486baefb29e2306715d991b69", "title": "Core Challenges of Social Robot Navigation: A Survey"}, {"paperId": "9c404d02aefd850ac3d5a8bdc5860738e6cd2b04", "title": "A Survey of Embodied AI: From Simulators to Research Tasks"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "33428914d6d4bd646054c44610c101b1444f2903", "title": "AGENT: A Benchmark for Core Psychological Reasoning"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "806725595f04849b3b4cc9f6c28d4a84744e8d95", "title": "iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes"}, {"paperId": "604e0c54580a0530392f86b48a6183581a47b66d", "title": "VLN\u21bbBERT: A Recurrent Vision-and-Language BERT for Navigation"}, {"paperId": "dab94f11874acdd5ffe63cfb22c5f93723dc519f", "title": "Where Are You? Localization from Embodied Dialog"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "398a0625e8707a0b41ac58eaec51e8feb87dd7cb", "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning"}, {"paperId": "94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b", "title": "A Survey of Deep Active Learning"}, {"paperId": "f0152a8fd87cb60ad30c296823829802c13a9986", "title": "Object-and-Action Aware Model for Visual Language Navigation"}, {"paperId": "d3dd97403d8fdb9aa9541ea84db029c1d6bbc9ed", "title": "Learning Object Relation Graph and Tentative Policy for Visual Navigation"}, {"paperId": "fb98a5e61efb13a76bd3b02d18860460ef012655", "title": "PackIt: A Virtual Environment for Geometric Planning"}, {"paperId": "c9f9ee3659c2a855b33ae256e98b05c51b2e30b7", "title": "Swoosh! Rattle! Thump! - Actions that Sound"}, {"paperId": "dbaa1b97034d647383f611c1010f8f9ac35aefce", "title": "The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose"}, {"paperId": "827c7d53d5d6cb2f5d4cf4b6e9c6b2a3c11bc15c", "title": "GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "89962792268cd60c2b09d8f44030032a0ec94c75", "title": "Robot perception of static and dynamic objects with an autonomous floor scrubber"}, {"paperId": "fd83bfb69b874509a964d9984061fdd6c1634fe6", "title": "Automated extraction of chemical synthesis actions from experimental procedures"}, {"paperId": "f4cf4246f3882aa6337e9c05d5675a3b8463a32e", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"}, {"paperId": "0bc855f84668b35cb65618d996d09f6e434d28c9", "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"}, {"paperId": "3e519d85cdcefdd1d2ad89829d6ad445695d8c58", "title": "RoboNet: Large-Scale Multi-Robot Learning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "6d5fd1c40604ebbda2de58b29fbdaa97745ce7d6", "title": "MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding"}, {"paperId": "a2fdfda785b3a2a0178d174daa515377c531f222", "title": "RLBench: The Robot Learning Benchmark & Learning Environment"}, {"paperId": "82a6c7aec68d81deeb6fc6858d63d51604fd7c3c", "title": "DISCOMAN: Dataset of Indoor SCenes for Odometry, Mapping And Navigation"}, {"paperId": "33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b", "title": "Emergent Tool Use From Multi-Agent Autocurricula"}, {"paperId": "4a3e9a51f6d7261a64aedae4dba4d097ef7d6a23", "title": "kPAM-SC: Generalizable Manipulation Planning using KeyPoint Affordance and Shape Completion"}, {"paperId": "79bc6e1fe465aec49d7f0252f295c0ad9cdaf389", "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning"}, {"paperId": "f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6", "title": "A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms"}, {"paperId": "fee1e72a23203f00943db68ba9c0e444e12097aa", "title": "Trends and challenges in robot manipulation"}, {"paperId": "5c09f5b0f22c1fb5fa7035a44ed933da835f5b3f", "title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "1bdd8f6d3900e6e7fe492ea21fcdd87f3d8c857f", "title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments"}, {"paperId": "c8c76626db4246c944642e86d19665025fa7deb4", "title": "Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout"}, {"paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466", "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"}, {"paperId": "e27e78c33288728f66f7dab2fe2696ddbc5c1026", "title": "COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis"}, {"paperId": "48e86729d2b60f47a6888dd57948ac484678d92c", "title": "Generating Grasp Poses for a High-DOF Gripper Using Neural Networks"}, {"paperId": "5d81569e4f35a7afcb23a49bd5185d0e8bd496e3", "title": "A review of ground-based robotic systems for the characterization of nuclear environments"}, {"paperId": "dda6fb309f62e2557a071522354d8c2c897a2805", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"paperId": "e41f00c02053aa4aaa1dd5537c549570cab95ccd", "title": "Robot learning\u2014Beyond imitation"}, {"paperId": "57c7ced90b8c635f8346983651363379d0357dc2", "title": "Learning ambidextrous robot grasping policies"}, {"paperId": "05146ae7935f56f8b1d082b490dc8d879b93d5f3", "title": "Ten robotics technologies of the year"}, {"paperId": "3ab84417632f676acc4db2d452c0848f6622fb29", "title": "Purposive learning: Robot reasoning about the meanings of human activities"}, {"paperId": "f75f0750a00f6f85107985c70eca9c275b5e0962", "title": "TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments"}, {"paperId": "c66b8e508718f4b7f14829e5c2cde0add31d2693", "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation"}, {"paperId": "0df7dd6d836a5df84dafd74bbd65df6d0fa94091", "title": "One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks"}, {"paperId": "cda470bede832f2965e594f9bdee79d6973a91e9", "title": "ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation"}, {"paperId": "0052b31f07eda7737b5e0e2bf3803c3a32f3f728", "title": "Supervising strong learners by amplifying weak experts"}, {"paperId": "cef5a3fab9bf7a576838e550c1de37a7e253a52f", "title": "O2A: One-Shot Observational Learning with Action Vectors"}, {"paperId": "1b19f433a3e8497e9d9bd67efb108521d16b5b85", "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning"}, {"paperId": "c7aea4b653d4e12cb47438960f5689f5f835e073", "title": "Visual Semantic Navigation using Scene Priors"}, {"paperId": "7fe203374ba3ddc8462a7c775b276e53f09dd036", "title": "NavigationNet: A Large-scale Interactive Indoor Navigation Dataset"}, {"paperId": "068d2251c62540c5147a713f02caf38224cd1ca3", "title": "Robots for the people, by the people: Personalizing human-machine interaction"}, {"paperId": "39e734da43eb8c72e9549b42e96760545036f8e5", "title": "QuAC: Question Answering in Context"}, {"paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"}, {"paperId": "56a0ead811a1bf15e42be8a9a007b0299636f213", "title": "Talk the Walk: Navigating New York City through Grounded Dialogue"}, {"paperId": "2e19e455327176d02f9c82e43bd9638d97a9908d", "title": "A dataset of daily interactive manipulation"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "7139a5f730652abbeabf9e140009907d2c7da3e5", "title": "VirtualHome: Simulating Household Activities Via Programs"}, {"paperId": "893186c6bc08a17cb3f9f94fa3f14e9ad20b0525", "title": "Speaker-Follower Models for Vision-and-Language Navigation"}, {"paperId": "fc50c9392fd23b6c88915177c6ae904a498aacea", "title": "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"}, {"paperId": "6272f7355e28148e576cad0a14e240c333299d78", "title": "Jacquard: A Large Scale Dataset for Robotic Grasp Detection"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "89c8aad71433f7638d2e2c009e1ea20e039f832d", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI"}, {"paperId": "c02a6b732abc4fbb8ac8237e7ce5f97cc9797845", "title": "Will robots be bodies with brains or brains with bodies?"}, {"paperId": "c37c23b12e00168833eccff8025a830ce27c5abc", "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"}, {"paperId": "482c0cbfffa77154e3c879c497f50b605297d5bc", "title": "One-Shot Visual Imitation Learning via Meta-Learning"}, {"paperId": "41f2a087031944f9b990eb102f59b4ff58d6b5ef", "title": "Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics"}, {"paperId": "e10a5e0baf2aa87d804795af071808a9377cc80a", "title": "Towards Automatic Learning of Procedures From Web Instructional Videos"}, {"paperId": "e296a89be7ce1cad7e4f2a86b2cc6a527442da62", "title": "Robot@Home, a robotic dataset for semantic mapping of home environments"}, {"paperId": "03eb382e04cca8cca743f7799070869954f1402a", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"}, {"paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1", "title": "Generative Adversarial Imitation Learning"}, {"paperId": "494e2d5b40dcebde349f9872c7317e5003f9c5d2", "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection"}, {"paperId": "f03b4ff1b4943691cec703b508c0a91f2d97a881", "title": "Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours"}, {"paperId": "e0ce0e87edb08038c1432755909bb7270026ec50", "title": "Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set"}, {"paperId": "da9e411fcf740569b6b356f330a1d0fc077c8d7c", "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"}, {"paperId": "49435aab7cdf259335725acc96691f755e436f55", "title": "A database for fine grained activity detection of cooking activities"}, {"paperId": "bce01bd0f070a4aec6600ba78b031cb8a7e554e3", "title": "Trajectories and keyframes for kinesthetic teaching: A human-robot interaction perspective"}, {"paperId": "8b3b8848a311c501e704c45c6d50430ab7068956", "title": "HMDB: A large video database for human motion recognition"}, {"paperId": "3c104b0e182a5f514d3aebecc93629bbcf1434ac", "title": "Efficient grasping from RGBD images: Learning using a new rectangle representation"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "176f4a67510b8cde4de72a6bf8586ca6957d9922", "title": "What determines our navigational abilities?"}, {"paperId": "9f0ffa7012b7a802535284a232a412f84dc6f7b0", "title": "The TUM Kitchen Data Set of everyday manipulation activities for motion tracking and action recognition"}, {"paperId": "8fa9c9568d8de9cd3536d6f99d99fe957d45e0a1", "title": "Robotic Grasping of Novel Objects using Vision"}, {"paperId": "342fe6a6338e73fd4d34c4f37f41e3bbad274dd2", "title": "Networks"}, {"paperId": "8a694ac007cad520325cbe1e2e150aa6e7d80d32", "title": "Tactile Pose Feedback for Closed-loop Manipulation Tasks"}, {"paperId": "62a869c8e9b116d813787102f277ac56fc3db31c", "title": "RH20T: A Robotic Dataset for Learning Diverse Skills in One-Shot"}, {"paperId": "5b06056345034e1559ef8680190cdccc79a2196d", "title": "VIMA: Robot Manipulation with Multimodal Prompts"}, {"paperId": "5ce94181ea702f69c3651dce721d6bd8026b8106", "title": "TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"}, {"paperId": "46299fee72ca833337b3882ae1d8316f44b32b3c", "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection"}, {"paperId": "4747e72c5bc706c50e76953188f0144df18992d0", "title": "Communicative Agents for Software Development"}, {"paperId": "e6b79c12032884be401da08177a7c33ca02a6985", "title": "Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots"}, {"paperId": "e13b52a3ee01320165d4ba49f20708f9907ed563", "title": "Demystifying GPT Self-Repair for Code Generation"}, {"paperId": "666d4aed971e87ca46c35d4f01fce0d78b8a9793", "title": "Overleaf Example"}, {"paperId": "69764fcc646e4c608ac08eeb4c784cf8465268d2", "title": "BEHAVIOR-1K: A Benchmark for Embodied AI with 1, 000 Everyday Activities and Realistic Simulation"}, {"paperId": "b48713978407f58653323ff94e35b81c624ea345", "title": "RFUniverse: A Physics-based Action-centric Interactive Environment for Everyday Household Tasks"}, {"paperId": "414a476f83634e3b452b243ed7460c9ef3d1aaa4", "title": "Benchmarking Progress to Infant-Level Physical Reasoning in AI"}, {"paperId": "c1f1ec160744ba4a9526cc6a007f2d637ac74732", "title": "BEVBert: Topo-Metric Map Pre-training for Language-guided Navigation"}, {"paperId": null, "title": "Robot manipulation: Perception, planning, and control"}, {"paperId": "d43991b2fc2921c4fff5dea8ecb60b32fc0e1512", "title": "Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning"}, {"paperId": "b9effa7f0d628ab825f07bee059a3997257348ce", "title": "Deep Reinforcement Learning with Skill Library : Exploring with Temporal Abstractions and coarse approximate Dynamics Models"}, {"paperId": null, "title": "Imitation learning: A survey of learning methods"}, {"paperId": "cc58c920e2988331e8509e6d7c4279cc6ea85c1d", "title": "Robot Learning from Demonstration : Kinesthetic Teaching vs . Teleoperation"}, {"paperId": "70d6dfdc40c4681ba5d51d60116db0311b5126ce", "title": "Language Models"}, {"paperId": "12bdfc10f7c17a0a647589d5b4f10ad2d0049c93", "title": "Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC) Database"}, {"paperId": "97efafdb4a3942ab3efba53ded7413199f79c054", "title": "Reinforcement Learning: An Introduction"}, {"paperId": null, "title": ". Internlm: A multilingual language model with progressively enhanced capabilities"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Moss: Training conversational language models from synthetic data"}, {"paperId": "cfee1826dd4743eab44c6e27a0cc5970effa4d80", "title": "Improving Image Generation with Better Captions"}, {"paperId": null, "title": "Robotgpt: From chatgpt to robot intelligence"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}]}