{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../model_discovery/model/library/base/mamba2/reports'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m all_results\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m BASELINES:\n\u001b[0;32m---> 48\u001b[0m     all_results[i]\u001b[38;5;241m=\u001b[39mread_eval_results(i)\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mread_eval_results\u001b[0;34m(baseline, mean_group)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_eval_results\u001b[39m(baseline,mean_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     reports\u001b[38;5;241m=\u001b[39mread_eval_reports(baseline)\n\u001b[1;32m     25\u001b[0m     results\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m reports:\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mread_eval_reports\u001b[0;34m(baseline)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_eval_reports\u001b[39m(baseline):\n\u001b[0;32m---> 17\u001b[0m     report\u001b[38;5;241m=\u001b[39mread_reports(baseline)\n\u001b[1;32m     18\u001b[0m     eval_reports\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m report:\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mread_reports\u001b[0;34m(baseline)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASEDIR, baseline,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m reports \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m         scale \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model_discovery/model/library/base/mamba2/reports'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "BASEDIR='../model_discovery/model/library/base'\n",
    "COREDIR='../model_discovery/model/library/core'\n",
    "\n",
    "def read_reports(baseline):\n",
    "    DIR=BASEDIR if 'RANDOM' in baseline else COREDIR\n",
    "    dir = os.path.join(DIR, baseline,'reports')\n",
    "    reports = {}\n",
    "    for file in os.listdir(dir):\n",
    "        if file.startswith('report_'):\n",
    "            scale = file.split('.')[0].split('_')[1]\n",
    "            with open(os.path.join(dir, file), 'r') as f:\n",
    "                reports[scale]=json.load(f)\n",
    "    return reports\n",
    "\n",
    "def read_eval_reports(baseline):\n",
    "    report=read_reports(baseline)\n",
    "    eval_reports={}\n",
    "    for i in report:\n",
    "        eval_reports[i]=report[i]['eval_results.json']\n",
    "    return eval_reports\n",
    "\n",
    "def read_eval_results(baseline,mean_group=None):\n",
    "    reports=read_eval_reports(baseline)\n",
    "    results={}\n",
    "    for i in reports:\n",
    "        results[i]=reports[i]['results']\n",
    "    return results\n",
    "\n",
    "BASELINES = ['gpt2','mamba2','ttt','retnet','rwkv6']\n",
    "\n",
    "GLUE_TASK_LIST = [\"cola\",\"mnli\",\"mrpc\",\"qnli\",\"qqp\",\"rte\",\"sst2\",\"wnli\"]\n",
    "STANDARD_EVAL_TASKS = [\"hellaswag\",\"piqa\",\"arc_easy\",\"arc_challenge\",\"winogrande\", \"openbookqa\",\"lambada_openai\"]\n",
    "MEAN_GROUPS = {\n",
    "    'standard':STANDARD_EVAL_TASKS,\n",
    "    'blimp':None,\n",
    "    'inverse_scaling':None,\n",
    "    'glue':GLUE_TASK_LIST,\n",
    "    'qa4mre': None,\n",
    "    'scrolls':None,\n",
    "}\n",
    "\n",
    "gpt2_eval_reports=read_eval_reports('gpt2')\n",
    "gpt2_results=read_eval_results('gpt2')\n",
    "random_results=read_eval_results('RANDOM')['0']\n",
    "all_results={}\n",
    "for i in BASELINES:\n",
    "    all_results[i]=read_eval_results(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybridflow_dir='/home/junyanc/model_discovery/ckpt/test_evo_000/ve/hybridflow_14M/report.json'\n",
    "ahan_transformer_dir='/home/junyanc/model_discovery/ckpt/test_evo_000/ve/ahan-transformer_14M/report.json'\n",
    "\n",
    "def read_eval(dir):\n",
    "    with open(dir, 'r') as f:\n",
    "        report = json.load(f)\n",
    "    return report['eval_results.json']['results']\n",
    "\n",
    "hybridflow=read_eval(hybridflow_dir)\n",
    "ahan_transformer=read_eval(ahan_transformer_dir)\n",
    "hybridflow={'14M':hybridflow}\n",
    "ahan_transformer={'14M':ahan_transformer}\n",
    "all_results['*hybridflow*']=hybridflow\n",
    "all_results['*ahan-transformer*']=ahan_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print datasets and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# random_results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m metrics\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m random_results:\n\u001b[1;32m      4\u001b[0m     res\u001b[38;5;241m=\u001b[39mrandom_results[i]\n\u001b[1;32m      5\u001b[0m     alias\u001b[38;5;241m=\u001b[39mres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_results' is not defined"
     ]
    }
   ],
   "source": [
    "# random_results\n",
    "metrics={}\n",
    "for i in random_results:\n",
    "    res=random_results[i]\n",
    "    alias=res['alias']\n",
    "    for m in res:\n",
    "        if m=='alias': continue\n",
    "        if '_stderr' in m: continue\n",
    "        if m not in metrics:\n",
    "            metrics[m]=[]\n",
    "        metrics[m].append(alias)\n",
    "for m in metrics:\n",
    "    print(m)\n",
    "    print(metrics[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_results(results,metrics,mean_group={}):\n",
    "    out={}\n",
    "    mean_groups={}\n",
    "    for group in mean_group:\n",
    "        mean_groups[group]=0\n",
    "        out[group]=0\n",
    "    for i in results:\n",
    "        res=results[i]\n",
    "        alias=res['alias']\n",
    "        for metric in metrics:\n",
    "            if metric in res:\n",
    "                mgroup=None\n",
    "                for group in mean_group:\n",
    "                    subtasks=mean_group[group]\n",
    "                    if subtasks is None:\n",
    "                        if group in alias:\n",
    "                            mgroup=group\n",
    "                    else:\n",
    "                        if alias in subtasks:\n",
    "                            mgroup=group\n",
    "                if mgroup:\n",
    "                    out[mgroup]+=res[metric]\n",
    "                    mean_groups[mgroup]+=1\n",
    "                else:\n",
    "                    out[alias]=res[metric]\n",
    "    for group in mean_group:\n",
    "        out[group]=out[group]/mean_groups[group]\n",
    "    return out\n",
    "\n",
    "def readout_allresults(allresults,metrics,mean_group={}):\n",
    "    out={}\n",
    "    for scale in allresults:\n",
    "        results=allresults[scale]\n",
    "        out[scale]=readout_results(results,metrics,mean_group)\n",
    "    return out\n",
    "\n",
    "def avg_results(results,keyword=None):\n",
    "    if keyword:\n",
    "        results=filter_results(results,keyword)\n",
    "    avg=0\n",
    "    for i in results:\n",
    "        avg+=results[i]\n",
    "    return avg/len(results)\n",
    "\n",
    "def filter_results(results,keyword):\n",
    "    out={}\n",
    "    for i in results:\n",
    "        if keyword in i:\n",
    "            out[i]=results[i]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 dict_keys(['14M', '70M', '31M'])\n",
      "mamba2 dict_keys(['14M', '31M'])\n",
      "ttt dict_keys(['14M', '31M'])\n",
      "retnet dict_keys(['14M'])\n",
      "rwkv6 dict_keys(['14M'])\n",
      "*hybridflow* dict_keys(['14M'])\n",
      "*ahan-transformer* dict_keys(['14M'])\n"
     ]
    }
   ],
   "source": [
    "for i in all_results:\n",
    "    print(i,all_results[i].keys())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     all_accs[i]\u001b[38;5;241m=\u001b[39mreadout_allresults(all_results[i],metrics,mean_group\u001b[38;5;241m=\u001b[39mMEAN_GROUPS)\n\u001b[1;32m     61\u001b[0m all_acc_14M \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 62\u001b[0m all_acc_14M[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mrandom_acc\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m all_accs:\n\u001b[1;32m     64\u001b[0m     all_acc_14M[i]\u001b[38;5;241m=\u001b[39mall_accs[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m14M\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_acc' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_grouped_bar_chart(results, baseline=None, ratio=False, title='Scores by group and metric', width=16, height=6):\n",
    "    \"\"\"\n",
    "    Plot a grouped bar chart.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A dictionary where keys are group names and values are lists of scores for each metric.\n",
    "    - metrics: A list of metric names.\n",
    "    - groups: A list of group names.\n",
    "    - title: The title of the chart.\n",
    "    \"\"\"\n",
    "    groups = list(results.keys())\n",
    "    metrics = list(results[groups[0]].keys())\n",
    "    data={}\n",
    "    for group in groups:\n",
    "        data[group]=[]\n",
    "        for metric in metrics:\n",
    "            score=results[group][metric]\n",
    "            if baseline:\n",
    "                score-=baseline[metric]\n",
    "                if ratio:\n",
    "                    score/=baseline[metric]\n",
    "            data[group].append(score)\n",
    "    x = np.arange(len(metrics))  # the label locations\n",
    "    bar_width = 0.8 / len(groups)  # the width of the bars, dynamically adjusted based on number of groups\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(width, height))\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        ax.bar(x + i * bar_width - bar_width * (len(groups) - 1) / 2, data[group], bar_width, label=group)\n",
    "    \n",
    "    # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "    \n",
    "    ax.set_xlabel('Metrics')\n",
    "    if baseline:\n",
    "        if ratio:\n",
    "            ax.set_ylabel('Relative Ratio to Baseline')\n",
    "        else:\n",
    "            ax.set_ylabel('Relative Scores to Baseline')\n",
    "    else:\n",
    "        ax.set_ylabel('Scores')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "metrics=['acc,none','mcc,none','em,none','contains,none','exact_match,strict-match','exact_match,flexible-extract']#,'perplexity,none']\n",
    "\n",
    "gpt2_accs = readout_allresults(gpt2_results,metrics,mean_group=MEAN_GROUPS)\n",
    "# random_acc = readout_results(random_results,metrics,mean_group=MEAN_GROUPS)\n",
    "all_accs = {}\n",
    "for i in all_results:\n",
    "    all_accs[i]=readout_allresults(all_results[i],metrics,mean_group=MEAN_GROUPS)\n",
    "    \n",
    "all_acc_14M = {}\n",
    "all_acc_14M['random']=random_acc\n",
    "for i in all_accs:\n",
    "    all_acc_14M[i]=all_accs[i]['14M']\n",
    "# gpt2_accs['random']=random_acc\n",
    "\n",
    "\n",
    "# ratio=False\n",
    "# baseline=random_acc\n",
    "# baseline.pop('squad_completion')\n",
    "# print(baseline)\n",
    "# plot_grouped_bar_chart(all_acc_14M,baseline,ratio)\n",
    "# plot_grouped_bar_chart(gpt2_accs,baseline,ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_acc_14M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cols\u001b[38;5;241m=\u001b[39mall_acc_14M[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m      4\u001b[0m rows\u001b[38;5;241m=\u001b[39mall_acc_14M\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m      5\u001b[0m all_acc_14M_data\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_acc_14M' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols=all_acc_14M['gpt2'].keys()\n",
    "rows=all_acc_14M.keys()\n",
    "all_acc_14M_data=[]\n",
    "for i in rows:\n",
    "    all_acc_14M_data.append(all_acc_14M[i])\n",
    "df=pd.DataFrame(all_acc_14M_data,index=rows,columns=cols)\n",
    "# remove low sensitivity metrics or ineffective ones (e.g. all lower than random)\n",
    "df=df.drop(columns=['standard','squad_completion','tinyGSM8k','sciq','mnli_mismatch','scrolls','swag','tinyTruthfulQA'])\n",
    "# precision to 3 decimal places\n",
    "df=df.round(3)\n",
    "# bold the max value in each column\n",
    "# for col in df.columns:\n",
    "#     df.loc[df[col]==df[col].max(),col]=f'<span style=\"font-weight: bold;\">{df[col].max()}</span>'\n",
    "# average the scores for each row besides column scrolls\n",
    "df['avg']=(df.mean(axis=1)).round(3)\n",
    "# normalize the scores to random\n",
    "df_norm = df.div(df.loc['random'])\n",
    "# minus by 1\n",
    "df_norm=df_norm.sub(1)\n",
    "# show percent change\n",
    "df_norm=df_norm.mul(100)\n",
    "df_norm=df_norm.round(2)\n",
    "# drop random row\n",
    "df_norm=df_norm.drop(index=['random'])\n",
    "\n",
    "\n",
    "# df.style.format(\"{:.3f}\").to_html('all_acc_14M.html')\n",
    "df.to_csv('all_acc_14M.csv')\n",
    "df_norm.to_csv('all_acc_14M_norm.csv')\n",
    "\n",
    "\n",
    "print('Percent Change from Random, 14M, trained on 280M tokens:')\n",
    "df_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
