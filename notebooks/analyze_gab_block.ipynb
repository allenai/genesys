{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static analysis of GAB block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/model/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/model/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Failed to login to HuggingFace Hub, some datasets may not be available to download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/junyanc/.conda/envs/modis/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "2024-08-18:21:09:49,888 INFO     [__init__.py:29] Skipping import of cpp extensions\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.model.lab as lab\n",
    "from model_discovery.model.library import *\n",
    "from exec_utils import BuildTool\n",
    "from model_discovery.configs.gam_config import ( \n",
    "    GAMConfig,GAMConfig_14M,GAMConfig_31M,GAMConfig_70M,GAMConfig_125M,GAMConfig_350M,GAMConfig_760M,\n",
    "    GAMConfig_1300M,GAMConfig_2700M,GAMConfig_6700M,GAMConfig_13B,GAMConfig_175B,GAMConfig_1T,GAMConfig_debug\n",
    ")\n",
    "\n",
    "\n",
    "def load_gab(model_name: str, scale='14M'):\n",
    "    gab_code = MODEL2CODE[model_name]\n",
    "    checker = BuildTool(tool_type=\"checker\")\n",
    "    try:\n",
    "        checkpass, gab_code = checker._check_format_and_reformat(gab_code)\n",
    "        assert checkpass\n",
    "    except AssertionError as e:\n",
    "        print('Model does not pass the format checker')\n",
    "        raise e\n",
    "    \n",
    "    # Wrap len inside the executed code\n",
    "    gab_code = f\"{gab_code}\"\n",
    "    \n",
    "    module = {}\n",
    "    exec(gab_code.replace(\"class GAB\",\"class GABCustom\"),module)\n",
    "    assert \"GABCustom\" in module, \"Class GAB not found in module. You should never ever change the class name of GAB and it should always inherit from GABBase.\"\n",
    "    GAB = module[\"GABCustom\"]\n",
    "\n",
    "    cfg = eval(f\"GAMConfig_{scale}()\")\n",
    "    gab_config = {} \n",
    "    assert \"gab_config\" in module, \"Dictionary gab_config not found in module.\"\n",
    "    gab_config = module[\"gab_config\"]\n",
    "\n",
    "    gab= GAB(cfg.d_model,block_loc=(0,cfg.n_block),device=None,dtype=None, **gab_config)\n",
    "\n",
    "    return gab,cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from /home/junyanc/model_discovery/data/wikitext-2/tokenized/meta-llama/Llama-2-7b-hf/2048\n",
      "Checking code format...\n",
      "Code after reformatted:\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GABBase\n",
      "from typing import Any, Dict, Optional, Tuple, Union\n",
      "import torch.nn.functional as F\n",
      "import torch.utils.checkpoint\n",
      "from torch.utils._pytree import tree_map\n",
      "from transformers.utils import logging\n",
      "from transformers.activations import ACT2FN\n",
      "try:\n",
      "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
      "except:\n",
      "    causal_conv1d_update, causal_conv1d_fn = None, None\n",
      "\n",
      "\n",
      "def rotate_half(x):\n",
      "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
      "    x1 = x[..., :x.shape[-1] // 2]\n",
      "    x2 = x[..., x.shape[-1] // 2:]\n",
      "    return torch.cat((-x2, x1), dim=-1)\n",
      "\n",
      "\n",
      "def permute_qk(q, k):\n",
      "    bsz, num_head, seq_len, head_dim = q.shape\n",
      "    q = q.reshape(bsz, num_head, seq_len, head_dim // 2, 2).transpose(3, 4\n",
      "        ).reshape(bsz, num_head, seq_len, head_dim)\n",
      "    k = k.reshape(bsz, num_head, seq_len, head_dim // 2, 2).transpose(3, 4\n",
      "        ).reshape(bsz, num_head, seq_len, head_dim)\n",
      "    return q, k\n",
      "\n",
      "\n",
      "def undo_permute_qk(q, k):\n",
      "    bsz, num_head, seq_len, head_dim = q.shape\n",
      "    q = q.reshape(bsz, num_head, seq_len, 2, head_dim // 2).transpose(3, 4\n",
      "        ).reshape(bsz, num_head, seq_len, head_dim)\n",
      "    k = k.reshape(bsz, num_head, seq_len, 2, head_dim // 2).transpose(3, 4\n",
      "        ).reshape(bsz, num_head, seq_len, head_dim)\n",
      "    return q, k\n",
      "\n",
      "\n",
      "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
      "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
      "\n",
      "    Args:\n",
      "        q (`torch.Tensor`): The query tensor.\n",
      "        k (`torch.Tensor`): The key tensor.\n",
      "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
      "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
      "        position_ids (`torch.Tensor`, *optional*):\n",
      "            Deprecated and unused.\n",
      "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
      "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
      "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
      "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
      "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
      "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
      "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
      "    Returns:\n",
      "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
      "    \"\"\"\n",
      "    cos = cos.unsqueeze(unsqueeze_dim)\n",
      "    sin = sin.unsqueeze(unsqueeze_dim)\n",
      "    q_embed = q * cos + rotate_half(q) * sin\n",
      "    k_embed = k * cos + rotate_half(k) * sin\n",
      "    return q_embed, k_embed\n",
      "\n",
      "\n",
      "class RMSNorm(nn.Module):\n",
      "\n",
      "    def __init__(self, hidden_size, eps=1e-06):\n",
      "        super().__init__()\n",
      "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
      "        self.variance_epsilon = eps\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.\n",
      "            variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)\n",
      "\n",
      "\n",
      "class SwiGluMLP(nn.Module):\n",
      "\n",
      "    def __init__(self, hidden_size, intermediate_size):\n",
      "        super().__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.intermediate_size = intermediate_size\n",
      "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n",
      "            bias=False)\n",
      "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n",
      "            bias=False)\n",
      "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n",
      "            bias=False)\n",
      "        self.act_fn = ACT2FN['silu']\n",
      "\n",
      "    def forward(self, x):\n",
      "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.\n",
      "            up_proj(x))\n",
      "        return down_proj\n",
      "\n",
      "\n",
      "class RotaryEmbedding(nn.Module):\n",
      "\n",
      "    def __init__(self, dim, max_position_embeddings=16, base=10000, device=\n",
      "        None, scaling_factor=1.0):\n",
      "        super().__init__()\n",
      "        self.scaling_factor = scaling_factor\n",
      "        self.dim = dim\n",
      "        self.max_position_embeddings = max_position_embeddings\n",
      "        self.base = base\n",
      "        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n",
      "            torch.int64).float().to(device) / self.dim)\n",
      "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def forward(self, x, position_ids):\n",
      "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n",
      "            position_ids.shape[0], -1, 1)\n",
      "        position_ids_expanded = position_ids[:, None, :].float()\n",
      "        device_type = x.device.type\n",
      "        device_type = device_type if isinstance(device_type, str\n",
      "            ) and device_type != 'mps' else 'cpu'\n",
      "        with torch.autocast(device_type=device_type, enabled=False):\n",
      "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n",
      "                ).transpose(1, 2)\n",
      "            emb = torch.cat((freqs, freqs), dim=-1)\n",
      "            cos = emb.cos()\n",
      "            sin = emb.sin()\n",
      "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
      "\n",
      "\n",
      "class Conv(nn.Module):\n",
      "\n",
      "    def __init__(self, hidden_size, conv_kernel, rms_norm_eps):\n",
      "        super().__init__()\n",
      "        self.norm = RMSNorm(hidden_size, eps=rms_norm_eps)\n",
      "        self.conv = nn.Conv1d(hidden_size, hidden_size, bias=True,\n",
      "            kernel_size=conv_kernel, groups=hidden_size, padding=\n",
      "            conv_kernel - 1)\n",
      "\n",
      "    def __call__(self, hidden_states):\n",
      "        seq_len = hidden_states.shape[1]\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "        hidden_states = hidden_states.transpose(1, 2)\n",
      "        if causal_conv1d_fn is None:\n",
      "            hidden_states = self.conv(hidden_states)[..., :seq_len]\n",
      "        else:\n",
      "            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n",
      "                self.conv.weight.size(2))\n",
      "            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n",
      "                self.conv.bias, activation=None)\n",
      "        hidden_states = hidden_states.transpose(1, 2)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "def scan(f, init, xs, out, checkpoint_group=0):\n",
      "    \"\"\"Minic jax.lax.scan function.\"\"\"\n",
      "    carry = init\n",
      "    if isinstance(xs, dict):\n",
      "        num_items = len(next(iter(xs.values())))\n",
      "    else:\n",
      "        num_items = len(xs[0])\n",
      "\n",
      "    def scan_fn(carry, i_start, i_end):\n",
      "        for i in range(i_start, i_end):\n",
      "            if isinstance(xs, dict):\n",
      "                x = {key: tensor[i] for key, tensor in xs.items()}\n",
      "            else:\n",
      "                x = [x[i] for x in xs]\n",
      "            carry, y = f(carry, x)\n",
      "            out[i] = y\n",
      "        return carry\n",
      "    if checkpoint_group > 0:\n",
      "        ckpt_every_n = num_items // checkpoint_group\n",
      "        for k in range(0, num_items, ckpt_every_n):\n",
      "            carry = torch.utils.checkpoint.checkpoint(scan_fn, carry, k,\n",
      "                min(k + ckpt_every_n, num_items), use_reentrant=False)\n",
      "    else:\n",
      "        carry = scan_fn(carry, 0, num_items)\n",
      "    return carry, out\n",
      "\n",
      "\n",
      "def ln_fwd(x, gamma, beta, eps=1e-06):\n",
      "    \"\"\"Batch forward for LayerNorm.\"\"\"\n",
      "    mu = x.mean(dim=-1, keepdim=True)\n",
      "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
      "    std = torch.sqrt(var + eps)\n",
      "    x_hat = (x - mu) / std\n",
      "    y = gamma * x_hat + beta\n",
      "    return y\n",
      "\n",
      "\n",
      "def ln_fused_l2_bwd(x, l2_target, gamma, beta, eps=1e-06):\n",
      "    \"\"\"Batch backward for LayerNorm fused with L2 loss.\"\"\"\n",
      "    D = x.shape[-1]\n",
      "    mu = x.mean(dim=-1, keepdim=True)\n",
      "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
      "    std = torch.sqrt(var + eps)\n",
      "    x_hat = (x - mu) / std\n",
      "    y = gamma * x_hat + beta\n",
      "    grad_output = y - l2_target\n",
      "    grad_x_hat = grad_output * gamma\n",
      "    z = 1.0 / D * (D * grad_x_hat - grad_x_hat.sum(dim=-1, keepdim=True) - \n",
      "        x_hat * (grad_x_hat * x_hat).sum(dim=-1, keepdim=True)) / std\n",
      "    return z\n",
      "\n",
      "\n",
      "class TTTLinear(nn.Module):\n",
      "\n",
      "    def __init__(self, hidden_size, num_attention_heads,\n",
      "        scan_checkpoint_group_size, conv_kernel, mini_batch_size,\n",
      "        rope_theta, ttt_base_lr):\n",
      "        super().__init__()\n",
      "        self.num_heads = num_attention_heads\n",
      "        self.width = hidden_size\n",
      "        self.hidden_size = hidden_size\n",
      "        self.head_dim = self.width // self.num_heads\n",
      "        self.mini_batch_size = mini_batch_size\n",
      "        self.rope_theta = rope_theta\n",
      "        self.ttt_base_lr = ttt_base_lr\n",
      "        token_idx = 1.0 / torch.arange(1, self.mini_batch_size + 1)\n",
      "        self.register_buffer('token_idx', token_idx, persistent=False)\n",
      "        self.learnable_token_idx = nn.Parameter(torch.zeros((self.\n",
      "            mini_batch_size,)))\n",
      "        self.conv_kernel = conv_kernel\n",
      "        self._init_qkvo_proj()\n",
      "        self._init_rope()\n",
      "        self._init_ttt_lr_gate()\n",
      "        self._init_ttt_ln()\n",
      "        self.post_norm = nn.LayerNorm(self.width, eps=1e-06)\n",
      "        self.scan_checkpoint_group_size = scan_checkpoint_group_size\n",
      "        self.W1 = nn.Parameter(torch.normal(0, 0.02, size=(self.num_heads,\n",
      "            self.head_dim, self.head_dim)))\n",
      "        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim))\n",
      "\n",
      "    def _init_qkvo_proj(self):\n",
      "        self.q_proj = nn.Linear(self.width, self.num_heads * self.head_dim,\n",
      "            bias=False)\n",
      "        self.k_proj = nn.Linear(self.width, self.num_heads * self.head_dim,\n",
      "            bias=False)\n",
      "        self.v_proj = nn.Linear(self.width, self.num_heads * self.head_dim,\n",
      "            bias=False)\n",
      "        self.o_proj = nn.Linear(self.width, self.num_heads * self.head_dim,\n",
      "            bias=False)\n",
      "\n",
      "    def _init_rope(self):\n",
      "        self.rope_theta = self.rope_theta\n",
      "        self.rotary_emb = RotaryEmbedding(self.head_dim,\n",
      "            max_position_embeddings=self.mini_batch_size, base=self.rope_theta)\n",
      "\n",
      "    def _init_ttt_lr_gate(self):\n",
      "        linear_weight_data = nn.Linear(self.width, 1, bias=True).weight.data\n",
      "        self.learnable_ttt_lr_weight = nn.Parameter(torch.stack([torch.\n",
      "            normal(0, 0.02, size=linear_weight_data.shape) for _ in range(\n",
      "            self.num_heads)], dim=0))\n",
      "        linear_bias_data = nn.Linear(self.width, 1, bias=True).bias.data\n",
      "        self.learnable_ttt_lr_bias = nn.Parameter(torch.stack([torch.\n",
      "            zeros_like(linear_bias_data) for _ in range(self.num_heads)],\n",
      "            dim=0))\n",
      "\n",
      "    def _init_ttt_ln(self):\n",
      "        ln_weight_data = nn.LayerNorm(self.head_dim).weight.data\n",
      "        self.ttt_norm_weight = nn.Parameter(torch.tile(ln_weight_data.\n",
      "            unsqueeze(0), (self.num_heads, 1)))\n",
      "        ln_bias_data = nn.LayerNorm(self.head_dim).bias.data\n",
      "        self.ttt_norm_bias = nn.Parameter(torch.tile(ln_bias_data.unsqueeze\n",
      "            (0), (self.num_heads, 1)))\n",
      "\n",
      "    def get_qkv_projections(self, hidden_states):\n",
      "        XQ, XK, XV = self.q_proj(hidden_states), self.k_proj(hidden_states\n",
      "            ), self.v_proj(hidden_states)\n",
      "        return XQ, XK, XV\n",
      "\n",
      "    def _split_heads(self, hidden_states):\n",
      "        return hidden_states.reshape(hidden_states.shape[:2] + (self.\n",
      "            num_heads, self.head_dim))\n",
      "\n",
      "    def get_eta(self, X, mini_batch_size):\n",
      "        ttt_lr = torch.einsum('bnkc,hdc->bhnkd', X, self.\n",
      "            learnable_ttt_lr_weight) + self.learnable_ttt_lr_bias.reshape(1,\n",
      "            -1, 1, 1, 1)\n",
      "        ttt_lr = F.sigmoid(ttt_lr)\n",
      "        ttt_lr = ttt_lr.permute(0, 1, 2, 4, 3)\n",
      "        ttt_lr_eta = self.ttt_base_lr * ttt_lr / self.head_dim\n",
      "        token_idx = self.token_idx + self.learnable_token_idx\n",
      "        token_idx = token_idx[0:mini_batch_size]\n",
      "        token_idx = torch.clamp_min(token_idx, 0.0)\n",
      "        token_eta = torch.broadcast_to(token_idx.reshape(1, 1, 1,\n",
      "            mini_batch_size, 1), (X.shape[0], self.num_heads, X.shape[1],\n",
      "            mini_batch_size, 1))\n",
      "        return token_eta, ttt_lr_eta\n",
      "\n",
      "    def get_ttt_inputs(self, inputs, mini_batch_size):\n",
      "        XQ = inputs['XQ']\n",
      "        XK = inputs['XK']\n",
      "        XV = inputs['XV']\n",
      "        X = inputs['X']\n",
      "        B, L, C = X.shape\n",
      "        num_mini_batch = L // mini_batch_size\n",
      "        X = X.reshape(B, num_mini_batch, mini_batch_size, self.width)\n",
      "        XQ = XQ.reshape(B, self.num_heads, L // mini_batch_size,\n",
      "            mini_batch_size, self.head_dim)\n",
      "        XK = XK.reshape(B, self.num_heads, L // mini_batch_size,\n",
      "            mini_batch_size, self.head_dim)\n",
      "        XV = XV.reshape(B, self.num_heads, L // mini_batch_size,\n",
      "            mini_batch_size, self.head_dim)\n",
      "        token_eta, ttt_lr_eta = self.get_eta(X, mini_batch_size)\n",
      "        eta = token_eta * ttt_lr_eta\n",
      "        inputs = {'XQ': XQ, 'XK': XK, 'XV': XV, 'eta': eta, 'token_eta':\n",
      "            token_eta, 'ttt_lr_eta': ttt_lr_eta}\n",
      "        return inputs\n",
      "\n",
      "    def ttt(self, inputs, mini_batch_size, last_mini_batch_params_dict):\n",
      "        if mini_batch_size is None:\n",
      "            mini_batch_size = self.mini_batch_size\n",
      "        B = inputs['XV'].shape[0]\n",
      "        num_mini_batch = inputs['XV'].shape[2]\n",
      "        L = inputs['XV'].shape[2] * inputs['XV'].shape[3]\n",
      "        device = inputs['XV'].device\n",
      "        dtype = inputs['XV'].dtype\n",
      "        use_dual_form = True\n",
      "\n",
      "        def compute_mini_batch(params_dict, inputs):\n",
      "            W1_init = params_dict['W1_states']\n",
      "            b1_init = params_dict['b1_states']\n",
      "            XQ_mini_batch = inputs['XQ']\n",
      "            XV_mini_batch = inputs['XV']\n",
      "            XK_mini_batch = inputs['XK']\n",
      "            eta_mini_batch = inputs['eta']\n",
      "            token_eta_mini_batch = inputs['token_eta']\n",
      "            ttt_lr_eta_mini_batch = inputs['ttt_lr_eta']\n",
      "            X1 = XK_mini_batch\n",
      "            Z1 = X1 @ W1_init + b1_init\n",
      "            reconstruction_target = XV_mini_batch - XK_mini_batch\n",
      "            ln_weight = self.ttt_norm_weight.reshape(self.num_heads, 1,\n",
      "                self.head_dim)\n",
      "            ln_bias = self.ttt_norm_bias.reshape(self.num_heads, 1, self.\n",
      "                head_dim)\n",
      "            grad_l_wrt_Z1 = ln_fused_l2_bwd(Z1, reconstruction_target,\n",
      "                ln_weight, ln_bias)\n",
      "            if use_dual_form:\n",
      "                Attn1 = torch.tril(XQ_mini_batch @ X1.transpose(-2, -1))\n",
      "                b1_bar = b1_init - torch.tril(eta_mini_batch) @ grad_l_wrt_Z1\n",
      "                Z1_bar = (XQ_mini_batch @ W1_init - eta_mini_batch * Attn1 @\n",
      "                    grad_l_wrt_Z1 + b1_bar)\n",
      "                last_eta_mini_batch = eta_mini_batch[:, :, -1, :, None]\n",
      "                W1_last = W1_init - (last_eta_mini_batch * X1).transpose(-1, -2\n",
      "                    ) @ grad_l_wrt_Z1\n",
      "                b1_last = b1_init - torch.sum(last_eta_mini_batch *\n",
      "                    grad_l_wrt_Z1, dim=-2, keepdim=True)\n",
      "                grad_W1_last = torch.zeros_like(W1_last)\n",
      "                grad_b1_last = torch.zeros_like(b1_last)\n",
      "            else:\n",
      "                ttt_lr_eta_mini_batch = torch.broadcast_to(\n",
      "                    ttt_lr_eta_mini_batch, (*ttt_lr_eta_mini_batch.shape[:2\n",
      "                    ], mini_batch_size, mini_batch_size))\n",
      "                grad_W1 = torch.einsum('bhki,bhkj->bhkij', X1, grad_l_wrt_Z1)\n",
      "                grad_W1 = torch.einsum('bhnk,bhkij->bhnij', torch.tril(\n",
      "                    ttt_lr_eta_mini_batch), grad_W1)\n",
      "                grad_W1 = grad_W1 + params_dict['W1_grad'].unsqueeze(2)\n",
      "                grad_b1 = torch.einsum('bhnk,bhki->bhni', torch.tril(\n",
      "                    ttt_lr_eta_mini_batch), grad_l_wrt_Z1)\n",
      "                grad_b1 = grad_b1 + params_dict['b1_grad']\n",
      "                W1_bar = W1_init.unsqueeze(2\n",
      "                    ) - grad_W1 * token_eta_mini_batch.unsqueeze(-1)\n",
      "                b1_bar = b1_init - grad_b1 * token_eta_mini_batch\n",
      "                Z1_bar = (XQ_mini_batch.unsqueeze(3) @ W1_bar).squeeze(3\n",
      "                    ) + b1_bar\n",
      "                W1_last = W1_bar[:, :, -1]\n",
      "                b1_last = b1_bar[:, :, -1:]\n",
      "                grad_W1_last = grad_W1[:, :, -1]\n",
      "                grad_b1_last = grad_b1[:, :, -1:]\n",
      "            Z1_bar = ln_fwd(Z1_bar, ln_weight, ln_bias)\n",
      "            XQW_mini_batch = XQ_mini_batch + Z1_bar\n",
      "            last_param_dict = {'W1_states': W1_last, 'b1_states': b1_last,\n",
      "                'W1_grad': grad_W1_last, 'b1_grad': grad_b1_last}\n",
      "            return last_param_dict, XQW_mini_batch\n",
      "        if last_mini_batch_params_dict is not None:\n",
      "            init_params_dict = last_mini_batch_params_dict\n",
      "        else:\n",
      "            init_params_dict = {'W1_states': torch.tile(self.W1.unsqueeze(0\n",
      "                ), dims=(B, 1, 1, 1)), 'b1_states': torch.tile(self.b1.\n",
      "                unsqueeze(0), dims=(B, 1, 1, 1))}\n",
      "            init_params_dict.update(W1_grad=torch.zeros_like(\n",
      "                init_params_dict['W1_states']))\n",
      "            init_params_dict.update(b1_grad=torch.zeros_like(\n",
      "                init_params_dict['b1_states']))\n",
      "        inputs = tree_map(lambda x: x.permute(2, 0, 1, 3, 4), inputs)\n",
      "        XQW_batch = torch.empty((num_mini_batch, B, self.num_heads,\n",
      "            mini_batch_size, self.head_dim), device=device, dtype=dtype)\n",
      "        batch_params_dict, XQW_batch = scan(compute_mini_batch,\n",
      "            init_params_dict, inputs, XQW_batch, self.\n",
      "            scan_checkpoint_group_size if self.training else 0)\n",
      "        XQW_batch = XQW_batch.permute(1, 0, 3, 2, 4)\n",
      "        XQW_batch = XQW_batch.reshape(B, L, self.width)\n",
      "        return XQW_batch, batch_params_dict\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor, position_ids: Optional[\n",
      "        torch.LongTensor]=None):\n",
      "        B, L = hidden_states.shape[:2]\n",
      "        reminder_len = L % self.mini_batch_size\n",
      "        num_mini_batch = L // self.mini_batch_size\n",
      "        last_mini_batch_params_dict = None\n",
      "        XQ, XK, XV = self.get_qkv_projections(hidden_states)\n",
      "        XQ = XQ.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        XK = XK.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        XV = XV.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        cos, sin = self.rotary_emb(XV, position_ids % self.mini_batch_size)\n",
      "        XQ, XK = permute_qk(XQ, XK)\n",
      "        XQ, XK = apply_rotary_pos_emb(XQ, XK, cos, sin)\n",
      "        XQ, XK = undo_permute_qk(XQ, XK)\n",
      "        output_hidden_states = []\n",
      "        if num_mini_batch > 0:\n",
      "            inputs = {'XQ': XQ[:, :, :num_mini_batch * self.mini_batch_size\n",
      "                ], 'XK': XK[:, :, :num_mini_batch * self.mini_batch_size],\n",
      "                'XV': XV[:, :, :num_mini_batch * self.mini_batch_size], 'X':\n",
      "                hidden_states[:, :num_mini_batch * self.mini_batch_size]}\n",
      "            output_mod, last_mini_batch_params_dict = self.ttt(self.\n",
      "                get_ttt_inputs(inputs, self.mini_batch_size),\n",
      "                mini_batch_size=self.mini_batch_size,\n",
      "                last_mini_batch_params_dict=last_mini_batch_params_dict)\n",
      "            output_hidden_states.append(output_mod)\n",
      "        if reminder_len > 0:\n",
      "            inputs = {'XQ': XQ[:, :, -reminder_len:], 'XK': XK[:, :, -\n",
      "                reminder_len:], 'XV': XV[:, :, -reminder_len:], 'X':\n",
      "                hidden_states[:, -reminder_len:]}\n",
      "            output_reminder, _ = self.ttt(self.get_ttt_inputs(inputs,\n",
      "                reminder_len), mini_batch_size=reminder_len,\n",
      "                last_mini_batch_params_dict=last_mini_batch_params_dict)\n",
      "            output_hidden_states.append(output_reminder)\n",
      "        output_hidden_states = torch.cat(output_hidden_states, dim=1)\n",
      "        output_hidden_states = self.post_norm(output_hidden_states)\n",
      "        output_hidden_states = self.o_proj(output_hidden_states)\n",
      "        return output_hidden_states\n",
      "\n",
      "\n",
      "class GAB(GABBase):\n",
      "    \"\"\"Generalized Autoregressive Block\n",
      "        Input:        X: (batch, seqlen, embed_dim)\n",
      "        Output:       Y: (batch, seqlen, embed_dim)\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, block_loc, device=None, dtype=None,\n",
      "        scan_checkpoint_group_size=4, conv_kernel=4, mini_batch_size=16,\n",
      "        rope_theta=10000.0, rms_norm_eps=1e-06, ttt_base_lr=1.0, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim, block_loc)\n",
      "        self.hidden_size = embed_dim\n",
      "        num_attention_heads = max(4, embed_dim // 64)\n",
      "        self.seq_modeling_block = TTTLinear(hidden_size=embed_dim,\n",
      "            num_attention_heads=num_attention_heads,\n",
      "            scan_checkpoint_group_size=scan_checkpoint_group_size,\n",
      "            conv_kernel=conv_kernel, mini_batch_size=mini_batch_size,\n",
      "            rope_theta=rope_theta, ttt_base_lr=ttt_base_lr)\n",
      "        self.mlp = SwiGluMLP(embed_dim, int(embed_dim * 2.5))\n",
      "        self.conv = Conv(embed_dim, conv_kernel, rms_norm_eps)\n",
      "        self.seq_norm = RMSNorm(embed_dim, eps=rms_norm_eps)\n",
      "        self.ffn_norm = RMSNorm(embed_dim, eps=rms_norm_eps)\n",
      "        self.seq_modeling_block = self.seq_modeling_block.to(device=device,\n",
      "            dtype=dtype)\n",
      "        self.mlp = self.mlp.to(device=device, dtype=dtype)\n",
      "        self.conv = self.conv.to(device=device, dtype=dtype)\n",
      "        self.seq_norm = self.seq_norm.to(device=device, dtype=dtype)\n",
      "        self.ffn_norm = self.ffn_norm.to(device=device, dtype=dtype)\n",
      "\n",
      "    def _forward(self, X, *Z, **intermediate_vars):\n",
      "        hidden_states = X\n",
      "        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n",
      "            =X.device).unsqueeze(0)\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.conv(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.seq_norm(hidden_states)\n",
      "        hidden_states = self.seq_modeling_block(hidden_states=hidden_states,\n",
      "            position_ids=position_ids)\n",
      "        hidden_states = residual + hidden_states\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.ffn_norm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "gab_config = {'scan_checkpoint_group_size': 0, 'conv_kernel': 4,\n",
      "    'mini_batch_size': 16, 'rope_theta': 10000.0, 'rms_norm_eps': 1e-06,\n",
      "    'ttt_base_lr': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "Code format is correct and reformatted.\n",
      "\n",
      "\n",
      "Warnings:\n",
      "\n",
      "The \"__init__\" method of \"GAB\" is missing the \"block_loc\" argument. Automatically added by the reformatter.\n",
      "\n",
      "The super().__init__(embed_dim, block_loc) call in GAB is force overwritten by the reformatter. It may cause error if you modified this line.\n",
      "\n",
      "The \"_forward\" method of \"GAB\" is missing the \"**intermediate_vars\" argument. Automatically adding the argument.\n",
      "\n",
      "The statement \"logger = logging.get_logger(__name__)\" is removed by the reformatter.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:415: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "<string>:162: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "<string>:425: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "from types import MethodType\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 64  # Increase the limit as needed\n",
    "\n",
    "\n",
    "# Redefining the necessary classes with torch imports\n",
    "\n",
    "\n",
    "class ModuleNode:\n",
    "    def __init__(self, name, graph_module=None):\n",
    "        self.name = name\n",
    "        self.graph_module = graph_module\n",
    "        self.children = []\n",
    "\n",
    "    def print_tree(self, indent=\"\"):\n",
    "        print(indent + self.name)\n",
    "        if self.graph_module:\n",
    "            print(indent + \"  (GraphModule captured)\")#, self.graph_module)\n",
    "        for child in self.children:\n",
    "            child.print_tree(indent + \"  \")\n",
    "\n",
    "@dataclass\n",
    "class BlockAnalysis:\n",
    "    root: ModuleNode\n",
    "    nodes: dict\n",
    "\n",
    "class BlockAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.module_tree_root = None\n",
    "        self.current_inputs = {}  # To store inputs for each module during forward pass\n",
    "        self.current_nodes = {}  # To store ModuleNode instances for each module by path\n",
    "\n",
    "    def track_input_wrapper(self, original_forward, module_path):\n",
    "        # Custom wrapper for the forward method to capture both positional and keyword arguments\n",
    "        def wrapped_forward(module_self, *inputs, **kwargs):\n",
    "            self.current_inputs[module_path] = (inputs, kwargs)\n",
    "            # Call the original forward method without re-binding `self`\n",
    "            return original_forward(*inputs, **kwargs)\n",
    "\n",
    "        return wrapped_forward\n",
    "\n",
    "    def wrap_forward_methods(self, model,wrapper):\n",
    "        # Replace the forward method of each submodule with the custom wrapped version\n",
    "        for module_path, module in self._get_full_module_paths(model):\n",
    "            original_forward = module.forward\n",
    "            module.forward = MethodType(wrapper(original_forward, module_path), module)\n",
    "\n",
    "    def _get_full_module_paths(self, model):\n",
    "        # Recursively generate the full path for each module in the model\n",
    "        module_paths = []\n",
    "\n",
    "        def recursive_collect_modules(parent, prefix):\n",
    "            for name, module in parent.named_children():\n",
    "                full_path = f\"{prefix}.{name}\" if prefix else name\n",
    "                module_paths.append((full_path, module))\n",
    "                recursive_collect_modules(module, full_path)\n",
    "\n",
    "        recursive_collect_modules(model, \"\")\n",
    "        return module_paths\n",
    "\n",
    "    def analyze_submodule(self, module_path, module):\n",
    "        # Retrieve the inputs and kwargs for this module captured during the forward pass\n",
    "        inputs, kwargs = self.current_inputs.get(module_path, (None, None))\n",
    "\n",
    "        module = copy.deepcopy(module)\n",
    "        if inputs is None:\n",
    "            return None\n",
    "        if kwargs:\n",
    "            for key in kwargs:\n",
    "                try:\n",
    "                    kwargs[key] = kwargs[key].detach()\n",
    "                except:\n",
    "                    pass\n",
    "            module.forward=partial(module.forward,**kwargs)\n",
    "\n",
    "        # Trace the current module with the captured inputs and kwargs\n",
    "        if isinstance(inputs,tuple):\n",
    "            new_inputs = []\n",
    "            for inp in inputs:\n",
    "                try:\n",
    "                    new_inputs.append(inp.detach())\n",
    "                except:\n",
    "                    new_inputs.append(inp)\n",
    "            inputs = tuple(new_inputs)\n",
    "        else:\n",
    "            try:\n",
    "                inputs = inputs.detach()\n",
    "            except:\n",
    "                pass\n",
    "        traced_module = torch.jit.trace(module, inputs)\n",
    "        # Create a ModuleNode for the current module\n",
    "        node = ModuleNode(module_path, traced_module)\n",
    "        self.current_nodes[module_path] = node\n",
    "\n",
    "        # Recursively trace submodules\n",
    "        for name, submodule in module.named_children():\n",
    "            child_path = f\"{module_path}.{name}\"\n",
    "            child_node = self.analyze_submodule(child_path.replace('root.',''), submodule)\n",
    "            if child_node is not None:\n",
    "                node.children.append(child_node)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def analyze(self, model, cfg):\n",
    "        # Wrap the forward methods to capture both positional and keyword arguments\n",
    "        self.current_inputs = {}\n",
    "        self.current_nodes = {}\n",
    "        model_wrap = copy.deepcopy(model)   \n",
    "        self.wrap_forward_methods(model_wrap,self.track_input_wrapper)\n",
    "\n",
    "        # Run the model with an example input\n",
    "        input_tensor = torch.randn(2, 100, cfg.d_model)\n",
    "        model_wrap(input_tensor)  # This will trigger the wrapped forward methods and capture inputs\n",
    "        del model_wrap\n",
    "        \n",
    "        # Start with the root module and analyze it recursively\n",
    "        self.current_inputs['root'] = (input_tensor, None)\n",
    "        self.module_tree_root = self.analyze_submodule('root', model)\n",
    "\n",
    "        return BlockAnalysis(self.module_tree_root, self.current_nodes)\n",
    "\n",
    "# This update removes the re-binding of `self` and directly calls the original forward method with the provided inputs and kwargs.\n",
    "\n",
    "# Example usage:\n",
    "gab,cfg = load_gab('ttt')\n",
    "analyzer = BlockAnalyzer()\n",
    "analysis = analyzer.analyze(gab, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def forward(self) -> Tensor:\\n  o_proj = self.o_proj\\n  post_norm = self.post_norm\\n  ttt_norm_bias = self.ttt_norm_bias\\n  ttt_norm_weight = self.ttt_norm_weight\\n  b1 = self.b1\\n  W1 = self.W1\\n  learnable_token_idx = self.learnable_token_idx\\n  token_idx = self.token_idx\\n  learnable_ttt_lr_bias = self.learnable_ttt_lr_bias\\n  learnable_ttt_lr_weight = self.learnable_ttt_lr_weight\\n  rotary_emb = self.rotary_emb\\n  v_proj = self.v_proj\\n  k_proj = self.k_proj\\n  q_proj = self.q_proj\\n  B = ops.prim.NumToTensor(torch.size(CONSTANTS.c0, 0))\\n  _0 = int(B)\\n  _1 = int(B)\\n  _2 = int(B)\\n  L = ops.prim.NumToTensor(torch.size(CONSTANTS.c0, 1))\\n  _3 = int(L)\\n  _4 = int(L)\\n  _5 = int(L)\\n  reminder_len = torch.remainder(L, 16)\\n  _6 = int(reminder_len)\\n  _7 = int(reminder_len)\\n  _8 = int(reminder_len)\\n  _9 = int(reminder_len)\\n  _10 = int(reminder_len)\\n  _11 = int(reminder_len)\\n  _12 = int(reminder_len)\\n  _13 = int(reminder_len)\\n  num_mini_batch = torch.floor_divide(L, CONSTANTS.c1)\\n  _14 = (q_proj).forward(CONSTANTS.c0, )\\n  _15 = (k_proj).forward(CONSTANTS.c0, )\\n  _16 = (v_proj).forward(CONSTANTS.c0, )\\n  q = torch.transpose(torch.reshape(_14, [_2, _5, 4, 32]), 1, 2)\\n  k = torch.transpose(torch.reshape(_15, [_1, _4, 4, 32]), 1, 2)\\n  XV = torch.transpose(torch.reshape(_16, [_0, _3, 4, 32]), 1, 2)\\n  position_ids = torch.remainder(CONSTANTS.c2, 16)\\n  _17, _18, = (rotary_emb).forward(position_ids, )\\n  bsz = ops.prim.NumToTensor(torch.size(q, 0))\\n  _19 = int(bsz)\\n  _20 = int(bsz)\\n  _21 = int(bsz)\\n  _22 = int(bsz)\\n  num_head = ops.prim.NumToTensor(torch.size(q, 1))\\n  _23 = int(num_head)\\n  _24 = int(num_head)\\n  _25 = int(num_head)\\n  _26 = int(num_head)\\n  seq_len = ops.prim.NumToTensor(torch.size(q, 2))\\n  _27 = int(seq_len)\\n  _28 = int(seq_len)\\n  _29 = int(seq_len)\\n  _30 = int(seq_len)\\n  head_dim = ops.prim.NumToTensor(torch.size(q, 3))\\n  _31 = int(head_dim)\\n  _32 = int(head_dim)\\n  _33 = torch.floor_divide(head_dim, CONSTANTS.c3)\\n  _34 = torch.reshape(q, [_22, _26, _30, int(_33), 2])\\n  q0 = torch.reshape(torch.transpose(_34, 3, 4), [_21, _25, _29, _32])\\n  _35 = torch.floor_divide(head_dim, CONSTANTS.c3)\\n  _36 = torch.reshape(k, [_20, _24, _28, int(_35), 2])\\n  k0 = torch.reshape(torch.transpose(_36, 3, 4), [_19, _23, _27, _31])\\n  cos = torch.unsqueeze(_17, 1)\\n  sin = torch.unsqueeze(_18, 1)\\n  _37 = torch.mul(q0, cos)\\n  _38 = ops.prim.NumToTensor(torch.size(q0, 3))\\n  _39 = int(torch.floor_divide(_38, CONSTANTS.c3))\\n  x1 = torch.slice(q0, 3, 0, _39)\\n  _40 = ops.prim.NumToTensor(torch.size(q0, 3))\\n  _41 = int(torch.floor_divide(_40, CONSTANTS.c3))\\n  x2 = torch.slice(q0, 3, _41, 9223372036854775807)\\n  _42 = torch.mul(torch.cat([torch.neg(x2), x1], -1), sin)\\n  q1 = torch.add(_37, _42)\\n  _43 = torch.mul(k0, cos)\\n  _44 = ops.prim.NumToTensor(torch.size(k0, 3))\\n  _45 = int(torch.floor_divide(_44, CONSTANTS.c3))\\n  x10 = torch.slice(k0, 3, 0, _45)\\n  _46 = ops.prim.NumToTensor(torch.size(k0, 3))\\n  _47 = int(torch.floor_divide(_46, CONSTANTS.c3))\\n  x20 = torch.slice(k0, 3, _47, 9223372036854775807)\\n  _48 = torch.mul(torch.cat([torch.neg(x20), x10], -1), sin)\\n  k1 = torch.add(_43, _48)\\n  bsz0 = ops.prim.NumToTensor(torch.size(q1, 0))\\n  _49 = int(bsz0)\\n  _50 = int(bsz0)\\n  _51 = int(bsz0)\\n  _52 = int(bsz0)\\n  num_head0 = ops.prim.NumToTensor(torch.size(q1, 1))\\n  _53 = int(num_head0)\\n  _54 = int(num_head0)\\n  _55 = int(num_head0)\\n  _56 = int(num_head0)\\n  seq_len0 = ops.prim.NumToTensor(torch.size(q1, 2))\\n  _57 = int(seq_len0)\\n  _58 = int(seq_len0)\\n  _59 = int(seq_len0)\\n  _60 = int(seq_len0)\\n  head_dim0 = ops.prim.NumToTensor(torch.size(q1, 3))\\n  _61 = int(head_dim0)\\n  _62 = int(head_dim0)\\n  _63 = torch.floor_divide(head_dim0, CONSTANTS.c3)\\n  _64 = torch.reshape(q1, [_52, _56, _60, 2, int(_63)])\\n  XQ = torch.reshape(torch.transpose(_64, 3, 4), [_51, _55, _59, _62])\\n  _65 = torch.floor_divide(head_dim0, CONSTANTS.c3)\\n  _66 = torch.reshape(k1, [_50, _54, _58, 2, int(_65)])\\n  XK = torch.reshape(torch.transpose(_66, 3, 4), [_49, _53, _57, _61])\\n  _67 = torch.mul(num_mini_batch, CONSTANTS.c1)\\n  _68 = int(_67)\\n  _69 = torch.slice(XQ, 0, 0, 9223372036854775807)\\n  _70 = torch.slice(_69, 1, 0, 9223372036854775807)\\n  XQ0 = torch.slice(_70, 2, 0, _68)\\n  _71 = torch.mul(num_mini_batch, CONSTANTS.c1)\\n  _72 = int(_71)\\n  _73 = torch.slice(XK, 0, 0, 9223372036854775807)\\n  _74 = torch.slice(_73, 1, 0, 9223372036854775807)\\n  XK0 = torch.slice(_74, 2, 0, _72)\\n  _75 = torch.mul(num_mini_batch, CONSTANTS.c1)\\n  _76 = int(_75)\\n  _77 = torch.slice(XV, 0, 0, 9223372036854775807)\\n  _78 = torch.slice(_77, 1, 0, 9223372036854775807)\\n  XV0 = torch.slice(_78, 2, 0, _76)\\n  _79 = torch.mul(num_mini_batch, CONSTANTS.c1)\\n  _80 = int(_79)\\n  _81 = torch.slice(CONSTANTS.c0, 0, 0, 9223372036854775807)\\n  X = torch.slice(_81, 1, 0, _80)\\n  B0 = ops.prim.NumToTensor(torch.size(X, 0))\\n  _82 = int(B0)\\n  _83 = int(B0)\\n  _84 = int(B0)\\n  _85 = int(B0)\\n  _86 = ops.prim.NumToTensor(torch.size(X, 1))\\n  num_mini_batch0 = torch.floor_divide(_86, CONSTANTS.c1)\\n  X0 = torch.reshape(X, [_85, int(num_mini_batch0), 16, 128])\\n  _87 = int(torch.floor_divide(_86, CONSTANTS.c1))\\n  x = torch.reshape(XQ0, [_84, 4, _87, 16, 32])\\n  _88 = int(torch.floor_divide(_86, CONSTANTS.c1))\\n  x0 = torch.reshape(XK0, [_83, 4, _88, 16, 32])\\n  _89 = int(torch.floor_divide(_86, CONSTANTS.c1))\\n  x3 = torch.reshape(XV0, [_82, 4, _89, 16, 32])\\n  _90 = torch.einsum(\"bnkc,hdc->bhnkd\", [X0, learnable_ttt_lr_weight])\\n  _91 = torch.reshape(learnable_ttt_lr_bias, [1, -1, 1, 1, 1])\\n  input = torch.add(_90, _91)\\n  ttt_lr = torch.sigmoid(input)\\n  ttt_lr0 = torch.permute(ttt_lr, [0, 1, 2, 4, 3])\\n  ttt_lr_eta = torch.div(torch.mul(ttt_lr0, CONSTANTS.c4), CONSTANTS.c5)\\n  token_idx0 = torch.add(token_idx, learnable_token_idx)\\n  token_idx1 = torch.slice(token_idx0, 0, 0, 16)\\n  token_idx2 = torch.clamp_min(token_idx1, 0.)\\n  _92 = torch.reshape(token_idx2, [1, 1, 1, 16, 1])\\n  _93 = ops.prim.NumToTensor(torch.size(X0, 0))\\n  _94 = int(_93)\\n  _95 = ops.prim.NumToTensor(torch.size(X0, 1))\\n  token_eta = torch.broadcast_to(_92, [_94, 4, int(_95), 16, 1])\\n  x4 = torch.mul(token_eta, ttt_lr_eta)\\n  B1 = ops.prim.NumToTensor(torch.size(x3, 0))\\n  _96 = int(B1)\\n  _97 = int(B1)\\n  _98 = int(B1)\\n  _99 = int(B1)\\n  num_mini_batch1 = ops.prim.NumToTensor(torch.size(x3, 2))\\n  _100 = int(num_mini_batch1)\\n  _101 = ops.prim.NumToTensor(torch.size(x3, 2))\\n  _102 = ops.prim.NumToTensor(torch.size(x3, 3))\\n  L0 = torch.mul(_101, _102)\\n  _103 = int(L0)\\n  W1_init = torch.tile(torch.unsqueeze(W1, 0), [_99, 1, 1, 1])\\n  b1_init = torch.tile(torch.unsqueeze(b1, 0), [_98, 1, 1, 1])\\n  tensor = torch.permute(x, [2, 0, 1, 3, 4])\\n  tensor0 = torch.permute(x0, [2, 0, 1, 3, 4])\\n  tensor1 = torch.permute(x3, [2, 0, 1, 3, 4])\\n  tensor2 = torch.permute(x4, [2, 0, 1, 3, 4])\\n  out = torch.empty([_100, _97, 4, 16, 32], dtype=6, layout=None, device=torch.device(\"cpu\"), pin_memory=False)\\n  XQ_mini_batch = torch.select(tensor, 0, 0)\\n  XK_mini_batch = torch.select(tensor0, 0, 0)\\n  XV_mini_batch = torch.select(tensor1, 0, 0)\\n  eta_mini_batch = torch.select(tensor2, 0, 0)\\n  x5 = torch.add(torch.matmul(XK_mini_batch, W1_init), b1_init)\\n  l2_target = torch.sub(XV_mini_batch, XK_mini_batch)\\n  gamma = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D = ops.prim.NumToTensor(torch.size(x5, 3))\\n  mu = torch.mean(x5, [-1], True)\\n  var = torch.var(x5, [-1], False, True)\\n  std = torch.sqrt(torch.add(var, CONSTANTS.c6))\\n  x_hat = torch.div(torch.sub(x5, mu), std)\\n  y = torch.add(torch.mul(gamma, x_hat), beta)\\n  grad_output = torch.sub(y, l2_target)\\n  grad_x_hat = torch.mul(grad_output, gamma)\\n  _104 = torch.mul(torch.reciprocal(D), CONSTANTS.c4)\\n  _105 = torch.sub(torch.mul(D, grad_x_hat), torch.sum(grad_x_hat, [-1], True))\\n  _106 = torch.sum(torch.mul(grad_x_hat, x_hat), [-1], True)\\n  _107 = torch.sub(_105, torch.mul(x_hat, _106))\\n  grad_l_wrt_Z1 = torch.div(torch.mul(_104, _107), std)\\n  _108 = torch.transpose(XK_mini_batch, -2, -1)\\n  Attn1 = torch.tril(torch.matmul(XQ_mini_batch, _108))\\n  _109 = torch.matmul(torch.tril(eta_mini_batch), grad_l_wrt_Z1)\\n  b1_bar = torch.sub(b1_init, _109)\\n  _110 = torch.matmul(XQ_mini_batch, W1_init)\\n  _111 = torch.matmul(torch.mul(eta_mini_batch, Attn1), grad_l_wrt_Z1)\\n  x6 = torch.add(torch.sub(_110, _111), b1_bar)\\n  _112 = torch.slice(eta_mini_batch, 0, 0, 9223372036854775807)\\n  _113 = torch.slice(_112, 1, 0, 9223372036854775807)\\n  _114 = torch.slice(torch.select(_113, 2, -1), 2, 0, 9223372036854775807)\\n  last_eta_mini_batch = torch.unsqueeze(_114, 3)\\n  _115 = torch.mul(last_eta_mini_batch, XK_mini_batch)\\n  _116 = torch.matmul(torch.transpose(_115, -1, -2), grad_l_wrt_Z1)\\n  W1_last = torch.sub(W1_init, _116)\\n  _117 = torch.mul(last_eta_mini_batch, grad_l_wrt_Z1)\\n  b1_last = torch.sub(b1_init, torch.sum(_117, [-2], True))\\n  mu0 = torch.mean(x6, [-1], True)\\n  var0 = torch.var(x6, [-1], False, True)\\n  std0 = torch.sqrt(torch.add(var0, CONSTANTS.c6))\\n  x_hat0 = torch.div(torch.sub(x6, mu0), std0)\\n  Z1_bar = torch.add(torch.mul(gamma, x_hat0), beta)\\n  y0 = torch.add(XQ_mini_batch, Z1_bar)\\n  _118 = torch.copy_(torch.select(out, 0, 0), y0)\\n  XQ_mini_batch0 = torch.select(tensor, 0, 1)\\n  XK_mini_batch0 = torch.select(tensor0, 0, 1)\\n  XV_mini_batch0 = torch.select(tensor1, 0, 1)\\n  eta_mini_batch0 = torch.select(tensor2, 0, 1)\\n  x7 = torch.add(torch.matmul(XK_mini_batch0, W1_last), b1_last)\\n  l2_target0 = torch.sub(XV_mini_batch0, XK_mini_batch0)\\n  gamma0 = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta0 = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D0 = ops.prim.NumToTensor(torch.size(x7, 3))\\n  mu1 = torch.mean(x7, [-1], True)\\n  var1 = torch.var(x7, [-1], False, True)\\n  std1 = torch.sqrt(torch.add(var1, CONSTANTS.c6))\\n  x_hat1 = torch.div(torch.sub(x7, mu1), std1)\\n  y1 = torch.add(torch.mul(gamma0, x_hat1), beta0)\\n  grad_output0 = torch.sub(y1, l2_target0)\\n  grad_x_hat0 = torch.mul(grad_output0, gamma0)\\n  _119 = torch.mul(torch.reciprocal(D0), CONSTANTS.c4)\\n  _120 = torch.sub(torch.mul(D0, grad_x_hat0), torch.sum(grad_x_hat0, [-1], True))\\n  _121 = torch.sum(torch.mul(grad_x_hat0, x_hat1), [-1], True)\\n  _122 = torch.sub(_120, torch.mul(x_hat1, _121))\\n  grad_l_wrt_Z10 = torch.div(torch.mul(_119, _122), std1)\\n  _123 = torch.transpose(XK_mini_batch0, -2, -1)\\n  Attn10 = torch.tril(torch.matmul(XQ_mini_batch0, _123))\\n  _124 = torch.matmul(torch.tril(eta_mini_batch0), grad_l_wrt_Z10)\\n  b1_bar0 = torch.sub(b1_last, _124)\\n  _125 = torch.matmul(XQ_mini_batch0, W1_last)\\n  _126 = torch.matmul(torch.mul(eta_mini_batch0, Attn10), grad_l_wrt_Z10)\\n  x8 = torch.add(torch.sub(_125, _126), b1_bar0)\\n  _127 = torch.slice(eta_mini_batch0, 0, 0, 9223372036854775807)\\n  _128 = torch.slice(_127, 1, 0, 9223372036854775807)\\n  _129 = torch.slice(torch.select(_128, 2, -1), 2, 0, 9223372036854775807)\\n  last_eta_mini_batch0 = torch.unsqueeze(_129, 3)\\n  _130 = torch.mul(last_eta_mini_batch0, XK_mini_batch0)\\n  _131 = torch.matmul(torch.transpose(_130, -1, -2), grad_l_wrt_Z10)\\n  W1_last0 = torch.sub(W1_last, _131)\\n  _132 = torch.mul(last_eta_mini_batch0, grad_l_wrt_Z10)\\n  b1_last0 = torch.sub(b1_last, torch.sum(_132, [-2], True))\\n  mu2 = torch.mean(x8, [-1], True)\\n  var2 = torch.var(x8, [-1], False, True)\\n  std2 = torch.sqrt(torch.add(var2, CONSTANTS.c6))\\n  x_hat2 = torch.div(torch.sub(x8, mu2), std2)\\n  Z1_bar0 = torch.add(torch.mul(gamma0, x_hat2), beta0)\\n  y2 = torch.add(XQ_mini_batch0, Z1_bar0)\\n  _133 = torch.copy_(torch.select(out, 0, 1), y2)\\n  XQ_mini_batch1 = torch.select(tensor, 0, 2)\\n  XK_mini_batch1 = torch.select(tensor0, 0, 2)\\n  XV_mini_batch1 = torch.select(tensor1, 0, 2)\\n  eta_mini_batch1 = torch.select(tensor2, 0, 2)\\n  _134 = torch.matmul(XK_mini_batch1, W1_last0)\\n  x9 = torch.add(_134, b1_last0)\\n  l2_target1 = torch.sub(XV_mini_batch1, XK_mini_batch1)\\n  gamma1 = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta1 = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D1 = ops.prim.NumToTensor(torch.size(x9, 3))\\n  mu3 = torch.mean(x9, [-1], True)\\n  var3 = torch.var(x9, [-1], False, True)\\n  std3 = torch.sqrt(torch.add(var3, CONSTANTS.c6))\\n  x_hat3 = torch.div(torch.sub(x9, mu3), std3)\\n  y3 = torch.add(torch.mul(gamma1, x_hat3), beta1)\\n  grad_output1 = torch.sub(y3, l2_target1)\\n  grad_x_hat1 = torch.mul(grad_output1, gamma1)\\n  _135 = torch.mul(torch.reciprocal(D1), CONSTANTS.c4)\\n  _136 = torch.sub(torch.mul(D1, grad_x_hat1), torch.sum(grad_x_hat1, [-1], True))\\n  _137 = torch.sum(torch.mul(grad_x_hat1, x_hat3), [-1], True)\\n  _138 = torch.sub(_136, torch.mul(x_hat3, _137))\\n  grad_l_wrt_Z11 = torch.div(torch.mul(_135, _138), std3)\\n  _139 = torch.transpose(XK_mini_batch1, -2, -1)\\n  Attn11 = torch.tril(torch.matmul(XQ_mini_batch1, _139))\\n  _140 = torch.matmul(torch.tril(eta_mini_batch1), grad_l_wrt_Z11)\\n  b1_bar1 = torch.sub(b1_last0, _140)\\n  _141 = torch.matmul(XQ_mini_batch1, W1_last0)\\n  _142 = torch.matmul(torch.mul(eta_mini_batch1, Attn11), grad_l_wrt_Z11)\\n  x11 = torch.add(torch.sub(_141, _142), b1_bar1)\\n  _143 = torch.slice(eta_mini_batch1, 0, 0, 9223372036854775807)\\n  _144 = torch.slice(_143, 1, 0, 9223372036854775807)\\n  _145 = torch.slice(torch.select(_144, 2, -1), 2, 0, 9223372036854775807)\\n  last_eta_mini_batch1 = torch.unsqueeze(_145, 3)\\n  _146 = torch.mul(last_eta_mini_batch1, XK_mini_batch1)\\n  _147 = torch.matmul(torch.transpose(_146, -1, -2), grad_l_wrt_Z11)\\n  W1_last1 = torch.sub(W1_last0, _147)\\n  _148 = torch.mul(last_eta_mini_batch1, grad_l_wrt_Z11)\\n  b1_last1 = torch.sub(b1_last0, torch.sum(_148, [-2], True))\\n  mu4 = torch.mean(x11, [-1], True)\\n  var4 = torch.var(x11, [-1], False, True)\\n  std4 = torch.sqrt(torch.add(var4, CONSTANTS.c6))\\n  x_hat4 = torch.div(torch.sub(x11, mu4), std4)\\n  Z1_bar1 = torch.add(torch.mul(gamma1, x_hat4), beta1)\\n  y4 = torch.add(XQ_mini_batch1, Z1_bar1)\\n  _149 = torch.copy_(torch.select(out, 0, 2), y4)\\n  XQ_mini_batch2 = torch.select(tensor, 0, 3)\\n  XK_mini_batch2 = torch.select(tensor0, 0, 3)\\n  XV_mini_batch2 = torch.select(tensor1, 0, 3)\\n  eta_mini_batch2 = torch.select(tensor2, 0, 3)\\n  _150 = torch.matmul(XK_mini_batch2, W1_last1)\\n  x12 = torch.add(_150, b1_last1)\\n  l2_target2 = torch.sub(XV_mini_batch2, XK_mini_batch2)\\n  gamma2 = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta2 = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D2 = ops.prim.NumToTensor(torch.size(x12, 3))\\n  mu5 = torch.mean(x12, [-1], True)\\n  var5 = torch.var(x12, [-1], False, True)\\n  std5 = torch.sqrt(torch.add(var5, CONSTANTS.c6))\\n  x_hat5 = torch.div(torch.sub(x12, mu5), std5)\\n  y5 = torch.add(torch.mul(gamma2, x_hat5), beta2)\\n  grad_output2 = torch.sub(y5, l2_target2)\\n  grad_x_hat2 = torch.mul(grad_output2, gamma2)\\n  _151 = torch.mul(torch.reciprocal(D2), CONSTANTS.c4)\\n  _152 = torch.sub(torch.mul(D2, grad_x_hat2), torch.sum(grad_x_hat2, [-1], True))\\n  _153 = torch.sum(torch.mul(grad_x_hat2, x_hat5), [-1], True)\\n  _154 = torch.sub(_152, torch.mul(x_hat5, _153))\\n  grad_l_wrt_Z12 = torch.div(torch.mul(_151, _154), std5)\\n  _155 = torch.transpose(XK_mini_batch2, -2, -1)\\n  Attn12 = torch.tril(torch.matmul(XQ_mini_batch2, _155))\\n  _156 = torch.matmul(torch.tril(eta_mini_batch2), grad_l_wrt_Z12)\\n  b1_bar2 = torch.sub(b1_last1, _156)\\n  _157 = torch.matmul(XQ_mini_batch2, W1_last1)\\n  _158 = torch.matmul(torch.mul(eta_mini_batch2, Attn12), grad_l_wrt_Z12)\\n  x13 = torch.add(torch.sub(_157, _158), b1_bar2)\\n  _159 = torch.slice(eta_mini_batch2, 0, 0, 9223372036854775807)\\n  _160 = torch.slice(_159, 1, 0, 9223372036854775807)\\n  _161 = torch.slice(torch.select(_160, 2, -1), 2, 0, 9223372036854775807)\\n  last_eta_mini_batch2 = torch.unsqueeze(_161, 3)\\n  _162 = torch.mul(last_eta_mini_batch2, XK_mini_batch2)\\n  _163 = torch.matmul(torch.transpose(_162, -1, -2), grad_l_wrt_Z12)\\n  W1_last2 = torch.sub(W1_last1, _163)\\n  _164 = torch.mul(last_eta_mini_batch2, grad_l_wrt_Z12)\\n  b1_last2 = torch.sub(b1_last1, torch.sum(_164, [-2], True))\\n  mu6 = torch.mean(x13, [-1], True)\\n  var6 = torch.var(x13, [-1], False, True)\\n  std6 = torch.sqrt(torch.add(var6, CONSTANTS.c6))\\n  x_hat6 = torch.div(torch.sub(x13, mu6), std6)\\n  Z1_bar2 = torch.add(torch.mul(gamma2, x_hat6), beta2)\\n  y6 = torch.add(XQ_mini_batch2, Z1_bar2)\\n  _165 = torch.copy_(torch.select(out, 0, 3), y6)\\n  XQ_mini_batch3 = torch.select(tensor, 0, 4)\\n  XK_mini_batch3 = torch.select(tensor0, 0, 4)\\n  XV_mini_batch3 = torch.select(tensor1, 0, 4)\\n  eta_mini_batch3 = torch.select(tensor2, 0, 4)\\n  _166 = torch.matmul(XK_mini_batch3, W1_last2)\\n  x14 = torch.add(_166, b1_last2)\\n  l2_target3 = torch.sub(XV_mini_batch3, XK_mini_batch3)\\n  gamma3 = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta3 = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D3 = ops.prim.NumToTensor(torch.size(x14, 3))\\n  mu7 = torch.mean(x14, [-1], True)\\n  var7 = torch.var(x14, [-1], False, True)\\n  std7 = torch.sqrt(torch.add(var7, CONSTANTS.c6))\\n  x_hat7 = torch.div(torch.sub(x14, mu7), std7)\\n  y7 = torch.add(torch.mul(gamma3, x_hat7), beta3)\\n  grad_output3 = torch.sub(y7, l2_target3)\\n  grad_x_hat3 = torch.mul(grad_output3, gamma3)\\n  _167 = torch.mul(torch.reciprocal(D3), CONSTANTS.c4)\\n  _168 = torch.sub(torch.mul(D3, grad_x_hat3), torch.sum(grad_x_hat3, [-1], True))\\n  _169 = torch.sum(torch.mul(grad_x_hat3, x_hat7), [-1], True)\\n  _170 = torch.sub(_168, torch.mul(x_hat7, _169))\\n  grad_l_wrt_Z13 = torch.div(torch.mul(_167, _170), std7)\\n  _171 = torch.transpose(XK_mini_batch3, -2, -1)\\n  Attn13 = torch.tril(torch.matmul(XQ_mini_batch3, _171))\\n  _172 = torch.matmul(torch.tril(eta_mini_batch3), grad_l_wrt_Z13)\\n  b1_bar3 = torch.sub(b1_last2, _172)\\n  _173 = torch.matmul(XQ_mini_batch3, W1_last2)\\n  _174 = torch.matmul(torch.mul(eta_mini_batch3, Attn13), grad_l_wrt_Z13)\\n  x15 = torch.add(torch.sub(_173, _174), b1_bar3)\\n  _175 = torch.slice(eta_mini_batch3, 0, 0, 9223372036854775807)\\n  _176 = torch.slice(_175, 1, 0, 9223372036854775807)\\n  _177 = torch.slice(torch.select(_176, 2, -1), 2, 0, 9223372036854775807)\\n  last_eta_mini_batch3 = torch.unsqueeze(_177, 3)\\n  _178 = torch.mul(last_eta_mini_batch3, XK_mini_batch3)\\n  _179 = torch.matmul(torch.transpose(_178, -1, -2), grad_l_wrt_Z13)\\n  W1_last3 = torch.sub(W1_last2, _179)\\n  _180 = torch.mul(last_eta_mini_batch3, grad_l_wrt_Z13)\\n  b1_last3 = torch.sub(b1_last2, torch.sum(_180, [-2], True))\\n  mu8 = torch.mean(x15, [-1], True)\\n  var8 = torch.var(x15, [-1], False, True)\\n  std8 = torch.sqrt(torch.add(var8, CONSTANTS.c6))\\n  x_hat8 = torch.div(torch.sub(x15, mu8), std8)\\n  Z1_bar3 = torch.add(torch.mul(gamma3, x_hat8), beta3)\\n  y8 = torch.add(XQ_mini_batch3, Z1_bar3)\\n  _181 = torch.copy_(torch.select(out, 0, 4), y8)\\n  XQ_mini_batch4 = torch.select(tensor, 0, 5)\\n  XK_mini_batch4 = torch.select(tensor0, 0, 5)\\n  XV_mini_batch4 = torch.select(tensor1, 0, 5)\\n  eta_mini_batch4 = torch.select(tensor2, 0, 5)\\n  _182 = torch.matmul(XK_mini_batch4, W1_last3)\\n  x16 = torch.add(_182, b1_last3)\\n  l2_target4 = torch.sub(XV_mini_batch4, XK_mini_batch4)\\n  gamma4 = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta4 = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D4 = ops.prim.NumToTensor(torch.size(x16, 3))\\n  mu9 = torch.mean(x16, [-1], True)\\n  var9 = torch.var(x16, [-1], False, True)\\n  std9 = torch.sqrt(torch.add(var9, CONSTANTS.c6))\\n  x_hat9 = torch.div(torch.sub(x16, mu9), std9)\\n  y9 = torch.add(torch.mul(gamma4, x_hat9), beta4)\\n  grad_output4 = torch.sub(y9, l2_target4)\\n  grad_x_hat4 = torch.mul(grad_output4, gamma4)\\n  _183 = torch.mul(torch.reciprocal(D4), CONSTANTS.c4)\\n  _184 = torch.sub(torch.mul(D4, grad_x_hat4), torch.sum(grad_x_hat4, [-1], True))\\n  _185 = torch.sum(torch.mul(grad_x_hat4, x_hat9), [-1], True)\\n  _186 = torch.sub(_184, torch.mul(x_hat9, _185))\\n  grad_l_wrt_Z14 = torch.div(torch.mul(_183, _186), std9)\\n  _187 = torch.transpose(XK_mini_batch4, -2, -1)\\n  Attn14 = torch.tril(torch.matmul(XQ_mini_batch4, _187))\\n  _188 = torch.matmul(torch.tril(eta_mini_batch4), grad_l_wrt_Z14)\\n  b1_bar4 = torch.sub(b1_last3, _188)\\n  _189 = torch.matmul(XQ_mini_batch4, W1_last3)\\n  _190 = torch.matmul(torch.mul(eta_mini_batch4, Attn14), grad_l_wrt_Z14)\\n  x17 = torch.add(torch.sub(_189, _190), b1_bar4)\\n  _191 = torch.slice(eta_mini_batch4, 0, 0, 9223372036854775807)\\n  _192 = torch.slice(_191, 1, 0, 9223372036854775807)\\n  _193 = torch.slice(torch.select(_192, 2, -1), 2, 0, 9223372036854775807)\\n  last_eta_mini_batch4 = torch.unsqueeze(_193, 3)\\n  _194 = torch.mul(last_eta_mini_batch4, XK_mini_batch4)\\n  _195 = torch.matmul(torch.transpose(_194, -1, -2), grad_l_wrt_Z14)\\n  W1_last4 = torch.sub(W1_last3, _195)\\n  _196 = torch.mul(last_eta_mini_batch4, grad_l_wrt_Z14)\\n  b1_last4 = torch.sub(b1_last3, torch.sum(_196, [-2], True))\\n  mu10 = torch.mean(x17, [-1], True)\\n  var10 = torch.var(x17, [-1], False, True)\\n  std10 = torch.sqrt(torch.add(var10, CONSTANTS.c6))\\n  x_hat10 = torch.div(torch.sub(x17, mu10), std10)\\n  Z1_bar4 = torch.add(torch.mul(gamma4, x_hat10), beta4)\\n  y10 = torch.add(XQ_mini_batch4, Z1_bar4)\\n  _197 = torch.copy_(torch.select(out, 0, 5), y10)\\n  XQW_batch = torch.permute(out, [1, 0, 3, 2, 4])\\n  output_mod = torch.reshape(XQW_batch, [_96, _103, 128])\\n  _198 = int(torch.neg(reminder_len))\\n  _199 = torch.slice(XQ, 0, 0, 9223372036854775807)\\n  _200 = torch.slice(_199, 1, 0, 9223372036854775807)\\n  XQ1 = torch.slice(_200, 2, _198, 9223372036854775807)\\n  _201 = int(torch.neg(reminder_len))\\n  _202 = torch.slice(XK, 0, 0, 9223372036854775807)\\n  _203 = torch.slice(_202, 1, 0, 9223372036854775807)\\n  XK1 = torch.slice(_203, 2, _201, 9223372036854775807)\\n  _204 = int(torch.neg(reminder_len))\\n  _205 = torch.slice(XV, 0, 0, 9223372036854775807)\\n  _206 = torch.slice(_205, 1, 0, 9223372036854775807)\\n  XV1 = torch.slice(_206, 2, _204, 9223372036854775807)\\n  _207 = int(torch.neg(reminder_len))\\n  _208 = torch.slice(CONSTANTS.c0, 0, 0, 9223372036854775807)\\n  X1 = torch.slice(_208, 1, _207, 9223372036854775807)\\n  B2 = ops.prim.NumToTensor(torch.size(X1, 0))\\n  _209 = int(B2)\\n  _210 = int(B2)\\n  _211 = int(B2)\\n  _212 = int(B2)\\n  _213 = ops.prim.NumToTensor(torch.size(X1, 1))\\n  num_mini_batch2 = torch.floor_divide(_213, reminder_len)\\n  _214 = [_212, int(num_mini_batch2), _13, 128]\\n  X2 = torch.reshape(X1, _214)\\n  _215 = torch.floor_divide(_213, reminder_len)\\n  x18 = torch.reshape(XQ1, [_211, 4, int(_215), _12, 32])\\n  _216 = torch.floor_divide(_213, reminder_len)\\n  x19 = torch.reshape(XK1, [_210, 4, int(_216), _11, 32])\\n  _217 = torch.floor_divide(_213, reminder_len)\\n  x21 = torch.reshape(XV1, [_209, 4, int(_217), _10, 32])\\n  _218 = torch.einsum(\"bnkc,hdc->bhnkd\", [X2, learnable_ttt_lr_weight])\\n  _219 = torch.reshape(learnable_ttt_lr_bias, [1, -1, 1, 1, 1])\\n  input0 = torch.add(_218, _219)\\n  ttt_lr1 = torch.sigmoid(input0)\\n  ttt_lr2 = torch.permute(ttt_lr1, [0, 1, 2, 4, 3])\\n  ttt_lr_eta0 = torch.div(torch.mul(ttt_lr2, CONSTANTS.c4), CONSTANTS.c5)\\n  token_idx3 = torch.add(token_idx, learnable_token_idx)\\n  token_idx4 = torch.slice(token_idx3, 0, 0, _9)\\n  token_idx5 = torch.clamp_min(token_idx4, 0.)\\n  _220 = torch.reshape(token_idx5, [1, 1, 1, _8, 1])\\n  _221 = ops.prim.NumToTensor(torch.size(X2, 0))\\n  _222 = int(_221)\\n  _223 = ops.prim.NumToTensor(torch.size(X2, 1))\\n  token_eta0 = torch.broadcast_to(_220, [_222, 4, int(_223), _7, 1])\\n  x22 = torch.mul(token_eta0, ttt_lr_eta0)\\n  B3 = ops.prim.NumToTensor(torch.size(x21, 0))\\n  _224 = int(B3)\\n  _225 = int(B3)\\n  num_mini_batch3 = ops.prim.NumToTensor(torch.size(x21, 2))\\n  _226 = int(num_mini_batch3)\\n  _227 = ops.prim.NumToTensor(torch.size(x21, 2))\\n  _228 = ops.prim.NumToTensor(torch.size(x21, 3))\\n  L1 = torch.mul(_227, _228)\\n  _229 = int(L1)\\n  tensor3 = torch.permute(x18, [2, 0, 1, 3, 4])\\n  tensor4 = torch.permute(x19, [2, 0, 1, 3, 4])\\n  tensor5 = torch.permute(x21, [2, 0, 1, 3, 4])\\n  tensor6 = torch.permute(x22, [2, 0, 1, 3, 4])\\n  out0 = torch.empty([_226, _225, 4, _6, 32], dtype=6, layout=None, device=torch.device(\"cpu\"), pin_memory=False)\\n  XQ_mini_batch5 = torch.select(tensor3, 0, 0)\\n  XK_mini_batch5 = torch.select(tensor4, 0, 0)\\n  XV_mini_batch5 = torch.select(tensor5, 0, 0)\\n  eta_mini_batch5 = torch.select(tensor6, 0, 0)\\n  _230 = torch.matmul(XK_mini_batch5, W1_last4)\\n  x23 = torch.add(_230, b1_last4)\\n  l2_target5 = torch.sub(XV_mini_batch5, XK_mini_batch5)\\n  gamma5 = torch.reshape(ttt_norm_weight, [4, 1, 32])\\n  beta5 = torch.reshape(ttt_norm_bias, [4, 1, 32])\\n  D5 = ops.prim.NumToTensor(torch.size(x23, 3))\\n  mu11 = torch.mean(x23, [-1], True)\\n  var11 = torch.var(x23, [-1], False, True)\\n  std11 = torch.sqrt(torch.add(var11, CONSTANTS.c6))\\n  x_hat11 = torch.div(torch.sub(x23, mu11), std11)\\n  y11 = torch.add(torch.mul(gamma5, x_hat11), beta5)\\n  grad_output5 = torch.sub(y11, l2_target5)\\n  grad_x_hat5 = torch.mul(grad_output5, gamma5)\\n  _231 = torch.mul(torch.reciprocal(D5), CONSTANTS.c4)\\n  _232 = torch.sub(torch.mul(D5, grad_x_hat5), torch.sum(grad_x_hat5, [-1], True))\\n  _233 = torch.sum(torch.mul(grad_x_hat5, x_hat11), [-1], True)\\n  _234 = torch.sub(_232, torch.mul(x_hat11, _233))\\n  grad_l_wrt_Z15 = torch.div(torch.mul(_231, _234), std11)\\n  _235 = torch.transpose(XK_mini_batch5, -2, -1)\\n  Attn15 = torch.tril(torch.matmul(XQ_mini_batch5, _235))\\n  _236 = torch.matmul(torch.tril(eta_mini_batch5), grad_l_wrt_Z15)\\n  b1_bar5 = torch.sub(b1_last4, _236)\\n  _237 = torch.matmul(XQ_mini_batch5, W1_last4)\\n  _238 = torch.matmul(torch.mul(eta_mini_batch5, Attn15), grad_l_wrt_Z15)\\n  x24 = torch.add(torch.sub(_237, _238), b1_bar5)\\n  mu12 = torch.mean(x24, [-1], True)\\n  var12 = torch.var(x24, [-1], False, True)\\n  std12 = torch.sqrt(torch.add(var12, CONSTANTS.c6))\\n  x_hat12 = torch.div(torch.sub(x24, mu12), std12)\\n  Z1_bar5 = torch.add(torch.mul(gamma5, x_hat12), beta5)\\n  y12 = torch.add(XQ_mini_batch5, Z1_bar5)\\n  _239 = torch.copy_(torch.select(out0, 0, 0), y12)\\n  XQW_batch0 = torch.permute(out0, [1, 0, 3, 2, 4])\\n  output_reminder = torch.reshape(XQW_batch0, [_224, _229, 128])\\n  input1 = torch.cat([output_mod, output_reminder], 1)\\n  _240 = (o_proj).forward((post_norm).forward(input1, ), )\\n  return _240\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis.nodes\n",
    "# # TODO: 1. modularize everything; 2. track the flow of tensor\n",
    "analysis.nodes['seq_modeling_block'].graph_module.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
